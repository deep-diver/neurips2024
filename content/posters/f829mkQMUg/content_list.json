[{"type": "text", "text": "Boundary Decomposition for Nadir Objective Vector Estimation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruihao Zheng Zhenkun Wang\u2217 ", "page_idx": 0}, {"type": "text", "text": "School of System Design and Intelligent Manufacturing, Southern University of Science and Technology 12132686@mail.sustech.edu.cn, wangzhenkun90@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The nadir objective vector plays a key role in solving multi-objective optimization problems (MOPs), where it is often used to normalize the objective space and guide the search. The current methods for estimating the nadir objective vector perform effectively only on specific MOPs. This paper reveals the limitations of these methods: exact methods can only work on discrete MOPs, while heuristic methods cannot deal with the MOP with a complicated feasible objective region. To fill this gap, we propose a general and rigorous method, namely boundary decomposition for nadir objective vector estimation (BDNE). BDNE scalarizes the MOP into a set of boundary subproblems. By utilizing bilevel optimization, boundary subproblems are optimized and adjusted alternately, thereby refining their optimal solutions to align with the nadir objective vector. We prove that the bilevel optimization identifies the nadir objective vector under mild conditions. We compare BDNE with existing methods on various black-box MOPs. The results conform to the theoretical analysis and show the significant potential of BDNE for real-world application. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The multi-objective optimization problem (MOP) can be written as ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}.}&{\\mathbf{f}(\\mathbf{x})=(f_{1}(\\mathbf{x}),\\ldots,f_{m}(\\mathbf{x}))^{\\top},}\\\\ {\\mathrm{s.t.}}&{\\mathbf{x}\\in\\Omega,}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where ${\\mathbf x}\\,=\\,(x_{1},\\ldots,x_{n})^{\\intercal}$ is the decision vector (also called solution), and $\\Omega\\,\\subset\\,\\mathbb{R}^{n}$ denotes the feasible region. f $\\colon\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ is composed of $m$ objective functions, and $\\mathbf{f}\\left(\\mathbf{x}\\right)$ is the objective vector corresponding to $\\mathbf{x}$ . ", "page_idx": 0}, {"type": "text", "text": "Definition 1. Given two vectors $\\mathbf{u}$ , $\\mathbf{v}\\in\\mathbb{R}^{m}$ , u is said to dominate v (denoted as $\\mathbf{u}\\prec\\mathbf{v},$ ), if and only i $\\mathrm{:}_{u_{i}}\\leq v_{i}$ for every $i\\in\\{1,\\ldots,m\\}$ and $u_{j}<v_{j}$ for at least one $j\\in\\{1,\\ldots,m\\}$ . ", "page_idx": 0}, {"type": "text", "text": "Definition 2. A decision vector $\\mathbf{x}^{*}$ and the corresponding objective vector f $\\left(\\mathbf{x}^{*}\\right)$ are Pareto-optimal, if there is no $\\mathbf{x}\\in\\Omega$ such that $\\mathbf{f}(\\mathbf{x})$ dominates $\\mathbf{f}\\left(\\mathbf{x}^{*}\\right)$ according to Definition $^{\\,l}$ . ", "page_idx": 0}, {"type": "text", "text": "Definition 3. A decision vector $\\mathbf{x}^{\\ast}\\in\\Omega$ and the corresponding objective vector f $\\left(\\mathbf{x}^{*}\\right)$ are weakly Pareto-optimal, if there does not exist another decision vector $\\mathbf{x}\\in\\Omega$ such that $f_{i}(\\mathbf{x})<f_{i}(\\mathbf{x}^{*})$ for all $i=1,\\hdots,m$ . ", "page_idx": 0}, {"type": "text", "text": "Definition 4. The set of all Pareto-optimal solutions is called the Pareto set (denoted as $P S$ ), and the set of all Pareto-optimal objective vectors is called the Pareto front (denoted as $P F$ ). ", "page_idx": 0}, {"type": "text", "text": "Definition 5. The relative complement of the $P F$ in the set of all weakly Pareto-optimal objective vectors is called the weakly Pareto-optimal boundary (denoted as $W P B$ ). ", "page_idx": 0}, {"type": "text", "text": "Definition 6. The ideal objective vector $\\mathbf{z}^{i d e}$ is composed of the lower bounds of the $P F$ , i.e., $z_{i}^{i d e}=\\mathrm{min}_{\\mathbf{x}\\in P S}\\;f_{i}(\\mathbf{x})$ for $i=1,\\hdots,m$ . The nadir objective vector $\\mathbf{z}^{n a d}$ consists of the upper bounds of the $P F$ , i.e., $z_{i}^{n a d}=\\operatorname*{max}_{\\mathbf{x}\\in P S}f_{i}(\\mathbf{x})\\,f\\!o r\\,i=1,\\ldots,m.$ ", "page_idx": 1}, {"type": "text", "text": "Definition 7. For the $i$ -th objective function $f_{i}$ with $i\\,\\in\\,\\{1,\\ldots,m\\}$ , an objective vector ${\\mathbf z}^{(i)^{c}}$ is called the critical point of $f_{i}$ if it is Pareto-optimal and has the worst $f_{i}$ value, i.e., ${\\mathbf z}^{(i)^{c}}\\ \\in$ $\\{\\mathbf{f}(\\mathbf{x})|\\mathbf{x}=\\arg\\operatorname*{max}_{\\mathbf{x}\\in P S}f_{i}(\\mathbf{x})\\}$ . The nadir objective vector is often determined by obtaining the critical point on each objective, since $z_{i}^{n a d}=z_{i}^{(i)^{c}}$ for each $i\\in\\{1,\\ldots,m\\}$ . ", "page_idx": 1}, {"type": "text", "text": "The nadir objective vector is a fundamental concept of multi-objective optimization and has been widely applied to real-world problems [1, 2, 3]. Specifically, numerous optimization methods\u2019 operations involve the nadir objective vector. Firstly, the nadir objective vector is often used to guide the search in various tasks, including both continuous MOPs [4, 5] and discrete MOPs [6, 7]. An inaccurate estimation of the nadir objective vector degrades the performance of exact algorithms [8], evolutionary algorithms [9], and multi-objective learning [10]. Secondly, the nadir objective vector offers a comprehensive view of the $P F$ for decision-makers, which can facilitate the application of preference-based algorithms [11, 12, 2]. For example, an accurate nadir objective vector is the assumption of many interactive algorithms. A poor approximation may cause biased decisions. In addition, the nadir objective vector and the ideal objective vector are widely used to normalize the objective space [13]. Normalization with an inappropriately estimated nadir objective vector can cause performance deterioration of the optimization algorithm [14, 15]. ", "page_idx": 1}, {"type": "text", "text": "The ideal objective vector can be acquired by minimizing each objective separately. Unfortunately, obtaining the nadir objective vector is much more complicated [16, 17]. Existing exact methods are developed for discrete MOPs, posing significant challenges for their extension to other MOPs. The remaining methods are heuristic, showing satisfactory performance on simple MOPs. For example, DTLZ1 has an equilateral-triangle-shaped $P F$ , and its nadir objective vector can be easily estimated using several heuristic methods [9]. However, when the MOP possesses a complicated feasible objective region (e.g., an irregular $P F$ and the $W P B$ ), all heuristic methods fail to approximate the nadir objective vector accurately. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we first demonstrate the shortcomings of existing methods in detail. After that, we propose a general method with theoretical guarantees called boundary decomposition for nadir objective vector estimation (BDNE). We implement BDNE for black-box MOPs and use 28 blackbox problems to validate its performance. The results indicate that BDNE remarkably outperforms the existing methods. The major contributions of BDNE are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Scalarization method for boundary decomposition. We define the boundary subproblem that converts an objective vector into a scalar via boundary weight vectors. We prove that the critical point of each objective can be found by optimizing a particular boundary subproblem under mild conditions (i.e., the critical point satisfies proper Pareto optimality). We also prove that the optimal solution to any boundary subproblem is Pareto-optimal, thereby facilitating the search of the particular boundary subproblems.   \n\u2022 Bilevel optimization based on boundary decomposition. We formulate a bilevel optimization problem for each objective, aiming to identify the particular boundary subproblem by comparing the optimal solutions of boundary subproblems. The upper-level optimization seeks each objective\u2019s boundary weight vector that maximizes the objective function value; the lower-level optimization searches for the optimal solutions of given boundary subproblems. Besides, the trade-off of decision-makers can be involved in finding a satisfactory nadir objective vector. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Existing nadir objective vector estimation methods can be divided into two categories: 1) exact methods and 2) heuristic methods. ", "page_idx": 1}, {"type": "text", "text": "Exact Method. Exact methods are all designed for discrete MOPs and cannot be applied to other problems. For example, the methods in [18, 19] assume that the objective function values are integers, limiting their applicability to continuous problems. Moreover, some exact methods are exclusively designed for multi-objective integer linear programming [20]. m bilevel optimization problems are formulated in [21] and guarantee that their optimal solutions construct the nadir objective vector. The bilevel optimization is implemented solely for the discrete search space where an exhaustive search is available. However, the exact solver is often unavailable for many problems such as non-linear continuous ones, leading to uncertain optimality gaps. As a result, the pay-off table, which this method relies heavily on, may be tough to obtain. In addition, its lower-level optimization, including two stages, may exhibit significant unreliability and high computational costs. Another significant limitation of exact methods is their inability to solve beyond small-scale problems within reasonable runtimes, thus posing substantial challenges to their real-world applicability. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Heuristic Method. Heuristic methods readily apply to diverse MOPs yet lack theoretical guarantees. Generally, the heuristic method estimates the nadir objective vector as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{z}_{i}^{n a d}=\\operatorname*{max}_{\\mathbf{x}\\in S}f_{i}(\\mathbf{x})\\;\\mathrm{for}\\;i=1,\\dots,m,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $S$ is an iteratively improved solution set. $S$ can be specified as the population of the multiobjective evolutionary algorithm (MOEA) [22] (denoted as SF1). When the population is the $P S$ , Eq. (2) becomes the definition of the nadir objective vector. That is, SF1 can achieve the nadir objective vector under the ideal situation where the population is the $P S$ . However, this is almost impossible because retaining inferior solutions, such as weakly Pareto-optimal or dominance-resistant ones, in the population is often inevitable [23]. In case the population contains these inferior solutions, the SF1-based algorithm may be severely misled and fail to estimate the nadir objective vector, as shown in Figure 2b and Figure 2f (see Section 4). Several methods are proposed to mitigate the impact of inferior solutions by selecting a subset from the population [24, 25, 26]. Additionally, $S$ can be obtained by identifying extreme points in the population. In [27, 28, 29], the extreme points are determined by finding the objective vector with minimum values on each objective function. This method is also known as the pay-off table method, which can overestimate or underestimate the nadir objective vector [30, 20]. Some methods identify the objective vectors that are closest to axis vectors as extreme points. The distance between the objective vector and the axis vector can be measured by the Minkowski distance (e.g., $L_{2}$ [31] and $L_{\\infty}$ [32]), the perpendicular distance [33], and the cosine similarity [9]. In [34], each objective is associated with a single-objective optimization subproblem to determine $m$ extreme points. In Appendix A, we use an example to illustrate the deficiencies of heuristic methods in estimating the nadir objective vector. We can observe from Figure 4 and Table 4 that all methods incorrectly estimate the critical points as well as the nadir objective vector. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To address the limited applicability of exact methods and the unreliability of heuristic methods, we propose a method with general applicability to any MOP and theoretical guarantees in this section. Specifically, our method does not necessitate the objective function value to be an integer as opposed to [18, 19] and involves simpler optimization tasks compared to [21]. Our method also enables finding the nadir objective vector, unlike heuristic methods. Furthermore, our method supports using a user-defined trade-off. In the following, we begin by introducing boundary decomposition and establishing its theoretical foundations. After that, we develop a bilevel optimization method based on boundary decomposition to achieve alignment with the nadir objective vector. This method is then implemented for the black-box MOP, and its effectiveness is evaluated in the subsequent section. ", "page_idx": 2}, {"type": "text", "text": "3.1 Boundary Subproblem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We define a boundary subproblem for the $i$ -th objective as ", "page_idx": 2}, {"type": "equation", "text": "$$\ng_{i}^{b d}(\\mathbf{x}|\\mathbf{w}^{i},\\mathbf{z}^{r},\\alpha)=\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\left((1-\\alpha)f_{j}(\\mathbf{x})+\\frac{\\alpha}{m}\\sum_{k=1}^{m}f_{k}(\\mathbf{x})-z_{j}^{r}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{w}^{i}$ is called the boundary weight vector of the $i$ -th objective, ${\\mathbf z}^{r}$ is a reference point, and $0\\,<\\,\\alpha\\,<\\,1$ . $\\mathbf{w}^{i}$ satisfies three conditions: 1) $w_{i}^{i}\\,=\\,0;\\,2)\\;\\breve{\\forall j}\\;\\in\\;\\{1,\\dots,m\\},w_{j}^{i}\\;\\geq\\,0;\\,3)\\;\\exists\\;j\\;\\in$ $\\underline{{\\{1,}}}\\ldots,m\\},w_{j}^{i}>0$ . The definition of the boundary subproblem is inspired by the modified weighted Tchebycheff metric [35]. The contour surface of the boundary subproblem is illustrated in Figure 1. Additionally, several examples are presented to demonstrate the optimal objective vector of the boundary subproblem. In the following, we provide the theoretical foundations. Claims are followed by corresponding explanations. All proofs are presented in Appendix $\\boldsymbol{\\mathrm{F}}$ . ", "page_idx": 2}, {"type": "image", "img_path": "f829mkQMUg/tmp/cee9e7f0808a381f39bf3df9979f4eac14fa90eb5ea7926e37d70d4b1660b32e.jpg", "img_caption": ["Figure 1: The contour surfaces of boundary subproblems. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Definition 8 (From [36]). Let $\\tilde{\\mathbf{z}}$ be the image of an objective vector z, where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{z}_{i}=(1-\\alpha)z_{i}+\\frac{\\alpha}{m}\\sum_{j=1}^{m}z_{j}\\ f o r\\ i=1,\\dots,m\\ \\ (0<\\alpha<1).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given a value of $\\alpha$ and two objective vectors u and v, u is said to cone-dominate $\\mathbf{v}$ (denoted as $\\mathbf{u}\\prec^{c}\\mathbf{v})$ if and only if $\\tilde{\\mathbf{u}}\\prec\\tilde{\\mathbf{v}}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 9. Given a value of $\\alpha$ , an objective vector is cone-optimal if no objective vector can cone-dominate it. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. The optimal objective vector to the boundary subproblem with $\\mathbf{w}^{i}$ (denoted as $\\mathbf{z}^{\\ast}$ ) must be Pareto-optimal, if and only $i f\\exists j\\in\\{1,\\dots,m\\}\\setminus\\{i\\}$ such that $z_{j}^{r}\\le\\tilde{z}_{j}^{*}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Let $z_{j}^{r}\\,\\le\\,\\tilde{z}_{j}^{(i)^{c}}$ for every $j\\,\\in\\,\\{1,\\ldots,m\\}\\,\\setminus\\,\\{i\\}$ . ${\\mathbf z}^{(i)^{c}}$ uniquely solves the boundary subproblem with ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{j}^{i}=\\left\\{\\frac{0,}{\\widetilde{z}_{j}^{(i)^{c}}-z_{j}^{r}},\\ \\ j=i,\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "if and only $i f\\mathbf{z}^{(i)^{c}}$ is cone-optimal. ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. To ensure the validity of Theorem 2 without further information, it is necessary to satisfy the condition $z_{j}^{r}\\le\\tilde{z}_{j}^{(i)^{c}}$ for every $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Firstly, we reveal the connection between the boundary subproblem and the nadir objective vector. The boundary subproblem involves two steps: transforming the given objective vector and scalarizing the transformed objective vector with the boundary weight vector. The transformation implies cone domination, which defines a strict partial order (see Appendix F.1 for details) and belongs to the family of generalized Pareto domination [37]. We assume that the critical points are cone-optimal and a set of objective vectors is obtained by optimizing infinitely sampling boundary subproblems. According to Theorems 1 and 2, this set is a subset of the $P F$ and the critical points are included in it. Finally, the nadir objective vector can be identified by Eq. (2) where $S$ is specified as the obtained set. ", "page_idx": 3}, {"type": "text", "text": "Definition 10 (From [38]). A decision vector $\\mathbf{x}^{*}$ and the corresponding objective vector $\\mathbf{f}\\left(\\mathbf{x}^{*}\\right)$ are properly Pareto-optimal if they are Pareto-optimal and if there exists a finite number $M>0$ such that, for each $i$ and any $\\mathbf{x}\\in\\Omega$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{f_{i}(\\mathbf{x}^{*})-f_{i}(\\mathbf{x})}{f_{j}(\\mathbf{x})-f_{j}(\\mathbf{x}^{*})}\\leq M,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $j$ satisfies $f_{j}(\\mathbf{x}^{*})<f_{j}(\\mathbf{x})$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 3. An objective vector is cone-optimal, if and only if the objective vector is Pareto-optimal and the value of \u03b1 satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha\\leq\\frac{m}{(m-1)M+m}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Corollary 2. A cone-optimal objective vector is also properly Pareto-optimal. ", "page_idx": 3}, {"type": "text", "text": "The following question arises: under what conditions can a solution be considered cone-optimal? Let $\\mathbf{u}=(0,\\bar{1},\\bar{1})^{\\intercal}$ and $\\mathbf{v}=(0.2,0.5,0.5)^{\\intercal}$ be a critical point and Pareto-optimal objective vector, respectively. If $\\alpha=0.5$ , then $\\mathbf{v}\\prec^{c}\\mathbf{u}$ (since $\\tilde{\\mathbf{u}}=(1,2,2)^{\\intercal}$ and $\\tilde{\\mathbf{v}}=(0.8,1.1,1.1)^{\\top})$ ; If $\\alpha=0.1$ , then u $\\mathcal{k}^{c}\\,\\mathbf{v}$ and v $\\mathcal{k}^{c}\\,\\mathbf{u}$ (since $\\tilde{\\mathbf{u}}=(0.2,1.2,1.2)^{\\top}$ and $\\tilde{\\mathbf{v}}=(0.32,0.62,0.62)^{\\top}$ ). The critical point is cone-dominated if an excessive $\\alpha$ value is used. We should know how to set an appropriate $\\alpha$ to preserve the specific Pareto-optimal objective vectors. We find that cone domination is related to proper Pareto optimality. The idea of proper Pareto optimality is to divide the $P S$ into proper and improper ones. The definition of proper Pareto optimality is shown in Definition 10. That is, a solution can be considered properly Pareto-optimal when at least one pair of objectives satisfies: a finite decrement in one objective requires a reasonable increment in the other objective. We also can infer that different proper Pareto-optimal solutions may necessitate distinct minimum values of $M$ . Then we derive that $\\alpha$ is bounded by $M$ as shown in Theorem 3. Corollary 2 is established accordingly. We can let $\\alpha$ be a sufficiently small value such that the critical points are cone-optimal. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4. Let $\\mathbf{z}^{\\ast}$ be an objective vector satisfying $z_{j}^{r}\\le\\tilde{z}_{j}^{*}$ for every $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}.$ . If an objective vector $\\mathbf{z}^{\\prime}$ does not cone-dominate $\\mathbf{z}^{\\ast}$ and $z_{i}^{*}\\geq z_{i}^{\\prime},$ , then $\\mathbf{z}^{\\ast}$ has a lower function value than $\\mathbf{z}^{\\prime}$ with respect to some boundary subproblem of the $i$ -th objective. ", "page_idx": 4}, {"type": "text", "text": "Corollary 3. Let ${\\bf z}^{e}$ consist of the optimal values of $m$ BLOPs and $\\{{\\bf z}|{\\bf z}\\prec{\\bf z}^{e}\\}$ be the promising region. Any Pareto-optimal objective vector outside the promising region fails to address the trade-off of decision-makers, namely, $M>\\mu$ . ", "page_idx": 4}, {"type": "text", "text": "The remaining issue is that $M$ is not known in advance, making it difficult to set $\\alpha$ according to $M$ . We introduce a user-defined parameter $\\mu$ $(\\mu>0)$ ) and let $\\begin{array}{r}{\\alpha=\\frac{m}{(m-1)\\mu+m}}\\end{array}$ . $\\mu$ represents the upper bound of the trade-off between the $k$ -th and $l$ -th objectives, where $k=\\arg\\operatorname*{max}_{1\\leq j\\leq m}f_{j}(\\mathbf{x}^{*})-f_{j}(\\mathbf{x})$ and $l=\\arg\\operatorname*{max}_{1\\leq j\\leq m}f_{j}(\\mathbf{x})-f_{j}(\\mathbf{x}^{*})$ . In other words, $\\mu$ is the amount of increment in the value of one objective function that the decision-maker is willing to tolerate in exchange for a one-unit decrement in another objective function. If the preference of decision-makers about $\\mu$ is available, we have the guarantee shown in Corollary 3 derived from Theorems 3 and 4. Corollary 3 can guarantee to obtain a satisfactory nadir objective vector. When the preference is not provided, $\\mu$ can take a sufficiently large value. On the one hand, if a finite trade-off $M$ exists for the critical point, then $\\mu$ can take an appropriate value from $[M,\\infty)$ to obtain the nadir objective vector. On the other hand, $M\\rightarrow\\infty$ results in $\\alpha\\rightarrow0$ , which means the corresponding objective vector is not properly Pareto-optimal. In practical terms, a Pareto-optimal solution with a very large value of $M$ does not essentially differ from an inferior solution for decision-makers. If the critical points are improper, using a generally agreeable value of $\\mu$ is reasonable. ", "page_idx": 4}, {"type": "text", "text": "Theorem 5. The optimal objective vector to the boundary subproblem with $\\mathbf{w}^{i}$ (denoted as $\\mathbf{z}^{\\ast}$ ) must be cone-optimal (or properly Pareto-optimal), $i f\\exists j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ such that $z_{j}^{r}<\\tilde{z}_{j}^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 6. Let $z_{j}^{r}<\\tilde{z}_{j}^{*}$ for every $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ . If an objective vector $\\mathbf{z}^{\\ast}$ is optimal for the boundary subproblem with $\\mathbf{w}^{i}$ , $\\mathbf{z}^{\\ast}$ must be an optimal objective vector to the boundary subproblem with ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{j}^{*}=\\left\\{\\frac{0,}{\\tilde{z}_{j}^{*}-z_{j}^{r}},\\ \\ j=i,\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, we can make minor modifications to Theorem 1, resulting in Theorem 5. Theorem 6 can be deduced using Theorem 5. Theorem 6 reveals the relationship between the boundary subproblem\u2019s optimal solution and the boundary weight vector, which is useful for designing the algorithm in Section 3.3. ", "page_idx": 4}, {"type": "text", "text": "3.2 Nadir Objective Vector Estimation via Bilevel Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Practically, we cannot have infinite boundary subproblems. In this subsection, an optimization problem is defined to obtain the nadir objective vector. Its two goals are: 1) converging to Paretooptimal objective vectors and 2) identifying critical points from Pareto-optimal ones. In the 2-objective case, both goals can be achieved through a single optimization procedure. Specifically, the critical points can be obtained by optimizing boundary subproblems using $(1,0)^{\\intercal}$ and $(0,1)^{\\intercal}$ as the weight vectors. This is because solving the boundary subproblem with $\\mathbf{w}^{i}$ is equivalent to solving that with $\\beta\\mathbf{w}^{i}$ where $\\beta\\,>\\,0$ is a constant. Unfortunately, the critical points cannot be obtained by a fixed boundary subproblem when the MOP involves three or more objectives. That is, both goals cannot be achieved simultaneously. This motivates the formulation of a bilevel optimization problem (BLOP) for each objective to achieve the two goals: the lower-level optimization problem (LLOP) corresponds to the first goal while the upper-level optimization problem (ULOP) is for the second goal. The BLOP is formulated below, based on the boundary subproblem. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Firstly, $M$ is affected by different value ranges of objective functions, which might lead to algorithm performance deterioration [39]. The objective space should be normalized properly. We let $z_{j}^{r}=0$ for $j=1,\\dots,m$ in Eq. (3) and introduce two reference points ${\\mathbf z}^{r1}$ and ${\\mathbf{z}}^{r2}$ $(z_{j}^{r2}>z_{j}^{r1}$ for $j=1,\\dots,m)$ for normalization. Then, the boundary subproblem with normalization can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{i}^{b d n}(\\mathbf{x}|\\mathbf{w}^{i},\\mathbf{z}^{r1},\\mathbf{z}^{r2},\\alpha)=\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\left((1-\\alpha)f_{j}^{\\prime}(\\mathbf{x})+\\frac{\\alpha}{m}\\sum_{k=1}^{m}f_{k}^{\\prime}(\\mathbf{x})\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f_{k}^{\\prime}(\\mathbf{x})=\\left(f_{k}(\\mathbf{x})-z_{k}^{r1}\\right)/\\left(z_{k}^{r2}-z_{k}^{r1}\\right)$ for $k=1,\\hdots,m$ . In this formulation, ${\\mathbf z}^{r1}$ should be set according to Theorem 1 and Corollary 1. For example, it is reasonable to set ${\\mathbf z}^{r1}$ to $\\mathbf{z}^{i d e}$ since $z_{j}^{i d e}\\leq f_{j}(\\mathbf{x})$ for $j=1,\\dots,m$ such that $\\begin{array}{r}{(1-\\alpha)f_{j}^{\\prime}(\\mathbf{x})+\\frac{\\alpha}{m}\\sum_{k=1}^{m}f_{k}^{\\prime}(\\mathbf{x})\\geq0}\\end{array}$ km=1 f k\u2032(x) \u22650. Furthermore, zr2 should be set according to [40]. Secondly, several boundary subproblems might have the same optimal solution. On the one hand, solving the boundary subproblem with $\\mathbf{w}^{i}$ is equivalent to solving that with $\\beta\\mathbf{w}^{i}$ where $\\beta>0$ is a constant. We can let $\\textstyle\\sum_{j=1\\land j\\neq i}^{m^{-}}w_{j}^{i}=1$ . On the other hand, Theorem 6 indicates that the optimal objective vectors to some boundary subproblems may not align with the corresponding boundary weight vectors. For example, this scenario is common on discrete MOPs. We can penalize the flat landscape of the ULOP according to the distance between the boundary weight vector and its optimal objective vector. Let $\\mathbf{x}^{*}$ be the optimal solution of the boundary subproblem with $\\mathbf{w}^{i}$ . In this paper, we calculate the distance as $\\begin{array}{r}{d(\\mathbf{w}^{i},\\mathbf{x}^{*})=\\sqrt{\\sum_{j=1\\wedge j\\neq i}^{m}\\left(w_{j}^{i}-u_{j}/\\sum_{k=1}^{m}u_{k}\\right)^{2}}}\\end{array}$ where ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{j}=\\left\\{\\frac{0,}{(1-\\alpha)f_{j}^{\\prime}(\\mathbf{x})+\\frac{\\alpha}{m}\\sum_{k=1}^{m}f_{k}^{\\prime}(\\mathbf{x})},\\ \\ j=i,\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\epsilon$ be a sufficiently small constant, $l\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ , and $I=\\{1,\\dots,m\\}\\setminus\\{i,l\\}$ . Finally, the BLOP with respect to the $i$ -th objective for $i=1,\\hdots,m$ has the ULOP formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}.\\quad f_{i}^{u}(\\mathbf{w}^{i})=f_{i}(\\mathbf{x}^{*})-\\epsilon d(\\mathbf{w}^{i},\\mathbf{x}^{*}),}\\\\ {\\mathrm{s.t.}\\quad w_{i}^{i}=0,\\quad w_{l}^{i}=1-\\displaystyle\\sum_{j\\in I}w_{j}^{i},\\quad\\displaystyle\\sum_{j\\in I}w_{j}^{i}\\leq1,}\\\\ {\\displaystyle}&{0\\leq w_{j}^{i}\\leq1\\mathrm{~for~every~}j\\in I,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{x}^{*}$ is the optimal decision vector to the LLOP of the following form ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}.}&{{}f_{i}^{l}(\\mathbf{x})=g_{i}^{b d n}(\\mathbf{x}|\\mathbf{w}^{i},\\mathbf{z}^{r1},\\mathbf{z}^{r2},\\alpha),}\\\\ {\\mathrm{s.t.}}&{{}\\mathbf{x}\\in\\Omega.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The search is performed on the original decision space in the LLOP, while the search space of the ULOP can be viewed as $\\mathbb{R}^{m-2}$ (i.e., $(m-2)$ -dimensional Euclidean space). For the $i$ -th objective, the ULOP uses a boundary weight vector $\\mathbf{w}^{i}$ as a solution and requires maximizing the $i^{\\th}$ -th penalized objective function value of the Pareto-optimal solution obtained by the LLOP. In other words, a feasible solution of the ULOP requires an optimal solution of the LLOP. We propose an algorithm framework called BDNE for estimating the nadir objective vector on the MOP with more than two objectives. Specifically, BDNE aims to solve these $m$ BLOPs. The steps of BDNE are given in Algorithm 1, where $\\mu$ is the user-defined upper bound of the trade-off (see Section 3.1 for details). Three following issues should be specified: determine the reference points ${\\mathbf z}^{r1}$ and ${\\mathbf{z}}^{r2}$ , choose suitable single-objective optimizers, and set the stopping criteria. ", "page_idx": 5}, {"type": "text", "text": "3.3 Algorihm for Black-Box Multi-Objective Optimization Problems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To evaluate the viability of BDNE, we implement it for black-box MOPs. We adopt evolutionary algorithms as the solvers in BDNE. Each ULOP uses CMA-ES [41] to search the eligible boundary ", "page_idx": 5}, {"type": "text", "text": "Input: An MOP, stopping criteria of $m$ BLOPs, $\\mu$ Output: $\\hat{\\mathbf{z}}^{n a d}$ ", "page_idx": 6}, {"type": "text", "text": "1: Initialization: $\\alpha\\leftarrow\\frac{m}{(m-1)\\mu+m}$ ; determine ${\\mathbf z}^{r1}$ and ${\\mathbf{z}}^{r2}$ ; configure solvers.   \n2: while stopping criteria of $m$ ULOPs are not all satisfied do   \n3: Generate boundary weight vectors by the solvers of unstopped ULOPs.   \n4: Minimize LLOPs with the generated boundary weight vectors by the corresponding solvers.   \n5: Update parameters: ${\\mathbf z}^{r1}$ and ${\\mathbf{z}}^{r2}$ (optional); parameters in the solvers of unstopped ULOPs.   \n6: end while ", "page_idx": 6}, {"type": "text", "text": "weight vector(s). We utilize CMA-ES to optimize the ULOP for two reasons. First, CMA-ES is a state-of-the-art algorithm for single-objective black-box optimization. Second, optimal solutions to the LLOPs are not always available; instead, approximate solutions are often obtained. Consequently, the function values of the approximate solutions may exhibit noise. CMA-ES is suitable for this task as it demonstrates strong robustness in optimizing noisy functions [41]. In each iteration, $m$ ULOPs generate the pending LLOPs simultaneously. Then all LLOPs are solved collaboratively by the MOEA instead of optimizing each LLOP separately. This is because the superiority of MOEAs is demonstrated empirically and theoretically in solving multi-objective black-box optimization problems [42, 43]. ", "page_idx": 6}, {"type": "table", "img_path": "f829mkQMUg/tmp/c7b7c005ead805d8e286f347f0ffeebe25ddf6bc1fd694ec41829e9c906e00a0.jpg", "table_caption": ["Table 1: Notation used in Section 3.3. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Stopping Criteria and Reference Points. The maximal number of iterations is employed as the stopping criterion for the ULOP as well as the LLOP. We use $\\tau_{u}$ and $\\tau_{l}$ to denote the maximum number of iterations for the ULOP and the LLOP respectively. ${\\mathbf z}^{r1}$ is set to the current best objective function values. ${\\mathbf{z}}^{r2}$ is constructed by the current best objective function values of $m$ ULOPs. Initially, EC-NSGA-II [22] runs for $\\tau_{l}$ iterations. The minimum and maximum objective function values of its final population determine the settings of ${\\mathbf z}^{r1}$ and ${\\mathbf{z}}^{r2}$ . Then ${\\mathbf z}^{r1}$ is updated once a new solution is generated. ${\\mathbf{z}}^{r2}$ is only renewed when the parameters of CMA-ES update. During the optimization process, ${\\mathbf z}^{r1}$ and ${\\mathbf{z}}^{r2}$ iteratively approximate $\\mathbf{z}^{i d e}$ and $\\mathbf{z}^{n a d}$ respectively. ", "page_idx": 6}, {"type": "text", "text": "Upper-Level Optimization. $m$ ULOPs represents $m$ CMA-ES procedures. Let $\\textstyle\\iota={\\frac{N}{m}}-1$ . The population size of each CMA-ES procedure is $\\iota$ . That is, $(N-m)$ boundary weight vectors are used to align with the eligible ones and evenly assigned to $m$ objectives. Let $\\{\\bar{\\mathbf{w}^{i,j}},j^{\\overline{{\\mathbf{\\alpha}}}}=1,\\ldots,\\iota\\}$ denote the set of adjustable boundary weight vectors for the $i$ -th objective. Initially, most of them are sampled from the initial distribution of the CMA-ES procedure. The sampled points may need to be repaired to satisfy the constraints in the ULOP. For each objective, one boundary weight vector is initialized to the vector with $(m-1)$ elements being $\\frac{1}{m-1}$ (e.g., $\\left(0,\\frac{1}{2},\\frac{1}{2}\\right)^{\\top})$ . The remaining $m$ ones, which are $m$ different unit vectors, are fixed throughout the optimization of $m$ BLOPs. The LLOPs with these $m$ boundary weight vectors can motivate the search of $\\mathbf{z}^{i d e}$ . Besides, they enhance population diversity when the adjustable boundary subproblems become similar. When LLOPs are stopped, the parameters of each CMA-ES procedure are updated first according to the $\\iota$ boundary weight vectors. Let $\\mathbf{x}^{i,j}\\in A$ be the best solution so far for the subproblem with $\\mathbf{\\bar{w}}^{i,j}$ , and then the fitness of $\\mathbf{w}^{i,j}$ is defined as $\\left(\\displaystyle f_{i}\\left(\\mathbf{x}^{i,j}\\right)_{.}-\\epsilon d\\left(\\mathbf{w}^{i,j},\\mathbf{x}^{i,j}\\right)\\right)$ . Note that some boundary weight vectors, such as repaired ones, might not be directly generated from the current distribution of the CMA-ES procedure. These injected solutions should obey the injection rule [44, 45]. Subsequently, the boundary weight vectors of the $i$ -th objective are updated as follows: ", "page_idx": 6}, {"type": "text", "text": "Step 1 $W\\leftarrow\\{\\mathbf{w}^{i,j},j=1,\\dots,\\iota\\}$ and then delete $\\textstyle{\\left\\lfloor{\\frac{\\iota}{2}}\\right\\rfloor}$ the worst boundary weight vector in $W$ .   \nStep 2 Transform the best one in $W$ according to Eq. (10). ", "page_idx": 6}, {"type": "text", "text": "The updated ${\\mathbf{z}}^{r2}$ may substantially alter the normalized space in Eq. (9), potentially changing the optimal solution to the LLOP. As Theorem 6 reports, Step 2 can effectively preserve the best solution for the LLOP, irrespective of changes in the normalized space. Particularly, in the last iteration of the ULOP (i.e., the $\\tau_{u}$ -th iteration), $\\bar{\\mathbf{w}}^{i,1},\\dots,\\mathbf{w}^{i,\\iota}$ are all changed to the boundary weight vector with the best fitness for $i=1,\\hdots,m$ . More computational resources are allocated to the $i$ -th objective\u2019s current best LLOP in the final iteration. It helps to obtain a better approximation for these $m$ LLOPs. ", "page_idx": 7}, {"type": "text", "text": "Lower-Level Optimization. $N$ LLOPs are optimized by an MOEA with population size $N$ . An evolutionary algorithm executes solution reproduction and environmental selection iteratively. The reproduction procedure of the proposed MOEA includes two steps: ", "page_idx": 7}, {"type": "text", "text": "Step 1 Use binary tournament selection based on $\\pi$ to obtain the mating pool from $P$ . ", "page_idx": 7}, {"type": "text", "text": "Step 2 Apply reproduction operators to create an offspring set of size $N$ (denoted as $O$ ). ", "page_idx": 7}, {"type": "text", "text": "$\\pi$ is a utility vector including utility values of solutions in $P$ . A solution with a lower utility value is more likely to enter the mating pool. The selection procedure is performed as follows: ", "page_idx": 7}, {"type": "text", "text": "Step 1 Calculate $\\mathbf{r}=\\left(r_{1},\\ldots,r_{|P\\cup O|}\\right)^{\\intercal}$ for $P\\cup O$ . ", "page_idx": 7}, {"type": "text", "text": "Step 2 Select the smallest $N$ elements of $\\mathbf{r}$ and their corresponding solutions for the new $\\pi$ and the new generation of $P$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "Step 3 Identify the best solutions for the $N$ subproblems from $P\\cup O$ for the new generation of $A$ . ", "page_idx": 7}, {"type": "text", "text": "Given a decision vector $\\mathbf{x}^{k}\\in P\\cup O$ , we let $R_{i,j}^{k}$ be the ascending rank of $g_{i}^{b d n}(\\mathbf{x}^{k}|\\mathbf{w}^{i,j},\\mathbf{z}^{r1},\\mathbf{z}^{r2},\\alpha)$ within $\\left\\{g_{i}^{b d n}(\\mathbf{x}|\\mathbf{w}^{i,j},\\mathbf{z}^{r1},\\mathbf{z}^{r2},\\alpha)\\mid\\mathbf{x}\\in P\\cup O\\right\\}$ . $r_{k}$ is formulated as $r_{k}=\\operatorname*{min}_{(i,j)\\in I}\\{R_{i,j}^{k}\\}$ where $I=\\{(i,j)|i=1\\ldots,m,j=1,\\ldots,\\iota\\}$ . $\\mathbf{x}^{k}$ with a smaller $r_{k}$ implies a higher quality. ", "page_idx": 7}, {"type": "text", "text": "4 Experimental Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Instances. We proposed 4 scalable test problems denoted as TN1-TN4. The feasible objective regions of TN1 and TN2 are shown in Figure 2a and Figure 2e. The $P F$ of TN1 is a $(m-1)$ -dimensional simplex. TN2 has a concave $P F$ constructed by two $(m-1)$ -dimensional simplices. TN3 and TN4 are the modified versions of TN1 and TN2 respectively. Their objective functions have different value ranges. Details of TN1-TN4 are available in Appendix B. Moreover, we select problems with different shapes of feasible objective regions, including 6 existing test problems [46, 47, 48, 49] (DTLZ3, mDTLZ3, MaF2, DTLZ5, IMOP4, and IMOP6) and 4 real-world problems [50, 51, 52] (MP-DMP, ML-DMP, RE3-4-7, and RE5-3-1). We consider test problems with 3, 5, and 8 objectives, which accordingly have 8, 12, and 16 variables. The $P F$ s of RE3-4-7 and RE5-3-1 are unknown and represented by their current best solution sets. ", "page_idx": 7}, {"type": "text", "text": "Comparison Algorithms2. BDNE is compared with 2 representative heuristic algorithms for black-box MOPs: ECRNSGA-II [54] (based on SF1) and DNPE [34] (based on EP6). ECR-NSGA-II is parameter-free. DNPE is configured according to the corresponding reference (i.e., $\\lambda=100$ ). For BDNE, we set $\\mu\\,=\\,100$ and $\\tau_{l}\\,=\\,200$ . Then $\\tau_{u}\\,=\\,14$ according to the setting of $\\tau_{l}$ and $F E_{m a x}$ . The general algorithm settings are summarized in Table 2, where $F E_{m a x}$ means maximum number of function evaluations. Each algorithm is executed 30 times on each instance. Additionally, the computer resources and algorithm runtimes can be found in Appendix C. ", "page_idx": 7}, {"type": "table", "img_path": "f829mkQMUg/tmp/83c6fa914e586c1b0611c06fbc0cd94f84cb9b55e0ba500e953e96ef6484007f.jpg", "table_caption": ["Table 2: General algorithm settings. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Performance Metric. The estimated nadir objective vector is extracted by Eq. (2) where $S$ is the final population. Let $\\hat{z}_{i}^{n a d}$ be the estimated nadir objective vector. The error metric value is computed by $\\begin{array}{r}{E=\\sqrt{\\sum_{i=1}^{m}\\left((z_{i}^{n a d}-\\hat{z}_{i}^{n a d})/(z_{i}^{n a d}-z_{i}^{i d e})\\right)^{2}}}\\end{array}$ . The result table records the mean metric value and the standard deviation across all runs for each instance. The performance rank on each instance is inside parentheses. $\"+\"$ , $\\bullet\\bullet=\\ '\\,}$ or \u201c-\u201d denotes that the performance of the corresponding algorithm is statistically better than, similar to, or worse than that of BDNE based on Wilcoxon\u2019s rank sum test at 0.05 significant level. \u201c $\\mathbf{\\nabla}\\Delta^{\\bullet}\\mathbf{\\Phi}$ indicates the gap between the mean metric value of the corresponding algorithm and that of BDNE. The best mean metric values are also emphasized. ", "page_idx": 7}, {"type": "table", "img_path": "f829mkQMUg/tmp/fc141ede53ce9887cd015c4ca5914b95e305723bea9ea4830f18586c5eb6afc2.jpg", "table_caption": ["Table 3: Comparisons of error metric values among ECR-NSGA-II, DNPE, and BDNE. "], "table_footnote": ["\u2020The approximate $P F$ may be the reason for the large error on CRE5-3-1 since the results of DNPE and BDNE are similar. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 3 shows the statistical results on 28 instances. Across most instances, BDNE outperforms the other algorithms and has mean metric values below $5\\%$ . In Figure 2, we can see that BDNE accurately approximates at least one critical point for each objective, consistent with our theoretical analysis. DNPE ranks second. DNPE has competitive results on MaF2, DTLZ5, DTLZ2, mDTLZ2, IMOP6, 3-objective ML-DMP, and CRE5-3-1. We can infer that the extreme points determined by DNPE are very close to the critical points of these MOPs. Nevertheless, the performance of DNPE deteriorates on other instances. Besides, comparing the results between TN1 and TN3, value ranges of objective functions significantly impact DNPE but hardly affect BDNE. ECR-NSGA-II achieves the worst overall performance. Moreover, it has large standard deviations on many instances, which indicates its highly unstable performance. This is because ECR-NSGA-II suffers from dominance-resistant ", "page_idx": 8}, {"type": "image", "img_path": "f829mkQMUg/tmp/c1e45610a4a57bb58fdccc26f39a66f49ba07fe4ac02739fee955d52244e0d39.jpg", "img_caption": ["Figure 2: Plots of the final solution sets with median error metric values. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "solutions. For example, ECR-NSGA-II retains solutions close to the $W P B$ in Figure 2b and Figure 2f.   \nEach of these solutions has at least one inferior objective function value. ", "page_idx": 9}, {"type": "text", "text": "In general, the two algorithms have huge performance gaps compared with BDNE. The effectiveness and superiority of BDNE are demonstrated. More experiments are presented in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion, Limitation, and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion. In this paper, we have revealed the deficiency of existing methods in estimating the nadir objective vector. Specifically, exact methods suffer from limited applicability and high computational costs, while the irregular $P F$ and the $W P B$ can cause significant challenges for heuristic methods. We have proposed a new scalarization method, which can define specific boundary subproblems to find the nadir objective vector under mild conditions. We have formulated $m$ BLOPs using boundary subproblems and designed a corresponding algorithm framework called BDNE. We have also conducted experimental studies to validate the effectiveness of BDNE. In experiments, BDNE adopts evolutionary algorithms and effectively approximates the nadir objective vectors of various black-box MOPs. ", "page_idx": 9}, {"type": "text", "text": "Limitation and Future Work. The estimated nadir objective vector can be obtained beforehand or improved with the optimization process. In the paper, we present BDNE as an independent algorithm (i.e., estimate the nadir objective vector beforehand). In the future, we will investigate how to integrate BDNE into the iteration of an algorithm to enhance its overall performance (see Appendix E.4 for some pilot studies). Furthermore, BDNE has been implemented in a general manner. We plan to refine BDNE for specific applications, including multi-objective discrete optimization problems. Potential societal impacts can be found in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (Grant No. 62106096 and Grant No. 62476118), the Natural Science Foundation of Guangdong Province (Grant No. 2024A1515011759), the National Natural Science Foundation of Shenzhen (Grant No.JCYJ20220530113013031). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Asiri Umenga Weerasuriya, Xuelin Zhang, Jiayao Wang, Bin Lu, Kam Tim Tse, and Chun-Ho Liu. Performance evaluation of population-based metaheuristic algorithms and decision-making ", "page_idx": 9}, {"type": "text", "text": "for multi-objective optimization of building design. Building and Environment, 198:107855, 2021.   \n[2] R Mena, M Godoy, C Catal\u00e1n, P Viveros, and Enrico Zio. Multi-objective two-stage stochastic unit commitment model for wind-integrated power systems: A compromise programming approach. International Journal of Electrical Power & Energy Systems, 152:109214, 2023.   \n[3] Mostafa Ekhtiari, Mostafa Zandieh, and Erfan Babaee Tirkolaee. Optimizing the dam site selection problem considering sustainability indicators and uncertainty: An integrated decisionmaking approach. Journal of Cleaner Production, 428:139240, 2023.   \n[4] Xinye Cai, Zhiwei Mei, and Zhun Fan. A decomposition-based many-objective evolutionary algorithm with two types of adjustments for direction vectors. IEEE Transactions on Cybernetics, 48(8):2335\u20132348, 2018.   \n[5] Yingbo Xie, Shengxiang Yang, Ding Wang, Junfei Qiao, and Baocai Yin. Dynamic transfer reference point-oriented MOEA/D involving local objective-space knowledge. IEEE Transactions on Evolutionary Computation, 26(3):542\u2013554, 2022.   \n[6] Jiahai Wang, Taiyao Weng, and Qingfu Zhang. A two-stage multiobjective evolutionary algorithm for multiobjective multidepot vehicle routing problem with time windows. IEEE Transactions on Cybernetics, 49(7):2467\u20132478, 2019.   \n[7] Ilg\u0131n Dog\u02d8an, Banu Lokman, and Murat K\u00f6ksalan. Representing the nondominated set in multiobjective mixed-integer programs. European Journal of Operational Research, 296(3):804\u2013818, 2022.   \n[8] Mariana Mesquita-Cunha, Jos\u00e9 Rui Figueira, and Ana Paula Barbosa-P\u00f3voa. New $\\epsilon$ -constraint methods for multi-objective integer linear programming: A Pareto front representation approach. European Journal of Operational Research, 306(1):286\u2013307, 2023.   \n[9] Yanan Sun, Bing Xue, Mengjie Zhang, and Gary G Yen. A new two-stage evolutionary algorithm for many-objective optimization. IEEE Transactions on Evolutionary Computation, 23(5):748\u2013761, 2019.   \n[10] Xiaoyuan Zhang, Xi Lin, Bo Xue, Yifan Chen, and Qingfu Zhang. Hypervolume maximization: A geometric view of Pareto set learning. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \n[11] Jurgen Branke, J\u00fcrgen Branke, Kalyanmoy Deb, Kaisa Miettinen, and Roman Slowi\u00b4nski. Multiobjective optimization: Interactive and evolutionary approaches, volume 5252. Springer, 2008.   \n[12] Handing Wang, Markus Olhofer, and Yaochu Jin. A mini-review on preference modeling and articulation in multi-objective optimization: current status and challenges. Complex & Intelligent Systems, 3:233\u2013245, 2017.   \n[13] Kaisa Miettinen. Nonlinear multiobjective optimization. Springer, 1998.   \n[14] Oleg Grodzevich and Oleksandr Romanko. Normalization and other topics in multi-objective optimization. In Proceedings of the Fields-MITACS Industrial Problems Workshop, pages 89\u2013101, 2006.   \n[15] Linjun He, Hisao Ishibuchi, Anupam Trivedi, Handing Wang, Yang Nan, and Dipti Srinivasan. A survey of normalization methods in multiobjective evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 25(6):1028\u20131048, 2021.   \n[16] Kalyanmoy Deb and Kaisa Miettinen. A review of nadir point estimation procedures using evolutionary approaches: A tale of dimensionality reduction. In Proceedings of the Multiple Criterion Decision Making Conference (MCDM), 2009.   \n[17] Julian Blank, Kalyanmoy Deb, and Proteek Chandan Roy. Investigating the normalization procedure of NSGA-III. In International Conference on Evolutionary Multi-Criterion Optimization (EMO), pages 229\u2013240. Springer, 2019.   \n[18] Murat K\u00f6ksalan and Banu Lokman. Finding nadir points in multi-objective integer programs. Journal of Global Optimization, 62:55\u201377, 2015.   \n[19] \u00d6zg\u00fcr \u00d6zpeynirci. On nadir points of multiobjective integer programming problems. Journal of Global Optimization, 69:699\u2013712, 2017.   \n[20] Natashia Boland, Hadi Charkhgard, and Martin Savelsbergh. A new method for optimizing a linear function over the efficient set of a multiobjective integer program. European Journal of Operational Research, 260(3):904\u2013919, 2017.   \n[21] Gokhan Kirlik and Serpil Say\u0131n. Computing the nadir point for multiobjective discrete optimization problems. Journal of Global Optimization, 62:79\u201399, 2015.   \n[22] Kalyanmoy Deb, Kaisa Miettinen, and Shamik Chaudhuri. Toward an estimation of nadir objective vector using a hybrid of evolutionary and local search approaches. IEEE Transactions on Evolutionary Computation, 14(6):821\u2013841, 2010.   \n[23] Kokolo Ikeda, Hajime Kita, and Shigenobu Kobayashi. Failure of pareto-based moeas: Does non-dominated really mean near to optimal? In IEEE Congress on Evolutionary Computation (CEC), volume 2, pages 957\u2013962. IEEE, 2001.   \n[24] Xinye Cai, Zhiwei Mei, Zhun Fan, and Qingfu Zhang. A constrained decomposition approach with grids for evolutionary multiobjective optimization. IEEE Transactions on Evolutionary Computation, 22(4):564\u2013577, 2018.   \n[25] Mengzhen Wang, Fangzhen Ge, Debao Chen, and Huaiyu Liu. A many-objective evolutionary algorithm with adaptive convergence calculation. Applied Intelligence, 53(14):17260\u201317291, 2023.   \n[26] Rammohan Mallipeddi, Kedar Nath Das, et al. A twin-archive guided decomposition based multi/many-objective evolutionary algorithm. Swarm and Evolutionary Computation, 71:101082, 2022.   \n[27] Ryoji Tanabe, Hisao Ishibuchi, and Akira Oyama. Benchmarking multi-and many-objective evolutionary algorithms under two optimization scenarios. IEEE Access, 5:19597\u201319619, 2017.   \n[28] Hemant Kumar Singh, Kalyan Shankar Bhattacharjee, and Tapabrata Ray. Distance-based subset selection for benchmarking in evolutionary multi/many-objective optimization. IEEE Transactions on Evolutionary Computation, 23(5):904\u2013912, 2019.   \n[29] Zhenkun Wang, Hui-Ling Zhen, Jingda Deng, Qingfu Zhang, Xijun Li, Mingxuan Yuan, and Jia Zeng. Multiobjective optimization-aided decision-making system for large-scale manufacturing planning. IEEE Transactions on Cybernetics, 52(8):8326\u20138339, 2022.   \n[30] Matthias Ehrgott and Dagmar Tenfelde-Podehl. Computation of ideal and nadir values and implications for their use in MCDM methods. European Journal of Operational Research, 151(1):119\u2013139, 2003.   \n[31] Yiping Liu, Dunwei Gong, Jing Sun, and Yaochu Jin. A many-objective evolutionary algorithm using a one-by-one selection strategy. IEEE Transactions on Cybernetics, 47(9):2689\u20132702, 2017.   \n[32] Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I: Solving problems with box constraints. IEEE Transactions on Evolutionary Computation, 18(4):577\u2013601, 2014.   \n[33] Zhengping Liang, Kaifeng Hu, Xiaoliang Ma, and Zexuan Zhu. A many-objective evolutionary algorithm based on a two-round selection strategy. IEEE Transactions on Cybernetics, 51(3):1417\u20131429, 2021.   \n[34] Yanan Sun, Gary G Yen, and Zhang Yi. IGD indicator-based evolutionary algorithm for many-objective optimization problems. IEEE Transactions on Evolutionary Computation, 23(2):173\u2013187, 2019.   \n[35] Ignacy Kaliszewski. A modified weighted Tchebycheff metric for multiple objective programming. Computers & Operations Research, 14(4):315\u2013323, 1987.   \n[36] Cristian Ramirez-Atencia, Sanaz Mostaghim, and David Camacho. A knee point based evolutionary multi-objective optimization for mission planning problems. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 1216\u20131223, 2017.   \n[37] Chenwen Zhu, Lihong Xu, and Erik D Goodman. Generalization of Pareto-optimality for many-objective evolutionary optimization. IEEE Transactions on Evolutionary Computation, 20(2):299\u2013315, 2016.   \n[38] Arthur M Geoffrion. Proper efficiency and the theory of vector maximization. Journal of Mathematical Analysis and Applications, 22(3):618\u2013630, 1968.   \n[39] Hisao Ishibuchi, Lie Meng Pang, and Ke Shang. Effects of dominance modification on hypervolume-based and IGD-based performance evaluation results of NSGA-II. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 679\u2013687, 2023.   \n[40] Linjun He, Hisao Ishibuchi, and Dipti Srinivasan. Metric for evaluating normalization methods in multiobjective optimization. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 403\u2013411, 2021.   \n[41] Nikolaus Hansen. The CMA evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.   \n[42] Qingfu Zhang and Hui Li. MOEA/D: A multiobjective evolutionary algorithm based on decomposition. IEEE Transactions on Evolutionary Computation, 11(6):712\u2013731, 2007.   \n[43] Duc-Cuong Dang, Andre Opris, and Dirk Sudholt. Crossover can guarantee exponential speed-ups in evolutionary multi-objective optimisation. Artificial Intelligence, page 104098, 2024.   \n[44] Nikolaus Hansen. Injecting external solutions into CMA-ES. arXiv preprint arXiv:1110.4181, 2011.   \n[45] Sa\u00fal Zapotecas-Mart\u00ednez, Bilel Derbel, Arnaud Liefooghe, Dimo Brockhoff, Hern\u00e1n E Aguirre, and Kiyoshi Tanaka. Injecting CMA-ES into MOEA/D. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 783\u2013790, 2015.   \n[46] Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems for evolutionary multiobjective optimization. In Evolutionary multiobjective optimization: Theoretical advances and applications, pages 105\u2013145. Springer, 2005.   \n[47] Ran Cheng, Miqing Li, Ye Tian, Xingyi Zhang, Shengxiang Yang, Yaochu Jin, and Xin Yao. A benchmark test suite for evolutionary many-objective optimization. Complex & Intelligent Systems, 3:67\u201381, 2017.   \n[48] Zhenkun Wang, Yew-Soon Ong, and Hisao Ishibuchi. On scalable multiobjective test problems with hardly dominated boundaries. IEEE Transactions on Evolutionary Computation, 23(2):217\u2013 231, 2019.   \n[49] Ye Tian, Ran Cheng, Xingyi Zhang, Miqing Li, and Yaochu Jin. Diversity assessment of multiobjective evolutionary algorithms: Performance metric and benchmark problems [research frontier]. IEEE Computational Intelligence Magazine, 14(3):61\u201374, 2019.   \n[50] Mario K\u00f6ppen and Kaori Yoshida. Substitute distance assignments in NSGA-II for handling many-objective optimization problems. In International Conference on Evolutionary MultiCriterion Optimization (EMO), pages 727\u2013741. Springer, 2007.   \n[51] Miqing Li, Crina Grosan, Shengxiang Yang, Xiaohui Liu, and Xin Yao. Multiline distance minimization: A visualized many-objective test problem suite. IEEE Transactions on Evolutionary Computation, 22(1):61\u201378, 2018.   \n[52] Ryoji Tanabe and Hisao Ishibuchi. An easy-to-use real-world multi-objective optimization problem suite. Applied Soft Computing, 89:106078, 2020.   \n[53] Robin C Purshouse and Peter J Fleming. On the evolutionary optimization of many confilcting objectives. IEEE Transactions on Evolutionary Computation, 11(6):770\u2013784, 2007.   \n[54] Handing Wang, Shan He, and Xin Yao. Nadir point estimation for many-objective optimization problems based on emphasized critical regions. Soft Computing, 21(9):2283\u20132295, 2017.   \n[55] Yicun Hua, Qiqi Liu, Kuangrong Hao, and Yaochu Jin. A survey of evolutionary algorithms for multi-objective optimization problems with irregular pareto fronts. IEEE/CAA Journal of Automatica Sinica, 8(2):303\u2013318, 2021.   \n[56] Simon Huband, Philip Hingston, Luigi Barone, and Lyndon While. A review of multiobjective test problems and a scalable test problem toolkit. IEEE Transactions on Evolutionary Computation, 10(5):477\u2013506, 2006.   \n[57] \u00d6zg\u00fcr \u00d6zpeynirci and Murat K\u00f6ksalan. An exact algorithm for finding extreme supported nondominated points of multiobjective mixed integer programs. Management Science, 56(12):2302\u2013 2315, 2010.   \n[58] Qite Yang, Zhenkun Wang, and Hisao Ishibuchi. It is hard to distinguish between dominance resistant solutions and extremely convex Pareto optimal solutions. In International Conference on Evolutionary Multi-Criterion Optimization (EMO), pages 3\u201314. Springer, 2021.   \n[59] Ye Tian, Ran Cheng, Xingyi Zhang, and Yaochu Jin. PlatEMO: A MATLAB platform for evolutionary multi-objective optimization [educational forum]. IEEE Computational Intelligence Magazine, 12(4):73\u201387, 2017.   \n[60] Ruihao Zheng and Zhenkun Wang. A generalized scalarization method for evolutionary multiobjective optimization. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 37, pages 12518\u201312525, 2023.   \n[61] Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: A comparative case study and the strength Pareto approach. IEEE Transactions on Evolutionary Computation, 3(4):257\u2013271, 1999. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "This is an appendix for \u201cBoundary Decomposition for Nadir Objective Vector Estimation\u201d. Specifically, we provide: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Detailed analyses of heuristic methods (Appendix A);   \n\u2022 Mathematical models of the proposed benchmark problems, i.e., TN1-TN4 (Appendix B);   \n\u2022 The computational environment, algorithm runtimes, and the complexity analysis of BDNE (Appendix C);   \n\u2022 The potential societal impact of this work (Appendix D);   \n\u2022 More experimental results and analyses including ablation studies, the impact of $\\mu$ , and the comparison with other heuristic methods (Appendix E);   \n\u2022 Properties of the cone domination and proofs of theorems and corollaries (Appendix F). ", "page_idx": 14}, {"type": "text", "text": "A Deficiencies of Heuristic Methods ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Existing heuristic methods for the nadir objective vector estimation can be divided into two categories [15]: 1) straightforward methods and 2) extreme-point-based methods. We use an example $P F$ to illustrate the drawbacks of heuristic methods in estimating the nadir objective vector. As shown in Figure 3, the $P F$ consists of two triangles and is concave. We uniformly sample 225 points along the $P F$ in order to include sufficient critical points. We use these sampled points as each method\u2019s population and test its performance in estimating the nadir objective vector. ", "page_idx": 14}, {"type": "text", "text": "The experimental results are summarized in Table 4 and plotted in Figure 4. We can observe from Table 4 that all methods incorrectly estimate the nadir objective vector. Specifically, Figure 4 show that all methods except for SF4 acquire the critical points of $f_{1}$ and $f_{2}$ but miss that of $f_{3}$ . As shown in Figure 3, the critical point of $f_{3}$ is distant from every axis and thereby difficult to obtain by these methods. ", "page_idx": 14}, {"type": "image", "img_path": "f829mkQMUg/tmp/fadcc604cea302e56c66c2200008a63d3a4394bf7b92da22fa25be8121d43faa.jpg", "img_caption": ["(a) Detailed information "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "f829mkQMUg/tmp/84fe4780f63deb79e38a1b11a8d39c067ee4a7a772466febb34181aac2cc8568.jpg", "img_caption": ["(b) Another perspective "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 3: Illustrations of an example $P F$ . The critical points of $f_{1}$ are convex combinations of $(1,\\bar{0},1)^{\\intercal}$ and $(1,1,0)^{\\intercal}$ . The critical points of $f_{2}$ are convex combinations of $(0,1,1)^{\\intercal}$ and $(1,1,0)^{\\intercal}$ . The critical point of $f_{3}$ is $(0.9,0.7,1.5)^{\\intercal}$ . The nadir objective vector is $(1,1,1.5)^{\\intercal}$ . ", "page_idx": 14}, {"type": "text", "text": "Table 4: Estimated nadir objective vectors of existing heuristic methods. The nadir objective vector is $(1,1,1.5)^{\\intercal}$ . \u201cSFx\u201d denotes the straightforward method; \u201cEPx\u201d denotes the extreme-point-based method. Parameters settings: the minimum number of selected points is set to 10 for SF3 and SF4; $\\lambda$ in EP6 is set to 100. ", "page_idx": 15}, {"type": "table", "img_path": "f829mkQMUg/tmp/eac23e5f0ae23f39bfe85cc41ebbfb7b099b96b50c3783a70350b58f8ad02662.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "f829mkQMUg/tmp/38e185776aa8884aab40189061e83e3e827d24e446b4f76516e5c305ec252945.jpg", "img_caption": ["Figure 4: Illustrations of selected points obtained by existing heuristic methods. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Test Problem with Adjustable Critical Points ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Test Problem Generator ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The existing test problems [55] can not have various critical points. Many test problems share similar (or even identical) nadir objective vector configurations. For example, most problems of DTLZ [46] and WFG [56] are constructed from the unit simplex (i.e., equilateral triangle). The critical points of the unit simplex are its vertices. We design two problem generators that have controllable critical points. ", "page_idx": 16}, {"type": "text", "text": "Each objective function of the proposed test problem generators takes the following form ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{i}(\\mathbf{x})=s_{i}h_{i}(\\mathbf{x}_{I})(1+g_{i}(\\mathbf{x}_{I I})),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $s_{i}$ is a constant such that different objectives can have different ranges of values; $\\textbf{x}\\in$ $\\Omega\\,=\\,[0,1]^{n}\\,\\subset\\,\\mathbb{R}^{n}$ , and $\\mathbf{x}_{I}\\,=\\,(x_{1},\\ldots,x_{m})^{\\intercal}$ and $\\mathbf{x}_{I I}\\,=\\,(x_{m+1},\\ldots,x_{n})^{\\intercal}$ are subvectors of $\\mathbf{x}$ $g_{i}(\\mathbf{x}_{I I})\\geq0$ is called the distance function, which specifies as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g_{i}({\\bf x}_{I I})=10\\cdot\\sum_{j\\in J_{i}}(x_{j}-0.3)^{2},}}\\\\ {{\\mathrm{}}}\\\\ {{J_{i}=\\left\\{m+i,2m+i,\\ldots,\\left\\lfloor\\frac{n-i}{m}\\right\\rfloor m+i\\right\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$h_{i}({\\bf x}_{I})$ is termed as the position function, which determines the shape of the $P F$ . ", "page_idx": 16}, {"type": "text", "text": "B.1.1 Generator 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The $P F$ is a simplex. Firstly, we define $\\mathbf{y}(\\mathbf{x}_{I})\\,:\\,[0,1]^{m}\\,\\rightarrow\\,[0,1]^{m}$ as a base. It returns an $m$ - dimensional vector $(y_{1},\\ldots,y_{m})^{\\boldsymbol{\\mathsf{T}}}$ as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{i}={\\frac{x_{i}}{\\sum_{j=1}^{m}x_{j}}}{\\mathrm{~for~}}i=1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\{\\mathbf{y}(\\mathbf{x}_{I})|\\mathbf{x}_{I}\\in[0,1]^{m}\\}$ is an $(m-1)$ -dimensional unit simplex. Then the position function value vector is calculated by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{h}(\\mathbf{x}_{I})=V\\mathbf{y}(\\mathbf{x}_{I}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $V=[\\mathbf{v}^{1},\\ldots,\\mathbf{v}^{m}]$ determines vertices of the $P F$ . When $\\mathbf{s}=(1,\\ldots,1)^{\\intercal}$ , $\\mathbf{v}^{i}$ is a vertex of the $P F$ . Any $\\mathbf{v}^{i}$ should be non-dominated. $V$ is the particular parameter to be specified. The critical points depend on the settings of $V$ . ", "page_idx": 16}, {"type": "text", "text": "B.1.2 Generator 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The $P F$ is two adjacent simplices. We modify the base of Generator 1 to ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{i}=\\left\\{\\begin{array}{l l}{\\frac{x_{i}}{2x_{m}+1},i=1,\\ldots,m-1\\wedge x_{m}\\leq0.5,}\\\\ {\\frac{2x_{m}}{2(1-x_{m}+1)},\\;}\\\\ {\\frac{x_{i}}{2(1-x_{m})+\\frac{1}{2\\left(1-x_{j}\\right)}},i=1,\\ldots,m-1\\wedge x_{m}>0.5,}\\\\ {\\frac{2x_{i}}{2x_{m}+1},i=m\\wedge x_{m}\\leq0.5,}\\\\ {\\frac{\\sum_{j=1}^{m}x_{j}}{2x_{m}+\\frac{1}{2\\left(1-x_{i}\\right)}},i=m\\wedge x_{m}>0.5.}\\\\ {\\frac{2(1-x_{i})}{2(1-x_{m})+\\frac{1}{2\\left(x_{i}\\right)}},i=m\\wedge x_{m}>0.5.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Letting ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{j}>1,j=3,}\\\\ {u_{j}<1,j\\neq3,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\nv_{j}^{i}=\\left\\{1,\\begin{array}{l l}{{j\\ne i,}}\\\\ {{\\j=i,}}\\end{array}\\right.\\,\\,j=i,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "the position function value vector is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{h}(\\mathbf{x}_{I})=\\left\\{V\\mathbf{y}(\\mathbf{x}_{I}),\\quad x_{m}\\leq0.5,\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{V=[\\mathbf{v}^{1},\\mathbf{u},\\mathbf{v}^{5},\\dots,\\mathbf{v}^{m},\\mathbf{v}^{2}],}\\\\ {V^{\\prime}=[\\mathbf{v}^{1},\\mathbf{u},\\mathbf{v}^{5},\\dots,\\mathbf{v}^{m},\\mathbf{v}^{4}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The $P F$ of this generator consists of two simplices. Assume that $\\mathbf{s}=(1,\\ldots,1)^{\\intercal}$ . The vertices of two simplices are the column vectors of $V$ and $V^{\\prime}$ respectively. The difference between $V$ and $V^{\\prime}$ is the last column. That is to say, two simplices share many common edges and they are adjacent. u is the particular parameter to be specified. The unique critical point of $f_{3}$ is $\\mathbf{u}$ . The critical points of other objectives are the same as the inverted unit simplex whose objective vectors on any edge are critical points. ", "page_idx": 17}, {"type": "image", "img_path": "f829mkQMUg/tmp/f0f88b87a916993f0af97b36be8bcb08035242faa0cb2e21fc05d29c015c7291.jpg", "img_caption": ["Figure 5: The feasible objective region of 3-objective TN1. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "f829mkQMUg/tmp/a28b561875ba2dfd462f35130364fd5ea1ad06ef7b7f4f8673c17dcec9a69ca3.jpg", "img_caption": ["Figure 6: The feasible objective region of 3-objective TN2. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Proposed Test Problems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use the proposed test problem generators to obtain 4 scalable MOPs denoted as TN1-TN4. We first specify 1 matrix and 3 vectors: ", "page_idx": 17}, {"type": "equation", "text": "$$\n(V_{1})_{i j}={\\left\\{\\begin{array}{l l}{0,}&{{\\mathrm{if~}}i=j,}\\\\ {1.2,}&{{\\mathrm{if~}}i=2\\wedge j=1,}\\\\ {0.8,}&{{\\mathrm{if~}}(i=1\\vee i=3)\\wedge j=2,}\\\\ {1,}&{{\\mathrm{otherwise;}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "\u2022 $\\mathbf{s}^{1}=(1,\\ldots,1)^{\\intercal}$ ; ", "page_idx": 17}, {"type": "text", "text": "\u2022 if $m=3$ , $\\mathbf{s}^{2}=(1,100,10000)^{\\top}$ ; otherwise, $\\mathbf{s}^{2}=(1,100,10000,1,\\ldots,1)^{\\intercal}$ ;   \n\u2022 if $m=3$ , $\\mathbf{u}^{1}=(0.9,0.7,1.5)^{\\top}$ ; otherwise, $\\mathbf{u}^{1}=(0.9,0.7,1.5,0.9,\\ldots,0.9)^{\\intercal}$ . ", "page_idx": 17}, {"type": "text", "text": "Then the test problems are summarized in Table 5. We can find that $V_{1}$ is obtained by modifying such matrix: an $m\\times m$ square matrix with zeros on the main diagonal and ones elsewhere. This matrix implies that the $P F$ is an inverted unit simplex whose objective vectors on any edge are critical points (e.g., mDTLZ1 [48]). The modified vertices are the first and the second ones and the modification performs across $f_{1}$ to $f_{3}$ . Consequently, the critical points of $f_{1},\\,f_{2}$ , and $f_{3}$ are some vertices only. For example, we show the feasible objective region of the 3-objective TN1 in Figure 5. Vertices $(0,\\bar{1}.2,1)^{\\intercal}$ and $(1,1,0)^{\\intercal}$ are the only critical points. TN2 is the specification of the studied case in Section 2. More specifically, as illustrated in Figure 6, we can find that the critical point of $f_{3}$ is unsupported (i.e., there exists convex combinations of objective vectors that dominate the critical point [57]), making the $P F$ exhibit a concave shape. Besides, the critical point of $f_{3}$ is distant from all axes and not optimal for any objective. TN3 and TN4 are extended from TN1 and TN2 respectively. They have huge magnitude differences between some objective functions. ", "page_idx": 18}, {"type": "table", "img_path": "f829mkQMUg/tmp/a1cf321aaa1e1118d22b66cfb34357d618efce264fc454d9218299667906d891.jpg", "table_caption": ["Table 5: Parameter settings of TN1-TN4. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 5 and Figure 6 also indicate these 4 problems process the $W P B$ . The $W P B$ has evident dominance resistance [23]. That is, the objective vectors of the $W P B$ are usually non-dominated in the population, especially in many objective cases. The reason is that each of them can only improve $k$ ( $1\\leq k\\leq m-1)$ objectives to obtain an objective vector dominating it. Thus, they are difficult to distinguish from the population. The $W P B$ causes the population to suffer poor convergence and diversity such that the nadir objective vector is hard to estimate. As found in [48], the objective vectors with $k\\,=\\,1$ on the $W P B$ exhibit severe dominance resistance, even when presenting in 3-objective cases. Unfortunately, how to cope with the $W P B$ is still an open question [58]. ", "page_idx": 18}, {"type": "text", "text": "C Runtime ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In experiments, our codes rely on the MATLAB-based platform called PlatEMO [59], which is freely available for research purposes (see https://github.com/BIMK/PlatEMO). Experiments are executed on a computer equipped with two 3.00-GHz Intel Xeon Gold 6248R CPUs (48 cores in total), an NVIDIA T400 GPU, and 128GB of RAM. We report the runtime for each algorithm on each instance in Table 6. DNPE exhibits the shortest runtime, followed by ECR-NSGA-II. BDNE has a longer runtime than these two representative algorithms. ", "page_idx": 19}, {"type": "text", "text": "We want to emphasize the runtime for BDNE strongly depends on the implementation (e.g., the chosen solvers). We adopt an effective selection procedure stated in [60], which can facilitate the application of BDNE to various problems. The main loop of our BDNE implementation has a time complexity of $O\\left(N^{2}\\log(N)\\right)$ where $N$ is the population size. The specific analysis is provided as follows. The reproduction procedure has a time complexity of $O(N)$ . The calculation of $\\mathbf{r}$ governs the complexity of the selection procedure, which has $O\\left(N^{2}\\log(N)\\right)$ . The adjustment of boundary weight vectors with $\\begin{array}{r}{O\\left(N\\log\\left(\\frac{N}{m}\\right)\\right)}\\end{array}$ is executed periodically. Therefore, The selection procedure governs the overall complexity, and the main loop has the complexity $O\\left(N^{2}\\log(N)\\right)$ . We can significantly reduce the complexity by using a simple selection procedure or choosing an alternative solver, which deserves further investigation. ", "page_idx": 19}, {"type": "table", "img_path": "f829mkQMUg/tmp/fac9656c4d199fba40a737ae710948290b1a39b2129b6affa47ba35906f892e5.jpg", "table_caption": ["Table 6: Comparisons of runtimes among ECR-NSGA-II, DNPE, and BDNE. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Potential societal impact ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Many real-world problems, such as logistics dispatch, printed-circuit board assembly, efficient chemical reaction, and Internet of Things service, require simultaneous optimization of multiple conflicting objectives. The nadir objective vector mainly has two strengths in multi-objective optimization: 1) it can normalize the objective space together with the ideal objective vector and 2) it provides a range for decision-makers or algorithms to more easily explore different trade-offs. Our proposed BDNE is good for better nadir objective vector estimation in terms of theoretical guarantee and wide applicability. BDNE can be applied to various problems by employing appropriate solvers. Furthermore, the parameter $\\mu$ , which indicates the tolerance of decision-makers for trade-offs, could also be an important component in developing a more user-friendly multi-criteria decision system. ", "page_idx": 20}, {"type": "text", "text": "On the downside, relying solely on the nadir objective vector for decision-making can be risky. Decision-makers should use additional information for a more informed and comprehensive decision. Moreover, the leakage of the nadir objective vector might inadvertently expose problem information and user preferences. This is particularly troublesome in certain applications where the confidentiality of such information is paramount, indicating a need for caution to avoid these situations. In addition, there is no guarantee that the evolutionary algorithm version can approximate the nadir objective vector within a desired timeframe. This might delay decision-making processes or lead to decisions based on incomplete or inadequate information. ", "page_idx": 20}, {"type": "text", "text": "E More Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We conduct a more comprehensive empirical study in this section. We omit comparisons with existing exact methods, as they are implemented exclusively on the discrete MOP (where the exact solver is available) and are nearly impossible to apply to other types of MOPs. Nevertheless, we will propose the implementation of BDNE for the discrete MOP. Audiences can follow our work at https://github.com/EricZheng1024/BDNE. ", "page_idx": 21}, {"type": "text", "text": "E.1 Ablation Studies ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We investigate 3 schemes in BDNE. \u201cCFF\u201d denotes the coping strategy for flat ftiness (see Section 3.2). \u201cTBW\u201d denotes the transformation of the best boundary weight vector (see Step 2 of the upper-level optimization in Section 3.3). \u201cBSU\u201d denotes the boundary subproblems using unit vectors as their weight vectors (see the upper-level optimization in Section 3.3). ", "page_idx": 21}, {"type": "text", "text": "Table 7 presents the experimental results. Removing CFF or TBW merely does not evidently affect the performance of BDNE. However, the performance deteriorates when both schemes are absent. Particularly, BDNE without CFF and TBW is less effective on MOPs with complicated feasible regions, i.e., TN2 and TN4. BDNE outperforms BDNE without BSU across most instances, indicating that boundary subproblems using unit vectors are important. Overall, the effectiveness of the two strategies is validated. ", "page_idx": 21}, {"type": "table", "img_path": "f829mkQMUg/tmp/eca839a318915eaa1166c69fcc51dce352080075fac00d4ca6bf4e4123c7fc19.jpg", "table_caption": ["Table 7: Comparisons of error metric values among variants of BDNE. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.2 Impact of $\\mu$ and $\\tau_{u}$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We investigate the impact of parameters in BDNE. Recall that $\\mu$ is the user-defined upper bound of the trade-off, and $\\tau_{u}$ denotes the maximum number of iterations involved in the upper-level optimization. ", "page_idx": 21}, {"type": "text", "text": "The Pareto-optimal objective vector, situated on the boundary of a convex $P F$ , usually has a substantially higher value of $M$ (see Definition 10) than that in the central region. We test BDNE with different settings of $\\mu$ on mDTLZ2, since this problem has a convex $P F$ and thus effectively discloses the impact of $\\mu$ . We consider 6 values for $\\mu$ : 1, 20, 40, 60, 80, and 100. We find that the estimated nadir objective vector is dominated by the exact one on every instance. We introduce another metric, representing the distance between the estimated nadir objective vector and the ideal objective vector. This metric is defined as $\\begin{array}{r}{E^{\\prime}=\\sqrt{\\sum_{i=1}^{m}\\left(\\left(\\hat{z}_{i}^{n a d}-z_{i}^{i d e}\\right)/\\left(z_{i}^{n a d}-z_{i}^{i d e}\\right)\\right)^{2}}}\\end{array}$ . Figure 7 further illustrate this observation, plotting the means of $E$ and $E^{\\prime}$ . Consistently, the obtained nadir objective vector deviates from the exact nadir objective vector and moves closer to the ideal objective vector, as the value of $\\mu$ decreases. A smaller $\\mu$ implies that decision-makers have a stricter requirement for preferred solutions, resulting in a smaller set of preferred solutions. Accordingly, the promising region becomes more refined and the estimated nadir objective vector is getting farther away from the exact one. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "f829mkQMUg/tmp/0c78126b439aeeae62f300516eb114dcea4ad0e0c2559df65aec5d63470df3e1.jpg", "img_caption": ["Figure 7: Results of different $\\mu$ settings on mDTLZ2. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Different MOPs have different difficulties in estimating the nadir objective vector. To clearly show the effects of $\\tau_{u}$ , we employ TN4, which possesses a complicated feasible objective region. Figure 8 plots the results. The value for $\\tau_{u}$ begins at 2, increases by 2, and ends at 18, while $\\tau_{l}$ remains 200. The runtime is getting shorter as the value of $\\tau_{u}$ decreases, which is straightforward. Besides, the relationship between the runtime and $\\tau_{u}$ is linear. The curve of $E$ versus $\\tau_{u}$ exhibits a declining trend with intermittent fluctuations. Generally, the error is getting smaller (or the accuracy is getting higher) as the value of $\\tau_{u}$ increases. The fluctuations occur because the lower-level optimization algorithm is stochastic (i.e., it does not guarantee the attainment of a Pareto-optimal solution within a finite number of iterations). ", "page_idx": 22}, {"type": "image", "img_path": "f829mkQMUg/tmp/8d8fbf01d2c22c41723f6f0a7d5278224d50bc2296f0450bdf75f6cfd67bbbf7.jpg", "img_caption": ["Figure 8: Results of different $\\tau_{u}$ settings on TN4. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.3 Comparison with Other Heuristic Methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In previous sections, we employ theoretical analysis, illustrative examples, and experimental comparisons with two representative baseline methods to demonstrate the advantage of BDNE over the existing heuristic ones. To more comprehensively validate the effectiveness of BDNE, we present the experimental results comparing our method with the remaining heuristic methods in Table 8. The majority of heuristic methods cannot surpass our method on any instance, while our method significantly outperforms them on most instances. Table 9 shows the detailed results of the methods having better performance than BDNE on some instances. Although EP1 achieves lower errors on 9 instances, the gaps are relatively small $\\Delta\\,<\\,0.03)$ . In contrast, BDNE has remarkably better performance than EP1 on many instances. For example, $\\Delta<-0.3$ on TN4 and TN4; $\\Delta<-1.5$ on 5- and 8-objective DTLZ2; $\\Delta<-12$ on 5-objective ML-DMP. For EP4, the better result only appears on 3-objective ML-DMP, and the gap is very small $\\left<\\Delta<10^{-4}\\right>$ ). EP4 shows poor results on the rest of the instances. We can conclude that our method still outperforms other heuristic methods. ", "page_idx": 23}, {"type": "table", "img_path": "f829mkQMUg/tmp/44ffc4eedd64cd4d870702d800073bdab11e512bf9eedf8d58df9fdde9905811.jpg", "table_caption": ["Table 8: Overall comparisons of error metric values between other heuristic methods and BDNE. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "f829mkQMUg/tmp/62a847956965afb940169dedaeafc3827eebd5c04e81241276dc42418f2ae6a6.jpg", "table_caption": ["Table 9: Comparisons of error metric values among EP1, EP4, and BDNE. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.4 Pilot Studies on Integrating BDNE in Algorithm Iteration ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We present BDNE as an independent algorithm in this paper. This subsection provides initial investigations about integrating BDNE into the iteration of an algorithm. We select the MOEA to conduct experimental studies and propose two simple implementations. First, we replace the nadir objective vector estimation method of an MOEA with ours and denote the variant as \u201cV1\u201d. We define the promising region as the dominating region of the estimated nadir objective vector. Then V1 is further improved by focusing the search within the promising region. Specifically, in the environment selection of each iteration, the objective vector outside the promising region is considered to have a worse fitness value than the one inside. The improved version of V1 is termed \u201cV2\u201d. We consider two famous MOEAs: MOEA/D [42] and NSGA-III [32]. The population size of the BDNE is set to 12, whereas the population size of the MOEA is set to 91. The number of function evaluations is consistent with the original settings, i.e., 180,000. The MOEA and its improved versions use the same function evaluation budget. V1 or V2 consumes 12 additional function evaluations for nadir objective vector estimation in each iteration, and as a result, it uses about 230 fewer iterations than the original MOEA. ", "page_idx": 24}, {"type": "text", "text": "The statistical results of the hypervolume metric [61] are reported in Table 10. The results of Wilcoxon\u2019s rank sum test are shown by two symbols in $\\{+,=,-\\}$ in the cell. The first and second symbols are the comparison results with V1 and V2, respectively. We begin by analyzing the comparative results of the MOEA and V1. Both MOEA/D-V1 and NSGA-III-V1 significantly outperform their original versions, respectively. Besides, MOEA/D-V1 exhibits clearly smaller standard deviations compared with its original versions (e.g., 0.0106 versus 0.00289), indicating more stable performance. Analogously, NSGA-III-V1 yields stable performance. These results also emphasize that inaccurate nadir objective vector estimation can have a significant impact on algorithm performance. According to the remaining results, V2 of the MOEA has a better performance than V1, indicating the promising region determined by BDNE is useful for guiding the search. Furthermore, we find that V2, on average, retains more solutions within the promising region compared with V1. For MOEA/D, 73 versus 83 on TN1, 46 versus 53 on TN2, 70 versus 84 on TN3, and 48 versus 53 on TN4. For NSGA-III, 30 versus 89 on TN1, 25 versus 67 on TN2, 32 versus 88 on TN3, and 25 versus 66 on TN4. This explains the performance improvement and demonstrates the effectiveness of the scheme. ", "page_idx": 24}, {"type": "text", "text": "In conclusion, our method shows great potential to improve the performance of an algorithm. ", "page_idx": 24}, {"type": "table", "img_path": "f829mkQMUg/tmp/b44b436af85b2c54e7c9bd044b0b83223614a345a5f965825094798d7bb343cb.jpg", "table_caption": ["Table 10: Comparisons of hypervolume metric values between the MOEA and its improved variants. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.5 Results on All Problems in the Used Test Suites ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In previous experiments, we select the problems with different kinds of feasible objective regions from several test suites to comprehensively compare the algorithms and avoid showing similar results. Specifically, we employ 4 new and 6 existing test problems (28 instances in total), including instances with many objectives (e.g., 5 and 8 objectives cases), weakly Pareto-optimal boundaries (e.g., TN1- TN4 and mDTLZ3), linear PFs (e.g., TN1 and TN3), convex PFs (e.g., mDTLZ3), concave $P F{\\bf s}$ (e.g., TN2, TN4, and DTLZ3), irregular PFs (e.g., TN3, TN4, DTLZ5, IMOP4, and IMOP6). ", "page_idx": 24}, {"type": "text", "text": "Table 11 summarizes the complete results on all the test problems by showing \u201cTotal $+/{=}/-^{\\circ}$ . DTLZ, MP-DMP, and ML-DMP are absent in this experiment, as the MaF test suite already covers them. Besides, IMOP1-IMOP3 are non-scalable 2-objective MOPs, which are also omitted. We can find that our method outperforms the two representative algorithms on most instances. Table 12 records the detailed results indicating either of the two algorithms statistically outperforms BDNE, and the positive gaps in the table are marked. Most of the positive gaps are lower than 0.02, while the absolute values of negative gaps can be very large. Besides, BDNE always ranks second on these instances, exhibiting competitive performance. In short, BDNE still shows its superiority on the three test suites. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "table", "img_path": "f829mkQMUg/tmp/3c41f3d8ef41ec9911c0c64d5738c3b1d6e3903895fbe4532cea3fe0095b085f.jpg", "table_caption": ["Table 11: Overall comparisons of error metric values among ECR-NSGA-II, DNPE, and BDNE on MaF, mDTLZ, and IMOP test suites. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "f829mkQMUg/tmp/70ac50d2c378933aa2f83131ba4a848137cda7aa10f89ed25460b72f7926a407.jpg", "table_caption": ["Table 12: Comparisons of error metric values among ECR-NSGA-II, DNPE, and BDNE on the three test suites\u2019 instances where either of the two algorithms outperform BDNE. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "F Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1 Properties of Cone Domination ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Property 1 (Reflexivity). \u2200u, $\\mathbf{u}\\neq^{c}\\mathbf{u}.$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Since $\\tilde{u}_{i}=\\tilde{u}_{i}$ for $i=1,\\hdots,m$ , we have $\\mathbf{u}\\neq^{c}\\mathbf{u}$ . ", "page_idx": 26}, {"type": "text", "text": "Property 2 (Anti-symmetry). If $\\mathbf{u}\\prec^{c}\\mathbf{v},$ , then v $\\mathcal{k}^{c}\\,\\mathbf{u}.$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Since u ${\\prec}^{c}\\ \\mathbf{v}$ , we have $\\tilde{\\mathbf{u}}\\prec\\tilde{\\mathbf{v}}$ . Thus, $\\mathbf{v}\\neq^{c}\\mathbf{u}$ according to Definition 8. ", "page_idx": 26}, {"type": "text", "text": "Property 3 (Transitivity). If t $\\prec^{c}$ u and $\\mathbf{u}\\prec^{c}\\mathbf{v},$ , then t $\\prec^{c}\\mathbf{v}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Since t $\\prec^{c}\\mathbf{u}$ and u ${\\prec}^{c}\\,\\mathbf{v}$ , we have $\\tilde{\\mathbf{t}}\\prec\\tilde{\\mathbf{u}}$ and $\\tilde{\\mathbf{u}}\\prec\\tilde{\\mathbf{v}}$ . According to the transitivity of Pareto domination, $\\tilde{\\mathbf{t}}\\prec\\tilde{\\mathbf{v}}$ . Therefore, t ${\\prec}^{c}\\,\\mathbf{v}$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "The three properties imply that cone domination defines a strict partial order. ", "page_idx": 26}, {"type": "text", "text": "F.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. Without loss of generality, we let $z_{j}^{r}=0$ for $j=1,\\dots,m$ . ", "page_idx": 26}, {"type": "text", "text": "(Sufficiency) Let $z_{j}^{*}\\geq z_{j}^{r}=0$ for at least one $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ . We consider an objective vector ${\\bf z}$ satisfying $\\mathbf{z}^{\\ast}\\prec\\mathbf{z}$ . We have $\\scriptstyle\\sum_{k=1}^{m}z_{k}\\ >\\ \\sum_{k=1}^{m}z_{k}^{*}$ and then ", "page_idx": 26}, {"type": "equation", "text": "$$\n(1-\\alpha)z_{j}+\\frac{\\alpha}{m}\\sum_{k=1}^{m}z_{k}>(1-\\alpha)z_{j}^{*}+\\frac{\\alpha}{m}\\sum_{k=1}^{m}z_{k}^{*}\\quad\\mathrm{for}\\,\\mathrm{every}\\quad j\\in\\{1,\\ldots,m\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consequently, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{w_{j}^{i}\\cdot\\tilde{z}_{j}=w_{j}^{i}\\cdot\\tilde{z}_{j}^{*}=0,}&{\\mathrm{if}\\;w_{j}^{i}=0,}\\\\ &{w_{j}^{i}\\cdot\\tilde{z}_{j}>w_{j}^{i}\\cdot\\tilde{z}_{j}^{*},}&{\\mathrm{if}\\;w_{j}^{i}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\exists j\\in\\{1,\\cdot\\cdot\\cdot,m\\}\\setminus\\{i\\}$ such that $\\tilde{z}_{j}^{*}\\geq0$ , we can deduce ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}\\right\\}>\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{*}\\right\\}\\geq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The dominated objective vector can not be optimal for the boundary subproblem. Therefore, the optimal solution of the boundary subproblem must be Pareto-optimal. ", "page_idx": 26}, {"type": "text", "text": "(Necessity) Let $\\mathbf{z}^{\\ast}$ be Pareto-optimal. We suppose that $z_{j}^{r}\\,>\\,\\tilde{z}_{j}^{*}$ for each $j\\,\\in\\,\\{1,\\ldots,m\\}\\,\\setminus\\,\\{i\\}$ . We consider $z_{j}\\,=\\,z_{j}^{*}+\\epsilon_{j}$ for $j=1,\\dots,m$ where $\\epsilon_{j}\\ge\\breve{0}$ is a sufficiently small value such that $\\tilde{z}_{j}^{*}<\\tilde{z}_{j}<z_{j}^{r}\\,\\,\\bar{=}\\,0$ . Then the following equation holds ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{*}\\right\\}=\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}\\right\\}=0,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which indicates that ${\\bf z}$ dominated by $\\mathbf{z}^{\\ast}$ can also be optimal for the boundary subproblem. But this is a contradiction. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "F.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. Without loss of generality, we let $z_{j}^{r}\\,=\\,0$ for $j\\,=\\,1,\\ldots,m$ . Since $\\tilde{z}_{j}^{(i)^{c}}\\,\\geq\\,z_{j}^{r}$ for every $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{(i)^{c}}\\right\\}=w_{j}^{i}\\cdot\\tilde{z}_{j}^{(i)^{c}}~(j\\neq i)\\geq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(Sufficiency) Let ${\\mathbf z}^{(i)^{c}}$ be cone-optimal. We assume that ${\\bf z}$ , rather than ${\\mathbf z}^{(i)^{c}}$ , is the optimal objective vector to this boundary subproblem. That is, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}\\right\\}<\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{(i)^{c}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\textbf{z}\\neq^{c}\\textbf{z}^{(i)^{c}}$ and $w_{i}^{i}\\,\\cdot\\,\\tilde{z}_{i}^{(i)^{c}}\\;=\\;0$ , Eq. (28) implies that $\\tilde{z}_{i}^{(i)^{c}}\\;<\\;\\tilde{z}_{i}$ and $\\tilde{z}_{j}^{(i)^{c}}\\;>\\;\\tilde{z}_{j}$ for all $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ . Theorem 1 signifies ${\\bf z}$ is Pareto-optimal, and thus the corresponding objective function value is larger than that of the critical point (i.e., $z_{i}^{(i)^{c}}\\,\\geq\\,z_{i})$ ). Based on z\u02dc(i) < z\u02dci and $z_{i}^{(i)^{c}}\\geq z_{i}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{m}z_{j}^{(i)^{c}}<\\sum_{j=1}^{m}z_{j}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "There exists an index $l\\neq i$ such that $z_{l}^{(i)^{c}}<z_{l}$ . Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\nz_{l}^{(i)^{c}}+\\frac{\\alpha}{m}\\sum_{j=1}^{m}z_{l}^{(i)^{c}}<z_{l}+\\frac{\\alpha}{m}\\sum_{j=1}^{m}z_{l},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which means ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}\\right\\}>w_{l}^{i}\\cdot\\tilde{z}_{l}^{(i)^{c}}=\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{(i)^{c}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This conclusion confilcts with the assumption. ${\\mathbf z}^{(i)^{c}}$ is optimal for this boundary subproblem. Since ${\\bf z}$ can be arbitrary, ${\\mathbf z}^{(i)^{c}}$ is also the unique optimal objective vector. ", "page_idx": 27}, {"type": "text", "text": "(Necessity) Let ${\\mathbf z}^{(i)^{c}}$ uniquely solve the boundary subproblem with ", "page_idx": 27}, {"type": "equation", "text": "$$\nw_{j}^{i}=\\left\\{\\frac{0,}{\\tilde{z}_{j}^{(i)^{c}}-z_{j}^{r}},~~j=i,\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We assume that $\\mathbf{z}$ cone-dominates ${\\mathbf z}^{(i)^{c}}$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{w_{j}^{i}\\cdot\\tilde{z}_{j}=w_{j}^{i}\\cdot\\tilde{z}_{j}^{(i)^{c}}=0,}&{\\mathrm{if~}w_{j}^{i}=0,}\\\\ &{w_{j}^{i}\\cdot\\tilde{z}_{j}\\leq w_{j}^{i}\\cdot\\tilde{z}_{j}^{(i)^{c}},}&{\\mathrm{if~}w_{j}^{i}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Consequently, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}\\right\\}\\leq\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{(i)^{c}}\\right\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "indicating that ${\\mathbf z}^{(i)^{c}}$ is not uniquely optimal for the boundary subproblem. But this is a contradiction. ", "page_idx": 27}, {"type": "text", "text": "F.4 Proof of Corollary 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. Since the index $l$ in Inequality (30) is not arbitrary, zj(i) should be larger than zjr for j = $1,\\ldots,m$ . ", "page_idx": 27}, {"type": "text", "text": "F.5 Proof of Theorem 3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. We first prove that a cone-optimal objective vector is Pareto-optimal. Letting $\\mathbf{z}^{\\ast}$ be a coneoptimal objective vector and $\\mathbf{z}\\in\\{\\bar{\\mathbf{f}^{}}(\\mathbf{x})|\\mathbf{x}\\in\\dot{\\Omega}\\}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\exists k,(1-\\alpha)(z_{k}^{*}-z_{k})+\\frac{\\alpha}{m}\\left(\\sum_{j=1}^{m}z_{j}^{*}-\\sum_{j=1}^{m}z_{j}\\right)<0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq j\\leq m}(z_{j}^{*}-z_{j})<\\frac{1}{m}\\sum_{j=1}^{m}\\left(z_{j}^{*}-z_{j}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then $\\exists k,z_{k}^{*}-z_{k}<0$ . ", "page_idx": 27}, {"type": "text", "text": "We now prove that \u03b1 \u2264(m\u22121)mM+m holds for a cone-optimal objective vector. Given a Pareto-optimal objective vector $\\mathbf{z}^{\\ast}$ and an objective vector $\\mathbf{z}\\in\\{\\mathbf{f}(\\mathbf{x})|\\mathbf{x}\\in\\Omega\\}$ , to make sure that ${\\tilde{\\mathbf{z}}}^{*}$ is Pareto-optimal for each $\\tilde{\\mathbf{z}}$ , the following inequality should hold ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\operatorname*{min}_{1\\leq j\\leq m}\\tilde{z}_{j}^{*}-\\tilde{z}_{j}}\\\\ &{=\\!(1-\\alpha)\\operatorname*{min}_{1\\leq j\\leq m}(z_{j}^{*}-z_{j})+\\displaystyle\\frac{\\alpha}{m}\\left(\\sum_{j=1}^{m}z_{j}^{*}-\\sum_{j=1}^{m}z_{j}\\right)}\\\\ &{<\\!0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\exists j,z_{j}^{*}-z_{j}<0$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{m}\\left(z_{j}^{*}-z_{j}\\right)<\\left(m-1\\right)\\operatorname*{max}_{1\\leq j\\leq m}\\left(z_{j}^{*}-z_{j}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The Definition 10 is equivalent to a properly Pareto-optimal decision vector $\\mathbf{x}^{\\ast}\\in\\Omega$ satisfying that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{1\\leq j\\leq m}f_{j}(\\mathbf{x}^{*})-f_{j}(\\mathbf{x})}{\\operatorname*{max}_{1\\leq j\\leq m}f_{j}(\\mathbf{x})-f_{j}(\\mathbf{x}^{*})}\\leq M<\\infty\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbf{x}\\in\\Omega$ and $\\mathrm{max}_{1\\leq j\\leq m}\\,f_{j}(\\mathbf{x})-f_{j}(\\mathbf{x}^{*})>0$ . According to Inequalities (38) and (39), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{m}\\left(\\sum_{j=1}^{m}z_{j}^{*}-\\sum_{j=1}^{m}z_{j}\\right)<\\frac{(m-1)\\alpha}{m}\\operatorname*{max}_{1\\le j\\le m}\\left(z_{j}^{*}-z_{j}\\right)\\le\\frac{(m-1)\\alpha M}{m}\\operatorname*{max}_{1\\le j\\le m}\\left(z_{j}-z_{j}^{*}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(1-\\alpha)\\operatorname*{min}_{1\\le j\\le m}(z_{j}^{*}-z_{j})+\\frac{\\alpha}{m}\\left(\\sum_{j=1}^{m}z_{j}^{*}-\\sum_{j=1}^{m}z_{j}\\right)}\\\\ {\\displaystyle<-\\left(1-\\alpha\\right)\\operatorname*{max}_{1\\le j\\le m}(z_{j}-z_{j}^{*})+\\frac{(m-1)\\alpha M}{m}\\operatorname*{max}_{1\\le j\\le m}\\left(z_{j}-z_{j}^{*}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\mathrm{max}_{1\\leq j\\leq m}\\big(z_{j}-z_{j}^{*}\\big)>0$ , then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\alpha-1+\\cfrac{(m-1)\\alpha M}{m}\\leq0,}\\\\ {\\alpha\\leq\\cfrac{m}{(m-1)M+m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "F.6 Proof of Corollary 2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "If a cone-optimal objective vector is not properly Pareto-optimal, $\\alpha\\,\\leq\\,0$ should hold according to Theorem 3. Therefore, a cone-optimal objective vector has a finite value of $M$ , indicating it is properly Pareto-optimal. However, $\\alpha>0$ as shown in Definition 8. The formal proof is as follows. ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $\\mathbf{z}^{\\ast}$ be a cone-optimal objective vector. We assume that $\\mathbf{z}^{\\ast}$ is not properly Pareto-optimal. Then there exists an objective vector $\\mathbf{z}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{1\\leq j\\leq m}z_{j}^{*}-z_{j}}{\\operatorname*{max}_{1\\leq j\\leq m}z_{j}-z_{j}^{*}}\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\mathbf{z}^{\\ast}$ is a Pareto-optimal objective vector, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{m}\\left(z_{j}^{*}-z_{j}\\right)}\\\\ &{\\geq\\displaystyle\\operatorname*{max}_{1\\leq j\\leq m}\\left(z_{j}^{*}-z_{j}\\right)-\\left(m-1\\right)\\displaystyle\\operatorname*{max}_{1\\leq j\\leq m}\\left(z_{j}-z_{j}^{*}\\right)}\\\\ &{>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The inequality can be rewritten as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle(1-\\alpha)\\operatorname*{min}_{1\\leq j\\leq m}\\left(z_{j}^{*}-z_{j}\\right)+\\displaystyle\\frac{\\alpha}{m}\\sum_{j=1}^{m}\\left(z_{j}^{*}-z_{j}\\right)}\\\\ {\\displaystyle\\geq\\frac{\\alpha}{m}\\operatorname*{max}_{1\\leq j\\leq m}\\left(z_{j}^{*}-z_{j}\\right)-}\\\\ {\\displaystyle\\left(\\frac{\\alpha(m-1)}{m}+1-\\alpha\\right)\\operatorname*{max}_{1\\leq j\\leq m}\\left(z_{j}-z_{j}^{*}\\right)}\\\\ {\\displaystyle>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This conclusion conflicts with the optimality of $\\mathbf{z}^{\\ast}$ . ", "page_idx": 29}, {"type": "text", "text": "Moreover, leveraging Theorem 3 and Corollary 2, we illustrate the relationships among Pareto optimality, proper Pareto optimality, and cone optimality in Figure 9. ", "page_idx": 29}, {"type": "image", "img_path": "f829mkQMUg/tmp/d025fb03482034a3edf860c2f00b003729b76fdc2e3090a4768c2665f7d91696.jpg", "img_caption": ["Figure 9: Relationships among Pareto optimality, proper Pareto optimality, and cone optimality. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "F.7 Proof of Theorem 4 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The proof is analogous to the sufficiency proof of Theorem 2. ", "page_idx": 29}, {"type": "text", "text": "Proof. Without loss of generality, we let $z_{j}^{r}=0$ for $j=1,\\dots,m$ . According to Theorem 6, we can let $w_{j}^{i}=\\beta/\\tilde{z}_{j}^{*}$ for every $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ . Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{*}\\right\\}=\\beta\\geq0,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\beta$ is a constant. ", "page_idx": 29}, {"type": "text", "text": "Suppose that $\\mathbf{z}^{\\prime}$ yields a lower function value than $\\mathbf{z}^{\\ast}$ for the boundary subproblem with $\\mathbf{w}^{i}$ . That is, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\le j\\le m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{\\prime}\\right\\}<\\beta,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $\\mathbf z^{\\ast}\\neq\\mathbf z^{\\prime}$ according to Theorem 1. Since $\\mathbf{z}^{\\prime}\\nprec\\!\\mathbf{z}^{c}\\,\\mathbf{z}^{*}$ , Eq. (47) implies that $\\tilde{z}_{i}^{*}<\\tilde{z}_{i}^{\\prime}$ and $\\tilde{z}_{j}^{*}>\\tilde{z}_{j}^{\\prime}$ for every $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ . Based on $\\tilde{z}_{i}^{*}<\\tilde{z}_{i}^{\\prime}$ and $z_{i}^{*}\\geq z_{i}^{\\prime}$ , we can deduce that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{m}z_{j}^{*}<\\sum_{j=1}^{m}z_{j}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "There exists an index $l\\neq i$ such that $z_{l}^{*}<z_{l}^{\\prime}$ . Therefore, ", "page_idx": 29}, {"type": "equation", "text": "$$\nz_{l}^{*}+\\frac{\\alpha}{m}\\sum_{j=1}^{m}z_{l}^{*}<z_{l}^{\\prime}+\\frac{\\alpha}{m}\\sum_{j=1}^{m}z_{l}^{\\prime},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which means ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\le j\\le m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{\\prime}\\right\\}\\ge w_{l}^{i}\\cdot\\tilde{z}_{l}^{\\prime}>\\beta.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This conclusion conflicts with the assumption. ", "page_idx": 29}, {"type": "text", "text": "F.8 Proof of Corollary 3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof. According to Theorem 3, the Pareto-optimal objective vector deviating from the user-defined trade-off is cone-dominated by some objective vector. And Theorem 4 reports the cone-optimal objective vector that maximizes a given objective is optimal for at least one boundary subproblem. ", "page_idx": 30}, {"type": "text", "text": "F.9 Proof of Theorem 5 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The proof is modified from the sufficiency proof of Theorem 1. ", "page_idx": 30}, {"type": "text", "text": "Proof. Without loss of generality, we let $z_{j}^{r}=0$ for $j=1,\\dots,m$ . We consider an objective vector ${\\bf z}$ satisfying $\\mathbf{z}^{\\ast}\\prec\\mathbf{z}^{c}\\textbf{z}$ . Then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(1-\\alpha)z_{j}+\\frac{\\alpha}{m}\\sum_{k=1}^{m}z_{k}\\geq(1-\\alpha)z_{j}^{*}+\\frac{\\alpha}{m}\\sum_{k=1}^{m}z_{k}^{*}}\\\\ {\\mathrm{for}\\,\\mathrm{every}\\quad j\\in\\{1,\\ldots,m\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Consequently, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{w_{j}^{i}\\cdot\\tilde{z}_{j}=w_{j}^{i}\\cdot\\tilde{z}_{j}^{*}=0,}&{\\mathrm{if}\\;w_{j}^{i}=0,}\\\\ &{w_{j}^{i}\\cdot\\tilde{z}_{j}\\geq w_{j}^{i}\\cdot\\tilde{z}_{j}^{*},}&{\\mathrm{if}\\;w_{j}^{i}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\exists j\\in\\{1,\\cdot\\cdot\\cdot,m\\}\\setminus\\{i\\}$ such that $\\tilde{z}_{j}^{*}>0$ , we can deduce ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}\\right\\}>\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{*}\\right\\}\\geq0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "z can not be optimal for the boundary subproblem. Therefore, the optimal solution to the boundary subproblem must be cone-optimal. According to Corollary 2, the optimal solution is also properly Pareto-optimal. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "F.10 Proof of Theorem 6 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof. Let $z_{j}^{r}=0$ for $j=1,\\dots,m$ . Suppose that ${\\bf z}$ , rather than $\\mathbf{z}^{\\ast}$ , is optimal for the subproblem with $\\mathbf{w}^{*}$ . Letting $w_{j}^{*}\\tilde{z}_{j}^{*}=\\beta$ for $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ where $\\beta>0$ is a constant, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{w_{j}^{*}\\cdot\\tilde{z}_{j}^{*}=w_{j}^{*}\\cdot\\tilde{z}_{j}=0,}&{\\mathrm{if~}w_{j}^{*}=0,}\\\\ &{\\beta>w_{j}^{*}\\cdot\\tilde{z}_{j}\\geq0,}&{\\mathrm{if~}w_{j}^{*}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\mathbf{z}^{\\ast}$ is properly Pareto-optimal according to Theorem 5, we have $w_{j}^{*}>0,j\\neq i$ . We can deduce that $\\tilde{z}_{j}^{*}>\\tilde{z}_{j}\\geq0$ for $j\\in\\{1,\\ldots,m\\}\\setminus\\{i\\}$ . For the subproblem with $\\mathbf{w}^{i}$ we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}^{*}\\right\\}>\\operatorname*{max}_{1\\leq j\\leq m}\\left\\{w_{j}^{i}\\cdot\\tilde{z}_{j}\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies ${\\bf z}$ , rather than $\\mathbf{z}^{\\ast}$ , is the optimal objective vector to the boundary subproblem with $\\mathbf{w}^{i}$ . This is a contradiction. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper proposes a general and rigorous method for nadir objective vector estimation, as demonstrated in the abstract and introduction. The main contributions of this paper are summarized at the end of the introduction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The limitations are summarized in the conclusion (Section 5). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The theoretical results are shown in Section 3.1. All proofs are presented in Appendix F. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The implementation details of BDNE are shown in Section 3.3. The experimental setup is detailed in Section 4.1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our code is available at https://github.com/EricZheng1024/BDNE. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201dis an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The full details of experimental settings are available in Section 4.1. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: See Section 4. In this paper, each algorithm is executed 30 times for every instance. The results tables include both the mean and the standard deviation. Wilcoxon\u2019s rank sum test is used to confirm the statistical results. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See Appendix C. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not propose any dataset and potential societal impacts can be found in Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: See Appendix C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]