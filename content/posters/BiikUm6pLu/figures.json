[{"figure_path": "BiikUm6pLu/figures/figures_6_1.jpg", "caption": "Table 1: Running times to compute \u025b-optimal policies in the offline setting. In this table, E denotes an upper bound on the ergodicity of the MDP.", "description": "The table compares the running time and space complexity of different algorithms for computing \u025b-optimal policies in the offline setting of discounted Markov Decision Processes (DMDPs).  The algorithms compared include Value Iteration, Empirical QVI, Randomized Primal-Dual Method, High Precision Variance-Reduced Value Iteration, and the algorithm presented in this paper. The table highlights the improvements achieved by the new algorithm in terms of runtime complexity, particularly when the discount factor is not too small relative to the average sparsity of rows of the transition matrix.  E represents an upper bound on the ergodicity of the MDP, which affects runtime for some algorithms.", "section": "Our results"}]