[{"type": "text", "text": "Truncated Variance-Reduced Value Iteration ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yujia Jin Stanford University yujiajin@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Ishani Karmarkar Stanford University ishanik@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Aaron Sidford Stanford University sdiford@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Jiayi Wang Stanford University jyw@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We provide faster randomized algorithms for computing an $\\varepsilon$ -optimal policy in a discounted Markov decision process with $A_{\\mathrm{{tot}}}$ -state-action pairs, bounded rewards, and discount factor $\\gamma$ . We provide an $\\tilde{O}(A_{\\mathrm{tot}}[(1\\mathrm{~-~}\\gamma)^{-3}\\varepsilon^{-2}\\mathrm{~+~}(1\\mathrm{~-~}\\gamma)^{-2}])$ - time algorithm in the sampling setting, where the probability transition matrix is unknown but accessible through a generative model which can be queried in $\\tilde{O}(1)$ - time, and an $\\tilde{O}(s+\\mathcal{A}_{\\mathrm{tot}}(1-\\gamma)^{-2})$ -time algorithm in the offilne setting where the probability transition matrix is known and $s$ -sparse. These results improve upon the prior state-of-the-art which either ran in $\\tilde{O}\\bar{(}A_{\\mathrm{tot}}[(1-\\gamma)^{-3}\\varepsilon^{-2}+\\bar{(}1-\\gamma)^{-3}])$ time ([1, 2]) in the sampling setting, $\\tilde{O}(s+\\mathcal{A}_{\\mathrm{tot}}(1-\\gamma)^{-3})$ time ([3]) in the offline setting, or time at least quadratic in the number of states using interior point methods for linear programming. We achieve our results by building upon prior stochastic variance-reduce value iteration methods [1, 2]. We provide a variant that carefully truncates the progress of its iterates to improve the variance of new variance-reduced sampling procedures that we introduce to implement the steps. Our method is essentially model-free and can be implemented in $\\Tilde{O}(A_{\\mathrm{tot}}).$ - space when given generative model access. Consequently, our results take a step in closing the sample-complexity gap between model-free and model-based methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Markov decision processes (MDPs) are a fundamental mathematical model for decision making under uncertainty. They play a central role in reinforcement learning and prominent problems in computational learning theory (see e.g., [4, 5, 6, 7]). MDPs have been studied extensively for decades ([8, 9]), and there have been numerous algorithmic advances in efficiently optimizing them $([3,1,2,10,11,12,13,14])$ . ", "page_idx": 0}, {"type": "text", "text": "In this paper, we consider the standard problem of optimizing a discounted Markov Decision Process (DMDP) $\\mathcal{M}=(S,\\mathcal{A},P,r,\\gamma)$ . We consider the tabular setting where there is a known finite set of states $\\boldsymbol{S}$ and at each state $s\\in S$ there is a finite, non-empty, set of actions, $\\mathcal{A}_{s}$ for an agent to choose from; $\\mathcal{A}=\\{(s,a):s\\in\\mathcal{S},a\\in\\mathcal{A}_{s}\\}$ denotes the full set of state action pairs and $A_{\\mathrm{tot}}:=|A|\\geq|S|$ . The agent proceeds in rounds $t=0,1,2,\\ldots$ In each round $t$ , the agent is in state $s_{t}\\in\\mathcal S$ ; chooses action $a_{t}\\in A_{s_{t}}$ , which yields a known reward $\\pmb{r}_{t}=\\pmb{r}_{s_{t},a}\\in[0,1]$ ; and transitions to random state $s_{t+1}$ sampled (independently) from a (potentially) unknown distribution $p_{a}(s_{t})\\,\\in\\,\\Delta^{s}$ for round $t+1$ , where $p_{a}(s_{t})^{\\top}$ is the $(s_{t},a)$ -th row of $P\\in[0,1]^{\\mathcal{A}\\times\\mathcal{S}}$ . The goal is to compute an $\\varepsilon$ -optimal policy, where a (deterministic) policy $\\pi$ , is a mapping from each state $s\\in S$ to an action $\\pi(s)\\in A_{s}$ and is $\\varepsilon$ -optimal if for every initial $s_{0}\\in\\mathcal S$ the expected discounted reward of $\\pi\\,\\mathbb{E}[\\sum_{t\\ge0}r_{t}\\dot{\\gamma}^{t}]$ is at least $\\pmb{v}_{s_{0}}^{*}-\\varepsilon$ . Here, $\\pmb{v}_{s_{0}}^{*}$ is the maximum expected discounted reward of any policy applied starting from initial state $s_{0}$ and $\\pmb{v}^{*}\\in\\mathbb{R}^{S}$ is called the optimal value of the MDP. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Excitingly, a line of work [15, 16, 2, 3, 17, 18] recently resolved the query complexity for solving DMDPs (up to polylogarithmic factors) in what we call the sample setting where the transitions $\\ensuremath{\\boldsymbol{p}}_{a}(\\ensuremath{\\boldsymbol{s}})$ are accessible only through a generative model ([16]). A generative model is an oracle which when queried with any $s\\in S$ and $a\\in\\mathcal{A}_{s}$ returns a random $s^{\\prime}\\in\\mathcal{S}$ sampled independently from $\\ensuremath{\\boldsymbol{p}}_{a}(\\ensuremath{\\boldsymbol{s}})$ [19]. It was shown in [18] that for all $\\varepsilon\\in(0,(1-\\gamma)^{-1}]$ there is an algorithm which computes an $\\varepsilon$ -optimal policy with probability $1\\!-\\!\\delta$ using $\\tilde{O}(A_{\\mathrm{tot}}(1\\!-\\!\\gamma)^{-3}\\varepsilon^{-2})$ queries where we use $\\tilde{O}(\\cdot)$ to hide polylogarithmic factors in $A_{\\mathrm{tot}},\\varepsilon^{-1}$ , $(1-\\bar{\\gamma})^{-1}$ , and $\\delta^{-1}$ . This result improved upon a prior result of [17] which achieved the same query complexity for $\\varepsilon\\in[0,(1-\\gamma)^{-1/2}]$ , of [2] which achieved this query complexity for $\\varepsilon\\in[0,1]$ , and of [16] which achieved it for $\\varepsilon\\in\\bar{[0,(|S|\\,(1-\\gamma))^{-1/2}]}$ . This query complexity is known to be optimal in the worst case (up to polylogarithmic factors) due to lower bounds of [16] (and extensions of [20]), which established that the optimal query complexity for finding $\\varepsilon$ -optimal policies with probability $1-\\delta$ is $\\Omega(A_{\\mathrm{tot}}(1-\\gamma)^{-3}\\varepsilon^{\\frac{\\cdot}{-2}}\\log(\\dot{A}_{\\mathrm{tot}}\\mathring{\\delta}^{-1}))$ . ", "page_idx": 1}, {"type": "text", "text": "Interestingly, recent state-of-the-art results [17, 18] (as well as [16]) are model-based: they query the oracle for every state-action pair, use the resulting samples to build an empirical model of the MDP, and then solve this empirical model. State-of-the-art computational complexities for the methods are then achieved by applying high-accuracy, algorithms for optimizing MDPs in what we call the offilne setting, when the transition probabilities are known [2, 17]. ", "page_idx": 1}, {"type": "text", "text": "Correspondingly, obtaining optimal query complexities for large $\\varepsilon$ , e.g., $\\varepsilon\\gg1$ , comes with certain costs. This setting is of interest when the goal is to efficiently compute a coarse approximation of the optimal policy. Model-based methods use space $\\Omega(A_{\\mathrm{tot}}\\cdot\\mathrm{{min}}((1-\\gamma)^{-3}\\varepsilon^{-2},|\\hat{S}|))$ \u2013rather than the $\\tilde{O}(A_{\\mathrm{tot}})$ memory used by model-free methods (e.g., [2, 3, 21]), which run stochastic, low memory analogs of classic popular algorithms for solving DMDPs (e.g., value iteration). Moreover, although state-of-the-art model-based methods use $\\Omega(\\bar{A_{\\mathrm{tot}}}(1-\\gamma)^{-3}\\varepsilon^{-2})$ samples, the state-of-the-art runtime to compute the optimal policy is either $\\tilde{O}(A_{\\mathrm{tot}}(1-\\gamma)^{-3}\\operatorname*{max}\\{1,\\varepsilon^{-2}\\})$ (using [2]) or has a larger larger polynomial dependence on $A_{\\mathrm{{tot}}}$ and $|{\\cal S}|$ by using interior point methods (IPMs) for linear programming (see Section 1.1). Consequently, in the worst case, the runtime cost per sample is more than polylogarithmic for $\\varepsilon$ sufficiently larger than 1, and it is natural to ask if this can be improved. ", "page_idx": 1}, {"type": "text", "text": "These costs are connected to the state-of-the-art runtimes for optimizing DMDPs in the offilne setting. Ignoring IPMs (discussed in Section 1.1), the state-of-the-art runtime for optimizing a DMDP is $\\tilde{O}(\\mathrm{nnz}(P)+A_{\\mathrm{tot}}(1-\\gamma)^{-3})$ due to [2] where $\\mathrm{nnz}(P)$ denotes the number of non-zero entries in $_{P}$ , i.e., the number of triplets $(s,s^{\\prime},a)$ where taking action $a\\in\\mathcal{A}_{s}$ at state $s\\in S$ has a non-zero probability of transitioning to $s^{\\prime}\\in\\mathcal{S}$ . This method is essentially model-free; it simply performs a variant of stochastic value iteration where passes on $_{P}$ are used to reduce the variance of sampling and can be implemented in $\\tilde{O}(A)$ -space given access to a generative model and the ability to multiply $_{P}$ with vectors. The difficulty in further improving the runtimes in the sample setting and improving the performance of model-free methods seems connected to the difficulty in improving the additive $\\bar{A_{\\mathrm{tot}}}(1-\\gamma)^{-3}$ -term in this runtime (see the discussion in Section 1.2.) ", "page_idx": 1}, {"type": "text", "text": "In this paper, we ask whether these complexities can be improved. Is it possible to lower the memory requirements of near-optimal query algorithms for large $\\varepsilon$ ? Can we improve the runtime for optimizing MDPs in the offilne setting and can we improve the computational cost per sample in computing optimal policies in DMDPs? More broadly, is it possible to close the sample-complexity gap between model-free and model-based methods for optimizing DMDPs? ", "page_idx": 1}, {"type": "text", "text": "1.1 Our results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we show how to answer each of these motivating questions in the affirmative. We provide faster algorithms for optimizing DMDPs in both the sample and offline setting that are implementable in $\\tilde{O}(A_{\\mathrm{tot}})$ -space provided suitable access to the input. In addition to computing $\\varepsilon$ -optimal policies, these methods also compute $\\varepsilon$ -optimal values: we call any $\\pmb{v}\\in\\mathbb{R}^{S}$ a value vector and say that it is $\\varepsilon$ -optimal if $\\|\\pmb{v}-\\pmb{v}^{*}\\|_{\\infty}\\leq\\varepsilon$ . ", "page_idx": 1}, {"type": "text", "text": "Here we present our main results on algorithms for solving DMDPs in sample setting and in the offilne setting and compare to prior work. For simplicity of comparison, we defer any discussion and comparison of DMDP algorithms that use general IPMs for linear program to the end of this section. The state-of-the-art such IPM methods obtain improved running times but use $\\Omega(|S|^{2})$ space and $\\Omega(|S|^{2})$ time and use general-purpose linear system solvers. As such they are perhaps qualitatively different from the more combinatorial or dyanmic-programming based methods, e.g., value iteration and stochastic value iteration, more commonly discussed in this introduction. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the sample setting, our main result is an algorithm that uses $\\tilde{O}(A_{\\mathrm{tot}}[(1-\\gamma)^{-3}\\varepsilon^{-2}+(1-\\gamma)^{-2}])$ samples and time and $O(A_{\\mathrm{tot}})$ -space. It improves upon the prior, non-IPM, state-of-the-art which uses $\\tilde{O}(A_{\\mathrm{tot}}[(1-\\gamma)^{-3}\\varepsilon^{-2}+(1-\\gamma)^{-3}])$ time [3] and nearly matches the state-of-the-art sample complexity for all $\\varepsilon=O((1-\\gamma)^{-1/2})$ . See Table 2 for a more complete comparison. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1. In the sample setting, there is an algorithm that uses $\\tilde{O}(\\mathcal{A}_{\\mathrm{tot}}[(1{-}\\gamma)^{-3}\\varepsilon^{-2}{+}(1{-}\\gamma)^{-2}])$ samples and time and $O(A_{\\mathrm{tot}})$ space, and computes an $\\varepsilon$ -optimal policy and $\\varepsilon$ -optimal values with probability $1-\\delta$ . ", "page_idx": 2}, {"type": "text", "text": "Particularly excitingly, the algorithm in Theorem 1.1 runs in time nearly-linear in the number of samples whenever $\\varepsilon=O((1-\\gamma)^{-1/2})$ and therefore, provided querying the oracle costs $\\Omega(1)$ , has a near-optimal runtime for such $\\varepsilon$ ! Prior to this work such a near-optimal, non-IPM, runtime (for non-trivially small $\\gamma$ ) was only known for $\\varepsilon=\\tilde{O}(1)$ ([2]). Similarly, Theorem 1.1 shows that there are model-free algorithms (which for our purposes we define as an $\\tilde{O}(A_{\\mathrm{tot}})$ space algorithm) which are nearly-sample optimal whenever $\\varepsilon=O((1-\\gamma)^{-1/2})$ . Previously this was only known for $\\varepsilon=\\tilde{O}(1)$ . As discussed in prior-work ([18, 17]), this large $\\varepsilon$ regime is potentially of particular importance in large-scale learning settings, where one would like to quickly compute a coarse approximation of the optimal policy. ", "page_idx": 2}, {"type": "text", "text": "In the offline setting, our main result is an algorithm that uses $\\tilde{O}(\\mathrm{nnz}(P)+A_{\\mathrm{tot}}(1-\\gamma)^{-2})$ time. It improves upon the prior, non-IPM, state-of-the-art which use $\\tilde{O}(\\mathrm{nnz}(P)+A_{\\mathrm{tot}}(1-\\gamma)^{-3})$ time ([2]). See Table 1 for a more complete comparison with prior work. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.2. In the offline setting, there is an algorithm that uses $\\tilde{O}(\\mathrm{nnz}(P)+A_{\\mathrm{tot}}(1-\\gamma)^{-2})$ time, and computes an $\\varepsilon$ -optimal policy and $\\varepsilon$ -optimal values with probability $1-\\delta$ . ", "page_idx": 2}, {"type": "text", "text": "The method of Theorem 1.2 runs in nearly-linear time when $(1-\\gamma)^{-1}\\leq(\\mathrm{nnz}(P)/A_{\\mathrm{tot}})^{1/2}$ , i.e., the discount factor is not too small relative to the average sparsity of rows of the transition matrix. Prior to this paper, such nearly-linear, non-IPM, runtimes (for non-trivially small $\\gamma$ ) were only known for $(1-\\gamma)^{-1}\\overset{\\cdot}{\\leq}(\\mathrm{nnz}(P)/\\dot{A}_{\\mathrm{tot}})^{1/3}$ ([2]). Thus, Theorem 1.2 expands the set of DMDPs which can be solved in nearly-linear time. The space usage and input access for this offline algorithm differs from the algorithm in Theorem 1.1 in that the algorithm in Theorem 1.2 assumes that access to the transition $_{P}$ is provided as input and uses this to compute matrix-vector products with value vectors. The algorithm in Theorem 1.2 also requires access to samples from the generative model; if access to the generative model is not provided as input, then using the access to $_{P}$ , the algorithm can build a $\\tilde{O}(\\mathrm{nnz}(P))$ data-structure so that queries to the generative model can be implemented in $\\tilde{O}(1)$ time (e.g., see discussion in [2]). Hence, if matrix-vector products and queries to the generative model can be implemented in $\\tilde{O}(A_{\\mathrm{tot}})$ -space then so can the algorithm in Theorem 1.2. ", "page_idx": 2}, {"type": "table", "img_path": "BiikUm6pLu/tmp/02ca6cfcf43d6c3a06a7c456abd800cdb8560c3351db8f7fcd123e6c0579d3cd.jpg", "table_caption": ["Table 1: Running times to compute $\\varepsilon$ -optimal policies in the offline setting. In this table, $E$ denotes an upper bound on the ergodicity of the MDP. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Exact DMDP Algorithms. In our our comparison of offline DMDP algorithms in Table 1, we ignored poly $\\bar{(\\log(\\varepsilon^{-1}))}$ -factors. Consequently, we did not distinguish between algorithms which solve DMDPs to high accuracy, i.e., only depend on $\\varepsilon$ polylogarithmically, and those which solve it exactly, e.g., have no dependence on $\\varepsilon$ . There is a line of work on designing such exact methods and the current state-of-the-art is policy iteration, which can be implemented in $\\tilde{O}(\\left|S\\right|^{2}A_{\\mathrm{tot}}^{2}(1-\\gamma)^{-1})$ time ([13, 14]) and a combinatorial interior point method that can be implemented in $\\tilde{O}(A_{\\mathrm{tot}}^{4})$ time ([10] with no dependence on $\\varepsilon$ . Note that these methods obtain improved runtime dependence on $\\varepsilon$ at the cost of larger dependencies on $|{\\mathcal{S}}|$ and $A_{\\mathrm{{tot}}}$ . ", "page_idx": 3}, {"type": "table", "img_path": "BiikUm6pLu/tmp/08e84c698df6ab0f72dd42c6bbd04fe313ee3c40cb67a9d5ff273d28c25031f5.jpg", "table_caption": ["Table 2: Query complexities to compute $\\varepsilon$ -optimal policy in the sample setting. $M_{\\mathrm{erg}}$ denotes an upper bound on the MDP\u2019s ergodicity. Here, model-free refers to $\\tilde{O}(A_{\\mathrm{tot}})$ space methods. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Comparison with IPM Approaches. In the offline setting, [2] showed how to reduce solving DMDPs to an $\\ell_{1}$ -regression problem in $\\textbf{\\textit{P}}\\in\\mathbb{R}^{\\varDelta\\times\\varDelta}$ . For $\\ell_{1}$ regression in a matrix $A\\,\\in\\,\\mathbb{R}^{n\\times{\\bar{d}}}$ for $n\\,>\\,d$ , [12] provides an algorithm that runs in $\\tilde{O}(d^{0.5}(\\mathrm{nnz}(A)+d^{2}))$ -time, [24] provides an algorithm that runs in ${\\tilde{O}}(n d+d^{2.5})$ , and [25, 26, 27] yields an algorithm that runs in $\\tilde{O}(A_{\\mathrm{tot}}^{\\omega})$ time for the current value of the fast matrix multiplication exponent $\\omega<2.371552$ [28]. These offline IPM approaches can be coupled with model-based approaches to yield algorithms in the sample setting. [18] shows that given a DMDP $\\mathcal{M}$ , with $\\tilde{O}\\left(\\overbar{A}_{\\mathrm{tot}}(1-\\gamma)^{-\\dot{2}}\\varepsilon^{-3}\\right)$ queries to the generative model and time, one can construct a DMDP $\\hat{\\mathcal{M}}$ such that an optimal policy in $\\hat{\\mathcal{M}}$ is an $\\varepsilon,$ -optimal for $\\mathcal{M}$ . Consequently, provided polynomial accuracy in computing the policy suffices, applying the IPMs to $\\hat{\\mathcal{M}}$ yields runtimes of $\\tilde{O}(\\operatorname{nnz}(P)\\sqrt{|S|}+|S|^{2.5})$ ([12]), $\\tilde{O}(A_{\\mathrm{tot}}\\left|S\\right|+\\left|S\\right|^{2.5})$ ([24]), and $\\tilde{O}(A_{\\mathrm{tot}}^{\\omega})$ time [25]. This combination of model-based and IPM-based approaches use super-quadratic time and space, but they may yield better runtimes than Theorem 1.2 in certain regimes where $\\gamma$ is sufficiently large relative to $\\boldsymbol{S}$ and $A_{\\mathrm{{tot}}}$ in the offline setting, or when, additionally, $\\varepsilon$ is sufficiently small relative to $\\boldsymbol{S}$ and $A_{\\mathrm{{tot}}}$ in the sample setting. ", "page_idx": 3}, {"type": "text", "text": "1.2 Overview of approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we provide an overview of our approach to proving Theorem 1.1 and Theorem 1.2. We motivate our approach from previous methods and discuss the main obstacles and insights needed to obtain our results. For simplicity, we focus on the problem of computing $\\varepsilon$ -optimal values and discuss computing $\\varepsilon$ -optimal policies at the end of this section. ", "page_idx": 3}, {"type": "text", "text": "Value iteration. Our approach stems from classic value-iteration method ([22, 11]) for computing $\\varepsilon$ -optimal and its more modern $Q$ -value and stochastic counterparts ([16, 3, 29, 30, 31, 32]). As the name suggests, value iteration proceeds in iterations $t=0,1,\\dots$ computing values, $\\pmb{v}^{(t)}\\in\\mathbb{R}^{S}$ . Starting from initial ${\\pmb v}^{(0)}\\in\\mathbb{R}^{S}$ , in iteration $t\\geq1$ , the value vector $\\pmb{v}^{(t)}$ is computed as the result of applying the (Bellman) value operator $\\mathcal{T}:\\mathbb{R}^{S}\\mapsto\\mathbb{R}^{S}$ , i.e., ", "page_idx": 4}, {"type": "text", "text": "It is well-known that the value operator is $\\gamma$ -contractive and therefore, $\\begin{array}{r l}{\\|\\mathcal{T}(\\pmb{v})-\\pmb{v}^{*}\\|_{\\infty}}&{{}\\leq}\\end{array}$ $\\gamma\\left\\|\\pmb{v}-\\pmb{v}^{*}\\right\\|_{\\infty}$ for all $v\\in\\mathbb{R}^{S}$ ([11, 22, 2]). If we initialize $\\pmb{v}^{(0)}=\\mathbf{0}$ then since $\\|\\pmb{v}^{*}\\|_{\\infty}\\leq(1-\\gamma)^{-1}$ [22, 11], we see that $\\|v^{(t)}-v^{*}\\|_{\\infty}\\leq\\gamma^{t}\\|v^{(0)}-v^{*}\\|_{\\infty}\\leq\\gamma^{t}(1-\\gamma)^{-1}\\leq(1-\\gamma)^{-1}\\exp(-t(1-\\gamma))$ . Thus, $\\pmb{v}^{(t)}$ are $\\varepsilon$ -optimal values for any $t~\\geq~(1~-~\\gamma)^{-1}\\log(\\varepsilon^{-1}(1~-~\\gamma)^{-1})$ . This yields an $\\tilde{O}(\\mathrm{nnz}(P)(1-\\gamma)^{-1})$ time algorithm in the offline setting. ", "page_idx": 4}, {"type": "text", "text": "Stochastic value iteration and variance reduction. To improve on the runtime of value iteration and apply it in the sample setting, a line of work implements stochastic variants of value iteration ([16, 2, 3, 23, 17, 18]). Those methods take approximate value iteration steps where the expected utilities ${p_{a}}(s)^{\\top}{v}$ in (1) for each state-action pair are replaced by a stochastic estimate of the expected utilities. In particular, note that $\\pmb{p}_{a}(s)^{\\top}\\pmb{v}=\\mathbb{E}_{i\\sim\\pmb{p}_{a}(s)}\\pmb{v}_{i}.$ , i.e., the expected value of $\\pmb{v}_{i}$ where $i$ is drawn from the distribution given by $\\ensuremath{\\boldsymbol{p}}_{a}(\\ensuremath{\\boldsymbol{s}})$ . This is compatible in the sample setting, as computing $\\pmb{v}_{i}$ for $i$ drawn from $\\ensuremath{\\boldsymbol{p}}_{a}(\\ensuremath{\\boldsymbol{s}})$ yields an unbiased estimate of ${p_{a}}(s)^{\\top}{v}$ with 1 query and $O(1)$ time. ", "page_idx": 4}, {"type": "text", "text": "State-of-the-art model-free methods in the sample setting ([3]) and non-IPM runtimes in the offline setting ([3]) improve further by more carefully approximating the expected utilities ${p_{a}(s)^{\\top}}v$ of each state-action pair $(s,a)\\,\\in\\,A$ . Broadly, given an arbitrary $\\pmb{v}^{(0)}$ they first compute $\\textbf{\\em x}\\in\\mathbb{R}^{A}$ that approximates ${\\pmb P}{\\pmb v}^{(0)}$ , i.e., $\\mathbf{\\deltax}_{a}(s)$ approximates $[{\\pmb{P}}{\\pmb{v}}^{(0)}]_{(s,a)}={\\pmb{p}}_{a}(s)^{\\top}{\\pmb{v}}^{(0)}$ for all $(s,a)\\in A$ In the offline setting, $\\pmb{x}=P\\pmb{v}^{(0)}$ can be computed directly in $O(\\mathrm{nnz}(P))$ -time. In the sample setting, $\\boldsymbol{x}\\approx\\boldsymbol{P}\\boldsymbol{v}^{(0)}$ can be approximated to sufficient accuracy using multiple queries for each state-action pair. Then, in each iteration $t\\geq1$ of the algorithm, fresh samples are taken to compute $\\pmb{g}^{(t)}\\approx P(\\pmb{v}^{(t-1)}-\\pmb{v}^{(0)})$ and perform the following update: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{(t)}(s)\\leftarrow\\underset{a\\in\\mathcal{A}_{s}}{\\operatorname*{max}}(r_{a}(s)+\\gamma(\\mathbf{\\varalpha}_{a}(s)+\\mathbf{\\boldsymbol{g}}_{a}(s)^{(t)})\\mathrm{~for~all~}s\\in S\\mathrm{~and~}v\\in\\mathbb{R}^{S}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This approach is advantageous because sampling errors for estimating $P(\\boldsymbol{v}^{(t-1)}-\\boldsymbol{v}^{(0)})$ depend on the magnitude of $\\pmb{v}^{(t-1)}-\\pmb{v}^{(0)}$ . After approximately computing $\\textbf{\\em x}$ , the remaining task of computing $\\pmb{g}^{(t)}\\approx\\mathbf{\\bar{\\alpha}}\\mathbf{P}(\\pmb{v}^{(t-1)}\\!-\\!\\pmb{v}^{(0)})$ so that $\\pmb{x}{+}\\pmb{g}^{(t)}\\overset{\\quad}{\\approx}P\\pmb{v}^{(t-1)}$ may be easier than the task of directly estimating $\\b{P v}^{(t)}$ (since $\\pmb{v}^{(t-1)}-\\pmb{v}^{(0)}$ is smaller in magnitude than $\\pmb{v}^{(t)}$ entrywise.) Due to similarities of this approach to variance-reduced optimization methods, e.g. ([33, 34]), this technique is called variance reduction [2]. ", "page_idx": 4}, {"type": "text", "text": "The works [2, 3], showed that if $\\textbf{\\em x}$ is computed sufficiently accurately and $\\pmb{v}^{(0)}$ are $\\alpha$ -optimal values then applying (2) for $t=\\Theta((1-\\gamma)^{-1})$ yields $\\pmb{v}^{(t)}$ that is $\\alpha/2$ -optimal in just $\\tilde{O}(\\bar{A_{\\mathrm{tot}}}(1-\\gamma)^{-3})$ time and samples! [2] leverages this technique to compute $\\varepsilon$ -optimal values in the offline setting in $\\tilde{O}(\\mathrm{nnz}(P)+A_{\\mathrm{tot}}(1-\\gamma)^{-3})$ time. [3] uses a similar approach to compute $\\varepsilon$ -optimal values in $\\tilde{O}(A_{\\mathrm{tot}}[(1-\\gamma)^{-3}\\varepsilon^{-2}+(1-\\gamma)^{-3})$ time and samples in the sample setting. A key difference in [2] and [3] is the accuracy to which they must approximate the initial utility $x\\approx P v^{(0)}$ . ", "page_idx": 4}, {"type": "text", "text": "Recursive variance reduction. To improve upon the prior model-free approaches of [2, 3] we improve how exactly the variance reduction is performed. We perform a similar scheme as in (2) and use essentially the same techniques as in [3, 2] towards estimating $\\textbf{\\em x}$ . Where we differ from prior work is in how we estimate the change in approximate utilities $\\bar{\\pmb{g^{(t)}}}\\approx\\pmb{P}(\\pmb{v}^{(t-1)}-\\pmb{v}^{(0)})$ . Rather than directly sampling to estimate this difference we instead sample to estimate each individual $P(v^{(t-1)}-v^{(t)})$ and maintain the sum. Concretely, for $t\\geq1$ , we compute $\\Delta^{(t)}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta^{(t)}\\approx P(\\boldsymbol{v}^{(t)}-\\boldsymbol{v}^{(t-1)})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "so that these recursive approximations telescope. More precisely, setting $\\pmb{g}^{(0)}=\\mathbf{0}$ , for $t\\geq1$ , we set ", "page_idx": 4}, {"type": "equation", "text": "$$\ng^{(t)}\\gets g^{(t-1)}+\\Delta^{(t-1)}\\approx P(v^{(t-2)}-v^{(0)})+P(v^{(t-1)}-v^{(t-2)})=P(v^{(t-1)}-v^{(0)}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This difference is perhaps similar to how methods such as SARAH ([34]) differ from SVRG ([33]). Consequently, we similarly call this approximation scheme recursive variance reduction. Interestingly, in constrast to the finite sum setting considered in [33, 34], in our setting, recursive variance reduction for solving DMDPs ultimately leads to direct quantitative improvements on worst case complexity. ", "page_idx": 5}, {"type": "text", "text": "To analyze this recursive variance reduction method, we treat the error in $\\pmb{g}^{(t)}\\approx\\pmb{P}(\\pmb{v}^{(t-1)}-\\pmb{v}^{(0)})$ as a martingale and analzye it using Freedman\u2019s inequality [35] (as stated in [36]). The hope in applying this approach is that by better bounding and reasoning about the changes in $\\pmb{v}^{(t)}$ , better bounds on the error of the sampling could be obtained by leveraging structural properties of the iterates. ", "page_idx": 5}, {"type": "text", "text": "Unfortunately, without further information about the change in $\\pmb{v}^{(t)}$ or larger change to the analysis of variance reduced value iteration, in the worst case, the variance can be too large for this approach to work naively. Concretely, prior work ([2]) showed that it sufficed to maintain that $\\|\\bar{\\pmb{g}^{(i+1)}}-$ $P v^{(t)}\\|_{\\infty}\\,\\leq\\,O((1-\\gamma)\\alpha)$ . However, imagine that $\\pmb{v}^{*}\\,=\\,\\alpha\\mathbf{1}$ , ${\\pmb v}^{(0)}\\,=\\,{\\bf0}$ , and in each iteration $t$ one coordinate of $\\pmb{v}^{(t)}-\\pmb{v}^{(t-1)}$ is $\\Omega(\\alpha)$ . If $|S|\\approx(1-\\gamma)^{-1}$ and $\\|p_{a}(s)\\|_{\\infty}=O(1/|S|)$ for some $(s,a)\\in A$ then the variance of each sample used to estimate $\\begin{array}{r}{{p}_{a}(s)^{\\top}({\\boldsymbol v}^{(t)}-{\\boldsymbol v}^{(t-1)})=\\Omega(1/|\\mathcal{S}|)=}\\end{array}$ $\\Omega((1-\\gamma))$ . Applying Freedman\u2019s inequality, e.g., [36], and taking $b$ samples for eac\u221ah $O((1-\\gamma)^{-1})$ iteration would yield, roughly, $\\|g^{(t+1)}-P(v^{(t)}-v^{(0)})\\|_{\\infty}=O((1-\\gamma)^{-1}(1-\\gamma)/\\sqrt{b})=O(1/\\sqrt{b})$ Consequently $\\dot{b}=\\Omega((1-\\dot{\\gamma})^{-2})$ and $\\Omega((1-\\gamma)^{-3})$ samples would be needed in total, i.e., there is no improvement. Next, we will discuss how we circumvent this obstacle by combining recursive variance reduction with a second algorithm technique, which we call truncation. ", "page_idx": 5}, {"type": "text", "text": "Truncated-value iteration. The key insight to make our new recursive variance reduction scheme for value iteration yield faster runtimes is to modify the value iteration scheme itself. Recall that in the previous paragraph, we described that the case challenging case for recursive variance reduction occurs when, for example, in every iteration, a single coordinate of $v$ changes by $\\Omega(\\alpha)$ . We observe that there is a simple modification that one could make to value iteration to ensure that there is not such a large change between each iteration; simply truncate the change in each iteration so that no coordinate of $\\pmb{v}^{(t)}$ changes too much! To motivate our algorithm, consider the following truncated variant of value iteration where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{v}^{(t)}=\\mathrm{median}(\\boldsymbol{v}^{(t-1)}-(1-\\gamma)\\alpha,\\mathcal{T}(\\boldsymbol{v}^{(t-1)}),\\boldsymbol{v}^{(t-1)}+(1-\\gamma)\\alpha)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where median applies the median of the arguments entrywise. In other words, suppose we apply value iteration where we decrease or truncate the change from $\\pmb{v}^{(t-1)}$ to $\\pmb{v}^{(t)}$ so that it is no more than $(1-\\gamma)\\alpha$ in absolute value in any coordinate. Then, provided that $\\pmb{v}^{(t)}$ is $\\alpha$ -optimal, we can show that it is still the case that $\\|\\pmb{v}^{(t)}-\\pmb{v}^{*}\\|_{\\infty}\\,\\le\\,\\gamma\\|\\pmb{v}^{(t-1)}-\\pmb{v}^{*}\\|_{\\infty}$ . In other words, the worst-case progress of value iteration is unaffected! This follows immediatly from the fact that $\\|\\pmb{v}^{(t)}-\\pmb{v}^{*}\\|_{\\infty}\\stackrel{=}{\\leq}\\gamma\\|\\pmb{v}^{(t-1)}-\\pmb{v}^{*}\\|_{\\infty}$ in value iteration and the following simple technical lemma. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1.3. For $a,b,x\\in\\mathbb{R}^{n}$ and $\\gamma$ , $\\alpha>0$ , let $\\begin{array}{r}{c:=\\mathrm{median}\\{a-(1-\\gamma)\\alpha{\\bf1},b,a+(1-\\gamma)\\alpha{\\bf1}\\},}\\end{array}$ , where median is applied entrywise. Then, $i f\\left\\|b-x\\right\\|_{\\infty}\\leq\\gamma\\left\\|\\dot{\\pmb{a}}-\\pmb{x}\\right\\|_{\\infty}$ and $\\|\\pmb{a}-\\pmb{x}\\|_{\\infty}\\leq\\alpha$ , then $\\left\\|c-x\\right\\|_{\\infty}\\leq\\gamma\\left\\|a-x\\right\\|_{\\infty}$ . ", "page_idx": 5}, {"type": "text", "text": "Applying truncated value iteration, we know that $\\|v^{(t)}-v^{(t-1)}\\|_{\\infty}\\leq(1-\\gamma)\\alpha$ . In other words, the worst-case change in a coordinate has decreased by a factor of $(1-\\gamma)$ ! We show that this smaller movement bound does indeed decrease the variance in the martingale when using the aforementioned averaging scheme. We show this truncation scheme, when combined with our recursive variance reduction scheme (4) for estimating $\\pmb{P}(\\pmb{v}^{(t)}-\\pmb{v}^{(0)})$ , reduces the total samples required to estimate this and halve the error from ${\\tilde{O}}((1-\\gamma)^{-3})$ to just $\\tilde{O}((1-\\gamma)^{-2}$ per state-action pair. ", "page_idx": 5}, {"type": "text", "text": "Our method. Our algorithm applies stochastic truncated value iteration using sampling to estimate each $\\pmb{g}^{(t)}\\approx\\pmb{P}(\\pmb{v}^{(t)}-\\bar{\\pmb{v}}^{(0)})$ as described. Some minor additional modifications are needed, however, to obtain our results. Perhaps the most substantial is our use of the monotonicity technique, as in prior work ([2, 3]). That is, we modify our method so that each $\\pmb{v}^{(t)}$ is always an underestimate of $v^{*}$ and the $\\pmb{v}^{(t)}$ increase monotonically as $t$ increases. Thus, we only truncate the increase in the $\\pmb{v}^{(t)}$ (since they do not decrease, and the median operation in (5) reduces to a minimum in Lemma 1.3). ", "page_idx": 5}, {"type": "text", "text": "Beyond simplifying this aspect of the algorithm, as in prior work, this monotonicity technique allows us to simultaneously compute an $\\varepsilon$ -approximate policy as well as an $\\varepsilon$ -optimal value vector. We do this by tracking the actions associated with changed $\\pmb{v}^{(t)}$ values, i.e., the argmax in (2) in a variable $\\pi^{(t)}$ , which denotes the current estimated policy in iteration $t$ of value iteration. Concretely, the monotonicity technique allows us to maintain the invariant that at each iteration $t$ , the current value estimate and policy estimate $\\pi^{(t)},v^{(t)}$ satisfy the relation $\\pmb{v}^{(t)}\\leq\\mathcal{T}[\\pmb{v}^{(t)}]$ . Note that this ensures that the value of $\\pi^{(t)}$ (denoted $\\pmb{v}^{\\pi^{(t)}}$ ) is at least $\\pmb{v}^{(t)}$ because ", "page_idx": 5}, {"type": "image", "img_path": "BiikUm6pLu/tmp/b20d539cd05dfaac5394e6d0afbdc4e5da6f5311e829ffe8b6e11f79ecc4f32d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{v}^{(t)}\\leq\\mathcal{T}[\\pmb{v}^{(t)}]\\leq\\mathcal{T}^{2}[\\pmb{v}^{(t)}]\\leq\\cdot\\cdot\\cdot\\mathcal{T}^{\\infty}[\\pmb{v}^{(t)}]=\\pmb{v}^{\\pi^{(t)}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, whenever $\\pmb{v}^{(t)}$ is an $\\varepsilon$ -optimal value, $\\pi^{(t)}$ is an at least $\\varepsilon,$ -optimal policy. ", "page_idx": 6}, {"type": "text", "text": "By computing initial expected utilities $\\pmb{x}=P\\pmb{v}^{(0)}$ exactly, we obtain our offilne results. By carefully estimating $x\\approx P v^{(0)}$ as in [3] we obtain our sampling results. Finally, building off of the analysis of [37] for deterministic or highly-mixing MDPs, we also show our method obtains even faster convergence guarantees under additional non-worst-case assumptions on the MDP structure. ", "page_idx": 6}, {"type": "text", "text": "1.3 Notation and paper outline ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "General notation. Caligraphic upper case letters denote sets and operators, lowercase boldface letters denote vectors, and uppercase boldface letters (e.g., $P,I)$ denote matrices. 0 and 1 denote the all-ones and all-zeros vectors, $[m]:=\\{1,...,m\\}$ , and $\\Delta^{n}:=\\left\\{x\\in\\mathbb{R}^{n}:\\mathbf{0}\\leq x\\right.$ and $\\left\\|x\\right\\|_{1}=1\\}$ is the simplex. For $\\pmb{v}\\in\\mathbb{R}^{S}$ , we use $\\pmb{v}_{i}$ or ${\\pmb v}(i)$ for the $i$ -th entry of vect\u221aor $\\pmb{v}$ . For vectors $\\pmb{v}\\in\\mathbb{R}^{A}$ , we use ${\\pmb v}_{a}(s)$ to denote the $(s,a)$ -th entry of $\\pmb{v}$ , where $(s,a)\\in A$ . We use $\\sqrt{\\pmb{v}},\\pmb{v}^{2},|\\pmb{v}|\\in\\mathbb{R}^{n}$ for the elementwise square root, square, and absolute value of $\\pmb{v}$ respectively and max $\\{\\boldsymbol{u},\\boldsymbol{v}\\}$ and median $\\{u,v,w\\}$ for element-wise maximum and median respectively. For $\\pmb{v},\\pmb{x}\\in\\mathbb{R}^{n}$ , $\\pmb{v}\\leq\\pmb{x}$ denotes that ${\\pmb v}(i)\\leq{\\pmb x}(i)$ for each $i\\;\\in\\;[n]$ (analogously for $<,\\geq,>.$ ) We call $\\pmb{x}\\in\\mathbb{R}^{n}$ an $\\alpha$ -underestimate of $\\pmb{y}\\in\\mathbb{R}^{n}$ if ${\\pmb y}-\\alpha{\\bf1}\\leq{\\pmb x}\\leq{\\pmb y}$ for $\\alpha\\geq0$ (see the discussion of monotonicity in Section 1.2 for motivation). ", "page_idx": 6}, {"type": "text", "text": "DMDP. As discussed, the objective in optimizing a DMDP is to find an $\\varepsilon$ -approximate policy $\\pi$ and values. For a policy $\\pi$ , we use $\\mathcal{T}_{\\pi}(\\pmb{u}):\\mathbb{R}^{\\dot{S}}\\mapsto\\mathbb{R}^{S}$ to denote the value operator associated with $\\pi$ , i.e., $\\begin{array}{r}{\\mathcal{T}_{\\pi}(\\pmb{u})(s):=\\pmb{r}_{\\pi(s)}(s)+\\gamma\\pmb{p}_{\\pi(s)}(s)^{\\top}\\pmb{u}}\\end{array}$ for all value vectors $\\pmb{u}\\in\\mathbb{R}^{S}$ and $s\\in S$ . We let $v^{\\pi}$ denote the unique value vector such that $\\boldsymbol{\\mathcal{T}}_{\\pi}(\\boldsymbol{v}^{\\pi})=\\boldsymbol{v}^{\\pi}$ and define its variance as $\\sigma_{{\\pmb u}^{\\pi}}:=P^{\\pi}({\\pmb u}^{\\pi})^{2}-(P^{\\pi}{\\pmb u}^{\\pi})^{2}$ , where $P^{\\pi}\\in\\mathbb{R}^{S\\times S}$ is the matrix such that $P_{s,s^{\\prime}}^{\\pi}=P_{s,\\pi(s)}\\!\\left(s^{\\prime}\\right)$ . The optimal value vector $\\pmb{v}^{\\star}\\in\\mathbb{R}^{S}$ of the optimal policy $\\pi^{\\star}$ is the unique vector with $\\boldsymbol{\\mathcal{T}}(\\boldsymbol{v}^{\\star})=\\boldsymbol{v}^{\\star}$ , and $P^{\\star}\\in\\mathbb{R}^{S\\times S}:=P^{\\pi^{\\star}}$ . ", "page_idx": 6}, {"type": "text", "text": "Outline. Section 2 presents our offline setting results and Section 3 our sample setting results.   \nSection A discusses specialized settings where we can obtain even faster convergence guarantees.   \nOmitted proofs are deferred to Appendix B. ", "page_idx": 6}, {"type": "text", "text": "2 Offline algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present our high-precision algorithm for finding an approximately optimal policy in the offilne setting. We first define Sample (Algorithm 1), which approximately computes products between $\\pmb{p}\\in\\Delta^{S}$ and a value vector $\\pmb{u}\\in\\mathbb{R}^{S}$ using samples from a generative model. The following lemma states some immediate estimation bounds on Sample using linearity and the fact that $p\\in\\Delta^{\\mathcal{S}}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 2.1. Let $\\boldsymbol{x}\\,=\\,\\mathtt{S a m p l e}(\\boldsymbol{u},\\boldsymbol{p},M,0)$ for $\\pmb{p}\\in\\Delta^{n}$ , $M\\,\\in\\,\\mathbb{Z}_{>0}$ , $\\varepsilon\\,>\\,0$ , and $\\pmb{u}\\in\\mathbb{R}^{S}$ . Then, $\\mathbb{E}\\left[x\\right]=p^{\\top}u,$ , $|x|\\leq\\|u\\|_{\\infty}$ , and Var $[x]\\leq1/M\\left\\|u\\right\\|_{\\infty}^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "We can naturally apply Sample to each state-action pair in $\\mathcal{M}$ as in the subroutine ApxUtility (Algorithm 2). If $\\pmb{x}\\,=\\,\\mathtt{A p x U t i l i t y}(\\pmb{u},M,\\eta)$ , then $\\pmb{x}(s,a)$ is an estimate of the expected utility of taking action $a\\in\\mathcal{A}_{s}$ from state $s\\in S$ (as discussed in Section 1.2). When $\\eta>0$ , this estimate may potentially be shifted to increase the probability that $\\textbf{\\em x}$ underestimates the true changes in utilities; we leverage this in Section 3 (see also the discussion of monotonicity in Section 1.2). The terms arising in the definition of $\\tilde{x}$ arise from applying Bernstein\u2019s inequality (Theorem B.2) to guarantee that $\\tilde{x}\\leq x-\\eta$ with high probability. ", "page_idx": 7}, {"type": "text", "text": "The following algorithm TVRVI (Algorithm 3) takes as input an initial value vector $\\pmb{v}^{(0)}$ and policy $\\pi^{(0)}$ such that $\\pmb{v}^{(\\bar{0})}$ is an $\\alpha$ -underestimate of $v^{\\star}$ along with an approximate offset vector $\\textbf{\\em x}$ , which is a $\\beta$ -underestimate of ${\\pmb P}{\\pmb v}^{(0)}$ . It runs runs $L=\\tilde{O}((1-\\gamma)^{-1})$ iterations of approximate value iteration, making one call to Sample(Algorithm 1) with a sample size of $M=\\tilde{O}((1-\\gamma)^{-1})$ in each iteration. The algorithm outputs $\\mathbf{\\dot{v}}^{L}$ which we show is an $\\alpha/2$ -underestimate of $v^{\\star}$ (Corollary 2.5). ", "page_idx": 7}, {"type": "text", "text": "TVRVI (Algorithm 3) is similar to variance reduced value iteration [2], in that each iteration, we draw $M$ samples and use Sample to maintain underestimates of $\\pmb{p}_{a}(s)^{\\top}(\\pmb{v}^{(\\ell)}-\\pmb{v}^{(\\ell-1)})$ for each sate-action pair $(s,a)$ . However, there are two key distinctions between TVRVIand variance-reduced value iteration [2] that enable our improvement. First, we use the recursive variance reduction technique, as described by (3) and (4), and second we apply truncation (Line 7), which essentially implements the truncation described in Lemma 1.3. Lemma 2.2 below illustrates how these two techniques can be combined to bound the necessary sample complexity for maintaining approximate transitions $\\pmb{p}_{a}(s)^{\\top}(\\pmb{w}^{(t)}-\\pmb{w}^{(0)})$ for a general sequence of $\\ell_{\\infty}$ -bounded vectors $\\{\\pmb{w}^{(i)}\\}_{i=1}^{T}$ . The analysis leverages Freedman\u2019s Inequality [35] as stated in [36] and restated in Theorem B.1. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2.2. Let $T\\in\\mathbb{Z}_{>0}$ and $\\pmb{w}^{(0)},\\pmb{w}^{(1)},...,\\pmb{w}^{(T)}\\in\\mathbb{R}^{S}$ such that $\\left\\|\\pmb{w}^{(i)}-\\pmb{w}^{(i-1)}\\right\\|_{\\infty}\\leq\\tau$ for all $i\\,\\in\\,[T]$ . Then, for any $\\pmb{p}\\in\\Delta^{S}$ , $\\delta\\,\\in\\,(0,1)$ , and $M\\,\\geq\\,2^{8}T\\log(2/\\delta)$ with probability $1-\\delta$ , $\\begin{array}{r}{|p^{\\top}(w^{(t)}-w^{(0)})-\\sum_{i\\in[t]}\\sum_{j\\in[M]}\\mathsf{S a m p l e}(w^{(i)}-w^{(i-1)},p,1,0)\\cdot1/M|\\le\\tau/8,}\\end{array}$ for all $t\\in[T]$ . ", "page_idx": 7}, {"type": "text", "text": "Algorithm $\\mathfrak{\\{:\\,T V R V I}}(\\pmb{v}^{(0)},\\pi^{(0)},\\pmb{x},\\alpha,\\delta)$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: Initial values ${\\pmb v}^{(0)}\\in\\mathbb{R}^{S}$ , which is an $\\alpha$ -underestimate of $v^{\\star}$ . Input: Initial policy $\\pi^{(0)}$ such that ${\\pmb v}^{(0)}\\leq T_{\\pi^{(0)}}({\\pmb v}^{(0)})$ . Input: Accuracy $\\overset{\\cdot}{\\alpha}\\in[0,(1-\\gamma)^{-1}]$ and failure probability $\\delta\\in(0,1)$ . Input: Offsets $\\pmb{x}\\in\\mathbb{R}^{A}$ ; // entrywise underestimate of ${\\cal P}v^{(0)}$ 1 Initialize $\\pmb{g}^{(1)}\\in\\mathbb{R}^{A}$ and $\\hat{\\pmb g}^{(1)}\\in\\mathbb{R}^{A}$ to ${\\mathbf0}$ ; 2 $L=\\lceil\\log(8)(1-\\gamma)^{-1}\\rceil$ and $M=\\lceil L\\cdot2^{8}\\log(2{\\mathcal{A}}_{\\mathrm{tot}}/\\delta)\\rceil$ ; 3 for each iteration $\\ell\\in[L]$ do 4 $\\tilde{Q}=r+\\gamma({\\pmb x}+\\hat{\\pmb g}^{(\\ell)})$ ; 5 $\\pmb{v}^{(\\ell)}=\\pmb{v}^{(\\ell-1)}$ and $\\pi^{(\\ell)}=\\pi^{(\\ell-1)}$ ; 6 for each state $i\\in S$ do // Compute truncated value update (and associated action) 7 $\\tilde{{\\boldsymbol v}}^{(\\ell)}(i)=\\operatorname*{min}\\{\\operatorname*{max}_{a\\in A_{i}}\\tilde{Q}_{i,a},{\\boldsymbol v}^{(\\ell-1)}+(1-\\gamma)\\alpha\\}$ and $\\tilde{\\pi}_{i}^{(\\ell)}=\\mathrm{argmax}_{a\\in\\mathcal{A}_{i}}\\,\\tilde{Q}_{i,a}$ ; // Update value and policy if it improves 8 if $\\tilde{\\pmb{v}}^{(\\ell)}(i)\\geq\\pmb{v}^{(\\ell)}(i)$ then $\\pmb{v}^{(\\ell)}(i)=\\tilde{\\pmb{v}}^{(\\ell)}(i)$ and $\\pi_{i}^{(\\ell)}=\\tilde{\\pi}_{i}^{(\\ell)}$ ; // Update for maintaining estimates of ${\\pmb{P}}({\\pmb{v}}^{(l)}-{\\pmb{v}}^{0})$ . 9 \u2206(\u2113) = ApxUtility $\\mathbf{\\nabla}(\\pmb{v}^{(\\ell)}-\\pmb{v}^{(\\ell-1)},M,0)$ and $\\pmb{g}^{(\\ell+1)}=\\pmb{g}^{(\\ell)}+\\pmb{\\Delta}^{(\\ell)}$ ; $//$ Shift estimates so that $\\hat{\\pmb g}^{(\\ell+1)}$ always underestimates $p_{a}(s)^{\\top}v^{(\\ell)}$ . 10 $\\begin{array}{r}{\\hat{\\pmb{g}}^{(\\ell+1)}=\\pmb{g}^{(\\ell+1)}-\\frac{(1-\\gamma)\\alpha}{8}\\mathbf{1}}\\end{array}$ ; 8 11 return $(\\pmb{v}^{(L)},\\pi^{(L)})$ ", "page_idx": 7}, {"type": "text", "text": "While it is unclear how to significantly improve the constant of $2^{8}=256$ appearing in Lemma 2.2 (and consequently Algorithm 3), we note that tightening these constants in the application of Freedman\u2019s inequality could be of practical interest. By applying Lemma 2.2 to the iterates ${\\pmb v}^{(\\ell)}$ in TVRVI, the following Corollary 2.3 shows that we can maintain additive $O((1-\\gamma)\\alpha)$ -underestimates of the transitions ${\\pmb{p}}_{a}(s)^{\\top}({\\pmb{v}}^{(\\ell)}-{\\pmb{v}}^{(0)})$ using only $\\tilde{O}(L)$ samples (as opposed to the $\\tilde{O}(L^{2})$ samples required in [2]) per state-action pair. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Corollary 2.3. In TVRVI (Algorithm 3), with probability $1\\,-\\,\\delta$ , in Lines 9, 10 and 2, for all $s\\;\\in\\;{\\cal S},a\\;\\in\\;{\\cal A}_{s}$ , and $\\ell\\,\\in\\,[L]$ , we have $\\left|g_{a}^{(\\ell)}(s)-p_{a}(s)^{\\top}(v^{(\\ell-1)}-v^{(0)})\\right|\\ \\leq\\ (1-\\gamma)\\alpha/8$ and therefore $\\hat{g}_{a}^{(\\ell)}$ is $\\iota\\ (1-\\gamma)\\alpha/4$ -underestimate of ${\\pmb{p}}_{a}(s)^{\\top}({\\pmb{v}}^{(\\ell-1)}-{\\pmb{v}}^{(0)})$ . ", "page_idx": 8}, {"type": "text", "text": "The following Lemma 2.4 shows that whenever the event in Corollary 2.3 holds, TVRVI (Algorithm 3) is approximately contractive and maintains monotonicity of the approximate values. By accumulating the error bounds in Lemma 2.4, we also obtain the following Corollary 2.5, which guarantees that TVRVI halves the error in the initial estimate $\\pmb{v}^{(0)}$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 2.4. Suppose that for some $\\beta\\in\\mathbb{R}_{\\geq0}^{A}$ , $P{\\pmb v}^{(0)}-\\beta\\leq{\\pmb x}\\leq P{\\pmb v}^{(0)}$ and let $\\beta_{\\pi^{\\star}}\\in\\mathbb{R}^{S}$ be defined as $\\beta_{\\pi^{\\star}}(s):=\\beta_{\\pi^{\\star}(s)}(s)$ for each $s\\in S$ . Then, with probability $1-\\delta$ , at the end of every iteration $\\ell\\in[L]\\left(L i n e\\;3\\right)$ in $\\mathrm{TVRVI}(\\pmb{v}^{(0)},\\pmb{\\pi}^{(0)},\\pmb{x},\\alpha,\\delta),$ , the following hold for $\\pmb{\\xi}:=\\gamma((1-\\gamma)\\alpha/4\\mathbf{1}+\\beta_{\\pi^{\\star}})$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pmb v^{(\\ell-1)}\\leq\\pmb v^{(\\ell)}\\leq T_{\\pi^{(\\ell)}}(\\pmb v^{(\\ell)}),}}\\\\ {{0\\leq\\pmb v^{\\star}-\\pmb v^{(\\ell)}\\leq\\operatorname*{max}\\left(\\gamma P^{\\star}(\\pmb v^{\\star}-\\pmb v^{(\\ell-1)})+\\pmb\\xi,\\gamma(\\pmb v^{\\star}-\\pmb v^{(\\ell-1)})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Corollary 2.5. Suppose that for some $\\alpha\\geq0$ and $\\beta\\in\\mathbb{R}_{\\geq0}^{A}$ , $P{\\pmb v}^{(0)}-\\beta\\leq{\\pmb x}\\leq P{\\pmb v}^{(0)}$ ; $\\pmb{v}^{(0)}$ is an $\\alpha$ -underestimate of $v^{\\star}$ ; and $\\pmb{v}^{(0)}\\leq T_{\\pi^{(0)}}(\\pmb{v}^{(0)})$ . Let $\\beta_{\\pi^{\\star}}\\in\\mathbb{R}^{s}$ be defined as $\\beta_{\\pi^{\\star}}(s):=\\beta_{\\pi^{\\star}(s)}(s)$ for each $s\\in S$ . Let $(\\boldsymbol{v}^{(L)},\\pi^{(L)})=\\mathrm{TVRVI}(\\boldsymbol{v}^{(0)},\\pi^{(0)},\\alpha,\\delta),$ , and $L,M$ be as in Line 2. Define $\\xi:=$ $\\gamma\\,((1-\\gamma)\\alpha/4\\cdot{\\bf1}+\\beta_{\\pi^{\\star}})$ . Then, with probability $1-\\delta$ , $\\begin{array}{r}{\\mathbf{0}\\leq v^{\\star}-v^{(L)}\\leq\\gamma^{L}\\alpha\\cdot\\mathbf{1}+(I-\\gamma P^{\\star})^{-1}\\xi,}\\end{array}$ , and $\\pmb{v}^{(L)}\\leq T_{\\pi^{(L)}}(\\pmb{v}^{(L)})$ . In particular, if $\\beta={\\bf0}$ , then for $L>\\log(8)(1-\\gamma)^{-1}$ we can reduce the error in $\\pmb{v}^{(0)}$ by half: $\\mathbf{0}\\leq v^{\\star}-v^{(L)}\\leq(v^{\\star}-v^{(0)})/2$ . Additionally, TVRVI is implementable with $\\tilde{O}(\\mathcal{A}_{\\mathrm{tot}}M L)$ sample queries to the generative model and time and $O(A_{\\mathrm{tot}})$ space. ", "page_idx": 8}, {"type": "text", "text": "Theorem 1.2 now follows by recursively applying Corollary 2.5. OfflineTVRVI (Algorithm 4) provides the pseudocode for the algorithm guaranteed by Theorem 1.2. ", "page_idx": 8}, {"type": "table", "img_path": "BiikUm6pLu/tmp/a4006b77a291dfd3df0991dbdd7abe693ad8cfd4b2150ae7fac3faaa9fafca21.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "3 Sample setting algorithm ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we show how to extend the analysis in the previous section in the sample setting, where we do not have explicit access to $_{P}$ . We follow a similar framework as in [3] to show that we can instead estimate the offsets $\\textbf{\\em x}$ in OfflineTVRVI by taking additional samples from the generative model. The pseudocode is shown in SampleTVRVI(Algorithm 5.) To analyze the algorithm, we first bound the error incurred when approximating the exact offsets $\\textbf{\\em x}$ in Line 4 of OfflineTVRVI (Algorithm 4) with approximate offsets $\\tilde{\\pmb{x}}\\approx\\pmb{P}\\pmb{v}_{k-1}$ computed by sampling from the generative model. The proof leverages Hoeffding\u2019s and Bernstein\u2019s inequality, and follows a similar structure as the proof of Lemma 5.1 of [3]. ", "page_idx": 8}, {"type": "text", "text": "Input: Target precision $\\varepsilon$ and failure probability $\\delta\\in(0,1)$   \n1 $\\bar{K^{'}}\\!\\!=\\lceil\\!\\log_{2}\\!\\left(\\varepsilon^{-\\!\\!1}(1-\\gamma)^{-1}\\right)\\rceil$ ;   \n2 ${\\pmb v}_{0}={\\bf0}$ , $\\pi_{0}$ is an arbitrary policy, and $\\alpha_{0}=(1-\\gamma)^{-1}$ ;   \n3 for each iteration $k\\in[K]$ do   \n4 $\\alpha_{k}=\\alpha_{k-1}/2=2^{-\\bar{k}}(\\mathrm{1}-\\gamma)^{-1}$ ;   \n5 $N=6500(1-\\gamma)^{-3}\\log(8A_{\\mathrm{tot}}K\\delta^{-1})$ ;   \n6 $N_{k-1}=N\\operatorname*{max}((1-\\gamma),\\alpha_{k-1}^{-2})$ ;   \n7 $\\eta_{k-1}=N_{k-1}^{-1}\\log(8{\\cal A}_{\\mathrm{tot}}K\\delta^{-1})$ ;   \n8 $\\mathbf x_{k}=\\mathrm{ApxUti1ity}(v_{k-1},N_{k-1},\\eta_{k-1});$   \n9 $(\\pmb{v}_{k},\\pi_{k})=\\mathrm{TVRVI}(\\pmb{v}_{k-1},\\pi_{k-1},\\pmb{x}_{k},\\alpha_{k-1},\\delta/K)$ ;   \n10 return $(\\pmb{v}_{K},\\pi_{K})$ ", "page_idx": 9}, {"type": "text", "text": "Lemma 3.1. Consider $\\pmb{u}\\in\\mathbb{R}^{S}$ . Let x = ApxUtility $(u,m\\cdot\\mathcal{A}_{\\mathrm{tot}},\\eta)$ , $m\\geq\\log(1/2\\delta^{-1}).$ , and $\\eta=(m\\mathcal{A}_{\\mathrm{tot}})^{-1}\\log(1/2\\delta^{-1})$ . Then, with probability $1-\\delta$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\nP u-2\\sqrt{2\\eta\\pmb{\\sigma_{v^{\\star}}}}+\\left(2\\sqrt{2\\eta}\\,\\|\\pmb{u}-\\pmb{v}^{\\star}\\|_{\\infty}+18\\eta^{3/4}\\,\\|\\pmb{u}\\|_{\\infty}\\right)\\leq\\pmb{x}\\leq P u.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Finally, to obtain our main result Theorem 1.1, we utilize worst-case bounds on $\\sigma_{v^{\\star}}$ from prior work [1] (see Lemma B.3, Lemma B.4) and inductively apply Lemma 3.1 and Corollary 2.5. ", "page_idx": 9}, {"type": "text", "text": "The constant of 6500 appearing in the initialization of $N$ in Algorithm 5 arises due to technical reasons, from applying Bernstein\u2019s inequality, Hoeffding\u2019s inequality, union bound over all $K$ outer loop iterations, and bounds on $\\pmb{\\sigma}_{\\pmb{v}^{\\star}}$ from prior work [3] to prove Lemma 3.1. While it is unclear how to directly further tighten this constant, the proof of Lemma 3.1 shows that in the expression $N=6500(1\\stackrel{\\cdot}{-}\\gamma)^{-3}\\log(8A_{\\mathrm{tot}}K\\delta^{-1})$ there is a natural trade-off between the leading constant (in this case 6500) and the number of outer loop iterations $K$ . By increasing the number of outer-loop iterations $K$ by constants, one can relax the error requirements of each iteration (i.e., decrease $N$ by constants at the cost of increased logarithmic dependence on $\\vert S\\vert\\,,A_{\\mathrm{tot}})$ . Although not the primary focus of our work, such trade-offs might be of practical importance. ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We provided faster and more space-efficient algorithms for solving DMDPs. We showed how to apply truncation and recursive variance reduction to improve upon prior variance-reduced value iterations methods. Ultimately, these techniques reduced an additive $\\tilde{O}((1-\\gamma)^{-3})$ term in the time and sample complexity of prior variance-reduced value iteration methods to $\\tilde{O}((1-\\gamma)^{-2})$ . ", "page_idx": 9}, {"type": "text", "text": "Natural open problems left by our work include exploring the practical implications of our techniques and exploring whether further runtime improvements are possible. For example, it may be of practical interest to explore whether there exist other analogs of truncation that do not need to limit the progress in individual steps of value iteration. Additionally, the question of whether the $\\tilde{O}((1-\\gamma)^{\\bar{-}2})$ term in our time and sample complexities can be further improved to $\\tilde{O}((1-\\gamma)^{-1})$ is a natural open problem; an affirmative answer to this question would yield the first near-optimal running times for solving a DMDP with a generative model for all $\\varepsilon$ and fully bridge the sample complexity gap between model-based and model-free methods. We hope this paper supports further studying these questions and establishing the optimal runtime for solving MDPs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Thank you to Yuxin Chen for interesting and motivating discussion about model-based methods in RL. Thank you to the anonymous reviewers for their helpful feedback. Yujia Jin and Ishani Karmarkar were funded in part by NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and a PayPal research award. Aaron Sidford was funded in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF1955039, and a PayPal research award. Part of this work was conducted while visiting the Simons Institute for the Theory of Computing. Yujia Jin\u2019s contributions to the project occurred while she was a graduate student at Stanford. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster algorithms for solving markov decision processes. Naval Research Logistics (NRL), 70, 2023.   \n[2] Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster algorithms for solving markov decision processes. 29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2018.   \n[3] Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample complexities for solving markov decision processes with a generative model. Advances in Neural Information Processing Systems 31 (NeurIPS), 2018.   \n[4] Qiying Hu and Wuyi Yue. Markov decision processes with their applications, volume 14. Springer Science & Business Media, 2007.   \n[5] Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. Reinforcement learning to rank with markov decision process. Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, 2017.   \n[6] Thomas Degris, Olivier Sigaud, and Pierre-Henri Wuillemin. Learning the structure of factored markov decision processes in reinforcement learning problems. 23rd International Conference on Machine Learning (ICML), 2006.   \n[7] Olivier Sigaud and Olivier Buffet. Markov decision processes in artificial intelligence. John Wiley & Sons, 2013.   \n[8] Martijn Van Otterlo and Marco Wiering. Reinforcement learning and markov decision processes. Reinforcement learning: State-of-the-art, 2012.   \n[9] Martijn Van Otterlo. Markov decision processes: Concepts and algorithms. Course on \u2018Learning and Reasoning, 2009.   \n[10] Yinyu Ye. A new complexity result on solving the markov decision problem. Mathematics of Operations Research, 30, 2005.   \n[11] Michael L Littman, Thomas L Dean, and Leslie Pack Kaelbling. On the complexity of solving markov decision problems. 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI), 1995.   \n[12] Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in o (vrank) iterations and faster algorithms for maximum flow. 55th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2014.   \n[13] Yinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the markov decision problem with a fixed discount rate. Mathematics of Operations Research, 36, 2011.   \n[14] Bruno Scherrer. Improved and generalized upper bounds on the complexity of policy iteration. Advances in Neural Information Processing Systems 26 (NeurIPS)), 2013.   \n[15] Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect algorithms. Advances in Neural Information Processing Systems 11 (NeurIPS), 11, 1998.   \n[16] Mohammad Gheshlaghi Azar, R\u00e9mi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91, 2013.   \n[17] Alekh Agarwal, Sham M. Kakade, and Lin F. Yang. Model-based reinforcement learning with a generative model is minimax optimal. 33rd Annual Conference on Computational Learning Theory (COLT), 2020.   \n[18] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier in model-based reinforcement learning with a generative model. Advances in Neural Information Processing Systems 33 (NeurIPS), 2020.   \n[19] Sham Machandranath Kakade. On the sample complexity of reinforcement learning. University of London, University College London (United Kingdom), 2003.   \n[20] Fei Feng, Wotao Yin, and Lin F Yang. How does an approximate model help in reinforcement learning? arXiv preprint arXiv:1912.02986, 2019.   \n[21] Yujia Jin and Aaron Sidford. Efficiently solving MDPs with stochastic mirror descent. In 37th International Conference on Machine Learning (ICML), 2020.   \n[22] Paul Tseng. Solving h-horizon, stationary markov decision problems in time proportional to log (h). Operations Research Letters, 9, 1990.   \n[23] Mengdi Wang. Randomized linear programming solves the discounted markov decision problem in nearly-linear (sometimes sublinear) running time. Mathematics of Operations Research, 42, 2019.   \n[24] Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and Di Wang. Minimum cost flows, mdps, and l1-regression in nearly linear time for dense instances. In 53rd Annual ACM Symposium on Theory of Computing (STOC), 2021.   \n[25] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. Journal of the ACM, 2020.   \n[26] Jan van den Brand. A deterministic linear program solver in current matrix multiplication time. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 259\u2013278. SIAM, 2020.   \n[27] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving general lps. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 823\u2013832, 2021.   \n[28] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multiplication: from alpha to omega. In 35th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2024.   \n[29] Pengqian Yu, William B Haskell, and Huan Xu. Approximate value iteration for risk-aware markov decision processes. IEEE Transactions on Automatic Control, 63, 2018.   \n[30] Mohand Hamadouche, Catherine Dezan, David Espes, and Kalinka Branco. Comparison of value iteration, policy iteration and q-learning for solving decision-making problems. In 2021 International Conference on Unmanned Aircraft Systems (ICUAS), 2021.   \n[31] Christopher W Zobel and William T Scherer. An empirical study of policy convergence in markov decision process value iteration. Computers & operations research, 32, 2005.   \n[32] Dileep Kalathil, Vivek S Borkar, and Rahul Jain. Empirical q-value iteration. Stochastic Systems, 11, 2021.   \n[33] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. Advances in Neural Information Processing Systems 26 (NeurIPS), 2013.   \n[34] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak\u00e1\u02c7c. Sarah: A novel method for machine learning problems using stochastic recursive gradient. 34th International Conference on Machine Learning (ICML), 2017.   \n[35] David A Freedman. On tail probabilities for martingales. pages 100\u2013118, 1975.   \n[36] Joel A. Tropp. Freedman\u2019s inequality for matrix martinglaes. Electronic Communications in Probability, 16, 2011.   \n[37] Andrea Zanette and Emma Brunskill. Problem dependent reinforcement learning bounds which can identify bandit structure in mdps. In International Conference on Machine Learning, pages 5747\u20135755. PMLR, 2018.   \n[38] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. 36th International Conference on Machine Learning (ICML), 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Faster problem-dependent convergence ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we propose a modified version of the SampleTVRVI algorithm, named ProblemDependentTVRVI. This algorithm adjusts the number of required samples based on the structure of the MDP under consideration. Inspired by [38], we then consider MDPs with small ranges of optimal values and the extreme case of highly mixing MDPs in which state transitions are sampled from a fixed distribution. ", "page_idx": 12}, {"type": "text", "text": "Note that in the proof of Theorem 1.1, the error during convergence caused by approximations of values is bounded by $\\begin{array}{r}{\\mathbf{\\Xi}^{\\prime}-\\gamma P^{\\star})^{-1}\\pmb{\\xi}_{k}\\operatorname{for}{\\xi_{k}}\\leq\\frac{(1-\\gamma)\\alpha_{k}}{4}\\mathbf{1}+2\\sqrt{2\\eta_{k}\\pmb{\\sigma}_{v^{\\star}}}+(2\\sqrt{2\\eta_{k}}\\alpha_{k}+18\\eta_{k}^{3/4}\\left\\|\\pmb{v}^{(0)}\\right\\|_{\\infty})\\mathbf{1}}\\end{array}$ In its proof, we upper bound the variance term $\\left\\|(\\boldsymbol{I}-\\gamma\\boldsymbol{P}^{\\star})^{-1}\\sqrt{\\sigma_{v^{\\star}}}\\right\\|_{\\infty}$ by $3(1\\mathrm{~-~}\\gamma)^{-1.5}$ using Lemma B.4. However, as $\\alpha_{k}$ decreases and the variance term becomes dominant, a number of samples proportional to the size of the variance t\u221aerm suffices to control the error during each iteration. Given $V$ which upper bounds $\\left\\|(\\boldsymbol{I}-\\gamma\\boldsymbol{P}^{\\star})^{-1}\\sqrt{\\sigma_{v^{\\star}}}\\right\\|_{\\infty}$ , we can further refine SampleTVRVI to reduce the number of samples taken after an initial burn-in phase and obtain improved complexities when $V$ is signficantly small. Hence, we obtain the following Algorithm 6 and Theorem A.1. ", "page_idx": 12}, {"type": "text", "text": "Algorithm 6: ProblemDependentTVRVI(\u03b5, \u03b4, V ) ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Input: Target precision $\\varepsilon$ , failure probability $\\delta\\in(0,1)$ , and $V\\geq\\left\\|(I-\\gamma P^{\\star})^{-1}\\sqrt{\\sigma_{v^{\\star}}}\\right\\|_{\\infty}$ . 1 $K=\\lceil\\log_{2}(\\varepsilon^{-1}(1-\\gamma)^{-1})\\rceil$ ; 2 v0 = 0, \u03c00 is an arbitrary policy, and \u03b10 =1\u22121\u03b3 ; 3 for each iteration $k\\in[K]$ do 4 $\\alpha_{k}=\\alpha_{k-1}/2=2^{-k}(1-\\gamma)^{-1}$ ; 5 if $\\begin{array}{r}{k<\\lceil\\log_{2}\\left(\\frac{128\\left(1-\\gamma\\right)^{-5}}{V^{3}}\\right)\\rceil}\\end{array}$ then $N_{k-1}=6500\\cdot(1-\\gamma)^{-3}\\operatorname*{max}((1-\\gamma),\\alpha_{k-1}^{-2})\\log(8A_{\\mathrm{tot}}K\\delta^{-1}):$ ; // Burn-in phase else 8 $\\begin{array}{r l r l}&{\\ L_{k-1}=1024\\cdot\\alpha_{k-1}^{-2}V^{2}\\log(8\\mathcal{A}_{\\mathrm{tot}}K\\delta^{-1})\\;;}&&{\\;\\//\\;\\;\\mathsf{V a r i a n c e\\mathrm{-}d e p e n d e n t~p h a s e}}\\\\ &{\\eta_{k-1}=N_{k-1}^{-1}\\log(8\\mathcal{A}_{\\mathrm{tot}}K\\delta^{-1})\\;;}\\\\ &{x_{k}=\\mathsf{A p x U t i\\,i\\,i\\,t y}(v_{k-1},N_{k-1},\\eta_{k-1});}\\\\ &{(v_{k},\\pi_{k})=\\mathsf{T W R I}(v_{k-1},\\pi_{k-1},x_{k},\\alpha_{k-1},\\delta/K);}\\end{array}$ 9 10 11 12 return $(\\pmb{v}_{K},\\pi_{K})$ ", "page_idx": 12}, {"type": "text", "text": "Theorem A.1. In the sample setting, there is an algorithm (Algorithm 6) that, given $3(1-\\gamma)^{-1.5}\\geq$ $V\\geq\\left\\|(I-\\gamma P^{\\star})^{-1}\\sqrt{\\sigma_{v^{\\star}}}\\right\\|_{\\infty}$ , uses $\\tilde{O}\\left(A_{\\mathrm{tot}}\\left(\\varepsilon^{-2}V^{2}+(1-\\gamma)^{-2}\\right)\\right)$ samples and time and $O(A_{\\mathrm{tot}})$ space, and computes an $\\varepsilon$ -optimal policy and $\\varepsilon$ -optimal values with probability $1-\\delta$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $K,\\alpha_{k}$ , and $(\\pmb{v}_{k},\\pi_{k})$ be as defined in Lines 1, 4, and 11 of ProblemDependentTVRVI $(\\varepsilon,\\delta,V)$ . ", "page_idx": 12}, {"type": "text", "text": "For the correctness of the algorithm, we first induct on $k$ to show that for each $k\\ \\in\\ [K]$ , with probability $1-k\\delta/K$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf0\\leq v^{\\star}-v^{\\pi_{k}}\\leq v^{\\star}-v_{k}\\leq\\alpha_{k},\\quad\\mathrm{~and~}v_{k}\\leq\\mathcal{T}_{\\pi_{k}}(v_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The base case is trivial, as $\\mathbf{0}\\leq v^{\\star}-v^{\\pi_{0}}\\leq v^{\\star}-v_{0}\\leq(1-\\gamma)^{-1}\\mathbf{1}.$ ", "page_idx": 12}, {"type": "text", "text": "For the inductive step, observe that by Lemma 3.1, we see that with probability $1-\\delta/K$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P v_{k-1}-\\left[2\\sqrt{2\\eta_{k-1}\\sigma_{v^{\\star}}}+\\left(2\\sqrt{2\\eta_{k-1}}\\alpha_{k-1}+18\\eta_{k-1}^{3/4}\\left\\Vert v_{k-1}\\right\\Vert_{\\infty}\\right)\\mathbf{1}\\right]\\le x_{k}\\le P v_{k-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Additionally, by the inductive hypothesis, with probability $1-(k-1)\\delta/K$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq v^{\\star}-v^{\\pi_{k}-1}\\leq v^{\\star}-v_{k}\\leq\\gamma^{L}\\alpha_{k-1}\\cdot1+(I-\\gamma P^{\\star})^{-1}\\xi_{k-1}\\leq\\alpha_{k}{\\bf1},\\quad\\mathrm{~and~}v_{k}\\leq7_{\\pi_{k}}(v_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, by union bound, with probability $1-k\\delta/K$ , both (8) and (9) hold. We condition on this event in the remainder of the inductive step. ", "page_idx": 12}, {"type": "text", "text": "Now, we apply Corollary 2.5 with ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\beta}=2{\\sqrt{2\\eta_{k-1}\\sigma_{v^{\\star}}}}+\\left(2{\\sqrt{2\\eta_{k-1}}}\\alpha_{k-1}+18\\eta_{k-1}^{3/4}\\left\\|{v_{k-1}}\\right\\|_{\\infty}\\right)\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consequently, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n0\\leq\\pmb{v^{\\star}}-\\pmb{v_{k}}\\leq\\gamma^{L}\\alpha_{k-1}\\cdot\\mathbf{1}+(\\boldsymbol{I}-\\gamma\\pmb{P^{\\star}})^{-1}\\xi_{k-1}\\leq\\frac{\\alpha_{k-1}}{8}\\mathbf{1}+(\\boldsymbol{I}-\\gamma\\pmb{P^{\\star}})^{-1}\\xi_{k-1},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for $\\begin{array}{r}{\\xi_{k-1}\\,\\le\\,\\frac{(1-\\gamma)\\alpha_{k-1}}{4}{\\bf1}+2\\sqrt{2\\eta_{k-1}\\sigma_{v^{\\star}}}\\,+\\,\\left(2\\sqrt{2\\eta_{k-1}}\\alpha_{k-1}+18\\eta_{k-1}^{3/4}\\left\\|v_{k-1}\\right\\|_{\\infty}\\right){\\bf1},}\\end{array}$ , and $\\pmb{v}_{k}~\\leq$ ${\\mathcal{T}}_{\\pi_{k}}({\\pmb v}_{k})$ . ", "page_idx": 13}, {"type": "text", "text": "Note that $\\begin{array}{r}{(I-\\gamma P^{\\star})^{-1}\\mathbf{1}\\leq\\frac{1}{1-\\gamma}\\mathbf{1}}\\end{array}$ . Hence, if $k<\\lceil\\log_{2}(1-\\gamma)^{-5}/V^{3}\\rceil$ , we use Lemma B.4 along with the facts that $(I-\\gamma P^{\\star})^{-1}{\\bf1}=1/(1-\\gamma){\\bf1}$ and the choice of $\\eta_{k-1}$ to obtain (identical to the proof of Theorem 1.1): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{I}-\\gamma P^{\\star})^{-1}\\xi_{k-1}\\le\\left[\\displaystyle\\frac{\\alpha_{k-1}}{4}+2\\sqrt{6\\frac{\\eta_{k-1}}{(1-\\gamma)^{3}}}+2\\sqrt{\\frac{2(1-\\gamma)^{3}\\operatorname*{min}((1-\\gamma)^{-1},\\alpha_{k-1}^{2})}{6500(1-\\gamma)^{2}}}\\alpha_{k-1}\\right]\\mathbf{1}}&{}\\\\ {+\\left[\\mathrm{I8}\\left(\\frac{((1-\\gamma)^{3}\\operatorname*{min}((1-\\gamma)^{-1},\\alpha_{k-1}^{2})}{6500(1-\\gamma)^{8/3}}\\right)^{3/4}\\right]\\mathbf{1}}&{}\\\\ {\\le[\\alpha_{k-1}/4+2\\sqrt{6/6500}\\cdot\\alpha_{k-1}+2\\sqrt{2/6500}(1-\\gamma)^{1/2}\\operatorname*{min}((1-\\gamma)^{-1/2},\\alpha_{k-1}}\\\\ {+18\\cdot(10^{-3})(1-\\gamma)^{1/4}\\operatorname*{min}((1-\\gamma)^{-3/4},\\alpha_{k-1}^{3/2})]\\mathbf{1}}&{}\\\\ {\\le[\\alpha_{k-1}/4+4\\sqrt{6/6500}\\cdot\\alpha_{k-1}+18\\cdot(10^{-3})\\alpha_{k-1}]\\mathbf{1}\\le\\frac{3}{8}\\alpha_{k-1}\\mathbf{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If instead $k\\,\\geq\\,\\lceil\\log_{2}(1-\\gamma)^{-5}/V^{3}\\rceil$ , then $\\begin{array}{r}{\\alpha_{k}\\,\\leq\\,\\frac{1}{128}(1-\\gamma)^{4}V^{3}}\\end{array}$ , and $\\eta_{k-1}=\\alpha_{k-1}^{2}/(1024\\cdot V^{2})$ . Consequently, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I-\\gamma P^{\\star})^{-1}\\xi_{k-1}\\leq2\\sqrt{2\\eta_{k-1}}(\\mathbf I-\\gamma P^{\\star})^{-1}\\sqrt{\\sigma_{v^{\\star}}}}\\\\ &{\\phantom{\\leq}\\left(\\frac{\\alpha_{k-1}}{4}+2\\sqrt{2\\eta_{k-1}}(I-\\gamma P^{\\star})^{-1}\\alpha_{k-1}+18\\eta_{k-1}^{3/4}(I-\\gamma P^{\\star})^{-1}\\left\\|v_{k-1}\\right\\|_{\\infty}\\right)\\mathbf1}\\\\ &{\\phantom{\\leq}\\leq\\frac{\\alpha_{k-1}}{4}\\mathbf1+\\frac{2\\sqrt{2}\\alpha_{k-1}}{4(1-\\gamma)\\sqrt{1024}V}V\\mathbf1+\\frac{18}{(1-\\gamma)^{2}}\\left(\\frac{\\alpha_{k-1}^{2}}{1024\\cdot V^{2}}\\right)^{3/4}\\mathbf1}\\\\ &{\\phantom{\\leq\\left(\\frac{\\alpha_{k-1}^{2}}{4}+\\frac{\\alpha_{k-1}}{4}\\right)\\mathbf1}\\leq\\frac{3}{8}\\alpha_{k-1}\\mathbf1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore in either case, ", "page_idx": 13}, {"type": "equation", "text": "$$\nv^{\\star}-v_{k-1}\\leq\\frac{\\alpha_{k-1}}{2}{\\bf1}=\\alpha_{k}{\\bf1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, we can use that ${\\pmb v}_{k}\\leq T_{\\pi_{k}}({\\pmb v}_{k})$ to see that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{k}\\le{\\mathcal T}_{\\pi_{k}}(v_{k})\\le{\\mathcal T}_{\\pi_{k}}^{2}(v_{k})\\le\\cdots\\le{\\mathcal T}_{\\pi_{k}}^{\\infty}(v_{k})=v^{\\pi_{k}}\\le v^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the inductive step. ", "page_idx": 13}, {"type": "text", "text": "Consequently, taking $k=K=\\lceil\\log_{2}(\\varepsilon^{-1}(1-\\gamma)^{-1})\\rceil$ iterations of the outer loop, with probability $1-\\delta$ , we have that $0\\leq{\\pmb v}^{\\star}-{\\pmb v}^{\\pi_{K}}\\leq{\\pmb v}^{\\star}-{\\pmb v}_{K}\\leq\\alpha_{K}\\leq\\varepsilon$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{k}\\le{\\mathcal T}_{\\pi_{k}}(v_{k})\\le{\\mathcal T}_{\\pi_{k}}^{2}(v_{k})\\le\\cdots\\le{\\mathcal T}_{\\pi_{k}}^{\\infty}(v_{k})=v^{\\pi_{k}}\\le v^{\\star},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "that is, $\\pmb{v}_{k}$ is an $\\varepsilon$ -optimal value and $\\pi_{K}$ is an $\\varepsilon$ -optimal policy. ", "page_idx": 13}, {"type": "text", "text": "The total number of samples and time required is $\\tilde{O}\\left(A_{\\mathrm{tot}}\\left(\\varepsilon^{-2}V^{2}+(1-\\gamma)^{-2}\\right)\\right)$ . For the space complexity, note that the algorithm can be implemented to maintain only $O(1)$ vectors in $\\mathbb{R}^{\\mathcal{A}_{\\mathrm{tot}}}$ . \u25a0 ", "page_idx": 13}, {"type": "text", "text": "Theorem A.1 yields improved complexities for solving MDPs when $\\left\\|(\\pmb{I}-\\gamma\\pmb{P}^{\\star})^{-1}\\sqrt{\\pmb{\\sigma}_{v^{\\star}}}\\right\\|_{\\infty}$ is nontrivially bounded. Following [37] we mention two particular such settings where we can apply Theorem A.1 to obtain better problem-dependent sample and runtime bounds than Theorem 1.1. ", "page_idx": 13}, {"type": "text", "text": "Deterministic MDPs For a deterministic MDP, each action deterministically transitions to a single state. That is, for all $(s,a)\\in A$ , ${\\pmb p}_{a}(s)\\,=\\,{\\bf1}_{s^{\\prime}}$ (the indicator vector of $s^{\\prime}\\in S\\!\\rightleftharpoons$ ) for some $s^{\\prime}\\in\\mathcal{S}$ . In this case, $\\sigma_{v^{\\star}}=\\mathbf{0}$ . Consequently, if the MDP is deterministic, the algorithm converges with just $\\tilde{O}((1-\\gamma)^{3})$ samples to the generative model and time. We note that in this setting of deterministic MDPs, there may be alternative approaches to obtain the same or better runtime and sample complexity. ", "page_idx": 14}, {"type": "text", "text": "Small range. Define the range of optimal values for\u221a a MDP as $\\begin{array}{r}{\\operatorname{rng}(\\boldsymbol{v}^{*})\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}_{s\\in{\\mathcal{S}}}\\boldsymbol{v}_{s}^{*}\\!-\\!\\operatorname*{min}_{s\\in{\\mathcal{S}}}\\boldsymbol{v}_{s}^{*}}\\end{array}$ Note that $\\sigma_{v^{\\star}}\\,\\leq\\,\\mathrm{rng}(v^{\\ast})^{2}{\\bf1}$ . So, $\\begin{array}{r}{\\left\\|(I-\\gamma P^{\\star})^{-1}\\sqrt{\\sigma_{v^{\\star}}}\\right\\|_{\\infty}\\leq(1-\\gamma)^{-1}\\mathrm{rng}(v^{\\ast})}\\end{array}$ . Therefore, by Theorem A.1, given an approximate upper bound of $\\left\\|(\\boldsymbol{I}-\\gamma\\boldsymbol{P}^{\\star})^{-1}\\sqrt{\\sigma_{v^{\\star}}}\\right\\|_{\\infty}$ our algorithm is implementable with $\\tilde{O}(A_{\\mathrm{tot}}(\\varepsilon^{-2}(1-\\gamma)^{-2}\\mathrm{rng}(v^{\\star})^{2}+(1-\\gamma)^{-2}))$ samples and time. ", "page_idx": 14}, {"type": "text", "text": "Highly mixing domains. [37] showed that a contextual bandit problem can be modeled as an MDP where the next state is sampled from a fixed stationary distribution. Using the fact that the transition function is independent of the prior state and action, the authors of [38] show that $\\mathrm{rng}(\\boldsymbol{v}^{*})\\,\\leq\\,1$ with a simple proof in its Appendix A.2. Hence, by the argument in the preceding paragraph $\\tilde{O}\\left(A_{\\mathrm{tot}}\\left(\\varepsilon^{-2}(\\stackrel{\\cdot}{1}-\\gamma)^{-2}+(1-\\gamma)^{-2}\\right)\\right)$ samples and time suffice in this setting. ", "page_idx": 14}, {"type": "text", "text": "B Omitted proofs from the main body ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Omitted proof of Lemma 1.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 1.3. For $a,b,x\\in\\mathbb{R}^{n}$ and $\\gamma$ , $\\alpha>0$ , let $\\begin{array}{r}{c:=\\mathrm{median}\\{a-(1-\\gamma)\\alpha{\\bf1},b,a+(1-\\gamma)\\alpha{\\bf1}\\},}\\end{array}$ , where median is applied entrywise. Then, $\\left.\\left.f\\left\\|b-x\\right\\|_{\\infty}\\leq\\gamma\\left\\|\\overset{.}{a}-x\\right\\|_{\\infty}$ and $\\|\\pmb{a}-\\pmb{x}\\|_{\\infty}\\leq\\alpha$ , then $\\left\\|c-x\\right\\|_{\\infty}\\leq\\gamma\\left\\|a-x\\right\\|_{\\infty}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Consider the $i^{\\th}$ -th entry $(c-x)_{i}$ . There are three cases. ", "page_idx": 14}, {"type": "text", "text": "First, suppose $a_{i}-(1-\\gamma)\\alpha\\leq b_{i}\\leq a_{i}+(1-\\gamma)\\alpha$ . Then, $|c_{i}-x_{i}|=|b_{i}-x_{i}|\\leq\\gamma\\left\\|\\pmb{a}-\\pmb{x}\\right\\|_{\\infty}$ Second, suppose $b_{i}\\leq a_{i}-(1-\\gamma)\\alpha\\leq a_{i}+(1-\\gamma)\\alpha.$ . Then, $c_{i}-x_{i}\\geq b_{i}-x_{i}\\geq-\\left\\|b-x\\right\\|_{\\infty}\\geq$ $-\\gamma\\left\\|\\pmb{a}-\\pmb{x}\\right\\|_{\\infty}$ . Meanwhile, ci \u2212xi $=a_{i}-(1-\\gamma)\\alpha-x_{i}\\leq\\left\\|\\pmb{a}-\\pmb{x}\\right\\|_{\\infty}-(1-\\gamma)\\alpha.$ . Now, because $\\|\\pmb{\\mathscr{a}}-\\pmb{\\mathscr{x}}\\|_{\\infty}\\,\\leq\\,\\alpha$ , we have that $\\left(1-\\gamma\\right)\\left\\|\\boldsymbol{a}-\\boldsymbol{x}\\right\\|_{\\infty}\\leq\\,(1-\\gamma)\\alpha$ . So, $\\left\\|\\pmb{\\mathscr{a}}-\\pmb{\\mathscr{x}}\\right\\|_{\\infty}-(1-\\gamma)\\alpha\\,\\leq$ $\\gamma\\left\\|\\pmb{\\mathscr{a}}-\\pmb{\\mathscr{x}}\\right\\|_{\\infty}$ . ", "page_idx": 14}, {"type": "text", "text": "Lastly, suppose $a_{i}-(1-\\gamma)\\alpha\\leq a_{i}+(1-\\gamma)\\alpha\\leq b_{i}$ . Then, $c_{i}-x_{i}\\leq b_{i}-x_{i}\\leq\\left\\|\\pmb{b}-\\pmb{x}\\right\\|_{\\infty}\\leq$ $\\gamma\\left\\|\\pmb{a}-\\pmb{x}\\right\\|_{\\infty}.$ Meanwhile, $z_{i}-x_{i}=a_{i}+(1-\\gamma)\\alpha-x_{i}\\geq-\\left\\|a-x\\right\\|_{\\infty}+(1-\\gamma)\\alpha$ . Now, because $\\|\\pmb{a}-\\pmb{x}\\|_{\\infty}\\leq\\alpha$ , we have that $\\left(1-\\gamma\\right)\\left\\|\\pmb{a}-\\pmb{x}\\right\\|_{\\infty}\\leq(1-\\gamma)\\alpha$ . S $\\begin{array}{r}{|0,\\stackrel{\\_}{-}\\|a-\\pmb{x}\\|_{\\infty}+(1-\\gamma)\\alpha\\geq}\\end{array}$ $\\gamma\\left\\|\\pmb{a}-\\pmb{x}\\right\\|_{\\infty}$ . \u25a0 ", "page_idx": 14}, {"type": "text", "text": "B.2 Omitted proofs from Section 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, we prove Lemma 2.1. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2.1. Let $\\boldsymbol{x}\\,=\\,\\mathtt{S a m p l e}(\\boldsymbol{u},\\boldsymbol{p},M,0)$ for $\\pmb{p}\\in\\Delta^{n},\\:M\\,\\in\\,\\mathbb{Z}_{>0},\\,\\varepsilon\\,>\\,0,$ , and $\\pmb{u}\\in\\mathbb{R}^{S}$ . Then, $\\mathbb{E}\\left[x\\right]=p^{\\top}u,$ , $|x|\\leq\\|u\\|_{\\infty}$ , and Var $\\left[x\\right]\\leq1/M\\left\\Vert u\\right\\Vert_{\\infty}^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The first statement follows from linearity of expectation and the second from definitions. The third statement follows from independence and that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[v_{i_{m}}\\right]=\\sum_{i\\in S}p_{i}v_{i}^{2}-(p^{\\top}v)^{2}\\leq\\sum_{i\\in S}p_{i}\\left\\Vert v\\right\\Vert_{\\infty}^{2}=\\left\\Vert v\\right\\Vert_{\\infty}^{2}\\mathrm{~for~any~}m\\in\\left[M\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we state Freedman\u2019s inequality [35], which we use to prove the following Lemma 2.2. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.1 (Freedman\u2019s Inequality, restated from [36]). Consider a real-valued martingale $\\{Y_{k}\\,:\\,k\\,=\\,0,1,\\ldots\\}$ with difference sequence $\\{X_{k}\\,:\\,k\\,=\\,1,2,\\ldots\\}$ given by $X_{k}\\,=\\,Y_{k}\\,-\\,Y_{k-1}$ . Assume that $X_{k}\\,\\leq\\,R$ almost surely for $k=1,2,\\dots$ . Define the predictable quadratic variation process of the martingale: $\\begin{array}{r}{W_{k}:=\\sum_{j=1}^{k}\\mathbb{E}\\left[X_{j}^{2}|X_{1},...,X_{j-1}\\right]}\\end{array}$ . Then, for all $t\\geq0$ and $\\sigma^{2}>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left\\{\\exists k\\geq0:Y_{k}\\geq t\\,a n d\\,W_{k}\\leq\\sigma^{2}\\right\\}\\leq\\exp\\left(-t^{2}/(2(\\sigma^{2}+R t/3))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 2.2. Let $T\\in\\mathbb{Z}_{>0}$ and $\\pmb{w}^{(0)},\\pmb{w}^{(1)},...,\\pmb{w}^{(T)}\\in\\mathbb{R}^{S}$ such that $\\left\\|\\pmb{w}^{(i)}-\\pmb{w}^{(i-1)}\\right\\|_{\\infty}\\leq\\tau$ for all $i\\,\\in\\,[T]$ . Then, for any $\\pmb{p}\\in\\Delta^{S}$ , $\\delta\\,\\in\\,(0,1)$ , and $M\\,\\geq\\,2^{8}T\\log(2/\\delta)$ with probability $1-\\delta$ , $\\begin{array}{r}{|p^{\\top}(w^{(t)}-w^{(0)})-\\sum_{i\\in[t]}\\sum_{j\\in[M]}\\mathbf{S}\\mathsf{a m p l e}(w^{(i)}-w^{(i-1)},p,1,0)\\cdot1/M|\\leq\\tau/8\\beta<\\tau/8,}\\end{array}$ for all $t\\in[T]$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. For each $i\\in[T],j\\in[M]$ , let ", "page_idx": 15}, {"type": "equation", "text": "$$\nX_{i,j}:=\\left(\\mathtt{S a m p l e}(w^{(i)}-w^{(i-1)},p,1,0)-p^{\\top}(w^{(i)}-w^{(i-1)})\\right)/M.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\pmb{p}\\in\\Delta^{S}$ , Lemma 2.1 yields that $\\begin{array}{r}{|X_{i,j}|\\leq\\frac{2\\tau}{M}}\\end{array}$ . Next, define $\\begin{array}{r}{Y_{t,k}:=\\sum_{i\\in[t-1]}\\sum_{j\\in[M]}X_{i,j}+}\\end{array}$ $\\textstyle\\sum_{j=1}^{k}X_{t,j}$ . The predictable quadratic variation process (as defined in Theorem B.1) is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t,k}=\\displaystyle\\sum_{i\\in[t-1]}\\displaystyle\\sum_{j\\in[M]}\\mathbb{E}\\left[X_{i,j}^{2}|X_{1,1:M},...,X_{i-1,1:M},X_{i,1:j-1}\\right]+\\displaystyle\\sum_{j\\in[k]}\\mathbb{E}\\left[X_{t,j}^{2}|X_{1,1:M},...,X_{t-1,1:M},X_{i,1:j-1}\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{i\\in[t-1]}\\displaystyle\\sum_{j\\in[M]}\\mathrm{Var}\\left[\\frac{5\\mathrm{ample}(w^{(i)}-w^{(i-1)},p,1,0)}{M}\\right]+\\displaystyle\\sum_{j\\in[k]}\\mathrm{Var}\\left[\\frac{5\\mathrm{ample}(w^{(t)}-w^{(t-1)},p,1,0)}{M}\\right]}\\\\ &{\\qquad\\le\\displaystyle\\sum_{i\\in[t]}\\displaystyle\\sum_{j\\in[M]}\\frac{\\tau^{2}}{M^{2}}=\\frac{T\\tau^{2}}{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where, in the last line we used Lemma 2.1 to bound the variance. Now, by telescoping, ", "page_idx": 15}, {"type": "equation", "text": "$$\nY_{t,M}=\\left(\\sum_{i\\in[t]}\\sum_{j\\in[M]}{\\frac{\\mathrm{Samp1e}(w^{(i)}-w^{(i-1)},p,1,0)}{M}}\\right)-p^{\\top}(w^{(t)}-w^{(0)})\\;{\\mathrm{for~all}}\\;t\\in[T]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consequently, applying Theorem B.1 twice (once to $Y_{t,M}$ and once to $-Y_{t,M}$ yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\exists t\\in[T]:|Y_{t,M}|\\ge\\frac{\\tau}{8}\\right\\}\\le2\\exp\\left(-\\frac{(\\tau/8)^{2}}{2(\\frac{T\\tau^{2}}{M}+\\frac{2\\tau}{M}\\cdot\\frac{\\tau}{8}\\cdot\\frac{1}{3})}\\right)=2\\exp\\left(\\frac{-M}{2^{7}\\left(T+\\frac{1}{12}\\right)}\\right)\\le\\delta\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As an immediate corollary of Lemma 2.2, we obtain Corollary 2.3. ", "page_idx": 15}, {"type": "text", "text": "Corollary 2.3. In TVRVI (Algorithm 3), with probability $1\\,-\\,\\delta$ , in Lines 9, 10 and 2, for all $s\\;\\in\\;{\\cal S},a\\;\\in\\;{\\cal A}_{s}$ , and $\\ell\\,\\in\\,[L]$ , we have $\\left|g_{a}^{(\\ell)}(s)-p_{a}(s)^{\\top}(v^{(\\ell-1)}-v^{(0)})\\right|\\ \\leq\\ (1-\\gamma)\\alpha/8$ and therefore $\\hat{g}_{a}^{(\\ell)}$ is $\\iota\\ (1-\\gamma)\\alpha/4$ -underestimate of $\\pmb{p}_{a}(s)^{\\top}(\\pmb{v}^{(\\ell-1)}-\\pmb{v}^{(0)})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Consider some $s\\in S$ and $a\\in\\mathcal{A}_{s}$ . Note that $g_{a}^{(\\ell)}(s)$ is equal in distribution to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\sum_{i\\in[\\ell-1]}\\sum_{j\\in[M]}\\frac{\\mathrm{Samp}\\mathbf{1}\\mathbf{e}(v^{(i)}-v^{(i-1)},p_{a}(s),\\boldsymbol{1},\\boldsymbol{0})}{M}\\right)-p_{a}(s)^{\\top}(v^{(\\ell-1)}-v^{(0)}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, by Lemma 2.2 and union bound, whenever $M\\,\\geq\\,L\\cdot2^{8}\\log(2{\\mathcal{A}}_{\\mathrm{tot}}/\\delta)$ we have that with probability $1-\\delta$ , for all $\\begin{array}{r}{(s,a)\\in\\mathcal{A},\\left|g_{a}^{(\\ell)}(s)-p_{a}(s)^{\\top}(v^{(\\ell-1)}-v^{(0)})\\right|\\leq\\frac{1-\\gamma}{8}\\alpha}\\end{array}$ and conditioning on this event, we have $\\begin{array}{r}{p_{a}(s)^{\\top}({\\pmb v}^{(\\ell-1)}-{\\pmb v}^{(0)})-\\frac{1-\\gamma}{4}\\alpha\\leq\\hat{g}_{a}^{(\\ell)}(s)\\leq p_{a}(s)^{\\top}({\\pmb v}^{(\\ell-1)}-{\\pmb v}^{(0)})}\\end{array}$ due to the shift in Line 10. \u25a0 ", "page_idx": 15}, {"type": "text", "text": "Conditioning on the event that the implication of Corollary 2.3 holds, we can prove the following Lemma 2.4 ", "page_idx": 15}, {"type": "text", "text": "Lemma 2.4. Suppose that for some $\\beta\\in\\mathbb{R}_{\\geq0}^{A}$ $,P{\\pmb v}^{(0)}-\\beta\\leq{\\pmb x}\\leq P{\\pmb v}^{(0)}$ and let $\\beta_{\\pi^{\\star}}\\in\\mathbb{R}^{S}$ be defined as $\\beta_{\\pi^{\\star}}(s):=\\beta_{\\pi^{\\star}(s)}(s)$ for each $s\\in S$ . Then, with probability $1-\\delta$ , at the end of every iteration $\\ell\\in[L]$ (Line 3) in $\\mathrm{TVRVI}(\\pmb{v}^{(0)},\\pmb{\\pi}^{(0)},\\pmb{x},\\alpha,\\delta)$ , the following hold for $\\pmb{\\xi}:=\\gamma((1-\\gamma)\\alpha/4\\mathbf{1}+\\beta_{\\pi^{\\star}})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pmb v^{(\\ell-1)}\\leq\\pmb v^{(\\ell)}\\leq T_{\\pi^{(\\ell)}}(\\pmb v^{(\\ell)}),}}\\\\ {{0\\leq\\pmb v^{\\star}-\\pmb v^{(\\ell)}\\leq\\operatorname*{max}\\left(\\gamma P^{\\star}(\\pmb v^{\\star}-\\pmb v^{(\\ell-1)})+\\pmb\\xi,\\gamma(\\pmb v^{\\star}-\\pmb v^{(\\ell-1)})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. In the remainder of this proof, condition on the event that the implications of Corollary 2.3 hold (as they occur with probability $1-\\delta)$ . By Line 7 and 8 of Algorithm 3, for all $\\ell\\in[L]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{v}^{(\\ell-1)}\\leq\\pmb{v}^{(\\ell)}\\leq\\pmb{v}^{(\\ell-1)}+(1-\\gamma)\\alpha\\pmb{1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This immediately implies the lower bound in (6). ", "page_idx": 16}, {"type": "text", "text": "We prove the upper bound in (6) by induction. In the base case when $\\ell=0$ , $\\pmb{v}^{(0)}\\leq T_{\\pi^{(0)}}(\\pmb{v}^{(0)})$ holds by assumption. For the $\\ell$ -th iteration, there are two cases. If $\\pmb{v}^{(\\ell)}(s)>\\pmb{v}^{(\\ell-1)}(s)$ for $s\\in S$ then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{(\\ell)}(s)=r_{\\pi^{(\\ell)}}(s)+\\gamma\\left(x(s)+\\hat{g}_{\\pi^{(\\ell)}}^{(\\ell)}(s)\\right)\\le r_{\\pi^{(\\ell)}}(s)+\\gamma p_{\\pi^{(\\ell)}}(s)^{\\top}v^{(\\ell-1)}(s)}\\\\ &{\\qquad\\quad\\le\\mathcal{T}_{\\pi^{(\\ell)}}(v^{(\\ell-1)})\\le\\mathcal{T}_{\\pi^{(\\ell)}}(v^{(\\ell)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Otherwise, if $\\pmb{v}^{(\\ell)}(s)=\\pmb{v}^{(\\ell-1)}(s)$ , then by the inductive hypothesis, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{v}^{(\\ell)}(s)=\\pmb{v}^{(\\ell-1)}(s)\\leq\\mathcal{T}_{\\pi^{(\\ell-1)}}(\\pmb{v}^{(\\ell-1)})(s)=\\mathcal{T}_{\\pi^{(\\ell)}}(\\pmb{v}^{(\\ell)})(s)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof of (6). ", "page_idx": 16}, {"type": "text", "text": "Next, we prove (7). For the lower bound, by induction and (10), we have that for each $s\\in S$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{v}^{(\\ell)}(s)\\leq\\operatorname*{max}_{a\\in A_{s}}\\{r_{a}(s)+\\gamma p_{a}(s)^{\\top}v^{(\\ell-1)}(s)\\}\\leq\\operatorname*{max}_{a\\in A_{s}}\\{r_{a}(s)+\\gamma p_{a}(s)^{\\top}v^{\\star}(s)\\}=v^{\\star},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}(\\tilde{{\\boldsymbol v}}^{(\\ell)},{\\boldsymbol v}^{(\\ell-1)}+(1-\\gamma)\\alpha)\\leq{\\boldsymbol v}^{\\star}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we prove the upper bound of (7). For each $(s,a)\\in A$ and $\\ell\\in[L]$ , let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{\\xi}_{a}^{(\\ell)}(s):={p_{a}(s)}^{\\top}\\pmb{v}^{(\\ell-1)}-(\\pmb{x}_{a}(s)+\\hat{\\pmb{g}}_{a}^{(\\ell)})(s)),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and observe that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\xi_{a}^{(\\ell)}(s)=[p_{a}(s)^{\\top}v^{(0)}-x_{a}(s)]+[p_{a}(s)^{\\top}(v^{(\\ell-1)}-v^{(0)})-\\hat{g}_{a}^{(\\ell)})(s))]\\leq\\beta_{a}(s)+\\frac{(1-\\gamma)\\alpha}{4}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that for any $s\\in S$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\pmb{v}^{\\star}-\\tilde{\\pmb{v}}^{(\\ell)})(s)=\\underset{a\\in\\mathcal{A}_{i}}{\\mathrm{max}}[r_{a}(s)+\\gamma p_{a}(s)^{\\top}\\pmb{v}^{\\star}(s)]-\\underset{a\\in\\mathcal{A}_{s}}{\\mathrm{max}}[r_{a}(s)+\\gamma(\\pmb{x}_{a}(s)+\\hat{g}_{a}^{(\\ell)})(s))]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq[r_{\\pi^{\\star}(s)}(s)+\\gamma\\left(\\pmb{P}^{\\star}\\pmb{v}^{\\star}\\right)(s)]-\\underset{a\\in\\mathcal{A}_{s}}{\\mathrm{max}}[r_{a}(s)+\\gamma p_{a}(s)^{\\top}\\pmb{v}^{(\\ell-1)}-\\gamma\\xi_{a}^{(\\ell)}(s)]}\\\\ &{\\quad\\quad\\quad\\quad\\leq[r_{\\pi^{\\star}(s)}(s)+\\gamma\\left(\\pmb{P}^{\\star}\\pmb{v}^{\\star}\\right)(s)]-[r_{\\pi^{\\star}(s)}(s)+\\gamma(\\pmb{P}^{\\star}\\pmb{v}^{(\\ell-1)})(s)-\\gamma\\xi_{\\pi^{\\star}(s)}^{(\\ell)}(s)]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\gamma\\left(\\pmb{P}^{\\star}(\\pmb{v}^{\\star}-\\pmb{v}^{(\\ell-1)})\\right)(s)+\\xi(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consequently, for all $s\\in S$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\boldsymbol{v}^{\\star}-\\tilde{\\boldsymbol{v}}^{(\\ell)})(s)\\leq\\gamma P^{\\star}(\\boldsymbol{v}^{\\star}-\\boldsymbol{v}^{(\\ell-1)})(s)+\\xi(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider two cases for ${\\pmb v}^{(\\ell)}(s)$ . First, if ${\\pmb v}^{(\\ell)}(s)=\\tilde{\\pmb v}^{(\\ell)}(s)$ for some $s\\in S$ then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(v^{\\star}-v^{(\\ell)}\\right)(s)\\leq\\gamma\\left(P^{\\star}\\left(v^{\\star}-v^{(\\ell-1)}\\right)\\right)(s)+\\pmb{\\xi}(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "holds immediately. If not, $\\pmb{v}^{(\\ell)}(s)=\\pmb{v}^{(\\ell-1)}(s)+(1-\\gamma)\\alpha\\leq\\tilde{\\pmb{v}}^{(\\ell)}(s)$ and (6) guarantees that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|v^{\\star}-v^{(\\ell-1)}\\right\\|_{\\infty}\\leq\\left\\|v^{\\star}-v^{(0)}\\right\\|_{\\infty}\\leq\\alpha,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which ensures that $(1-\\gamma)({\\pmb v}^{\\star}-{\\pmb v}^{(\\ell-1)})(s)\\leq(1-\\gamma)\\alpha$ and yields the results as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(v^{\\star}-v^{(\\ell)}\\right)(s)=\\left(v^{\\star}-v^{(\\ell-1)}\\right)(s)-(1-\\gamma)\\alpha\\leq\\gamma\\left(v^{\\star}-v^{(\\ell-1)}\\right)(s)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now inductively apply Lemma 2.4 to obtain Corollary 2.5, which allows us to bound the number of iterates required to halve the initial error in TVRVI. ", "page_idx": 16}, {"type": "text", "text": "Corollary 2.5. Suppose that for some $\\alpha\\geq0$ and $\\beta\\in\\mathbb{R}_{\\geq0}^{A}$ , $P{\\pmb v}^{(0)}-\\beta\\leq{\\pmb x}\\leq P{\\pmb v}^{(0)}$ ; $\\pmb{v}^{(0)}$ is an $\\alpha$ -underestimate of $v^{\\star}$ ; and $\\pmb{v}^{(0)}\\leq T_{\\pi^{(0)}}(\\pmb{v}^{(0)})$ . Let $\\beta_{\\pi^{\\star}}\\in\\mathbb{R}^{s}$ be defined as $\\beta_{\\pi^{\\star}}(s):=\\beta_{\\pi^{\\star}(s)}(s)$ for each $s\\in S$ . Let $(\\pmb{v}^{(L)},\\pi^{(L)})=\\mathsf{T V R V I}(\\pmb{v}^{(0)},\\pi^{(0)},\\alpha,\\delta)$ , and $L,M$ be as in Line 2. Define $\\xi:=$ $\\gamma\\,((1-\\gamma)\\alpha/4\\cdot{\\bf1}+\\beta_{\\pi^{\\star}})$ . Then, with probability $1-\\delta$ , $\\mathbf{0}\\leq v^{\\star}-v^{(L)}\\leq\\gamma^{L}\\alpha\\cdot\\mathbf{1}+(I-\\gamma P^{\\star})^{-1}\\xi$ , and $\\pmb{v}^{(L)}\\leq T_{\\pi^{(L)}}(\\pmb{v}^{(L)})$ . In particular, if $\\beta={\\bf0}$ , then for $L>\\log(8)(1-\\gamma)^{-1}$ we can reduce the error in $\\pmb{v}^{(0)}$ by half: $\\mathbf{0}\\leq v^{\\star}-v^{(L)}\\leq(v^{\\star}-v^{(0)})/2$ . Additionally, TVRVI is implementable with $\\tilde{O}(\\mathcal{A}_{\\mathrm{tot}}M L)$ sample queries to the generative model and time and $O(A_{\\mathrm{tot}})$ space. ", "page_idx": 17}, {"type": "text", "text": "Proof. Condition on the event that the implication of Lemma 2.4 holds. First, we observe that $\\mathbf{0}\\leq v^{\\star}-v_{\\pi^{(L)}}\\leq v^{\\star}-v^{(L)}$ follows by monotonicity (Equation (6) of Lemma 2.4). Next, we show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{\\star}-v^{(L)}\\leq\\gamma^{L}\\alpha\\cdot\\mathbf{1}+(I-\\gamma P^{\\star})^{-1}\\xi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by induction. We will show that for all $i\\in S$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{v}^{\\star}-\\pmb{v}^{(\\ell)}\\leq\\left[\\gamma^{\\ell}\\alpha\\mathbf{1}+\\sum_{k=0}^{\\ell}\\gamma^{k}\\pmb{P}^{\\star k}\\pmb{\\xi}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the base case when $\\ell=0$ , this is trivially true, as ${\\pmb v}^{\\star}-{\\pmb v}^{(\\ell)}\\leq\\alpha{\\bf1}$ by assumption. Assume that the statement is true up to $\\pmb{v}^{(\\ell-1)}$ . Now, by Lemma 2.4, we have two cases for $[\\bar{\\pmb{v^{\\star}}}-\\pmb{v}^{(\\ell)}](i)$ . ", "page_idx": 17}, {"type": "text", "text": "First, suppose that $[{\\boldsymbol{v}}^{\\star}-{\\boldsymbol{v}}^{(\\ell)}](i)\\,\\leq\\,\\gamma[{\\boldsymbol{v}}^{\\star}-{\\boldsymbol{v}}^{(\\ell-1)}](i)$ . Then, note that $P^{\\star}$ and $\\xi$ are entrywise non-negative, so $[\\gamma^{\\ell}{\\cal P}^{\\star\\ell}\\pmb{\\xi}](i)\\geq0$ . By inductive hypothesis, and the fact that $\\gamma\\in(0,1)$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{\\star}-v^{(\\ell)}](i)\\leq\\gamma\\left(\\gamma^{(\\ell-1)}\\alpha+\\left[\\displaystyle\\sum_{k=0}^{\\ell-1}\\gamma^{k}P^{\\star k}\\xi\\right](i)\\right)}\\\\ &{\\qquad\\qquad=\\gamma^{\\ell}\\alpha+\\gamma\\left[\\displaystyle\\sum_{k=0}^{\\ell-1}\\gamma^{k}P^{\\star k}\\xi\\right](i)\\leq\\gamma^{\\ell}\\alpha+\\left[\\displaystyle\\sum_{k=0}^{\\ell-1}\\gamma^{k}P^{\\star k}\\xi\\right](i)\\leq\\gamma^{\\ell}\\alpha+\\left[\\displaystyle\\sum_{k=0}^{\\ell}\\gamma^{k}P^{\\star k}\\xi\\right](i)}\\\\ &{\\qquad\\qquad=\\left[\\gamma^{\\ell}\\alpha\\mathbf{1}+\\displaystyle\\sum_{k=0}^{\\ell}\\gamma^{k}P^{\\star k}\\xi\\right](i).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Second, suppose that instead, $[{\\pmb v}^{\\star}-{\\pmb v}^{(\\ell)}](i)\\leq\\left[\\gamma{\\pmb P}^{\\star}\\left({\\pmb v}^{\\star}-{\\pmb v}^{(\\ell-1)}\\right)\\right](i)+{\\pmb\\xi}(i)$ . By monotonicity (equation (6) of Lemma 2.4) we know that $\\pmb{v}^{\\star}-\\pmb{v}^{(\\ell-1)}\\geq0$ . Moreover, $P^{\\star}$ is non-negative, and consequently, we can use the inductive hypothesis as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left^{\\prime}v^{\\star}-v^{(\\ell-1)}\\right)\\leq\\left[\\gamma^{\\ell-1}\\alpha\\mathbf{1}+\\sum_{k=0}^{\\ell-1}\\gamma^{k}P^{\\star k}\\xi\\right],{\\mathrm{~hence~}}P^{\\star}\\left(v^{\\star}-v^{(\\ell-1)}\\right)\\leq P^{\\star}\\left[\\gamma^{\\ell-1}\\alpha\\mathbf{1}+\\sum_{k=0}^{\\ell-1}\\gamma^{k}P^{\\star k}\\xi\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can rearrange terms to obtain the following bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{[{\\pmb v}^{\\star}-{\\pmb v}^{(\\ell)}](i)\\leq\\left[\\gamma{\\pmb P}^{\\star}\\left(\\gamma^{(\\ell-1)}\\alpha{\\pmb1}+\\displaystyle\\sum_{k=0}^{\\ell-1}\\gamma^{k}{\\pmb P}^{\\star k}\\xi\\right)\\right](i)+\\pmb\\xi(i)}}\\\\ {{=\\gamma^{\\ell}\\alpha[{\\pmb P}^{\\star}{\\bf1}](i)+\\left[\\displaystyle\\sum_{k=0}^{\\ell-1}\\gamma^{k+1}{\\pmb P}^{\\star k+1}\\xi\\right](i)+\\pmb\\xi(i)\\leq\\left[\\gamma^{\\ell}\\alpha{\\pmb1}+\\displaystyle\\sum_{k=0}^{\\ell}\\gamma^{k}{\\pmb P}^{\\star k}\\xi\\right](i).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, by induction, the bound holds. When $L>\\log(8)(1-\\gamma)^{-1}$ , $\\gamma^{L}\\leq1/8$ and we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{v^{\\star}}-\\pmb{v}_{k}\\leq\\gamma^{L}\\alpha\\cdot\\mathbf{1}+(\\pmb{I}-\\gamma\\pmb{P^{\\star}})^{-1}\\frac{\\gamma(1-\\gamma)}{4}\\alpha\\mathbf{1}\\leq\\gamma^{L}\\alpha+\\gamma\\frac{\\alpha}{4}\\leq\\frac{\\alpha}{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, the sample complexity and runtime follow from the algorithm pseudocode. For the space complexity, at each iteration $\\ell$ of the outer for loop in TVRVI, the algorithm needs only to maintain $\\hat{\\pmb g}^{(\\ell)},\\pmb g^{(\\ell)}\\,\\Tilde{\\in}\\,\\mathbb R^{A_{\\mathrm{tot}}}$ , $\\pmb{v}^{(\\ell)}\\in\\mathbb{R}^{S}$ , $\\pi^{(L)}$ , and at most $M A_{\\mathrm{{tot}}}$ samples in invoking Sample. ", "page_idx": 17}, {"type": "text", "text": "Finally, we are ready to prove Theorem 1.2. ", "page_idx": 18}, {"type": "text", "text": "Theorem 1.2. In the offline setting, there is an algorithm that uses $\\tilde{O}(\\mathrm{nnz}(P)+A_{\\mathrm{tot}}(1-\\gamma)^{-2})$ time, and computes an $\\varepsilon$ -optimal policy and $\\varepsilon$ -optimal values with probability $1-\\delta$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. To run OfflineTVRVI, we can implement a generative model from which we can draw samples in $O(\\mathrm{nnz}(P))$ pre-processing time, so that each query to the generative model requires $\\tilde{O}(1)$ time. For the correctness, we induct on $k$ to show that after each iteration $k$ $,0\\leq{\\pmb v}^{\\star}-{\\pmb v}_{\\pi_{K}}\\leq$ $\\pmb{v}^{\\star}-\\pmb{v}_{K}\\,\\leq\\,\\alpha_{k}$ with probability $1-k\\delta/K$ . In the base case when $k\\,=\\,0$ , the bound is trivially true as $\\|\\pmb{v}^{\\star}\\|_{\\infty}\\leq(1-\\gamma)^{-1}$ . Now, by Applying Corollary 2.5 and a union bound, we see that with probability 1 \u2212k\u03b4/K, v\u22c6\u2212vk \u2264\u03b1k2\u22121 , whenever $L>\\log(8)(1-\\gamma)^{-1}$ . Thus, $\\pmb{v}_{K}$ satisfies the required guarantee whenever $\\alpha_{K}\\leq\\varepsilon$ , which is guaranteed by our choice of $K$ . To see that $\\pi_{k}$ is an $\\varepsilon$ -optimal policy, we observe that Corollary 2.5 ensures ", "page_idx": 18}, {"type": "equation", "text": "$$\nv_{k}\\le{\\mathcal{T}}_{\\pi_{k}}(v_{k})\\le{\\mathcal{T}}_{\\pi_{k}}^{2}(v_{k})\\le\\cdots\\le{\\mathcal{T}}_{\\pi_{k}}^{\\infty}(v_{k})=v^{\\pi_{k}}\\le v^{\\star}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the runtime, the algorithm completes only $K=\\tilde{O}(1)$ iterations, and can be implemented with $\\tilde{O}(1)$ calls to the offset oracle. Each inner loop iteration can be implemented with $\\tilde{O}(\\mathcal{A}_{\\mathrm{tot}}L^{2})=$ $\\tilde{O}\\left(A_{\\mathrm{tot}}(1-\\gamma)^{-2}\\right)$ additional time and queries to the generative model. The algorithm only requires $O(\\mathrm{\\dot{\\mathcal{A}}_{\\mathrm{tot}}})$ space in order to store offsets, values, and approximate utilities. \u25a0 ", "page_idx": 18}, {"type": "text", "text": "B.3 Omitted proofs from Section 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem B.2 (Hoeffding\u2019s Inequality and Bernstein\u2019s Inequality, restated from Lemma E.1 and E.2 of [3]). Let $\\pmb{p}\\in\\Delta^{S}$ be a probability vector, $\\pmb{v}\\in\\mathbb{R}^{n}$ , and let $\\begin{array}{r}{\\pmb{y}:=\\frac{1}{m}\\sum_{j=1}^{m}\\pmb{v}(i_{j})}\\end{array}$ jm=1 v(ij) where ij are random indices drawn such that $i_{j}=k$ with probability $\\pmb{p}(k)$ . Define $\\sigma:=\\bar{(}p^{\\top}v^{2}-(p^{\\top}v)^{2})$ . For any $\\delta\\in(0,1).$ , the following hold, each with probability $1-\\delta$ : ", "page_idx": 18}, {"type": "text", "text": "(Hoeffding\u2019s Inequality) $\\left|p^{\\top}v-y\\right|\\leq\\left\\|v\\right\\|_{\\infty}\\cdot{\\sqrt{2m^{-1}\\log(2\\delta^{-1})}},$ (Bernstein\u2019s Inequality) $\\begin{array}{r}{\\left|p^{\\top}v-y\\right|\\leq\\sqrt{2m^{-1}\\sigma\\cdot\\log(2\\delta^{-1})}+(2/3)m^{-1}\\left\\|v\\right\\|_{\\infty}\\cdot\\log(2\\delta^{-1}).}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Theorem B.2 illustrates that the error in estimating $_{P u}$ for some value vector $\\textbf{\\em u}$ depends on the variance $\\sigma_{u}:=P u^{2}-(P u)^{2}\\in\\mathbb{R}^{\\mathcal{A}}$ . To bound this variance term, we appeal to the following two lemmas from [3]. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.3 (Lemma 5.2 of ([3]), restated). $\\begin{array}{r}{\\sqrt{\\pmb{\\sigma}_{v}}\\leq\\sqrt{\\pmb{\\sigma}_{v^{\\star}}}+\\|\\pmb{v}^{\\star}-\\pmb{v}\\|_{\\infty}\\,\\mathbf{1}.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Lemma B.4 (Lemma C.1 of ([3]), restated). For any $\\pi$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Big\\|(I-\\gamma P^{\\pi})^{-1}\\sqrt{\\sigma_{v^{\\pi}}}\\Big\\|_{\\infty}^{2}\\leq\\frac{1+\\gamma}{\\gamma^{2}(1-\\gamma)^{3}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can now bound the error in estimating $_{P u}$ using $\\mathsf{A p x U t i l i t y}(\\boldsymbol{u},N,\\eta)$ . The following Lemma 3.1 obtains such a bound by following a similar argument to that of Lemma 5.1 of [3]. ", "page_idx": 18}, {"type": "text", "text": "Lemma 3.1. Consider $\\pmb{u}\\in\\mathbb{R}^{S}$ . Let x = ApxUtility $(u,m\\cdot\\mathcal{A}_{\\mathrm{tot}},\\eta)$ , $m\\geq\\log(1/2\\delta^{-1}).$ , and $\\eta=(m\\mathcal{A}_{\\mathrm{tot}})^{-1}\\log(1/2\\delta^{-1})$ . Then, with probability $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nP u-2\\sqrt{2\\eta\\pmb{\\sigma_{v^{\\star}}}}+\\left(2\\sqrt{2\\eta}\\,\\|\\pmb{u}-\\pmb{v}^{\\star}\\|_{\\infty}+18\\eta^{3/4}\\,\\|\\pmb{u}\\|_{\\infty}\\right)\\leq\\pmb{x}\\leq P u.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For $s~\\in~s$ and $a\\in A_{s}$ . Let $i_{1},...,i_{N}\\;\\in\\;{\\cal S}$ be random indices such that $\\mathbb{P}\\left\\{i_{j}=t\\right\\}\\;=$ $(p_{a}(s))(t)$ for each $j\\in[N]$ . Define the vectors $\\tilde{\\pmb{x}}$ and $\\hat{\\pmb{\\sigma}}$ as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{x}}_{a}(s):=\\frac{1}{N}\\sum_{j=1}^{N}\\mathbf{\\boldsymbol{u}}(i_{j})\\ \\mathrm{and}\\ \\hat{\\sigma}_{a}(s):=\\frac{1}{N}\\sum_{j=1}^{N}\\big(\\mathbf{\\boldsymbol{u}}(i_{j})\\big)^{2}-\\big(\\tilde{\\mathbf{\\boldsymbol{x}}}_{a}(s)\\big)^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the pseudocode of ApxUtility (Algorithm 1), we see that that $\\pmb{x}=\\tilde{\\pmb{x}}-\\sqrt{2\\eta\\hat{\\pmb{\\sigma}}}-4\\eta^{3/4}\\left\\|\\pmb{u}\\right\\|_{\\infty}-$ $(2/3)\\eta\\,\\|\\bar{\\boldsymbol{u}}\\|_{\\infty}$ . Now, by union bound over all state-action pairs $(s,a)$ and Theorem B.2, we have that with probability $1-\\delta/2$ for each sate-action pair $(s,a)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\Vert x-P u_{\\infty}\\leq\\sqrt{2\\eta\\pmb{\\sigma}_{u}}\\right\\Vert+\\frac23\\eta\\left\\Vert\\pmb{u}\\right\\Vert_{\\infty}\\pmb{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and with probability $1-\\delta/2$ for each sate-action pair $(s,a)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{N}\\sum_{j\\in[N]}\\left(\\hat{\\pmb{\\sigma}}_{a}(s)\\right)^{2}-\\pmb{p}_{a}(s)^{\\top}\\pmb{u}^{2}\\right\\|\\leq\\|\\pmb{u}\\|_{\\infty}^{2}\\,\\sqrt{2\\eta}_{\\infty}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, by union bound and triangle inequality and (11), we have that with probability $1-\\delta$ both of the following hold. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|{\\tilde{x}}-P u\\right\\|_{\\infty}\\leq\\sqrt{2\\eta\\sigma_{u}}+\\frac{2}{3}\\eta\\left\\|u\\right\\|_{\\infty}{\\mathbf{1}},\\mathrm{~and~}\\left\\|{\\hat{\\sigma}}-\\sigma_{u}\\right\\|_{\\infty}\\leq4\\left\\|u\\right\\|_{\\infty}^{2}\\cdot\\sqrt{2\\eta}{\\mathbf{1}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We condition on (12) in the remainder of the proof. Now, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\tilde{\\mathbf{\\boldsymbol{x}}}-\\mathbf{\\boldsymbol{P}}\\mathbf{\\boldsymbol{u}}\\right|\\leq\\sqrt{2\\eta\\hat{\\sigma}}+\\left(4\\eta^{3/4}\\left\\|\\mathbf{\\boldsymbol{u}}\\right\\|_{\\infty}+\\frac{2}{3}\\eta\\left\\|\\mathbf{\\boldsymbol{u}}\\right\\|_{\\infty}\\right)\\mathbf{1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\nP u-2\\sqrt{2\\eta\\hat{\\sigma}}-\\left(8\\eta^{3/4}\\left\\|u\\right\\|_{\\infty}+\\frac{4}{3}\\eta\\left\\|u\\right\\|_{\\infty}\\right)\\mathbf{1}\\leq x\\leq P u.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By (12) and Lemma B.3, we have that for $\\alpha:=\\|\\pmb{u}-\\pmb{v}^{\\star}\\|_{\\infty}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{\\hat{\\pmb{\\sigma}}}\\leq\\sqrt{\\pmb{\\sigma}_{u}}+2\\left\\|\\pmb{u}\\right\\|_{\\infty}(2\\eta)^{1/4}\\pmb{1}\\leq\\sqrt{\\pmb{\\sigma}_{v^{\\star}}}+\\alpha\\mathbf{1}+2\\left\\|\\pmb{u}\\right\\|_{\\infty}(2\\eta)^{1/4}\\mathbf{1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\nx\\geq P u-2\\sqrt{2\\eta\\pmb{\\sigma_{v}}_{\\star}}-2\\sqrt{2\\eta}\\alpha\\mathbf{1}-16\\eta^{3/4}\\left\\|\\pmb{u}\\right\\|_{\\infty}\\mathbf{1}-\\frac43\\eta\\left\\|\\pmb{u}\\right\\|_{\\infty}\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\eta\\leq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{2\\eta\\sigma_{v^{\\star}}}+\\left(2\\sqrt{2\\eta}\\alpha+16\\eta^{3/4}\\left\\|u\\right\\|_{\\infty}+\\frac{4}{3}\\eta\\left\\|u\\right\\|_{\\infty}\\right)\\mathbf{1}\\leq2\\sqrt{2\\eta\\sigma_{v^{\\star}}}+\\left(2\\sqrt{2\\eta}\\alpha+18\\eta^{3/4}\\left\\|u\\right\\|_{\\infty}\\right)\\lambda^{2}\\left\\|u\\right\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 1.1. In the sample setting, there is an algorithm that uses $\\tilde{O}(\\mathcal{A}_{\\mathrm{tot}}[(1{-}\\gamma)^{-3}\\varepsilon^{-2}{+}(1{-}\\gamma)^{-2}])$ samples and time and $O(A_{\\mathrm{tot}})$ space, and computes an $\\varepsilon$ -optimal policy and $\\varepsilon$ -optimal values with probability $1-\\delta$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $K$ , $\\alpha_{k}$ , $(\\pmb{v}_{k},\\pi_{k})$ , and $N_{k}$ be as defined in Lines 1, 4, 9, and 6 of SampleTVRVI $(\\varepsilon,\\delta)$ . First, we show, by induction that for each $k\\in[K]$ , with probability $1-k\\delta/K$ , ", "page_idx": 19}, {"type": "text", "text": "In the base case when $k=0$ , the bound is trivially true because $\\mathbf{0}\\leq v^{\\star}-v_{\\pi_{0}}\\leq v^{\\star}-v_{0}\\leq(1\\!-\\!\\gamma)^{-1}$ . Now, for the inductive step, by Lemma 3.1 we see that with probability $1-\\delta/K$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\nP v_{k-1}-\\left[2\\sqrt{2\\eta_{k-1}\\sigma_{v^{\\star}}}+\\left(2\\sqrt{2\\eta_{k-1}}\\alpha_{k-1}+18\\eta_{k-1}^{3/4}\\left\\|v_{k-1}\\right\\|_{\\infty}\\right)\\mathbf{1}\\right]\\leq x_{k}\\leq P v_{k-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and, by inductive hypothesis, with probability $1-(k-1)\\delta/K$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{0}\\le v^{\\star}-v^{\\pi_{k-1}}\\le v^{\\star}-v_{k-1}\\le\\alpha_{k-1}\\mathbf{1},\\mathrm{~and~}v_{k}\\le{\\mathcal{T}}_{\\pi_{k-1}}(v_{k-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, by a union bound, with probability $1-k\\delta/K$ , both (13) and (14) hold. Condition on this event for the remainder of the inductive step. ", "page_idx": 19}, {"type": "text", "text": "Next, we can apply Corollary 2.5 with ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\beta}=2{\\sqrt{2\\eta_{k-1}\\sigma_{v^{\\star}}}}+\\left(2{\\sqrt{2\\eta_{k-1}}}\\alpha_{k-1}+18\\eta_{k-1}^{3/4}\\left\\|{v_{k-1}}\\right\\|_{\\infty}\\right)\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\leq v^{\\star}-v_{k}\\leq\\gamma^{L}\\alpha_{k-1}\\cdot\\mathbf{1}+(I-\\gamma P^{\\star})^{-1}\\xi_{k-1}\\leq{\\frac{\\alpha_{k-1}}{8}}\\mathbf{1}+(I-\\gamma P^{\\star})^{-1}\\xi_{k-1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $\\begin{array}{r}{\\mathbf{\\dot{\\xi}_{k-1}}\\leq\\frac{(1-\\gamma)\\alpha_{k-1}}{4}\\mathbf{1}+2\\sqrt{2\\eta_{k-1}\\pmb{\\sigma_{v^{\\star}}}}+\\left(2\\sqrt{2\\eta_{k-1}}\\alpha_{k-1}+18\\eta_{k-1}^{3/4}\\left\\|\\pmb{v}_{k-1}\\right\\|_{\\infty}\\right)\\mathbf{1}}\\end{array}$ . By Lemma B.4 and the facts that $\\eta_{k-1}\\leq(6500\\!\\cdot\\!(1\\!-\\!\\gamma)^{-3}\\operatorname*{max}((1\\!-\\!\\gamma),\\alpha_{k-1}^{-2}))^{-1}$ and $(I\\!-\\!\\gamma P^{\\star})^{-1}{\\bf1}=1/(1\\!-\\!\\gamma){\\bf1}$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{I}-\\gamma P^{\\star})^{-1}\\xi_{k-1}\\le\\left[\\frac{\\alpha_{k-1}}{4}+2\\sqrt{\\frac{6\\eta_{k-1}}{(1-\\gamma)^{3}}}+2\\sqrt{\\frac{2(1-\\gamma)^{3}\\operatorname*{min}((1-\\gamma)^{-1},\\alpha_{k-1}^{2})}{6500(1-\\gamma)^{2}}}\\alpha_{k-1}\\right]\\mathbf{1}}\\\\ &{\\qquad\\qquad+\\left[\\mathbf{18}\\left(\\frac{((1-\\gamma)^{3}\\operatorname*{min}((1-\\gamma)^{-1},\\alpha_{k-1}^{2})}{6500(1-\\gamma)^{8/3}}\\right)^{3/4}\\right]\\mathbf{1}}\\\\ &{\\qquad\\qquad\\le\\left[\\alpha_{k-1}/4+2\\sqrt{6/6500}\\cdot\\alpha_{k-1}+2\\sqrt{2/6500}(1-\\gamma)^{1/2}\\operatorname*{min}((1-\\gamma)^{-1/2},\\alpha_{k-1}}\\\\ &{\\qquad\\qquad+18\\cdot(10^{-3})(1-\\gamma)^{1/4}\\operatorname*{min}((1-\\gamma)^{-3/4},\\alpha_{k-1}^{3/2})\\right]\\mathbf{1}}\\\\ &{\\qquad\\qquad\\le\\left[\\alpha_{k-1}/4+4\\sqrt{6/6500}\\cdot\\alpha_{k-1}+18\\cdot(10^{-3})\\alpha_{k-1}\\right]\\mathbf{1}\\le\\frac{3}{8}\\alpha_{k-1}\\mathbf{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consequently, $\\pmb{v}^{\\star}\\,-\\,\\pmb{v}_{k}\\,\\leq\\,\\alpha/2\\mathbf{1}$ . To see that $\\pi_{k}$ is also an $\\alpha_{k}$ -optimal policy, we observe that Corollary 2.5 also ensures that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{k}\\le{\\mathcal T}_{\\pi_{k}}(v_{k})\\le{\\mathcal T}_{\\pi_{k}}^{2}(v_{k})\\le\\cdots\\le{\\mathcal T}_{\\pi_{k}}^{\\infty}(v_{k})=v^{\\pi_{k}}\\le v^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This completes the inductive step. ", "page_idx": 20}, {"type": "text", "text": "Consequently, for $k=K=\\lceil\\log_{2}(\\varepsilon^{-1}(1-\\gamma)^{-1})\\rceil$ iterations, $\\varepsilon\\ge\\alpha_{K}\\ge\\varepsilon/4$ and with probability $1-\\delta$ , $v_{K}$ is an $\\varepsilon$ -optimal value and $\\pi_{K}$ is an $\\varepsilon$ -optimal policy. ", "page_idx": 20}, {"type": "text", "text": "For runtime and sample complexity, note that the algorithm can be implemented using only $\\tilde{O}(N_{K})=$ $\\tilde{O}((1-\\gamma)^{-3}\\varepsilon^{-2}+(1-\\gamma)^{3})$ -samples and time per state-action pair. For the space complexity, note that the algorithm can be implemented to maintain only $O(1)$ vectors in $\\mathbb{R}^{\\mathcal{A}_{\\mathrm{tot}}}$ . ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, the abstract and introduction state our main results and improvements over previous work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we discuss regimes where our result is optimal and where it may be suboptimal in the introduction. In the conclusion we also discuss directions for future work and open problems left open by our work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Sections 2 and 3 of our paper give a sketch of how we obtain our main theorems, and full proofs of all intermediate results as well as the full theorems can be found in the supplemental material/appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper focuses on theory and does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper focuses on theoretical results and mathematical analysis and does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we have read and conformed to the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper focuses on foundational theory for solving MDPs and is not directly tied to any specific societal impacts (positive or negative). We do not expect any direct, immediate, substantial societal impacts. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper focuses on foundational theory and does not pose any such risks. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not use any existing code/data/model assets because we do not have any experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper did not involve crowdsourcing or research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper did not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]