[{"figure_path": "WRd9LCbvxN/tables/tables_9_1.jpg", "caption": "Table 1: FID Score of edited images with Imagic, MasaCtrl (with T2I adapter), and ours.", "description": "This table presents a quantitative comparison of the proposed PA-Diffusion model against two state-of-the-art image editing methods, Imagic and MasaCtrl (using a T2I adapter), in terms of Fr\u00e9chet Inception Distance (FID) scores.  The FID score is a metric used to evaluate the realism and quality of generated images, with lower scores indicating higher quality and similarity to real images. The comparison is performed across six object categories: Storage, Laptop, Microwave, Trashcan, Refrigerator, and Drawer. The results demonstrate that the PA-Diffusion model significantly outperforms the other methods, achieving substantially lower FID scores.", "section": "4.5 Quantitative Evaluation"}, {"figure_path": "WRd9LCbvxN/tables/tables_9_2.jpg", "caption": "Table 2: Prediction accuracy of the model developed with half (left) and full (right) training set separately. bbox+axis(rot) bbox+axis(tran)", "description": "This table presents the quantitative evaluation results of a model trained using two different training sets.  The results are split to show the performance when half of the training set is used (left), versus the full training set (right). The model's performance is evaluated across six different object categories using three distinct metrics: bounding box (bbox) accuracy, bounding box plus axis (bbox+axis) accuracy (for both rotation and translation), and normal vector accuracy (within a 30\u00b0 error margin).  The table demonstrates the effect of training data size on the model's ability to accurately predict these aspects of the articulated objects.", "section": "4.5 Quantitative Evaluation"}, {"figure_path": "WRd9LCbvxN/tables/tables_9_3.jpg", "caption": "Table 2: Prediction accuracy of the model developed with half (left) and full (right) training set separately. bbox+axis(rot) bbox+axis(tran) Dataset AUROC\u2191 bbox\u2191 bbox+axis(rot) \u2191 normal \u2191 bbox\u2191 bbox+axis(tran) \u2191 normal \u2191 Internet Video 74.0 62.1 28.2 16.4 32.0 26.2 14.3 Mixed 75.6 65.1 28.5 16.4 35.2 27.2 21.7", "description": "This table presents the quantitative results of the experiment on articulated object understanding.  The model is trained on either half or the full training dataset, and then evaluated on the test set. The metrics reported include AUROC (Area Under the ROC Curve), bounding box accuracy (bbox), bounding box plus axis prediction accuracy (bbox+axis), and surface normal prediction accuracy (normal).  The table is separated into two parts: (1) a dataset containing only the Internet Video Dataset and (2) a dataset combining Internet Video with other images.  Rotation (rot) and Translation (tran) joint types are evaluated separately. Higher scores generally indicate better performance.", "section": "4.6 Articulated Object Understanding"}, {"figure_path": "WRd9LCbvxN/tables/tables_16_1.jpg", "caption": "Table 4: FID score of edited images with different additional losses: no SCSL and TCSL, with SCSL only, with TCSL only, and with all losses. The performance improves more than 57% with the assistance of the two losses.", "description": "This table presents the FID (Frechet Inception Distance) scores for image editing results using different combinations of loss functions: Texture Consistency Score Loss (TCSL) and Style Consistency Score Loss (SCSL).  It shows that using both TCSL and SCSL significantly improves the quality of the edited images (as measured by FID score), resulting in a more than 57% improvement compared to using neither loss function.", "section": "Additional Ablation Study"}]