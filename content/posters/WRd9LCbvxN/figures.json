[{"figure_path": "WRd9LCbvxN/figures/figures_1_1.jpg", "caption": "Figure 1: We propose the Part-Aware Diffusion Model: Abstract 3D model of the articulated object is constructed referring to the input 2D real image. Arbitrary manipulation can be done in 3D space based on the text instruction or human interaction, the generation model then creates the edited image according to the manipulation.", "description": "This figure illustrates the overall pipeline of the Part-Aware Diffusion Model.  It begins with a real 2D image of an object. This 2D image is then mapped to a 3D representation using an abstract 3D model.  In the 3D space, manipulations based on text instructions or direct user interaction are performed.  Finally, the manipulated 3D model is used to generate a new, edited 2D image reflecting the changes.", "section": "1 Introduction"}, {"figure_path": "WRd9LCbvxN/figures/figures_3_1.jpg", "caption": "Figure 2: The overall image editing process. (1) In the Pre-Process stage, articulated objects in 2D images are part-level segmented and reconstructed to abstract 3D models. Meanwhile, inverted noise maps of input images are created with DDIM Inversion. (2) In the Manipulation stage, arbitrary manipulation can be implemented in the 3D space based on text guidance or human interaction. (3) After manipulation, part-level masks and sketches are rendered and exported. The inverted noise maps are transformed according to these masks. (4) Finally, with the transformed inverted noise maps, sketch maps, and part-level masks, the generation model creates the edited images.", "description": "This figure illustrates the four main stages of the PA-Diffusion model's image editing process: Pre-process, Manipulation, Feature Process, and Generation.  The pre-process stage involves segmenting the articulated object from the input image, creating a 3D model using the Primitive Prototype Library, and inverting the noise map of the input image using DDIM. The manipulation stage involves manipulating the 3D model based on text or interactive input. The feature process then transforms the inverted noise map and creates compositional activation maps, while the generation stage uses a denoising U-Net to create the final edited image.", "section": "3 Method"}, {"figure_path": "WRd9LCbvxN/figures/figures_4_1.jpg", "caption": "Figure 2: The overall image editing process. (1) In the Pre-Process stage, articulated objects in 2D images are part-level segmented and reconstructed to abstract 3D models. Meanwhile, inverted noise maps of input images are created with DDIM Inversion. (2) In the Manipulation stage, arbitrary manipulation can be implemented in the 3D space based on text guidance or human interaction. (3) After manipulation, part-level masks and sketches are rendered and exported. The inverted noise maps are transformed according to these masks. (4) Finally, with the transformed inverted noise maps, sketch maps, and part-level masks, the generation model creates the edited images.", "description": "This figure illustrates the pipeline of the PA-Diffusion model for image editing. It starts with preprocessing the input image to generate 3D models and inverted noise maps. Then, the manipulation stage involves using text or interactive means to adjust the 3D object. Finally, the edited image is generated using the transformed noise maps, sketch maps and part-level masks. Each stage is clearly shown with different components and their interactions.", "section": "3 Method"}, {"figure_path": "WRd9LCbvxN/figures/figures_6_1.jpg", "caption": "Figure 4: Results of basic manipulations: move, scale/shear, rotate, and manipulate. Blank regions caused by the manipulation are in-painted automatically. The novel views and novel-appearing parts match with the style of the seen parts (left). Articulated objects are opened from 0\u00b0 to 120\u00b0. The appearance of the increasing novel-appearing parts keeps being consistent throughout the whole process (right).", "description": "This figure demonstrates the results of basic manipulations (move, scale/shear, rotate) and a sequential manipulation process on articulated objects using the proposed PA-Diffusion model.  The left side shows how the model handles basic manipulations, seamlessly integrating edited objects with their backgrounds and automatically in-painting blank areas. The right side showcases a sequential opening of an object (from 0\u00b0 to 120\u00b0), highlighting the model's ability to maintain consistent style and appearance even as new parts of the object become visible.", "section": "4.2 Results"}, {"figure_path": "WRd9LCbvxN/figures/figures_7_1.jpg", "caption": "Figure 5: Manipulate non-rigid objects, non-uniform shapes, and objects with weird or multiple joint types.", "description": "This figure showcases the PA-Diffusion model's ability to manipulate a variety of objects, demonstrating its versatility in handling different types of objects.  It highlights the model's capacity to handle non-rigid objects (like toys), objects with non-uniform shapes (the toy shark), and those with unusual or multiple joint types (the broken cup, kitchen pot, and storage furniture).  The results show the model's robustness in generating realistic and coherent edits, even for complex objects and manipulations.", "section": "4. Results"}, {"figure_path": "WRd9LCbvxN/figures/figures_8_1.jpg", "caption": "Figure 6: TCSL and SCSL are employed to release the blurry and style mismatch problem. The model is required to move and open the object (left). Comparison of Imagic, DragDiffusion, MasaCtrl (with T2I adapter), Image Sculpting, and our PA-Diffusion model. The target state is \u2018a photo of an opened object\u2019 (right).", "description": "The figure demonstrates a comparison of different image editing models' ability to manipulate articulated objects. The left side shows the effect of using TCSL and SCSL loss functions in the PA-Diffusion model to reduce blurriness and style inconsistencies. The right side compares the PA-Diffusion model with several state-of-the-art methods (Imagic, DragDiffusion, MasaCtrl, Image Sculpting), highlighting its superior performance in generating realistic and coherent edits of various objects (storage furniture, laptop, trashcan, microwave, etc.).", "section": "4 Experiment"}, {"figure_path": "WRd9LCbvxN/figures/figures_9_1.jpg", "caption": "Figure 2: The overall image editing process. (1) In the Pre-Process stage, articulated objects in 2D images are part-level segmented and reconstructed to abstract 3D models. Meanwhile, inverted noise maps of input images are created with DDIM Inversion. (2) In the Manipulation stage, arbitrary manipulation can be implemented in the 3D space based on text guidance or human interaction. (3) After manipulation, part-level masks and sketches are rendered and exported. The inverted noise maps are transformed according to these masks. (4) Finally, with the transformed inverted noise maps, sketch maps, and part-level masks, the generation model creates the edited images.", "description": "This figure illustrates the pipeline of the proposed Part-Aware Diffusion Model (PA-Diffusion). It shows the process from pre-processing the input image (2D articulated object reconstruction and noise map inversion), to manipulating the objects in the 3D space (text or interaction-based), and finally generating the edited image using transformed noise maps, sketch maps, and part-level masks.", "section": "3 Method"}, {"figure_path": "WRd9LCbvxN/figures/figures_15_1.jpg", "caption": "Figure 2: The overall image editing process. (1) In the Pre-Process stage, articulated objects in 2D images are part-level segmented and reconstructed to abstract 3D models. Meanwhile, inverted noise maps of input images are created with DDIM Inversion. (2) In the Manipulation stage, arbitrary manipulation can be implemented in the 3D space based on text guidance or human interaction. (3) After manipulation, part-level masks and sketches are rendered and exported. The inverted noise maps are transformed according to these masks. (4) Finally, with the transformed inverted noise maps, sketch maps, and part-level masks, the generation model creates the edited images.", "description": "This figure illustrates the four stages of the image editing process using the Part-Aware Diffusion Model.  First, the input image is pre-processed, segmenting articulated objects and creating their abstract 3D models and inverted noise maps. Second, these 3D models are manipulated using text or human interaction. Third, masks and sketches based on the manipulation are generated. Finally, these masks, sketches, and transformed inverted noise maps are used in a generation model to create the final edited image.", "section": "3 Method"}, {"figure_path": "WRd9LCbvxN/figures/figures_15_2.jpg", "caption": "Figure 2: The overall image editing process. (1) In the Pre-Process stage, articulated objects in 2D images are part-level segmented and reconstructed to abstract 3D models. Meanwhile, inverted noise maps of input images are created with DDIM Inversion. (2) In the Manipulation stage, arbitrary manipulation can be implemented in the 3D space based on text guidance or human interaction. (3) After manipulation, part-level masks and sketches are rendered and exported. The inverted noise maps are transformed according to these masks. (4) Finally, with the transformed inverted noise maps, sketch maps, and part-level masks, the generation model creates the edited images.", "description": "This figure illustrates the overall pipeline of the proposed PA-Diffusion model for image editing.  It shows four stages: 1) Pre-processing, where input images are processed and 3D models created; 2) Manipulation, allowing for arbitrary changes to the 3D model based on text or user interaction; 3) Feature Processing, where masks and sketches are generated for use in the next stage; and 4) Generation, the final image generation step using a diffusion model.", "section": "3 Method"}, {"figure_path": "WRd9LCbvxN/figures/figures_16_1.jpg", "caption": "Figure 4: Results of basic manipulations: move, scale/shear, rotate, and manipulate. Blank regions caused by the manipulation are in-painted automatically. The novel views and novel-appearing parts match with the style of the seen parts (left). Articulated objects are opened from 0\u00b0 to 120\u00b0. The appearance of the increasing novel-appearing parts keeps being consistent throughout the whole process (right).", "description": "This figure shows the results of basic manipulations (move, scale/shear, rotate) and a sequential manipulation process using the PA-Diffusion model. The left side demonstrates the model's ability to seamlessly integrate manipulations into the scene, with in-painting automatically handling blank regions. The right side showcases the consistency in appearance of novel parts created during a sequential process (opening an object from 0\u00b0 to 120\u00b0).", "section": "4.2 Results"}, {"figure_path": "WRd9LCbvxN/figures/figures_17_1.jpg", "caption": "Figure 10: Additional demonstration of the images edited with our proposed PA-Diffusion model, including storage furniture, laptop, microwave, trashcan, door, drawer, refrigerator, and toilet.", "description": "This figure shows additional examples of images edited using the proposed PA-Diffusion model.  The results demonstrate the model's ability to manipulate various articulated objects across different categories (storage furniture, laptop, microwave, trashcan, door, drawer, refrigerator, and toilet) while maintaining realistic image quality and consistency.  Each row shows the original image and then the manipulated image, highlighting the successful application of the model to a wider range of objects and manipulation tasks.", "section": "Additional Manipulation Results"}, {"figure_path": "WRd9LCbvxN/figures/figures_18_1.jpg", "caption": "Figure 12: Reconstructed 3D object models with Image Sculpting, and our abstract 3D models created with Primitive Prototype Library.", "description": "This figure compares the 3D models reconstructed by the Image Sculpting method and the proposed method.  Image Sculpting uses a 2D-to-3D reconstruction technique to create detailed 3D models, while the proposed method uses a simpler, abstract 3D model based on primitive prototypes (like cubes and planes). The figure shows the original images, front and side views of the reconstructed 3D models from Image Sculpting, the manipulated 3D models from Image Sculpting, the front and side views of the abstract 3D models generated by the proposed method, and the manipulated abstract 3D models. This visual comparison highlights the differences in model complexity and the efficiency of the proposed method, which uses simpler primitives for faster and more efficient manipulation.", "section": "3D Models analysis"}, {"figure_path": "WRd9LCbvxN/figures/figures_20_1.jpg", "caption": "Figure 13: Demonstration of extracting bounding box annotations and rotation/translation axis annotations from part-level masks.", "description": "This figure demonstrates the process of obtaining bounding box and rotation/translation axis annotations from part-level masks.  It visually shows how these annotations are derived from the segmented parts of objects. Each row represents a different object, displaying the segmented parts (Part1 and Part2), the corresponding RGB image section and the extracted annotations overlaid on the RGB image. The annotations are crucial for the 3D manipulation and understanding stages of the PA-Diffusion model.", "section": "3.3 Arbitrary Manipulation in 3D Space"}, {"figure_path": "WRd9LCbvxN/figures/figures_20_2.jpg", "caption": "Figure 12: Reconstructed 3D object models with Image Sculpting, and our abstract 3D models created with Primitive Prototype Library.", "description": "This figure compares the 3D object models reconstructed by the Image Sculpting method and the proposed method. Image Sculpting uses a 2D-to-3D reconstruction pipeline, while the proposed method uses abstract 3D models built from a primitive prototype library.  The comparison shows that the proposed method's abstract 3D models are simpler, more easily manipulated, and more suitable for generating high-fidelity edited images than the complex and often inaccurate 3D models produced by Image Sculpting. This highlights the efficiency and effectiveness of the proposed approach for articulated object manipulation in real images.", "section": "3D Models analysis"}, {"figure_path": "WRd9LCbvxN/figures/figures_20_3.jpg", "caption": "Figure 15: Limitation of the PA-Diffusion model: dealing with low-quality images or large deformation.", "description": "This figure demonstrates limitations of the PA-Diffusion model. The left side shows that when the input image quality is low (low resolution), the model fails to regenerate the original image or perform simple manipulations like moving and scaling.  The right side illustrates that when the object undergoes significant shape deformation, the model struggles to maintain the object's appearance and the output image is not preserved well.  These limitations highlight scenarios where the model's performance is impacted by the input image quality and the extent of object shape changes.", "section": "Limitations"}]