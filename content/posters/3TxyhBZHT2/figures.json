[{"figure_path": "3TxyhBZHT2/figures/figures_1_1.jpg", "caption": "Figure 1: Evaluation settings and major results of different vision foundation models (VFMs) for complex 3D scene understanding. We assess the performance of VFMs on multimodal scene reasoning, grounding, segmentation, and registration tasks.", "description": "This figure shows the evaluation results of seven different vision foundation models (VFMs) on four tasks related to 3D scene understanding: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration.  The VFMs are categorized by input modality (Image, Video, 3D Points) and pretraining task (Self-supervised Learning, Language-guided Learning, Generation). The plot visualizes the performance of each VFM on each task, allowing for comparison across different model types and tasks.  This helps to identify the strengths and weaknesses of different VFMs for various 3D scene understanding scenarios.", "section": "Visual Foundation Models for Scene Understanding"}, {"figure_path": "3TxyhBZHT2/figures/figures_3_1.jpg", "caption": "Figure 2: Our unified probing framework to evaluate visual foundation models on various tasks.", "description": "This figure illustrates the architecture used to evaluate various visual foundation models on different tasks related to 3D scene understanding.  The framework takes as input posed images, videos, and 3D points representing the scene. These are then fed into seven different vision foundation models (DINOv2, LSeg, CLIP, Stable Diffusion, V-JEPA, Stable Video Diffusion, Swin3D).  A multi-view 3D projection module projects image and video features into 3D space to create a consistent 3D feature field. This 3D feature field is then used to perform four different downstream tasks: Vision-Language Scene Reasoning (assessing scene-level understanding), Visual Grounding (evaluating object-level understanding), Semantic Segmentation (measuring semantic understanding), and Registration (testing geometric understanding). The figure clearly depicts the unified workflow for evaluating different models and showcases the multi-faceted evaluation approach.", "section": "3 Probing Visual Encoders for Scene Understanding"}, {"figure_path": "3TxyhBZHT2/figures/figures_3_2.jpg", "caption": "Figure 3: Visualization of extracted scene features from different visual foundation models. We use principal component analysis (PCA) to compress the feature embeddings into three dimensions. The clear distinction between colors and patterns demonstrates the behaviors of different models.", "description": "This figure visualizes the features extracted by different visual foundation models (VFMs) using principal component analysis (PCA) to reduce dimensionality to 3D for better visualization.  The resulting visualizations show distinct patterns and colors for each model, highlighting their unique feature representations and demonstrating how different VFMs capture different aspects of the scene.", "section": "3 Probing Visual Encoders for Scene Understanding"}, {"figure_path": "3TxyhBZHT2/figures/figures_5_1.jpg", "caption": "Figure 1: Evaluation settings and major results of different vision foundation models (VFMs) for complex 3D scene understanding. We assess the performance of VFMs on multimodal scene reasoning, grounding, segmentation, and registration tasks.", "description": "This figure summarizes the experimental setup and key findings of the Lexicon3D paper. It shows the performance of seven different vision foundation models (VFMs) across four scene understanding tasks: Vision-Language Scene Reasoning, Visual Grounding, Semantic Segmentation, and Registration.  The VFMs are categorized by input modality (image, video, 3D points) and pretraining task (self-supervised learning, language-guided learning, generation). The results highlight the strengths and weaknesses of each model type across different tasks, revealing for example, that unsupervised image foundation models generally perform best overall, while video models excel in object-level tasks, and diffusion models are beneficial for geometric tasks.  The figure uses a scatter plot to visualize the performance of each VFM on each task, providing a concise overview of the extensive evaluation reported in the paper.", "section": "Abstract"}, {"figure_path": "3TxyhBZHT2/figures/figures_6_1.jpg", "caption": "Figure 5: Visualization of 3D semantic segmentation on ScanNet [20]. Image encoders obtain better performance.", "description": "This figure visualizes the results of 3D semantic segmentation on the ScanNet dataset using different vision foundation models.  Each row shows a different scene with the RGB image, ground truth segmentation, and the segmentation results obtained by seven different models: DINOv2, LSeg, CLIP, Stable Diffusion, V-JEPA, Stable Video Diffusion, and Swin3D. The results demonstrate that image-based encoders generally achieve superior performance compared to video-based and 3D point-based encoders in semantic segmentation.", "section": "3.4 Semantic Segmentation"}, {"figure_path": "3TxyhBZHT2/figures/figures_8_1.jpg", "caption": "Figure 6: Memory usage of different encoders. An ideal model should be a small circle and be positioned in the upper left.", "description": "This figure visualizes the memory usage and inference time of different visual foundation models used in the paper.  Each model is represented by a circle, where the horizontal position represents inference time (log scale) and the vertical position represents ScanQA BLEU-4 performance.  Ideally, a model would have both low memory usage and high performance, placing it in the upper-left corner of the graph.  The size of the circle is proportional to the memory used.", "section": "4 Complexity Analysis"}, {"figure_path": "3TxyhBZHT2/figures/figures_8_2.jpg", "caption": "Figure 7: Evaluation on different video downsampling strategies for V-JEPA on the segmentation task. Keyframe Sampling samples every N frames to form a new video sequence, while Clip Sampling directly samples consecutive video clips. The performance before downsampling is regarded as 100%. Keyframe sampling demonstrates less performance drop with the same level of downsampling.", "description": "This figure compares two video downsampling strategies: keyframe sampling and clip sampling.  Keyframe sampling selects frames at regular intervals, while clip sampling takes consecutive sequences.  The results show that keyframe sampling preserves performance better than clip sampling when reducing the number of frames.", "section": "4.2 Ablation Study \u2013 Insights into Optimal Usage of Visual Foundation Models"}, {"figure_path": "3TxyhBZHT2/figures/figures_9_1.jpg", "caption": "Figure 8: Evaluation on the segmentation task with (1) LSeg, (2) SD, (3) Swin3D, and their combinations.", "description": "This figure visualizes the results of an ablation study on the semantic segmentation task.  It shows the performance of using different combinations of three visual foundation models (LSeg, Stable Diffusion, and Swin3D). Each model's features are concatenated, and the resulting mIoU is measured. The results indicate that combining multiple encoders, particularly LSeg and Swin3D, can improve performance. However, simply combining the three best-performing individual models (1+2+3) doesn't necessarily guarantee the best overall performance. The experiment highlights the potential benefits and complexities of leveraging multiple visual foundation models for scene understanding tasks.", "section": "4.2 Ablation Study \u2013 Insights into Optimal Usage of Visual Foundation Models"}, {"figure_path": "3TxyhBZHT2/figures/figures_19_1.jpg", "caption": "Figure A: Evaluation curves of Relative Rotation Error (RRE) and Relative Translation Error (RTE) on the partial scene registration task during different training stages.", "description": "The figure shows the relative rotation error (RRE) and relative translation error (RTE) during the training of a partial scene registration task using different vision foundation models.  The x-axis represents the training epoch, and the y-axis shows the error metric. The lines represent different models' performance over the training epochs, allowing for a visual comparison of their convergence rates and final accuracy.  This visualization helps to understand how different models learn to align point clouds in a partial scene registration scenario. Notably, models like StableDiffusion and StableVideoDiffusion show relatively lower error rates and faster convergence compared to others. ", "section": "B.3 Evaluation Curves During Different Training Stages"}, {"figure_path": "3TxyhBZHT2/figures/figures_20_1.jpg", "caption": "Figure B: Visualization of partial scene registration results. The StableDiffusion and StableVideoDiffusion family of generative models receives superior performance. In addition, video encoders such as V-JEPA and StableVideoDiffusion have better geometric understanding capability than image encoders.", "description": "This figure visualizes the results of partial scene registration for different vision foundation models. The visualizations show two partial point clouds, P1 and P2, overlaid on top of each other, with a color-coded representation of the registration quality. The models that achieved higher accuracy in the registration task are displayed with more accurate alignment of the two point clouds.  Specifically, Stable Diffusion and Stable Video Diffusion models show better registration performance, and video-based models (V-JEPA and Stable Video Diffusion) generally perform better than image-based models.  This illustrates that generative models and models with temporal information are better suited for this geometric task.", "section": "Additional Qualitative Results"}]