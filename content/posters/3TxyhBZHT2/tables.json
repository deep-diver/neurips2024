[{"figure_path": "3TxyhBZHT2/tables/tables_2_1.jpg", "caption": "Table 1: Details of the seven evaluated VFMs. In supervision signals, we use \"SSL\" to represent self-supervised learning, and use \u201cVLM\u201d to represent vision-language modality alignment. A more detailed explanation of the evaluated VFMs is provided in the supplementary material A.", "description": "This table lists the seven visual foundation models (VFMs) used in the paper's experiments.  For each model, it specifies the input modality (image, video, or 3D points), the architecture used, the type of supervision (self-supervised learning or vision-language modality alignment), and the dataset used for pretraining.  More details about each VFM can be found in Appendix A of the supplementary material.", "section": "2 Related Work"}, {"figure_path": "3TxyhBZHT2/tables/tables_5_1.jpg", "caption": "Table 2: Evaluation of vision-language reasoning on ScanQA [5] and SQA3D [55] datasets. The top-2 results for each metric are shown in red and green, respectively. The 3D-LLM results [36] are shown for reference, indicating the relative position of our evaluation results with respect to the leading models trained on this task.", "description": "This table presents the quantitative results of vision-language reasoning experiments conducted on two benchmark datasets: ScanQA and SQA3D.  Seven different vision foundation models (VFMs) are evaluated based on several metrics (BLEU-1, BLEU-4, METEOR, ROUGE, CIDEr, EM-1). The results highlight the relative performance of different VFMs in this task and compare them to a state-of-the-art model (3D-LLM) for reference.  The top two performing models for each metric are highlighted.", "section": "3.2 Vision-Language Reasoning"}, {"figure_path": "3TxyhBZHT2/tables/tables_6_1.jpg", "caption": "Table 3: Evaluation of 3D object grounding on ScanRefer [16]. Video models exhibit significant advantages.", "description": "This table presents the results of the 3D object grounding task evaluation on the ScanRefer dataset.  It compares the performance of seven different vision foundation models (VFMs) across three categories: Unique (objects with a single semantic class in the scene), Multiple (objects with multiple instances of the same semantic class), and Overall (all objects). The results show that video encoding models significantly outperform image and 3D encoders, particularly in the Multiple category, highlighting the advantage of temporal information in distinguishing objects of the same semantic class.", "section": "3.3 Visual Grounding"}, {"figure_path": "3TxyhBZHT2/tables/tables_7_1.jpg", "caption": "Table 5: Evaluation of partial scene registration on ScanNet [20]. We employ Registration Recall (RR) at various RMSE thresholds, Relative Rotation Error (RRE), and Relative Translation Error (RTE) as evaluation metrics. A higher RR indicates better performance, while lower RRE and RTE values signify superior results.", "description": "This table presents the results of evaluating seven different visual foundation models on a partial scene registration task using the ScanNet dataset.  The models are assessed based on three metrics: Registration Recall (RR) at different distances (0.05m, 0.1m, 0.2m), Relative Rotation Error (RRE), and Relative Translation Error (RTE). Higher RR values indicate better performance, while lower RRE and RTE values are preferred.  The table shows that Stable Diffusion and Stable Video Diffusion models achieve the highest RR values and lowest error values, suggesting their superiority in this specific task compared to other models tested.", "section": "3.5 Registration: Geometric Correspondence"}, {"figure_path": "3TxyhBZHT2/tables/tables_7_2.jpg", "caption": "Table 4: Evaluation of semantic segmentation on ScanNet [20] benchmark.", "description": "This table presents the results of the semantic segmentation task on the ScanNet benchmark.  It compares the performance of different vision foundation models (VFMs) in terms of accuracy (Acc), mean accuracy (mAcc), and mean Intersection over Union (mIoU). Higher values indicate better performance.  The table includes results for DINOv2, LSeg, CLIP, Stable Diffusion, V-JEPA, Stable Video Diffusion, and Swin3D, with a comparison to the GrowSP baseline.", "section": "3.4 Semantic Segmentation"}, {"figure_path": "3TxyhBZHT2/tables/tables_8_1.jpg", "caption": "Table 6: Complexity analysis of visual foundation models.", "description": "This table presents a complexity analysis of seven different visual foundation models.  It shows the time taken to process a single sample, the time to process an entire scene, and the memory usage for each model.  The models are compared across various metrics to provide insights into their computational efficiency and resource requirements for 3D scene understanding.", "section": "4 Complexity Analysis"}, {"figure_path": "3TxyhBZHT2/tables/tables_9_1.jpg", "caption": "Table 7: Evaluation of diffusion noise level and feature layers when using StableDiffusion [73] for feature extraction. The settings we choose are highlighted with the grey color.", "description": "This table presents the ablation study on Stable Diffusion model, evaluating the impact of different noise levels and feature layers on the model's performance.  The noise levels refer to the number of steps in the diffusion process, while the feature layers correspond to different layers in the decoder network. The results show that the optimal noise level is 100 steps and the optimal feature layer is 1, suggesting the importance of choosing the right hyperparameters in using Stable Diffusion for feature extraction tasks.", "section": "4.3 Diffusion Noise Level and Feature Layer"}, {"figure_path": "3TxyhBZHT2/tables/tables_18_1.jpg", "caption": "Table A: Comparison between Uni3D and Swin3D on four of our evaluation tasks. Object-centric and scene-centric methods demonstrate significant differences.", "description": "This table compares the performance of two different 3D foundation models, Uni3D and Swin3D, across four scene understanding tasks: Vision-Language Question Answering (VQA), Visual Grounding, Semantic Segmentation, and Registration.  Uni3D, being object-centric, focuses on individual objects within a scene, while Swin3D takes a scene-centric approach, considering the overall scene context. The table highlights the significant performance differences between these two approaches across the four tasks, demonstrating the impact of model architecture and training strategy on downstream scene understanding capabilities.", "section": "B.1 Comparison Between Scene-level and Object-centric Models"}, {"figure_path": "3TxyhBZHT2/tables/tables_18_2.jpg", "caption": "Table B: Comparison between SAM and LSeg on four of our evaluation tasks. Instance-aware segmentation and semantic-aware segmentation methods demonstrate significant differences.", "description": "This table compares the performance of two different models, SAM (Segment Anything Model) and LSeg, across four tasks related to 3D scene understanding.  SAM is an instance segmentation model, focusing on identifying individual objects within a scene, while LSeg is a semantic segmentation model, focusing on assigning semantic labels to each pixel. The tasks evaluated include Vision-Language Reasoning, Visual Grounding, Semantic Segmentation, and Registration.  The results highlight that the choice of model significantly affects performance across these different aspects of 3D scene understanding, with SAM excelling in tasks requiring precise object localization and LSeg performing better on tasks requiring semantic understanding.", "section": "3.4 Semantic Segmentation"}]