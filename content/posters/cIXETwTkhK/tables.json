[{"figure_path": "cIXETwTkhK/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison by testing accuracy (%) on CIFAR-10 using VGG-Small and ResNet18 architectures with binarized activations and weights.", "description": "This table compares the performance of various methods on the CIFAR-10 dataset using two different network architectures: VGG-Small and ResNet18.  All methods use binarized activations and weights. The table shows the testing accuracy achieved by each method, providing a quantitative comparison of their effectiveness.", "section": "4 Experiments"}, {"figure_path": "cIXETwTkhK/tables/tables_7_2.jpg", "caption": "Table 2: Performance comparison by testing accuracy (%) of various approaches on CIFAR-10, CIFAR-100, and Tiny-ImageNet across VGG16 and ResNet18 architectures with binarized weights only. (\u2020) indicates that results are obtained from the numbers reported by Ajanthan et al. [2021]. (*) indicates that results are obtained from the numbers reported by Le et al. [2022].", "description": "This table compares the performance of different BNN training methods on three benchmark datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) using two different network architectures (VGG16 and ResNet18).  The methods are evaluated based on their testing accuracy.  The table highlights the superior performance of the proposed VISPA method compared to existing state-of-the-art techniques, particularly on more complex datasets like CIFAR-100 and Tiny-ImageNet.", "section": "4 Experiments"}, {"figure_path": "cIXETwTkhK/tables/tables_7_3.jpg", "caption": "Table 3: Performance comparison by testing accuracy of various methods on ImageNet dataset at AlexNet. W/A denotes the bit-width of weights and activations.", "description": "This table compares the performance of different methods for training binarized neural networks on the ImageNet dataset using the AlexNet architecture.  The comparison includes methods that use 1-bit weights and activations (W/A 1/1) and methods that use 1-bit weights and 32-bit activations (W/A 1/32). The table shows the Top-1 and Top-5 accuracy for each method.  The results highlight the performance of the proposed VISPA method in comparison to state-of-the-art approaches.", "section": "4 Experiments"}, {"figure_path": "cIXETwTkhK/tables/tables_8_1.jpg", "caption": "Table 4: Performance comparison by testing accuracy of optimizers on ImageNet dataset across ResNet18 architectures. W/A denotes the bit-width of weights and activations.", "description": "This table compares the performance of various methods for training binary neural networks (BNNs) on the ImageNet dataset using the ResNet18 architecture.  The methods are categorized by whether they use 1-bit weights (W) and 1-bit activations (A), or 1-bit weights and 32-bit activations.  The table shows the Top-1 and Top-5 accuracy achieved by each method.  Top-1 accuracy indicates the percentage of images correctly classified into the most likely category, while Top-5 accuracy indicates the percentage of images correctly classified into one of the five most likely categories. The results highlight the relative performance of VISPA (the authors' proposed method) against other state-of-the-art methods.", "section": "4 Experiments"}, {"figure_path": "cIXETwTkhK/tables/tables_8_2.jpg", "caption": "Table 2: Performance comparison by testing accuracy (%) of various approaches on CIFAR-10, CIFAR-100, and Tiny-ImageNet across VGG16 and ResNet18 architectures with binarized weights only. (\u2020) indicates that results are obtained from the numbers reported by Ajanthan et al. [2021]. (*) indicates that results are obtained from the numbers reported by Le et al. [2022].", "description": "This table compares the testing accuracy of various methods for training binary neural networks (BNNs) on three benchmark datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet.  Two different network architectures, VGG16 and ResNet18, are used for each dataset.  The table highlights the performance improvements achieved by the proposed VISPA method compared to several state-of-the-art BNN training techniques. Only weights are binarized in these experiments.", "section": "4 Experiments"}, {"figure_path": "cIXETwTkhK/tables/tables_9_1.jpg", "caption": "Table 1: Performance comparison by testing accuracy (%) on CIFAR-10 using VGG-Small and ResNet18 architectures with binarized activations and weights.", "description": "This table compares the testing accuracy achieved by different methods on the CIFAR-10 dataset using two different network architectures: VGG-Small and ResNet18.  Both architectures employ binarized activations and weights.  The table highlights the performance of the proposed method (VISPA) against several state-of-the-art approaches.", "section": "4 Experiments"}]