[{"figure_path": "B29BlRe26Z/figures/figures_9_1.jpg", "caption": "Figure 1: Test Accuracy vs. Local Iterations (K) for different numbers of workers (\u2191 is better).", "description": "This figure displays the test accuracy achieved by three different stochastic gradient descent (SGD) algorithms: SLowcal-SGD, Local-SGD, and Minibatch-SGD. The x-axis represents the number of local iterations (K), which is the number of local gradient updates performed by each machine before the global aggregation step.  The y-axis represents the test accuracy achieved. Three different subfigures show results for 16, 32, and 64 workers, respectively.  The upward-pointing arrow indicates that higher values are preferable in terms of accuracy. The figure shows that SLowcal-SGD generally outperforms Local-SGD and Minibatch-SGD across different numbers of workers and local iterations.", "section": "4 Experiments"}, {"figure_path": "B29BlRe26Z/figures/figures_9_2.jpg", "caption": "Figure 2: Test Loss vs. Local Iterations (K) for different numbers of workers (\u2193 is better).", "description": "This figure compares the test loss achieved by SLowcal-SGD, Local-SGD, and Minibatch-SGD across various numbers of workers (16, 32, and 64) and different numbers of local iterations (K).  Lower test loss indicates better performance. The subfigures (a) and (b) highlight the differences in performance between the algorithms. Specifically, subfigure (a) shows the performance gain of Local-SGD and SLowcal-SGD over Minibatch-SGD; (b) shows the relative performance of SLowcal-SGD compared to Local-SGD.  It demonstrates that SLowcal-SGD consistently outperforms Local-SGD with increasing local iterations, suggesting that SLowcal-SGD's approach is more effective at mitigating the bias associated with local updates.", "section": "4 Experiments"}]