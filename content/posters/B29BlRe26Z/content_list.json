[{"type": "text", "text": "SLowcal-SGD: Slow Query Points Improve Local-SGD for Stochastic Convex Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tehila Dahan Kfir Y. Levy Department of Electrical Engineering Department of Electrical Engineering Technion Technion Haifa, Israel Haifa, Israel t.dahan@campus.technion.ac.il kfirylevy@technion.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider distributed learning scenarios where $M$ machines interact with a parameter server along several communication rounds in order to minimize a joint objective function. Focusing on the heterogeneous case, where different machines may draw samples from different data-distributions, we design the first local update method that provably benefits over the two most prominent distributed baselines: namely Minibatch-SGD and Local-SGD. Key to our approach is a slow querying technique that we customize to the distributed setting, which in turn enables a better mitigation of the bias caused by local updates. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated Learning (FL) is a framework that enables huge scale collaborative learning among a large number of heterogeneous1 clients (or machines). FL may potentially promote fairness among participants, by allowing clients with small scale datasets to participate in the learning process and affect the resulting model. Additionally, participants are not required to directly share data, which may improve privacy. Due to these reasons, FL has gained popularity in the past years, and found use in applications like voice recognition [1, 4], fraud detection [2], drug discovery [3], and more [33]. ", "page_idx": 0}, {"type": "text", "text": "The two most prominent algorithmic approaches towards federated learning are Minibatch-SGD [10] and Local-SGD (a.k.a. Federated-Averaging) [26, 27, 32] . In Minibatch-SGD all machines (or clients) always compute unbiased gradient estimates of the same query points, while using large batch sizes; and it is well known that this approach is not degraded due to data heterogeneity [36]. On the downside, the number of model updates made by Minibatch-SGD may be considerably smaller compared to the number of gradient queries made by each machine; which is due to the use of minibatches. This suggests that there may be room to improve over this approach by employing local update methods like Local-SGD, where the number of model updates and the number of gradient queries are the same. And indeed, in the past years, local update methods have been extensively investigated, see e.g. [18] and references therein. ", "page_idx": 0}, {"type": "text", "text": "We can roughly divide the research on FL into two scenarios: the homogeneous case, where it is assumed that the data on each machine is drawn from the same distribution; and to the more realistic heterogeneous case where it is assumed that data distributions may vary between machines. ", "page_idx": 0}, {"type": "text", "text": "For the homogeneous case it was shown in [35, 13] that the standard Local-SGD method is not superior to Minibatch-SGD. Nevertheless, [38] have designed an accelerated variant of Local-SGD that provably benefits over the Minibatch baseline. These results are established for the fundamental Stochastic Convex Optimization (SCO) setting, which assumes that the learning objective is convex. ", "page_idx": 0}, {"type": "text", "text": "Table 1: We compare the best known guarantees for parallel learning, to our SLowcal-SGD approach for the heterogeneous SCO case. The bolded term in the Rate column is the one that compares least favourably against Minibatch SGD. Where $G$ and $G_{*}$ relate to the dissimilarity measures that are defined in Equations (1) and (3). The ${\\bf R}_{\\mathrm{min}}$ column presents the minimal number of communication rounds that are required to obtain a linear speedup (we fixed values of $K,M$ and take $\\sigma=1$ ). Note that we omit methods that do not enable a wall-clock linear speedup with $M$ , e.g. [28, 29]. ", "page_idx": 1}, {"type": "table", "img_path": "B29BlRe26Z/tmp/7db6002352d3645adb6807700f045845cb7c0446ae107a6a1dad07d31a5eef1b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Similarly to the homogeneous case, it was shown in [36, 13] that Local-SGD is not superior to Minibatch-SGD in heterogeneous scenarios. Nevertheless, several local approaches that compare with the Minibatch baseline were designed in [20, 14]. Unfortunately, we have so far been missing a local method that provably benefits over the Minibatch baseline in the heterogeneous SCO setting. ", "page_idx": 1}, {"type": "text", "text": "Our work focuses on the latter heterogeneous SCO setting, and provide a new Local-SGD-style algorithm that provably benefits over the minibatch baseline. Our algorithm named SLowcal-SGD, builds on customizing a recent technique for incorporating a slowly-changing sequence of query points [9, 21], which in turn enables to better mitigate the bias induced by the local updates. Curiously, we also found importance weighting to be crucial in order to surpass the minibatch baseline. ", "page_idx": 1}, {"type": "text", "text": "In Table 1 we compare our results to the state-of-the-art methods for the heterogeneous SCO setting. We denote $M$ to be the number of machines, $K$ is the number of local updates per round, and $R$ is the number of communications rounds. Additionally, $G$ (or $G_{*}$ ) measures the dissimilarity between machines. Our table shows that Local-SGD requires much more communication rounds compared to Minibatch-SGD, and that the dissimilarity $G$ (or $G^{*}$ ) substantially degrades its performance. Conversely, one can see that even if the dissimilarity measure is $G_{*}=\\dot{O}(1)$ , our approach SLowcalSGD still requires less communication rounds compared to Minibatch-SGD. ", "page_idx": 1}, {"type": "text", "text": "Similarly to the homogeneous case, accelerated-Minibatch-SGD [10, 24], obtains the best performance among all current methods, and it is still open to understand whether one can outperform this accelerated minibatch baseline. In App. A we elaborate on the computations of $R_{\\mathrm{min}}$ in Table 1. ", "page_idx": 1}, {"type": "text", "text": "Related Work. We focus here on centralized learning problems, where we aim to employ $M$ machines in order to minimize a joint learning objective. We allow the machines to synchronize during $R$ communication rounds through a central machine called the Parameter Server $(\\mathcal P S)$ ; and allow each machine to draw $K$ samples and perform $K$ local gradient computations in every such communication round. We assume that each machine $i$ may draw i.i.d. samples from a distribution $\\mathcal{D}_{i}$ , which may vary between machines. ", "page_idx": 1}, {"type": "text", "text": "The most natural approach in this context is Minibatch-SGD, and its accelerate variant [10], which have been widely adopted both in academy and in industry, see e.g. [16, 31, 37]. Local update methods like Local-SGD [27], have recently gained much popularity due to the rise of FL, and have been extensively explored in the past years. ", "page_idx": 1}, {"type": "text", "text": "Focusing on the SCO setting, it is well known that the standard Local-SGD is not superior (actually in most regimes it is inferior) to Minibatch-SGD [35, 36, 13]. Nevertheless, [38] devised a novel accelerated local approach that provably surpasses the Minibatch baseline in the homogeneous case. ", "page_idx": 1}, {"type": "text", "text": "The heterogeneous SCO case has also been extensively investigated, with several original and elegant approaches [23, 22, 20, 36, 29, 14, 28, 30]. Nevertheless, so far we have been missing a local approach that provably beneftis over Minibatch-SGD. Note that [29, 28] improve the communication complexity with respect to the condition number of the objective; However their performance does not improve as we increase the number of machines $M^{\\,2}$ , which is inferior to the minibatch baseline. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The heterogeneous non-convex setting was also extensively explored [20, 19, 14]; and the recent work of [30] has developed a novel algorithm that provably benefits over the minibatch baseline in this case. The latter work also provides a lower bound which demonstrates that their upper bound is almost tight. Finally, for the special case of quadratic loss functions, it was shown in [35] and in [20] that it is possible to surpass the minibatch baseline. ", "page_idx": 2}, {"type": "text", "text": "It is important to note that excluding the special case of quadratic losses, there does not exist a local update algorithm that provably beneftis over accelerated-Minibatch-SGD [10]. And the latter applies to both homogeneous and heterogeneous SCO problems. ", "page_idx": 2}, {"type": "text", "text": "Our local update algorithm utilizes a recent technique of employing slowly changing query points in SCO problems [9]. The latter has shown to be useful in designing universal accelerated methods [21, 12, 5], as well as in improving asynchronous training methods [6]. ", "page_idx": 2}, {"type": "text", "text": "2 Setting: Parallel Stochastic Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider Parallel stochastic optimization problems where the objective $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is convex and is of the following form, ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x):=\\frac{1}{M}\\sum_{i\\in[M]}f_{i}(x):=\\frac{1}{M}\\sum_{i\\in[M]}{\\bf E}_{z^{i}\\sim\\mathcal{D}_{i}}f_{i}(x;z^{i})\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thus, the objective is an average of $M$ functions $\\{f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\}_{i\\in[M]}$ , and each such $f_{i}(\\cdot)$ can be written as an expectation over losses $f_{i}(\\cdot,z^{i})$ where the $z^{i}$ are drawn from some distribution $\\mathcal{D}_{i}$ which is unknown to the learner. For ease of notation, in what follows we will not explicitly denote $\\mathbf{E}_{z^{i}\\sim\\mathcal{D}_{i}}$ but rather use $\\mathbf{E}$ to denote the expectation w.r.t. all randomization. ", "page_idx": 2}, {"type": "text", "text": "We assume that there exist $M$ machines (computation units), and that each machine may independently draw samples from the distribution $\\mathcal{D}_{i}$ , and can therefore compute unbiased gradient estimates to the gradients of $f_{i}(\\cdot)$ . Most commonly, we allow the machines to synchronize during $R$ communication rounds through a central machine called the Parameter Server $(\\mathcal P S)$ ; and allow each machine to perform $K$ local computations in every such communication round. ", "page_idx": 2}, {"type": "text", "text": "We consider first order optimization methods that iteratively employ samples and generate a sequence of query points and eventually output a solution $x_{\\mathrm{output}}$ . Our performance measure is the expected excess loss, ExcessLoss : $:=\\dot{\\mathbf{E}[f(x_{\\mathrm{output}}^{-})]}-f(w^{*})$ , where the expectation is w.r.t. the randomization of the samples, and $w^{*}$ is a global minimum of $f(\\cdot)$ in $\\mathbb{R}^{d}$ , i.e., $w^{*}\\in\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}f(x)$ . ", "page_idx": 2}, {"type": "text", "text": "More concretely, at every computation step, each machine $i\\in[M]$ may draw a fresh sample $z^{i}\\sim\\mathcal{D}_{i}$ , and compute a gradient estimate $g$ at a given point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ as follows, $g:=\\nabla f_{i}(x,z^{i})$ . and note that $\\mathbf{E}[g|\\bar{x}]=\\bar{\\nabla}f_{i}(x)$ , i.e. $g$ is an ubiased estimate of $\\nabla f_{i}(x)$ . ", "page_idx": 2}, {"type": "text", "text": "General Parallelization Scheme. A general scheme for parallel stochastic optimization is described in Alg. 1. It can be seen that the $\\mathcal{P}S$ communicates with the machines along $R$ communication rounds. In every round $r\\in[R]$ the $\\mathcal{P}S$ distributes an anchor point $\\Theta_{r}$ which is a starting point for the local computations in that round. Based on $\\Theta_{r}$ each machine performs $K$ local gradient computations based on $K$ i.i.d. draws from $\\mathcal{D}_{i}$ , and yields a message $\\Phi_{r}^{i}$ . At the end of round $r$ the $\\mathcal{P}S$ aggregates the messages from all machines and updates the anchor point $\\Theta_{r+1}$ . Finally, after the last round, the $\\mathcal{P}S$ outputs $x_{\\mathrm{output}}$ , which is computed based on the anchor points $\\{\\Theta_{r}\\}_{r=1}^{\\check{R}}$ . ", "page_idx": 2}, {"type": "text", "text": "Ideally, one would hope that using $M$ machines in parallel will enable to accelerate the learning process by a factor of $M$ . And there exists a rich line of works that have shown that this is indeed possible to some extent, depending on $K,R$ , and on the parallelization algorithm. ", "page_idx": 2}, {"type": "text", "text": "Input: $M$ machines, Parameter Server $\\mathcal{P}S$ , #Communication rounds $R$ , #Local computations $K$ ,   \ninitial point $x_{0}$   \n$\\mathcal{P}S$ Computes initial anchor point $\\Theta_{0}$ using $x_{0}$   \nfor $r=0,\\ldots,R-1$ do Distributing anchor: $\\mathcal{P}S$ Poais distributes anchor \u0398r to all M machines $\\Theta_{r}$ $\\mathbf{M}$ Local Computations: Each machine $i\\in[M]$ performs $K$ local gradient computations based on $K$ i.i.d. draws from $\\mathcal{D}_{i}$ , and yields a message $\\Phi_{r}^{i}$ Aggregation: $\\mathcal{P}S$ aggregates $\\{{\\dot{\\Phi}}_{r}^{i}\\}_{i\\in[M]}$ from all machines, and computes a new anchor $\\Theta_{r+1}$   \nend for   \noutput: $\\mathcal{P}S$ computes $x_{\\mathrm{output}}$ based on $\\{\\Theta_{r}\\}_{r=1}^{R}$ ", "page_idx": 3}, {"type": "text", "text": "(i) Minibatch SGD: In terms of Alg. 1, one can describe Minibatch-SGD as an algorithm in which the $\\mathcal{P}S$ sends a weight vector $\\boldsymbol{x}_{r}\\in\\ensuremath{\\mathbb{R}}^{d}$ in every round as the anchor point $\\Theta_{r}$ . Based on that anchor $\\Theta_{r}:=x_{r}$ , each machine $i$ computes an unbiased gradient estimate based on $K$ independent samples from $\\mathcal{D}_{i}$ , i.e. $\\begin{array}{r}{g_{r}^{i}:=\\frac{1}{K}\\sum_{k=1}^{K}\\nabla f_{i}(x_{r},z_{K r+k}^{i})}\\end{array}$ , and communicates $g_{r}^{i}$ as the message $\\Phi_{r}^{i}$ to the $\\mathcal{P}S$ . The latter aggregates the messages $\\{\\Phi_{r}^{i}:=g_{r}^{i}\\}_{i\\in[M]}$ and compute the next anchor point $x_{r+1}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{r+1}=x_{r}-\\eta\\cdot{\\frac{1}{M}}\\sum_{i\\in[M]}g_{r}^{i}\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta>0$ is the learning rate of the algorithm. The benefit in this approach is that all machines always compute gradient estimates at the same anchor points $\\{x_{r}\\}_{r}$ , which highly simplifies its analysis. On the downside, in this approach the number of gradient updates $R$ is smaller compared to the number of stochastic gradient computations made by each machine which is $K R$ . This gives the hope that there is room to improve upon Minibatch SGD, by mending this issue. ", "page_idx": 3}, {"type": "text", "text": "(ii) Local SGD: In terms of Alg. 1, one can describe Local-SGD as an algorithm in which the $\\mathcal{P}S$ sends a weight vector $\\boldsymbol{x}_{r K}\\in\\mathbb{R}^{\\check{d}}$ in every round $r\\in[R]$ as the anchor information $\\Theta_{r}$ . Based on the anchor $\\Theta_{r}:=x_{r K}$ , each machine performs a sequence of local gradient updates based on $K$ independent samples from $\\mathcal{D}_{i}$ as follows, $\\forall k\\in[K]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{r K+k+1}^{i}=x_{r K+k}^{i}-\\eta\\cdot\\nabla f_{i}(x_{r K+k}^{i},z_{r K+k}^{i})\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where for all machines $i\\in[M]$ we initialize $x_{r K}^{i}=x_{r K}:=\\Theta_{r}$ , and $\\eta>0$ is the learning rate of the algorithm. At the end of round $r$ each machine communicates $x_{(r+1)K}^{i}$ as the message $\\Phi_{r}^{i}$ to the $\\mathcal{P}S$ and the latter computes the next anchor as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta_{r+1}:=x_{(r+1)K}=\\frac{1}{M}\\sum_{i\\in[M]}x_{(r+1)K}^{i}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In local SGD the number of gradient steps is equal to the number of stochastic gradient computations made by each machine which is $K R$ . The latter suggests that such an approach may potentially surpass Minibatch SGD. Nevertheless, this potential benefti is hindered by the bias that is introduced between different machines during the local updates. And indeed, as we show in Table 1, this approach is inferior to Minibatch SGD in the prevalent case where $\\sigma=O(1)$ . ", "page_idx": 3}, {"type": "text", "text": "Assumptions. We assume that $f(\\cdot)$ is convex, and that the $f_{i}(\\cdot)$ are smooth i.e. $\\exists L>0$ such, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert\\nabla f_{i}(x)-\\nabla f_{i}(y)\\Vert\\leq L\\Vert x-y\\Vert\\;,\\ \\forall i\\in[M]\\;,\\ \\forall x,y\\in\\mathbb{R}^{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We also assume that variance of the gradient estimates is bounded, i.e. that there exists $\\sigma>0$ such, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}\\|\\nabla f_{i}(x;z)-\\nabla f_{i}(x)\\|^{2}\\leq\\sigma^{2}\\,,\\,\\forall x\\in\\mathbb{R}^{d}\\,,\\,\\forall i\\in[M]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Letting $w^{*}$ be a global minimum of $f(\\cdot)$ , we assume there exist $G_{*}\\geq0$ such that, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{i\\in[M]}\\|\\nabla f_{i}(w^{*})\\|^{2}\\leq G_{*}^{2}/2\\;,\\;\\;(G_{*}\\mathrm{-}\\mathbf{Dissimilarity})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above assumption together with the smoothness and convexity imply (see App. B) , ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{1}{M}}\\sum_{i\\in[M]}\\|\\nabla f_{i}(x)\\|^{2}\\leq G_{*}^{2}+4L{\\bigl(}f(x)-f(w^{*}){\\bigr)}\\,,\\quad\\forall x\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A stronger dissimilarity assumption that is often used in the literature is the following, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{i\\in[M]}\\|\\nabla f_{i}(x)-\\nabla f(x)\\|^{2}\\leq G^{2}/2\\;,\\;\\;\\forall x\\in\\mathbb{R}^{d}\\;\\;(G{\\mathrm{-}}\\mathbf{D}\\mathbf{i}\\mathrm{ssimilarity})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notation: For $\\{y_{t}\\}_{t}$ we denote $\\begin{array}{r}{y_{t_{1}:t_{2}}:=\\sum_{\\tau=t_{1}}^{t_{2}}y_{\\tau}}\\end{array}$ . For $N\\in\\mathbb{Z}^{+}$ we denote $[N]:=\\{0,\\dots,N-1\\}$ . ", "page_idx": 4}, {"type": "text", "text": "3 Our Approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Section 3.1 describes a basic (single machine) algorithmic template called Anytime-GD. Section 3.2 describes our SLowcal-SGD algorithm, which is a Local-SGD style algorithm in the spirit of Anytime GD. We describe our method in Alg. 2, and state its guarantees in Thm. 2. ", "page_idx": 4}, {"type": "text", "text": "3.1 Anytime GD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The standard GD algorithm computes a sequence of iterates $\\{w_{t}\\}_{t\\in[T]}$ and queries the gradients at theses iterates. It was recently shown that one can design a GD-style scheme that computes a sequence of iterates $\\{w_{t}\\}_{t\\in[T]}$ yet queries the gradients at a different sequence $\\{x_{t}\\}_{t\\in[T]}$ which may be slowly-changing, in the sense that $\\|x_{t+1}-x_{t}\\|$ may be considerably smaller than $\\bar{\\|\\boldsymbol{w}_{t+1}-\\boldsymbol{w}_{t}\\|}$ . ", "page_idx": 4}, {"type": "text", "text": "Concretely, the Anytime-GD algorithm [9, 21] that we describe in Equations (4) and (5), employs a learning rate $\\eta>0$ and a sequence of non-negative weights $\\left\\{\\alpha_{t}\\right\\}_{t}$ . The algorithm maintains two sequences $\\{w_{t}\\}_{t},\\{x_{t}\\}_{t}$ that are updated as follows $\\forall t$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-\\eta\\alpha_{t}g_{t}\\;,\\forall t\\in[T]\\;,\\mathrm{where~}g_{t}=\\nabla f(x_{t})\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and then, ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t+1}=\\frac{\\alpha_{0:t}}{\\alpha_{0:t+1}}x_{t}+\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}}w_{t+1}\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It can be shown that the above implies that $\\begin{array}{r}{x_{t+1}=\\frac{1}{\\alpha_{0:t+1}}\\sum_{\\tau=0}^{t+1}\\alpha_{\\tau}w_{\\tau}}\\end{array}$ 1 t\u03c4+=10 \u03b1\u03c4w\u03c4, i.e. the xt\u2019s are weighted averages of the $w_{t}$ \u2019s. Thus, at every iterate the gradient $g_{t}$ is queried at $x_{t}$ which is a weighted average of past iterates, and then $w_{t+1}$ is updated similarly to GD with a weight $\\alpha_{t}$ on the gradient $g_{t}$ . Moreover, at initialization $x_{0}=w_{0}$ . ", "page_idx": 4}, {"type": "text", "text": "Curiously, it was shown in [9] that Anytime-GD obtains the same convergence rates as GD for convex loss functions (both smooth and non-smooth). It was further shown and that one can employ a stochastic version (Anytime-SGD) where we query noisy gradients at $x_{t}$ instead of the exact ones, and that approach performs similarly to SGD. ", "page_idx": 4}, {"type": "text", "text": "Slowly changing query points. A recent work [6], demonstrates that if we use projected AnytimeSGD, i.e. project the $w_{t}$ sequence to a given bounded convex domain; then one can immediately show that for both $\\alpha_{t}=1$ and $\\alpha_{t}=t+1$ we obtain $\\|x_{t+1}-x_{t}\\|\\leq2D/t$ , where $D$ is the diameter of the convex domain. Conversely, for standard SGD we have $\\|w_{t+1}-w_{t}\\|\\leq\\eta\\|g_{t}\\|$ , where $g_{t}$ here is a (possibly noisy) unbiased estimate of $\\nabla f(w_{t})$ .\u221a Thus, while the change between consecutive SGD queries is controlled by $\\eta$ which is usually $\\propto1/\\sqrt{t}$ , and by magnitude of stochastic gradients; for Anytime-SGD the change decays with time, irrespective of the learning rate $\\eta$ . In [6], this is used to design better and more robust asynchronous training methods. ", "page_idx": 4}, {"type": "text", "text": "Relation to Momentum. In the appendix we show that Anytime-SGD can be explicitly written as a momentum method, and therefore is quite different from standard SGD. Concretely, for $\\alpha_{t}=1$ we show that $\\begin{array}{r}{x_{t+1}\\approx x_{t}{-\\eta\\sum_{\\tau=1}^{t}(\\tau/t^{2})\\cdot}g_{\\tau}}\\end{array}$ , and for $\\alpha_{t}\\propto t$ we show that $\\begin{array}{r}{\\underline{{x}}_{t+1}\\approx x_{t}\\!-\\!\\eta\\sum_{\\tau=1}^{t}(\\tau/t)^{3}\\!\\cdot\\!g_{\\tau}}\\end{array}$ Where $g_{\\tau}$ here is a (pos sibly noisy) unbiased estimate of $\\nabla f(x_{\\tau})$ . This momentu m interpretation provides a complementary intuition regarding the benefit of Anytime-SGD in the context of local update methods. Momentum brings more stability to the optimization process which in turn reduces the bias between different machines. ", "page_idx": 4}, {"type": "text", "text": "For the sake of this paper we will require a specific theorem that does not necessarily regard AnytimeGD, but is rather more general. We will require the following definition, ", "page_idx": 4}, {"type": "text", "text": "Input: $M$ machines, Parameter Server $\\mathcal{P}S$ , #Communication rounds $R$ , #Local computations $K$ , initial point $x_{0}$ , learning rate $\\eta>0$ , weights $\\left\\{\\alpha_{t}\\right\\}_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Initialize: set $w_{0}=x_{0}$ , initialize anchor point $\\Theta_{0}:=\\left(w_{0},x_{0}\\right)$ , and set $t=0$   \nfor $r=0,\\ldots,R-1$ do Distributing anchor: $\\mathcal{P}S$ distributes anchor $\\Theta_{r}:=\\,\\left(w_{t},x_{t}\\right)$ to all machines, each machine $i\\in[M]$ initializes $(w_{t}^{i},x_{t}^{i})=\\Theta_{r}:=\\left(w_{t},x_{t}\\right)$ for $k=0,\\ldots,K-1$ do Set $t=r K+k$ Every machine $i\\in[M]$ draws a fresh sample $z_{t}^{i}\\sim\\mathcal{D}_{i}$ , and computes $g_{t}^{i}=\\nabla f_{i}(x_{t}^{i},z_{t}^{i})$ Update $w_{t+1}^{i}=w_{t}^{i}-\\dot{\\eta}\\alpha_{t}g_{t}^{i}$ , and $\\begin{array}{r}{x_{t+1}^{i}=(\\bar{1}-\\frac{\\bar{\\alpha}_{t+1}}{\\alpha_{0:t+1}})x_{t}^{i}+\\frac{\\alpha_{t+\\bar{1}}}{\\alpha_{0:t+1}}w_{t+1}^{i}}\\end{array}$ end for ", "page_idx": 5}, {"type": "text", "text": "Aggregation: $\\mathcal{P}S$ aggregates $\\{(w_{t+1}^{i},x_{t+1}^{i})\\}_{i\\in[M]}$ from all machines, and computes a new anchor $\\begin{array}{r}{\\Theta_{r+1}:=\\left(w_{t+1},x_{t+1}\\right)=\\left(\\frac{1}{M}\\sum_{i\\in[M]}w_{t+1}^{i},\\frac{1}{M}\\sum_{i\\in[M]}x_{t+1}^{i}\\right)}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "output: $\\mathcal{P}S$ outputs $x_{T}$ (recall $T=K R$ ) ", "page_idx": 5}, {"type": "text", "text": "Definition Let $\\{\\alpha_{t}~\\geq~0\\}_{t}$ be a sequence of non-negative weights, and let $\\{w_{t}~\\in~\\mathbb{R}^{d}\\}_{t}$ , be an arbitrary sequence. We say that a sequence $\\{x_{t}\\in\\mathbb{R}^{d}\\}_{t}$ is an $\\left\\{\\alpha_{t}\\right\\}_{t}$ weighted average of $\\{w_{t}\\}_{t}\\;i f$ $x_{0}=w_{0}$ , and for any $t>0$ Eq. (5) is satisfied. ", "page_idx": 5}, {"type": "text", "text": "Next, we state the main theorem for this section, which applies for any sequence $\\{w_{t}\\in\\mathbb{R}^{d}\\}_{t}$ , ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Rephrased from Theorem 1 in [9]). Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ be a convex function with a global minimum $w^{*}$ . Also let $\\{\\alpha_{t}\\geq0\\}_{t}$ , and $\\{w_{t}\\in\\mathbb{R}^{d}\\}_{t},\\{\\dot{x}_{t}\\in\\mathbb{R}^{d}\\}_{t}$ such that $\\{x_{t}\\}_{t}$ is an $\\left\\{\\alpha_{t}\\right\\}_{t}$ weighted average of $\\{w_{t}\\}_{t}$ . Then the following holds for any $t\\geq0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n0\\leq\\alpha_{0:t}\\left(f(x_{t})-f(w^{*})\\right)\\leq\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot\\left(w_{\\tau}-w^{*}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.2 SLowcal-SGD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our approach is to employ an Anytime version of Local-SGD, which we name by SLowcal-SGD. ", "page_idx": 5}, {"type": "text", "text": "Notation: Prior to describing our algorithm we will define $t$ to be the total of per-machine local updates up to step $k$ of round $r$ , resulting $t:=r K+k$ . In what follows, we will often find it useful to denote the iterates and samples using $t$ , rather than explicitly denoting $t=r K+k$ . Additionally we use $\\left\\{\\alpha_{t}\\right\\}_{t}$ to denote a pre-defined sequence of non-negative weights. Finally, we denote $T:=R K$ . ", "page_idx": 5}, {"type": "text", "text": "In the spirit of Anytime-SGD our approach is to maintain two sequences per machine $i\\,\\in\\,[M]$ : $\\{w_{t}^{i}\\in\\mathbb{R}^{d}\\}_{t}$ and $\\{x_{t}^{i}\\in\\mathbb{R}^{d}\\}_{t}$ . Our approach is depicted explicitly in Alg. 2. Next we describe our algorithm in terms of the scheme depicted in Alg. 1: ", "page_idx": 5}, {"type": "text", "text": "(i) Distributing anchor. At the beginning of round $r$ the $\\mathcal{P}S$ distributes $\\Theta_{r}~=~(w_{t},x_{t})~=~$ $(w_{r K},x_{r K})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ to all machines. ", "page_idx": 5}, {"type": "text", "text": "(ii) Local Computations. For $t=r K$ , every machine initializes $(w_{t}^{i},x_{t}^{i})=\\Theta_{r}$ , and for the next $K$ rounds, i.e. for any $r K\\,\\le\\,t\\,\\le\\,(r+1)K\\,-\\,1$ , every machine performs a sequence of local Anytime-SGD steps as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\nw_{t+1}^{i}=w_{t}^{i}-\\eta\\alpha_{t}g_{t}^{i}\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where similarly to Anytime-SGD we query the gradients at the averages $\\boldsymbol x_{t}^{i}$ , meaning $g_{t}^{i}\\ =$ $\\nabla f_{i}(x_{t}^{i},z_{t}^{i})$ . And query points are updated as weighted averages of past iterates $\\dot{\\{w_{t}\\}}_{t}$ , , ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{t+1}^{i}=\\big(1-\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}}\\big)x_{t}^{i}+\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}}w_{t+1}^{i}\\;,\\qquad\\forall\\,r K\\leq t\\leq(r+1)K-1\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "At the end round $r$ , i.e. $t=(r+1)K$ , each machine communicates $(w_{t}^{i},x_{t}^{i})$ as a message to the $\\mathcal{P}S$ . (iii) Aggregation. The $\\mathcal{P}S$ aggregates the messages and computes the next anchor point $\\Theta_{r+1}=$ $\\begin{array}{r}{(w_{t},x_{t})=\\frac{1}{M}\\sum_{i\\in[M]}\\Phi_{r}^{i}:=\\left(\\frac{1}{M}\\sum_{i\\in[M]}w_{t}^{i},\\frac{1}{M}\\sum_{i\\in[M]}x_{t}^{i}\\right)}\\end{array}$ , where $t=(r+1)K$ . ", "page_idx": 5}, {"type": "text", "text": "Remark: Note that for $t=r K$ our notation for $(w_{t}^{i},x_{t}^{i})$ is inconsistent: at the end of round $r-1$ these values may vary between different machines, while at the beginning of round $r$ these values are all equal to $\\Theta_{r}:=\\left(w_{t},x_{t}\\right)$ . Nevertheless, for simplicity we will abuse notation, and explicitly state the right definition when needed. Importantly, in most of our analysis we will mainly need to refer to the averages $\\begin{array}{r}{\\left(\\frac{1}{M}\\sum_{i\\in[M]}w_{t}^{i},\\frac{1}{M}\\sum_{i\\in[M]}x_{t}^{i}\\right)}\\end{array}$ , and note the latter are consistent at the end and beginning of consecutive rounds due to the definition of $\\Theta_{r}$ , and $\\Phi_{r-1}^{i}$ . ", "page_idx": 6}, {"type": "text", "text": "3.2.1 Guarantees & Intuition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Below we state our main result for SLowcal-SGD (Alg. 2), ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $f(\\cdot)$ be a convex and $L$ -smooth function. Then under the assumption that we make in Sec. 2, invoking Alg. 2 with weights $\\{\\alpha_{t}=t+1\\}_{t\\in[T]}$ , and an appropriate learning rate $\\eta$ ensures, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{E}\\Delta_{T}\\,\\le\\,{\\cal O}\\left(L B_{0}^{2}\\left(\\frac{1}{K R}+\\frac{1}{R^{2}}+\\frac{1}{K^{1/3}R^{4/3}}\\right)+\\frac{\\sigma B_{0}}{\\sqrt{M K R}}+\\frac{L^{1/2}(\\sigma^{1/2}+G_{*}^{1/2})\\cdot B_{0}^{3/2}}{K^{1/4}R}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Delta_{T}:=f(x_{T})-f(x^{*}),\\,B_{0}:=\\|w_{0}-w^{*}\\|$ , and we choose the learning rate as follows, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}\\left\\{\\frac{1}{48L(T+1)},\\frac{1}{10L K^{2}},\\frac{1}{40L K(T+1)^{2/3}},\\frac{\\|w_{0}-w^{*}\\|\\sqrt{M}}{\\sigma T^{3/2}},\\frac{\\|w_{0}-w^{*}\\|^{1/2}}{L^{1/2}K^{7/4}R(\\sigma^{1/2}+G_{*}^{1/2})}\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As Table 1 shows, Thm. 2 implies that SLowcal-SGD improves over all existing upper bounds for Minibatch and Local SGD, by allowing less communication rounds to obtain a linear speedup of $M$ . ", "page_idx": 6}, {"type": "text", "text": "Intuition. The degradation in local SGD schemes (both standard and Anytime) is due to the bias that it introduces between different machines during each round, which leads to a bias in their gradients. Intuitively, this bias is small if the machines query the gradients at a sequence of slowly changing query points. This is exactly the benefit of SLowcal-SGD which queries the gradients at averaged iterates $\\boldsymbol x_{t}^{i}$ \u2019s. Intuitively these averages are slowly changing compared to the iterates themselves $\\bar{w}_{t}^{i}$ ; and recall that the latter are the query points used by standard Local-SGD. A complementary intuition to the benefit of our approach, is the interpretation of Anytime-SGD as a momentum method (see Sec. 3.1 and the appendix) which leads to decreased bias between machines. ", "page_idx": 6}, {"type": "text", "text": "To further simplify the more technical discussion here, we will assume the homogeneous case, i.e., that for any $i\\in[M]$ we have $\\mathcal{D}_{i}=\\mathcal{D}$ and $f_{i}(\\cdot)=f(\\cdot)$ . ", "page_idx": 6}, {"type": "text", "text": "So a bit more formally, let us discuss the bias between query points in a given round $r\\in[R]$ , and let us denote $t_{0}=r K$ . The following holds for standard Local SGD, ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{t}^{i}=w_{t_{0}}-\\eta\\sum_{\\tau=t_{0}}^{t-1}g_{\\tau}^{i}\\;,\\;\\;\\forall i\\in[M],t\\in[t_{0},t_{0}+K]\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $g_{\\tau}^{i}$ is the noisy gradients that Machine $i$ computes in $w_{\\tau}^{i}$ , and we can write $g_{\\tau}^{i}:=\\nabla f(w_{\\tau}^{i})+\\xi_{\\tau}^{i}$ , where $\\xi_{\\tau}^{\\i}$ is the noisy component of the gradient. Thus, for two machines $i\\neq j$ we can write, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{I}\\Vert w_{t}^{i}-w_{t}^{j}\\Vert^{2}=\\eta^{2}\\mathbb{E}\\left\\Vert\\sum_{\\tau=t_{0}}^{t-1}g_{\\tau}^{i}-g_{\\tau}^{j}\\right\\Vert^{2}\\approx\\eta^{2}\\mathbb{E}\\left\\Vert\\sum_{\\tau=t_{0}}^{t-1}\\nabla f(w_{\\tau}^{i})-\\nabla f(w_{\\tau}^{j})\\right\\Vert^{2}+\\eta^{2}\\mathbb{E}\\left\\Vert\\sum_{\\tau=t_{0}}^{t-1}\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j}\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "And it was shown in [35], that the noisy term is dominant and therefore we can bound, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta^{2}}\\mathbf{E}\\|w_{t}^{i}-w_{t}^{j}\\|^{2}\\lesssim\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j}\\right\\|^{2}\\approx t-t_{0}\\leq K\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similarly, for SLowcal-SGD we would like to bound $\\mathbf{E}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}$ for two machines $i\\neq j$ ; and in order to simplify the discussion we will assume uniform weights i.e., $\\alpha_{t}=1\\:,\\:\\forall t\\in[T]$ . Now the update rule for the iterates $w_{t}^{i}$ , is of the same form as in Eq. (9), only now $g_{\\tau}^{i}:=\\nabla f(x_{\\tau}^{i})+\\xi_{\\tau}^{i}$ , where $\\xi_{\\tau}^{i}$ is the noisy component of the gradient. Consequently, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{\\tau=t_{0}}^{t}(w_{\\tau}^{i}-w_{\\tau}^{j})\\approx-\\eta\\sum_{\\tau=t_{0}}^{t-1}(t-\\tau)(g_{\\tau}^{i}-g_{\\tau}^{j})\\approx-\\eta K\\sum_{\\tau=t_{0}}^{t-1}(g_{\\tau}^{i}-g_{\\tau}^{j})\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we took a crude approximation of $t\\,-\\,\\tau\\,\\approx\\,K$ . Now, by definition of $\\boldsymbol x_{t}^{i}$ and $\\alpha_{t}~=~1$ , $\\begin{array}{r}{x_{t}^{i}=\\frac{t_{0}}{t}\\cdot x_{t_{0}}+\\frac{1}{t}\\sum_{\\tau=t_{0}}^{t}w_{\\tau}^{i}}\\end{array}$ , $\\forall i\\in[M],t\\in[t_{0},t_{0}+K]$ . Thus, for two machines $i\\neq j$ we have, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\eta^{2}}\\mathbf{E}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}=\\frac{1}{\\eta^{2}}\\mathbf{E}\\left\\|\\frac{1}{t}\\sum_{\\tau=t_{0}}^{t}w_{\\tau}^{i}-w_{\\tau}^{j}\\right\\|^{2}\\approx\\frac{1}{\\eta^{2}}\\cdot\\frac{\\eta^{2}K^{2}}{t^{2}}\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}g_{\\tau}^{i}-g_{\\tau}^{j}\\right\\|^{2}}\\\\ {\\approx\\frac{K^{2}}{t^{2}}\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\nabla f(x_{\\tau}^{i})-\\nabla f(x_{\\tau}^{j})\\right\\|^{2}+\\frac{K^{2}}{t^{2}}\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j}\\right\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As we show in our analysis, the noisy term is dominant, so we can therefore bound, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta^{2}}\\mathbf{E}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}\\lesssim\\frac{K^{2}}{t^{2}}\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j}\\right\\|^{2}\\approx\\frac{K^{2}(t-t_{0})}{t^{2}}\\leq\\frac{K^{3}}{t^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Taking $t\\approx T=R K$ above yields a bound of $O(K/R^{2})$ . Thus Equations (10), (11), illustrate that the bias of SLowcal-SGD is smaller by a factor of $R^{2}$ compared to the bias of standard Local-SGD. In the appendix we demonstrate the same benefti of Anytime-SGD over SGD when both use $\\alpha_{t}\\propto t$ . ", "page_idx": 7}, {"type": "text", "text": "Finally, note that the biases introduced by the local updates come into play in a slightly different manner in Local-SGD compared to SLowcal-SGD 3. Consequently, the above discussion does not enable to demonstrate the exact rates that we derive. Nevertheless, it provides some intuition regarding the benefit of our approach. The full and exact derivations appear in the appendix. ", "page_idx": 7}, {"type": "text", "text": "Importance Weights. One may wonder whether it is necessary to employ increasing weights $\\alpha_{t}=t+1$ , rather than employing standard uniform weights $\\alpha_{t}=1\\,,\\forall t$ . Surprisingly, in our analysis we have found that increasing weights are crucial in order to obtain a benefit over Minibatch-SGD, and that upon using uniform weights SLowcal-SGD performs worse compared to Minibatch SGD! We elaborate on this in Appendix L. Below we provide an intuitive explanation. ", "page_idx": 7}, {"type": "text", "text": "Intuitive Explanation. The intuition behind the importance of using increasing weights is the following: Increasing weights are a technical tool to put more emphasis on the last rounds. Now, in the context of Local update methods, the iterates of the last rounds are more attractive since the bias between different machines shrinks as we progress. Intuitively, this happens since as we progress with the optimization process, the expected value of the gradients that we compute goes to zero (since we converge); and consequently the bias between different machines shrinks as we progress. ", "page_idx": 7}, {"type": "text", "text": "3.3 Proof Sketch for Theorem 2 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Proof Sketch for Theorem 2. As a starting point for the analysis, for every iteration $t\\in[T]$ we will define the averages of $(w_{t}^{i},x_{t}^{i},g_{t}^{i})$ across all machines as follows, ", "page_idx": 7}, {"type": "equation", "text": "$$\nw_{t}:=\\frac{1}{M}\\sum_{i\\in[M]}w_{t}^{i}\\;,\\quad\\&\\quad x_{t}:=\\frac{1}{M}\\sum_{i\\in[M]}x_{t}^{i}\\quad\\&\\quad g_{t}:=\\frac{1}{M}\\sum_{i\\in[M]}g_{t}^{i}\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that Alg. 2 explicitly computes $(w_{t},x_{t})$ only once every $K$ local updates, and that theses are identical to the local copies of every machine at the beginning of every round. Combining the above definitions with Eq. (6) yields, ", "page_idx": 7}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-\\eta\\alpha_{t}g_{t}\\mathrm{~,~}\\forall t\\in[T]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Further combining these definitions with Eq. (7) yields, ", "page_idx": 7}, {"type": "equation", "text": "$$\nx_{t+1}=(1-\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}})x_{t}+\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}}w_{t+1}\\;,\\;\\forall t\\in[T]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The above implies that the $\\{x_{t}\\}_{t\\in[T]}$ sequence is an $\\{\\alpha_{t}\\}_{t\\in[T]}$ weighted average of $\\{w_{t}\\}_{t\\in[T]}$ . This enables to employ Thm. 1 which yields, $\\begin{array}{r}{\\alpha_{0:t}\\Delta_{t}\\leq\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(w_{\\tau}-w^{*})}\\end{array}$ , where we denote $\\Delta_{t}:=f(x_{t})\\stackrel{}{-}\\dot{f}(w^{*})$ . This bound highlights the challenge in the analysis: our algorithm does not directly compute unbiased estimates of $x_{t}$ , except for the first iterate of each round. Concretely, Eq. (12) implies that our algorithm effectively updates using $g_{t}$ which is a biased estimate of $\\nabla f(x_{t})$ . ", "page_idx": 7}, {"type": "text", "text": "It is therefore natural to decompose $\\nabla f(x_{\\tau})=g_{\\tau}+(\\nabla f(x_{\\tau})-g_{\\tau})$ in the above bound, yielding, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\Delta_{t}\\leq\\underbrace{\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\cdot(w_{\\tau}-w^{*})}_{(\\mathrm{A})}+\\underbrace{\\sum_{\\tau=0}^{t}\\alpha_{\\tau}(\\nabla f(x_{\\tau})-g_{\\tau})\\cdot(w_{\\tau}-w^{*})}_{(\\mathrm{B})}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Thus, we intend to bound the weighted error $\\alpha_{0:t}\\Delta_{t}$ by bounding two terms: (A) which is related to the update rule of the algorithm, and (B) which accounts for the bias between $g_{t}$ and $\\nabla f(x_{t})$ . ", "page_idx": 8}, {"type": "text", "text": "Notation: In what follows we will find the following notation useful, $\\begin{array}{r}{\\bar{g}_{t}:=\\frac{1}{M}\\sum_{i\\in[M]}\\bar{\\nabla}\\dot{f}_{i}(\\dot{x_{t}^{i}}),}\\end{array}$ , and note that $\\bar{g}_{t}=\\mathbf{E}\\left[g_{t}\\big|\\{x_{t}^{i}\\}_{i\\in[M]}\\right]$ . We will also employ the following notations: $\\begin{array}{r}{V_{t}:=\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|\\bar{g}_{\\tau}-}\\end{array}$ $\\nabla f(x_{\\tau})\\|^{2}$ , and $D_{t}:=\\|w_{t}-w^{*}\\|^{2}$ , where $w^{*}$ is a global minimum of $f(\\cdot)$ . We will also denote $\\begin{array}{r}{D_{0:t}:=\\sum_{\\tau=0}^{t}\\|w_{\\tau}-w^{*}\\|^{2}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Bounding (A): Due to the update rule of Eq. (12), one can show by standard regret analysis that: $\\begin{array}{r}{\\mathrm{\\stackrel{\\r{A}}{A}}\\mathrm{:}=\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\leq\\frac{\\|w_{0}-w^{*}\\|^{2}}{2\\eta}+\\frac{\\eta}{2}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}}\\end{array}$ , ", "page_idx": 8}, {"type": "text", "text": "Bounding (B): We can bound (B) in expectation using $V_{t}$ and $D_{0:t}$ as follows for any $\\rho\\,>\\,0$ : $\\begin{array}{r}{\\mathbf{E}\\left[\\left(\\mathrm{B}\\right)\\right]\\le\\frac{1}{2\\rho}\\mathbf{E}V_{t}+\\frac{\\rho}{2}\\mathbf{E}D_{0:t}}\\end{array}$ , ", "page_idx": 8}, {"type": "text", "text": "Combining (A) and (B): Combining the above boounds on (A) and (B) into Eq. (14) we obtain the following bound which holds for any $\\rho>0$ and $t\\in[T]$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\mathbf{E}\\Delta_{t}\\leq\\frac{\\|w_{0}-w^{*}\\|^{2}}{2\\eta}+\\frac{\\eta}{2}\\mathbf{E}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}+\\frac{1}{2\\rho}\\mathbf{E}V_{T}+\\frac{\\rho}{2}\\mathbf{E}D_{0:T}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Now, to simplify the proof sketch we shall assume that $D_{t}\\leq D_{0}\\,\\forall t$ , implying that $D_{0:T}\\leq T D_{0}$ . Plugging this into the above equation and taking $\\begin{array}{r}{\\rho=\\frac{1}{4\\eta T}}\\end{array}$ 4\u03b7T gives, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\mathbf{E}\\Delta_{t}\\leq\\frac{\\Vert w_{0}-w^{*}\\Vert^{2}}{\\eta}+\\eta\\cdot\\underbrace{\\mathbf{E}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\Vert g_{\\tau}\\Vert^{2}}_{(*)}+4\\eta T\\mathbf{E}V_{T}\\ .\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Next we will bound $(*)$ and $\\mathbf{E}V_{T}$ , and plug them back into Eq. (16). ", "page_idx": 8}, {"type": "text", "text": "Bounding $(*)$ : To bound $(*)$ it is natural to decompose $g_{\\tau}=(g_{\\tau}-\\bar{g}_{\\tau})+(\\bar{g}_{\\tau}-\\nabla f(x_{\\tau}))\\!+\\!\\nabla f(x_{\\tau}).$ Using this decomposition we show that, $\\begin{array}{r l r}{(*)}&{\\lesssim}&{3\\frac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}+3{\\bf E}V_{T}+12L{\\bf E}\\sum_{t=0}^{T}\\alpha_{0:t}\\Delta_{t}\\;.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Bounding $\\mathbf{E}V_{T}$ The definition of $V_{t}$ shows that it is encompasses the bias that is introduced due to the local updates, which in turn relates to the distances $\\lVert x_{t}^{i}-x_{t}^{j}\\rVert\\mathrm{~,~}\\forall i,j\\in[M]$ . Thus, $\\mathbf{E}V_{T}$ is therefore directly related to the dissimilarity between the machines. Our analysis shows the following: $\\begin{array}{r}{{\\bf E}V_{T}\\leq400L^{2}\\dot{\\eta^{2}}K^{3}\\sum_{\\tau=0}^{T}\\alpha_{0:\\tau}\\langle G_{*}^{2}+4\\dot{L}\\Delta_{\\tau}\\rangle+90L^{2}\\eta^{2}K^{6}R^{3}\\sigma^{2}}\\end{array}$ . Plugging the above into Eq. (16), and using our choice for $\\eta$ , gives an almost explicit bound, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\nu_{0:t}\\mathbf{E}\\Delta_{t}\\lesssim\\frac{\\Vert w_{0}-w^{*}\\Vert^{2}}{\\eta}+\\eta\\frac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}+L^{2}\\eta^{3}T K^{6}R^{3}\\sigma^{2}+L^{2}\\eta^{3}T K^{3}\\sum_{\\tau=0}^{T}\\alpha_{0:\\tau}G_{*}^{2}+\\frac{1}{2(T+1)}\\mathbf{E}\\sum_{t=0}^{T}\\alpha_{t}^{2}G_{*}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The theorem follows by plugging above the choices of $\\eta,\\alpha_{t}$ , and using a technical lemma. ", "page_idx": 8}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the effectiveness of our proposed approach, we conducted experiments on the MNIST [25] dataset\u2014a widely used benchmark in image classification comprising 70,000 grayscale images of handwritten digits (0\u20139), divided into 60,000 training images and 10,000 test images. The dataset was accessed through torchvision (version 0.16.2). We implemented a logistic regression model [7] using the PyTorch framework and performed all computations on an NVIDIA RTX 3090 GPU. To ensure the reliability of our results, we averaged the outcomes over three different random seeds. The code used for these experiments is available in our GitHub repository.4 ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We compared our approach, utilizing parameters suggested by our theoretical framework $(\\alpha_{t}=t)$ ), against Local-SGD and Minibatch-SGD across various configurations. Specifically, we experimented with 16, 32, and 64 workers and varied the number of local updates $K$ (or minibatch sizes for Minibatch-SGD) among 4, 8, 16, 32, and 64. In SLowcal-SGD and Local-SGD, each local update was computed using a single sample. We optimized the learning rate through grid search, setting it to 0.01. All experiments involved a single pass over the MNIST dataset. ", "page_idx": 9}, {"type": "image", "img_path": "B29BlRe26Z/tmp/3f1a8b52fbe4876b17cae346b66f2b528fe6abf85654086d25cc58c06a0ec4ce.jpg", "img_caption": ["Figure 1: Test Accuracy vs. Local Iterations $(K)$ for different numbers of workers ( $\\uparrow$ is better). "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "B29BlRe26Z/tmp/806ede6d4329e285b0832fd171da8ef17f28ed2f25de40660612466e55062c42.jpg", "img_caption": ["Figure 2: Test Loss vs. Local Iterations $\\langle K\\rangle$ for different numbers of workers $\\downarrow$ is better). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Our results on the MNIST dataset, illustrated in Figures 1 and 2, highlight the effectiveness of using local steps over equivalently sized batches in this setting. As shown in Figures 1a and 2a, both SLowcal-SGD and Local-SGD demonstrate substantial improvements in test accuracy and loss over Minibatch-SGD, where these become more significant as the number of local steps grows. ", "page_idx": 9}, {"type": "text", "text": "At a closer look, Figures 1b and 2b indicate that with an increasing number of local iterations, the performance gap between SLowcal-SGD and Local-SGD generally becomes more pronounced. This gap suggests that SLowcal-SGD\u2019s approach, which maintains closer alignment of weights both within local updates and across workers, enhances convergence, resulting in superior performance over Local-SGD. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented the first local approach for the heterogeneous distributed Stochastic Convex Optimization (SCO) setting that provably benefits over the two most prominent baselines, namely Minibatch-SGD, and Local-SGD. There are several interesting avenues for future exploration: ", "page_idx": 9}, {"type": "text", "text": "(a) developing an adaptive variant that does not require the knowledge of the problem parameters like $\\sigma$ and $L$ ; (b) Allowing a per dimension step-size that could benefti in (the prevalent) scenarios where the scale of the gradients considerably changes between different dimensions; in the spirit of the well known AdaGrad method [11]. Finally, (c) it will be interesting to understand whether we can find an algorithm that provably dominates over the Accelerated Minibach-SGD baseline, which is an open question also in the homogeneous SCO setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially supported by Israel PBC-VATAT, the Technion Artificial Intelligent Hub (Tech.AI), and the Israel Science Foundation (grant No. 3109/24). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Apple. designing for privacy (video and slide deck). apple wwdc, 2019.   \n[2] Intel and consilient. intel and consilient join forces to fight financial fraud with ai, 2020.   \n[3] Melloddy. melloddy project meets its year one objective: Deployment of the world\u2019s first secure platform for multi-task federated learning in drug discovery among 10 pharmaceutical companies, 2020.   \n[4] Google. your voice and audio data stays private while google assistant improves, 2021.   \n[5] Kimon Antonakopoulos, Dong Quan Vu, Volkan Cevher, Kfir Levy, and Panayotis Mertikopoulos. Undergrad: A universal black-box optimization method with almost dimension-free convergence rate guarantees. In International Conference on Machine Learning, pages 772\u2013795. PMLR, 2022.   \n[6] Rotem Zamir Aviv, Ido Hakimi, Assaf Schuster, and Kfir Yehuda Levy. Asynchronous distributed learning: Adapting to gradient delays without prior knowledge. In International Conference on Machine Learning, pages 436\u2013445. PMLR, 2021.   \n[7] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, volume 4. Springer, 2006.   \n[8] Ashok Cutkosky. Lecture notes for ec525: Optimization for machine learning.   \n[9] Ashok Cutkosky. Anytime online-to-batch, optimism and acceleration. In International conference on machine learning, pages 1446\u20131454. PMLR, 2019.   \n[10] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13(1), 2012.   \n[11] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.   \n[12] Alina Ene, Huy L Nguyen, and Adrian Vladu. Adaptive gradient methods for constrained convex optimization and variational inequalities. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 7314\u20137321, 2021.   \n[13] Margalit R Glasgow, Honglin Yuan, and Tengyu Ma. Sharp bounds for federated averaging (local sgd) and continuous perspective. In International Conference on Artificial Intelligence and Statistics, pages 9050\u20139090. PMLR, 2022.   \n[14] Eduard Gorbunov, Filip Hanzely, and Peter Richt\u00e1rik. Local sgd: Unified theory and new efficient methods. In International Conference on Artificial Intelligence and Statistics, pages 3556\u20133564. PMLR, 2021.   \n[15] Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization, 2(3-4):157\u2013325, 2016.   \n[16] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Advances in neural information processing systems, 30, 2017.   \n[17] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. Advances in neural information processing systems, 26, 2013.   \n[18] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[19] Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning. arXiv preprint arXiv:2008.03606, 2020.   \n[20] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132\u20135143. PMLR, 2020.   \n[21] Ali Kavis, Kfir Y Levy, Francis Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algorithm with optimal guarantees for constrained optimization. Advances in neural information processing systems, 32, 2019.   \n[22] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt\u00e1rik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 4519\u20134529. PMLR, 2020.   \n[23] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pages 5381\u20135393. PMLR, 2020.   \n[24] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2):365\u2013397, 2012.   \n[25] Yann LeCun, Corinna Cortes, Chris Burges, et al. Mnist handwritten digit database, 2010. URL http://yann.lecun.com/exdb/mnist/. Licensed under CC BY-SA 3.0, available at https://creativecommons.org/licenses/by-sa/3.0/.   \n[26] Olvi L Mangasarian and Mikhail V Solodov. Backpropagation convergence via deterministic nonmonotone perturbed minimization. Advances in Neural Information Processing Systems, 6, 1993.   \n[27] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[28] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richt\u00e1rik. Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally! In International Conference on Machine Learning, pages 15750\u201315769. PMLR, 2022.   \n[29] Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients. Advances in Neural Information Processing Systems, 34:14606\u201314619, 2021.   \n[30] Kumar Kshitij Patel, Lingxiao Wang, Blake Woodworth, Brian Bullins, and Nathan Srebro. Towards optimal communication complexity in distributed non-convex optimization. In Advances in Neural Information Processing Systems.   \n[31] Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don\u2019t decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.   \n[32] Sebastian U. Stich. Local SGD converges fast and communicates little. In International Conference on Learning Representations, 2019.   \n[33] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Ag\u00fcera y Arcas, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, Suhas N. Diggavi, Hubert Eichner, Advait Gadhikar, Zachary Garrett, Antonious M. Girgis, Filip Hanzely, Andrew Hard, Chaoyang He, Samuel Horv\u00e1th, Zhouyuan Huo, Alex Ingerman, Martin Jaggi, Tara Javidi, Peter Kairouz, Satyen Kale, Sai Praneeth Karimireddy, Jakub Konec\u02c7n\u00fd, ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Sanmi Koyejo, Tian Li, Luyang Liu, Mehryar Mohri, Hang Qi, Sashank J. Reddi, Peter Richt\u00e1rik, Karan Singhal, Virginia Smith, Mahdi Soltanolkotabi, Weikang Song, Ananda Theertha Suresh, ", "page_idx": 12}, {"type": "text", "text": "Sebastian U. Stich, Ameet Talwalkar, Hongyi Wang, Blake E. Woodworth, Shanshan Wu, Felix X. Yu, Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong Zhang, Chunxiang Zheng, Chen Zhu, and Wennan Zhu. A field guide to federated optimization. CoRR, abs/2107.06917, 2021.   \n[34] Jun-Kun Wang, Jacob Abernethy, and Kfir Y Levy. No-regret dynamics in the fenchel game: A unified framework for algorithmic convex optimization. arXiv e-prints, pages arXiv\u20132111, 2021.   \n[35] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International Conference on Machine Learning, pages 10334\u201310343. PMLR, 2020.   \n[36] Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local sgd for heterogeneous distributed learning. Advances in Neural Information Processing Systems, 33:6281\u20136292, 2020.   \n[37] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.   \n[38] Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. Advances in Neural Information Processing Systems, 33:5332\u20135344, 2020. ", "page_idx": 12}, {"type": "text", "text": "A Explanations Regarding the Linear Speedup and Table 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we elaborate on the computations done in Table 1. First we will explain why the dominance of the term $\\frac{1}{\\sqrt{M K R}}$ implies a linear speedup by a factor of $M$ . ", "page_idx": 13}, {"type": "text", "text": "Explanation. Recall that using SGD with a single machine $M=1$ , yields a convergnece rate of $\\frac{1}{\\sqrt{K R}}$ (as a dominant term). Thus, in order to obtain an excess loss smaller than some $\\varepsilon>0$ , SGD requires $\\begin{array}{r}{R K\\ge\\Omega\\left(\\frac{1}{\\varepsilon^{2}}\\right)}\\end{array}$ . Where $R K$ is the wall-clock time required to compute the solution. ", "page_idx": 13}, {"type": "text", "text": "Now, when we use parallel optimization with $R$ communication rounds, $K$ local computations, and $M$ machines, the wall-clock time to compute a solution is still $R K$ . Now, if the dominant term in the convergence rate of this algorithm is\u221aM1KR then the wall clock time to obtain an $\\varepsilon$ -optimal solution should be $\\begin{array}{r}{R K\\geq\\Omega\\left(\\frac{1}{M\\varepsilon^{2}}\\right)}\\end{array}$ . And the latter is smaller by a factor of $M$ compared to a single machine. ", "page_idx": 13}, {"type": "text", "text": "Computation of $R_{\\mathrm{min}}$ in Table 1. The term \u221aM1KR appears in the bounds of all of the parallel optimization methods that we describe. Nevertheless, it is dominant up as long as the number of communication rounds $R$ is larger than some treshold value $R_{\\mathrm{min}}$ , that depends on the specific convergence rate. Clearly, smaller values of $R_{\\mathrm{min}}$ imply less communication. Thus, in the ${\\bf R}_{\\mathrm{min}}$ column of the table we compute $R_{\\mathrm{min}}$ for each method based on the term in the bound that compares least favourably against $\\frac{\\mathbf{\\bar{1}}}{\\sqrt{M K R}}$ . These terms are bolded in the Rate column of the table. ", "page_idx": 13}, {"type": "text", "text": "Concretely, denoting this less favourable term by Bparallel : $:=\\mathcal{B}^{\\mathrm{parallel}}(M,K,R,G_{*})$ 5, then ${\\bf R}_{\\mathrm{min}}$ is the lowest $R$ which satisfies, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\beta^{\\mathrm{parallel}}\\leq\\frac{1}{\\sqrt{M K R}}\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B On Heterogeneity Assumption ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let us assume that the following holds at the optimum $w^{*}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac1M\\sum_{i\\in[M]}\\|\\nabla f_{i}(w^{*})\\|^{2}\\leq G_{*}^{2}/2\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we can show the following relation for any $w\\in\\mathbb{R}^{d}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{M}\\sum_{i\\in[M]}\\|\\nabla f_{i}(w)\\|^{2}=\\displaystyle\\frac{1}{M}\\sum_{i\\in[M]}\\|\\nabla f_{i}(w)-\\nabla f_{i}(w^{*})+\\nabla f_{i}(w^{*})\\|^{2}}\\\\ {\\displaystyle\\leq\\frac{2}{M}\\sum_{i\\in[M]}\\|\\nabla f_{i}(w)-\\nabla f_{i}(w^{*})\\|^{2}+\\frac{2}{M}\\sum_{i\\in[M]}\\|\\nabla f_{i}(w^{*})\\|^{2}}\\\\ {\\displaystyle\\leq4L(f(w)-f(w^{*}))+G_{*}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we used $\\|a+b\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2}$ which holds for any $a,b\\in\\mathbb{R}^{d}$ , and the last line follows by the lemma below that we borrow from [17, 8]. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. Let $\\begin{array}{r}{\\mathcal{L}(x)=\\frac{1}{M}\\sum_{i\\in[M]}\\ell_{i}(x)}\\end{array}$ be a convex function with global minimum $w^{*}$ , and assume that every $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is $L$ -smooth. Then the following holds, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{i\\in[M]}\\|\\nabla\\ell_{i}(w)-\\nabla\\ell_{i}(w^{*})\\|^{2}\\leq2L(\\mathcal{L}(w)-\\mathcal{L}(w^{*}))\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . The lemma follows immediately from lemma 27.1 in [8], by taking $v\\,=\\,w^{*}$ therein. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "C Interpreting Anytime-SGD as Momentum ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we show how to interpret the Anytime-SGD algorithm that we present in Equations (4),(5), as a momentum method. For completeness we rewrite the update equations below, ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-\\eta\\alpha_{t}g_{t}\\;,\\forall t\\in[T]\\;,\\mathrm{where~}g_{t}=\\nabla f(x_{t})\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and then, ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t+1}=\\frac{\\alpha_{0:t}}{\\alpha_{0:t+1}}x_{t}+\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}}w_{t+1}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $g_{t}$ is an unbiased gradient estimate at $x_{t}$ , and $\\left\\{\\alpha_{t}\\right\\}_{t}$ is a sequence of non-negative scalars. And at initialization $x_{0}=w_{0}$ . ", "page_idx": 14}, {"type": "text", "text": "First note that Eq. (18) directly implies that, ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t+1}=\\frac{1}{\\alpha_{0:t+1}}\\sum_{\\tau=0}^{t+1}\\alpha_{\\tau}w_{\\tau}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, note that we can directly write, ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{\\tau}=w_{0}-\\eta\\sum_{n=0}^{\\tau-1}\\alpha_{n}g_{n}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging the above into the formula for $x_{t+1}$ yields, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{t+1}=w_{0}-\\eta\\frac{1}{\\alpha_{0:t+1}}\\sum_{\\tau=0}^{t+1}\\sum_{n=0}^{\\tau-1}\\alpha_{\\tau}\\alpha_{n}g_{n}}}\\\\ {~~}\\\\ {{\\displaystyle=w_{0}-\\eta\\frac{1}{\\alpha_{0:t+1}}\\sum_{n=0}^{t}\\sum_{\\tau=n+1}^{t+1}\\alpha_{\\tau}\\alpha_{n}g_{n}}}\\\\ {~~}\\\\ {{\\displaystyle=w_{0}-\\eta\\frac{1}{\\alpha_{0:t+1}}\\sum_{n=0}^{t}\\alpha_{n+1:t+1}\\alpha_{n}g_{n}~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{\\eta}\\left(x_{t+1}-x_{t}\\right)=\\frac{1}{\\alpha_{0\\ell t+1}}\\sum_{n=0}^{t-1}\\alpha_{n+1:t}\\alpha_{n}g_{n}-\\frac{1}{\\alpha_{0\\ell t+1}}\\sum_{n=0}^{t}\\alpha_{n+1:t+1}\\alpha_{n}g_{n}}&{}\\\\ {=\\frac{1}{\\alpha_{0\\ell t+1}}\\sum_{n=0}^{t-1}\\frac{\\alpha_{0\\ell+1}}{\\alpha_{0\\ell t}}\\alpha_{n+1:t}\\alpha_{n}g_{n}-\\frac{1}{\\alpha_{0\\ell t+1}}\\sum_{n=0}^{t-1}\\alpha_{n+1:t+1}\\alpha_{n}g_{n}-\\frac{1}{\\alpha_{0\\ell t+1}}\\alpha_{t+1:t}\\alpha_{t}g_{t}}\\\\ {=-\\frac{1}{\\alpha_{0\\ell t+1}}\\sum_{n=0}^{t-1}\\left(\\alpha_{n+1:t+1}-\\frac{\\alpha_{0\\ell t+1}}{\\alpha_{0\\ell t}}\\alpha_{n+1:t}\\right)\\alpha_{n}g_{n}-\\frac{1}{\\alpha_{0\\ell t+1}}\\alpha_{t+1:\\alpha_{1\\ell}}}\\\\ {=-\\frac{1}{\\alpha_{0\\ell t+1}}\\sum_{n=0}^{t-1}\\alpha_{t+1}\\frac{\\alpha_{0\\ell t}}{\\alpha_{0\\ell t}}\\alpha_{n}g_{n}-\\frac{1}{\\alpha_{0\\ell t+1}}\\alpha_{t+1:t}\\alpha_{t}g_{t}}&{}\\\\ {=-\\frac{1}{\\alpha_{0\\ell t+1}}\\sum_{n=0}^{t}\\alpha_{t+1:t}\\alpha_{n}\\frac{\\alpha_{0\\ell t}}{\\alpha_{0\\ell t}}g_{n}\\,,}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(19)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used the equality below, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha_{n+1:t+1}-\\frac{\\alpha_{0:t+1}}{\\alpha_{0:t}}\\alpha_{n+1:t}=\\alpha_{t+1}-\\alpha_{n+1:t}\\frac{\\alpha_{t+1}}{\\alpha_{0:t}}=\\alpha_{t+1}\\big(1-\\frac{\\alpha_{n+1:t}}{\\alpha_{0:t}}\\big)=\\alpha_{t+1}\\frac{\\alpha_{0:n}}{\\alpha_{0:t}}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we can write, ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t+1}\\approx x_{t}-\\eta\\frac{1}{\\alpha_{0:t+1}}\\sum_{n=0}^{t}\\alpha_{t+1}\\alpha_{n}\\frac{\\alpha_{0:n}}{\\alpha_{0:t}}g_{n}\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Uniform Weights. Thus, taking uniform weights $\\alpha_{t}=1$ yields, ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{t+1}\\approx x_{t}-\\eta\\sum_{n=0}^{t}{\\frac{n}{t^{2}}}g_{n}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Linear Weights. Similarly, taking linear weights $\\alpha_{t}=t+1$ yields, ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{t+1}\\approx x_{t}-\\eta\\sum_{n=0}^{t}{\\frac{n^{3}}{t^{3}}}g_{n}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 1. We rehearse the proof of Theorem 1 from [9]. First, since $w^{*}$ is a global minimum and $\\alpha_{0:t}$ are non-negative than clearly, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\left(f(x_{t})-f(w^{*})\\right)\\geq0\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, notice that the following holds, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{t}(x_{t}-w_{t})=\\alpha_{0:t-1}(x_{t-1}-x_{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the gradient inequality for $f$ gives, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{\\tau}(f(x_{\\tau})-f(w^{*}))\\leq\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(x_{\\tau}-w^{*})}&{}\\\\ &{\\displaystyle=\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(w_{\\tau}-w^{*})+\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(x_{\\tau}-w_{\\tau})}\\\\ &{\\displaystyle=\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(w_{\\tau}-w^{*})+\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{0:\\tau-1}\\nabla f(x_{\\tau})\\cdot(x_{\\tau-1}-x_{\\tau})}\\\\ &{\\displaystyle\\leq\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(w_{\\tau}-w^{*})+\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{0:\\tau-1}(f(x_{\\tau-1})-f(x_{\\tau}))\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we have used the gradient inequality again which implies $\\nabla f(x_{\\tau})\\cdot(x_{\\tau-1}-x_{\\tau})\\leq f(x_{\\tau-1})-$ $f(x_{\\tau})$ . ", "page_idx": 15}, {"type": "text", "text": "Now Re-ordering we obtain, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t}\\left(\\alpha_{0:\\tau}f(x_{\\tau})-\\alpha_{0:\\tau-1}f(x_{\\tau-1})\\right)-\\alpha_{0:t}f(w^{*})\\leq\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot\\left(w_{\\tau}-w^{*}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Telescoping the sum in the LHS we conclude the proof, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\left(f(x_{\\tau})-f(w^{*})\\right)\\leq\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot\\left(w_{\\tau}-w^{*}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "E More Intuition and Discussion Regarding the Benefit of SLowcal-SGD ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "More Elaborate Intuitive Explanation. The intuition is the following: We have two extreme baselines: (1) Minibatch-SGD where queries do not change at all during updates-implying that there is no bias between different machines. However, Minibatch-SGD is \u201clazy\u201d since among $K R$ queries it only performs $R$ gradient updates. Conversely (2) Local-SGD is not \u201clazy\u201d since each machine performs $K R$ gradient updates. Nevertheless, the queries of different machines change substantially during each round, which translates to bias between machines, which in turn degrades the convergence. ", "page_idx": 15}, {"type": "text", "text": "Ideally, we would like to have a \u201cnon-lazy\u201d method where each machine performs KR gradient updates (like Local-SGD), but where the queries of each machine do not change at all during rounds (like Minibatch-SGD) and therefore no bias is introduced between machines. Of course, this is too good to exist, but our method is a step in this direction: it is \u201cnon-lazy\u201d and the query points of different machines change slowly, and therefore introduce less bias between machines. This translates to a better convergence rate. ", "page_idx": 16}, {"type": "text", "text": "Additional Technical Intuition for $\\alpha_{t}\\propto t$ . Here we extend the technical explanation that we provide in Sec. 3.2.1 to the case where $\\alpha_{t}\\propto t$ , and show again that SLowcal-SGD yields smaller bias between different machines compared to Local-SGD. ", "page_idx": 16}, {"type": "text", "text": "As in the intuition for the case of uniform weights, to simplify the more technical discussion, we will assume the homogeneous case, i.e., that for any $i\\in[M]$ we have $\\mathcal{D}_{i}=\\mathcal{D}$ and $f_{i}(\\cdot)=f(\\cdot)$ . ", "page_idx": 16}, {"type": "text", "text": "Note that upon employing linear weights, the normalization factor $\\alpha_{0:T}$ that plays a major role in the convergence guarantees of Anytime-SGD (see Thm. 1) also grows as $\\dot{\\alpha_{0:T}}\\propto T^{2}$ . Thus, in order to make an proper comparison, we should compare the bias of weighted Anytime-SGD, to the appropriate weighted version of SGD; where the normalization factor $\\alpha_{0:T}$ also plays a similar role in the guarantees (see e.g. [34]). This weighted SGD is as follows [34], $\\forall t\\ge0$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-\\eta\\alpha_{t}g_{t}\\;;\\quad\\mathrm{where~}g_{t}\\;\\mathrm{is~unbiased~of~}\\nabla f\\big(w_{t}\\big)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and after $t$ iterations it outputs $\\begin{array}{r}{\\overline{{w}}_{T}=\\frac{1}{\\alpha_{0:T}}\\sum_{t=0}^{T}\\alpha_{t}w_{t}}\\end{array}$ . And for $\\alpha_{t}=t+1$ this version enjoys the same guarantees as standard SGD. ", "page_idx": 16}, {"type": "text", "text": "Next, we compare the Local-SGD version of the above weighted SGD (Eq. (20)) to our SLowcalSGDwhen both employ $\\alpha_{t}=t+1$ . So a bit more formally, let us discuss the bias between query points in a given round $r\\in[R]$ , and let us denote $t_{0}=r K$ . The following holds for weighted Local SGD, ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{t}^{i}=w_{t_{0}}-\\eta\\sum_{\\tau=t_{0}}^{t-1}\\alpha_{\\tau}g_{\\tau}^{i}\\;,\\;\\,\\forall i\\in[M],t\\in[t_{0},t_{0}+K]\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $g_{\\tau}^{i}$ is the noisy gradients that Machine $i$ computes in $w_{\\tau}^{i}$ , and we can write $g_{\\tau}^{i}:=\\nabla f(w_{\\tau}^{i})+\\xi_{\\tau}^{i}$ , where $\\xi_{\\tau}^{\\i}$ is the noisy component of the gradient. Thus, for two machines $i\\neq j$ we can write, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Im\\lVert w_{t}^{i}-w_{t}^{j}\\rVert^{2}=\\eta^{2}\\mathbf{E}\\left\\lVert\\sum_{\\tau=t_{0}}^{t-1}\\alpha_{\\tau}(g_{\\tau}^{i}-g_{\\tau}^{j})\\right\\rVert^{2}\\approx\\eta^{2}\\mathbf{E}\\left\\lVert\\sum_{\\tau=t_{0}}^{t-1}\\alpha_{\\tau}(\\nabla f(w_{\\tau}^{i})-\\nabla f(w_{\\tau}^{j}))\\right\\rVert^{2}+\\eta^{2}\\mathbf{E}\\left\\lVert\\sum_{\\tau=t_{0}}^{t-1}\\alpha_{\\tau}(\\nabla f(w_{\\tau}^{i})-\\nabla f(w_{\\tau}^{j}))\\right\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, we can be generous with respect to weighted SGD and only take the second noisy term into account and neglect the first term 6. Thus we obtain, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta^{2}}\\mathbf{E}\\|w_{t}^{i}-w_{t}^{j}\\|^{2}\\lesssim\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\alpha_{\\tau}(\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j})\\right\\|^{2}\\approx\\alpha_{t_{0}+K}^{2}\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}(\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j})\\right\\|^{2}\\approx(r K)^{2}\\cdot(t-t_{0})\\le r^{2}K^{\\frac{3}{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used $\\alpha_{t}\\le\\alpha_{t_{0}+K}$ , $\\forall t\\leq t_{0}+K$ , as well as $\\alpha_{t_{0}+K}^{2}=(r(K+1)+1)^{2}\\approx(r K)^{2}$ . We also used $t-t_{0}\\lessapprox K$ . ", "page_idx": 16}, {"type": "text", "text": "Similarly, for SLowcal-SGD we would like to bound $\\mathbf{E}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}$ for two machines $i\\neq j$ ; while assuming linear weights i.e., $\\alpha_{t}=t+1$ , $\\forall t\\in[T]$ . Now the update rule for the iterates $w_{t}^{i}$ , is of the same form as in Eq. (21), only now $g_{\\tau}^{i}:=\\nabla f(x_{\\tau}^{\\bar{i}})+\\xi_{\\tau}^{i}$ , where $\\xi_{\\tau}^{i}$ is the noisy component of the gradient. Consequently, we can show the following, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{=t_{0}}^{t}\\alpha_{\\tau}(w_{\\tau}^{i}-w_{\\tau}^{j})\\approx-\\eta\\sum_{\\tau=t_{0}}^{t}\\alpha_{\\tau}\\sum_{n=t_{0}}^{\\tau-1}\\alpha_{n}(g_{n}^{i}-g_{n}^{j})\\approx-\\eta\\sum_{n=t_{0}}^{t-1}\\alpha_{n+1:t}\\alpha_{n}(g_{n}^{i}-g_{n}^{j})\\approx-\\eta r^{2}K^{3}\\sum_{\\tau=t_{0}}^{t-1}(\\frac{\\alpha_{n+1}}{2})^{\\tau}\\alpha_{n-1}(g_{n}^{i}-g_{n}^{j})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we took a crude approximation of $\\alpha_{n+1:t}\\alpha_{n}\\approx\\alpha_{t_{0}:t+K}\\alpha_{t_{0}}\\lesssim r K^{2}\\cdot r K=r^{2}K^{3}$ . In the last $\"{\\approx}\"$ we also change the notation of summation variable from $n$ to $\\tau$ . ", "page_idx": 16}, {"type": "text", "text": "Now, by defin $\\begin{array}{r}{x_{t}^{i}\\approx\\frac{\\alpha_{0:t_{0}}}{\\alpha_{0:t}}\\cdot x_{t_{0}}+\\frac{1}{\\alpha_{0:t}}\\sum_{\\tau=t_{0}}^{t}\\alpha_{\\tau}w_{\\tau}^{i}}\\end{array}$ , $\\forall i\\in[M],t\\in[t_{0},t_{0}+K]$ . Thus, for $i\\neq j$ we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\eta^{2}}\\mathbb{E}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}=\\frac{1}{\\eta^{2}}\\mathbb{E}\\left\\|\\frac{1}{\\alpha_{0:t}}\\sum_{\\tau=t_{0}}^{t}\\alpha_{\\tau}(w_{\\tau}^{i}-w_{\\tau}^{j})\\right\\|^{2}\\approx\\frac{1}{\\eta^{2}}\\cdot\\frac{\\eta^{2}r^{4}K^{6}}{(\\alpha_{0:t})^{2}}\\mathbb{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}g_{\\tau}^{i}-g_{\\tau}^{j}\\right\\|^{2}}\\\\ {\\displaystyle\\approx\\frac{1}{\\eta^{2}}\\cdot\\frac{\\eta^{2}r^{4}K^{6}}{r^{4}K^{4}}\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}g_{\\tau}^{i}-g_{\\tau}^{j}\\right\\|^{2}}\\\\ {\\displaystyle\\approx K^{2}\\mathbb{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\nabla f(x_{\\tau}^{i})-\\nabla f(x_{\\tau}^{j})\\right\\|^{2}+K^{2}\\mathbb{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j}\\right\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have used $\\alpha_{0:t}\\approx\\alpha_{0:t_{0}}\\propto t_{0}^{2}\\approx r^{2}K^{2}$ . ", "page_idx": 17}, {"type": "text", "text": "As we show in our analysis, the noisy term is dominant, so we can therefore bound, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta^{2}}\\mathbf{E}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}\\lesssim K^{2}\\mathbf{E}\\left\\|\\sum_{\\tau=t_{0}}^{t-1}\\xi_{\\tau}^{i}-\\xi_{\\tau}^{j}\\right\\|^{2}\\approx K^{2}(t-t_{0})\\leq K^{3}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus Equations (22), (23), illustrate that for $\\alpha_{t}\\propto t$ , then the bias of SLowcal-SGD is smaller by a factor of $r^{2}$ compared to the bias of weighted Local-SGD.   \nSince $r^{2}$ can be as big as $R^{2}$ this coincides with the benefit of SLowcal-SGD over standard SGD in the case where $\\alpha_{t}=1$ , which we demonstrate in the main text. ", "page_idx": 17}, {"type": "text", "text": "Finally, note that upon dividing by the normalization factor $\\alpha_{0:T}$ ,we have, that for SLowcal-SGD with either $\\alpha_{t}=1$ or $\\alpha_{t}\\propto t$ then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha_{0:T}}\\frac{1}{\\eta}\\mathbf{E}\\|x_{t}^{i}-x_{t}^{j}\\|\\approx\\frac{1}{R^{2}K^{2}}\\cdot\\sqrt{K^{3}}\\approx\\frac{1}{R K}\\cdot\\sqrt{\\frac{K}{R^{2}}}=\\frac{1}{\\sqrt{K}R^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Comparably, upon dividing by the normalization factor $\\alpha_{0:T}$ ,we have, that for Local-SGD with either $\\alpha_{t}=1$ or $\\alpha_{t}\\propto t$ that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha_{0:T}}\\frac{1}{\\eta}\\mathbf{E}\\|w_{t}^{i}-w_{t}^{j}\\|\\approx\\frac{1}{R^{2}K^{2}}\\cdot\\sqrt{R^{2}K^{3}}\\approx\\frac{1}{R K}\\cdot\\sqrt{K}=\\frac{1}{\\sqrt{K}R}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, with respect to the approximate and intuitive analysis that we make here SLowcal-SGD maintains similar benefit over Local-SGD for both $\\alpha_{t}=1$ and $\\alpha_{t}=t+1$ . ", "page_idx": 17}, {"type": "text", "text": "As we explain in Appendix L, taking $\\alpha_{t}=1$ in SLowcal-SGD does not actually enable to provide a benefit over Local-SGD. The reason is that for $\\alpha_{t}=1$ , the condition for which the dominant term in the bias (between different machines) is the noisy term (this enables the approximate analysis that we make here and in the body of the paper), leads to limitation on the learning rate which in turn degrades the performance for SLowcal-SGD with $\\alpha_{t}=1$ . Conversely, for $\\alpha_{t}\\propto t$ there is no such degradation due to the limitation of the learning rate. For more details and intuition please see Appendix $\\mathrm{L}$ . ", "page_idx": 17}, {"type": "text", "text": "F Proof of Thm. 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Thm. 2. As a starting point for the analysis, for every iteration $t\\in[T]$ we will define the averages of $(w_{t}^{i},x_{t}^{i},g_{t}^{i})$ across all machines as follows, ", "page_idx": 17}, {"type": "equation", "text": "$$\nw_{t}:=\\frac{1}{M}\\sum_{i\\in[M]}w_{t}^{i}\\;,\\quad\\&\\quad x_{t}:=\\frac{1}{M}\\sum_{i\\in[M]}x_{t}^{i}\\quad\\&\\quad g_{t}:=\\frac{1}{M}\\sum_{i\\in[M]}g_{t}^{i}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that Alg. 2 explicitly computes $(w_{t},x_{t})$ only once every $K$ local updates, and that theses are identical to the local copies of every machine at the beginning of every round. Combining the above definitions with Eq. (6) yields, ", "page_idx": 17}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-\\eta\\alpha_{t}g_{t}\\mathrm{~,~}\\forall t\\in[T]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Further combining these definitions with Eq. (7) yields, ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{t+1}=(1-\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}})x_{t}+\\frac{\\alpha_{t+1}}{\\alpha_{0:t+1}}w_{t+1}\\;,\\;\\forall t\\in[T]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And the above implies that the $\\{x_{t}\\}_{t\\in[T]}$ sequence is an $\\{\\alpha_{t}\\}_{t\\in[T]}$ weighted average of $\\{w_{t}\\}_{t\\in[T]}$ . This enables to employ Thm. 1 which yields, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\Delta_{t}:=\\alpha_{0:t}(f(x_{t})-f(w^{*}))\\leq\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(w_{\\tau}-w^{*})\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we denote $\\Delta_{t}:=f(x_{t})-f(w^{*})$ . The above bound highlights the challenge in the analysis: our algorithm does not directly computes unbiased estimates of $x_{t}$ , except for the first iterate of each round. Concretely, Eq. (26) demonstrates that our algorithm effectively updates using $g_{t}$ which might be a biased estimate of $\\nabla f(x_{t})$ . ", "page_idx": 18}, {"type": "text", "text": "It is therefore natural to decompose $\\nabla f(x_{\\tau})=g_{\\tau}+(\\nabla f(x_{\\tau})-g_{\\tau})$ in the above bound, leading to, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\Delta_{t}\\leq\\underbrace{\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\cdot\\big(w_{\\tau}-w^{*}\\big)}_{(\\mathrm{A})}+\\underbrace{\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\big(\\nabla f(x_{\\tau})-g_{\\tau}\\big)\\cdot\\big(w_{\\tau}-w^{*}\\big)}_{(\\mathrm{B})}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we intend to bound the weighted error $\\alpha_{0:t}\\Delta_{t}$ by bounding two terms: (A) which is directly related to the update rule of the algorithm (Eq. (26)), and (B) which accounts for the bias between $g_{t}$ and $\\nabla f(x_{t})$ . ", "page_idx": 18}, {"type": "text", "text": "Notation: In what follows we will find the following notation useful, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{g}_{t}:=\\frac1{M}\\sum_{i\\in[M]}\\nabla f_{i}(x_{t}^{i})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the above definition implies that $\\bar{g}_{t}={\\bf E}\\left[g_{t}|\\{z_{0}^{i}\\}_{i\\in[M]},\\dots,\\{z_{t-1}^{i}\\}_{i\\in[M]}\\right]={\\bf E}\\left[g_{t}|\\{x_{t}^{i}\\}_{i\\in[M]}\\right].$ . We will also employ the following notations, ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{t}:=\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|\\bar{g}_{\\tau}-\\nabla f(x_{\\tau})\\|^{2}\\;,\\quad\\&\\quad D_{t}:=\\|w_{t}-w^{\\ast}\\|^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $w^{*}$ is a global minimum of $f(\\cdot)$ . Moreover, we will also use the notation $\\begin{array}{r}{D_{0:t}:=\\sum_{\\tau=0}^{t}\\|w_{\\tau}-}\\end{array}$ $w^{\\ast}\\Vert^{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Bounding (A): Due to the update rule of Eq. (26), one can show by standard regret analysis (see Lemma 2 below) that, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{A}):=\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\cdot\\big(w_{\\tau}-w^{*}\\big)\\leq\\frac{\\|w_{0}-w^{*}\\|^{2}}{2\\eta}+\\frac{\\eta}{2}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma 2. (OGD Regret Lemma -See e.g. [15]) Let $w_{0}\\in\\mathbb{R}^{d}$ and $\\eta>0$ . Also assume a sequence of $T$ non-negative weights $\\{\\alpha_{t}\\geq0\\}_{t\\in[T]}$ and $T$ vectors $\\{g_{t}\\in\\mathbb{R}^{d}\\}_{t\\in[T]}$ , and assume an update rule of the following form: ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-\\eta\\alpha_{t}g_{t}\\;,\\forall t\\in[T]\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then the following bound holds for any $u\\in\\mathbb{R}^{d}$ , and $t\\in[T],$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\cdot\\left(w_{\\tau}-u\\right)\\leq\\frac{\\|w_{0}-u\\|^{2}}{2\\eta}+\\frac{\\eta}{2}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for completeness we provide a proof in Appendix G. ", "page_idx": 18}, {"type": "text", "text": "Bounding (B): Since our goal is to bound the expected excess loss, we will bound the expected value of (B), thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{E}\\left[(\\mathbf{B})\\right]=\\mathbf{E}\\left[\\sum_{\\tau=0}^{t}\\alpha_{\\tau}(\\nabla f(x_{\\tau})-g_{\\tau})\\cdot(w_{\\tau}-w^{*})\\right]}\\quad}&{}\\\\ &{=\\mathbf{E}\\left[\\sum_{\\tau=0}^{t}\\alpha_{\\tau}(\\nabla f(x_{\\tau})-\\bar{g}_{\\tau})\\cdot(w_{\\tau}-w^{*})\\right]}\\\\ &{\\leq\\mathbf{E}\\displaystyle\\sum_{\\tau=0}^{t}\\left(\\frac{1}{2\\rho}\\alpha_{\\tau}^{2}\\|\\nabla f(x_{\\tau})-\\bar{g}_{\\tau}\\|^{2}+\\frac{\\rho}{2}\\mathbf{E}\\|w_{\\tau}-w^{*}\\|^{2}\\right)}\\\\ &{=\\frac{1}{2\\rho}\\mathbf{E}V_{t}+\\frac{\\rho}{2}\\mathbf{E}D_{0:t}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second line follows by the definition of $\\bar{g}_{\\tau}$ (see Eq. (29)) and due to the fact that $w_{\\tau}$ is measurable with respect to $\\left\\{\\big\\{z_{0}^{i}\\big\\}_{i\\in[M]},\\ldots,\\big\\{z_{\\tau-1}^{i^{-}}\\big\\}_{i\\in[M]}\\right\\}$ while $\\begin{array}{r l}{\\bar{g}_{\\tau}}&{{}=}\\end{array}$ ${\\bf E}\\left[g_{\\tau}\\big|\\big\\{z_{0}^{i}\\big\\}i\\{{\\sf E}[M],\\cdot\\cdot\\cdot,\\big\\{z_{\\tau-1}^{i}\\big\\}i{\\in}[M]\\right]$ implying that ${\\bf E}[g_{\\tau}\\mathrm{~\\boldmath~\\cdot~}(w_{\\tau}\\mathrm{~-~}w^{*})]\\;=\\;{\\bf E}[\\bar{g}_{\\tau}\\mathrm{~\\boldmath~\\cdot~}(w_{\\tau}\\mathrm{~-~}w^{*})]$ ; the third line uses Young\u2019s inequality $\\begin{array}{r}{a\\cdot b\\leq\\operatorname*{inf}_{\\rho>0}\\{\\frac{\\rho}{2}\\|a\\|^{2}+\\frac{1}{2\\rho}\\|b\\|^{2}\\}}\\end{array}$ which holds for any $a,b\\in\\mathbb{R}^{d}$ ; and the last two lines use the definition of $V_{t}$ and $D_{0:T}$ . ", "page_idx": 19}, {"type": "text", "text": "Combining (A) and (B): Combining Equations (30) and (31) into Eq. (28) we obtain the following bound which holds for any $\\rho>0$ and $t\\in[T]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\mathbf{E}\\Delta_{t}\\leq\\frac{\\|w_{0}-w^{*}\\|^{2}}{2\\eta}+\\frac{\\eta}{2}\\mathbf{E}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}+\\frac{1}{2\\rho}\\mathbf{E}V_{T}+\\frac{\\rho}{2}\\mathbf{E}D_{0:t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have used $V_{t}\\ \\leq\\ V_{T}$ which holds for any $t~\\in~[T]$ , as well as $\\begin{array}{r l r}{\\lefteqn{\\mathbf{E}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}\\ \\leq}}\\end{array}$ $\\mathbf{E}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\lVert g_{\\tau}\\rVert^{2}$ , which holds since $t\\leq T$ . ", "page_idx": 19}, {"type": "text", "text": "Next, we shall bound each of the above terms. The following lemma bounds $\\mathbf{E}D_{0:t}$ , ", "page_idx": 19}, {"type": "text", "text": "Lemma 3. The following bound holds for any $t\\in[T]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}D_{0:t}=\\mathbf{E}\\sum_{\\tau=0}^{t}\\Vert w_{\\tau}-w^{*}\\Vert^{2}\\leq2T\\Vert w_{0}-w^{*}\\Vert^{2}+2T\\eta^{2}\\mathbf{E}\\sum_{t=0}^{T}\\alpha_{t}^{2}\\Vert g_{t}\\Vert^{2}+16\\eta^{2}T^{2}\\cdot\\mathbf{E}V_{T}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above lemma into Eq. (33) gives, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{0:t}\\mathbf{E}\\Delta_{t}\\leq\\frac{\\|w_{0}-w^{*}\\|^{2}}{2\\eta}+\\left(\\frac{\\eta}{2}+\\rho T\\eta^{2}\\right)\\mathbf{E}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}+\\left(\\frac{1}{2\\rho}+8\\rho\\eta^{2}T^{2}\\right)\\mathbf{E}V_{T}+\\rho T\\|w_{0}-w^{*}\\|^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the above holds for any $\\rho>0$ let us pick a specific value of $\\begin{array}{r}{\\rho=\\frac{1}{4\\eta T}}\\end{array}$ ; by doing so we obtain, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{0:t}\\mathbf{E}\\Delta_{t}\\leq\\frac{\\|w_{0}-w^{*}\\|^{2}}{\\eta}+\\eta\\cdot\\underbrace{\\mathbf{E}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}}_{(\\mathrm{C})}+4\\eta T\\mathbf{E}V_{T}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we would like to bound (C); to do so it is natural to decompose $g_{\\tau}\\,=\\,(g_{\\tau}\\,-\\,{\\bar{g}}_{\\tau})+({\\bar{g}}_{\\tau}\\,-$ $\\nabla f(x_{\\tau}))+\\nabla f(x_{\\tau})$ . The next lemma provides a bound, and its proof goes directly through this decomposition, ", "page_idx": 19}, {"type": "text", "text": "Lemma 4. The following holds, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\mathrm{C}\\right)\\leq3\\frac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}+3\\mathbf{E}V_{T}+12L\\mathbf{E}\\sum_{t=0}^{T}\\alpha_{0:t}\\Delta_{t}\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above Lemma into Eq. (33) yields, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\alpha_{0;t}\\mathbf{E}\\Delta_{t}\\leq\\frac{\\Vert w_{0}-w^{*}\\Vert^{2}}{\\eta}+3\\eta\\frac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}+8\\eta T\\mathbf{E}V_{T}+12\\eta L\\mathbf{E}\\sum_{t=0}^{T}\\alpha_{0;t}\\Delta_{t}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have uses $3\\leq4T$ which holds since $T\\geq1$ . The next lemma provides a bound for $\\mathbf{E}V_{t}$ , ", "page_idx": 20}, {"type": "text", "text": "Lemma 5. For any $t\\leq T:=K R$ , Alg. 2 with the learning choice in Eq. (8) ensures the following bound, ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\bf E}V_{t}\\le400L^{2}\\eta^{2}K^{3}\\sum_{\\tau=0}^{T}\\alpha_{0:\\tau}\\cdot\\big(G_{*}^{2}+4L\\Delta_{\\tau}\\big)+90L^{2}\\eta^{2}K^{6}R^{3}\\sigma^{2}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging the above bound back into Eq. (34) gives an almost explicit bound, ", "page_idx": 20}, {"type": "text", "text": "\u03b10:tE\u2206t ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\cfrac{\\|w_{0}-w^{*}\\|^{2}}{\\eta}+3\\eta\\cfrac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}+720L^{2}\\eta^{3}T K^{6}R^{3}\\sigma^{2}}\\\\ &{\\quad+4\\cdot10^{3}L^{2}\\eta^{3}T K^{3}\\sum_{r=0}^{T}\\alpha_{s}\\cdot(G_{*}^{2}+4L\\Delta_{r})+12\\eta L K\\sum_{t=0}^{T}\\alpha_{s,t}\\Delta_{t}}\\\\ &{=\\cfrac{\\|w_{0}-w^{*}\\|^{2}}{\\eta}+3\\eta\\cfrac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}+720L^{2}\\eta^{3}T K^{6}R^{3}\\sigma^{2}+4\\cdot10^{3}L^{2}\\eta^{3}T K^{3}\\underset{r=0}{\\overset{T}{\\sum}}\\alpha_{0,r}G_{*}^{2}}\\\\ &{\\quad+\\left(12\\eta L+16\\cdot10^{3}L^{3}\\eta^{3}T K^{3}\\right)\\underset{t=0}{\\overset{T}{\\sum}}\\alpha_{0,t}\\Delta_{t}}\\\\ &{\\leq\\cfrac{\\|w_{0}-w^{*}\\|^{2}}{\\eta}+3\\eta\\cfrac{\\sigma^{2}}{M}\\underset{t=0}{\\overset{T}{\\sum}}\\alpha_{t}^{2}+720L^{2}\\eta^{3}T K^{6}R^{3}\\sigma^{2}+4\\cdot10^{3}L^{2}\\eta^{3}T K^{3}\\underset{r=0}{\\overset{T}{\\sum}}\\alpha_{0,r}G_{*}^{2}+\\frac{1}{2(T+1)}\\bigg\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and we used $12\\eta L\\,\\leq\\,1/4(T+1)$ which follows since 48L(1T +1) (see Eq. (8)), as well as $16\\cdot10^{3}L^{3}\\eta^{3}T K^{3}\\le1/4(T+1)$ , which follows since $\\begin{array}{r}{\\eta\\le\\frac{1}{40L K(T+1)^{2/3}}}\\end{array}$ (see Eq. (8)). Next we use the above bound and invoke the following lemma, ", "page_idx": 20}, {"type": "text", "text": "Lemma 6. Let $\\{A_{t}\\}_{t\\in[T]}$ be a sequence of non-negative elements and $B\\in\\mathbb{R}.$ , and assume that for any $t\\leq T$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{t}\\leq\\mathcal{B}+\\frac{1}{2(T+1)}\\sum_{t=0}^{T}A_{t}\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then the following bound holds, ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{T}\\leq2B\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking $A_{t}\\;\\;\\leftarrow\\;\\;\\alpha_{0:t}\\mathbf{E}\\Delta_{t}$ and $\\begin{array}{r l r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathcal{B}}&{\\gets}&{\\frac{\\|w_{0}-w^{*}\\|^{2}}{\\eta}\\,+\\,3\\eta\\frac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}\\,+\\,720L^{2}\\eta^{3}T K^{6}R^{3}\\sigma^{2}\\,+\\,2\\mathrm{~.~}}\\end{array}$ $\\begin{array}{r}{{10^{3}}L^{2}\\eta^{3}T K^{3}\\sum_{\\tau=0}^{T}\\alpha_{0:\\tau}G_{*}^{2}}\\end{array}$ provides the following explicit bound, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle_{^10\\cdot T}\\mathbb{E}\\Delta_{T}\\leq\\frac{2\\|w_{0}-w^{*}\\|^{2}}{\\eta}+6\\eta\\frac{\\sigma^{2}}{M}\\sum_{t=0}^{T}\\alpha_{t}^{2}+2\\cdot10^{3}L^{2}\\eta^{3}T K^{6}R^{3}\\sigma^{2}+8\\cdot10^{3}L^{2}\\eta^{3}T K^{3}\\sum_{\\tau=0}^{T}\\alpha_{0\\cdot\\tau}G_{*}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{2\\|w_{0}-w^{*}\\|^{2}}{\\eta}+6\\eta\\frac{\\sigma^{2}}{M}\\cdot(K R)^{3}+2\\cdot10^{3}L^{2}\\eta^{3}K^{7}R^{4}\\sigma^{2}+8\\cdot10^{3}L^{2}\\eta^{3}K^{7}R^{4}\\cdot G_{*}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have used $\\begin{array}{r}{\\sum_{\\tau=0}^{T}\\alpha_{0:\\tau}\\,\\le\\,\\sum_{t=0}^{T}\\alpha_{t}^{2}\\,\\le\\,\\sum_{r=0}^{R-1}\\sum_{k=0}^{K-1}(r+1)^{2}K^{2}\\,\\le\\,K^{3}R^{3}}\\end{array}$ , as well as , ", "page_idx": 20}, {"type": "text", "text": "Recalling that $T=K R$ and that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{48L(T+1)},\\frac{1}{10L K^{2}},\\frac{1}{40L K(T+1)^{2/3}},\\frac{\\|w_{0}-w^{*}\\|\\sqrt{M}}{\\sigma T^{3/2}},\\frac{\\|w_{0}-w^{*}\\|^{1/2}}{L^{1/2}K^{7/4}R(\\sigma^{1/2}+G_{*}^{1/2})}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The above bound translates into, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\iota_{0:T}\\mathbf{E}\\Delta_{T}\\leq}}\\\\ &{}&{O\\left(L(T+K^{2}+K T^{2/3})\\|w_{0}-w^{*}\\|^{2}+\\frac{\\sigma\\|w_{0}-w^{*}\\|T^{3/2}}{\\sqrt{M}}+L^{1/2}K^{7/4}R(\\sigma^{1/2}+G_{*}^{1/2})\\cdot\\|w_{0}-w^{*}\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noting that $\\alpha_{0:T}\\geq\\Omega(T^{2})$ and using $T=K R$ gives the final bound, ", "page_idx": 21}, {"type": "equation", "text": "$$\nO\\left(\\frac{L||w_{0}-w^{*}||^{2}}{K R}+\\frac{L||w_{0}-w^{*}||^{2}}{K^{1/3}R^{4/3}}+\\frac{L||w_{0}-w^{*}||^{2}}{R^{2}}+\\frac{\\sigma||w_{0}-w^{*}||}{\\sqrt{M K R}}+\\frac{L^{1/2}(\\sigma^{1/2}+G_{*}^{1/2})\\cdot||w_{0}-w^{*}||^{2}}{K^{1/4}R}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which establishes the Theorem. ", "page_idx": 21}, {"type": "text", "text": "G Proof of Lemma 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 2. The update rule implies for all $\\tau\\in[T]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|w_{\\tau+1}-u\\|^{2}=\\|(w_{\\tau}-u)-\\eta\\alpha_{\\tau}g_{\\tau}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|w_{\\tau}-u\\|^{2}-2\\eta\\alpha_{\\tau}g_{\\tau}\\cdot(w_{\\tau}-u)+\\eta^{2}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Re-ordering and gives, ", "page_idx": 21}, {"type": "equation", "text": "$$\n2\\eta\\alpha_{\\tau}g_{\\tau}\\cdot(w_{\\tau}-u)=\\left(\\|w_{\\tau}-u\\|^{2}-\\|w_{\\tau+1}-u\\|^{2}\\right)+\\eta^{2}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summing over $\\tau$ and telescoping we obtain, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{2\\eta\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\cdot(w_{\\tau}-u)=\\big(\\|w_{1}-u\\|^{2}-\\|w_{t+1}-u\\|^{2}\\big)+\\eta^{2}\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}}\\\\ {\\qquad\\qquad\\qquad\\leq\\|w_{1}-u\\|^{2}+\\eta^{2}\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Dividing the above by $2\\eta$ establishes the lemma. ", "page_idx": 21}, {"type": "text", "text": "H Proof of Lemma 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 3. Recalling the notations $D_{\\tau}:=\\|w_{\\tau}-w^{*}\\|^{2}$ , our goal is to bound $\\mathbf{E}D_{0:t}$ . To do so, we will derive a recursive formula for $D_{0:t}$ . Indeed, the update rule of Alg. 2 implies Eq. (26), which in turn leads to the following for any $t\\in[T]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|w_{t+1}-w^{*}\\|^{2}=\\|(w_{t}-w^{*})-\\eta\\alpha_{t}g_{t}\\|^{2}=\\|w_{t}-w^{*}\\|^{2}-2\\eta\\alpha_{t}g_{t}\\cdot(w_{t}-w^{*})+\\eta^{2}\\alpha_{t}^{2}\\|g_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Unrolling the above equation and taking expectation gives, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{E}\\|w_{t+1}-w^{*}\\|^{2}=\\|w_{0}-w^{*}\\|^{2}\\underbrace{-2\\eta\\mathbf{E}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\cdot(w_{\\tau}-w^{*})}_{(*)}+\\eta^{2}\\mathbf{E}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The next lemma provides a bound on $(*)$ , ", "page_idx": 21}, {"type": "text", "text": "Lemma 7. The following holds for any $t\\in[T]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n(*)\\leq2\\eta\\sqrt{\\mathbf{E}V_{T}}\\cdot\\sqrt{\\mathbf{E}D_{0:T}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and recall that $\\begin{array}{r}{D_{0:T}:=\\sum_{t=0}^{T}\\|w_{t}-w^{*}\\|^{2}}\\end{array}$ , and $\\begin{array}{r}{V_{T}:=\\sum_{t=0}^{T}\\alpha_{t}^{2}\\|\\bar{g}_{t}-\\nabla f(x_{t})\\|^{2}}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Plugging the bound of Lemma 7 into Eq. (39), and using the notation of $D_{t}$ we conclude that for any $t\\in[T]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}D_{t}\\leq D_{0}+2\\eta\\sqrt{\\mathbf{E}V_{T}}\\cdot\\sqrt{\\mathbf{E}D_{0:T}}+\\eta^{2}\\mathbf{E}\\displaystyle\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\Vert g_{\\tau}\\Vert^{2}}\\\\ {\\leq D_{0}+2\\eta\\sqrt{\\mathbf{E}V_{T}}\\cdot\\sqrt{\\mathbf{E}D_{0:T}}+\\eta^{2}\\mathbf{E}\\displaystyle\\sum_{t=0}^{T}\\alpha_{t}^{2}\\Vert g_{t}\\Vert^{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used $t\\leq T$ . Summing the above equation over $t$ gives, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{{E}}D_{0:T}\\leq T\\Vert w_{0}-w^{*}\\Vert^{2}+2\\eta T\\cdot\\sqrt{\\mathbf{{E}}V_{T}}\\cdot\\sqrt{\\mathbf{{E}}D_{0:T}}+T\\eta^{2}\\mathbf{{E}}\\sum_{t=0}^{T}\\alpha_{t}^{2}\\Vert g_{t}\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We shall now require the following lemma, ", "page_idx": 22}, {"type": "text", "text": "Lemma 8. Let $A,B,C\\geq0$ , and assume that $A\\leq B+C{\\sqrt{A}}$ , then the following holds, ", "page_idx": 22}, {"type": "equation", "text": "$$\nA\\leq2B+4C^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, using the above Lemma with Eq. (40) implies, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{\\deltaE}D_{0:T}\\leq2T\\|w_{0}-w^{*}\\|^{2}+2T\\eta^{2}\\mathbf{E}\\sum_{t=0}^{T}\\alpha_{t}^{2}\\|g_{t}\\|^{2}+16\\eta^{2}T^{2}\\cdot\\mathbf{E}V_{T}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we have taken $A\\leftarrow D_{0:T}^{2}$ , $\\begin{array}{r}{B\\gets T\\|w_{0}-w^{*}\\|^{2}\\!+\\!T\\eta^{2}\\mathbf{E}\\sum_{t=0}^{T}\\alpha_{t}^{2}\\|g_{t}\\|^{2}}\\end{array}$ , and $C\\gets2\\eta T\\!\\cdot\\!\\sqrt{\\mathbf{E}V_{T}}$ . Thus, Eq. (41) establishes the lemma. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "H.1 Proof of Lemma 7 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 7. Recall that $\\begin{array}{r}{(*)=-2\\eta\\mathbf{E}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}g_{\\tau}\\mathbf{\\cdot}(w_{\\tau}\\!-\\!w^{*})}\\end{array}$ , we shall now focus on bounding $(*)/2\\eta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underbrace{\\boldsymbol{X}\\sum_{r=0}^{t}\\alpha_{r,p^{\\prime}}(\\mathbf{u}_{r},\\mathbf{u}^{\\prime})}_{\\geq0}=-\\underbrace{\\mathbf{R}\\sum_{r=0}^{t}\\alpha_{r}\\cdot(\\mathbf{u}_{r}-\\mathbf{u}^{\\prime})}_{\\geq0}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\underbrace{\\mathbf{b}}_{r=0}^{t}\\sum_{\\alpha=0}^{\\infty}\\mathbf{r}\\{(x_{r}-\\mathbf{u}^{\\prime})-\\mathbf{b}\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\underbrace{\\mathbf{b}}_{r=0}^{t}\\alpha_{r}\\cdot\\nabla[(x_{r})]\\cdot(\\mathbf{u}_{r}-\\mathbf{u}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\leq0+\\underbrace{\\mathbf{R}\\sum_{r=0}^{t}\\alpha_{r}^{2}[|\\mathbf{g}_{r}-\\nabla f(x_{r})|]^{2}}_{\\geq0}\\cdot[\\mathbf{u}_{r}-\\mathbf{u}^{\\prime}]^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\underbrace{(\\mathbf{b}\\mathbf{f}_{r}-\\nabla f(x_{r}))}_{\\geq0}\\mathbb{I}_{\\[\\mathbf{x}_{r}][\\mathbf{g}_{r}-\\nabla f(x_{r})]]^{2}}\\cdot\\nabla\\mathbb{E}[\\mathbf{u}_{r}-\\mathbf{u}^{\\prime}]^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\underbrace{(\\mathbf{b}\\mathbf{f}_{r}-\\nabla f(\\mathbf{u}_{r}))}_{\\geq0}\\cdot\\nabla\\mathbb{E}[\\mathbf{u}_{r}-\\mathbf{u}^{\\prime}]^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\underbrace{(\\mathbf{b}\\sum_{r=0}^{t}\\alpha_{r}^{2}[|\\mathbf{g}_{r}-\\nabla f(x_{r})|]^{2})}_{\\geq0}\\cdot\\sqrt{\\mathbf{R}\\sum_{r=0}^{t}\\alpha_{r}\\cdot\\mathbf{u}^{\\prime}[|\\mathbf{g}_{r}-\\nabla f(x_{r})|]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\underbrace{(\\mathbf{b}\\mathbf{f}_{r}-\\nabla f(\\mathbf{u}_{r}))}_{\\geq0}\\cdot\\nabla\\mathbb{E}[\\mathbf{g\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first line is due to the definitions of $g_{\\tau}$ and $\\bar{g}_{\\tau}$ appearing in Eq. (29) (this is formalized in Lemma 9 below and in its proof); the third line follows by observing that the $\\{x_{t}\\}_{t}$ sequence is and $\\left\\{\\alpha_{t}\\right\\}_{t}$ weighted average of $\\{w_{t}\\}_{t}$ and thus Theorem 1 implies that $\\begin{array}{r}{\\mathbf{E}\\sum_{\\tau=0}^{t}\\alpha_{\\tau}\\nabla f(x_{\\tau})\\cdot(w_{\\tau}-}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "$w^{*})\\geq0$ for any $t$ , as well as from Cauchy-Schwarz; the fourth line follows from the CauchySchwarz in\u221aequality\u221a for random variables, which asserts that for every random variables $X,Y$ , then $\\mathbf{E}[X Y]\\leq{\\sqrt{\\mathbf{E}X^{2}}}{\\sqrt{\\mathbf{E}Y^{2}}}$ ; the fifth line is an application of the following inequality ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t}a_{\\tau}b_{\\tau}\\leq\\sqrt{\\sum_{\\tau=0}^{t}a_{\\tau}^{2}}\\sqrt{\\sum_{\\tau=0}^{t}b_{\\tau}^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which holds for any two sequences $\\{a_{\\tau}\\in\\mathbb{R}\\}_{\\tau},\\,\\{b_{\\tau}\\in\\mathbb{R}\\}_{\\tau}$ , and the above also follows from the standard Cauchy-Schwarz inequality. Thus, Eq. (42) establishes the lemma. ", "page_idx": 23}, {"type": "text", "text": "We are left to show that $\\mathbf{E}\\left[g_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]=\\mathbf{E}\\left[\\bar{g}_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]$ which is established in the lemma below, ", "page_idx": 23}, {"type": "text", "text": "Lemma 9. The following holds for any $\\tau\\in[T]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[g_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]=\\mathbf{E}\\left[\\bar{g}_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "H.1.1 Proof of Lemma 9 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 9. Let $\\{{\\mathcal{F}}_{\\tau}\\}_{\\tau\\in[T]}$ be the natural filtration induces by the history of samples up to every time step $\\tau$ . Then according to the definitions of $g_{t}$ and $\\bar{g}_{t}$ we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf E}\\left[g_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]={\\bf E}\\left[{\\bf E}\\left[g_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\middle|\\mathcal{F}_{\\tau-1}\\right]\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad={\\bf E}\\left[{\\bf E}\\left[g_{\\tau}\\middle|\\mathcal{F}_{\\tau-1}\\right]\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad={\\bf E}\\left[{\\bf E}\\left[\\bar{g}_{\\tau}\\middle|\\mathcal{F}_{\\tau-1}\\right]\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad={\\bf E}\\left[\\bar{g}_{\\tau}\\cdot\\left(w_{\\tau}-w^{*}\\right)\\right]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first line follows by the law of total expectations; the second line follows since $w_{\\tau}$ is measurable w.r.t. $\\mathcal{F}_{\\tau-1}$ ; the third line follows by definition of $g_{\\tau}$ and $\\bar{g}_{\\tau}$ ; and the last line uses the law of total expectations. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "H.2 Proof of Lemma 8 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 8. We will divide the proof into two case. ", "page_idx": 23}, {"type": "text", "text": "Case 1: $B\\geq C{\\sqrt{A}}$ . In this case, ", "page_idx": 23}, {"type": "equation", "text": "$$\nA\\leq B+C\\sqrt{A}\\leq2B\\leq2B+4C^{2}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Case 2: $B\\leq C{\\sqrt{A}}$ . In this case, ", "page_idx": 23}, {"type": "equation", "text": "$$\nA\\leq B+C{\\sqrt{A}}\\leq2C{\\sqrt{A}}\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "dividing by $\\sqrt{A}$ and taking the square implies, ", "page_idx": 23}, {"type": "equation", "text": "$$\nA\\leq4C^{2}\\leq2B+4C^{2}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "And therefore the lemma holds. ", "page_idx": 23}, {"type": "text", "text": "I Proof of Lemma 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 4. Recalling that $\\begin{array}{r}{(\\mathrm{C}):=\\mathbf{E}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\|g_{\\tau}\\|^{2}}\\end{array}$ , we will decompose $g_{\\tau}=(g_{\\tau}-\\bar{g}_{\\tau})+$ $(\\bar{g}_{\\tau}-\\bar{\\nabla}f(x_{\\tau}))+\\nabla f(x_{\\tau})$ which gives, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\ensuremath{\\mathbf{C}}):=\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\ensuremath{\\mathbf{\\Phi}}_{\\tau}^{2}\\|(g_{\\tau}-\\bar{g}_{\\tau})+(\\bar{g}_{\\tau}-\\nabla f(x_{\\tau}))+\\nabla f(x_{\\tau})\\|^{2}}\\\\ &{\\le3\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\ensuremath{\\mathbf{\\Phi}}_{\\tau}^{2}\\|g_{\\tau}-\\bar{g}_{\\tau}\\|^{2}+3\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\ensuremath{\\mathbf{\\Phi}}_{\\tau}^{2}\\|\\bar{g}_{\\tau}-\\nabla f(x_{\\tau})\\|^{2}+3\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\ensuremath{\\mathbf{\\Phi}}_{\\tau}^{2}\\|\\nabla f(x_{\\tau})\\|^{2}}\\\\ &{\\le3\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\ensuremath{\\mathbf{\\Phi}}_{\\tau}^{2}\\|g_{\\tau}-\\bar{g}_{\\tau}\\|^{2}+3\\ensuremath{\\mathbf{E}}V_{T}+6L\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\ensuremath{\\mathbf{\\Phi}}_{\\tau}^{2}\\Delta_{\\tau}}\\\\ &{\\le3\\ensuremath{\\mathbf{J}}_{\\tau}^{2}\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\ensuremath{\\mathbf{\\Phi}}_{\\tau}^{2}+3\\ensuremath{\\mathbf{E}}V_{T}+6L\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}\\Delta_{\\tau}}\\\\ &{\\le3\\frac{\\sigma^{2}}{M}\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}^{2}+3\\ensuremath{\\mathbf{E}}V_{T}+12L\\ensuremath{\\mathbf{E}}\\sum_{\\tau=0}^{T}\\alpha_{\\tau}\\Delta_{\\tau}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second line uses $\\|a+b+c\\|^{2}\\leq3(\\|a\\|^{2}+\\|b\\|^{2}+\\|c\\|^{2})$ which holds for any $a,b,c\\in\\mathbb{R}^{d}$ ; the third line uses the definition of $V_{T}$ as well as the smoothness of $f(\\cdot)$ implying that $\\|\\nabla f(x_{\\tau})\\|^{2}\\leq$ $2L(f(x_{\\tau})-f(w^{*})):=2L\\Delta_{\\tau}$ (see Lemma 10 below); the fourth line invokes Lemma 11; the fifth line uses $\\alpha_{\\tau}^{2}=(\\tau+1)^{2}\\leq2\\alpha_{0:\\tau}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma 10. Let $F:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ be an $L$ -smooth function with a global minimum $x^{*}$ , then for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F(x)\\|^{2}\\leq2L(F(x)-F(w^{*}))\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 11. The following bound holds for any $t\\in[T]$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\bf E}\\|g_{\\tau}-\\bar{g}_{\\tau}\\|^{2}\\leq\\frac{\\sigma^{2}}{M}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "I.1 Proof of Lemma 10 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Lemma $I O$ . The $L$ smoothness of $f$ means the following to hold $\\forall w,u\\in\\mathbb{R}^{d}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nF(x+u)\\leq F(x)+\\nabla F(x)^{\\top}u+{\\frac{L}{2}}\\|u\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking $\\begin{array}{r}{u=-\\frac{1}{L}\\nabla F(x)}\\end{array}$ we get, ", "page_idx": 24}, {"type": "equation", "text": "$$\nF(x+u)\\ \\leq\\ F(x)-\\frac{1}{L}\\|\\nabla F(x)\\|^{2}+\\frac{1}{2L}\\|\\nabla F(x)\\|^{2}=F(x)-\\frac{1}{2L}\\|\\nabla F(x)\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla F(x)\\|^{2}\\;\\leq\\;2L\\big(F(x)-F(x+u)\\big)}&{}\\\\ {\\,\\leq\\;2L\\big(F(x)-F(x^{*})\\big)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the last inequality we used $F(x^{*})\\leq F(x+u)$ which holds since $x^{*}$ is the global minimum. ", "page_idx": 24}, {"type": "text", "text": "I.2 Proof of Lemma 11 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Lemma $_{l l}$ . Recall that we can write, ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{\\tau}-\\bar{g}_{\\tau}:=\\frac1{M}\\sum_{i\\in[M]}(g_{\\tau}^{i}-\\bar{g}_{\\tau}^{i})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "owthheerr.e $\\bar{g}_{\\tau}^{i}\\;:=\\;\\nabla f_{i}(x_{\\tau}^{i})$ ,i nagn do v $g_{\\tau}^{i}\\ :=\\ \\nabla f_{i}(x_{\\tau}^{i},z_{\\tau}^{i})$ $z_{t}^{1},\\dots,z_{t}^{M}$ enardee nitn daenpde znedreon t moefa ne aic.eh. $\\{x_{t}^{i}\\}_{i=1}^{M}$ $\\{\\boldsymbol{g}_{\\tau}^{i}\\,-\\,\\bar{g}_{\\tau}^{i}\\}_{i=1}^{M}$   \n${\\bf E}[g_{\\tau}^{i}-{\\bar{g}}_{\\tau}^{i}|\\{x_{t}^{i}\\}_{i=1}^{M}]=0.$ . Consequently, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|g_{\\tau}-\\bar{g}_{\\tau}\\right\\|^{2}\\middle|\\{x_{t}^{i}\\}_{i=1}^{M}\\right]=\\frac{1}{M^{2}}\\mathbb{E}\\left[\\left\\|\\underset{i\\in[M]}{\\sum_{c}}(g_{\\tau}^{i}-\\bar{g}_{\\tau}^{i})\\right\\|^{2}|\\{x_{t}^{i}\\}_{i\\in[M]}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{M^{2}}\\underset{i\\in[M]}{\\sum_{c}}\\mathbb{E}\\left[\\|g_{\\tau}^{i}-\\bar{g}_{\\tau}^{i}\\|^{2}|\\{x_{t}^{i}\\}_{i=1}^{M}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1}{M^{2}}\\underset{i\\in[M]}{\\sum_{c}}\\sigma^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\sigma^{2}}{M}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the law of total expectation implies that $\\begin{array}{r}{\\mathbf{E}\\|g_{\\tau}-\\bar{g}_{\\tau}\\|^{2}\\leq\\frac{\\sigma^{2}}{M}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "J Proof of Lemma 5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof of Lemma $^{5}$ . To bound $\\mathbf{E}V_{t}$ we will first employ the definition of $x_{t}$ together with the smoothness of $f(\\cdot)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\|\\nabla f(x_{\\tau})-\\bar{g}_{\\tau}\\|^{2}=\\mathbb{E}\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{i\\in[M]}\\nabla f_{i}(x_{\\tau})-\\frac{1}{M}\\displaystyle\\sum_{i\\in[M]}\\nabla f_{i}(x_{\\tau}^{i})\\right\\|^{2}}&{}\\\\ &{\\phantom{\\sum}\\leq\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{i\\in[M]}\\mathbb{E}\\|\\nabla f_{i}(x_{\\tau})-\\nabla f_{i}(x_{\\tau}^{i})\\|^{2}}\\\\ &{\\phantom{\\sum}\\leq\\displaystyle\\frac{L^{2}}{M}\\displaystyle\\sum_{i\\in[M]}^{2}\\mathbb{E}\\|x_{\\tau}-x_{\\tau}^{i}\\|^{2}}\\\\ &{=\\displaystyle\\frac{L^{2}}{M}\\displaystyle\\sum_{i\\in[M]}^{2}\\mathbb{E}\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{j\\in[M]}x_{\\tau}^{j}-x_{\\tau}^{i}\\right\\|^{2}}\\\\ &{\\leq\\displaystyle\\frac{L^{2}}{M^{2}}\\displaystyle\\sum_{i\\neq j\\in[M]}\\mathbb{E}\\|x_{\\tau}^{j}-x_{\\tau}^{i}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first line uses the definition of $\\bar{g}_{t}$ , the second line uses Jensen\u2019s inequality, and the third line uses the smoothness of $f_{i}(\\cdot)$ \u2019s. The last line follows from Jensen\u2019s inequality. ", "page_idx": 25}, {"type": "text", "text": "We use the following notation for any $\\tau\\in[T]$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{i,j}{\\tau}:=\\alpha_{\\tau}^{2}\\|x_{\\tau}^{i}-x_{\\tau}^{j}\\|^{2}\\;,\\quad\\&\\;\\:\\:\\!\\!\\!\\!\\!\\!\\!\\!\\!q_{\\tau}^{i}:=\\alpha_{\\tau}^{2}\\sum_{j\\in[M]}\\|x_{\\tau}^{i}-x_{\\tau}^{j}\\|^{2}\\;,\\quad\\&\\;\\:\\:\\!\\!\\!\\!\\!\\!\\!Q_{\\tau}:=\\frac{1}{M^{2}}\\alpha_{\\tau}^{2}\\sum_{\\stackrel{i,j\\in[M]}{i,j\\in[M]}}\\|x_{\\tau}^{i}-x_{\\tau}^{j}\\|^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and notice that $\\sum_{j\\in[M]}q_{\\tau}^{i,j}=q_{\\tau}^{i}$ , and that $\\begin{array}{r}{\\sum_{i,j\\in[M]}q_{\\tau}^{i,j}=M^{2}Q_{\\tau}}\\end{array}$ . Moreover $q_{\\tau}^{i,j}=q_{\\tau}^{j,i}\\;,\\forall i,j\\in$ $[M]$ . ", "page_idx": 25}, {"type": "text", "text": "Thus, according to Eq. (44) it is enough to bound $\\mathbf{E}V_{t}$ as follows, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{E}V_{t}:=\\sum_{\\tau=0}^{t}\\alpha_{\\tau}^{2}\\mathbf{E}\\|\\nabla f(x_{\\tau})-\\bar{g}_{\\tau}\\|^{2}\\leq L^{2}\\cdot\\underbrace{\\sum_{\\tau=0}^{t}Q_{\\tau}}_{(\\star)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next we will bound the above term. ", "page_idx": 25}, {"type": "text", "text": "Bounding $(\\star)$ : Let $t\\in[T]$ , if $t=r K$ for some $r\\in[R]$ , then according to Alg. 2 $x_{t}^{i}=x_{t}$ for any machine $i\\in[M]$ , thus $\\boldsymbol{x}_{t}^{i}-\\boldsymbol{x}_{t}^{j}=0$ for any two machines $i,j\\in[M]$ . ", "page_idx": 26}, {"type": "text", "text": "More generally, if $t=r K+k$ for some $r\\in[R]$ , and $k\\in[K]$ , then by denoting $t_{0}:=r K$ we can write $t=t_{0}+k$ . Using this notation, the update rule for $\\boldsymbol{x}_{\\tau}^{i}$ implies the following for any $i\\in[M]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nx_{t}^{i}=\\frac{\\alpha_{0:t_{0}}}{\\alpha_{0:t}}x_{t_{0}}^{i}+\\frac{1}{\\alpha_{0:t}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}w_{\\tau}^{i}=\\frac{\\alpha_{0:t_{0}}}{\\alpha_{0:t}}x_{t_{0}}+\\frac{1}{\\alpha_{0:t}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}w_{\\tau}^{i}\\;,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used $x_{t_{0}}^{i}=x_{t_{0}}$ , $\\forall i\\in[M]$ . Thus, for any $i\\neq j$ we can write, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\alpha_{t}^{2}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}=\\frac{\\alpha_{t}^{2}}{(\\alpha_{0:t})^{2}}\\left\\|\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}(w_{\\tau}^{i}-w_{\\tau}^{j})\\right\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So our next goal is to derive an expression for $\\begin{array}{r}{\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}(w_{\\tau}^{i}-w_{\\tau}^{j})}\\end{array}$ . The update rule of Eq. (6) implies that for any $\\tau\\in[t_{0},t_{0}+K]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nw_{\\tau}^{i}=w_{t_{0}}^{i}-\\eta\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{n}g_{n}^{i}=w_{t_{0}}-\\eta\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{n}g_{n}^{i}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second equality is due to the initialization of each round implying that $w_{t_{0}}^{i}=w_{t_{0}}$ , $\\forall i\\in$ $[M]$ . ", "page_idx": 26}, {"type": "text", "text": "Next, we will require the following notation $\\bar{g}_{t}^{i}:=\\nabla f_{i}(x_{t}^{i})$ , and $\\xi_{t}^{i}:=g_{t}^{i}-\\bar{g}_{t}^{i}$ . We can therefore write, $g_{t}^{i}={\\bar{g}}_{t}^{i}+{\\bar{\\xi}}_{t}^{i}$ and it is immediate to show that $\\mathbf{E}[\\xi_{t}^{i}|x_{t}^{i}]=0$ . Using this notation together with Eq. (48), implies that for any $\\tau\\in[t_{0},t_{0}+K]$ and $i\\neq j$ we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{\\tau}(w_{\\tau}^{i}-w_{\\tau}^{j})=-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{\\tau}\\alpha_{n}(\\bar{g}_{n}^{i}-\\bar{g}_{n}^{j})-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{\\tau}\\alpha_{n}(\\xi_{n}^{i}-\\xi_{n}^{j})}\\\\ &{\\qquad\\qquad=-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{\\tau}\\alpha_{n}(\\bar{g}_{n}^{i}-\\bar{g}_{n}^{j})-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{\\tau}\\alpha_{n}\\xi_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and in the last line we use the following notation $\\xi_{n}:=\\xi_{n}^{i}-\\xi_{n}^{j}\\,^{\\intercal}$ . ", "page_idx": 26}, {"type": "text", "text": "Summing Eq. (49) over $\\tau\\in[t_{0}+1,t]$ we obtain, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}(w_{\\tau}^{i}-w_{\\tau}^{j})=-\\eta\\displaystyle\\sum_{\\tau=t_{0}+1}^{t}\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{\\tau}\\alpha_{n}(\\bar{g}_{n}^{i}-\\bar{g}_{n}^{j})-\\eta\\displaystyle\\sum_{\\tau=t_{0}+1}^{t}\\sum_{n=t_{0}+1}^{\\tau}\\alpha_{\\tau}\\alpha_{n}\\xi_{n}}&{}\\\\ {=-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{t}\\sum_{\\tau=n}^{t}\\alpha_{\\tau}\\alpha_{n}(\\bar{g}_{n}^{i}-\\bar{g}_{n}^{j})-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{t}\\sum_{\\tau=n}^{t}\\alpha_{\\tau}\\alpha_{n}\\xi_{n}}&{}\\\\ {=-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{t}\\alpha_{n}\\varepsilon\\alpha_{n}(\\bar{g}_{n}^{i}-\\bar{g}_{n}^{j})-\\eta\\displaystyle\\sum_{n=t_{0}+1}^{t}\\alpha_{n:t}\\alpha_{n}\\xi_{n}}&{}\\\\ {=-\\eta\\displaystyle\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau:t}\\alpha_{\\tau}(\\bar{g}_{\\tau}^{i}-\\bar{g}_{\\tau}^{j})-\\eta\\displaystyle\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau:t}\\alpha_{\\tau}\\xi_{\\tau}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last equation we replace the notation of the summation index from $n$ to $\\tau$ (only done to ease notation). ", "page_idx": 26}, {"type": "text", "text": "Plugging the above equation back into Eq. (47) we obtain for any $t\\in[t_{0},t_{0}+K]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{v}:\\displaystyle\\sum_{i=1}^{d}\\left[\\phantom{\\frac{b}{b}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{+\\alpha\\left(\\beta_{i}^{\\alpha}\\right)\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ {\\!\\!\\!\\!\\!\\!\\!}&{=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$$\\|a\\!+\\!b\\|^{2}\\leq2\\|a\\|^{2}\\!+\\!2\\|b\\|^{2}$ $\\forall a,b\\in\\mathbb{R}^{d}$ ", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "iwtyh eursee tsh $\\begin{array}{r}{\\|\\sum_{n=N_{1}+1}^{N_{2}}a_{n}\\|^{2}\\leq(N_{2}-N_{1})\\sum_{n=N_{1}+1}^{N_{2}}\\|a_{n}\\|^{2}}\\end{array}$ $\\{a_{n}\\in\\mathbb{R}^{d}\\}_{n=N_{1}+1}^{N_{2}}$ ${\\bar{g}}_{\\tau}^{i}$ $t-t_{0}\\le K$ $(\\alpha_{\\tau:t})^{2}\\leq(K\\alpha_{t})^{2}$ which holds since $\\tau\\leq t$ and since both $\\alpha_{\\tau}\\leq\\alpha_{t}$ ; and the last inequality uses the fact that $\\alpha_{t}=t+1$ implying that the following holds, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{t}^{4}}{(\\alpha_{0:t})^{2}}\\leq4\\;,\\quad\\&\\quad\\frac{\\alpha_{t}^{2}}{(\\alpha_{0:t})^{2}}\\leq\\frac{4}{\\alpha_{t}^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 12. The following holds for any $i,j\\in[M]$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i}(x_{\\tau}^{i})-\\nabla f_{j}(x_{\\tau}^{j})\\|^{2}\\leq\\frac{3L^{2}}{M\\alpha_{\\tau}^{2}}(q_{\\tau}^{i}+q_{\\tau}^{j})+6\\left(\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\|\\nabla f_{j}(x_{\\tau})\\|^{2}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the above lemma inside Eq. (51) yields, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle q_{t}^{i,j}:=\\alpha_{t}^{2}\\|x_{t}^{i}-x_{t}^{j}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\leq24\\eta^{2}K^{3}L^{2}\\cdot\\frac{1}{M}(q_{t_{0}+1:t}^{i}+q_{t_{0}+1:t}^{j})+48\\eta^{2}K^{3}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\left(\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\|\\nabla f_{j}(x_{\\tau})\\|^{2}\\right)}}\\\\ {{\\displaystyle\\qquad+8\\eta^{2}\\alpha_{t}^{2}\\left\\|\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau:t}\\frac{\\alpha_{\\tau}}{\\alpha_{t}^{2}}\\xi_{\\tau}\\right\\|^{2}}}\\\\ {{\\displaystyle=\\frac{\\theta}{2}\\cdot\\frac{1}{M}(q_{t_{0}+1:t}^{i}+q_{t_{0}+1:t}^{j})+\\frac{\\theta}{L^{2}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\left(\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\|\\nabla f_{j}(x_{\\tau})\\|^{2}\\right)+B_{t}^{i,j}\\;,\\qquad(5)=\\xi_{\\tau}^{j}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we have denoted 8 ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\theta:=48\\eta^{2}K^{3}L^{2}\\;,\\quad\\&\\quad B_{t}^{i,j}:=8\\eta^{2}\\alpha_{t}^{2}\\left\\lVert\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau:t}\\frac{\\alpha_{\\tau}}{\\alpha_{t}^{2}}\\xi_{\\tau}\\right\\rVert^{2}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Summing Eq. (52) over $i,j\\in[M]$ and using the definition of $Q_{t}$ gives, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M^{2}Q_{t}=\\displaystyle\\sum_{i,j\\in[M]}q_{t}^{i,j}}\\\\ &{\\qquad\\leq\\frac{\\theta}{2}\\cdot\\frac{1}{M}\\cdot2M^{3}Q_{t_{0}+1:t}+\\displaystyle\\frac{\\theta}{L^{2}}\\cdot2M\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\sum_{i\\in[M]}\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\displaystyle\\sum_{i,j\\in[M]}B_{t}^{i,j}}\\\\ &{\\qquad=M^{2}\\theta\\cdot Q_{t_{0}+1:t}+\\displaystyle\\frac{2M\\theta}{L^{2}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\sum_{i\\in[M]}\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\displaystyle\\sum_{i,j\\in[M]}B_{t}^{i,j}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{i,j\\in[M]}q_{\\tau}^{i}=\\sum_{j\\in[M]}\\sum_{i\\in[M]}q_{\\tau}^{i}=\\sum_{j\\in[M]}M^{2}Q_{\\tau}=M^{3}Q_{\\tau}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now dividing Eq. (53) by $M^{2}$ gives $\\forall t\\in[t_{0},t_{0}+K]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t}\\leq\\theta\\cdot Q_{t_{0}+1:t}+\\displaystyle\\frac{2\\theta}{L^{2}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\cdot\\frac{1}{M}\\sum_{i\\in[M]}\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\displaystyle\\frac{1}{M^{2}}\\sum_{i,j\\in[M]}B_{t}^{i,j}}\\\\ &{\\quad\\leq\\theta\\cdot Q_{t_{0}+1:t}+\\displaystyle\\frac{2\\theta}{L^{2}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\cdot(G_{*}^{2}+4L(f(x_{\\tau})-f(w^{*})))+\\displaystyle\\frac{1}{M^{2}}\\sum_{i,j\\in[M]}B_{t}^{i,j}}\\\\ &{\\quad\\leq\\theta\\cdot Q_{t_{0}+1:t}+\\displaystyle\\frac{2\\theta}{L^{2}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\cdot(G_{*}^{2}+4L\\Delta_{\\tau}))+\\displaystyle\\frac{1}{M^{2}}\\sum_{i,j\\in[M]}B_{t}^{i,j}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second line follows from the dissimilarity assumption Eq. (2), and the last line is due to the definition of $\\Delta_{\\tau}$ . ", "page_idx": 28}, {"type": "text", "text": "Thus, we can re-write the above equation as follows forall $t\\in[t_{0},t_{0}+K]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nQ_{t}\\le\\theta Q_{t_{0}+1:t}+H_{t}\\;,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{H_{t}=\\frac{2\\theta}{L^{2}}\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau}^{2}\\cdot\\big(G_{*}^{2}+4L\\Delta_{\\tau}\\big)\\big)+\\frac{1}{M^{2}}\\sum_{i,j\\in[M]}\\mathcal{B}_{t}^{i,j}}\\end{array}$ , and recall that $\\theta:=48\\eta^{2}K^{3}L^{2}$ Now, notice that $Q_{t},H_{t}\\mathrm{~\\geq~0~}$ , and that $\\theta$ satisfies $\\theta K=48\\eta^{2}K^{4}L^{2}\\leq1/2$ since we assume that $\\begin{array}{r}{\\eta^{2}\\leq\\frac{1}{100L^{2}K^{4}}}\\end{array}$ (see Eq. (8)). This enables to make use of Lemma 13 below to conclude, ", "page_idx": 28}, {"type": "equation", "text": "$$\nQ_{t_{0}+1:t_{0}+K}\\le2H_{t_{0}+1:t_{0}+K}=\\frac{4\\theta}{L^{2}}\\sum_{\\tau=t_{0}+1}^{t_{0}+K}\\alpha_{\\tau}^{2}\\cdot(G_{*}^{2}+4L\\Delta_{\\tau}))+\\frac{2}{M^{2}}\\sum_{i,j\\in[M]}B_{t_{0}+1:t_{0}+K}^{i,j}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 13. Let $\\{Q_{t}\\geq0\\}_{t=t_{0}+1}^{t_{0}+K}$ and another sequence $K,\\theta>0$ , and assume $\\{H_{t}\\geq0\\}_{t=t_{0}+1}^{t_{0}+K}$ $\\theta K\\leq1/2$ . Also assume a sequence of non-negative terms that satisfy the following inequality for any $t\\in[t_{0},t_{0}+K].$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nQ_{t}\\le\\theta Q_{t_{0}+1:t}+H_{t}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then the following holds, ", "page_idx": 28}, {"type": "equation", "text": "$$\nQ_{t_{0}+1:t_{0}+K}\\leq2H_{t_{0}+1:t_{0}+K}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Recalling that we like to bound the expectation of the LHS of Eq. (56), we will next bound $\\begin{array}{r l}{\\lefteqn{\\left\\|\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau:t}\\frac{\\alpha_{\\tau}}{\\alpha_{t}^{2}}\\xi_{\\tau}\\right\\|^{2}}}\\end{array}$ , which is done in the following Lemma 9, ", "page_idx": 29}, {"type": "text", "text": "Lemma 14. The following bound holds for any $t\\in[t_{0},t_{0}+K].$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\bf E}\\left\\|\\sum_{\\tau=t_{0}+1}^{t}\\frac{\\alpha_{\\tau:t}\\alpha_{\\tau}}{\\alpha_{t}^{2}}\\xi_{\\tau}\\right\\|^{2}\\leq4K^{3}\\sigma^{2}\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since the above lemma for any $i,j$ and $t\\in[t_{0},t_{0}+K]$ we can now bound Bt0+1:t0+K as follows, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{{\\bf E}B_{t_{0}+1:t_{0}+K}^{i,j}=8\\eta^{2}\\alpha_{t}^{2}\\sum_{t=t_{0}+1}^{t_{0}+K}{\\bf E}\\left\\|\\sum_{\\tau=t_{0}+1}^{t}\\frac{\\alpha_{\\tau:t}\\alpha_{\\tau}}{\\alpha_{t}^{2}}\\xi_{\\tau}\\right\\|^{2}}}\\\\ &{}&\\\\ &{\\leq8\\eta^{2}\\alpha_{t}^{2}\\sum_{t=t_{0}+1}^{t_{0}+K}4K^{3}\\sigma^{2}\\ \\ \\ }\\\\ &{}&\\\\ &{=32\\eta^{2}\\alpha_{t}^{2}K^{4}\\sigma^{2}}\\\\ &{}&{=32\\eta^{2}(r+1)^{2}K^{6}\\sigma^{2}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last lines $\\alpha_{t}\\leq(r+1)K$ for any iteration $t$ that belongs to round $r$ . ", "page_idx": 29}, {"type": "text", "text": "Since the above holds for any $i,j$ it follows that, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{2}{M^{2}}\\sum_{i,j\\in[M]}B_{t_{0}+1:t_{0}+K}^{i,j}\\leq2\\cdot32\\eta^{2}(r+1)^{2}K^{6}\\sigma^{2}=64\\eta^{2}(r+1)^{2}K^{6}\\sigma^{2}\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Plugging the above back into Eq. (56) gives, ", "page_idx": 29}, {"type": "equation", "text": "$$\nQ_{t_{0}+1:t_{0}+K}\\leq200\\eta^{2}K^{3}\\sum_{\\tau=t_{0}+1}^{t_{0}+K}\\alpha_{\\tau}^{2}\\cdot(G_{*}^{2}+4L\\Delta_{\\tau}))+64\\eta^{2}(r+1)^{2}K^{6}\\sigma^{2}\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Final Bound on $\\mathbf{E}V_{t}$ . Finally, using the above bound together with the Eq. (46) enables to bound $\\mathbf{E}V_{t}$ as follows, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{L^{2}}\\mathbf{E}W_{\\leq}\\leq\\frac{\\hat{\\mathbf{l}}}{\\gamma}Q_{r}}\\,,}\\\\ &{\\leq\\displaystyle\\sum_{r=0}^{T}Q_{r}}\\\\ &{\\leq200\\eta^{2}K^{3}\\sum_{\\sigma=\\rho^{\\prime}}^{r}\\left(G_{r}^{2}+4L\\Delta_{r}\\right)+\\sum_{r=\\lfloor r+1\\rfloor}^{R-1}e^{L\\Delta_{r}}}\\\\ &{\\leq200\\eta^{2}K^{3}\\sum_{\\sigma=\\rho^{\\prime}}^{r}\\left(G_{r}^{2}+4L\\Delta_{r}\\right))+\\displaystyle\\sum_{r=0}^{R-1}\\sum_{\\sigma=\\rho^{\\prime}}^{r}\\left(\\mathbf{r}_{\\sigma}^{2}+\\mathbf{r}_{\\sigma}^{2}\\right)^{2}K^{6}\\sigma^{2}}\\\\ &{\\leq200\\eta^{2}K^{3}\\sum_{\\sigma=\\rho^{\\prime}}^{r}\\left(\\langle G_{r}^{2}+4L\\Delta_{r}\\rangle\\right)+\\displaystyle\\sum_{r=0}^{R}61\\eta^{2}\\left(r+1\\right)^{2}K^{6}\\sigma^{2}}\\\\ &{\\geq200\\eta^{2}K^{3}\\sum_{\\sigma=\\rho^{\\prime}}^{\\sigma}\\left(\\langle G_{r}^{2}+4L\\Delta_{r}\\rangle\\right)+61\\eta^{2}K^{6}\\sigma^{2}\\cdot\\frac{8}{\\delta}R^{3}}\\\\ &{\\geq400\\eta^{2}K^{3}\\sum_{\\sigma=\\rho^{\\prime}}^{r}\\left(\\langle G_{r}^{2}+4L\\Delta_{r}\\rangle\\right)+90\\eta^{2}K^{6}\\beta^{3}\\sigma^{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we have used $\\begin{array}{r}{\\sum_{r=0}^{R-1}(r+1)^{2}~\\leq~\\frac{8}{6}R^{3}}\\end{array}$ , and the last line uses $\\alpha_{\\tau}^{2}\\,=\\,(\\tau+1)^{2}\\,\\leq\\,2\\alpha_{0:\\tau}$ Consequently, we can bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\bf E}V_{t}\\le400L^{2}\\eta^{2}K^{3}\\sum_{\\tau=0}^{T}\\alpha_{0:\\tau}\\cdot\\left(G_{*}^{2}+4L\\Delta_{\\tau}\\right))+90L^{2}\\eta^{2}K^{6}R^{3}\\sigma^{2}\\;,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which established the lemma. ", "page_idx": 29}, {"type": "text", "text": "J.1 Proof of Lemma 12 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof of Lemma $^{12}$ . First note that by definition of $x_{\\tau}$ we have for any $i\\in[M]$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|x_{\\tau}-x_{\\tau}^{i}\\|^{2}=\\left\\|\\frac{1}{M}\\sum_{l\\in[M]}x_{\\tau}^{l}-x_{\\tau}^{i}\\right\\|^{2}\\leq\\frac{1}{M}\\sum_{l\\in[M]}\\|x_{\\tau}^{l}-x_{\\tau}^{i}\\|^{2}=\\frac{1}{M\\alpha_{\\tau}^{2}}q_{\\tau}^{i}\\;.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we have used Jensen\u2019s inequality, and the definition of $q_{\\tau}^{i}$ . ", "page_idx": 30}, {"type": "text", "text": "Using the above inequality, we obtain, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{i}(x_{\\tau}^{i})-\\nabla f_{j}(x_{\\tau}^{j})\\|^{2}=\\|(\\nabla f_{i}(x_{\\tau}^{i})-\\nabla f_{i}(x_{\\tau}))+(\\nabla f_{i}(x_{\\tau})-\\nabla f_{j}(x_{\\tau}))-(\\nabla f_{j}(x_{\\tau}^{j})-\\nabla f_{j}(x_{\\tau}^{i}))\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq3\\|\\nabla f_{i}(x_{\\tau}^{i})-\\nabla f_{i}(x_{\\tau})\\|^{2}+3\\|\\nabla f_{i}(x_{\\tau})-\\nabla f_{j}(x_{\\tau})\\|^{2}+3\\|\\nabla f_{j}(x_{\\tau}^{j})-\\nabla f_{i}(x_{\\tau}^{i})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq3L^{2}\\left(\\|x_{\\tau}-x_{\\tau}^{i}\\|^{2}+\\|x_{\\tau}-x_{\\tau}^{j}\\|^{2}\\right)+6\\left(\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\|\\nabla f_{j}(x_{\\tau})\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{3L^{2}}{M\\alpha_{\\tau}^{2}}(q_{\\tau}^{i}+q_{\\tau}^{j})+6\\left(\\|\\nabla f_{i}(x_{\\tau})\\|^{2}+\\|\\nabla f_{j}(x_{\\tau})\\|^{2}\\right)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second and third lines use $\\begin{array}{r}{\\|\\sum_{n=1}^{N}a_{n}\\|^{2}\\leq N\\sum_{n=1}^{N}\\|a_{n}\\|^{2}}\\end{array}$ which holds for any $\\{a_{n}\\in$ $\\mathbb{R}^{d}\\}_{n=1}^{N}$ ; we also used the smoothness of the $f_{i}(\\cdot)$ \u2019s; and t he last line uses Eq. (59). \u53e3 ", "page_idx": 30}, {"type": "text", "text": "J.2 Proof of Lemma 13 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof of Lemma $^{l3}$ . Since the $Q_{t}$ \u2019s and $\\theta$ are non-negative, we can further bound $Q_{t}$ for all $t\\in$ $[t_{0},t_{0}+K]$ as follows, ", "page_idx": 30}, {"type": "equation", "text": "$$\nQ_{t}\\le\\theta Q_{t_{0}+1:t}+H_{t}\\le\\theta Q_{t_{0}+1:t_{0}+K}+H_{t}\\;.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Summing the above over $t$ gives, ", "page_idx": 30}, {"type": "equation", "text": "$$\nQ_{t_{0}+1:t_{0}+K}:=\\sum_{t=t_{0}+1}^{t_{0}+K}Q_{t}\\leq\\theta K\\cdot Q_{t_{0}+1:t_{0}+K}+H_{t_{0}+1:t_{0}+K}\\leq{\\frac{1}{2}}\\cdot Q_{t_{0}+1:t_{0}+K}+H_{t_{0}+1:t_{0}+K}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we used $\\theta K\\leq1/2$ . Re-ordering the above equation immediately establishes the lemma. ", "page_idx": 30}, {"type": "text", "text": "J.3 Proof of Lemma 14 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof of Lemma $I4.$ . Letting $\\{\\mathcal{F}_{t}\\}_{t}$ be the natural flitration that is induces by the random draws up to time $t$ , i.e., by $\\{\\underline{{\\{z_{1}^{i}\\}}}_{i\\in[M]},\\dots,\\{z_{t}^{i}\\}_{i\\in[M]}\\}$ . By the definition of $\\xi_{t}$ it is clear that $\\xi_{t}$ is measurable with respect to $\\mathcal{F}_{t}$ , and that, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{E}[\\xi_{t}|\\mathcal{F}_{t-1}]=0\\;.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Implying that $\\{\\xi_{t}\\}_{t}$ is martingale difference sequence with respect to the filtration $\\{\\mathcal{F}_{t}\\}_{t}$ . The following implies that, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\kappa\\bigg\\|\\displaystyle\\sum_{t=u+1}^{t}\\frac{\\partial w(z)}{\\partial z}\\xi\\xi\\frac{\\partial}{\\partial\\xi}\\bigg\\|^{2}}&{\\leq\\displaystyle\\sum_{t=u+1}^{t}\\left(\\frac{\\partial w(z)}{\\partial\\xi}\\right)^{2}\\mathbb{E}\\|\\xi\\xi\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{t=u+1}^{t}\\left(\\frac{\\partial R(u+u)}{\\partial\\xi}\\right)^{2}\\mathbb{E}\\|\\xi\\xi\\|^{2}}\\\\ &{=\\displaystyle\\mathcal{K}_{r}^{2}\\sum_{t=u+1}^{t}\\mathbb{E}\\|\\xi\\|^{2}}\\\\ &{\\leq\\displaystyle\\mathcal{K}_{r}^{2}\\sum_{t=u+1}^{t}\\mathbb{E}\\|\\xi\\xi\\|^{2}}\\\\ &{\\leq2\\|\\xi^{2}\\frac{\\partial\\xi}{\\partial\\xi}\\|^{2}}\\\\ &{\\leq2\\|\\xi^{2}\\frac{\\partial\\xi}{\\partial\\xi}\\|^{2}+\\mathbb{E}\\|\\xi\\xi\\|^{2}+\\mathbb{E}\\|\\xi^{2}\\|^{2}}\\\\ &{\\leq2\\|\\xi^{2}\\frac{\\partial\\xi}{\\partial\\xi\\|\\xi^{2}}\\int_{0}^{2\\xi}}\\\\ &{\\leq2\\|\\xi^{2}\\frac{\\partial\\xi}{\\partial\\xi\\|\\xi^{2}}}\\\\ &{\\leq4\\|\\xi^{2}\\frac{\\partial\\xi}{\\partial\\xi\\|\\xi^{2}}\\left(t-t_{0}\\right)}\\\\ &{\\leq4\\|\\xi^{2}\\frac{\\partial\\xi}{\\partial\\xi\\|\\xi^{2}}\\left(t-t_{0}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first line follows by Lemma 15 below, the second line holds since $\\alpha_{\\tau}\\leq\\alpha_{t}$ , and $\\alpha_{\\tau:t}\\leq K\\alpha_{t}$ (recall $\\alpha_{\\tau}\\,\\leq\\,\\alpha_{t}$ since $\\tau\\leq t)$ ); the fourth line follows due to $\\xi_{\\tau}\\,:=\\,\\xi_{\\tau}^{i}\\,-\\,\\xi_{\\tau}^{j}$ ; the fifth line uses $\\|a\\!+\\!b\\|^{2}\\leq2\\|a\\|^{2}\\!+\\!2\\|b\\|^{2}$ for any $a,b\\in\\mathbb{R}^{d}$ ; the sixth line follows since ${\\bf\\dot{E}}\\|\\xi_{\\tau}^{i}\\|^{2}:={\\bf E}\\|g_{\\tau}^{i}-\\bar{g}_{\\tau}^{i}\\|^{2}:=$ ${\\mathbf E}\\|\\nabla f(x_{\\tau}^{i},z_{\\tau}^{i})-\\nabla f(x_{\\tau}^{i})\\|^{2}\\leq\\sigma^{2}$ ; and the last line uses $(t-t_{0})\\leq K$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma 15. Let $\\{\\xi_{t}\\}_{t}$ be a martingale difference sequence with respect to a filtration $\\{\\mathcal{F}_{t}\\}_{t}$ , then the following holds for all time indexes $t_{1},t_{2}\\geq0$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left\\|\\sum_{\\tau=t_{1}}^{t_{2}}\\xi_{\\tau}\\right\\|^{2}=\\sum_{\\tau=t_{1}}^{t_{2}}\\mathbf{E}\\left\\|\\xi_{\\tau}\\right\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "J.3.1 Proof of Lemma 15 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Lemma 15. We shall prove the lemma by induction over $t_{2}$ . The base case where $t_{2}=t_{1}$ clearly holds since it boils down to the following identity, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left\\|\\sum_{\\tau=t_{1}}^{t_{1}}\\xi_{\\tau}\\right\\|^{2}=\\mathbf{E}\\left\\|\\xi_{t_{1}}\\right\\|^{2}=\\sum_{\\tau=t_{1}}^{t_{1}}\\mathbf{E}\\left\\|\\xi_{\\tau}\\right\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now for induction step let us assume that the equality holds for $t_{2}\\geq t_{1}$ and lets prove it holds for $t_{2}+1$ . Indeed, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left\\|\\displaystyle\\sum_{t=t_{i}}^{t+1}\\xi_{t}^{\\top}\\right\\|^{2}=\\mathbb{E}\\left\\|\\xi_{t+1}+\\displaystyle\\sum_{t=t_{i}+1}^{t}\\xi_{t}^{\\top}\\right\\|^{2}}\\\\ &{=\\mathbb{E}\\left\\|\\displaystyle\\sum_{t=0}^{t_{i}}\\xi_{t}^{\\top}\\right\\|^{2}+\\mathbb{E}\\|\\xi_{t+1}\\|^{2}+2\\mathbb{E}\\left(\\displaystyle\\sum_{t=t_{i}}^{t_{i}}\\xi_{t}\\right)\\cdot\\xi_{t+1}}\\\\ &{=\\displaystyle\\sum_{t=1}^{t_{i}+1}\\mathbb{E}\\|\\xi_{t}\\|^{2}+2\\mathbb{E}\\left[\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{t=t_{i}}^{t_{i}}\\xi_{t}\\right)\\cdot\\xi_{t+1}\\mathbb{E}_{t_{i}}\\right]\\right]}\\\\ &{=\\displaystyle\\sum_{t=1}^{t_{i}+1}\\mathbb{E}\\|\\xi_{t}\\|^{2}+2\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{t=t_{i}}^{t_{i}}\\xi_{t}\\right)\\cdot\\mathbb{E}\\{\\xi_{t+1}|\\xi_{t_{i}}\\|\\}\\right]}\\\\ &{=\\displaystyle\\sum_{t=1}^{t_{i}+1}\\mathbb{E}\\left\\|\\xi_{t}\\|^{2}+0\\right.}\\\\ &{\\=\\displaystyle\\sum_{t=1}^{t_{i}+1}\\mathbb{E}\\left\\|\\xi_{t}\\|^{2}+0\\right.}\\\\ &{\\phantom{2p t}\\mathbb{E}\\left\\|\\xi_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the third line follows from the induction hypothesis, as well as from the law of total expectations; the fourth lines follows since $\\{\\xi_{\\tau}\\}_{\\tau=0}^{t_{2}}$ are measurable w.r.t ${\\mathcal{F}}_{t_{2}}$ , and the fifth line follows since $\\mathbf{E}[\\xi_{t_{2}+1}|\\mathcal{F}_{t_{2}}]=0$ . Thus, we have established the induction step and therefore the lemma holds. ", "page_idx": 32}, {"type": "text", "text": "K Proof of Lemma 6 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof of Lemma 6. Summing the inequality $\\begin{array}{r}{A_{t}\\le B+\\frac{1}{2(T+1)}\\sum_{t=0}^{T}A_{t}}\\end{array}$ over $t$ gives, ", "page_idx": 32}, {"type": "equation", "text": "$$\nA_{0:T}\\leq(T+1)\\beta+(T+1)\\frac{1}{2(T+1)}A_{0:T}=(T+1)\\beta+\\frac{1}{2}A_{0:T}\\;,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Re-ordering we obtain, ", "page_idx": 32}, {"type": "equation", "text": "$$\nA_{0:T}\\leq2(T+1)\\mathcal{B}\\;.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plugging this back to the original inequality and taking $t=T$ gives, ", "page_idx": 32}, {"type": "equation", "text": "$$\nA_{T}\\leq\\mathcal{B}+\\frac{1}{2(T+1)}A_{0:T}\\leq2\\mathcal{B}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 32}, {"type": "text", "text": "L The Necessity of Non-uniform Weights ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "One may wonder, why should we employ increasing weights $\\alpha_{t}\\propto t$ rather than using standard uniform weights $\\alpha_{t}\\,=\\,1\\,\\,,\\forall t$ . Here we explain why uniform weights are insufficient and why increasing weights e.g. $\\alpha_{t}\\propto t$ are crucial to obtain our result. ", "page_idx": 32}, {"type": "text", "text": "Intuitive Explanation. Prior to providing an elaborate technical explanation we will provides some intuition. The intuition behind the importance of using increasing weights is the following: Increasing weights are a technical tool to put more emphasis on the last rounds. Now, in the context of Local update methods, the iterates of the last rounds are more attractive since the bias between different machines shrinks as we progress. Intuitively, this happens since as we progress with the optimization process, the bias in the stochastic gradients that we compute goes to zero (in expectation), and consequently the bias between different machines shrinks as we progress. ", "page_idx": 32}, {"type": "text", "text": "Technical Explanation. Assume general weights $\\left\\{\\alpha_{t}\\right\\}_{t}$ , and let us go back to the proof of Lemma 5 (see Section J). Recall that in this proof we derive a bound of the following form (see Eq. (55)) ", "page_idx": 33}, {"type": "equation", "text": "$$\nA_{t}\\leq\\theta A_{t_{0}+1:t}+B_{t}\\;,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{A_{t}:=\\alpha_{t}^{2}\\|x_{t}^{i}-x_{t}^{j}\\|^{2},B_{t}=8\\eta^{2}\\alpha_{t}^{2}\\left\\|\\sum_{\\tau=t_{0}+1}^{t}\\alpha_{\\tau:t}\\frac{\\alpha_{\\tau}}{\\alpha_{t}^{2}}\\xi_{\\tau}\\right\\|^{2}}\\end{array}$ , and importantly 10, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\theta:=\\eta^{2}\\frac{2\\alpha_{t}^{4}}{(\\alpha_{0:t})^{2}}\\cdot K^{3}L^{2}\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, a crucial part of the proof is the fact that $\\theta K\\leq1/2$ , which in turn enables to employ Lemma 13 in order to bound $\\mathbf{E}V_{t}$ . ", "page_idx": 33}, {"type": "text", "text": "Not let us inspect the constraint $\\theta K\\leq1/2$ for polynomial weights of the form $\\alpha_{t}\\propto t^{p}$ where $p\\geq0$ . This condition boils down to, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\eta^{2}\\frac{2\\alpha_{t}^{4}}{(\\alpha_{0:t})^{2}}\\cdot K^{3}L^{2}\\cdot K\\leq1/2\\;,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Implying the following bound should apply to $\\eta$ for any $t\\in[T]$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{2L K^{2}}\\cdot\\frac{\\alpha_{0:t}}{\\alpha_{t}^{2}}\\approx\\frac{1}{2L K^{2}}\\cdot\\frac{t^{p+1}}{t^{2p}}=\\frac{1}{2L K^{2}}\\cdot t^{1-p}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now since the bound should hold for any $t\\in[T]$ we could divide into two cases: ", "page_idx": 33}, {"type": "text", "text": "Case 1: $p\\leq1$ . In this case $t^{1-p}$ is monotonically increasing with $t$ so the above condition should be satisfied for the smallest $t$ , namely $t=1$ , implying, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{2L K^{2}}\\cdot\\frac{\\alpha_{0:t}}{\\alpha_{t}^{2}}\\approx\\frac{1}{2L K^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The effect of this term on the overall error stems from the first term in the error analysis (see e.g. Eq. (36)), namely, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha_{0:T}}\\cdot\\frac{\\|w_{0}-w^{*}\\|^{2}}{\\eta}=\\frac{2L K^{2}\\cdot\\|w_{0}-w^{*}\\|^{2}}{T^{p+1}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, for the extreme values $p=0$ (uniform weights) and $p=1$ (linear weights), the above expression results an error term of the following form, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{Err}({\\bf p}={\\bf0})=O\\left({K^{2}}/{T}\\right)=O(K/R)\\-\\ \\&\\ \\ \\mathrm{Err}({\\bf p}={\\bf1})=O(K^{2}/T^{2})=O(1/R^{2})\\ .\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, for $p=0$ , the error term is considerably worse even compared to Minibatch-SGD, and $p=1$ is clearly an improvement. ", "page_idx": 33}, {"type": "text", "text": "Case 2: $p>1$ . In this case $t^{1-p}$ is decreasing increasing with $t$ so the above condition should be satisfied for the largest $t$ , namely $t=T$ , implying, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{2L K^{2}}\\cdot\\frac{\\alpha_{0:t}}{\\alpha_{t}^{2}}\\approx\\frac{1}{2L K^{2}}\\cdot\\frac{1}{T^{p-1}}\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, the effect of this term on the overall error stems from the first term in the error analysis (see e.g. Eq. (36)), namely, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha_{0:T}}\\cdot\\frac{\\|w_{0}-w^{*}\\|^{2}}{\\eta}=\\frac{2L K^{2}\\cdot\\|w_{0}-w^{*}\\|^{2}\\cdot T^{p-1}}{T^{p+1}}=\\frac{2L K^{2}\\cdot\\|w_{0}-w^{*}\\|^{2}.}{T^{2}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, for any $p\\geq1$ we obtain, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{Err}({\\bf p})=O(K^{2}/T^{2})=O(1/R^{2})\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Conclusion: As can be seen from Equations (61) (62), using uniform weights or even polynomial weights $\\alpha_{t}\\propto t^{p}$ with $p<1$ yields strictly worse guarantees compared to taking $p\\geq1$ . ", "page_idx": 33}, {"type": "text", "text": "NeurIPS paper checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: we claim to present the first local-update method that improves over the minibatch baseline in the SCO heterogeneous setting, and this is exactly what we establish in the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In the intro and conclusion sections, we discuss that our approach does not aqpply to non-convex scenarios, and state it as future direction ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: we indeed provide the full set of assumptions and a complete (and correct) proof ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: we ensured that our paper conforms with the NeurIPS Code of Ethics Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: we do not foresee any special societal impact that arise due to our work Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: our work does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]