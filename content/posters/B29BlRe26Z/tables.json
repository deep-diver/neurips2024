[{"figure_path": "B29BlRe26Z/tables/tables_1_1.jpg", "caption": "Table 1: We compare the best known guarantees for parallel learning, to our SLowcal-SGD approach for the heterogeneous SCO case. The bolded term in the Rate column is the one that compares least favourably against Minibatch SGD. Where G and G* relate to the dissimilarity measures that are defined in Equations (1) and (3). The Rmin column presents the minimal number of communication rounds that are required to obtain a linear speedup (we fixed values of K, M and take \u03c3 = 1). Note that we omit methods that do not enable a wall-clock linear speedup with M, e.g. [28, 29].", "description": "This table compares the convergence rates and the minimum number of communication rounds required for linear speedup of different parallel learning algorithms in the heterogeneous stochastic convex optimization (SCO) setting.  It contrasts the performance of SLowcal-SGD (the proposed algorithm) against existing baselines like Minibatch-SGD and Local-SGD, highlighting the advantage of SLowcal-SGD in terms of communication efficiency.", "section": "3 Our Approach"}]