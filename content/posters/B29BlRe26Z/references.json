{"references": [{"fullname_first_author": "John Duchi", "paper_title": "Adaptive subgradient methods for online learning and stochastic optimization", "publication_date": "2011-00-00", "reason": "This paper is foundational for the development of adaptive gradient methods, which are crucial to handling non-IID data in federated learning and are discussed in this paper."}, {"fullname_first_author": "Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This is a seminal work introducing federated learning, directly referenced for its algorithmic approach and its relevance to the paper's focus on heterogeneous data in distributed learning."}, {"fullname_first_author": "Sai Praneeth Karimireddy", "paper_title": "Scaffold: Stochastic controlled averaging for federated learning", "publication_date": "2020-00-00", "reason": "This paper proposes the Scaffold algorithm, a significant contribution in addressing the challenges of heterogeneous data in federated learning that this paper builds upon."}, {"fullname_first_author": "Blake Woodworth", "paper_title": "Is local SGD better than minibatch SGD?", "publication_date": "2020-00-00", "reason": "This paper directly addresses the comparative performance of Local SGD and Minibatch SGD under heterogeneous data conditions, forming the basis of this paper's comparison."}, {"fullname_first_author": "Ashok Cutkosky", "paper_title": "Anytime online-to-batch, optimism and acceleration", "publication_date": "2019-00-00", "reason": "This paper introduces the Anytime-GD algorithm, a crucial component of SLowcal-SGD, making it a core theoretical foundation for the proposed method."}]}