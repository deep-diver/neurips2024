{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models and is frequently cited in the field, thus making it highly relevant to the current work."}, {"fullname_first_author": "Utku Evci", "paper_title": "Rigging the lottery: Making all tickets winners", "publication_date": "2020-06-01", "reason": "This paper introduces a novel approach to sparse training that is directly relevant to the current work, enhancing the understanding of sparse model optimization."}, {"fullname_first_author": "Greg Yang", "paper_title": "Feature learning in infinite-width neural networks", "publication_date": "2020-11-01", "reason": "This paper lays the theoretical groundwork for the maximal update parameterization (\u00b5P), which is a key component of SuPar and is central to the current work."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2018-03-01", "reason": "This seminal paper introduces the lottery ticket hypothesis, a concept that is fundamental to the field of sparse neural networks and is directly relevant to the current work."}, {"fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled Weight Decay Regularization", "publication_date": "2017-01-01", "reason": "This paper proposes an improvement to the Adam optimizer, which is crucial for the stability of training dynamics and is directly applied in this research."}]}