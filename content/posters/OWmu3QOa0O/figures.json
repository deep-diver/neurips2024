[{"figure_path": "OWmu3QOa0O/figures/figures_1_1.jpg", "caption": "Figure 1: SuPar (Our work) allows stable optimum HPs for any sparsity level, unlike standard practice.", "description": "This figure compares the standard practice of reusing hyperparameters (HPs) optimized for dense models in sparse training versus the SuPar approach.  The left panel (Standard Practice) shows that the optimal learning rate shifts significantly as sparsity increases, making it difficult to find good HPs for different levels of sparsity.  The right panel (SuPar) demonstrates that SuPar's reparameterization technique keeps the optimal learning rate stable across all sparsity levels, greatly simplifying hyperparameter tuning for sparse neural networks.  This highlights SuPar's key advantage: enabling efficient and effective training at different sparsity levels without the need for extensive hyperparameter tuning.", "section": "Abstract"}, {"figure_path": "OWmu3QOa0O/figures/figures_1_2.jpg", "caption": "Figure 2: S\u00b5Par enables sparse training at scale, helping to surpass dense and motivate sparsity in hardware.", "description": "This figure uses a flowchart to compare the standard practice of sparse training with the SuPar approach.  The standard practice is shown to lead to prohibitive tuning costs, inconclusive results, an unclear path to scale, and ultimately, dense models remaining dominant. In contrast, the SuPar approach is depicted as resulting in cheap tuning, results robust to changes in model width and sparsity, a clear path towards scaling to larger models, and ultimately, a potential to surpass dense models.", "section": "Abstract"}, {"figure_path": "OWmu3QOa0O/figures/figures_1_3.jpg", "caption": "Figure 3: For LLMs, SuPar forms the Pareto frontier loss across sparsity levels, with no HP tuning required.", "description": "This figure shows the validation loss of large language models (LLMs) trained with three different parameterizations: standard parameterization (SP), maximal update parameterization (\u00b5P), and sparse maximal update parameterization (S\u00b5Par).  The x-axis represents the density (1 - sparsity) of the model, and the y-axis represents the validation loss.  The figure demonstrates that S\u00b5Par consistently achieves the lowest validation loss across all sparsity levels, forming the Pareto frontier. This implies that S\u00b5Par provides the best trade-off between model sparsity and performance, and requires no hyperparameter tuning across different sparsity levels.", "section": "SuPar Training Results"}, {"figure_path": "OWmu3QOa0O/figures/figures_2_1.jpg", "caption": "Figure 4: The three operations associated with training a layer with weights that perform the function F: Forward activation calculation, backward gradient propagation, and the weight update.", "description": "This figure shows a high-level overview of the three main steps in training a neural network layer using sparse weight updates.  The input X is processed by a forward pass function F, which uses a weight matrix W and sparsity mask M. The output of the forward pass is Y.  Then, a backward pass calculates the gradient of the loss function with respect to X (\u2207xL).  Finally, an optimizer updates W based on the gradient, incorporating M, resulting in \u2206W. This update \u2206W is then applied, affecting the next forward pass.", "section": "3 Sparse maximal update parameterization (S\u00b5Par)"}, {"figure_path": "OWmu3QOa0O/figures/figures_3_1.jpg", "caption": "Figure 5: Mean absolute value activations for attention and feed forward blocks after training step t (10 seeds). In SP and \u00b5P models, decreasing density causes activations to vanish (note axes on log-scale). In S\u00b5Par models, density has little effect on activation scales and there is no vanishing.", "description": "This figure compares the mean absolute value of activations in attention and feed-forward blocks of a neural network after different training steps for three different parameterization methods: SP (standard parameterization), \u00b5P (maximal update parameterization), and S\u00b5Par (sparse maximal update parameterization).  The x-axis represents the density of the network (1-sparsity), and the y-axis represents the mean absolute activation value.  Different lines represent different training steps.  The plot shows that with SP and \u00b5P, as sparsity increases (density decreases), the activation values tend to vanish. In contrast, with S\u00b5Par, the activation values remain relatively stable across different sparsity levels.", "section": "Sparse maximal update parameterization (S\u00b5Par)"}, {"figure_path": "OWmu3QOa0O/figures/figures_5_1.jpg", "caption": "Figure 6: SuPar ensures stable optimal learning rate for any sparsity s, unlike SP and \u00b5P (3 seeds).", "description": "This figure demonstrates how the optimal learning rate changes with different sparsity levels for three different parameterizations: Standard Parameterization (SP), Maximal Update Parameterization (\u00b5P), and Sparse Maximal Update Parameterization (SuPar).  For SP and \u00b5P, the optimal learning rate shifts significantly as sparsity increases, making it difficult to find a single optimal learning rate that works across all sparsity levels.  In contrast, SuPar shows a stable optimal learning rate that remains consistent across various sparsity levels. This highlights SuPar's ability to stabilize training dynamics and reduce the cost of hyperparameter tuning when training sparse neural networks.", "section": "4.1 Sparse hyperparameter transfer"}, {"figure_path": "OWmu3QOa0O/figures/figures_6_1.jpg", "caption": "Figure 7: Across sparsity s, SP and \u00b5P show unstable optimal initialization. SuPar is stable (3 seeds).", "description": "This figure compares the optimal weight initialization (\u03c3w) for different sparsity levels (s) across three different parameterizations: Standard Parameterization (SP), Maximal Update Parameterization (\u00b5P), and Sparse Maximal Update Parameterization (SuPar).  The x-axis represents the initial weight variance (\u03c3w). The y-axis represents the validation loss after training. Each line represents a different sparsity level, ranging from dense (s=0) to highly sparse (s=0.984375).  The figure shows that for SP and \u00b5P, the optimal \u03c3w changes significantly as sparsity increases, demonstrating unstable behavior. In contrast, SuPar maintains a stable optimal \u03c3w across all sparsity levels, highlighting its robustness and efficiency in training sparse models.", "section": "4.1 Sparse hyperparameter transfer"}, {"figure_path": "OWmu3QOa0O/figures/figures_6_2.jpg", "caption": "Figure 6: S\u00b5Par ensures stable optimal learning rate for any sparsity s, unlike SP and \u00b5P (3 seeds).", "description": "This figure demonstrates the stability of optimal learning rates across different sparsity levels when using SuPar compared to standard parameterization (SP) and maximal update parameterization (\u00b5P).  For SP and \u00b5P, the optimal learning rate shifts significantly as sparsity increases, whereas SuPar maintains a consistent optimal learning rate near 2\u207b\u2076 across all sparsity levels. This highlights SuPar's ability to provide stable training dynamics across varying sparsity levels, which is crucial for efficient sparse training.", "section": "4.1 Sparse hyperparameter transfer"}, {"figure_path": "OWmu3QOa0O/figures/figures_6_3.jpg", "caption": "Figure 3: For LLMs, SuPar forms the Pareto frontier loss across sparsity levels, with no HP tuning required.", "description": "This figure shows the training loss for large language models (LLMs) at different sparsity levels using three different parameterization methods: Standard Parameterization (SP), Maximal Update Parameterization (\u00b5P), and Sparse Maximal Update Parameterization (S\u00b5Par).  The x-axis represents the model density (1-sparsity), while the y-axis shows the validation loss.  The figure demonstrates that SuPar consistently achieves the lowest training loss across all sparsity levels, forming the Pareto frontier \u2013 meaning there is no other parameterization that achieves better loss at any given sparsity.  Importantly, SuPar achieves this without requiring any hyperparameter tuning across different sparsity levels; the optimal hyperparameters remain stable.", "section": "SuPar Training Results"}, {"figure_path": "OWmu3QOa0O/figures/figures_7_1.jpg", "caption": "Figure 9: SuPar ensures stable optimal learning rate for any sparsity s, unlike SP and \u00b5P (3 seeds).", "description": "This figure demonstrates the stability of optimal learning rates across various sparsity levels when using SuPar (sparse maximal update parameterization).  It contrasts SuPar's performance with standard parameterization (SP) and maximal update parameterization (\u00b5P).  SuPar shows consistent optimal learning rates across different sparsity levels, unlike SP and \u00b5P, which exhibit significant drift in optimal learning rates as sparsity increases. This highlights SuPar's ability to maintain stable training dynamics across various sparsity levels, simplifying hyperparameter tuning and improving the efficiency of sparse training.", "section": "4.1 Sparse hyperparameter transfer"}, {"figure_path": "OWmu3QOa0O/figures/figures_7_2.jpg", "caption": "Figure 6: S\u00b5Par ensures stable optimal learning rate for any sparsity s, unlike SP and \u00b5P (3 seeds).", "description": "This figure demonstrates the stability of optimal learning rates across various sparsity levels using the SuPar approach, in contrast to the standard parameterization (SP) and maximal update parameterization (\u00b5P).  The graph shows training loss plotted against learning rate for different sparsity levels (s).  SuPar maintains a consistent optimal learning rate across all sparsity levels, whereas SP and \u00b5P show significant shifts in the optimal learning rate as sparsity increases. This highlights SuPar's ability to stabilize optimal hyperparameters, which is crucial for efficient and effective sparse training.", "section": "4.1 Sparse hyperparameter transfer"}, {"figure_path": "OWmu3QOa0O/figures/figures_8_1.jpg", "caption": "Figure 12: For dynamic sparse training methods RigL and GMP, none of SP, \u00b5P, or SuPar achieve stable optimal learning rate across sparsity (3 seeds). Missing points indicate diverged training runs.", "description": "This figure demonstrates the instability of optimal learning rates across different sparsity levels for various dynamic sparse training methods (RigL and GMP) when combined with standard parameterization (SP), maximal update parameterization (\u00b5P), and sparse maximal update parameterization (S\u00b5Par).  The instability is highlighted by the scattered optimal learning rate values across sparsity levels for all three parameterization methods, indicating that a single optimal learning rate does not exist across varying sparsity for these methods.  The missing data points for several configurations suggest the training process diverged for certain hyperparameter settings and sparsity levels.", "section": "4.4 Dynamic sparsity hyperparameter transfer"}, {"figure_path": "OWmu3QOa0O/figures/figures_15_1.jpg", "caption": "Figure 13: S\u00b5Par ensures stable optimal weight initialization standard deviation, unlike SP, \u03bcP, \u03bcP + SuPar init only, and \u03bcP + S\u00b5Par LR only.", "description": "This figure compares the weight initialization standard deviation across different sparsity levels for four different parameterizations: SP, \u03bcP, \u03bcP + SuPar init only (using SuPar initialization but \u03bcP learning rate), and \u03bcP + S\u00b5Par LR only (using SuPar learning rate but \u03bcP initialization). The x-axis represents \u03c3w at initialization, and the y-axis represents training loss.  The plot demonstrates that only SuPar achieves stable optimal weight initialization standard deviation across all sparsity levels.", "section": "4.1 Sparse hyperparameter transfer"}, {"figure_path": "OWmu3QOa0O/figures/figures_15_2.jpg", "caption": "Figure 14: S\u00b5Par ensures stable optimal learning rate (Bottom), unlike SP, \u00b5P, \u00b5P + S\u00b5Par init only, and \u00b5P + S\u00b5Par LR only.", "description": "This figure compares the performance of different parameterization methods (SP, \u00b5P, \u00b5P + SuPar init only, \u00b5P + SuPar LR only, and SuPar) across various sparsity levels.  Each line represents a different sparsity level, and the red dot on each line indicates the optimal learning rate for that sparsity level. The figure demonstrates that SuPar achieves a stable optimal learning rate across all sparsity levels, unlike the other methods, where the optimal learning rate shifts significantly as sparsity changes. This highlights SuPar's ability to maintain stable training dynamics despite varying sparsity.", "section": "4 SuPar Training Results"}]