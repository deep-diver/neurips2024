[{"figure_path": "OWmu3QOa0O/tables/tables_5_1.jpg", "caption": "Table 1: Summary of SP, \u00b5P, and SuPar implementations.", "description": "This table summarizes the differences in the implementations of the Standard Parameterization (SP), Maximal Update Parameterization (\u00b5P), and Sparse Maximal Update Parameterization (S\u00b5Par) for various parts of a transformer model.  It shows how the initialization variance, learning rate, and forward/backward pass calculations differ for embeddings, hidden layers, and attention logits across the three parameterization methods. The table highlights how S\u00b5Par incorporates scaling factors related to model width and sparsity to stabilize training dynamics.", "section": "Implementation summary"}, {"figure_path": "OWmu3QOa0O/tables/tables_14_1.jpg", "caption": "Table 2: Downstream evaluation accuracy; higher is better: S\u00b5Par performs best or within 0.01 of best across all sparsity levels and tasks, except boolq at 50% and 75% sparsity. Even at 99% sparsity, SuPar models maintain 40%+ average accuracy, whereas the SP model drops to 34%, close to the 30% accuracy of the random baseline.", "description": "This table presents the results of evaluating several language models on five downstream tasks (ARC-easy, LAMBADA, RACE, PIQA, and BoolQ) with varying levels of sparsity (0%, 50%, 75%, 87.5%, 93.75%, 96.875%, 98.4375%, and 99.2188%).  The models were trained using three different parameterization methods: standard parameterization (SP), maximal update parameterization (\u00b5P), and sparse maximal update parameterization (S\u00b5Par). The table shows the average accuracy across the five downstream tasks for each sparsity level and parameterization method.  It highlights that S\u00b5Par consistently outperforms SP and \u00b5P, particularly at higher sparsity levels, demonstrating its effectiveness in maintaining high accuracy even with significant sparsity.", "section": "4.3 SuPar scaling to large language model pretraining"}, {"figure_path": "OWmu3QOa0O/tables/tables_18_1.jpg", "caption": "Table 1: Summary of SP, \u00b5P, and SuPar implementations.", "description": "This table summarizes the differences in how the standard parameterization (SP), maximal update parameterization (\u00b5P), and sparse maximal update parameterization (S\u00b5Par) approaches handle various hyperparameters during training, focusing on weight initialization variance, learning rates, and forward weight transformations for embedding and hidden layers.  The table highlights the key distinctions in how each method scales these parameters with respect to changes in model width and sparsity.", "section": "SuPar Training Results"}, {"figure_path": "OWmu3QOa0O/tables/tables_19_1.jpg", "caption": "Table 1: Summary of SP, \u00b5P, and SuPar implementations.", "description": "This table summarizes the differences in implementation between the standard parameterization (SP), maximal update parameterization (\u00b5P), and the proposed sparse maximal update parameterization (S\u00b5Par). It shows how the initialization variance, learning rate, and forward pass calculations differ for embeddings, hidden layers, and attention logits across the three methods, highlighting the key differences in how each approach handles the scaling of activations, gradients, and weights with respect to changes in model width and sparsity.", "section": "3 Sparse maximal update parameterization (S\u00b5Par)"}]