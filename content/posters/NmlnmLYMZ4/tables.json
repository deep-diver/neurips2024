[{"figure_path": "NmlnmLYMZ4/tables/tables_4_1.jpg", "caption": "Table 1: Base and human-aligned model performance on semantic segmentation. Aligned models largely outperform baselines, with DINO-HA achieving the highest performance across models for 4 out of 5 datasets. Note that Pascal VOC, ADE20k, and Cityscapes were included in DINOv2's retrieval pretraining. \u2020 indicates best score in the column.", "description": "This table presents the performance comparison of base and human-aligned vision models on semantic segmentation tasks across five datasets: Pascal VOC, ADE20k, Cityscapes, COCO, and DAVIS2017.  The results show the mean Intersection over Union (mIoU) and Pixel Accuracy (P.A.) for each model.  It highlights that models fine-tuned with human perceptual judgments generally outperform their baselines, particularly DINO-HA (human-aligned DINO).  The note clarifies that some of these datasets were already used in the pre-training of DINOv2, which might impact the results.", "section": "4.1 Dense Prediction"}, {"figure_path": "NmlnmLYMZ4/tables/tables_5_1.jpg", "caption": "Table 2: Human-aligned DINO and DINOv2 performance on monocular depth estimation benchmarks. Note that NYUv2 and SUN-RGBD were included in DINOv2's retrieval pretraining set, yet human-aligned DINOV2 still outperforms the base model on SUN-RGBD. Along with the results on an unseen test data domain (train on NYUv2 \u2192 test on 4D Light Field), these results demonstrate strong generalization performance of models aligned to human perceptual judgments. \u2020 indicates best score in the column.", "description": "This table presents the results of the monocular depth estimation experiments.  It compares the performance of the original DINO and DINOv2 models against their human-aligned counterparts across three datasets: NYUv2, a transfer experiment from NYUv2 to 4D Light Field, and SUN-RGBD. The metrics used for evaluation are RMSE, AbsRel, log10, \u03b4 > 1.25, \u03b4 > 1.25<sup>2</sup>, and \u03b4 > 1.25<sup>3</sup>.  The results highlight the improved performance of human-aligned models, especially on the SUN-RGBD dataset, which was included in the DINOv2 pretraining data.  The table also shows strong generalization capabilities, evidenced by the improvement even on unseen data.", "section": "4.1 Dense Prediction"}, {"figure_path": "NmlnmLYMZ4/tables/tables_6_1.jpg", "caption": "Table 3: Error comparisons for base and human-aligned models on standard counting benchmarks. Though FSC147 and CARPK have examples with extreme object counts (tens and hundreds) unseen in the NIGHTS data, human-aligned models still achieve higher performance in each pair. \u2020 indicates best score in the column, lower is better.", "description": "This table presents a comparison of the performance of base and human-aligned vision models on three object counting benchmarks: FSC147, CARPK, and Clevr-Count.  The results show the mean absolute error (MAE) and root mean squared error (RMSE) for each model.  Despite the fact that the FSC147 and CARPK datasets contain object counts far exceeding those in the training data, human-aligned models consistently outperform their base counterparts.", "section": "4.3 Counting"}, {"figure_path": "NmlnmLYMZ4/tables/tables_7_1.jpg", "caption": "Table 4: Top-1, -3, and -5 recall scores for instance retrieval on DeepFashion2. \u2020 indicates best score in the column, higher is better.", "description": "This table presents the performance of various vision models (both base and human-aligned versions) on the DeepFashion2 instance retrieval benchmark. The benchmark involves retrieving images containing similar clothing items from a gallery of images given a query image.  The table shows the top-1, top-3, and top-5 recall scores, indicating the percentage of queries where the correct matching image was found within the top 1, 3, or 5 retrieved images, respectively. Higher recall scores denote better performance.", "section": "4.4 Retrieval-augmented generation"}, {"figure_path": "NmlnmLYMZ4/tables/tables_15_1.jpg", "caption": "Table 1: Base and human-aligned model performance on semantic segmentation. Aligned models largely outperform baselines, with DINO-HA achieving the highest performance across models for 4 out of 5 datasets. Note that Pascal VOC, ADE20k, and Cityscapes were included in DINOv2's retrieval pretraining. \u2020 indicates best score in the column.", "description": "This table presents the results of semantic segmentation experiments comparing the performance of base and human-aligned vision models (DINO and DINOv2) across five datasets.  It shows that aligning models to human perceptual judgments improves performance on most datasets, particularly DINO-HA, with some exceptions possibly due to datasets already present in the training of the DINOv2 model.", "section": "4.1 Dense Prediction"}, {"figure_path": "NmlnmLYMZ4/tables/tables_16_1.jpg", "caption": "Table 6: Performance on VTAB structured subset.", "description": "This table presents the performance of various vision models (both base and human-aligned versions) on a subset of the VTAB benchmark focusing on structured datasets.  The results show the accuracy of each model on several tasks that evaluate different aspects of visual understanding, such as object counting, distance estimation, and scene layout analysis. The table helps in evaluating the impact of aligning vision models to human perceptual judgments on these specific structured tasks.", "section": "4.1 Dense Prediction"}, {"figure_path": "NmlnmLYMZ4/tables/tables_18_1.jpg", "caption": "Table 2: Human-aligned DINO and DINOv2 performance on monocular depth estimation benchmarks. Note that NYUv2 and SUN-RGBD were included in DINOv2's retrieval pretraining set, yet human-aligned DINOV2 still outperforms the base model on SUN-RGBD. Along with the results on an unseen test data domain (train on NYUv2 \u2192 test on 4D Light Field), these results demonstrate strong generalization performance of models aligned to human perceptual judgments. \u2020 indicates best score in the column.", "description": "This table presents the results of monocular depth estimation experiments using DINO and DINOv2 models, both with and without human perceptual alignment.  The results are shown for different metrics (RMSE, AbsRel, log10, \u03b4 > 1.25, \u03b4 > 1.252, \u03b4 > 1.253) and datasets (NYUv2, NYUv2 \u2192 4D LF, SUN-RGBD).  The '+' symbol indicates that the aligned model outperforms the baseline. The key finding is that human-aligned models not only perform better on in-distribution datasets but also generalize well to out-of-distribution datasets.  The table highlights that alignment with human perception improves the model's depth estimation ability and its generalization capabilities.", "section": "4.1 Dense Prediction"}, {"figure_path": "NmlnmLYMZ4/tables/tables_20_1.jpg", "caption": "Table 1: Base and human-aligned model performance on semantic segmentation. Aligned models largely outperform baselines, with DINO-HA achieving the highest performance across models for 4 out of 5 datasets. Note that Pascal VOC, ADE20k, and Cityscapes were included in DINOv2's retrieval pretraining. \u2020 indicates best score in the column.", "description": "This table presents the results of semantic segmentation experiments.  It compares the performance of baseline models (DINO and DINOv2) against their human-aligned counterparts (DINO-HA and DINOv2-HA) across five datasets: Pascal VOC, ADE20k, Cityscapes, COCO, and DAVIS2017. The table shows that human alignment generally improves performance, especially for DINO-HA, with improvements in mIoU (mean Intersection over Union) and PA (Pixel Accuracy).  It notes that three of the datasets were already used in the pretraining of the DINOv2 model, which might affect the results.", "section": "4.1 Dense Prediction"}, {"figure_path": "NmlnmLYMZ4/tables/tables_20_2.jpg", "caption": "Table 1: Base and human-aligned model performance on semantic segmentation. Aligned models largely outperform baselines, with DINO-HA achieving the highest performance across models for 4 out of 5 datasets. Note that Pascal VOC, ADE20k, and Cityscapes were included in DINOv2's retrieval pretraining. \u2020 indicates best score in the column.", "description": "This table presents the results of semantic segmentation experiments using both base and human-aligned vision models.  It compares the performance (mIoU and Pixel Accuracy) of different models on five standard datasets (Pascal VOC, ADE20K, Cityscapes, COCO, and DAVIS2017).  The results demonstrate that aligning vision models with human perceptual judgments generally improves performance on semantic segmentation tasks, particularly for the DINO-HA model.  The table notes that three of the datasets were used in the pretraining of one of the base models (DINOv2), which could be a confounding factor.", "section": "4.1 Dense Prediction"}]