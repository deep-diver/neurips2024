[{"figure_path": "gUEBXGV8JM/figures/figures_3_1.jpg", "caption": "Figure 1: The overall architecture of MambaNO.", "description": "This figure shows the overall architecture of the Mamba Neural Operator (MambaNO).  It is a U-Net-like architecture with multiple stages of downsampling and upsampling.  Each stage involves two convolution integrations and two Mamba integrations.  The Mamba integration is a novel integral form proposed in this paper, which aims to balance global and local information processing for efficient PDE solving. The state space model (SSM) is also integrated into the architecture to capture global information effectively. Activation functions (Act) are applied after each integration and normalization step.  The architecture combines convolution and Mamba integrations to approximate a well-behaved mapping from input function spaces (boundary conditions) to output function spaces (solutions of PDEs). The different blocks in the diagram represent the various operations performed during inference.", "section": "3 Mamba Neural Operator"}, {"figure_path": "gUEBXGV8JM/figures/figures_7_1.jpg", "caption": "Figure 1: The overall architecture of MambaNO.", "description": "This figure shows the overall architecture of the proposed Mamba Neural Operator (MambaNO). It illustrates the multiple stages involved in processing the input function u, which undergoes multiple convolution and Mamba integrations, downsampling and upsampling operations, and finally yields an output function G(u).  Each layer includes a state space model (SSM) for kernel integration, ensuring the capture of comprehensive function features, both global and local. The architecture is U-shaped, reflecting the encoder-decoder structure that maintains a balance between holistic feature representation and detailed local information.", "section": "3 Mamba Neural Operator"}, {"figure_path": "gUEBXGV8JM/figures/figures_8_1.jpg", "caption": "Figure 4: Left & Center: Test errors vs. Resolutions. Right: Errors vs. Training samples.", "description": "This figure demonstrates the resolution invariance and data efficiency of the proposed MambaNO model compared to other models (UNet, FNO, CNO). The left and center panels show that MambaNO and CNO exhibit more stable performance across different resolutions compared to UNet and FNO, indicating better robustness to variations in input data resolution.  The right panel illustrates that MambaNO achieves high accuracy with fewer training samples, demonstrating superior data efficiency.", "section": "4.3 Resolution Invariance"}, {"figure_path": "gUEBXGV8JM/figures/figures_18_1.jpg", "caption": "Figure 1: The overall architecture of MambaNO.", "description": "This figure shows the overall architecture of the Mamba Neural Operator (MambaNO). It is a U-shaped architecture consisting of multiple layers including convolution integration, Mamba integration, downsampling, upsampling, and activation layers. The Mamba integration layer, which is a key component of MambaNO, is based on the state space model of Mamba and scans the entire function to capture global information. The convolution integration layer captures local information. The combination of these two types of integration enables MambaNO to effectively approximate operators from universal PDEs. The architecture uses a combination of convolutional layers and Mamba integration layers in a U-net like fashion with skip connections for downsampling and upsampling of the input feature map.", "section": "3 Mamba Neural Operator"}, {"figure_path": "gUEBXGV8JM/figures/figures_19_1.jpg", "caption": "Figure 3: Visual predictions on representative in- (top row) and out-of-distribution (bottom row).", "description": "This figure shows visual comparisons of predictions made by FNO, CNO, and MambaNO on representative in-distribution and out-of-distribution examples.  The top row shows in-distribution examples, and the bottom row shows out-of-distribution examples.  It demonstrates the performance of each model for both in-distribution and out-of-distribution data. The visual comparison shows that MambaNO generally produces predictions closest to the ground truth, particularly in the areas marked by a black box.", "section": "4 Experiments and Analysis"}, {"figure_path": "gUEBXGV8JM/figures/figures_19_2.jpg", "caption": "Figure 1: The overall architecture of MambaNO.", "description": "This figure shows the overall architecture of the Mamba Neural Operator (MambaNO). It illustrates the different components of the model, including the convolution integration layers, Mamba integration layers, downsampling and upsampling operations, and activation functions. The figure also highlights the use of a state space model (SSM) within the Mamba integration module.", "section": "3 Mamba Neural Operator"}, {"figure_path": "gUEBXGV8JM/figures/figures_20_1.jpg", "caption": "Figure 3: Visual predictions on representative in- (top row) and out-of-distribution (bottom row).", "description": "This figure shows visual comparisons of predictions made by FNO, CNO, and MambaNO on the Navier-Stokes equation for both in-distribution and out-of-distribution datasets. The top row displays in-distribution results, while the bottom row presents out-of-distribution results. Each column represents a different model's prediction, with the ground truth in the second column. The figure visually demonstrates the superior performance of MambaNO, especially in the out-of-distribution setting, where it more closely matches the ground truth.", "section": "4 Experiments and Analysis"}, {"figure_path": "gUEBXGV8JM/figures/figures_21_1.jpg", "caption": "Figure 3: Visual predictions on representative in- (top row) and out-of-distribution (bottom row).", "description": "This figure shows visual comparisons of predictions made by FNO, CNO, and MambaNO on representative PDEs. The top row displays in-distribution predictions while the bottom row shows out-of-distribution predictions, highlighting the model's ability to generalize to unseen data.  The visual comparison allows for an intuitive understanding of the performance differences between the three models.", "section": "4 Experiments and Analysis"}, {"figure_path": "gUEBXGV8JM/figures/figures_21_2.jpg", "caption": "Figure 1: The overall architecture of MambaNO.", "description": "This figure presents a detailed architecture of the proposed Mamba Neural Operator (MambaNO). It illustrates the flow of data through various layers, including multiple Mamba integrations and convolutional integrations, downsampling and upsampling operations, and activation functions.  The architecture shows a U-Net-like structure with skip connections, demonstrating the balance between local and global integration achieved by the model. The figure visually represents the integration of a state space model (SSM) into the neural operator architecture for efficient global information processing, combined with local convolutional layers for capturing fine-grained details in the function.", "section": "3 Mamba Neural Operator"}, {"figure_path": "gUEBXGV8JM/figures/figures_23_1.jpg", "caption": "Figure 1: The overall architecture of MambaNO.", "description": "This figure shows the overall architecture of the Mamba Neural Operator (MambaNO).  It's a U-Net like architecture with several stages of downsampling and upsampling.  The core components are convolutional integration layers, Mamba integration layers, and activation layers.  Mamba integration is a novel technique proposed in the paper which combines local and global information from the input function.", "section": "3 Mamba Neural Operator"}, {"figure_path": "gUEBXGV8JM/figures/figures_23_2.jpg", "caption": "Figure 1: The overall architecture of MambaNO.", "description": "This figure presents a detailed illustration of the Mamba Neural Operator (MambaNO) architecture. It visually outlines the distinct components and their arrangement within the model's structure, including the sequence of convolution and Mamba integration layers, downsampling and upsampling operations, the state space model (SSM) for mamba integration, and the utilization of activation functions. This visualization effectively clarifies how the model processes data through the layers, ultimately producing the desired output.  The diagram showcases the flow of information from the initial input to the final output, highlighting the key functional blocks and their interconnectedness.  This is an important figure in understanding the unique design and internal workings of the proposed MambaNO.", "section": "3 Mamba Neural Operator"}, {"figure_path": "gUEBXGV8JM/figures/figures_26_1.jpg", "caption": "Figure 3: Visual predictions on representative in- (top row) and out-of-distribution (bottom row).", "description": "This figure shows visual comparisons of predictions made by FNO, CNO, and MambaNO on the Navier-Stokes equation for both in-distribution and out-of-distribution data. It highlights the superior performance of MambaNO in accurately capturing the complex flow patterns, particularly in the out-of-distribution cases where other methods struggle.", "section": "4 Experiments and Analysis"}]