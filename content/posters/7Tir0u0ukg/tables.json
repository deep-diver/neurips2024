[{"figure_path": "7Tir0u0ukg/tables/tables_2_1.jpg", "caption": "Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as the cumulative regret divided by the total number of samples (transition pairs) used by the algorithm. Here d is the feature dimension, H is the episode length, K is the number of episodes, and M is the number of agents in a multi-agent setting.", "description": "This table compares the performance of various algorithms (including the authors' proposed CoopTS-PHE and CoopTS-LMC) on episodic, non-stationary, linear Markov Decision Processes (MDPs).  It shows the theoretical regret bounds, average regret, whether the algorithm uses randomized exploration, generalizability to deep reinforcement learning, and communication complexity for both single-agent and multi-agent settings.  The table highlights the superior performance of the authors' methods in terms of regret and communication complexity.", "section": "Experiments"}, {"figure_path": "7Tir0u0ukg/tables/tables_57_1.jpg", "caption": "Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as the cumulative regret divided by the total number of samples (transition pairs) used by the algorithm. Here d is the feature dimension, H is the episode length, K is the number of episodes, and M is the number of agents in a multi-agent setting.", "description": "This table compares various single-agent and multi-agent reinforcement learning algorithms on episodic, non-stationary, linear Markov Decision Processes (MDPs).  It shows the theoretical regret bounds (cumulative and average), whether the algorithm uses randomized exploration, whether it's generalizable to deep RL, and its communication complexity. The table helps illustrate the efficiency and scalability of the proposed CoopTS-PHE and CoopTS-LMC algorithms compared to existing methods.", "section": "Experiments"}, {"figure_path": "7Tir0u0ukg/tables/tables_57_2.jpg", "caption": "Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as the cumulative regret divided by the total number of samples (transition pairs) used by the algorithm. Here d is the feature dimension, H is the episode length, K is the number of episodes, and M is the number of agents in a multi-agent setting.", "description": "This table compares various algorithms (including the authors' proposed CoopTS-PHE and CoopTS-LMC) on episodic, non-stationary, linear Markov Decision Processes (MDPs).  It shows the theoretical regret and average regret bounds for each algorithm, along with whether they use randomized exploration and whether they are easily generalizable to deep reinforcement learning. It also lists the communication complexity for multi-agent settings. The table helps to highlight the performance and efficiency of the proposed methods in comparison to existing approaches.", "section": "Experiments"}, {"figure_path": "7Tir0u0ukg/tables/tables_63_1.jpg", "caption": "Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as the cumulative regret divided by the total number of samples (transition pairs) used by the algorithm. Here d is the feature dimension, H is the episode length, K is the number of episodes, and M is the number of agents in a multi-agent setting.", "description": "This table compares different algorithms for episodic, non-stationary, linear Markov Decision Processes (MDPs).  It shows the theoretical regret and average regret bounds for each algorithm, along with its communication complexity and whether it supports randomized exploration and generalizes to deep RL. The algorithms are categorized by whether they are for single-agent or multi-agent settings. The table highlights the proposed CoopTS-PHE and CoopTS-LMC algorithms, showing their superior performance in communication efficiency and regret bounds.", "section": "Experiments"}, {"figure_path": "7Tir0u0ukg/tables/tables_63_2.jpg", "caption": "Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as the cumulative regret divided by the total number of samples (transition pairs) used by the algorithm. Here d is the feature dimension, H is the episode length, K is the number of episodes, and M is the number of agents in a multi-agent setting.", "description": "This table compares the performance of various algorithms (including the authors' proposed methods) on episodic, non-stationary, linear Markov Decision Processes (MDPs).  It shows the theoretical regret and average regret bounds for each algorithm, along with whether they incorporate randomized exploration and are generalizable to deep reinforcement learning.  It also provides the communication complexity for multi-agent settings.", "section": "Experiments"}, {"figure_path": "7Tir0u0ukg/tables/tables_64_1.jpg", "caption": "Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as the cumulative regret divided by the total number of samples (transition pairs) used by the algorithm. Here d is the feature dimension, H is the episode length, K is the number of episodes, and M is the number of agents in a multi-agent setting.", "description": "This table compares the performance of different algorithms on episodic, non-stationary, linear Markov Decision Processes (MDPs). It shows the theoretical regret bound, average regret, whether the algorithm uses randomized exploration, if it is generalizable to deep reinforcement learning, and the communication complexity for each algorithm. The algorithms are categorized into single-agent and multi-agent settings, and the table highlights the proposed CoopTS-PHE and CoopTS-LMC algorithms alongside existing methods.", "section": "Experiments"}]