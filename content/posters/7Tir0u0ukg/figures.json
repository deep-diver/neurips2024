[{"figure_path": "7Tir0u0ukg/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "The figure compares the performance of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN) across four different environments.  The first two subfigures (a) and (b) show results from the N-chain environment with chain length N=25 and varying numbers of agents (m=2 and m=3 respectively). The last two subfigures (c) and (d) show results from a Super Mario Bros environment in parallel and federated settings.  All results are averaged over 10 runs, and shaded areas represent standard deviations, indicating the variability in the results.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_9_1.jpg", "caption": "Figure 2: Evaluation performance at Tampa (hot humid) in building energy systems. All results are averaged over 10 runs.", "description": "This figure shows the evaluation performance of different reinforcement learning algorithms for thermal control of building energy systems in Tampa, Florida (hot and humid climate).  The algorithms compared are PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, and DDQN, along with a random action baseline. The violin plot displays the distribution of the daily return (a measure of performance) for each algorithm, averaged over 10 independent runs.  The plot illustrates the central tendency and variability of the daily return for each algorithm, enabling a comparison of their effectiveness in this real-world application.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_58_1.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "This figure compares the performance of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN) across two different environments: N-chain and Super Mario Bros.  The N-chain experiment uses a chain of 25 states, while the Super Mario Bros experiment uses a parallel setting. The results are averaged over 10 runs, with shaded areas representing standard deviations.  The plots show episode return over training episodes per agent.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_58_2.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "This figure compares the performance of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN) across two different environments: N-chain and Super Mario Bros.  The N-chain environment is a simple, deep exploration problem with 25 states. Super Mario Bros is a more complex, partially observable environment. For both environments, the plot shows the average episode return for each algorithm across 10 independent runs, with shaded areas representing the standard deviation of the results. The figure demonstrates how different exploration strategies affect performance, particularly under conditions where exploration is challenging.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_59_1.jpg", "caption": "Figure 6: Different number of agents m with different synchronization strategies as well as the single-agent and no communication settings in N = 10. Top: PHE, Bottom: LMC", "description": "This figure compares the performance of PHE and LMC algorithms under different synchronization strategies (constant, exponential, and linear) and different numbers of agents (m=2,3,4) in a N-chain environment with N=10.  It also shows results for single-agent and no-communication settings as baselines. The top row displays results for the PHE algorithm, while the bottom row shows results for the LMC algorithm.  Each subplot shows episode return over total episodes, highlighting the impact of communication and different synchronization approaches on learning performance. The results indicate that, while centralized learning remains superior, various communication strategies substantially improve performance compared to no communication.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_59_2.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "This figure compares the performance of different exploration strategies (CoopTS-PHE, CoopTS-LMC, DQN, Double DQN, Bootstrapped DQN, and NoisyNet DQN) in two different environments: N-chain and Super Mario Bros.  The N-chain results (a, b) show performance for different numbers of agents (m=2, m=3). The Super Mario Bros results (c, d) show performance with parallel and federated learning respectively. The shaded areas represent standard deviation.  Overall, the CoopTS algorithms generally outperform the DQN baselines.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_60_1.jpg", "caption": "Figure 8: Computation time with different exploration strategies.", "description": "The figure shows the computation time of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN, NeuralTS, and NeuralUCB) with varying neural network sizes (32_2, 32_3, 64_2, and 64_3). The results indicate that PHE and LMC have relatively lower computation times compared to NeuralTS and NeuralUCB, especially as the neural network size increases.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_60_2.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "This figure compares the performance of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN) across various multi-agent reinforcement learning environments.  Subfigures (a) and (b) show results for the N-chain problem with 25 states, while subfigures (c) and (d) present results for the Super Mario Bros environment.  Each result is an average of 10 independent runs, and shaded areas indicate standard deviation.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_61_1.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "This figure compares the performance of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, and DDQN) across two different environments: N-chain and Super Mario Bros.  The N-chain results show the average episode return for 2 and 3 agents over a varying number of training episodes. The Super Mario Bros. results show average episode return for the parallel and federated settings.  In all cases, the shaded area represents the standard deviation across 10 independent runs, providing a visual representation of performance consistency. The figure demonstrates the effectiveness of the proposed randomized exploration strategies.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_61_2.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "This figure compares the performance of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN) across two different environments: N-chain and Super Mario Bros.  The N-chain results (a and b) show the average episode return for 2 and 3 agents respectively, over 2000 training episodes.  The Super Mario Bros results (c and d) show the performance of parallel and federated learning across 2000 training episodes. Shaded areas represent standard deviations, indicating the consistency of the results. The figure demonstrates that the proposed randomized exploration strategies (PHE and LMC) generally outperform the existing deep Q-network baselines, particularly in the N-chain deep exploration task.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_62_1.jpg", "caption": "Figure 12: Gap reduction improvement with prioritized experience replay for parallel learning without communication. Note that the same settings with standard and prioritized experience replay are in the same-ish color.", "description": "This figure compares the performance of different multi-agent reinforcement learning methods with and without prioritized experience replay in a parallel learning setting without inter-agent communication. The x-axis represents the total number of training episodes across all agents. The y-axis shows the average episode return. The results demonstrate that using prioritized experience replay can improve the learning performance, reducing the gap between centralized and parallel settings. The lines with similar colors represent the same settings with standard and prioritized experience replay, facilitating comparison of the performance improvement.", "section": "Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_62_2.jpg", "caption": "Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): N-chain with N = 25. (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.", "description": "The figure compares the performance of different exploration strategies (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN) in two different environments: N-chain and Super Mario Bros.  The N-chain results show the average episode return for 2 and 3 agents, demonstrating the effectiveness of the proposed PHE and LMC methods. The Super Mario Bros results showcase the performance in parallel and federated learning settings. All results are averages over 10 runs, with shaded areas representing standard deviations.", "section": "5 Experiments"}, {"figure_path": "7Tir0u0ukg/figures/figures_64_1.jpg", "caption": "Figure 14: Evaluation performance at different cities in building energy systems", "description": "This figure compares the performance of various reinforcement learning algorithms (PHE, LMC, DQN, Bootstrapped DQN, NoisyNet DQN, DDQN) on a building energy system control task across four different cities with varying weather conditions (Tampa - hot humid, Tucson - hot dry, Rochester - cold humid, Great Falls - cold dry). The y-axis represents the daily return, which is a measure of the algorithm's performance in terms of minimizing energy consumption while meeting temperature specifications.  The x-axis represents the different algorithms. Violin plots are used to show the distribution of the daily return for each algorithm across multiple runs. The figure demonstrates the performance of the proposed randomized exploration strategies (PHE, LMC) compared to other baselines, showcasing their robustness across diverse environmental settings.  Note that some algorithms (DQN, NoisyNet) perform poorly, suggesting that the discrete action space makes learning harder in this environment. ", "section": "5 Experiments"}]