[{"figure_path": "3l2HnZXNou/figures/figures_3_1.jpg", "caption": "Figure 1: (a) Payoff matrix for a one-step game. There are multiple local optima. (b) Evaluations of different methods for the game in terms of the mean reward and standard deviation of ten runs. A\u2192B, B\u2192A, Simultaneous, and Learned represent that agent A makes decisions first, agent B makes decisions first, two agents make decisions simultaneously, and there is another learned policy determining the priority of decision making, respectively. MAPPO [Yu et al., 2021] is used as the backbone.", "description": "This figure demonstrates a simple one-step game with multiple local optima and compares four different decision-making methods: A decides first, B decides first, simultaneous decisions, and a learned priority policy.  The graph shows the average reward and standard deviation across ten runs of each method, illustrating how the priority of decision-making impacts performance.", "section": "3 Problem Formulation"}, {"figure_path": "3l2HnZXNou/figures/figures_3_2.jpg", "caption": "Figure 1: (a) Payoff matrix for a one-step game. There are multiple local optima. (b) Evaluations of different methods for the game in terms of the mean reward and standard deviation of ten runs. A\u2192B, B\u2192A, Simultaneous, and Learned represent that agent A makes decisions first, agent B makes decisions first, two agents make decisions simultaneously, and there is another learned policy determining the priority of decision making, respectively. MAPPO [Yu et al., 2021] is used as the backbone.", "description": "This figure shows a simple 2-agent game with multiple local optima and compares four different methods for learning the optimal strategy: Agent A decides first, Agent B decides first, both agents decide simultaneously, and a learned policy decides the order.  The graph (b) displays the average reward for each method over ten runs, showing the impact of decision-making order on performance.", "section": "3 Problem Formulation"}, {"figure_path": "3l2HnZXNou/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of SeqComm. SeqComm has two communication phases, the negotiation phase (left) and the launching phase (right). In the negotiation phase, agents communicate hidden states of observations with others and obtain their own intention. The priority of decision-making is determined by sharing and comparing the value of all the intentions. In the launching phase, the agents who hold the upper-level positions will make decisions prior to the lower-level agents. Besides, their actions will be shared with anyone that has not yet made decisions.", "description": "This figure illustrates the two-phase communication scheme of SeqComm.  The negotiation phase shows agents communicating hidden state observations to determine decision-making priority based on intention values. The launching phase depicts upper-level agents making decisions first, communicating their actions to lower-level agents, and all agents executing actions simultaneously. The figure uses color-coding and arrows to represent information flow and decision-making order.", "section": "4 Sequential Communication"}, {"figure_path": "3l2HnZXNou/figures/figures_6_1.jpg", "caption": "Figure 3: Architecture of SeqComm. The critic and policy of each agent take input as its own observation and received messages. The world model takes as input the joint hidden states and predicted joint actions.", "description": "This figure illustrates the architecture of the SeqComm model. Each agent has a policy network, a critic network, and an attention module. The attention module takes in the hidden states of other agents as well as received messages from other agents. The policy network and critic network take in the agent's own observations, as well as the information from the attention module. The output from the agent's policy network is an action that is sent to the world model. The world model takes in the joint hidden states and predicted joint actions from all agents and outputs the next joint observations and rewards. This figure also shows that the agents' decision-making is sequential, and the agents' actions are executed simultaneously. ", "section": "4.4 Theoretical Analysis"}, {"figure_path": "3l2HnZXNou/figures/figures_7_1.jpg", "caption": "Figure 4: Learning curves of SeqComm and baselines in nine SMACv2 maps.", "description": "This figure presents the learning curves for SeqComm and several baseline methods across nine different maps in the StarCraft Multi-Agent Challenge v2 (SMACv2) environment. Each curve represents the average win rate over multiple training steps for a specific method on each map.  The maps are categorized by race (Protoss, Terran, Zerg) and number of agents (5 vs 5, 10 vs 10, 10 vs 11). The figure helps visualize the performance of SeqComm in comparison to other methods across various scenarios within the SMACv2 environment. The shaded areas around the lines represent standard deviation. ", "section": "5 Experiments"}, {"figure_path": "3l2HnZXNou/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation studies of the communication ranges.", "description": "This figure shows the ablation study of communication ranges conducted on two maps: protoss_10_vs_10 and terran_10_vs_10.  The performance of SeqComm is evaluated under different communication ranges (1, 3, 6, and 9), as well as the local communication version of SeqComm. The results demonstrate the impact of communication range on the performance of the model. A wider communication range generally leads to improved performance, but it also increases communication overhead.  MAPPO (communication-free) is shown as a baseline for comparison.", "section": "5 Experiments"}, {"figure_path": "3l2HnZXNou/figures/figures_8_2.jpg", "caption": "Figure 4: Learning curves of SeqComm and baselines in nine SMACv2 maps.", "description": "This figure compares the performance of SeqComm against several baseline methods across nine different maps in the StarCraft Multi-Agent Challenge (SMACv2) environment.  The x-axis represents the number of training steps (in millions), and the y-axis represents the average win rate. Each line represents a different algorithm or variant, and shaded areas represent standard deviations. The maps used are diverse, testing the algorithms under various scenarios and scales.", "section": "5 Experiments"}, {"figure_path": "3l2HnZXNou/figures/figures_9_1.jpg", "caption": "Figure 7: Ablation studies under local communication in SMACv2.", "description": "This figure presents the learning curves for different variants of SeqComm and baselines in nine SMACv2 maps. It compares the full communication version of SeqComm (SeqComm), a local communication version (SeqComm (local)), a version with random priority of decision making (SeqComm (random)), and a version with no action (SeqComm (no action)). The results show that SeqComm consistently outperforms the baselines and that the local communication version performs comparably well. The figure also demonstrates the importance of asynchronous decision making and the proper priority of decision making for achieving better coordination.", "section": "5.2 Ablation Studies"}, {"figure_path": "3l2HnZXNou/figures/figures_20_1.jpg", "caption": "Figure 8: Results of experiments with extra baseline algorithms.", "description": "The figure compares SeqComm with MAIC and CommFormer across six different maps in StarCraft II.  Each subplot represents a different map (Protoss, Terran, and Zerg, in 5v5 and 10v10 scenarios). The y-axis shows the win rate, and the x-axis shows the training steps. Error bars are included. The results demonstrate SeqComm's superior performance compared to the baselines in various multi-agent cooperative scenarios.", "section": "5 Experiments"}, {"figure_path": "3l2HnZXNou/figures/figures_20_2.jpg", "caption": "Figure 9: Illustration of the Emergence of Behavioral Patterns.", "description": "This figure shows a series of screenshots from a StarCraft II game illustrating the emergent behavior of agents using the SeqComm algorithm.  The screenshots show how the agents coordinate their actions over time. Initially, the agents act somewhat independently.  Then, through a negotiation phase of communication, they establish a hierarchy (indicated by the levels), allowing them to coordinate and focus their attacks. The screenshots highlight the transition from chaotic behavior to coordinated attacks on the enemy units.", "section": "5.2 Ablation Studies"}]