[{"figure_path": "TA5zPfH8iI/figures/figures_1_1.jpg", "caption": "Fig. 1: B-cosification: Obtaining inherently interpretable models with competitive accuracy at low cost. Left: Accuracy progression over epochs for a DenseNet-121 and a ViT-S, comparing B-cosified (blue) and B-cos (orange) training curves. B-cosified models achieve equivalent accuracy with a substantial reduction in training time, yielding 4.7x speedup for DenseNet-121 and 9.0x speedup for ViT-S. Right: Qualitative comparison of explanations for various images for B-cos [10] and our B-cosified models at various stages of training. Specifically, we show the dynamic linear mappings W(x) computed by the models in color as in [9, 10]; note that by formulating conventional models ('initial' in the plot) as a specific version of B-cos models, we are able to visualise the corresponding explanations in color too, see Sec. 3.2.1 for further details. We find that after only one epoch of training, the B-cosified models exhibit similar explanations as B-cos models.", "description": "This figure demonstrates the effectiveness of B-cosification in achieving high accuracy and interpretability with significantly reduced training time. The left panel shows training curves for DenseNet-121 and ViT-S, comparing the B-cosified approach (blue) to the standard B-cos approach (orange). The B-cosified method achieves comparable accuracy much faster. The right panel provides a qualitative comparison of the explanations generated by both methods across different stages of training, visually illustrating the similarity of explanations obtained by B-cosification and the standard B-cos method.", "section": "B-cosification Results"}, {"figure_path": "TA5zPfH8iI/figures/figures_2_1.jpg", "caption": "Fig. 2: B-cosified CLIP Models. After B-cosifying a CLIP model and fine-tuning it according to our proposed B-cosification scheme, see Sec. 3.2, we find that it is possible to endow the model with the same level of inherent interpretability as the B-cos models proposed in [10], whilst maintaining CLIP\u2019s zeroshot ability (see Fig. 5). The resulting linear summaries of the models (W(x)) can be visualised in color (row 3) and provide significantly more detail than GradCAM explanations (row 2), which are often used to explain conventional CLIP models.", "description": "This figure shows a comparison of the inherent interpretability of a B-cosified CLIP model and a standard CLIP model.  The B-cosified model, fine-tuned using the authors' method, produces more detailed and interpretable linear summaries (visualized in color in the bottom row) compared to GradCAM explanations (middle row) for the same images (top row). The B-cosified CLIP retains zero-shot capabilities.", "section": "4.2 B-cosifying CLIP Towards Inherently Interpretable Foundation Models"}, {"figure_path": "TA5zPfH8iI/figures/figures_7_1.jpg", "caption": "Fig. 3: Localisation Performance of W(x)x. We compute the contribution maps according to the dynamic linear summaries W(x) of the pre-trained models ('Standard'), their B-cosified versions, and the original pre-trained B-cos models and evaluate their localisation performance on the Grid Pointing Game as in [10]. We find localisation to significantly improve for B-cosified models, achieving results on par with the models of [10].", "description": "This figure compares the localization performance of the dynamic linear summaries W(x) of pre-trained models, their B-cosified versions, and the original B-cos models. The localization performance is evaluated using the Grid Pointing Game metric from reference [10].  The results show that B-cosified models significantly improve localization compared to standard pre-trained models, achieving similar results to the B-cos models trained from scratch.  The figure uses bar graphs to represent the localization scores for each model on different architectures (ResNet-18, ResNet-50-v1, ResNet-50-v2, DenseNet-121, ViT-Ti, ViT-S, ViT-B, ViT-L, and their convolutional counterparts). Numerical values showing improvement are also displayed.", "section": "4.1 Supervised Classification Models"}, {"figure_path": "TA5zPfH8iI/figures/figures_7_2.jpg", "caption": "Fig. 4: Comparison to Post-hoc Methods. For two of the models in Fig. 3 (ResNet-50-v1, DenseNet-121) we compare the localisation performance of the dynamic matrices W(x)x to post-hoc explanations for the pre-trained models. Similar to the original B-cos models [10], the model-inherent explanations perform favourably.", "description": "This figure compares the localization performance of model-inherent explanations (W(x)x) with several post-hoc explanation methods for ResNet-50-v1 and DenseNet-121 models.  It shows that model-inherent explanations significantly outperform post-hoc methods in terms of localization accuracy, achieving results comparable to the original B-cos models.", "section": "4.1 Supervised Classification Models"}, {"figure_path": "TA5zPfH8iI/figures/figures_8_1.jpg", "caption": "Fig. 5: Classification performance on the CLIP Benchmark [14] of various CLIP models for the zero-shot setting (left) and linear probing (right). Specifically, we compare two B-cosified CLIPs\u2014trained on ImageNet (IMN) and CC3M respectively\u2014to the Text2Concept approach by [29] and the original pre-trained CLIP model. We find B-cosified versions of CLIP to consistently outperform Text2Concept on natural and specialised data.", "description": "This figure compares the zero-shot and linear probe classification accuracy of different CLIP models on the CLIP Benchmark dataset.  The models include the standard pre-trained CLIP, the Text2Concept model, and two B-cosified CLIP models trained on ImageNet and CC3M respectively.  The results show that the B-cosified CLIP models generally outperform the Text2Concept model, particularly on natural and specialized data, indicating the effectiveness of the B-cosification technique in improving CLIP's performance.", "section": "4.2 B-cosifying CLIP Towards Inherently Interpretable Foundation Models"}, {"figure_path": "TA5zPfH8iI/figures/figures_9_1.jpg", "caption": "Fig. 6: CLIP Localisation. In (a), we compare GradCAM and dynamic linear explanations for the pre-trained CLIP model to the inherent explanations of the B-cosified CLIP, as well as to our proposed B-cos FeatureCLIP approach. We find that good localisation with highly detailed explanations (b) is possible via the B-cosified CLIP models, especially for high cosine powers, despite only explaining the similarity to text prompts.", "description": "This figure shows a comparison of different methods for localizing objects in images using the CLIP model.  Panel (a) presents a graph comparing the EPG (Explanations by Pixel-wise Gradient) score, a measure of localization accuracy, for four methods: B-cosified FeatureCLIP, CLIP GradCAM, CLIP W(x)x (dynamic linear summaries of CLIP), and B-cosified CLIP.  The graph shows the EPG score as a function of cosine power (a hyperparameter that controls the strength of alignment between image and text embeddings).  Panel (b) shows qualitative examples of localization using the B-cosified CLIP model, illustrating the ability of the method to accurately and precisely locate objects within images by using text prompts.", "section": "4.2 B-cosifying CLIP Towards Inherently Interpretable Foundation Models"}, {"figure_path": "TA5zPfH8iI/figures/figures_14_1.jpg", "caption": "Fig. 1: B-cosification: Obtaining inherently interpretable models with competitive accuracy at low cost.", "description": "This figure compares the performance of B-cosified models versus standard and B-cos models. It shows accuracy progression over epochs for a DenseNet-121 and a ViT-S, demonstrating that B-cosified models achieve similar accuracy to B-cos models with significantly reduced training time. Qualitative comparison of explanations from various image examples for B-cos and B-cosified models are also included, highlighting the similarity in explanations obtained after just one epoch of training.", "section": "B-cosification Results"}, {"figure_path": "TA5zPfH8iI/figures/figures_14_2.jpg", "caption": "Fig. A2: Additional randomly chosen examples highlighting the effect of increasing cosine power p on the specificity of the explanations in the B-cosified CLIP model. Each row corresponds to a specific object class, with explanations generated at different cosine power levels: cos, cos-7, cos-19, and cos-inf. Higher cosine power values result in increasingly precise and interpretable representations, capturing finer details and producing sharper focus on class-relevant features; further examples in Fig. 6b in the main paper.", "description": "This figure displays additional qualitative examples to demonstrate the effect of increasing the cosine power (p) in the B-cosified CLIP model. The images show that with higher cosine power, the explanations become more focused and interpretable. Fine details, often missed in the original CLIP explanations, are now highlighted, leading to a better understanding of the model's decision-making process.", "section": "A Additional Qualitative Results"}, {"figure_path": "TA5zPfH8iI/figures/figures_15_1.jpg", "caption": "Fig. 1: B-cosification: Obtaining inherently interpretable models with competitive accuracy at low cost. Left: Accuracy progression over epochs for a DenseNet-121 and a ViT-S, comparing B-cosified (blue) and B-cos (orange) training curves. B-cosified models achieve equivalent accuracy with a substantial reduction in training time, yielding 4.7x speedup for DenseNet-121 and 9.0x speedup for ViT-S. Right: Qualitative comparison of explanations for various images for B-cos [10] and our B-cosified models at various stages of training. Specifically, we show the dynamic linear mappings W(x) computed by the models in color as in [9, 10]; note that by formulating conventional models ('initial' in the plot) as a specific version of B-cos models, we are able to visualise the corresponding explanations in color too, see Sec. 3.2.1 for further details. We find that after only one epoch of training, the B-cosified models exhibit similar explanations as B-cos models.", "description": "This figure demonstrates the effectiveness of B-cosification in achieving both high accuracy and interpretability with significantly reduced training time compared to traditional B-cos models.  The left panel shows training curves, highlighting the faster convergence of B-cosified models. The right panel provides a qualitative comparison of explanations generated by B-cos and B-cosified models, illustrating the similarity of explanations achieved by B-cosified models after just one epoch of training.", "section": "B-cosification Results"}]