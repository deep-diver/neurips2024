[{"figure_path": "TA5zPfH8iI/tables/tables_4_1.jpg", "caption": "Table 1: Overview. To allow for comparing the models, we compiled the identified differences between the conventional models (Standard), their B-cosified version (B-cosified) and the original B-cos models (B-cos). For each design choice in the B-cosified models, we summarise the respective discussion in Sec. 3.2 (reason)).", "description": "This table summarizes the key differences between standard deep neural networks (DNNs), B-cos networks, and the proposed B-cosified networks.  It highlights aspects such as image encoding, input normalization, weight normalization, activation functions, biases, and the hyperparameter B in the B-cos transformation. The \"reason\" column briefly explains the motivation behind each design choice in B-cosified networks to achieve inherent interpretability. The table facilitates a clear understanding of the transformations involved in converting standard DNNs into B-cos networks and how the B-cosification technique bridges these differences.", "section": "3.2 B-cosification of Deep Neural Networks"}, {"figure_path": "TA5zPfH8iI/tables/tables_5_1.jpg", "caption": "Table 2: Increasing B for B-cosification.", "description": "This table presents ablation results on how to increase the B hyperparameter in the B-cosification process.  It compares different strategies for increasing B, including setting B to a discrete value, linearly interpolating B over a set number of epochs, or learning B as a trainable parameter. The results are evaluated in terms of accuracy and localization performance, using ResNet-18 as the model. The table shows that setting B=2 immediately yields good results and outperforms linear approaches.", "section": "3.2.2 Fine-tuning for Interpretability"}, {"figure_path": "TA5zPfH8iI/tables/tables_6_1.jpg", "caption": "Table 3: Decreasing biases for B-cosification.", "description": "This table presents ablation study results on the impact of removing biases for B-cosification. It compares different strategies for decreasing the bias parameter across various convolutional models and evaluates their performance using accuracy and localization scores. The baselines include the standard model and the B-cos model.  The strategies include removing all biases, using bias decay with different values (0.2, 0.5, 0.9), and maintaining the bias as in the original model.  The table shows the results separately for accuracy and localization performance metrics for each strategy and model.", "section": "3.2.2 Fine-tuning for Interpretability"}, {"figure_path": "TA5zPfH8iI/tables/tables_6_2.jpg", "caption": "Table 4: Classification Accuracy. We report the top-1 classification accuracy on the ImageNet validation set of the pre-trained models (pretrained) and the B-cosified models (B-cosified) after fine-tuning them. Additionally, we report the accuracy of the corresponding B-cos models trained from scratch (B-cos) as well as the difference to them (acc), and how much faster and at which epoch (t) the same accuracy as in [10] was achieved (speedup). Results for B-cosified models are averaged over three runs; full results including standard deviation in appendix.", "description": "This table presents a comparison of the top-1 accuracy achieved on the ImageNet validation set by three different model types: pre-trained models, B-cos models (trained from scratch), and B-cosified models (fine-tuned from pre-trained models).  For each model type, the accuracy is reported, along with the difference in accuracy compared to the B-cos models.  Additionally, the table shows how much faster the B-cosified models reached a comparable accuracy to the B-cos models and at which epoch this was achieved, showcasing the efficiency gains of the B-cosification method.", "section": "4.1 Supervised Classification Models"}, {"figure_path": "TA5zPfH8iI/tables/tables_16_1.jpg", "caption": "Table 4: Classification Accuracy. We report the top-1 classification accuracy on the ImageNet validation set of the pre-trained models (pretrained) and the B-cosified models (B-cosified) after fine-tuning them. Additionally, we report the accuracy of the corresponding B-cos models trained from scratch (B-cos) as well as the difference to them (acc), and how much faster and at which epoch (t) the same accuracy as in [10] was achieved (speedup). Results for B-cosified models are averaged over three runs; full results including standard deviation in appendix.", "description": "This table compares the classification accuracy on ImageNet for three types of models: pre-trained models, B-cosified models (the models from the paper), and B-cos models trained from scratch.  It highlights that B-cosified models achieve comparable or even superior accuracy to both pre-trained and B-cos models, often at a fraction of the training cost (indicated by the speedup).  The \"acc\" column shows the difference in accuracy compared to the B-cos models, providing insight into the performance improvement gained through B-cosification.", "section": "4.1 Supervised Classification Models"}, {"figure_path": "TA5zPfH8iI/tables/tables_17_1.jpg", "caption": "Table B2: Ablation normed weights for the ViT models. Extending Tab. 4 for a subset of models, we report the top-1 classification accuracy on the ImageNet validation set of the pre-trained models (pretrained) and the B-cosified models (B-cosified) after fine-tuning them along with the difference between them (acc). Additionally, we report the accuracy of the corresponding B-cos models trained from scratch (B-cos) as well as the difference to them (acc), and how much faster and at which epoch (t) the same accuracy as in [10] was achieved (speedup). Results for B-cosified models are averaged over three runs.", "description": "This table presents an ablation study on the impact of using normalized weights in the B-cos layers. It compares the performance of B-cosified ViT models with and without weight normalization against their corresponding B-cos counterparts trained from scratch. The metrics reported are top-1 accuracy on ImageNet validation set, accuracy difference compared to B-cos models (\u0394acc), accuracy gain over B-cos models (\u03942acc), the epoch at which the B-cosified model matches the accuracy of the B-cos model (t), and the speedup achieved. ", "section": "Additional Quantitative Results"}, {"figure_path": "TA5zPfH8iI/tables/tables_17_2.jpg", "caption": "Table B3: Impact of Pre-trained Weights. We report the top-1 classification accuracy on the ImageNet validation set of the B-cosified ResNet-50 model with different weight initialisations (acc), the difference to the results reported in [10] when training the pre-trained B-cos ResNet-50 model (\u0394acc) under the same B-cosification recipe, as well as how much faster the same accuracy was achieved (speedup). Additionally, we report the GridPG localisation scores (loc) similar to those reported in [10] and the difference to the B-cos ResNet-50 model's localisation trained (\u0394loc) under the same B-cosification training recipe. The pre-trained accuracy (in %) for: B-cos = 75.9, CLIP = 73.3, and DINO = 75.3. Random Init is the baseline with randomly initialized B-cosified model training. Results are for a random initialized single run.", "description": "This table shows the impact of using different pre-trained weights (Random Init, V1, V2, CLIP, DINO) on the performance of B-cosified ResNet-50 models.  It compares top-1 accuracy, accuracy gain relative to the original B-cos model, training speedup, localization score, and the improvement in localization score compared to the original B-cos model.", "section": "Additional Quantitative Results"}, {"figure_path": "TA5zPfH8iI/tables/tables_18_1.jpg", "caption": "Table B4: Experimental ablations results for the B-cosification design choices. The table shows the ablation results for the B-cosified ResNet-18 model (with B = 2 and no Bias) shown in row 1. Further in rows 2-6, the ablated components are shown in col. 1, the accuracy of the ablated model (acc) in col. 2, and the change in accuracy (\u0394acc) compared to the non-ablated model (row 1) is shown in col. 3. Further, col. 4 shows the GridPG localisation scores (loc) and the difference to the non-ablated B-cosified model (\u0394loc). Similar to components used in standard models, we replace the BatchNorm Uncentered with BatchNorm Centered, change the order of the Global Average Pool where features are first averaged and then passed to the last linear layer, and replace average pooling with max pooling at the stem. Further, we also replaced ReLU activation with Identity; and removed the Logit Bias layer. Results are for a random initialized single run.", "description": "This table presents ablation study results for the B-cosification process on a ResNet-18 model. It systematically removes or modifies components of the B-cosified model to analyze their impact on both classification accuracy and localization performance (interpretability). The table shows the accuracy and localization score for the baseline B-cosified model and for models with various components removed or modified. The changes in accuracy and localization are also shown, allowing for a detailed understanding of each component's contribution to the overall performance.", "section": "Additional Quantitative Results"}, {"figure_path": "TA5zPfH8iI/tables/tables_18_2.jpg", "caption": "Table B5: Increasing B for B-cosification Extended. Extending Tab. 2 for different convolutional models, we compare various strategies to increase the B parameter. We evaluate these strategies using accuracy and localization scores to measure interpretability performance. Columns 2-3 represent the baseline models: Standard ResNet-18 [53] and B-cos ResNet-18 [10]. Columns 4-8 set the B value directly \u2208 {1, 1.25, 1.5, 1.75, 2}. Columns 9-13 apply B = 2 over 'n' epochs with n \u2208 {5, 10, 20, 45, 90}. Column 14 shows results for increasing B as a learned parameter. Further, row blocks denote different models, each denoting accuracy, followed by localisation scores. Results are for a random initialized single run.", "description": "This table presents an ablation study on how to increase the B parameter in the B-cosification method. It compares different strategies for increasing B (discrete values, linear increase, and learning B) across several convolutional neural network models (ResNet-18, ResNet-50 v1, ResNet-50 v2, and DenseNet-121).  For each model, it reports accuracy and localization scores (a measure of interpretability) to evaluate the effectiveness of each strategy.  The results show the effect of different B values and training epochs on both accuracy and localization.", "section": "B Additional Quantitative Results"}, {"figure_path": "TA5zPfH8iI/tables/tables_19_1.jpg", "caption": "Table B6: Decreasing biases for B-cosification extended. Extending Tab. 3 for different convolutional models, we compare various strategies to decrease the bias parameter. We evaluate these strategies using accuracy and localization scores to measure interpretability performance. Columns 2-3 represent the baseline models: Standard ResNet-18 [53] and B-cos ResNet-18 [10]. Columns 4-5 show the setting with bias (col. 4) and without bias (col. 5). Columns 6-8 show the bias decay setup using the weight decay with different values \u03bb\u2208 {0.2, 0.5, 0.9}. Further, row blocks denote different models, each denoting accuracy, followed by localisation scores. Results are for a random initialized single run.", "description": "This table presents an ablation study on strategies for decreasing biases during B-cosification of various convolutional neural networks. It compares different methods, including removing biases completely and applying weight decay with varying coefficients (\u03bb).  The results are evaluated in terms of classification accuracy and localization performance (GridPG metric), offering a detailed comparison across different bias handling approaches.  The goal is to find the best strategy for reducing bias while maintaining or improving model performance and interpretability.", "section": "B Additional Quantitative Results"}, {"figure_path": "TA5zPfH8iI/tables/tables_20_1.jpg", "caption": "Table B7: Zero-shot performance of various CLIP-based models over 38 datasets using CLIP Benchmark [14]. Scores within the 99.5% Clopper-Pearson confidence interval of each dataset's top score are shown in bold. Baselines contain results for the Standard CLIP [38] and Text2Concept (T2C) [29] models; ImageNet and CC3M column sections contain the B-cosified RN-50 CLIP models trained with cosine and cyclic learning schedulers trained with ImageNet [15] and CC3M [46] datasets, respectively. The cyclic learning training inspired from [24]. Dataset type is taken from [14].", "description": "This table presents the zero-shot performance of different CLIP-based models on 38 datasets using the CLIP benchmark.  It compares the standard CLIP, the Text2Concept approach, and two B-cosified CLIP models (trained on ImageNet and CC3M datasets respectively) using cosine and cyclic learning schedulers.  Scores are presented as the accuracy, with bold values indicating those within the 99.5% Clopper-Pearson confidence interval of the dataset's top score. The table is organized by dataset type (Natural, Specialized, Structured).", "section": "4.2 B-cosifying CLIP Towards Inherently Interpretable Foundation Models"}, {"figure_path": "TA5zPfH8iI/tables/tables_21_1.jpg", "caption": "Table B7: Zero-shot performance of various CLIP-based models over 38 datasets using CLIP Benchmark [14]. Scores within the 99.5% Clopper-Pearson confidence interval of each dataset's top score are shown in bold. Baselines contain results for the Standard CLIP [38] and Text2Concept (T2C) [29] models; ImageNet and CC3M column sections contain the B-cosified RN-50 CLIP models trained with cosine and cyclic learning schedulers trained with ImageNet [15] and CC3M [46] datasets, respectively. The cyclic learning training inspired from [24]. Dataset type is taken from [14].", "description": "This table presents the zero-shot classification performance of different CLIP models on 38 diverse datasets.  It compares the standard CLIP model and a Text2Concept approach with B-cosified CLIP models trained using different training strategies (cosine and cyclic learning schedules) and datasets (ImageNet and CC3M).  The results highlight the performance improvements achieved through B-cosification.", "section": "4.2 B-cosifying CLIP Towards Inherently Interpretable Foundation Models"}]