[{"figure_path": "yRhrVaDOWE/figures/figures_8_1.jpg", "caption": "Figure 1: Test success rate for the algorithms under comparison on the three maze tasks.", "description": "This figure shows the success rate of different curriculum reinforcement learning algorithms across three maze environments (PointUMaze, PointNMaze, and PointSpiralMaze).  Each line represents the performance of a specific algorithm, with the x-axis showing the number of timesteps and the y-axis showing the success rate.  The figure allows for a visual comparison of the algorithms' performance in terms of their ability to guide an agent toward the goal in different maze complexities.  DICuRL's performance is compared to nine state-of-the-art algorithms from the literature.", "section": "5 Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_8_2.jpg", "caption": "Figure 2: The curriculum goal set Gc generated by DiCuRL during the reverse diffusion process (lines 5\u20137 in Algorithm 1) for the PointUMaze environment. The color indicates the goals generated at a specific iteration step during the reverse diffusion process of the diffusion model. Then these goals are selected in Eq. (12) based on the given cost function.", "description": "This figure visualizes the curriculum goals generated by the DiCuRL algorithm for the PointUMaze environment. Each color represents goals generated at different iteration steps during the reverse diffusion process.  The final goal selection is based on a cost function (Eq. 12) which finds the optimal curriculum goal from the generated set.", "section": "Methodology"}, {"figure_path": "yRhrVaDOWE/figures/figures_9_1.jpg", "caption": "Figure 3: Curriculum goals generated by DICURL, GRADIENT, and HGG in the PointSpiralMaze environment. The colors ranging from red to purple indicate the curriculum goals across different episodes of the training, and the orange dot and red dot are the agent and desired goal, respectively.", "description": "This figure compares the curriculum goals generated by three different algorithms: DICURL, GRADIENT, and HGG, in the PointSpiralMaze environment. Each algorithm's curriculum goals are visualized using different colors, ranging from red (earlier episodes) to purple (later episodes).  The orange dot represents the agent's position, while the red dot indicates the desired goal. The figure illustrates how each algorithm approaches the generation of curriculum goals, highlighting differences in exploration strategies and how they guide the agent towards the final goal.", "section": "Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_9_2.jpg", "caption": "Figure 4: (a) Test success rate in the ablation study of DiCuRL on the PointSpiralMaze environment. (b) Curriculum goals when using only Q. (c) Curriculum goals when using only the AIM reward r\u03c6. (d) AIM reward r\u03c6, across different training episodes (around 60k, 100k, 250k and 600k timesteps), displayed clockwise from the top-left to the bottom-left quadrant.", "description": "This figure presents the ablation study results of DiCuRL on the PointSpiralMaze environment.  Subfigure (a) shows the test success rate when using the full DiCuRL model, when only using the Q-function, and when using only the AIM reward function. Subfigures (b) and (c) visualize the generated curriculum goals for the respective cases. Subfigure (d) illustrates the AIM reward values throughout the training process at different time steps.", "section": "5 Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_9_3.jpg", "caption": "Figure 1: Test success rate for the algorithms under comparison on the three maze tasks.", "description": "This figure compares the success rate of DiCuRL and nine other state-of-the-art curriculum reinforcement learning (CRL) algorithms across three different maze environments: PointUMaze, PointNMaze, and PointSpiralMaze.  The x-axis represents the number of timesteps, and the y-axis represents the success rate (the percentage of trials where the agent successfully reached the goal). The figure shows that DiCuRL consistently outperforms or matches the other algorithms in all three environments, demonstrating its effectiveness in generating challenging yet achievable curriculum goals.", "section": "5 Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_18_1.jpg", "caption": "Figure 1: Test success rate for the algorithms under comparison on the three maze tasks.", "description": "The figure shows the success rate of DiCuRL and other state-of-the-art CRL algorithms on three different maze environments.  The x-axis represents the number of timesteps, and the y-axis represents the success rate. Each line represents a different algorithm.  This graph visually demonstrates the relative performance of DiCuRL against other approaches in reaching the target successfully.", "section": "5 Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_21_1.jpg", "caption": "Figure 7: Curriculum goals generated by DiCuRL, OUTPACE, GRADIENT, and HGG in the PointUMaze environment. The colors ranging from red to purple indicate the curriculum goals across different episodes of the training, and the orange dot and red dot are the agent and desired goal, respectively.", "description": "This figure shows the curriculum goals generated by four different curriculum reinforcement learning algorithms in the PointUMaze environment.  The colors represent the progression of the curriculum goals over training episodes. The orange dot represents the agent's location, and the red dot represents the desired goal location. The figure illustrates the differences in how each algorithm explores and generates curriculum goals during training.", "section": "Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_22_1.jpg", "caption": "Figure 3: Curriculum goals generated by DiCuRL, GRADIENT, and HGG in the PointSpiralMaze environment. The colors ranging from red to purple indicate the curriculum goals across different episodes of the training, and the orange dot and red dot are the agent and desired goal, respectively.", "description": "This figure compares the curriculum goals generated by DiCuRL, GRADIENT, and HGG algorithms in the PointSpiralMaze environment. Different colors represent goals generated during different training episodes, showing the progression of goals from the initial state to the target. The orange dot indicates the agent's position, and the red dot represents the desired goal.  The visualization helps understand how each algorithm generates and selects intermediate goals to guide the agent towards the target.", "section": "Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_23_1.jpg", "caption": "Figure 3: Curriculum goals generated by DiCuRL, GRADIENT, and HGG in the PointSpiralMaze environment. The colors ranging from red to purple indicate the curriculum goals across different episodes of the training, and the orange dot and red dot are the agent and desired goal, respectively.", "description": "This figure compares the curriculum goals generated by three different algorithms: DiCuRL, GRADIENT, and HGG, in the PointSpiralMaze environment. Each algorithm generates a sequence of goals that guide the agent's learning process. The colors represent the time progression, with red being the earliest goals and purple being the latest. The orange dot indicates the agent's current position, and the red dot indicates the desired goal.  The figure visually demonstrates how DiCuRL's generated goals effectively explore the environment, in contrast to GRADIENT and HGG, which seem less comprehensive in their goal selection.", "section": "5 Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_23_2.jpg", "caption": "Figure 2: The curriculum goal set Gc generated by DiCuRL during the reverse diffusion process (lines 5\u20137 in Algorithm 1) for the PointUMaze environment. The color indicates the goals generated at a specific iteration step during the reverse diffusion process of the diffusion model. Then these goals are selected in Eq. (12) based on the given cost function.", "description": "This figure shows the curriculum goals generated by the DiCuRL algorithm in the PointUMaze environment at different steps of the reverse diffusion process. Each image represents a different step, with the color of the points indicating the step in the process. The final selected goals are then used as curriculum goals in the RL process.", "section": "4 Methodology"}, {"figure_path": "yRhrVaDOWE/figures/figures_24_1.jpg", "caption": "Figure 11: Visualization of the AIM reward function and Q-function in the early stage of the training (~2000 timesteps) for the PointUMaze environment.", "description": "This figure shows a visualization of the AIM reward function and Q-function in the early stage of training for the PointUMaze environment.  The AIM reward function is a measure of how close the agent is to achieving its goal, while the Q-function estimates the expected cumulative reward. The visualization helps to understand how these two functions guide the agent's learning process in the early stages of training.", "section": "3. Background"}, {"figure_path": "yRhrVaDOWE/figures/figures_24_2.jpg", "caption": "Figure 2: The curriculum goal set Gc generated by DiCuRL during the reverse diffusion process (lines 5\u20137 in Algorithm 1) for the PointUMaze environment. The color indicates the goals generated at a specific iteration step during the reverse diffusion process of the diffusion model. Then these goals are selected in Eq. (12) based on the given cost function.", "description": "This figure shows the curriculum goals generated by the DiCuRL model during the reverse diffusion process for the PointUMaze environment. Each image represents a specific timestep in the reverse diffusion process, with the color of the points indicating the goals generated at that timestep.  The final goal is selected from this set based on a cost function (Eq. 12). This illustrates how DiCuRL progressively refines the curriculum goals, moving from more noisy samples towards the desired goal.", "section": "4 Methodology"}, {"figure_path": "yRhrVaDOWE/figures/figures_24_3.jpg", "caption": "Figure 11: Visualization of the AIM reward function and Q-function in the early stage of the training (~2000 timesteps) for the PointUMaze environment.", "description": "This figure shows a visualization of the AIM reward function and the Q-function in the early stage of training for the PointUMaze environment.  The AIM reward function estimates how close the agent is to achieving its goal, while the Q-function predicts the cumulative reward starting from a state and following a policy. The visualization helps to understand how these two functions influence the generation of curriculum goals by the diffusion model in DiCuRL.", "section": "Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_25_1.jpg", "caption": "Figure 2: The curriculum goal set Gc generated by DiCuRL during the reverse diffusion process (lines 5\u20137 in Algorithm 1) for the PointUMaze environment. The color indicates the goals generated at a specific iteration step during the reverse diffusion process of the diffusion model. Then these goals are selected in Eq. (12) based on the given cost function.", "description": "This figure shows curriculum goals generated by DiCuRL for the PointUMaze environment.  The color gradient represents the progression of goals throughout the reverse diffusion process. Goals start as noisy samples and gradually become refined, representing a curriculum of increasing difficulty.", "section": "4 Methodology"}, {"figure_path": "yRhrVaDOWE/figures/figures_25_2.jpg", "caption": "Figure 11: Visualization of the AIM reward function and Q-function in the early stage of the training (~2000 timesteps) for the PointUMaze environment.", "description": "This figure visualizes the AIM reward function and Q-function in the PointUMaze environment during the initial stage of training. The AIM reward function estimates how close an agent is to achieving its goal and is represented in a color map, with higher values indicating closer proximity to the goal. The Q-function predicts the cumulative reward starting from a state and following the policy and is also represented in a color map. The figure demonstrates how both functions evolve during the initial phase of learning, reflecting the agent's progress toward goal achievement. ", "section": "5 Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_26_1.jpg", "caption": "Figure 16: (a, b) Test success rate with solid lines indicating mean success rates and shaded areas variability over five seeds. (c, d) Overview of the two robot manipulation tasks. Blue and yellow regions denote the sampling areas for objects and goals, respectively.", "description": "This figure presents the results of applying the DiCuRL method to two robotic manipulation tasks: FetchPush and FetchPickAndPlace.  The success rate (the proportion of times the robot successfully completes the task) is plotted over time for DiCuRL, HER, and HGG.  Shaded areas represent variability across multiple trials. Panels (c) and (d) illustrate the tasks and their goal areas.  The results show that DiCuRL generally outperforms the baseline methods.", "section": "5 Experiments"}, {"figure_path": "yRhrVaDOWE/figures/figures_26_2.jpg", "caption": "Figure 1: Test success rate for the algorithms under comparison on the three maze tasks.", "description": "This figure showcases the success rates of different curriculum reinforcement learning (CRL) algorithms across three maze environments: PointUMaze, PointNMaze, and PointSpiralMaze.  The x-axis represents the number of timesteps, and the y-axis represents the success rate.  Each line corresponds to a different CRL algorithm. The figure visually demonstrates the comparative performance of DiCuRL against other state-of-the-art CRL algorithms in terms of achieving the desired goals within the maze environments. The PointSpiralMaze environment is particularly challenging, as shown by the lower success rates of most algorithms.", "section": "5 Experiments"}]