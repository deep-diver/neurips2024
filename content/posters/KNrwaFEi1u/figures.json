[{"figure_path": "KNrwaFEi1u/figures/figures_1_1.jpg", "caption": "Figure 2: A heterogeneous ROPE sample tested with Default multi-object query, where each of the 5 objects belongs to different object classes. We label the output class as either correct or hallucinated.", "description": "This figure shows an example from the Recognition-based Object Probing Evaluation (ROPE) dataset.  It illustrates a heterogeneous sample, meaning the five objects to be identified all belong to different classes (apple, knife, fork, apple, jar). The image shows a table with various food items and kitchen utensils. Red bounding boxes highlight the objects. The caption indicates that this example uses the Default multi-object query setting within the ROPE evaluation protocol and labels each identified object as either correctly identified or incorrectly hallucinated by the model.", "section": "3 Recognition-based Object Probing Evaluation"}, {"figure_path": "KNrwaFEi1u/figures/figures_1_2.jpg", "caption": "Figure 1: A case study that compares our Recognition-based Object Probing Evaluation (ROPE) benchmark with existing benchmarks for object hallucination in GPT-4V. ROPE offers an automated evaluation protocol with controlled output formatting and uses visual prompts to distinctly ground to objects, thus mitigating referential ambiguity. Unlike binary inquiries relying solely on textual descriptions, ROPE challenges the model to identify multiple objects concurrently. We observe that, while GPT-4V can identify the whisk to the left of a knife when prompted about it, the model hallucinates a \u201cfork\u201d when directly tasked to recognize multiple objects.", "description": "This figure demonstrates the limitations of existing benchmarks for evaluating object hallucination, particularly in scenarios involving multiple objects. It shows how the GPT-4V model can correctly identify individual objects when queried separately but hallucinates an object when asked to identify multiple objects simultaneously. The Recognition-based Object Probing Evaluation (ROPE) benchmark introduced in the paper addresses these limitations by using visual prompts and a controlled output format to eliminate ambiguity.", "section": "1 Introduction"}, {"figure_path": "KNrwaFEi1u/figures/figures_4_1.jpg", "caption": "Figure 2: A heterogeneous ROPE sample tested with Deafult multi-object query, where each of the 5 objects belongs to different object classes. We label the output class as either correct or hallucinated.", "description": "This figure shows a sample from the Recognition-based Object Probing Evaluation (ROPE) dataset used to evaluate multi-object hallucination in vision-language models.  The image contains five objects belonging to different classes (fork, knife, whisk, lemon, jar). The figure demonstrates the outputs of several vision-language models (GPT-4V, Gemini 1.0 Pro, Qwen-VL-Chat, LLaVA-7B, GPT-4O, Gemini 1.5 Pro, Qwen-VL-Max, LLaVA-34B) when asked to identify the class of each object. The ground truth is provided for comparison, highlighting which model predictions are correct and which are hallucinatory (incorrect). This helps in analyzing the performance of various models and identifying the types of errors they tend to make when dealing with multiple objects in a single image.", "section": "3 Recognition-based Object Probing Evaluation"}, {"figure_path": "KNrwaFEi1u/figures/figures_4_2.jpg", "caption": "Figure 2: A heterogeneous ROPE sample tested with Default multi-object query, where each of the 5 objects belongs to different object classes. We label the output class as either correct or hallucinated.", "description": "This figure shows an example from the Recognition-based Object Probing Evaluation (ROPE) dataset.  The image contains five objects belonging to different classes (fork, knife, whisk, lemon, jar). The figure compares the ground truth object classes with the classes predicted by various vision-language models (LLaVA-7B, Gemini 1.0 Pro, Qwen-VL-Chat, GPT-4V, Gemini 1.5 Pro, Qwen-VL-Max, LLaVA-34B).  The purpose is to illustrate how different models perform in identifying multiple objects simultaneously and showcases the occurrence of hallucination, where the model predicts an object class that is not actually present in the image.", "section": "3.2 Dataset Construction"}, {"figure_path": "KNrwaFEi1u/figures/figures_6_1.jpg", "caption": "Figure 4: The performance of the LLaVA on the adversarial split, organized by the query sequence of AAAAB and BAAAA, reveals significant vulnerabilities as the model's accuracy dramatically declines for object 5 in AAAAB. SO stands for single-object probing and TF stands for teacher-forcing probing.", "description": "This figure shows the performance of three different sizes of LLaVA models (7B, 13B, and 34B) on an adversarial subset of the Recognition-based Object Probing Evaluation (ROPE) dataset. The adversarial subset contains image-object sets where four of the five objects belong to the same class, and the last object belongs to a different class (AAAAB).  The figure compares the performance of single-object probing (SO) and teacher-forcing probing (TF) for each object position. It reveals a significant drop in accuracy when using the AAAAB query sequence, especially for the fifth object (object B), highlighting the model's vulnerability to this type of adversarial scenario.", "section": "4.2 Main Results and Findings"}, {"figure_path": "KNrwaFEi1u/figures/figures_8_1.jpg", "caption": "Figure 5: A comparison of the distribution of hallucinatory versus non-hallucinatory object classes in LLaVA-13B, across the unseen split under student forcing.", "description": "This figure displays the distribution of several factors for both hallucinatory and non-hallucinatory object classes within the LLaVA-13B model. The data used is from the unseen split of the dataset and uses the student forcing method.  The factors analyzed include query homogeneity, object token position, object homogeneity, object centrality, object salience, semantic salience, training salience, object token entropy, and visual modality contribution.  The distributions are visualized using bar charts and ridgeline plots to illustrate the differences between the two classes of objects.", "section": "5 Analysis of Hallucinatory Behaviors"}, {"figure_path": "KNrwaFEi1u/figures/figures_9_1.jpg", "caption": "Figure 6: A comparison of the distribution of actual versus predicted object classes for all hallucinatory objects in the student forcing setting on the unseen split using LLaVA-13B.", "description": "This figure compares the distribution of actual versus predicted object classes for all hallucinatory objects in the LLaVA-13B model, specifically focusing on the unseen split and using the student-forcing setting. It visually represents the frequency of actual and predicted object classes across three key data-specific factors: semantic salience, training salience, and input order. The plots show how often the model hallucinates certain classes based on these factors, providing insights into the model's behavior and potential biases.", "section": "5 Analysis of Hallucinatory Behaviors"}, {"figure_path": "KNrwaFEi1u/figures/figures_15_1.jpg", "caption": "Figure 7: Different types of instruction settings of ROPE. In a single turn of prompting without format enforcement, we probe the model to recognize the 5 objects referred to by the visual prompts (a) one at a time in the single-object setting and (b) concurrently in the multi-object setting. We further enforce the model to follow the format template and decode only the object tokens for each of the five objects (c) without output manipulation in student forcing and (d) replacing all previously generated object tokens with the ground truth classes in teacher forcing.", "description": "This figure illustrates the different prompting strategies used in the Recognition-based Object Probing Evaluation (ROPE) benchmark.  It contrasts single-object probing (a) with multi-object probing (b), showing how the task of identifying multiple objects simultaneously changes the model's behavior.  It also demonstrates the 'student forcing' (c) and 'teacher forcing' (d) methods which help isolate and analyze various sources of error in the model's responses.", "section": "3 Recognition-based Object Probing Evaluation"}, {"figure_path": "KNrwaFEi1u/figures/figures_15_2.jpg", "caption": "Figure 2: A heterogeneous ROPE sample tested with Default multi-object query, where each of the 5 objects belongs to different object classes. We label the output class as either correct or hallucinated.", "description": "This figure shows an example of a heterogeneous Recognition-based Object Probing Evaluation (ROPE) sample.  The image contains five objects, each belonging to a different class (fork, knife, whisk, lemon, jar).  The figure displays the ground truth class labels for each object and illustrates how different vision language models (VLMs) responded to the query of identifying all five objects simultaneously.  By examining the model's responses, we can assess the presence of hallucinations, where a model incorrectly identifies an object's class or hallucinates a class that isn't present in the image.", "section": "3 Recognition-based Object Probing Evaluation"}, {"figure_path": "KNrwaFEi1u/figures/figures_16_1.jpg", "caption": "Figure 8: Single and multi-object hallucination under the default setting in nuScenes [4].", "description": "This figure shows a comparison of single-object and multi-object hallucination results using various vision-language models (VLMs) on the nuScenes dataset.  The task was to identify the class of objects within bounding boxes in an image. The \"Single-object\" scenario presented each object individually for classification, while the \"Multi-object\" scenario presented multiple objects simultaneously. The figure highlights how VLMs perform differently when handling multiple objects compared to single objects, demonstrating the challenge of multi-object hallucination.", "section": "3.1 Task Setup"}, {"figure_path": "KNrwaFEi1u/figures/figures_16_2.jpg", "caption": "Figure 8: Single and multi-object hallucination under the default setting in nuScenes [4].", "description": "This figure shows a comparison of single-object and multi-object hallucination using the nuScenes dataset.  Five objects are identified within bounding boxes.  The results from different LLMs (GPT-40, Claude 3.5, LLaVA-13B, GPT-4V, Qwen 2.5, LLaVAPhi3Mini) are displayed, highlighting the variations in accuracy and instances of hallucination when identifying multiple objects compared to single objects.", "section": "3.2 Dataset Construction"}]