{"references": [{"fullname_first_author": "D. P. Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014", "reason": "This paper introduces the Adam optimizer, which MICROADAM builds upon and improves for memory efficiency."}, {"fullname_first_author": "S. J. Reddi", "paper_title": "On the convergence of Adam and beyond", "publication_date": "2019-04", "reason": "This paper addresses convergence issues in the original Adam optimizer, a key concern that MICROADAM directly tackles and solves."}, {"fullname_first_author": "D. Alistarh", "paper_title": "The convergence of sparsified gradient methods", "publication_date": "2018", "reason": "This paper introduces the error feedback mechanism, a core technique used in MICROADAM to control and correct for compression errors."}, {"fullname_first_author": "P. Zhou", "paper_title": "On the convergence of adaptive gradient methods for nonconvex optimization", "publication_date": "2024-03", "reason": "This provides theoretical convergence guarantees for AMSGrad, which MICROADAM's convergence analysis is benchmarked against."}, {"fullname_first_author": "J. Zhao", "paper_title": "GaLore: Memory-efficient LLM training by gradient low-rank projection", "publication_date": "2024-03", "reason": "This is a state-of-the-art memory-efficient training method for LLMs, providing a direct comparison point for MICROADAM in terms of both memory and performance."}]}