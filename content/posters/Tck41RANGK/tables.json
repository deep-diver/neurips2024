[{"figure_path": "Tck41RANGK/tables/tables_7_1.jpg", "caption": "Table 1: Finetuning results on GLUE/MNLI. We report the entire memory usage read from the GPU during training, that includes the optimizer state, activations and gradients. The asterisk flags the runs for which one or two seeds did not converge (we report the run with maximum performance).", "description": "This table compares the performance of MICROADAM against Adam, Adam-8b, CAME, and GaLore on the GLUE/MNLI benchmark.  Metrics include training loss, accuracy, and memory usage (in GB) for three different model sizes: BERT-BASE (110M parameters), BERT-LARGE (335M parameters), and OPT-1.3B (1.3B parameters). The asterisk (*) indicates runs where the optimizer failed to converge on one or more random seeds; the table reports the results from the best-performing seed for those cases.", "section": "Experiments"}, {"figure_path": "Tck41RANGK/tables/tables_8_1.jpg", "caption": "Table 2: FFT results for Llama-2 7B/13B on GSM-8k.", "description": "This table presents the results of fine-tuning (FFT) experiments using Llama-2 7B and 13B models on the GSM-8k dataset.  It compares the performance of Adam, Adam-8b, and MICROADAM (with two different window sizes, m = 10 and m = 20) in terms of accuracy, memory usage (both optimizer state and total), and training runtime.  The table highlights the memory efficiency of MICROADAM compared to the other optimizers while demonstrating that it maintains competitive accuracy.", "section": "Experiments"}, {"figure_path": "Tck41RANGK/tables/tables_8_2.jpg", "caption": "Table 3: FFT results on instruction-following Open-Platypus [Lee et al., 2023] dataset. The results show that MICROADAM fully recovers accuracy relative to baseline Adam, and outperforms the 8bit variant, despite using less memory.", "description": "This table compares the performance of different optimizers (AdamW, Adam-8b, and MICROADAM) on the Open-Platypus instruction-following dataset.  The metrics include average accuracy across multiple tasks and per-task accuracy (ARC-c, HellaSwag, MMLU, Winogrande) using different few-shot settings.  The table also shows the memory usage for each optimizer.  The results demonstrate that MICROADAM achieves comparable or better accuracy than the other optimizers while using significantly less memory.", "section": "Finetuning results for LLaMA2-7B on Open-Platypus"}, {"figure_path": "Tck41RANGK/tables/tables_9_1.jpg", "caption": "Table 1: Finetuning results on GLUE/MNLI. We report the entire memory usage read from the GPU during training, that includes the optimizer state, activations and gradients. The asterisk flags the runs for which one or two seeds did not converge (we report the run with maximum performance).", "description": "This table shows the results of fine-tuning experiments on the GLUE/MNLI dataset using various optimizers, including MICROADAM, Adam, Adam-8bit, CAME, and GaLore.  For each model (BERT-BASE, BERT-LARGE, OPT-1.3B), the table presents the train loss, accuracy, and total memory usage. The asterisk indicates runs where convergence wasn't achieved for all seeds, and the reported results are from the best performing run.", "section": "5 Experiments"}]