[{"figure_path": "2kZMtdjzSV/tables/tables_2_1.jpg", "caption": "Table 1: Comparisons of the settings, assumptions, and regret guarantees in this paper and previous works. A more comprehensive comparison is available in Table 2 of Appendix A.", "description": "This table compares the regret guarantees of various algorithms for multi-task linear bandits.  It contrasts the algorithms' assumptions about prior knowledge of the task structure (low-rank subspace), the task diversity assumption (whether tasks are diverse or not), and the resulting regret bounds. The table highlights the improvement of the proposed algorithm (BOSS) over existing methods by relaxing the task diversity assumption while still maintaining a provable regret bound.", "section": "1.1 Related work"}, {"figure_path": "2kZMtdjzSV/tables/tables_11_1.jpg", "caption": "Table 1: Comparisons of the settings, assumptions, and regret guarantees in this paper and previous works. A more comprehensive comparison is available in Table 2 of Appendix A.", "description": "This table compares the proposed BOSS algorithm with existing algorithms for multi-task linear bandits. It shows the setting (parallel or sequential), prior knowledge required, task diversity assumption, and regret guarantees for each algorithm. The table highlights that BOSS achieves the first non-trivial result for the sequential setting without the task diversity assumption, offering a significant improvement when the number of tasks is large.", "section": "Related work"}]