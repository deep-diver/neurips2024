[{"figure_path": "8ohsbxw7q8/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of GDPO. (1) In each optimization step, GDPO samples multiple generation trajectories from the current Graph DPM and queries the reward function with different Go. (2) For each trajectory, GDPO accumulates the gradient \u2207log p\u03b8(G0|Gt) of each (G0, Gt) pair and assigns a weight to the aggregated gradient based on the corresponding reward signal. Finally, GDPO estimates the eager policy gradient by averaging the aggregated gradient from all trajectories.", "description": "This figure illustrates the two main steps of the Graph Diffusion Policy Optimization (GDPO) algorithm.  The first step involves sampling multiple generation trajectories from a graph diffusion probabilistic model (DPM). Each trajectory represents a sequence of graph states, starting from a noisy graph (Gt) and progressing towards a cleaner graph (G0).  The reward function is then queried for each generated graph (G0) to obtain a reward signal reflecting the quality of the generated graph based on the defined objective. The second step uses these reward signals and gradients of the log probability of the generated graph given the noisy graph to estimate the policy gradient, which is then used to update the DPM parameters, ultimately optimizing the model to generate higher-quality graphs based on the specified objective function.", "section": "Method"}, {"figure_path": "8ohsbxw7q8/figures/figures_4_1.jpg", "caption": "Figure 2: Toy experiment comparing DDPO and GDPO. We generate connected graphs with increasing number of nodes. Node categories are disregarded, and the edge categories are binary, indicating whether two nodes are linked. The graph DPM is initialized randomly as a one-layer graph transformer from DiGress [61]. The diffusion step T is set to 50, and the reward signal r(Go) is defined as 1 if Go is connected and 0 otherwise. We use 256 trajectories for gradient estimation in each update. The learning curve illustrates the diminishing performance of DDPO as the number of nodes increases, while GDPO consistently performs well.", "description": "This figure shows a comparison of DDPO and GDPO's performance on a toy experiment involving generating connected graphs with varying numbers of nodes.  The results demonstrate that GDPO maintains its performance as graph complexity increases, unlike DDPO, highlighting GDPO's robustness in handling complex graphs.", "section": "4 Method"}, {"figure_path": "8ohsbxw7q8/figures/figures_9_1.jpg", "caption": "Figure 3: We investigate two key factors of GDPO on ZINC250k, with the target protein being 5ht1b. Similarly, the vertical axis represents the total queries, while the horizontal axis represents the average reward.(a) We vary the number of trajectories for gradient estimation. (b) We fix the weight of rdeg and rsa, and change the weight of rNOV while ensuring the total weight is 1.", "description": "This figure analyzes the impact of two key hyperparameters on the performance of GDPO: the number of trajectories used for gradient estimation and the weight assigned to the novelty reward signal (rNOV).  In (a), it shows that GDPO achieves good sample efficiency, reaching a significant improvement in average reward with relatively few queries (around 10k) even with fewer trajectories. In (b), it demonstrates that assigning too high a weight to novelty can lead to training instability and reduced performance.  It highlights the need for a balance between exploring novel molecules and optimizing overall drug efficacy.", "section": "6.3 Generalizability, Sample Efficiency, and A Failure Case"}, {"figure_path": "8ohsbxw7q8/figures/figures_15_1.jpg", "caption": "Figure 1: Overview of GDPO. (1) In each optimization step, GDPO samples multiple generation trajectories from the current Graph DPM and queries the reward function with different Go. (2) For each trajectory, GDPO accumulates the gradient \u2207elog po(Go|Gt) of each (Go, Gt) pair and assigns a weight to the aggregated gradient based on the corresponding reward signal. Finally, GDPO estimates the eager policy gradient by averaging the aggregated gradient from all trajectories.", "description": "This figure provides a visual overview of the Graph Diffusion Policy Optimization (GDPO) process.  It illustrates the two main steps involved: 1) Sampling multiple generation trajectories using a graph diffusion probabilistic model (DPM) and querying a reward function for each generated graph (Go). 2) Estimating the eager policy gradient by calculating the gradient of the log probability of each trajectory (from Go to Gt), weighting them according to their reward signals, and averaging the results. The figure uses a diagrammatic representation to show the flow of the process and the relationships between the different components.", "section": "Method"}, {"figure_path": "8ohsbxw7q8/figures/figures_18_1.jpg", "caption": "Figure 4: We investigate the L2 distance between two consecutive steps in two types of DPMs. The diffusion step is 1000 for two models.", "description": "This figure compares the L2 distance between consecutive steps in image diffusion models and graph diffusion models. The x-axis represents the diffusion steps, and the y-axis represents the L2 distance. The image diffusion model shows a consistently low and relatively stable L2 distance across all steps. In contrast, the graph diffusion model exhibits a much higher and more variable L2 distance, especially at the later steps. This illustrates the discontinuous and more erratic nature of graph diffusion processes compared to the continuous nature of image diffusion processes.", "section": "A.6 Discussions"}, {"figure_path": "8ohsbxw7q8/figures/figures_19_1.jpg", "caption": "Figure 5: Tree with Different Parameters. Node 0 is the root node.", "description": "This figure shows examples of graphs generated using a tree-like structure with varying parameters.  The parameters varied are the number of rewrite steps applied to the initial tree structure (affecting its complexity), and the size of the graph (number of nodes). Also shown are three variations in clique position, demonstrating placement at the shallow, middle, and deep levels of the tree structure.  These graphs demonstrate the diversity possible when manipulating the initial tree structure and the addition of a clique.", "section": "A.5 Additional Results of the GDPO"}, {"figure_path": "8ohsbxw7q8/figures/figures_19_2.jpg", "caption": "Figure 6: Ablation Study on the Synthetic Tree-like Dataset.", "description": "This figure presents an ablation study on a synthetic tree-like dataset, where the performance of GDPO is evaluated under different parameters. Specifically, it demonstrates how the model performs under varying numbers of rewrite steps, graph sizes, and clique positions. The results show the robustness of GDPO to these changes, showcasing its ability to consistently optimize the graph DPMs across a range of conditions. It also includes a comparison between GDPO and DDPO, highlighting GDPO's superior performance in handling challenging data generation tasks.", "section": "6.3 Generalizability, Sample Efficiency, and A Failure Case"}]