{"references": [{"fullname_first_author": "Jimmy Lei Ba", "paper_title": "Layer normalization", "publication_date": "2016-07-06", "reason": "This paper introduces layer normalization, a crucial technique used in the SNE model for stabilizing the training process and improving performance."}, {"fullname_first_author": "Andreis Bruno", "paper_title": "Mini-batch consistent slot set encoder for scalable set encoding", "publication_date": "2021-12-01", "reason": "This paper presents a mini-batch consistent slot set encoder, which is directly relevant to the set-based encoding methods used in SNE."}, {"fullname_first_author": "Manzil Zaheer", "paper_title": "Deep sets", "publication_date": "2017-12-01", "reason": "This foundational paper introduces DeepSets, a set function that is a core component of the SNE architecture."}, {"fullname_first_author": "Juho Lee", "paper_title": "Set transformer: A framework for attention-based permutation-invariant neural networks", "publication_date": "2019-12-01", "reason": "This paper proposes the Set Transformer, an important set function that is used within the SNE architecture."}, {"fullname_first_author": "Artem Moskalev", "paper_title": "On genuine invariance learning without weight-tying", "publication_date": "2023-01-01", "reason": "This paper introduces the Logit Invariance Regularizer, a key regularization technique used in SNE to respect symmetries in the weight space without resorting to weight tying."}]}