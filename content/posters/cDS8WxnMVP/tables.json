[{"figure_path": "cDS8WxnMVP/tables/tables_3_1.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table lists the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, the unfolding of inputs and kernels, vector-Jacobian products (VJPs), and Kronecker-factored curvature approximations (KFC/KFAC).  It shows how these operations can be expressed concisely and efficiently using the `einops` library's syntax, incorporating batching and channel grouping.  Note that some scalar factors are omitted for brevity.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_8_1.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table lists the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, their Jacobians, VJPs, and curvature approximations like KFAC.  It shows the input tensors and the corresponding einsum string for each operation, enabling concise representation of these computations.  The table also notes that some scalar factors are omitted for brevity and references supplementary material for visualizations and a more comprehensive list of operations.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_22_1.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table shows the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, their Jacobian, vector-Jacobian products (VJPs), and Kronecker-factored curvature approximations like KFAC.  It encompasses batching and channel groups and aims to provide concise formulas using the `einops` library's syntax.  The table also references a more detailed explanation and visualization in the supplement.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_27_1.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table presents the einsum contraction expressions for various operations related to 2D convolutions, including standard operations like convolution itself and its Jacobian, and second-order methods such as KFAC.  It shows the input tensors required and the einsum contraction string using the einops library's notation.  Batching and channel groups are included, and the table notes that some scalar factors are omitted for brevity.  Further visualizations and additional operations are available in the supplementary material.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_27_2.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table lists the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, their Jacobians, and popular curvature approximations like KFAC.  It shows how to express these operations using the `einops` library's syntax, which is compact and allows for flexible index manipulation. The table also includes batching and channel groups, common features in modern deep learning implementations.  Note that some quantities are only accurate up to a scalar factor, which is omitted for brevity.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_27_3.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by providing a more extensive list of convolution and related operations.  It includes operations such as convolution, unfolded input, unfolded kernel, folded output, transpose-unfolded input, weight VJP, input VJP, KFC/KFAC-expand, KFAC-reduce, GGN Gram matrix, GGN/Fisher diagonal, and approximate Hessian diagonals. For each operation, it specifies the operands and the contraction string using the einops library's convention. The table shows that the operations include batching and channel groups, and are generalized to any number of spatial dimensions.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_27_4.jpg", "caption": "Table E4: Hyper-parameters of convolutions in different CNNs. For convolutions with identical hyper-parameters, we only show one instance and its multiplicity.", "description": "This table lists the hyperparameters of convolutional layers from nine different Convolutional Neural Networks.  The networks are categorized by their dataset (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNet) and architecture (3c3d, 2c2d, All-CNN-C, AlexNet, ResNet18, ResNext101, ConvNeXt-base, InceptionV3, and MobileNetV2). For each convolutional layer, the table shows its name, input shape, output shape, kernel size, stride, padding, dilation, number of groups, whether a bias term is used, and the type of convolution (general, dense, down-sampling).  The \"count\" column indicates how many layers share the same hyperparameters.", "section": "E Convolution Layer Hyper-parameter Analysis"}, {"figure_path": "cDS8WxnMVP/tables/tables_27_5.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table presents the einsum contraction expressions for various operations related to 2D convolutions.  It includes common operations such as convolution itself (with and without bias), unfolding of inputs and kernels, and the vector-Jacobian products (VJPs) for weights and inputs (relevant for backpropagation and transpose convolutions). It also covers  Kronecker-factored curvature approximations such as KFC/KFAC expand and reduce. The table indicates the input tensors needed for each operation and provides the einsum contraction string using the einops library's syntax, allowing for batching and channel grouping.  Note that some quantities might be only correct up to a scalar factor, which is omitted for simplicity. For further details and visualizations, readers are referred to section B and Table B3 in the supplementary material.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_27_6.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ..., and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more operations related to convolutions.  It shows the operations' operands, contraction strings (using the einops convention), and includes support for batching and channel groups, extending the coverage to various common operations beyond those presented in the main paper.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_27_7.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table lists the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, their Jacobians, vector-Jacobian products, and Kronecker-factored curvature approximations (KFC/KFAC).  It includes support for batching and channel groups, and shows how to express these operations using the einops library's syntax.  Note that some quantities are approximate (up to a scalar factor).  More details and visualizations can be found in Appendix B.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_28_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main text by including more operations related to convolutions.  It provides a comprehensive list of contraction expressions for various operations, including convolutions, Jacobian-vector products (JVPs), vector-Jacobian products (VJPs), Kronecker-factored approximate curvature (KFAC) variants, and Hessian approximations.  Each operation is described by its operands and a contraction string using the einops library's syntax.  The table explicitly incorporates batching and channel groups, and indicates how the operations can be extended to higher dimensions.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_28_2.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more convolution-related operations and including batching and channel groups.  The table includes the operands needed and einsum contraction strings for each operation. This is a comprehensive listing of many operations related to 2D convolutions.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_1.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed breakdown of the forward pass performance for various convolutional neural networks (CNNs) across different categories (General, Dense mix, Dense, Down). For each CNN and layer, the table provides the run times for the Tensor Network (TN) implementation, the PyTorch (PT) implementation, and the performance factors (TN/PT and TN+opt/PT). The performance factors indicate the speedup or slowdown achieved by the TN implementation compared to the PT implementation.  Lower values indicate better performance.  The table provides detailed numbers for comparison.  The CNNs are categorized into four groups: 3c3d (CIFAR-10), F-MNIST 2c2d, CIFAR-100 All-CNN-C, AlexNet, ResNet18, ResNext101, and ConvNeXt-base.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_2.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed comparison of the forward pass performance between the Tensor Network (TN) implementation and the PyTorch (PT) implementation for various CNN architectures and convolution types. The table includes the run times for TN, TN with optimizations (TN+opt), and PT. For each architecture and convolution layer, the factor of TN/PT and TN+opt/PT, calculated using the run times, is given. This allows to compare the performance of the TN implementation against PyTorch's highly optimized functions for the forward pass. The table is divided into sections for different datasets (CIFAR-10, Fashion MNIST, ImageNet), and each section shows results for different CNN architectures (3c3d, 2c2d, All-CNN-C, AlexNet, ResNet18, ResNext101, ConvNeXt-base) and their respective convolution layers.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_3.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed breakdown of the forward pass performance comparison between the Tensor Network (TN) implementation and the PyTorch (PT) implementation for various CNNs.  It provides the runtimes (in seconds) for both TN and PT, along with the performance factor (TN time / PT time). The performance factor indicates the speedup or slowdown of the TN implementation relative to the PT implementation.  The table includes results for different convolution types (general, mixed-dense, dense, and down-sampling) and architectures to comprehensively assess the efficiency gains provided by the TN approach.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_4.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed breakdown of the forward pass performance comparison between the proposed Tensor Network (TN) implementation and the standard PyTorch (PT) implementation across various CNN architectures and convolution types. For each CNN and convolution layer, the table reports the run times for the TN, TN with optimizations (TN+opt), and PT implementations, along with the corresponding performance factors (TN/PT and TN+opt/PT). The performance factor indicates the speed-up or slow-down achieved by the TN implementation relative to the PT implementation.  A performance factor greater than 1 means the TN implementation is slower, while a factor less than 1 indicates the TN implementation is faster.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_5.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table shows the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, their Jacobians, and related quantities such as the Kronecker factors used in KFAC approximations.  The table includes support for batching and channel groups and uses the einops library's syntax for concise representation.  Note that some scalar factors are omitted for brevity.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_6.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed comparison of the forward pass performance between the Tensor Network (TN) implementation and the PyTorch (PT) implementation for various convolution types across different CNN architectures.  It includes the run times for both TN and TN+opt (TN with simplifications), along with the corresponding PyTorch run times and performance factors (ratios of TN/PT and TN+opt/PT run times). The results are broken down by convolution type (General, Mixed dense, Dense, Down).", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_7.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of the KFAC-reduce factor computation using different methods (TN, TN+opt, and PT) across various CNN architectures and convolution types.  For each convolution layer, the table provides the execution time in seconds for each method ('TN [s]', 'PT [s]', 'TN + opt [s]') and calculates the performance ratio ('Factor') which is the ratio of execution times between the TN-based methods and the PyTorch method (PT).  Lower values for the ratio indicate that the TN-based method outperformed PyTorch.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_8.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the run times for computing the input-based KFAC-reduce factor using different methods (TN, TN+opt, and PyTorch's implementation) across various CNN architectures and convolution types.  It provides the run times for each method (in seconds), and the performance ratios which represent the speed-up achieved by TN and TN+opt relative to PyTorch's approach. The table helps demonstrate the efficiency gains obtained using the proposed Tensor Network (TN) methods, particularly when the simplifications in section 4 of the paper are applied.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_32_9.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table lists contraction expressions (using einops syntax) for various operations related to 2D convolutions.  It shows the input tensors required and the corresponding einsum-style contraction string.  The operations covered include basic convolution (with and without bias), unfolding the input and kernel, vector-Jacobian products (VJPs), and components of Kronecker-factored curvature approximations (KFAC).  Batching and channel groups are supported. Note that some scalar factors are omitted for brevity.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_33_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more operations related to 2D convolutions.  It shows the operands and einsum contraction strings for various operations, including convolutions with and without bias, unfolded input and kernel, VJPs (vector-Jacobian products), JVPs (Jacobian-vector products), KFAC (Kronecker-factored approximate curvature) expansions and reductions, and approximations for the Hessian diagonal.  The table also includes support for batching and channel groups, and indicates how to generalize to higher dimensions.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_33_2.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including batching and channel groups, providing more comprehensive contraction expressions for various operations related to 2D convolutions.  It covers a wider range of operations, including convolutions, Jacobian-vector products, and various curvature approximations. The table is organized by operation, providing the operands and contraction strings for each using einops library syntax.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_35_1.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table provides detailed results for forward pass performance comparison on GPU. It includes the runtimes and performance factors (ratio of TN implementation to PT) for different convolution types across various CNN architectures (3c3d, F-MNIST 2c2d, CIFAR-100 All-CNN-C, AlexNet, ResNet18, ResNext101, and ConvNeXt-base).  For each architecture and convolution, the table shows the runtime for both the standard TN implementation and TN + opt (with simplifications), alongside the corresponding PyTorch (PT) runtime and the performance factor. This allows for a detailed comparison of performance improvements achieved with the proposed tensor network (TN) approach.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_35_2.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed breakdown of the forward pass performance comparison between the Tensor Network (TN) and PyTorch (PT) implementations across various CNN architectures on a GPU.  The table is organized by architecture (3c3d, F-MNIST 2c2d, CIFAR-100 All-CNN-C, AlexNet, ResNet18, ResNext101, ConvNeXt-base, Inception V3, MobileNetV2), then by layer name within each architecture.  For each layer, the table provides the run time in seconds for the TN implementation, the PT implementation, and the TN implementation with optimizations applied (TN+opt).  Finally, it shows the performance ratio, which is the TN run time divided by the PT run time.  A ratio less than 1 indicates that the TN implementation is faster than PT. The table helps demonstrate the relative speed improvements of the TN-based approach across different network types and layers.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_35_3.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table lists the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, their Jacobians (input and weight), and second-order methods like Kronecker-factored approximate curvature (KFAC).  It shows the input tensors required for each operation and the corresponding einsum string using the einops library's convention.  Note that some quantities are only correct up to a scalar factor, which is omitted for brevity.  Further details, including visualizations, are available in Section B and Table B3.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_35_4.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed breakdown of the forward pass performance comparison between the Tensor Network (TN) implementation, the simplified TN+opt implementation, and PyTorch's default implementation (PT) for various convolution types across several architectures.  The table displays the runtimes (in seconds) for each implementation and calculates the performance factor by dividing the runtime of the TN implementations by the PT runtime. A performance factor less than 1 indicates the TN-based method is faster. The results are categorized by architecture and convolution type, providing a comprehensive assessment of the efficiency gains achieved with the proposed tensor network approach.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_35_5.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed comparison of the forward pass performance between the Tensor Network (TN) implementation and the PyTorch implementation (PT) for various convolutional layers across different CNN architectures.  It shows the runtimes for both TN and TN with optimizations applied (TN+opt), along with the performance factor (ratio of TN/PT and TN+opt/PT runtimes). The table is categorized by dataset and CNN architecture, and includes the results for multiple layers within the networks.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_35_6.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table summarizes the einsum contraction strings for various operations related to 2D convolutions, including convolutions themselves, their Jacobian calculations (VJP and JVP), and Kronecker-factored approximations of curvature (KFC/KFAC).  It shows the input tensors and the einops contraction string for each operation.  Batching and channel groups are included, along with references to visualizations and additional operations in supplementary material.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_35_7.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the runtimes for computing the input-based KFAC-reduce factor using different methods (TN, TN+opt, and PyTorch's standard implementation) across various convolution types and specific layers from different CNN architectures.  It shows the runtime in seconds for each method, along with the performance factor (runtime ratio relative to the PyTorch standard implementation). This allows for a precise quantitative assessment of the performance gains achieved using the proposed Tensor Network (TN) approach with and without simplifications (TN+opt).", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_36_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ..., and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including additional convolution-related operations.  It provides the einsum contraction strings and operands for each operation, showing how batching and channel groups are handled, along with the generalization to any number of spatial dimensions.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_36_2.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more operations related to convolutions.  It provides the operands and einsum contraction strings for various operations, including convolutions with and without bias, unfolding and folding operations, Jacobian-vector products (JVPs), vector-Jacobian products (VJPs),  Kronecker-factored approximate curvature (KFAC) operations (expand and reduce), and approximations for Hessian diagonals. The table also includes batching and channel groups, and notes that generalization to higher dimensions is straightforward by adding more spatial and kernel indices.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_38_1.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed comparison of the forward pass performance between the proposed Tensor Network (TN) implementation and the standard PyTorch (PT) implementation.  It breaks down the runtime for various convolution operations across several different CNN architectures and datasets.  The results are presented in seconds, with a performance factor calculated for each operation by comparing the TN runtime to the PT runtime.  Both TN implementations with and without optimizations are shown in the table. The table allows for a granular assessment of the efficiency gains obtained by using Tensor Networks for CNN forward pass calculations.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_38_2.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed breakdown of the forward pass performance comparison between the Tensor Network (TN) implementation and the PyTorch (PT) implementation across various CNNs. For each CNN and each convolution layer within the CNN, the table shows the run times for TN, TN with optimization (TN+opt), and PT. It also indicates the performance factors (ratio of TN/PT and TN+opt/PT) which illustrate the relative speed of the TN implementations compared to PyTorch's built-in functions. The table categorizes convolutions into four types: general, mixed dense, dense, and downsampling.  The results highlight the performance improvements and efficiency gains achieved using the TN approach.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_38_3.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of the KFAC-reduce factor computation using different methods (TN, TN+opt, and PyTorch's standard implementation) across various CNN architectures.  For each CNN and its convolutional layers, the table lists the run time for each method (TN, TN+opt, and PT). The factor column displays the ratio of the run time for each method against the PyTorch (PT) standard implementation, indicating the relative speedup or slowdown.  The table is organized by CNN architecture and then by individual layer. The data in this table forms the basis of the boxplots in Figure F23.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_38_4.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed comparison of the forward pass performance between the TN implementation and PyTorch's implementation across various CNN architectures.  It breaks down the run times for each method, calculating the performance factor (ratio of TN time to PyTorch time) for each layer in the networks. The table categorizes the results by CNN architecture and convolution type (general, mixed dense, dense, down-sampling). Lower performance factors indicate that the TN implementation is faster than PyTorch's implementation.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_38_5.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of KFAC-reduce factor computations using Tensor Network (TN) and standard PyTorch (PT) implementations across various CNN architectures and convolution types.  It shows the measured runtimes (in seconds) for both TN and PT, along with a performance factor representing the ratio of TN runtime to PT runtime.  The table also includes results for simplified TN implementations (TN + opt), further highlighting the performance gains achieved with the proposed TN approach.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_38_6.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of KFAC-reduce factor computation using Tensor Network (TN) and standard PyTorch implementations. It breaks down the results by different convolution types (general, mixed dense, dense, and down-sampling) and shows the TN run time, PyTorch run time, and the performance ratio (TN/PyTorch) for each convolution.  The table also includes results with TN simplifications applied (TN+opt). The purpose is to demonstrate speed-ups and memory efficiencies of the TN approach, especially for less standard convolution operations.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_38_7.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed breakdown of the performance comparison between the Tensor Network (TN) implementation and the PyTorch implementation of the input-based KFAC-reduce factor for various convolution types across different CNN architectures. It includes run times for both TN and TN+opt (TN with simplifications), along with the performance ratios (TN/PT and TN+opt/PT). The table offers a granular view of the computational gains achieved by the TN approach, particularly highlighting its efficacy for optimizing the computation of the Kronecker factor.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_39_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more operations related to convolutions.  It shows the contraction strings, operands, and includes batching and channel groups. The table also describes how to generalize the operations to higher dimensions.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_39_2.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table presents a comprehensive list of convolution-related operations, expanding upon Table 1 in the main paper.  It details the operands and einsum contraction strings for various operations, including convolutions (with and without bias), unfolding and folding operations (im2col and col2im), Jacobian-vector products (JVPs) and vector-Jacobian products (VJPs), Kronecker-factored curvature approximations (KFC/KFAC expand and reduce), and second-order information approximations (GGN Gram matrix, Fisher/GGN diagonals).  The table explicitly includes support for batching and channel groups, and indicates how the operations can be generalized to higher dimensions by extending the index notation.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_41_1.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table presents a detailed breakdown of the forward pass performance for various Convolutional Neural Networks (CNNs) architectures, comparing the runtimes of Tensor Network (TN) implementations against PyTorch (PT).  The results are categorized by CNN architecture and convolution type (general, dense, mixed-dense, down-sampling), providing TN runtime, PT runtime, and the performance ratio (TN/PT) for both the standard TN implementation and the optimized TN+opt implementation.  Lower ratios indicate superior performance of the TN implementation.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_41_2.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table provides detailed runtimes and performance factors for the forward pass of various convolutions across different CNN architectures. It compares the performance of Tensor Network (TN) and optimized TN (TN+opt) implementations against PyTorch's built-in functionality (PT). The results are presented for various categories of convolutions: general, mixed dense, dense, and down-sampling.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_41_3.jpg", "caption": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library's syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See \u00a7B for visualizations and Table B3 for more operations.", "description": "This table lists the einsum contraction strings for various operations related to 2D convolutions.  It shows how to express convolutions, unfolding operations (im2col), kernel unfolding (Toeplitz), vector-Jacobian products (VJPs), and Kronecker-factored curvature approximations (KFC/KFAC) using the einops library. The table includes support for batching and channel groups, and the notation is explained in the paper's supplementary materials.", "section": "3 TNs for Convolution Operations"}, {"figure_path": "cDS8WxnMVP/tables/tables_41_4.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the run times for computing the input-based KFAC-reduce factor using different methods (TN, TN+opt, and PyTorch). It breaks down the performance by different convolution types (General, Mixed dense, Dense, and Down), showing the run time for each method in seconds and the performance factor (ratio of TN/TN+opt run time to PyTorch run time).  This allows for a precise assessment of the computational efficiency gains achieved by using the proposed tensor network (TN) approach.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_41_5.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the run times and performance factors for computing the input-based KFAC-reduce factor using different methods.  The comparison includes the standard PyTorch implementation ('PT'), a Tensor Network implementation ('TN'), and a Tensor Network implementation with simplifications ('TN + opt'). Results are broken down by convolution type (general, mixed dense, dense, and downsampling) and for various layers within different CNN architectures (3c3d, F-MNIST 2c2d, CIFAR-100 All-CNN-C, AlexNet, ResNet18, ResNext101, and ConvNeXt-base).  The 'Factor' column indicates the ratio of run times for each method relative to the PyTorch implementation. A factor less than 1 suggests that the TN methods are faster than the PyTorch implementation.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_41_6.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table details the performance of KFAC-reduce factor computation using Tensor Network (TN) and standard PyTorch implementations across various CNN architectures and different convolution types (General, Mixed dense, Dense, Down).  It presents the runtimes of both TN and TN+opt (with optimizations) against the PyTorch runtime (PT) and shows the performance ratios (TN/PT, TN+opt/PT) for each convolution layer. The ratios indicate speedup or slowdown compared to PyTorch. The table allows for direct comparison and analysis of the impact of Tensor Network optimizations on a key component of KFAC.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_41_7.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of KFAC-reduce factor computation using different methods (TN, TN+opt, and PyTorch's implementation) across various CNN architectures (3c3d, F-MNIST 2c2d, CIFAR-100 All-CNN-C, Alexnet, ResNet18, ResNext101, and ConvNeXt-base) on a GPU.  For each CNN and its convolution layers, the table lists the run times in seconds for each method and calculates the performance factor (ratio of TN or TN+opt run time to PyTorch run time). The performance factor indicates the speedup or slowdown achieved by using the TN-based methods compared to the standard PyTorch implementation. Lower values indicate a greater speedup.  The table also indicates the type of convolution (General, Dense mix, Dense, Down) used in each layer.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_42_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more operations related to 2D convolutions.  It includes batching and channel groups and is expandable to higher dimensions. Each row describes an operation with operands, and its einsum contraction string using the einops library's convention. The table shows the various operations the authors considered, including convolutions, Jacobian-vector products (VJPs), and Kronecker-factored approximations for curvature.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_42_2.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of the KFAC-reduce factor computation using three different implementations: TN (Tensor Network), TN + opt (Tensor Network with optimizations), and PT (PyTorch).  The results are broken down by different convolution types (General, Mixed dense, Dense, Down) across various CNN architectures and datasets.  Each entry shows the run time in seconds for each implementation and the performance factor (ratio of TN or TN+opt run time to PT run time).  A factor less than 1 indicates that the TN or TN+opt implementation is faster than PyTorch's implementation.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_44_1.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of the KFAC-reduce factor computation using Tensor Network (TN) and standard PyTorch (PT) implementations. It provides run times and performance factors (ratios of TN/PT and TN+opt/PT) for various convolution types across several different CNN architectures (3c3d, F-MNIST 2c2d, CIFAR-100 All-CNN-C, Alexnet, ResNet18, ResNext101, ConvNeXt-base). The 'TN + opt' column represents the performance after applying the index pattern simplifications described in the paper. The table offers a granular view of the improvements achieved by the TN approach, especially evident in the reduction of execution times and sometimes memory usage.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_44_2.jpg", "caption": "Table F5: Forward pass performance comparison on GPU.", "description": "This table details the performance of forward pass operations (inference and training) for various CNNs.  For each CNN, it shows the runtimes for the Tensor Network (TN) approach, the Tensor Network approach with optimizations (TN+opt), and the PyTorch (PT) implementation. The 'Factor' column shows the ratio of the TN or TN+opt runtimes to the PT runtime, indicating relative speedup or slowdown. The table helps demonstrate the efficiency of the TN approach for forward pass calculations.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_44_3.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of KFAC-reduce factor calculations using different methods (TN and TN+opt) against a PyTorch baseline (PT). The comparison is broken down by convolution type (general, mixed dense, dense, down-sampling), and for various CNN architectures and datasets.  The table shows the run times in seconds for each method and calculates performance factors indicating the relative speed of TN and TN+opt compared to PT.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_44_4.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of the KFAC-reduce factor computation using Tensor Networks (TN) and a standard PyTorch implementation.  It shows the runtimes for both TN and TN+opt (with simplifications) across different convolution types from nine CNN architectures (3c3d, F-MNIST 2c2d, CIFAR-100 All-CNN-C, AlexNet, ResNet18, ResNext101, ConvNeXt-base, Inception V3, MobileNetV2).  For each convolution layer, the table lists the time taken by the TN approach and the PyTorch implementation, along with the performance ratio (Factor).  The \"Factor\" column indicates how much faster or slower the TN method is compared to the PyTorch baseline (a value <1 indicates TN is faster). The table facilitates a detailed comparison of efficiency improvements across various convolution types and network architectures.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_44_5.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of the KFAC-reduce factor computation using Tensor Network (TN) and standard PyTorch (PT) implementations.  It breaks down the results for various CNN architectures, covering general, mixed-dense, dense, and downsampling convolutions. For each convolution type, it shows the TN run time, PT run time, the performance ratio (TN/PT), the optimized TN run time (TN+opt), the optimized PT run time, and the optimized performance ratio (TN+opt/PT).  The ratios indicate the speedup or slowdown achieved by using TN methods compared to standard PyTorch. A factor less than 1 signifies that the TN approach was faster.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_44_6.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table presents a detailed comparison of the performance of KFAC-reduce factor computation using three different methods: the standard PyTorch implementation, the Tensor Network (TN) implementation, and the optimized Tensor Network (TN+opt) implementation. The comparison is done across various convolution types and CNN architectures, showing the runtimes (in seconds) and performance factors (ratios of runtimes) for each method.  The performance factors indicate speedups or slowdowns compared to the PyTorch baseline.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_44_7.jpg", "caption": "Table F9: KFAC-reduce factor performance comparison on GPU.", "description": "This table details the performance comparison of KFAC-reduce factor calculations between the Tensor Network (TN) and standard PyTorch (PT) implementations.  It shows runtimes and performance ratios (TN/PT and TN+opt/PT) for various CNN architectures (3c3d, F-MNIST 2c2d, CIFAR-100 All-CNN-C, Alexnet, ResNet18, ResNext101, ConvNeXt-base) and different convolution types within each architecture (general, mixed dense, dense, downsampling).  The TN+opt column represents results where algorithmic simplifications described in the paper were applied. The 'Factor' column indicates the speed-up or slow-down achieved by the TN approach relative to the PT approach.  The table provides a detailed view of the computational efficiency gains obtained by employing the TN approach for calculating the KFAC-reduce factor.", "section": "F Run Time Evaluation Details (GPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_45_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table provides a comprehensive list of convolution-related operations, expanding upon Table 1 in the main paper.  It details the operands and contraction strings (using the einops library convention) for various operations, including convolutions, Jacobian-vector products (JVPs), vector-Jacobian products (VJPs), Kronecker-factored approximate curvature (KFAC) calculations (both expand and reduce variants), and approximations of the generalized Gauss-Newton (GGN) matrix. The table also covers the inclusion of batching and channel groups, with a note indicating how to generalize to higher dimensions by including additional spatial and kernel indices.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_45_2.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by providing a more comprehensive list of convolution-related operations.  It includes additional operations and details for hyperparameters such as batching and channel groups, extending the coverage to include a wider array of operations used in the field. The table uses the einops library's syntax for contraction strings, making the expressions concise and readily understandable for those familiar with the library.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_47_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ..., and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including batching and channel groups.  It lists a large number of convolution-related operations and their corresponding einsum contraction strings for implementation using the `einops` library.  The operations cover forward and backward passes, Jacobians, VJPs, JVPs, and various curvature approximations (including KFAC and its variants). The table also demonstrates the extensibility of the approach to higher-dimensional convolutions.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_47_2.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ..., and kernel indices k_3, ...", "description": "This table provides an extended list of convolution and related operations, including the einsum contraction strings and operands for each operation.  It expands on Table 1 from the main paper by including batching and channel groups and generalizing to higher dimensions.  The table is useful for understanding the variety of operations that can be efficiently expressed using the tensor network approach.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_47_3.jpg", "caption": "Table G10: Additional internally required memory to compute the KFAC-reduce factor (measured on CPU). The value 0 indicates that an implementation's peak memory matches the memory consumption of its input X and result \u03a9.", "description": "This table shows the extra memory usage beyond the memory used for input and output tensors for the computation of the KFAC-reduce factor. It compares the memory usage of the standard implementation with two versions of the tensor network implementation: one without simplifications and one with simplifications. The results are categorized by convolution type (general, dense, etc.) and show that the tensor network implementations require significantly less additional memory.", "section": "G Memory Evaluation Details (CPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_47_4.jpg", "caption": "Table G10: Additional internally required memory to compute the KFAC-reduce factor (measured on CPU). The value 0 indicates that an implementation's peak memory matches the memory consumption of its input X and result \u03a9.", "description": "This table shows a comparison of the peak memory usage for computing the KFAC-reduce factor using three different methods: the standard PyTorch implementation, the proposed tensor network (TN) implementation, and the proposed TN implementation with simplifications. The memory usage is measured in MiB (mebibytes). The table is broken down by different convolution types found in various CNN architectures.", "section": "G Memory Evaluation Details (CPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_47_5.jpg", "caption": "Table G10: Additional internally required memory to compute the KFAC-reduce factor (measured on CPU). The value 0 indicates that an implementation's peak memory matches the memory consumption of its input X and result \u03a9.", "description": "This table presents a comparison of the peak memory usage for different implementations of the KFAC-reduce factor calculation. The implementations are compared across various CNN architectures and convolution types (general, dense, mixed dense, downsampling).  The table shows the additional memory required beyond that needed to store the input and output tensors, categorized by implementation type (TN, TN + opt, PT). A value of 0.0 indicates that the implementation uses no additional memory beyond the input and output.", "section": "G Memory Evaluation Details (CPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_47_6.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more operations related to convolution, such as different types of Jacobian and VJPs, KFAC approximations, and GGN calculations.  It also shows how batching and channel groups are handled.  The table provides the operands and einsum contraction strings for each operation, offering a comprehensive reference for implementing various convolution-related routines.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_47_7.jpg", "caption": "Table G10: Additional internally required memory to compute the KFAC-reduce factor (measured on CPU). The value 0 indicates that an implementation\u2019s peak memory matches the memory consumption of its input X and result \u03a9.", "description": "This table shows the additional memory required to compute the KFAC-reduce factor for different CNN architectures using different implementations.  The \"TN\" column represents the Tensor Network implementation, \"TN + opt\" is the Tensor Network implementation with optimizations, and \"PT\" is the PyTorch implementation. A value of 0 indicates no additional memory usage beyond the input and output.", "section": "G Memory Evaluation Details (CPU)"}, {"figure_path": "cDS8WxnMVP/tables/tables_48_1.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ..., and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including more operations and hyperparameters, such as batching and channel groups.  It provides the einops contraction strings for each operation, which represent the tensor network operations in a concise notation.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}, {"figure_path": "cDS8WxnMVP/tables/tables_48_2.jpg", "caption": "Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, ... and kernel indices k_3, ...", "description": "This table extends Table 1 from the main paper by including additional operations related to convolutions, such as various Jacobian-vector products (JVPs), vector-Jacobian products (VJPs), and curvature approximations.  The table shows the operands involved and the einsum contraction string for each operation, illustrating the flexibility and expressiveness of the einsum notation for representing these operations.", "section": "B Visual Tour of Tensor Network Operations for Convolutions"}]