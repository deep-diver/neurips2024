[{"type": "text", "text": "Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Felix Dangel Vector Institute Toronto, Canada fdangel@vectorinstitute.ai ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the transfer of theoretical and algorithmic ideas to convolutions. We simplify convolutions by viewing them as tensor networks (TNs) that allow reasoning about the underlying tensor multiplications by drawing diagrams, manipulating them to perform function transformations like differentiation, and efficiently evaluating them with einsum. To demonstrate their simplicity and expressiveness, we derive diagrams of various autodiff operations and popular curvature approximations with full hyper-parameter support, batching, channel groups, and generalization to any convolution dimension. Further, we provide convolutionspecific transformations based on the connectivity pattern which allow to simplify diagrams before evaluation. Finally, we probe performance. Our TN implementation accelerates a recently-proposed KFAC variant up to $4.5\\,\\mathrm{x}$ while removing the standard implementation\u2019s memory overhead, and enables new hardware-efficient tensor dropout for approximate backpropagation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Convolutional neural networks [CNNs, 39] mark a milestone in the development of deep learning architectures as their \u2018sliding window\u2019 approach represents an important inductive bias for vision tasks. Their intuition is simple to explain with graphical illustrations [e.g. 21]. Yet, convolutions are more challenging to analyze than dense layers in multi-layer perceptrons (MLPs) or transformers [71]. One reason is that they are hard to express in matrix notation and\u2014even in index notation\u2014compact expressions that are convenient to work with only exist for special hyper-parameters [e.g. 27, 2]. Many hyper-parameters (stride, padding, ...) and additional features like channel groups [36] introduce even more complexity that is inherited by related routines, e.g. for autodiff. We observe a delay of analytic and algorithmic developments between MLPs vs. CNNs, e.g. ", "page_idx": 0}, {"type": "text", "text": "\u2022 Approximate Hessian diagonal: 1989 vs. 2024   \n\u2022 Hessian rank: 2021 vs. 2023   \n\u2022 Gradient descent learning dynamics: 2014 vs. 2023   \n\u2022 Neural tangent kernel (NTK): 2018 vs. 2019   \n\u2022 Kronecker-factored quasi-Newton methods: 2021 vs. 2022   \n\u2022 Kronecker-factored curvature (KFAC, KFRA, KFLR): (2015, 2017, 2017) vs. (2016, 2020, 2020) ", "page_idx": 0}, {"type": "text", "text": "The software support for less standard routines some of these methods require also reflects this gap. Some functions only support special dimensions [15]. Others use less efficient workarounds (\u00a75.1) or are not provided at all (\u00a7B.4). And they are hard to modify as the code is either closed-source [12] or ", "page_idx": 0}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/628ac25195de398124b3d4713f1ddc34400da3c6369181d5f05cbfe35244e8aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (top) requires unfolding the input (high memory). The TN (middle) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (Bottom) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost. ", "page_idx": 1}, {"type": "text", "text": "written in a low-level language. This complicates the advance of existing, and the exploration of new, algorithmic ideas for convolutions. ", "page_idx": 1}, {"type": "text", "text": "Here, we seek to reduce this complexity gap by viewing convolutions as tensor networks [TNs, 53, 6, 9] which express the underlying tensor multiplications as diagrams. These diagrams are simpler to parse than mathematical equations and can seamlessly be (i) manipulated to take derivatives, add batching, or extract sub-tensors, (ii) merged with other diagrams, and (iii) evaluated with einsum. This yields simple, modifiable implementations that benefti from automated under-the-hoodoptimizations for efficient TN contraction developed by the quantum simulation community [e.g. 66, 25, 74, 13], like finding a high-quality contraction order or distributing computations: ", "page_idx": 1}, {"type": "text", "text": "1. We use the TN format of convolution from Hayashi et al. [29] to derive diagrams and einsum formulas for autodiff and less standard routines for curvature approximations with support for all hyper-parameters, batching, groups, and any dimension (Table 1).   \n2. We present transformations based on the convolution\u2019s connectivity pattern to re-wire and symbolically simplify TNs before evaluation (example in Figure 1).   \n3. We compare default and TN implementations, demonstrating optimal peak memory reduction and run time improvements up to $4.5\\,\\mathrm{x}$ for a recent KFAC variant, and showcase their flexibility to impose hardware-efficient dropout for randomized backpropagation. ", "page_idx": 1}, {"type": "text", "text": "Our work not only provides simpler perspectives and implementations that facilitate the exploration of algorithmic ideas for convolutions, but also directly advances second-order methods like KFAC: It enables more frequent pre-conditioner updates, using larger batches without going out of memory, and extending KFAC to transpose convolution. These improvements are important for second-order optimization and other applications like Laplace approximations [20] and influence functions [28]. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We briefly review 2d convolution (\u00a72.1), tensor multiplication and einsum (\u00a72.2), then introduce the graphical TN notation and apply it to convolution (\u00a72.3). Bold lower-case $(a)$ , upper-case $(A)$ , and upper-case sans-serif $(\\pmb{\\mathsf{A}})$ symbols indicate vectors, matrices, and tensors. Entries follow the same convention but use regular font weight; $[\\cdot]$ denotes slicing ( $[\\pmb{A}]_{i,j}=A_{i,j}$ ). Parenthesized indices mean reshapes, e.g. $[\\pmb{a}]_{(i,j)}=[\\pmb{A}]_{i,j}$ with $\\textbf{\\em a}$ the flattened matrix $\\pmb{A}$ . ", "page_idx": 1}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/4a2246e67a681e3a047fe834352582e242dfdf2cd267b58b0702b870b2e14e3c.jpg", "img_caption": ["Figure 2: TNs of (a) 2d convolution and (b,c) connections to its matrix multiplication view. The connectivity along each dimension is explicit via an index pattern tensor \u03a0. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.1 Convolution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2d convolutions process channels of 2d signals $\\pmb{\\mathbf{X}}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times I_{1}\\times I_{2}}$ with $C_{\\mathrm{in}}$ channels of spatial dimensions1 $I_{1},I_{2}$ by sliding a collection of $C_{\\mathrm{out}}$ filter banks, arranged in a kernel $\\boldsymbol{\\mathsf{W}}\\in\\mathbb{R}^{C_{\\mathrm{out}}\\mathrm{\\hat{\\times}}C_{\\mathrm{in}}\\times K_{1}\\times K_{2}}$ with kernel size $K_{1},K_{2}$ , over the input. The sliding operation depends on various hyperparameters [padding, stride, . . . , see 21]. At each step, the fliters are contracted with the overlapping area, yielding the channel values of a pixel in the output $\\mathbf{\\DeltaV}\\in\\mathbb{R}^{C_{\\mathrm{out}}\\times O_{1}\\times O_{2}}$ with spatial dimensions $O_{1},O_{2}$ . Optionally, a bias from $\\pmb{b}\\in\\mathbb{R}^{\\pmb{C}_{\\mathrm{out}}}$ is added per channel. ", "page_idx": 2}, {"type": "text", "text": "One way to implement convolution is via matrix multiplication [10], similar to fully-connected layers. First, one extracts the overlapping patches from the input for each output, then flattens and column-stacks them into a matrix $\\|\\mathbf{\\dot{X}}\\|\\in\\mathbb{R}^{C_{\\mathrm{in}}K_{1}K_{2}\\times O_{1}O_{2}}$ , called the unfolded input (or $\\mathtt{i m2c o l}$ ). Multiplying a matrix view $W\\in\\mathbb{R}^{\\hat{C}_{\\mathrm{out}}\\hat{\\times}C_{\\mathrm{in}}K_{1}K_{2}}$ of the kernel onto the unfolded input then yields a matrix view $\\mathbf{\\deltaY}$ of $\\mathbf{Y}$ (the vector of ones, ${\\bf1}_{O_{1}O_{2}}$ , copies the bias for each channel), ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\pmb{W}[\\pmb{[\\pmb{X}]}+b\\,\\mathbf{1}_{O_{1}O_{2}}^{\\top}\\in\\mathbb{R}^{C_{\\mathrm{out}}\\times O_{1}O_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We can also view convolution as an affine map of the flattened input $\\pmb{x}\\in\\mathbb{R}^{C_{\\mathrm{in}}I_{1}I_{2}}$ into a vector view y of Y with a Toeplitz-structured matrix A(W) \u2208RCoutO1O2\u00d7CinI1I2, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\mathscr{y}}=\\pmb{\\mathscr{A}}(\\pmb{\\mathscr{W}})\\pmb{\\mathscr{x}}+\\pmb{\\mathscr{b}}\\otimes\\mathbf{1}_{O_{1}O_{2}}\\in\\mathbb{R}^{C_{\\mathrm{out}}O_{1}O_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This perspective is uncommon in code, but used in theoretical works [e.g. 65] as it highlights the similarity between convolutions and dense layers. ", "page_idx": 2}, {"type": "text", "text": "2.2 Tensor Multiplication ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Tensor multiplication unifies outer (Kronecker), element-wise (Hadamard), and inner products and uses the input-output index relation to infer the multiplication type. We start with the binary case, then generalize to more inputs: Consider $\\mathbf{A},\\mathbf{B},\\mathbf{c}$ whose index names are described by the index tuples $S_{1},S_{2},S_{3}$ where $S_{3}\\subseteq(S_{1}\\cup S_{2})$ (converting tuples to sets if needed). Any product of $\\pmb{\\triangle}$ and $\\mathbf{B}$ can be described by the multiplication operator $^{*}(S_{1},S_{2},S_{3})$ with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\mathsf{C}}=\\ast_{(S_{1},S_{2},S_{3})}(\\boldsymbol{\\mathsf{A}},\\boldsymbol{\\mathsf{B}})\\quad\\Leftrightarrow\\quad[\\boldsymbol{\\mathsf{C}}]_{S_{3}}=\\sum_{(S_{1}\\cup S_{2})\\setminus S_{3}}[\\boldsymbol{\\mathsf{A}}]_{S_{1}}[\\boldsymbol{\\mathsf{B}}]_{S_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "summing over indices that are not present in the output. E.g., for two matrices $A,B$ , their product is $A B=*_{((i,j),(j,k),(i,k))}(A,B)$ (see $\\S\\Huge{H.}2\\Huge{)}$ , their Hadamard product $A\\odot B=*_{((i,j),(i,j),(i,j))}(A,B)$ , and their Kronecker product $A\\otimes B=*_{((i,j),(k,l),((i,k),(j,l)))}({\\cal{A}},{\\cal{B}})$ . Libraries support this functionality via einsum, which takes a string encoding of $S_{1},S_{2},S_{3}$ , followed by $\\mathsf{A},\\mathsf{B}$ . It also accepts longer sequences $\\pmb{\\mathscr{A}}_{1},\\ldots,\\pmb{\\mathscr{A}}_{N}$ with index tuples $S_{1},S_{2},\\ldots,S_{N}$ and output index tuple $S_{N+1}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\mathsf{A}}_{N+1}=*_{(S_{1},\\ldots,S_{N},S_{N+1})}(\\pmb{\\mathsf{A}}_{1},\\ldots,\\pmb{\\mathsf{A}}_{N})\\,\\Leftrightarrow\\,[\\pmb{\\mathsf{A}}_{N+1}]_{S_{N+1}}=\\sum_{\\left(\\bigcup_{n=1}^{N}S_{n}\\right)\\setminus S_{N+1}}\\left(\\prod_{n=1}^{N}[\\pmb{\\mathsf{A}}_{n}]_{S_{n}}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Table 1: Contraction expressions of operations related to 2d convolution. They include batching and channel groups, which are standard features in implementations. We describe each operation by a tuple of input tensors and a contraction string that uses the einops library\u2019s syntax [59] which can express index (un-)grouping. Some quantities are only correct up to a scalar factor which is suppressed for brevity. See $\\mathrm{\\SB}$ for visualizations and Table B3 for more operations. ", "page_idx": 3}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/7e692fc671136343987f8728ae51b92d28c74479ac69a3bbd6a1ee81de6c67e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Binary and $N$ -ary tensor multiplication are commutative: We can simultaneously permute operands and their index tuples without changing the result, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{*_{(S_{1},S_{2},S_{3})}(\\mathbf{A},\\mathbf{B})=*_{(S_{2},S_{1},S_{3})}(\\mathbf{B},\\mathbf{A})\\,,}&{*_{(\\mathcal{S},S_{i},\\ldots,S_{j},\\cdot)}(\\ldots,\\mathbf{A}_{i},\\ldots,\\mathbf{A}_{j},\\ldots)}&{=*_{(\\cdot,S_{j},\\cdot,S_{i},\\cdot)}(\\cdot,\\mathbf{A}_{j},\\ldots,\\mathbf{A}_{i},\\cdot)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "They are also associative, i.e. we can multiply operands in any order. However, the notation becomes involved as it requires additional set arithmetic to detect summable indices (see $\\S\\mathrm{H.1}$ for an example). ", "page_idx": 3}, {"type": "text", "text": "2.3 Tensor Networks & Convolution ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A simpler way to understand tensor multiplications is via diagrams developed by e.g. Penrose [53]. Rank- $K$ tensors are represented by nodes with $K$ legs labelled by the index\u2019s name2. ${\\underline{{\\left(a\\right)}}}-i$ denotes a vector $\\textbf{\\em a}$ , $i\\sqrt{B}\\!\\!\\!\\!\\int\\!\\!\\!\\!-j$ a matrix $_B$ , and $\\scriptstyle i\\,{\\overset{}{-}}(\\mathbf{c}\\,)=j$ a rank-3 tensor C. A Kronecker delta $[\\pmb{\\delta}]_{i,j}\\,=\\,\\delta_{i,j}$ is simply a line, $j-\\!\\!\\!\\!\\left(\\delta\\right)\\!\\!-\\!i\\;=j\\!\\!-\\!\\!\\!\\left(I\\right)\\!\\!-\\!\\!i\\;=\\!j\\!\\!-\\!\\!\\!\\!-i$ . Multiplications are indicated by connections between legs. For inner multiplication, we join the legs of the involved indices, e.g. the matrix multiplication diagram is $\\scriptstyle i-\\!{\\sqrt{A B}}-k\\;{\\bar{\\;}}=\\;i-\\!{\\sqrt{A}}-j-\\!{\\sqrt{B}}-k$ . Element-wise multiplication is similar, but with a leg sticking out. The Hadamard and Kronecker product diagrams are ", "page_idx": 3}, {"type": "equation", "text": "$$\ni-\\!\\!\\left(\\!\\!\\!\\frac{A\\odot B}{A\\odot B}\\!\\!\\!\\right)\\!\\!-\\!j=i-\\!\\!\\!\\left(\\!\\!\\!\\frac{\\!\\!\\!\\left(A\\right)\\!\\!\\!}{\\!\\!\\!\\left(B\\right)\\!\\!\\!}\\!\\!\\right)\\!\\!\\!-\\!j\\,,\\qquad\\quad(i,k)\\!\\!\\!-\\!\\!\\left(\\!\\!\\!\\!\\frac{A\\odot B}{A\\odot B}\\!\\!\\!\\right)\\!\\!-\\!(j,l)=(i,k)\\!\\!\\!-\\!\\!\\!\\left(\\!\\!\\!\\frac{\\!\\!\\!-d\\!\\!\\!\\!}{k\\!\\!\\!}\\!\\!\\right)\\!\\!-\\!\\!l^{\\!-j}\\!\\!\\!,\\!\\!\\right>\\!\\!\\!-(j,l)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the outer tensor product is a rank-4 tensor and must be reshaped (indicated by black triangles3) into a matrix. This syntax allows for extracting and embedding tensors along diagonals; e.g. taking a matrix diagonal, $\\overbrace{\\left(\\mathrm{diag}(A)\\!\\!\\right)}\\!\\!\\!-i=\\overbrace{\\left(\\!\\!\\!\\begin{array}{c}{A}\\end{array}\\!\\!\\!\\right)}\\!\\!\\!-i\\int$ , or forming a diagonal matrix, $\\iota\\!-\\!\\!\\!\\{\\!\\!\\!\\!\\{\\mathrm{diag}(a)\\!\\!\\!\\}\\!\\!-\\!\\!\\!i\\!\\!=\\!\\!\\!\\!\\!\\bar{\\iota}\\!\\!\\!-\\!\\!\\!\\!\\!\\bar{\\phi}_{\\overline{{{\\Delta}}}}\\bar{\\!\\!\\!-\\!\\!\\!\\!1}\\!\\!\\!\\!-\\!\\!\\!\\!i\\!\\!=\\!\\!\\!\\!\\!\\bar{\\iota}\\!\\!\\!-\\!\\!\\!\\!\\bar{\\phi}_{\\overline{{{\\Delta}}}}\\bar{\\!\\!\\!-\\!\\!\\!\\!1}\\!\\!\\!\\!-\\!\\!\\!\\!i$ ; and generalizes to larger diagonal blocks (\u00a7B). In the following, we stick to the simplest case to avoid the more advanced syntax. However, it shows the expressive power of TNs and is required to support common features of convolutions like channel groups (known as separable convolutions). ", "page_idx": 3}, {"type": "text", "text": "Application to convolution: We define a binary tensor $\\mathsf{P}\\in\\{0,1\\}^{I_{1}\\times O_{1}\\times K_{1}\\times I_{2}\\times O_{2}\\times K_{2}}$ which represents the connectivity pattern between input, output, and kernel. $P_{i_{1},o_{1},k_{1},i_{2},o_{2},k_{2}}$ is 1 if input locations $(i_{1},i_{2})$ overlap with kernel positions $(k_{1},k_{2})$ when computing output locations $\\left(o_{1},o_{2}\\right)$ and 0 otherwise. The spatial couplings are independent along each dimension, hence $\\mathsf{P}$ decomposes into $P_{i_{1},o_{1},k_{1},i_{2},o_{2},k_{2}}\\stackrel{\\cdot}{=}\\Pi_{i_{1},o_{1},k_{1}}^{(1)}\\bar{\\Pi}_{i_{2},o_{2},k_{2}}^{(\\tilde{2})}$ \u03a0i(22,)o2,k2 where the index pattern tensor \u03a0(j) \u2208{0, 1}Ij\u00d7Oj\u00d7Kj encodes the connectivity along dimension $j$ . With that, one obtains ", "page_idx": 3}, {"type": "equation", "text": "$$\nY_{c_{\\mathrm{out}},o_{1},o_{2}}=b_{c_{\\mathrm{out}}}+\\sum_{c_{\\mathrm{in}}=1}^{C_{\\mathrm{in}}}\\sum_{i_{1},i_{2}=1}^{I_{1},I_{2}}\\sum_{k_{1},k_{2}=1}^{K_{1},K_{2}}X_{c_{\\mathrm{in}},i_{1},i_{2}}\\Pi_{i_{1},o_{1},k_{1}}^{(1)}\\Pi_{i_{2},o_{2},k_{2}}^{(2)}W_{c_{\\mathrm{out}},c_{\\mathrm{in}},k_{1},k_{2}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Without bias, this translates into the diagram in Figure 2a. ", "page_idx": 3}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/c67ac183b8ca824967fbce9a446912087c7d74e3691eab2d4bce5ec0b4e627c8.jpg", "img_caption": ["Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. W is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. X. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with $\\boldsymbol{\\mathsf{V}}^{(\\mathsf{Y})}$ is highlighted. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3 TNs for Convolution Operations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now demonstrate the elegance of TNs for computing derivatives (\u00a73.1), autodiff operations (\u00a73.2), and approximate second-order information (\u00a73.3) by graphical manipulation. For simplicity, we exclude batching (vmap-ing like in JAX [8]) and channel groups, and provide the diagrams with full support in $\\mathrm{\\SB}$ . Table 1 summarizes our derivations (with batching and groups). As a warm-up, we identify the unfolded input and kernel from the matrix-multiplication view (Equations (1) and (2)). They follow by contracting the index patterns with either the input or kernel (Figures 2b and 2c), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle[\\left\\lVert\\mathbf{X}\\right\\rVert]_{(c_{\\mathrm{in}},k_{1},k_{2}),(o_{1},o_{1})}=\\sum_{i_{1},i_{2}}X_{c_{\\mathrm{in}},i_{1},i_{2}}\\Pi_{i_{1},o_{1},k_{1}}^{(1)}\\Pi_{i_{2},o_{2},k_{2}}^{(2)}\\,,}\\\\ &{\\displaystyle[\\mathbf{A}(\\mathbf{W})]_{(c_{\\mathrm{out}},o_{1},o_{2}),(c_{\\mathrm{in}},i_{1},i_{2})}=\\sum_{k_{1},k_{2}}\\Pi_{i_{1},o_{1},k_{1}}^{(1)}\\Pi_{i_{2},o_{2},k_{2}}^{(2)}W_{c_{\\mathrm{out}},c_{\\mathrm{in}},k_{1},k_{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.1 Tensor Network Differentiation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Derivatives play a crucial role in theoretical and practical ML. First, we show that differentiating a TN diagram amounts to a simple graphical manipulation. Then, we derive the Jacobians of convolution. Consider an arbitrary TN represented by the tensor multiplication from Equation (4). The Jacobian tensor $[\\mathsf{J}_{\\mathsf{A}_{j}}\\mathsf{A}_{N+1}]_{S_{\\star}+1},S_{\\star_{j}}^{\\prime}=\\partial[\\mathsf{A}_{N+1}]_{S_{N+1}}/\\partial[\\mathsf{A}_{j}]_{S_{\\star}^{\\prime}}$ w.r.t. an input ${\\pmb\\mathscr{A}}_{j}$ collects all partial derivatives and is addressed through indices $S_{n+1}\\times S_{j}^{\\prime}$ with $\\boldsymbol{S}_{j}^{\\boldsymbol{\\phi}}$ an independent copy of $S_{j}$ . Assume that ${\\pmb\\mathscr{A}}_{j}$ only enters once in the tensor multiplication. Then, taking the derivative of Equation (4) w.r.t. $[\\pmb{\\mathsf{A}}_{j}]_{S_{j}^{\\prime}}$ simply replaces the tensor by a Kronecker delta $\\delta_{S_{j},S_{j}^{\\prime}}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial[\\mathbf{A}_{N+1}]_{S_{N+1}}}{\\partial[\\mathbf{A}_{j}]_{S_{j}^{\\prime}}}=\\sum_{\\left(\\bigcup_{n=1}^{N}S_{n}\\right)\\setminus S_{n+1}}[\\mathbf{A}_{1}]_{S_{1}}\\cdot\\cdot\\cdot[\\mathbf{A}_{j-1}]_{S_{j-1}}\\prod_{i\\in S_{j}}\\delta_{i,i^{\\prime}}[\\mathbf{A}_{j+1}]_{S_{j+1}}\\cdot\\cdot\\cdot[\\mathbf{A}_{N}]_{S_{N}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If an index $i\\in S_{j}$ is summed, $i\\not\\in S_{n+1}$ , we can sum the Kronecker delta $\\delta_{i,i^{\\prime}}$ , effectively replacing all occurrences of $i$ by $i^{\\prime}$ . If instead $i$ is part of the output index, $i\\,\\in\\,S_{n+1}$ , the Kronecker delta remains part of the Jacobian and imposes structure. Figure 3a illustrates this process in diagrams for differentiating a convolution w.r.t. its kernel. Equation (6) amounts to cutting out the argument of differentiation and assigning new indices to the resulting open legs. For the weight Jacobian $\\mathbf{J}_{\\mathbf{W}}\\mathbf{Y}$ , this introduces structure: If we re-interpret the two sub-diagrams in Figure 3a as matrices, compare with the Kronecker diagram from Equation (5) and use Figure 2b, we find $\\mathbb{[}\\pmb{\\mathscr{X}}\\mathbb{]}^{\\top}\\otimes\\pmb{I}_{C_{\\mathrm{out}}}$ for the Jacobian\u2019s matrix view [e.g. 16]. Figure 3b shows the input Jacobian $\\mathbf{J}_{\\mathbf{X}}\\mathbf{Y}$ whi c h i s a tensor view of $A(\\mathsf{W})$ , as expected from the matrix-vector perspective of Equation (2). ", "page_idx": 4}, {"type": "text", "text": "Differentiating a TN is more convenient than using matrix calculus [44] as it amounts to a simple graphical manipulation, does not rely on a flattening convention, and therefore preserves the full index structure. The resulting TN can still be translated back to matrix language, if desired. It also simplifies the computation of higher-order derivatives (e.g. $\\partial^{2}\\mathbf{v}/\\partial\\mathbf{w}\\partial\\mathbf{x})$ , since differentiation yields another TN and can thus be repeated. If a tensor occurs more than once in a TN, the product rule applies and the derivative is a sum of TNs with one occurrence removed. ", "page_idx": 4}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/0907cd4b62b34ce58c7fc7199755546a9fc714284e4e1f916d03a582559d949b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: TNs of input-based Kronecker factors for KFAC approximations of the Fisher/GGN (no batching, no groups). The unfolded input is shaded, only additional contractions are highlighted. (a) $\\Omega$ (KFC/KFAC-expand) from Grosse & Martens [27], (b) $\\hat{\\Omega}$ (KFAC-reduce) from Eschenhagen et al. [23] (vectors of ones effectively amount to sums). ", "page_idx": 5}, {"type": "text", "text": "3.2 Autodiff & Connections to Transpose Convolution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although Jacobians are useful, crucial routines for autodiff are vector-Jacobian and Jacobian-vector products (VJPs, JVPs). Both are simple to realize with TNs due to access to full Jacobians. VJPs are used in backpropagation to pull back a tensor $\\mathbf{V}^{(\\mathsf{Y})}\\in\\mathbb{R}^{C_{\\mathrm{out}}\\times O_{1}\\times O_{2}}$ from the output to the input or weight space. The VJP results $\\mathbf{V}^{(\\mathbf{X})}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times I_{1}\\times I_{2}}$ and $\\boldsymbol{\\mathsf{V}}^{(\\mathsf{W})}\\in\\mathbb{R}^{C_{\\mathrm{out}}\\times C_{\\mathrm{in}}\\times K_{1}\\times K_{2}}$ are ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{c_{\\mathrm{m}}^{\\prime},i_{1}^{\\prime},i_{2}^{\\prime}}^{\\left(\\mathbf{X}\\right)}=\\sum_{c_{\\mathrm{out}},\\sigma_{1},\\sigma_{2}}V_{c_{\\mathrm{out}},\\sigma_{1},\\sigma_{2}}^{\\left(\\mathbf{Y}\\right)}\\frac{\\partial Y_{\\mathrm{cout},\\sigma_{1},\\sigma_{2}}}{\\partial X_{c_{\\mathrm{in}}^{\\prime},i_{1}^{\\prime},i_{2}^{\\prime}}}\\,,\\qquad V_{c_{\\mathrm{out}}^{\\prime},c_{\\mathrm{in}}^{\\prime},k_{1}^{\\prime},k_{2}^{\\prime}}^{\\left(\\mathbf{W}\\right)}=\\sum_{c_{\\mathrm{out}},\\sigma_{1},\\sigma_{2}}V_{c_{\\mathrm{out}},\\sigma_{1},\\sigma_{2}}^{\\left(\\mathbf{Y}\\right)}\\frac{\\partial Y_{\\mathrm{cout},\\sigma_{1},\\sigma_{2}}}{\\partial W_{c_{\\mathrm{out}}^{\\prime},c_{\\mathrm{in}}^{\\prime},k_{1}^{\\prime},k_{2}^{\\prime}}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Both are simply new TNs constructed from contracting the vector with the respective Jacobian, see Figures $3\\mathrm{c}$ and 3d (VJPs are analogous). The input VJP is often used to define transpose convolution [21]. In the matrix-multiplication perspective (Equation (2)), this operation is defined relative to a convolution with kernel $\\boldsymbol{\\mathsf{W}}$ by multiplication with $\\bar{\\b{A}}(\\mathbf{\\dot{W}})^{\\top}$ , i.e. using the same connectivity pattern but mapping from the convolution\u2019s output to input space. The TN in Figure 3d makes this sharing explicit and cleanly defines transpose convolution.4 ", "page_idx": 5}, {"type": "text", "text": "3.3 Kronecker-factored Approximate Curvature ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Jacobian diagrams allow us to construct the TNs of second-order information like the Fisher/generalized Gauss-Newton (GGN) matrix and sub-tensors like its diagonal (\u00a7C). Here, we focus on the popular Kronecker-factored approximation of the GGN [47, 27, 23, 48] whose input-based Kronecker factor relies on the unfolded input $[\\![\\pmb{\\ X}]\\!]$ which requires large memory. State-of-the-art libraries that provide access to KFAC [17, 51] also use this approach. Using TNs, we can often avoid expanding $\\bar{\\mathbb{I}}{\\pmb X}\\mathbb{I}$ explicitly and save memory. Here, we describe the existing KFAC approximations and their TNs ( se e $\\S5.1$ for their run time evaluation). ", "page_idx": 5}, {"type": "text", "text": "KFC (KFAC-expand): Grosse & Martens [27] introduce a Kronecker approximation for the kernel\u2019s GGN, $G\\,\\approx\\,\\Omega\\,\\otimes\\,\\Gamma$ where $\\mathbf{T}\\,\\in\\,\\mathbb{R}^{C_{\\mathrm{out}}\\times\\bar{C}_{\\mathrm{out}}}$ and the input-based factor $\\pmb{\\Omega}\\;=\\;\\mathbb{[X]}\\mathbb{[X]}^{\\top}\\;\\in$ $\\mathbb{R}^{C_{\\mathrm{in}}K_{1}K_{2}\\times C_{\\mathrm{in}}K_{1}K_{2}}$ (Figure 4a), the unfolded input\u2019s self-inner product (averaged over  a  ba t ch ). ", "page_idx": 5}, {"type": "text", "text": "KFAC-reduce: Eschenhagen et al. [23] generalized KFAC to graph neural networks and transformers based on the concept of weight sharing, also present in convolutions. They identify two approximations: KFAC-expand and KFAC-reduce. The former corresponds to KFC [27]. The latter shows similar performance in downstream tasks, but is cheaper to compute. It relies on the column-averaged unfolded input, i.e. the average over all patches sharing the same weights. KFAC-reduce approximates $G\\approx\\hat{\\Omega}\\otimes\\hat{\\hat{\\Gamma}}$ with $\\hat{\\mathbf{T}}\\in\\mathbb{R}^{C_{\\mathrm{out}}\\times C_{\\mathrm{out}}}$ and $\\hat{\\Omega}={}^{1}/(O_{1}O_{2})^{2}\\mathbf{1}_{O_{1}O_{2}}^{\\top}[\\![\\mathbf{X}]\\!](\\mathbf{1}_{O_{1}O_{2}}^{\\top}[\\![\\mathbf{X}]\\!]\\!)^{\\top}\\in\\mathbb{R}^{C_{\\mathrm{in}}K_{1}K_{2}\\times C_{\\mathrm{in}}K_{1}K_{2}}$ (Figure $4\\mathrm{b}$ ; averaged over a batch). For convolutions, this is a rg uably a \u2018 mo re natural\u2019 approximation as it becomes exact in certain limits [23], in contrast to the expand approximation. ", "page_idx": 5}, {"type": "text", "text": "KFAC for transpose convolution: Our approach enables us to derive KFAC for transpose convolutions. We are not aware of previous works doing so. This seems surprising because, similar to $\\S2.1$ , transpose convolution can be seen as matrix multiplication between the kernel and an unfolded input. From this formulation we can immediately obtain KFAC through the weight sharing view of Eschenhagen et al. [23]. The Kronecker factor requires unfolding the input similar to im2col, but for transpose convolutions. This operation is currently not provided by ML libraries. We can overcome this limitation, express the unfolding operation as TN, and\u2014for the first time\u2014establish KFAC (expand and reduce) for transpose convolutions (see $\\S B.4$ for details). ", "page_idx": 5}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/ac0d452fb17f167cfc2012ba19e0ab38638d785bd5aeaaf0237ecb283b4c0594.jpg", "img_caption": ["(c) Down-sampling convolution "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 5: TN illustrations of index pattern simplifications and transformations. See $\\S\\,\\mathrm{D}.3$ for the math formulation. ", "page_idx": 6}, {"type": "text", "text": "4 TN Simplifications & Implementation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Many convolutions in real-world CNNs use structured connectivity patterns that allow for simplifications which we describe here along with implementation aspects. ", "page_idx": 6}, {"type": "text", "text": "4.1 Index Pattern Structure & Simplifications ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The index pattern $\\boldsymbol{\\Pi}$ encodes the connectivity of a convolution and depends on its hyper-parameters. Along one dimension, $\\pmb{\\Pi}=\\pmb{\\Pi}(I,K,S,P,D)$ with input size $I$ , kernel size $K$ , stride $S$ , padding $P$ , and dilation $D$ . We provide pseudo-code for computing $\\boldsymbol{\\Pi}$ in $\\S D$ which is easy to implement efficiently with standard functions from any numerical library (Algorithm D1). Its entries are ", "page_idx": 6}, {"type": "equation", "text": "$$\n[\\Pi(I,K,S,P,D)]_{i,o,k}=\\delta_{i,1+(k-1)D+(o-1)S-P}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $i\\ =\\ 1,\\ldots,I,o\\ =\\ 1,\\ldots,O,k\\ =\\ 1,\\ldots,K$ and output size $O(I,K,S,P,D)\\ =\\ 1\\ +$ $\\lfloor(I+2P-(K+(K-1)(D-1)))/S\\rfloor$ . Since $\\sqcap$ is binary and has size linear in $I,O,K$ , it is cheap to precompute and cache. The index pattern\u2019s symmetries allow for re-wiring a TN. For instance, the symmetry of $(k,D)$ and $(o,S)$ in Equation (7) and $O(I,K,S,P,D)$ permits a kernel-output swap, exchanging the role of kernel and output dimension (Figure 5a). Rochette et al. [58] used this to phrase the per-example gradient computation (Figure 3c) as convolution. ", "page_idx": 6}, {"type": "text", "text": "For many convolutions of real-world CNNs (see $\\mathrm{\\SE}$ for a hyper-parameter study) the index pattern possesses structure that simplifies its contraction with other tensors into either smaller contractions or reshapes: Dense convolutions use a shared kernel size and stride, and thus process non-overlapping adjacent tiles of the input. Their index pattern\u2019s action can be expressed as a cheap reshape (Figure 5b). Such convolutions are common in DenseNets [33], MobileNets [31, 60], ResNets [30], and ConvNeXts [42]. InceptionV3 [69] has 2d mixed-dense convolutions that are dense along one dimension. Down-sampling convolutions use a larger stride than kernel size, hence only process a sub-set of their input, and are used in ResNet18 [30], ResNext101 [72], and WideResNet101 [73]. Their pattern contracts with a tensor $\\mathsf{v}$ like that of a dense convolution with a sub-tensor $\\tilde{\\bf{y}}$ (Figure 5c). $\\S5.1$ shows that those simplifications accelerate computations. ", "page_idx": 6}, {"type": "text", "text": "4.2 Practical Benefits of the TN Abstraction & Limitations for Convolutions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Contraction order optimization: There exist various orders in which to carry out the summations in a TN and their performance can vary by orders of magnitude. One extreme approach is to carry out all summations via nested for-loops. This so-called Feynman path integral algorithm requires little memory, but many FLOPS since it does not re-cycle intermediate results. The other extreme is sequential pair-wise contraction. This builds up intermediate results and can greatly reduce FLOPS. The schedule is represented by a binary tree, but the underlying search is in general at least #Phard [14]. Fortunately, there exist heuristics to find high-quality contraction trees for TNs with hundreds of tensors [32, 25, 13], implemented in packages like opt_einsum [66]. ", "page_idx": 6}, {"type": "text", "text": "Index slicing: A common problem with high-quality schedules is that intermediates exceed memory. Dynamic slicing [32] (e.g. cotengra [25]) is a simple method to decompose a contraction until it becomes feasible by breaking it up into smaller identical sub-tasks whose aggregation adds a small overhead. This enables peak memory reduction and distribution. ", "page_idx": 6}, {"type": "text", "text": "Sparsity: \u03a0 is sparse as only a small fraction of the input contributes to an output element. For a convolution with stride $S\\,<\\,K$ and default parameters $\\'P=0,D=1\\rangle$ , for fixed output and kernel indices $k,o$ , there is exactly one non-zero entry in $[\\pmb{\\Pi}]_{:,o,k}$ . Hence $\\mathrm{nnz}(\\boldsymbol{\\Pi})=O K$ , which corresponds to a sparsity of $^1\\!/\\!I$ . Padding leads to more kernel elements that do not contribute to an output pixel, and therefore a sparser \u03a0. For down-sampling and dense convolutions, we showed how \u03a0\u2019s algebraic structure allows to simplify its contraction. However, if that is not possible, \u03a0 contains explicit zeros that add unnecessary FLOPS. One way to circumvent this is to match a TN with that of an operation with efficient implementation (like im2col, (transpose) convolution) using transformations like the kernel-output swap or by introducing identity tensors to complete a template, as done in Rochette et al. [58], Dangel [15] for per-sample gradients and im2col. ", "page_idx": 6}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/82440b4eae399c06a4d47f851307089e9c239828dc6a079b25c1763cb66d808f.jpg", "img_caption": ["Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Approximate contraction & structured dropout: TNs offer a principled approach for stochastic approximation via Monte-Carlo estimation to save memory and run time at the cost of accuracy. The basic idea is best explained on a matrix product $\\begin{array}{r}{C:=A B=\\sum_{n=1}^{N}\\ensuremath{[\\mathbf{A}]_{:,n}}\\left[B\\right]_{n,1}}\\end{array}$ : with $A\\overset{\\cdot}{\\in}\\mathbb{R}^{I\\times N}$ , $\\boldsymbol{B}\\in\\mathbb{R}^{N,O}$ . To approximate the sum, we introduce a distribu tion over $n$ \u2019s range, then use column-row-sampling [CRS, 1] to form an unbiased Monte-Carlo approximation with sampled indices, which only requires the sub-matrices with active column-row pairs. Bernoulli-CRS samples without replacement by assigning a Bernoulli random variable Bernoulli $\\left(\\pi_{n}\\right)$ with probability $\\pi_{n}$ for column-row pair $n$ to be included in the contraction. The Bernoulli estimator is $\\begin{array}{r}{\\tilde{C}:=\\sum_{n=1}^{N}z_{n}/\\pi_{n}\\left[\\boldsymbol{A}\\right]_{n,:}\\left[\\boldsymbol{B}\\right]_{n,:}}\\end{array}$ with $z_{n}\\sim\\mathrm{Bernoulli}(\\pi_{n})$ . W ith a shar ed keep probabi lity, $\\pi_{n}:=p$ this yields the unbiased estimator $C^{\\prime}={}^{1}\\!/p\\sum_{n=1,...,N}A^{\\prime}B^{\\prime}$ where and $B^{\\prime}=K B$ with $\\begin{array}{r}{K=\\mathrm{diag}(z_{1},\\dots,z_{N})}\\end{array}$ are the sub-matrices of $A,B$ containing the active column-row pairs. CRS applies to a single contraction. For TNs with multiple sums, we can apply it individually, and also impose a distribution over the result indices, which computes a (scaled) sub-tensor. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we demonstrate computational benefits of TNs for less standard routines of second-order methods and showcase their flexibility to perform stochastic autodiff in novel ways. ", "page_idx": 7}, {"type": "text", "text": "5.1 Run Time Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We implement the presented TNs\u2019 contraction strings and operands5 in PyTorch [52]. The simplifications from $\\S4$ can be applied on top and yield a modified einsum expression. To find a contraction schedule, we use opt_einsum [66] with default settings. We extract the unique convolutions of 9 architectures for ImageNet and smaller data sets, then compare some operations from Table 1 with their standard implementation on an Nvidia Tesla T4 GPU (16 GB); see $\\S\\mathrm{F}$ for all details. Due to space constraints, we highlight important insights here and provide references to the corresponding material in the appendix. In general, the performance gap between standard and TN implementation decreases the less common an operation is (Figure F17); from forward pass (inference & training), to VJPs (training), to KFAC (training with a second-order method). This is intuitive as more frequently used routines have been optimized more aggressively. ", "page_idx": 7}, {"type": "text", "text": "Impact of simplifications: While general convolutions remain unaffected (Figure F18d) when applying the transformations of $\\S4$ , mixed dense, dense, and down-sampling convolutions consistently enjoy significant run time improvements (Figures F18a to F18c). As an example, we show the performance comparison for dense convolutions in Figure 6: The performance ratio\u2019s median between TN and standard forward and input VJP is close to 1, that is both require almost the same time. In the median, the TN even outperforms PyTorch\u2019s highly optimized weight VJP, also for down-sampling ", "page_idx": 7}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/ffa9c91f6d95be214f4d864623d45add0f181a3148b98c43544645380ed6b990.jpg", "img_caption": ["convolutions (Figure F21). For KFC, the median performance ratios are well below 1 for dense, mixed dense & sub-sampling convolutions (Figure F22). ", "Figure 7: Extra memory used by the standard versus our TN implementation (simplifications enabled) of KFAC-reduce. Each point represents a convolution from 9 CNNs, clipped below by 1 MiB. TNs consistently use less memory than the standard implementation (one exception), and often no extra memory at all. We observe memory savings up to 3 GiB. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "KFAC-reduce: For all convolution types, the TN implementation achieves its largest improvements for $\\hat{\\Omega}$ and consistently outperforms the PyTorch implementation in the median when simplifications are enabled (Figure F23). The standard implementation unfolds the input, takes the row-average, then forms its outer product. The TN does not need to expand $[\\![\\pmb{\\ X}]\\!]$ in memory and instead averages the index pattern tensors, which reduces peak memory and run  t im e. We observe performance ratios down to $0.22\\,\\mathrm{x}$ (speed-ups up to $\\approx4.5\\,\\mathrm{{x}}$ , Table F9) and consistently lower memory consumption with savings up to 3 GiB (Figure 7). Hence, our approach not only significantly reduces the overhead of 2nd-order optimizers based on KFAC-reduce, but also allows them to operate on larger batches without exceeding memory (Eschenhagen et al. [23] specifically mention memory as important limitation of their method). Other examples for KFAC algorithms where computing the input-based Kronecker factor adds significant time and memory overhead are that of Petersen et al. [54], Benzing [5] which only use $\\pmb{\\Omega}$ (setting $\\mathbf{\\Gamma}\\Gamma\\propto I^{\\mathbf{\\partial}}$ ), or Lin et al. [41, 40] which remove matrix inversion. ", "page_idx": 8}, {"type": "text", "text": "Downstream improvements with KFAC-reduce: To demonstrate the speed-ups of KFAC-reduce in practical algorithms, we apply our work to the SINGD optimizer [41] and benchmark the impact of our TN implementation on its memory and run time in comparison to SGD without momentum. Concretely, we investigate SINGD with KFAC-reduce and diagonal pre-conditioners on ResNet18 and VGG19 on ImageNet-like synthetic data (3, 256, 256) using a batch size of 128. We measured per-iteration time and peak memory on an NVIDIA A40 with $48\\ \\mathrm{GiB}$ of RAM. For SINGD, we compare computing the Kronecker factors with the standard approach (\u2018SINGD\u2019) via input unfolding versus our TN implementation ( $^{\\bullet}\\mathrm{SINGD+TN^{\\circ}})$ ). Table 2 summarizes the results. ", "page_idx": 8}, {"type": "text", "text": "On both nets, our TN implementation halves SINGD\u2019s run time, and almost completely eliminates the memory, overhead compared to SGD. On VGG19, it dramatically lowers the memory overhead, cutting it down by a factor of 2 from 32 GiB to 16 GiB. This enables using larger batches or more frequently updating the pre-conditioner, underlining the utility of our approach for reducing the computational gap between approximate second-order and first-order methods. ", "page_idx": 8}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/7ee9e29d91ede0e7dcb7ed2d6d8e3e18698fa11dbe1117b19c66426fb36a2db9.jpg", "table_caption": ["Table 2: Impact of our TN implementation on SINGD\u2019s run time and peak memory compared to SGD. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Randomized Autodiff via Approximate Contraction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "CRS is an alternative to checkpointing [26] to lower memory consumption of backpropagation [50, 11, 1]. Here, we focus on unbiased gradient approximations by applying the exact forward pass, but CRS when computing the weight VJP, which requires storing a sub-tensor of $\\pmb{\\Upsilon}$ . For convolutions, the approaches of existing works are limited by the supported functionality of ML libraries. Adelman et al. [1] restrict to sampling $\\pmb{\\Upsilon}$ along $c_{\\mathrm{in}}$ , which eliminates many gradient entries as the index is part of the gradient. The randomized gradient would thus only train a sub-tensor of the kernel per step. Oktay et al. [50], Chen et al. [11] apply unstructured dropout to $\\pmb{\\Upsilon}$ , store it in sparse form, and restore the sparsified tensor during the backward pass. This reduces memory, but not computation. ", "page_idx": 8}, {"type": "text", "text": "Our TN implementation is more flexible and can, for example, tackle spatial dimensions with CRS. This reduces memory to the same extent, but also run time due to fewer contractions. Importantly, it does not zero out the gradient for entire filters. In Figure 8 we compare the gradient approximation errors of channel and spatial sub-sampling. For the same memory reduction, spatial sub-sampling yields a smaller approximation error on both real $\\&$ synthetic data. E.g., instead of keeping $75\\,\\%$ of channels, we achieve the same approximation quality using only $35\\,\\%$ of pixels. ", "page_idx": 8}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/d1918627959f78d1336e8b862d8db83b58f82f58314daafc3fe23d1a7fd87dc1.jpg", "img_caption": ["Figure 8: Sampling spatial axes is more effective than channels on both (a) real-world and (b) synthetic data. We take the untrained All-CNN-C [68] for CIFAR-100 with cross-entropy loss, disable dropout, and modify the convolutions to use a fraction $p$ of $\\pmb{\\Upsilon}$ when computing the weight gradient via Bernoulli-CRS. For mini-batches of size 128, we compute the deterministic gradients for all kernels, then flatten and concatenate them into a vector $\\textbf{\\textit{g}}$ ; likewise for its proxy $\\hat{\\pmb g}$ . CRS is described by $(p_{c_{\\mathrm{in}}},p_{i_{1}},p_{i_{2}})$ , the keep rates along the channel and spatial dime\u221ansio\u221ans. We compare channel and spatial sampling with same memory reduction, i.e. $(p,1,1)$ and $(1,{\\sqrt{p}},{\\sqrt{p}})$ . To measure approximation quality, we use the normalized residual norm $\\|\\pmb{g}-\\hat{\\pmb{g}}\\|_{2}\\Big/\\|\\pmb{g}\\|_{2}$ and report mean and standard deviation of 10 different model and batch initializations. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Structured convolutions: We use the TN formulation of convolution from Hayashi et al. [29] who focus on connecting kernel factorizations to existing (depth-wise separable [31, 60], factored [69], bottleneck [30], flattened/CP decomposed, low-rank fliter [67, 57, 70]) convolutions and explore new factorizations. Our work focuses on operations related to convolutions, diagram manipulations, the index pattern structure, and computational performance/flexibility. Structured convolutions integrate seamlessly with our framework by replacing the kernel with its factorized TN. ", "page_idx": 9}, {"type": "text", "text": "Higher-order autodiff: ML frameworks focus on differentiating scalar-valued objectives once. Recent works [37, 38, 43] developed a tensor calculus to compute higher-order derivatives of tensorvalued functions and compiler optimizations through linear algebra and common sub-expression elimination. Phrasing convolution as einsum, we allow it to be integrated into such frameworks, benefti from their optimizations, and complement them with our convolution-specific simplifications. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We used tensor networks (TNs), a diagrammatic representation of tensor multiplications, to simplify convolutions and many related operations. We derived the diagrams of autodiff and less standard routines for curvature approximations like KFAC with support for all hyper-parameters, channel groups, batching, and generalization to arbitrary dimensions. All amount to simple einsum expressions that can easily be modified\u2014e.g. to perform stochastic backpropagation\u2014and benefit from under-the-hood optimizations before evaluation. We complemented those by convolution-specific symbolic simplifications based on structure in the connectivity pattern and showed their effectiveness to advance second-order methods. Our TN implementation accelerates the computation of KFAC up to $4.5\\,\\mathrm{x}$ and uses significantly less memory. Beyond performance improvements, the simplifying perspective also allowed us to formulate KFAC for transpose convolution. More broadly, our work underlines the elegance of TNs for reasoning about tensor multiplications and function transformations (differentiation, batching, slicing, simplification) in terms of diagrams at less cognitive load without sacrificing rigour. We believe they are a powerful tool for the ML community that will open up new algorithmic possibilities due to their simplicity & flexibility. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The author would like to thank Luca Thiede, Andres Fernandez Rodr\u00edguez, and Kirill Neklyudov for providing feedback to the manuscript. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Adelman, M., Levy, K., Hakimi, I., and Silberstein, M. Faster neural network training with approximate tensor operations. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[2] Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely wide neural net. Advances in neural information processing systems (NeurIPS), 2019.   \n[3] Bahamou, A., Goldfarb, D., and Ren, Y. A mini-block fisher method for deep neural networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.   \n[4] Becker, S. and Lecun, Y. Improving the convergence of back-propagation learning with second-order methods. 1989.   \n[5] Benzing, F. Gradient descent on neurons and its link to approximate second-order optimization. In International Conference on Machine Learning (ICML), 2022.   \n[6] Biamonte, J. and Bergholm, V. Tensor networks in a nutshell, 2017.   \n[7] Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In International Conference on Machine Learning (ICML), 2017.   \n[8] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-Milne, S. JAX: composable transformations of Python+NumPy programs, 2018. [9] Bridgeman, J. C. and Chubb, C. T. Hand-waving and interpretive dance: an introductory course on tensor networks. Journal of Physics A: Mathematical and theoretical, 2017.   \n[10] Chellapilla, K., Puri, S., and Simard, P. High performance convolutional neural networks for document processing. In International Workshop on Frontiers in Handwriting Recognition, 2006.   \n[11] Chen, J., Xu, K., Wang, Y., Cheng, Y., and Yao, A. DropIT: Dropping intermediate tensors for memoryefficient DNN training. In International Conference on Learning Representations (ICLR), 2023.   \n[12] Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., and Shelhamer, E. cudnn: Efficient primitives for deep learning. 2014.   \n[13] cuQuantum development team, T. cuQuantum SDK: A high-performance library for accelerating quantum information science, 2023.   \n[14] Damm, C., Holzer, M., and McKenzie, P. The complexity of tensor calculus. computational complexity, 2002.   \n[15] Dangel, F. unfoldNd: (n=1,2,3)-dimensional unfold (im2col) and fold (col2im) in pytorch, 2021.   \n[16] Dangel, F., Harmeling, S., and Hennig, P. Modular block-diagonal curvature approximations for feedforward architectures. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.   \n[17] Dangel, F., Kunstner, F., and Hennig, P. BackPACK: Packing more into backprop. In International Conference on Learning Representations (ICLR), 2020.   \n[18] Dangel, F., Tatzel, L., and Hennig, P. ViViT: Curvature access through the generalized gauss-newton\u2019s low-rank structure. Transactions on Machine Learning Research (TMLR), 2022.   \n[19] Dangel, F. J. Backpropagation beyond the gradient. 2023.   \n[20] Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux - effortless bayesian deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[21] Dumoulin, V. and Visin, F. A guide to convolution arithmetic for deep learning. 2016.   \n[22] Elsayed, M., Farrahi, H., Dangel, F., and Mahmood, A. R. Revisiting scalable hessian diagonal approximations for applications in reinforcement learning. In International Conference on Machine Learning (ICML), 2024.   \n[23] Eschenhagen, R., Immer, A., Turner, R. E., Schneider, F., and Hennig, P. Kronecker-factored approximate curvature for modern neural network architectures. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[24] Goldfarb, D., Ren, Y., and Bahamou, A. Practical quasi-newton methods for training deep neural networks, 2021.   \n[25] Gray, J. and Kourtis, S. Hyper-optimized tensor network contraction. Quantum, 2021.   \n[26] Griewank, A. and Walther, A. Evaluating derivatives: principles and techniques of algorithmic differentiation. SIAM, 2008.   \n[27] Grosse, R. and Martens, J. A kronecker-factored approximate Fisher matrix for convolution layers. In International Conference on Machine Learning (ICML), 2016.   \n[28] Grosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., Hubinger, E., Luko\u0161i\u00afut\u02d9e, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., and Bowman, S. R. Studying large language model generalization with influence functions, 2023.   \n[29] Hayashi, K., Yamaguchi, T., Sugawara, Y., and Maeda, S.-i. Exploring unexplored tensor network decompositions for convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[30] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition (CVPR), 2016.   \n[31] Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. 2017.   \n[32] Huang, C., Zhang, F., Newman, M., Ni, X., Ding, D., Cai, J., Gao, X., Wang, T., Wu, F., Zhang, G., et al. Efficient parallelization of tensor network contraction for simulating quantum computation. Nature Computational Science, 2021.   \n[33] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In IEEE conference on computer vision and pattern recognition (CVPR), 2017.   \n[34] Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks, 2020.   \n[35] Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho\\*, K., and Geras\\*, K. The break-even point on optimization trajectories of deep neural networks. In International Conference on Learning Representations (ICLR), 2020.   \n[36] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012.   \n[37] Laue, S., Mitterreiter, M., and Giesen, J. Computing higher order derivatives of matrix and tensor expressions. Advances in Neural Information Processing Systems (NeurIPS), 2018.   \n[38] Laue, S., Mitterreiter, M., and Giesen, J. A simple and efficient tensor calculus. In AAAI Conference on Artificial Intelligence, 2020.   \n[39] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1989.   \n[40] Lin, W., Duruisseaux, V., Leok, M., Nielsen, F., Khan, M. E., and Schmidt, M. Simplifying momentumbased riemannian submanifold optimization. 2023.   \n[41] Lin, W., Dangel, F., Eschenhagen, R., Neklyudov, K., Kristiadi, A., Turner, R. E., and Makhzani, A. Structured inverse-free natural gradient descent: Memory-efficient & numerically-stable KFAC. In International Conference on Machine Learning (ICML), 2024.   \n[42] Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[43] Ma, L., Ye, J., and Solomonik, E. Autohoot: Automatic high-order optimization for tensors. In International Conference on Parallel Architectures and Compilation Techniques (PACT), 2020.   \n[44] Magnus, J. R. and Neudecker, H. Matrix Differential Calculus with Applications in Statistics and Econometrics. Probabilistics and Statistics. 1999.   \n[45] Martens, J. Deep learning via Hessian-free optimization. In International Conference on Machine Learning (ICML), 2010.   \n[46] Martens, J. New insights and perspectives on the natural gradient method, 2020.   \n[47] Martens, J. and Grosse, R. Optimizing neural networks with Kronecker-factored approximate curvature. In International Conference on Machine Learning (ICML), 2015.   \n[48] Martens, J., Ba, J., and Johnson, M. Kronecker-factored curvature approximations for recurrent neural networks. In International Conference on Learning Representations (ICLR), 2018.   \n[49] Novak, R., Sohl-Dickstein, J., and Schoenholz, S. S. Fast finite width neural tangent kernel. In International Conference on Machine Learning (ICML), 2022.   \n[50] Oktay, D., McGreivy, N., Aduol, J., Beatson, A., and Adams, R. P. Randomized automatic differentiation. In International Conference on Learning Representations (ICLR), 2021.   \n[51] Osawa, K., Ishikawa, S., Yokota, R., Li, S., and Hoefler, T. Asdl: A unified interface for gradient preconditioning in pytorch, 2023.   \n[52] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS). 2019.   \n[53] Penrose, R. Applications of negative dimensional tensors. Combinatorial Mathematics and its Applications, 1971.   \n[54] Petersen, F., Sutter, T., Borgelt, C., Huh, D., Kuehne, H., Sun, Y., and Deussen, O. ISAAC newton: Input-based approximate curvature for newton\u2019s method. In International Conference on Learning Representations (ICLR), 2023.   \n[55] Pinson, H., Lenaerts, J., and Ginis, V. Linear cnns discover the statistical structure of the dataset using only the most dominant frequencies. In International Conference on Machine Learning (ICML), 2023.   \n[56] Ren, Y., Bahamou, A., and Goldfarb, D. Kronecker-factored quasi-newton methods for deep learning, 2022.   \n[57] Rigamonti, R., Sironi, A., Lepetit, V., and Fua, P. Learning separable filters. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.   \n[58] Rochette, G., Manoel, A., and Tramel, E. W. Efficient per-example gradient computations in convolutional neural networks, 2019.   \n[59] Rogozhnikov, A. Einops: Clear and reliable tensor manipulations with einstein-like notation. In International Conference on Learning Representations (ICLR), 2022.   \n[60] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In IEEE conference on computer vision and pattern recognition (CVPR), 2018.   \n[61] Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2014.   \n[62] Schneider, F., Balles, L., and Hennig, P. DeepOBS: A deep learning optimizer benchmark suite. In International Conference on Learning Representations (ICLR), 2019.   \n[63] Schraudolph, N. N. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 2002.   \n[64] Singh, S. P., Bachmann, G., and Hofmann, T. Analytic insights into structure and rank of neural network hessian maps. Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[65] Singh, S. P., Hofmann, T., and Sch\u00f6lkopf, B. The hessian perspective into the nature of convolutional neural networks. 2023.   \n[66] Smith, D. G. A. and Gray, J. opt_einsum - A python package for optimizing contraction order for einsum-like expressions. Journal of Open Source Software (JOSS), 2018.   \n[67] Smith, S. W. The scientist and engineer\u2019s guide to digital signal processing. 1997.   \n[68] Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all convolutional net, 2015.   \n[69] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In IEEE conference on computer vision and pattern recognition (CVPR), 2016.   \n[70] Tai, C., Xiao, T., Zhang, Y., Wang, X., et al. Convolutional neural networks with low-rank regularization. 2015.   \n[71] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[72] Xie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., and He, K. Aggregated residual transformations for deep neural networks. In IEEE conference on computer vision and pattern recognition (CVPR), 2017.   \n[73] Zagoruyko, S. and Komodakis, N. Wide residual networks. 2016.   \n[74] Zhang, F. A parallel tensor network contraction algorithm and its applications in quantum computation. 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods (Supplementary Material) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Limitations 16   \nB Visual Tour of Tensor Network Operations for Convolutions 16   \nB.1 Convolution & First-order Derivatives 16   \nB.2 Exact Second-order Information 16   \nB.3 Kronecker-factored Approximate Curvature (KFAC) for Grouped Convolutions 18   \nB.4 Kronecker-factored Approximate Curvature (KFAC) for Transpose Convolution . 19   \nB.5 Further Operations & Extensive Overview . . . 20   \nC Exact Second-Order Information 24   \nD Implementation Details 25   \nD.1 Index Pattern Tensor Computation for Convolutions 25   \nD.2 Index Pattern Tensor for Standalone Transpose Convolution . 25   \nD.3 Details on Index Pattern Simplifications 25   \nE Convolution Layer Hyper-parameter Analysis 27   \nRun Time Evaluation Details (GPU) 30   \nF.1 Protocol & Overview 30   \nF.2 Forward Pass 32   \nF.3 Input VJP 35   \nF.4 Weight VJP 38   \nF.5 KFC Factor (KFAC-expand) 41   \nF.6 KFAC-reduce Factor 44   \nG Memory Evaluation Details (CPU) 47   \nG.1 Theoretical & Empirical Analysis for KFAC-reduce Factor 47   \nH Miscellaneous 50   \nH.1 Example: Associativity of Tensor Multiplication . . 50   \nH.2 Example: Matrix-matrix Multiplication as Tensor Multiplication 50 ", "page_idx": 14}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we comment on limitations on our approach. ", "page_idx": 15}, {"type": "text", "text": "No common sub-expression elimination (CSE): Our implementation relies on opt_einsum which focuses on contraction order optimization. This optimization is efficient when all operands are different. However, with multiple occurrences of operands, computing shared sub-expressions might be an advantageous optimization approach which opt_einsum does not account for. The second-order quantity TNs from $\\S C$ and $\\S3.3$ contain such sub-expressions, for instance $[\\![\\pmb{\\ X}]\\!]$ and $\\mathbf{1}_{O_{1}O_{2}}^{\\top}\\mathbb{[X]}$ in KFAC-expand and KFAC-reduce, and $\\pmb{\\ S}^{(\\mathsf{W})}$ in the GGN quantities from Fig u re  C16. The eff ici ency of CSE depends on how costly the shared tensor is to compute. For instance, computing $\\pmb{\\ S}^{(\\mathsf{W})}$ is expensive and therefore CSE is the more suitable optimization technique. For the input-based Kronecker factors which require the unfolded input, either contraction path optimization or CSE might be better. This is because the optimal contraction order may not correspond to $2\\mathbf{x}$ input unfolding and exhibit more parallelism which may lead to faster run times on a GPU. It would be interesting to integrate CSE into the contraction path optimization and develop a heuristic to choose a contraction path, for instance based on a weighted sum of FLOPs and memory. ", "page_idx": 15}, {"type": "text", "text": "No index slicing: We mention index slicing as a technique to reduce peak memory of, and distribute, TN contractions. However, our implementation does not use index slicing, although there are packages like cotengra [25] with an interface similar to opt_einsum. We did not experiment with index slicing as our benchmark uses a single GPU and did not encounter out-of-memory errors. Still, we mention this technique, as, in combination with CSE, it could automatically reduce peak memory of the GGN quantities from Figure C16 which suffer from high memory requirements. ", "page_idx": 15}, {"type": "text", "text": "B Visual Tour of Tensor Network Operations for Convolutions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we extend the presented operations with a batch axis and allow for grouped convolutions. ", "page_idx": 15}, {"type": "text", "text": "B.1 Convolution & First-order Derivatives ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Adding a batch dimension (vmap-ing): Adding a batch axis to all presented operations is trivial. We only need to add an additional leg to the batched tensors, and connect these legs via element-wise or inner multiplication, depending on whether the result tensor is batched or not. ", "page_idx": 15}, {"type": "text", "text": "Grouped convolutions: Grouped convolutions were originally proposed by Krizhevsky et al. [36] and allow for parallelizing, distributing, and reducing the parameters of the convolution operation. They split $C_{\\mathrm{in}}$ input channels into $G$ groups of size $\\bar{C}_{\\mathrm{in}}:=\\bar{C}_{\\mathrm{in}}/G$ , then perform independent convolutions per group, each producing $\\tilde{C}_{\\mathrm{out}}:=C_{\\mathrm{out}}/G$ output channels which are concatenated in the output. Each group uses a kernel $\\pmb{{\\mathsf{W}}}_{g}$ of size $\\tilde{C}_{\\mathrm{out}}\\times\\tilde{C}_{\\mathrm{in}}\\times K_{1}\\times K_{2}$ . These kernels are stacked into a single tensor $\\mathbf{W}\\in\\mathbb{R}^{C_{\\mathrm{out}},\\tilde{C}_{\\mathrm{in}},K_{1},K_{2}}$ such that $[\\pmb{\\mathsf{W}}]_{(g,:),:,:,:}=\\pmb{\\mathsf{W}}_{g}.$ To support groups, we thus decompose the channel indices into $c_{\\mathrm{in}}:=(\\tilde{c}_{\\mathrm{in}},g)$ and $c_{\\mathrm{out}}:=(\\tilde{c}_{\\mathrm{out}},g)$ . For the forward pass this yields the grouped convolution (without bias) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y_{(g,\\tilde{c}_{\\mathrm{out}}),o_{1},o_{2}}=\\sum_{i_{1},i_{2},\\tilde{c}_{\\mathrm{in}},k_{1},k_{2}}X_{(g,\\tilde{c}_{\\mathrm{in}}),i_{1},i_{2}}\\Pi_{i_{1},o_{1},k_{1}}^{(1)}\\Pi_{i_{2},o_{2},k_{2}}^{(2)}W_{(g,\\tilde{c}_{\\mathrm{out}}),c_{\\mathrm{in}},k_{1},k_{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Figure B9a shows the batched version of Equation (B8) as TN. Applying the differentiation rule from $\\S3$ leads to the Jacobians and VJPs shown in the remaining panels of Figure B9. ", "page_idx": 15}, {"type": "text", "text": "B.2 Exact Second-order Information ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Figure B12 we show the TNs for the GGN diagonal and the GGN Gram matrix (empirical NTK matrix) from Figure C16 extended by channel groups and a batch axis. ", "page_idx": 15}, {"type": "text", "text": "Diagonal block extraction: Combined with index un-grouping, diagonal extraction generalizes to larger blocks: Let $\\pmb{A}\\in\\mathbb{R}^{K I\\times K J}$ be a matrix of $K$ horizontally and vertically concatenated blocks ", "page_idx": 15}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/8020574c1b45a4c62415745c994913d9e64eb6ecd3db35e24824422e19e4d778.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure B9: TNs of the (a) forward pass, (b, c) Jacobians, and (d, e) VJPs with batch axis and channel groups. They generalize Figures 2 and 3 from the main text. For the VJPs, the Jacobians are shaded. ", "page_idx": 16}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/cac02dda3582bba5a8e98e6b4b8274635af0b51ec7d8cee72b7dc193833b0f6d.jpg", "img_caption": ["(a) GGN diagonal ", "(b) GGN Gram matrix (empirical NTK) "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure B10: TNs of (a) the GGN diagonal and (b) the GGN Gram matrix with batching and channel groups. They extend Figures C16b and C16c from the main text. ", "page_idx": 16}, {"type": "text", "text": "$\\pmb{A}^{(k_{1},k_{2})}\\in\\mathbb{R}^{I\\times J},k_{i}=1\\ldots,K$ . We can extract the diagonal blocks by restoring the sub-structure, ", "page_idx": 16}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/9f81a6866ab55d4486eaf12a6c56665c08e41488a5cad042c8375d30ef6e2552.jpg", "img_caption": ["Figure B11: TN of a GGN mini-block diagonal without batching and channel groups. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/b5fc0846e0ff04253c32e1c35a3e80adbd173aaafef57866f4c67128401df55a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure B12: TN diagrams of input-based factors in Kronecker approximations of the GGN for convolutions with batching and channel groups. They extend Figure 4 from the main text. ", "page_idx": 17}, {"type": "text", "text": "then taking the diagonal along the $K$ -dimensional index, ", "page_idx": 17}, {"type": "equation", "text": "$$\nk_{\\mathbf{\\lambda}}=\\!\\!\\left(\\!\\left\\{A^{(k,k)}\\right\\}\\!\\!\\right)\\!\\!-\\!j\\,=\\,{\\overset{k}{i}}{\\overset{\\sum}{\\sum}}\\!\\!\\!-(k,i){\\overbrace{\\!\\!-({\\underline{{A}}})\\!\\!-\\!(k,j)\\!\\!-\\!{\\underline{{\\mathcal{Q}}}}}^{k}}\\!\\!\\!-\\!j\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can apply this procedure to the GGN from Figure C16a. Assume we want to divide the output channel, input channel, and spatial dimensions into $G_{C_{\\mathrm{out}}},G_{C_{\\mathrm{in}}},G_{K_{1}},G_{K_{2}}$ groups. A group will thus be indexed with a tuple $\\left(g_{C_{\\mathrm{out}}},g_{C_{\\mathrm{in}}},g_{K_{1}},g_{K_{2}}\\right)$ and the corresponding GGN block will be of dimension $C_{\\mathrm{out}}/G_{C_{\\mathrm{out}}}\\times C_{\\mathrm{in}}/\\bar{G_{C_{\\mathrm{in}}}}\\times\\bar{K_{1}^{\\prime}}/\\bar{G_{K_{1}}}\\times\\bar{K_{2}}/\\bar{G_{K_{2}}}\\times C_{\\mathrm{out}}/G_{C_{\\mathrm{out}}}\\times C_{\\mathrm{in}}/G_{C_{\\mathrm{in}}}\\times K_{1}/G_{K_{1}}\\times K_{2}/G_{K_{2}}$ and correspond to the GGN for $[\\mathbf{W}]_{(g_{C_{00}},:),(g_{C_{\\mathrm{in}}},:),(g_{K_{1}},:),(g_{K_{2}},:)}.$ This process of un-grouping the output dimensions, then taking the diagonal along the group indices, is illustrated in Figure B11. Note that if we choose $G_{C_{\\mathrm{out}}}=C_{\\mathrm{out}},G_{C_{\\mathrm{in}}}=C_{\\mathrm{in}},G_{K_{1}}=K_{1},G_{K_{2}}=K_{2}$ , each block will be a single number and hence we recover the GGN diagonal from Figure C16b. If instead we $G_{C_{\\mathrm{out}}}=G_{C_{\\mathrm{in}}}G_{K_{1}}G_{K_{2}}=1$ , we obtain the full GGN from Figure C16a. The outlined schemes allows to extract mini-blocks of arbitrary size along the diagonal (subject to the total dimension). ", "page_idx": 17}, {"type": "text", "text": "B.3 Kronecker-factored Approximate Curvature (KFAC) for Grouped Convolutions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We were unable to find a definition of KFAC for grouped convolutions. Hence, we derive it here and present the TN diagrams. We use the perspective that grouped convolutions are independent convolutions over channel groups which are then concatenated. For each of those convolutions, we can then apply established the KFAC approximation for convolutions without groups. For a group $g$ we have the kernel $\\begin{array}{r}{\\mathbf{W}_{g}\\,=\\,[\\mathbf{W}]_{(g,:),:,:,:}}\\end{array}$ and the unfolded input of its associated input channels, $[\\pmb{\\Upsilon}_{g}]=[\\pmb{\\Upsilon}]_{(g,:),:,:}=[\\pmb{[\\pmb{\\Upsilon}]}_{(g,:),:,:}]$ (or $[\\![\\mathsf{X}_{n,g}]\\!]=[\\![\\mathsf{X}_{n}]\\!]_{(g,:),:,:}=[\\![\\mathsf{X}]\\!_{n,(g,:),:,:}]$ in the batched setting). ", "page_idx": 17}, {"type": "text", "text": "KFC/KFAC-expand for grouped convolutions: Applying the regular KFC approximation to the kernel of group $g$ , this yields the Fisher approximation $\\Omega_{g}\\otimes\\mathbf{\\Gamma}_{g}$ with $\\Gamma_{g}\\,\\in\\,\\mathbb{R}^{\\tilde{C}_{\\mathrm{out}}\\times\\tilde{C}_{\\mathrm{out}}}$ and $\\begin{array}{r}{\\Omega_{g}\\,=\\,^{1}\\!/N\\sum_{n=1}^{N}\\!\\mathbb{I}\\!\\big[\\pmb{\\mathsf{X}}_{n,g}\\big]\\!\\|\\pmb{\\mathsf{X}}_{n,g}\\mathbb{I}^{\\top}\\,\\in\\,\\mathbb{R}^{\\tilde{C}_{\\mathrm{in}}K_{1}K_{2}\\times\\tilde{C}_{\\mathrm{in}}K_{1}K_{2}}}\\end{array}$ where ${\\pmb X}_{n,g}$ is the input tensor for sample $n$ and grou p $g$ (re memb  er the  index structure ${\\pmb X}_{n,(g,\\tilde{c}_{\\mathrm{in}}),i_{1},i_{2}})$ . Figure B12a shows the diagram for $\\{N\\Omega_{g}\\}_{g=1}^{G}$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "KFAC-reduce for grouped convolutions: Proceeding in the same way, but using the unfolded input averaged over output locations, we obtain the Fisher approximation $\\hat{\\Omega}_{g}\\otimes\\hat{\\Gamma}_{g}$ with $\\hat{\\Gamma}_{g}\\in\\mathbb{R}^{\\tilde{C}_{\\mathrm{out}}\\times\\tilde{C}_{\\mathrm{out}}}$ and $\\begin{array}{r}{\\hat{\\Omega}_{g}=1/N(O_{1}O_{2})^{2}\\sum_{n=1}^{N}\\mathbf{1}_{O_{1}O_{2}}^{\\top}\\big\\lVert\\mathbf{X}_{n,g}\\big\\rVert\\big(\\mathbf{1}_{O_{1}O_{2}}^{\\top}\\big\\lVert\\mathbf{X}_{n,g}\\big\\rVert\\big)^{\\top}\\in\\mathbb{R}^{\\tilde{C}_{\\mathrm{in}}K_{1}\\bar{K}_{2}\\times\\tilde{C}_{\\mathrm{in}}\\bar{K}_{1}K_{2}}}\\end{array}$ for the kernel of group $g$ . Figure B12b shows the diag ram f or $\\{N(O_{1}O_{2})^{2}\\hat{\\Omega}_{g}\\}_{g=1}^{G}$ . ", "page_idx": 18}, {"type": "text", "text": "B.4 Kronecker-factored Approximate Curvature (KFAC) for Transpose Convolution ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we derive the KFAC approximation for transpose convolutions. ", "page_idx": 18}, {"type": "text", "text": "We describe transpose convolution in terms of its associated convolution from an input space $\\mathcal{X}\\,=\\,\\mathbb{R}^{C_{\\mathrm{in}}\\times I_{1}\\times I_{2}}$ to an output space $\\boldsymbol{\\mathcal{V}}\\,=\\,\\mathbb{R}^{C_{\\mathrm{out}}\\times O_{1}\\times O_{2}}$ . The convolution has hyper-parameters $K_{1,2},S_{1,2},P_{1,2},D_{1,2}$ with index patterns $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ and $\\pmb{\\Pi}^{(2)}=\\pmb{\\Pi}(I_{2},K_{2},S_{2},P_{2},D_{2})\\in\\mathbb{R}^{I_{2}\\times O_{2}\\times K_{2}}$ . ", "page_idx": 18}, {"type": "text", "text": "Transpose convolution as matrix multiplication: Transpose convolution maps a $\\mathbf{\\boldsymbol{Y}}\\in\\mathcal{Y}$ into an $\\mathbf{x}\\in\\mathcal{X}$ . In ML frameworks like PyTorch, its kernel $\\Tilde{\\mathbf{W}}$ is stored as $C_{\\mathrm{out}}\\times C_{\\mathrm{in}}\\times K_{1}\\times K_{2}$ tensor. The relation $\\pmb{\\chi}=\\tilde{\\pmb{W}}\\star_{\\mathrm{T}}\\pmb{\\Upsilon}$ where $\\star_{\\mathrm{T}}$ denotes transpose convolution is given by Figure 3d, ", "page_idx": 18}, {"type": "equation", "text": "$$\nX_{c_{\\mathrm{in}},i_{1},i_{2}}=\\sum_{c_{\\mathrm{out}}=1}^{C_{\\mathrm{out}}}\\sum_{k_{1}=1}^{K_{1}}\\sum_{k_{2}=1}^{K_{2}}\\sum_{o_{1}=1}^{O_{1}}\\sum_{o_{2}=1}^{O_{2}}\\Pi_{i_{1},o_{1},k_{1}}^{(1)}\\Pi_{i_{2},o_{2},k_{2}}^{(2)}\\,Y_{c_{\\mathrm{out}},k_{1},k_{2}}\\,\\tilde{W}_{c_{\\mathrm{out}},c_{\\mathrm{in}},k_{1},k_{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Our goal is to turn the express the above as matrix multiplication. To do that, we first define the matrix reshape $\\mathbf{\\deltaX}$ of $\\pmb{\\Upsilon}$ via $\\pmb{X}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times I_{1}I_{2}}$ such that $[X]_{c_{\\mathrm{in}},(i_{1},i_{2})}=X_{c_{\\mathrm{in}},i_{1},i_{2}}$ . Next, we consider a transposed kernel $\\boldsymbol{\\mathsf{W}}$ of $\\Tilde{\\boldsymbol{\\mathsf{W}}}$ with changed order of the first two indices, i.e. $\\boldsymbol{\\mathsf{W}}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times C_{\\mathrm{out}}\\times K_{1}\\times K_{2}}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{c_{\\mathrm{in}},c_{\\mathrm{out}},k_{1},k_{2}}=\\tilde{W}_{c_{\\mathrm{out}},c_{\\mathrm{in}},k_{1},k_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This transposition is necessary to convert the kernel\u2019s layout in the ML framework to a layout that admits Equation (B9) to be expressed as matrix multiplication. Using a matrix reshape $W$ of $\\boldsymbol{\\mathsf{W}}$ via $W\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times C_{\\mathrm{out}}K_{1}K_{2}}$ such that $[\\boldsymbol{W}]_{c_{\\mathrm{in}},(c_{\\mathrm{out}},k_{1},k_{2})}=\\bar{W}_{c_{\\mathrm{in}},c_{\\mathrm{out}},k_{1},k_{2}}$ , we can express Equation (B9) as matrix multiplication ", "page_idx": 18}, {"type": "equation", "text": "$$\nX=W[\\mathbb{Y}]_{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $[\\mathbf{V}]_{\\mathrm{T}}\\in\\mathbb{R}^{C_{\\mathrm{out}}K_{1}K_{2}\\times I_{1}I_{2}}$ is the transpose-unfolded input to the transpose convolution (note that $[\\![\\cdot]\\!]\\neq[\\![\\cdot]\\!]\\bar{\\operatorname{T}}!)$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n[\\left\\lVert\\mathbf{Y}\\right\\rVert_{\\mathrm{T}}]_{(c_{\\mathrm{out}},k_{1},k_{2}),(i_{1},i_{2})}=\\sum_{o_{1}=1}^{O_{1}}\\sum_{o_{2}=1}^{O_{2}}\\Pi_{i_{1},o_{1},k_{1}}^{(1)}\\Pi_{i_{2},o_{2},k_{2}}^{(2)}\\,Y_{c_{\\mathrm{out}},o_{1},o_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To the best of our knowledge there is no API for $[\\![\\cdot]\\!]_{\\mathrm{T}}$ in existing ML frameworks. Our approach can provide a simple and efficient implementation o  f $[\\![\\cdot]\\!]\\quad$ through the TN shown in Figure B13a which corresponds to Equation (B12). As Equation (B1  1)  is of the same form as Equation (1), it is now straightforward to write down the KFAC approximations for transpose convolution. ", "page_idx": 18}, {"type": "text", "text": "KFAC-expand: We will define the KFAC-expand approximation for the GGN w.r.t. the flattened kernel $\\mathbf{\\nabla}w$ of $W$ . Note that, in practise, this approximation must be properly transformed back to the layout $\\tilde{\\mathbf{W}}$ of the ML framework. We have $\\mathbf{\\boldsymbol{G}}(\\boldsymbol{w})\\approx\\boldsymbol{\\Omega}\\otimes\\mathbf{\\Gamma}$ , with $\\mathbf{T}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times C_{\\mathrm{in}}}$ computed from backpropagated gradients, and the input-based Kronecker factor ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol\\Omega=\\lVert\\boldsymbol\\Upsilon\\rVert_{\\mathrm{T}}\\lVert\\boldsymbol\\Upsilon\\rVert_{\\mathrm{T}}^{\\top}\\in\\mathbb{R}^{C_{\\mathrm{out}}K_{1}K_{2}\\times C_{\\mathrm{out}}K_{1}K_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "See Figure B13b for the corresponding TN. ", "page_idx": 18}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/180fe58d0485d9fd2b9930549a7534ffaa8410eb6f132d31850b3dd1b7aaf360.jpg", "img_caption": ["Figure B13: TNs for extending KFAC to transpose convolutions (no batching and groups). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "KFAC-reduce: For KFAC-reduce, we have $\\mathbf{\\boldsymbol{G}}(\\boldsymbol{w})\\approx\\hat{\\Omega}\\otimes\\hat{\\Gamma}$ , with $\\hat{\\mathbf{T}}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times C_{\\mathrm{in}}}$ computed from backpropagated gradients, and the input-based Kronecker factor ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{\\Omega}}=\\frac{1}{(I_{1}I_{2})^{2}}\\left(\\mathbf{1}_{I_{1}I_{2}}^{\\top}[\\boldsymbol{\\mathsf{Y}}]_{\\mathrm{T}}\\right)\\left(\\mathbf{1}_{I_{1}I_{2}}^{\\top}[\\boldsymbol{\\mathsf{Y}}]_{\\mathrm{T}}\\right)^{\\top}\\in\\mathbb{R}^{C_{\\mathrm{out}}K_{1}K_{2}\\times C_{\\mathrm{out}}K_{1}K_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "See Figure B13c for the corresponding TN. ", "page_idx": 19}, {"type": "text", "text": "With batching and groups: In the presence of $G$ groups, we have per-group kernels $\\tilde{\\pmb{W}}_{g}\\;=$ $[\\Tilde{\\pmb{\\mathsf{W}}}]_{(g,:),:,:,:}\\in\\mathbb{R}^{C_{\\mathrm{out}}/_{G}\\times C_{\\mathrm{in}}/_{G}\\times K_{1}\\times K_{2}}$ and $\\boldsymbol{\\mathsf{W}}_{g}\\in\\mathbb{R}^{C_{\\mathrm{in}}/G\\times C_{\\mathrm{out}}/G\\times K_{1}\\times K_{2}}$ , as well as per-group transposeunfolded inputs $\\begin{array}{r}{\\|\\pmb{\\mathsf{Y}}_{g}\\|_{\\mathrm{T}}=\\|\\pmb{\\mathsf{Y}}\\|_{\\mathrm{T}(g,:),:,:}=\\|\\pmb{\\mathsf{Y}}\\|_{(g,:),:,:}\\|_{\\mathrm{T}}\\in\\mathbb{R}^{C_{\\mathrm{out}}/\\alpha K_{1}K_{2}\\times I_{1}I_{2}}}\\end{array}$ . Each group corresponds to a transpose convolution in itself. With batching, we have an additional leading batch dimension, i.e. $[\\mathsf{Y}_{n,g}]_{\\mathsf{T}}$ . Applying the same steps from above, we can define the KFAC approximation for the GG  N w.r. t. the flattened per-group kernel ${\\pmb w}_{g}$ of $W_{g}$ . ", "page_idx": 19}, {"type": "text", "text": "For KFAC-expand, we have $G(w_{g})\\approx\\Omega_{g}\\otimes\\mathbf{{T}}_{g}$ , with $\\Gamma_{g}\\in\\mathbb{R}^{C_{\\mathrm{in}}/G\\times C_{\\mathrm{in}}/G}$ computed from backpropagated gradients, and the input-based Kronecker factor ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Omega_{g}=\\frac{1}{N}\\sum_{n=1}^{N}\\[\\mathsf{Y}_{n,g}]_{\\mathsf{T}}\\[\\mathsf{Y}_{n,g}]_{\\mathsf{T}}^{\\top}\\in\\mathbb{R}^{C_{\\mathrm{out}}/{G K_{1}K_{2}}\\times C_{\\mathrm{out}}/G K_{1}K_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For KFAC-reduce, we have $G(w_{g})\\approx\\hat{\\Omega}_{g}\\otimes\\hat{\\Gamma}_{g}$ , with $\\hat{\\Gamma}_{g}\\in\\mathbb{R}^{C_{\\mathrm{in}}/G\\times C_{\\mathrm{in}}/G}$ computed from backpropagated gradients, and the input-based Kronecker factor ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\Omega}_{g}=\\frac{1}{N(O_{1}O_{2})^{2}}\\sum_{n=1}^{N}\\left(\\mathbf{1}_{I_{1}I_{2}}^{\\top}[\\mathbf{Y}_{n,g}]_{\\mathsf{T}}\\right)\\,\\left(\\mathbf{1}_{I_{1}I_{2}}^{\\top}[\\mathbf{Y}_{n,g}]_{\\mathsf{T}}\\right)^{\\top}\\in\\mathbb{R}^{C_{\\mathrm{out}}/G K_{1}K_{2}\\times C_{\\mathrm{out}}/G K_{1}K_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.5 Further Operations & Extensive Overview ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Consecutive convolutions: We can chain two, or more, convolutions into a single TN diagram (Figure B14) to obtain a deep linear CNN [65] similar to deep linear networks which are popular for analytical studies. ", "page_idx": 19}, {"type": "text", "text": "Convolution weight/input JVPs: In the main text, we derived the Jacobians of convolution (\u00a73.1) which can be used to derive the JVPs. A JVP propagates perturbations $\\mathbf{V}^{(\\mathsf{W})}\\,\\in\\,\\mathbb{R}^{C_{\\mathrm{out}}\\times C_{\\mathrm{in}}\\times K_{1}\\times K_{2}}$ and $\\mathbf{V}^{(\\mathbf{X})}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times I_{1}\\times I_{2}}$ in the input space to perturbations in the output space by contracting the perturbation with the Jacobian. See Table B3 for the general einsum expressions. ", "page_idx": 19}, {"type": "text", "text": "Batched convolution weight VJP: To obtain per-sample gradients, the weight VJP must be carried out without summing over the batch axis which amounts to keeping the batch index in the output index tuple. ", "page_idx": 19}, {"type": "text", "text": "VJPs and JVPs of im2col: With the TN differentiation technique described in $\\S3.1$ we can compute the Jacobian of the unfolding operation, then contract it with perturbations $V^{(\\mathbf{X})}\\in\\mathbb{R}^{C_{\\mathrm{in}}\\times K_{1}\\times K_{2}}$ in input space to obtain the JVP, or with perturbations $V^{(\\mathbb{N})}\\in\\bar{\\mathbb{R}}^{O_{1}O_{2}\\times C_{\\mathrm{in}}K_{1}K_{2}}$ to obtain the VJP. ", "page_idx": 19}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/14ffc3a71db120eb562accc2cea0c41d6dccf675763343679436f5ce8b22ec6c.jpg", "img_caption": ["Figure B14: TN of two consecutive convolutions without groups and without batch axis. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Approximate Hessian diagonals (HesScale/BL89): Becker & Lecun [4], Elsayed et al. [22] proposed approximate procedures for the Hessian diagonal which cost roughly a gradient. They can be understood as modifications of the Hessian backpropagation equations from Dangel et al. [16]. ", "page_idx": 20}, {"type": "text", "text": "Consider a layer with input $\\textbf{\\em x}$ , output $\\textit{\\textbf{y}}$ , and weights $\\mathbf{\\nabla}w$ inside a sequential feedforward neural network (for a convolutional layer, these correspond to the flattened input, output, and kernel). To compute per-layer Hessians of a loss $\\ell$ , each layer backpropagates its incoming Hessian $\\nabla_{\\pmb{y}}^{2}\\ell$ according to [16] ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}^{2}\\ell=(J_{x}y)^{\\top}\\nabla_{y}^{2}\\ell(J_{x}y)+\\displaystyle\\sum_{i}\\frac{\\partial\\ell}{\\partial y_{i}}\\nabla_{x}^{2}y_{i}\\,,}\\\\ &{\\nabla_{w}^{2}\\ell=(J_{w}y)^{\\top}\\nabla_{y}^{2}\\ell(J_{w}y)+\\displaystyle\\sum_{i}\\frac{\\partial\\ell}{\\partial y_{i}}\\nabla_{w}^{2}y_{i}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The scheme of [4, 22] imposes diagonal structure on the backpropagated quantity. A layer receives a backpropagated diagonal $\\pmb{d}^{(y)}$ such that $\\mathrm{diag}(d^{(y)})\\approx\\nabla_{y}^{2}\\ell,$ , and backpropagates it according to Equation (B15), but with a post-processing step to obtain a diagonal backpropagated quantity, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{d}^{(x)}=\\mathrm{diag}\\left((J_{x}y)^{\\top}\\,\\mathrm{diag}(d^{(y)})(J_{x}y)\\right)+\\mathrm{diag}\\left(\\sum_{i}\\frac{\\partial\\ell}{\\partial y_{i}}\\nabla_{x}^{2}y_{i}\\right)\\,,}\\\\ &{\\boldsymbol{d}^{(w)}=\\mathrm{diag}\\left((J_{w}y)^{\\top}\\,\\mathrm{diag}(d^{(w)})(J_{w}y)\\right)+\\mathrm{diag}\\left(\\sum_{i}\\frac{\\partial\\ell}{\\partial y_{i}}\\nabla_{w}^{2}y_{i}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathrm{diag}(\\pmb{d}^{(x)})\\approx\\nabla_{\\pmb{x}}^{2}\\ell$ and $\\mathrm{diag}(d^{(w)})\\approx\\nabla_{w}^{2}\\ell$ is an approximation to the Hessian diagonal. ", "page_idx": 20}, {"type": "text", "text": "For convolutional layers, which are linear in the input and weight, the second summands are zero due to $\\nabla_{\\mathbf{\\boldsymbol{x}}}^{2}y_{i}=\\mathbf{0}=\\nabla_{\\mathbf{\\boldsymbol{w}}}^{2}y_{i}$ . The first terms of Equation (B16) require (i) embedding a diagonal vector into a matrix, (ii) applying MJPs and JMPs, and (iii) extracting the result\u2019s diagonal. Those can be expressed as a single TN. We show the diagrams in Figure B15, using tensors rather than their flattened versions, that is $(x,y,w,d^{(x)},d^{(y)},\\bar{d}^{(w)})\\to(\\bar{\\mathbf{X}_{\\cdot}}\\nabla,\\mathsf{W},\\mathsf{D}^{(\\mathsf{X})},\\bar{\\mathsf{D}^{(\\mathsf{Y})}},\\mathsf{D}^{(\\mathsf{W})})$ . ", "page_idx": 20}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/17f0cba1cd1038cfae169df799218906dd7f07a09022e448ecd24209a267baab.jpg", "img_caption": ["(a) HesScale/BL89 input backpropagation "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/8a28ef688940f0fb860d956e450d9d84b5aab1e938bca1ae8a9e59454085314a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/ca05f6b7eba0948f493356e8d1306aa30092c7daace59fae882b27c934a193ff.jpg", "img_caption": ["(b) HesScale/BL89 weight backpropagation "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/921ee1c6d99c8e12370243f097d5a08b5d4da8e1102744914c7d3d6ffcf53b9b.jpg", "img_caption": ["(c) HesScale/BL89 input backpropagation $^{+}$ batch,(d) HesScale/BL89 weight backpropagation ( $^+$ batch, groups) groups) "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure B15: TN diagrams for HesScale/BL89 [4, 22] backpropagations through convolutional layers to approximate the Hessian diagonals $\\mathbf{D}^{(\\mathbf{X})}$ , $\\pmb{\\mathsf{D}}^{(\\mathsf{W})}$ . JMPs and MJPs are shaded. (a, b) show the simple versions without batching and without channel groups. (c, d) include batching and channel groups. ", "page_idx": 21}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/a8df1c719496cb01eede4e5886b35fda9889b7f9822e75b74c2c4313edf4c2a7.jpg", "table_caption": ["Table B3: Extensive list of convolution and related operations (extension from Table 1 in the main text). All operations consider two spatial dimensions and support batching and channel groups. Generalization to other dimensions follow by introducing more spatial indices i_3, o_3, .. . and kernel indices k_3, .. .. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/dea3eb138ea868aea4150d5768c9330f517719a081b27a9eb3b5be13a284b1f3.jpg", "img_caption": ["Figure C16: TN composition and sub-tensor extraction for second-order information. Weight MJPs from Figure 3c are shaded. (a) exact and (b) diagonal of the kernel\u2019s GGN (the same applies to structurally similar matrices like the gradient covariance [35]). (c) TN of the GGN Gram matrix. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C Exact Second-Order Information ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we look at computing second-order information of a loss w.r.t. to the kernel of a convolution. Its computation can be phrased as backpropagation with a final extraction step [19] which contains less standard operations like Jacobian-matrix products (JMPs) and sub-tensor extraction. TNs can express this extraction step in a single diagram. ", "page_idx": 23}, {"type": "text", "text": "Consider a datum $\\left({\\boldsymbol{x}},t\\right)$ and its loss $\\ell({\\pmb w})~=~\\ell({\\pmb f},{\\pmb t})$ where ${\\textbf{\\textit f}}:=\\mathbf{\\textit{f}}_{\\mathbf{\\boldsymbol{w}}}({\\boldsymbol{\\mathbf{x}}})\\ \\in\\ \\mathbb{R}^{C}$ is the prediction of a CNN with a convolution with flattened kernel $\\pmb{w}$ and flattened output $\\textit{\\textbf{y}}$ (derivations carry over to a batch loss). The kernel\u2019s generalized Gauss-Newton (GGN) matrix [63] $G(w)=\\(J_{w}f)^{\\top}\\nabla_{f}^{2}\\ell(J_{w}f)\\in\\mathbb{R}^{C_{\\mathrm{out}}C_{\\mathrm{in}}K_{1}K_{2}\\times C_{\\mathrm{out}}\\widecheck C_{\\mathrm{in}}K_{1}K_{2}}$ is a positive semi-definite Hessian proxy preferred by many applications [e.g. 20, 45] and coincides with the Fisher information matrix for many common losses [46]. It is the self-outer product of a backpropagated symmetric factorization $\\pmb{S}^{(y)}\\,=\\,(\\pmb{J}_{y}\\pmb{f})^{\\top}\\pmb{S}^{(f)}\\,\\in\\,\\mathbb{R}^{C_{\\mathrm{out}}O_{1}O_{2}\\times C}$ of the loss Hessian, $\\nabla_{f}^{2}\\ell(f,\\pmb{y})\\,=\\,\\pmb{S}^{(f)}(\\pmb{S}^{(f)})^{\\top}$ . During backpropagation, the convolution extracts information about $G(w)=(J_{w}y)^{\\top}S^{(y)}(S^{(y)})^{\\top}J_{w}y$ . ", "page_idx": 23}, {"type": "text", "text": "In TN notation, this is easy to express without flattening: We simply compose two VJP diagrams from Figure 3c with an extra leg (MJP) and add the outer-product contraction to obtain the tensor version $\\ensuremath{\\mathbf{\\bar{G}}}(\\mathbf{W})\\,\\in\\,\\mathbb{R}^{C_{\\mathrm{out}}\\times C_{\\mathrm{in}}\\times K_{1}\\times K_{2}\\times C_{\\mathrm{out}}\\times C_{\\mathrm{in}}\\times K_{1}\\times K_{2}}$ of ${\\cal G}(w)$ (Figure C16a). The GGN is often further approximated by sub-tensors as it is too large. These slicing operations are also easy to integrate into the diagrams, e.g. to extract diagonal elements (Figure C16b [17, 51]), or mini-block diagonals (Figure B11 [16, 3]). This also removes redundant computations compared to computing, then slicing, the matrix. The same ideas apply to the GGN Gram matrix $(S^{(\\pmb{w})})^{\\top}S^{(\\pmb{w})}\\in\\dot{\\mathbb{R}}^{C\\times\\tilde{C}}$ (Figure C16c). It contains the GGN spectrum [18] and is related to the empirical NTK for square loss [49]. ", "page_idx": 23}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here we present details on the index pattern computation, and additional transformations. ", "page_idx": 24}, {"type": "text", "text": "D.1 Index Pattern Tensor Computation for Convolutions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Algorithm D1 lists pseudo-code for the index pattern computation from the convolution hyperparameters $K,S,P,D$ , and the spatial input dimension $I$ , that is $\\Pi(I,K,S,P,D)$ . Unlike in the main text, we use 0-based indexing which is more common in numerical libraries. For self-consistency, we re-state the relation of the hyper-parameters to output dimension from [21, Relationship 15], ", "page_idx": 24}, {"type": "equation", "text": "$$\nO(I,K,S,P,D)=1+\\left\\lfloor\\frac{I+2P-K-(K-1)(D-1)}{S}\\right\\rfloor\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Algorithm D1 Computing the convolution index pattern tensor $\\sqcap$ for a spatial dimension. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Require: Input size $I\\in\\mathbb{N}^{+}$ , kernel size $K\\in\\mathbb{N}^{+}$ , stride $S\\in\\mathbb{N}^{+}$ , padding $P\\in\\mathbb{N}_{0}^{+}$ , dilation $D\\in\\mathbb{N}^{+}$ $\\begin{array}{r}{O\\gets1+\\left\\lfloor\\frac{I+2P-K-(K-1)(D-1)}{S}\\right\\rfloor}\\end{array}$ \u25b7Compute output dimension [21, Relationship 15] \u03a0 \u21900I\u00d7O\u00d7K \u25b7Initialize index pattern tensor for $o=0,\\ldots,O-1,k=0,\\dots,K-1\\,\\mathrm{d}$ o \u25b7Use 0-based indexing! $i\\leftarrow k D+o S-P$ \u25b7Reconstruct contributing input element if $0\\leq i\\leq I-1$ then \u25b7Check in bounds $\\Pi_{i,o,k}\\leftarrow1$ end if end for return Index pattern tensor \u03a0 \u2208{0, 1}I\u00d7O\u00d7K ", "page_idx": 24}, {"type": "text", "text": "D.2 Index Pattern Tensor for Standalone Transpose Convolution ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Although a transpose convolution is defined w.r.t. a reference convolution with hyper-parameters $K,S,P,D$ , most libraries offer standalone implementations of transpose convolution. We describe the transpose convolution by its associated convolution, that is as a mapping from $\\mathbb{R}^{C_{\\mathrm{out}}\\times O_{1}\\times O_{2}}$ (the convolution\u2019s output space) to $\\mathbb{R}^{C_{\\mathrm{in}}\\times I_{1}\\times I_{2}}$ (the convolution\u2019s input space). For convolution with $S>1$ , we cannot infer $I$ from $O,K,S,P,D$ , as multiple $I\\mathrm{s}$ map to the same $O$ if $(I+2P-K-$ $(K-1)(D-1))$ mod $S\\ne0$ (see the floor operation in Algorithm D1). We need to either supply $I$ directly, or the remainder ", "page_idx": 24}, {"type": "equation", "text": "$$\nA=I+2P-K-(K-1)(D-1)-S(O-1)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(often called output_padding) to make $I$ unambiguous. Then, we compute ", "page_idx": 24}, {"type": "equation", "text": "$$\nI=(O-1)S-2P+K+(K-1)(D-1)+A\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "to get $I(O,A)$ and call Algorithm D1 to obtain $\\Pi(I(O,A),K,S,P,D)$ . ", "page_idx": 24}, {"type": "text", "text": "D.3 Details on Index Pattern Simplifications ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the following, we will assume the absence of boundary pixels that don\u2019t overlap with the kernel, that is ", "page_idx": 24}, {"type": "equation", "text": "$$\nI+2P-(K+(K-1)(D-1))\\quad\\mathrm{mod}\\ S=0\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the floor operation in $O(I,K,S,P,D)$ is obsolete. This can always be assured by narrowing $\\pmb{\\Upsilon}$ before a convolution. Based on our hyper-parameter analysis of real-world CNNs $(\\mathrm{\\SE})$ , we identify: ", "page_idx": 24}, {"type": "text", "text": "Transformation D1 (Dense convolutions) Assume Equation (D19). For $K\\ =\\ S$ with default padding and dilation $^{\\prime}P\\,=\\,0$ , $D\\,=\\,1,$ ), patches are adjacent non-overlapping tiles, accessible by un-grouping the input index $i$ into a tuple index $(\\tilde{i},\\tilde{k})$ of size $I/\\kappa\\times K$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n[\\Pi(I,K,K,0,1)]_{i,o,k}=[\\Pi(I,K,K,0,1)]_{(\\tilde{i},\\tilde{k}),o,k}=\\delta_{\\tilde{i},o}\\delta_{\\tilde{k},k}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Point-wise convolutions $\\begin{array}{r}{^{\\prime}K=S=1,}\\end{array}$ ) are a special case with pattern $[\\Pi(I,1,1,0,1)]_{i,o,k}=\\delta_{i,o}$ . ", "page_idx": 24}, {"type": "text", "text": "Point-wise convolutions with $K=S=1$ are common in DenseNets [33], MobileNets [31, 60] and ResNets [30]. InceptionV3 [69] has 2d \u2018mixed dense\u2019 convolutions that are point-wise along one spatial dimension. ConvNeXt [42] uses dense convolutions with $K=S\\in\\{2,4\\}$ . ", "page_idx": 25}, {"type": "text", "text": "Transformation D2 (Down-sampling convolutions) For $S>K$ with default padding and dilation $P=0$ , $D=1$ ), some elements do not overlap with the kernel. If the input dimension $i$ is summed, all participating tensors can be pruned to remove the explicit zeros. Assume $I$ mod $S=0$ . Then, pruning amounts to un-grouping $i$ into $(i^{\\prime},s)$ of size $I/S\\times S$ , narrowing s to $K$ entries, and grouping back into an index $\\Tilde{i}$ of size $K I/S$ . After pruning, the index pattern represents a dense convolution with input size $K I/S$ , kernel size $K$ , and stride $K$ . In a contraction with some tensor $\\mathbf{\\upnu}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{i=1}^{I}\\left[\\boldsymbol{\\mathsf{V}}_{\\perp,\\ldots,i,\\ldots}\\left[\\boldsymbol{\\mathsf{n}}(I,\\boldsymbol{K},\\boldsymbol{S}>\\boldsymbol{K},\\boldsymbol{0},\\boldsymbol{1})\\right]_{i,o,k}=\\sum_{i=1}^{I/s}[\\widetilde{\\boldsymbol{\\mathsf{V}}}_{\\perp,\\ldots,\\widetilde{\\boldsymbol{\\imath}},\\ldots}\\left[\\boldsymbol{\\mathsf{n}}(\\boldsymbol{K}I/\\boldsymbol{s},\\boldsymbol{K},\\boldsymbol{K},\\boldsymbol{0},\\boldsymbol{1})\\right]_{\\widetilde{i},o,k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with sub-tensor $[\\tilde{\\mathbf{V}}]_{...,\\tilde{i},...}=[[\\mathbf{V}]_{...,(i^{\\prime},s),...}]_{...,(:,:K),...}$ where : $K$ means narrowing to $K$ elements. ", "page_idx": 25}, {"type": "text", "text": "Transformation D2 converts down-sampling convolutions to dense convolutions, which can be further simplified with Transformation D1. We find down-sampling convolutions with $S=2>K=1$ in ResNet18 [30], ResNext101 [72], and WideResNet101 [73]. Those convolutions discard $75\\,\\%$ of their input! Knowledge that an operation only consumes a fraction of its input could be used to eliminate those \u2018dead\u2019 computations in preceding operations, reducing FLOPS and memory. ", "page_idx": 25}, {"type": "text", "text": "Transformation D3 (Kernel-output dimension swap) Assume Equation (D19). Transposing kernel and output dimensions in an index pattern yields another index pattern with same input size, kernel size $O(I,K,S,P,D)$ , and swapped stride and dilation: ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\boldsymbol{\\Pi}(I,\\boldsymbol{K},\\boldsymbol{S},\\boldsymbol{P},\\boldsymbol{D})]_{i,o,k}=[\\boldsymbol{\\Pi}(I,\\boldsymbol{O},\\boldsymbol{D},\\boldsymbol{P},\\boldsymbol{S})]_{i,k,o}\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This transformation is easy to see from the symmetry of $(k,D)$ and $(o,S)$ in Equation (7) and $O(I,K,S,P,D)$ . It converts index pattern contractions over output into kernel dimensions, like in convolutions. An example is the weight VJP from Figure $3\\mathrm{c}$ , which\u2014after swapping kernel and output dimensions\u2014resembles the TN for convolution from Figure 2 with kernel $\\mathsf{v}$ . Rochette et al. [58] use this to phrase the computation of per-example gradients as convolution. ", "page_idx": 25}, {"type": "text", "text": "$\\S\\mathrm{D}.3$ presents more properties of $\\sqcap$ based on the sub-sampling interpretation of stride and dilation along the output and kernel dimensions. We also provide a transformation for swapping input and output dimensions, relating convolution and transpose convolution as described in [21]. ", "page_idx": 25}, {"type": "text", "text": "For completeness, we state additional index pattern tensor properties here (using 1-based indexing): ", "page_idx": 25}, {"type": "text", "text": "Transformation D4 (Sub-sampling interpretation of stride) Strided convolutions $^{\\prime}S\\,>\\,1,$ ) subsample non-strided convolutions along the output dimension, ignoring all but every Sth output $I2I J$ . In other words, $[\\Pi(I,K,S,P,D)]_{i,o,k}=[\\Pi(\\bar{I},K,1,P,D)]_{i,1+S(o-1),k}\\;o r,$ in tensor notation $([\\cdot]_{::S}$ denotes slicing with steps of $S$ ), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Pi(I,K,S,P,D)=[\\Pi(I,K,1,P,D)]_{:,::S,:}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Transformation D5 (Sub-sampling interpretation of dilation) Dilated convolutions $(D\\ >\\ 1,$ with kernel size $K$ sub-sample the kernel of a non-dilated convolution of kernel size $K+(D-$ $1)(K-1)$ , ignoring all but every Dth kernel element. In other words, $[\\Pi(I,K,S,P,D)]_{i,o,k}=$ $\\left[\\Pi(I,K+(K-1)(D-1),S,P,1)\\right]_{i,o,1+D(k-1)}\\,o r,\\,i n\\,t$ tensor notation, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Pi(I,K,S,P,D)=[\\Pi(I,K+(K-1)(D-1),S,P,1)]_{:,:,:::D}\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Transformation D6 (Transpose convolution as convolution) Assume Equation (D19). Consider a non-strided $^{\\prime}S=1,$ ), non-dilated $\\left/D=1\\right.$ ) convolution with index pattern $\\sqcap(I,K,1,P,1)$ and output dimension $O(I,K,1,P,1)$ . Transposing the spatial dimensions and flipping the kernel dimension yields another index pattern with modified padding $P^{\\prime}\\,=\\,K\\,-\\,P\\,-\\,1$ . In other words, for all $i=1,\\dots,I$ , $k=1,\\ldots,K$ , $o=1,\\ldots,O$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\boldsymbol{\\Pi}(I,K,1,P,1)]_{i,o,k}=[\\boldsymbol{\\Pi}(O,K,1,P^{\\prime},1)]_{o,i,K+1-k}\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E Convolution Layer Hyper-parameter Analysis ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we give an overview of and characterize convolutions in popular architectures (see Table E4). We include moderately deep CNNs on Fashion MNIST, CIFAR-10, and CIFAR-100 from the DeepOBS benchmark [62], and deep CNNs on ImageNet (AlexNet, ResNet18, InceptionV3, MobileNetV2, ResNext101). Regarding the hyper-parameters, we make the following observations: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Many CNNs do not use a bias term. This is because the output of those layers feeds directly into a batch normalization layer, which is invariant under the addition of a bias term.   \n\u2022 All investigated convolutions use default dilation.   \n\u2022 Group convolutions are rarely used. MobileNetV2 and ConvNeXt-base (Tables $\\mathrm{E4g}$ and E4i) use group convolutions that interpret each individual channel as a group. ResNext101 (Table E4f) uses group convolutions that interpret a collection of channels as a group. ConvNeXt-base (Table E4g) uses dense convolutions with $P=0$ and $S=K\\in\\{2,\\bar{4}\\}$ .   \n\u2022 Many networks use dense convolutions, that is convolutions with unit kernel size $\\mathcal{K}=1$ ), unit stride $[S\\,=\\,1]$ ), and no padding $(P=0)$ ). These convolutions have a trivial index pattern and can therefore be simplified.   \n\u2022 InceptionV3 (Table E4h) uses two-dimensional convolutions with one trivial dimension (\u2018mixed dense\u2019) with unit kernel size, unit stride, and no padding along one direction. For this spatial dimension, the index pattern can be simplified.   \n\u2022 ResNet18 (Table E4e) and ResNext101 (Table E4f) use convolutions with $S\\,>\\,K$ for down-sampling whose kernel only overlaps with a fraction of the input. The index pattern can be simplified. ", "page_idx": 26}, {"type": "text", "text": "Table E4: Hyper-parameters of convolutions in different CNNs. For convolutions with identical hyper-parameters, we only show one instance and its multiplicity. ", "page_idx": 27}, {"type": "text", "text": "(a) 3c3d, CIFAR-10 (3, 32, 32) ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/46040c7f17ce820e4eed018111b64f05909cfe71f2b3476175daf382350634de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/7e48ce7aff1bb1112777c6cea4f3bff1a2bb5d1e68ccb9995af16573f5842331.jpg", "table_caption": ["(b) 2c2d, Fashion MNIST (1, 28, 28) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/da4d9dea2863e4adb0731967714af707a1ae05090ddfbc6dd8cf27e89890ef2b.jpg", "table_caption": ["(c) All-CNN-C, CIFAR-100 (3, 32, 32) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/67c37c959c4916f73476d6772ea1054ef418f1fe814c8c1c7f794625d4a82849.jpg", "table_caption": ["(d) AlexNet, ImageNet (3, 256, 256) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/8517d31e7790031c42af23cecd9d478602302b55bc96f575e4735a35b3367885.jpg", "table_caption": ["(e) ResNet18, ImageNet (3, 256, 256) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/eeafe07d343911dd91f92f2e2551b8649cc6036cef9d5f670d8af6279917b822.jpg", "table_caption": ["(f) ResNext101_32x8d, ImageNet (3, 256, 256) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/bed60b3780f5876a461b916d66bd97f88f8e9919672afca38726193d5e99824b.jpg", "table_caption": ["(g) ConvNeXt-base, ImageNet (3, 256, 256) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/93133dd1b094e1be9a26a06970c2ceca02c07edca2e85e52a2f8b63aae414204.jpg", "table_caption": ["(h) InceptionV3, ImageNet (3, 299, 299) "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/a7cf97bf776e32e0e17751fe945777836b2614b3d45d91d59fc9ffa7f600f8ed.jpg", "table_caption": ["(i) MobileNetV2, ImageNet (3, 256, 256) "], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/fe4e19fa21632f38b1e988dc276a71538e6b361b56812072240389c55e4270e1.jpg", "img_caption": ["Figure F17: Benchmark overview. We measure the performance ratios of our TN implementation w.r.t. a base line in PyTorch (PT). Blue boxes show the performance ratios of TN versus PT, secondcolor boxes show the performance ratios of $\\mathrm{TN+}$ opt versus PT. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "F Run Time Evaluation Details (GPU) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here we provide all details on the run time evaluation from the main text. We consider the convolutions from the CNNs from $\\mathrm{\\SE}$ . Experiments were carried out on an Nvidia Tesla T4 (16 GB memory). We use a batch size of 32 for the ImageNet architectures, and 128 for the others. ", "page_idx": 29}, {"type": "text", "text": "F.1 Protocol & Overview ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We compare different implementations of the same operations in PyTorch. The base line (referenced by \u2018PT\u2019) uses PyTorch\u2019s built-in functionalities for convolutions and related operations, such as torch.nn.functional.conv2d (forward), torch.nn.functional.unfold (KFC, KFACreduce), and PyTorch\u2019s built-in automatic differentiation torch.autograd.grad (VJPs). ", "page_idx": 29}, {"type": "text", "text": "Our TN implementation (referenced by \u2018TN\u2019) sets up operands and the string-valued equation for each routine. Optionally, we can apply the simplifications from $\\S4$ as a post-processing step before contraction, which yields a modified equation and operand list $\\mathrm{(^{\\circ}T N+o p t^{\\circ})}$ ). Finally, we determine the contraction path using opt_einsum.contract_path and perform the contraction with its PyTorch back-end (opt_einsum.contract). We only measure the contraction time as in practical settings, the contraction path search would be executed once, then cached. We also exclude final operations to obtain the correct shape or scale (flattening, reshaping, scaling by constant) in all implementations (including the base line). ", "page_idx": 29}, {"type": "text", "text": "For each operation and each convolution layer, we perform 50 independent repetitions and report the minimum time in tables. To summarize those tables, we extract the performance ratios, that is the TN implementation\u2019s run time divided by the base line\u2019s. Ratios larger than 1 mean that the TN implementation is slower, ratios smaller than 1 indicate that it is faster than the base line. We collect those ratios for the different convolution types (general, mixed dense, dense, sub-sampling) and display them separately using box plots. Each operation has two boxes, corresponding to the un-simplified (TN), and the simplified $\\mathrm{(TN+opt)}$ implementation. For the box plots, we use matplotlib\u2019s default settings (a box extends from the first quartile to the third quartile of the data, with a line at the median; whiskers extend from the box by $1.5\\mathrm{x}$ the inter-quartile range; filer points are those past the end of the whiskers). Figure F17 summarizes the entire GPU benchmark. Figure F18 shows the same information with each convolution type as an individual plot. ", "page_idx": 29}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/426324426ef04736ff9aec8fedf3192b8985c1a267c0e3e6038367d04e6f7a99.jpg", "img_caption": ["Figure F18: Impact of TN simplifications (non-simplified performance ratios shown in blue). TN simplifications improve performance on (a) mixed dense, (b) dense, and (c) down-sampling convolutions. (d) General convolutions are not affected by TN simplifications. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "F.2 Forward Pass ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We compare TN and $\\mathrm{TN+}$ opt with PyTorch\u2019s torch.nn.functional.conv2d. Figure F19 visualizes the performance ratios for different convolution categories. Table F5 contains the detailed run times and performance factors. ", "page_idx": 31}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/0e6afefd3d328b001d2b4d04de809cb803fbc2f5f3858435128197f2288a5e84.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure F19: Forward pass performance ratios of TN versus PT and $\\mathrm{TN+}$ opt versus PT for different convolution types on GPU. ", "page_idx": 31}, {"type": "text", "text": "Table F5: Forward pass performance comparison on GPU. (a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32) ", "text_level": 1, "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/39656b503d41ae392a7880d6207f5c4e97a1e386dfcc01bb1abee4c6f51435cd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/224eb13f2e54e8d18741306247a741d7f621bca6c850b67385dadb8e0a7e6274.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/40bcf018d92ca8193a806bbaa155b909b0b2ead1925ad6ec586b5ee4b6fb6112.jpg", "table_caption": ["(b) F-MNIST 2c2d, input shape (128, 1, 28, 28) "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/97771ca2c54b390ec1c300234d6394d22e3ed5a4da8f0f2aedd3545302895261.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32) ", "text_level": 1, "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/f23a7e5c4fa666e85fdff48e28c26aa1db926f9a0068a53e42fd1e124974c1ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/deccc9f08a206fffc3d12afb88269ada17677dbdee150ef2735e4a72e30d8501.jpg", "table_caption": ["(d) Alexnet, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/a438a101b414321ab7858edc7f6d074eace7f46e3c3a03b643c278950730696c.jpg", "table_caption": ["(e) ResNet18, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/ebb9feb5623786207052d2e852060376bfd0d4c98ca84a009c3500dca9bccf99.jpg", "table_caption": ["(f) ResNext101, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/2f5d9aed273e149623fa1fc713cf8c0055b7d5d6b66777e5c6c09299320dd4b7.jpg", "table_caption": ["(g) ConvNeXt-base, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/02d72e3269db3a4d133fa5cb549a783e1729d3fbc39ea2a73e0e058d98bf8633.jpg", "table_caption": ["(h) InceptionV3, input shape (32, 3, 299, 299) "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/028a94d62a46e1d58df2acb3c715ac703aa258349b7819fe6afec110bb940509.jpg", "table_caption": ["(i) MobileNetV2, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "F.3 Input VJP ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We compare TN and $\\mathrm{TN+}$ opt with a PyTorch implementation of the input VJP via torch.autograd.grad. Figure F20 visualizes the performance ratios for different convolution categories. Table F6 contains the detailed run times and performance factors. ", "page_idx": 34}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/cb0feefd5068718453d7a3498cce9cd04fd010fed9cc6e9436079ac07339e1c2.jpg", "img_caption": ["Figure F20: Input VJP performance ratios of TN versus PT and $\\mathrm{TN+}$ opt versus PT for different convolution types on GPU. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table F6: Input VJP performance comparison on GPU. (a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32) ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/abedc77cdec21a547f973764f06fdb907b1508526f4200f507065dd0cb05e797.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/d25e0c94564cf2b351098d7f80d68b5cf0461c8c1e202ce342555d226fd08616.jpg", "table_caption": ["(b) F-MNIST 2c2d, input shape (128, 1, 28, 28) "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/e991a3c5deaecba44c4a15f3eaf8f98f5b94e82accce24892542f48222a15039.jpg", "table_caption": ["(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32) "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/a20d47ffbc6617a31dc8fff20d22be9640982463aa2e90922d7d02c9be89cdf5.jpg", "table_caption": ["(d) Alexnet, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/12d0f0165b4eb7c061b69aaff9d888f55b7fd1aa15d662e2082eccf57d9f8ffb.jpg", "table_caption": ["(e) ResNet18, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/126e72295d82d9c5cc7bd04a403a39aa30222d3eea31243881c0f94f0fe27a33.jpg", "table_caption": ["(f) ResNext101, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/5099c34bbb570273cb385ea0c9a53b4a65c4c98caabb17d1f02c5b15dd19088d.jpg", "table_caption": ["(g) ConvNeXt-base, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/1e7fe402dd692229924281958af88fa6215b6e5e4a2ad4f84b7fdf6fa06004fc.jpg", "table_caption": ["(h) InceptionV3, input shape (32, 3, 299, 299) "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/90dd6eddba7a3b06005ff83f57be2ed19cd24ebc33d49330f438147033eae591.jpg", "table_caption": ["(i) MobileNetV2, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "F.4 Weight VJP ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We compare TN and $\\mathrm{TN+}$ opt with a PyTorch implementation of the weight VJP via torch.autograd.grad. Figure F21 visualizes the performance ratios for different convolution categories. Table F7 contains the detailed run times and performance factors. ", "page_idx": 37}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/a74924879efdf910de017c3f98bd67e547428654eb9f356dc3dd9e762e5b3eaf.jpg", "img_caption": ["Figure F21: Weight VJP performance ratios of TN versus PT and TN+opt versus PT for different convolution types on GPU. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Table F7: Weight VJP performance comparison on GPU. (a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32) ", "text_level": 1, "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/52fa7a95e213017a687a4c20c6c395589d9fd7fb5908fb2b4b6589cb5f4130cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/b4804d7389eed79ff4768c033e048798930d76156552509dbe3bcaf04e782794.jpg", "table_caption": ["(b) F-MNIST 2c2d, input shape (128, 1, 28, 28) "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/6c44166793009ef5951210914e91f16047cfb945672e78e50d2f56c8a8daf1c2.jpg", "table_caption": ["(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32) "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/551559a931f9302ea3acb1810602caf692a711e43c1313f2e1628ed4583c4d2e.jpg", "table_caption": ["(d) Alexnet, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/09b9de81051b6614df19a37de86b1253227aab6420348c4d3427b618ce0299e4.jpg", "table_caption": ["(e) ResNet18, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/790c4403ab80d5c9c641261353b51a136830d499527a1ab6d2669d7e33ea422f.jpg", "table_caption": ["(f) ResNext101, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/e5ce4be4eef3486faa49be1a9ac7ac7deed994d6e32d543b20241f5aa37f8190.jpg", "table_caption": ["(g) ConvNeXt-base, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/0300e7849e75292524d8c394daab40dd383c9530d87c9604d3457d0f80a73fa0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/1596d2a86392b252e24d4194dd3c0b7f08c4f05ece07d573a496ad11372bb27b.jpg", "table_caption": ["(i) MobileNetV2, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "F.5 KFC Factor (KFAC-expand) ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We compare TN and $\\mathrm{TN+}$ opt with a PyTorch implementation of the input-based KFC factor based on torch.nn.functional.unfold. Figure F22 visualizes the performance ratios for different convolution categories. Table F8 contains the detailed run times and performance factors. ", "page_idx": 40}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/ee8bb6ba5612fddeb40e623ecea7026d1a1e50498cf2eb4007410c34a8065aad.jpg", "img_caption": ["Figure F22: KFC/KFAC-expand factor performance ratios of TN versus PT and TN+opt versus PT for different convolution types on GPU. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Table F8: KFC (KFAC-expand) factor performance comparison on GPU. (a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32) ", "text_level": 1, "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/1e58dd07f19a89de5be37bf05c4a00e13fcbce75f1edf5aad5ea28c8f90b8408.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/32c97c2ef690998ecffa6bd30ffc69bf0a95577e28f45076721dc9dbb9dcfd27.jpg", "table_caption": ["(b) F-MNIST 2c2d, input shape (128, 1, 28, 28) "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/235ef4693e9e5777bfe8ba81ef4baef336f93e68af2270fd5bda88d5c5b90f14.jpg", "table_caption": ["(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32) "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/f056bc78003d367ef7d8da3782783c9317c5829191ef867bf12e51431a42fa99.jpg", "table_caption": ["(d) Alexnet, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/4e5b6af463da1c9de9359e7eb3b29f9488096fd29910ea9d061ccf42b7131f96.jpg", "table_caption": ["(e) ResNet18, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/b49d1173d91bd17276c90b4dde48c3f6da774e3ce897180f4519c2aa57d88138.jpg", "table_caption": ["(f) ResNext101, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/be872d245f16c3b5f410fcc4c2091ab11413252e6f5c2b7c2b91c0af365d7304.jpg", "table_caption": ["(g) ConvNeXt-base, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/1d3465f8dd352d26ce161184db8a5b0a78beafa9af60310f3992f787aed40008.jpg", "table_caption": [], "table_footnote": [], "page_idx": 42}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/b09abc337d2b977d8ae4479ed4063a96612a8c3f42aa32222842a31c005c19cc.jpg", "table_caption": ["(i) MobileNetV2, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "F.6 KFAC-reduce Factor ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We compare TN and $\\mathrm{TN+}$ opt with a PyTorch implementation of the input-based KFAC-reduce factor based on torch.nn.functional.unfold. Figure F23 visualizes the performance ratios for different convolution categories. Table F9 contains the detailed run times and performance factors. ", "page_idx": 43}, {"type": "image", "img_path": "cDS8WxnMVP/tmp/0f4e3967d200efee7d9b72a5f4b5ae67cdf13f1eb1fe3a8147e0f905e9f8c897.jpg", "img_caption": [], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Figure F23: KFAC-reduce factor performance ratios of TN versus PT and TN+opt versus PT for different convolution types on GPU. ", "page_idx": 43}, {"type": "text", "text": "Table F9: KFAC-reduce factor performance comparison on GPU. (a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32) ", "text_level": 1, "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/b75a64828e94039fed68cc75fbd1642d032a11349a81dea421b7137594c5ead7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/4333b078d67dea88a33160868ae709b9ca259bd9e212abbb98c0e7635c8efa86.jpg", "table_caption": ["(b) F-MNIST 2c2d, input shape (128, 1, 28, 28) "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/b505d3b68ad5ed955a773810cfba020763af296354379568bffb348fbcf46ad0.jpg", "table_caption": ["(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32) "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/51fc6f84c632163831dff3d50aa6dbbfff43c4daa1649ea3b77dc8ca41382dbb.jpg", "table_caption": ["(d) Alexnet, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/401c380529f72b13d5979891be24658e92d4aa405aa222811b527ad0e1ffaaa4.jpg", "table_caption": ["(e) ResNet18, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/dcf5681775c9b55528b1eae1b7621fbdd3f101ac8e504f0c778258617706c74c.jpg", "table_caption": ["(f) ResNext101, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/7fdce973708e02933a262243ff798f71ec4f3a310e3eb9f7da47def81a6f9cfc.jpg", "table_caption": ["(g) ConvNeXt-base, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/a415cad1a0b6157501c2c5708e2193ff6115260ed9ae4dcfc0ffab8209acf179.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/d81fb3d77356e6544bc9be36bca1db93596b8a8a5a8deae80914b4b362001dd6.jpg", "table_caption": ["(i) MobileNetV2, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "G Memory Evaluation Details (CPU) ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Here, we investigate the peak memory consumption of our proposed TN implementations. ", "page_idx": 46}, {"type": "text", "text": "G.1 Theoretical & Empirical Analysis for KFAC-reduce Factor ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We assume a two-dimensional convolution with input $\\pmb{\\Upsilon}$ of shape $\\left(C_{\\mathrm{in}},I_{1},I_{2}\\right)$ , output of shape $(C_{\\mathrm{out}},O_{1},O_{2})$ and kernel of shape $(C_{\\mathrm{out}},C_{\\mathrm{in}},K_{1},K_{2})$ . The analysis with a batch dimension is analogous; hence we suppress it here to de-clutter the notation. ", "page_idx": 46}, {"type": "text", "text": "The main difference between the default and our proposed TN implementation of $\\hat{\\Omega}$ from $\\S3.3$ lies in the computation of the averaged unfolded input $[\\![\\mathsf{X}]\\!]^{(\\mathrm{avg})}:={\\imath}/(O_{1}O_{2})\\mathbf{1}_{O_{1}O_{2}}^{\\top}[\\![\\mathsf{X}]\\!]$ which consists of $C_{\\mathrm{in}}K_{1}K_{2}$ numbers. In the following, we will look  a t t he extra memory on top  of  storing the input $\\pmb{\\Upsilon}$ , the averaged unfolded input $[\\mathbf{X}]^{\\mathrm{(avg)}}$ , and the result $\\hat{\\Omega}$ . ", "page_idx": 46}, {"type": "text", "text": "Default implementation: The standard implementation computes $[\\mathbf{X}]^{\\mathrm{(avg)}}$ via the unfolded input $[\\![X]\\!]$ and thus requires extra storage of $C_{\\mathrm{in}}K_{1}K_{2}O_{1}O_{2}$ numbers. ", "page_idx": 46}, {"type": "text", "text": "TN implementation (general case): The TN implementation requires storing the averaged index patterns $\\begin{array}{r}{\\pmb{\\Pi}^{(i,\\mathrm{avg})}\\;:=\\;\\mathtt{i}\\bar{j}_{O_{i}}\\sum_{o=1}^{O_{i}}[\\pmb{\\Pi}^{(i)}]_{:,o,}}\\end{array}$ : for $i\\,=\\,1,2$ . These are directly computed via a slight modification of Algorithm D1 and require storing $I_{1}K_{1}+I_{2}K_{2}$ numbers. In contrast to the default implementation, spatial dimensions are de-coupled and there is no dependency on $C_{\\mathrm{in}}$ . ", "page_idx": 46}, {"type": "text", "text": "TN implementation (structured case): For structured convolutions (Figure 5) we can describe the action of the index pattern tensor through reshape and narrowing operations. ML libraries usually perform these without allocating additional memory. Hence, our symbolic simplifications completely eliminate the allocation of temporary intermediates to compute $\\mathbb{[X]}^{(\\mathrm{avg})}$ . ", "page_idx": 46}, {"type": "text", "text": "Empirical results: To demonstrate the memory reduction inside the computation of $\\hat{\\Omega}$ we measure its peak memory with the memory-profiler library and subtract the memory required to store $\\pmb{\\Upsilon}$ and $\\hat{\\Omega}$ . This approximates the extra internal memory requirement of an implementation. With the setup of $\\S\\mathrm{F}$ we report the minimum additional memory over 50 independent runs in Table G10. We consistently observe that the TN implementation has lower peak memory, which is further reduced by our symbolic simplifications (see for example the effect on ResNext101\u2019s dense and down-sampling convolutions in Table G10f). ", "page_idx": 46}, {"type": "text", "text": "Our theoretical analysis from above suggests that the peak memory difference becomes most visible for many channels with large kernel and output sizes. One example are ConxNeXt-base\u2019s features.1.0.block.0 convolutions with $K_{1}=K_{2}=7$ , $O_{1}=O_{2}=64$ , and $C_{\\mathrm{in}}=128$ (Table $\\mathrm{E4g},$ ). For those convolutions, we observe that the default implementation requires an additional 3,140 MiB $\\mathrm{(\\approx3\\,GiB!)}$ ) of memory, whereas the TN implementation has zero extra memory demand (Table G10g). This is consistent with our theoretical analysis in that the overhead is storing the unfolded input, which has $\\left[N=32\\right]$ ) \u00b7 $C_{\\mathrm{in}}=128)\\cdot(O_{1}\\stackrel{\\cdot}{=}64)\\cdot(O_{2}=64)\\cdot(K_{1}=7)\\cdot(\\bar{K}_{2}=$ $7)=822,083,58\\iota$ 4 float32 entries, corresponding to 3,136 MiB. ", "page_idx": 46}, {"type": "text", "text": "Table G10: Additional internally required memory to compute the KFAC-reduce factor (measured on CPU). The value 0 indicates that an implementation\u2019s peak memory matches the memory consumption of its input $\\pmb{\\Upsilon}$ and result $\\hat{\\Omega}$ . ", "page_idx": 47}, {"type": "text", "text": "(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32) ", "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/d051c366c83977c18cbe77ee989d69badb372da4ac12fca2ea19eef617095299.jpg", "table_caption": [], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "(b) F-MNIST 2c2d, input shape (128, 1, 28, 28) ", "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/b725a546d0fbbf4b8dd95258de465ec29440e4aaade465cc0cde42c12e041001.jpg", "table_caption": [], "table_footnote": [], "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/6ba5bba7f9be44b5375a7ca75e8085a52a0bdb9df3547cf7737b0c417f8bf944.jpg", "table_caption": ["(c) CIFAR-100 All-CNN-C, input shape (128, 3, 32, 32) "], "table_footnote": [], "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/283015730a0c8d5e778974278494a5e1b516c7243d98207d72bfdfb03712031e.jpg", "table_caption": ["(d) Alexnet, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/c5412f30fdf57ee692548da55fc7aa077cf84a5ec715e066ac7a4049b501b101.jpg", "table_caption": ["(e) ResNet18, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/a683e81d2b1bb5fd12fb4ef53a862bdb2efeafaccca63b125f87a7e2a0c84a60.jpg", "table_caption": ["(f) ResNext101, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/3ff41f89fc31c23d92740b608eb3662f277f49682b8f641dfe1aa2238a247abd.jpg", "table_caption": ["(g) ConvNeXt-base, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 47}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/126b8c8e4b15ea8fe41e133a2904f98a6845191cca820000423ef6c580cd4c58.jpg", "table_caption": ["(h) InceptionV3, input shape (32, 3, 299, 299) "], "table_footnote": [], "page_idx": 48}, {"type": "table", "img_path": "cDS8WxnMVP/tmp/942cf62b29b51f8ae98592f05b01791ba1d767b193dd8de62e171352230d1b9f.jpg", "table_caption": ["(i) MobileNetV2, input shape (32, 3, 256, 256) "], "table_footnote": [], "page_idx": 48}, {"type": "text", "text": "H Miscellaneous ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "H.1 Example: Associativity of Tensor Multiplication ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Here, we demonstrate associativity of tensor multiplication through an example. The technical challenge is that an index can only be summed once there are no remaining tensors sharing it. Therefore, we must carry indices that are summed in later multiplications in the intermediate results, which requires some set arithmetic on the index sets. ", "page_idx": 49}, {"type": "text", "text": "Let $S_{1},S_{2},S_{3}$ be index tuples of the input tensors $\\mathbf{A},\\mathbf{B},\\mathbf{c}$ , and $S_{4}\\subseteq(S_{1}\\cup S_{2}\\cup S_{3})$ a valid output index tuple of their tensor multiplication $\\pmb{\\mathsf{D}}=*_{(S_{1},S_{2},S_{3},S_{4})}(\\pmb{\\mathsf{A}},\\pmb{\\mathsf{B}},\\pmb{\\mathsf{C}})$ . We can either first multiply $\\pmb{\\triangle}$ with $\\mathbf{B}$ to obtain an intermediate tensor of index structure $S_{1,2}$ , or $\\mathbf{B}$ with $\\mathfrak{c}$ to obtain an intermediate tensor of index structure $S_{2,3}$ , before carrying out the remaining multiplications. To construct the intermediate index structures, we divide the indices $\\tilde{S}\\,=\\,(S_{1}\\cup S_{2}\\cup S_{3})\\mid S_{4}$ that are summed over into those only shared between $\\mathsf{A},\\mathsf{B}$ given by $\\tilde{S}_{1,2}=(S_{1}\\cup S_{2})\\setminus(S_{4}\\cup S_{3})$ , and those only shared among $\\mathbf{B},\\mathbf{c}$ given by $\\tilde{S}_{2,3}=(S_{2}\\cup S_{3})\\setminus(S_{4}\\cup S_{1})$ . This yields the intermediate indices $S_{1,2}=(S_{1}\\cup S_{2})\\setminus\\tilde{S}_{1,2}$ and $S_{2,3}=(S_{2}\\cup S_{3})\\setminus\\tilde{S}_{2,3}$ , and the parenthesizations ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathbf{\\bar{D}}]_{S_{4}}=\\left(\\sum_{\\tilde{S}\\backslash\\tilde{S}_{1,2}}\\left(\\sum_{\\tilde{S}_{1,2}}[\\mathbf{\\hat{A}}]_{S_{1}}[\\mathbf{\\bar{B}}]_{S_{2}}\\right)[\\mathbf{\\bar{C}}]_{S_{3}}\\right)=\\left(\\sum_{\\tilde{S}\\backslash\\tilde{S}_{2,3}}[\\mathbf{\\hat{A}}]_{S_{1}}\\left(\\sum_{\\tilde{S}_{2,3}}[\\mathbf{\\hat{B}}]_{S_{2}}[\\mathbf{\\bar{C}}]_{S_{3}}\\right)\\right)}\\\\ &{\\Leftrightarrow\\ \\mathbf{\\bar{D}}=\\ast_{(S_{1,2},S_{3},S_{4})}\\left(\\ast_{(S_{2},S_{3},S_{2,3})}(\\mathbf{\\hat{A}},\\mathbf{\\bar{B}}),\\mathbf{\\bar{C}}\\right)=\\ast_{(S_{1},S_{2,3},S_{4})}\\left(\\mathbf{\\hat{A}},\\ast_{(S_{1},S_{2},S_{1,2})}(\\mathbf{\\bar{B}},\\mathbf{C})\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This generalizes to $n$ -ary multiplication, allowing to break it down into smaller multiplications.   \nHowever, the index notation and set arithmetic from Equation (H20) quickly becomes impractical. ", "page_idx": 49}, {"type": "text", "text": "H.2 Example: Matrix-matrix Multiplication as Tensor Multiplication ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Here we provide a small self-contained example that demonstrates Equation (3) for matrix-matrix multiplication. ", "page_idx": 49}, {"type": "text", "text": "Consider two matrices $A,B$ which are compatible for multiplication and let $C=A B$ . In index notation, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n[C]_{i,k}=\\sum_{j}[A]_{i,j}[B]_{j,k}\\;.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The index tuples are $S_{A}=(i,j)$ , $S_{B}=(j,k)$ , and $S_{C}=(i,k)$ . Next, we evaluate which indices are summed over. Since the order of those indices does not matter, we can interpret the tuples as sets and use set arithmetic: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left(S_{A}\\cup S_{B}\\right)\\backslash S_{C}=\\left(\\left(i,j\\right)\\cup\\left(j,k\\right)\\right)\\backslash\\left(i,k\\right)=\\left(j\\right)\\backslash\\left(i,k\\right)=\\left(j\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now we see that matrix-matrix multiplication is a case of tensor multiplication (Equation (3)), ", "page_idx": 49}, {"type": "equation", "text": "$$\n[{\\cal C}]_{S_{\\cal C}}=\\sum_{(S_{\\cal A}\\cup S_{\\cal B})\\setminus{S_{\\cal C}}}[{\\cal A}]_{S_{\\cal A}}[{\\cal B}]_{S_{\\cal B}}=*_{(S_{\\cal A},S_{\\cal B},S_{\\cal C})}({\\cal A},{\\cal B})\\,.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We provide a bullet point list of our contributions in $\\S1$ , each of which references the part of the paper that outlines the contribution. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 50}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: See $\\S4.2$ and $\\S\\mathrm{A}$ . ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 50}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The TN simplifications we provide in $\\S4.1$ follow straightforward from the index pattern\u2019s structure Equation (7) and are stated rigorously in $\\S D.3$ , including their assumptions. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 51}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We provide implementation details in $\\S D$ , experimental and hardware details in $\\S\\mathrm{F}$ and G. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 51}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We will open-source the code to reproduce all our experiments, as well as the raw data containing the results shown in the manuscript. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 52}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: See $\\S\\mathrm{F}$ and G for details on the experimental setting. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 52}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: Our run time plots contain box plots with medians and quartiles reported over different convolutions, and the randomized backpropagation results show mean and standard deviations for 10 different model and batch initializations. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 52}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 53}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: All results were obtained on a single GPU to be comparable in terms of run time. See $\\S\\mathrm{F}$ for the details. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 53}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We have read the Code of Ethics and believe that our work conforms to it. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 53}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: This work aims to provide a simplifying perspective and implementations of otherwise hard-to-access operations for convolutions to facilitate the exploration of algorithmic ideas and advance existing second-order methods. We don\u2019t see any direct negative societal impacts. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 54}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: The paper does not release any data or models that have a high risk for misuse. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 54}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We cite the papers introducing the neural network architectures and data sets used in our experiments. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 54}, {"type": "text", "text": "", "page_idx": 55}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 55}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 55}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 55}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 56}]