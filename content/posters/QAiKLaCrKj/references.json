{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides background information on GPT-4, a large language model, which is relevant to the study of parameter heterogeneity and quantization in LLMs."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper discusses Qwen, another large language model which helps to expand the scope of the study on parameter heterogeneity and quantization in LLMs."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "publication_date": "2013-08-13", "reason": "This paper introduces a method for estimating or propagating gradients through stochastic neurons, which is relevant to the optimization of mixed-precision parameters in CherryQ."}, {"fullname_first_author": "Yelysei Bondarenko", "paper_title": "Understanding and overcoming the challenges of efficient transformer quantization", "publication_date": "2021-11-18", "reason": "This paper explores the challenges of efficient transformer quantization, which provides context for the research on parameter heterogeneity and quantization in LLMs."}, {"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality", "publication_date": "2023-03-30", "reason": "This paper introduces Vicuna, a large language model chatbot used in the experiments of this paper, and therefore is highly relevant to the main findings."}]}