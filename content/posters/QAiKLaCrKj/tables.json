[{"figure_path": "QAiKLaCrKj/tables/tables_5_1.jpg", "caption": "Table 1: Overlap ratio (%) of cherry parameters (top 1/256), where L2: LLaMA-2, V: Vicuna-1.5, M: Mistral, G: Gemma.", "description": "This table shows the overlap of cherry parameters (the top 1/256 most impactful parameters) across different datasets and models.  The \"Within dataset\" row indicates the overlap when selecting cherry parameters from multiple random subsets within the same dataset. The \"Across datasets\" row shows the overlap of cherry parameters identified from different datasets (C4, WikiText-2, ShareGPT). This demonstrates the consistency and data-independence of cherry parameters across different model families and data sources.", "section": "6 Quantization Experiments"}, {"figure_path": "QAiKLaCrKj/tables/tables_6_1.jpg", "caption": "Table 2: Perplexity (\u2193) of 3-bit quantization on LLaMA2 models. gX means the group size is X. The results of OmniQuant and AWQ are from [21]. The results of SqueezeLLM are from [12].", "description": "This table presents the perplexity results of different 3-bit quantization methods on LLaMA2 models with varying group sizes (g64 and g128).  It compares the performance of CherryQ against several baselines including QAT, GPTQ, AWQ, OmniQuant, and SqueezeLLM, using the C4 and WikiText-2 datasets. Lower perplexity scores indicate better performance.", "section": "6.2.1 Perplexity Results"}, {"figure_path": "QAiKLaCrKj/tables/tables_6_2.jpg", "caption": "Table 3: Perplexity (\u2193) of 4-bit quantization on LLaMA2 models.", "description": "This table presents the perplexity scores achieved by different 4-bit quantization methods on the LLaMA2 model.  It compares the performance of CherryQ against baselines such as QAT, GPTQ, AWQ, and OmniQuant across two datasets (c4 and wiki2) and two model sizes (7B and 13B parameters). Lower perplexity indicates better performance. The results demonstrate the superior performance of CherryQ.", "section": "6.2 Effect of Base LLM Quantization"}, {"figure_path": "QAiKLaCrKj/tables/tables_7_1.jpg", "caption": "Table 4: Performance of different 3-bit quantization methods on Huggingface OpenLLM for LLaMA2-7B and LLaMA2-13B.", "description": "This table compares the performance of different 3-bit quantization methods (FP16, QAT, GPTQ, and CherryQ) on various downstream tasks from the HuggingFace OpenLLM Leaderboard, using LLaMA2-7B and LLaMA2-13B models.  The results show the average scores across multiple tasks for different group sizes (g64 and g128) to demonstrate the effectiveness of the CherryQ quantization approach.", "section": "6.2.2 Downstream Task Performance"}, {"figure_path": "QAiKLaCrKj/tables/tables_7_2.jpg", "caption": "Table 4: Performance of different 3-bit quantization methods on Huggingface OpenLLM for LLaMA2-7B and LLaMA2-13B.", "description": "This table compares the performance of different 3-bit quantization methods (FP16, QAT, GPTQ, and CherryQ) on various downstream tasks from the HuggingFace OpenLLM Leaderboard for two different sizes of LLaMA2 models (7B and 13B parameters).  The results are presented as average scores across multiple tasks, illustrating the relative effectiveness of each quantization technique in maintaining model performance after reducing the precision of the model parameters.", "section": "6.2.2 Downstream Task Performance"}, {"figure_path": "QAiKLaCrKj/tables/tables_8_1.jpg", "caption": "Table 6: Perplexity (\u2193) of 2-bit quantization on LLaMA2 models. The results of GPTQ, AWQ and OmniQuant are from [21].", "description": "This table presents the perplexity results of 2-bit quantization on LLaMA2 models using different methods.  It compares the performance of CherryQ against GPTQ, AWQ, and OmniQuant, showing perplexity scores for both 7B and 13B parameter models with different group sizes (g64 and g128). The results demonstrate CherryQ's superior performance in 2-bit quantization across various settings.", "section": "6.4 Extreme 2-Bit Quantization"}, {"figure_path": "QAiKLaCrKj/tables/tables_8_2.jpg", "caption": "Table 7: Perplexity (\u2193) of different parameter selection criteria.", "description": "This table compares the perplexity results of using different parameter selection criteria (Weight, Activation, Impact) for 3-bit and 4-bit quantization of LLaMA2 models with group sizes of 64 and 128.  It demonstrates the superiority of the Impact-based criterion for identifying cherry parameters.", "section": "6.5 Comparison of Parameter Selection Criteria"}, {"figure_path": "QAiKLaCrKj/tables/tables_11_1.jpg", "caption": "Table 8: Comparison of different 3-bit quantization methods on zero-shot MMLU accuracy applied to Vicuna-1.5.", "description": "This table presents the results of a comparison of different 3-bit quantization methods on the zero-shot MMLU (Massive Multitask Language Understanding) accuracy for Vicuna-1.5, a large language model.  The methods compared include FP16 (full precision), QAT (quantization-aware training), GPTQ (quantized GPT), and CherryQ (the proposed method). The accuracy is broken down by category (Humanities, STEM, Social Sciences, Other) and also shown as an average across all categories for both the 7B and 13B parameter versions of the model.  The table highlights the performance of CherryQ in comparison to existing methods.", "section": "6.3 Effect of Chat LLM Quantization"}]