{"references": [{"fullname_first_author": "Z. Allen-Zhu", "paper_title": "A convergence theory for deep learning via over-parameterization", "publication_date": "2019-00-00", "reason": "This paper provides a theoretical foundation for deep learning convergence, which is relevant to the understanding of Transformer model optimization dynamics."}, {"fullname_first_author": "S. Bhojanapalli", "paper_title": "Low-rank bottleneck in multi-head attention models", "publication_date": "2020-00-00", "reason": "This paper identifies a crucial limitation in Transformer models, explaining the low-rank bottleneck issue and its implications for optimization and practical performance."}, {"fullname_first_author": "Y. Chen", "paper_title": "Skyformer: Remodel self-attention with gaussian kernel and nystr\"om method", "publication_date": "2021-00-00", "reason": "This paper explores alternative attention mechanisms in Transformers, specifically the Gaussian kernel, which is compared to Softmax in the target paper."}, {"fullname_first_author": "S. Du", "paper_title": "Gradient descent finds global minima of deep neural networks", "publication_date": "2019-00-00", "reason": "This paper examines the convergence properties of gradient descent in deep neural networks, establishing the conditions for global minimum finding and is highly relevant to the current paper's focus on training Transformers."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture which is the primary subject of the target paper's analysis of optimization dynamics."}]}