[{"type": "text", "text": "M3-Impute: Mask-guided Representation Learning for Missing Value Imputation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Missing values are a common problem that poses significant challenges to data   \n2 analysis and machine learning. This problem necessitates the development of an   \n3 effective imputation method to flil in the missing values accurately, thereby en  \n4 hancing the overall quality and utility of the datasets. Existing imputation meth  \n5 ods, however, fall short of considering the \u2018missingness\u2019 information in the data   \n6 during initialization and modeling the entangled feature and sample correlations   \n7 explicitly during the learning process, thus leading to inferior performance. We   \n8 propose $\\mathbf{\\dot{M}}^{3}$ -Impute, which aims to leverage the missingness information and such   \n9 correlations with novel masking schemes. $\\mathbf{M}^{3}$ -Impute first models the data as a   \n10 bipartite graph and uses an off-the-shelf graph neural network, equipped with a   \n1 refined initialization process, to learn node embeddings. They are then optimized   \n12 through $\\mathbf{M}^{3}$ -Impute\u2019s novel feature correlation unit (FCU) and sample correlation   \n13 unit (SCU) that enable explicit consideration of feature and sample correlations   \n14 for imputation. Experiment results on 15 benchmark datasets under three different   \n15 missing patterns show the effectiveness of $\\mathbf{M}^{3}$ -Impute by achieving 13 best and 2   \n16 second-best MAE scores on average. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Missing values in a dataset are a pervasive issue in real-world data analysis. They arise for various   \n19 reasons, ranging from the limitations of data collection methods to errors during data transmission   \n20 and storage. Since many data analysis algorithms cannot directly handle missing values, the most   \n21 common way to deal with them is to discard the corresponding samples or features with missing   \n22 values, which would compromise the quality of data analysis. To tackle this problem, missing value   \n23 imputation algorithms have been proposed to preserve all samples and features by imputing missing   \n24 values with estimated ones based on the observed values in the dataset, so that the dataset can be   \n25 analyzed as a complete one without losing any information.   \n26 The imputation of missing values usually requires modeling of correlations between different fea  \n27 tures and samples. Feature-wise correlations help predict missing values from other observed fea  \n28 tures in the same sample, while sample-wise correlations help predict them in one sample from other   \n29 similar samples. It is thus important to jointly model the feature-wise and sample-wise correlations   \n30 in the dataset. In addition, the prediction of missing values also largely depends on the \u2018missingness\u2019   \n31 of the data, i.e., whether a certain feature value is observed or not in the dataset. Specifically, the   \n32 missingness information directly determines which observed feature values can be used for imputa  \n33 tion. For example, even if two samples are closely related, it may be less effective to use them for   \n34 imputation if they have missing values in exactly the same features. It still remains a challenging   \n35 problem how to jointly model feature-wise and sample-wise correlations with such data missingness.   \n36 Among existing methods for missing value imputation, statistical methods [4, 9, 14, 16, 18, 19, 22,   \n37 28, 30, 31, 37, 43] extract data correlations with statistical models, which are generally not flexible   \n38 in handling mixed data types and struggles to scale up to large datasets. Learning-based imputation   \n39 methods [10, 24, 27, 29, 33, 42, 50, 51, 53], instead, take advantage of the strong expressiveness   \n40 and scalability of machine/deep learning algorithms to model data correlations. However, most of   \n41 them are still built upon the raw tabular data structure as is, which greatly restricts them from jointly   \n42 modeling the feature-wise and sample-wise correlations. In light of this, graph-based methods [52,   \n43 54] have been proposed to model the raw data as a bipartite graph, with samples and features being   \n44 two different types of nodes. A sample node and a feature node are connected if the feature value   \n45 is observed in that sample. The missing values are then predicted as the inner product between   \n46 the embeddings of the corresponding sample and feature nodes. However, this simple prediction   \n47 does not consider the specific missingness information as mentioned above. For instance, the target   \n48 feature to impute may have different correlations with features in the samples which have different   \n49 kinds of missingness; however, the same feature-node embedding is still used for their imputation.   \n50 A similar issue also arises for sample-node embeddings.   \n51 In this work, we address these problems by proposing $\\mathbf{M}^{3}$ -Impute, a mask-guided representation   \n52 learning method for missing value imputation. The key idea behind $\\mathbf{M}^{3}$ -Impute is to explicitly   \n53 utilize the data-missingness information as model input with our proposed novel masking schemes   \n54 so that it can accurately learn feature-wise and sample-wise correlations in the presence of different   \n55 kinds of data missingness. $\\mathbf{M}^{3}$ -Impute first builds a bipartite graph from the data as used in [52].   \n56 In the embedding initialization for graph representation learning, however, we not only use the the   \n57 relationships between samples and their associated features but also the missingness information so   \n58 as to initialize the embeddings of samples and features jointly and effectively. We then propose novel   \n59 feature correlation unit (FCU) and sample correlation unit (SCU) in $\\mathbf{M}^{3}$ -Impute to explicitly take   \n60 feature-wise and sample-wise correlations into account for imputation. FCU learns the correlations   \n61 between the target missing feature and observed features within each sample, which are then further   \n62 updated via a soft mask on the sample missingness information. SCU then computes the sample  \n63 wise correlations with another soft mask on the missingness information for each pair of samples   \n64 that have values to impute. We then integrate the output embeddings of FCU and SCU to estimate   \n65 the missing values in a dataset. We carry out extensive experiments on 15 open datasets. The results   \n66 show that $\\mathbf{\\bar{M}}^{3}$ -Impute outperforms state-of-the-art methods in 13 of the 15 datasets on average under   \n67 three different settings of missing value patterns, achieving up to $11.47\\%$ improvement in MAE   \n68 compared to the second-best method. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 Statistical methods: These imputation approaches include joint modeling with expectation  \n71 maximization (EM) [9, 16, 22], $k$ -nearest neighbors (kNN) [14, 43], and matrix completion [5,   \n72 6, 18, 32]. However, joint modeling with EM and matrix completion often lack the flexibility to   \n73 handle data with mixed modalities, while kNN faces scalability issues due to its high computational   \n74 complexity. In contrast, $\\mathbf{M}^{3}$ -Impute is scalable and adaptive to different data distributions.   \n75 Learning-based methods: Iterative imputation frameworks [1, 2, 15, 20, 23, 24, 35, 41, 44, 45],   \n76 such as MICE [45] and HyperImpute [23], have been extensively studied. These iterative frame  \n77 works apply different imputation methods for each feature and iteratively estimate missing val  \n78 ues until convergence. In addition, for deep neural network learners, both generative mod  \n79 els [27, 29, 36, 50, 51, 53], such as GAIN [50] and MIWAE [29], and discriminative mod  \n80 els [10, 24, 48], such AimNet [48], have also been proposed. However, these methods are built   \n81 upon raw tabular data structures, which fall short of capturing the complex correlations in features,   \n82 samples, and their combination [54]. In contrast, $\\mathbf{M}^{3}$ -Impute is based on the bipartite graph model  \n83 ing of the data, which is more suitable for learning the data correlations for imputation.   \n84 Graph neural network-based methods: GNN-based methods [40, 52, 54] are proposed to address   \n85 the drawbacks mentioned above due to their effectiveness in modeling complex relations between   \n86 entities. Among them, GRAPE [52] transforms tabular data into a bipartite graph where features are   \n87 one type of node and samples are the other. A sample node is connected to a feature node only if the   \n88 corresponding feature value is present. This transformation allows the imputation task to be framed   \n89 as a link prediction problem, where the inner product of the learned node embeddings is computed   \n90 as the predicted values. IGRM [54] further enhances the bipartite graph by explicitly introducing   \n91 linkages between sample nodes to facilitate message propagation between samples. However, these   \n92 methods do not effectively encode the missingness information of different samples and features into   \n93 the imputation process, which can impair their imputation accuracy. In contrast, $\\mathbf{M}^{3}$ -Impute enables   \n94 explicit modeling of missingness information through novel masking schemes so that feature-wise   \n95 and sample-wise correlations can be accurately captured in the imputation process. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "1MQXBnEbE8/tmp/411c98c3d43b19f6b9b035c64daa6c6d1b3305d21e79ed0eed81cfd736136a0b.jpg", "img_caption": ["Figure 1: Overview of the $\\mathbf{M}^{3}$ -Impute model. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "96 3 $\\mathbf{M}^{3}$ -Impute ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "98 We here provide an overview of $\\mathbf{M}^{3}$ -Impute to impute the missing value of feature $f$ for a given   \n99 sample $s$ , as depicted in Figure 1. Initially, the data matrix with missing values is modeled as an   \n100 undirected bipartite graph, and the missing value is imputed by predicting the edge weight $\\hat{e}_{s f}$ of   \n101 its corresponding missing edge (Section 3.2). $\\mathbf{M}^{3}$ -Impute next employs a GNN model, such as   \n102 GraphSAGE [17], on the bipartite graph to learn the embeddings of samples and features. These   \n103 embeddings, along with the known masks of the data matrix (used to indicate which feature values   \n104 are available in each sample), are then input into our novel feature correlation unit (FCU) and   \n105 sample correlation unit (SCU), which shall be explained in Section 3.3 and Section 3.4, to obtain   \n06 feature-wise and sample-wise correlations, respectively. Finally, $\\mathbf{M}^{3}$ -Impute takes the feature-wise   \n107 and sample-wise correlations into a multi-layer perceptron (MLP) to predict the missing feature   \n108 value $\\hat{e}_{s f}$ (Section 3.5). The whole process, including the embedding generation, is trained in an   \n109 end-to-end manner. ", "page_idx": 2}, {"type": "text", "text": "110 3.2 Initialization Unit ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 Let $\\mathbf{A}\\,\\in\\,\\mathbb{R}^{n\\times m}$ be an $n\\times m$ matrix that consists of $n$ data samples and $m$ features, where $\\mathbf{A}_{i j}$   \n112 denotes the $j$ -th feature value of the $i$ -th data sample. We introduce an $n\\times m$ mask matrix $\\mathbf{M}\\in$   \n113 $\\{0,1\\}^{n\\times m}$ for A to indicate that the value of $\\mathbf{A}_{i j}$ is observed when $\\mathbf{M}_{i j}=1$ . In other words, the   \n114 goal of imputation here is to predict the missing feature values $\\mathbf{A}_{i j}$ for $i$ and $j$ such that $\\mathbf{M}_{i j}=0$ .   \n115 We define the masked data matrix $\\mathbf{D}$ to be $\\mathbf{D}=\\mathbf{A}\\odot\\mathbf{M}$ , where $\\odot$ is the Hadamard product, i.e., the   \n116 element-wise multiplication of two matrices.   \n117 As used in recent studies [52, 54], we model the masked data matrix $\\mathbf{D}$ as a bipartite graph and tackle   \n118 the missing value imputation problem as a link prediction task on the bipartite graph. Specifically,   \n119 $\\mathbf{D}$ is modeled as an undirected bipartite graph $\\bar{\\mathcal{G}}=(S\\cup\\mathcal{F},\\mathcal{E})$ , where $\\bar{S}=\\bar{\\{s_{1},s_{2},...,s_{n}\\}}$ is the   \n120 set of \u2018sample\u2019 nodes and $\\mathcal{F}=\\{f_{1},f_{2},\\ldots,f_{m}\\}$ is the set of \u2018feature\u2019 nodes. Also, $\\mathcal{E}$ is the set   \n121 of edges that only exist between sample node $s$ and feature node $f$ when $\\mathbf{D}_{s f}\\neq0$ , and each edge   \n122 $(s,f)\\bar{\\in}\\ \\mathcal{E}$ is associated with edge weight $e_{s f}$ , which is given by $e_{s f}\\,=\\,{\\bf D}_{s f}$ . Then, the missing   \n123 value imputation problem becomes, for any missing entries in $\\mathbf{D}$ (where $\\mathbf{D}_{s f}=0$ ), to predict their   \n124 corresponding edge weights by developing a learnable mapping $F(\\cdot)$ , i.e., ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{e}_{s f}=F(\\mathcal{G},(s,f)\\notin\\mathcal{E}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "125 The recent studies that use the bipartite graph modeling [52, 54] initialize all sample node embed  \n126 dings as all-one vectors and feature node embeddings as one-hot vectors, which have a value 1 in the   \n127 positions representing their respective features and 0\u2019s elsewhere. We observe, however, that such an   \n128 initialization does not effectively utilize the information from the masked data matrix, which leads   \n129 to inferior imputation accuracy, as shall be demonstrated in Section 4.3. Thus, in $\\mathbf{M}^{3}$ -Impute, we   \n130 propose to initialize each sample node embedding based on its associated (initial) feature embed  \n131 dings instead of initializing them separately. While the feature embeddings are randomly initialized,   \n132 the sample node embeddings are initialized in a way that reflects the embeddings of the features   \n133 whose values are available in their corresponding samples.   \n134 Let $\\mathbf{h}_{f}^{0}$ be the initial embedding of feature $f$ , which is a randomly initialized $d$ -dimensional vector,   \n135 and define $\\mathbf{H}_{F}^{0}=[\\mathbf{h}_{f_{1}}^{0}\\mathbf{h}_{f_{2}}^{0}\\cdot\\cdot\\cdot\\mathbf{h}_{f_{m}}^{0}]\\in\\mathbb{R}^{d\\times m}$ . Also, let $\\mathbf{d}_{s}\\in\\mathbb{R}^{m}$ be the $s_{\\mathrm{{}}}$ -th column vector of $\\mathbf{D}^{\\top}$ ,   \n136 which is a vector of the feature values of sample $s$ , and let $\\mathbf{m}_{s}\\,\\in\\,\\mathbb{R}^{m}$ be its corresponding mask   \n137 vector, i.e., $\\mathbf{m}_{s}=\\mathrm{col}_{s}(\\mathbf{M}^{\\top})$ , where $\\mathrm{col}_{s}(\\cdot)$ denotes the $s$ -th column vector of the matrix. We then   \n138 initialize the embedding $\\mathbf{h}_{s}^{0}$ of each sample node $s$ as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}_{s}^{0}=\\phi\\Bigl(\\mathbf{H}_{F}^{0}\\bigl[\\mathbf{d}_{s}+\\epsilon(\\mathbb{1}-\\mathbf{m}_{s})\\bigr]\\Bigr),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "139 where $\\mathbb{1}\\in\\mathbb{R}^{m}$ is an all-one vector, and $\\phi(\\cdot)$ is an MLP. Note that the term $\\mathbf{d}_{s}+\\epsilon(\\mathbb{1}-\\mathbf{m}_{s})$ indicates   \n140 a vector that consists of observable feature values of $s$ and some small positive values $\\epsilon$ in the places   \n141 where the feature values are unavailable (masked out). ", "page_idx": 3}, {"type": "text", "text": "142 3.3 Feature Correlation Unit ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "143 To improve the accuracy of missing value imputation, we aim to fully exploit feature correlations   \n144 which often appear in the datasets. While the feature correlations are naturally captured by GNNs,   \n145 we observe that there is still room for improvement. We propose FCU as an integral component of   \n146 $\\mathbf{M}^{3}$ -Impute to fully exploit the feature correlations.   \n147 To impute the missing value of feature $f$ for a given sample $s$ , FCU begins by computing the   \n148 feature \u2018context\u2019 vector of sample $s$ in the embedding space that reflects the correlations between   \n149 the target missing feature $f$ and observed features. Let $\\dot{\\mathbf{h}}_{f}\\in\\mathbb{R}^{d}$ be the learned embedding vector   \n150 of feature $f$ from the GNN, and let ${\\bf{H}}_{F}$ be the $d\\times m$ matrix that consists of all the learned feature   \n151 embedding vectors. We first obtain dot-product similarities between feature $f$ and all the features   \n152 in the embedding space, i.e., ${\\mathbf{H}}_{F}^{\\top}\\mathbf{h}_{f}$ . We then mask out the similarity values with respect to non  \n153 observed features in sample $s$ . Here, instead of applying the mask vector $\\mathbf{m}_{s}$ of sample $s$ directly,   \n154 we use a learnable \u2018soft\u2019 mask vector, denoted by $\\mathbf{m}_{s}^{\\prime}$ , which is defined to be ${\\bf m}_{s}^{\\prime}=\\sigma_{1}({\\bf m}_{s})\\in\\mathbb{R}^{m}$ ,   \n155 where $\\sigma_{1}(\\cdot)$ is an MLP with the GELU activation function [21]. In other words, we obtain feature  \n156 wise similarities with respect to sample $s$ , denoted by ${\\bf r}_{s}^{f}$ , as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{r}_{s}^{f}=\\sigma_{2}\\left((\\mathbf{H}_{F}^{\\top}\\mathbf{h}_{f})\\odot\\mathbf{m}_{s}^{\\prime}\\right)\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 where $\\sigma_{2}(\\cdot)$ denotes another MLP with the GELU activation function. FCU next obtains the   \n158 Hadamard product between the learned embedding vector of sample $s,\\,\\mathbf{h}_{s}$ , and the feature-wise   \n159 similarities with respect to sample s, ${\\bf r}_{s}^{f}$ , to learn their joint representations in a multiplicative man  \n160 ner. Specifically, FCU obtains the feature context vector of sample $s$ , denoted by $\\mathbf{c}_{s}^{f}$ , as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{c}_{s}^{f}=\\sigma_{3}\\left(\\mathbf{h}_{s}\\odot\\mathbf{r}_{s}^{f}\\right)\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "161 where $\\sigma_{3}(\\cdot)$ is also an MLP with the GELU activation function. That is, FCU fuses the represen  \n162 tation vector of $s$ and the vector that has embedding similarity values between the target feature $f$   \n163 and the available features in $s$ through the effective use of the soft mask $\\mathbf{m}_{s}^{\\prime}$ . From (3) and (4), the   \n164 operations of FCU can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{c}_{s}^{f}=\\mathbf{FCU}(\\mathbf{h}_{s},\\mathbf{m}_{s},\\mathbf{H}_{F})=\\sigma_{3}\\left(\\mathbf{h}_{s}\\odot\\sigma_{2}\\left((\\mathbf{H}_{F}^{\\top}\\mathbf{h}_{f})\\odot\\sigma_{1}(\\mathbf{m}_{s})\\right)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "165 3.4 Sample Correlation Unit ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "166 To measure similarities between $s$ and other samples, a common approach would be to use the   \n167 dot product or cosine similarity between their embedding vectors. This approach, however, fails   \n168 to take into account the observability or availability of each feature in a sample. It also does   \n169 not capture the fact that different observed features are of different importance to the target fea  \n170 ture to impute when it comes to measuring the similarities. We introduce SCU as another inte  \n171 gral component of $\\mathbf{M}^{3}$ -Impute to compute the sample \u2018context\u2019 vector of sample $s$ by incorpo  \n172 rating the embedding vectors of its similar samples as well as different weights of observed fea  \n173 tures. SCU works based on the two novel masking schemes, which shall be explained shortly.   \n174   \n175 Suppose we are to impute the missing value of feature $f$ for a given sample   \n176 $s$ . SCU aims to leverage the information from the samples that are similar   \n177 to $s$ . As a first step to this end, we create a subset of samples $\\mathcal P\\subset S$ that   \n178 are similar to $s$ . Specifically, we randomly choose and put a sample into   \n179 $\\mathcal{P}$ with probability that is proportional to the cosine similarity between $s$   \n180 and the sample. This operation is repeated without replacement until $\\mathcal{P}$   \n181 reaches a given size.   \n182 Mutual Sample Masking: Given a subset of samples $\\mathcal{P}$ that include $s$ ,   \n183 we first compute the pairwise similarities between $s$ and other samples in   \n184 the subset $\\mathcal{P}$ . While they are computed in a similar way to FCU, we only   \n185 consider the commonly observed features (or the common ones that have   \n186 feature values) in both $s$ and its peer $p\\in\\mathcal{P}\\setminus\\{s\\}$ , to calculate their pair  \n187 wise similarity in the sense that the missing value of feature $f$ is inferred.   \n188 Specifically, we compute the pairwise similarity between $s$ and $p\\!\\in\\!\\mathcal{P}\\backslash\\{s\\}$ ,   \n189 which is denoted by $\\sin(s,p\\mid f)$ , as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sin(s,p\\mid f)=\\mathbf{FCU}(\\mathbf{h}_{s},\\mathbf{m}_{p},\\mathbf{H}_{f})\\cdot\\mathbf{FCU}(\\mathbf{h}_{p},\\mathbf{m}_{s},\\mathbf{H}_{f})\\in\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "1MQXBnEbE8/tmp/468b4cdbd7370f6b36125a571dffa935ecd9b278db03e101c52f801041f2e8ab.jpg", "img_caption": ["Figure 2: SCU. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "190 where $\\mathbf{h}_{s}$ and $\\mathbf{h}_{p}$ are the learned embedding vectors of samples $s$ and $p$ from the GNN, respectively,   \n191 and $\\mathbf{m}_{s}$ and $\\mathbf{m}_{p}$ are their respective mask vectors. Note that the multiplication in the RHS of (6) is   \n192 the dot product.   \n193 Irrelevant Feature Masking: After we obtain the pairwise similarities between $s$ and other samples   \n194 in $\\mathcal{P}$ , it would be natural to consider a weighted sum of their corresponding embedding vectors, i.e.,   \n195 $\\begin{array}{r}{\\sum_{p\\in{\\mathcal{P}}\\backslash\\{s\\}}\\sin(s,p\\mid f)\\;\\mathbf{h}_{p}.}\\end{array}$ , in imputing the value of the target feature $f$ . However, we observe that   \n196 $\\mathbf{h}_{p}$ contains the information from the features whose values are available in $p$ as well as possibly   \n197 other features as it is learned via the so-called neighborhood aggregation mechanism that is central   \n198 to GNNs, but some of the features may be irrelevant in inferring the value of feature $f$ . Thus, instead   \n199 of using $\\{\\mathbf{h}_{p}\\}$ directly, we introduce a $d$ -dimensional mask vector $\\mathbf{r}_{p}^{f}$ for $\\mathbf{h}_{p}$ , which is to mask out   \n200 potentially irrelevant feature information in $\\mathbf{h}_{p}$ , when it comes to imputing the value of feature $f$ .   \n201 Specifically, it is defined by ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{r}_{p}^{f}=\\sigma_{4}\\left([\\mathbf{m}_{p};\\overline{{\\mathbf{m}}}_{f}]\\right)\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "202 where $\\overline{{\\mathbf{m}}}_{f}$ is an $m$ -dimensional one-hot vector that has a value 1 in the place of feature $f$ and 0\u2019s   \n203 elsewhere, $[\\cdot\\,;\\cdot]$ denotes the vector concatenation operation, and $\\sigma_{4}(\\cdot)$ is an MLP with the GELU   \n204 activation function. Note that the rationale behind the design of $\\mathbf{r}_{p}^{f}$ is to embed the information on   \n205 the features whose values are present in $p$ as well as the information on the target feature $f$ to impute.   \n206 The mask $\\mathbf{r}_{p}^{f}$ is then applied to $\\mathbf{h}_{p}$ to obtain the masked embedding vector of $p$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi_{p}(\\mathbf{h}_{p},\\mathbf{r}_{p}^{f})=\\sigma_{5}\\left(\\mathbf{h}_{p}\\odot\\mathbf{r}_{p}^{f}\\right)\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "207 where $\\sigma_{5}(\\cdot)$ is also an MLP with the GELU activation function. Once we have the masked embed  \n208 ding vectors of samples (excluding $s$ ) in $\\mathcal{P}$ , we finally compute the sample context vector of sample   \n209 $s$ , denoted by $\\mathbf{z}_{s}^{f}$ , which is a weighted sum of the masked embedding vectors with weights being the   \n210 pairwise similarity values, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{z}_{s}^{f}=\\sigma_{6}\\left(\\sum_{p\\in\\mathcal{P}\\backslash\\{s\\}}\\sin(s,p\\mid f)\\;\\phi_{p}(\\mathbf{h}_{p},\\mathbf{r}_{p}^{f})\\right)\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "211 where $\\sigma_{6}(\\cdot)$ is again an MLP with the GELU activation function. From (6)\u2013(9), the operations of   \n212 SCU can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{z}_{s}^{f}=\\mathbf{SCU}(\\mathbf{H}_{\\mathcal{P}},\\mathbf{M}_{\\mathcal{P}},\\mathbf{H}_{F})=\\sigma_{6}\\left(\\sum_{p\\in\\mathcal{P}\\setminus\\{s\\}}\\sin(s,p\\mid f)\\;\\sigma_{5}\\left(\\mathbf{h}_{p}\\odot\\sigma_{4}\\left([\\mathbf{m}_{p};\\mathbf{\\overline{{m}}}_{f}]\\right)\\right)\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "213 where ${\\bf H}_{\\mathcal{P}}=\\{{\\bf h}_{p},p\\in\\mathcal{P}\\}$ and $\\mathbf{M}_{\\mathcal{P}}=\\{\\mathbf{m}_{p},p\\in\\mathcal{P}\\}$ . ", "page_idx": 4}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/cd0fb647cf708c46e86432c84e56d950f19eb027e881d14ea6955df2f49dd8f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "214 3.5 Imputation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "215 For a given sample $s$ , to impute the missing value of feature $f,{\\bf M}^{3}$ -Impute obtains its feature context   \n216 vector $\\mathbf{c}_{s}^{f}$ and sample context vector $\\mathbf{z}_{s}^{f}$ through FCU and SCU, respectively, which are then used   \n217 for imputation. Specifically, it is done by predicting the corresponding edge weight $\\hat{e}_{s f}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{e}_{s f}=\\phi_{\\alpha}\\left((1-\\alpha)\\mathbf{c}_{s}^{f}+\\alpha\\mathbf{z}_{s}^{f}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "218 where $\\phi_{\\alpha}(\\cdot)$ denotes an MLP with a non-linear activation function (i.e., ReLU for continuous values   \n219 and softmax for discrete ones), and $\\alpha$ is a learnable scalar parameter. This scalar parameter $\\alpha$ is   \n220 introduced to strike a balance between leveraging feature-wise correlation and sample-wise correla  \n221 tion. It is necessary because the quality of $\\mathbf{z}_{s}^{\\widecheck{f}}$ relies on the quality of the samples chosen in $\\mathcal{P}$ , so   \n222 overly relying on $\\mathbf{z}_{s}^{f}$ would backfire if their quality is not as desired. To address this problem, instead   \n223 of employing a fixed weight $\\alpha$ , we make $\\alpha$ learnable and adaptive in determining the weights for   \n224 $\\mathbf{c}_{s}^{f}$ and $\\mathbf{z}_{s}^{\\tilde{f}}$ . Note that this kind of learnable parameter approach has been widely adopted in natural   \n225 language processing [26, 34, 38, 46] and computer vision [8, 55, 56], showing superior performance   \n226 to its fixed counterpart. In $\\mathbf{M}^{3}$ -Impute, the scalar parameter $\\alpha$ is learned based on the similarity   \n227 values between $s$ and its peer samples $p\\in\\mathcal{P}\\setminus\\{s\\}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha=\\phi_{\\gamma}\\Big(\\prod_{p\\in\\mathcal{P}\\backslash\\{s\\}}\\sin\\left(s,p\\mid f\\right)\\Big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "228 where $\\parallel$ represents the concatenation operation, and $\\phi_{\\gamma}(\\cdot)$ is an MLP with the activation function   \n229 $\\gamma(x)=1-1\\,/\\,e^{|x|}$ . The overall operation of $\\mathbf{M}^{3}$ -Impute is summarized in Algorithm 1. To learn   \n230 network parameters, we use cross-entropy loss and mean square error loss for imputing discrete and   \n231 continuous feature values, respectively. ", "page_idx": 5}, {"type": "text", "text": "232 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "233 4.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "234 Datasets: We conduct experiments on 15 open datasets. These real-world datasets consist of mixed   \n235 data types with both continuous and discrete values and cover different domains including civil   \n236 engineering (CONCRETE, ENERGY), physics and chemistry (YACHT), thermal dynamics (NAVAL),   \n237 etc. Since the datasets are fully observed, we introduce missing values by applying a randomly   \n238 generated mask to the data matrix. Specifically, as used in prior studies [23, 24], we apply three   \n239 masking generation schemes, namely missing completely at random (MCAR), missing at random   \n240 (MAR), and missing not at random (MNAR).1We use MCAR with a missing ratio of $\\bar{3}0\\%$ , unless   \n241 otherwise specified. We follow the preprocessing steps adopted in [52, 54] to scale feature values   \n242 to [0, 1] with a MinMax scaler [25]. Due to the space limit, we below present the results of eight   \n243 datasets that are used in Grape [52] and report the other results in Appendix.   \n244 Baseline models: $\\mathbf{M}^{3}$ -Impute is compared against popular and state-of-the-art imputation methods,   \n245 including statistical methods, deep generative methods, and graph-based methods listed as follows:   \n246 MEAN: It imputes the missing value $\\hat{e}_{s f}$ as the mean of observed values in feature $f$ from all   \n247 the samples. K-nearest neighbors (kNN) [43]: It imputes the missing value $\\hat{e}_{s f}$ using the kNNs   \n248 that have observed values in feature $f$ with weights that are based on the Euclidean distance to   \n249 sample $s$ . Multivariate imputation by chained equations (Mice) [45]: This method runs multiple   \n250 regressions where each missing value is modeled upon the observed non-missing values. Iterative   \n51 SVD (Svd) [18]: It imputes missing values by solving a matrix completion problem with iterative   \n52 low-rank singular value decomposition. Spectral regularization algorithm (Spectral) [30]: This   \n53 matrix completion algorithm uses the nuclear norm as a regularizer and imputes missing values with   \n54 iterative soft-thresholded SVD. Miwae [29]: It works based on an autoencoder generative model   \n55 trained to maximize a potentially tight lower bound of the log-likelihood of the observed data and   \n56 Monte Carlo techniques for imputation. Miracle [24]: It uses the imputation results from naive   \n57 methods such as MEAN and refines them iteratively by learning a missingness graph (m-graph) and   \n58 regularizing an imputation function. Gain [50]: This method trains a data imputation generator with   \n59 a generalized generative adversarial network in which the discriminator aims to distinguish between   \n60 real and imputed values. Grape [52]: It models the data as a bipartite graph and imputes missing   \n6 values by predicting the weights of the missing edges, each of which is done based on the inner   \n62 product between the embeddings of its corresponding sample and feature nodes. HyperImpute [23]:   \n63 HyperImpute is a framework that conducts an extensive search among a set of imputation methods,   \n64 selecting the optimal imputation method with fine-tuned parameters for each feature in the dataset.   \n265 Model configurations: Parameters of $\\mathbf{M}^{3}$ -Impute are updated by the Adam optimizer with a learn  \n266 ing rate of 0.001 for 40,000 epochs. For graph representation learning, we use a variant of Graph  \n267 SAGE [17], which not only learns node embeddings but also edge embeddings via the neighborhood   \n268 aggregation mechanism, as similarly used in [52]. We consider its three-layer GNN model. We em  \n269 ploy mean-pooling as the aggregation function and use ReLU as the activation function for the GNN   \n270 layers. We set the embedding dimension $d$ to 128. It is known that randomly dropping out a subset   \n271 of observable edges during training improves the model\u2019s generalization ability. We also leverage   \n272 the observation and randomly drop $50\\%$ of observable edges during training. For each experiment,   \n273 we conduct five runs with different random seeds and report the average results. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/e56440b6a2210b5c249fb1c0c40c9eb6693ef4f8e892d93a9317a2c03fa1e400.jpg", "table_caption": ["Table 1: Imputation accuracy in MAE. MAE scores are enlarged by 10 times. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "274 4.2 Overall Performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "275 We first compare the feature imputation performance of $\\mathbf{M}^{3}$ -Impute with popular and state-of-the  \n276 art imputation methods. As shown in Table 1, $\\mathbf{M}^{3}$ -Impute achieves the lowest imputation MAE   \n277 for six out of the eight examined datasets and the second-best MAE scores in the other two, which   \n278 validates the effectiveness of $\\mathbf{M}^{3}$ -Impute. For KIN8NM dataset, $\\mathbf{M}^{3}$ -Impute underperforms Miracle.   \n279 It is mainly because each feature in $\\mathrm{KIN8NM}$ is independent of the others, so none of the observed   \n280 features can help impute missing feature values. For NAVAL dataset, the only model that outperforms   \n28 $\\mathbf{M}^{3}$ -Impute is HyperImpute [23]. In the NAVAL dataset, nearly every feature exhibits a strong linear   \n282 correlation with the other features, i.e., every pair of features has correlation coefficient close to   \n283 one. This allows HyperImpute to readily select a linear model from its model pool for each feature   \n284 to impute. Nonetheless, $\\bar{\\mathbf{M}}^{3}$ -Impute exhibits overall superior performance to the baselines as it   \n285 can be well adapted to each dataset that possesses different amounts of correlations over features   \n286 and samples. In other words, $\\mathbf{M}^{3}$ -Impute benefits from explicitly incorporating feature-wise and   \n287 sample-wise correlations together with our carefully designed mask schemes. Furthermore, we   \n288 evaluate the performance of $\\mathbf{M}^{3}$ -Impute under MAR and MNAR settings. We observe that $\\mathbf{M}^{3}$ -   \n289 Impute consistently outperforms all the baselines under all datasets and achieves a larger margin in   \n290 the improvement compared to the case with MCAR setting. This implies that $\\mathbf{M}^{3}$ -Impute is also   \n291 effective in handling different patterns of missing values in the input data. Comprehensive results   \n292 are provided in Appendix. ", "page_idx": 6}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/9bbe03e848a5933c1ae68766976de39c2480d5b1188b648490e7d47af3805dc8.jpg", "table_caption": ["Table 2: Ablation study. $\\mathbf{M}^{3}$ -Uniform stands for $\\mathbf{M}^{3}$ -Impute with the uniform sampling strategy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "293 4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "294 To study the effectiveness of three integral components of $\\mathbf{M}^{3}$ -Impute, we consider three variants of   \n295 $\\mathbf{M}^{3}$ -Impute, each with a subset of the components, namely initialization only (Init Only), initializa  \n296 $\\mathrm{tion}+\\mathbf{FCU}$ $(\\mathrm{Init}+\\mathbf{FCU})$ ), and initialization $\\mathbf{+\\DeltaSCU}$ $\\operatorname{Init}+\\mathbf{SCU})$ . The performance of these variants   \n297 are evaluated against the top-performing imputation baselines such as Grape and HyperImpute. As   \n298 shown in Table 2, the three variants derived from $\\mathbf{M}^{3}$ -Impute achieve lower MAE values than both   \n299 baselines in most datasets, demonstrating the effectiveness of our novel components in $\\mathbf{M}^{3}$ -Impute.   \n300 Specifically, for initialization only, the key difference between $\\mathbf{M}^{3}$ -Impute and Grape lies in our   \n301 refined initialization process of feature-node and sample-node embeddings. The reduced MAE val  \n302 ues observed by the Init Only variant demonstrate that our proposed initialization process is more   \n303 effective in utilizing information between samples and their associated features, including missing   \n304 ones, as compared to the basic initialization used in [52]. In addition, we observe that when FCU or   \n305 SCU is incorporated, MAE values are further reduced for most datasets. This validates that explicitly   \n306 modeling feature-wise or sample-wise correlations through our novel masking schemes can improve   \n307 imputation accuracy. When all the three components are combined together as in $\\mathbf{M}^{3}$ -Impute, they   \n308 work synergistically to lower MAE values, validating the efficacy of explicit consideration of both   \n309 sample-wise and feature-wise correlations (in addition to the refined initialization process) for miss  \n310 ing data imputation. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "311 4.4 Robustness ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "312 Missing ratio: In practice, datasets may possess different missing ratios. To validate the model\u2019s   \n313 robustness under such circumstances, we evaluate the performance of $\\mathbf{M}^{3}$ -Impute and other baseline   \n314 models with varying missing ratios, i.e., 0.1, 0.3, 0.5, and 0.7. Figure 3 shows their performance. We   \n315 use the MAE of HyperImpute $(H I)$ as the reference performance and offset the performance of each   \n316 model by $\\boldsymbol{\\mathrm{MAE}_{x}}\\mathrm{~-~}\\boldsymbol{\\mathrm{MAE}_{H I}}$ , where $x$ represents the considered model. For clarity, we here only   \n317 report the results of four top-performing models. As shown in Figure 3, $\\mathbf{M}^{3}$ -Impute outperforms   \n318 other baseline models for almost all the cases, especially under YACHT, CONCRETE, ENERGY,   \n319 and HOUSING datasets. It is worth noting that modeling feature correlations in these datasets is   \n320 particularly challenging due to the presence of considerable amounts of weakly correlated features,   \n32 along with a few strongly correlated ones. Nonetheless, FCU and SCU in $\\dot{\\mathbf{M}}^{3}$ -Impute were able   \n322 to better capture such correlations with our efficient masking schemes, thereby resulting in a large   \n323 improvement in imputation accuracy. In addition, for KIN8NM dataset, $\\mathbf{M}^{3}$ -Impute ties with the   \n324 second-best model, Grape. As mentioned in Section 4.2, each feature in KIN8NM is independent   \n325 of the others, so none of the observed features can help impute missing feature values. For NAVAL   \n326 dataset, where each feature strongly correlates with the others, $\\mathbf{M}^{3}$ -Impute surpasses Grape but falls   \n327 short of HyperImpute, due to the same reason as discussed above. Overall, $\\Bar{\\mathbf{M}}^{3}$ -Impute is robust to   \n328 various missing ratios. Comprehensive results for all the baseline models can be found in Appendix.   \n329 Sampling strategy in SCU: While SCU uses a sampling strategy based on pairwise cosine similari  \n330 ties to construct a subset of samples $\\mathcal{P}$ , the simplest sampling strategy to build $\\mathcal{P}$ would be to choose   \n331 samples uniformly at random without replacement ( $\\mathbf{M}^{3}$ -Uniform). Intuitively, this approach cannot   \n332 identify similar peer samples accurately and thus would lead to inferior performance. Nonetheless,   \n333 as shown in Table 2, even with this naive uniform sampling strategy, $\\mathbf{M}^{3}$ -Uniform still outperforms   \n334 the two leading imputation baselines.   \n335 Size of $\\mathcal{P}$ in SCU: Intuitively, neither an excessively small nor overly large size of the sample subset   \n336 $\\mathcal{P}$ is optimal. Too few peer samples leave SCU with insufficient information to learn sample-wise   \n337 correlations, while too many peer samples may include quite a few dissimilar ones, which may   \n338 introduce significant noise to the computation of SCU and thus degrade the performance. Table 3   \n339 shows the performance of $\\mathbf{M}^{3}$ -Impute with varying numbers of peer samples. In general, the trends   \n340 agree with our intuition. Although the optimal size varies across different datasets, we observe that   \n341 having the number of peer samples to be 5 to 10 achieves the overall best imputation accuracy.   \n342 Initialization parameter $\\epsilon;$ We also evaluate whether a non-zero value of $\\epsilon$ in the initialization   \n343 process of $\\mathbf{M}^{3}$ -Impute indeed lead to an improvement in imputation accuracy. As shown in Table 3,   \n344 for YACHT and WINE datasets, the introduction of a non-zero value of $\\epsilon$ results in lower MAE scores.   \n345 Another insight that we have from Table 3 is that $\\epsilon$ should not be set too large, as a large value of $\\epsilon$   \n346 might impose incorrect weights to the features with missing values. We observe that it is an overall   \n347 good choice to set $\\epsilon$ to $1\\!\\times\\!1\\overline{{0}}^{-5}$ or $1\\!\\times\\!10^{-4}$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "1MQXBnEbE8/tmp/ee5962b1debbffd0c131750f495e0abe3500f2acd160f2b5c381da7e9238f44a.jpg", "img_caption": ["Figure 3: Model performance vs. missing ratios. MAE scores are offset by HyperImpute [23]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/0c04b3a8532670552c87a3b2c29d332284e7c9d263faf9dc519390fdb263fac0.jpg", "table_caption": ["Table 3: MAE scores for varying peer-sample size $(|\\mathcal{P}|\\!-\\!1)$ and different values of $\\epsilon$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "348 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "349 We have presented $\\mathbf{M}^{3}$ -Impute, a mask-guided representation learning for missing data imputation.   \n350 $\\mathbf{M}^{3}$ -Impute improved the initialization process by considering the relationships between samples and   \n351 their associated features (including missing ones) even in initializing the embeddings. In addition,   \n352 for more effective representation learning, we introduced two novel components in $\\mathbf{M}^{3}$ -Impute \u2013   \n353 FCU and SCU, which learn feature-wise and sample-wise correlations, respectively, to capture data   \n354 correlations explicitly and leverage them for imputation. Extensive experiment results demonstrate   \n355 the effectiveness of $\\mathrm{i}\\mathrm{q}^{3}$ -Impute. $\\mathbf{\\breve{M}}^{3}$ -Impute achieves overall superior performance to popular and   \n356 state-of-the-art methods on 15 open datasets, with 13 best and two second-best MAE scores on   \n357 average under three different settings of missing value patterns. ", "page_idx": 8}, {"type": "text", "text": "358 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "359 [1] Melissa J Azur, Elizabeth A Stuart, Constantine Frangakis, and Philip J Leaf. Multiple imputa  \n360 tion by chained equations: what is it and how does it work? International journal of methods   \n361 in psychiatric research, 20(1):40\u201349, 2011.   \n362 [2] Jaap Brand. Development, implementation and evaluation of multiple imputation strategies for   \n363 the statistical analysis of incomplete data sets. 1999.   \n364 [3] Thomas Brooks, D. Pope, and Michael Marcolini. Airfoil self-noise. https://doi.org/10.   \n365 24432/C5VW2C, March 2014.   \n366 [4] Lane F Burgette and Jerome P Reiter. Multiple imputation for missing data via sequential   \n367 regression trees. American journal of epidemiology, 172(9):1070\u20131076, 2010.   \n368 [5] Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm   \n369 for matrix completion. SIAM Journal on optimization, 20(4):1956\u20131982, 2010.   \n370 [6] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization.   \n371 Communications of the ACM, 55(6):111\u2013119, 2012.   \n372 [7] Paulo Cortez, Antonio Cerdeira, Fernando Almeida, Telmo Matos, and Jos\u00e9 Reis. Wine quality.   \n373 https://doi.org/10.24432/C56S3T, October 2009.   \n374 [8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. De  \n375 formable convolutional networks. In 2017 IEEE International Conference on Computer Vision   \n376 (ICCV), pages 764\u2013773, 2017.   \n377 [9] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete   \n378 data via the em algorithm. Journal of the royal statistical society: series B (methodological),   \n379 39(1):1\u201322, 1977.   \n380 [10] Tianyu Du, Luca Melis, and Ting Wang. Remasker: Imputing tabular data with masked au  \n381 toencoding. In The Twelfth International Conference on Learning Representations, 2024.   \n382 [11] Dheeru Dua and Casey Graff. Uci machine learning repository, 2017.   \n383 [12] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression.   \n384 Ann. Statist., 32(1):407\u2013499, 2004.   \n385 [13] Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugen  \n386 ics, 7(2):179\u2013188, 1936.   \n387 [14] Pedro J Garc\u00eda-Laencina, Jos\u00e9-Luis Sancho-G\u00f3mez, and An\u00edbal R Figueiras-Vidal. Pattern   \n388 classification with missing data: a review. Neural Computing and Applications, 19:263\u2013282,   \n389 2010.   \n390 [15] Andrew Gelman. Parameterization and bayesian modeling. Journal of the American Statistical   \n391 Association, 99(466):537\u2013545, 2004.   \n392 [16] Zoubin Ghahramani and Michael Jordan. Supervised learning from incomplete data via an em   \n393 approach. Advances in neural information processing systems, 6, 1993.   \n394 [17] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on   \n395 large graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob   \n396 Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information   \n397 Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,   \n398 December 4-9, 2017, Long Beach, CA, USA, pages 1024\u20131034, 2017.   \n399 [18] Trevor Hastie, Rahul Mazumder, Jason D. Lee, and Reza Zadeh. Matrix completion and low  \n400 rank svd via fast alternating least squares. J. Mach. Learn. Res., 16(1):33673402, jan 2015.   \n401 [19] Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and   \n402 low-rank svd via fast alternating least squares. The Journal of Machine Learning Research,   \n403 16(1):3367\u20133402, 2015.   \n404 [20] David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, and   \n405 Carl Kadie. Dependency networks for inference, collaborative filtering, and data visualization.   \n406 Journal of Machine Learning Research, 1(Oct):49\u201375, 2000.   \n407 [21] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with   \n408 gaussian error linear units. CoRR, abs/1606.08415, 2016.   \n409 [22] James Honaker, Gary King, and Matthew Blackwell. Amelia ii: A program for missing data.   \n410 Journal of statistical software, 45:1\u201347, 2011.   \n411 [23] Daniel Jarrett, Bogdan Cebere, Tennison Liu, Alicia Curth, and Mihaela van der Schaar. Hy  \n412 perimpute: Generalized iterative imputation with automatic model selection. In Kamalika   \n413 Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, edi  \n414 tors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,   \n415 Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9916\u20139937.   \n416 PMLR, 2022.   \n417 [24] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. MIRACLE: causally  \n418 aware imputation via learning missing data mechanisms. In Marc\u2019Aurelio Ranzato, Alina   \n419 Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Ad  \n420 vances in Neural Information Processing Systems 34: Annual Conference on Neural Informa  \n421 tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 23806\u2013   \n422 23817, 2021.   \n423 [25] Jure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman. Mining of Massive Datasets, 2nd   \n424 Ed. Cambridge University Press, 2014.   \n425 [26] Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, and Rui Yan.   \n426 VMSMO: Learning to generate multimodal summary for video-based news articles. In Bonnie   \n427 Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference   \n428 on Empirical Methods in Natural Language Processing (EMNLP), pages 9360\u20139369, Online,   \n429 November 2020. Association for Computational Linguistics.   \n430 [27] Steven Cheng-Xian Li, Bo Jiang, and Benjamin M. Marlin. Misgan: Learning from incom  \n431 plete data with generative adversarial networks. In 7th International Conference on Learning   \n432 Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n433 [28] Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793.   \n434 John Wiley & Sons, 2019.   \n435 [29] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: deep generative modelling and imputation   \n436 of incomplete data sets. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceed  \n437 ings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,   \n438 Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages   \n439 4413\u20134423. PMLR, 2019.   \n440 [30] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for   \n441 learning large incomplete matrices. J. Mach. Learn. Res., 11:22872322, aug 2010.   \n442 [31] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms   \n443 for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287\u2013   \n444 2322, 2010.   \n445 [32] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms   \n446 for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287\u2013   \n447 2322, 2010.   \n448 [33] Boris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi. Missing data imputation using   \n449 optimal transport. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th Inter  \n450 national Conference on Machine Learning, volume 119 of Proceedings of Machine Learning   \n451 Research, pages 7130\u20137140. PMLR, 13\u201318 Jul 2020.   \n452 [34] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive   \n453 summarization. In International Conference on Learning Representations, 2018.   \n454 [35] Trivellore E Raghunathan, James M Lepkowski, John Van Hoewyk, Peter Solenberger, et al.   \n455 A multivariate technique for multiply imputing missing values using a sequence of regression   \n456 models. Survey methodology, 27(1):85\u201396, 2001.   \n457 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High  \n458 resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Com  \n459 puter Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022,   \n460 pages 10674\u201310685. IEEE, 2022.   \n461 [37] Joseph L Schafer. Analysis of incomplete multivariate data. CRC press, 1997.   \n462 [38] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with   \n463 pointer-generator networks. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of   \n464 the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long   \n465 Papers), pages 1073\u20131083, Vancouver, Canada, July 2017. Association for Computational Lin  \n466 guistics.   \n467 [39] V. Sigillito, S. Wing, L. Hutton, and K. Baker. Ionosphere. https://doi.org/10.24432/   \n468 C5W01B, December 1988.   \n469 [40] Indro Spinelli, Simone Scardapane, and Aurelio Uncini. Missing data imputation with   \n470 adversarially-trained graph convolutional networks. Neural Networks, 129:249\u2013260, 2020.   \n471 [41] Daniel J Stekhoven and Peter B\u00fchlmann. Missforestnon-parametric missing value imputation   \n472 for mixed-type data. Bioinformatics, 28(1):112\u2013118, 2012.   \n473 [42] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. CSDI: conditional score  \n474 based diffusion models for probabilistic time series imputation. In Marc\u2019Aurelio Ranzato,   \n475 Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors,   \n476 Advances in Neural Information Processing Systems 34: Annual Conference on Neural Infor  \n477 mation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 24804\u2013   \n478 24816, 2021.   \n479 [43] Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshi  \n480 rani, David Botstein, and Russ B. Altman. Missing value estimation methods for dna microar  \n481 rays. Bioinformatics, 17(6):520\u2013525, 2001.   \n482 [44] Stef Van Buuren, Jaap PL Brand, Catharina GM Groothuis-Oudshoorn, and Donald B Rubin.   \n483 Fully conditional specification in multivariate imputation. Journal of statistical computation   \n484 and simulation, 76(12):1049\u20131064, 2006.   \n485 [45] Stef van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained   \n486 equations in r. Journal of Statistical Software, 45(3):167, 2011.   \n487 [46] Wenbo Wang, Yang Gao, Heyan Huang, and Yuxiang Zhou. Concept pointer network for   \n488 abstractive summarization. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, ed  \n489 itors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro  \n490 cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP  \n491 IJCNLP), pages 3076\u20133085, Hong Kong, China, November 2019. Association for Computa  \n492 tional Linguistics.   \n493 [47] William H. Wolberg, Olvi L. Mangasarian, and W. Nick Street. Breast cancer wisconsin (diag  \n494 nostic). https://doi.org/10.24432/C5DW2B, October 1995.   \n495 [48] Richard Wu, Aoqian Zhang, Ihab Ilyas, and Theodoros Rekatsinas. Attention-based learning   \n496 for missing data imputation in holoclean. Proceedings of Machine Learning and Systems,   \n497 2:307\u2013325, 2020.   \n498 [49] I-Cheng Yeh. Blood transfusion service center. https://doi.org/10.24432/C5GS39, Oc  \n499 tober 2008.   \n500 [50] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: missing data imputation   \n501 using generative adversarial nets. In Jennifer G. Dy and Andreas Krause, editors, Proceedings   \n502 of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,   \n503 Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Re  \n504 search, pages 5675\u20135684. PMLR, 2018.   \n505 [51] Seongwook Yoon and Sanghoon Sull. GAMIN: generative adversarial multiple imputation net  \n506 work for highly missing data. In 2020 IEEE/CVF Conference on Computer Vision and Pattern   \n507 Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8453\u20138461. Computer   \n508 Vision Foundation / IEEE, 2020.   \n509 [52] Jiaxuan You, Xiaobai Ma, Daisy Yi Ding, Mykel J. Kochenderfer, and Jure Leskovec. Handling   \n510 missing data with graph representation learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato,   \n511 Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Infor  \n512 mation Processing Systems 33: Annual Conference on Neural Information Processing Systems   \n513 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n514 [53] Shuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation   \n515 in tabular data. CoRR, abs/2210.17128, 2022.   \n516 [54] Jiajun Zhong, Ning Gui, and Weiwei Ye. Data imputation with iterative graph reconstruction.   \n517 In Brian Williams, Yiling Chen, and Jennifer Neville, editors, Thirty-Seventh AAAI Conference   \n518 on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of   \n519 Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial   \n520 Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 11399\u201311407.   \n521 AAAI Press, 2023.   \n522 [55] Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, and Jifeng Dai. An empirical study of   \n523 spatial attention mechanisms in deep networks. In Proceedings of the IEEE/CVF International   \n524 Conference on Computer Vision (ICCV), October 2019.   \n525 [56] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable,   \n526 better results. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition   \n527 (CVPR), pages 9300\u20139308, 2019.   \n529 In this section, we discuss further experimental details. We first give an overview of the dataset   \n530 details in Section A.1, followed by the implementation of different missing types and present corre  \n531 sponding imputation performance under MAR and MNAR settings (Section A.2). We then provide   \n532 the comprehensive results of the robustness experiments (Section A.3). Finally, we extend our eval  \n533 uation of $\\mathbf{M}^{3}$ -Impute to seven additional datasets (Section A.4) and elaborate on the computational   \n534 resources in Section A.5. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/5f68189b5cef94ddd2c857f370fa43f162a98a59fd1ce50aaa65478660293687.jpg", "table_caption": ["Table 4: Overview of Datasets. "], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "1MQXBnEbE8/tmp/f18ea25e56fbd5e9d35dde85b9fd2a7c0bc88368f1be56dffc76a860ce49b257.jpg", "img_caption": ["Figure 4: Pearson correlation coefficients of UCI datasets. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "535 A.1 Dataset Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "536 Table 4 presents the statistics of the eight UCI datasets [11] used throughout Section 4. Figure 4 il  \n537 lustrates the Pearson correlation coefficients among the features. In the $\\mathrm{Kin}8\\mathrm{nm}$ dataset, all features   \n538 are linearly independent, whereas the Naval dataset exhibits strong correlations among its features.   \n539 Under the MCAR setting, $\\mathbf{M}^{3}$ -Impute performs comparably to the baseline imputation methods on   \n540 these two datasets (shown in Table 1). However, in real-world scenarios, features are not always   \n541 entirely independent or strongly correlated. In the other six datasets, we observe a mix of weakly   \n542 correlated features along with a few that are strongly correlated. In these cases, $\\mathbf{M}^{3}$ -Impute consis  \n543 tently outperforms all baseline methods. ", "page_idx": 13}, {"type": "text", "text": "544 A.2 Detailed Results of Different Missing Types ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "545 We adopt the same procedure outlined in [52, 54] to generate missing values under different settings. ", "page_idx": 13}, {"type": "text", "text": "546 \u2022 MCAR: A $n\\times m$ matrix is sampled from a uniform distribution. Positions with values no greater   \n547 than the ratio of missingness are viewed as missing and the remaining positions are observable.   \n548 \u2022 MAR: First, a subset of features is randomly selected to be fully observed. Then, these remaining   \n549 features have values removed according to a logistic model with random weights, using the fully   \n550 observed feature values as input. The desired rate of missingness is achieved by adjusting the bias   \n551 term.   \n552 \u2022 MNAR: This is done by first apply the MAR mechanism above. Then, the remaining feature   \n553 values are masked out by the MCAR mechanism. ", "page_idx": 13}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/050b22f8e52b08cd644e5298dd446e9bbd4ab9b4650a462c3792c81598ef6f43.jpg", "table_caption": ["Table 5: MAE scores under MAR setting. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/aeaae22da7bd5778c97999ca87f412c024d18a387ce78500885cd1d31a5914b5.jpg", "table_caption": ["Table 6: MAE scores under MNAR setting. "], "table_footnote": ["554 In addition to the results for MCAR setting presented in Table 4.2, Table 5 and Table 6 present the 555 MAE scores under MAR and MNAR settings, respectively. $\\mathbf{M}^{3}$ -Impute consistently outperforms all 556 baseline methods in both scenarios. "], "page_idx": 14}, {"type": "text", "text": "557 A.3 Robustness against Various Ratios of Missingness ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "558 Table 8 presents the performance of various imputation methods across different ratios of missing  \n559 ness. $\\mathbf{M}^{\\dot{3}}$ -Impute achieves the lowest MAE scores in most cases and the second-best MAE scores in   \n560 the remaining ones. ", "page_idx": 14}, {"type": "text", "text": "561 A.4 Further Evaluation on Seven Additional Datasets ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/e260820f092cae5a88a553d71d1c8d3fe3034ce65bdb23d9fc643afefc472cac.jpg", "table_caption": ["Table 7: Overview of seven additional datasets. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "562 In this experiment, we further evaluate $\\mathbf{M}^{3}$ -Impute on seven datasets: Airfoil [3], Blood [49], Wine  \n563 White [7], Ionosphere [39], Breast Cancer [47], Iris [13], and Diabetes [12]. An overview of dataset   \n564 details is provided in Table 7, and feature correlations are illustrated in Figure 5. We simulate   \n565 missingness in data under MCAR, MAR, and MNAR conditions, each with a missing ratio of 0.3.   \n566 Results are demonstrated in Table 9. Across all three types of missingness, $\\mathbf{M}^{3}$ -Impute achieves five   \n567 best and two second-best MAE scores on average. ", "page_idx": 14}, {"type": "image", "img_path": "1MQXBnEbE8/tmp/0a60cddbd08c6e3fdb70d4235261a431a02196d42e8a9b65bb709cd8eaaa6c51.jpg", "img_caption": ["Figure 5: Pearson correlation coefficient of 7 extra datasets. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "568 A.5 Computational Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "569 All our experiments are conducted on a GPU server running Ubuntu 22.04, with PyTorch 2.1.0   \n570 and CUDA 12.1. We train and test $\\mathbf{M}^{3}$ -Impute using a single NVIDIA A100 80G GPU. With the   \n571 experimental setup described in Section 4.1, the total runtime (including both training and testing)   \n572 for each of the five repeated runs ranged from 1 to 5 hours, depending on the scale of the datasets. ", "page_idx": 15}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/600fc717dc130a62ef62cfd3edcd92cc0f3206e1b66fad5e9563cd801dba94dd.jpg", "table_caption": ["Table 8: MAE scores across different levels of missingness. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "1MQXBnEbE8/tmp/7ec0482734a4a4e71b2e4909563b66786b488a202aa13d6b1cfbe7265d146905.jpg", "table_caption": ["Table 9: MAE scores on seven additional datasets "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "573 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "78 Justification: In the abstract and introduction sections, we clearly define the scope of this   \n79 paper, focusing on missing value imputation. We propose $\\mathbf{M}^{3}$ -Impute, a mask-guided im  \n80 putation method designed to compute feature-wise and sample-wise correlations based on   \n81 missing data patterns. A concise summary of the experimental results is provided at the   \n82 end of both sections. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In Section 4.2, we discussed two cases of MAE degradation for the KIN8NM and NAVAL datasets. It is mainly because 1. Each feature in KIN8NM is independent of the others, so none of the observed features can help impute missing feature values. 2. In the NAVAL dataset, nearly every feature exhibits a strong linear correlation with the other features. While it is true that $\\dot{\\mathbf{M}}^{3}$ -Impute does not achieve the best MAE on these two datasets, our model has outperformed all the other baselines on the majority of datasets. This demonstrates the unique strengths of graph modeling in $\\mathbf{M}^{3}$ -Impute over tabular data modeling in baselines like Hyperimpute. In real-world scenarios, the correlation structure of datasets is often unpredictable, and such extreme cases are relatively rare. Thus, we design a scheme to handle general cases for data imputation tasks. The empirical evidence suggests that our approach has been quite successful and exhibits overall superior performance to the baselines as it can be well adapted to each dataset that possesses different levels of correlations over features and samples. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. ", "page_idx": 18}, {"type": "text", "text": "626 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n627 and how they scale with dataset size.   \n628 \u2022 If applicable, the authors should discuss possible limitations of their approach to ad  \n629 dress problems of privacy and fairness.   \n630 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n631 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n632 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n633 judgment and recognize that individual actions in favor of transparency play an impor  \n634 tant role in developing norms that preserve the integrity of the community. Reviewers   \n635 will be specifically instructed to not penalize honesty concerning limitations.   \n636 3. Theory Assumptions and Proofs   \n637 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n638 a complete (and correct) proof?   \n639 Answer: [NA]   \n640 Justification: This paper does not present theoretical results. We do not assume that the   \n641 data is missing under MCAR, MAR, or MNAR conditions for $\\mathbf{M}^{3}$ -Impute to be effective.   \n642 Instead, $\\mathbf{M}^{3}$ -Impute demonstrates robust performance across all three settings.   \n643 Guidelines:   \n644 \u2022 The answer NA means that the paper does not include theoretical results.   \n645 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n646 referenced.   \n647 \u2022 All assumptions should be clearly stated or referenced in the statement of any theo  \n648 rems.   \n649 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n650 they appear in the supplemental material, the authors are encouraged to provide a   \n651 short proof sketch to provide intuition.   \n652 \u2022 Inversely, any informal proof provided in the core of the paper should be comple  \n653 mented by formal proofs provided in appendix or supplemental material.   \n654 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n655 4. Experimental Result Reproducibility   \n656 Question: Does the paper fully disclose all the information needed to reproduce the main   \n657 experimental results of the paper to the extent that it affects the main claims and/or conclu  \n658 sions of the paper (regardless of whether the code and data are provided or not)?   \n659 Answer: [Yes]   \n660 Justification: In Section 3, we explain the computational pipeline of the proposed model   \n661 in detail and provide a pseudo-code to better outline the methodology. The experimental   \n662 setup is comprehensively described in Section 4.1. In addition, supplementary materials   \n663 include our complete codebase to reproduce the results presented in this paper, including   \n664 the model implementation, training and testing pipeline, configuration files, and execution   \n665 scripts.   \n666 Guidelines:   \n667 \u2022 The answer NA means that the paper does not include experiments.   \n668 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n669 well by the reviewers: Making the paper reproducible is important, regardless of   \n670 whether the code and data are provided or not.   \n671 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps   \n672 taken to make their results reproducible or verifiable.   \n673 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n674 For example, if the contribution is a novel architecture, describing the architecture   \n675 fully might suffice, or if the contribution is a specific model and empirical evaluation,   \n676 it may be necessary to either make it possible for others to replicate the model with   \n677 the same dataset, or provide access to the model. In general. releasing code and data   \n678 is often one good way to accomplish this, but reproducibility can also be provided via   \n679 detailed instructions for how to replicate the results, access to a hosted model (e.g., in   \n680 the case of a large language model), releasing of a model checkpoint, or other means   \n681 that are appropriate to the research performed.   \n682 \u2022 While NeurIPS does not require releasing code, the conference does require all sub  \n683 missions to provide some reasonable avenue for reproducibility, which may depend   \n684 on the nature of the contribution. For example   \n685 (a) If the contribution is primarily a new algorithm, the paper should make it clear   \n686 how to reproduce that algorithm.   \n687 (b) If the contribution is primarily a new model architecture, the paper should describe   \n688 the architecture clearly and fully.   \n689 (c) If the contribution is a new model (e.g., a large language model), then there should   \n690 either be a way to access this model for reproducing the results or a way to re  \n691 produce the model (e.g., with an open-source dataset or instructions for how to   \n692 construct the dataset).   \n693 (d) We recognize that reproducibility may be tricky in some cases, in which case au  \n694 thors are welcome to describe the particular way they provide for reproducibility.   \n695 In the case of closed-source models, it may be that access to the model is limited in   \n696 some way (e.g., to registered users), but it should be possible for other researchers   \n697 to have some path to reproducing or verifying the results.   \n698 5. Open access to data and code   \n699 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n700 tions to faithfully reproduce the main experimental results, as described in supplemental   \n701 material?   \n702 Answer: [Yes]   \n703 Justification: In the supplementary material, we provide the complete code for our model,   \n704 including the scripts for experiments and evaluations, as well as the execution scripts used   \n705 in the experiments. We have also included the data preprocessed by us, along with the   \n706 download links for publicly available datasets. We will release the formatted codebase on   \n707 GitHub following the conclusion of the anonymity period.   \n708 Guidelines:   \n709 \u2022 The answer NA means that paper does not include experiments requiring code.   \n710 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n711 public/guides/CodeSubmissionPolicy) for more details.   \n712 \u2022 While we encourage the release of code and data, we understand that this might not   \n713 be possible, so No is an acceptable answer. Papers cannot be rejected simply for not   \n714 including code, unless this is central to the contribution (e.g., for a new open-source   \n715 benchmark).   \n716 \u2022 The instructions should contain the exact command and environment needed to run to   \n717 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n718 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n719 \u2022 The authors should provide instructions on data access and preparation, including how   \n720 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n721 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n722 proposed method and baselines. If only a subset of experiments are reproducible, they   \n723 should state which ones are omitted from the script and why.   \n724 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n725 versions (if applicable).   \n726 \u2022 Providing as much information as possible in supplemental material (appended to the   \n727 paper) is recommended, but including URLs to data and code is permitted.   \n728 6. Experimental Setting/Details   \n729 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n730 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "33 Justification: Experimental setup is detailed in Section 4.1 and Appendix A.2. We also   \n34 explore the hyperparameters utilized in $\\mathbf{M}^{3}$ -Impute. Results are presented in Table 3.   \n35 Guidelines:   \n36 \u2022 The answer NA means that the paper does not include experiments.   \n37 \u2022 The experimental setting should be presented in the core of the paper to a level of   \n38 detail that is necessary to appreciate the results and make sense of them.   \n39 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n40 material. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "741 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "742 Question: Does the paper report error bars suitably and correctly defined or other appropri  \n743 ate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We conduct all the experiments over five runs and report the mean MAE scores, along with the standard deviations. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We train and test $\\mathbf{M}^{3}$ -Impute on a single Nvidia A100 80G GPU (Detailed setup described in A.5). With the experimental setup described in Section 4.1, the total running time (including training and testing) for one of the five repeated runs varies from 1 to 5 hours, depending on the scale of the datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "786 9. Code Of Ethics   \n787 Question: Does the research conducted in the paper conform, in every respect, with the   \n788 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n789 Answer: [Yes]   \n790 Justification: Our paper adheres to the NeurIPS Code of Ethics in every respect. 1. We   \n791 ensured fair wages for all human participants involved in our study, abiding by regional   \n792 minimum hourly rates. 2. Our research methodology adhered to institutional protocols for   \n793 human subjects and data privacy. 3. We obtained informed consent from all participants   \n794 and minimized exposure of personally identifiable information. 4. The datasets used are   \n795 publicly available and have not been deprecated, with all copyrights respected. 5. We   \n796 have transparently communicated the societal impact of our research, considering potential   \n797 misuse and its effects on discrimination, surveillance, and environmental impact. 6. We   \n798 have also reflected on the biases in our models and datasets and taken steps to mitigate them.   \n799 7. Our data and models are documented and released with appropriate licenses, and we\u2019ve   \n800 employed secure data storage and distribution practices. 8. We ensured legal compliance   \n801 and provided all necessary elements for the reproducibility of our research.   \n802 Guidelines:   \n803 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n804 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n805 deviation from the Code of Ethics.   \n806 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n807 eration due to laws or regulations in their jurisdiction).   \n808 10. Broader Impacts   \n809 Question: Does the paper discuss both potential positive societal impacts and negative   \n810 societal impacts of the work performed?   \n811 Answer: [NA]   \n812 Justification: The method proposed in this work is only applicable for missing value impu  \n813 tation and is unlikely to have a negative social impact.   \n814 Guidelines:   \n815 \u2022 The answer NA means that there is no societal impact of the work performed.   \n816 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n817 impact or why the paper does not address societal impact.   \n818 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n819 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n820 (e.g., deployment of technologies that could make decisions that unfairly impact spe  \n821 cific groups), privacy considerations, and security considerations.   \n822 \u2022 The conference expects that many papers will be foundational research and not tied   \n823 to particular applications, let alone deployments. However, if there is a direct path to   \n824 any negative applications, the authors should point it out. For example, it is legitimate   \n825 to point out that an improvement in the quality of generative models could be used to   \n826 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n827 that a generic algorithm for optimizing neural networks could enable people to train   \n828 models that generate Deepfakes faster.   \n829 \u2022 The authors should consider possible harms that could arise when the technology is   \n830 being used as intended and functioning correctly, harms that could arise when the   \n831 technology is being used as intended but gives incorrect results, and harms following   \n832 from (intentional or unintentional) misuse of the technology.   \n833 \u2022 If there are negative societal impacts, the authors could also discuss possible mitiga  \n834 tion strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n835 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n836 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "837 11. Safeguards ", "page_idx": 22}, {"type": "text", "text": "838 Question: Does the paper describe safeguards that have been put in place for responsible   \n839 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n840 image generators, or scraped datasets)?   \n841 Answer: [NA]   \n842 Justification: The model we propose does not carry the risk of misuse; the datasets were   \n843 selected under fair use conditions, from publicly available sources with undisputed licenses.   \n844 Therefore, our work does not require additional safeguard protections.   \n845 Guidelines:   \n846 \u2022 The answer NA means that the paper poses no such risks.   \n847 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n848 necessary safeguards to allow for controlled use of the model, for example by re  \n849 quiring that users adhere to usage guidelines or restrictions to access the model or   \n850 implementing safety filters.   \n851 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n852 should describe how they avoided releasing unsafe images.   \n853 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n854 not require this, but we encourage authors to take this into account and make a best   \n855 faith effort.   \n856 12. Licenses for existing assets   \n857 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n858 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n859 properly respected?   \n860 Answer: [Yes]   \n861 Justification: In our research, we have carefully credited all the code and data used, provid  \n862 ing explicit citations for each. The licenses for this code and data are notably permissive,   \n863 including MIT, BSD 3-clause, and CC BY 4.0. In accordance with these licenses, we have   \n864 properly acknowledged the contributions of the original authors.   \n865 Guidelines:   \n866 \u2022 The answer NA means that the paper does not use existing assets.   \n867 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n868 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n869 URL.   \n870 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n871 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n872 service of that source should be provided.   \n873 \u2022 If assets are released, the license, copyright information, and terms of use in the pack  \n874 age should be provided. For popular datasets, paperswithcode.com/datasets has   \n875 curated licenses for some datasets. Their licensing guide can help determine the li  \n876 cense of a dataset.   \n877 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n878 the derived asset (if it has changed) should be provided.   \n879 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n880 the asset\u2019s creators.   \n881 13. New Assets   \n882 Question: Are new assets introduced in the paper well documented and is the documenta  \n883 tion provided alongside the assets?   \n884 Answer: [Yes]   \n885 Justification: We have detailed the new datasets employed in this research in Appendix A.4.   \n886 We commit to making these datasets publicly accessible following the anonymity period to   \n887 foster transparency and reproducibility.   \n888 Guidelines:   \n889 \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.   \n14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.   \n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]