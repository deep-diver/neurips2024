[{"type": "text", "text": "Unveiling the Tapestry of Consistency in Large Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuan Zhang1,2, Fei Xiao2, Tao Huang3, Chun-Kai $\\mathbf{Fan}^{1}$ , Hongyuan Dong2, Jiawen $\\mathbf{Li^{2}}$ , Jiacong Wang2,4, Kuan Cheng1, Shanghang Zhang1,\u2217 Haoyuan $\\mathbf{\\bar{Guo}^{2*}}$ ", "page_idx": 0}, {"type": "text", "text": "1 School of Computer Science, Peking University \u2020 2 ByteDance Inc 3 The University of Sydney 4 School of Artificial Intelligence, UCAS https://github.com/foundation-multimodal-models/ConBench ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large vision-language models (LVLMs) have recently achieved rapid progress, exhibiting great perception and reasoning abilities concerning visual information. However, when faced with prompts in different sizes of solution spaces, LVLMs fail to always give consistent answers regarding the same knowledge point. This inconsistency of answers between different solution spaces is prevalent in LVLMs and erodes trust. To this end, we provide a multi-modal benchmark ConBench, to intuitively analyze how LVLMs perform when the solution space of a prompt revolves around a knowledge point. Based on the ConBench tool, we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers. (2) Establish the relationship between the discriminative and generative realms: the accuracy of the discriminative question type exhibits a strong positive correlation with its Consistency with the caption. (3) Compared to open-source models, closed-source models exhibit a pronounced bias advantage in terms of Consistency. Eventually, we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement, indirectly improving the performance of their caption. We hope this paper will accelerate the research community in better evaluating their models and encourage future advancements in the consistency domain. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, benefiting from notable advancements in large language models (LLMs) [1; 25; 2], the realm of large vision-language models (LVLMs) has undergone a revolutionary transformation. These novel LVLMs [18; 24; 3; 8; 15; 13] try to combine visual signals with textual semantics and spark cognitive brilliance across modalities. Although LVLMs can generate high-quality responses to task prompts, we discover that for correctly answered cases, simply modifying the prompt will result LVLMs in providing contradictory responses. In Figure 1 (a.2), LLaVA-7B [18] properly describes the picture as \u201cIt is a man wearing a dinosaur costume.\u201d, but when prompted \u201cIs the dinosaur played by humans? Please answer yes or no.\u201d, it responds with \u201cNo, they are dinosaurs\u201d. The above phenomenon of Inconsistency is widely observed across mainstream LVLMs, and a preliminary study was conducted only on LLMs [14]. In practice, in contrast to the fixed patterns of questions, designed in existing multimodal benchmarks, the users tend to pose questions in arbitrary ways. Therefore, it is necessary to ensure the LVLMs in predicting correct and consistent answers, even when faced with various formats of queries. ", "page_idx": 0}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/5788914168fb74b9fc448b91f2e1f1c98f28bcd35f9e384a1d862458abfbfd1f.jpg", "img_caption": ["Figure 1: Here is the overview of our paper. Part (a) indicates two examples of Inconsistency between discriminative answers and generative captions, where the answers marked in blue contradict the answers marked in purple. Part (b) shows the Consistency evaluation method Conbench and its discriminative top three leaderboard. Part(c) reveals the main three findings built upon ConBench. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, there are currently no benchmarks or research studies that specifically focus on evaluating the Consistency of LVLMs responses. These single-prompt type evaluation approaches [12; 10; 28; 21; 6] lead to a disconnect between benchmark accuracy and real-world user practical experience. ", "page_idx": 1}, {"type": "text", "text": "Based on the above observations, we systematically introduce a Consistency Benchmark dubbed ConBench, to estimate the capabilities of LVLMs more thoroughly via diverse question formats. It consists of 1, 000 public pictures, and each was manually selected from four multimodal benchmarks [10; 12; 28; 21]. Apart from the original discriminative prompt, we constructed two additional discriminative types of questions3 by ChatGPT/GPT-4 [1]. Notably, three types of questions of each case are around the same knowledge point. Besides, every set is accompanied by a generative question without ground truth. Consequently, ConBench serves as an evaluation tool that observes the Consistency performance of LVLMs and surpasses the limitations of previous assessments. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, grounded on the ConBench, we conduct an in-depth analysis and visualization of Consistency on 14 popular LVLMs. In a nutshell, the conclusions of noteworthy insight are threefold: ", "page_idx": 1}, {"type": "text", "text": "C1 In the discriminative question-answering (QA) domain: (1) A decrease in LVLMs accuracy as the prompt\u2019s solution space increases. (2) Instances of erroneous yet consistent answers are scarce. ", "page_idx": 1}, {"type": "text", "text": "C2 Extended to the generative domain, we establish a connection between discriminative and generative domains by the perspective of Consistency. (1) As the solution space of discriminative questions expands, the Consistency between its answer and caption grows stronger. (2) The accuracy of discriminative answer and its Consistency with the caption exhibit a positive correlation. ", "page_idx": 1}, {"type": "text", "text": "C3 Closed-source models exhibit a pronounced bias advantage in terms of Consistency, compared to open-source models. This provides an alternative perspective to demonstrate why closed-source models, despite sometimes having lower accuracy, offer a better user experience in practical applications. ", "page_idx": 1}, {"type": "text", "text": "Eventually, leveraging the insights gained from our theoretical discoveries, we enhance the caption performance of LVLMs without any additional costs associated with training. Specifically, we construct discriminative prompts based on the low-confidence words in the answers of LVLMs, forcing the LVLMs to introspect. Then, through iterative refinement in multiple rounds of questionanswering, the quality of LVLMs\u2019 captions gets an impressive achievement (e.g., our method improves the LLaVA-NeXT-34B [19] by $9.1\\%$ and MiniGemini-34B [15] by $9.6\\%$ on metric[C] in Sec. 3.4). ", "page_idx": 1}, {"type": "text", "text": "In summary, to the best of our knowledge, we are the first to propose a Consistency evaluation method and conduct a comprehensive analysis of Inconsistency in LVLMs. We hope this paper serves as a catalyst for further exploration, and look forward to the community applying the above findings to polish up the usability and practicality of large vision-language models. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Large Vison Language Models With the impressive success of large language models (LLMs) [1; 25; 2; 4; 29], recent studies work on generative large vision-language models (LVLMs) [18; 24; 3; 8; 15; 27] to improve multimodal comprehension and generation through utilizing the strong generality of LLMs. Built upon the CLIP [23] image encoder which is somewhat aligned with the language modality, current LVLMs typically utilize vast image-text pairs to connect the vision encoder and LLM, enabling LLM to receive and understand visual content. For instance, LLaVA [20] directly connects the vision encoder and LLM with MLPs, showing proficiency in multi-modal dialogues. Subsequent works have further enhanced LVLMs by improving the multi-modal instruction data [18; 27; 5] and designing novel modules [3; 4; 26] for more sufficient modality alignment. ", "page_idx": 2}, {"type": "text", "text": "Conventional Multimodal Evaluation A multitude of public multimodal benchmarks, such as MME [10], SeedBench [12], and MMBench [21], further advance objective evaluation of LVLMs by only constructing True/False questions or multiple-choice questions, where the absence of diverse question types causes instability. In addition, their objective metrics solely emphasize the LVLM\u2019s accuracy, disregarding its robustness and security. The above issues can lead to a situation where some LVLMs have lower accuracy in evaluation results but provide a better user experience. To systematically assess the comprehensive capability of LVLMs, we propose a simple and efficient evaluation approach that relies on checking the Consistency between different kinds of prompts. ", "page_idx": 2}, {"type": "text", "text": "Inconsistency in LLMs A amount of prior work has been conducted on investigating Inconsistency in LLMs. [14] is the first to find the Inconsistency phenomenon in question-answering and validator tasks and define GV-consistency. Besides, it leverages consistency pair for training to improve LLMs\u2019 performance. While [17] utilizes Consistency to check for hallucination detection in LLMs, a logic consistency-based method that involves logic-related questions and answers. Compared to LLMs, Inconsistency in LVLMs is more likely to occur due to the additional visual modality, which deserves further exploration. ", "page_idx": 2}, {"type": "text", "text": "3 ConBench ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose a novel multimodal evaluation pipeline named ConBench to comprehensively assess LVLMs. The ConBench has a total of 4K questions on 1K images and corresponding 3K discriminative ground truths, guaranteeing evaluation quality in terms of the quantity and diversity of questions. In Sec. 3.1, we present the generation of ConBench and the construction pipeline for prompts. Sec. 3.2 introduces the hierarchical core capabilities and discusses the design philosophy. Sec. 3.3 and 3.4 describe the evaluation strategy for scoring various types of answers. ", "page_idx": 2}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/9c3f7b00f1cc04b7b8f9ba0b1176304fddfc2f3a4aed7e1c5fc33908ecb38058.jpg", "img_caption": ["Figure 2: Overview of 19 evaluation detailed categories in ConBench. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.1 Data Generation Process ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Image Filter We manually chose 1K images from four high-quality multimodal benchmarks: MME [10], SeedBench [12], MMBench [21], and MMMU [28]. MME is a true/false question type, while SeedBench and MMBench cover comprehensive multiple-choice questions. Meanwhile, MMMU emphasizes the knowledge level. The criteria for the image filter include: (1) resolution is more than $224\\times224$ (2) the image rarely occurs in the mainstream training dataset (e.g., COCO [16] and Cityscapes [9]) (3) There are more than 3 foreground objects in the image. The above criteria ensure the quality of content in images. ", "page_idx": 2}, {"type": "text", "text": "Prompt Construction Each image is accompanied by its original discriminative prompt, and we constructed two extra discriminative questions. Therefore, a case owns three discriminative prompts (true/false, multiple-choice and limited VQA questions) with a generative caption prompt around the same knowledge point. Firstly, we modified the original prompts whose answers can be directly inferred from the text instead of the image, to force LVLMs to utilize information from the visual features. Next, we employed GPT/GPT-4 to generate the extra discriminative types of questions, which were then subjected to the manual review, and the proposed prompt is listed in Figure 3. Finally, to avoid bias in the LVLMs that may affect the evaluation results, the true/false questions have a $50\\%$ distribution for both correct and wrong ground truths. For the multiple-choice questions, each option (e.g., A, B, C, D) has an equal probability distribution of $25\\%$ for being the correct answer. Notably, to ensure an accurate evaluation parser, limited VQA questions are subject to certain restrictions, like specifying the word count and answer format (e.g., fractions / abbreviations / numbers). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Hierarchical Core Capabilities ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The ConBench comprises three core capabilities, arranged in ascending order of difficulty, namely: Sensation, Cognition, and Knowledge, with nineteen fine-grained dimensions shown in Figure 2. [Easy Mode] Sensation: What you see is what Dr ", "page_idx": 3}, {"type": "text", "text": "You are a question expert. Give you a [Discriminative type] question, and you should generate two other kinds questions. The [Discriminative type] question is that [Original Prompt]. Based on the [Discriminative type] question above, a [The other discriminative type] about the [Category] with following answer, and a VQA question about [Category] with following answer are generated for the same knowledge point. ", "page_idx": 3}, {"type": "text", "text": "you get. We assume that sensation is the most fundamental expertise of LVLMs, and it is the \"eye\" of the LLMs. While perceived questions appear simple and basic, they are nonetheless essential. Therefore, this capability accounts for $50\\%$ of the ConBench. Count, color, optical character recognition (OCR) and scene categories focus on subtle details, while poster, attribute recognition and position types emphasize the overall picture. ", "page_idx": 3}, {"type": "text", "text": "Figure 3: The prompt for generation of discriminative questions. Please zoom in to view. ", "page_idx": 3}, {"type": "text", "text": "[Medium Mode] Cognition: Go beyond the surface. The cognitive process needs the model to integrate visual and language modalities: observing the content of an image, combining it with the text of question, and retrieving knowledge from within the LLMs. It is more challenging than the single sensation task. This section constitutes $26\\%$ of the ConBench, including numerical calculation, code inference, text translation, math, cross-instance reasoning and attribute reasoning categories. ", "page_idx": 3}, {"type": "text", "text": "[Hard Mode] Knowledge: Master the art of synthesis and integration. Mastering professional knowledge is an essential pathway for next-generation LVLMs to become Expert AGI, as it requires a higher level of understanding of images and the application of expert knowledge. We carefully selected knowledge from diverse and extensive fields, such as celebrities, chemistry, physics, biology, art and geography. This part takes up $24\\%$ of the total, and functions as the upper limit of ConBench. ", "page_idx": 3}, {"type": "text", "text": "3.3 Results Parser ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For true/false questions, we first extract the \"yes\" and \"no\" from the answer. If both of them are absent, the answer would be considered as \"none\". Then, we strictly compare the extracted answer with the ground truth. If they match exactly, the true/false response is considered correct. ", "page_idx": 3}, {"type": "text", "text": "When parsing the outcome of multiple choices, we derive the choice label (e.g., A, B, C, D) from it. If successful, utilize this as the prediction and match the ground truth. If not, we will not proceed with further extracting the answers. Because in each prompt of choices, we specified that only one letter needs to be answered. Doing so would be unfair to LVLMs that excel in following instructions. ", "page_idx": 3}, {"type": "text", "text": "We still utilize character matching for the answer of limited VQA instead of GPTs. On one hand, we have taken strict formatting constraints on the prompts. For instance, in physics and math, there are restrictions on answering with fractions (e.g., 1/2), while in geography at the city level. On the other hand, the cost of the GPT\u2019s judgment is high and the waiting time is delayed. Specifically, the parser is based on the Average Normalized Levenshtein Similarity (ANLS) [22], where the threshold $\\tau$ is set to 0.95 and $M=N=1$ . When parsed result $s>0.4$ , we consider the answer to be exactly right. ", "page_idx": 3}, {"type": "text", "text": "3.4 Multidimensional Evaluation Metric ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we provide two evaluation metrics, each from the perspective of discriminative and generative domains, aiming to provide a more comprehensive understanding of LVLMs consistency. The former does not rely on AI tools and quickly produces Consistency results among discriminative responses via Sec. 3.3, primarily evaluating the knowledge. The latter employs GPT to indirectly assess the quality of captions, by judging the consistency between discriminative responses and captions. ", "page_idx": 3}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/806d9b67c6b08f7916f1bd23fcd6da3e1ab059ed269d759a94bb29764216c1e3.jpg", "img_caption": ["Figure 4: The pipeline of judging Consistency between caption and discriminative answers via GPT/GPT4. Please zoom in to view the prompt. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Table 1: Evaluation[D] of mainstreams series of LVLMs on ConBench. The detailed results of the Sensation, Cognition, and Knowledge core capabilities are listed below. T, C, and V represent true-false, multiple-choice, and limited VQA questions, respectively. The ranking can be found below the respective numbers. $\\dagger$ : Due to safety considerations, GPT-4V declined to answer the celebrity category. ", "page_idx": 4}, {"type": "table", "img_path": "tu1oC7zHGW/tmp/8859ab915d24218f6f45bc49494fca8dadaee8e201b32c339aff188e8fb1a3eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Discriminative Domain Evaluation Metric We define the ConScore[D] as that: when all three discriminative types of questions within the same case are answered correctly, the model gets one point. The maximum score is 1000 points. The final format is presented as a percentage $(\\%)$ . ", "page_idx": 4}, {"type": "text", "text": "Generative Domain Evaluation Metric Due to the high variability in captions, it is not possible to calculate Consistency based on character matching alone. Therefore, we rely on GPT/GPT4 for judgment. The judging process and the constructed prompts are shown in Figure 4. We formulate it as a machine reading comprehension task. We manually sample the judgment results, and GPT4 achieved an accuracy rate of $95\\%$ , which is reliable and trustworthy. Next, we define the ConScore[C] as the average score of Consistency between the caption and the other three discriminative responses. ", "page_idx": 4}, {"type": "text", "text": "4 Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Evaluation Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, 6 closed-source and 8 open-source representative LVLMs with varying sizes and architectures are evaluated on our Consistency benchmark, including GPT-4V [1], GPT4-Omni [1], Gemini-Vision [24], Qwen-VL series [3], LLaVA series [18; 19], MiniGemini series [15] and InternVL series [8]. The evaluation results on ConBench are listed in Table 1 and 2. In the metric[D] ", "page_idx": 4}, {"type": "table", "img_path": "tu1oC7zHGW/tmp/94af39634e6bc52603818bfd39afa4d1bd2899d16605b50002b64f27026455b4.jpg", "table_caption": ["Table 2: Evaluation of Consistency between caption and three discriminative types of answer on ConBench. The \"rank diff\" means the difference between ConScore[D] and Score[C]. The $\\operatorname{Con}[X]$ is the Consistency ratio between discriminative answer type $X$ and caption. The \"ordered\" represents whether $\\mathrm{Con[T]}<\\mathrm{Con[C]}<\\mathrm{Con[V]}$ is in its line. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "1, Qwen-VL-Max [3] secures the top position, leading the second-place GPT4-Omni [1] by a margin of $1.3\\%$ . The InternVL-v1.2P-40B [8] performs best in the open-sourced community, especially in cognition capability. The LLaVA series did not make it to the top ten. In the metric[C], the newest GPT4-Omni [1] leads the leaderboard, which is the only model that surpasses 60. It has a significant advantage over the second-place model Qwen-VL-Max [3], with a gap of 3.8. We observed that although the GPT series slightly underperforms Qwen-Max in metric[D], it significantly outperforms the Qwen series in metric[C], which aligns with our actual user experience. Actually, ConScore[C] provides an alternative quality description of captions, because higher recall and precision rates usually match better Consistency. Besides, rankings of LVLMs show a slight variation between metric[C] and metric[D]. The GPT series models claim better performance of caption generation. ", "page_idx": 5}, {"type": "text", "text": "4.2 Discriminative Domain ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To investigate what causes the Inconsistency between different types of prompts, we first conduct analyses on the discriminative domain to compare the performance differences. We summarize our findings into the following facts: ", "page_idx": 5}, {"type": "text", "text": "Fact 4.2.1 (Inconsistency in Accuracy). The accuracy of the answer decreases as the solution space of the discriminative prompt increases. ", "page_idx": 5}, {"type": "text", "text": "As shown in the columns of \"T\", \"C\", and \"V\" in Table 1, the accuracy decreases as the solution space expands in all core capabilities. For instance (e.g., the Sensation of ", "page_idx": 5}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/c9e1160ca7d15c823ef1c990d385286099f2085ca178d749754cea58ad28c26b.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "GPT-4-Omni), the double-choice true-false questions achieve an accuracy of 89.2, whereas the accuracy for multiple-choice and VQA questions on the same case declines to 79.4 and 61.7, respectively. This is understandable, as the number of potential choices increases, the difficulty in identifying the correct answer also rises. ", "page_idx": 5}, {"type": "text", "text": "Fact 4.2.2 (Inconsistency in Wrong Answers). Cases of erroneous yet consistent answers are scarce. ", "page_idx": 5}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/1185b5cd660469c1c916229762a45e33f6e194233babb42d7ae4e2a75e8d6fa2.jpg", "img_caption": ["Figure 6: Visualization of the relationship between the correct rate of discriminative answer and its Consistency with the caption on different answer types. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We analyze the answers that fail in all three question types and find that, despite all resulting in incorrect predictions, they do not demonstrate a consistent understanding of the same images, leading to distinct answers. For example, we calculated the proportion of consistent incorrect responses in VQA and multiple-choice questions. We found a very small consistency, and it did not exceed $0.50\\%$ across the entire benchmark. This indicates that the models struggle to interpret the visual content uniformly, revealing significant variability in their failure modes. ", "page_idx": 6}, {"type": "text", "text": "Fact 4.2.3 (Inconsistency in Confidence). The confidence of models in their answers reveals signs of inconsistent and incorrect predictions. ", "page_idx": 6}, {"type": "text", "text": "Taking Fact 4.2.1 and Fact 4.2.2 into account, we perform a deeper analysis of the model\u2019s predictions by measuring their confidence in the answers. We use the predicted probabilities and logits of the answer tokens to represent confidence (see Appendix B for details). As summarized in Figure 5, we measure the average probabilities and logits of the correct and incorrect answers4, respectively. The three types of questions share similar confidence levels for the correct answers. However, for the incorrect answers, their confidence levels vary significantly with a clear trend: the larger the solution space, the smaller the confidence. This analysis provides crucial insights for our method in enhancing the consistency and accuracy of LVLMs, which we will further discuss in Sec. 5. ", "page_idx": 6}, {"type": "text", "text": "4.3 Generative Domain ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we extend our attention to the generative domain. Based on Consistency, we first build a bridge between the discriminative and generative domains. We consolidate our findings as the below facts: ", "page_idx": 6}, {"type": "text", "text": "Fact 4.3.1 (Inconsistency to Generative Answers). As the solution space of discriminative questions increases, the Consistency between their answers and generative answers increases. ", "page_idx": 6}, {"type": "text", "text": "As indicated in the last column of Table 2, \u201cOrdered\u201d means $\\mathrm{Con[T]}<\\mathrm{Con[C]}<\\mathrm{Con[V]}$ . The answers of all closed-source models and most open-source models adhere to this pattern. Here is the theoretical explanation. Assume the distribution for the generative domain (Caption) is $\\boldsymbol{S}$ , and the sample space of $\\boldsymbol{S}$ is $W$ . For the discriminative domain, the sample space is limited to $W^{\\prime}$ , which only contains some candidates from $W$ . Assume the model handles the discriminative domain by creating another distribution $S^{\\prime}$ according to $\\boldsymbol{S}$ and $W^{\\prime}$ . Then the total variation distance (TVD) [11] ", "page_idx": 6}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/707a0c1d78b5a46c9f6f6ed88b7192629a29aff65c252c93b5a57c56f4c1e40c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: Visualization of the relationship between the correct rate of discriminative answer and its Consistency with the caption on different capability types. ", "page_idx": 7}, {"type": "text", "text": "between $\\boldsymbol{S}$ and $S^{\\prime}$ is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left\\|\\boldsymbol{S}-\\boldsymbol{S}^{\\prime}\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This becomes larger when $\\left|W\\right\\rangle\\left|W^{\\prime}\\right|$ becomes larger. For instance, if the model creates $S^{\\prime}$ by simply doing reject sampling5, then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left\\|S-S^{\\prime}\\right\\|_{1}=P r[S\\in W\\setminus W^{\\prime}].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It is obvious that when $W^{\\prime}$ is more \u201ddifferent\u201d from $W$ , the distance will be larger. ", "page_idx": 7}, {"type": "text", "text": "Fact 4.3.2 (Connection between Discriminative and Generative Domain). The accuracy of the discriminative answer exhibits a strong positive correlation with its Consistency with the generative. ", "page_idx": 7}, {"type": "text", "text": "As shown in Figure 6 and 7, we conduct visualizations for all tested LVLMs: The vertical axis represents the accuracy of their discriminative answers, while the horizontal axis represents the consistency of the answers with caption. Figure 6 displays the distribution across different question types, while Figure 7 illustrates the distribution across different core capabilities. The green lines represent a ftited linear equation. Additionally, we utilize the Pearson coefficient $P[X,Y]$ to quantitatively analyze the degree of linear correlation, and the 6 coefficients in the above figures are all more than 0.85. ", "page_idx": 7}, {"type": "text", "text": "4.4 Consistency Bias ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Fact 4.4.1 (Consistency Bias). Closed-source models exhibit a pronounced bias advantage on Consistency, compared to open-source models. ", "page_idx": 7}, {"type": "text", "text": "When we fit a linear regression to all evaluated models and get the green line in Figure 6 (a): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}:y=k x+b,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $x$ is the accuracy, and $y$ means Consistency between its answer and caption. We found that the majority of open-source models lie below this line, while closed-source models lie above it. In other words, at the same level of accuracy, the responses from closed-source models tend to exhibit better consistency with their captions. So we fti a linear regression to closed-source models and get the red line. The line they reside on has a higher bias $b_{c}$ (e.g., $b_{c}-b=3.24$ in Figure 6 (a)), which aligns with our experience where closed-source models provide more comprehensive and reliable answers. ", "page_idx": 7}, {"type": "text", "text": "5 Trigger-based Diagnostic Refinement ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In light of the previous findings, we summarize two key insights: (1) LVLMs exhibit higher accuracy when operating within a narrower discriminative solution space; (2) Incorrect answers are usually associated with significantly lower confidence and logits. Consequently, we propose a simple but efficient method dubbed Trigger-based Diagnostic Refinement (TDR) to ameliorate the generation skill of LVLMs without any additional training. The proposed pipeline is presented in Figure 8. ", "page_idx": 7}, {"type": "table", "img_path": "tu1oC7zHGW/tmp/69eaa4831677cdca72b0db8c226effc59f76602e24ac2cfe1b4b071d3ebf170f.jpg", "table_caption": ["Table 4: Results on LLaVA-34B and MiniGemini-34B via Trigger-based Diagnostic Refinement. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Method We start by making the LVLM generate a caption, with each word accompanied by its corresponding probability. Next, uninformative words are dropped based on their parts of speech, and we only keep nouns, adjectives and quantifiers. When the remaining words with probabilities below a threshold $\\tau$ (we set $\\tau=0.85$ here), trigger subsequent diagnostic processes. Since low probabilities of words indicate a lack of confidence, we formulate True/False discriminative questions to force the LVLM to selfverify (e.g., Is there {cat} in the picture?). The self-diagnostic prompt and its response will be drafted into a new prompt, which is fed back into the LVLM to generate a higher-quality caption. ", "page_idx": 8}, {"type": "text", "text": "Results We carried out experiments on the LLaVANeXT-34B and MiniGemini-34B and evaluated them on the metric[C] of ConBench. The experimental results are detailed in Table 4. Notably, the LLaVANeXT-34B sees an improvement of 9.1 points, while the MiniGemini experiences an overall enhancement of 9.6 points. Although our approach primarily employs True/False questions for self-verify, there is still a noticeable improvement in ConScore[C]. Hence, our method effectively boosts the quality of captions by triggering the model to self-check. ", "page_idx": 8}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/f518f89c335b51799d925327fb3f24449fad7b04c190e9c9912f270d69ba2eb1.jpg", "img_caption": ["Figure 8: The Trigger-based Diagnostic Refinement pipeline. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In theory, we can further construct multiple discriminative questions for the caption, enabling the model to verify multiple elements within the caption. Additionally, the process can be iterated multiple rounds, leading to ongoing enhancements in the quality of the generated output. Our method is a simplified implementation of the above approaches. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this study, we investigate the Consistency issues in large vision-language models (LVLMs). Consistency reflects the overall ability of LVLMs, as it not only requires LVLMs to provide correct answers but also demands sufficient confidence in their knowledge point, regardless of the type of question encountered. We first introduce the ConBench, a benchmark that fills the gap in assessing Consistency. It includes 1K images with 4K prompts and two evaluation metrics: ConScore[D] and ConScore[C]. Then, our findings shed light on the nature of Consistency in LVLMs according to the ConBench. We observe that as the solution space of a prompt increases, the accuracy of the answers tends to decrease. Besides, we establish a relationship between the discriminative and generative realms, highlighting the importance of Consistency between the discriminative answer and caption. Furthermore, we discover that closed-source models exhibit a bias advantage over open-source models in terms of consistency. Finally, we propose a solution by forcing LVLMs to self-think, where a discriminative prompt is constructed via uncertain words in the caption. Our method makes the quality of LVLMs\u2019 captions an impressive achievement. We believe that our research contributes to the evaluation of LVLMs and encourages future advancements for achieving Consistency in LVLMs. ", "page_idx": 8}, {"type": "text", "text": "7 Acknowledgment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Shanghang Zhang was supported by the National Science and Technology Major Project (No.   \n2022ZD0117800). ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \n[3] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   \n[4] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.   \n[5] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.   \n[6] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024.   \n[7] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024.   \n[8] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P. Luo, T. Lu, Y. Qiao, and J. Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.   \n[9] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213\u20133223, 2016.   \n[10] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu, and R. Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.   \n[11] J. Kennedy and M. Quine. The total variation distance between the binomial and poisson distributions. The Annals of Probability, pages 396\u2013400, 1989.   \n[12] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.   \n[13] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[14] X. L. Li, V. Shrivastava, S. Li, T. Hashimoto, and P. Liang. Benchmarking and improving generatorvalidator consistency of language models. arXiv preprint arXiv:2310.01846, 2023.   \n[15] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024.   \n[16] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[17] Z. Lin, S. Trivedi, and J. Sun. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187, 2023.   \n[18] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning, 2023.   \n[19] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.   \n[20] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning, 2023.   \n[21] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.   \n[22] M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2200\u20132209, 2021.   \n[23] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[24] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[25] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[26] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.   \n[27] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[28] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.   \n[29] R. Zhang, J. Han, C. Liu, P. Gao, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, and Y. Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A Showcases of Trigger-based Diagnostic Refinement ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "MGM-34B ", "page_idx": 11}, {"type": "text", "text": "Image. Count-000000236721.jpg ", "page_idx": 11}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/8cceb30385bcf7f7682e1d4e09aa43f63a8ba69c94cb8ef067f89288e13758f6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "Original Caption. The image contains a variety of objects, which can be counted as follows: one tray, one cake, one bowl of sauce, one carton of milk, one banana, one glass of orange juice, one cup of coffee, and one sugar packet. The objects are arranged neatly on the tray, which is placed on a table. ", "page_idx": 11}, {"type": "text", "text": "Refined Caption The image contains a total of two bananas, which are positioned on the right side of the tray. The bananas are yellow, indicating that they are ripe, and they are the only fruits visible in the image. They are placed next to a carton of milk, ice cream, a cake, two cups of coffee, a glass of orange juice, which are a common accompaniment in a breakfast setting. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "LLava-Next-34B ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Image. Attribute Reason-1724416_2902943546.jpg ", "page_idx": 11}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/8f110dffd73ebeae1a8f585a15827fb7f430e4220483d1f35a5603832ea4061e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "Original Caption. The image captures a vibrant live music scene. At the center of the frame, a musician is seated on a stool, engrossed in playing a red electric guitar. He is dressed casually in a white t-shirt and blue jeans, adding to the relaxed atmosphere of the performance. The stage, adorned with a colorful banner, is set against a backdrop of a brick wall, lending an urban feel to the setting. The banner prominently features the text \"DOWNTOWN $5^{\\prime\\prime}$ , possibly indicating the name of the band or the event. The musician is surrounded by other band members, each engrossed in their own instruments, contributing to the overall harmony of the performance. The stage is well-equipped with a microphone and a speaker, essential for amplifying the sound and reaching the audience. In the background, a crowd of people can be seen, their attention riveted on the performance. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Refined Caption The image captures a vibrant scene of a live music performance. At the center of the stage, a musician is seated on a stool, engrossed in playing an electric guitar. He is dressed in a casual white t-shirt, and his focus is evident as he strums the strings. The stage is set against a backdrop of a colorful banner that proudly proclaims \"DOWNTOWN 5\". The banner\u2019s bold colors contrast with the musician\u2019s white t-shirt, drawing attention to both the performer and the event. The stage is equipped with various musical instruments, including a drum set and a keyboard, suggesting a full band performance. A microphone stands ready, capturing the musician\u2019s performance for the audience. The stage is situated in a space that appears to be partially open to the elements, as suggested by the presence of a roof structure in the background. This setup indicates that the concert is being held in a venue that can accommodate outdoor or semi-outdoor performances, adding a unique ambiance to the event. Overall, the image portrays a dynamic and engaging live music event, with the musician at the heart of the performance, surrounded by his instruments. ", "page_idx": 11}, {"type": "text", "text": "B Computation for Probabilities and Logits ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "For each model (MGM-13B and LLaVA-13B), we randomly selected 50 consistent and 50 inconsistent samples, respectively. The JSON samples are in support materials. To measure the LVLMs\u2019 response confidence accurately, we manually simplified the answers and maintained the words that are the most critical to the judgment. The specific principles are as listed: ", "page_idx": 11}, {"type": "text", "text": "For true/false questions, we only keep the word \u2019yes\u2019 or \u2019no\u2019 and their probabilities. ", "page_idx": 11}, {"type": "text", "text": "For multiple-choice questions, we only keep the choice labels (e.g., A, B, C, D) and their probabilities. ", "page_idx": 11}, {"type": "text", "text": "For limited VQA questions, we manually picked out keywords that matched ground truth from the answers, and computed the average probabilities of them as the final probability. ", "page_idx": 12}, {"type": "text", "text": "C Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The introduced ConBench offers a new perspective on evaluating model performance through the consistency between multiple types of questions, providing a more comprehensive measurement and understanding of existing LVLMs. However, due to the distinct response forms of captions, assessing the consistency between captions and discriminative answers is judged by GPT, posing a risk of inaccurate evaluations. Besides, by delving deeper into our benchmark analysis, we propose triggerbased diagnostic refinement to improve the consistency and accuracy of LVLMs. This, however, introduces additional computational costs and is limited by the inherent capabilities of the LVLMs. Further improvements can be achieved by designing and training LVLMs with a focus on consistency. ", "page_idx": 12}, {"type": "text", "text": "D Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Overall, this research has broader impacts on the evaluation, performance, fairness, and future development of LVLMs, fostering progress and advancements in the field of vision-language models. ", "page_idx": 12}, {"type": "text", "text": "Advancing Evaluation: The introduction of ConBench, a benchmark for assessing Consistency in LVLMs, flils a crucial gap in the evaluation of these models. This benchmark provides a standardized framework for measuring the performance and reliability of LVLMs across different prompts. ", "page_idx": 12}, {"type": "text", "text": "Novel Insights: we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers. (2) Establish the relationship between the discriminative and generative realms. (3) Compared to open-source models, closed-source models exhibit a bias advantage in terms of Consistency. ", "page_idx": 12}, {"type": "text", "text": "Inspiring Future Research: By contributing to the evaluation and understanding of Consistency in LVLMs, this research paves the way for future advancements in the field. It encourages researchers to explore new techniques, methodologies, and approaches to achieve higher levels of Consistency in LVLMs, ultimately pushing the boundaries of language and vision understanding. ", "page_idx": 12}, {"type": "text", "text": "E Detailed Cases in ConBench ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We have uploaded the ConBench dataset, including images and their prompts, to the Hugging Face platform. The dataset can be accessed at the following URL: https://huggingface.co/ datasets/ConBench/ConBench. Here, we enumerate several representative cases from ConBench. Arrange in order from easy to difficult, respectively, based on sensation, cognition, and knowledge. ", "page_idx": 12}, {"type": "text", "text": "Count ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/5e28105790f121a58b9ee9c391f843bad412265358690cf0885049e7c7fb93d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "T:\u00a0Are\u00a0there\u00a0three\u00a0laptops\u00a0in\u00a0the picture?\u00a0Please\u00a0only\u00a0answer\u00a0yes\u00a0or\u00a0no. A:\u00a0Yes ", "page_idx": 13}, {"type": "text", "text": "V:\u00a0How\u00a0many\u00a0laptops\u00a0are\u00a0depicted? Please\u00a0answer\u00a0with\u00a0a\u00a0number. A:\u00a03 ", "page_idx": 13}, {"type": "text", "text": "C:\u00a0How\u00a0many\u00a0laptops\u00a0are\u00a0in\u00a0the   \npicture?   \nA)\u00a0One   \nB)\u00a0Two   \nC)\u00a0Three   \nD)\u00a0Four.   \nPlease\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA:\u00a0C   \nCaption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0image description.\u00a0You\u00a0need\u00a0to\u00a0describe\u00a0this picture\u00a0with\u00a0accurate\u00a0object\u00a0count   \ninformation. ", "page_idx": 13}, {"type": "text", "text": "Scene ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/d33a31fc5956890a8e115674b9a2c5aabd995028e00037c387cb6d9aa2ab83f5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "T:\u00a0Is\u00a0this\u00a0photo\u00a0taken\u00a0in\u00a0a\u00a0place\u00a0of corridor?\u00a0Please\u00a0answer\u00a0yes\u00a0or\u00a0no. A:\u00a0Yes ", "page_idx": 13}, {"type": "text", "text": "V:\u00a0Where\u00a0was\u00a0the\u00a0photo\u00a0taken? Answer\u00a0within\u00a0a\u00a0word. A:\u00a0Corridor ", "page_idx": 13}, {"type": "text", "text": "C:\u00a0Where\u00a0was\u00a0this\u00a0photo\u00a0taken?   \nA)\u00a0Corridor   \nB)\u00a0Park   \nC)\u00a0Office   \nD)\u00a0Street.   \nPlease\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA:\u00a0A   \nCaption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0scene. You\u00a0need\u00a0to\u00a0describe\u00a0the\u00a0scene\u00a0in\u00a0the picture. ", "page_idx": 13}, {"type": "text", "text": "OCR ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/7d31ddf808fab7aaf9b11ea9978ea28f83fb982339c148a84aa637fa4dc0f0a1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "T:\u00a0Is\u00a0the\u00a0word\u00a0in\u00a0the\u00a0logo \\\u02baangle\u02b9s\\\u02ba?\u00a0Please\u00a0answer\u00a0yes\u00a0or\u00a0no. A:\u00a0No ", "page_idx": 13}, {"type": "text", "text": "V:\u00a0What\u00a0is\u00a0the\u00a0word\u00a0in\u00a0the\u00a0referenced logo?\u00a0Answer\u00a0within\u00a0a\u00a0word. A:\u00a0Angie\u02b9s ", "page_idx": 13}, {"type": "text", "text": "C:\u00a0What\u00a0is\u00a0the\u00a0word\u00a0in\u00a0the\u00a0referenced logo?   \nA)\u00a0Angie\u2019s   \nB)\u00a0Angie   \nC)\u00a0Agnes\u2019s   \nD)\u00a0Anjie\u2019s.   \nPlease\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA:\u00a0A   \nCaption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0image OCR.\u00a0What\u00a0is\u00a0the\u00a0word\u00a0in\u00a0the   \nreferenced\u00a0logo? ", "page_idx": 13}, {"type": "text", "text": "Position ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/ea8673f8f98c459daa09f095f80bc128023d4b3958f931f3520c62346f503d57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "T:\u00a0Is\u00a0the\u00a0white\u00a0couch\u00a0positioned   \nbehind\u00a0the\u00a0glass\u00a0coffee\u00a0table?\u00a0Please answer\u00a0yes\u00a0or\u00a0no.   \nA:\u00a0Yes ", "page_idx": 13}, {"type": "text", "text": "V:\u00a0Where\u00a0is\u00a0the\u00a0white\u00a0couch\u00a0located\u00a0in relation\u00a0to\u00a0the\u00a0glass\u00a0coffee\u00a0table? Please\u00a0answer\u00a0within\u00a04\u00a0words. A:\u00a0behind\u00a0the\u00a0coffee\u00a0table ", "page_idx": 13}, {"type": "text", "text": "C:\u00a0What\u00a0is\u00a0the\u00a0position\u00a0of\u00a0the\u00a0white couch\u00a0relative\u00a0to\u00a0the\u00a0glass\u00a0coffee\u00a0table? A) The\u00a0couch\u00a0is\u00a0in\u00a0front\u00a0of\u00a0the\u00a0coffee table   \nB)\u00a0The\u00a0couch\u00a0is\u00a0to\u00a0the\u00a0right\u00a0of\u00a0the   \ncoffee\u00a0table   \nC)\u00a0The\u00a0couch\u00a0is\u00a0to\u00a0the\u00a0left\u00a0of\u00a0the\u00a0coffee table   \nD)\u00a0The\u00a0couch\u00a0is\u00a0behind\u00a0the\u00a0coffee\u00a0table. Please\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA:\u00a0D ", "page_idx": 13}, {"type": "text", "text": "Caption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0position. You\u00a0need\u00a0to\u00a0describe\u00a0this\u00a0picture\u00a0with accurate\u00a0information\u00a0about\u00a0position about\u00a0objects\u00a0in\u00a0the\u00a0image. ", "page_idx": 13}, {"type": "text", "text": "Math ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/4139223527a38c810ab45b26fca62287265b7b8abdb82bda7039c914c2198cea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "T:\u00a0Is\u00a0the\u00a0function\u00a0f(x)\u00a0=\u00a0x^2\u00a0\u2010 6x\u00a0+\u00a04 convex?\u00a0Please\u00a0answer\u00a0yes\u00a0or\u00a0no. A:\u00a0Yes ", "page_idx": 14}, {"type": "text", "text": "V:\u00a0Is\u00a0the\u00a0function\u00a0f(x)\u00a0=\u00a0x^2\u00a0\u2010 6x\u00a0+\u00a04 convex\u00a0or\u00a0concave?\u00a0Answer\u00a0within\u00a0a word.   \nA:\u00a0Convex C:\u00a0Is\u00a0the\u00a0function\u00a0f(x)\u00a0=\u00a0x^2\u00a0\u2010 6x\u00a0+\u00a04   \nconvex\u00a0or\u00a0concave?   \nA)\u00a0Convex   \nB)\u00a0Concave   \nC)\u00a0Neither   \nD)\u00a0Both.   \nPlease\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA: A   \nCaption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0math. Describe\u00a0the\u00a0concave\u00a0and\u00a0convex   \nproperties\u00a0of\u00a0the\u00a0function. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "$\\textbf{x}=\\textbf{10}$   \nif $\\begin{array}{r}{\\mathrm{~x~}<~20:}\\end{array}$ : print(\"Hello\")   \nelse: print(\"world\") T:\u00a0The\u00a0image\u00a0shows\u00a0a\u00a0python\u00a0code.\u00a0Is the\u00a0output\u00a0of\u00a0the\u00a0code\u00a0\u02b9World\u02b9?\u00a0Please only\u00a0tell\u00a0me\u00a0\u02b9yes\u02b9\u00a0or\u00a0\u02b9no\u02b9\u00a0without\u00a0any other\u00a0words.   \nA:\u00a0No ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "V:\u00a0What\u00a0is\u00a0the\u00a0output\u00a0of\u00a0the\u00a0Python code?\u00a0Please\u00a0answer\u00a0with\u00a0one\u00a0word without\u00a0any\u00a0other\u00a0words. A:\u00a0Hello ", "page_idx": 14}, {"type": "text", "text": "C:\u00a0What\u00a0is\u00a0the\u00a0output\u00a0of\u00a0the\u00a0Python code?   \nA)\u00a0Goodbye   \nB)\u00a0Hello   \nC)\u00a0Error   \nD)\u00a0Nothing.   \nChoose\u00a0one\u00a0from\u00a0the\u00a0four\u00a0letters\u00a0[A,\u00a0B, C,\u00a0D]\u00a0without\u00a0any\u00a0other\u00a0words.   \nA:\u00a0B   \nCaption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0code reasoning.\u00a0What\u00a0is\u00a0the\u00a0programming language\u00a0of\u00a0the\u00a0code\u00a0and\u00a0tell\u00a0me\u00a0the output\u00a0without\u00a0any\u00a0other\u00a0words. ", "page_idx": 14}, {"type": "text", "text": "Translation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u7f8e\u5473\u7684\u665a\u9910", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "T: Is\u00a0it\u00a0appropriate\u00a0to\u00a0translate\u00a0the Chinese\u00a0in\u00a0the\u00a0image\u00a0into\u00a0English\u00a0\u02b9a delicious\u00a0dinner\u02b9\u00a0in\u00a0the\u00a0picture? Please\u00a0answer\u00a0yes\u00a0or\u00a0no. A: Yes ", "page_idx": 14}, {"type": "text", "text": "V: Translate\u00a0the\u00a0Chinese\u00a0in\u00a0the   \npicture\u00a0to\u00a0English.\u00a0Answer\u00a0within\u00a03 words.   \nA: a\u00a0delicious\u00a0dinner C: How\u00a0to\u00a0translate\u00a0the\u00a0Chinese\u00a0in\u00a0the image\u00a0into\u00a0English?   \nA)\u00a0a\u00a0delicious\u00a0dinner   \nB)\u00a0traditional\u00a0flavor   \nC)\u00a0hamburger\u00a0and\u00a0chips   \nD)\u00a0vintage\u00a0taste.   \nPlease\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA: A ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Caption: You\u00a0are\u00a0an\u00a0expert\u00a0in translation.\u00a0You\u00a0need\u00a0to\u00a0translate\u00a0the Chinese\u00a0in\u00a0this\u00a0picture\u00a0into\u00a0English. ", "page_idx": 14}, {"type": "text", "text": "Attribute Reasoning ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/5ed10b8b5ae3827d4ec5d75eb84a28f910885337266eed79b0dc640d5aac53cc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "T: Is\u00a0the\u00a0position\u00a0and\u00a0activity\u00a0of\u00a0the horses\u00a0indicative\u00a0of\u00a0them\u00a0engaging in\u00a0a\u00a0competitive\u00a0or\u00a0playful interaction\u00a0rather\u00a0than\u00a0stationary activities\u00a0like\u00a0grazing\u00a0or\u00a0standing still?\u00a0Please\u00a0answer\u00a0yes\u00a0or\u00a0no. A: Yes ", "page_idx": 14}, {"type": "text", "text": "V: Based\u00a0on\u00a0the\u00a0attributes\u00a0and positions\u00a0of\u00a0the\u00a0horses,\u00a0which conclusion\u00a0could\u00a0be\u00a0drawn?\u00a0Answer within\u00a05\u00a0words. A: The\u00a0horses\u00a0are\u00a0grazing ", "page_idx": 14}, {"type": "text", "text": "C: Based\u00a0on\u00a0the\u00a0attributes\u00a0and   \npositions\u00a0of\u00a0the\u00a0horses,\u00a0which   \nconclusion\u00a0could\u00a0be\u00a0drawn?   \nA)\u00a0The\u00a0horses\u00a0are\u00a0grazing   \nB)\u00a0The\u00a0horses\u00a0are\u00a0all\u00a0standing\u00a0still   \nC)\u00a0The\u00a0horses\u00a0are\u00a0playing\u00a0together   \nD)\u00a0The\u00a0horses\u00a0are\u00a0racing.   \nPlease\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA: A ", "page_idx": 14}, {"type": "text", "text": "Caption: You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0attribute reason,\u00a0please\u00a0describe\u00a0this\u00a0image\u00a0in detail ", "page_idx": 14}, {"type": "text", "text": "Artwork ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/d44ef9b2eadc10aae175002d035ed481788f279d4e2229a27d30d389724547fe.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "T:\u00a0Does\u00a0this\u00a0artwork\u00a0exist\u00a0in\u00a0the\u00a0form of\u00a0painting?\u00a0Please\u00a0answer\u00a0yes\u00a0or\u00a0no. A:\u00a0Yes ", "page_idx": 15}, {"type": "text", "text": "V:\u00a0What\u00a0is\u00a0the\u00a0form\u00a0of\u00a0this\u00a0artwork? Please\u00a0answer\u00a0within\u00a03\u00a0words. A:\u00a0Painting ", "page_idx": 15}, {"type": "text", "text": "C:\u00a0In\u00a0which\u00a0form\u00a0does\u00a0this\u00a0artwork exist? ", "page_idx": 15}, {"type": "text", "text": "A)\u00a0Sculpture   \nB)\u00a0Painting   \nC)\u00a0Digital\u00a0Art   \nD)\u00a0Performance\u00a0Art.   \nPlease\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA:\u00a0B   \nCaption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0artwork. You\u00a0need\u00a0to\u00a0describe\u00a0this\u00a0picture\u00a0with accurate\u00a0information\u00a0about\u00a0its\u00a0title, actor,\u00a0form\u00a0and\u00a0the\u00a0location\u00a0of\u00a0display. ", "page_idx": 15}, {"type": "text", "text": "Celebrity ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/bd925cf3553940c365f78e08fe57332f32c71b5d955c1e3a90aa4ca8be9b6f70.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "T:\u00a0Is\u00a0the\u00a0actor\u00a0inside\u00a0the\u00a0red\u00a0bounding box\u00a0called\u00a0Hugh\u00a0Jackman?\u00a0Please   \nanswer\u00a0yes\u00a0or\u00a0no.   \nA:\u00a0Yes ", "page_idx": 15}, {"type": "text", "text": "V:\u00a0Who\u00a0is\u00a0the\u00a0actor\u00a0identified\u00a0inside the\u00a0red\u00a0bounding\u00a0box?\u00a0Please\u00a0answer with\u00a0a\u00a0name\u00a0within\u00a03\u00a0words. A:\u00a0Hugh\u00a0Jackman ", "page_idx": 15}, {"type": "text", "text": "C:\u00a0Which\u00a0actor\u00a0is\u00a0identified\u00a0within\u00a0the red\u00a0bounding\u00a0box? ", "page_idx": 15}, {"type": "text", "text": "B)\u00a0Robert\u00a0Downey\u00a0Jr ", "page_idx": 15}, {"type": "text", "text": "C)\u00a0Hugh\u00a0Jackman ", "page_idx": 15}, {"type": "text", "text": "D)\u00a0Chris\u00a0Hemsworth. ", "page_idx": 15}, {"type": "text", "text": "Please\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA:\u00a0C   \nCaption:\u00a0You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0image description.\u00a0You\u00a0need\u00a0to\u00a0describe\u00a0this picture\u00a0with\u00a0accurate\u00a0information   \nabout\u00a0the\u00a0actor. ", "page_idx": 15}, {"type": "text", "text": "Biology ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/9a45b77759efc28cb48165699174a15b861140aaf8d72b93927eedd6022cbace.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "T: Does\u00a0letter\u00a0A\u00a0indicate\u00a0the\u00a0anode? Please\u00a0answer\u00a0yes\u00a0or\u00a0no. A: No ", "page_idx": 15}, {"type": "text", "text": "V: The\u00a0apparatus\u00a0in\u00a0figure\u00a0is\u00a0used   \nfor\u00a0SDSPAGE\u00a0(polyacrylamide\u00a0gel   \nelectrophoresis).\u00a0Which\u00a0letter   \nindicates\u00a0the\u00a0anode?   \nA: B ", "page_idx": 15}, {"type": "text", "text": "C: The\u00a0apparatus\u00a0in\u00a0figure\u00a0is\u00a0used\u00a0for SDSPAGE\u00a0(polyacrylamide\u00a0gel electrophoresis).\u00a0Which\u00a0letter\u00a0indicates the\u00a0anode? ", "page_idx": 15}, {"type": "text", "text": "A) A B)\u00a0B ", "page_idx": 15}, {"type": "text", "text": "C)\u00a0Neither ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D)\u00a0Unknown. ", "page_idx": 15}, {"type": "text", "text": "Please\u00a0choose\u00a0an\u00a0answer\u00a0from\u00a0[A,\u00a0B,\u00a0C, D].   \nA: B   \nCaption: You\u00a0are\u00a0an\u00a0expert\u00a0in\u00a0biology. The\u00a0apparatus\u00a0in\u00a0figure\u00a0is\u00a0used\u00a0for   \nSDSPAGE\u00a0(polyacrylamide\u00a0gel   \nelectrophoresis).\u00a0Which\u00a0letter\u00a0indicates the\u00a0anode? ", "page_idx": 15}, {"type": "text", "text": "Landmark ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "tu1oC7zHGW/tmp/9b4210abfef87b8cf5fd5ecf526ded4285753bde9e6be405fdac298905d9c5c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "T: Is\u00a0this\u00a0a\u00a0photo\u00a0of\u00a0Great\u00a0Palace Mosaic\u00a0Museum?\u00a0Please\u00a0answer\u00a0yes or\u00a0no.   \nA: No ", "page_idx": 15}, {"type": "text", "text": "V: In\u00a0which\u00a0city\u00a0can\u00a0you\u00a0find\u00a0the landmark\u00a0shown\u00a0in\u00a0the\u00a0picture? Answer\u00a0within\u00a02\u00a0words. A: Utrecht ", "page_idx": 15}, {"type": "text", "text": "C: Which\u00a0of\u00a0the\u00a0following\u00a0landmarks is\u00a0pictured\u00a0in\u00a0this\u00a0photo?   \nA)\u00a0The\u00a0Eiffel\u00a0Tower\u00a0in\u00a0Paris,\u00a0France B)\u00a0The\u00a0Leaning\u00a0Tower\u00a0of\u00a0Pisa\u00a0in\u00a0Pisa, Italy   \nC)\u00a0The\u00a0Dom\u00a0Tower\u00a0in\u00a0Utrecht,   \nNetherlands   \nD)\u00a0Big\u00a0Ben\u00a0in\u00a0London,\u00a0England.   \nChoose\u00a0one\u00a0from\u00a0the\u00a0four\u00a0letters\u00a0[A,\u00a0B, C,\u00a0D].   \nA: C   \nCaption: You\u00a0are\u00a0an\u00a0expert\u00a0in   \nlandmark.\u00a0You\u00a0need\u00a0to\u00a0describe\u00a0this picture\u00a0with\u00a0accurate\u00a0location   \ninformation. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope, including a benchmark, three main analyses and a method to improve the caption of LVLMs. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We point out our limitations of the work in Appendix C. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our paper releases a dataset and evaluation method, and the experimental results are reproducible. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We will release the dataset and evaluation code after the paper is accepted. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Our paper only includes LVLMs evaluation and specifies all the test details. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We investigate the robustness of our results in Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The evaluation in our paper only needs an A100-80GB GPU. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts of the work in Appendix D. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper re-packages the public datasets, and provides both the original license and the license of the derived asset. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper introduces a new benchmark, and we created an anonymized URL in Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]