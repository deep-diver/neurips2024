[{"type": "text", "text": "BERTs are Generative In-Context Learners ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "David Samuel Language Technology Group University of Oslo davisamu@uio.no ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While in-context learning is commonly associated with causal language models, such as GPT, we demonstrate that this capability also \u2018emerges\u2019 in masked language models. Through an embarrassingly simple inference technique, we enable an existing masked model, DeBERTa, to perform generative tasks without additional training or architectural changes. Our evaluation reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. These complementary strengths suggest that the field\u2019s focus on causal models for in-context learning may be limiting \u2013 both architectures can develop these capabilities, but with distinct advantages; pointing toward promising hybrid approaches that combine the strengths of both objectives. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Masked language models used to dominate the field of natural language processing due to their adaptability across diverse tasks and their superior performance compared to causal language models (Radford et al., 2018; Devlin et al., 2019). Between 2018 and 2020, the field witnessed a surge in the development of these models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020, inter alia). However, the field dramatically shifted with GPT-3 and its introduction of in-context learning \u2013 the ability to infer and perform tasks from prompts and examples without any finetuning (Brown et al., 2020). This capability eliminated the need for task-specific training data and deep-learning expertise, making such models far more practical for real-world applications. This perceived advantage led many researchers and practitioners to abandon masked language models in favor of GPT-style architectures. ", "page_idx": 0}, {"type": "image", "img_path": "BCA9NMZkLS/tmp/81cd40a930bc26940a941c058122c1041e33907b370b6407d25a0fb207c5b79d.jpg", "img_caption": ["Figure 1: The average 1-shot performance across four groups of NLP tasks We compare the scaling abilities of DeBERTa (four sizes in red) with GPT-3 (eight sizes in blue). Even though these models rely on different training objectives, they scale in a similar log-linear manner overall. Yet, on a task-by-task basis, the pretraining methods lead to substantial differences between them. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Previous studies of \u2018emergent\u2019 in-context learning abilities have focused almost exclusively on causal language models, creating a widespread assumption that this capability is unique to them (Saunshi et al., 2021; Olsson et al., 2022; Wei et al., 2022; Wang et al., 2023, inter alia). In this paper, we challenge this assumption by demonstrating that in-context learning can emerge in masked language models as well. In-context learning is a more general phenomenon and should not be studied with a singular pretraining objective in mind. Moreover, the assumed inability of masked language models to perform (generative) in-context learning has rendered them outdated \u2013 as explicitly noted by Tay et al. (2023): \u201cBERT-style models are very restricted in their generative capabilities. Because of the cumbersomeness of task-specific classification heads, we strongly do not recommend using this class of autoencoding models moving forward and consider them somewhat deprecated.\u201d ", "page_idx": 1}, {"type": "text", "text": "In this paper, we challenge these prevailing assumptions about masked language models (MLMs). We present empirical evidence showing that DeBERTa, an MLM released just one month after GPT-3, is equally adept at in-context learning. Our findings suggest that the capacity for in-context learning is not tied to the training objective, but can be achieved across different types of language models. To our surprise, we found that DeBERTa does not simply mimic the performance of GPT-3 \u2013 the two model behave very differently \u2013 DeBERTa is clearly much better on tasks such as language understanding, and, on the other hand, much worse on tasks such as closed-book question answering. This suggests that masked and causal language modeling are two complementary training objectives and that there is a great potential for a training method that combines the strengths of both objectives. Finally, scaling (performance improvement with increased size of pretrained language models) is a crucial feature of modern language models; we demonstrate that MLMs do scale on in-context learning (Figure 1). ", "page_idx": 1}, {"type": "text", "text": "We introduce a simple inference technique that transforms an MLM into a generative model without any further training. Using publicly available DeBERTa checkpoints, we show that the MLM training objective not only provides a versatile way of encoding text, but is also competitive in text generation and text completion ranking. This claim is tested by following the same evaluation suite as GPT-3, speculating on an \u2018alternative reality\u2019 in which a masked language model is the first model reported to achieve the so-called \u2018emergent\u2019 in-context learning abilities. While other masked language models could potentially demonstrate similar capabilities, we deliberately target DeBERTa because of its large size and its length-generalization abilities. Ultimately, our goal is to demonstrate that MLMs can perform in-context learning and that they can be surprisingly good at doing so. ", "page_idx": 1}, {"type": "text", "text": "Outline First, Section 2 (Method) describes the inference methods used to evaluate the in-context learning abilities of an off-the-shelf masked language model. Then Section 3 (DeBERTa) describes the details of the particular model used in this study. Section 4 (Evaluation) details the evaluation setup and compares DeBERTa with GPT-3. Finally, Section 5 (Related work) talks about other relevant work within this topic, and the paper concludes with Section 6 (Conclusion). ", "page_idx": 1}, {"type": "image", "img_path": "BCA9NMZkLS/tmp/aadb4bac2c7a67a337f9a8744f82c5b450128184d24afca3681d0cd012e2d148.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 2: Illustration of the proposed methods for using a masked language model for text generation and text ranking We show how to adapt a masked language model for in-contextlearning tasks through simple input reformatting, requiring no additional training. LEFT: Text generation is achieved by 1) appending [MASK] tokens to the input prompt, 2) predicting the next token for the first mask, and 3) iteratively appending new masks and predicting tokens. RIGHT: A similar approach is used to retrieve a pseudo-log-likelihood score of a text sequence that can be used to rank multiple sequences by their individual likelihoods. Both methods maintain the model\u2019s original architecture while enabling new capabilities through careful input formatting. ", "page_idx": 1}, {"type": "text", "text": "2 Method: text generation and ranking with masked language models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of this article is to reuse an existing pretrained masked language model for (generative) in-context learning. We achieve this without any additional training or finetuning, our method only slightly changes the sequence of input tokens, as illustrated in Figure 2. There are two methods used to solve tasks with in-context learning: text generation where the model completes a given prompt (e.g. for translation) and ranking where the model chooses an answer from several options (e.g. for multiple choice questions). ", "page_idx": 2}, {"type": "text", "text": "2.1 Text generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Masked language models are trained on semi-supervised fill-in-the-blanks tasks and so they cannot be used to generate straight out of the box. One possibility is to interpret these models as Markov random fields and produce text by Gibbs sampling (Wang and Cho, 2019). However, a simpler and more consistent way to produce text is to do the familiar left-to-right autoregressive generation \u2013 we could place a [MASK] token next to a text prompt and let the model generate next token by unmasking the appended token \u2013 then, when we repeat this process in a loop, we can generate text in the same way as causal language models (and apply the same advanced generation techniques). ", "page_idx": 2}, {"type": "text", "text": "This straightforward inference scheme would be enough if the pretraining process were designed with this use case in mind. However, since our goal is to repurpose an existing masked language model, we have to complicate the method with two modifications that are also illustrated in Figure 2: ", "page_idx": 2}, {"type": "text", "text": "1. Masked language models are typically trained with a special end-of-sequence [SEP] token. This token is always present during pretraining and so we also have to include it as the last token during inference. 2. However, the addition of this end-of-sequence token creates a problem \u2013 it raises the probability that the masked token should end the sequence (for example with a full stop). Thus, in order to obtain a less restricted continuation, we include additional [MASK] tokens to pad the space in front of the end-of-sequence token. Specifically, we use two additional masks for the DeBERTa models.1 This decision is later ablated in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "In the end, this approach gives a probability distribution over the next token prediction, thus we can use any existing method for searching or sampling an output sequence. We follow GPT-3 and use beam search with four candidate beams for all generative tasks. ", "page_idx": 2}, {"type": "text", "text": "Limitations While this method works with the same quadratic time complexity (in sequence length), it is slower in practice because it is not possible to cache the intermediate self-attention key and value vectors. Instead, these have to be recomputed every step due to the bidirectional nature of the model. While our current implementation prioritizes demonstrating the core capability over optimization, several promising approaches could address these computational limitations in future work. For example, using prefix language modeling or selectively updating hidden vectors could significantly improve efficiency. We leave these optimizations for future work to maintain focus on establishing the fundamental ability of MLMs to generate text. ", "page_idx": 2}, {"type": "text", "text": "2.2 Ranking ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Many of the existing tasks for evaluating LLMs can be formulated as classification tasks where models have to select the correct answer from a number of different options. Brown et al. (2020) rank the candidate completions based on their estimated conditional log-likelihood, which can be computed exactly by the chain rule (where $w_{0}\\oplus w_{1}\\dots w_{k}$ is a completion of a prompt $c$ ): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\log\\mathbb{P}(w_{0}\\oplus w_{1}\\dots w_{k}\\,|\\,c)=\\sum_{i=0}^{k}\\log\\mathbb{P}(w_{i}\\,|\\,c\\oplus w_{0}\\dots w_{i-1})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While this equation matches the training objective of causal language models, it is not suitable for masked language models because they are not trained to estimate $\\mathbb{P}\\overline{{(w_{i}\\,|\\,c\\oplus w_{0}\\,.\\,.\\,.\\,w_{i-1})}}$ . Instead, ", "page_idx": 2}, {"type": "text", "text": "Wang and Cho (2019) proposed to modify Equation (1) to make it more appropriate for BERT-like models. Salazar et al. (2020) then empirically showed that the resulting pseudo-log-likelihood (PLL) score can be used to accurately rank text sequences by their likelihood. Specifically, the PLL score is approximately proportional to the conditional probability of a text sequence and is computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log\\mathbb{P}(w_{0}\\oplus w_{1}\\dots w_{k}\\,|\\,c)\\stackrel{k}{\\sim}\\sum_{i=0}^{k}\\log\\mathbb{P}(w_{i}\\,|\\,c\\oplus w_{0}\\dots w_{i-1}\\oplus[\\mathsf{M}\\mathsf{A}\\mathsf{S}\\mathsf{K}]\\oplus w_{i+1}\\dots w_{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, this approximation gets very inaccurate when there are strong local dependencies between tokens. As a counterexample, the estimated likelihood of the multi-token word \u2018supercalifragilisticexpialidocious\u2019 is seven orders of magnitude higher than that of the single-token word \u2018super\u2019, which is clearly an incorrect estimation of the relative frequencies of these words.2 ", "page_idx": 3}, {"type": "text", "text": "We improve on this behavior by interpolating between the mathematically correct unidirectional derivation in Equation (1) and the bidirectional approximation in Equation (2). Our approach is to simply mask two additional tokens in the right context to reduce the effect of local dependencies while still taking into account the global bidirectional context. This process is illustrated in Figure 2. We conduct an ablation study of this approach in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Limitations Even though Equations (1) and (2) look similar, the later sum is substantially more compute intensive when calculated with a transformer architecture \u2013 for a token sequence of length $k$ , the first equation can be computed with passing a single sequence through a language model, while the second equation needs $k$ sequences to be processed. However, Salazar et al. (2020) showed that the PLL score can be accurately estimated in a single pass after a short self-supervised finetuning. ", "page_idx": 3}, {"type": "text", "text": "2.3 Length generalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A potentially limiting factor of using BERT-like models is that they are typically pretrained on shorter sequences than causal language models \u2013 arguably because the training of modern causal models is already optimized for in-context learning, which requires processing of long few-shot prompts. DeBERTa is not an exception to such pretraining; it was only trained with a relatively short maximum sequence length of 512 tokens (He et al., 2021). Fortunately, the architecture of DeBERTa can easily process much longer sequences than seen during training due to its use of relative positional embeddings with logarithmic buckets (Raffel et al., 2020). ", "page_idx": 3}, {"type": "image", "img_path": "BCA9NMZkLS/tmp/8ac68166d0b893ce3cdf1c24ada16f202ada8edd4773218ebfb20a1ab4b4f0c1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Length generalization measured with a \u2018needle in a haystack\u2019 benchmark The $x$ -axis indicates the total size of the \u2018haystack\u2019 and the $y$ -axis indicates the position of the \u2018needle\u2019; the values show the average exact-match accuracy for a particular configuration. Unfortunately, GPT-3 is a closed-source model and the original version is not accessible, so we use an open-source replication of GPT-3, OPT by Zhang et al. (2022), which should perform similarly on this task because of the the same transformer architecture as GPT-3. In particular, it uses absolute positional encoding, which strictly limits any model from generalizing to longer inputs than trained on. ", "page_idx": 3}, {"type": "text", "text": "We measure the extent to which DeBERTa generalizes to longer sequences with the \u2018needle in a haystack\u2019 test from RULER (Hsieh et al., 2024). Specifically, in our formulation of this task, a random 6-digit number (needle) is hidden in a long collection of essays (haystack). We then measure the exact-match accuracy of retrieving the hidden number given two variables: the total sequence length and the position of the needle in the haystack (more details about the evaluation setup are given in Appendix E.1). ", "page_idx": 4}, {"type": "text", "text": "The results in Figure 3 demonstrate that DeBERTa generalizes to sequences well beyond its training length, which is enabled by its relative positional encoding. For comparison, we also show results from OPT (Zhang et al., 2022), which uses absolute positional encoding like GPT-3. As expected from models using absolute positional encoding, performance drops sharply beyond the training length. This comparison highlights the importance of positional encoding choice for length generalization, independent of whether the model is masked or causal. In practice, this observation means that DeBERTa should be able to handle as many task demonstrations as models trained with longer sequences. ", "page_idx": 4}, {"type": "text", "text": "3 DeBERTa family of language models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This study uses the largest openly available English masked language model, DeBERTa with 1.5 billion parametrs, and its smaller configurations \u2013 0.1B, 0.4B and 0.9B (He et al., 2021). DeBERTa is an improved version of a BERT language model (Devlin et al., 2019) that uses an advanced attention mechanism with relative positional embeddings \u2013 apart from being trained on a larger corpus and with a larger number of training steps. ", "page_idx": 4}, {"type": "text", "text": "Training corpus Compared to GPT-3 and modern large language models, DeBERTa was pretrained on a relatively small and clean text corpus \u2013 totalling 78GB of data after deduplication, the corpus is comprised of the English Wikipedia (12GB), BookCorpus (6GB; Zhu et al., 2015), OpenWebText (38GB; Gokaslan and Cohen, 2019), and STORIES (31GB; Trinh and Le, 2019). This is almost an order of magnitude less data than what was used to pretrain GPT-3. Notably, our strong results \u2013 despite this data disparity \u2013 could suggest that masked language models are more data-efficient than causal models for developing in-context learning capabilities. This claim would however need to be evaluated with a comprehensive study. In comparison, GPT-3 uses 570GB of flitered CommonCrawl, WebText2 (roughly 26GB), two web-scraped book corpora (roughly 17GB and 76GB), and the English Wikipedia (roughly 4GB, estimated from Brown et al. (2020)). ", "page_idx": 4}, {"type": "text", "text": "Total training compute Interestingly, even though DeBERTa uses a substantially smaller training corpus, it is trained on more than three times more tokens than GPT-3 (1 trillion compared to 300 billion).3 However, the loss is computed only on $15\\%$ of tokens (150 billion) and it is not clear what would be the effective number of tokens used for pretraining. Nevertheless, the total compute used for training depends on the number of input tokens and it is roughly $8.0\\cdot10^{21}$ FLOPs for the 1.5B DeBERTa, and $2.4\\cdot10^{21}$ FLOPs for the 1.3B GPT-3. ", "page_idx": 4}, {"type": "text", "text": "Causal conversion for HuggingFace We have converted the officially available DeBERTa checkpoint into a HuggingFace (Wolf et al., 2020) implementation of AutoModelForCausalLM (following the method in Section 2.1), and released it openly at https://hf.co/ltg/deberta-xxlarge-fixed. The weights of this model are exactly the same as the official release from microsoft/debertav2-xxlarge, but we have fixed some bugs found in the original modeling script in addition to implementing the text generation abilities.4 Similarly, we have also converted the smaller DeBERTa models and released them as ltg/deberta-base-fixed, ltg/deberta-large-fixed, and ltg/deberta-xlarge-fixed. ", "page_idx": 4}, {"type": "text", "text": "4 Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As our goal is to compare two language models released around the same time in 2020 \u2013 GPT-3 and DeBERTa \u2013 we replicate the evaluation setup used for GPT-3 (Brown et al., 2020) and apply it to the latter model. This also means that we follow GPT-3 and divide the tasks into generative ones (such as machine translation) and into classification tasks (such as BoolQ) \u2013 the first group uses the method described in Section 2.1 and the second type of task uses the ranking described in Section 2.2. Generation is performed with beam search (4 candidate beams), and ranking uses the modified PLL scores (and the normalized unconditional probability of completions P(coP(mcpolemtipolnet i|o an n|s wcoern tceoxntt)ext) for ARC and OpenBookQA), again replicating the choices for GPT-3). We also use the exact same prompt templates, with the exception of the machine translation task \u2013 its template did not produce any meaningful output, and so we decided to use the simple prompt template from Garcia et al. (2023) instead. More details on the evaluation setup can be found in Appendix E. Note that using prompts optimized for GPT-3 is slightly unfair to all other models, as prompting has a strong influence on performance (Gonen et al., 2023), but we believe that it makes the results more convincing than if we were to do extensive prompt engineering. ", "page_idx": 5}, {"type": "text", "text": "To show the strengths and weaknesses of DeBERTa in (generative) in-context learning, we evaluate it on four groups of tasks and compare it to the results from Brown et al. (2020). The four groups are language understanding (SuperGLUE), language modeling (text completion and Winograd-like tasks), machine translation, and question answering (closed-book question answering and commonsense reasoning). We detail each of these groups of tasks below. ", "page_idx": 5}, {"type": "text", "text": "Before looking into the details of each group, we show the overall aggregated scores for each group in Figure 1 and Figure 4. The first figure shows how the performance of both models scales with their size, while the latter figure compares the in-context learning abilities of the two language models. We also provide a qualitative evaluation of text generation in Appendix A and full results in Appendix F. ", "page_idx": 5}, {"type": "image", "img_path": "BCA9NMZkLS/tmp/0921652270999111b68bd0197bfc609d253ab4958940bf65c339c2282eb7ed4f.jpg", "img_caption": ["Figure 4: The performance improvement with increased number of in-context examples We compare the in-context learning ability of 1.5B DeBERTa (in red) with 1.3B GPT-3 (in blue) using prompts without any completed examples (0-shot), prompts with a single randomly sampled gold sample (1-shot), and prompts with few examples (4 \u2013 64 examples, depending on the task). This figure demonstrates that a masked language model behaves similarly to a causal language model in the in-context learning regime. More detailed few-shot evaluation is in Figure 5. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Language understanding (SuperGLUE) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use SuperGLUE (Wang et al., 2019) as a popular collection of standard NLP tasks, allowing us to evaluate the performance on different aspects of natural language understanding. ", "page_idx": 5}, {"type": "text", "text": "In total, this benchmark consists of eight datasets, selected to be difficult for the contemporary (finetuned) language models. The Boolean Questions dataset is a yes/no reading comprehension dataset evaluated with accuracy (BoolQ; Clark et al., 2019); CommitmentBank is a three-class textual entailment dataset evaluated with accuracy and $\\mathrm{F_{1}}$ score, where the multi-class $\\mathrm{F_{1}}$ is computed as the unweighted average of the $\\mathrm{F_{1}}$ per class (CB; de Marneffe et al., 2019); the Choice of Plausible Alternatives dataset is a causal reasoning task evaluated with accuracy (COPA; Roemmele et al., ", "page_idx": 5}, {"type": "text", "text": "2011); Multi-Sentence Reading Comprehension is a multiple choice dataset, evaluated with exactmatch (of all answers per question) accuracy and $\\operatorname{F}_{1\\alpha}$ score computed over all flattened answers (MultiRC; Khashabi et al., 2018); Reading Comprehension with Commonsense Reasoning Dataset is another reading comprehension dataset, it is evaluated with the exact-match accuracy and token-level $\\mathrm{F_{1}}$ score (ReCoRD; Zhang et al., 2018); the collection of Recognizing Textual Entailment datasets is a textual entailment task evaluated with accuracy (RTE; Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007); the Word-in-Context dataset is a word sense disambiguation dataset evaluated with accuracy (WiC; Pilehvar and Camacho-Collados, 2019); and finally, the Winograd Schema Challenge evaluates coreference resolution capabilities (WSC; Levesque et al., 2012). ", "page_idx": 6}, {"type": "text", "text": "Results We show the resulting scores of evaluation with the same prompts as GPT-3 in Table 1 and Appendix D. DeBERTa clearly outperforms its contemporary and scales much more favorably than the family of GPT models (Figure 1). Interestingly, the average performance of the 1.5B DeBERTa gets close to the reported performance of the largest 175B GPT-3 (68.4 vs. 68.9, 1-shot). However, this average score is still far from the performance of a finetuned DeBERTa, which is more than 20 percentage points higher (He et al., 2021); the average few-shot performance of DeBERTa is slightly better than a finetuned BERT-large (Devlin et al., 2019; Wang et al., 2019). ", "page_idx": 6}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/23a6a2be33687b50b2b480167f37e86da0a03f913aafeedeafd808d7f8ffb9da.jpg", "table_caption": ["Table 1: Natural language understanding results All results in this table are evaluated with accuracy (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. The average score is calculated over averaged task scores (in case a task uses more than one metric). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Language modeling, Winograd-style and text completion tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The tasks in this category are defined in a familiar language-modeling form and focus on particularly difficult cases of language modeling, cases that involve commonsense reasoning, language understanding, and intricate coreference resolution. ", "page_idx": 6}, {"type": "text", "text": "In this group, we consider four NLP tasks: HellaSwag is a text completion task where a language model has to choose the most appropriate multi-word ending, the examples are adversarially flitered to be difficult for language models but easy for humans (Zellers et al., 2019). StoryCloze consists of five-sentence-long stories, the goal is to select the best final sentence based on commonsense knowledge (Mostafazadeh et al., 2016). Winograd is a language-modeling formulation of the WSC task from SuperGLUE (Levesque et al., 2012). WinoGrande is similar to Winograd in its form, but is adversarially mined to contain more difficult examples of coreference resolution (Sakaguchi et al., 2020). We do not include the LAMBADA benchmark (Paperno et al., 2016) here because Brown et al. (2020) used an unknown preprocessing step that disallows direct comparison with GPT-3. ", "page_idx": 6}, {"type": "text", "text": "Results We show the in-context-learning results from this group in Table 2, where we evaluate the 1.5B DeBERTa with a comparable GPT-3 model. The scores showcase consistently stronger performance of the masked language model, similarly to the language understanding tasks. One difference to those tasks is the rate of scaling, which appears to be similar between the two types of language models (Figure 1). ", "page_idx": 6}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/4e317d85ac54667eb9d8c2a03c49dded3d0147a16bf593e5555eb4d44949a2b7.jpg", "table_caption": ["Table 2: Results of text completion, language modeling and Winograd-style tasks All tasks are measured with accuracy, we show the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Translation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Translation is a useful benchmark for language models as it evaluates their ability to understand text in one language and produce fluent text in another language. Even though the performance on the translation tasks is arguably very dependent on the composition of training data (especially when we are concerned with monolingual English models), we include translation to demonstrate the generative performance of masked language models. ", "page_idx": 7}, {"type": "text", "text": "To directly compare DeBERTa with GPT-3, we use the same SacreBLEU metric (Post, 2018) and the same bitexts. Thus, even though there are more recent (and arguably more thought-out) datasets, we use the French\u2013English pair from the outdated 2014 shared task at the Workshop on Statistical Machine Translation (WMT14; Bojar et al., 2014), and also the Romanian\u2013English and German\u2013English pairs from the WMT16 workshop (Bojar et al., 2016). Our approach differs only in using a different prompt template, as we had to opt for the prompt from Garcia et al. (2023) to get consistent translations: \"{\\$source_language}: {\\$source_text}\\\\n {\\$target_language}: {\\$target_text}\". ", "page_idx": 7}, {"type": "text", "text": "Results The SacreBLEU scores on each language pair are given in Table 3. Unlike in the previous two task groups, the tables have turned, and the causal language model clearly outperforms the masked model in all comparisons. We believe that the subpar performance of DeBERTa can be (at least) in part explained by its relatively small and clean monolingual training corpus (Section 3), because the performance on this task is highly dependent on the presence of multilingual data in the corpus (Lin et al., 2022). The rate of improved translation performance with larger scale appears to be similar between the two models (Figure 1). ", "page_idx": 7}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/11839fd4b9be760d1634b42511e9b87d7841b741365abf6f8e4dc791bf257c6a.jpg", "table_caption": ["Table 3: Machine translation results We report SacreBLEU scores (Post, 2018) with signature BLEU+case.mixed $^+$ numrefs. $^{1+}$ smooth.exp+tok.intl $^+$ version.1.2.20 (higher is better). The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarlysized GPT-3 model, the best results are boldfaced. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Closed-book question answering and commonsense reasoning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "An important quality of modern-day large language models is their ability to learn and retrieve world knowledge, and to have a degree of common sense. The final group of tasks attempts to evaluate these two qualities. ", "page_idx": 8}, {"type": "text", "text": "This category of tasks consists of seven datasets in total: Natural Questions (NQs; Kwiatkowski et al., 2019) and Web Questions (WebQs; Berant et al., 2013) are closed-book question-answering datasets sourced from natural web queries; while the original datasets are accompanied by relevant articles that contain the answer, we only ask models a question and then evaluate the exact-match accuracy of their answers. TriviaQA is a very similar dataset, but based on online quizzes (Joshi et al., 2017). The next four tasks fall more into a subcategory of commonsense reasoning datasets. The Physical Interaction: Question Answering dataset evaluates how well a language model is grounded in the real physical world (PIQA; Bisk et al., 2020). The AI2 Reasoning Challenge is a dataset sourced from grade-school science questions that evaluates knowledge and reasoning abilities; this task is divided into ARC-Easy and ARC-Challenge splits, based on their difficulty (Clark et al., 2018). Finally, OpenBookQA evaluates the understanding of common knowledge (Mihaylov et al., 2018). ", "page_idx": 8}, {"type": "text", "text": "Results The question-answering performance is given in Table 4. Apparently, the results of DeBERTa are substantially worse on closed-book question answering compared to GPT-3. We believe that this highlights a more general disadvantage of the MLM training objective \u2013 the model can often retrieve world knowledge from the rich bidirectional context during training, not needing to store it in its learned weights; similar effect has been shown in retrieval-augmented language models (Samuel et al., 2024). However, the commonsense reasoning abilities are comparable between the two models. The scaling behavior is again similar between the two models (Figure 1). The same is also true about the improvement when given more in-context examples, which are especially important for the tasks evaluated with exact-match accuracy, where the goal is not only to answer correctly but also to match the expected style and form of the gold answers (Figure 4). ", "page_idx": 8}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/d03ff8a19dc5fcd4d91728bc7db2948506b051bded72df4cb2224fe89f11ad70.jpg", "table_caption": ["Table 4: Closed-book question answering and commonsense reasoning The first three tasks are measured with the exact-match accuracy and the rest is measured with classification accuracy. The table shows the performance of the largest available DeBERTa (1.4 billion parameters) and of a similarly-sized GPT-3 model, the best results are boldfaced. A detailed description of the evaluation method is given in Appendix E, full results are in Appendix F. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Few-shot finetuning with masked language models While our work demonstrates the emergence of in-context learning in masked language models, prior research has explored different approaches to few-shot learning with these architectures. The dominant paradigm has been few-shot finetuning, where the model\u2019s weights are updated using a small number of examples. Studies by Schick and Sch\u00fctze (2021), Gao et al. (2021), and Xia et al. (2022) showed promising results with this approach. However, these methods require additional training steps with a complicated training objective, making them more complex to implement compared to the simple prompting-based in-context learning demonstrated in our work. Despite the generally lower performance of in-context learning compared to few-shot finetuning (Liu et al., 2022), its simplicity and immediacy have made it the preferred choice in many practical applications. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Other large masked language models Our choice of DeBERTa for this study was motivated by its unique combination of size and capability to handle extended context lengths. While larger masked language models exist, such as Megatron BERT with 3.9 billion parameters (unfortunately not publicly available; Shoeybi et al., 2019) and XLM-RoBERTa with 10.7 billion parameters (Goyal et al., 2021), they have limitations that make them less suitable for studying in-context learning. Megatron BERT lacks mechanisms for length generalization, which is crucial for processing long prompts with multiple examples, while XLM-RoBERTa\u2019s multilingual nature and restricted sequence length of 512 tokens would confound our analysis. DeBERTa\u2019s architecture, particularly its relative positional embeddings, makes it an ideal candidate for exploring how masked language models scale with in-context learning. ", "page_idx": 9}, {"type": "text", "text": "Hybrid masked-causal models Our empirical findings, particularly the complementary strengths of masked and causal models demonstrated in Section 4, suggest significant potential in combining these approaches. Several architectures have already explored this direction, even if inadvertently: T5 (Raffel et al., 2020), BART (Lewis et al., 2020) and GLM (Du et al., 2022) introduced autoregressive fill-in-the-blank objectives; CM3 developed a causal-mask approach (Aghajanyan et al., 2022); and PrefixLM implemented a partially bidirectional causal model (Dong et al., 2019; Raffel et al., 2020). These efforts align with our observations about the distinct advantages of masked and causal objectives. The recent work by Ding et al. (2024) provides theoretical support for this direction, demonstrating that prefix language models, which combine aspects of both architectures, are particularly well-suited for in-context learning. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper demonstrates that masked language models can be capable in-context learners. We show that these models \u2013 often considered deprecated and limited only to finetuning \u2013 can match and sometimes even exceed the performance of their causal counterparts in this domain. Our evaluation reveals that masked and causal models exhibit remarkably similar characteristics in terms of overall performance, scaling behavior, and improvements with additional in-context demonstrations. Most notably, we validate these capabilities using DeBERTa without any architectural modifications or additional training. We achieve this through carefully designed inference methods that unlock the model\u2019s latent generative abilities. ", "page_idx": 9}, {"type": "text", "text": "Our findings point to several promising directions for future research. First, DeBERTa\u2019s performance could likely be enhanced through straightforward improvements such as training on larger and more diverse corpora, increasing model scale, and extending the pretraining context length. More fundamentally, the complementary strengths we observed between masked and causal models \u2013 where each architecture excels in different tasks \u2013 suggest an exciting opportunity to develop hybrid approaches that combine the best of both paradigms. Rather than viewing these as competing architectures, future work might explore how to synthesize their distinct advantages into more capable and versatile language models. ", "page_idx": 9}, {"type": "text", "text": "These results argue for a broader reconsideration of how we approach language model architecture and training. The field\u2019s recent focus on causal models, while productive, may have prematurely discounted the potential of alternative approaches that are not limited to unidirectional text processing. Our work demonstrates that the path forward likely involves embracing architectural diversity rather than converging on a single dominant paradigm. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "I am deeply grateful to Lilja \u00d8vrelid, Andrey Kutuzov, and Erik Velldal for providing insightful feedback, for their never-ending encouragement and support, and for making Oslo a warm and welcoming place. ", "page_idx": 10}, {"type": "text", "text": "This work is fully funded by the University of Oslo. The computations were performed on resources provided through Sigma2 \u2013 the national research infrastructure provider for high-performance computing and large-scale data storage in Norway. We acknowledge Norway and Sigma2 for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through project 5000144. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. 2022. CM3: A causal masked multimodal model of the internet. Preprint, arXiv:2201.07520. ", "page_idx": 10}, {"type": "text", "text": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. 2006. The second PASCAL recognising textual entailment challenge. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment. ", "page_idx": 10}, {"type": "text", "text": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA. Association for Computational Linguistics. ", "page_idx": 10}, {"type": "text", "text": "Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7432\u20137439. ", "page_idx": 10}, {"type": "text", "text": "Ond\u02c7rej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12\u201358, Baltimore, Maryland, USA. Association for Computational Linguistics. ", "page_idx": 10}, {"type": "text", "text": "Ond\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131\u2013198, Berlin, Germany. Association for Computational Linguistics. ", "page_idx": 10}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc. ", "page_idx": 10}, {"type": "text", "text": "Susan Carey and Elsa Bartlett. 1978. Acquiring a single new word. Proceedings of the Stanford Child Language Conference, 15:17\u201329. ", "page_idx": 10}, {"type": "text", "text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota. Association for Computational Linguistics. ", "page_idx": 10}, {"type": "text", "text": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. Preprint, arXiv:1803.05457. ", "page_idx": 10}, {"type": "text", "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177\u2013190, Berlin, Heidelberg. Springer Berlin Heidelberg. ", "page_idx": 10}, {"type": "text", "text": "Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The CommitmentBank: Investigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107\u2013124. ", "page_idx": 11}, {"type": "text", "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, and Radu Soricut. 2024. CausalLM is not optimal for in-context learning. In The Twelfth International Conference on Learning Representations. ", "page_idx": 11}, {"type": "text", "text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. ", "page_idx": 11}, {"type": "text", "text": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335, Dublin, Ireland. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816\u20133830, Online. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of few-shot learning for machine translation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10867\u201310878. PMLR. ", "page_idx": 11}, {"type": "text", "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1\u20139, Prague. Association for Computational Linguistics. ", "page_idx": 11}, {"type": "text", "text": "Aaron Gokaslan and Vanya Cohen. 2019. OpenWebText corpus. ", "page_idx": 11}, {"type": "text", "text": "Hila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and Luke Zettlemoyer. 2023. Demystifying prompts in language models via perplexity estimation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10136\u201310148, Singapore. Association for Computational Linguistics.   \nNaman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale transformers for multilingual masked language modeling. CoRR, abs/2105.00572.   \nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTa: Decoding-enhanced BERT with disentangled attention. In International Conference on Learning Representations.   \nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.   \nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. RULER: What\u2019s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654.   \nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada. Association for Computational Linguistics.   \nCarina Kauf and Anna Ivanova. 2023. A better way to do masked language model scoring. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 925\u2013935, Toronto, Canada. Association for Computational Linguistics.   \nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252\u2013262, New Orleans, Louisiana. Association for Computational Linguistics.   \nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466.   \nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In International Conference on Learning Representations.   \nHector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The Winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR\u201912, page 552\u2013561. AAAI Press.   \nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.   \nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019\u20139052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.   \nHaokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Advances in Neural Information Processing Systems.   \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. Preprint, arXiv:1907.11692.   \nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381\u20132391, Brussels, Belgium. Association for Computational Linguistics.   \nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849, San Diego, California. Association for Computational Linguistics.   \nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023. Scaling data-constrained language models. In Thirty-seventh Conference on Neural Information Processing Systems.   \nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learning and induction heads. Preprint, arXiv:2209.11895.   \nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534, Berlin, Germany. Association for Computational Linguistics.   \nMohammad Taher Pilehvar and Jose Camacho-Collados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267\u20131273, Minneapolis, Minnesota. Association for Computational Linguistics.   \nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013191, Brussels, Belgium. Association for Computational Linguistics.   \nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).   \nMelissa Roemmele, Cosmin Bejan, and Andrew Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. AAAI Spring Symposium - Technical Report.   \nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. WinoGrande: An adversarial Winograd Schema Challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8732\u20138740.   \nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. 2020. Masked language model scoring. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699\u20132712, Online. Association for Computational Linguistics.   \nDavid Samuel, Lucas Georges Gabriel Charpentier, and Sondre Wold. 2024. More room for language: Investigating the effect of retrieval on language models. Preprint, arXiv:2404.10939.   \nNikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. 2021. A mathematical exploration of why language models help solve downstream tasks. In International Conference on Learning Representations.   \nTimo Schick and Hinrich Sch\u00fctze. 2021. It\u2019s not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339\u20132352, Online. Association for Computational Linguistics.   \nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training multi-billion parameter language models using model parallelism. CoRR, abs/1909.08053.   \nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UL2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations.   \nTrieu H. Trinh and Quoc V. Le. 2019. A simple method for commonsense reasoning. Preprint, arXiv:1806.02847.   \nAlex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a Markov random field language model. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 30\u201336, Minneapolis, Minnesota. Association for Computational Linguistics.   \nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.   \nXinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems.   \nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.   \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.   \nMengzhou Xia, Mikel Artetxe, Jingfei Du, Danqi Chen, and Veselin Stoyanov. 2022. Prompting ELECTRA: Few-shot learning with discriminative pre-trained models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11351\u201311361, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.   \nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791\u20134800, Florence, Italy. Association for Computational Linguistics.   \nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint.   \nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open pre-trained transformer language models. Preprint, arXiv:2205.01068.   \nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 19\u201327. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Examples of text generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To also give a sense of the quality of the text produced by DeBERTa, we include some examples of text generation in this section. We use the exact same example prompts that were used in the GPT-3 paper (Brown et al., 2020), to provide a fair estimate of the generative qualities. All text completions were generated with nucleus sampling (Holtzman et al., 2020).5 We compare the largest DeBERTa 1.5B with OPT 1.3B, an openly available replication of the GPT-3 1.3B (Zhang et al., 2022). ", "page_idx": 15}, {"type": "text", "text": "A.1 Learning and using novel words ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Based on studies in developmental linguistics (Carey and Bartlett, 1978), this task tests the ability to understand and productively use new words; specifically using a word in a sentence after seeing it defined only once. We qualitatively test this ability in a generative one-shot setting, using the prompts provided below \u2013 there, the human-provided prompts are rendered as normal text while the generated completions are rendered in boldface. The prompts are taken from Brown et al. (2020, Section 3.9.5). ", "page_idx": 15}, {"type": "text", "text": "Results Overall, DeBERTa provides more appropriate example sentences than OPT. While the \u2018farduddle\u2019 and \u2018screeg\u2019 sentences from DeBERTa are not very descriptive, the rest of sentences are informative and fitting the word definitions. Note how the model tried to invent a plural inflection of \u2018yalubalu\u2019, the suffix \u2018-a\u2019 is morphologically plausible, but the stem is fumbled, possibly because of subword tokenization. The examples generated by OPT are of lesser quality; it either repeats the definition (in \u2018farduddle\u2019), repeats the one-shot example (in \u2018yalubalu\u2019) or provides an unfitting example (in \u2018screeg\u2019). ", "page_idx": 15}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/c694d1d46459e5d8cda777f7b3c0aa86904e41761508d965dec801d92fcf6137.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/dffa1cee058225736502539fd78a46ff9045c6abb57c8ebcc157caafa1244653.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "English grammar correction is another task used for qualitative evaluation of GPT-3. Provided three demonstrations, the model is given an incorrect English sentence and is tasked to correct it. The prompts are taken from Brown et al. (2020, Section 3.9.6) and are shown below. ", "page_idx": 16}, {"type": "text", "text": "Results While the corrections are not perfect, DeBERTa outputs more sensible completions compared to OPT, similarly to the previous task. A notable correction is in the second example, where DeBERTa tried to improve the understandability, not only grammar. ", "page_idx": 16}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/a9b2681ece852bfbbaed3cb410c17d5d6365e4f3d033e61a0eff44ab9191be80.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Ablation study of text generation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We empirically evaluate different approaches for text generation with DeBERTa language models. In particular, we study how many additional mask tokens should be used during autoregressive generation, and we also compare our approach with previously proposed methods based on Markovchain Monte Carlo sampling (Wang and Cho, 2019). For this analysis, we use German-to-English machine translation as a representative generative task. We evaluate different generation methods using one-shot prompts (following Section 2.1) and the largest 1.5B DeBERTa model. The translation quality is measured with SacreBLEU score, using the same signature as in the main experiments. ", "page_idx": 17}, {"type": "text", "text": "Results The results shown in Table 5 demonstrate that using additional mask tokens during generation substantially improves the performance of DeBERTa. This aligns with our hypothesis that additional masks help to reduce the effect of the end-of-sequence token on the generated text. The results also show that while adding a fourth mask token still marginally improves the performance, the gain is negligible compared to using three mask tokens. ", "page_idx": 17}, {"type": "text", "text": "We also compare our autoregressive approach with the Gibbs sampling method proposed by Wang and Cho (2019). There we relied on the same hyperparameters that are suggested in the official implementation: 500 total iterations, 250 burn-in iterations with top-100 sampling and temperature of 1.0.6 Their sampling-based approach performs substantially worse than autoregressive generation, regardless of the initialization strategy. We noticed that it often produces infinite token repetitions or locally-coherent but globally-disconnected pieces of text. ", "page_idx": 17}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/48e7d14bbbcb408fbdde70e8850203db50a7af8f873f3eb24c546dfabe4bc023.jpg", "table_caption": ["Table 5: Ablation study of different generation methods applied to DeBERTa Evaluated using one-shot setting and the largest DeBERTa 1.5B model on German-to-English translation with SacreBLEU score. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Ablation study of ranking implementation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We mentioned some drawbacks of calculating the pseudo-log-likelihood score (PLL) as per Salazar et al. (2020), and how we mitigate these problems, in Section 2.2. This section supports our decision with a quantitative analysis of different ranking approaches. We test them on the ReCoRD task from SuperGLUE (Zhang et al., 2018; Wang et al., 2019), where the goal is to rank different named entities based on their appropriatness. Since the problem of the original PLL is in estimating likelihoods of long multi-loken expressions, we choose this task to highlight the differences. An example of a prompt-formatted sample from ReCoRD is given below, the possible completions (all of which are named entities) are boldfaced: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "1 Suspended hundreds of feet in the air amid glistening pillars of ice illuminated $\\hookrightarrow$ with ghostly lights from below, this could easily be a computer-generated scene $\\hookrightarrow$ from the latest sci-fi blockbuster movie. But in fact these ethereal photographs $\\hookrightarrow$ were taken in real life, and show extreme sportsman and climber Stephan $\\hookrightarrow$ Siegrist, 43, ascending the Voringsfossen icefall which is part of a gigantic $\\hookrightarrow$ glacier in Eidfjord, Norway. The stunning images were captured by fellow $\\hookrightarrow$ \u2192 mountaineer and photographer Thomas Sanf. While the 500ft frozen waterfall is $\\hookrightarrow$ regularly scaled by climbers during daylight, he said he wanted to capture the $\\hookrightarrow$ beauty of the falls by night.   \n2 - Stunning images captured by photographer Thomas Sanf as climber Stephan Siegrist, $\\hookrightarrow\\;\\;43$ , scaled frozen waterfall   \n3 - The Voringsfossen fall is liquid for most of the year, but in winter freezes into $\\hookrightarrow$ a 500ft cliff favoured by climbers   \n4 - Hundreds of adventurers attempt the climb by day, but very few attempt the ascent $\\hookrightarrow$ at night, as pictured here   \n5 - With bright lights illuminating his efforts from below, Mr {\\$answer:-Stephan $\\hookrightarrow$ Siegrist/Voringsfossen/Eidfjord/Norway/Thomas Sanf} appears to be on the set of $\\hookrightarrow$ a sci-fi movie ", "page_idx": 18}, {"type": "text", "text": "Previous work After running the initial experiments, we were informed about the related work by Kauf and Ivanova (2023). Their study addresses a similar problem of the naive pseudo-log-likelihood scoring as this paper, but they do not target strongly correlated contiguous tokens in general, they focus on words that are split into multiple tokens \u2013 as they observed that PLL overestimates the likelihood of those sequences. We include their two proposed solutions in the comparison of different ranking methods. ", "page_idx": 18}, {"type": "text", "text": "Results In Table 6, we compare the original implementation of PLL by Salazar et al. (2020) that only masks the target subword; our approach that also masks next two subwords; other two alternatives that mask two or four subwords in total; the PLL-word-l2r and PLL-whole-word scoring functions by Kauf and Ivanova (2023); and the exact unidirectional computation of log-likelihood (using the same input formatting as for generation). The additional masks clearly help to make better estimates while the exact computation seems to not be appropriate for inherently bidirectional models. ", "page_idx": 18}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/068e42b61598de994279be6cdf42af2ba9161598300f72f6267d78b686a9c11d.jpg", "table_caption": ["Table 6: Ablation study of different ranking methods applied to DeBERTa Evaluated using zero-shot setting and the largest DeBERTa 1.5B model on ReCoRD. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Detailed SuperGLUE results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This appendix provides a detailed analysis of DeBERTa\u2019s performance on the SuperGLUE benchmark, focusing on two aspects: the relationship between the number of demonstration examples and model performance (Figure 5), and the task-specific scaling behavior (Figure 6). ", "page_idx": 19}, {"type": "text", "text": "Effect of shot count Figure 5 demonstrates how DeBERTa\u2019s performance changes as we increase the number of in-context examples from 0 to 32. Unlike the main results where we optimize the number of shots for each subtask independently, here we deliberately use the same number of shots across all tasks to provide a controlled comparison with GPT-3\u2019s results from Brown et al. (2020); there, SuperGLUE tasks are the only ones with such detailed few-shot evaluation. ", "page_idx": 19}, {"type": "text", "text": "The plot reveals several interesting patterns: ", "page_idx": 19}, {"type": "text", "text": "1. The steepest improvement occurs between 0 and 4 shots, suggesting that even a small number of examples provides substantial benefit.   \n2. The overall trend is very similar between DeBERTa and GPT-3, which again shows that masked language models can be just as good in-context learners as causal language models.   \n3. The performance of DeBERTa decreases after 8 or more shots. We believe that this is mainly caused by its imperfect processing of contexts longer than 512 tokens \u2013 as discussed in Section 2.3. ", "page_idx": 19}, {"type": "image", "img_path": "BCA9NMZkLS/tmp/8a697215b8ddce412235597282c1434e853eda6c7f4106a467979c7a7ceb3f85.jpg", "img_caption": ["Average GLUE performance vs. number of shots ", "Figure 5: The average performance on the SuperGLUE benchmarks as a function of number of shots As opposed to the other SuperGLUE few-shot results, where we select the number of shots for each subtask according to the performance on its training split, here all subtasks are evaluated with the same number of shots. In this way, we can compare DeBERTa 1.5B directly to Figure 3.8 from Brown et al. (2020), which gives the same evaluation for GPT 175B (unfortunately not for smaller, more comparable, models). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Task-specific scaling analysis Figure 6 breaks down the scaling behavior for each SuperGLUE task individually, revealing the varying impacts of model size across different language understanding challenges. The plots demonstrate that while DeBERTa and GPT-3 both exhibit generally positive scaling trends, their scaling patterns differ substantially across task types. Most notably, DeBERTa shows steeper scaling curves than GPT-3 on the majority of tasks. This suggests that bidirectional attention mechanisms may provide particular advantages for scaling some language understanding capabilities. ", "page_idx": 19}, {"type": "image", "img_path": "BCA9NMZkLS/tmp/4cef90080450c2c66a54c3ad9716a5c0eb43cc97fbc835fcfaca80fa7159dfd9.jpg", "img_caption": ["Average 1-shot SuperGLUE performance vs. paremeter count ", "Figure 6: Detailed evaluation of natural language understanding As opposed to the overall results presented in Figure 1, these plots show the scaling behavior on each subtask of the SuperGLUE benchmark. We can see the DeBERTa out-scales GPT-3 consistently across all tasks, with the exception of WiC where both models fail to show any learning ability. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Evaluation details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This appendix provides more details about our evaluation setup to make it easier to reproduce our results; the source code is available at https://github.com/ltgoslo/bert-in-context. In general, we follow Brown et al. (2020) in all decisions about the implementation. ", "page_idx": 20}, {"type": "text", "text": "Newline separator Note we use newlines in the displayed prompts for better readability, but we do not actually use them as the DeBERTa tokenizers cannot encode a newline character (they convert it to the standard whitespace character instead). Instead, we convert all newline characters to double-escaped \u2018\\\\n \u2019 string with a whitespace character, which then acts as paragraph/information separator. ", "page_idx": 20}, {"type": "text", "text": "Few-shot prompting For each evaluated sample, the example demonstrations are randomly selected (without replacement) from the training set of each task; if the training set is not available, we sample from the only available dataset split, making sure not to select the same sample as the evaluated one. We format these examples using the respective prompt templates and concatenate them together, joined by two newline characters. The numbers of shots used for each task are given in Appendix F. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "E.1 Needle in a haystack ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The needle is a randomly generated 6-digit number (from 100 000 to 999 999). The prediction is produced via constrained generation that only allows sequences of tokens that form 6-digit numbers. We consider a prediction to be correct only if it exactly matches the needle, which means that a trivial baseline has the accuracy of $^{1}\\!/\\!900\\,000$ . ", "page_idx": 21}, {"type": "text", "text": "The evaluated models are prompted with the template shown below. Similarly to Hsieh et al. (2024), we use Paul Graham\u2019s essays to fill the context (\\$prefix_lines and $\\ngtr$ suffix_lines).7 The essays are sentence-segmented and concatenated to fill the desired total sequence length. ", "page_idx": 21}, {"type": "text", "text": "1 $>$ Some special magic number is hidden within the following articles. Make sure to   \n$\\hookrightarrow$ memorize it. I will quiz you about the magic number afterwards.   \n2   \n3 {\\$prefix_lines}   \n4 The magic number is {\\$needle}.   \n5 {\\$suffix_lines}   \n6   \n7 $>$ Question: What is the special magic number mentioned in the provided text?   \n8 $>$ Answer: The special magic number mentioned in the provided text is ", "page_idx": 21}, {"type": "text", "text": "E.2 Language understanding ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This section provides the prompts used for the eight SuperGLUE tasks, all prompt templates are taken from the GPT-3 evaluation setup. ", "page_idx": 21}, {"type": "text", "text": "BoolQ This task is evaluated by ranking texts formatted as shown below with two possible \\$answers, yes or no. ", "page_idx": 21}, {"type": "text", "text": "1 {\\$passage}   \n2 question: {\\$question}?   \n3 answer: {\\$answer:-yes/no} ", "page_idx": 21}, {"type": "text", "text": "CB Evaluated by ranking texts formatted as shown below with three possible \\$answers, true, false or neither. ", "page_idx": 21}, {"type": "text", "text": "{\\$premise} 2 question: {\\$hypothesis}; true, false, or neither? 3 answer: {\\$answer:-true/false/neither} ", "page_idx": 21}, {"type": "text", "text": "COPA We rank two possible substitutions of $\\oint$ answer that follow the premise. \\$connector is formatted based on the question type: \u2018because\u2019 if the type is \u2018cause\u2019, otherwise \u2018therefore\u2019 is used. ", "page_idx": 21}, {"type": "text", "text": "{\\$premise} {\\$connector:-because/therefore} {\\$answer} ", "page_idx": 21}, {"type": "text", "text": "MultiRC The potential answer is substituted for \\$option and then we rank two possible substitutions for $\\oint$ answer: [True] or [False]. ", "page_idx": 22}, {"type": "text", "text": "1 READING COMPREHENSION ANSWER KEY   \n2   \n3 {\\$paragraph}   \n4   \n5 {\\$question}   \n{\\$answer:-[True]/[False]} {\\$option} ", "page_idx": 22}, {"type": "text", "text": "ReCoRD Here we rank the possible entity names as substitutions for $\\pmb{\\mathfrak{s}}$ answer. Note that \\$paragraph often includes summaries that, in a way, act as few-shot examples (see the formatted zero-shot example in Appendix C). ", "page_idx": 22}, {"type": "text", "text": "{\\$passage} 2 {\\$answer_prefix}{\\$answer}{\\$answer_suffix} ", "page_idx": 22}, {"type": "text", "text": "RTE Ranking of two possible completions in this binary classification task: True or False. ", "page_idx": 22}, {"type": "text", "text": "1 {\\$premise}   \n2 question: {\\$hypothesis} True or False?   \n3 answer: {\\$answer:-True/False} ", "page_idx": 22}, {"type": "text", "text": "WiC Another binary task with two possible substitutions: yes or no. Note that this prompt template was not working for any GPT-3 models and it is also not working for the models evaluated in this paper, all models are just randomly guessing the answers (Table 1). ", "page_idx": 22}, {"type": "text", "text": "1 {\\$sentence1}   \n2 {\\$sentence2}   \n3 question: Is the word '{\\$word}' used in the same way in the two sentences above?   \n4 answer: {\\$answer:-yes/no} ", "page_idx": 22}, {"type": "text", "text": "WSC We rank possible substitutions for $\\ngtr$ answer. ", "page_idx": 22}, {"type": "text", "text": "1 Final Exam with Answer Key   \n2 Instructions: Please carefully read the following passages. For each passage, you   \n$\\hookrightarrow$ must identify which noun the pronoun marked in $*_{\\Delta0}[d^{*}$ refers to.   \n3 $======$   \n4   \n5 Passage: {\\$text}   \n6 Question: In the passage above, what does the pronoun \"\\*{\\$span_text}\\*\" refer to?   \n7 Answer: {\\$answer} ", "page_idx": 22}, {"type": "text", "text": "E.3 Language modeling ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This group of tasks uses very straightforward prompt templates as all of these tasks are different variants of text completion. ", "page_idx": 23}, {"type": "text", "text": "HellaSwag Here, the task is to select the most likely completion (\\$answer) that follows after \\$context. ", "page_idx": 23}, {"type": "text", "text": "{\\$activity_label}: {\\$context}{\\$answer} ", "page_idx": 23}, {"type": "text", "text": "StoryCloze The goal is to select the most suitable $\\pmb{\\mathfrak{s}}$ answer that completes a story laid out by four previous sentences. ", "page_idx": 23}, {"type": "text", "text": "{\\$sentence_1} {\\$sentence_2} {\\$sentence_3} {\\$sentence_4} {\\$answer} ", "page_idx": 23}, {"type": "text", "text": "Winograd \\$answer should be substituted by the correct entity (coreference resolution). ", "page_idx": 23}, {"type": "text", "text": "{\\$context_prefix} {\\$answer} {\\$context_suffix} ", "page_idx": 23}, {"type": "text", "text": "Winogrande Same as Winograd: ", "page_idx": 23}, {"type": "text", "text": "{\\$context_prefix} {\\$answer} {\\$context_suffix} ", "page_idx": 23}, {"type": "text", "text": "E.4 Translation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "All language pairs and all evaluation setups (zero-shot, one-shot and few-shot) use the same prompt template given below. This is the only time when we decided to differ from the GPT-3 setup as its very simple (and non-informative) prompt was not working for one-shot evaluation of DeBERTa models. The models are then asked to complete the prompt \u2013 using beam search decoding with 4 beams and the default HuggingFace hyperparameters8 \u2013 and the generation is stopped after producing the special newline character \\\\n. ", "page_idx": 23}, {"type": "text", "text": "{\\$source_language}: {\\$source_text} 2 {\\$target_language}: ", "page_idx": 23}, {"type": "text", "text": "For reference, here are the two prompt templates used for GPT-3 (for zero-shot and for one/few-shot, respectively): ", "page_idx": 23}, {"type": "text", "text": "Q: What is the {\\$target_language} translation of {\\$source_text} 2 A: ", "page_idx": 23}, {"type": "text", "text": "{\\$source_text} = ", "page_idx": 23}, {"type": "text", "text": "E.5 Closed-book question answering ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This group of tasks mixes two types of in-context evaluation: text generation (Natural Questions, TriviaQA and Web Questions) and text ranking (PIQA, ARC and OpenBookQA). The prompt setup exactly follows GPT-3. ", "page_idx": 24}, {"type": "text", "text": "Natural Questions Here, the goal is to generate an answer based on \\$question. ", "page_idx": 24}, {"type": "text", "text": "Q: {\\$question} A: ", "page_idx": 24}, {"type": "text", "text": "TriviaQA Same as Natural Questions: ", "page_idx": 24}, {"type": "text", "text": "Q: {\\$question} A: ", "page_idx": 24}, {"type": "text", "text": "Web Questions Same as Natural Questions: ", "page_idx": 24}, {"type": "text", "text": "Q: {\\$question} A: ", "page_idx": 24}, {"type": "text", "text": "PIQA Here, the goal is to select the most suitable text completion by substituting for \\$answer. ", "page_idx": 24}, {"type": "text", "text": "{\\$goal} {\\$answer} ", "page_idx": 24}, {"type": "text", "text": "ARC (challenge) and ARC (easy) This a multiple choice test, the correct \\$answer has to be selected. ", "page_idx": 24}, {"type": "text", "text": "Question: {\\$question} Answer: {\\$answer} ", "page_idx": 24}, {"type": "text", "text": "OpenBookQA Similarly, the goal is to select the correct \\$answer. ", "page_idx": 24}, {"type": "text", "text": "{\\$question} {\\$answer} ", "page_idx": 24}, {"type": "text", "text": "F All results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For reference, we list all results of the DeBERTa models evaluated throughout this paper in Table 7.   \nThe GPT-3 results that were used for comparison are published in Brown et al. (2020, Table H.1). ", "page_idx": 25}, {"type": "text", "text": "Table 7: Results of all evaluations performed in this paper The second and third column shots the dataset splits and evaluation metrics, both of them replicating the GPT-3 evaluation setup. Note the BLEU scores used for evaluating translation are SacreBLEU scores with signature BLEU $\\lvert+$ case.mixed+numrefs. $^{1+}$ smooth.exp+tok.intl+version.1.2.20. ", "page_idx": 25}, {"type": "table", "img_path": "BCA9NMZkLS/tmp/c6bb60c881149bc812ccfe38557703c9e244603124c07c89144a9600c6b46786.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All claims are directly supported by the results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: There are explicit sections about limitations in 2. Method ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper is purely empirical. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper only proposes simple inference methods, and the evaluation setup is fully described. We also intend to publish the adapted model code. We published the code at https://github.com/ltgoslo/bert-in-context. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper should provide enough information to fully reproduce our results, but we also intend to publish the source code. We published the code at https://github. com/ltgoslo/bert-in-context. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: As described above. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper replicates a previous work, including its evaluation setup. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve any training, only inference with negligable cost. However, we still give the compute cost of the pretrained language models (even though they were not pretrained as part of this paper). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we only evaluate language models on published benchmarks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No apparent societal impact. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Not applicable. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Section References Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not applicable   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer:[NA]   \nJustification: Not applicable   \nGuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not applicable   \nGuidelines: ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]