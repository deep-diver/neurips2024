[{"figure_path": "Vn0FWRImra/tables/tables_3_1.jpg", "caption": "Table 1: Best known regret bounds for combinatorial multiarmed bandits under different assumptions. By lemma 1.1 our upperbound can also be stated for R1-e-1. We note that our lower bound proven for the stochastic setting immediately applies to the adversarial setting in the table.", "description": "This table summarizes the state-of-the-art regret bounds for combinatorial multi-armed bandits, focusing on submodular maximization problems.  It compares upper and lower bounds under various assumptions about the reward function (submodular and monotone, submodular without monotonicity, and degree-d polynomial), feedback type (stochastic and adversarial), and the measure of regret used (Rgr and R(S*)). The table highlights the contributions of the current work by presenting novel upper and lower bounds for submodular and monotone functions under stochastic feedback, which closes the gap between previous best known results.", "section": "1.2 Related Work"}, {"figure_path": "Vn0FWRImra/tables/tables_6_1.jpg", "caption": "Table 1: Best known regret bounds for combinatorial multiarmed bandits under different assumptions. By lemma 1.1 our upperbound can also be stated for R1-e-1. We note that our lower bound proven for the stochastic setting immediately applies to the adversarial setting in the table.", "description": "This table compares the best-known regret bounds for combinatorial multi-armed bandits under various assumptions, including submodularity and monotonicity of the reward function.  It shows upper and lower bounds on the regret for different settings (stochastic and adversarial) and feedback types (full-feedback and bandit feedback). The table highlights the novel contributions of the paper by showing improved regret bounds and a matching lower bound.  It distinguishes between regret relative to the optimal solution and regret against a noisy greedy solution (Rgr).", "section": "1.2 Related Work"}]