{"references": [{"fullname_first_author": "Lukas Berglund", "paper_title": "The reversal curse: LLMs trained on \"a is b\" fail to learn \"b is a\"", "publication_date": "2023-00-00", "reason": "This paper introduces the concept of the reversal curse, a key phenomenon motivating the research in the target paper."}, {"fullname_first_author": "Gregor Bachmann", "paper_title": "The pitfalls of next-token prediction", "publication_date": "2024-00-00", "reason": "This paper analyzes limitations of autoregressive next-token prediction, which is directly relevant to the factorization curse discussed in the target paper."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.2, knowledge manipulation", "publication_date": "2023-00-00", "reason": "This paper provides theoretical background on language models that is directly relevant to the concepts explored in the target paper."}, {"fullname_first_author": "Olga Golovneva", "paper_title": "Reverse training to nurse the reversal curse", "publication_date": "2024-00-00", "reason": "This paper proposes a solution to the reversal curse, which is a closely related problem to the factorization curse studied in the target paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-00-00", "reason": "This paper is foundational work on large language models, providing context for the current state of the art in language modeling which is the basis for the target paper."}]}