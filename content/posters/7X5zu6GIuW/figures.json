[{"figure_path": "7X5zu6GIuW/figures/figures_1_1.jpg", "caption": "Figure 1: (a) The offline instruction video dataset includes videos of desirable behaviors (Do's) and undesirable behaviors (Don'ts). (b) Unsupervised skill discovery algorithms tend to learn undesirable behaviors. (c) In DoDont, an instruction network is first trained with the Do's and Don'ts videos to distinguish desirable and undesirable behaviors. Then, this instruction network adjusts the intrinsic reward of the skill discovery algorithm, promoting desirable skills while avoiding undesirable ones.", "description": "This figure illustrates the core idea of the DoDont algorithm.  Panel (a) shows an offline dataset of instruction videos, labeled as either desirable ('Do') or undesirable ('Don't'). Panel (b) depicts how unsupervised skill discovery methods often fail to learn complex and desirable skills, instead learning simpler or undesirable ones. Panel (c) demonstrates how DoDont uses an instruction network (trained on the labeled video data) to adjust the reward function of a skill discovery algorithm, guiding it towards learning desirable skills and away from undesirable behaviors.", "section": "1 Introduction"}, {"figure_path": "7X5zu6GIuW/figures/figures_4_1.jpg", "caption": "Figure 1: (a) The offline instruction video dataset includes videos of desirable behaviors (Do's) and undesirable behaviors (Don'ts). (b) Unsupervised skill discovery algorithms tend to learn undesirable behaviors. (c) In DoDont, an instruction network is first trained with the Do's and Don'ts videos to distinguish desirable and undesirable behaviors. Then, this instruction network adjusts the intrinsic reward of the skill discovery algorithm, promoting desirable skills while avoiding undesirable ones.", "description": "This figure illustrates the core idea of the DoDont algorithm.  Panel (a) shows the instruction dataset with examples of desirable ('Do') and undesirable ('Don't') behaviors. Panel (b) highlights the problem of unsupervised skill discovery methods often learning undesirable behaviors.  Panel (c) shows how DoDont solves this by using an instruction network to distinguish between desirable and undesirable transitions, thereby adjusting the reward function of the skill discovery algorithm to encourage desirable behaviors and avoid undesirable ones.", "section": "1 Introduction"}, {"figure_path": "7X5zu6GIuW/figures/figures_5_1.jpg", "caption": "Figure 3: Left: State coverage and zero-shot task reward for Cheetah and Quadruped. Right: Visualization of Do videos in our instruction video dataset and learned skills by DoDont. We are able to observe that DoDont does not simply mimic instruction videos but extracts desirable behaviors (e.g., run) from the videos and learn diverse skills.", "description": "This figure presents a comparison of the performance of DoDont and other baselines on Cheetah and Quadruped locomotion tasks. The left side shows the state coverage and zero-shot run reward for both environments, demonstrating DoDont's superior ability to learn diverse and desirable skills. The right side visually compares the instruction videos used to train DoDont with the skills learned by the model.  The visualization shows that DoDont successfully learns the essential characteristics of desired behaviors such as running, rather than simply imitating the instruction videos.", "section": "5.2.1 How efficiently does DoDont learn complex behaviors in locomotion tasks?"}, {"figure_path": "7X5zu6GIuW/figures/figures_6_1.jpg", "caption": "Figure 4: Visualization and comparison of learned skills. In both environments, the left side is hazardous and the right side is safe. Safe state coverage assesses the agent's ability to cover safe areas and avoid hazards.", "description": "This figure visualizes and compares the learned skills of different methods in Cheetah and Quadruped environments.  The environments are designed with a hazardous area on the left and a safe area on the right. The visualization shows the trajectories learned by each algorithm. The bar charts show the \"Safe State Coverage\", which measures the extent to which each algorithm explores the safe area while avoiding the hazardous area. This metric helps evaluate the effectiveness of each algorithm in learning safe behaviors while avoiding hazardous ones.", "section": "5.2.2 Does DoDont learn diverse behaviors while avoiding hazardous areas?"}, {"figure_path": "7X5zu6GIuW/figures/figures_6_2.jpg", "caption": "Figure 3: Left: State coverage and zero-shot task reward for Cheetah and Quadruped. Right: Visualization of Do videos in our instruction video dataset and learned skills by DoDont. We are able to observe that DoDont does not simply mimic instruction videos but extracts desirable behaviors (e.g., run) from the videos and learn diverse skills.", "description": "This figure compares the performance of DoDont with other methods (METRA, METRA+, DGPO, SMERL) on two locomotion tasks: Cheetah and Quadruped.  The left side shows state coverage and zero-shot task reward over training time, highlighting DoDont's superior ability to learn diverse and complex skills. The right side provides a visual comparison of the instruction videos (Do's) and the skills learned by DoDont, demonstrating its capacity to extract and generalize desirable behaviors rather than just mimicking the videos.", "section": "5.2.1 How efficiently does DoDont learn complex behaviors in locomotion tasks?"}, {"figure_path": "7X5zu6GIuW/figures/figures_7_1.jpg", "caption": "Figure 3: Left: State coverage and zero-shot task reward for Cheetah and Quadruped. Right: Visualization of Do videos in our instruction video dataset and learned skills by DoDont. We are able to observe that DoDont does not simply mimic instruction videos but extracts desirable behaviors (e.g., run) from the videos and learn diverse skills.", "description": "This figure presents a comparison of the performance of DoDont against METRA, METRA+, DGPO, and SMERL across two locomotion tasks: Cheetah and Quadruped. The left panel shows the state coverage and zero-shot task reward for each algorithm over time. The right panel provides a visualization of the Do videos used to train the instruction network and the skills learned by DoDont, highlighting DoDont's ability to learn more diverse and complex skills compared to other methods.", "section": "5.2.1 How efficiently does DoDont learn complex behaviors in locomotion tasks?"}, {"figure_path": "7X5zu6GIuW/figures/figures_7_2.jpg", "caption": "Figure 3: Left: State coverage and zero-shot task reward for Cheetah and Quadruped. Right: Visualization of Do videos in our instruction video dataset and learned skills by DoDont. We are able to observe that DoDont does not simply mimic instruction videos but extracts desirable behaviors (e.g., run) from the videos and learn diverse skills.", "description": "This figure shows the results of an experiment comparing DoDont's performance to other methods in learning locomotion skills. The left panel shows state coverage and zero-shot task reward, demonstrating DoDont's superior performance.  The right panel visualizes DoDont's ability to learn diverse skills from a small number of instruction videos, without simply mimicking them.", "section": "5.2.1 How efficiently does DoDont learn complex behaviors in locomotion tasks?"}, {"figure_path": "7X5zu6GIuW/figures/figures_8_1.jpg", "caption": "Figure 8: Left: Visualization of acquired skills, with the hazardous zone on the left and the safe zone on the right. Right: Quantitative comparison of each method.", "description": "This figure shows a comparison of different methods for learning skills, specifically focusing on the ability to avoid hazardous areas while learning diverse behaviors.  The left side presents visual representations of learned skills for each method (METRA, Additive methods with varying coefficients, and the proposed Multiplicative method).  Each visualization shows trajectories, possibly color-coded to indicate different skills learned.  The right side offers a quantitative comparison of the methods concerning \"Preferred State Coverage\", which likely reflects the proportion of time agents spend in the safe zone versus the hazardous zone.  This showcases how effectively each method uses an instruction network to guide learning toward safe and desirable behaviors.", "section": "5.3.2 The importance of utilizing instruction network as the distance metric"}, {"figure_path": "7X5zu6GIuW/figures/figures_14_1.jpg", "caption": "Figure 9: Aggregated performance. The overall results are aggregated over 4 environments, 4 tasks, 2 datasets, and 4 seeds (i.e., 128 values in total).", "description": "This figure shows a bar chart summarizing the overall performance of DoDont and three baseline methods (FDM, FB, HILP) across various environments, tasks, and datasets.  The performance is aggregated across 128 individual trials (4 environments x 4 tasks x 2 datasets x 4 seeds).  DoDont significantly outperforms the baselines.", "section": "5.1 Experimental setup"}, {"figure_path": "7X5zu6GIuW/figures/figures_14_2.jpg", "caption": "Figure 3: Left: State coverage and zero-shot task reward for Cheetah and Quadruped. Right: Visualization of Do videos in our instruction video dataset and learned skills by DoDont. We are able to observe that DoDont does not simply mimic instruction videos but extracts desirable behaviors (e.g., run) from the videos and learn diverse skills.", "description": "This figure shows the results of the DoDont algorithm on Cheetah and Quadruped locomotion tasks.  The left side displays graphs comparing DoDont's performance to other methods (METRA, METRA+, DGPO, and SMERL) in terms of state coverage and zero-shot task reward over training time.  The right side provides a visual comparison between the instruction videos used to train DoDont (videos showing desirable 'Do' behaviors), and the types of locomotion skills the algorithm learns.  This visual comparison highlights that DoDont learns diverse and complex behaviors, rather than simply mimicking the specific behaviors shown in the training videos.", "section": "5.2.1 How efficiently does DoDont learn complex behaviors in locomotion tasks?"}]