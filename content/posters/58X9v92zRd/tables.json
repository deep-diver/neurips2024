[{"figure_path": "58X9v92zRd/tables/tables_4_1.jpg", "caption": "Table 1: CommonGen results. All methods are applied to the GPT2-large model.", "description": "This table presents the results of the CommonGen experiment.  The experiment evaluates several methods (FUDGE, A*esque, NADO, GeLaTo, and Ctrl-G) for generating text that includes specific keywords.  The table shows the performance of these methods using BLEU-4, ROUGE-L, CIDEr, and SPICE metrics, comparing supervised and unsupervised approaches.  It highlights Ctrl-G's superior performance in achieving 100% constraint satisfaction.", "section": "4.1 Commonsense Generation"}, {"figure_path": "58X9v92zRd/tables/tables_5_1.jpg", "caption": "Table 1: CommonGen results. All methods are applied to the GPT2-large model.", "description": "This table presents the results of the CommonGen experiment, comparing various methods (FUDGE, A*esque, NADO, GeLaTo, and Ctrl-G) on two versions of the GPT2-large model: one trained with full supervision and one not trained with keywords. The metrics used are BLEU-4, ROUGE-L, CIDEr, SPICE, and the constraint satisfaction rate. Ctrl-G demonstrates superior performance in terms of both generation quality and constraint satisfaction.", "section": "4.1 Commonsense Generation"}, {"figure_path": "58X9v92zRd/tables/tables_6_1.jpg", "caption": "Table 3: Text infilling results (BLEU-4/ROUGE-L) across different masking ratios.", "description": "This table presents the results of text infilling experiments using different LLMs.  It shows the BLEU-4 and ROUGE-L scores achieved by both the ILM model (a baseline model trained with full supervision) and the proposed Ctrl-G model. Different masking ratios (13%, 21%, 32%, and 40%) are applied to the test data to evaluate the performance of both LLMs in handling different levels of text missingness.  The 'diff.' row highlights the difference in performance between Ctrl-G and the ILM model at each masking ratio.", "section": "4.2 Text infilling"}, {"figure_path": "58X9v92zRd/tables/tables_8_1.jpg", "caption": "Table 4: Evaluation results of interactive text editing. K&W indicates that the model should adhere to both keyphrase (K) and word count (W) constraints simultaneously. We present the human evaluation score (Quality), constraint success rate (Success), and overall satisfaction rate (Overall), which represents the proportion of examples meeting logical constraints with a Quality score above 3.", "description": "This table presents the results of a human evaluation comparing different LLMs' performance on interactive text editing tasks.  The models were evaluated on their ability to generate text continuations and insertions while adhering to keyphrase and word count constraints.  The metrics used are Quality (average human rating), Success rate (percentage of successful constraint satisfaction), and Overall satisfaction rate (percentage of text that meets quality and constraint criteria).", "section": "5.2 Results"}, {"figure_path": "58X9v92zRd/tables/tables_16_1.jpg", "caption": "Table 1: CommonGen results. All methods are applied to the GPT2-large model.", "description": "This table presents the results of the CommonGen experiment using the GPT2-large model.  It compares several methods (FUDGE, A*esque, NADO, GeLaTo, and Ctrl-G) across different metrics, including BLEU-4, ROUGE-L, CIDEr, and SPICE, both for supervised and unsupervised scenarios.  The \"Constraint\" column indicates the constraint satisfaction rate. The results showcase Ctrl-G's superior performance compared to existing methods in terms of constraint satisfaction and text generation quality.", "section": "4.1 Commonsense Generation"}]