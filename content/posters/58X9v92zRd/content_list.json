[{"type": "text", "text": "Adaptable Logical Control for Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Honghua Zhang\u2217 Po-Nien Kung\u2217 Masahiro Yoshida\u2020 UCLA UCLA UCLA & Sony Group Corporation hzhang19@cs.ucla.edu ponienkung@cs.ucla.edu masahiroyoshida@ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Guy Van den Broeck Nanyun Peng UCLA UCLA guyvdb@cs.ucla.edu violetpeng@cs.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation to follow strict constraints at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, a neuro-symbolic framework that enables tractable and adaptable control of LLM generation to follow logical constraints reliably. Ctrl-G combines any production-ready LLM with a Hidden Markov Model (HMM), guiding LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that CtrlG, when a TULU2-7B model is coupled with a 2B-parameter HMM, outperforms GPT4 in text editing: on the task of generating text insertions/continuations following logical constraints, our approach achieves over $30\\%$ higher satisfaction rate in human evaluation. When applied to medium-size language models (e.g., GPT2- large), Ctrl-G also beats its counterparts on standard benchmarks by large margins. Additionally, as a proof-of-concept study, we use Ctrl-G to assist LLM reasoning on the GSM benchmark, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have achieved remarkable performance on a wide range of challenging language generation tasks including translation [4, 48, 41], summarization [49], and open-domain creative generation [45, 38]. Nevertheless, many downstream applications benefti from fine-grained control of LLMs to follow logical constraints, e.g., avoid using bad words for detoxification [9, 1] or inserting text that is coherent with contexts for document revision [16]. Despite the recent advancement of LLM finetuning techniques such as instruction-tuning [5, 42, 35] and preference optimization [28, 33], LLMs still fail to reliably follow logical constraints [37, 20]. ", "page_idx": 0}, {"type": "image", "img_path": "58X9v92zRd/tmp/70d6c80efb9e8cc2b562bda978f53bac080b9c71db7b9b387ede4c8d59c3f9f2.jpg", "img_caption": ["Figure 1: Ctrl-G pipeline; both the LLM and the HMM are frozen once trained. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "The major difficulty of achieving constrained generation from LLMs lies in the intractability of conditioning LLMs on logical constraints [34]. One recently proposed framework called GeLaTo [47] uses tractable generative models, which can be conditioned on logical constraints efficiently, to guide autoregressive generation from LLMs. Though GeLaTo guarantees that the logical constraints will be satisfied, it only works for the keyword-inclusion constraint. Significantly generalizing the GeLaTo framework, we propose Ctrl-G (shorthand for controllable generation while mimicking the keyboard shortcuts Ctrl-C and Ctrl-V) for reliable, scalable and adaptable control of LLMs to follow logical constraints. Ctrl-G consists of three major steps (see Fig. 1): (1) distillation: given a LLM, we distill a Hidden Markov Model as its white-box approximation; (2) constraint specification: we construct a deterministic finite automaton (DFA) to (compactly) represent the desired logical constraint; (3) inference: we condition the HMM on the DFA-specified constraint and compute this conditional probability to steer LLM generation towards satisfying the constraint. ", "page_idx": 0}, {"type": "image", "img_path": "58X9v92zRd/tmp/4bc9f59a61fd70e9920c08c441833e1df09b0989db104d7adb188a405a1cd490.jpg", "img_caption": ["Figure 2: An example usage of Ctrl-G for text insertion with multiple constraints. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "$\\mathrm{Ctrl}{-}\\mathrm{G}^{3}$ has three major advantages compared to its counterparts: (1) the desired logical constraints are guaranteed to be satisfied [47]; (2) once we have the distilled HMM, it can be applied to arbitrary constraints without retraining; (3) Ctrl-G works for any constraints specified as DFAs, which can be easily constructed for various applications by leveraging existing algorithms. ", "page_idx": 1}, {"type": "text", "text": "We evaluate Ctrl-G on the task of text editing: in the domain of story writing, we evaluate models\u2019 ability to generate suggestions for text insertions/continuations under combinations of logical constraints (e.g. keyphrase inclusion and length control; see Fig. 2). Human evaluation shows that Ctrl-G, where a TULU2-7B model [13] is combined with a 2B-parameter HMM, outperforms prominent LLMs including GPT3.5 and GPT4 [27] by over $30\\%$ in overall satisfaction rate (i.e., percentage of the generated text that is not only fluent but also satisfies the constraints). We note that as the constraints become more complex, while the generation quality of GPT4 declines, Ctrl-G consistently produces high-quality text, highlighting its strong generalizability to complex constraints. Even when no constraint is present, Ctrl-G still matches with the generation quality of GPT4 in text insertion. ", "page_idx": 1}, {"type": "text", "text": "In addition, we demonstrate the extensive adaptability of Ctrl-G on two commonly used benchmarks: commonsense generation [18] and text infilling [7]. When applied to variants of the GPT2 models, Ctrl-G outperforms prior constrained generation approaches by producing outputs of substantially higher quality while achieving $100\\%$ constraint satisfaction. ", "page_idx": 1}, {"type": "text", "text": "To further explore the potential of Ctrl-G, as a proof-of-concept, we conduct an empirical study on the Grade School Math (GSM) benchmark [6]; here, we use Ctrl-G to assist the LLM reasoning process by enforcing keyphrase-inclusion constraints. Performance improvement suggests the potential of Ctrl-G in applications of a scope broader than the traditional constrained generation tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we briefly summarize the background for (logically-)constrained generation and the basics for Hidden Markov Models. Notations introduced here will be used throughout the paper. ", "page_idx": 1}, {"type": "text", "text": "Constrained generation For simplicity, we assume that the lengths of token sequences generated by LLMs are always bounded by some number $n$ and denote the LLM distribution as $p_{\\mathrm{lm}}(x_{1:n}^{\\bar{}})^{4}$ . Given ", "page_idx": 1}, {"type": "text", "text": "logical constraint $\\alpha$ , our goal is to generate from $p_{\\mathrm{lm}}(x_{1:n}\\mid\\alpha)$ , which decomposes autoregressively: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathrm{lm}}(\\boldsymbol{x}_{1:n}\\mid\\alpha)=\\prod_{t}p_{\\mathrm{lm}}(x_{t}\\mid\\boldsymbol{x}_{<t},\\alpha),\\;\\;\\mathrm{where}\\;\\;p_{\\mathrm{lm}}(x_{t}\\mid\\boldsymbol{x}_{<t},\\alpha)\\propto p_{\\mathrm{lm}}(x_{t}\\mid\\boldsymbol{x}_{<t})\\cdot p_{\\mathrm{lm}}(\\alpha\\mid\\boldsymbol{x}_{t},\\boldsymbol{x}_{<t});\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "that is, given that we have generated the first $t-1$ tokens $x_{<t}$ , we want to generate the next token $x_{t}$ from $p_{\\mathrm{lm}}(x_{t}\\mid x_{<t})\\cdot p_{\\mathrm{lm}}(\\alpha\\mid x_{t},x_{<t})$ . The first term $p_{\\mathrm{lm}}(x\\mid x_{<t})$ is just the next-token distribution of the LLM, but the marginal probability $p_{\\mathrm{lm}}(\\alpha\\mid x_{t},x_{<t})$ , which characterizes how likely the constraint $\\alpha$ will be satisfied in the future, cannot be efficiently computed; specifically, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\mathrm{lm}}(\\alpha\\mid x_{t},x_{<t})=\\sum_{x>t\\;\\mathrm{s.t.}\\;x_{1:n}\\mathrm{\\;satisfies}\\;\\alpha}p(x_{>t}\\mid x_{t},x_{<t});}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "that is, we need to marginalize over all possible future sequences $x_{>t}$ such that, together with $x_{\\leq t}$ , satisfy $\\alpha$ . For example, say $\\alpha$ is the constraint that the phrase \u201cin the park\u201d must appear at the end of the generated text; to compute the desired marginal probability, we need to enumerate over all future token sequences with this phrase at the end, and there are exponentially many of them. ", "page_idx": 2}, {"type": "text", "text": "Prior work To solve the problem of constrained generation, one line of work proposes search-based decoding algorithms like NeuroLogic Decoding [22, 21], which explicitly performs heuristic search to find high-probability token sequences that would (partially) satisfy the logical constraint; however such methods scale poorly because the search space grows exponentially with respect to the sequence length. The other line of works including GeDi [15], FUDGE [44] and NADO [25] train auxiliary neural classifiers to approximate the intractable term $p_{\\mathrm{lm}}(\\alpha\\mid x_{t},x_{<t})$ ; however, they do not guarantee that the constraints will be satisfied and the classifiers need to be retrained for different constraints. Some other methods use approximate inference techniques (e.g., sequential Monte Carlo sampling) to approximate the intractable conditional distributions [30, 11, 17], which provide no guarantee on the convergence rate and often suffer from the high-variance of sampling. ", "page_idx": 2}, {"type": "text", "text": "From GeLaTo to Ctrl-G A recent framework called GeLaTo [47] uses tractable generative models, in particular, Hidden Markov Models (HMMs), to guide LLM generation to satisfy the given logical constraints. Specifically, GeLaTo first (1) distills an HMM $p_{\\mathrm{hmm}}(x_{1:n})$ to approximate the LLM distribution $p_{\\mathrm{lm}}(x_{1:n})$ and then (2) computes $p_{\\mathrm{hmm}}(\\alpha|x_{t},x_{<t})$ as an approximation for $p_{\\mathrm{lm}}(\\alpha|x_{t},x_{<t})$ . Compared to its counterparts, GeLaTo guarantees that the constraints will be satisfied. Nevertheless, two major questions remain unanswered, limiting its downstream applications: ", "page_idx": 2}, {"type": "text", "text": "\u2022 GeLaTo only handles the keyword-inclusion constraint and it is unclear whether $p_{\\mathrm{hmm}}(\\alpha|x_{t+1},x_{1:t})$ can be tractably computed for other logical constraints;   \n\u2022 despite the success of GeLaTo on language models at the scale of 0.1 billion parameters, it is unclear whether the assumption $p_{\\mathrm{hmm}}(\\bar{\\alpha}\\mid x_{\\leq t}^{-})\\!\\approx\\!p_{\\mathrm{lm}}(\\alpha\\mid x_{\\leq t})$ would still hold for the more recent LLMs (e.g., Llama2), which have over 100 times more parameters. ", "page_idx": 2}, {"type": "text", "text": "We propose Ctrl-G as a generalization of GeLaTo and give positive answers to both questions. ", "page_idx": 2}, {"type": "text", "text": "Hidden Markov Models A Hidden Markov Model (HMM) [32] represents a joint probability distribution over $n$ observed variables $x_{1:n}$ and $n$ hidden variables $z_{1:n}$ . Specifically, for language modeling, $x_{t}$ represents the token at position $t$ and $z_{t}$ is the corresponding hidden state; $z_{t}$ takes values in $\\{1,2,\\ldots,h\\}$ , where $h$ is the number of hidden states. An HMM models the joint distribution: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(x_{1:n},z_{1:n})=p(x_{1}\\mid z_{1})\\cdot p(z_{1})\\cdot\\prod_{2\\leq t\\leq n}p(x_{t}\\mid z_{t})\\cdot p(z_{t}\\mid z_{t-1});\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in particular, the parameters of an HMM are given by the initial probability $p(z_{1})$ , the emission matrix $p(x_{t}|\\boldsymbol{z}_{t})$ and the transition matrix $p(z_{t+1}|z_{t})$ ; the number of parameters of HMMs grows quadratically with respect to $h$ . To perform inference on HMMs efficiently, we leverage the Markov property: $p(x_{\\geq t}\\mid z_{t},x_{<t})\\!=\\!p(x_{\\geq t}\\mid z_{t})$ . For example, we can efficiently compute $\\begin{array}{r}{p(\\bar{x_{\\le t}})=\\sum_{z_{t}}p(\\bar{x_{\\le t}},\\bar{z}_{t})}\\end{array}$ by the following recurrence relation, referred to as the forward algorithm [32]: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(x_{\\leq t},z_{t})\\!=\\!\\sum_{1\\leq z_{t-1}\\leq h}\\!p(x_{t}\\mid z_{t})\\cdot p(z_{t}\\mid z_{t-1})\\cdot p(x_{\\leq t-1},z_{t-1}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Tractable probabilistic reasoning over logical constraints ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The Ctrl-G pipeline consists of three steps (Fig. 1): (1) distillation: we train an HMM on samples drawn from the LLM to minimize their KL-divergence; (2) constraint specification: we construct a (compact) deterministic finite automaton (DFA) $\\mathcal{M}$ representing the desired logical constraint $\\alpha$ (i.e., $\\mathcal{M}$ accepts $x_{1:n}$ if and only if $x_{1:n}$ satisfies $\\alpha$ ); (3) inference: for each step of the autoregressive generation from the LLM, we compute $p_{\\mathrm{hmm}}(\\alpha\\mid x_{t},x_{<t})$ as an approximation for $p_{\\mathrm{lm}}(\\alpha\\mid x_{t},x_{<t})$ and then sample the next token from ", "page_idx": 2}, {"type": "image", "img_path": "58X9v92zRd/tmp/ada60de39479009d9bb22e54d2e6002b7c8207b2546df7ede0cde75b4982fe14.jpg", "img_caption": ["Figure 3: Example of a DFA representing the logical constraint that the phrase \u201cgets cold\u201d must appear in the generated text along with pseudo-code for representing this DFA in Ctrl-G. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\mathrm{ctrl-g}}(x_{t}\\mid x_{<t},\\alpha)\\propto p_{\\mathrm{lm}}(x_{t}\\mid x_{<t})\\cdot p_{\\mathrm{hmm}}(\\alpha\\mid x_{t},x_{<t});\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "here, given that $\\alpha$ is represented as $\\mathcal{M}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\scriptstyle p_{\\mathrm{hmm}}(\\alpha\\mid x_{t},x_{<t})\\;=\\;\\sum_{x_{>t}{\\mathrm{~s.t.~}}{\\mathcal{M}}{\\mathrm{~accepts~}}x_{1:n}}p_{\\mathrm{hmm}}(x_{>t}\\mid x_{t},x_{<t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For step (1) (distillation) we follow the procedure proposed by [47], and we describe step (2) and step (3) in Sec. 3.1 and Sec. 3.2, respectively. In the end of this section, we briefly discuss the distinction between pure logical reasoning and probabilistic reasoning over constraints. ", "page_idx": 3}, {"type": "text", "text": "3.1 Logical constraints as DFAs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Deterministic finite automata (DFAs) [24, 31, 10] are computation models that accept or reject some given strings. Figure 3a shows an example DFA encoding the constraint that the phrase \u201cgets cold\u201d must appear: it accepts all strings containing this phrase and rejects the others. The DFA consists of 3 different states labeled $A$ , $B$ and $C$ , where $A$ is the initial state and $C$ an accept state. The states are connected by edges marked with sets of words (tokens, to be precise), which fully specify the transition function of the DFA. A DFA decides whether a given string satisfies the constraint by consuming it left-to-right while transitioning from state to state accordingly; in the end, the DFA accepts the string if it is in an accept state. See Figure 3b for an example. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. A deterministic finite automaton (DFA) is a tuple $\\mathcal{M}\\!=\\!(Q,\\Sigma,\\delta,q_{0},F)$ , where $Q$ is a finite set of states, $\\Sigma$ a finite set of symbols (i.e., tokens of an LLM), $\\delta:Q{\\times}\\Sigma\\,{\\rightarrow}\\,Q$ a transition function, $q_{0}$ an initial state, and $F\\subseteq Q$ a set of accept states. A string of tokens $w_{1}w_{2}\\ldots w_{n}$ is accepted by $\\mathcal{M}$ if there exists a sequence of states $q_{0},q_{1}\\ldots q_{n}$ s.t. $\\delta(q_{i},w_{i+1})\\!=\\!q_{i+1}$ for $1\\!\\leq\\!i\\!\\leq\\!n,q_{n}\\!\\in\\!F$ . ", "page_idx": 3}, {"type": "text", "text": "One question naturally arises: how can we come up with DFA representations for logical constraints? We first note that in the real world, we can always assume that the lengths of the generated token sequences are bounded by a constant; hence DFAs can represent any logical constraints defined on this bounded set and the important question is whether we can do this efficiently. For many common logical constraints, we can efficiently construct their DFA representations via existing algorithms. For example, given a string consisting of $n$ tokens, to encode the constraint that the string must appear, we can construct a DFA of size $O(n)$ by adapting the well-known Knuth\u2013Morris\u2013Pratt (KMP) algorithm [14] for string matching (e.g., Fig. 3a). One can also easily specify compositional logical constraints via DFAs by taking their intersection (logical and), union (logical or), complement (logical negation) or concatenation, which we illustrate throughout the rest of this paper. ", "page_idx": 3}, {"type": "text", "text": "3.2 An efficient algorithm for marginalizing HMMs over DFAs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Now assume that we have a constraint $\\alpha$ encoded as a DFA $\\mathcal{M}$ with $k$ states $Q=\\{1,2,\\cdot\\cdot\\cdot k\\}$ and $m$ edges, and we are given a distilled HMM with $h$ hidden states. To sample the next token from Eq. 1, we need to compute $p_{\\mathrm{hmm}}(\\alpha\\mid x_{t},x_{<t})$ , which is the marginal probability over all strings accepted by $\\mathcal{M}$ (see Eq. 2). In the following, we describe a tractable algorithm for computing this probability. ", "page_idx": 3}, {"type": "text", "text": "In autoregressive generation, $\\mathcal{M}$ starts from the initial state and transitions according to the transition function as each new token is generated; we denote the state of $\\mathcal{M}$ after sampling the first $t$ tokens $x_{\\leq t}$ as $s_{t}$ . In addition, we use the uppercase $S_{t}$ to denote the random variable representing the state of $\\mathcal{M}$ after sampling the first $t$ tokens: e.g., $S_{n}\\in F$ denotes the event that the token sequence $x_{1:n}$ is accepted by $\\mathcal{M}$ . Dropping the subscript \u201chmm\u201d from $p_{\\mathrm{hmm}}(\\alpha\\mid x_{t},x_{<t})$ , we compute ", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\alpha\\mid x_{t},x_{<t})=p(S_{n}\\in F\\mid x_{t},x_{<t})=p(S_{n}\\in F,x_{t},x_{<t})/p(x_{t},x_{<t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The denominator $p(x_{t},x_{<t})$ can be easily computed by the forward algorithm [32]; so we compute ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p(S_{n}\\in F,x_{t},x_{<t})=\\sum_{z_{t}}p(S_{n}\\in F\\mid z_{t},x_{t},x_{<t})\\cdot p(z_{t},x_{t},x_{<t})}}\\\\ &{}&{=\\sum_{z_{t}}\\Bigl[p(S_{n}\\in F\\mid z_{t},s_{t})\\Bigr]\\cdot p(z_{t},x_{t},x_{<t})\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the first step follows from the law of total probability and the second step follows from the Markov properties of HMMs and DFAs, as well as the fact that $s_{t}$ is fully determined by $x_{\\leq t}$ . Again, the term $\\bar{p(z_{t},x_{t},x_{<t})}$ can be computed by the forward algorithm and we reduce the problem to computing the boxed term. We compute $p(S_{n}\\in F\\mid z_{t},s_{t})$ for all $1\\leq t\\leq n$ , $1\\le z_{t}\\le h$ and $1\\le s_{t}\\le k$ via the following recurrence relation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underline{{\\left[p(S_{n}\\in F\\mid z_{t},s_{t})\\right]}}=\\sum_{z_{t+1}}p(z_{t+1}\\mid z_{t})\\cdot\\sum_{s_{t+1}}\\underline{{\\left[p(S_{n}\\in F\\mid z_{t+1},s_{t+1})\\right]}}\\cdot\\sum_{x_{t+1}\\in\\mathrm{edge}(s_{t},s_{t+1})}p(x_{t+1}\\mid z_{t+1});\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "here ed $\\mathbf{ge}(s_{t},s_{t+1}):=\\{w:\\delta(s_{t},w)\\!=\\!s_{t+1}\\}$ denotes the set of tokens $w$ that transition $\\mathcal{M}$ from $s_{t}$ to $s_{t+1}$ . The base case of the recurrence relation is given by $p(S_{n}\\!\\in\\!F\\mid z_{n},s_{n})\\!=\\!1$ if $s_{n}\\in F$ and 0 otherwise. We refer readers to the appendix for its derivation. Algorithm 1 shows the pseudo-code for sampling from $p_{\\mathrm{ctrl-g}}(x_{1:n}\\mid\\alpha)$ autoregressively, using the recurrence relations above. ", "page_idx": 4}, {"type": "text", "text": "Runtime analysis of Algorithm 1. To sample from Ctrl-G, the computation overhead (i.e. in addition to the LLM inference cost) is dominated by the computation of $p(S_{n}\\!\\in\\!F\\mid z_{t},s_{t})$ for all $t$ , $z_{t}$ and $s_{t}$ as shown in Eq. 4. Since $\\begin{array}{r}{\\sum_{x_{t+1}\\in\\mathrm{edge}(s_{t},s_{t+1})}p(x_{t+1}\\,\\bar{|}\\,z_{t+1})}\\end{array}$ does not depend on t, we can precompute and cache their values, resulting a one-time cost of $O(m h|\\Sigma|)$ . Then, note that for $s_{t}$ , we only need to consider the $s_{t+1}$ where $\\mathbf{\\nabla}\\mathbf{:}\\mathrm{d}\\mathrm{ge}(s_{t},s_{t+1})\\,\\neq\\,\\emptyset$ . Hence, fixing $t$ and $z_{t}$ , when we compute $p(S_{n}\\!\\in\\!F\\mid z_{t},s_{t})$ for all $1\\le s_{t}\\le k$ , we only need to (1) enumerate through $1\\le z_{t+1}\\!\\le h$ and (2) for each $z_{t+1}$ , we only need to go through each edge exactly once. There are $m$ edges in total, so it follows that the cost is $O(n\\cdot\\bar{h}\\cdot h\\cdot m)\\sp{\\sp{\\bullet}}=O(n m h^{\\overline{{{2}}}})$ . The total time complexity is ${\\bar{O(n m h^{2}{+}m h|\\Sigma|)}}$ , which simplifies to $O(n m h^{2})$ given that $|\\Sigma|\\!<\\!n h$ in practice. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Given a constraint $\\alpha$ represented as a DFA with m edges and an HMM with $h$ hidden states, the time complexity for sampling a sequence of n tokens from $p_{c t r l-g}(x_{1:n}\\mid\\alpha)$ is $O(n m h^{2})$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Logical reasoning vs. probabilistic reasoning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Some recent work as well as open source projects have proposed to use regular expressions (regex) to achieve structured generation from LLMs [23, 43, 50]. Regex are equivalent to DFAs in terms of the logical constraints they can represent, but the aforementioned approaches only perform pure logical reasoning over regex, which is not suitable for many constrained generation tasks. For example, consider the task of generating a sentence that ends with the phrase \u201c in the park\u201d: ", "page_idx": 4}, {"type": "table", "img_path": "58X9v92zRd/tmp/39b4a530bf0ea4594b3364c3f9a26f3749494c217014ebd2c1ce6aeeb963e773.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "guidance [23] (logical reasoning): silhouette of suspected ... an heavily secured.in the park \u2022 Ctrl-G (probabilistic reasoning): A man and a woman are walking in the park ", "page_idx": 4}, {"type": "table", "img_path": "58X9v92zRd/tmp/219b1302cef19f79828c8516c706567b3a93aa52937251d64652cb6a1e362f2c.jpg", "table_caption": ["Table 1: CommonGen results. All methods are applied to the GPT2-large model. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Even though both generations end with \u201c in the park\u201d, it is clear that the output from guidance is not desirable as it forcefully appends the phrase to some irrelevant text. The reason is that guidance, by performing pure logical reasoning, only discard the next tokens $x_{t}$ that would make $\\alpha$ unsatisfiable, while the probabilities of the other next tokens remain unchanged; in contrast, Ctrl-G performs probabilistic reasoning by estimating $p_{\\mathrm{lm}}(\\alpha\\mid x_{t},x_{<t})$ , i.e., we estimate how likely each next token $x_{t}$ would eventually lead to $\\alpha$ being satisfied. Ctrl-G subsumes the other approaches in the sense that if we set $p_{\\mathrm{hmm}}\\bar{(}\\alpha\\mid x_{t},x_{<t})=1$ for all non-zero values, that is, if we remove all probabilistic information, then it degenerates to pure logical reasoning. ", "page_idx": 5}, {"type": "text", "text": "4 Evaluating Ctrl-G on constrained generation benchmarks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Commonsense Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following prior work [21, 25], we first evaluate Ctrl-G on the Commonsene Generation (CommonGen) benchmark [18]. Each test example of CommonGen provides 3 to 5 concepts (keywords) as input and the goal is to generate a natural sentence that incorporates all keywords, allowing for any of their inflections. For example, given \u201ccar\u201d, \u201csnow\u201d and \u201cdrive\u201d as concepts, both \u201ca man drives a car on a snow covered road\u201d and \u201cthe car drove through the snow\u201d are considered acceptable. ", "page_idx": 5}, {"type": "text", "text": "DFA construction For CommonGen, given one keyword, say, \u201csnow\u201d, we adapt the Aho-Corasick algorithm [2] to construct a DFA enforcing the constraint that at least one of its inflections (e.g., \u201csnow\u201d, \u201csnowing\u201d or \u201csnowy\u201d) must appear. To encode the constraint that multiple keywords must appear, we can simply take the intersection of the individual DFAs [10]; see appendix for an example. ", "page_idx": 5}, {"type": "text", "text": "Experiments & results We use the GPT2-large checkpoint (only finetuned for domain adaptation) released by [47] as our base model and we follow the same pipeline to distill an HMM with 32768 hidden states: we sample 4M examples from the base model and train the HMM for 40 EM steps, each consisting of 100K examples. We compare Ctrl-G against FUDGE [44], NADO [25], NeuroLogic A\\*esque decoding [21] and GeLaTo [47]; GeLaTo uses the same base model as Ctrl-G. The results are summarized in Table 1, where the Constraint column shows the percentage of the outputs containing all concepts. Compared to all baselines, Ctrl-G achieves not only $100\\%$ constraint satisfaction rate but also substantially higher generation quality measured by automatic evaluation metrics [29, 19, 40, 3]. ", "page_idx": 5}, {"type": "text", "text": "Runtime comparison From an algorithmic perspective, GeLaTo only handles keyword constraints hence it is a special case of Ctrl-G. Nevertheless, Ctrl-G also runs significantly faster than GeLaTo, as shown in Table 2. The GeLaTo implementation only tensorizes the HMM inference component, while the component that reasons about the constraints runs sequentially on CPU. In contrast, by representing DFAs as (weighted) adjacency matrices, Ctrl-G tensorizes the inference procedure for both HMMs and DFAs and runs on GPUs with full parallelization. Besides, both GeLaTo and Ctrl-G runs significantly faster than A\\*esque, which explicitly performs heuristic search. ", "page_idx": 5}, {"type": "text", "text": "Generalization to more keywords To evaluate the generalization performance of Ctrl-G, we construct test examples containing 6 to 9 concepts (CommonGen+): we randomly select 100 examples with 5 concepts from the dev split of CommonGen, and then augment them with additional keywords sampled from their reference sentences. As shown in Fig. 4, Ctrl-G achieves $100\\%$ constraint satisfaction rate while preserving high generation quality across all settings. ", "page_idx": 5}, {"type": "image", "img_path": "58X9v92zRd/tmp/41e8129e9adbf103f976c879364175d2ad3d77502ef2857de15546fcf485ef17.jpg", "img_caption": ["Figure 4: CommonGen $^{+}$ results; Ctrl-G generalizes well on test examples with more than 5 concepts. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Text infilling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We also evaluate Ctrl-G on a text infliling benchmark [7] constructed from the ROC stories corpus [26]. Each test example consists of a short story with some fragments masked out, each of a specified granularity; the goal is to fill in the masks. Here is an example: \u201cJill wanted to knit her $I W O R D J\\,a$ sweater. [SENTENCE] She finished [NGRAM] for her boyfriend\u2019s birthday. Jill was [WORD].\u201d ", "page_idx": 6}, {"type": "text", "text": "DFA construction The underlying logical constraint for the task of text infliling is similar to that of CommmonGen. We can view the non-masked parts, e.g., \u201cJill wanted to knit her\u201d and \u201ca sweater.\u201d from the example above, as keyphrases, and the task reduces to generating a piece of text such that all keyphrases appear in the given order. In this setting, given $k$ text fragments, we first construct $\\mathcal{M}_{1},\\ldots,\\mathcal{M}_{k}$ using the KMP algorithm [14]; then, we concatenate them to represent the constraint that they must appear in the given order. Though DFA concatenation is intractable in general [46], we observe that the KMP DFAs can actually be concatenated in linear time. See appendix for details. ", "page_idx": 6}, {"type": "table", "img_path": "58X9v92zRd/tmp/aaf46760a0c6bdd6fa9357f2e0f45384b3ab5513e67e29f9985ca66fea201577.jpg", "table_caption": ["Table 2: Time (seconds) of generating one example on CommonGen (dev); # of HMM hidden states shown in brackets. Beam sizes used by A\\*esque, GeLaTo and Ctrl-G are 20, 128 and 128. ", "Table 3: Text infilling results (BLEU-4/ROUGE-L) across different masking ratios. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Experiments & results We use the GPT2-small checkpoint (only finetuned for domain adaptation with no supervision on the task of text infilling) released by [7] as the base model for Ctrl-G and compare against the ILM model, which is a GPT2-small model trained on this text infliling benchmark with full supervision. By applying the mask function from [7], we construct 4 test sets with different masking ratios (i.e., different percentage of masked characters) by changing the hyper-parameters. We measure the BLEU and ROUGE scores of the completed stories (i.e., including both the masked and unmasked parts) with respect to the original stories. The ILM model adopts sampling for decoding, so we run the ILM inference for 10 times to report the means and standard deviations. The results are summarized in Table 3. Based on [7], ILM is trained on a distribution with a masking ratio of approximately $15\\%$ , explaining why it achieves the best performance on the test set with $13\\%$ masking ratio. Note that the performance gap between Ctrl-G and ILM improves almost monotonically as the masking ratio increases, again illustrating the strong generalization performance of Ctrl-G. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Scaling up Ctrl-G for interactive text editing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Human-AI collaborative writing has been a long studied topic in the Human-Computer Interaction (HCI) community [12, 36]. One prior work [16] proposed CoAuthor, a graphical user interface for querying LLMs to generate continuation/insertion suggestions in arbitrary positions of a document. However, when using CoAuthor to ask for LLM suggestions, users are unable to specify their preferences. We propose to extend the CoAuthor system by allowing users to have fine-grained control over the suggestions generated by LLMs: for example, users can control the topic of the generated content by instructing LLMs to incorporate certain keyphrases, and they can also ask for more concise/detailed suggestions by controlling their lengths. For this application, we apply Ctrl-G to the TULU2-7B model and compare against prominent LLMs including GPT3.5 and GPT4. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experiment setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset construction We construct an evaluation dataset consisting of 800 test examples, each based on one story passage extracted from the CoAuthor dataset [16]. These stories are jointly written by humans and the GPT3.5-turbo-instruct model, falling under ten different topics. For each story, we randomly split it into prefix, infix and suffix; we mask out the infix and view it as a gold reference. We consider two scenarios when evaluating the models: continuation and insertion. For continuation, we only provide prefix to the model, and the model is supposed to generate one suggestion for continuation; for insertion, we provide both prefix and $s u f\\!f\\!x$ to the model and the model is required to generate a piece of text that is coherent with both prefix and suffix. Additionally, we consider imposing combinations of the following two constraints: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Keyphrase: suggestions should include one to three given keyphrases.   \n\u2022 Word Count: suggestions should contain $a$ to $b$ words where $1\\!\\le\\!a\\!\\le\\!b\\!\\le\\!32$ . ", "page_idx": 7}, {"type": "text", "text": "We consider all combinations of the following settings: insertion or continuation, w/ or w/o keyphrase constraint, w/ or w/o word-count constraint, resulting in 8 different settings. For each setting, we sample 100 stories from the CoAuthor dataset and create 100 test examples (e.g., Fig. 2). ", "page_idx": 7}, {"type": "text", "text": "Scaling up Ctrl-G We adopt the TULU2-7B [13] model, which is an instruction-tuned variant of the Llama2 [39] model with 7 billion parameters, as the base model for Ctrl-G. We further finetune the base model on 3000 examples extracted from the WritingPrompt dataset [8] for the task of text continuation, following the prompt \u201cContinue the given text:\u201d along with a story prefix. After finetuning, we use the same prompt to sample 5 million examples from the base model and train an HMM with 32768 hidden states (approx. 2 billion parameters). Note that for the task of text insertion, the base model only sees the prefix, while the suffix is incorporated as a part of the constraint $\\alpha$ ; i.e., the HMM is fully responsible for guiding the base model to generate a piece of text that will be coherent with the suffix. For generation, we sample 128 examples from $p_{\\mathrm{ctrl-g}}$ with temperature 0.7 and pick the one with the highest likelihood given by the base model as the final output. ", "page_idx": 7}, {"type": "text", "text": "Baselines We compare Ctrl-G against prominent LLMs including the GPT3.5 model and the GPT4 model. To generate output from the GPT models, we adopt the prompt provided by the OpenAI documentation for text insertion/continuation, with constraints specified in the instructions. See appendix for the specific prompt templates. In addition to the GPT models, we also compare Ctrl-G against pure instruction-tuning: specifically, we construct 1000 training examples for the task of text insertion based on the WritingPrompt dataset and further finetune the TULU2-7B model for text insertion, following the prompt \u201cGenerate the text at [INSERT_TEXT] tag:\\n{prefix}[INSERT_TEXT]{suffix}.\u201d For all baselines, for the purpose of fair comparison, we generate 128 samples for each test example and select the one with the highest probability as the final output. ", "page_idx": 7}, {"type": "text", "text": "Human evaluation To evaluate the quality of the generated outputs, we conduct human evaluation through the Amazon Mechanical Turk (MTurk) platform. For each test example, we generate the outputs from TULU2 (prompt only), GPT3.5, GPT4 and Ctrl-G respectively, and ask annotators to rate their quality on a scale from 1 to 5. For each test example, we present the generated outputs from all models, along with their original context, to the annotators side-by-side and ask them to evaluate their quality; specifically, we ask the annotators to answer the following questions: ", "page_idx": 7}, {"type": "table", "img_path": "58X9v92zRd/tmp/def5138a820d910eb343871b18edefb724b12375e5deb8418b7805a73f9c3a3d.jpg", "table_caption": ["Table 4: Evaluation results of interactive text editing. K&W indicates that the model should adhere to both keyphrase $(K)$ and word count $(W)$ constraints simultaneously. We present the human evaluation score (Quality), constraint success rate (Success), and overall satisfaction rate (Overall), which represents the proportion of examples meeting logical constraints with a Quality score above 3. "], "table_footnote": ["\u2022 Q1. is the paragraph coherent and grammatically correct? \u2022 $Q2$ . is the paragraph consistent and semantically reasonable? \u2022 Q3. based on your answers to Q1&Q2, what is your rating for the overall quality? "], "page_idx": 8}, {"type": "text", "text": "Note that we only ask human annotators to evaluate the coherency and fluency of the generated text and they are not aware of the required logical constraints. We ask three annotators to evaluate each output and compute their inter-annotator agreement score. See appendix for more details. ", "page_idx": 8}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The evaluation results are summarized in Table 4, showing the quality score,5 constraint satisfaction rate, and overall satisfaction rate. In particular, the overall satisfaction rate denotes the percentage of test examples that (1) satisfy the constraint and (2) attain average quality scores ${>}\\,3$ . For continuation, in terms of generation quality, GPT4 beats all other models; this is no surprise, as gigantic models like GPT3.5 (with 175B parameters) and GPT4 have significant advantage in generating high quality text continuations. However, despite the high generation quality, the success rates for GPT3.5 and GPT4 are relatively low (the highest $59\\%$ ) while $\\mathrm{Ctrl-G}$ always satisfy the specified constraints; hence in terms of the overall satisfaction rate, Ctrl-G beats all baselines by large margins when constraints are present. For the case of insertion, the \u201cimplicit\u201d soft constraint here is that the generated parts need to be coherent with the given suffix, which is challenging for autoregressive models; in this case, in terms of pure generation quality, Ctrl-G beats/matches with the performance of GPT4 in all settings; for insertion, the success rate of all baselines becomes even lower compared to continuation, while Ctrl-G achieves $100\\%$ success rate in all settings. In terms of overall satisfaction rate, Ctrl-G again beats all baselines. The other observation is that the generation quality of GPT4 decreases as the logical constraints become more complex, while the generation quality of Ctrl-G stays relatively consistent across all settings, demonstrating strong generalization performance. ", "page_idx": 8}, {"type": "text", "text": "5.3 Runtime analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide an empirical analysis on the runtime of $\\mathrm{Ctrl-G}$ , with TULU2-7B as the base model. In addition to the computation cost of the base LLM, the major cost of Ctrl-G lies in the computation of $p_{\\mathrm{hmm}}(\\alpha\\mid x\\!\\leq\\!t)$ , with a time complexity of $O(n m h^{2})$ (Thm. 3.2); here $n$ is the maximum sequence length, $m$ is the size (i.e. # of edges) of the DFA, and $h$ is the number of HMM hidden states. First, fixing the sequence length $n$ , we change the size of the DFA and verify that the time for generating each token scales roughly linearly with respect to the DFA size (Fig. 5 left). Then, fixing a DFA of size $\\approx900$ , we change the sequence length $n$ and measure the time for generating each token from Ctrl-G and the base LLM respectively. The gap between the two lines in Fig. 5 (right) shows the computation overhead introduced by Ctrl-G, which stays constant with respect to the sequence length. On the other hand, however, due to the attention mechanism, the time for generating each token from the base LLM scales linearly with respect to $n$ . Hence, the computation cost will be dominated by the base model when generating long sequences. The runtime measurements are conducted on an NVIDIA-A100 GPU with 80GB memory. ", "page_idx": 8}, {"type": "image", "img_path": "58X9v92zRd/tmp/1168033d9990a3a1bb9c8511a2fe1760ebdbf66dfe406cd22030cf5a93751ac4.jpg", "img_caption": ["Figure 5: Runtime analysis of Ctrl-G; Left: the generation time per token scales linearly w/ respect to DFA size. Right: the generation time per token stays constant w/ respect to sequence length. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Perspectives: improving LLM reasoning abilities via logical constraints ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we explore the use of Ctrl-G on a non-traditional constrained generation application. As a case study, we apply Ctrl-G to assist the reasoning process of the TULU2-7B model on the grade school math (GSM) benchmark. As we naively apply chain-of-thought prompting, we observe that for 293 out of the 1319 test examples, the model fails to use all numbers provided in the problem statement; this leads to a much lower accuracy on the 293 examples compare to that on the complete test set. For such 293 test examples, we apply Ctrl-G to the TULU2-7B model to enforce the constraint that all numbers from the problem statement must be generated as part of the chain-of-thought reasoning process. We sample 16 outputs from the TULU2-7B model and do a majority vote; with Ctrl-G, the model achieves $28.3\\%$ accuracy, which is $3.4\\%$ higher than the marjoity-vote accuracy without Ctrl-G. ", "page_idx": 9}, {"type": "text", "text": "Our proof-of-concept study on the GSM benchmark illustrates one potential use case of Ctrl-G beyond traditional language generation tasks. Specifically, we demonstrate the possibility of \u201capproximating\u201d soft control (i.e., better reasoning ability in this setting) via logical constraints. For future work, we motivate the application of Ctrl-G, as well as other constrained generation approaches, on a broader scope of downstream tasks: e.g., helping LLM detoxification by conditioning on a set of bad words/phrases not appearing, improving the reasoning ability of LLMs by conditioning on generating longer reasoning sequences, and controlling the topic of the generated content by conditioning on the occurrence of certain keyphrases. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose Ctrl-G, a versatile framework that enables reliable and flexible inference-time control of LLMs; given any production-ready LLM, Ctrl-G distills an HMM as its approximation and uses it to guide the LLM to generate outputs that comply with any logical constraints specified as DFAs. We show that Ctrl-G, where a 7B-parameter TULU2 model is combined with a 2B-parameter HMM, beats significantly larger LLMs like GPT4 on the task of generating text insertions/continuations with logical constraints. On commonly used constrained generation benchmarks like CommonGen, Ctrl-G beats other constrained generation approaches, as well as supervised training, by large margins. In addition to the dominant paradigm of prompt engineering, our work opens up new avenues for achieving tractable, reliable and fine-grained inference-time control of LLMs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was funded in part by the DARPA ANSR program under award FA8750-23-2-0004, the DARPA PTG Program under award HR00112220005, NSF grant #IIS-1943641, NSF CAREER Award #2339766, and AFOSR MURI via grant #FA9550-22-1-0380. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kareem Ahmed, Kai-Wei Chang, and Guy Van den Broeck. A pseudo-semantic loss for autoregressive models with logical constraints. Advances in Neural Information Processing Systems, 36, 2024.   \n[2] Alfred V Aho and Margaret J Corasick. Efficient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333\u2013340, 1975.   \n[3] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In European Conference on Computer Vision (ECCV). Springer, 2016.   \n[4] Yujin Baek, Koanho Lee, Dayeon Ki, Hyoung-Gyu Lee, Cheonbok Park, and Jaegul Choo. Towards accurate translation via semantically appropriate application of lexical constraints. arXiv preprint arXiv:2306.12089, 2023.   \n[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.   \n[6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.   \n[7] Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to fill in the blanks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2492\u20132501, 2020.   \n[8] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.   \n[9] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356\u20133369, Online, November 2020. Association for Computational Linguistics.   \n[10] John E Hopcroft, Rajeev Motwani, Rotwani, and Jeffrey D Ullman. Introduction to automata theory, languages and computability, 2000.   \n[11] Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[12] Daphne Ippolito, Ann Yuan, Andy Coenen, and Sehmon Burnam. Creative writing with an ai-powered writing assistant: Perspectives from professional writers. arXiv preprint arXiv:2211.05030, 2022.   \n[13] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023.   \n[14] Donald E Knuth, James H Morris, Jr, and Vaughan R Pratt. Fast pattern matching in strings. SIAM journal on computing, 6(2):323\u2013350, 1977.   \n[15] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. Gedi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4929\u20134952, 2021.   \n[16] Mina Lee, Percy Liang, and Qian Yang. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. In CHI Conference on Human Factors in Computing Systems, CHI \u201922. ACM, April 2022.   \n[17] Alexander K Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash K Mansinghka. Sequential monte carlo steering of large language models using probabilistic programs. arXiv preprint arXiv:2306.03081, 2023.   \n[18] Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823\u20131840, Online, November 2020. Association for Computational Linguistics.   \n[19] Chin-Yew Lin and Eduard Hovy. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2003.   \n[20] Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, and Diyi Yang. Bounding the capabilities of large language models in open text generation with prompt constraints, 2023.   \n[21] Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et al. Neurologic a\\* esque decoding: Constrained text generation with lookahead heuristics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 780\u2013799, 2022.   \n[22] Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288\u20134299, Online, June 2021. Association for Computational Linguistics.   \n[23] Scott Lundberg, Marco Ribeiro, Richard Edgar, and Harsha-Nori. Guidance: a guidance language for controlling large language models., 2024.   \n[24] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5:115\u2013133, 1943.   \n[25] Tao Meng, Sidi Lu, Nanyun Peng, and Kai-Wei Chang. Controllable text generation with neurally-decomposed oracle. In Advances in Neural Information Processing Systems 35 (NeurIPS), 2022.   \n[26] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849, 2016.   \n[27] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. ", "page_idx": 12}, {"type": "text", "text": "[28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.   \n[29] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (ACL), 2002.   \n[30] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:9538\u20139551, 2022.   \n[31] Michael O Rabin and Dana Scott. Finite automata and their decision problems. IBM journal of research and development, 3(2):114\u2013125, 1959.   \n[32] Lawrence Rabiner and Biinghwang Juang. An introduction to hidden markov models. IEEE ASSP Magazine, (1), 1986.   \n[33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.   \n[35] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2022.   \n[36] Shuming Shi, Enbo Zhao, Wei Bi, Deng Cai, Leyang Cui, Xinting Huang, Haiyun Jiang, Duyu Tang, Kaiqiang Song, Longyue Wang, et al. Effidit: An assistant for improving writing efficiency. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 508\u2013515, 2023.   \n[37] Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, and Xuezhe Ma. Evaluating large language models on controlled generation tasks, 2023.   \n[38] Yufei Tian and Nanyun Peng. Zero-shot sonnet generation with discourse-level planning and aesthetics features. arXiv preprint arXiv:2205.01821, 2022.   \n[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[40] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.   \n[41] Ke Wang, Xin Ge, Jiayi Wang, Yu Zhao, and Yuqi Zhang. Easy guided decoding in providing suggestions for interactive machine translation. arXiv preprint arXiv:2211.07093, 2022.   \n[42] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-naturalinstructions: Generalization via declarative instructions on $1600+$ nlp tasks, 2022.   \n[43] Brandon T Willard and R\u00e9mi Louf. Efficient guided generation for large language models. arXiv e-prints, pages arXiv\u20132307, 2023.   \n[44] Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2021.   \n[45] L Yao, N Peng, W Ralph, K Knight, D Zhao, and R Yan. Plan-and-write: Towards better automatic storytelling, association for the advancement of artificial intelligence. Cited on, page 92, 2019.   \n[46] Sheng Yu, Qingyu Zhuang, and Kai Salomaa. The state complexities of some basic operations on regular languages. Theoretical Computer Science, 125(2):315\u2013328, 1994.   \n[47] Honghua Zhang, Meihua Dang, Nanyun Peng, and Guy Van den Broeck. Tractable control for autoregressive language generation. In International Conference on Machine Learning, pages 40932\u201340945. PMLR, 2023.   \n[48] Jinpeng Zhang, Nini Xiao, Ke Wang, Chuanqi Dong, Xiangyu Duan, Yuqi Zhang, and Min Zhang. Disambiguated lexically constrained neural machine translation. arXiv preprint arXiv:2305.17351, 2023.   \n[49] Yusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, and Rui Zhang. Macsum: Controllable summarization with mixed attributes. Transactions of the Association for Computational Linguistics, 11:787\u2013803, 2023.   \n[50] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution of structured language model programs. arXiv preprint arXiv:2312.07104, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Derivation of Eq. (4) ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{p\\left(S_{n}\\in F\\mid z_{t},s_{t}\\right)}{z}\\Bigg\\rvert}\\\\ &{\\quad=\\sum_{x_{t+1},z_{t+1}}p(S_{n}\\in F,x_{t+1},z_{t+1}\\mid z_{t},s_{t})}\\\\ &{\\quad=\\displaystyle\\sum_{x_{t+1},z_{t+1}}p(S_{n}\\in F\\mid x_{t+1},z_{t+1},z_{t})\\cdot p(x_{t+1},z_{t+1}\\mid z_{t},s_{t})}\\\\ &{\\quad=\\displaystyle\\sum_{x_{t+1},z_{t+1}}p(S_{n}\\in F\\mid S_{t+1}=\\delta(s_{t},x_{t+1}),z_{t+1})\\cdot p(x_{t+1}\\mid z_{t+1})\\cdot p(z_{t+1}\\mid z_{t})}\\\\ &{\\quad=\\displaystyle\\sum_{z_{t+1},z_{t+1}}\\sum_{x_{t+1}<u_{t}\\in\\mathbb{R}^{n}\\setminus\\{x_{t},z_{t+1}\\}}p(S_{n}\\in F\\mid s_{t+1},z_{t+1})\\cdot p(x_{t+1}\\mid z_{t+1})\\cdot p(z_{t+1}\\mid z_{t})}\\\\ &{\\quad=\\displaystyle\\sum_{z_{t+1},z_{t}}p(z_{t+1}\\mid z_{t})\\cdot\\sum_{s_{t+1}}\\left[p(S_{n}\\in F\\mid z_{t+1},s_{t+1})\\right]\\cdot\\sum_{x_{t+1}\\in\\mathbb{R}\\oplus(s_{t},z_{t+1})}p(x_{t+1}\\mid z_{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B DFA operations ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "58X9v92zRd/tmp/5858f32174c14eff6233977493b867b6984448b93b8bafeb9da664e59252bb11.jpg", "img_caption": ["Figure 6: An example showing the intersection (logical and) and concatenation of two DFAs. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Proposition B.1. Let $\\mathcal{M}_{1}$ be a DFA such that for each accept state s, $\\delta(s,w)$ goes to a dead state for all $w\\in\\Sigma$ . Then $\\mathcal{M}_{1}$ can be concatenated with any other DFA $\\mathcal{M}_{2}$ by merging the accept states of $\\mathcal{M}_{1}$ with the initial state of $\\mathcal{M}_{2}$ ; ", "page_idx": 15}, {"type": "text", "text": "here a dead state denotes a DFA state that is (1) not an accept state and (2) only transitions to itself. Instead of formally defining what it means by \u201cmerging\u201d the initial state of $\\mathcal{M}_{2}$ with the accept states of $\\mathcal{M}_{1}$ , we refer readers to Figure 3c for such an example. ", "page_idx": 15}, {"type": "text", "text": "C Human evaluation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 5 presents the aggregated results for all questions from the Human Evaluation. Each question was answered by three workers, and we compute their inter-annotator agreement. Each worker evaluated the outputs generated by four different models for the same prefix (and suffix) within each batch. We converted these evaluations for each batch into rankings and then used the Kendall Coefficient of Concordance to assess the correlation between the rankings assigned by each worker. The average coefficient was 0.449, indicating a moderate level of agreement among the annotators. ", "page_idx": 15}, {"type": "text", "text": "Paragraph A ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Dad, the ultimate judge and jury, is now being judged. Based on dad\u2019s values? Of course not! Why would anyone put their own needs and desires aside to stroke dad's ego? It is high time dad\u2019s It is high time dad\u2019s brainstorm a new approach to handle this growing trendof judgment.ideahowtohandle thisgrowing trend of judgement. ", "page_idx": 16}, {"type": "text", "text": "", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "(1) Fluency: Is Paragraph A well-formed and fluent?   \n5: Yes, it is well-formed and fluent.   \n4:Between3and5   \n3:Somewhat,therearefwinteruptonsbutunderstandable. O2:Between1and3   \n1:No,the writing iscompletely broken.   \n(2) Coherence: Is Paragraph A consistent and logical?   \n$\\textdegree$ 5: Yes, it is consistent and logical.   \n4:Between3and5   \nmwhathrererhtrasitnsvl   \nconsistent.   \n\u25cb 2: Between 1 and 3   \n1:No,the storymakesno sense.   \n(3) Overall: Based on your answers to (1) and (2), is Paragraph A overall well-writen and logical?   \nO 5: Perfect, it is well-written and logical.   \n4:Between3and5   \nkaymsralla   \n2:Between1and3   \nO 1: Poor, the writing is broken and the story makes no sense. ", "page_idx": 16}, {"type": "text", "text": "ParagraphB ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Dad, the ultimate judge and jury, is now being judged. Based on dad's values?Of coursenot!Why would anyoneput their own needsand desiresaside to stroke dad's ego? It is high time dad's values be judged? idea how to handle this growing trend of judgement. ", "page_idx": 16}, {"type": "text", "text": "Questions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "(1) Fluency: Is Paragraph B well-formed and fluent?   \n5: Yes,it is well-formed and fluent.   \nO 4: Between 3 and 5   \n3:mwhattherarfintuintderstanl. 2:Between1and3   \n1:No,the writing iscompletelybroken.   \n(2) Coherence: Is Paragraph B consistent and logical?   \nO 5: Yes, it is consistent and logical.   \nO4:Between3and5   \n3mwhattherarfruhtranstnll   \nconsistent.   \n2:Between1and3   \n1:No,the storymakesno sense.   \n(3) Overall: Based on your answers to (1) and (2), is Paragraph B overall well-written and logical?   \nO 5: Perfect,it is well-written and logical.   \n4:Between3and5   \n3:kayasmerallcta   \n2:Between1and3   \n:Poor,thewritibroenandthestrymkesne. ", "page_idx": 16}, {"type": "text", "text": "Figure 7: Human evaluation interface on Amazon Mechanical Turk. ", "page_idx": 16}, {"type": "table", "img_path": "58X9v92zRd/tmp/ca7afcc15bcc15579eb501ce16f7d828e9e6d19b73194ff64cee2ef849de8dca.jpg", "table_caption": ["Table 5: Full human evaluation results "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 6: Prompt templates for querying the GPT3.5 and GPT4 models on the task of text editing. ", "page_idx": 17}, {"type": "text", "text": "Continuation: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Below is the opening of a story. Continue the narrative by writing the next few sentences that includes the specified keywords. Your continuation should naturally follow the themes, tone, and setting established in the opening. Aim to write a compelling and coherent continuation that could lead the story forward. Your answer must consist of at least (WordRangeStart) words and no more than (WordRangeEnd) words. Please make sure to incorporate the given keywords in to your answer. Keywords: (Keyword). ", "page_idx": 17}, {"type": "text", "text": "Insertion: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fill in the text at the [INSERT] in the following story with an appropriate sentence that includes the specified keywords. Feel free to use your knowledge, guesses, or interpretations to craft your answer, but ensure it is relevant to the context provided by the prefix and suffix. Your answer must consist of at least (WordRangeStart) words and no more than (WordRangeEnd) words. Please make sure to incorporate the given keywords in to your answer. Keywords: (Keyword).   \nStory: (Prefix)[INSERT](Suffix) ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 18}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 18}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 18}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 18}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 18}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] Justification: Our abstract accurately summarize the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: It is clearly stated that our proposed method only works for constraints that can be specified as DFAs. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Proof for Theorem 3.2. is given in the main paper and the full derivation of Equation (4) is presented in the appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code for reproducing the main results is released on github and the model checkpoints are released on huggingface hub. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The code for reproducing the main results is released on github and the model checkpoints are released on huggingface hub. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 20}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All hyper-parameters are given in the released code repository. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: For human evaluation, we computed inter-annotator agreement scores. For the text infilling experiments, which involve sampling, we reported standard deviation of the evaluation results over multiple runs. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The spec of the GPUs used for running our experiments is given. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our application has no non-trivial societal impact. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not release risky models or data. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The creators or original owners of assets used in the paper are properly credited. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The released code repository is well-documented. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See appendix for screenshots. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We did not collect any personal information about the human annotators and they have never been exposed to any form of risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]