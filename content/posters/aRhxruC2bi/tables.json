[{"figure_path": "aRhxruC2bi/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative comparison on open-vocabulary semantic segmentation. We compare in open-vocabulary semantic segmentation with vision-language models, as well as image-level supervised methods. *: Images were seen during training. \u2020: Masks from SA-1B [13] were used.", "description": "This table quantitatively compares the performance of PixelCLIP with other state-of-the-art open-vocabulary semantic segmentation methods.  It shows results on several benchmark datasets (COCO-Stuff, ADE-20K, PASCAL-Context, Cityscapes, and VOC), breaking down the methods by whether they use pixel-level labels, image-level captions, or no labels at all. The table highlights PixelCLIP's competitive performance even without using pixel-level semantic labels, surpassing some methods that do utilize them.", "section": "4.3 Results"}, {"figure_path": "aRhxruC2bi/tables/tables_6_2.jpg", "caption": "Table 2: Quantitative comparison on zero-shot mask classification. We compare the results for mask classification using ground truth masks and generated masks from ZegFormer [1] and FC-CLIP [24]. To evaluate zero-shot mask classification from CLIP, we report the results from the zero-shot branch for both methods.", "description": "This table compares the performance of PixelCLIP against other methods on zero-shot mask classification.  It shows results using both ground truth masks and masks generated by other methods, highlighting PixelCLIP's ability to perform competitively even without using ground truth data.", "section": "4.3 Results"}, {"figure_path": "aRhxruC2bi/tables/tables_7_1.jpg", "caption": "Table 3: Ablation studies. We show results on open-vocabulary semantic segmentation for validating our design choices. We also report results from OpenCLIP [55] as baseline in the results.", "description": "This table presents the ablation study results of the PixelCLIP model. It shows the impact of removing key components of the model on the performance of open-vocabulary semantic segmentation.  The components ablated include semantic clustering, the CLIP text encoder, learnable class prompts, and the momentum encoder. The baseline results are also given for comparison, using OpenCLIP [55]. Each row shows the mIoU (mean Intersection over Union) scores on several datasets (COCO, ADE-150, Context, Cityscapes, and VOC).", "section": "4.4 Ablation studies"}, {"figure_path": "aRhxruC2bi/tables/tables_7_2.jpg", "caption": "Table 3: Ablation studies. We show results on open-vocabulary semantic segmentation for validating our design choices. We also report results from OpenCLIP [55] as baseline in the results.", "description": "This table presents the ablation study results for the PixelCLIP model. It shows the impact of removing or modifying key components of the model, such as semantic clustering, the CLIP text encoder, learnable class prompts, and the momentum encoder.  The baseline results are also provided for comparison using OpenCLIP. The results are evaluated on several standard semantic segmentation datasets and metrics, allowing researchers to understand the contribution of individual components to the model's overall performance.", "section": "4.4 Ablation studies"}, {"figure_path": "aRhxruC2bi/tables/tables_7_3.jpg", "caption": "Table 3: Ablation studies. We show results on open-vocabulary semantic segmentation for validating our design choices. We also report results from OpenCLIP [55] as baseline in the results.", "description": "This table presents ablation study results on open-vocabulary semantic segmentation.  It shows the impact of different components of the PixelCLIP model, such as semantic clustering, the CLIP text encoder, learnable class prompts, and the momentum encoder, on the performance.  A baseline using OpenCLIP is also provided for comparison.", "section": "4.4 Ablation studies"}, {"figure_path": "aRhxruC2bi/tables/tables_7_4.jpg", "caption": "Table 3: Ablation studies. We show results on open-vocabulary semantic segmentation for validating our design choices. We also report results from OpenCLIP [55] as baseline in the results.", "description": "This ablation study table presents the results of experiments conducted to validate the design choices made in the PixelCLIP model.  It compares the performance of the full PixelCLIP model against versions where key components (semantic clustering, CLIP text encoder, class prompts, and momentum encoder) are removed or modified. A baseline using OpenCLIP is also included for comparison.  The results are reported across various evaluation datasets, allowing for a comprehensive assessment of each component's contribution to the overall performance.", "section": "4.4 Ablation studies"}, {"figure_path": "aRhxruC2bi/tables/tables_14_1.jpg", "caption": "Table 1: Quantitative comparison on open-vocabulary semantic segmentation. We compare in open-vocabulary semantic segmentation with vision-language models, as well as image-level supervised methods. *: Images were seen during training. \u2020: Masks from SA-1B [13] were used.", "description": "This table presents a quantitative comparison of various methods for open-vocabulary semantic segmentation. It compares the performance of PixelCLIP against other state-of-the-art methods including vision-language models and image-level supervised methods, across various evaluation datasets.  The table highlights PixelCLIP's improvements over existing methods, particularly in scenarios without densely annotated labels. The results show mean Intersection over Union (mIoU) scores for each dataset and method.", "section": "4.3 Results"}, {"figure_path": "aRhxruC2bi/tables/tables_15_1.jpg", "caption": "Table 5: Additional experiments on prompt ensembling. We show results on open-vocabulary semantic segmentation with prompt ensembling being used during only training, only inference, or both. The default setting of prompt ensembling only being used during inference is highlighted in gray.", "description": "This table presents the results of experiments evaluating the effect of prompt ensembling on open-vocabulary semantic segmentation.  Three scenarios are compared: using prompt ensembling during training only, during inference only (the default setting), and during both training and inference.  The results are shown across five evaluation datasets (COCO-Stuff, ADE-150, Context, Cityscapes, and VOC) using the mean Intersection over Union (mIoU) metric.  The numbers in parentheses represent the change in mIoU compared to the default setting (inference-only).", "section": "B.2 Analysis on Prompt Ensembling"}, {"figure_path": "aRhxruC2bi/tables/tables_15_2.jpg", "caption": "Table 6: Additional ablation studies. We show results on open-vocabulary semantic segmentation with a larger number of clusters and different training datasets.", "description": "This table presents ablation study results focusing on two key hyperparameters: the momentum update rate (\u03b3) and the number of clusters (k) used in the online clustering algorithm.  It shows the impact of these parameters on the performance of PixelCLIP across various datasets (COCO, ADE-150, Context, Cityscapes, and VOC).  Different values of \u03b3 are tested, along with results obtained using COCO-Stuff dataset instead of SA-1B to evaluate the effect of the training dataset.", "section": "B.3 Additional Ablation Studies"}, {"figure_path": "aRhxruC2bi/tables/tables_15_3.jpg", "caption": "Table 6: Additional ablation studies. We show results on open-vocabulary semantic segmentation with a larger number of clusters and different training datasets.", "description": "This table presents ablation study results focusing on two key aspects: the number of clusters used in the online clustering algorithm and the choice of training dataset.  It compares the performance of the PixelCLIP model across different settings.  The baseline results are from OpenCLIP [55], and the table shows how changes in the number of clusters (k) and the source of training data (COCO-Stuff vs. SA-1B) impact the model's performance on several evaluation datasets (COCO, ADE-150, Context, Cityscapes, VOC).", "section": "4.3 Results"}]