{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a vision-language model crucial to the proposed method, which is fundamentally based on adapting CLIP for pixel-level understanding."}, {"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment anything", "publication_date": "2023-04-04", "reason": "This paper introduces SAM, a vision foundation model that provides the masks used as supervision in the proposed approach, significantly contributing to the method's novel design."}, {"fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-10-12", "reason": "This paper introduces DINO, another vision foundation model used for mask generation; it contributes to the method's flexibility and adaptability to various foundation models."}, {"fullname_first_author": "Junbum Cha", "paper_title": "Learning to generate text-grounded mask for open-world semantic segmentation from only image-text pairs", "publication_date": "2023-06-19", "reason": "This is a highly relevant work addressing open-vocabulary semantic segmentation without pixel-level labels, providing a comparative baseline for evaluating the proposed method."}, {"fullname_first_author": "Seokju Cho", "paper_title": "Cat-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation", "publication_date": "2024-01-01", "reason": "This paper, also by the current authors, explores a related approach, contributing crucial knowledge and context for understanding and improving the proposed method. "}]}