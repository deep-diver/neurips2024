[{"figure_path": "PRAsjrmXXK/tables/tables_20_1.jpg", "caption": "Table 1: Hyperparameters for synthetic experiments.", "description": "This table lists the hyperparameters used for the synthetic experiments in the paper.  It includes the learning rates, beta values, and step sizes for each of the four training types: DPO, IPO, GR-DPO, and GR-IPO.  The beta values are related to the regularization of the model during the training. The step sizes control the magnitude of the parameter updates during optimization.  Understanding these hyperparameters is crucial for replicating the experiments and interpreting the results. ", "section": "4 Algorithm"}, {"figure_path": "PRAsjrmXXK/tables/tables_24_1.jpg", "caption": "Table 2: Hyperparameters for SFT, IPO, and GR-IPO training.", "description": "This table shows the hyperparameters used for training the supervised fine-tuning (SFT), Identity Preference Optimization (IPO), and Group Robust IPO (GR-IPO) models in the Global Opinion experiments.  It includes the learning rate, beta (\u03b2) parameter for IPO and GR-IPO, step size, and optimizer used for each training method.", "section": "5.2 Global Opinion Experiments"}]