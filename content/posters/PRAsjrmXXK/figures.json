[{"figure_path": "PRAsjrmXXK/figures/figures_1_1.jpg", "caption": "Figure 1: Current reward-free preference optimization methods typically optimize based on average human feedback. This often aligns predominantly with the preferences of the majority group (G1, R1 > R2) at the expense of minority groups (G2, R2 > R1). In contrast, our GRPO algorithm introduces adaptive weighting for different user groups and prioritizes optimizing for the worst-case group performance, leading to better alignment for the most disadvantaged groups.", "description": "This figure illustrates the core difference between traditional reward-free preference optimization methods and the proposed GRPO method. Traditional methods optimize for average performance across all groups, often leading to bias towards the majority group's preferences.  GRPO, on the other hand, focuses on the worst-performing group, adaptively weighting the importance of different groups to ensure robust and equitable alignment across all groups.", "section": "1 Introduction"}, {"figure_path": "PRAsjrmXXK/figures/figures_7_1.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure displays the results of synthetic experiments comparing Algorithm 1 (GR-DPO and GR-IPO) against other methods (DPO, IPO, IS-DPO, IS-IPO) in terms of worst-case validation loss and reward error.  Algorithm 1 consistently outperforms the others, demonstrating its effectiveness in handling diverse group sizes and response distributions.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_9_1.jpg", "caption": "Figure 3: Global opinion data: Top plots: GR-IPO leads to better worst-case final test loss and reward accuracy compared to IPO. Moreover, it leads to more balanced losses across the different groups, reducing the gap between best and worst-group loss (Group-1 vs. Group-5). Bottom plots: Log-prob. accuracy (left plot) and group weights (middle plot) during GR-IPO training. GR-IPO increases the weight on worse-performing groups (Groups-2,5) and decreases it on high-performing ones (Groups-1,3,4), leading to better worst-case accuracy. Groups-2,5 are the ones with worse log-prob. accuracy at the beginning of training (right plot with a random subset of the training data). We show the corresponding end-of-training log-prob. accuracies for GR-IPO in Figure 13 of Appendix D.", "description": "This figure shows the results of applying GR-IPO and IPO to real-world data from a global opinion survey.  The top row demonstrates that GR-IPO achieves lower maximum group loss and higher minimum reward accuracy than IPO. It also shows a more balanced distribution of losses across different groups.  The bottom row examines the training process itself. The left plot shows log-probability accuracy, indicating that GR-IPO improves accuracy over time for all groups. The middle plot shows the weights assigned to each group during training, illustrating that GR-IPO prioritizes underperforming groups. The right plot displays the initial log-probability accuracies before training.", "section": "Global Opinion Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_21_1.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "The figure presents results from synthetic experiments comparing the performance of the proposed GRPO algorithm (GR-DPO and GR-IPO) to baseline methods (DPO, IPO, IS-DPO, IS-IPO). The results show that GRPO significantly reduces the maximum validation group loss and reward error, especially in scenarios with diverse group sizes and response distributions.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_21_2.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure displays the results of synthetic experiments comparing Algorithm 1 (GR-DPO and GR-IPO) against importance sampling methods (IS-DPO/IPO) and standard DPO and IPO methods.  The results show that Algorithm 1 achieves a significantly lower worst-case validation loss and reward error. The experiment used a scenario with groups of different sizes and response distributions to test the robustness of the algorithms.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_21_3.jpg", "caption": "Figure 6: Ablation study for the trade-off parameter x in the synthetic experimental setup. Results refer to the scenario in which groups have different sizes and different responses' distribution. Note that increasing x improves worst group performance at the expense of average performance.", "description": "This figure shows the ablation study for the trade-off parameter 'x' in the synthetic experiments of the paper. The x parameter balances the worst-case and average performance of the model. The left plot shows the maximum validation group loss and the right plot shows the average validation loss.  As the value of x increases, the worst-group performance improves while the average performance decreases, indicating the trade-off between these two metrics. The shaded areas represent standard deviations across multiple runs.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_22_1.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure presents the results of synthetic experiments comparing the performance of the proposed GRPO algorithm (GR-DPO and GR-IPO) against standard DPO and IPO methods, as well as importance sampling (IS) variants.  The results show that GRPO consistently achieves a lower worst-case validation loss and reward error across different scenarios, especially when group sizes and response distributions vary.  The x-axis represents training iterations and the y-axis shows the loss or error metrics.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_22_2.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure displays the results of synthetic experiments comparing Algorithm 1 (GR-DPO and GR-IPO) against standard DPO and IPO methods, as well as importance sampling versions.  It shows that Algorithm 1 significantly reduces both worst-case validation loss and reward error.  The specific scenario shown involves groups with differing sizes and response distributions, demonstrating the algorithm's robustness to these variations.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_23_1.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure presents the results of synthetic experiments comparing different preference optimization methods.  Algorithm 1, a novel group robust preference optimization method (GRPO), shows significantly lower worst-case validation loss and reward error than importance sampling (IS) versions of DPO and IPO, as well as the standard DPO and IPO methods.  The experiment specifically tests a scenario where groups have different sizes and response distributions, highlighting GRPO's robustness in diverse group settings.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_23_2.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure presents the results of synthetic experiments comparing the performance of the proposed GRPO algorithm (GR-DPO and GR-IPO) with several baselines (DPO, IPO, IS-DPO, IS-IPO). The experiments were conducted under a scenario where groups had different sizes and response distributions. The results show that GRPO significantly outperformed other methods in terms of both worst-case validation loss and reward error, demonstrating its effectiveness in handling imbalanced group sizes and diverse response distributions.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_23_3.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure presents results from synthetic experiments comparing the performance of the proposed GRPO algorithm (GR-DPO and GR-IPO) against standard DPO and IPO methods, as well as importance sampling versions.  The results show that GRPO significantly reduces the maximum validation group loss and reward error across different experimental scenarios, particularly when group sizes and response distributions vary.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_23_4.jpg", "caption": "Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses' distribution.", "description": "This figure displays the results of synthetic experiments comparing the performance of four algorithms: Algorithm 1 (GR-DPO and GR-IPO), Importance Sampling (IS-DPO/IPO), and vanilla DPO and IPO methods. The experiment simulates scenarios with varying group sizes and response distributions.  The plots illustrate that Algorithm 1 consistently achieves lower worst-case validation group loss and reward error, showcasing its robustness in handling diverse group characteristics.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "PRAsjrmXXK/figures/figures_24_1.jpg", "caption": "Figure 3: Global opinion data: Top plots: GR-IPO leads to better worst-case final test loss and reward accuracy compared to IPO. Moreover, it leads to more balanced losses across the different groups, reducing the gap between best and worst-group loss (Group-1 vs. Group-5). Bottom plots: Log-prob. accuracy (left plot) and group weights (middle plot) during GR-IPO training. GR-IPO increases the weight on worse-performing groups (Groups-2,5) and decreases it on high-performing ones (Groups-1,3,4), leading to better worst-case accuracy. Groups-2,5 are the ones with worse log-prob. accuracy at the beginning of training (right plot with a random subset of the training data). We show the corresponding end-of-training log-prob. accuracies for GR-IPO in Figure 13 of Appendix D.", "description": "This figure displays the results of the Global Opinion experiments, comparing GR-IPO and IPO.  The top plots show that GR-IPO achieves lower maximum group loss and higher minimum reward accuracy.  The bottom plots illustrate the evolution of the log probability accuracy, group weights during training, and initial log probability accuracies. GR-IPO successfully addresses the performance imbalance across groups by adaptively weighting group losses during training.", "section": "5.2 Global Opinion Experiments"}]