[{"type": "text", "text": "Why Do We Need Weight Decay in Modern Deep Learning? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Francesco D\u2019Angelo,\u2217 Maksym Andriushchenko,\u2217Aditya Varre, Nicolas Flammarion ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Theory of Machine Learning Lab EPFL, Lausanne, Switzerland {francesco.dangelo,maksym.andriushchenko,aditya.varre,nicolas.flammarion}@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. The code is available at https://github.com/ tml-epfl/why-weight-decay ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The training of modern neural networks broadly falls into two   \nregimes: over-training, which involves multiple passes through a   \ndataset and necessitates effective regularization strategies to avoid   \noverftiting; and under-training, characterized by fewer passes due to   \nlarge amounts of training data and computational constraints (Hoff  \nmann et al., 2022). Modern deep learning unequivocally embodies   \nboth training regimes: ResNet architectures on computer vision tasks   \n(He et al., 2016) serve as quintessential examples of the over-training   \nregime, while the training of large language models (Brown et al.,   \n2020) stands as a hallmark of the under-training regime. Despite   \ntheir differences, both regimes extensively adopt weight decay as   \na regularization technique, though its effectiveness and role remain   \nsubjects of ongoing debate. For the first regime, Zhang et al. (2016)   \nshowed that even when using weight decay, neural networks can still   \nfully memorize the data, thus questioning its regularization prop  \nerties. For the second, regularization is inherently unnecessary as   \nthe limited number of passes already prevents overfitting. These   \nconsiderations raise important questions about the necessity and purpose of weight decay, introducing ", "page_idx": 0}, {"type": "image", "img_path": "YrAxxscKM2/tmp/058675457b3f0b1dacf57f69508b7d96bd6cb01a35289c9d3f38d1184df4aef9.jpg", "img_caption": ["Figure 1: Test error vs. dataset size on CIFAR- $10{-}5\\mathrm{m}$ for a fixed number of training iteration. Weight decay is helpful in both: the over-training and the undertraining, one-pass regime. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "uncertainty about its widespread usage. To illustrate the effect of weight decay in the two regimes, we conduct a simple experiment. We train a ResNet18 on subsets of the CIFAR-5m dataset (Nakkiran et al., 2020) with sizes from 10 000 to $5\\;\\mathrm{mln}$ . The computational budget of each training session is fixed to $5\\,\\mathrm{mln}$ iterations, which amounts to a range of passes between 500 and one. In the over-training regime (left in Fig. 1), weight decay does not prevent the models from achieving zero training error, but its presence still improves the test error. Attempting to explain this generalization benefti, recent works (Li & Arora, 2019; Li et al., 2020) bring forth the hypothesis that it is inadequate to think about weight decay as a capacity constraint since it still bears an effect on the training of scale-invariant models. As a result, understanding the effect of weight decay on the optimization dynamics becomes crucial to understanding generalization. Nevertheless, this line of work heavily relies on an effective learning rate (ELR) which only emerges as a consequence of scale-invariance and therefore does not apply to general architectures. In the under-training regime (right in Fig. 1), where the generalization gap vanishes, weight decay seem to facilitate faster training for slightly better accuracy. However, a characterization of the mechanisms through which weight decay impacts the training speed in this regime remains underexplored. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our work delves into the mechanisms underlying the beneftis of weight decay by training established machine learning models in both regimes: ResNet on popular vision tasks (over-training) and Transformer on text data (under-training). Towards this goal, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In the over-training regime, we unveil the mechanism by which weight decay effectively reduces the generalization gap. We demonstrate that combining weight decay with large learning rates enables non-vanishing SGD noise, which through its implicit regularization controls the norm of the Jacobian leading to improved performance. Moreover, our investigation offers a thorough explanation for the effectiveness of employing exponential moving average and learning rate decay in combination with weight decay.   \n\u2022 In the under-training regime, particularly for LLMs trained with one-pass Adam, we confirm experimentally that weight decay does not bring any regularization effect and is simply equivalent to a modified ELR. We explain the training curves commonly observed with weight decay: through this ELR, weight decay better modulates the bias-variance trade-off, resulting in lower loss. Additionally, we show that weight decay has another important practical benefit: enabling stable training with the bfloat16 precision. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The concept of employing $\\ell_{2}$ weight penalty traces back to studies on the stability of solutions for ill-posed problems (Tikhonov, 1943). It has since been extensively explored in statistics (Foster, 1961; Hoerl, 1962; Hoerl & Kennard, 1970). Krogh & Hertz (1991) present one of the earliest systematic studies on weight decay tailored for neural networks. Generalization bounds, such as those by Shalev-Shwartz & Ben-David (2014), suggest that weight decay can be sufficient for generalization, although not necessary, e.g., due to the implicit regularization of gradient methods (Soudry et al., 2018). Zhang et al. (2016) argue that while weight decay improves test accuracy, the improvement is not substantial $\\approx1{-}2\\%$ on ImageNet), indicating the key role of implicit regularization. Loshchilov & Hutter (2019) highlight the distinct effects of weight decay and $\\ell_{2}$ regularization, particularly for Adam, suggesting that Adam combined with weight decay (AdamW) leads to superior regularization and simpler hyperparameter tuning. For GPT-3 training, Brown et al. (2020) suggest that they include weight decay to provide a small amount of regularization, although we believe it is not the primary reason as we discuss in Sec. 3. ", "page_idx": 1}, {"type": "text", "text": "Multiple works have focused on weight decay as a tool influencing optimization dynamics. Van Laarhoven (2017) emphasizes that weight decay\u2019s impact on scale-invariant networks is primarily seen in terms of an effective learning rate. Zhang et al. (2018) propose three mechanisms of weight decay regularization: (1) increasing the effective learning rate for scale-invariant networks, although as we discuss, the same holds for networks beyond scale invariance (2) approximating the regularization of the input Jacobian for an optimizer inspired by second-order methods, (3) inducing a specific dampening effect in this optimizer. Li & Arora (2019); Li et al. (2020) explore the optimization properties of scale-invariant deep networks for which the effective learning rate can be derived. Lewkowycz & Gur-Ari (2020) suggest that the best generalization is achieved with the smallest $\\lambda$ although it necessitates longer training. Additionally, Lewkowycz (2021) propose a criterion for detecting when to decay the learning rate based on the evolution of the weight norm. Bjorck et al. (2021) explore the effect of decoupling weight decay, especially in the early stage of training. Li et al. (2022a) make BERT architecture scale-invariant to enhance training stability and make it more compatible with standard SGD. Recently, Kosson et al. (2023) show a mechanism through which weight decay balances rotational updates across different layers that motivates a new optimizer. ", "page_idx": 1}, {"type": "image", "img_path": "YrAxxscKM2/tmp/befca4ff28eaedaa2bbc21297fb8188cd5944dac8ec74badb9ce0486f2203b57.jpg", "img_caption": ["Figure 2: Training with and w/o weight decay. We report the test error for Resnet18 on CIFAR-10 (2a) and Tiny-ImageNet (2d) trained with and without weight decay and with small and large learning rates. We also include the correspondent EMA, represented by dashed lines. After the first 250 epochs the learning rate is decayed to $\\eta=10^{-3}$ for all the curves. We report also the L2 norm of the parameters (2c) and Train CE (2b) which after the decay converges to the same value for all the runs with the same $\\lambda$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The seminal paper of Krizhevsky et al. (2012) that introduced AlexNet suggest that weight decay serves not only as a regularizer but also reduces the model\u2019s training error, functioning as an optimization tool. In recent work, Hoffmann et al. (2022) briefly observe that weight decay enhances the training performance of Adam for training LLMs, but only after $\\approx80\\%$ of the total iterations. However, they do not provide an explanation for this behavior, a point we delve into in Sec. 3. ", "page_idx": 2}, {"type": "text", "text": "2 Weight decay in the over-training regime ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we delve into the influence of weight decay in the over-training regime, with a specific focus on image classification tasks. We focus on the training of ResNet models (He et al., 2016) using SGD on Tiny-ImageNet (Wu et al., 2017) and report additional experiments in appendix C for VGG, Resnet32 and scale-invariant Resnet architectures on CIFAR10 and CIFAR100 (Krizhevsky & Hinton, 2009). ", "page_idx": 2}, {"type": "text", "text": "Notations and setup. Let $(x_{i},y_{i})_{i=1}^{n}$ be the training inputs and labels where $x_{i}\\in\\mathbb{R}^{d},\\;y_{i}\\in\\mathbb{R}^{c}$ , and $c$ is number of classes. Let $h:\\mathbb{R}^{p}\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{c}$ be the hypothesis class of neural network and for any parameter $\\mathbf{w}\\in\\mathbb{R}^{p}$ where the function $h_{\\mathbf{w}}(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbf{\\dot{R}}^{c}$ represents the network predictions. The training loss $\\mathcal{L}$ and the $\\ell_{2}$ -regularized loss $\\mathcal{L}_{\\lambda}$ , for $\\lambda\\geq0$ , are given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{w})=\\frac{1}{N}\\sum_{i=1}^{N}\\ell\\left(y_{i},h_{\\mathbf{w}}(x_{i})\\right)\\quad\\mathcal{L}_{\\lambda}(\\mathbf{w})=\\mathcal{L}(\\mathbf{w})+\\frac{\\lambda}{2}\\big\\|\\mathbf{w}\\big\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell(\\cdot,\\cdot):\\mathbb{R}^{c}\\times\\mathbb{R}^{c}\\rightarrow\\mathbb{R}$ denotes the cross-entropy (CE) loss function. With $i_{t}\\sim\\mathbb{U}([N])$ , the SGD algorithm on $\\mathcal{L}_{\\lambda}$ (here with batch size 1 and with replacement) with a learning rate (LR) $\\eta$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\eta\\nabla_{\\mathbf{w}}\\ell\\left(y_{i_{t}},h(\\mathbf{w}_{t},x_{i_{t}})\\right)-\\eta\\lambda\\mathbf{w}_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Along the training we track three different iterates: (1) large-LR denoted by ${\\bf w}_{t}$ which use a large constant LR to exploit the SGD noise, (2) fine-tuning $\\tilde{\\mathbf{w}}_{t}$ which starting from ${\\bf w}_{t}$ use a small LR and (3) the exponential moving average (EMA) $\\bar{\\bf w}_{t}$ along the large-LR iterates. ", "page_idx": 2}, {"type": "text", "text": "2.1 Loss stabilization and weight decay ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To understand whether minimizing the regularized objective $\\mathcal{L}_{\\lambda}$ alone ensures optimal generalization, we compare test errors in Fig. 2a across various settings. Although both large and small LRs minimize the regularized objective, the evidence that optimal performance is achieved exclusively with large LRs indicates that the objective alone is insufficient to explain the benefits of WD or ensure generalization.2 This experiment reaffirms the widely acknowledged consensus that implicit regularization induced by the LR is crucial (Keskar et al., 2016; Li et al., 2019; Andriushchenko et al., ", "page_idx": 2}, {"type": "text", "text": "2023). Despite revealing an interplay between weight decay and large initial LR, the understanding of the corresponding dynamics remains limited. In this section, our goal is to comprehensively understand these dynamics, particularly to elucidate the difference in generalization between training with and without weight decay and using different learning rates, as observed in Fig. 2a. Given the regularization of the $\\ell_{2}$ norm of parameters, it is natural to wonder whether weight decay\u2019s improvement primarily stems from its ability to control the norm of the trained model. The experiment in Fig. 2c clearly illustrates that distinct training trajectories, while resulting in the same final $\\ell_{2}$ norm for parameters, can yield different levels of generalization stating that the $\\ell_{2}$ -norm of the learned model\u2019s parameters is inconsequential. This observation suggests that once the norm is constrained by weight decay, the critical factor influencing the model\u2019s generalization is the subsequent choice of LR. We should note that neural networks can be explicitly made scale-invariant by means of normalization layers and small architectural changes. Li & Arora (2019) have used this setting to reveal that the training dynamics has an effective learning rate which, depending on the L2-norm of the parameters, reduces the effect of WD to merely a scheduler for the learning rate. Our analysis presents a broader perspective that does not depend on scale invariance. At the core of our examination are the unique properties of exponentially tailed loss functions, such as CE: when the data is separable and WD is not applied, the infimum of the loss is at infinity, leading to the unbounded growth of the weight norm (Ji & Telgarsky, 2019; Soudry et al., 2018). The application of WD, by inhibiting this growth, prevents the decrease of CE loss, which in turn, significantly alters the optimization dynamics. ", "page_idx": 3}, {"type": "text", "text": "Indeed, examining the parameter norm evolution in Fig. 2c, we notice how it rapidly decreases to stabilize within a small, approximately constant interval. Similarly, the Train CE in Fig. 2b displays a stabilization effect beyond which it cannot decrease without a reduction in the learning rate. We hypothesize that WD enables an optimization dynamic akin to that on the surface of a sphere of certain radius thus triggering the following essential mechanism: ", "page_idx": 3}, {"type": "text", "text": "Constraining the parameter norm hinders the decrease of the CE, thereby enabling non-vanishing noise in SGD. This allows SGD implicit regularization to unfold and steer the optimization trajectory. ", "page_idx": 3}, {"type": "text", "text": "Next, we empirically characterize this implicit regularization mechanism. ", "page_idx": 3}, {"type": "text", "text": "2.2 The noise driven process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The long-held belief that the implicit regularization property of SGD is pivotal to the generalization capabilities of Neural Networks has been a cornerstone in the field of deep learning (Keskar et al., 2016). Many theoretical studies (Blanc et al., 2020; Li et al., 2021b; Damian et al., 2021), attempting to understand this phenomenon, draw upon the essential finding that, in the case of regression and when Gaussian noise is added to the labels, the shape of the covariance of the stochastic gradients matches the shape of the Hessian. This allows Damian et al. (2021) and Pillaud-Vivien et al. (2022) to show that the trajectory of the SGD iterates closely tracks the solution of a regularized problem. In our analysis, we conjecture a similar result; the dynamics of SGD with CE, closely track a regularized process. The important difference in our statement is that unlike Blanc et al. (2020) and Damian et al. (2021), we do not need to add noise to the labels at each iteration. Instead, weight decay, in combination with large-LR induces a label noise-like behavior via loss stabilization (Andriushchenko et al., 2023). To better understand the interplay between weight decay, loss stabilization and the noise of SGD, it is convenient to consider the binary classification case where $y_{i}\\in\\{0,1\\}$ . We also define the Jacobian of the network as $J(x_{i},\\mathbf{w}):=\\nabla h_{\\mathbf{w}}(x_{i})\\,\\in\\mathbb{R}^{p}$ and its norm averaged across the dataset: $\\begin{array}{r}{\\left\\|J(\\mathbf{w})\\right\\|_{F}^{2}=\\frac{1}{N}\\sum_{i=1}^{N}\\operatorname{Tr}\\left(\\nabla h_{\\mathbf{w}}(x_{i})\\nabla h_{\\mathbf{w}}(x_{i})^{\\top}\\right)}\\end{array}$ . Denoting the noise of the gradient by $g_{t}=\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}_{t})-\\nabla_{\\mathbf{w}}\\ell\\left(y_{i_{t}},h(\\mathbf{w}_{t},x_{i_{t}})\\right)$ , the SGD update in equation 1 becomes: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{t+1}=(1-\\eta\\lambda)\\mathbf{w}_{t}-\\eta\\nabla{\\mathcal L}(\\mathbf{w}_{t})+\\eta g_{t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, we consider a Gaussian approximation of the SGD noise, matching the first and second moment of $g_{t}$ . A substantial body of research has built upon this approximation and verified its validity (Li et al., 2020, 2021a; Smith et al., 2020; Xie et al., 2020; Li et al., 2021b). In particular Li et al. (2021a) demonstrated how modelling the SGD noise by a Gaussian is sufficient to understand its generalization. The mean is zero due to the unbiased mini-batch gradients: $\\bar{g}(\\mathbf{w}_{t}):=\\mathbb{E}[g_{t}]=0$ ", "page_idx": 3}, {"type": "image", "img_path": "YrAxxscKM2/tmp/c075789631f86aa73dc933cecb3f07090cc4423b0054e7acab596c7257e78558.jpg", "img_caption": ["Figure 3: Resnet18 on Tiny-ImageNet. Heatmap of the test error and Jacobian norm for the EMA for all the different combinations of $\\eta$ and $\\lambda$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "whereas for the second moment: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Sigma_{\\mathbf{w}_{t}}:=\\frac{1}{N}\\sum_{i=1}^{N}\\nabla\\ell_{i}(\\mathbf{w}_{t})\\nabla\\ell_{i}^{\\top}(\\mathbf{w}_{t})-\\nabla\\mathcal{L}(\\mathbf{w}_{t})\\nabla\\mathcal{L}^{\\top}(\\mathbf{w}_{t})\\approx\\frac{1}{N}\\sum_{i=1}^{N}\\nabla\\ell_{i}(\\mathbf{w}_{t})\\nabla\\ell_{i}^{\\top}(\\mathbf{w}_{t})}\\\\ {\\displaystyle\\approx\\frac{1}{N}\\sum_{i=1}^{N}(\\ell_{i}^{\\prime}(\\mathbf{w}_{t}))^{2}\\nabla h_{\\mathbf{w}_{t}}(x_{i})\\nabla h_{\\mathbf{w}_{t}}(x_{i})^{\\top}\\approx\\frac{1}{N}\\sigma_{\\eta,\\lambda}^{2}(\\mathbf{w}_{t})\\sum_{i=1}^{N}\\nabla h_{\\mathbf{w}_{t}}(x_{i})\\nabla h_{\\mathbf{w}_{t}}(x_{i})^{\\top}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In equation 3, we consider $\\nabla\\mathcal{L}(\\mathbf{w}_{t})\\nabla\\mathcal{L}^{\\top}(\\mathbf{w}_{t})$ negligible compared to $\\nabla\\ell_{i}({\\bf w}_{t})\\nabla\\ell_{i}^{\\top}({\\bf w}_{t})$ as the gradient noise variance dominates the square of the gradient noise mean. This fact has been used in previous works (Mori et al., 2022; Zhu et al., 2018; Jastrzebski et al., 2017) and in particular Jastrzebski et al. (2017) and Saxe et al. (2019) empirically verify it. Finally, we assume that the first derivative is approximately constant across all datapoints: $\\ell_{i}^{\\prime}(\\mathbf{\\dot{w}}_{t})\\approx\\ell_{j}^{\\prime}(\\mathbf{\\dot{w}}_{t})\\,\\forall i,j$ and denote this common quantity as $\\sigma_{\\eta,\\lambda}(\\mathbf{w}_{t})$ . This last approximation is referred to as \"decoupling approximation\" and has been empirically verified for classification (Mori et al., 2022). Furthermore, in App. C.6 we performed additional experiments to verify the decoupling approximation by comparing the spectrum of the SGD covariance with and without this approximation during the large LR phase. ", "page_idx": 4}, {"type": "text", "text": "Altogether, these considerations lead to the following formulation of the SGD update: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}\\approx(1-\\eta\\lambda)\\mathbf{w}_{t}-\\eta\\nabla\\mathcal{L}(\\mathbf{w}_{t})-\\frac{\\eta}{N}\\sigma_{\\eta,\\lambda}(\\mathbf{w}_{t})\\sum_{i=1}^{N}\\nabla h_{\\mathbf{w}_{t}}(x_{i})\\,\\xi_{i}^{t},\\qquad\\mathrm{where~}\\xi_{i}^{t}\\sim\\mathcal{N}(0,\\mathbb{I}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This series of approximations, allows us to define the quantity $\\sigma_{\\eta,\\lambda}(\\mathbf{w}_{t})$ , which has a fundamental role in the characterization of the training dynamics. We refer to it as the scale of the noise because it regulates its intensity. Although $\\eta$ and $\\lambda$ influence the noise scale indirectly through the trajectory $\\mathrm{ofw}_{t}$ , we explicitly highlight this dependence to emphasize our objective: to characterize the influence of WD and LR on the stochastic dynamics of SGD. We develop this characterization building upon the connection between the Jacobian of the network and the covariance of the SGD noise, which motivates the introduction of the following implicit regularization mechanism: ", "page_idx": 4}, {"type": "text", "text": "Conjecture 1. Consider the algorithm in Eq. 1 with ${\\bf w}_{0}$ initialized from a distribution $\\mu_{0}\\left(\\mathbb{R}^{\\left(p\\right)}\\right)$ . For any input $x$ , let $\\mathbf{w}_{t},h(\\mathbf{w}_{t},x)$ be the random variables that denote the iterate at time t and its functional value. The stochastic process $(h(\\mathbf{w}_{t},x))_{t\\in\\mathbb{N}}$ converges to the stationary distribution $\\mu_{\\eta,\\lambda}^{\\infty}(x)$ with mean $\\bar{\\mu}_{\\eta,\\lambda}(x)=h\\left(\\mathbf{w}_{\\eta,\\lambda}^{*},x\\right)$ for which $\\mathbf{w}_{\\eta,\\lambda}^{*}$ is a first-order stationary point of the following regularized loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{\\lambda}(\\mathbf{w}):=\\mathcal{L}_{\\lambda}(\\mathbf{w})+\\eta\\sigma_{\\eta,\\lambda}^{2}\\big\\|J(\\mathbf{w})\\big\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The conjecture illustrates how varying noise levels correspond to distinct processes, wherein the mean of each solves a unique regularized problem. Moreover, it describes how each noise level determines the strength of the regularization. Using the mean to formulate the conjecture is a natural choice; even at stationarity,3 the values of the loss ${\\mathcal{L}}({\\bf w}_{t})$ and the regularizer $\\left\\|J(\\mathbf{w})\\right\\|_{F}^{2}$ would be dominated by the noise. To unveil the existence of an implicit regularization and to analyze the evolution of the distribution, we need to look at its summary statistics, in this case, the mean. While insights from Langevin dynamics suggest employing learning rate annealing to converge towards the mean of the stationary distribution, this approach introduces additional complexities which we discuss in Section 2.3. We instead consider an exponential moving average (EMA) $(\\bar{\\mathbf{w}}_{t})_{t\\geq0}$ of the SGD iterates with parameter $\\beta=0.999$ . ", "page_idx": 4}, {"type": "image", "img_path": "YrAxxscKM2/tmp/e2be28c409934c1d34042e72775b07adc4363620eef0f1222fb0d13d58ccf85b.jpg", "img_caption": ["Figure 4: Resnet18 on Tiny-ImageNet. Training for 200 epochs with different $\\eta$ and $\\lambda$ ; the scale of the noise monotonically increases with the train loss and $\\eta\\times\\lambda$ Fig. 4a, 4b. The test error instead, presents an optimal value of $\\eta\\times\\lambda$ Fig. $4\\mathrm{c}$ while the Jacobian norm decreases monotonically Fig. 4d. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The most important implication of the conjecture is that the strength of the regularization $\\sigma_{\\eta,\\lambda}$ depends on both the LR $\\eta$ and the WD parameter $\\lambda$ . Our experiments in Fig. 3, provide empirical validation for this conjecture. When trained with different combinations of $\\eta$ and $\\lambda$ , the EMA converges to models with different test performances. When fixing $\\lambda$ there exists an optimal value of learning rate $\\eta$ which gives the best test performance while the Jacobian norm monotonically increases. A similar picture can be drawn when fixing the learning rate. ", "page_idx": 5}, {"type": "text", "text": "Therefore, given two solutions $\\bar{\\mu}_{\\eta_{l},\\lambda_{l}}$ and $\\bar{\\mu}_{\\eta_{s},\\lambda_{s}}$ for which Test error $\\begin{array}{r}{(\\bar{\\mu}_{\\eta_{l},\\lambda_{l}})<\\mathrm{Test}\\,\\mathrm{error}(\\bar{\\mu}_{\\eta_{s},\\lambda_{s}})}\\end{array}$ ; the difference in their performances can be explained with the difference in their regularization strengths $\\sigma_{\\eta,\\lambda}$ . The solution $\\bar{\\mu}_{\\eta_{l},\\lambda_{l}}$ benefits from better regularization and therefore endows better generalization properties. Furthermore, the heatmap for test error in Fig. 3 indicates that the minimum error is not achieved by a single combination of $\\eta$ and $\\lambda$ , but rather along a contour where their product $\\eta\\times\\lambda$ appears to be constant. This observation suggests an optimal trade-off between the learning rate and weight decay parameter, characterized by a curve in the parameter space where their product remains constant. Characterizing this relationship might reveal a useful tool which practitioners can adopt to optimally select values of weight decay and learning rate. Fig. 4c, 4d confirm that the product $\\eta\\lambda$ is the quantity controlling the regularization; combinations of $\\eta$ and $\\lambda$ with the same product show similar test performances and Jacobian norm. For Tiny-Imagenet, the test error exhibits an optimal value for $\\eta\\lambda\\sim0.005$ where increases beyond this point lead to over-regularization and decreases result in under-regularization. Simultaneously, the Jacobian norm $\\|J\\|_{F}$ consistently exhibits a monotonically decreasing trend. ", "page_idx": 5}, {"type": "text", "text": "To better understand the properties of the noise scale during training, we can observe in Fig. 4a, 4b how higher values of the training loss given by larger $\\eta\\lambda$ correspond to higher values of the noise scale $\\sigma_{\\eta,\\lambda}$ . The latter is measured by computing the Frobenius norm of the first derivative of the loss with respect to the predictions4 averaged over all training datapoints $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\left|\\left|\\ell_{i}^{\\prime}(\\mathbf{w})\\right|\\right|_{F}}\\end{array}$ . The scale of the noise and therefore the strength of the regularization vanish when $\\mathcal{L}(\\mathbf{w})\\approx0$ which happens for small values of $\\eta\\lambda$ . Therefore, WD combined with a large LR stabilizes the CE to a larger value, preventing the noise from vanishing and regulating the implicit regularization. This fact further confirms Conjecture 1, demonstrating that with smaller noise scales, the mean (EMA) tends towards a point where the Jacobian\u2019s norm is higher, compared to trajectories with larger noise scales. To further validate our conjecture, we created snapshot ensembles in the ResNet18 on CIFAR-10 setting, by averaging in function space along the SGD trajectory every 10 epochs for the combinations of learning rate (LR) and weight decay (WD) considered. To assess whether the mean of the stationary distribution in function space aligns closely with the EMA, where the Jacobian norm is regularized, we compared the performance of snapshot ensembles with that of the EMA. Additionally, we computed the Total Variation Distance between the softmax outputs of the ensemble and the EMA. The results are reported in App. C.7 and show a strong alignment. ", "page_idx": 5}, {"type": "text", "text": "The role of the dynamics of the norm. As discussed at the end of previous sub-section, after a rapid initial decrease of norm, the optimization resembles the dynamics of SGD projected onto a sphere. We stress that this is the crucial phase in training and the implicit regularization induced by SGD during this spherical optimization leads to better generalization. To validate this observation and isolate it from the initial norm drop, we investigate the behavior of SGD on a sphere with scale-invariant networks (Li & Arora, 2019). Scale invariance is chosen for its ease of LR tuning and for comparison with existing works (Kodryan et al., 2022; Li et al., 2020). Fig. 15 depicts a similar phenomenon as Fig. 3, where the test error vs. LR exhibits a U-shaped curve. While Kodryan et al. (2022) makes a similar observation, they do not provide an explanation. We demonstrate that the implicit regularization of the Jacobian norm is the key factor, elucidating its dependence on LR. ", "page_idx": 6}, {"type": "text", "text": "Effective learning rate vs. high training loss. Zhang et al. (2018); Van Laarhoven (2017) explored the relationship between LR and WD, introducing the concept of effective LR $\\eta_{e}=\\eta/\\lVert\\mathbf{w}\\rVert_{2}^{2}$ . These studies highlight that WD, preventing unbounded growth of the norm, enables the training process to evolve with a higher effective LR. This hypothesis is justified only with scale-invariance (which does not hold for general architecture). Furthermore, the underlying mechanism by which a higher LR enhances generalization is understood only in limited settings (Li et al., 2019). We propose that a high LR, combined with WD, leads to an increase in $\\sigma_{\\eta,\\lambda}$ . This hypothesis allows us to fully characterize and understand the mechanism through which generalization is enhanced. ", "page_idx": 6}, {"type": "text", "text": "Mixing in the function space. A simpler conjecture could have been stated in terms of the mixing of the iterates $(\\mathbf{w}_{t})_{t\\geq0}$ towards a solution of the regularized objective $\\mathbf{w}_{\\eta}^{*}$ . However, Li et al. (2020) shows empirical evidence against mixing in the parameter space, emphasizing the necessity of considering the function space. Hence, our conjecture is formulated to capture stationarity in function space. ", "page_idx": 6}, {"type": "text", "text": "On the benefit of normalization. Our conjecture characterizes the mixing distribution but does not delve into the speed of the mixing process. In our experiments, we observe that normalization layers enable faster mixing. Li et al. (2020) observes a similar phenomenon in the case of scale-invariant networks, specifically the fast equilibrium conjecture, which is addressed by Li et al. (2022b). We note that this phenomenon persists even when the models are not exactly scale-invariant. ", "page_idx": 6}, {"type": "text", "text": "2.3 EMA and Fine-tuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The large-LR phase sets the stage for SGD\u2019s inherent biases to emerge but to actually exploit such bias, reducing the stochastic noise is necessary. This can be attained in two different ways: averaging (EMA) or decaying the learning rate (fine-tuning), both strategies are widely adopted in practice. This section illustrates the relation between the two and highlights the benefits of using one over the other and the implications for our analysis. From a practical standpoint, implementing EMA is more efficient than LR-decay because it does not require additional gradient iterations or hyperparameter tuning. While both methods enhance performance, their effectiveness is contingent on being combined with loss stabilization, supporting the hypothesis that the noisy dynamics is the underlying factor for their success. Although EMA shows only a slight advantage, our experiments in Fig. 5b, 2a, 2d demonstrate that it consistently outperforms learning rate decay in various settings. ", "page_idx": 6}, {"type": "text", "text": "When empirically validating Conjecture 1, the EMA is useful to characterize the limit point (i.e., $t\\to\\infty$ ) but cannot adequately capture the dynamics throughout the entire trajectory. This limitation arises because different points along the trajectory are at different loss values, making the comparison of any relevant regularized quantities inconsistent. An approach to overcome this inconsistency is to project the iterate $\\mathbf{w}_{t}$ onto a manifold of constant loss. This can be achieved via early-stopped gradient flow (Li et al., 2021b) (SGD with small LR) on the CE loss with $\\lambda=0$ where ${\\bf w}_{t}$ is projected to a nearby point $\\tilde{\\mathbf{w}}_{t}$ , such that $\\mathscr{L}(\\tilde{\\mathbf{w}}_{t})\\sim$ const., $\\forall t$ . In practice, this corresponds to fine-tuning via LR-decay. Since after fine-tuning $\\mathcal{L}(\\tilde{\\mathbf{w}}_{t})\\approx\\mathcal{L}(\\tilde{\\mathbf{w}}_{t^{\\prime}})$ , $\\forall t,t^{\\prime}$ see Fig. 5a, we can compare $\\left\\Vert\\mathbf{\\nabla}J(\\tilde{\\mathbf{w}}_{t})\\right\\Vert_{F}$ and $\\left\\Vert J(\\tilde{\\mathbf{w}}_{t^{\\prime}})\\right\\Vert_{F}$ and understand its evolution. In the experiments detailed in Fig. 5c, we report $\\left\\|J\\right\\|_{F}$ along the fine-tuned iterates $\\tilde{\\mathbf{w}}_{t}$ and observe a decreasing trend, i.e., the sequence $\\left\\{\\left\\|J\\right(\\tilde{\\mathbf{w}}_{t}\\right)\\right\\|\\right\\}_{t\\geq0}$ is decreasing. This fact empirically validates that the entire trajectory of the iterates $(\\mathbf{w}_{t})_{t\\geq0}$ , closely following the trajectory of the fine-tuned iterates $(\\tilde{\\mathbf{w}}_{t})_{t\\geq0}$ , bias the model towards a regularized solution that might enhances generalization. This also explains why learning rate schedules, such as step-decay, which starts with a large value and then decrease it, can enhance generalization. ", "page_idx": 6}, {"type": "image", "img_path": "YrAxxscKM2/tmp/eb46eee9ae5bb378f712be7f0bfe3762930afd35fcfcb8adc94c08cafa3d0109.jpg", "img_caption": ["Figure 5: EMA vs Fine-tuning. Training of standard Resnet18 on CIFAR-10 for 100 epochs fixing $\\lambda=0.0125$ and varying the learning rate. In Fig. 5a we report different levels of loss stabilization, in Fig. 5b we report the test errors and in Fig. 5c and Fig. 5d the norm of the Jacobian and of the weights respectively. The quantities are measured for the SGD iterates, the EMA and the fine-tuning. The latter is performed for 100 epochs every 3 with $\\eta=10^{-3}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Despite providing a straightforward methodology to analyze the trajectory, LR-decay introduces additional complexities that cause deviations from the conjecture. Indeed, in Fig. 5c we observe that the final points of the fine-tuned iterates report the opposite trend compared to the EMA (smaller $\\eta$ lead to larger $\\|J\\|_{F})$ . This discrepancy is potentially due to the state-dependent nature of the SGD noise covariance in equation 3; decreasing the LR and removing WD can alter the stationary distribution and the regularized objective, leading to a different solution than anticipated by Conjecture 1. ", "page_idx": 7}, {"type": "text", "text": "3 Weight decay in the under-training regime ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we investigate how WD enhances optimization in the under-training regime. Although the phenomenon is more general, we focus on LLMs for which one-epoch training is typically used. ", "page_idx": 7}, {"type": "text", "text": "Two key effects of weight decay in the under-training regime. WD is widely used in training state-of-the-art LLMs like GPT-3, Chinchilla, and Llama (Brown et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023). While Brown et al. (2020) suggest that WD offers \"a small amount of regularization,\" its necessity remains unclear in the context of one-pass training where the population loss is directly minimized. As a sanity check, in Fig. 19 in Appendix, we verify that the generalization gap is close to zero even for models trained without WD. Instead of the regularization effect, we suggest that the two most crucial effects of WD in the under-training regime are (1) better optimization of the training loss as briefly observed by Hoffmann et al. (2022), (2) prevention of loss divergences under the bfloat16 weight precision. We reproduce this phenomenon at a smaller scale with 124M parameters in Fig. 6: the final training loss is smaller for $\\lambda$ equal to 0.1 and 0.3 compared to 0. We study both mechanisms which stand in contrast to the over-training regime of Sec. 2, where the primary concerns are not optimization and stability, but rather generalization. ", "page_idx": 7}, {"type": "image", "img_path": "YrAxxscKM2/tmp/d96a1ddaafd4725146e02e67912f66c883df82155f1143bcbada76582ccc512b.jpg", "img_caption": ["Figure 6: GPT-2-124M on OpenWebText. We reproduce the improvement from WD as in Hoffmann et al. (2022) but at a much smaller scale. Performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better point in terms of the final loss after fine-tuning. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Experimental setup. We use the NanoGPT repository (Karpathy, 2023) for training GPT-2 models (Radford et al., 2019) on OpenWebText. We train a 124M parameter model (known as GPT-2-Small) for 50 000 iterations with a batch size of 256. For most experiments, we reduce the default context length from 1024 to 256 to ensure practicality within an academic budget. Alternatively, we could have reduced the number of training iterations or batch size, but this would lead to insufficiently trained models. Unless mentioned otherwise, we train with AdamW using the default LR 0.0006, a short 400-iteration LR warmup, gradient clipping with the $\\ell_{2}$ -threshold 1.0, and $10\\times$ cosine LR decay. We keep all other hyperparameters at their default values (see App. B). ", "page_idx": 7}, {"type": "text", "text": "Better optimization with WD is reproducible at a smaller scale. The findings from Hoffmann et al. (2022) (Fig. A7 therein) indicate that WD in AdamW leads to lower training loss $\\approx0.02$ lower), primarily towards the end of training. The reduction of training loss directly translates to a better downstream performance and makes this observation practically relevant. Additionally, performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better starting point in terms of the final loss after fine-tuning. Moreover, in Fig. 21, we show that decoupling WD, as advocated by Loshchilov & Hutter (2019), is not necessary to achieve this effect: a simple $\\ell_{2}$ penalty added to the loss suffices. Lastly, in Fig. 22, we show that a similar improvement in training loss is observed for SGD with momentum suggesting that adaptive LRs are not key for this phenomenon. ", "page_idx": 7}, {"type": "image", "img_path": "YrAxxscKM2/tmp/70ddadfed78dd761d2e35166d5fe1f5440dcef9f2137ba8d970bece72437c288.jpg", "img_caption": ["Figure 7: GPT-2-124M on OpenWebText. Left: The effective LR $\\eta_{t}/\\lVert\\mathbf{w}_{t}\\rVert_{2}$ for the models reported in Fig. 6. Middle: The LR schedule that matches the effective LR $\\eta_{t}/\\lVert\\mathbf{w}_{t}\\rVert_{2}$ of the runs with weight decay 0.1 and 0.3. Right: Matching the effective LR is sufficient to match the whole training dynamics of the loss if we avoid the loss spikes by using full precision (float32 instead of bfloat16). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Effective LR induced by weight decay in AdamW. We hypothesize that the use of WD for LLM training results in an increased effective LR, even in the absence of scale invariance of the training loss for modern transformer architectures. Here we show that WD in combination with sign SGD\u2014 utilized as a surrogate for Adam (Balles & Hennig, 2018)\u2014is equivalent to projected SGD on the sphere, with an effective LR $\\eta_{\\mathrm{eff}}\\propto\\eta_{t}/\\Vert\\mathbf{w}_{t}\\Vert_{2}$ , similarly to Van Laarhoven (2017). Consider the update rule of sign SGD on loss $\\ell$ with WD: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t+1}=(1-\\eta_{t}\\lambda_{t})\\,\\mathbf{w}_{t}-\\eta_{t}\\,\\mathrm{sign}(\\nabla\\ell_{t}(\\mathbf{w}_{t}))=(1-\\eta_{t}\\lambda_{t})\\left\\|\\mathbf{w}_{t}\\right\\|_{2}\\left[\\frac{\\mathbf{w}_{t}}{\\left\\|\\mathbf{w}_{t}\\right\\|_{2}}-\\frac{\\eta_{t}\\cdot\\mathrm{sign}(\\nabla\\ell_{t}(\\mathbf{w}_{t}))}{\\left(1-\\eta_{t}\\lambda_{t}\\right)\\left\\|\\mathbf{w}_{t}\\right\\|_{2}}\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Considering the evolution of the direction $\\tilde{\\mathbf{w}}:=\\mathbf{w}/\\|\\mathbf{w}\\|_{2}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{w}}_{t+1}\\propto\\left[\\tilde{\\mathbf{w}}_{t}-\\frac{\\eta_{t}}{\\left(1-\\eta_{t}\\lambda_{t}\\right)\\left\\Vert\\mathbf{w}_{t}\\right\\Vert_{2}}\\cdot\\mathrm{sign}(\\nabla\\ell_{t}(\\mathbf{w}_{t}))\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "When $\\mathrm{sign}\\big(\\nabla\\ell_{t}\\big(\\mathbf{w}_{t}\\big)\\big)$ is determined solely by the direction $\\tilde{\\mathbf{w}}_{t}$ , the evolution of the direction of weights becomes the primary matter. This scenario occurs when the function $\\ell$ is scale-invariant or homogeneous. Observing the trend of the gradient norm and the parameter norm $\\left\\|\\mathbf{w}_{t}\\right\\|_{2}$ from Fig. 26, we note an inverse relationship, i.e., the gradient norm is higher when the parameter norm is lower. This behavior is reminiscent of scale-invariant networks for which $\\begin{array}{r}{\\nabla\\ell(\\alpha\\mathbf{\\dot{w}})=\\frac{1}{\\alpha}\\nabla\\ell(\\alpha\\mathbf{w})}\\end{array}$ , for any $\\alpha\\neq0$ . Thus, controlling parameter norms with WD allows implicit changes to the LR schedule which we verify experimentally next. ", "page_idx": 8}, {"type": "text", "text": "Matching the effective LR without weight decay. To verify the key role of the ELR $\\eta_{t}\\big/\\|\\mathbf{w}_{t}\\|_{2}$ , we train a model without WD but with ELR corresponding to models trained with $\\lambda\\in\\{0.1,0.3\\}$ and report results in Fig. 7. We observe that the whole training dynamics is matched which confirms our hypothesis. This fully explains the observation from Hoffmann et al. (2022) about the advantage of AdamW over Adam: there exists an LR schedule (albeit a non-standard one) shown in Fig. 7 (middle) that leads to the same loss proflie as the original AdamW run. However, we note that this holds only for full float32 precision, and models trained with bfloat16 precision diverge in the middle of training. This experience suggests that WD is still necessary in practice to prevent loss divergence. We also note that matching the ELR $\\eta_{t}\\big/\\|\\mathbf{w}_{t}\\|_{2}$ derived above for sign SGD instead of $\\eta_{t}\\big/\\|\\mathbf{w}_{t}\\|_{2}^{2}$ for plain SGD (Zhang et al., 2018; Hoffer et al., 2018) is critical for AdamW. Otherwise, the runs diverge very early in training, even with float32 parameter precision. ", "page_idx": 8}, {"type": "text", "text": "Explaining the training dynamics of AdamW. Classical optimization theory suggests that convergence of SGD-based algorithms primarily depends on two factors: the bias term that influences the rate at which initial conditions are forgotten and the variance term that results from noise in the gradient estimates (Moulines & Bach, 2011). We argue that these two factors, together with the observation about higher ELR for WD, can explain the loss profiles from Fig. 6. If we consider the simple case of SGD with a constant LR $\\eta$ applied to a linear least-squares problem, the expected excess risk after $t$ iterations can be bounded as a sum of a bias and variance terms: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname{Excess}\\operatorname{Risk}\\leq(1-\\eta\\mu)^{t}\\big\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\big\\|^{2}+\\eta\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\sigma$ is a uniform bound on the variance of the noise of gradient estimates, $\\mu$ a lower bound on the objective function\u2019s Hessian, ${\\bf w}_{0}$ the initial point and $\\mathbf{w}_{*}$ the optimum. For linear models, it is well-established that a larger LR accelerates the contraction of the bias term but has a detrimental impact on the variance term, ultimately leading the variance term to dominate. Coming back to the dynamics in Fig. 6, with a large ELR at the start, the convergence becomes primarily bottlenecked by the high variance term proprtional to the learning rate, leading to higher loss values in the presence of WD. Conversely, towards the end of training, when ELR and the variance term are reduced, we see that WD catches up and performs better at the end, thanks to its relatively higher ELR throughout the training and thus better bias contraction. This perspective sheds light on the observation that EMA for LLMs is most advantageous when employed with large LRs (Sanyal et al., 2023) as we also illustrate in Fig. 23. As the variance dominates in this case, variance reduction of the averaging helps. ", "page_idx": 9}, {"type": "text", "text": "Experiments with bfloat16. Training in reduced precision is essential for speeding up training and reducing GPU memory requirements (Kalamkar et al., 2019). We further elaborate on the fact that WD is not fully equivalent to higher ELR and remains necessary for stable bfloat16 training. While Scao et al. (2022) observe that usage of float16 can cause loss divergences, bfloat16 is considered much more stable and is de-facto standard in LLM training. Although bfloat16 shares the same floating-point exponent size as float32 (thus, the range of possible values is the same), it offers lower precision, with only 7 bits for the fraction instead of 23. ", "page_idx": 9}, {"type": "text", "text": "We observe that even more stable bfloat16 data type can still exhibit late-training spikes that irreparably harm model performance in standard practical settings, such as with a larger context length (e.g., 1024 instead of 256 as in the previous experiments). Therefore, we focus on this configuration for the experiments shown in Fig. 8. Runs with a moderate LR 0.001 (the default LR of Adam) without WD exhibit late-training divergence for all random seeds when using bfloat16. By comparison, training with float32 remains entirely stable. Importantly, we observe that the model does not recover after the loss spikes which contrasts with the loss spikes described in the Edge of Stability phenomenon (Cohen et al., 2021, 2022). We emphasize that all these runs use gradient clipping with the standard $\\ell_{2}$ -threshold. Finally, we observe that divergences can be prevented by reducing the LR, e.g., from 0.001 to 0.0006. However, this adjustment leads to slower training, as illustrated in Fig. 24 in the Appendix. Instead, the most effective approach is to use a higher LR of 0.001 with WD, which enables stable bfloat16 training and yields a better final training loss. ", "page_idx": 9}, {"type": "image", "img_path": "YrAxxscKM2/tmp/ba6815c35f8f459e165617aef19c073ff474e0d4717f7dcde3e5efa39b527f4d.jpg", "img_caption": ["Figure 8: GPT-2-124M on OpenWebText with context length 1024. Weight decay prevents divergence for LR 0.001 and enables stable bfloat16 training. The three random seeds are denoted with $-,\\cdot\\cdot,\\cdot\\cdot$ lines. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we demonstrate how weight decay, a single hyperparameter, can manifest three distinct effects across different training regimes: it offers regularization when combined with stochastic noise, improves optimization of the training loss, and guarantees stability in low-precision training environments. In the over-training regime, the scale of the noise $\\sigma_{\\eta,\\lambda}$ is the fundamental quantity governing the implicit regularization strength of SGD. Weight decay combined with large LR enables the noisy dynamics to evolve by maintaining the scale at a constant level. Techniques such as EMA or fine-tuning work by reducing noise, thereby allowing for the effective exploitation of the accumulated hidden regularization. Coming to the under-training regime, AdamW (Loshchilov & Hutter, 2019) was introduced as a regularization method. Instead, we argue that it is effective as it modulates the ELR to attune the bias-variance tradeoff. In addition, it also improves the stability of training. In summary, weight decay is seldom valuable as an explicit regularizer; instead, its widespread adoption can be attributed to its ability to induce desirable changes in optimization dynamics. We also acknowledge limitations of our work: given our limited computational resources, we do not conduct truly large-scale experiments. Moreover, we do not prove new theoretical results. Instead, we strive to provide a clear experimental picture and formulate general explanations for the effectiveness of weight decay in different training regimes. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Atli Kosson and Alex Damian for fruitful discussions and suggestions. M.A. was supported by the Google Fellowship and Open Phil AI Fellowship. A.V. was supported by the Swiss Data Science Center Fellowship. F.D. was supported by the Swiss National Science Foundation (grant number 212111) ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Andriushchenko, M., Varre, A., Pillaud-Vivien, L., and Flammarion, N. SGD with large step sizes learns sparse features. In International Conference on Machine Learning, 2023. ", "page_idx": 10}, {"type": "text", "text": "Balles, L. and Hennig, P. Dissecting adam: The sign, magnitude and variance of stochastic gradients. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 404\u2013413. PMLR, 10\u201315 Jul 2018.   \nBjorck, J., Weinberger, K. Q., and Gomes, C. Understanding decoupled and early weight decay. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 6777\u20136785, 2021.   \nBlanc, G., Gupta, N., Valiant, G., and Valiant, P. Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process. In Conference on Learning Theory, 2020.   \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.   \nCohen, J. M., Kaur, S., Li, Y., Kolter, J. Z., and Talwalkar, A. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations, 2021.   \nCohen, J. M., Ghorbani, B., Krishnan, S., Agarwal, N., Medapati, S., Badura, M., Suo, D., Cardoze, D., Nado, Z., Dahl, G. E., et al. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.   \nDamian, A., Ma, T., and Lee, J. D. Label noise sgd provably prefers flat global minimizers. In Advances in Neural Information Processing Systems, volume 34, pp. 27449\u201327461, 2021.   \nFoster, M. An application of the wiener-kolmogorov smoothing theory to matrix inversion. Journal of the Society for Industrial and Applied Mathematics, 9(3):387\u2013392, 1961.   \nGokaslan, A., Cohen, V., Pavlick, E., and Tellex, S. Openwebtext corpus, 2019. http: //Skylion007.github.io/OpenWebTextCorpus.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. ICCV, 2015.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, 2016.   \nHoerl, A. E. and Kennard, R. W. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55\u201367, 1970.   \nHoerl, A. R. Application of ridge analysis to regression problems. Chemical Engineering Progress, 58:54\u201359, 1962.   \nHoffer, E., Banner, R., Golan, I., and Soudry, D. Norm matters: efficient and accurate normalization schemes in deep networks. Advances in Neural Information Processing Systems, 31, 2018.   \nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. In Advances in Neural Information Processing Systems, 2022.   \nHuang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., and Weinberger, K. Q. Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017. ", "page_idx": 10}, {"type": "text", "text": "Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017. ", "page_idx": 11}, {"type": "text", "text": "Ji, Z. and Telgarsky, M. The implicit bias of gradient descent on nonseparable data. In Conference on Learning Theory, pp. 1772\u20131798. PMLR, 2019. ", "page_idx": 11}, {"type": "text", "text": "Kalamkar, D., Mudigere, D., Mellempudi, N., Das, D., Banerjee, K., Avancha, S., Vooturi, D. T., Jammalamadaka, N., Huang, J., Yuen, H., et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. ", "page_idx": 11}, {"type": "text", "text": "Karpathy, A. Nanogpt repository, 2023. URL https://github.com/karpathy/nanoGPT/. ", "page_idx": 11}, {"type": "text", "text": "Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2016. ", "page_idx": 11}, {"type": "text", "text": "Kodryan, M., Lobacheva, E., Nakhodnov, M., and Vetrov, D. P. Training scale-invariant neural networks on the sphere can happen in three regimes. Advances in Neural Information Processing Systems, 35:14058\u201314070, 2022. ", "page_idx": 11}, {"type": "text", "text": "Kosson, A., Messmer, B., and Jaggi, M. Rotational equilibrium: How weight decay balances learning across neural networks. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023. ", "page_idx": 11}, {"type": "text", "text": "Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical Report, 2009. ", "page_idx": 11}, {"type": "text", "text": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012. ", "page_idx": 11}, {"type": "text", "text": "Krogh, A. and Hertz, J. A simple weight decay can improve generalization. Advances in Neural Information Processing Systems, 1991. ", "page_idx": 11}, {"type": "text", "text": "Lewkowycz, A. How to decay your learning rate. arXiv preprint arXiv:2103.12682, 2021. ", "page_idx": 11}, {"type": "text", "text": "Lewkowycz, A. and Gur-Ari, G. On the training dynamics of deep networks with $l\\_2$ regularization. In Advances in Neural Information Processing Systems, volume 33, pp. 4790\u20134799, 2020. ", "page_idx": 11}, {"type": "text", "text": "Li, Y., Wei, C., and Ma, T. Towards explaining the regularization effect of initial large learning rate in training neural networks. In Advances in Neural Information Processing Systems, 2019.   \nLi, Z. and Arora, S. An exponential learning rate schedule for deep learning. arXiv preprint arXiv:1910.07454, 2019.   \nLi, Z., Lyu, K., and Arora, S. Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate. In Advances in Neural Information Processing Systems, volume 33, pp. 14544\u201314555, 2020.   \nLi, Z., Malladi, S., and Arora, S. On the validity of modeling sgd with stochastic differential equations (sdes). In Advances in Neural Information Processing Systems, 2021a.   \nLi, Z., Wang, T., and Arora, S. What happens after sgd reaches zero loss?\u2013a mathematical framework. arXiv preprint arXiv:2110.06914, 2021b.   \nLi, Z., Bhojanapalli, S., Zaheer, M., Reddi, S., and Kumar, S. Robust training of neural networks using scale invariant architectures. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 12656\u201312684. PMLR, 17\u201323 Jul 2022a.   \nLi, Z., Wang, T., and Yu, D. Fast mixing of stochastic gradient descent with normalization and weight decay. In Advances in Neural Information Processing Systems, 2022b. ", "page_idx": 11}, {"type": "text", "text": "Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. ", "page_idx": 11}, {"type": "text", "text": "Mori, T., Liu, Z., Liu, K., and Ueda, M. Power-law escape rate of SGD. In International Conference on Machine Learning. PMLR, 2022.   \nMoulines, E. and Bach, F. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, volume 24, 2011.   \nNakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I. Deep double descent: Where bigger models and more data hurt. In International Conference on Learning Representations, 2020.   \nPapyan, V. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size. arXiv preprint arXiv:1811.07062, 2018.   \nPillaud-Vivien, L., Reygner, J., and Flammarion, N. Label noise (stochastic) gradient descent implicitly solves the lasso for quadratic parametrisation. In Conference on Learning Theory, 2022.   \nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nSagun, L., Evci, U., Guney, V. U., Dauphin, Y., and Bottou, L. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.   \nSanyal, S., Kaddour, J., Kumar, A., and Sanghavi, S. Understanding the effectiveness of early weight averaging for training large language models. arXiv preprint arXiv:2306.03241, 2023.   \nSaxe, A. M., Bansal, Y., Dapello, J., Advani, M., Kolchinsky, A., Tracey, B. D., and Cox, D. D. On the information bottleneck theory of deep learning. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.   \nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic\u00b4, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. S., Yvon, F., Gall\u00e9, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.   \nShalev-Shwartz, S. and Ben-David, S. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \nSmith, S., Elsen, E., and De, S. On the generalization benefti of noise in stochastic gradient descent. In International Conference on Machine Learning, pp. 9058\u20139067. PMLR, 2020.   \nSoudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.   \nTikhonov, A. N. On the stability of inverse problems. In Dokl. Akad. Nauk SSSR, volume 39, pp. 195\u2013198, 1943.   \nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \nVan Laarhoven, T. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.   \nWojtowytsch, S. Stochastic gradient descent with noise of machine learning type. Part I: Discrete time analysis. arXiv preprint arXiv:2105.01650, 2021.   \nWu, J., Zhang, Q., and Xu, G. Tiny imagenet challenge. Technical report, 2017.   \nXie, Z., Sato, I., and Sugiyama, M. A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. arXiv preprint arXiv:2002.03495, 2020.   \nYao, Z., Gholami, A., Keutzer, K., and Mahoney, M. W. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pp. 581\u2013590. IEEE, 2020.   \nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2016.   \nZhang, G., Wang, C., Xu, B., and Grosse, R. Three mechanisms of weight decay regularization. In International Conference on Learning Representations, 2018.   \nZhu, Z., Wu, J., Yu, B., Wu, L., and Ma, J. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects. arXiv preprint arXiv:1803.00195, 2018. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A An additional comparison with related works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our focus in Section 2 is on an empirical illustration of the implicit regularization phenomenon, hence we refrain from attempting to prove this general conjecture, which we believe is a challenging task. The existing theoretical works Blanc et al. (2020); Li et al. (2021b); Damian et al. (2021) present two major weaknesses; they are essentially limiting analysis and as such fail at capturing the entire optimization trajectory and they primarily target regression tasks. The powerful mathematical framework for scale-invariant networks developed by Li & Arora (2019); Li et al. (2020) allows them to study in detail the benefits of normalization and its interplay with weight decay. By means of this framework, they state a fast equilibrium conjecture, which gives qualitative guarantees for the speed of convergence of the stochastic process to the stationary distribution in function space. They disentangle the evolution of the norm and the direction of the parameters and show how the evolution of the direction only depends on the intrinsic LR $\\lambda_{i}\\,=\\,\\eta\\lambda$ . However, a qualitative description of the stationary distribution, its dependence on this intrinsic LR and the relationship with generalization is missing Li et al. (2020). We attempt to fill this gap by providing a qualitative depiction of the stationary distribution and its dependence on the intrinsic LR shading some light towards understanding the relationship with generalization. The work of Kodryan et al. (2022) reports a similar observation, where the best test loss is achieved at a LR where the loss neither converges nor diverges but does not provide any explanation. ", "page_idx": 14}, {"type": "table", "img_path": "YrAxxscKM2/tmp/c5761da6e4bcb42d67262380c38be1becb2dda155338bcf87b5aa45cf2c99a14.jpg", "table_caption": ["Table 1: Comparison of our work with closely related works on regression and implicit regularization phenomenon induced by noise in the algorithm. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Training details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Full experimental details are available in our public repository https://github.com/tml-epfl/ why-weight-decay but we also list the main training details here. All the experiments are conducted for 3 different random seeds, the error-bars report one standard deviation. CIFAR-10/100 experiments. We train a VGG network without BatchNorm and preactivation ResNet-18 on CIFAR-10 and ResNet-34 on CIFAR-100 without data augmentations. We use standard SGD without momentum for all experiments. We note that $\\ell_{2}$ regularization and weight decay are exactly the same in this case. We use the standard He initialization (He et al., 2015) for all parameters. To make ResNets scale-invariant, we follow the approach of Li et al. (2020) consisting of fixing the last layer, removing the learnable parameters of the normalization layers and adding a normalization layer in the skip connection. For the experiments in Fig.11, VGG is trained with $\\mathrm{LR}=0.1$ and $\\mathrm{LR}=0.01$ and weight decay parameter is fixed to be either $\\lambda=0.0$ or $\\lambda=0.008$ . The ResNet-18 is trained with $\\mathrm{LR}=0.08$ and $\\mathrm{LR}=0.001$ and $\\lambda=0.0$ or $\\lambda=0.0125$ . The ResNet-34 is trained with $\\mathrm{LR}=0.15$ and $\\mathbf{LR}=$ 0.001 and weight decay parameter $\\lambda=0.0$ or $\\lambda=0.01$ . The total number of epochs is 1000 in all experiments in Fig.11 and all the LR are decayed at epoch 500 to 0.0001. For the experiments in Fig. 15 we use scale-invariant ResNet-18 and project the SGD iterates on the unitary sphere. We test the following LRs in the large-LR phase $(0.0001,0.0005,0.00075,0.001,0.002,0.003,0.004,0.005)$ to show different generalization performance. After 100 epochs all the learning rates are decayed to the same value 0.0001. In Fig. 15 we fine-tune every 2 epochs for 100 additional epochs with $\\scriptstyle\\mathrm{LR=0.0001}$ . To measure the Norm of the Jacobian or the Trace of the Hessian we use a subset of 5000 training datapoints. Each run requires approximately 2 GPU hours on an Nvidia A100 GPU. ", "page_idx": 14}, {"type": "text", "text": "Tiny-ImageNet experiments. We train Resnet-18 without data augmentation. We use standard SGD without momentum in all our experiments. We use the following learning rates (0.0005, 0.0010, 0.0050, 0.0100, 0.0500, 0.1000, 0.1500, 0.2000, 0.2500) and weight decay parameter (0.0200, 0.0150, 0.0125, 0.0100, 0.0075, 0.0050, 0.0025, 0.0010, 0.0005, 0.0000). To measure the norm of the Jacobian we use a subset of the training data of 2500 examples. Each run requires approximately 5GPU hour on A100. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "LLM experiments. We use the NanoGPT repository (Karpathy, 2023) for training GPT-2 models (Radford et al., 2019) on OpenWebText (Gokaslan et al., 2019). All training documents are concatenated in a single stream from which a new batch is sampled with replacement on every iteration of training. We train a 124M parameter model known as GPT-2-small for 50 000 iterations instead of the default 600 000 to make grid searches over the learning rate and weight decay parameters more accessible within an academic budget. We mostly use the context length of 256 for faster experiments except for Fig. 8 where we use the context length of 1024 since we observed that a larger context length is crucial to observe loss divergences with moderate learning rates (such as 0.001 for Adam). We train with AdamW (Loshchilov & Hutter, 2019) using batch size 256, default LR 0.0006 (unless mentioned otherwise), $\\beta_{1}=0.9$ , $\\beta_{2}=0.95$ , a short 400-iteration LR warmup, and $10\\times$ cosine LR decay. For the runs with SGD with momentum, we use the learning rate 0.3 and momentum parameter 0.9 using the same LR schedule as for AdamW. We initialize all parameters with the standard deviation equal to 0.02. We keep all other hyperparameters at their default values as in the NanoGPT repository. We perform all experiments on A100 Nvidia GPUs that support fast bfloat16 training. Each training run of GPT-2-small for 50 000 iterations takes around 12 hours on a single GPU. ", "page_idx": 15}, {"type": "text", "text": "C Weight decay for overparametrized deep networks: additional experiments and details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 A graphical illustration of the fine-tuning phase ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we plot an illustrative graphic in Fig. 9 to give an idea of what happens during the fine-tuning phase. ", "page_idx": 15}, {"type": "image", "img_path": "YrAxxscKM2/tmp/96954c42dd7e399f9e00b1d8288d91b9372c0fc047f2449a30cddf655bd10a6a.jpg", "img_caption": ["Figure 9: A graphical illustration of the fine-tuning phase. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Supporting derivations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we prove that the scale of noise is well approximated by training loss in the case of binary classification instead of classification in the case of multiple classes. The proof follows the lines of Wojtowytsch (2021). ", "page_idx": 15}, {"type": "text", "text": "Proposition 2. Assume $\\left\\|\\mathbf{w}\\right\\|\\mathbf{\\epsilon}\\in\\mathbf{\\Gamma}[a,b]$ , for any $x\\ \\in\\ {\\mathcal{D}}$ , $\\left\\|\\nabla h\\left(\\mathbf{w},x\\right)\\right\\|\\ \\in\\ [m,M]$ holds. For $n$ sufficiently large, there exists constants $c_{1},c_{2}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert g(\\mathbf{w})\\right\\rVert^{2}\\right]\\leq c_{2}\\mathcal{L}(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. The noise in the case when the gradient is computed at $(x_{i},y_{i})$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(\\mathbf{w})=\\ell^{'}(y_{i},h(\\mathbf{w},x_{i}))\\nabla h(\\mathbf{w},x_{i})-\\frac{1}{n}\\sum_{i}\\nabla\\ell^{'}(y_{i},h(\\mathbf{w},x_{i}))\\nabla h(\\mathbf{w},x_{i}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking the expectation over uniform sampling over $i$ , we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big\\|g\\big\\|^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\ell^{'}(y_{i},h(\\mathbf{w},x_{i})\\right)^{2}\\big\\|\\nabla h(\\mathbf{w},x_{i})\\big\\|^{2}-\\frac{1}{n^{2}}\\big\\|\\sum_{i}\\nabla\\ell^{'}(y_{i},h(\\mathbf{w},x_{i}))\\nabla h(\\mathbf{w},x_{i})\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Upper bound: Using the self-bounding property of the binary CE, i.e., ${\\big(}\\ell^{\\prime}{}^{2}{\\big)}~~\\leq~~l$ and $\\overline{{\\left\\|\\nabla h\\left(\\mathbf{w},x\\right)\\right\\|^{2}}}\\leq M^{2}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big\\lVert g\\big\\rVert^{2}\\le M^{2}\\frac{1}{n}\\sum_{i=1}^{n}\\ell(y_{i},h(\\mathbf{w},x_{i}))=M^{2}\\mathcal{L}(\\mathbf{w}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Comment on the Lower bound: Since the iterates are bound, we can assume there exists a constant $c$ such that $\\left(\\ell^{\\prime2}\\right)\\geq c l$ . as the second term in 6 is decreasing with $O(n^{-2})$ , we can assume that the first term is dominating and relevant and can lower bound the first term as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big\\|g\\big\\|^{2}\\geq c m^{2}\\frac{1}{n}\\sum_{i=1}^{n}\\ell(y_{i},h(\\mathbf{w},x_{i}))=c m^{2}\\mathcal{L}(\\mathbf{w}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.3 Additional figures for the over-training regime ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we report additional experimental results related to Section 2 in the main text. In Fig. 10 we report analogous results for the jacobian norm and test error of the EMA for Resnet18 on CIFAR10. In Fig. 11 and 2 we report the train CE for VGG and ResNet18 on CIFAR-10 and ResNet34 trained on CIFAR-100. We can observe how when weight decay is used in combination with large LR, the train CE stabilizes at some approximately constant level. ", "page_idx": 16}, {"type": "image", "img_path": "YrAxxscKM2/tmp/72a29aa97ff011f011c9810fe9623f26c1ca743ca6ce2529531224f822bff1ad.jpg", "img_caption": ["Figure 10: Resnet18 on CIFAR10 We train Resnet18 on CIFAR10 for 100 epochs with different $\\eta$ and $\\lambda$ . Fig. 10a reports a heatmap of the test error and Jacobian norm for the EMA for all the different combinations of parameters. The test error presents an optimal value of $\\eta$ when $\\lambda$ is fixed and, consistently with conjecture 1, the Jacobian norm decreases monotonically. More over, Fig. 10b shows how the optimality might depend only on the product $\\eta\\lambda$ for which the test error has a U-shape and the Jacobian norm decreases monotonically. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "YrAxxscKM2/tmp/926b827a0784e3d8466c8a10932c841627f78238d0b7de221b019763714dea54.jpg", "img_caption": ["Figure 11: Training with and w/o weight decay. We report the test error for VGG (11a) and ResNet (11b, 11c) trained on CIFAR-10/100 with and without weight decay and with small and large learning rates. After the first 500 epochs the learning rate is decayed to $\\eta=\\overline{{10}}^{-4}$ for all the curves. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "YrAxxscKM2/tmp/09ee0781bf734b4ecdbcbaecf25edf6a9a8b9d0bbb139caa52a30ea7f528093d.jpg", "img_caption": ["Figure 12: Training with and w/o weight decay. We report the train CE for VGG (12a) and ResNet (12b, 12c) trained on CIFAR-10/100 with and without weight decay and with small and large learning rates. After the first 500 epochs the learning rate is decayed to $\\eta=\\bar{1}0^{-4}$ for all the curves. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Connection between SGD covariance and Hessian. Much of the literature related to implicit bias relies on the assumption that the covariance of the noise of SGD is strictly related to the hessian of the loss function as discussed in Sec 2. Denoting the Hessian $\\mathrm{H}(\\mathbf{w}):=\\nabla^{\\Dot{2}}\\mathcal{L}(\\mathbf{w})$ we can write it as the so-called Gauss-Newton decomposition (Sagun et al., 2017; Papyan, 2018) $\\mathrm{H}(\\mathbf{w})=\\mathbf{G}(\\mathbf{w})+\\mathrm{E}(\\mathbf{w})$ . To measure the cosine similarity (CS) between $\\operatorname{H}(\\mathbf{w})$ and the covariance $\\Sigma_{t}$ we compute ", "page_idx": 17}, {"type": "equation", "text": "$$\nC S=\\mathbb{E}\\left[\\cos\\left(\\mathrm{H}(\\mathbf{w})\\pmb{v},\\boldsymbol{\\Sigma}_{t}\\pmb{v}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $v$ is sampled from the Gaussian distribution in $\\mathbb{R}^{p}$ and $\\cos({\\boldsymbol{u}},{\\boldsymbol{v}})=\\langle{\\boldsymbol{u}},{\\boldsymbol{v}}\\rangle/\\|{\\boldsymbol{u}}\\|\\|{\\boldsymbol{v}}\\|$ . The results are reported in Fig. 13. ", "page_idx": 17}, {"type": "image", "img_path": "YrAxxscKM2/tmp/a684d500540fa8a1c768bfcec49f89a4c19e3ee3f83500c9a24b10b4f9142517.jpg", "img_caption": ["Figure 13: Cosine similarity between hessian and Noise covariance: we compute the cosine similarity between the hessian and the covariance of the SGD noise for a scale-invariant ResNet after one epoch with large lr $\\eta=0.005$ . The results show how the two matrices are correlated and in particular how the SGD noise covariance is highly correlated with $\\operatorname{G}(\\mathbf{w})$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.4 Trace of Hessian and Jacobian Norm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As reported in Table 1 previous works related with label noise, theoretically derived the connection between the trajectory of SGD and a trajectory which regularizes either the trace of the Hessian or the Jacobian norm. The Hessian of a loss function $\\mathcal{L}$ can be decomposed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla^{2}\\mathcal{L}(\\mathbf{w})=\\sum_{i=1}^{N}\\underbrace{\\left[\\nabla h(x_{i};\\mathbf{w})\\big[\\nabla_{h}^{2}l(h(x_{i};\\mathbf{w}))\\big]\\nabla h(x_{i};\\mathbf{w})^{\\top}}_{G_{i}(\\mathbf{w})}+\\underbrace{\\sum_{c=1}^{K}[\\nabla_{h}l(h(x_{i};\\mathbf{w}))]_{c}\\nabla^{2}h(x_{i};\\mathbf{w})}_{E_{i}(\\mathbf{w})}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Many works demonstrated empirically that the $G_{i}$ is the dominant part of the Hessian decomposition and $\\begin{array}{r}{\\dot{\\nabla}^{2}L(\\mathbf{w})\\sim\\sum_{i}G_{i}}\\end{array}$ (Sagun et al., 2017). The Jacobian (J) norm instead is defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|J(\\mathbf{w})\\right\\|_{F}^{2}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{Tr}\\left(\\nabla h_{\\mathbf{w}}(x_{i})\\nabla h_{\\mathbf{w}}(x_{i})^{\\top}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "in the case of square loss, $\\nabla_{h}^{2}l=I$ where $I$ is the identity matrix. Hence, $\\mathrm{Tr}\\left(\\nabla^{2}L(\\mathbf{w})\\right)\\sim\\left\\|J\\right\\|_{F}^{2}$ . The similarity is an exact equality at an interpolating solution since $\\nabla_{h}l(h(x_{i};{\\mathbf{w}}))=0$ and therefore not much ambiguity is left regarding which quantity to study. However, in the case of classification, this fact does not hold. In particular, the trace of $\\nabla_{h}^{2}\\bar{l}$ can significantly deviate from the identity matrix and varies depending on the value of the training loss. Consequently, although the two quantities seem closely related even when using the CE, we opt for analyzing $\\|\\dot{J}\\|_{F}$ . This choice is motivated by its lack of explicit dependence on the training loss, enabling straightforward comparisons between different solutions.In the following, we report an empirical comparison of the two quantities along the training trajectory. In Fig. (14) we report a comparison between the trace of the Hessian and the Jacobian norm for both the fine-tuned iterates and the EMA. We can observe that for the finetuned iterates, both quantities display a decreasing trend; nevertheless, the ranking appears to be inverted to what is expected from previous theoretical works, i.e. larger LRs should lead to a stronger regularization effect. If we compare the EMA instead, we can see that both quantities are still decreasing along the iterations but for the Hessian we don\u2019t observe any meaningful ranking for the final solutions whereas the Jacobian norm is lower for higher LRs. ", "page_idx": 18}, {"type": "image", "img_path": "YrAxxscKM2/tmp/02c0d8ba1d0ff2c66e59d2b1106a1dbe057af8d1bdd14485a506aacb1f250e6b.jpg", "img_caption": ["Figure 14: Trace of Hessian and Jacobian Norm We train standard Resnet18 on CIFAR-10 for 100 epochs fixing $\\lambda=0.0125$ and varying the learning rate. We report the EMA and we finetune for 100 epochs every 3 with $\\eta=10^{-3}$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.5 Experiments with scale-invariant Resnet on the sphere ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In order to isolate the implicit regularization mechanism from the large initial drop and even small fluctuations in the dynamics of the $\\ell_{2}$ norm, we consider a simplified setting. We train scale-invariant networks (Li & Arora, 2019; Li et al., 2020) with projected SGD on the unitary sphere ${\\mathbb S}^{(p-1)}$ . We project the SGD iterates on the unitary sphere $\\mathbb{S}^{(p-1)}$ within the context of scale-invariant ResNet architectures (Li & Arora, 2019; Li et al., 2020). This setup is helpful for two reasons: (a) it simplifies the selection of the LR and hence tremendously reduces the experimental overhead (b) scale-invariant networks have been extensively studied in previous works Li & Arora (2019); Li et al. (2020); Kodryan et al. (2022). Moreover, in the context of scale invariance, the optimization on the sphere is the natural object to study as the evolution of the direction is the only quantity that matters. The ", "page_idx": 18}, {"type": "text", "text": "projected SGD update writes as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{t+1}=\\Pi_{\\mathbb{S}^{(p-1)}}\\left(\\mathbf{w}_{t}-\\eta\\nabla_{\\mathbf{w}}\\ell\\left(y_{i_{t}},h(\\mathbf{w}_{t},x_{i_{t}})\\right)\\right)}\\\\ {\\mathrm{where}\\quad\\Pi_{\\mathbb{S}^{(p-1)}}:\\mathbf{w}\\mapsto\\mathbf{w}/\\Big|\\vert\\mathbf{w}\\Big|\\Big|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The training framework still consists of two phases separated by a LR decay. The primary insight from our experiments on the sphere is depicted in Fig. 15: the test performance achieved in the fine-tuning phase depends on the LR used in the large-LR phase and, moreover, there is an optimal value. The work of Kodryan et al. (2022) reports a similar observation, where the best test loss is achieved at a LR where the loss neither converges too fast nor diverges but doesn\u2019t provide any explanation. Once again, our investigation reveals that the key to understand this behaviour and the dependence on the LR lies in the noisy dynamics in the large LR phase which closely tracks a regularized process. To summarize this idea we postulate a conjecture similar to the one reported in Section 2.2. ", "page_idx": 19}, {"type": "text", "text": "Conjecture 3. Consider the algorithm Eq. 8 with ${\\bf w}_{0}$ initialized from a distribution $\\mu_{0}\\left(\\mathbb{S}^{(p-1)}\\right)$ . For any input $x$ , let ${\\bf w}_{t}$ , $h(\\mathbf{w}_{t},x)$ be the random variables that denote the iterate at time $t$ and its functional value. The stochastic process $(h(\\mathbf{w}_{t},x))_{t\\in\\mathbb{N}}$ will converge to a stationary distribution $\\mu_{\\eta}^{\\infty}(x)$ with mean $\\bar{\\mu}_{\\eta}(\\boldsymbol{x})$ for which $\\mathbf{w}_{\\eta}^{*}$ is a first-order stationary point of the following regularized loss: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}(\\mathbf{w}):=\\mathcal{L}(\\mathbf{w})+\\eta\\sigma_{\\eta}^{2}\\big\\|J(\\mathbf{w})\\big\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "image", "img_path": "YrAxxscKM2/tmp/08653f5eac4612c81e8eba6c18d6b097aa7aa3572c2109621ff627ff251548bb.jpg", "img_caption": ["Figure 15: Training scale-invariant ResNets on the sphere. We train on CIFAR-10 with different large LR for the first 100 epochs and decay it to $\\eta=10^{-4}$ afterwards. Fig. (15a) reports the test error with respect to different LRs in the first phase showing the existence of an optimal value. Fig. (15b) reports the test error for the SGD iterates $(-)$ and for the EMA $(--)$ . Figure (15c) reports the decreasing trend of the test error after fine-tuning for 100 epochs with $\\eta=10^{-4}$ every 2 epochs. Finally, Fig. (15e) reports the norm of the Jacobian for the EMA and the fine-tuned iterates and Figure (15f) reports a comparison with the trace of the Hessian for the fine-tuning iterates. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.6 Empirical validation of Covariance approximation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We empirically verify the validity of the \"decoupling approximation\" introduced in Sec. 2.2 we use Stochastic Lanczos Quadrature (Yao et al., 2020) to estimate the empirical spectral density of the SGD covariance with and without the decoupling approximation, during the large-LR phase. The experiments are performed for ResNet-18 trained on the cifar10 dataset for different combinations of learning rate and weight decay which are used in the manuscript. The results in Figures 16, 17, and 18 illustrate a substantial overlap in the two spectra which serves as a validation of the reliability of our approximation. ", "page_idx": 20}, {"type": "image", "img_path": "YrAxxscKM2/tmp/d850278a761b218481aa56b8517ea0302b66fa8f6895974a6c678edd2401a955.jpg", "img_caption": ["Figure 16: We use Stochastic Lanczos Quadrature to estimate the empirical spectral density of the SGD covariance, with and without the decoupling approximation, during the large-LR phase. Experiments with ResNet-18 on the CIFAR-10 dataset, varying learning rate and weight decay, show a substantial overlap in the two spectra, validating our approximation. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "YrAxxscKM2/tmp/a7f9364c49f5918443f05faa5c886da9d8a25b64f2056f9a15f90edd009a795e.jpg", "img_caption": ["Figure 17: We use Stochastic Lanczos Quadrature to estimate the empirical spectral density of the SGD covariance, with and without the decoupling approximation, during the large-LR phase. Experiments with ResNet-18 on the CIFAR-10 dataset, varying learning rate and weight decay, show a substantial overlap in the two spectra, validating our approximation. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "YrAxxscKM2/tmp/30111168e5b58a51d0715b56f87012394010cdd912c1899af44cb68161751d45.jpg", "img_caption": ["Figure 18: We use Stochastic Lanczos Quadrature to estimate the empirical spectral density of the SGD covariance, with and without the decoupling approximation, during the large-LR phase. Experiments with ResNet-18 on the CIFAR-10 dataset, varying learning rate and weight decay, show a substantial overlap in the two spectra, validating our approximation. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.7 Empirical verification of the conjecture through snapshot ensembles ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To further validate our conjecture, we performed additional experiments. Specifically, within the same ResNet18 on CIFAR-10 setting as in our main experiments, we created snapshot ensembles (Huang et al., 2017) by averaging in function space along the SGD trajectory every 10 epochs for the combinations of learning rate (LR) and weight decay (WD) considered in the paper. ", "page_idx": 23}, {"type": "text", "text": "To assess whether the mean of the stationary distribution in function space aligns closely with the EMA, where the Jacobian norm is regularized, we compared the performance of snapshot ensembles with that of the EMA. Additionally, we computed the Total Variation Distance $\\mathcal{D}_{T V}$ between the softmax outputs of the ensemble and the EMA on the Test set ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{D}_{T V}=\\frac{1}{2N}\\sum_{i=1}^{N}\\sum_{j=1}^{C}\\left|p_{\\mathrm{ensemble},j}^{(i)}-p_{\\mathrm{EMA},j}^{(i)}\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The results in Table 2 show a strong alignment in test accuracies, while those in Table 3 indicate a low Total Variation across all combinations. Together, these findings offer further validation for our conjecture. ", "page_idx": 23}, {"type": "table", "img_path": "YrAxxscKM2/tmp/50ecd4b698fcab52f0ea01ed21694afbb1711ffec728dcdf8c2be8d2e249431d.jpg", "table_caption": ["Table 2: Test Error for Snapshot Ensemble and EMA for different values of learning rate (LR) and weight decay (WD). "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "YrAxxscKM2/tmp/e3b1f465ffe6215e26d9f9eddd73f2559687f5dae3ad5156407bd62b31422f66.jpg", "table_caption": ["Table 3: Total Variation Distance between softmax output of Ensemble and EMA. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D Weight decay for large language models: additional figures and details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We present the following additional figures related to the LLM experiments. We show that the validation loss of a GPT-2-124M model is determined by the training loss and not influenced by $\\lambda$ in Fig. 19. We also show that the generalization gap stays close to zero throughout training for different $\\lambda$ for both 124M and 774M parameter models. We show the results for models trained weight decay on LayerNorm weights in Fig. 20. We see that penalizing all parameters in weight decay (i.e., including the LayerNorm parameters) leads to the same effect for smaller $\\lambda$ (like 0.1) but underperforms on larger $\\lambda$ (like 0.3). Note that when WD is applied on all weights, this changes the optimal value of the objective. In Fig. 21, we train models with $\\ell_{2}$ regularization instead of decoupled weight decay as in AdamW (Loshchilov & Hutter, 2019). We observe that $\\ell_{2}$ regularization instead of weight decay leads to the same effect as decoupled weight decay (Loshchilov & Hutter, 2019). We train models using SGD with momentum and show the results in Fig. 22. We see that weight decay leads to a similar improvement in training loss for SGD with momentum as well. We show multiple metrics in Fig. 26 for the models shown in Fig. 6: gradient variance, gradient norm, and weight norm plots that complement Fig. 7 in the main part. In Fig. 23, we show results of weight averaging that suggests the suboptimality gap between runs with different $\\lambda$ is much smaller than what the loss at $w_{t}$ suggests. However, weight averaging is still less effective than fine-tuning with a tiny LR as in Fig. 6. Finally, in Fig. 25, we show results of models trained context length 1024. We see that the training loss over iterations for models trained with a range of LR and WD (all are bfloat16). All runs with LR smaller than 0.001 successfully converge but the final training loss is higher than for LR 0.001. In addition, we observe that lower learning rates prevent the weights from growing too much. ", "page_idx": 24}, {"type": "image", "img_path": "YrAxxscKM2/tmp/61fc0f0afd4d15007064c82a5ad5a5109b6ec915c1c550df6ece89c6bea96d8b.jpg", "img_caption": ["Figure 19: Left: The validation loss of a GPT-2-124M model is determined by the training loss and not influenced by $\\lambda$ . Right: The generalization gap stays close to zero throughout training for different $\\lambda$ for both 124M and 774M parameter models. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "YrAxxscKM2/tmp/ae42ccae75d44304900ffb06dade903da5f756fe5c6c22923f1069d9dfa61a52.jpg", "img_caption": ["Figure 20: GPT-2-124M on OpenWebText with weight decay on LayerNorm weights. Penalizing all parameters in weight decay (i.e., including the LayerNorm parameters) leads to the same effect for smaller $\\lambda$ (like 0.1) but underperforms on larger $\\lambda$ (like 0.3). Note that when WD is applied on all weights, this changes the optimal value of the objective. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "YrAxxscKM2/tmp/70b22c75649ad000cdc83b0c72f6105d0f999bea98528a912a669921cbdfb70e.jpg", "img_caption": ["Figure 21: GPT-2-124M on OpenWebText with $\\ell_{2}$ regularization. We observe that $\\ell_{2}$ regularization instead of weight decay leads to the same effect as decoupled weight decay (Loshchilov & Hutter, 2019). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "YrAxxscKM2/tmp/25fcefb336b03e4cb908da286a20ac523a061a0fa3e9bbbc699bcf6f386a43c7.jpg", "img_caption": ["Figure 22: GPT-2-124M on OpenWebText trained with SGD with momentum. Weight decay leads to a similar improvement in training loss for SGD with momentum as well (all other experiments are done with AdamW). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "YrAxxscKM2/tmp/bcea6793826a39e0e8f45d76e270baa1e77c0e79968684106ec68d5f3b196ddf.jpg", "img_caption": ["Figure 23: Weight averaging for GPT-2-124M on OpenWebText. Weight averaging $(w_{t}^{a v g})$ ) shows that the suboptimality gap between runs with different $\\lambda$ is much smaller than what the loss at $w_{t}$ suggests. However, weight averaging is still less effective than fine-tuning with a tiny LR as in Fig. 6. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "YrAxxscKM2/tmp/af826a9e3a09229fc5e274e66272f50960422d89e93066cec9f7c75b0420592d.jpg", "img_caption": ["Figure 24: GPT-2-124M on OpenWebText with context length 1024. The model trained with a moderate LR 0.001 diverges for bfloat16 but not for float32; lowering the LR prevents the divergence but leads to a worse loss. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "YrAxxscKM2/tmp/451d77e4bf4df8bee6c72fd7cbc3d80f226a569bae09e87a3e22107eb9c7f28d.jpg", "img_caption": ["Figure 25: GPT-2-124M on OpenWebText with context length 1024. (Left) The training loss over iterations for models trained with a range of LR and WD (all are bfloat16). All runs with LR smaller than 0.001 successfully converge but the final training loss is higher than for LR 0.001. (Right) Weight norms for LR in 0.0003, 0.0006, 0.001 for $\\lambda=0.1$ which does not diverge. Lower learning rates prevent the weights from growing too much. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "YrAxxscKM2/tmp/235632a3c42fb953fc189ce8cf1a5e60fe4ba57efdc6ca54ecab02dabd37e5f8.jpg", "img_caption": ["Figure 26: GPT-2-124M on OpenWebText. The gradient norm and weight norm plots for the models reported in Fig. 7. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction accurately reflect all our contributions presented in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We mention the limitations of our work at the end of Section 4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not have new standalone theoretical results like theorems or lemma, but we provide short derivations presented in Section 2.2, Section 3, and Appendix C.2 that support our experimental results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide detailed settings of our experiments in Appendix B. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The code is uploaded in supplemental material. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We describe them in the corresponding sections (Section 2 and Section 3) and provide further details in Appendix B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide error bars in figures wherever it is feasible (i.e., most experiments in Section 2), but we omit them in Section 3 since LLM experiments are much more computationally demanding. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide these details in Appendix B. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We confirm that our submissions complies with the Code of Ethics ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: In our opinion, this work does not directly lead to societal impacts since it focuses on a general improvement of our understanding of deep learning optimization methods. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate ", "page_idx": 30}, {"type": "text", "text": "deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not release data or models. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We cite the code, data, and models we use in our work, most of them directly when we discuss the corresponding experimental results (Section 2 and Section 3) and some of them in the appendix (Appendix B). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not introduce new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not conduct studies with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work does not require an IRB approval. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]