[{"figure_path": "99rOAM7Jfm/figures/figures_1_1.jpg", "caption": "Figure 1: Meta-training (left) and meta-testing (right) using our method. We train a model on multiple tasks with non-private (simulated or proxy) data to predict on target (t) points using the context (c) points. Crucially, by including a DP mechanism, which clips and adds noise to the data during training, the parameter updates (dashed arrow) teach the model to make well-calibrated and accurate predictions in the presence of DP noise. At test time, we deploy the model on real data using the same mechanism, which protects the context set with DP guarantees.", "description": "This figure illustrates the meta-learning process of the proposed DPConvCNP model.  The left panel shows the training phase using simulated or proxy data.  The model learns to map context points to predictions, with a differential privacy (DP) mechanism (clipping and adding noise) integrated into the training loop. This ensures the model learns to handle noisy data and makes well-calibrated predictions even under DP constraints. The right panel shows the testing phase, applying the trained model to real sensitive data with the same DP mechanism protecting the context set before prediction, guaranteeing privacy.", "section": "1 Introduction"}, {"figure_path": "99rOAM7Jfm/figures/figures_2_1.jpg", "caption": "Figure 2: Training our proposed model with a DP mechanism inside it, enables the model to make accurate well-calibrated predictions, even for modest privacy budgets and dataset sizes. Here, the context data (black) are protected with different (\u20ac, \u03b4) DP budgets as indicated. The model makes predictions (blue) that are remarkably close to the optimal (non-private) Bayes predictor.", "description": "The figure shows the results of training a model with a differential privacy (DP) mechanism.  The model is trained on data where the context points are protected using DP.  Despite the noise added for privacy, the model produces predictions that are very close to the optimal predictions achievable without DP, demonstrating the effectiveness of the DP mechanism and the model's ability to learn from noisy data.", "section": "Training with a DP mechanism"}, {"figure_path": "99rOAM7Jfm/figures/figures_6_1.jpg", "caption": "Figure 4: Noise magnitude comparison for the classical functional mechanism of Hall et al. [2013], the RDP-based mechanism of Jiang et al. [2023] and our improved GDP-based mechanism. The line for Hall et al. cuts off at  \u03b5 = 1 since their bound has only been proven for \u03b5 \u2264 1. We set \u0394\u00b2 = 10 and \u03b4 = 10\u207b\u00b3, which are representative values from our experiments. See Appendix A.6 for more details.", "description": "This figure compares noise magnitudes (\u03c3) required to achieve the same privacy guarantees (\u03b5,\u03b4) using three different DP mechanisms: the classical functional mechanism, the RDP mechanism, and the proposed GDP mechanism.  The GDP mechanism consistently requires significantly less noise than the other two methods, particularly at higher epsilon values.  The plot illustrates the improvement in noise reduction achieved by using the GDP mechanism compared to the RDP mechanism.", "section": "Improving the Functional Mechanism"}, {"figure_path": "99rOAM7Jfm/figures/figures_7_1.jpg", "caption": "Figure 5: Deployment-time comparison on Gaussian (top) and non-Gaussian (bottom) data. We ran the DP-SVGP for different numbers of DP-SGD steps to determine a speed versus quality-of-fit tradeoff. Reporting 95% confidence intervals.", "description": "The figure compares the inference time of DPConvCNP and DP-SVGP models on Gaussian and non-Gaussian data. The DP-SVGP's inference time is significantly longer than that of DPConvCNP, especially for larger datasets. The DP-SVGP time increases as the number of DP-SGD steps increases, representing a quality/speed trade-off.", "section": "5 Experiments & Discussion"}, {"figure_path": "99rOAM7Jfm/figures/figures_8_1.jpg", "caption": "Figure 6: Negative log-likelihoods (NLL) of the DPConvCNP and the DP-SVGP baseline on synthetic data from a EQ GP (top two rows; EQ lengthscale l) and non-Gaussian data from sawtooth waveforms (bottom two rows; waveform period \u03c4). For each point shown we report the mean NLL with its 95% confidence intervals (error bars too small to see). See Appendix C.2 for example fits.", "description": "This figure displays the negative log-likelihood (NLL) performance comparison between DPConvCNP and DP-SVGP on synthetic datasets generated from both Gaussian (EQ GP) and non-Gaussian (sawtooth waveforms) processes.  The top two rows show results for the Gaussian process, varying the lengthscale (l) of the EQ kernel, while the bottom two rows show results for the sawtooth process, varying the period (\u03c4).  Different privacy budgets (\u03b5) and dataset sizes (N) are evaluated.  The plots illustrate how both models perform under different conditions and highlight the DPConvCNP's ability to handle non-Gaussian data effectively.", "section": "Experiments & Discussion"}, {"figure_path": "99rOAM7Jfm/figures/figures_9_1.jpg", "caption": "Figure 7: Left; Negative log-likelihoods of the DPConvCNP and the DP-SVGP baseline on the sim to real task with the !Kung dataset, predicting individuals' height from their age (left col.) or their weight from their age (right col.). For each point shown here, we partition each dataset into a context and target at random, make predictions, and repeat this procedure 512 times. We report mean NLL with its 95% confidence intervals. Error bars are to small to see here. Right; Example predictions for the DPConvCNP and the DP-SVGP, showing the mean and 95% confidence intervals, with N = 300, \u20ac = 1.00, \u03b4 = 10-3. The DPConvCNP is visibly better-calibrated than the DP-SVGP.", "description": "This figure compares the performance of DPConvCNP and DP-SVGP on a sim-to-real task using the !Kung dataset. The left panels show negative log-likelihood (NLL) results for predicting height and weight from age, demonstrating DPConvCNP's superior performance, especially with smaller datasets. The right panels visualize example predictions with confidence intervals, highlighting DPConvCNP's better calibration.", "section": "Sim-to-real tasks"}, {"figure_path": "99rOAM7Jfm/figures/figures_19_1.jpg", "caption": "Figure S1: DPConvCNP performance on the GP modelling task, where the data are generated using an EQ GP with lengthscale l. We train three models per e, l combination, keeping d = 10\u22123 fixed as well as the clipping threshold C = 2.00 and noise weight t = 0.50 fixed. Specifically, we train one model where only noise to the signal channel (red; no clip, no density), one model where noise and clipping are applied to the signal channel (orange; clip, no density noise) and another model where noise and clipping to the signal channel as well as noise to the density channel are applied (green; clip, density noise). We also show the NLL of the oracle, non-DP, Bayesian posterior, which is the best average NLL that can be obtained on this task (blue). Lastly, we show a bound to the functional mechanism (black), which is a lower bound on the NLL that can be obtained with the functional mechanism with C = 2.00, t = 0.50 on this task. We used 512 evaluation tasks for each N, l, e combination, and report mean NLLs together with their 95% confidence intervals. Note that the error bars are plotted but are too small to see in the plot.", "description": "This figure compares the performance of different DPConvCNP models with varying levels of noise and clipping on a Gaussian process regression task. It shows how the negative log-likelihood (NLL) changes with the number of data points (N) for different privacy budgets (epsilon). The results are compared to the optimal Bayesian posterior (oracle) and a lower bound based on the functional mechanism.", "section": "C Additional results"}, {"figure_path": "99rOAM7Jfm/figures/figures_19_2.jpg", "caption": "Figure 2: Training our proposed model with a DP mechanism inside it, enables the model to make accurate well-calibrated predictions, even for modest privacy budgets and dataset sizes. Here, the context data (black) are protected with different (\u20ac, \u03b4) DP budgets as indicated. The model makes predictions (blue) that are remarkably close to the optimal (non-private) Bayes predictor.", "description": "The figure shows the well-calibrated predictions of the proposed model (DPConvCNP) even with modest privacy budgets and dataset sizes. The context data (black) is protected with different (\u03b5, \u03b4) DP budgets. The model makes predictions (blue) which are very close to the optimal (non-private) Bayes predictor.", "section": "Training with a DP mechanism"}, {"figure_path": "99rOAM7Jfm/figures/figures_24_1.jpg", "caption": "Figure 2: Training our proposed model with a DP mechanism inside it, enables the model to make accurate well-calibrated predictions, even for modest privacy budgets and dataset sizes. Here, the context data (black) are protected with different (\u20ac, \u03b4) DP budgets as indicated. The model makes predictions (blue) that are remarkably close to the optimal (non-private) Bayes predictor.", "description": "The figure shows the well-calibrated predictions of the DPConvCNP model trained with the differential privacy mechanism.  Even with small datasets and modest privacy budgets, the model's predictions are very close to the optimal non-private Bayes predictor.  This demonstrates that the model effectively learns to produce accurate and calibrated predictions in the presence of differential privacy noise.", "section": "3.2 Differential Privacy"}, {"figure_path": "99rOAM7Jfm/figures/figures_25_1.jpg", "caption": "Figure 2: Training our proposed model with a DP mechanism inside it, enables the model to make accurate well-calibrated predictions, even for modest privacy budgets and dataset sizes. Here, the context data (black) are protected with different (\u20ac, \u03b4) DP budgets as indicated. The model makes predictions (blue) that are remarkably close to the optimal (non-private) Bayes predictor.", "description": "The figure shows the well-calibrated predictions of the proposed DPConvCNP model compared to the optimal Bayes predictor (non-private).  The context data is protected using Differential Privacy (DP) with varying privacy budgets (epsilon and delta).  Even with modest privacy budgets and dataset sizes, the model makes accurate predictions, demonstrating the effectiveness of incorporating the DP mechanism into the meta-learning process.", "section": "Training with a DP mechanism"}, {"figure_path": "99rOAM7Jfm/figures/figures_26_1.jpg", "caption": "Figure 6: Negative log-likelihoods (NLL) of the DPConvCNP and the DP-SVGP baseline on synthetic data from a EQ GP (top two rows; EQ lengthscale l) and non-Gaussian data from sawtooth waveforms (bottom two rows; waveform period \u03c4). For each point shown we report the mean NLL with its 95% confidence intervals (error bars too small to see). See Appendix C.2 for example fits.", "description": "This figure compares the performance of DPConvCNP and DP-SVGP on synthetic datasets generated from both Gaussian (EQ GP) and non-Gaussian (sawtooth waveforms) processes.  The top two rows show results for Gaussian data with varying lengthscales (l), while the bottom two rows show results for non-Gaussian data with varying periods (\u03c4).  Different privacy budgets (\u03b5) and dataset sizes (N) are also tested.  The plot displays the negative log-likelihood (NLL), a measure of predictive accuracy, with 95% confidence intervals represented by error bars (though they are too small to be visible in the figure). Appendix C.2 provides detailed example fits for further analysis.", "section": "5.1 Synthetic tasks"}, {"figure_path": "99rOAM7Jfm/figures/figures_27_1.jpg", "caption": "Figure 6: Negative log-likelihoods (NLL) of the DPConvCNP and the DP-SVGP baseline on synthetic data from a EQ GP (top two rows; EQ lengthscale l) and non-Gaussian data from sawtooth waveforms (bottom two rows; waveform period \u03c4). For each point shown we report the mean NLL with its 95% confidence intervals (error bars too small to see). See Appendix C.2 for example fits.", "description": "This figure compares the performance of DPConvCNP and DP-SVGP on synthetic datasets generated from both Gaussian (EQ) and non-Gaussian (sawtooth) processes.  The top two rows show results for Gaussian processes with varying lengthscales (l), while the bottom two rows show results for non-Gaussian sawtooth waveforms with varying periods (\u03c4).  For each combination of data type, lengthscale/period, privacy budget (\u03b5), and dataset size (N), the negative log-likelihood (NLL) and its 95% confidence interval are reported.  The figure demonstrates that DPConvCNP is competitive with DP-SVGP in Gaussian settings, and outperforms it significantly in non-Gaussian settings.", "section": "5.1 Synthetic tasks"}, {"figure_path": "99rOAM7Jfm/figures/figures_28_1.jpg", "caption": "Figure S6: Additional results using the DPConvCNP on the EQ and sawtooth synthetic tasks with stricter DP parameters, namely all combinations of  \u2208 = {1/3, 1} and \u03b4 = {10\u22125,10-3}. The overall setup in this figure is identical to that in Figure 6, except the amortised DPConvCNP is trained on randomly chosen \u2208 ~ U[1/3, 1] and fixed \u03b4 = 10\u22125 or 10\u22123, and the non-amortised DPConvCNP models are trained on \u2208 and \u03b4 values as indicated on the plots. Then, both amortised and non-amortised models are evaluated with the parameters shown on the plots. The DP-SVGP baseline was not run due to time constraints in the rebuttal period: it is significantly slower and more challenging to optimise than the DPConvCNP. We note that the amortisation gap, due to training a model to handle a continuous range of \u2208 values, is negligible. We also note that as the number of context points N increases, the performance of the DPConvCNP approaches that of the oracle predictors.", "description": "The figure displays negative log-likelihood (NLL) results for both the EQ and sawtooth synthetic tasks.  Two privacy budgets (\u2208 = 0.33 and \u2208 = 1.00) and two delta values (\u03b4 = 10\u207b\u2075 and \u03b4 = 10\u207b\u00b3) are compared for both amortised and non-amortised models, illustrating the performance of the DPConvCNP across various privacy settings and numbers of context points (N). The results are juxtaposed against an oracle (non-private) model's performance.", "section": "C Additional results"}, {"figure_path": "99rOAM7Jfm/figures/figures_28_2.jpg", "caption": "Figure S7: Illustrations of model fits on the synthetic EQ and sawtooth tasks, using stricter DP paramters, for different context sizes N. Left: model fits of amortised DPConvCNPs trained on EQ data using \u2208 ~ U[1/3, 1] and fixed \u03b4 = 10-3 (first column) or \u03b4 = 10-5 (second column) and evaluated on the DP parameters shown in the plots. Right: same as the left plot, except the data generating process is the sawtooth waveform rather than an EQ Gaussian process. We observe that the DPConvCNP produces sensible predictions even under strict privacy settings.", "description": "This figure shows example model fits from the DPConvCNP on synthetic EQ and sawtooth data with stricter DP parameters.  The left panel displays fits for the EQ data (amortized DPConvCNP), while the right panel shows fits for sawtooth data.  The results demonstrate that the model generates sensible predictions even with stringent privacy constraints.", "section": "C.2 Supplementary model fits for the synthetic tasks"}]