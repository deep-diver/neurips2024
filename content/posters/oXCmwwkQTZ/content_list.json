[{"type": "text", "text": "Implicit Regularization Paths of Weighted Neural Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jin-Hong Du Pratik Patil Carnegie Mellon University University of California Berkeley jinhongd@andrew.cmu.edu pratikpatil@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the implicit regularization effects induced by (observation) weighting of pretrained features. For weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels. Specifically, we show that ridge estimators trained on weighted features along the same path are asymptotically equivalent when evaluated against test vectors of bounded norms. These paths can be interpreted as matching the effective degrees of freedom of ridge estimators ftited with weighted features. For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in [50]. We also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity. As a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pretrained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, neural networks have become state-of-the-art models for tasks in computer vision and natural language processing by learning rich representations from large datasets. Pretrained neural networks, such as ResNet, which are trained on massive datasets like ImageNet, serve as valuable resources for new, smaller datasets [32]. These pretrained models reduce computational burden and generalize well in tasks such as image classification and object detection due to their rich feature space [32, 69]. Furthermore, pretrained features or neural embeddings, such as the neural tangent kernel, extracted from these models, serve as valuable representations of diverse data [33, 66]. ", "page_idx": 0}, {"type": "text", "text": "However, despite their usefulness, fitting models based on pretrained features on large datasets can be challenging due to computational and memory constraints. When dealing with high-dimensional pretrained features and large sample sizes, direct application of even simple linear regression may be computationally infeasible or memory-prohibitive [23, 44]. To address this issue, subsampling has emerged as a practical solution that reduces the dataset size, thereby alleviating the computational and memory burden. Subsampling involves creating smaller datasets by randomly selecting a subset of the original data points. Beyond these computational and memory advantages, subagging can also greatly improve predictive performance in overparameterized regimes, especially near model interpolation thresholds [53]. Moreover, through distributed learning, models fitted on multiple subsampled datasets can be aggregated as an ensemble to provide more stable predictions [20, 21, 51]. ", "page_idx": 0}, {"type": "text", "text": "There has been growing interest in understanding the effects of subsampling (without replacement) [16, 25, 37, 50, 51]. These works relate subsampling to explicit ridge regularization, assuming either ", "page_idx": 0}, {"type": "table", "img_path": "oXCmwwkQTZ/tmp/2cc1e15a7c039fe4cc6b9d9789e075e86cb5a9ce6ae6a0376cf37bbf8c83742d.jpg", "table_caption": ["Table 1: Overview of related work on the equivalence of implicit regularization and explicit ridge regularization. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Gaussian features $\\phi\\sim\\mathcal{N}(\\mathbf{0}_{p},\\Sigma)$ or linearly decomposable features (referred to as linear features in this paper) $\\phi=\\Sigma^{1/2}z$ , where $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{p\\times p}$ is the covariance matrix and $z\\,\\in\\,\\mathbb{R}^{p}$ contains i.i.d. entries with zero means and bounded $4+\\delta$ moments for some $\\delta>0$ . Specifically, [50] establish a connection between implicit regularization induced by subsampling and explicit ridge regularization through a path defined by the tuple $(k/n,\\lambda)$ , where $k$ and $n$ are the subsample size and the full sample size, respectively, and $\\lambda$ is the ridge regularization level. Along this path, any subsample estimator with the corresponding ridge regularization exhibits the same first-order (or estimator equivalence) and second-order (or risk equivalence) asymptotic limits. Moreover, the endpoints of all such paths along the two axes of $k=n$ (no subsampling) and $\\lambda=0$ (no regularization) span the same range. Although these results have been demonstrated for linear features, [50] also numerically observe similar equivalence behavior in more realistic contexts and propose conjectures for random features and kernel features based on heuristic \u201cuniversality\u201d justifications. However, extending these results to encompass more general feature structures and other sampling schemes remains an open question. ", "page_idx": 1}, {"type": "text", "text": "Towards answering this question, in this paper, we view subsampling as a weighted regression problem [67]. This perspective allows us to study the equivalence in its most general form, considering arbitrary feature structures and weight structures. The general weight matrix approach used in this study encompasses various applications, including subsampling, bootstrapping, variance-adaptive weighting, survey, and importance weighting, among others. By interpreting subsampling as a weighted regression problem, we leverage recent tools from free probability theory, which have been developed to analyze feature sketching [39, 42, 54]. Building on these theoretical tools, we establish implicit regularization paths for general weighting and feature structures. We summarize our main results below and provide an overview of our results in the context of recent related work in Table 1. ", "page_idx": 1}, {"type": "text", "text": "1.1 Summary of results and paper outline ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We summarize our main results and provide an outline for the paper below. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 Paths of weighted representations. In Section 3, we demonstrate that general weighted models exhibit first-order equivalence along a path (Theorem 1) when the weight matrices are asymptotically independent of the data matrices. This path of equivalence can be computed directly from the data using the formula provided in Equation (2). Furthermore, we provide a novel interpretation of this path in terms of matching effective degrees of freedom of models along the path for general feature structures when the weights correspond to those arising from subsampling (Theorem 2). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Paths of subsampled representations. We further specialize our general result in Theorem 2 for the weights induced by subsampling without replacement to structured features in Section 3.2. These include results for linear random features, nonlinear random features, and kernel features, as shown in Propositions 3\u20135, respectively. The latter two results also resolve Conjectures 7 and 8 raised by [50] regarding subsampling regularization paths for random and kernel features, respectively. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Risk equivalences and tuning. In Section 4, we demonstrate that an ensemble of weighted models has general quadratic risk equivalence on the path, with an error term that decreases inversely as $1/\\bar{M}$ as the number of ensemble size $M$ increases (Theorem 6). The risk equivalence holds for both in-distribution and out-of-distribution settings. For subsampling general features, we derive an upper bound for the optimal subsample size (Proposition 7) and propose a cross-validation method to tune the subsample and ensemble sizes (Algorithm 1), validated on real datasets in Section 4.3. ", "page_idx": 2}, {"type": "text", "text": "This level of generality is achievable because we do not analyze the risk of either the full model or the weighted models in isolation. Instead, we relate these two sets of models, allowing us to maintain weak assumptions about the features. The key assumption underlying our results is the asymptotic freeness of weight matrices with respect to the data matrices. While directly testing this assumption is generally challenging, we verify its validity through its consequences on real datasets in Section 4.3. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related literature ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide a brief account of other related work below to place our work in a better context. ", "page_idx": 2}, {"type": "text", "text": "Linear features. Despite being overparameterized, neural networks generalize well in practice [70, 71]. Recent work has used high-dimensional \u201clinearized\u201d networks to investigate the various phenomena that arise in deep learning, such as double descent [12, 46, 48], benign overfitting [10, 35, 45], and scaling laws [7, 19, 66]. This literature analyzes linear regression using statistical physics [14, 60] and random matrix theory [22, 30]. Risk approximations hold under random matrix theory assumptions [6, 30, 66] in theory and apply empirically on a variety of natural data distributions [43, 60, 66]. ", "page_idx": 2}, {"type": "text", "text": "Random and kernel features. Random feature regression, initially introduced in [56] as a way to scale kernel methods, has recently been used for theoretical analysis of neural networks and trends of double descent in deep networks [1, 46]. The generalization of kernel ridge regression has been studied in [11, 40, 57]. The risks of kernel ridge regression are also analyzed in [9, 19, 29]. The neural representations we study are motivated by the neural tangent kernel (NTK) and related theoretical work on ultra-wide neural networks and their relationships to NTKs [34, 68]. ", "page_idx": 2}, {"type": "text", "text": "Resampling analysis. Resampling and weighted models are popular in distributed learning to provide more stable predictions and handle large datasets [20, 21, 51]. Historically, for ridge ensembles, [36, 61] derived risk asymptotics under Gaussian features. Recently, there has been growing interest in analyzing the effect of subsampling in high-dimensional settings. [37] considered least squares ensembles obtained by subsampling, where the final subsampled dataset has more observations than the number of features. For linear models in the underparameterized regime, [59] also provide certain equivalences between subsampling and iterative least squares approaches. The asymptotic risk characterization for general data models has been derived by [51]. [25, 50] extended the scope of these results by characterizing risk equivalences for both optimal and suboptimal risks and for arbitrary feature covariance and signal structures. Very recently, different resampling strategies for high-dimensional supervised regression tasks have been analyzed by [17] under isotropic Gaussian features. Cross-validation methods for tuning the ensemble of ridge estimators and other penalized estimators are discussed in [13, 25, 26]. Our work adds to this literature by considering ensembles of models with general weighting and feature structures. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally define our weighted estimator and state the main assumption on the weight matrix. Let $f_{\\mathrm{nn}}\\colon\\ensuremath{\\mathbb{R}^{d}}\\;{\\overset{\\bullet}{\\to}}\\;\\ensuremath{\\mathbb{R}^{p}}$ be a pretrained model. Let $\\{(\\pmb{x}_{i},y_{i})\\colon i=1,\\dots,n\\}$ in $\\mathbb{R}^{d}\\times\\mathbb{R}$ be the given dataset. Applying $f_{\\mathrm{nn}}$ to the raw dataset, we obtain the pretrained features $\\phi_{i}=f_{\\mathrm{nn}}({\\pmb x}_{i})$ for $i=1,\\hdots,n$ as the resulting neural representations or neural embeddings. In matrix notation, we denote the pretrained feature matrix by $\\begin{array}{r}{\\Phi\\,=\\,[\\phi_{1},\\dots,\\phi_{n}]^{\\intercal}\\,\\in\\,\\mathbb{R}^{n\\times p}}\\end{array}$ . Let $W\\,\\in\\,\\mathbb{R}^{n\\times n}$ be a general weight matrix used for weighting the observations. The weight matrix $W$ is allowed to be asymmetric, in general. ", "page_idx": 2}, {"type": "text", "text": "We consider fitting ridge regression on the weighted dataset $(W\\Phi,W y)$ . Given a ridge penalty $\\lambda$ , the ridge estimator fitted on the weighted dataset is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\beta}_{W,\\lambda}:=\\underset{\\beta\\in\\mathbb{R}^{p}}{\\operatorname{argmin}}\\left(\\frac{\\|W y-W\\Phi\\beta\\|_{2}^{2}}{n}+\\lambda\\|\\beta\\|_{2}^{2}\\right)=(\\Phi^{\\top}W^{\\top}W\\Phi+n\\lambda I_{p})^{\\dagger}\\Phi^{\\top}W^{\\top}W y.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the definition above, we allow for $\\lambda=0$ , in which case the corresponding ridgeless estimator is defined as the limit $\\lambda\\to0^{+}$ . For $\\lambda<0$ , we use the Moore-Penrose pseudoinverse. An important special case is where $W$ is a diagonal matrix, in which case the above estimator reduces to weighted ridge regression. This type of weight matrix encompasses various applications, such as resampling, bootstrapping, and variance weighting. Our main application in this paper will be subsampling. ", "page_idx": 3}, {"type": "text", "text": "For our theoretical results, we assume that the weight matrix $W$ preserves some spectral structure of the feature matrix $\\Phi$ . This assumption is captured by the condition of asymptotic freeness between $W^{\\top}W$ and the feature Gram matrix $\\Phi\\Phi^{\\top}$ . Asymptotic freeness is a concept from free probability theory [64]. ", "page_idx": 3}, {"type": "text", "text": "Assumption A (Weight structure). Let $W^{\\top}W$ and $\\Phi\\Phi^{\\top}/n$ converge almost surely to bounded operators that are infinitesimally free with respect to $(\\overline{{\\mathrm{tr}}}[\\cdot],\\mathrm{tr}[C(\\cdot)])$ for any $_{C}$ independent of $W$ with $\\|C\\|_{\\mathrm{tr}}$ uniformly bounded. Additionally, let $W^{\\top}W$ have a limiting $S$ -transform that is analytic on the lower half of the complex plane. ", "page_idx": 3}, {"type": "text", "text": "At a high level, Assumption A captures the notion of independence but is adapted for non-commutative random variables of matrices. We provide background on free probability theory and asymptotic freeness in Appendix A.3. Here, we briefly list a series of invertible transformations from free probability to help define the $S$ -transform [47]. The Cauchy transform is given by $\\mathcal{G}_{A}(z)\\ =$ $\\bar{\\bar{\\mathrm{tr}}}[(z\\pmb{I}-\\dot{\\pmb{A}})^{-1}]$ . The moment generating series is given by ${\\bar{\\mathcal M}}_{A}(z)\\,=\\,z^{-1}{\\bar{\\mathcal G}}_{A}(z^{-1})\\,-\\,1$ . The $S$ -transform is given by $S_{A}(w)=(1+w^{-1})\\mathcal{M}_{A}^{\\langle-1\\rangle}(w)$ . These are the Cauchy transform (negative of the Stieltjes transform), moment generating series, and $S$ -transform of $\\pmb{A}$ , respectively. Here, $\\mathcal{M}_{A}^{(-1)}$ denotes the inverse under the composition of $\\mathcal{M}_{A}$ . The notation $\\overline{{\\operatorname{tr}}}[A]$ denotes the average trace $\\mathrm{tr}[\\boldsymbol{A}]/p$ of . ", "page_idx": 3}, {"type": "text", "text": "The freeness of a pair of matrices $\\pmb{A}$ and $_B$ means that the eigenvectors of one are completely unaligned or incoherent with those of the other. For example, if $A=U R U^{\\top}$ for a uniformly random unitary matrix $U$ drawn independently of the positive semidefinite $_B$ and $\\boldsymbol{R}$ , then $\\pmb{A}$ and $_B$ are almost surely asymptotically infinitesimally free [15]. Other well-known examples include Wigner matrices, which are asymptotically free with respect to deterministic matrices [4, Theorem 5.4.5]. Gaussian matrices, where the Gram matrix $G\\sp{\\\"}=\\,\\Phi\\Phi^{\\top}/n\\,=\\,U(V V^{\\top}/n)\\dot{U}^{\\top}$ and any deterministic $\\boldsymbol{S}$ , are almost surely asymptotically free [47, Chapter 4, Theorem 9]. Although not proven in full generality, it is expected that diagonal matrices are asymptotically free from data Gram matrices constructed using i.i.d. data. In Section 3.2, we will provide additional examples of feature matrices, such as random and kernel features from machine learning, for which our results apply. ", "page_idx": 3}, {"type": "text", "text": "Our results involve the notion of degrees of freedom from statistical optimism theory [27, 28]. Degrees of freedom in statistics count the number of dimensions in which a statistical model may vary, which is simply the number of variables for ordinary linear regression. To account for regularization, this notion has been extended to effective degrees of freedom (Chapter 3 of [31]). Under some regularity conditions, from Stein\u2019s relation [63], the degrees of freedom of a predictor $\\widehat{f}$ are measured by the trace of the operators $y\\,\\mapsto\\,(\\partial/\\partial{\\pmb y}){\\widehat{f}}(\\Phi)$ . For the ridge estimator $\\widehat{\\beta}_{I,\\mu}$ fitted on $(\\Phi,y)$ with penalty $\\mu$ , the degrees of freedom is co nsequently the trace of its predi ction operator $y\\mapsto\\Phi(\\Phi^{\\top}\\Phi+\\mu I_{p})^{\\dagger}\\Phi^{\\top}y.$ , which is also referred to as the ridge smoother matrix. That is, $\\mathsf{d}\\mathsf{f}(\\widehat{\\beta}_{I,\\mu})=\\mathrm{tr}[\\Phi^{\\top}\\Phi(\\Phi^{\\top}\\Phi+\\mu I_{p})^{\\dagger}]$ . We denote the normalized degrees of freedom by ${\\overline{{\\mathsf{d f}}}}={\\mathsf{d f}}/n$ . Note that d $\\overline{{\\mathsf{f}}}(\\widehat{\\beta}_{I,\\mu})\\leq\\operatorname*{min}\\{n,p\\}/n\\leq1$ . ", "page_idx": 3}, {"type": "text", "text": "Finally, we express our asymptotic results using the asymptotic equivalence relation. Consider sequences $\\{A_{n}\\}_{n\\ge1}$ and $\\{B_{n}\\}_{n\\ge1}$ of (random or deterministic) matrices (which includes vectors and scalars). We say that $\\textstyle A_{n}$ and $B_{n}$ are equivalent and write $A_{n}\\simeq B_{n}$ if $\\begin{array}{r}{\\operatorname*{lim}_{p\\to\\infty}|\\operatorname{tr}[C_{n}(\\mathbf{A}_{n}-}\\end{array}$ $B_{n})]|\\;=\\;0$ almost surely for any sequence $C_{n}$ of matrices with bounded trace norm such that lim sup $\\|C_{n}\\|_{\\mathrm{tr}}<\\infty$ as $n\\to\\infty$ . Our forthcoming results apply to a sequence of problems indexed by $n$ . For notational simplicity, we omit the explicit dependence on $n$ in our statements. ", "page_idx": 3}, {"type": "text", "text": "3 Implicit regularization paths ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by characterizing the implicit regularization induced by weighted pretrained features. We will show that the degrees of freedom of the unweighted estimator $\\widehat{\\beta}_{I,\\mu}^{\\,\\,\\,\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!\\,\\,\\!^{\\!}$ on the full data $(\\Phi_{\\lambda}y)$ with regularization parameter $\\mu$ are equal to the degrees of freedom of the weighted estimator $\\beta_{W,\\lambda}$ for some regularization parameter $\\lambda$ . For estimator equivalence, our data-dependent set of weighted ridge estimators $(W,\\lambda)$ that connect to the unweighted ridge estimator $(\\bar{\\pmb{I}_{*}^{}}\\bar{\\mu})$ is defined in terms of \u201cmatching\u201d effective degrees of freedom of component estimators in the set. ", "page_idx": 4}, {"type": "text", "text": "To state the upcoming result, denote the Gram matrix of the weighted data as $G_{W}=W\\Phi\\Phi^{\\top}W^{\\top}/n$ and the Gram matrix of the unweighted data as $G_{I}=\\Phi\\Phi^{\\top}/n$ . Furthermore, let $\\lambda_{\\operatorname*{min}}^{+}(A)$ denote the minimum positive eigenvalue of a symmetric matrix $\\pmb{A}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Implicit regularization of weighted representations). For $G_{I}\\in\\mathbb{R}^{n\\times n}$ , suppose that the weight matrix $\\bar{W}\\in\\mathbb{R}^{\\bar{n}\\times n}$ satisfies Assumption $A$ and lim sup $\\|\\pmb{\\boldsymbol{y}}\\|_{2}^{2}/n<\\infty$ as $n\\to\\infty$ . For any $\\mu>-\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}\\lambda_{\\operatorname*{min}}^{+}(G_{I})$ , let $\\lambda>-\\lambda_{\\operatorname*{min}}^{+}(G_{W})$ be given by the following equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda=\\mu/S_{W^{\\top}W}(-\\overline{{{\\mathsf{d f}}}}(\\widehat{\\beta}_{I,\\mu})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $S_{W^{\\top}W}$ is the $S$ -transform of the operator $W^{\\top}W$ . Then, as $n\\to\\infty$ , it holds that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{d f}\\bigl(\\widehat{\\beta}_{W,\\lambda}\\bigr)\\simeq\\mathsf{d f}\\bigl(\\widehat{\\beta}_{I,\\mu}\\bigr)\\quad a n d\\quad\\widehat{\\beta}_{W,\\lambda}\\simeq\\widehat{\\beta}_{I,\\mu}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In other words, to achieve a target regularization of $\\mu$ on the unweighted data, Theorem 1 provides a method to compute the regularization penalty $\\lambda$ with given weights $W$ from the available data using (2). The weighted estimator then has asymptotically the same degrees of freedom as the unweighted estimator. This means that the level of effective regularization of the two estimators is the same. Moreover, the estimators themselves are structurally equivalent; that is, $c^{\\top}({\\widehat{\\beta}}_{W,\\lambda}-{\\widehat{\\beta}}_{I,\\mu})\\ {\\xrightarrow{\\mathrm{a.s.}}}\\ 0$ for every constant vector $^c$ with bounded norm. The estimator equivalence in Theorem 1 is a \u201cfirst-order\u201d result, while we will also characterize the \u201csecond-order\u201d effects in Section 4. ", "page_idx": 4}, {"type": "text", "text": "The notable aspect of Theorem 1 is its generality. The equivalence results hold for a wide range of weight matrices and allow for negative values for the regularization levels. Furthermore, we have not made any direct assumptions about the feature matrix $\\Phi$ , the weight matrix $W$ , and the response vector $\\textit{\\textbf{y}}$ (other than mild bounded norms). The main underlying ingredient is the asymptotic freeness between $W$ and $\\Phi$ , which we then exploit using tools developed in [39] in the context of feature sketching. We discuss special cases of interest for $W$ and $\\Phi$ in the upcoming Sections 3.1 and 3.2. ", "page_idx": 4}, {"type": "text", "text": "3.1 Examples of weight matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There are two classes of weighting matrices that are of practical interest: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Non-diagonal weighting matrices. One can consider observation sketching, which involves some random linear combinations of the rows of the data matrix. Such observation sketching is beneficial for privacy, as it scrambles the rows of the data matrix, which may contain identifiable information about individuals. It also helps in reducing the effect of non-i.i.d. data that arise in time series or spatial data, where one wants to smooth away the impact of irregularities or non-stationarity. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Diagonal weighting matrices. When observations are individually weighted, $W$ is a diagonal matrix, which includes scenarios such as resampling, bootstrapping, and subsampling. Note that even with subsampling, one can have a non-binary diagonal weighting matrix. For example, one can consider sampling with replacement or sampling with a particular distribution, which yields non-binary diagonal weighting matrices. Other examples of non-binary diagonal weighting matrices include inverse-variance weighting sampling to mitigate the effects of heterogeneous variations if the responses have different variances for different units. ", "page_idx": 4}, {"type": "text", "text": "In general, the set of equivalent weighted estimators depends on the corresponding $S$ -transform as in (2), and it can be numerically evaluated. When focusing on subsampling without replacement, the data-dependent path for equivalent estimators with associated subsampling and regularization levels can be explicitly characterized in the following result by analyzing the $S$ -transform of subsampling operators. ", "page_idx": 4}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/3fb14fb203cb8ba7dfdc20337c6f4394a700e5eb2384c18c09866db9129833d4.jpg", "img_caption": ["Figure 1: Equivalence under subsampling. The left panel shows the heatmap of degrees of freedom, and the right panel shows the random projection $\\mathbb{E}_{W}[{\\pmb a}^{\\top}{\\widehat{\\beta}}_{W,\\lambda}]$ where $\\mathbf{\\boldsymbol{a}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}}_{p},I_{p}/p)$ . In both heatmaps, the red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths by matching empirical degrees of freedom. The data is generated according to Appendix F.1 with $n=10000$ and $p=1000$ , and the results are averaged over $M=100$ random weight matrices $W$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Regularization paths due to subsampling). For a subsampling matrix $W^{(k)}$ consisting of $k$ unit diagonal entries, the path (2) in terms of $(k,\\lambda)$ simplifies to: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(1-\\mathsf{d f}/n\\right)\\cdot\\left(1-\\lambda/\\mu\\right)=\\left(1-k/n\\right)\\!,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we denote by ${\\mathsf{d f}}={\\mathsf{d f}}({\\widehat{\\beta}}_{I,\\mu})={\\mathsf{d f}}({\\widehat{\\beta}}_{W,\\lambda})$ for notational simplicity. ", "page_idx": 5}, {"type": "text", "text": "The relation (4) is remarkably simple, yet quite general! It provides an interplay between the normalized target complexity $\\mathsf{d f}/n$ , regularization inflation $\\lambda/\\mu$ , and subsample fraction $k/n$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n(1-\\mathrm{normalized\\;complexity})\\cdot(1-\\mathrm{regularization\\;inflation})=(1-\\mathrm{subsample\\;fraction}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the normalized target complexity and subsample fraction are no greater than one, (5) also implies that the regularization level $\\lambda$ for the subsample estimator is always lower than the regularization level $\\mu$ for the full estimator. In other words, subsampling induces (positive) implicit regularization, reducing the need for explicit ridge regularization. This is verified numerically in Figure 1. ", "page_idx": 5}, {"type": "text", "text": "For a fixed target regularization amount $\\mu$ , the degrees of freedom df $(\\widehat{\\beta}_{I,\\mu})$ of the ridge estimator on full data is fixed. Thus, we can observe that the path in the $(k/n,\\dot{\\lambda})$ -plane is a line. There are two extreme cases: (1) when the subsample size $k$ is close to $n$ , we have $\\mu\\approx\\lambda$ ; and (2) when the subsample size is near 0, we have $\\mu\\approx\\infty$ . When $\\lambda=0$ , the effective regularization level $\\lambda$ is such that d $\\mathsf{f}\\big(\\widehat{\\beta}_{W^{(k)},\\lambda}\\big)=\\mathsf{d f}\\big(\\widehat{\\beta}_{I,\\mu}\\big)=k$ , which we find to be a neat relation! ", "page_idx": 5}, {"type": "text", "text": "Beyond subsampling without replacement, one can also consider other subsample matrixs. For example, for bootstrapping $k$ entries, we observe a similar equivalent path in Figure 5. Additionally, for random sample reweighting, as shown in Figure 6, we also observe certain equivalence behaviors of degrees of freedom. This indicates that Theorem 1 also applies to more general weighting schemes. ", "page_idx": 5}, {"type": "text", "text": "3.2 Examples of feature matrices ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As mentioned in Section 2, when the feature matrix $\\Phi$ consists of i.i.d. Gaussian features, any deterministic matrix $W$ satisfies the condition stated in Assumption A. However, our results are not limited to Gaussian features. In this section, we will consider more general families of features commonly analyzed in machine learning and demonstrate the applicability of our results to them. ", "page_idx": 5}, {"type": "text", "text": "(1) Linear features. As a first example, we consider linear features composed of (multiplicatively) transformed i.i.d. entries with sufficiently bounded moments by a deterministic covariance matrix. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3 (Regularization paths with linear features). Suppose the feature $\\phi$ can be decomposed as $\\phi=\\Sigma^{1/2}z$ , where $z\\in\\mathbb{R}^{p}$ contains i.i.d. entries $z_{i}$ for $i=1,\\hdots,p$ with mean 0, variance 1, and satisfies $\\mathbb{E}[|z_{i}|^{4+\\mu}]\\leq M_{\\mu}<\\infty$ for some $\\mu>0$ and a constant $M_{\\mu}$ , and $\\Sigma\\in\\mathbb{R}^{p\\times p}$ is a deterministic ", "page_idx": 5}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/ac6e67abb21a62767fd3a39c92cbeab5001599440978ff9b0820d6b07bef7020.jpg", "img_caption": ["Figure 2: Equivalence of degrees of freedom for various feature structures under subsampling. The three panels correspond to linear features, random features with ReLU activation function (2-layer), and kernel features (polynomial kernel with degree 3 and without intercept), respectively. In all heatmaps, the red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths by matching the empirical degrees of freedom. The data is generated according to Appendix F.1 with $n=5000$ and $p=500$ , and the results are averaged over $M=100$ random weight matrices $W$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "symmetric matrix with eigenvalues uniformly bounded between constants $r_{\\operatorname*{min}}>0$ and $r_{\\operatorname*{max}}<\\infty$ .   \nThen, as $n,p\\to\\infty$ such that $p/n\\to\\gamma>0$ , the equivalences in (3) hold along the path (4). ", "page_idx": 6}, {"type": "text", "text": "Features of this type are common in random matrix theory [8] and in a wide range of applications, including statistical physics [14, 60], high-dimensional statistics [22, 55, 58], machine learning [18], among others. The generalized path (2) in Theorem 2 recovers the path in Proposition 4 of [50]. Although the technique in this paper is quite different and more general than that of [50]. ", "page_idx": 6}, {"type": "text", "text": "(2) Kernel features. As the second example, Theorem 2 also applies to kernel features. Kernel features are a generalization of linear features and lift the input feature space to a high- or infinitedimensional feature space by applying a feature map $x\\mapsto\\phi(x)$ . Kernel methods use the kernel function $K(\\pmb{x}_{i},\\pmb{x}_{j})=\\bar{\\langle\\phi(\\pmb{x}_{i}),\\bar{\\phi(\\pmb{x}_{j})}\\rangle}$ to compute the inner product in the lifted space. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4 (Regularization paths with kernel features). Suppose the same conditions as in Proposition 3 and the kernel function is of the form $K(\\pmb{x}_{i},\\pmb{x}_{j})=\\bar{g}(\\|\\pmb{x}_{i}\\|_{2}^{2}/p,\\langle\\pmb{x}_{i},\\pmb{x}_{j}\\rangle/p,\\|\\pmb{x}_{j}\\|_{2}^{2}/p)$ , where $g$ is $\\mathcal{C}^{1}$ around $(\\tau,\\tau,\\tau)$ and $\\mathcal{C}^{3}$ around $(\\tau,0,\\tau)$ and $\\tau:=\\operatorname*{lim}_{p\\to\\infty}\\operatorname{tr}[\\Sigma]/d$ . Then, as $n\\to\\infty$ , the equivalences in (3) hold in probability along the path (4). ", "page_idx": 6}, {"type": "text", "text": "The assumption in Proposition 4 is commonly used in the risk analysis of kernel ridge regression [9, 19, 29, 57], among others. Here, $\\mathcal{C}^{k}$ denotes the class of functions that are $k$ -times continuously differentiable. It includes neural tangent kernels (NTKs) as a special case. Proposition 4 confirms Conjecture 8 of [50] for these types of kernel functions. ", "page_idx": 6}, {"type": "text", "text": "(3) Random features. Finally, we consider random features that were introduced by [56] as a way to scale kernel methods to large datasets. Linked closely to two-layer neural networks [46], the random feature model has $f_{\\mathrm{nn}}({\\pmb x})\\stackrel{=}{=}\\sigma({\\pmb F}{\\pmb x})$ , where $\\pmb{F}\\in\\mathbb{R}^{d\\times p}$ is some randomly initialized weight matrix, and $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a nonlinear activation function applied element-wise to $\\pmb{F x}$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 5 (Regularization paths with random features). Suppose $\\pmb{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{\\Sigma})$ and the activation function $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is differentiable almost everywhere and there are constants $c_{0}$ and $c_{1}$ such that $|\\sigma(x)|,|\\sigma^{\\prime}(x)|\\le c_{0}e^{c_{1}x}$ , whenever $\\sigma^{\\prime}(x)$ exists. Then, as $n,p,d\\to\\infty$ such that $p/n\\to\\gamma>0$ and $d/n\\to\\xi>0$ , the equivalences in (3) hold in probability along the path (4). ", "page_idx": 6}, {"type": "text", "text": "As mentioned in the related work, random feature models have recently been used as a standard model to study various generalization phenomena observed in neural networks theoretically [1, 46]. Proposition 5 resolves Conjecture 7 of [50] under mild regularity conditions on the activation function. ", "page_idx": 6}, {"type": "text", "text": "It is worth noting that the prior works mentioned above, including [50], have focused on first characterizing the risk asymptotics in terms of various population quantities for each of the cases above. In contrast, our work in this paper deviates from these approaches by not expressing the risk in population quantities but rather by directly relating the estimators at different regularization levels. In the next section, we will explore the relationship between their squared prediction risks. ", "page_idx": 6}, {"type": "text", "text": "4 Prediction risk asymptotics and risk estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The results in the previous section provide first-order equivalences of the estimators, which are related to the bias of the estimators. In practice, we are also interested in the predictive performance of the estimators. In this section, we investigate the second-order equivalence of weighting and ridge regularization through ensembling. Specifically, we show that aggregating estimators ftited on different weighted datasets also reduces the additional variance. Furthermore, the prediction risks of the full-ensemble weighted estimator and the unweighted estimator also match along the path. ", "page_idx": 7}, {"type": "text", "text": "Before presenting our risk equivalence result, we first introduce some additional notation. Assume there are $M$ i.i.d. weight matrices $W_{1},\\dots,W_{M}\\in\\mathbb{R}^{n\\times n}$ . The $M$ -ensemble estimator is defined as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{\\beta}_{W_{1:M},\\lambda}=M^{-1}\\sum_{m=1}^{M}\\widehat{\\beta}_{W_{m},\\lambda},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and its performance is quantified by the conditional squared prediction risk, given by: ", "page_idx": 7}, {"type": "equation", "text": "$$\nR(\\widehat{\\beta}_{W_{1:M},\\lambda})=\\mathbb{E}_{\\mathbf{x}_{0},y_{0}}[(y_{0}-\\phi_{0}^{\\top}\\widehat{\\beta}_{W_{1:M},\\lambda})^{2}\\mid\\Phi,y,\\{W_{m}\\}_{m=1}^{M}],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $(x_{0},y_{0})$ is a test point sampled independently from some distribution $P_{x_{0},y_{0}}$ that may be different from the training distribution $P_{x,y}$ , and $\\phi_{0}=f_{\\mathrm{nn}}(\\mathbf{x}_{0})$ is the pretrained feature at the test point. The covariance matrix of the test features $\\phi_{\\mathrm{0}}$ is denoted by $\\Sigma_{0}$ . When $P_{x_{0},y_{0}}=P_{x,y}$ , we refer to it as the in-distribution risk. On the other hand, when $P_{x_{0},y_{0}}$ differs from $P_{x,y}$ , we refer to it as the out-of-distribution risk. Note that the conditional risk $R_{M}$ is a scalar random variable that depends on both the dataset $(\\Phi,y)$ and the weight matrix $W_{m}$ for $m\\in[M]$ . Our goal in this section is to analyze the prediction risk of the ensemble estimator (6) for any ensemble size $M$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 6 (Risk equivalence along the path). Under the setting of Theorem $I$ , assume that the operator norm of $\\pmb{\\Sigma}_{0}$ is uniformly bounded in $p$ and that each response variable $y_{i}$ for $i=1,\\hdots,n$ has mean 0 and satisfies $\\mathbb{E}[|y_{i}|^{4+\\mu}]\\leq M_{\\mu}<\\dot{\\infty}$ for some $\\mu,M_{\\mu}>0$ . Then, along the path (4), ", "page_idx": 7}, {"type": "equation", "text": "$$\nR(\\widehat{\\beta}_{W_{1:M},\\lambda})\\simeq R(\\widehat{\\beta}_{I,\\mu})+\\frac{C}{M}\\,\\overline{{{\\mathrm{tr}}}}[(G_{I}+\\mu I)^{\\dagger}y y^{\\top}(G_{I}+\\mu I_{n})^{\\dagger}],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the constant $C$ is given by: ", "page_idx": 7}, {"type": "equation", "text": "$$\nC=-\\partial\\mu/\\partial\\lambda\\cdot\\lambda^{2}S_{W^{\\top}W}^{\\prime}(-\\mathbf{d}\\{(\\widehat{\\beta}_{I,\\mu})\\}\\,\\mathrm{\\overline{{tr}}}[(G_{I}+\\mu I)^{\\dagger}(\\Phi\\Sigma_{0}\\Phi^{\\top}/n)(G_{I}+\\mu I)^{\\dagger}].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "At a high level, Theorem 6 provides a bias-variance-like risk decomposition for both the squared risks of weighted ensembles. The risk of the weighted predictor is equal to the risk of the unweighted equivalent implicit ridge regressor (bias) plus a term due to the randomness due to weighting (variance). The inflation factor $C$ controls the magnitude of this term, and it decreases at a rate of $1/M$ as the ensemble size $M$ increases (see Figure 7 for a numerical verification of this rate). Therefore, by using a resample ensemble with a sufficiently large size $M$ , we can retain the statistical properties of the full ridge regression while reducing memory usage and increasing parallelization. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6 extends the risk equivalence results in [50, 52]. Compared to previous results, Theorem 6 provides a broader risk equivalence that holds for general weight and feature matrices, as well as an arbitrary ensemble size $M$ . It is important to note that Theorem 6 holds even when the test distribution differs from the training data, making it applicable to out-of-distribution risks. Furthermore, our results do not rely on any specific distributional assumptions for the response vector, making them applicable in a model-free setting. The key idea behind this result is to exploit asymptotic freeness between the subsample and data matrices. Next, we will address the question of optimal tuning. ", "page_idx": 7}, {"type": "text", "text": "4.1 Optimal oracle tuning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As in Theorem 2, we next analyze various properties related to optimal subsampling weights and their implications for the risk of optimal ridge regression. Recall that the subsampling matrix $W^{(k)}$ is a diagonal matrix with $k\\in\\{1,\\ldots,n\\}$ nonzero diagonal entries, which is parameterized by the subsample size $k$ . Note that the optimal regularization parameter $\\mu^{*}$ for the full data $\\mathbf{\\nabla}W^{(k)}=I$ or $k=n$ ) is a function of the distribution of pretrained data and the test point. Based on the risk equivalence in Theorem 6, there exists an optimal path of $(k,\\lambda)$ with the corresponding full-ensemble estimator $\\widehat{\\beta}_{W_{1:\\infty}^{(k)},\\lambda}:=\\operatorname*{lim}_{M\\rightarrow\\infty}\\widehat{\\beta}_{W_{1:M}^{(k)},\\lambda}$ that achieves the optimal predictive performance at $(n,\\mu^{*})$ . In particular, the ridgeless ensemble with $\\lambda^{*}=0$ happens to be on the path. From previous work [25, 50], the optimal subsample size $k^{*}$ for $\\lambda^{*}=0$ has the property that $k^{*}\\leq p$ under linear features. We show in the following that this property can be extended to include general features. ", "page_idx": 7}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/7479bf631a36fe116bed65d5b9ef75b4473d6240a4f829011036dfd9b3632ddf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Proposition 7 (Optimal subsample ratio). Assume the subsampling matrix $W$ as defined in Theorem 2. Let $\\mu^{*}=\\mathrm{argmin}_{\\mu\\geq0}\\,R(\\widehat{\\beta}_{W_{1:\\infty}^{(k)},\\mu})$ . Then the corresponding subsample size satisfies: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{k^{*}=\\mathsf{d f}(\\widehat{\\beta}_{W_{1:\\infty}^{(k)},\\mu^{*}})\\le\\mathrm{rank}(G_{I}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The optimal subsample size $k^{*}$ obtained from Proposition 7 is asymptotically optimal. For linear features, in the underparameterized regime where $n>p$ , [25, 50] show that the optimal subsample size $k^{*}$ is asymptotically no larger than $p$ . This result is covered by Proposition 7 by noting that $\\operatorname{rank}(G_{I})\\leq p$ under linear features. It is interesting and somewhat surprising to note that in the underparameterized regime (when $p\\leq n]$ ), we do not need more than $p$ observations to achieve the optimal risk. In this sense, the optimal subsampled dataset is always overparameterized. ", "page_idx": 8}, {"type": "text", "text": "When the limiting risk profiles $\\begin{array}{r}{\\mathcal{R}(\\gamma,\\psi,\\mu):=\\operatorname*{lim}_{p/n\\to\\gamma,p/k\\to\\psi}R(\\widehat{\\beta}_{W_{1:\\infty}^{(k)},\\mu})}\\end{array}$ exist for subsample ensembles, the limiting risk of the optimal ridge predictor $\\operatorname{inf}_{\\mu\\geq0}\\mathcal{R}(\\gamma,\\gamma,\\mu)$ is monotonically decreasing in the limiting sample aspect ratio $\\gamma$ [50]. This also (provably) confirms the sample-wise monotonicity of optimally-tuned risk for general features in an asymptotic sense [48]. Due to the risk equivalence in Theorem 6, for any $\\mu>0$ , there exists $\\psi$ such that $\\mathcal{R}(\\gamma,\\gamma,\\mu)=\\mathcal{R}(\\gamma,\\psi,0)$ . This implies that $\\operatorname*{inf}_{\\mu\\geq0}\\mathcal{R}(\\gamma,\\gamma,\\mu)=\\operatorname*{inf}_{\\psi\\geq\\gamma}\\mathcal{R}(\\gamma,\\psi,0)$ . In other words, tuning over subsample sizes with sufficiently large ensembles is equivalent to tuning over the ridge penalty on the full data. ", "page_idx": 8}, {"type": "text", "text": "4.2 Data-dependent tuning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As suggested by Proposition 7, the optimal subsample size is smaller than the rank of the Gram matrix. This result has important implications for real-world datasets where the number of observations $(n)$ is much larger than the number of features $(p)$ . In such cases, instead of using the entire dataset, we can efficiently build small ensembles with a subsample size $k\\leq p$ . This approach is particularly beneficial when $n$ is significantly higher than $p$ , for example, when $n=1000p$ . By ftiting ensembles with only $M=100$ base predictors, we can potentially reduce the computational burden while still achieving optimal predictive performance. Furthermore, this technique can be especially valuable in scenarios where computational resources are limited or when dealing with massive datasets that cannot be easily processed in their entirety. ", "page_idx": 8}, {"type": "text", "text": "In the following, we propose a method to determine the optimal values of the regularization parameter $\\mu^{*}$ for the full ridge regression, as well as the corresponding subsample size $k^{*}$ and the optimal ensemble size $M^{*}$ . According to Theorem 6, the optimal value of $M^{*}$ is theoretically infinite. However, in practice, the prediction risk of the $M$ -ensemble predictor decreases at a rate of $1/M$ as $M$ increases. Therefore, it is important to select a suitable value of $M$ that achieves the desired level of performance while considering computational constraints and the specified error budget. By carefully choosing an appropriate $M$ , we can strike a balance between model accuracy and efficiency, ensuring that the subsampled neural representations are effectively used in downstream tasks. ", "page_idx": 8}, {"type": "text", "text": "Consider a grid of subsample size $K_{n}\\subseteq\\{1,\\ldots,n\\}$ ; for instance, $\\mathcal{K}_{n}=\\{0,k_{0},2k_{0},\\ldots,n\\}$ where $k_{0}$ is a subsample size unit. For a prespecified subsample size $k\\in\\kappa_{n}$ and ensemble size $M_{0}\\ \\in$ ", "page_idx": 8}, {"type": "text", "text": "Input: A dataset $\\begin{array}{r}{\\mathcal{D}_{n}=\\{(\\pmb{x}_{i},y_{i})\\in\\mathbb{R}^{p}\\times\\mathbb{R}:1\\leq i\\leq n\\}}\\end{array}$ , a regularization parameter $\\lambda$ , a class of subsample matrix distribution $\\mathcal{P}_{n}=\\{P_{k}\\}_{k\\in K_{n}}$ , a ensemble size $M_{0}\\geq2$ for risk estimation, and optimality tolerance parameter $\\delta$ .   \n1: Build ensembles $\\widehat{\\beta}_{W_{1:M_{0}}^{(k)},\\lambda}$ with $M_{0}$ base estimators, where $\\boldsymbol{W_{1}^{(k)},\\dots,\\boldsymbol{W}_{M_{0}}^{(k)}\\overset{\\mathrm{i.i.d.}}{\\sim}P_{k}}$ for each $k\\in\\kappa_{n}$ .   \n2: Estimate the prediction risk of $\\widehat{\\beta}_{W_{1:M_{0}}^{(k)},\\lambda}$ with $\\widehat{R}_{m,k}$ by CV methods such as CGCV [13], for $k\\in\\kappa_{n}$ and $m=1,\\dots,M_{0}$ .   \n3: Extrapolate the risk estimations $\\widehat{R}_{m,k}$ for $m>M_{0}$ using (11) and (12).   \n4: Select a subsample size $\\widehat{k}\\in\\mathrm{argmin}_{k\\in\\mathcal{K}_{n}}\\,\\widehat{R}_{\\infty,k}.$ . that minimizes the extrapolated estimates.   \n5: Select an ensemble size $\\begin{array}{r}{\\widehat{M}\\in\\operatorname{argmin}_{m\\in\\mathbb{N}}\\mathbb{1}\\{\\widehat{R}_{m,\\widehat{k}}>\\widehat{R}_{\\infty,\\widehat{k}}+\\delta\\}}\\end{array}$ for the $\\delta$ -optimal risk.   \n6: If $\\widehat{M}>M_{0}$ , fit a $\\widehat{M}$ -ensemble estimator $\\widehat{\\beta}_{W_{1:\\widehat{M}}^{(\\widehat{k})},\\lambda}$ . ", "page_idx": 9}, {"type": "text", "text": "Output: Return the tuned estimator $\\widehat{\\beta}_{W_{1:\\widehat{M}}^{(\\widehat{k})},\\lambda}$ , and the risk estimators $\\widehat{R}_{M,k}$ for all $M,k$ . ", "page_idx": 9}, {"type": "text", "text": "$\\mathbb{N}$ , suppose we have multiple risk estimates $\\widehat{R}_{m}$ of $R_{m}$ for $m\\,=\\,1,\\ldots,M_{0}$ . The squared risk decomposition [51, Eq (7)] along with the e quivalence path (8) implies that ${\\cal R}_{m}\\;=\\;\\stackrel{\\cdot}{m}^{-1}{\\cal R}_{1}\\;+$ 1 \u2212m\u22121 R\u221e, for m = 1, . . . , M0. Summing these equations yields  mM0=1 Rm =  mM0=1m1R $\\begin{array}{r}{\\sum_{m=1}^{M_{0}}\\left(1-m^{-1}\\right)R_{\\infty}}\\end{array}$ . Thus, we can estimate $R_{\\infty}$ by: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\infty}=\\Big(\\sum_{m=1}^{M_{0}}\\widehat{R}_{m}-\\sum_{m=1}^{M_{0}}m^{-1}\\widehat{R}_{1}\\Big)\\ \\big/\\sum_{m=1}^{M_{0}}\\big(1-m^{-1}\\big).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Then, the extrapolated risk estimates $\\widehat{R}_{m}$ (with $m>M_{0}$ ) are defined as: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\widehat{R}_{m}:=m^{-1}\\widehat{R}_{1}+\\left(1-m^{-1}\\right)\\widehat{R}_{\\infty}\\quad\\mathrm{for}\\quad m>M_{0}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The meta-algorithm that implements the above cross-validation procedure is provided in Algorithm 1. To efficiently tune the parameters of ridge ensembles, we use and combine the corrected generalized cross-validation (CGCV) method [13] and the extrapolated cross-validation (ECV) method [26]. The improved CV method is implemented in the Python library [24]. ", "page_idx": 9}, {"type": "text", "text": "4.3 Validation on real-world datasets ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we present numerical experiments to validate our theoretical results on real-world datasets. Figure 3 provides evidence supporting Assumption A on pretrained features extracted from commonly used neural networks applied to real-world datasets. The first panel of the figure demonstrates the equivalence of degrees of freedom for these pretrained features. Furthermore, we also observe consistent behavior across different neural network architectures and different datasets (see Figures 8 and 9). Remarkably, the path of equivalence can be accurately predicted, offering valuable insight into the underlying dynamics of these models. This observation suggests that the pretrained features from widely used neural networks exhibit similar properties when applied to realworld data, regardless of the specific architecture employed. The ability to predict the equivalence path opens up new possibilities for optimizing the performance of these models in practical applications. ", "page_idx": 9}, {"type": "text", "text": "One implication of the equivalence results explored in Theorems 1 and 6 is that instead of tuning for the full ridge penalty $\\mu$ on the large datasets, we can fix a small value of the ridge penalty $\\lambda$ , fit subsample ridge ensembles, and tune for an optimal subsample size $k$ . To illustrate the validity of the tuning procedure described in Algorithm 1, we present both the actual prediction errors and their estimates by Algorithm 1 in Figure 4. We observe that the risk estimates closely match the prediction risks at different ensemble sizes across different datasets. Even with a subsampling ratio $k/n$ of 0.01 and a sufficiently large $M$ , the risk estimate is close to the optimal risk. A smaller subsample size could also yield even smaller prediction risk in certain datasets. ", "page_idx": 9}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/b7b31080c7635c8549c1beb0fd175e71e5ada747c1cd9ff45f8f13ad6573c9cd.jpg", "img_caption": ["Figure 4: Risk estimation by corrected and extrapolated generalized cross-validation. The risk estimates are computed based on $M_{0}=25$ base estimators using Algorithm 1 with $\\lambda=10^{-3}$ . "], "img_footnote": [], "page_idx": 10}, {"type": "text", "text": "5 Limitations and outlook ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "While our results are quite general in terms of applying to a wide variety of pretrained features, they are limited in that they only apply to ridge regression fitted on the pretrained features. The key challenge for extending the analysis based on Assumption A to general estimators beyond ridge regression is the characterization of the effect of subsampling general resolvents as additional ridge regularization. To extend to generalized linear models, one approach is to view the optimization as iteratively reweighted least squares [38] in combination with the current results. Another approach is to combine our results with the techniques in [41] to obtain deterministic equivalents for the Hessian, enabling an understanding of implicit regularization due to subsampling beyond linear models. ", "page_idx": 10}, {"type": "text", "text": "Beyond implicit regularization due to subsampling, there are other forms of implicit regularization, such as algorithmic regularization due to early stopping in gradient descent [2, 3, 49], dropout regularization [62, 65], among others. In some applications, multiple forms of implicit regularization are present simultaneously. For instance, during a mini-batch gradient step, implicit regularization arises from both iterative methods and mini-batch subsampling. The results presented in this paper may help to make explicit the combined effect of various forms of implicit regularization. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Benson Au, Daniel LeJeune, Ryan Tibshirani, and Alex Wei for the helpful conversations surrounding this work. We also thank the anonymous reviewers for their valuable feedback and suggestions. ", "page_idx": 10}, {"type": "text", "text": "We acknowledge the computing support the ACCESS allocation MTH230020 provided for some of the experiments performed on the Bridges2 system at the Pittsburgh Supercomputing Center. The code for reproducing the results of this paper can be found at https://jaydu1.github.io/ overparameterized-ensembling/weighted-neural. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Adlam, B., Levinson, J. A., and Pennington, J. (2022). A random matrix perspective on mixtures of nonlinearities in high dimensions. In International Conference on Artificial Intelligence and Statistics.   \n[2] Ali, A., Dobriban, E., and Tibshirani, R. J. (2020). The implicit regularization of stochastic gradient flow for least squares. In International conference on machine learning.   \n[3] Ali, A., Kolter, J. Z., and Tibshirani, R. J. (2019). A continuous-time view of early stopping for least squares regression. In International Conference on Artificial Intelligence and Statistics.   \n[4] Anderson, G. W., Guionnet, A., and Zeitouni, O. (2010). An Introduction to Random Matrices. Cambridge University Press.   \n[5] Ando, R. and Komaki, F. (2023). On high-dimensional asymptotic properties of model averaging estimators. arXiv preprint arXiv:2308.09476.   \n[6] Bach, F. (2023). High-dimensional analysis of double descent for linear regression with random projections. arXiv:2303.01372.   \n[7] Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. (2021). Explaining neural scaling laws. arXiv:2102.06701.   \n[8] Bai, Z. and Silverstein, J. W. (2010). Spectral Analysis of Large Dimensional Random Matrices. Springer. Second edition.   \n[9] Barthelm\u00e9, S., Amblard, P.-O., Tremblay, N., and Usevich, K. (2023). Gaussian process regression in the flat limit. The Annals of Statistics, 51(6):2471\u20132505.   \n[10] Bartlett, P. L., Long, P. M., Lugosi, G., and Tsigler, A. (2020). Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070.   \n[11] Bartlett, P. L., Montanari, A., and Rakhlin, A. (2021). Deep learning: A statistical viewpoint. Acta Numerica, 30:87\u2013201.   \n[12] Belkin, M., Hsu, D., and Xu, J. (2020). Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167\u20131180.   \n[13] Bellec, P., Du, J.-H., Koriyama, T., Patil, P., and Tan, K. (2023). Corrected generalized cross-validation for finite ensembles of penalized estimators. arXiv preprint arXiv:2310.01374.   \n[14] Bordelon, B., Canatar, A., and Pehlevan, C. (2020). Spectrum dependent learning curves in kernel regression and wide neural networks. In International Conference on Machine Learning.   \n[15] C\u00e9bron, G., Dahlqvist, A., and Gabriel, F. (2022). Freeness of type $B$ and conditional freeness for random matrices. arXiv preprint arXiv:2205.01926.   \n[16] Chen, X., Zeng, Y., Yang, S., and Sun, Q. (2023). Sketched ridgeless linear regression: The role of downsampling. In International Conference on Machine Learning.   \n[17] Clart\u00e9, L., Vandenbroucque, A., Dalle, G., Loureiro, B., Krzakala, F., and Zdeborov\u00e1, L. (2024). Analysis of bootstrap and subsampling in high-dimensional regularized regression. arXiv preprint arXiv:2402.13622.   \n[18] Couillet, R. and Liao, Z. (2022). Random Matrix Methods for Machine Learning. Cambridge University Press.   \n[19] Cui, H., Loureiro, B., Krzakala, F., and Zdeborov\u00e1, L. (2021). Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural Information Processing Systems, 34:10131\u201310143.   \n[20] Dobriban, E. and Sheng, Y. (2020). Wonder: Weighted one-shot distributed ridge regression in high dimensions. Journal of Machine Learning Research, 21(66):1\u201352.   \n[21] Dobriban, E. and Sheng, Y. (2021). Distributed linear regression by averaging. The Annals of Statistics, 49(2):918\u2013943.   \n[22] Dobriban, E. and Wager, S. (2018). High-dimensional asymptotics of prediction: Ridge regression and classification. The Annals of Statistics, 46(1):247\u2013279.   \n[23] Drineas, P., Mahoney, M. W., and Muthukrishnan, S. (2006). Sampling algorithms for $\\ell_{2}$ regression and applications. Proceedings of the ACM-SIAM Symposium on Discrete Algorithm.   \n[24] Du, J.-H. and Patil, P. (2023). Python package sklearn_ensemble_cv v0.2.1. PyPI.   \n[25] Du, J.-H., Patil, P., and Kuchibhotla, A. K. (2023). Subsample ridge ensembles: Equivalences and generalized cross-validation. In International Conference on Machine Learning.   \n[26] Du, J.-H., Patil, P., Roeder, K., and Kuchibhotla, A. K. (2024). Extrapolated cross-validation for randomized ensembles. Journal of Computational and Graphical Statistics.   \n[27] Efron, B. (1983). Estimating the error rate of a prediction rule: improvement on cross-validation. Journal of the American Statistical Association, 78(382):316\u2013331.   \n[28] Efron, B. (1986). How biased is the apparent error rate of a prediction rule? Journal of the American Statistical Association, 81(394):461\u2013470.   \n[29] Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2020). When do neural networks outperform kernel methods? Advances in Neural Information Processing Systems.   \n[30] Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949\u2013986.   \n[31] Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models. Chapman & Hall.   \n[32] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR).   \n[33] Jacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems.   \n[34] Jacot, A., Simsek, B., Spadaro, F., Hongler, C., and Gabriel, F. (2020). Kernel alignment risk estimator: Risk prediction from training data. Advances in Neural Information Processing Systems.   \n[35] Koehler, F., Zhou, L., Sutherland, D. J., and Srebro, N. (2021). Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting. Advances in Neural Information Processing Systems.   \n[36] Krogh, A. and Sollich, P. (1997). Statistical mechanics of ensemble learning. Physical Review E, 55(6):811.   \n[37] LeJeune, D., Javadi, H., and Baraniuk, R. (2020). The implicit regularization of ordinary least squares ensembles. In International Conference on Artificial Intelligence and Statistics.   \n[38] LeJeune, D., Javadi, H., and Baraniuk, R. G. (2021). The flip side of the reweighted coin: Duality of adaptive dropout and regularization. In Advances in Neural Information Processing Systems.   \n[39] LeJeune, D., Patil, P., Javadi, H., Baraniuk, R. G., and Tibshirani, R. J. (2024). Asymptotics of the sketched pseudoinverse. SIAM Journal on Mathematics of Data Science, 6(1):199\u2013225.   \n[40] Liang, T. and Rakhlin, A. (2020). Just interpolate: Kernel \u201cridgeless\u201d regression can generalize. The Annals of Statistics.   \n[41] Liao, Z. and Mahoney, M. W. (2021). Hessian eigenspectra of more realistic nonlinear models. In Advances in Neural Information Processing Systems, volume 34.   \n[42] Liu, S. and Dobriban, E. (2020). Ridge regression: Structure, cross-validation, and sketching. In International Conference on Learning Representations.   \n[43] Loureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., Mezard, M., and Zdeborova, L. (2021). Learning curves of generic features maps for realistic datasets with a teacher-student model. In Advances in Neural Information Processing Systems.   \n[44] Mahoney, M. W. (2011). Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning, 3(2):123\u2013224.   \n[45] Mallinar, N., Simon, J. B., Abedsoltan, A., Pandit, P., Belkin, M., and Nakkiran, P. (2022). Benign, tempered, or catastrophic: A taxonomy of overfitting. arXiv:2207.06569.   \n[46] Mei, S. and Montanari, A. (2022). The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667\u2013766.   \n[47] Mingo, J. A. and Speicher, R. (2017). Free Probability and Random Matrices, volume 35. Springer.   \n[48] Nakkiran, P., Venkat, P., Kakade, S. M., and Ma, T. (2021). Optimal regularization can mitigate double descent. In International Conference on Learning Representations.   \n[49] Neu, G. and Rosasco, L. (2018). Iterate averaging as regularization for stochastic gradient descent. In Conference On Learning Theory.   \n[50] Patil, P. and Du, J.-H. (2023). Generalized equivalences between subsampling and ridge regularization. Advances in Neural Information Processing Systems.   \n[51] Patil, P., Du, J.-H., and Kuchibhotla, A. K. (2023). Bagging in overparameterized learning: Risk characterization and risk monotonization. Journal of Machine Learning Research, 24(319):1\u2013113.   \n[52] Patil, P., Du, J.-H., and Tibshirani, R. J. (2024). Optimal ridge regularization for out-ofdistribution prediction. arXiv preprint arXiv:2404.01233.   \n[53] Patil, P., Kuchibhotla, A. K., Wei, Y., and Rinaldo, A. (2022). Mitigating multiple descents: A model-agnostic framework for risk monotonization. arXiv preprint arXiv:2205.12937.   \n[54] Patil, P. and LeJeune, D. (2024). Asymptotically free sketched ridge ensembles: Risks, crossvalidation, and tuning. In International Conference on Learning Representations.   \n[55] Paul, D. and Aue, A. (2014). Random matrix theory in statistics: A review. Journal of Statistical Planning and Inference, 150:1\u201329.   \n[56] Rahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines. Advances in neural information processing systems, 20.   \n[57] Sahraee-Ardakan, M., Emami, M., Pandit, P., Rangan, S., and Fletcher, A. K. (2022). Kernel methods and multi-layer perceptrons learn linear models in high dimensions. arXiv preprint arXiv:2201.08082.   \n[58] Serdobolskii, V. I. (2007). Multiparametric Statistics. Elsevier.   \n[59] Slagel, J. T., Chung, J., Chung, M., Kozak, D., and Tenorio, L. (2019). Sampled tikhonov regularization for large linear inverse problems. Inverse Problems, 35(11):114008.   \n[60] Sollich, P. (2001). Gaussian process regression with mismatched models. Advances in Neural Information Processing Systems, 14.   \n[61] Sollich, P. and Krogh, A. (1995). Learning with ensembles: How overfitting can be useful. In Advances in Neural Information Processing Systems.   \n[62] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958.   \n[63] Stein, C. M. (1981). Estimation of the mean of a multivariate normal distribution. The Annals of Statistics, pages 1135\u20131151.   \n[64] Voiculescu, D. V. (1997). Free Probability Theory. American Mathematical Society.   \n[65] Wager, S., Wang, S., and Liang, P. S. (2013). Dropout training as adaptive regularization. In Advances in Neural Information Processing Systems.   \n[66] Wei, A., Hu, W., and Steinhardt, J. (2022). More than a toy: Random matrix models predict how real-world neural representations generalize. In International Conference on Machine Learning.   \n[67] Weisberg, S. (2005). Applied Linear Regression. John Wiley & Sons.   \n[68] Yang, G. (2019). Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760.   \n[69] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems.   \n[70] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations.   \n[71] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This serves as an appendix to the paper \u201cImplicit Regularization Paths of Weighted Neural Representations.\u201d The beginning (unlabeled) section of the appendix provides an organization for the appendix, followed by a summary of the general notation used in both the paper and the appendix. Any other specific notation is explained inline where it is first used. ", "page_idx": 15}, {"type": "text", "text": "Organization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 In Appendix A, we provide a brief technical background on free probability theory and various transforms that we need and collect known asymptotic ridge equivalents that we use in our proofs.   \n\u2022 In Appendix B, we present proofs of the theoretical results in Section 3 (Theorems 1 and 2 and Propositions 3\u20135).   \n\u2022 In Appendix C, we present proofs of the theoretical results in Section 4 (Theorem 6 and Proposition 7).   \n\u2022 In Appendix D, we provide additional illustrations for the results in Section 3 (Figures 5 and 6).   \n\u2022 In Appendix E, we provide additional illustrations for the results in Section 4 (Figures 7\u20139), including our meta-algorithm for tuning (Algorithm 1) that is not included in the main text due to space constraints.   \n\u2022 In Appendix F, we provide additional details on the experiments in both Section 3 and Section 4. ", "page_idx": 15}, {"type": "text", "text": "Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use blackboard letters to denote some special sets: $\\mathbb{N}$ denotes the set of natural numbers, $\\mathbb{R}$ denotes the set of real numbers, $\\mathbb{R}_{+}$ denotes the set of positive real numbers, $\\mathbb{C}$ denotes the set of complex numbers, $\\mathbb{C}^{+}$ denotes the set of complex numbers with positive imaginary part, and $\\mathbb{C}^{-}$ denotes the set of complex numbers with negative imaginary part. We use $[n]$ to denote the index set $\\{1,2,\\ldots,n\\}$ . ", "page_idx": 15}, {"type": "text", "text": "We denote scalars and vectors using lower-case letters and matrices using upper-case letters. For a vector $\\beta,\\beta^{\\top}$ denotes its transpose, and $\\lVert\\beta\\rVert_{2}$ denotes its $\\ell_{2}$ norm. For a pair of vectors $\\textbf{\\em u}$ and $\\pmb{v}$ $\\langle\\boldsymbol{u},\\boldsymbol{v}\\rangle$ denotes their inner product. For a matrix $\\boldsymbol{X}\\,\\in\\,\\mathbb{R}^{n\\times p}$ , $\\pmb{X}^{\\top}\\in\\mathbb{R}^{p\\times n}$ denotes its transpose, and $\\pmb{X}^{\\dag}\\in\\mathbb{R}^{p\\times n}$ denotes its Moore-Penrose inverse. For a square matrix $A\\in\\mathbb{R}^{p\\times p}$ , $\\operatorname{tr}[A]$ denotes its trace, $\\overline{{\\operatorname{tr}}}[A]$ denotes its average trace $\\mathrm{tr}[\\boldsymbol{A}]/p$ , and $A^{-1}$ denotes its inverse, provided that $\\pmb{A}$ is invertible. For a symmetric matrix $\\pmb{A}$ , $\\lambda_{\\operatorname*{min}}^{+}(A)$ denotes its minimum nonzero eigenvalue. For a positive semidefinite matrix $\\boldsymbol{G}$ , $G^{1/2}$ denotes its principal square root. For a matrix $\\mathbf{\\deltaX}$ , we denote by $\\bar{\\|}X\\|_{\\mathrm{op}}$ its operator norm with respect to the $\\ell_{2}$ vector norm. It is also the spectral norm of $\\mathbf{\\deltaX}$ . For a matrix $\\mathbf{\\deltaX}$ , we denote by $\\|X\\|_{\\mathrm{tr}}$ its trace norm. It is given by $\\mathrm{tr}[(X^{\\top}X)^{1/2}]$ , and is also the nuclear norm $\\mathbf{\\deltaX}$ . We denote $p\\times p$ identity matrix by $I_{p}$ , or simply by $\\boldsymbol{\\mathit{I}}$ when it is clear from the context. ", "page_idx": 15}, {"type": "text", "text": "For symmetric matrices $\\pmb{A}$ and $_B$ , we use $A\\preceq B$ to denote the Loewner ordering to mean that $A-B$ is a positive semidefinite matrix. For two sequences of matrices $A_{p}$ and $B_{p}$ , we use $A_{p}\\simeq B_{p}$ to denote a certain asymptotic equivalence; see Appendix A.3 for a precise definition. ", "page_idx": 15}, {"type": "text", "text": "A Technical background ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Basics of free probability theory ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we briefly review definitions from free probability theory and its applications to random matrices. This review will help set the stage by introducing the various mathematical structures and spaces we are working with. It will also introduce some of the notation used throughout the text. ", "page_idx": 15}, {"type": "text", "text": "Free probability is a mathematical framework that deals with non-commutative random variables [19]. The use of free probability theory has appeared in various recent works in statistical machine learning, including [1, 2, 11, 12, 17]. Good references on free probability theory include [3, 13], from ", "page_idx": 15}, {"type": "text", "text": "which we borrow some basic definitions in the following. All the material in this section is standard in free probability theory and mainly serves to keep the definitions self-contained. ", "page_idx": 16}, {"type": "text", "text": "Definition 8 (Non-commutative algebra). A set $\\boldsymbol{\\mathcal{A}}$ is called a (complex) algebra (over the field of complex numbers $\\mathbb{C}$ ) if it is a vector space (over $\\mathbb{C}$ with addition $+.$ ), equipped with a bilinear multiplication $\\cdot$ , such that for all $x,y,z\\in A$ and $\\alpha\\in\\mathbb{C}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x\\cdot(y\\cdot z)=(x\\cdot y)\\cdot z,}\\\\ &{(x+y)\\cdot z=x\\cdot z+y\\cdot z,}\\\\ &{x\\cdot(y+z)=x\\cdot y+x\\cdot z,}\\\\ &{\\alpha(x\\cdot y)=(\\alpha x)\\cdot y=x\\cdot(\\alpha y}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, an algebra is called unital if a multiplicative identity element exists. We will use $1_{\\mathcal{A}}$ to denote this identity element. We will drop the \u201c\u00b7\u201d symbol to denote multiplication over the algebra. ", "page_idx": 16}, {"type": "text", "text": "Definition 9 (Non-commutative probability space). Let $\\boldsymbol{\\mathcal{A}}$ over $\\mathbb{C}$ be a unital algebra with identity $^{1}\\!{\\mathcal A}$ . Let $\\varphi:A\\rightarrow\\mathbb{C}$ be a linear functional which is unital (that is, $\\varphi(1_{\\mathcal{A}})\\,=\\,\\mathrm{{i}})$ ). Then $({\\mathcal{A}},\\varphi)$ is called a non-commutative probability space, and $\\varphi$ is called a state. A state $\\varphi$ is said to be tracial if $\\varphi(x y)=\\varphi(y x)$ for all $x,y\\in A$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 10 (Moments). Let $({\\mathcal{A}},\\varphi)$ be a non-commutative probability space. The numbers $\\{\\varphi(x^{k})\\}_{k=1}^{\\infty}$ are called the moments of the variable $x\\in A$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 11 ( $^*$ -algebra). An algebra $\\boldsymbol{\\mathcal{A}}$ is called a $^*$ -algebra if there exists a mapping $x\\to x^{*}$ from $A\\rightarrow A$ such that, for all $x,y\\in A$ and $\\alpha\\in\\mathbb{C}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(x+y)^{*}=x^{*}+y^{*},}}\\\\ {{(\\alpha x)^{*}=\\bar{\\alpha}x^{*},}}\\\\ {{(x y)^{*}=y^{*}x^{*},}}\\\\ {{(x^{*})^{*}=x.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A variable $x$ of a $^*$ -algebra is called self-adjoint if $x=x^{*}$ . A unital linear functional $\\varphi$ on a $^*$ -algebra is said to be positive if $\\varphi(x^{*}x)\\geq0$ for all $x\\in A$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 12 ( $^*$ -probability space). Let $\\boldsymbol{\\mathcal{A}}$ be a unital $^*$ -algebra with a positive state $\\varphi$ . Then $({\\mathcal{A}},\\varphi)$ is called a $^*$ -probability space. ", "page_idx": 16}, {"type": "text", "text": "Example 1. Denote by $\\mathcal{M}_{p}(\\mathbb{C})$ the collection of all $p\\times p$ matrices with complex entries. Let the multiplication and addition operations be defined in the usual way. The $^*$ -operation is the same as taking the conjugate transpose. Let $\\mathrm{tr}:\\mathcal{M}_{p}(\\mathbb{C})\\rightarrow\\mathbb{C}$ be the normalized trace defined by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overleftarrow{\\operatorname{tr}}(A)=\\frac{1}{p}\\operatorname{tr}[A].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The state $\\operatorname{tr}$ is tracial and positive. ", "page_idx": 16}, {"type": "text", "text": "Definition 13 (Free independence). Suppose $({\\mathcal{A}},\\varphi)$ is a $^*$ -probability space. Then, the $^*$ -subalgebras $\\{\\boldsymbol{\\mathcal{A}}_{i}\\}_{i\\in I}$ of $\\boldsymbol{\\mathcal{A}}$ are said to be $^*\\cdot$ -freely independent (or simply $^*$ -free) if, for all $n\\geq2$ and all $x_{1},x_{2},\\cdots\\,,x_{n}$ from $\\{A_{i}\\}_{i\\in I},\\kappa_{n}(x_{1},x_{2},\\dot{\\cdot}\\cdot\\cdot\\dot{,}x_{n}^{-})=0$ whenever at least two of the $x_{i}$ are from different $A_{i}$ . In particular, any collection of variables is said to be $^*$ -free if the sub-algebras generated by these variables are $^*$ -free. ", "page_idx": 16}, {"type": "text", "text": "Lemma 14. Suppose $({\\mathcal{A}},\\varphi)$ is a $^*$ -probability space. If $x$ and $y$ are free in $({\\mathcal{A}},\\varphi)$ , then for all non-negative integers $n$ and $m$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi(x^{n}y^{m})=\\varphi(x^{n})\\varphi(y^{m})=\\varphi(y^{m}x^{n}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In other words, elements of the algebra are considered free if any alternating product of centered polynomials is also centered. ", "page_idx": 16}, {"type": "text", "text": "In this work, we will consider $\\varphi$ to be the normalized trace. The normalized trace is the generalization of ${\\begin{array}{r}{{\\frac{1}{p}}\\operatorname{tr}[A]}\\end{array}}$ for $A\\in\\mathbb{C}^{p\\times p}$ to elements of a $C^{*}$ -algebra $\\boldsymbol{\\mathcal{A}}$ . Specifically, for any self-adjoint $a\\in{\\mathcal{A}}$ and any polynomial $p$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi(p(a))=\\int p(z)\\,\\mathrm{d}\\mu_{a}(z),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mu_{a}$ is the probability measure that characterizes the spectral distribution of $a$ . ", "page_idx": 17}, {"type": "text", "text": "Definition 15 (Convergence in spectral distribution). Let $({\\mathcal{A}},\\varphi)$ be a $C^{*}$ -probability space. We say that $A_{1},\\dots,A_{m}\\in\\mathbb{C}^{p\\times p}$ converge in spectral distribution to elements $a_{1},\\dotsc,a_{m}\\in{\\mathcal{A}}$ if, for all $1\\leq\\ell<\\infty$ and $1\\leq i_{j}\\leq m$ for $1\\leq j\\leq\\ell$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\operatorname{tr}[\\pmb{A}_{i_{1}}\\cdot\\cdot\\cdot\\pmb{A}_{i_{\\ell}}]\\rightarrow\\varphi(a_{i_{1}}\\cdot\\cdot\\cdot a_{i_{\\ell}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, with slight abuse of notation, two matrices $\\pmb{A},\\pmb{B}\\in\\mathbb{R}^{p\\times p}$ are said to be free if ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{p}\\,\\mathrm{tr}\\left[\\prod_{\\ell=1}^{L}\\mathrm{poly}_{\\ell}^{A}(A)\\mathrm{poly}_{\\ell}^{B}(B)\\right]=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $L\\geq1$ and all centered polynomials, that is, $\\overline{{{\\mathrm{tr}}}}[\\mathrm{poly}_{\\ell}^{{\\cal A}}({\\cal A})]=0$ . This notation is an abuse of notation because finite matrices cannot satisfy this condition. However, they can satisfy it asymptotically as $p\\rightarrow\\infty$ , and in this case, we say that $\\pmb{A}$ and $_B$ are asymptotically free. ", "page_idx": 17}, {"type": "text", "text": "Note: With some abuse of notation, we will let matrices in boldface denote both the finite matrix and the limiting element in the free probability space. The limiting element can be understood, for example, as a bounded linear operator on a Hilbert space. We also remark that all notions we need are well-defined in this limit as well, as long as they are appropriately normalized. ", "page_idx": 17}, {"type": "text", "text": "A.2 Useful transforms and their relationships ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we review the key transforms used in free probability theory and their interrelationships. ", "page_idx": 17}, {"type": "text", "text": "Definition 16 (Cauchy transform). Let $a$ be an element of a $^*$ -probability space $({\\mathcal{A}},\\varphi)$ . Suppose there exists some $C>0$ such that $|\\varphi(a^{n})|\\leq C^{n}$ for all $n\\in\\mathbb N$ . Then the Cauchy transform of $a$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathcal G}_{a}(z)=\\sum_{n=0}^{\\infty}\\frac{\\varphi(a^{n})}{z^{n+1}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $z\\in\\mathbb{C}$ with $|z|>C$ . ", "page_idx": 17}, {"type": "text", "text": "Note that the Cauchy transform is the negative of the Stieltjes transform. In this paper, we will focus only on the Cauchy transform. Recall that for a probability measure $\\nu$ on $\\mathbb{R}$ and for $z\\not\\in\\mathbb{R}$ , the Cauchy transform of $\\nu$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathcal{G}}(z)=\\int_{\\mathbb{R}}{\\frac{1}{z-x}}\\,\\mathrm{d}\\nu(x).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The definition above is motivated by the following property of the Cauchy transform of a measure. Suppose $\\nu$ is a probability measure whose support is contained in $[-C,C]$ for some $C>0$ and which has moments $\\{m_{k}(\\nu)\\}_{k=0}^{\\infty}$ . Then the Cauchy transform of $\\nu$ is defined for $z\\in\\mathbb{C}$ with $|z|>C$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathcal G}_{\\nu}(z)=\\sum_{k=0}^{\\infty}\\frac{m_{k}(\\nu)}{z^{k+1}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Definition 17 (Moment generating function). Let $a$ be an element of a $^*$ -probability space $({\\mathcal{A}},\\varphi)$ . The moment generating function of $a$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{M}_{a}(z)=1+\\sum_{k=1}^{\\infty}\\varphi(a^{k})z^{k}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $z\\in\\mathbb{C}$ such that $|z|<r_{a}$ . Here, $r_{a}$ is the radius of convergence of the series. ", "page_idx": 17}, {"type": "text", "text": "For a probability measure $\\nu$ , the moment generating function is defined analogously. (Note: The definition above is not to be confused with the moment generating function of a random variable in probability theory.) The Cauchy transform is related to the moment series via: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{G}_{a}(z)=\\frac{1}{z}\\mathcal{M}_{a}\\left(\\frac{1}{z}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the other direction, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{M}_{a}(z)=\\frac{1}{z}\\mathcal{G}_{a}\\left(\\frac{1}{z}\\right)-1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Definition 18 ( $S$ -transform). For ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{M}_{a}(z)=\\sum_{m=0}^{\\infty}\\varphi(a^{m})z^{m},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we define the $S$ -transform of $a$ by: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{a}(w)=\\frac{1+w}{w}\\mathcal{M}_{a}^{(-1)}(w),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathcal{M}^{\\langle-1\\rangle}$ denotes the inverse under composition of $\\mathcal{M}$ . ", "page_idx": 18}, {"type": "text", "text": "Finally, in terms of operator $\\pmb{A}$ , we summarize the series of invertible transformations between the various transforms introduced in this section. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Cauchy transform: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{G}_{A}(z)=\\overline{{\\mathrm{tr}}}\\bigl[(z I-A)^{-1}\\bigr].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u2022 Moment generating series: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{M}}_{A}(z)=\\frac{1}{z}\\mathcal{G}_{A}\\left(\\frac{1}{z}\\right)-1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u2022 $S$ -transform: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{S}_{\\boldsymbol{A}}(\\boldsymbol{w})=\\frac{1+w}{w}\\boldsymbol{\\mathcal{M}}_{\\boldsymbol{A}}^{\\langle-1\\rangle}(\\boldsymbol{w}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here: ", "page_idx": 18}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\mathcal{M}_{A}(z)=\\sum_{k=1}^{\\infty}\\overline{{\\operatorname{tr}}}[A^{k}]z^{k}}\\end{array}$ is the moment generating series.   \n\u2022 $\\mathcal{M}_{A}^{(-1)}$ denotes the inverse under composition of $\\mathcal{M}_{A}$ .   \n\u2022 tr $[A]$ denotes the average trace $\\mathrm{tr}[\\boldsymbol{A}]/p$ of a matrix $A\\in\\mathbb{R}^{p\\times p}$ . ", "page_idx": 18}, {"type": "text", "text": "A.3 Asymptotic ridge resolvents ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide a brief background on the language of asymptotic equivalents used in the proofs throughout the paper. We will state the definition of asymptotic equivalents and point to useful calculus rules. For more details, see [16, Appendix S.7]. ", "page_idx": 18}, {"type": "text", "text": "To concisely present our results, we will use the framework of asymptotic equivalence [5, 6, 16], defined as follows. Let $A_{p}$ and $B_{p}$ be sequences of matrices of arbitrary dimensions (including vectors and scalars). We say that $A_{p}$ and $B_{p}$ are asymptotically equivalent, denoted as $A_{p}\\simeq B_{p}$ , if $\\begin{array}{r}{\\operatorname*{lim}_{p\\to\\infty}|\\operatorname{tr}[C_{p}(A_{p}-B_{p})]|=0}\\end{array}$ almost surely for any sequence of random matrices $C_{p}$ with bounded trace norm that are independent of $A_{p}$ and $B_{p}$ . Note that for sequences of scalar random variables, the definition simply reduces to the typical almost sure convergence of sequences of random variables involved. ", "page_idx": 18}, {"type": "text", "text": "The notion of deterministic equivalents obeys various calculus rules such as sum, product, differentiation, conditioning, and substitution. We refer the reader to [16] for a comprehensive list of these calculus rules, their proofs, and other related details. ", "page_idx": 18}, {"type": "text", "text": "Next, we collect first- and second-order asymptotic equivalents for sketched ridge resolvents from [11, 17], which will be useful for our extensions to weighted ridge resolvents. ", "page_idx": 18}, {"type": "text", "text": "Assumption B (Sketch structure). Let $S\\in\\mathbb{R}^{p\\times q}$ be the feature sketching matrix and $\\boldsymbol{X}\\in\\mathbb{R}^{n\\times p}$ be the data matrix. Let $S S^{\\top}$ and $\\scriptstyle{{\\frac{1}{n}}X^{\\top}X}$ converge almost surely to bounded operators that are infinitesimally free with respect to $\\begin{array}{r}{(\\frac{1}{p}\\tan[\\cdot],\\mathrm{tr}[\\Theta(\\cdot)])}\\end{array}$ for any $\\Theta$ independent of $\\boldsymbol{S}$ with $\\|\\Theta\\|_{\\mathrm{tr}}$ uniformly bounded. Additionally, let $S S^{\\top}$ have a limiting $S$ -transform that is analytic on the lower half of the complex plane. ", "page_idx": 18}, {"type": "text", "text": "For the statement to follow, let us define $\\begin{array}{r}{\\widehat{\\Sigma}:=\\frac{1}{n}X^{\\top}X}\\end{array}$ . Let $\\widetilde{\\lambda}_{0}:=-\\operatorname*{lim}\\operatorname*{inf}_{p\\rightarrow\\infty}\\lambda_{\\operatorname*{min}}^{+}(S^{\\top}\\widehat{\\Sigma}S)$ .   \nHere, recall that $\\lambda_{\\operatorname*{min}}^{+}(A)$ represents the minimum nonzero eigenvalue of a symmetric matrix $\\pmb{A}$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem 19 (Free sketching equivalence; [11], Theorem 7.2). Under Assumption $B_{i}$ , for all $\\lambda>{\\widetilde\\lambda}_{0}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S(S^{\\top}\\widehat{\\Sigma}S+\\lambda I_{q})^{\\dagger}S^{\\top}\\simeq(\\widehat{\\Sigma}+\\nu I_{p})^{\\dagger},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\nu>-\\lambda_{\\operatorname*{min}}^{+}(\\widehat{\\Sigma})$ is increasing in $\\lambda>\\widetilde{\\lambda}_{0}$ and satisfies: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu\\simeq\\lambda S_{S S^{\\top}}(-\\,\\overline{{\\mathrm{tr}}}[\\widehat{\\Sigma}S(S^{\\top}\\widehat{\\Sigma}S+\\lambda I_{q})^{\\dagger}S^{\\top}])\\simeq\\lambda S_{S S^{\\top}}(-\\,\\overline{{\\mathrm{tr}}}[\\widehat{\\Sigma}(\\widehat{\\Sigma}+\\nu I_{p})^{\\dagger}]).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 20 (Second-order equivalence for sketched ridge resolvents; [17], Lemma 15). Under the settings of Lemma 21, for any positive semidefinite $\\Psi$ with uniformly bounded operator norm, for all $\\lambda>\\tilde{\\bar{\\lambda}}_{0}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S(S^{\\top}\\widehat{\\Sigma}S+\\lambda I_{q})^{\\dagger}S^{\\top}\\Psi S(S^{\\top}\\widehat{\\Sigma}S+\\lambda I_{q})^{\\dagger}S^{\\top}\\simeq(\\widehat{\\Sigma}+\\nu I_{p})^{\\dagger}(\\Psi+\\nu_{\\Psi}^{\\prime}I_{p})(\\widehat{\\Sigma}+\\nu I_{p})^{\\dagger},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\nu_{\\Psi}^{\\prime}\\ge0$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{\\Psi}^{\\prime}=-\\frac{\\partial\\nu}{\\partial\\lambda}\\lambda^{2}{\\mathcal{S}}_{S S^{\\top}}^{\\prime}(-\\,\\overline{{\\mathrm{tr}}}[\\widehat{\\Sigma}(\\widehat{\\Sigma}+\\nu I_{p})^{\\dagger}])\\,\\overline{{\\mathrm{tr}}}[(\\widehat{\\Sigma}+\\nu I_{p})^{\\dagger}\\Psi(\\widehat{\\Sigma}+\\nu I_{p})^{\\dagger}].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B Proofs in Section 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our main ingredient in the proof is Lemma 21. We will first show estimator equivalence and then show degrees of freedom equivalence. ", "page_idx": 19}, {"type": "text", "text": "Estimator equivalence. Recall from (1) the ridge estimator on the weighted data is: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\beta}_{W,\\lambda}=(\\Phi^{\\top}W^{\\top}W\\Phi/n+\\lambda I_{p})^{\\dagger}\\Phi^{\\top}W^{\\top}W y/n.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This is the \u201cprimal\u201d form of the ridge estimator. Using the Woodbury matrix identity, we first write the estimator into its \u201cdual\u201d form. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\beta}_{W,\\lambda}=\\Phi^{\\top}W^{\\top}(W\\Phi\\Phi^{\\top}W^{\\top}/n+\\lambda I_{n})^{\\dagger}W y/n}\\\\ &{\\qquad\\quad=\\Phi^{\\top}W^{\\top}(G_{W}+\\lambda I_{n})^{\\dagger}W y/n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we can apply the first part of Lemma 21 to the matrix $W^{\\top}(G_{W}+\\lambda I_{n})^{\\dagger}W$ . From (31), we then have the following equivalence: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\beta}_{W,\\lambda}\\simeq\\Phi^{\\top}(G_{I}+\\mu I_{n})^{\\dagger}\\pmb{y}/n}\\\\ &{\\qquad=\\Phi^{\\top}(\\Phi\\Phi^{\\top}+\\mu I_{n})^{\\dagger}\\pmb{y}/n}\\\\ &{\\qquad=(\\Phi^{\\top}\\Phi/n+\\mu I_{n})^{\\dagger}\\Phi^{\\top}\\pmb{y}/n=\\widehat{\\beta}_{I,\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mu$ satisfies the following equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu=\\lambda S_{W^{\\top}W}\\left(-\\frac{\\mathrm{tr}[G_{I}(G_{I}+\\mu I_{n})^{\\dagger}]}{n}\\right)=\\lambda S_{W^{\\top}W}(-\\,\\overline{{{\\mathsf{d f}}}}(\\widehat\\beta_{I,\\mu})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that in the simplification, we used the Woodbury identity again to go back from the dual form into the primal form for the ridge estimator based on the full data. Rearranging, we obtain the desired estimator equivalence. We next move on to showing the degrees of freedom equivalence. ", "page_idx": 19}, {"type": "text", "text": "Degrees of freedom equivalence. For the subsampled estimator $\\widehat{\\beta}_{W,\\lambda}$ , the effective degrees of freedom is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{d f}(\\widehat{\\beta}_{W,\\lambda})=\\mathrm{tr}[\\Phi^{\\top}\\Phi/n(\\Phi^{\\top}\\Phi/n+\\lambda I_{p})^{\\dagger}]}\\\\ &{\\qquad\\qquad=\\mathrm{tr}[\\Phi(\\Phi^{\\top}\\Phi/n+\\lambda I_{p})^{\\dagger}\\Phi^{\\top}/n]}\\\\ &{\\qquad\\qquad=\\mathrm{tr}[(\\Phi\\Phi^{\\top}/n+\\lambda I_{n})^{\\dagger}\\Phi\\Phi^{\\top}/n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second equality above follows from the push-through identity $\\Phi(\\Phi^{\\top}\\Phi/n\\,+\\,\\lambda I_{p})^{\\dagger}\\Phi^{\\top}\\;=$ $(\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^{\\intercal}+\\lambda\\boldsymbol{I}_{n})^{\\dagger}\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^{\\intercal}$ . Recognizing the quantity inside the trace as the degrees of freedom of the full ridge estimator, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mu=\\lambda S_{W^{\\top}W}(-\\mathsf{d f}(\\widehat{\\beta}_{I,\\mu})).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can equivalently write the equation above as ", "page_idx": 20}, {"type": "equation", "text": "$$\n-S_{W^{\\top}W}^{-1}\\left(\\frac{\\mu}{\\lambda}\\right)=\\mathsf{d f}(\\widehat{\\beta}_{I,\\mu})\\quad\\mathrm{or}\\quad\\frac{\\mu}{\\lambda}=S_{W^{\\top}W}(-\\mathsf{d f}(\\widehat{\\beta}_{I,\\mu})).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Rearranging the display above provides the desired degrees of freedom equivalence and finishes the proof. ", "page_idx": 20}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We will apply Theorem 1 to the subsampling weight matrix $W$ . The main ingredient that we need is the $S$ -transform of the spectrum of the matrix $\\mathbf{\\dot{W}}^{\\top}\\mathbf{W}$ . As summarized in Appendix A.2, one approach to compute the $S$ -transform is to go through the following chain of transforms. First, we apply the Cauchy transform, then the moment-generating series, and finally, take the inverse to obtain the $S$ -transform. We will do this in the following steps. ", "page_idx": 20}, {"type": "text", "text": "Cauchy transform. Recall that the Cauchy transform from Definition 16 can be computed as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{G}_{W^{\\top}W}(z)=\\overline{{\\mathrm{tr}}}[(z I_{n}-W^{\\top}W)^{-1}].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moment generating series. We can then compute the moment series from Definition 17 using (14) as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{W^{\\top}W}(z)=\\frac{1}{z}\\,\\mathrm{\\bar{tr}}\\left[\\left(\\frac{1}{z}I_{n}-W^{\\top}W\\right)^{-1}\\right]-1}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{\\bar{tr}}[(I_{n}-z W^{\\top}W)^{-1}]-\\mathrm{\\bar{tr}}[I_{n}]}\\\\ &{\\qquad\\qquad\\quad=-\\mathrm{\\bar{tr}}[I_{n}]+\\mathrm{\\bar{tr}}[(I_{n}-z W^{\\top}W)^{-1}]}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{\\bar{tr}}[(z W^{\\top}W-I_{n}+I_{n})(I_{n}-z W^{\\top}W)^{-1}]}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{\\bar{tr}}[z W^{\\top}W(I_{n}-z W^{\\top}W)^{-1}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now note that the matrix $W^{\\top}W$ has $k$ eigenvalues of 1 and $n-k$ eigenvalues of 0. Therefore, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{W^{\\top}W}(z)=\\overline{{\\mathrm{tr}}}[z\\pmb{W}^{\\top}\\pmb{W}(I_{n}-z\\pmb{W}^{\\top}\\pmb{W})^{-1}]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{n}\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{z d_{i}}{1-z d_{i}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{k}{n}\\cdot\\frac{z}{1-z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$S$ -transform. The inverse of the moment generating series map $z\\mapsto\\mathcal{M}_{W^{\\top}W}(z)$ from (20) is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{M}^{\\langle-1\\rangle}(w)=\\frac{w}{w+k/n}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, from Definition 18 and using (21), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nS(w)=\\frac{1+w}{w}\\cdot\\frac{w}{w+k/n}=\\frac{1+w}{w+k/n}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, we are ready to apply Theorem 1 to the subsampling matrix $W$ . ", "page_idx": 20}, {"type": "text", "text": "Substituting (22) into (2), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mu}{\\lambda}=S(-\\,\\overline{{{\\mathsf{d f}}}}(\\widehat{\\beta}_{I,\\mu}))=\\frac{1-\\overline{{{\\mathsf{d f}}}}(\\widehat{\\beta}_{I,\\mu})}{-\\,\\overline{{{\\mathsf{d f}}}}(\\widehat{\\beta}_{I,\\mu})+k/n}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Rearranging, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overline{{\\mathsf{d f}}}(\\widehat{\\beta}_{I,\\mu})\\cdot(\\mu-\\lambda)=\\mu\\cdot(k/n)-\\lambda.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overline{{{\\mathsf{d f}}}}(\\widehat{\\beta}_{I,\\mu})=-\\frac{\\lambda-\\mu\\cdot(k/n)}{\\mu-\\lambda}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In other words, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n1-\\overline{{{\\mathsf{d f}}}}(\\widehat{\\beta}_{I,\\mu})=\\left(\\frac{\\mu}{\\mu-\\lambda}\\right)\\cdot\\left(1-\\frac{k}{n}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Multiplying $(1-\\lambda/\\mu)$ on both sides, we arrive at the desired relation. This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "B.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We prove this by matching the path (4) with the one in [15]. Let $\\gamma=p/n,\\,\\psi\\,=\\,p/k,\\,H_{p}$ be the spectral distribution of ${\\widehat{\\pmb{\\Sigma}}}={\\pmb X}^{\\top}{\\pmb X}/n$ . The path from Equation (5) of [15] is given by the following equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu=(\\psi-\\gamma)\\int\\frac{r}{1+v(\\mu,\\gamma)r}\\,\\mathrm{d}H_{p}(r),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $v(\\mu,\\gamma)$ is the unique solution to the following fixed-point equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{v(\\mu,\\gamma)}=\\mu+\\gamma\\int\\frac{r}{1+v(\\mu,\\gamma)r}\\,\\mathrm{d}H_{p}(r)=\\psi\\int\\frac{r}{1+v(\\mu,\\gamma)r}\\,\\mathrm{d}H_{p}(r).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For given $\\gamma$ and $\\mu$ , we will show that $\\psi$ that solves (23) gives rise $k=p/\\psi$ that also solves (4) with $\\lambda=0$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n-{\\frac{1}{n}}\\operatorname{tr}\\left[{\\frac{1}{n}}X X^{\\top}\\left({\\frac{1}{n}}X X^{\\top}+\\mu I_{n}\\right)^{\\dagger}\\right]=-{\\frac{k}{n}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Rearranging the above equation yields: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{k}{n}=1-\\mu\\,\\overline{{\\mathrm{tr}}}\\left[\\left(\\frac{1}{n}X X^{\\top}+\\mu I_{n}\\right)^{\\dagger}\\right]=1-\\mu v(\\mu,\\gamma),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second equality is from Lemma B.2 of [15]. This implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{\\mu=\\left(1-{\\frac{k}{n}}\\right){\\frac{1}{v(\\mu,\\gamma)}}}\\\\ {\\ \\ =\\left(1-{\\frac{k}{n}}\\right)\\psi\\int{\\frac{r}{1+v(\\mu,\\gamma)r}}\\,\\mathrm{d}H_{p}(r)}\\\\ {\\ \\ =(\\psi-\\gamma)\\int{\\frac{r}{1+v(\\mu,\\gamma)r}}\\,\\mathrm{d}H_{p}(r),}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second equality follows from (24). The above is the same as the path (23) in [15]. This finishes the proof. ", "page_idx": 21}, {"type": "text", "text": "B.4 Proof of Proposition 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first describe the setup for the kernel ridge regression formulation and then show the desired equivalence. ", "page_idx": 21}, {"type": "text", "text": "Setup. Let $K(\\cdot,\\cdot):\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a kernel function. Let $\\mathcal{H}$ denote the reproducing kernel Hilbert space associated with kernel $K$ . Kernel ridge regression with the subsampling matrix $W$ solves the following problem with tuning parameter $\\lambda\\geq0$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{f}_{{W,\\lambda}}=\\underset{f\\in\\mathcal{H}}{\\operatorname{argmin}}\\,\\|{W}y-{W f(X)}\\|_{2}^{2}/n+\\lambda\\|f\\|_{\\mathcal{H}}^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $f(X)\\ =\\ [f(\\pmb{x}_{1}),\\dots,f(\\pmb{x}_{n})]^{\\intercal}$ . Kernel ridge regression predictions have a closed-form expression: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{f}_{W,\\lambda}(\\pmb{x})=K(\\pmb{x},\\pmb{X})^{\\top}W^{\\top}(W K(\\pmb{X},\\pmb{X})W^{\\top}+\\lambda I_{n})^{\\dagger}W y.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, $K(\\mathbf{{x}},X)\\ \\in\\ \\mathbb{R}^{n}$ with $i$ -th entry $K(\\pmb{x},\\pmb{x}_{i})$ , and $K(\\mathbf{\\boldsymbol{X}},\\mathbf{\\boldsymbol{X}})\\ \\in\\ \\mathbb{R}^{n\\times n}$ with the $i j$ -th entry $K(x_{i},\\pmb{x}_{j})$ . ", "page_idx": 22}, {"type": "text", "text": "The predicted values on the training data $\\mathbf{\\deltaX}$ are given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{f}_{\\pmb{W},\\lambda}(\\pmb{X})=K(\\pmb{X},\\pmb{X})^{\\top}\\pmb{W}^{\\top}(\\pmb{W}K(\\pmb{X},\\pmb{X})\\pmb{W}^{\\top}+\\lambda\\pmb{I}_{n})^{\\dagger}\\pmb{W}\\pmb{y}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, the matrix $K(X,X)^{\\top}W^{\\top}(W K(X,X)W^{\\top}+\\lambda I_{n})^{\\top}W$ is the smoothing matrix. ", "page_idx": 22}, {"type": "text", "text": "Define $G_{I}=K(X,X)$ and $G_{W}=W K(X,X)W^{\\top}$ . Leveraging the kernel trick, the preceding optimization problem translates into solving the following problem (in the dual domain): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\alpha}_{W,\\lambda}=\\operatorname*{argmin}_{\\alpha\\in\\mathbb{R}^{n}}\\alpha^{\\top}\\left(G_{W}+\\lambda I_{n}\\right)\\alpha+2\\alpha^{\\top}W y,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the dual solution is given by $\\widehat{\\alpha}_{W,\\lambda}\\ =\\ (G_{W}\\,+\\,\\lambda I_{n})^{\\dagger}W y$ . The correspondence between the dual and primal solutions is simply given by: $\\widehat{\\beta}_{W,\\lambda}\\;=\\;\\Phi^{\\top}W^{\\top}\\widehat{\\alpha}_{W,\\lambda}$ where $\\Phi\\,=$ $[\\phi(\\mathbf{x}_{1}),\\ldots,\\phi(\\mathbf{x}_{n})]^{\\top}$ is the feature matrix and $\\phi\\colon\\ensuremath{\\mathbb{R}}^{d}\\mapsto\\mathcal{H}$ is the feature map of the Hilbert space $\\mathcal{H}$ with kernel $K$ . Thus, $\\widehat{f}_{W,\\lambda}(X)=W\\Phi\\widehat{\\beta}_{W,\\lambda}=W\\Phi\\Phi^{\\top}W^{\\top}\\widehat{\\alpha}_{W,\\lambda}=G_{W}(G_{W}+\\lambda I_{n})^{\\dagger}W y$ and the degrees of freedom is given by $\\mathsf{d}\\mathsf{f}(\\widehat{\\beta}_{I,\\mu})=\\mathrm{tr}[G_{W}(G_{W}+\\lambda I_{n})^{\\dagger}]$ . ", "page_idx": 22}, {"type": "text", "text": "Next, we show that (3) holds. Alternatively, one can also show that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\alpha}_{W,\\lambda}\\simeq\\widehat{\\alpha}_{I,\\mu},\\quad\\mathrm{and}\\quad\\widehat{f}_{W,\\lambda}({\\pmb x}_{0})\\simeq\\widehat{f}_{I,\\mu}({\\pmb x}_{0}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which we omit due to similarity. Our proof strategy consists of two steps. We first show that it suffices to establish the desired result for the linearized version. We then show that we can suitably adapt our result for the linearized version. ", "page_idx": 22}, {"type": "text", "text": "Linearization of kernels. In the below, we will show that for $\\mu\\geq0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{W}^{\\top}(\\pmb{G}_{W}+\\lambda\\pmb{I}_{n})^{\\dagger}\\pmb{W}\\simeq(\\pmb{G}_{I}+\\mu\\pmb{I}_{n})^{\\dagger},}\\\\ &{\\quad\\mpb{\\mathrm{fr}}[\\lambda(\\pmb{G}_{W}+\\lambda\\pmb{I}_{n})^{\\dagger}]\\simeq\\mpb{\\mathrm{fr}}[\\mu(\\pmb{G}_{I}+\\mu\\pmb{I}_{n})^{\\dagger}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $W$ and $\\lambda\\geq0$ satisfy that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu=\\lambda S_{W^{\\top}W}(-\\frac{1}{n}\\operatorname{tr}[G_{I}(G_{I}+\\lambda I_{n})^{\\dagger}])=\\lambda S_{W^{\\top}W}(-\\frac{1}{n}\\operatorname{tr}[G_{W}(G_{W}+\\mu I_{n})^{\\dagger}]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using assumptions of Proposition 3 and the assumption in Proposition 4, by [18, Proposition 5.1], ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|G_{I}-G_{I}^{\\mathrm{lin}}\\|_{\\mathrm{op}}\\stackrel{\\mathrm{p}}{\\to}0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\nG_{I}^{\\mathrm{lin}}=c_{0}I_{n}+c_{1}{\\bf1}_{n}{\\bf1}_{n}^{\\top}+c_{2}X X^{\\top}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\left(c_{0},c_{1},c_{2}\\right)$ associated with function $g$ in Proposition 4 and $\\tau=\\mathrm{lim}_{p\\rightarrow\\infty}\\,\\mathrm{tr}[\\Sigma]/p$ are defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{0}=g(\\tau,\\tau,\\tau)-g(\\tau,0,\\tau)-c_{2}\\displaystyle\\frac{\\mathrm{tr}[\\Sigma]}{p},}\\\\ &{c_{1}=g(\\tau,0,\\tau)+g^{\\prime\\prime}(\\tau,0,\\tau)\\displaystyle\\frac{\\mathrm{tr}[\\Sigma^{2}]}{2p^{2}},}\\\\ &{c_{2}=g^{\\prime}(\\tau,0,\\tau).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Assume $\\textstyle C_{n}$ is a sequence of random matrices with bounded trace norm. Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}[C_{p}((G_{I}+\\mu I_{n})^{\\dagger}-(G_{I}^{\\mathrm{lin}}+\\mu I_{n})^{\\dagger})]}\\\\ &{\\leq\\mathrm{tr}[C_{p}]\\|(G_{I}+\\mu I_{n})^{\\dagger}-(G_{I}^{\\mathrm{lin}}+\\mu I_{n})^{\\dagger}\\|_{\\mathrm{op}}}\\\\ &{\\leq\\mathrm{tr}[C_{p}]\\|(G_{I}+\\mu I_{n})^{\\dagger}\\|_{\\mathrm{op}}\\|(G_{I}^{\\mathrm{lin}}+\\mu I_{n})^{\\dagger}\\|_{\\mathrm{op}}\\|G_{I}-G_{I}^{\\mathrm{lin}}\\|_{\\mathrm{op}}\\xrightarrow{\\mathrm{a.s.}}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in the last inequality, we use a matrix identity $A^{-1}-B^{-1}=A^{-1}(B-A)B^{-1}$ for two invertible matrices $\\pmb{A}$ and $_B$ . Thus, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n(G_{I}+\\mu I_{n})^{\\dagger}\\simeq_{p}(G_{I}^{\\mathrm{lin}}+\\mu I_{n})^{\\dagger}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, combining (29) and the transition property of asymptotic equivalence [15, Lemma S.7.4 (1)], it suffices to show ", "page_idx": 23}, {"type": "equation", "text": "$$\nW^{\\top}(G_{W}^{\\mathrm{lin}}+\\lambda I_{n})^{\\dagger}W\\simeq(G_{I}^{\\mathrm{lin}}+\\mu I_{n})^{\\dagger},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $G_{W}^{\\mathrm{lin}}=W G_{I}^{\\mathrm{lin}}W^{\\top}$ , and $\\lambda$ and $\\mu$ satisfy (25). Similarly, we can also show that the path (25) is asymptotically equivalent to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu=\\lambda S_{W^{\\top}W}(-\\frac{1}{n}\\operatorname{tr}[G_{I}^{\\mathrm{lin}}(G_{I}^{\\mathrm{lin}}+\\lambda I_{n})^{\\dagger}])=\\lambda S_{W^{\\top}W}(-\\frac{1}{n}\\operatorname{tr}[G_{W}^{\\mathrm{lin}}(G_{W}^{\\mathrm{lin}}+\\mu I_{n})^{\\dagger}]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Equivalence for linearized kernels. We next show that the resolvent equivalence result holds for $K^{\\mathrm{lin}}$ .   \nThis follows from additional manipulations building on Lemma 21. ", "page_idx": 23}, {"type": "text", "text": "B.5 Proof of Proposition 5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the below, we will show that for $\\mu\\geq0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W^{\\top}(W\\Phi\\Phi^{\\top}W^{\\top}/n+\\lambda I_{n})^{\\dagger}W\\simeq(\\Phi\\Phi^{\\top}/n+\\mu I_{n})^{\\dagger},}\\\\ &{\\quad\\overline{{\\mathrm{tr}}}[\\lambda(W\\Phi\\Phi^{\\top}W^{\\top}/n+\\lambda I_{n})^{\\dagger}]\\simeq\\overline{{\\mathrm{tr}}}[\\mu(\\Phi\\Phi^{\\top}/n+\\mu I_{n})^{\\dagger}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Under assumptions in Proposition 5, the linearized features take the form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{\\Phi}^{\\mathrm{lin}}=\\sqrt{\\frac{\\rho_{s}}{d}}\\pmb{F}\\pmb{X}+\\sqrt{\\rho_{s}\\omega_{s}}\\pmb{U},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the constants $\\rho_{s}$ and $\\omega_{s}$ are given in Proposition 5 and $U\\in\\mathbb{R}^{n\\times p}$ has i.i.d. standard normal entries. From Claim A.13 of [10], the linear functionals of the estimators $\\widehat{\\beta}_{D,\\lambda}$ and $\\widehat{\\beta}_{I,\\mu}$ with random features $\\Phi$ and $\\Phi^{\\mathrm{lin}}$ are asymptotically equivalent. Now, following the proof of Proposition 4, we apply Lemma 21 on $\\Phi^{\\mathrm{lin}}$ to yield the desired result. ", "page_idx": 23}, {"type": "text", "text": "B.6 Technical lemmas ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In preparation for the forthcoming statement, define $\\lambda_{0}=-\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}\\lambda_{\\operatorname*{min}}^{+}(G_{W})$ . Recall the Gram matrices $G=\\Phi\\Phi^{\\top}/n$ and $G_{W}=W\\Phi\\Phi^{\\top}W^{\\top}/n$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 21 (General first-order equivalence for freely subsampled ridge resolvents). For $W\\in\\mathbb{R}^{n\\times n}$ , suppose Assumption $_\\mathrm{A}$ holds for $W W^{\\top}$ . Then, for all $\\lambda>\\lambda_{0}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{W}^{\\top}(\\pmb{G}_{W}+\\lambda\\pmb{I})^{\\dagger}\\pmb{W}\\simeq(\\pmb{G}+\\mu\\pmb{I})^{\\dagger},}\\\\ &{~~~\\mpb{\\mathrm{tr}}[\\lambda(\\pmb{G}_{W}+\\lambda\\pmb{I}_{n})^{\\dagger}]\\simeq\\mpb{\\mathrm{tr}}[\\mu(\\pmb{G}+\\mu\\pmb{I}_{n})^{\\dagger}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mu>-\\lambda_{\\operatorname*{min}}^{+}(G)$ solves the equation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mu=\\lambda S_{W W^{\\top}}(-\\operatorname{\\overline{{tr}}}[G(G+\\mu V)^{\\dagger}])\\simeq\\lambda S_{W W^{\\top}}(-\\operatorname{\\overline{{tr}}}[G_{W}(G_{W}+\\lambda V)^{\\dagger}]).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 21. The first result follows from using Theorem 19 by suitably changing the roles of $\\mathbf{\\deltaX}$ and $\\Phi$ . In particular, we set $\\Phi$ to be $X^{\\top}$ and $W$ to be $\\boldsymbol{S}$ and apply Theorem 19 to obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\nW^{\\top}(W\\Phi\\Phi^{\\top}W^{\\top}/n+\\lambda I_{n})^{\\dagger}W\\simeq(\\Phi\\Phi^{\\top}/n+\\mu I_{n})^{\\dagger}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Writing in terms of $\\boldsymbol{G}$ and $G_{W}$ , this proves the first part (31). ", "page_idx": 23}, {"type": "text", "text": "For the second part, we use the result (34) in the first part and multiply both sides by $\\Phi\\Phi^{\\top}/n$ to get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\Phi\\Phi^{\\top}/n)\\cdot W^{\\top}(W\\Phi\\Phi^{\\top}W^{\\top}/n+\\lambda I_{n})^{\\dagger}W\\simeq(\\Phi\\Phi^{\\top}/n)\\cdot(\\Phi\\Phi^{\\top}/n+\\mu I_{n})^{\\dagger}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the trace property of asymptotic equivalence [15, Lemma S.7.4 (4)], we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\overline{{\\operatorname{tr}}}[(\\Phi\\Phi^{\\top}/n)\\cdot W^{\\top}(W\\Phi\\Phi^{\\top}W^{\\top}/n+\\lambda I_{n})^{\\dagger}W]\\simeq\\overline{{\\operatorname{tr}}}[(\\Phi\\Phi^{\\top}/n)\\cdot(\\Phi\\Phi^{\\top}/n+\\mu I_{n})^{\\dagger}].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the cyclic property of the trace operator yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{tr}}}[({W\\Phi\\Phi^{\\top}/n})\\cdot{W^{\\top}({W\\Phi\\Phi^{\\top}W^{\\top}/n+\\lambda I_{n}})^{\\dagger}}]\\simeq\\overline{{\\mathrm{tr}}}[({\\Phi\\Phi^{\\top}/n})\\cdot({\\Phi\\Phi^{\\top}/n+\\mu I_{n}})^{\\dagger}].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In terms of $\\boldsymbol{G}$ and $G_{W}$ , this is the same as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathrm{tr}}}[G_{W}(G_{W}+\\lambda I_{n})^{\\dagger}]\\simeq\\overline{{\\mathrm{tr}}}[G(G+\\mu I_{n})^{\\dagger}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Adding and subtracting $\\lambda I_{n}$ and $\\mu I_{n}$ on the left- and right-hand resolvents, we arrive at the second part (32). This completes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C Proofs in Section 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Proof of Theorem 6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The main ingredients of the proof are Lemmas 21 and 22. We begin by decomposing the unknown response $y_{0}$ into its linear predictor and residual. Specifically, let $\\beta_{0}$ be the optimal projection parameter given by $\\beta_{0}\\;=\\;\\mathbf{\\bar{\\Sigma}}_{0}^{-1}\\mathbb{E}[\\phi_{0}y_{0}]$ . Then, we can express the response as the sum of its best linear predictor, $\\phi_{0}^{\\top}\\beta_{0}$ , and the residual, $y_{0}-\\phi_{0}^{\\top}\\beta_{0}$ . Denote the variance of this residual by $\\sigma_{0}^{2}=\\mathbb{E}[(y_{0}-\\phi_{0}^{\\top}\\beta_{0})^{2}]$ . It is easy to see that the risk decomposes as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R(\\widehat{\\beta}_{W_{1:M},\\lambda})=\\mathbb{E}\\big[(y_{0}-\\phi_{0}^{\\top}\\widehat{\\beta}_{W_{1:M},\\lambda})^{2}\\mid\\Phi,y,\\{W_{m}\\}_{m=1}^{M}\\big]}\\\\ {=(\\widehat{\\beta}_{W_{1:M},\\lambda}-\\beta_{0})^{\\top}\\Sigma(\\widehat{\\beta}_{W_{1:M},\\lambda}-\\beta_{0})+\\sigma_{0}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, we used the fact that $(y_{0}-\\phi_{0}^{\\top}\\beta_{0})$ is uncorrelated with $\\phi_{\\mathrm{0}}$ , that is, $\\mathbb{E}[\\phi_{0}(y_{0}-\\phi_{0}^{\\top}\\beta_{0})]=\\mathbf{0}_{p}$ .   \nWe note that $\\|\\beta_{0}\\|_{2}<\\infty$ and $\\Sigma_{0}$ has uniformly bounded operator norm. ", "page_idx": 24}, {"type": "text", "text": "Observe that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R(\\widehat{\\beta}_{{\\pmb{W}}_{1:M},\\lambda})=(\\widehat{\\beta}_{{\\pmb{W}}_{1:M},\\lambda}-\\beta_{0})^{\\top}\\Sigma_{0}(\\widehat{\\beta}_{{\\pmb{W}}_{1:M},\\lambda}-\\beta_{0})+\\sigma_{0}^{2}}}\\\\ &{=\\bigg(\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\widehat{\\beta}_{{\\pmb{W}}_{m,\\lambda}-\\beta_{0}}\\bigg)^{\\top}\\Sigma_{0}\\bigg(\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\widehat{\\beta}_{{\\pmb{W}}_{m,\\lambda}-\\beta_{0}}\\bigg)+\\sigma_{0}^{2}}\\\\ &{=\\displaystyle\\frac{1}{M^{2}}\\displaystyle\\sum_{k,\\ell=1}^{M}\\widehat{\\beta}_{{\\pmb{W}}_{k,\\lambda}\\Sigma_{0}}^{\\top}\\widehat{\\beta}_{{\\pmb{W}}_{\\ell,\\lambda}-\\frac{2}{M}}\\displaystyle\\sum_{m=1}^{M}\\beta_{0}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{{\\pmb{W}}_{m,\\lambda}+\\beta_{0}^{\\top}}\\Sigma_{0}\\beta_{0}+\\sigma_{0}^{2}}\\\\ &{=\\displaystyle\\frac{1}{M^{2}}\\displaystyle\\sum_{k,\\ell=1}^{M}(\\widehat{\\beta}_{{\\pmb{W}}_{k,\\lambda}}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{{\\pmb{W}}_{\\ell,\\lambda}-\\beta_{1,\\ell}}\\Sigma_{0}\\widehat{\\beta}_{{\\pmb{J}}_{\\pmb{\\mu}}})+\\widehat{\\beta}_{{\\pmb{I}}_{\\mu,\\ell}}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{{\\pmb{I}}_{\\mu}}}\\\\ &{\\quad\\quad-\\displaystyle\\frac{2}{M}\\displaystyle\\sum_{m=1}^{M}\\beta_{0}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{{\\pmb{W}}_{m,\\lambda}+\\beta_{0}^{\\top}}\\Sigma_{0}\\beta_{0}+\\sigma_{0}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma 21, note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{k=1}^{M}\\widehat{\\beta}_{W_{m},\\lambda}\\simeq\\widehat{\\beta}_{I,\\mu}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\widehat{\\beta}_{W_{1:M},\\lambda})\\simeq\\displaystyle\\frac{1}{M^{2}}\\sum_{k,\\ell=1}^{M}\\big(\\widehat{\\beta}_{W_{k},\\lambda}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{W_{\\ell},\\lambda}-\\widehat{\\beta}_{I,\\mu}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{I,\\mu}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\widehat{\\beta}_{I,\\mu}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{I,\\mu}-\\displaystyle\\frac{2}{M}\\sum_{m=1}^{M}\\beta_{0}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{I,\\mu}+\\beta_{0}^{\\top}\\Sigma_{0}\\beta_{0}+\\sigma_{0}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, by two applications of Lemma 21, we know that $\\widehat{\\beta}_{W_{k},\\lambda}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{W_{\\ell},\\lambda}-\\widehat{\\beta}_{I,\\mu}\\Sigma_{0}\\widehat{\\beta}_{I,\\mu}\\xrightarrow{\\mathrm{a.s.}}0$ when $k\\neq\\ell$ since $W_{k}$ and $W_{\\ell}$ are independent. Hence, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{1}(\\widehat{\\beta}_{W_{1:M},\\lambda})\\simeq\\frac{1}{M^{2}}\\sum_{m=1}^{M}\\big(\\widehat{\\beta}_{W_{m},\\lambda}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{W_{m},\\lambda}-\\widehat{\\beta}_{I,\\mu}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{I,\\mu}\\big)+(\\widehat{\\beta}_{I,\\mu}-\\beta_{0})^{\\top}\\Sigma_{0}(\\widehat{\\beta}_{I,\\mu}-\\beta_{0})+\\sigma_{0}^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\simeq\\displaystyle\\frac{1}{M}\\big(\\widehat{\\beta}_{W,\\lambda}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{W,\\lambda}-\\widehat{\\beta}_{I,\\mu}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{I,\\mu}\\big)+(\\widehat{\\beta}_{I,\\mu}-\\beta_{0})^{\\top}\\Sigma_{0}\\big(\\widehat{\\beta}_{I,\\mu}-\\beta_{0}\\big)+\\sigma_{0}^{2}}\\\\ &{=\\displaystyle\\frac{1}{M}\\big(\\widehat{\\beta}_{W,\\lambda}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{W,\\lambda}-\\widehat{\\beta}_{I,\\mu}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{I,\\mu}\\big)+R(\\widehat{\\beta}_{I,\\mu})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used the fact that the $M$ terms where $k=\\ell$ converge identically in the second to last line and a risk decomposition similar to that for $\\widehat{\\beta}_{W_{1:M},\\lambda}$ in the last line. Thus, it suffices to evaluate the difference $\\widehat{\\beta}_{\\lambda}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{\\lambda}-\\widehat{\\beta}_{\\mu}^{\\top}\\Sigma_{0}\\widehat{\\beta}_{\\mu}$ to finish the proof. ", "page_idx": 25}, {"type": "text", "text": "We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\beta}_{W,X}^{\\top}\\Sigma_{0}\\hat{\\beta}_{W,X}-\\hat{\\beta}_{f,X}^{\\top}\\Sigma_{0}\\hat{\\beta}_{X,\\mu}}\\\\ &{=(y^{\\top}W^{\\top}/n)W\\Phi(\\Phi^{\\top}W^{\\top}W\\Phi(\\cdot\\lambda R_{P})^{\\top}\\Sigma_{0}(\\Phi^{\\top}W^{\\top}W\\Phi/n+\\lambda I_{P})^{\\top}\\Phi^{\\top}W^{\\top}W^{\\top}(W y/n)}\\\\ &{\\quad-(y^{\\top}\\Phi/n)(\\Phi^{\\top}\\Phi/n+\\mu I_{P})^{\\top}\\Sigma_{0}(\\Phi^{\\top}\\Phi/n+\\mu I_{P})^{\\top}(\\Phi^{\\top}Y^{\\top}/n)}\\\\ &{=\\bar{\\mathrm{tr}}\\Big[W^{\\top}W\\Phi(\\frac{1}{n}\\Phi^{\\top}W^{\\top}W\\Phi+\\lambda I_{P})\\Sigma_{0}(\\Phi^{\\top}\\Phi(\\cdot\\lambda R_{P})^{\\top}W^{\\top}W\\Phi+\\lambda I_{P})^{\\top}\\Phi^{\\top}W^{\\top}W^{\\top}W\\cdot(y\\Psi^{\\top})\\Big]}\\\\ &{\\quad-\\bar{\\mathrm{tr}}\\big[\\Phi(\\Phi^{\\top}\\Phi/n+\\mu I_{P})\\Sigma_{0}(\\Phi^{\\top}\\Phi/n+\\mu I_{P})^{\\top}\\Phi^{\\top}/n\\cdot(y\\Psi^{\\top})\\big]}\\\\ &{\\simeq\\bar{\\mathrm{tr}}\\Big[(\\Phi^{\\top}\\Phi^{\\top}/n+\\mu I_{P})^{\\top}(\\Phi\\Sigma_{0}\\Phi^{\\top}/n+\\mu I_{Q})^{\\top}(\\Phi^{\\top}\\Phi^{\\top}/n+\\mu I_{n})^{\\top}(y\\Psi^{\\top})\\Big]}\\\\ &{\\quad-\\bar{\\mathrm{tr}}\\Big[\\Phi(\\Phi^{\\top}\\Phi^{\\top}/n+\\mu I_{P})^{\\top}\\Sigma_{0}(\\Phi^{\\top}\\Phi/n+\\mu I_{P})^{\\top}\\Phi^{\\top}/n\\cdot(y\\Psi^{\\top})\\Big]}\\\\ &{=\\bar{\\mathrm{tr}}\\Big[(\\Phi^{\\top}\\Phi^{\\top}/n+\\mu I_{P})^{\\top}(\\Phi\\Sigma_{0}\\Phi^{\\top}/n)(\\Phi^{\\top}\\Phi^{\\top}/n+\\mu I_{P})^{\\top}(y\\Psi^{\\top})\\Big]}\\\\ &{\\quad+\\mu_ \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in the third line, we used the second-order equivalence for freely weighted ridge resolvents from Lemma 22; in the fourth line, we employed the push-through identity multiple times. Substituting for $\\mu_{\\Sigma_{0}}^{\\prime}$ from Lemma 22 in (36) and substituting this back into (35), we arrive at the desired decomposition. This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "C.2 Proof of Proposition 7 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We use the path (4) with $k^{*}$ and $\\mu^{*}$ , and setting $\\lambda^{*}=0$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(1-\\frac{\\mathsf{d f}(\\widehat{\\beta}_{I,\\mu^{*}})}{n}\\right)=\\left(1-\\frac{k^{*}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This suggests that ", "page_idx": 25}, {"type": "equation", "text": "$$\nk^{*}=\\mathsf{d f}(\\widehat{\\beta}_{I,\\mu^{*}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $r:=\\operatorname{rank}(X^{\\top}X)=\\operatorname{rank}(G_{I})$ . By the definition of degrees of freedom, it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{d f}(\\widehat{\\beta}_{I,\\mu^{*}})=\\mathrm{tr}[X^{\\top}X(X^{\\top}X+\\mu^{*}I_{p})^{\\dagger}]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{r}\\frac{s_{i}}{s_{i}+\\mu^{*}}\\leq r=\\mathrm{rank}(G_{I}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $s_{1},\\ldots,s_{r}$ are non-zero eigenvalues of $X^{\\top}X$ . This finishes the proof. ", "page_idx": 25}, {"type": "text", "text": "C.3 Technical lemmas ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Recall from Appendix B.6 that we define $\\begin{array}{r}{\\lambda_{0}=-\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}\\lambda_{\\operatorname*{min}}^{+}(W\\Phi\\Phi^{\\top}W^{\\top}/n).}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Lemma 22 (General second-order equivalence for freely weighted ridge resolvents). Under the settings of Lemma 21, for any positive semidefinite $\\Sigma_{0}$ with uniformly bounded operator norm, for all $\\lambda>\\lambda_{0}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}W^{\\top}W\\Phi(\\frac{1}{n}\\Phi^{\\top}W^{\\top}W\\Phi+\\lambda I_{p})^{\\dagger}\\Sigma_{0}(\\frac{1}{n}\\Phi^{\\top}W^{\\top}W\\Phi+\\lambda I_{p})^{\\dagger}\\Phi^{\\top}W^{\\top}W}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mu_{\\Sigma_{0}}^{\\prime}\\geq0$ is given by: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mu_{\\Sigma_{0}}^{\\prime}=-\\frac{\\partial\\mu}{\\partial\\lambda}\\lambda^{2}S_{W W^{\\top}}^{\\prime}\\big(-\\frac{1}{n}\\operatorname{tr}\\big[\\frac{1}{n}\\Phi\\Phi^{\\top}\\big(\\frac{1}{n}\\Phi\\Phi^{\\top}+\\mu I_{n}\\big)^{\\dagger}\\big]\\big)}}\\\\ {{\\displaystyle\\quad\\cdot\\,\\frac{1}{p}\\operatorname{tr}\\big[\\big(\\frac{1}{n}\\Phi\\Phi^{\\top}+\\mu I_{n}\\big)^{\\dagger}\\big(\\frac{1}{n}\\Phi\\Sigma_{0}\\Phi^{\\top}\\big)\\big(\\frac{1}{n}\\Phi\\Phi^{\\top}+\\mu I_{n}\\big)^{\\dagger}\\big].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We use the Woodbury matrix identity to write ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}{W}^{\\top}\\Phi(\\frac{1}{n}\\Phi^{\\top}{W}{W}^{\\top}\\Phi+\\lambda I_{p})^{\\dag}\\Sigma_{0}(\\frac{1}{n}\\Phi^{\\top}{W}{W}^{\\top}\\Phi+\\lambda I_{p})^{\\dag}\\Phi^{\\top}{W}{W}^{\\top}}\\\\ &{=\\frac{1}{n}{W}(\\frac{1}{n}{W}^{\\top}\\Phi\\Phi^{\\top}{W}+\\lambda I_{m})^{\\dag}{W}^{\\top}\\Phi\\Sigma_{0}\\Phi^{\\top}{W}(\\frac{1}{n}{W}^{\\top}\\Phi\\Phi^{\\top}{W}+\\lambda I_{m})^{\\dag}{W}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The equivalence in (37) and the inflation parameter in (38) now follow from the second-order result for feature sketch by substituting $W$ for $\\boldsymbol{S}$ , $\\Phi$ for $\\Phi^{\\top}$ , and $\\frac{1}{n}\\Phi\\Sigma_{0}\\Phi^{\\top}$ for $\\Sigma_{0}$ in (18). \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D Additional illustrations for Section 3 ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/a45662cdd9ccca1349351882cbe6ca021236363a233a7c4d2d776f099951f54d.jpg", "img_caption": ["D.1 Implicit regularization paths for bootstrapping "], "img_footnote": ["Figure 5: Equivalence under bootstrapping. The left panel shows the heatmap of degrees of freedom, and the right panel shows the random projection $\\mathbb{E}_{W}[\\pmb{a}^{\\top}\\widehat{\\beta}_{W,\\lambda}]$ where $\\mathbf{\\boldsymbol{a}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}}_{p},I_{p}/p)$ . In both heatmaps, the red lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths obtained by matching empirical degrees of freedom. Despite the complexity of the theoretical path for bootstrapping, we observe that the empirical paths closely resemble it. Therefore, the theoretical path for sampling without replacement from (4) serves as a good approximation. "], "page_idx": 27}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/0d0182b0e5223218b16f9bc7e324dd67d78830fdb6ee86c64956a29fd612820d.jpg", "img_caption": ["D.2 Implicit regularization paths with non-uniform weights "], "img_footnote": ["Figure 6: Equivalence under non-uniform weighting. The left panel shows the heatmap of degrees of freedom, and the right panel shows the random projection $\\mathbb{E}_{W}[\\pmb{a}^{\\top}\\widehat{\\beta}_{W,\\lambda}]$ , where $\\pmb{a}\\sim\\mathcal{N}(\\mathbf{0}_{p},\\pmb{I}_{p}/p)$ . The weights $(\\mathrm{diag}(W))$ for observations are initially generated as $(9/10)^{i}$ for $i=0,\\ldots,n-1$ , subsample $k$ entries from $\\{1,\\ldots,n\\}$ , zero out the other $n-k$ entries, and then normalized to have norm $k$ . The black dashed lines indicate the empirical paths obtained by matching the empirical degrees of freedom. "], "page_idx": 27}, {"type": "text", "text": "E Additional illustrations for Section 4 ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/863f18f0a35c461e2c5825801397ab7d057bc4f699a48249e4fb33c9c575bc0c.jpg", "img_caption": ["E.1 Rate illustration for ensemble risk against ensemble size "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 7: Risk equivalence for random feature structures when sampling without replacement. The solid lines represent the prediction risks and their estimates of the subsample ridge ensemble, and the red dashed lines indicate the prediction error of the full ridge predictor. The data and random features with the ReLU activation function are generated according to Appendix F.1 with $n=5000$ and $p=500$ . The regularization level for the full ridge is set as $\\mu=1$ , and each subsampled ridge ensemble is fitted with $M=100$ randomly sampled subsampling matrices. For each value of $\\lambda$ , the subsample ratio is determined by solving Equation (4). ", "page_idx": 28}, {"type": "image", "img_path": "oXCmwwkQTZ/tmp/69398f6aff6c55e33f20bbbbe2804f5816bdbb2b050afcb033d7a2f9c526b17b.jpg", "img_caption": ["E.2 Real data illustrations for implicit regularization paths "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F Details of experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "F.1 Simulation details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The simulation settings are as follows. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Covariance model. The covariance matrix of an auto-regressive process of order 1 (AR(1)) is given by $\\Sigma_{\\mathrm{ar1}}\\in\\mathbb{R}^{d\\times d}$ , where $(\\Sigma_{\\mathrm{ar1}})_{i j}\\,=\\,\\rho_{\\mathrm{ar1}}^{|i-j|}$ for some parameter $\\rho_{\\mathrm{ar1}}\\,\\in\\,(0,1)$ . For the simulations, we set $\\rho_{\\mathrm{ar1}}=0.25$ . ", "page_idx": 28}, {"type": "text", "text": "\u2022 Signal model. Define $\\begin{array}{r}{\\beta_{0}=\\frac{1}{5}\\sum_{j=1}^{5}{\\pmb w}_{(j)}}\\end{array}$ where $\\pmb{w}_{(j)}$ is the eigenvector of $\\pmb{\\Sigma}_{\\mathrm{ar1}}$ associated with the top jth eigenvalue r(j). ", "page_idx": 29}, {"type": "equation", "text": "$$\ny_{i}=\\pm_{i}^{\\top}\\beta_{0}+\\frac{1}{p}(\\Vert\\pmb{x}_{i}\\Vert_{2}^{2}-\\mathrm{tr}[\\Sigma_{\\mathrm{arl}}])+\\varepsilon_{i},\\quad\\pmb{x}_{i}=\\Sigma_{\\mathrm{arl}}^{\\frac{1}{2}}z_{i},\\quad z_{i j}\\overset{i i d}{\\sim}\\frac{t_{5}}{\\sigma_{5}},\\quad\\varepsilon_{i}\\sim\\frac{t_{5}}{\\sigma_{5}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\sigma_{5}=\\sqrt{5/3}$ is the standard deviation of $t_{5}$ distribution. ", "page_idx": 29}, {"type": "text", "text": "The benefit of using the above nonlinear model is that we can clearly separate the linear and the nonlinear components and compute the quantities of interest because $\\beta_{0}$ happens to be the best linear projection. ", "page_idx": 29}, {"type": "text", "text": "The linear, random, and kernel features are generated as follows. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Linear features. For a given feature dimension $p$ , we use $d=p$ raw features from (M-AR1) as linear features.   \n\u2022 Random features. For generating random features, we use $d=2p$ raw features from (M-AR1) and sample a randomly initialized weight matrix $\\pmb{F}\\in\\mathbb{R}^{p\\times d}$ whose entries are i.i.d. samples from $\\mathcal{N}(0,d^{-1/2})$ . Then the transform feature is given by $\\widetilde{\\pmb{x}}_{i}=\\varphi(\\pmb{F}\\pmb{x}_{i})\\in\\mathbb{R}^{p}$ , where $\\varphi$ is a nonlinear transformation and set to be ReLU function in our experiment.   \n\u2022 Kernel features. For kernel features, we use $d=p$ raw features from (M-AR1) to construct the kernel matrix. ", "page_idx": 29}, {"type": "text", "text": "In the simulations, the estimates are averaged across 20 simulations with different random seeds. ", "page_idx": 29}, {"type": "text", "text": "F.2 Experimental details in Section 4.3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Following the similar experimental setup in [20], we use residual networks to extract features on several computer vision datasets, both at random initialization and after pretraining. More specifically, we consider ResNet-{18, 34, 50, 101} applied to the CIFAR-{10,100} [9], Fashion-MNIST [21], Flowers-102 [14], and Food-101 [4] datasets. All random initialization was done following [8]; pretrained networks (obtained from PyTorch) were pretrained on ImageNet, and the outputs of the last pretrained layer on each dataset mentioned above were used as the embedding feature $\\Phi$ . ", "page_idx": 29}, {"type": "text", "text": "After obtaining the embedding features from the last layer of the neural network model, we further normalize each row of the pretrained feature to have a norm of $p$ , and center the one-hot labels to have zero means. To reduce the computational burden, we only consider the first 10 one-hot labels of all datasets. For datasets with different data aspect ratios, we stratify $10\\%$ of the training samples as the training set for the CIFAR-100 dataset. The training and predicting errors are the mean square errors on the training and test sets, respectively, aggregated over all the labels. ", "page_idx": 29}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "[1] Adlam, B., Levinson, J. A., and Pennington, J. (2022). A random matrix perspective on mixtures of nonlinearities in high dimensions. In International Conference on Artificial Intelligence and Statistics. ", "page_idx": 29}, {"type": "text", "text": "[2] Adlam, B. and Pennington, J. (2020). The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization. In International Conference on Machine Learning. ", "page_idx": 29}, {"type": "text", "text": "[3] Bose, A. (2021). Random Matrices and Non-Commutative Probability. CRC Press. ", "page_idx": 29}, {"type": "text", "text": "[4] Bossard, L., Guillaumin, M., and Van Gool, L. (2014). Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference. Springer. ", "page_idx": 29}, {"type": "table", "img_path": "oXCmwwkQTZ/tmp/0249ef2998f18f43aaff3143f01dba6b82540be6edf1f01569bd8cf98ea62b54.jpg", "table_caption": ["Table 2: Summary of pretrained features from different real datasets. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "[5] Dobriban, E. and Sheng, Y. (2020). Wonder: Weighted one-shot distributed ridge regression in high dimensions. Journal of Machine Learning Research, 21(66):1\u201352. ", "page_idx": 30}, {"type": "text", "text": "[6] Dobriban, E. and Sheng, Y. (2021). Distributed linear regression by averaging. The Annals of Statistics, 49(2):918\u2013943.   \n[7] Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949\u2013986.   \n[8] He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In International Conference on Computer Vision.   \n[9] Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images.   \n[10] Lee, D., Moniri, B., Huang, X., Dobriban, E., and Hassani, H. (2023). Demystifying disagreement-on-the-line in high dimensions. In International Conference on Machine Learning.   \n[11] LeJeune, D., Patil, P., Javadi, H., Baraniuk, R. G., and Tibshirani, R. J. (2024). Asymptotics of the sketched pseudoinverse. SIAM Journal on Mathematics of Data Science, 6(1):199\u2013225.   \n[12] Mel, G. and Pennington, J. (2021). Anisotropic random feature regression in high dimensions. In International Conference on Learning Representations.   \n[13] Mingo, J. A. and Speicher, R. (2017). Free Probability and Random Matrices, volume 35. Springer.   \n[14] Nilsback, M.-E. and Zisserman, A. (2008). Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing.   \n[15] Patil, P. and Du, J.-H. (2023). Generalized equivalences between subsampling and ridge regularization. Advances in Neural Information Processing Systems.   \n[16] Patil, P., Du, J.-H., and Kuchibhotla, A. K. (2023). Bagging in overparameterized learning: Risk characterization and risk monotonization. Journal of Machine Learning Research, 24(319):1\u2013113.   \n[17] Patil, P. and LeJeune, D. (2024). Asymptotically free sketched ridge ensembles: Risks, crossvalidation, and tuning. In International Conference on Learning Representations.   \n[18] Sahraee-Ardakan, M., Emami, M., Pandit, P., Rangan, S., and Fletcher, A. K. (2022). Kernel methods and multi-layer perceptrons learn linear models in high dimensions. arXiv preprint arXiv:2201.08082. ", "page_idx": 30}, {"type": "text", "text": "[19] Voiculescu, D. V. (1997). Free Probability Theory. American Mathematical Society. ", "page_idx": 31}, {"type": "text", "text": "[20] Wei, A., Hu, W., and Steinhardt, J. (2022). More than a toy: Random matrix models predict how real-world neural representations generalize. In International Conference on Machine Learning. [21] Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All the claims made the abstract are justified by both theoretical and experimental results in Sections 3 and 4 and the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The assumptions are discussed and explained after their first mention (either in Section 2 or right after the theoretical result that uses them). The main limitations of the paper are discussed in the last section (Section 5). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The assumptions for each of the results are included in the main text, and the complete proofs for each of the results are included in the appendix. The beginning of the appendix provides an organization for the proofs of all the results mentioned in the main text. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The code and instructions for reproducing experimental results in this paper are included in the supplementary materials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 33}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We use publicly available data. The code and instructions for reproducing experimental results in this paper are included in the supplementary materials. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The experimental details are included in the appendix, and the source code is provided in the supplementary materials. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The standard errors across multiple random seeds are included for Figure 7. Note that for the heatmaps, we only report the mean statistics because of visual constraints. However, the standard errors for the heatmaps are small enough not to impact the regularization paths indicated. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The compute resources are described in the README file of the submitted code in the supplementary materials. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper provides a theoretical analysis and does not have immediate societal impacts. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not pose any risks that require safeguards ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper correctly cites papers of related assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]