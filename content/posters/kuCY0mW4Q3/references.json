{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the transformer architecture, a foundational model for many large language models, including those used in the current paper's experiments."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the few-shot learning capabilities of large language models, a key concept related to parameter-efficient fine-tuning, the focus of the current paper."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-05-01", "reason": "This paper introduced the LoRA algorithm, a parameter-efficient fine-tuning method that is directly improved upon in the current paper."}, {"fullname_first_author": "Junxian He", "paper_title": "Towards a unified view of parameter-efficient transfer learning", "publication_date": "2021-05-01", "reason": "This paper provided a comprehensive overview of parameter-efficient transfer learning methods, which provides context for understanding the current paper's contributions."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper introduced the GPT-2 language model, which is used in the current paper's experiments on natural language generation tasks."}]}