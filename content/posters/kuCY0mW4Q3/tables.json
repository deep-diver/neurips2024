[{"figure_path": "kuCY0mW4Q3/tables/tables_5_1.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqy are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents the results of the experiments conducted on the GLUE benchmark using different parameter-efficient fine-tuning methods: LoRA, VeRA, Tied-LoRA, and the proposed VB-LoRA.  The table compares the performance of these methods on six tasks from GLUE (SST-2, MRPC, CoLA, QNLI, RTE, and STS-B) using both RoBERTabase and RoBERTalarge models.  The results are presented as the median accuracy or correlation from five runs, and the best-performing method for each task is highlighted in bold.  The table also shows the number of parameters used by each method, highlighting the efficiency of VB-LORA.", "section": "4 Experiments"}, {"figure_path": "kuCY0mW4Q3/tables/tables_6_1.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqy are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents the results of experiments conducted on the GLUE benchmark using different parameter-efficient fine-tuning (PEFT) methods:  Full Fine-Tuning (FT), LoRA, VeRA, Tied-LoRA, and VB-LoRA.  Results are shown for both RoBERTa-base and RoBERTa-large models.  The table shows performance on six GLUE tasks (SST-2, MRPC, CoLA, QNLI, RTE, and STS-B),  reporting Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for the other tasks.  Results are the median of five runs, and the best result for each model size is highlighted in bold. The '# Params' column indicates the number of trainable parameters for each PEFT method.  Query and value only (qv) and all linear layers (all) variations are included for comparison.", "section": "4.1 Natural Language Understanding"}, {"figure_path": "kuCY0mW4Q3/tables/tables_7_1.jpg", "caption": "Table 2: Results with GPT-2 Medium and GPT-2 Large on the E2E benchmark. The results for FT and LoRA are taken from Hu et al. [2021], and the results for VeRA are taken from Kopiczko et al. [2024]. We report the mean of 3 runs using different random seeds.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods on the E2E dataset for natural language generation using the GPT-2 Medium and Large language models.  It shows the number of parameters used by each method (Full Fine-tuning, LoRA, VeRA, and VB-LORA), along with their respective BLEU, NIST, METEOR, ROUGE-L, and CIDEr scores. The results highlight the parameter efficiency of VB-LORA compared to other methods while maintaining competitive performance.", "section": "4.2 Natural Language Generation"}, {"figure_path": "kuCY0mW4Q3/tables/tables_8_1.jpg", "caption": "Table 3: Results with Llama2 on MT-Bench, scored by GPT-4 out of 10. LoRA and VeRA are sourced from Kopiczko et al. [2024]. LoRA# and VB-LoRA are from our implementations. The discrepancy between LoRA and LoRA may be due to changes in the GPT-4 model over time.", "description": "This table presents the results of instruction tuning experiments using the Llama2 model (7B and 13B parameters) on the MT-Bench dataset.  The models were fine-tuned using different parameter-efficient fine-tuning (PEFT) methods: LoRA, VeRA, and the authors' proposed VB-LoRA.  The evaluation metric is a score out of 10, assigned by GPT-4. The table highlights the performance of VB-LoRA in achieving comparable or better results than other methods while using significantly fewer parameters.  Note that slight discrepancies exist between the LoRA scores reported in this table and those reported by Kopiczko et al. [2024], likely due to variations in the GPT-4 model over time.", "section": "4.3 Instruction Tuning"}, {"figure_path": "kuCY0mW4Q3/tables/tables_8_2.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqy are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents the results of experiments conducted on the GLUE benchmark using different parameter-efficient fine-tuning (PEFT) methods, including the proposed VB-LoRA.  The table compares the performance of VB-LoRA against LoRA, VeRA, and Tied-LoRA on six tasks from the GLUE benchmark (SST-2, MRPC, CoLA, QNLI, RTE, and STS-B).  Both RoBERTa-base and RoBERTa-large models were used, and the results are reported as the median across five runs with different random seeds, showing accuracy for most tasks, Matthews correlation for CoLA, and Pearson correlation for STS-B. Results for LoRAqv and VeRAqv were obtained from their original papers, while the others were reproduced by the authors using their own implementations.", "section": "4 Experiments"}, {"figure_path": "kuCY0mW4Q3/tables/tables_9_1.jpg", "caption": "Table 5: Ablation study of different vector selection methods. S.: Softmax, GS: Gumbel-Softmax, ST-GS: Straight Through Gumbel-Softmax.", "description": "This ablation study compares the performance of different vector selection methods used in VB-LoRA on the CoLA dataset.  It evaluates several approaches, including using all vectors, selecting the top k vectors using Softmax (Top-k), a noisy version of the top k selection, Gumbel-Softmax (GS), and Straight-Through Gumbel-Softmax (ST-GS). The results highlight the importance of a careful choice of vector selection method in achieving good performance.", "section": "4.5 Ablation Study"}, {"figure_path": "kuCY0mW4Q3/tables/tables_9_2.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqy are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents the results of experiments on the GLUE benchmark using different parameter-efficient fine-tuning (PEFT) methods: full fine-tuning (FT), LoRA, VeRA, Tied-LoRA, and the proposed VB-LoRA.  The benchmark includes six tasks: CoLA, SST-2, MRPC, STS-B, QNLI, and RTE.  Results are shown for both RoBERTa-base and RoBERTa-large models, with metrics varying across tasks (Matthew's correlation, Pearson correlation, or accuracy). The number of parameters used by each method is also given, along with the median performance over five runs with different random seeds.  The table highlights VB-LoRA's competitive performance with significantly fewer parameters compared to existing methods.", "section": "4.1 Natural Language Understanding"}, {"figure_path": "kuCY0mW4Q3/tables/tables_14_1.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqv are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents the results of experiments comparing different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark using RoBERTa-base and RoBERTa-large models.  The methods compared are full fine-tuning (FT), LoRA, VERA, Tied-LoRA, and the proposed VB-LORA.  The table shows the performance of each method on six different GLUE tasks (SST-2, MRPC, CoLA, QNLI, RTE, and STS-B), measured using metrics appropriate to each task (accuracy, Matthews correlation, Pearson correlation).  The number of parameters used by each method is also provided. The results are averages over 5 runs, with the best results highlighted in bold.", "section": "4.1 Natural Language Understanding"}, {"figure_path": "kuCY0mW4Q3/tables/tables_15_1.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqv are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents a comparison of the performance of several parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark using two different sizes of RoBERTa models.  It shows the median performance (across 5 runs with different random seeds) for each method on six tasks, including the number of trainable parameters used by each method.  The results for LoRAqv and VeRAqv are taken from the original papers, while the rest were obtained via the authors' implementations. The best results for each model size are highlighted in bold.", "section": "4.1 Natural Language Understanding"}, {"figure_path": "kuCY0mW4Q3/tables/tables_15_2.jpg", "caption": "Table 7: Hyperparameters and computing resources for natural language understanding experiments on the GLUE benchmark. Training time and GPU memory are reported as \"query and value only\" / \"all linear modules\". h: hour, m: minute.", "description": "This table presents the hyperparameters and computational resources used in the natural language understanding experiments conducted on the GLUE benchmark.  It details settings for both RoBERTa base and large models, comparing two fine-tuning strategies:  fine-tuning only the query and value modules (VB-LoRAqv), and fine-tuning all linear modules (VB-LoRAall). The table includes optimizer, warmup ratio, learning rate schedule, vector bank initialization, and logit parameters' initialization methods.  It also shows the number of GPUs used, the number of epochs, batch sizes, maximum sequence length, training time, and GPU memory consumption for each configuration. Training time and GPU memory usage are reported separately for the two fine-tuning strategies, providing a comprehensive overview of the resources required for each experimental setup.", "section": "4 Experiments"}, {"figure_path": "kuCY0mW4Q3/tables/tables_20_1.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqy are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents the results of the experiments conducted on the GLUE benchmark using RoBERTa Base and Large models.  It compares the performance of VB-LoRA against other parameter-efficient fine-tuning (PEFT) methods (LoRA, VeRA, and Tied-LoRA) across six GLUE tasks: CoLA, SST-2, MRPC, STS-B, QNLI, and RTE. The table shows the number of parameters used by each method, and their performance (Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for the rest).  Results for LoRAqv and VeRAqy are taken from the original papers.  VB-LoRA results are the median of 5 runs with different random seeds.", "section": "4.1 Natural Language Understanding"}, {"figure_path": "kuCY0mW4Q3/tables/tables_21_1.jpg", "caption": "Table 1: Results with RoBERTabase and ROBERTalarge on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for LoRAqv and VeRAqy are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds.", "description": "This table presents the results of experiments conducted on the GLUE benchmark using different parameter-efficient fine-tuning (PEFT) methods: full fine-tuning (FT), LoRA, VeRA, Tied-LoRA, and VB-LoRA.  The experiments were performed on two different sized RoBERTa models: RoBERTa-base and RoBERTa-large.  The table shows the performance of each method on six GLUE tasks: SST-2, MRPC, CoLA, QNLI, RTE, and STS-B.  Performance is measured using accuracy (for most tasks), Matthew's correlation (for CoLA), and Pearson correlation (for STS-B).  The table also reports the number of parameters used by each method.  The best results for each model size are shown in bold, and the results are averages of 5 runs with different random seeds.", "section": "4.1 Natural Language Understanding"}]