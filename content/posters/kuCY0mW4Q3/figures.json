[{"figure_path": "kuCY0mW4Q3/figures/figures_0_1.jpg", "caption": "Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LORA achieves higher scores with significantly smaller number of stored parameters.", "description": "This figure compares the performance and parameter efficiency of several parameter-efficient fine-tuning (PEFT) methods on the RoBERTa-Large model.  The x-axis represents the number of stored parameters used by each method, while the y-axis represents Matthew's correlation, a metric used to evaluate the performance on a specific task (not specified in the caption).  The figure shows that VB-LORA (the proposed method) significantly outperforms other methods such as LoRA, Tied-LoRA, and VeRA, while using considerably fewer parameters.  This demonstrates the superior parameter efficiency of VB-LORA.", "section": "1 Introduction"}, {"figure_path": "kuCY0mW4Q3/figures/figures_1_1.jpg", "caption": "Figure 2: Left: The model parameters can be represented as a composition of vectors from a vector bank, which is shared across sub-vectors, modules and layers. Right: Architecture of VB-LoRA. We use a top-k softmax function to select k vectors from the vector bank. The selected vectors are then pooled into a sub-vector, which is arranged at a desired position, forming the parameters of LoRA.", "description": "The figure illustrates the concept of VB-LoRA. The left side shows how model parameters are composed of vectors from a shared vector bank across different layers, modules, and sub-vectors.  The right side details the VB-LoRA architecture. It shows how a top-k softmax module selects k vectors from the vector bank, pools them into sub-vectors, and then uses these sub-vectors to form the LoRA parameters. This approach enables extreme parameter efficiency by sharing parameters globally.", "section": "3 Proposed Method"}, {"figure_path": "kuCY0mW4Q3/figures/figures_9_1.jpg", "caption": "Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LoRA achieves higher scores with significantly smaller number of stored parameters.", "description": "This figure compares the performance and the number of stored parameters of several parameter-efficient fine-tuning (PEFT) methods on the RoBERTa-Large model.  The x-axis represents the number of stored parameters (log scale), and the y-axis represents the Matthew's correlation coefficient for the model's performance.  The figure shows that VB-LoRA achieves a higher score (better performance) while using a significantly smaller number of stored parameters compared to other methods such as LoRA, Tied-LoRA, and VERA. This demonstrates the extreme parameter efficiency of VB-LoRA.", "section": "1 Introduction"}, {"figure_path": "kuCY0mW4Q3/figures/figures_16_1.jpg", "caption": "Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LoRA achieves higher scores with significantly smaller number of stored parameters.", "description": "This figure compares the performance and the number of stored parameters of several parameter-efficient fine-tuning (PEFT) methods on the RoBERTa-Large model.  The x-axis represents the number of stored parameters (log scale), and the y-axis represents the Matthew's correlation, a performance metric.  The figure shows that VB-LoRA outperforms other methods (LoRA, Tied-LoRA, VERA) while using significantly fewer parameters, demonstrating its extreme parameter efficiency.", "section": "1 Introduction"}, {"figure_path": "kuCY0mW4Q3/figures/figures_16_2.jpg", "caption": "Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LoRA achieves higher scores with significantly smaller number of stored parameters.", "description": "This figure compares the performance and the number of stored parameters of various parameter-efficient fine-tuning (PEFT) methods on the RoBERTa-Large model.  The x-axis represents the number of stored parameters (log scale), and the y-axis shows the Matthew's correlation score.  The figure demonstrates that VB-LoRA achieves a higher score (better performance) with significantly fewer parameters compared to other methods like LoRA, Tied-LoRA, and VERA.", "section": "Introduction"}, {"figure_path": "kuCY0mW4Q3/figures/figures_17_1.jpg", "caption": "Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LoRA achieves higher scores with significantly smaller number of stored parameters.", "description": "This figure compares the performance and the number of stored parameters of various parameter-efficient fine-tuning (PEFT) methods on the RoBERTa-Large model.  The x-axis represents the number of stored parameters (on a logarithmic scale), and the y-axis represents Matthew's correlation, a metric assessing the model's performance.  VB-LoRA outperforms other methods such as LoRA, Tied-LoRA, and VERA while using significantly fewer parameters.", "section": "Introduction"}, {"figure_path": "kuCY0mW4Q3/figures/figures_17_2.jpg", "caption": "Figure 3: VB-LORA's vector selection footprints during training. The x-axis represents the 96 sub-vectors formed by the vectors from a bank of 90 vectors, while the y-axis represents the indices of selected vectors from the bank. The blue blocks indicate the selection footprint during training.", "description": "This figure visualizes the vectors selected by VB-LORA during training. The x-axis represents the 96 sub-vectors, and the y-axis shows the indices of the selected vectors from the vector bank. The blue blocks represent the selection footprint at different epochs. Each sub-vector selects a subset of vectors from the bank. The visualization helps understand how the vector selection dynamics change during training, showing the interplay between sub-vectors and the vector bank.", "section": "4.5 Ablation Study"}, {"figure_path": "kuCY0mW4Q3/figures/figures_18_1.jpg", "caption": "Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LoRA achieves higher scores with significantly smaller number of stored parameters.", "description": "This figure compares the performance and the number of stored parameters of several parameter-efficient fine-tuning (PEFT) methods on the RoBERTa-Large model.  The x-axis represents the number of stored parameters (log scale), and the y-axis represents Matthew's correlation, a metric used to evaluate the performance of the model. The graph shows that VB-LORA outperforms other methods such as LoRA, Tied-LoRA, and VERA while using significantly fewer parameters, demonstrating its extreme parameter efficiency.", "section": "1 Introduction"}, {"figure_path": "kuCY0mW4Q3/figures/figures_19_1.jpg", "caption": "Figure 2: Left: The model parameters can be represented as a composition of vectors from a vector bank, which is shared across sub-vectors, modules and layers. Right: Architecture of VB-LoRA. We use a top-k softmax function to select k vectors from the vector bank. The selected vectors are then pooled into a sub-vector, which is arranged at a desired position, forming the parameters of LoRA.", "description": "The figure illustrates the VB-LoRA architecture. The left panel shows how model parameters are composed of vectors from a shared vector bank across different layers, modules, and sub-vectors.  The right panel details VB-LoRA's architecture, showing how a top-k softmax function selects k vectors from the vector bank and combines them into a sub-vector that is then used to form the parameters of LoRA. This illustrates the \"divide-and-share\" paradigm of VB-LoRA, where parameters are shared globally via the vector bank to improve parameter efficiency.", "section": "3 Proposed Method"}]