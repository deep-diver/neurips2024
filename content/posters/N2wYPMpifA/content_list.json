[{"type": "text", "text": "Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alex Havrilla ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenjing Liao ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Mathematics Georgia Institute of Technology Atlanta, GA, 30308 ahavrilla3@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics Georgia Institute of Technology Atlanta, GA, 30308 wliao60@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When training deep neural networks, a model\u2019s generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformerbased large language models (LLMs), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension $d$ of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in $d$ By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning has made remarkable breakthroughs in various real-world applications, such as natural language processing [Graves et al., 2013, Bahdanau et al., 2014, Liu et al., 2023, Vaswani et al., 2017], computer vision [Krizhevsky et al., 2012, Goodfellow et al., 2014, Song et al., 2020], healthcare [Miotto et al., 2018], and robotics [Gu et al., 2017]. A neural scaling law between the generalization error (or test loss) and several quantities, including the model size, the training data size, and the amount of compute, plays a key role in the performance of neural networks. Perhaps the best known example of such scaling laws are for transformer-based LLMs. Recent works in Hestness et al. [2017], Rosenfeld et al. [2019], Kaplan et al. [2020], Bahri et al. [2021] demonstrated a power law between the test loss and the network size, the training data size, and the amount of compute for transformer-based LLMs. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. ", "page_idx": 0}, {"type": "text", "text": "Understanding the theory behind neural scaling laws provides invaluable insights into practical applications of deep learning. A mathematical principal of neural scaling laws enables researchers and practitioners to describe and analyze the performance of neural networks with precision and rigor. The neural scaling law between the generalization error and the network size can be partially explained via neural network representation theory [Yarotsky, 2016]. Further, the neural scaling law between the generalization error and the training data size $n$ can be explained via statistical estimation theory. For feedforward neural networks [Schmidt-Hieber, 2020] and convolutional residual networks [Oono and Suzuki, 2019], a generalization error bound has been established for regression. SchmidtHieber [2020], Oono and Suzuki [2019] predicted Generalization Error $\\sim n^{-c/D}$ where $n$ is the training data size, $D$ is the data dimension and $c$ is a constant. This predicted rate of convergence is extremely slow for high dimensional data when $D$ is large, while the rate of convergence observed in real-world applications is significantly faster, which reveals a gap between theory and practice. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This gap can be bridged by exploiting low-dimensional structures of data. Real-world data sets often exhibit low-dimensional geometric structures due to rich local regularities, global symmetries, or repetitive patterns [Tenenbaum et al., 2000, Roweis and Saul, 2000]. According to Min et al. [2023, Figure 1], the intrinsic dimension of CIFAR-100, CelebA and ImageNet datasets are about 20, 20 and 40 respectively. When the low-dimensional geometric structure of data is modeled by a manifold, the predicted scaling for regression, classification and distribution estimation becomes Generalization Error $\\sim n^{-c/d}$ , where $n$ is the training data size, $d$ is the intrinsic dimension of the data manifold, and $c$ is a constant [Chen et al., 2022, Liu et al., 2021, Dahal et al., 2022, Nakada and Imaizumi, 2020]. In Sharma and Kaplan [2022], the neural scaling law between the test loss and the network size was predicted to be Test loss $\\sim(s i z e)^{-4/d}$ where $d$ is the intrinsic dimension of data. While the theoretical studies focus on feedforward neural networks [Chen et al., 2022, Nakada and Imaizumi, 2020] and convolutional residual networks [Liu et al., 2021], a generalization to transformer-based neural networks [Vaswani et al., 2017] is of great interest but widely open. ", "page_idx": 1}, {"type": "text", "text": "This paper establishes mathematical approximation and statistical estimation theories to predict and justify the scaling law between the generalization error and the model/data size for transformer neural networks. We consider regression of a $\\beta$ -H\u00f6lder continuous function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ where $\\mathcal{M}$ is a $d$ -dimensional compact Riemannian manifold isometrically embedded in $\\mathbb{R}^{D}$ . After embedding the input $\\boldsymbol{x}\\in\\mathcal{M}\\subset\\dot{\\mathbb{R}}^{D}$ to a proper sequence, we apply a transformer network on the embedded sequence to learn the function $f$ . Our main results are on the statistical estimation and universal approximation theories of H\u00f6lder continuous functions on $\\mathcal{M}$ by transformer neural networks. ", "page_idx": 1}, {"type": "text", "text": "Statistical Theory: In Theorem 1, we consider the global empirical risk minimizer ${\\hat{\\mathrm{T}}}_{n}$ from $n$ i.i.d. training data $\\{(x_{i},f(x_{i}))\\}_{i=1}^{n}$ , given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathrm{T}}_{n}=\\arg\\operatorname*{min}_{\\mathrm{T}\\in\\mathcal{T}}\\frac{1}{n}\\sum_{i=1}^{n}\\big(\\mathrm{T}(x_{i})-f(x_{i})\\big)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "under a properly chosen transformer network architecture $\\tau$ . We prove that, the generalization error of ${\\hat{\\mathbf{T}}}_{n}$ satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\hat{\\mathbf{T}}_{n}-f\\|_{L^{2}(Q)}^{2}\\leq\\tilde{O}\\big(D d^{2}n^{-\\frac{2\\beta}{2\\beta+d}}\\big)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $Q$ denotes the distribution of $x$ , and $\\tilde{O}$ hides constants and $\\log n$ terms. ", "page_idx": 1}, {"type": "text", "text": "Approximation Theory: In Theorem 2, we construct a transformer network to universally approximate $\\beta$ -H\u00f6lder continuous functions on $\\mathcal{M}$ with an arbitrarily given accuracy $\\varepsilon$ . Notably, the network is shallow, requiring only $O\\big(\\log(d)\\big)$ independent of the desired accuracy $\\epsilon$ to approximate $f$ locally. This highlights a major advantage of Transformers over feed-forward ReLU networks, which require $O\\big(\\log\\!\\big(\\frac{1}{\\epsilon}\\big)\\big)$ layers to achieve the same accuracy. ", "page_idx": 1}, {"type": "text", "text": "In our proof, we embed the entries of $x=[x^{1},\\cdot\\cdot\\cdot,x^{D}]\\in\\mathcal{M}$ into tokens such that the $x^{i}$ \u2019s appear in a sequence. Our proof for the approximation theory explicitly constructs transformers to realize the interaction between different tokens efficiently via a crucial Interaction Lemma 3. This lemma allows us to flexibly implement many common operations including addition, multiplication, and parallelization, and so may of independent interest. In our proof for the statistical theory, we calculate the covering number of our transformer network class, which is also of independent interest. ", "page_idx": 1}, {"type": "text", "text": "Neural Scaling Laws and the Intrinsic Dimension: Our generalization error bound in (2) predicts the following neural scaling law between the generalization error and the data size $n$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{red\\;Generalization\\;Error:}=\\mathbb{E}\\|\\hat{\\boldsymbol{\\mathrm{T}}}_{n}-\\boldsymbol{f}\\|_{L^{2}(Q)}^{2}\\lesssim n^{-\\alpha_{D}},\\;\\;\\mathrm{where\\;}\\alpha_{D}=\\frac{2\\beta}{2\\beta+d},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with sufficient data. Our approximation theory in Theorem 2 predicts the following neural scaling law between the approximation error and the network size $N$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\exists\\mathrm{rror}:=\\operatorname*{inf}_{\\mathrm{T}\\in\\mathcal{T}}\\|\\mathrm{T}-f\\|_{L^{\\infty}(\\mathcal{M})}^{2}\\lesssim N^{-\\alpha_{N}},\\ \\ \\mathrm{where}\\ \\alpha_{N}=\\frac{2\\beta}{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a sufficiently large network class $\\tau$ . Our prediction of the power scaling law is consistent with our own empirical observations, and those in Kaplan et al. [2020] and Biderman et al. [2023]. More importantly, our theory quantifies the power $\\alpha_{D},\\alpha_{N}$ in terms of the intrinsic dimension of data. ", "page_idx": 2}, {"type": "text", "text": "Experimental Validation on LLMs: After establishing our theory we seek to validate it in practice by predicting empirical scaling laws for LLMs trained on natural language data. To test our predictions for the data scaling law, we pretrain a series of small (125 million parameter) LLMs on three datasets [Gokaslan et al., 2019, Eldan and Li, 2023, Kocetkov et al., 2022]. We find close agreement $(\\pm0.02)$ between our predicted scaling exponent $\\alpha_{D}$ and the observed exponents $\\hat{\\alpha}_{D}$ . To evaluate our predictions for the model scaling exponent $\\alpha_{N}$ , we rely on publicly available scaling suites [Biderman et al., 2023, Radford et al., 2019] whose intrinsic data dimensions we can estimate. We find our predictions are still close but less accurate for $\\alpha_{N}$ . Finally, we carry out a series of ablations investigating factors impacting the estimated intrinsic data dimension $d$ . For a fixed dataset, we find the estimated $d$ is stable with respect to several factors including the model size, model embedding dimension, and context length1. ", "page_idx": 2}, {"type": "text", "text": "In summary, we make the following contributions: ", "page_idx": 2}, {"type": "text", "text": "\u2022 A novel approximation theory for transformers approximating H\u00f6lder continuous functions on a $d$ -dimensional manifold, requiring $O(\\log(\\bar{d})\\bar{)}$ depth independent of the accuracy $\\epsilon$ . \u2022 A novel computation of the covering number of our transformer network class. This is used to establish generalization bounds exponentially depending on the intrinsic dimension $d$ . \u2022 Empirical experiments demonstrating our theory predicts data scaling laws for LLMs as a function of the estimated intrinsic data dimension $d$ . \u2022 An empirical study of several factors affecting the estimated intrinsic data dimension for transformers including model size, embedding dimension, layer depth, and context length. ", "page_idx": 2}, {"type": "text", "text": "We will present our main theory in Section 2, numerical validation of our theory and the prediction of neural scaling laws in Section 3. We will discuss related work in Section 4 and conclude our paper in Section 5. Our pre-training hyperparameters are given in Appendix A. The derivation of neural scaling laws is presented in Appendix B. Our notation is given in Appendix C, and proofs are presented in Appendix E and F. ", "page_idx": 2}, {"type": "text", "text": "2 Transformer Generalization and Approximation Theory ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This paper establishes statistical estimation and mathematical approximation theory of transformers for the regression of H\u00f6lder functions on a low-dimensional manifold. We start by defining transformer neural networks. ", "page_idx": 2}, {"type": "text", "text": "2.1 Transformer Neural Networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 1 (Transformer Neural Network). We define a transformer neural network T as a composition of functions of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathrm{T}}(\\boldsymbol{x})=\\boldsymbol{\\mathrm{D}}\\circ\\boldsymbol{\\mathrm{B}}_{L_{T}}\\circ\\dots\\circ\\boldsymbol{\\mathrm{B}}_{1}\\circ\\left(\\boldsymbol{\\mathrm{PE}}+\\boldsymbol{\\mathrm{E}}(\\boldsymbol{x})\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is parameterized by ", "page_idx": 2}, {"type": "text", "text": "\u2022 $L_{T}$ : The number of transformer blocks $\\mathbf{B}_{i}$ in T.   \n\u2022 m: The maximum number of attention heads per transformer block.   \n\u2022 $L_{\\mathrm{FFN}}$ : The max depth of the feed-forward layers per block.   \n\u2022 $w_{\\mathrm{FFN}}$ : The max width of the feed-forward layers per block. \u2022 $d_{e m b d}$ : The token embedding dimension.   \n\u2022 $D$ : The input dimension.   \n\u2022 l: The number of hidden tokens.   \n\u2022 \u03ba: A bound on the magnitude of network parameters. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We define each component of the composition as ", "page_idx": 3}, {"type": "text", "text": "\u2022 $x\\in\\mathbb{R}^{D}$ is the input.   \n\u2022 A linear embedding layer $\\mathrm{E}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{d_{e m b d}\\times l}$ . In this work we will always take $\\mathrm{E}=E^{\\prime}\\circ U$ where $U\\,\\in\\,\\mathbb{R}^{l\\times D}$ and $E^{\\prime}\\,\\in\\,\\mathbb{R}^{d_{e m b d}\\times1}$ applied columnwise is fixed. We call embedded output $H=\\operatorname{E}(x)$ the first embedding matrix whose columns are referred to as tokens.   \n\u2022 $\\mathrm{PE}\\in\\mathbb{R}^{d_{e m b d}\\times l}$ is a fixed matrix implementing the transformer positional encoding.   \n\u2022 Transformer blocks $\\mathbf{B}_{i}:\\mathbb{R}^{d\\times l}\\rightarrow\\mathbb{R}^{d\\times l}$ for $i\\in\\{1,...,L_{T}\\}$ which are residual compositions of multi-headed attention (MHA) layers MHA and feed-forward layers FFN acting tokenwise.   \n\u2022 A decoding layer $\\mathrm{D}:\\mathbb{R}^{d_{e m b d}\\times l}\\rightarrow\\mathbb{R}$ which is fixed to outputting the first element of the last column. ", "page_idx": 3}, {"type": "text", "text": "We use the ReLU activation function $\\sigma(x)=\\operatorname*{max}(0,x)$ in the network. ", "page_idx": 3}, {"type": "text", "text": "For a complete definition of the components of the transformer neural networks, we refer to Appendix D. The transformer network T may sometimes be written $\\mathrm{T}_{\\theta}$ which makes explicit the dependence on learnable weights $\\theta$ . We can also define a class of transformer neural networks $\\tau$ of interest. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Transformer Network Class). We define a class of transformer networks as ", "page_idx": 3}, {"type": "text", "text": "$T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)=\\left\\{\\mathrm{T}_{\\theta}\\ |\\ \\mathrm{T}_{\\theta}\\right.$ is in (5) with at most $m$ attention heads in each block, $L_{\\mathrm{FFN}}$ layer feed-forward networks with hidden width $w_{\\mathrm{FFN}}$ , $d_{e m b d}$ token dimension, $l$ hidden tokens , and have $\\|\\mathrm{T}_{\\theta}\\|_{L^{\\infty}(\\mathbb{R}^{D})}\\le R,\\|\\theta\\|_{\\infty}\\le\\kappa\\right\\}$ ", "page_idx": 3}, {"type": "text", "text": "where $\\|\\mathrm{T}_{\\theta}\\|_{L^{\\infty}(\\mathbb{R}^{D})}\\leq R$ bounds the output of T and $\\|\\theta\\|_{\\infty}$ bounds the weight magnitude of $\\mathrm{T}_{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a manifold $\\mathcal{M}$ and the target function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ , with the following assumptions: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Manifold). Let $\\mathcal{M}$ be a compact Riemannian manifold with intrinsic dimension $d$ isometrically embedded in $\\mathbb{R}^{D}$ . Because $\\mathcal{M}$ is compact, there exists $M>0$ such that $\\|x\\|_{\\infty}\\leq M$ for $x\\in\\mathcal{M}$ . Additionally, we assume $\\mathcal{M}$ has positive reach $\\tau>0$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Target function). The target function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ is $\\beta$ -H\u00f6lder continuous on $\\mathcal{M}$ , for some $0<\\beta\\leq1$ and H\u00f6lder constant $H_{f}>0$ , and in addition $\\|f\\|_{L^{\\infty}(\\mathcal{M})}\\leq R$ for some $R>0$ . ", "page_idx": 3}, {"type": "text", "text": "In Assumption 1, the reach [Federer, 1959, Aamari et al., 2019] $\\tau$ of $\\mathcal{M}$ can be defined as ", "page_idx": 3}, {"type": "text", "text": "Informally, reach is the smallest distance at which a projection onto the manifold is no longer unique.   \nIn practice this can be used to establish a bound on the number of charts covering the manifold. ", "page_idx": 3}, {"type": "text", "text": "2.3 Transformer Generalization Theory ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given $n$ training samples $\\{(x_{i},f(x_{i}))\\}_{i=1}^{n}$ where $\\{x_{i}\\}_{i=1}^{n}$ are i.i.d. samples of a distribution $Q$ supported on $\\mathcal{M}$ , we aim to learn an approximation ${\\hat{\\mathrm{T}}}_{n}$ to $f$ by minimizing the empirical risk (1) over ", "page_idx": 3}, {"type": "text", "text": "a class of a transformer neural networks $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ . The corresponding generalization error is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|\\hat{\\mathrm{T}}_{n}-f\\|_{L^{2}(Q)}=\\mathbb{E}\\sqrt{\\int_{\\mathcal{M}}\\left(\\hat{\\mathrm{T}}_{n}(x)-f(x)\\right)^{2}}d Q(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If $\\mathcal{M}$ and $f$ satisfy Assumptions 1 and 2, we prove the following generalization error bound. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $M,\\tau,R,H_{f}\\;>\\;0,\\;0\\;<\\;\\;\\beta\\;\\;\\leq\\;\\;1,\\;\\;d,D\\;\\in\\;\\;\\mathbb{N},\\;\\mathcal{M}$ and $f$ satisfy Assumption $^{\\,l}$ and 2 respectively. Given n training samples $\\{(x_{i},f(x_{i}))\\}_{i=1}^{n}$ where $\\{x_{i}\\}_{i=1}^{n}$ are i.i.d. samples of a distribution $Q$ supported on $\\mathcal{M}$ , if we use the transformer neural network class $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ with parameters ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{L}_{T}=O\\big(\\log(d)\\big),\\quad\\boldsymbol{L}_{\\mathrm{FFN}}=O\\big(\\log(n)\\big),\\quad\\boldsymbol{w}_{\\mathrm{FFN}}=O\\big(1\\big),\\quad\\boldsymbol{l}=O\\big(d n^{\\frac{d}{2\\beta+d}}\\big)}\\\\ &{d_{e m b d}=O\\big(1\\big),\\quad\\boldsymbol{m}=O\\big(d n^{\\frac{d}{2\\beta+d}}\\big),\\quad\\kappa=O\\big(d^{2}n^{\\frac{2d}{2\\beta+d}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "in the empirical risk minimization (1), where $O(\\,\\cdot\\,)$ hides terms in $C_{\\mathcal{M}}$ (the number of charts), $D,H_{f},M$ , then the empirical risk minimizer ${\\hat{\\mathrm{T}}}_{n}$ given by (1) satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\int_{\\mathcal{M}}\\big(\\hat{\\mathrm{T}}_{n}(x)-f(x)\\big)^{2}d Q\\leq\\tilde{O}\\big(D d^{2}n^{\\frac{-2\\beta}{2\\beta+d}}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{O}$ hides logarithmic terms in $n,d$ and linear terms in $C_{\\mathcal{M}}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 is proved in Appendix $\\boldsymbol{\\mathrm F}$ , via a bias-variance decomposition. The bias represents the approximation error of $f$ by transformer neural networks, and the variance represents the stochastic error in the parameter estimation of transformer neural networks. To quantify the bias, we explicitly construct a transformer neural network to universally approximate $\\beta.$ -H\u00f6lder continuous functions on $\\mathcal{M}$ , to be detailed in Section 2.4. The variance is bounded by a novel calculation of the covering number of the transformer network class used in Theorem 1. ", "page_idx": 4}, {"type": "text", "text": "2.4 Transformer Approximation Theory ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $M,\\tau,R,H_{f}>0,\\,0<\\beta\\leq1,\\,d,D\\in\\mathbb{N}$ and $\\mathcal{M}$ satisfy Assumption $^{\\,l}$ . For any $\\epsilon\\in(0,1).$ , there exists a transformer neural network class $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ with parameters ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{L}_{T}=O\\big(\\log(d)\\big),\\quad\\boldsymbol{L}_{\\mathrm{FFN}}=O\\big(\\log(\\epsilon^{-1})\\big),\\quad\\boldsymbol{w}_{\\mathrm{FFN}}=O\\big(1\\big),\\quad\\boldsymbol{l}=O\\big(d\\epsilon^{-\\frac{d}{\\beta}}\\big)}\\\\ &{d_{e m b d}=O\\big(1\\big),\\quad\\boldsymbol{m}=O\\big(d\\epsilon^{-\\frac{d}{\\beta}}\\big),\\quad\\kappa=O\\big(d^{2}\\epsilon^{-\\frac{2d}{\\beta}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $O(\\,\\cdot\\,)$ hides terms in $C_{\\mathcal{M}},D,H_{f},\\tau_{;}$ , such that for any target function $f$ satisfying Assumption 2, if the network parameters $\\theta$ are properly chosen, then the network yields a function $\\mathbf{T}_{\\theta}\\in\\mathcal{T}(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ with the approximation error ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\mathrm{T}_{\\theta}-f\\|_{L^{\\infty}(\\mathcal{M})}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 2 is proved in Appendix E.2. In our proof, we decompose $f(x)$ as a sum of terms over local neighborhoods $U_{1},...,U_{C_{\\mathcal{M}}}\\subseteq\\mathcal{M}$ covering $\\mathcal{M}$ . Approximations on overlapping neighborhoods containing $x$ will then be combined via a partition of unity $(\\bf P_{o U})$ $\\{\\rho_{n}\\}_{n=1}^{C_{\\mathcal{M}}}$ which subordinates $\\{U_{n}\\}_{n=1}^{C_{M}}$ . This will give us the expression $\\begin{array}{r}{f(x)=\\sum_{n=1}^{C_{\\mathcal{M}}}f_{n}(x)\\mathbf{1}_{U_{n}}(x)}\\end{array}$ with $f_{n}=f\\rho_{n}:\\mathcal{M}\\to\\mathbb{R}$ On each local neighborhood $U_{n}$ , we project the input $\\boldsymbol{x}\\in\\mathcal{M}\\subseteq\\mathbb{R}^{D}$ to the tangent coordinate in $[0,1]^{d}$ . This will give us the following local decomposition of the target function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(x)=\\sum_{n=1}^{C_{M}}\\tilde{f}_{n}\\circ\\phi_{n}(x)\\mathbf{1}_{U_{n}}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widetilde{f}_{n}=f_{n}\\circ\\phi_{n}^{-1}:[0,1]^{d}\\to\\mathbb{R}$ and $\\phi_{n}:\\mathcal{M}\\to[0,1]^{d}$ is a projection onto the local tangent space. We then construct transformers to approximate the $\\tilde{f}_{n},\\phi_{n},\\mathbf{1}_{U_{n}}$ components in (7). A diagram of the constructed transformer network approximating $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ is given in Figure 1. The following key lemma is used to efficiently approximate each low-dimensional function $\\tilde{f}_{n}$ on $d$ -dimensional coordinates. ", "page_idx": 4}, {"type": "image", "img_path": "N2wYPMpifA/tmp/d2e4b5214c27fd0391c70b3f256f9df9fda495c9f96d79471bf2debdfd34b868.jpg", "img_caption": ["Figure 1: Diagram of the transformer architecture constructed in Theorem 2. T computes approximations of $\\bar{f(x)}$ on each local chart $U_{n}\\subseteq{\\mathcal{M}}$ by first projecting $x$ to the tangent coordinates in $\\mathbb{R}^{d}$ via $\\phi_{n}(x)$ and then approximating $f(x)$ with local Taylor polynomials. A shallow sub-network computes indicators $\\mathbf{1}_{U_{n}}$ for each local chart in parallel. The results of the two sub-networks are then multiplied together and summed to produce the final result. Here $H_{i}$ denotes the embedding matrix before the $i$ th transformer block ${\\bf B}_{i}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $H_{f},R>0,$ , $d\\in\\mathbb{N}$ and $0<\\beta\\leq1$ . For any $\\epsilon\\in(0,1)$ , there exists a transformer neural network class $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ with parameters ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{T}=O\\big(\\log(d)\\big),\\quad L_{\\mathtt{F F N}}=O\\big(1\\big),\\quad w_{\\mathtt{F F N}}=O\\big(1\\big),\\quad l=O\\big(d\\epsilon^{-\\frac{d}{\\beta}}\\big)}\\\\ &{d_{e m b d}=O\\big(1\\big),\\quad m=O\\big(d\\epsilon^{-\\frac{d}{\\beta}}\\big),\\quad\\kappa=O\\big(d^{2}\\epsilon^{-\\frac{2d}{\\beta}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $O\\left(\\cdot\\right)$ hides terms in $H_{f}$ , such that, for any $\\beta$ -H\u00f6lder continuous function $f:[0,1]^{d}\\rightarrow\\mathbb{R}.$ , with H\u00f6lder constant no more than $H_{f}$ and $\\|f\\|_{L^{\\infty}([0,1]^{d})}\\leq R,$ , if the network parameters $\\theta$ are properly chosen, this transformer network yields a function $\\dot{\\mathrm{T}}_{\\theta}\\in\\mathcal{T}(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\mathrm{T}_{\\theta}-f\\|_{L^{\\infty}([0,1]^{d})}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 1 is proved in Appendix E.1. We develop a novel lemma - Interaction Lemma 3, implementing a highly-sparse pairwise interaction between two arbitrary tokens $h_{t_{1}},h_{t_{2}}$ , as a crucial architectural building block allowing us to easily implement more complex functions, architecture serialization, and parallelization (7). This result highlights a distinct advantage of transformer function approximation over ReLU function approximation [Yarotsky, 2016]: A transformer network only needs a constant $O\\big(\\log(d)\\big)$ number of layers to approximate $f:[0,1]^{d}\\rightarrow\\mathbb{R}$ independent of the desired accuracy \u03f5. In contrast, the depth of ReLU feed-forward networks is in the order of $\\log(\\epsilon^{-1})$ [Yarotsky, 2016] This is desirable from an empirical point of view, where wider networks instead of deeper ones tend to achieve superior performance [Kaplan et al., 2020, Lee et al., 2020]. ", "page_idx": 5}, {"type": "text", "text": "3 Predicting Empirical Scaling Laws and Validation on LLMs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our theory provides practical insights by predicting neural scaling laws for transformers, as given in (3) and (4), by explicitly quantifying the data scaling exponent $\\alpha_{D}$ and the model scaling exponent $\\alpha_{N}$ as a function of the intrinsic dimension (ID) $d$ . If we assume the language modeling objective has Lipschitz regularity such that $\\beta=1$ in Assumption 2, then Theorem 1 predicts the scaling law between the squared generalization error and the data size $n$ , as given in (3), with $\\begin{array}{r}{\\alpha_{D}=\\frac{2}{2+d}}\\end{array}$ , and the model scaling law with exponent given by $\\begin{array}{r}{\\alpha_{N}=\\frac{2}{d}}\\end{array}$ . For a full derivation refer to Section B. We will observe how well our theory predicts these exponents both by pretraining small models from scratch and evaluating existing open-source model suites [Biderman et al., 2023]. ", "page_idx": 5}, {"type": "text", "text": "In the following, we denote $\\alpha_{D}$ and $\\alpha_{N}$ as the scaling exponents predicted by our theory, where we numerically estimate the intrinsic dimension of data, denoted by $d_{D}$ . The empirical exponents are denoted by $\\hat{\\alpha}_{D}$ and $\\hat{\\alpha}_{N}$ . To obtain the data scaling exponent $\\hat{\\alpha}_{D}$ , we plot the test loss (comparable to squared error) versus the data size $n$ in log-log scale, fti the log-log curve with a line, and then obtain $\\hat{\\alpha}_{D}$ from the magnitude of the slope. The model scaling exponent $\\hat{\\alpha}_{N}$ is obtained similarly. ", "page_idx": 5}, {"type": "image", "img_path": "N2wYPMpifA/tmp/59ce2d938eb44536a9d7a8aea0f9f17f5e6019610eae4e98c1642339d1bcedf0.jpg", "img_caption": ["Figure 2: Observed and predicted data scaling laws on OpenWebText, The Stack-SQL, and Tiny Stories pretraining datasets. All estimates are close $(\\pm0.02)$ and appear to reflect varying levels of pretraining data complexity. Note: $\\hat{\\alpha}_{D}$ denotes the empirically observed data scaling exponent and $\\alpha_{D}$ denotes the theoretically estimated exponent. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Estimating the ID of Text To predict scaling exponents, we must first estimate the intrinsic dimension of our pretraining dataset $\\mathcal{D}$ . While we can do this directly for image datasets [Russakovsky et al., 2014, Pope et al., 2021], we cannot do this directly for textual datasets. Instead, we will estimate the intrinsic dimension of the input data by estimating the intrinsic dimension of token embeddings. Specifically, we will represent each input token with its corresponding final-layer token embedding. Given a pretraining test set $\\mathcal{D}_{\\mathrm{test}}$ we embed a random $l\\;=\\;1024$ length subsequence from each document $D_{k}\\,\\in\\,\\mathcal{D}_{\\mathrm{test}}$ . We then randomly sub-sample 32 final-layer tokens from the embedded subsequence and shuffle together all the embeddings. To estimate the ID of the embeddings we use the Maximum Likelihood Estimation ID algorithm [Levina and Bickel, 2004, Pedregosa et al., 2011] with $K=20$ neighbors. We split the sampled embedding tokens into batches of 4096 and run the MLE estimator on each batch, averaging together for the final result. Unless otherwise specified, we embed each document $D_{k}\\in\\mathcal{D}_{\\mathrm{test}}$ using a 125 million parameter model $M$ with $L_{T}=12$ layers and embedding dimension $d_{e m b d}=768$ . We first pretrain $M$ on the full $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ for 200, 000 steps or until convergence. ", "page_idx": 6}, {"type": "text", "text": "Intrinsic dimension predicts the empirical data scaling exponent $\\hat{\\alpha}_{D}$ To validate our prediction of the dataset scaling exponent $\\alpha_{D}$ we pretrain a series of 125 million parameter GPT-style LLMs on three different datasets: OpenWebText [Gokaslan et al., 2019], the SQL portion of The Stack [Kocetkov et al., 2022], and Tiny Stories [Eldan and Li, 2023]. We train across three orders of dataset size to fit scaling laws. Detailed hyperparameters can be found in the Appendix A. We report the observed scaling laws in log scale in Figure 2. In addition, we plot our predicted test loss whose slope is given by $\\begin{array}{r}{\\alpha_{D}=\\frac{2}{2+d_{\\mathcal{D}}}}\\end{array}$ where $d_{D}$ is the our estimated intrinsic dimension. ", "page_idx": 6}, {"type": "text", "text": "Empirically, we find all three datasets produce nearly log-linear laws whose exponents lie between $0.1<\\hat{\\alpha}_{D}<0.15$ Tiny Stories has the largest exponent, indicating the fastest rate of convergence, followed by the SQL dataset, followed by OpenWebText. This matches our rough intuition, since Tiny Stories is a synthetically generated dataset with less complexity than the other two datasets and thus easier to learn. The predicted exponents $\\alpha_{D}$ generally over-estimate $\\hat{\\alpha}_{D}$ but otherwise closely match up to $\\pm0.02$ absolute error. Additionally, the predicted $\\alpha_{D}$ reflect the previouly mentioned differences in complexity of pretraining datasets. In particular, Tiny Stories has a smaller estimated ID than both OpenWebText and SQL, resulting in a larger predicted $\\alpha_{D}$ as desired. ", "page_idx": 6}, {"type": "text", "text": "Predicting empirical model scaling exponent $\\hat{\\alpha}_{N}$ with intrinsic dimension To validate our predictions of the model scaling exponent $\\begin{array}{r}{\\alpha_{N}=\\frac{2}{d_{\\mathcal{D}}}}\\end{array}$ =d2D , we evaluate two model scaling suites: GPT2 [Radford et al., 2019] and Pythia [Biderman et al., 2023]. We refer to Kaplan et al. [2020] for GPT2\u2019s $\\hat{\\alpha}_{N}$ and estimate $\\hat{\\alpha}_{N}$ using OpenWebText as a proxy for GPT2\u2019s pretraining data. We compute $\\alpha_{N}$ for Pythia by evaluating each model on The Pile\u2019s test set [Gao et al., 2021]. We also estimate $d_{D}$ on the publicly available pretraining data to predict $\\alpha_{N}$ . The results are reported in Figure 3. Our predicted $\\alpha_{N}$ under-estimates the empirical $\\hat{\\alpha}_{N}$ . We conjecture this to be due to a number of factors including possible under-training of the largest models and the intrinsic entropy of the data distribution. ", "page_idx": 6}, {"type": "image", "img_path": "N2wYPMpifA/tmp/bbede87656298c9401daf623e990edebd0843eea2ae0754a9912952563bd09bc.jpg", "img_caption": ["Figure 3: Observed and predicted model scaling laws in model size on GPT2 and Pythia scaling suites. $\\alpha_{N}$ denotes the empirically observed scaling exponent, and $\\hat{\\alpha}_{N}$ denotes the theoretically predicted exponent. Note: we estimate $\\alpha_{N}$ for GPT2 using OpenWebText. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "N2wYPMpifA/tmp/73dba088621a4974ff7e38d58ee6e49df159b5deab6d9d1295436c1351334107.jpg", "img_caption": ["Figure 4: Top left: Estimated ID vs. number of parameters. Top right: Estimated ID vs. the embedding dimension. Bottom left: Variation of estimated ID across model layers. Bottom right: Variation of estimated ID across context position. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Ablating the impact of model architecture on the estimated ID Practical application of our theory relies on a good estimate of the intrinsic dimension. However, there are many factors potentially biasing our estimate. Of particular interest is the embedding model\u2019s embedding dimension, depth, context length, and number of parameters. We ablate these factors in Figure 4, plotting estimated ID against each factor. ", "page_idx": 7}, {"type": "text", "text": "Overall, we find the estimated ID is fairly stable across each factor. As the number of parameters increases, the estimated ID of The Pile slightly increases from 15.56, via a 410 million parameter model, to 20.02 with a 12.8 billion parameter model. ID on OpenWebText behaves similarly when increasing the embedding dimension, increasing from 15.56 when $d_{e m b d}\\,=\\,768$ to 18.68 when $d_{e m b d}=1536$ . When fixing a single model, we find the ID across intermediate embedding layers is small initially but then increases and decreases again, stabilizing around the ID of the final layer. We observe that the ID appears to inversely correlate with sequence length, decreasing from 15.86 for very short sequences to 12.9 for sequences around 1024 tokens. ", "page_idx": 7}, {"type": "text", "text": "Predicting $\\alpha_{N}$ from $\\alpha_{D}$ (and vice versa) without estimating ID Above we estimated $\\alpha_{D}$ and $\\alpha_{N}$ by first estimating the intrinsic dimension $d$ for a model\u2019s pretraining dataset. However, estimating $d$ may not always be possible when pretraining data is not public. Alternatively, we can predict $\\alpha_{D}$ in terms of $\\alpha_{N}$ (and vice versa) without ever needing to estimate $d$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha_{D}=\\frac{2}{2+d}=\\frac{2\\frac{1}{d}}{2\\frac{1}{d}+1}=\\frac{\\alpha_{N}}{\\alpha_{N}+1},\\quad\\alpha_{N}=\\frac{\\alpha_{D}}{1-\\alpha_{D}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "See Table 1 for ID-free estimations of empirically observed exponents in the literature [Sharma and Kaplan, 2022, Hoffmann et al., 2022]. ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\frac{\\mathrm{GPT}-2}{\\hat{\\alpha}_{N}=0.076\\quad\\alpha_{D}=0.070}}\\,\\bigg|\\,{\\frac{\\mathrm{Chinchilla}}{\\hat{\\alpha}_{N}=0.34\\quad\\alpha_{D}=0.25}}}\\\\ {{\\hat{\\alpha}_{D}=0.095\\quad\\alpha_{N}=0.106}\\,\\,\\Big|\\,\\,\\hat{\\alpha}_{D}=0.28\\quad\\alpha_{N}=0.33}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Table 1: ID-free estimation of scaling exponents for GPT-2 and Chincilla. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The theoretical properties and advantages of transformers have been studied from many different perspectives [Jelassi et al., 2022, Zhang et al., 2022, Bai et al., 2023, P\u00e9rez et al., 2021, Sanford et al., 2024]. Most related to us are Yun et al. [2019], Edelman et al. [2022], Wei et al. [2022], Takakura and Suzuki [2023] in which transformers were studied from an approximation viewpoint. The work in Yun et al. [2019] proved that transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, while the network size suffers from the curse of dimensionality (the number of entries in the input sequence). Takakura and Suzuki [2023] studied the approximation and estimation ability of Transformers as seq-to-seq functions with infinite dimensional input, where anisotropic smoothness avoids the curse of dimensionality. ", "page_idx": 8}, {"type": "text", "text": "In the applications of Large Language Models (LLMs), empirical findings have demonstrated some correlation between the performance of transformers and the low-dimensional data structures [Razzhigaev et al., 2023, Min et al., 2023, Aghajanyan et al., 2020]. Razzhigaev et al. [2023] investigated the intrinsic dimension of embeddings in transformer architectures, and suggested an encoder and decoder embedding property. Most similar to our work is Sharma and Kaplan [2022] which demonstrates an empirical connection between neural scaling laws and the intrinsic dimension of data. While they briefly discuss predictions for LLMs, their theory works best for predicting student-teacher model setups and image classification tasks which seem to enjoy more regularity (and a faster rate of convergence) than language modeling. Despite these empirical findings, we are not aware of any rigorous theoretical justification connecting the scaling laws of transformers with the intrinsic dimension of data. Our paper complements this line of research with statistical estimation and mathematical approximation theories which well-predict the behavior observed in practice. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Conclusion This paper establishes statistical and approximation theory results for transformers approximating H\u00f6lder continuous functions on low-dimensional data manifolds. The resulting bound on the generalization error suffers from exponential dependence only in the intrinsic dimension $d$ . The constructed approximations of low-dimensional functions are shallow, requiring only $O\\big(\\log(d)\\big)$ layers independent of the desired accuracy. We demonstrate this theory is accurate in practice by predicting scaling laws in both model size and data size for LLMs trained on natural language datasets. We pay careful attention to the sensitivity of the estimated intrinsic data dimension, finding it is relatively stable with respect to several relevant hyperparameters. ", "page_idx": 8}, {"type": "text", "text": "Limitations and Broader Impact One important question unanswered by this work is how the intrinsic data dimension may affect the computational scaling exponent $\\alpha_{C}$ . Future work may investigate this direction. Additionally, our empirical experiments make the simplifying assumption that the underlying target function possesses Lipschitz regularity $\\left.\\left(\\beta\\right.\\right.=\\left.1\\right)$ ). Better estimates of the correct regularity would likely improve the accuracy of our predictions. More broadly, our work improves fundamental understanding of transformer-based LLMs and improves our ability to theoretically and safely predict future capabilities. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Eddie Aamari, Jisu Kim, Fr\u00e9d\u00e9ric Chazal, Bertrand Michel, Alessandro Rinaldo, and Larry Wasserman. Estimating the reach of a manifold, 2019. ", "page_idx": 9}, {"type": "text", "text": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. ArXiv, abs/2012.13255, 2020. URL https://api. semanticscholar.org/CorpusID:229371560. ", "page_idx": 9}, {"type": "text", "text": "Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. 2007.   \nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.   \nYasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. ArXiv, abs/2102.06701, 2021. URL https://api.semanticscholar.org/ CorpusID:231918701.   \nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection, 2023.   \nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.   \nMinshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Efficient approximation of deep relu networks for functions on low dimensional manifolds. Advances in neural information processing systems, 32, 2019.   \nMinshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on lowdimensional manifolds using deep relu networks: Function approximation and statistical recovery. Information and Inference: A Journal of the IMA, 11(4):1203\u20131253, 2022.   \nJohn Conway and N. Sloane. Sphere Packings, Lattices and Groups, volume 290. 01 1988. ISBN 978-1-4757-2018-1. doi: 10.1007/978-1-4757-2016-7.   \nBiraj Dahal, Alexander Havrilla, Minshuo Chen, Tuo Zhao, and Wenjing Liao. On deep generative models for approximation and estimation of distributions on manifolds. Advances in Neural Information Processing Systems, 35:10615\u201310628, 2022.   \nBenjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 5793\u20135831. PMLR, 2022.   \nRonen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023.   \nHerbert Federer. Curvature measures. Transactions of the American Mathematical Society, 93(3): 418\u2013491, 1959.   \nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. URL https://arxiv.org/abs/2101.00027.   \nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus, 2019.   \nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \nAlex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 6645\u20136649. Ieee, 2013.   \nShixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In 2017 IEEE international conference on robotics and automation (ICRA), pages 3389\u20133396. IEEE, 2017.   \nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.   \nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.   \nSamy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822\u201337836, 2022.   \nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \nYongdai Kim, Ilsang Ohn, and Dongha Kim. Fast convergence rates of deep neural networks for classification, 2019. URL https://arxiv.org/abs/1812.03599.   \nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint, 2022.   \nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   \nJaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent \\*. Journal of Statistical Mechanics: Theory and Experiment, 2020(12): 124002, December 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc62b. URL http: //dx.doi.org/10.1088/1742-5468/abc62b.   \nElizaveta Levina and Peter Bickel. Maximum likelihood estimation of intrinsic dimension. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 17. MIT Press, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/ file/74934548253bcab8490ebd74afed7031-Paper.pdf.   \nHao Liu, Minshuo Chen, Tuo Zhao, and Wenjing Liao. Besov function approximation and binary classification on low-dimensional manifolds using convolutional residual networks. In International Conference on Machine Learning, pages 6770\u20136780. PMLR, 2021.   \nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \nZeping Min, Qian Ge, and Zhong Li. An intrinsic dimension perspective of transformers for sequential modeling, 2023. URL https://openreview.net/forum?id=0UzYWLzPBjA.   \nRiccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley. Deep learning for healthcare: review, opportunities and challenges. Briefings in bioinformatics, 19(6):1236\u20131246, 2018.   \nRyumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep neural network with intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1\u201338, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Kenta Oono and Taiji Suzuki. Approximation and non-parametric estimation of resnet-type convolutional neural networks. In International conference on machine learning, pages 4922\u20134931. PMLR, 2019. ", "page_idx": 11}, {"type": "text", "text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \nPhillip E. Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. ArXiv, abs/2104.08894, 2021. URL https: //api.semanticscholar.org/CorpusID:233296562.   \nJorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. Attention is turing-complete. Journal of Machine Learning Research, 22(75):1\u201335, 2021. URL http://jmlr.org/papers/v22/20-302.html.   \nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. URL https://api.semanticscholar.org/ CorpusID:160025533.   \nAnton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Ivan Oseledets, Denis Dimitrov, and Andrey Kuznetsov. The shape of learning: Anisotropy and intrinsic dimensions in transformerbased models. ArXiv, abs/2311.05928, 2023. URL https://api.semanticscholar.org/ CorpusID:265128603.   \nJonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019.   \nSam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290(5500):2323\u20132326, 2000.   \nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li FeiFei. Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. URL http://arxiv.org/abs/1409.0575.   \nClayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and logarithmic depth. ArXiv, abs/2402.09268, 2024. URL https://api.semanticscholar.org/ CorpusID:267657804.   \nJohannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. The Annals of Statistics, 48(4):1875 \u2013 1897, 2020. doi: 10.1214/19-AOS1875. URL https://doi.org/10.1214/19-AOS1875.   \nUtkarsh Sharma and Jared Kaplan. Scaling laws from the data manifold dimension. J. Mach. Learn. Res., 23(1), jan 2022. ISSN 1532-4435.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \nShokichi Takakura and Taiji Suzuki. Approximation and estimation ability of transformers for sequence-to-sequence functions with infinite dimensional input. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:258967774.   \nJoshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323, 2000.   \nLoring W. Tu. An Introduction to Manifolds. 01 2011. ISBN 978-1-4419-7399-3.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz ", "page_idx": 11}, {"type": "text", "text": "Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. ", "page_idx": 11}, {"type": "text", "text": "Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 12071\u201312083. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 4ebf1d74f53ece08512a23309d58df89-Paper-Conference.pdf.   \nDmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural networks : the official journal of the International Neural Network Society, 94:103\u2013114, 2016. URL https://api.semanticscholar.org/CorpusID:426133.   \nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? ArXiv, abs/1912.10077, 2019. URL https://api.semanticscholar.org/CorpusID:209444410.   \nYufeng Zhang, Boyi Liu, Qi Cai, Lingxiao Wang, and Zhaoran Wang. An analysis of attention via the lens of exchangeability and latent variable models. arXiv preprint arXiv:2212.14852, 2022. ", "page_idx": 12}, {"type": "text", "text": "A Pretraining Hyperparameters ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "N2wYPMpifA/tmp/c5ece5d39c4497273c1f2c1c97911fbdc8275e7f0ca01601dbc96370d23d38c2.jpg", "table_caption": [], "table_footnote": ["Table 2: Default hyperparameters for all training jobs. All training was done on four RTX 6000s. "], "page_idx": 13}, {"type": "text", "text": "Table 2 lists the default hyper-parameters we use for pretraining. All training was done on four RTX 6000s. ", "page_idx": 13}, {"type": "text", "text": "B Deriving Lanugage Model Scaling Laws from Statistical and Approximation Theory ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Squared Regression Error ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First we extract bounds on scaling laws in the case of regression squared error. We have the bound from the proof of Theorem 1 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{M}}\\big(f(x)-\\hat{\\Upsilon}_{n}(x)\\big)^{2}d Q(x)\\le\\tilde{O}\\big(\\epsilon^{2}+\\frac{D d^{2}\\epsilon^{-\\frac{d}{\\beta}}}{n}\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\epsilon$ is the approximation error such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathrm{T}\\in\\mathrm{T}}\\|f-\\mathrm{T}\\|_{L^{\\infty}(\\mathcal{M})}<\\epsilon\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let the model size $N$ of a transformer $\\mathrm{T}\\in\\mathcal{T}$ be $N=L_{T}(d_{e m b d}^{2}(3m+L_{\\mathrm{FFN}}))=\\log(d)(25(3\\epsilon^{-\\frac{d}{\\beta}}+$ $\\log(\\epsilon^{-1}))=\\tilde{O}\\bigl(\\epsilon^{-\\frac{d}{\\beta}}\\bigr)$ . Write the squared generalization error as ", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{\\mathrm{sq}}(N,n)=\\int_{\\mathcal{M}}\\big(f(x)-\\hat{\\mathrm{T}}_{n}(x)\\big)^{2}d Q(x).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then ", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{\\mathrm{sq}}(N,n)\\leq\\tilde{O}\\big(\\epsilon^{2}+\\frac{D d^{2}\\epsilon^{-\\frac{d}{\\beta}}}{n}\\big)=\\tilde{O}\\big(N^{-\\frac{2\\beta}{d}}+\\frac{N}{n}\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the model scaling regime, when data is plentiful, we have $\\frac{N}{n}\\;<<\\;N^{-\\frac{2\\beta}{d}}$ which implies the behavior of $L_{\\mathrm{sq}}(N,n)$ is dominated by $\\textstyle N^{-{\\frac{2\\beta}{d}}}$ . This gives us the model scaling exponent as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\alpha_{N}=\\frac{2\\beta}{d}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the data scaling exponent $\\alpha_{D}$ we will choose $N$ to balance both error terms. The will predict how data size and model size should scale together to achieve a minimal generalization error. We should have ", "page_idx": 13}, {"type": "equation", "text": "$$\nN^{-{\\frac{2\\beta}{d}}}\\asymp{\\frac{N}{n}}\\iff n\\asymp N^{1+{\\frac{2\\beta}{d}}}\\asymp N^{\\frac{2\\beta+d}{d}}\\iff N\\asymp n^{\\frac{d}{2\\beta+d}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\asymp$ denotes in the same order. Substituting this into the error bound gives ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{\\mathrm{sq}}(N,n)\\leq\\tilde{O}\\big(2\\frac{N}{n}\\big)=\\tilde{O}\\big(n^{\\frac{d}{2\\beta+d}-1}\\big)=\\tilde{O}\\big(n^{-\\frac{2\\beta}{2\\beta+d}}\\big),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which gives rise to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha_{D}=\\frac{2\\beta}{2\\beta+d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B.2 From Regression to Classification ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "After establishing statistical and approximation theory for transformers for regression, we now seek to apply our theory to classification. In language models, the next-token prediction task is a multi-class classification problem, where transformers are trained to predict the probability of the next word out of a large dictionary. ", "page_idx": 14}, {"type": "text", "text": "For simplicity, we consider binary classification here. Let $(x,y)$ be a random couple taking values in $\\mathcal{M}\\times\\{\\bar{0},1\\}$ with joint distribution $P$ , where $\\mathcal{M}$ is a manifold satisfying Assumption 1. Let $P_{x}$ be the marginal distribution of $x$ . Here $x$ stands for the input feature and $y$ is the corresponding label. The classification goal is to predict the label $y$ given the value of $x$ . A decision rule is given by a function $f:\\mathcal{M}\\rightarrow\\{0,\\bar{1}\\}$ . The performance of a decision rule $f$ is measured by the misclassification error ", "page_idx": 14}, {"type": "equation", "text": "$$\nR(f):=P(y\\neq f(x)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The Bayes decision rule has the form ", "page_idx": 14}, {"type": "equation", "text": "$$\nf^{*}(x)=\\mathbf{1}\\left\\{\\eta(x)\\geq1/2\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where 1 denotes the indicator function and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\eta(x):=P(y=1|x)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is the regression function of $y$ on $x$ . For binary classification, the goal is to estimate the probability function $\\eta(x)$ . If $\\eta$ is a $\\beta$ -H\u00f6lder function satisfying Assumption 2, our approximation theory in Theorem 2 gives rise to a transformer neural network $\\eta_{\\theta}$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\eta_{\\theta}-\\eta\\|_{L^{\\infty}(\\mathcal{M})}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for an arbitrary small $\\epsilon>0$ . ", "page_idx": 14}, {"type": "text", "text": "We next consider the plug-in estimate of $\\eta_{\\theta}$ [Audibert and Tsybakov, 2007]: $f_{\\theta}(x)=\\mathbf{1}\\left\\{\\eta_{\\theta}\\geq1/2\\right\\}$ . The excess risk of the plug-in estimate $f_{\\theta}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(f_{\\theta})=R(f_{\\theta})-R(f^{*})}\\\\ &{\\quad\\quad=P(y\\ne f_{\\theta}(x))-P(y\\ne f^{*}(x))}\\\\ &{\\quad\\quad=P(y=f^{*}(x))-P(y=f_{\\theta}(x))}\\\\ &{\\quad\\quad=\\mathbb{E}_{x}[\\mathbf{1}\\left\\{f^{*}(x)=1\\right\\}\\eta(x)+\\mathbf{1}\\left\\{f^{*}(x)=0\\right\\}(1-\\eta(x))}\\\\ &{\\quad\\quad\\quad-\\mathbf{1}\\left\\{f_{\\theta}(x)=1\\right\\}\\eta(x)-\\mathbf{1}\\left\\{f_{\\theta}(x)=0\\right\\}(1-\\eta(x))]}\\\\ &{\\quad\\quad=\\mathbb{E}_{x}\\left[|2\\eta(x)-1|\\mathbf{1}\\{f_{\\theta}(x)\\ne f^{*}(x)\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We next discuss how the regression error in (9) impacts the excess risk $\\mathcal{E}(f_{\\theta})$ . For classification problems, the classification error depends on how well the conditional probability $\\eta(x)$ is estimated, and how many points are close to the decision boundary. Following Audibert and Tsybakov [2007], we assume the following margin condition: There exist $c_{M}>0$ and $\\gamma\\geq0$ , such that for any $t>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{x}(0<|\\eta(x)-1/2|\\leq t)\\leq c_{M}t^{\\gamma}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A smaller $\\gamma$ means more samples cluster around the decision boundary with a higher likelihood of falling into either class. This is not expected to be the case for natural data, where for most samples it is easy to tell to which class they belong [Kim et al., 2019, Figure 2]. As a result, we assume $\\gamma\\geq1$ ", "page_idx": 14}, {"type": "text", "text": "Under the margin condition in (10), we follow Audibert and Tsybakov [2007] to decompose the excess risk $\\mathcal{E}(\\bar{f_{\\theta}})$ such that for any $\\delta>0$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(f_{\\theta})=\\mathbb{E}_{x}\\left[|2\\eta(x)-1|\\mathbf{1}\\{f_{\\theta}(x)\\neq f^{*}(x)\\}\\mathbf{1}\\{|\\eta(x)-1/2|\\leq\\delta\\}\\right]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}_{x}\\left[|2\\eta(x)-1|\\mathbf{1}\\{f_{\\theta}(x)\\neq f^{*}(x)\\}\\mathbf{1}\\{|\\eta(x)-1/2|>\\delta\\}\\right]}\\\\ &{\\quad\\quad\\quad\\leq2\\delta P_{x}(|\\eta(x)-1/2|\\leq\\delta)+2\\mathbb{E}_{x}\\left[|\\eta_{\\theta}(x)-\\eta(x)|\\mathbf{1}\\left\\{|\\eta_{\\theta}(x)-\\eta(x)|>\\delta\\right\\}\\right]}\\\\ &{\\quad\\quad\\quad\\leq2c_{M}\\delta^{1+\\gamma}+2\\mathbb{E}_{x}\\left[|\\eta_{\\theta}(x)-\\eta(x)|\\mathbf{1}\\left\\{|\\eta_{\\theta}(x)-\\eta(x)|>\\delta\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging the regression error in (9) to the excess risk bound in (11) with $\\delta=\\epsilon$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}(f_{\\theta})\\le2c_{M}\\epsilon^{1+\\gamma}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When the margin assumption satisfies $\\gamma=1$ , the excess risk bound becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}(f_{\\theta})\\leq2c_{M}\\epsilon^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which demonstrates a connection between the classification risk bound and the squared regression error for the $\\eta$ function in (8). This argument partially justifies the connection between empirical neural scaling laws in large language models and the squared regression error. We will leave a rigorous mathematical argument as future work. ", "page_idx": 15}, {"type": "text", "text": "B.3 Cross-Entropy Based Language Model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In practice, language models are trained and evaluated using cross-entropy loss. We consider the nexttoken prediction in language models. Per-sample (token), language models are trained to minimize the multi-class cross-entropy loss over a large number of classes/tokens: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{\\mathrm{cr}}(n)=-\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{y=1}^{V}\\mathbf{1}_{y=y_{i}^{*}}\\ln(P(y|x_{i}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $V$ is the vocabulary size of the model (number of classes), $y_{i}^{*}$ is the ground-truth label of $x_{i}$ , $\\mathbf{1}_{y=y_{i}^{*}}$ is the indicator function for the event $y=y_{i}^{*}$ , and $P(\\boldsymbol{y}|\\boldsymbol{x})$ is the conditional probability of labels given $x$ . For the next-token prediction, transformers are trained to estimate the probability function $P(\\boldsymbol{y}|\\boldsymbol{x})$ , and the next token is predicted to the word with the highest probability. The test loss is evaluated using cross-entropy on test data. We will leave the error bound on the cross-entropy loss as future work. ", "page_idx": 15}, {"type": "text", "text": "C Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$H$ will represent the embedding matrix of a transformer and $h_{i}$ will represent the ith token (column) of the embedding matrix. $h_{i}^{j}$ will denote the $j$ th component of the vector $h_{i}$ . $\\|\\cdot\\|_{p},p>1$ , will denote the $p$ th norm of vectors, and $\\|\\cdot\\|_{p,q}$ will denote the component-wise matrix norm. For a matrix $A\\in\\mathbb{R}^{m\\times n}$ , $\\begin{array}{r}{\\|A\\|_{p,q}=(\\sum_{j=1}^{n}(\\sum_{i=1}^{m}|A_{i j}|^{p})^{\\frac{q}{p}})^{\\frac{1}{q}}}\\end{array}$ . In particular, we denote $\\|A\\|_{\\infty}=\\|A\\|_{\\infty,\\infty}$ and $\\|A\\|_{1}=\\|A\\|_{1,1}$ for matrices. $\\|\\cdot\\|_{L^{p}(U)}$ will denote the $L^{p}$ norm of a function on $U\\subseteq\\mathbb{R}^{D}$ . Neural network blocks will always be bolded. Sometimes neural network blocks will also be sub-scripted with their corresponding vector of weights $\\theta$ . We use $\\|\\theta\\|_{\\infty}$ to denote the largest magnitude of the weight parameters. However we will often omit $\\theta$ and leave the dependence as implicit. In many places, we will also write $\\theta_{\\mathbf{NN}}$ to denote the weights for a neural network NN. $e_{i}$ will denote the ith standard basis vector where the ith component is 1 and all other entries are 0. $\\sigma$ will always denote the ReLU activation. $B(x,r)$ denotes the Euclidean ball of radius $r$ centered at $x\\in\\mathbb{R}^{D}$ . For a vector $h_{t}$ $\\mathbf{\\u}_{t}^{i:j}\\in\\mathbb{R}^{j-i}$ will denote the vector such that $(h_{t}^{i:j})^{k}=h_{t}^{k}$ i.e. the contiguous components of $h_{t}$ from $i,...,j-1$ . ", "page_idx": 15}, {"type": "text", "text": "D Details about Transformer Neural Networks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We have defined transformer neural networks in Definition 1. Some detailed definitions are given below. ", "page_idx": 15}, {"type": "text", "text": "Definition 3 (Transformer Block). We define a transformer block B as a residual composition of the form ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{B}(H)=\\mathrm{FFN}(\\mathrm{MHA}(H)+H)+\\mathrm{MHA}(H)+H\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "taking as input an embedding matrix $H\\in\\mathbb{R}^{d_{e m b d}\\times l}$ and MHA is a multi-headed attention layer and FFN is a feed-forward layer. To define the multi-headed attention layer we first define the attention mechanism. ", "page_idx": 15}, {"type": "image", "img_path": "N2wYPMpifA/tmp/8c1aafff6b01d4b843fb70806bfec3e4040d3b22e7e3f613137cabbe4c196d97.jpg", "img_caption": ["Figure 5: Diagram of transformer block. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Definition 4 (Attention). ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{A}_{Q,K,V}(H)=V H\\sigma((K H)^{T}Q H)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $Q,K,V\\in\\mathbb{R}^{d_{e m b d}\\times d_{e m b d}}$ are referred to as the query, key, and value matrices and $\\sigma(x)=$ $\\operatorname*{max}(0,x)$ is the ReLU activation function. Often we will omit the dependence of $A$ on $Q,K,V$ . We note it will be convenient to write the action of $A$ on the ith column $h_{i}$ of $H$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf A}(h_{i})=\\sum_{j=1}^{l}\\sigma(\\langle Q h_{i},K h_{j}\\rangle)V h_{j}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": ". This allows us to interpret the ith column of the output of $A$ as a linear combination of the values weighted by the interaction of $h_{i}t h$ query and the $h_{j}t h$ key. Multi-headed attention can then be defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{MHA}(H)=W_{O}(\\mathrm{concat}_{j}(V_{j}H\\sigma((K_{j}H)^{T}Q_{j}H))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the output of each of $j\\;\\;\\in\\;\\;\\{1,...,M\\}$ attention heads is concatenated and $W_{O}\\quad\\in$ $\\mathbb{R}^{d_{e m b d}\\times m d_{e m b d}}$ . Frequently we will simply take $W_{O}$ to be a sum so that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{MHA}(H)=\\sum_{j=1}^{m}V_{j}H\\sigma((K_{j}H)^{T}Q_{j}H)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We also formally define the feed-forward layer FFN: ", "page_idx": 16}, {"type": "text", "text": "Definition 5 (Feed-forward Layer). $A$ feed-forward layer of depth $L_{\\mathrm{FFN}}$ and width $w_{\\mathrm{FFN}}$ is of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{FFN}(h)=W_{L_{\\mathrm{FFN}}}\\sigma(W_{L_{\\mathrm{FN}}-1}...\\sigma(W_{1}h+b_{1})...+b_{L_{\\mathrm{FFN}}-1})+b_{L_{\\mathrm{FN}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{W_{2},...,W_{L_{\\mathrm{FFN}}-1}\\in\\mathbb{R}^{d_{e m b d}\\times d_{e m b d}},\\,W_{1}\\in\\mathbb{R}^{w_{\\mathrm{FFN}}\\times d_{e m b d}},W_{L_{\\mathrm{FFN}}}\\in\\mathbb{R}^{d_{e m b d}\\times w_{\\mathrm{FN}}},}\\end{array}$ $b_{1},...,b_{L_{\\mathtt{F F N}}}\\in$ $\\mathbb{R}^{d_{e m b d}}$ and the activation is ReLU. Note, each feed-forward layer is applied tokenwise to an embedding matrix $H$ . ", "page_idx": 16}, {"type": "text", "text": "We can also define a class $\\mathcal{F F N}$ of feed-forward networks: ", "page_idx": 16}, {"type": "image", "img_path": "N2wYPMpifA/tmp/2cc3e3c277cbd19237a4a11e82bdcca46204d5b53a388244cc2ab8925575ba1a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: Diagram of a structured token. The first two rows contain mutable data used to compute the target function. The remaining rows are never changed after initialization. ", "page_idx": 17}, {"type": "text", "text": "Definition 6 (Feed-forward Network Class). ", "page_idx": 17}, {"type": "text", "text": "Sometimes we may omit the dependence of $\\mathcal{F}$ on $w_{\\mathrm{FFN}}$ . In these cases we assume $w_{\\mathrm{FFN}}=d_{e m b d}$ . We similarly define multi-headed attention and transformer block classes: ", "page_idx": 17}, {"type": "text", "text": "Definition 7 (Multi-headed Attention and Transformer Block Classes). ", "page_idx": 17}, {"type": "text", "text": "$\\mathscr{B}(m,L_{\\mathrm{FFN}},w_{\\mathrm{FFN}})=\\left\\{\\mathbf{B}_{\\theta}\\vert\\ \\mathbf{B}_{\\theta}\\right.$ is a transformer block with weights $\\theta$ , a $m$ -headed multi-headed attention layer, and a $L_{\\mathrm{FFN}}$ deep feed-forward layer with width $w_{\\mathrm{FFN}}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "As with $\\mathcal{F}$ , we may sometimes omit the dependence of $\\mathbf{B}$ on $w_{\\mathrm{FFN}}$ . In these cases we take $w_{\\mathrm{FFN}}=$ $d_{e m b d}$ . We can parameterize the class of transformer neural networks $\\tau$ as Definition 2. ", "page_idx": 17}, {"type": "text", "text": "Lastly, our constructions will rely heavily on a particular structuring of the tokens/columns in the transformer\u2019s hidden states. The first two rows will contain mutable data used to compute the target function. The remaining rows will be immutable, containing positional/interaction vectors allowing us to uniquely identify each row and a constant term serving as a kind of \u201cscratchpad\u201d. See Figure 6 for a diagram. ", "page_idx": 17}, {"type": "text", "text": "E Proof of Transformer Approximation Theory ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first prove the approximation theories in Section 2.4. We first prove Lemma 1 in Section E.1, and then prove Theorem 2 in Section E.2. ", "page_idx": 17}, {"type": "text", "text": "E.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Fix $f\\,:\\,[0,1]^{d}\\,\\rightarrow\\,\\mathbb{R}$ and $\\epsilon\\,>\\,0$ . We aim to approximate $f$ efficiently with a transformer $\\mathrm{~T~\\in~}\\mathcal{T}(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ . We will proceed similarly to Yarotsky [2016] by approximating $f$ locally with piecewise constant functions. First, we partition the domain uniformly into blocks indexed by $n\\in\\{1,...,N\\}^{d}$ for some $N\\geq1$ with $U_{n}=\\{x\\in[0,1]^{d}:\\forall i\\in$ $\\begin{array}{r}{\\{1,...,d\\},\\left|x^{i}-\\frac{n^{i}}{N-1}\\right|\\leq\\frac{1}{N-1}\\}}\\end{array}$ . In this proof, we use $x^{i}$ and $n^{i}$ to denote the $i$ th coordinate of $x$ and $n$ respectively. Then we construct a partition of unity $(\\bf P_{0}U)$ via ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi_{n}(x)=\\prod_{i=1}^{d}\\psi\\left(3N(x^{i}-\\frac{n^{i}-1}{N-1})\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with the trapezoid function $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\psi(x)={\\binom{1}{1-|x|}}\\quad|x|\\leq1}\\\\ {0\\quad\\qquad\\quad|x|\\geq2}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can now write our approximation for $f$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{f}(x)=\\sum_{n}\\phi_{n}(x)f_{n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $f_{n}=f(x_{n})$ with $x_{n}$ being the center point of $U_{n}$ . This yields the following approximation error: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|f-\\widehat{f}\\|_{L^{1}(\\mathbb{R}),1}\\neq}&{\\displaystyle\\operatorname*{sup}_{(1)\\neq}|f|\\left(Z\\mapsto-\\widehat{f}\\right)(x)|}\\\\ &{=\\displaystyle\\sum_{x\\in[0],1\\}_{|\\xi_{1}|\\xi_{2}|\\in[0]}\\int_{\\mathbb{R}^{d}(1-\\chi)}\\sum_{\\ell=1}^{d}f_{\\alpha}\\phi_{\\alpha}(x)\\Big|}\\\\ &{=\\displaystyle\\sum_{x\\in[0],1\\}_{|\\xi_{1}|\\in[0]}\\sum_{\\ell=1,\\cdots,N\\}f_{\\ell}(z)\\phi_{\\alpha}(x)-\\sum_{w\\in\\{1,\\cdots,N\\}}\\int_{\\mathbb{R}^{d}(\\mathbb{R})}(z)}\\\\ &{=\\displaystyle\\sum_{x\\in[0],1\\}_{|\\xi_{2}|\\in[0]}\\sum_{\\ell=1,\\cdots,N\\}f(z)-f_{\\alpha}|\\phi_{\\alpha}(x)}\\\\ &{=\\displaystyle\\operatorname*{sup}_{x\\in[0],1\\atop\\ell\\in[0]}\\sum_{\\ell=1}^{d}\\|f(x)-f_{\\alpha}\\|}\\\\ &{=\\displaystyle\\operatorname*{sup}_{x\\in[0],1\\atop d\\in[0]}\\sum_{\\ell=1}^{d}\\|f(x)-f_{\\alpha}\\|}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{(1)\\neq1,1\\atop d\\in[0]}\\sum_{\\ell=1}^{d}\\|f_{\\ell}\\|=-x_{1}\\|_{2}^{d}}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{(1)\\neq1,1\\atop d\\in[0]}\\sum_{\\ell=1}^{d}\\|f_{\\ell}\\|=-x_{1}\\|_{2}^{d}}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{(1)\\neq1,1\\atop d\\in[0]}\\sum_{\\ell=1}^{d}\\|f_{\\ell}\\|=-x_{1}\\|_{2}^{d}}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{(1)\\neq1,1\\atop d\\in[0,d]}2\\operatorname*{sup}_{(1)\\leq1,1\\leq N}2\\alpha^{d}g_{d}}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{(1)\\neq1,1\\atop d\\in[0,d]}2\\operatorname*{sup}_{(1)\\leq1,1\\leq N}2\\alpha^{d}g_{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus we can control this error simply by increasing $N$ . It suffices to pick $N=d(\\frac{2^{d}H_{f}}{\\epsilon})^{\\frac{1}{\\beta}}+1$ to obtain an $\\epsilon$ error bound. ", "page_idx": 18}, {"type": "text", "text": "We next construct an approximation to $\\hat{f}$ with a transformer neural network T. In fact we can represent $\\hat{f}$ exactly as a transformer neural network. ", "page_idx": 18}, {"type": "text", "text": "At a high level, we will proceed by building up each $\\phi_{n}$ as an accumulated partial product $p_{n}$ in parallel over $d$ transformer blocks. Each block will use at most $d_{e m b d}N_{e m b d}^{d}$ N edmbd attention heads. Each head will be responsible for multiplying two partial products. Conveniently $\\phi_{n}$ can be exactly implemented with a two-layer FFN and applied column-wise to each term. The constant terms $f_{n}$ will then be multiplied in and summed at the final layer to compute the output. ", "page_idx": 18}, {"type": "text", "text": "Step 1: Embed the input Now fix input $\\boldsymbol{x}\\in\\mathbb{R}^{1\\times d}$ . First we augment the input, via concatenation, with a sequence of 0s: $\\mathbf{0}_{N d+N^{d}}$ forming input $x^{\\prime}\\in\\mathbb{R}^{1\\times d+N d+N^{d}}$ (note this is a linear operation). We can then construct a linear embedding layer $[1,0,0,0,0]^{T}=E\\in\\mathbb{R}^{d_{e m b d}\\times1}$ resulting in the input embedding matrix $H\\in\\mathbb{R}^{d_{e m b d}\\times(d+N d+N^{d}d)}$ of the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c c c c}{x^{1}}&{\\ldots}&{x^{d}}&{{\\bf0}_{N d}}&{\\ldots}&{{\\bf0}_{N^{d}d}}\\\\ {{\\bf0}}&{\\ldots}&{{\\bf0}}&{{\\bf0}_{N d}}&{\\ldots}&{{\\bf0}_{N^{d}d}}\\\\ {{\\bf0}}&{\\ldots}&{{\\bf0}}&{{\\bf0}_{N d}}&{\\ldots}&{{\\bf0}_{N^{d}}}\\\\ {{\\bf0}}&{\\ldots}&{{\\bf0}}&{{\\bf0}_{N d}}&{\\ldots}&{{\\bf0}_{N^{d}d}}\\\\ {{\\bf0}}&{\\ldots}&{{\\bf0}}&{{\\bf0}_{N d}}&{\\ldots}&{{\\bf0}_{N^{d}d}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $d_{e m b d}=5$ . To this we add a fixed positional encoding $\\mathrm{PE}\\in\\mathbb{R}^{d_{e m b d}\\times(d+N d+N^{d}d)}$ , producing the first hidden embedding matrix ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{1}=\\left[\\!\\!\\begin{array}{c c c c c c}{x^{1}}&{\\ldots}&{x^{d}}&{\\mathbf{0}_{N d}}&{\\ldots}&{\\mathbf{0}_{N^{d}d}}\\\\ {0}&{\\ldots}&{0}&{\\mathbf{0}_{N d}}&{\\ldots}&{\\mathbf{0}_{N^{d}d}}\\\\ {\\mathcal{T}_{1}}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{\\mathcal{T}_{d+d N+N^{d}d}}\\\\ {1}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $I_{i}\\in\\mathbb{R}^{2}$ represents an interaction term determining when each token embedding will interact with another in the attention mechanism. We can set $\\begin{array}{r}{I_{i}=(\\cos(\\frac{i}{l}\\frac{\\pi}{2}),\\sin(\\frac{i}{l}\\frac{\\pi}{2}))}\\end{array}$ where $l$ is the number of hidden tokens. Note, this type of positional embedding is commonly known as the sinusoidal positional encoding and can be visualized as rotations of the unit vector $e_{1}$ around the first quadrant of the unit circle. ", "page_idx": 19}, {"type": "text", "text": "Step 2: Pre-compute $\\begin{array}{r}{\\psi(3N(x^{i}-\\frac{j-1}{N-1}))}\\end{array}$ Now we pre-compute the terms $\\begin{array}{r}{\\psi(3N(x^{i}-\\frac{j-1}{N-1}))}\\end{array}$ from which we can construct our partition of unity. We define $\\begin{array}{r}{s_{i,j}=\\psi(3(N-1)(x^{i}-\\frac{j-1}{N-1}))}\\end{array}$ for $1\\leq i\\leq d,1\\leq j\\leq N$ . We know $\\psi(3(N-1)\\cdot)$ ) can be computed exactly with a two-layer FFN. So all we must do is prepare the input $\\begin{array}{r}{x^{i}-\\frac{j-1}{N-1}}\\end{array}$ . We will do this with one transformer block ${\\bf{B}}_{1}$ . The results will be stored in token columns $h_{d+1},...,h_{d+N d}$ . ", "page_idx": 19}, {"type": "text", "text": "Define $\\mathbf{B}_{1}=B(N d,7)$ i.e. ${\\bf B}_{1}$ has a multi-headed attention layer with $N d$ attention heads and a feed-forward layer with depth 7. We will construct each of the $i j$ th blocks using the Interaction Lemma 3. We may choose data kernels ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{i j}^{d a t a}=\\left[\\begin{array}{c c c c c}{0}&{0}&{0}&{0}&{1}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\right]\\in\\mathbb{R}^{2\\times d_{e m b d}},\\quad K_{i j}^{d a t a}=\\left[\\begin{array}{c c c c c}{1}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{-\\frac{j}{N-1}+1}\\end{array}\\right]\\in\\mathbb{R}^{2\\times d_{e m b d}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so that for the token embedding $h_{i j}=h_{d+i\\ast d+j}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{A}_{i j}\\big(h_{i j}\\big)=\\sum_{k=1}^{l}\\sigma(\\langle Q_{i j}h_{i j},K_{i j}h_{k}\\rangle)V_{i j}h_{k}}}\\\\ {{\\displaystyle\\qquad\\qquad=\\sum_{k=1}^{l}\\sigma(\\langle Q_{i j}^{d a t a}h_{i j},K_{i j}^{d a t a}h_{k}\\rangle+\\langle Q_{i j}^{T}h_{i j},K_{i j}^{T}h_{k}\\rangle-C)e_{2}}}\\\\ {{\\displaystyle\\qquad=\\sum_{k=1}^{l}\\sigma(h_{k}^{1}-\\frac{j-1}{N-1}+1+\\langle Q_{i j}^{T}h_{i j},K_{i j}^{T}h_{k}\\rangle-C)e_{2}}}\\\\ {{\\displaystyle\\qquad=\\sigma(x^{i}-\\frac{j-1}{N-1}+1)e_{2}=(x^{i}-\\frac{j-1}{N-1}+1)e_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we can choose token $h_{i j}$ to only interact with token $h_{i}$ . Otherwise we have $\\mathbf{A}_{i j}(h_{t})=0$ for $h_{t}\\neq h_{i j}$ . Further, the weights of $\\theta_{\\mathrm{A}_{i j}}$ of $\\mathrm{A}_{i j}$ are bounded such that $\\lVert\\theta_{\\mathrm{A}_{i j}}\\rVert_{\\infty}=O\\left(l^{2}\\right)$ according to Lemma 3. ", "page_idx": 19}, {"type": "text", "text": "The added output of the multi-headed attention layer is then $H_{1.5}=\\mathrm{MHA}_{1}(H)+H$ and is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{1.5}=\\left[\\!\\!\\begin{array}{c c c c c c c c}{x^{1}}&{\\dots}&{x^{d}}&{\\mathbf{0}_{N d}}&{\\dots}&&&{\\mathbf{0}_{N^{d}d}}\\\\ {0}&{\\dots}&{0}&{x^{1}+c}&{\\dots}&{x^{d}-1+c}&{\\mathbf{0}_{N^{d}d}}\\\\ {\\!\\!\\!\\!Z_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\!\\!\\!\\!Z_{d+N d+N^{d}d}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\!\\!\\!\\!1}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It remains to subtract the positive term $c=1$ and then apply $\\psi$ for the tokens $d+1,...,d+d N$ while leaving the other tokens untouched. Define ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{1}=\\left[\\!\\!\\begin{array}{c c c c c}{{0}}&{{1}}&{{0}}&{{0}}&{{-1+M}}\\\\ {{0}}&{{-1}}&{{0}}&{{0}}&{{M}}\\\\ {{0}}&{{0}}&{{1}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{1}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\end{array}\\!\\!\\right],\\quad b_{1}=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{2}=\\left[\\!\\!\\begin{array}{c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{-M}}\\\\ {{0}}&{{1}}&{{0}}&{{0}}&{{-M}}\\\\ {{0}}&{{0}}&{{1}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{1}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\end{array}\\!\\!\\right],\\quad b_{2}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here $W_{1}$ shifts values in the second component of each token to the first and subtracts 1. The second component is negated to be subtracted from $H_{1.5}$ . $I_{t}$ and the last component are preserved. A large positive number $\\bar{M}>2\\|x\\|_{\\infty}$ is added to prevent negative terms from getting erased by ReLU. $W_{2}$ removes $M$ after the activation is applied. We can construct a two-layer network $\\mathrm{FFN_{1}}$ to apply $\\psi$ to the first component of each token $h_{t}$ . In tokens $h_{d},...,h_{d+N d}$ this produces the desired $s_{i j}$ terms. ", "page_idx": 20}, {"type": "text", "text": "Now we simply must ensure we zero-out changes to tokens outside the range $d+1,...,d+N d.$ . Via Lemma 4 we can construct a two-layer feed-forward gating network $\\mathrm{FFN_{2}}$ such that $\\mathrm{FFN}_{2}(h_{t})=h_{t}$ for $t\\leq d$ and $\\mathrm{FFN}_{2}(h_{t})$ is zero except for the last three rows when $t\\,>\\,d+d N$ . We can again invoke the same lemma to produce $\\bar{\\mathrm{FFN}}_{3}\\in\\mathcal{F F N}(2)$ such that $\\mathrm{FFN}_{3}(h_{t})=h_{t}$ when $t>d$ and zero except for the last three rows otherwise. Applying $\\mathrm{FFN_{2}}$ and $\\mathrm{FFN_{3}}$ zeroes out tokens outside $d+1,...,d+d N$ while preserving the rest. Finally, we define the last layer ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{7}=\\left[\\!\\!{\\begin{array}{c c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{1}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}}\\!\\!\\right],\\quad b_{7}=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which zeros out all except the first two components. This produces the next embedding matrix $H_{2}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nH_{2}=\\left[\\!\\!\\begin{array}{c c c c c c c}{x^{1}}&{\\dots}&{x^{d}}&{s_{1,1}}&{\\dots}&{s_{d,N}}&{\\mathbf{0}_{N^{d}d}}\\\\ {0}&{\\dots}&{0}&{\\mathbf{0}_{N d}}&{\\dots}&{\\dots}&{\\mathbf{0}_{N^{d}d}}\\\\ {\\mathcal{T}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\mathcal{T}_{d+N d+N^{d}d}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 3: Building up partial products Now we can start construction of the partial products for each patch $\\phi_{n}$ , $\\bar{n}\\;\\in\\;\\{1,....,N\\}^{d}$ . For a fixed patch $n$ , we write $p_{n,k,i}$ to indicate the ith partial product of $2^{k-1}$ terms. Formally, $p_{n,k,i}=p_{n,k-1,2i-1}\\cdot p_{n,k-1,2i}$ with $p_{n,1,i}=s_{i,n^{i}}$ where $\\begin{array}{r}{s_{i,n^{i}}=\\psi(3(N-1)(x^{i}-\\frac{n^{i}-1}{N-1}))}\\end{array}$ and $n\\in\\{1,...,N\\}^{d}$ , $1\\leq k\\leq\\log_{2}(d)$ , $\\begin{array}{r}{1\\leq i\\leq\\frac{d}{2^{k}}}\\end{array}$ . See Figure 7 for a diagram of how each partial product is assembled. Architecturally, this will be done in $\\log(d)+1$ transformer blocks. The first block will copy the product terms into the last $N^{d}d$ tokens. The remaining $\\log(d)$ blocks compute the recursively assembled partial products by multiplying $p_{n,k-1,2i-1}\\cdot p_{n,k-1,2i}$ . Storage and assembly will be done in the last $d+N d+1,...,d+N d+N^{d}d$ tokens. ", "page_idx": 20}, {"type": "text", "text": "For the first transformer block we define $\\mathbf{B}_{2}=B(N^{d}d,0)$ . We index each attention head as $A_{n,i}$ where $n\\in\\{1,...,N\\}^{d}$ and $1\\leq i\\leq d$ . Fix a patch $n$ and the corresponding product $\\textstyle\\prod_{i=1}^{d}s_{i,n^{i}}$ . These terms have been pre-computed and stored in tokens $h_{d+1},...,h_{d+N d}$ . We will then use each attention head An,i to copy si,ni into its corresponding token hn,i = hd+Nd+d dp=1(np\u22121)Nd\u2212p+i. Concretely, for a fixed attention head $A_{n,i}$ define the data kernels ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ_{n,i}^{d a t a}=\\left[\\!\\!\\begin{array}{c c c c c}{{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\right]\\!\\!\\!\\!\\!\\!\\!\\!K_{n,i}^{d a t a}=\\left[\\!\\!\\!\\begin{array}{c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "N2wYPMpifA/tmp/d616deebfc0d468e76cc760331ffe9b73231677efe626c8f7ab9ade221c161ad.jpg", "img_caption": ["Figure 7: Recursive assembly of partial products from constituent terms. Formally, $p_{n,k,i}~=$ $p_{n,k-1,2i-1}\\cdot p_{n,k-1,2i}$ with $p_{n,1,i}=s_{i,n^{i}}$ for $n\\in\\{1,...,N\\}^{d}$ , $\\begin{array}{r}{1\\leq k\\leq\\log_{2}(d),1\\leq i\\leq\\frac{d}{2^{k}}}\\end{array}$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "via the Interaction Lemma 3 we may pick $\\mathrm{A}_{n,i}$ so that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle A_{n,i}h_{n,i}=\\sum_{k=1}^{l}\\sigma(\\langle Q_{n,i}h_{n,i},K_{n,i}h_{k}\\rangle)V_{n,i}h_{k}}}\\\\ {{\\displaystyle\\qquad=\\sum_{k=1}^{l}\\sigma(h_{n,i}^{5}h_{k}^{1}+Q_{n,i}^{I}\\mathcal{Z}_{n,i}\\cdot K_{n,i}^{I}I_{k}-C)e_{1}}}\\\\ {{\\displaystyle=\\sigma(1\\cdot s_{i,n^{i}})e_{1}=s_{i,n^{i}}e_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the interaction terms zero-out all terms in the sum except when $k\\,=\\,d+i d+n^{i}$ (which is the index of the token containing $s_{i,n^{i}}$ ). Similarly to as in ${\\bf{B}}_{1}$ we have $A_{n,i}h_{k}\\;=\\;0$ when $\\begin{array}{r}{k\\neq d+N d+\\sum_{p=1}^{d}(n^{p}-1)N^{d-p}+i}\\end{array}$ . Set the feed-forward layer $\\mathrm{FFN}=0$ . Then the next token embedding matrix can be written as ", "page_idx": 21}, {"type": "equation", "text": "$$\nH_{3}=\\left[\\!\\!\\begin{array}{c c c c c c c}{{x^{1}}}&{{\\dots}}&{{x^{d}}}&{{s_{1,1}}}&{{\\dots}}&{{s_{d,N}}}&{{P_{1}}}\\\\ {{0}}&{{\\dots}}&{{0}}&{{{\\bf0}_{N d}}}&{{\\dots}}&{{\\dots}}&{{{\\bf0}_{N^{d}d}}}\\\\ {{\\mathcal{T}_{1}}}&{{\\dots}}&{{\\dots}}&{{\\dots}}&{{\\dots}}&{{\\dots}}&{{\\mathcal{T}_{d+N d+N^{d}d}}}\\\\ {{1}}&{{\\dots}}&{{\\dots}}&{{\\dots}}&{{\\dots}}&{{\\dots}}&{{1}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $P_{1}\\in\\mathbb{R}^{N^{d}d}$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{1}=\\big[p_{\\{1\\}^{d},1,1}\\quad p_{\\{1\\}^{d},2,2}\\quad\\cdots\\quad p_{\\{1\\}^{d},1,d}\\quad p_{\\{2\\}\\times\\{1\\}^{d-1},1,1}\\quad\\cdots\\quad p_{\\{N\\}^{d},1,d}\\big]}\\\\ &{\\quad=\\big[s_{1,1}\\quad s_{2,1}\\quad\\dots\\quad s_{d,1}\\quad s_{1,2}\\quad\\dots\\quad s_{d,N}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we recall $p_{\\{1\\}^{d},1,1}$ denotes the first term in the base level product for for the patch $n=$ $\\{1,...,1\\}$ . Now define the next block $\\mathbf{B}_{3}\\in B(N^{d}\\frac{d}{2},1)$ . Index the corresponding attention heads as $A_{n,i}$ for $n\\in\\{1,...,N\\}^{d}$ , $1\\leq i\\leq\\frac{d}{2}$ . Define data kernels ", "page_idx": 21}, {"type": "equation", "text": "$$\nQ_{n,i}^{d a t a}=\\left[\\!\\!1\\quad\\!0\\quad\\!0\\quad\\!0\\quad\\!0\\quad\\!0\\right]\\quad\\!K_{n,i}^{d a t a}=\\left[\\!\\!1\\quad\\!0\\quad\\!0\\quad\\!0\\quad\\!0\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we choose the interaction terms so that the token containing $p_{n,1,2i-1}$ only interacts with the token containing $p_{n,1,2i}$ so that they multiply to form $p_{n,2,i}$ as shown in Figure 7. Note, in addition to computing the product, we must also subtract $p_{n,1,2i-1}$ from each corresponding token so that the output of the residual from ${\\bf{B}}_{3}$ cancels with the existing terms in the embedding matrix $H_{3}$ . Write ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{n,i}h_{n,i}=\\sum_{k=1}^{l}\\sigma(\\langle Q_{n,i}h_{n,i},K_{n,i}h_{k}\\rangle)V_{n,i}h_{k}=\\sigma(p_{n,1,2i-1}p_{n,1,2i})e_{2}=\\sigma(p_{n,2,i})e_{2}=p_{n,2,i}e_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Adding the result onto $H_{3}$ gives the intermediate embedding matrix ", "page_idx": 22}, {"type": "equation", "text": "$$\nH_{3.5}=\\left[\\!\\!\\begin{array}{c c c c c c c c}{x^{1}}&{\\dots}&{x^{d}}&{s_{1,1}}&{\\dots}&{s_{d,N}}&{P_{1}}\\\\ {0}&{\\dots}&{0}&{\\mathbf{0}_{N d}}&{\\dots}&{\\dots}&{P_{1.5}}\\\\ {\\!\\!\\!Z_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\!\\!\\!-\\!\\!-\\!\\!\\!}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $P_{1.5}\\in\\mathbb{R}^{N^{d}d}$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{1.5}=\\left[p_{\\{1\\}^{d},2,1}\\quad*\\quad p_{\\{1\\}^{d},2,2}\\quad*\\quad\\ldots\\quad p_{\\{N\\}^{d},2,\\frac{d}{2}}\\quad*\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note: $^*$ denotes a non-essential entry which can take any value. We then design our FFN network as $W_{1}x$ where ", "page_idx": 22}, {"type": "equation", "text": "$$\nW_{1}=\\left[\\begin{array}{c c c c c}{{-1}}&{{1}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{-1}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\right],\\quad b_{1}=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which when added to $H_{3.5}$ swaps the first component for the second component of each token and zeros out the second component. Then the next embedding matrix $H_{4}$ gives ", "page_idx": 22}, {"type": "equation", "text": "$$\nH_{4}=\\left[\\!\\!\\begin{array}{c c c}{\\ast}&{\\cdots}&{P_{2}}\\\\ {\\!\\!0_{d+N d+N^{d}d}}&{\\cdots}&{0}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathcal{I}_{1}}&{\\cdots}&{\\mathcal{I}_{d+N d+N^{d}d}}\\\\ {1}&{\\cdots}&{1}\\end{array}\\!\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $P_{2}=P_{1.5}\\in\\mathbb{R}^{N^{d}d}$ . More generally define the $k$ th block implementing the $k$ th level of the parallel product as $\\begin{array}{r}{B_{k+2}=\\mathcal{B}(N^{d}\\overset{\\cdot}{\\frac{d}{2^{k}}},\\mathcal{F}\\mathcal{F}\\overset{\\cdot}{\\mathcal{N}}(1))}\\end{array}$ with attention heads $A_{n,k,i}$ where $n\\in\\{1,...,N\\}^{d}$ , $\\begin{array}{r}{1\\leq i\\leq\\frac{d}{2^{k}}}\\end{array}$ . We can construct each attention head $A_{n,k,i}$ so that the token containing $p_{n,k-1,2i-1}$ only interacts with the token containing $p_{n,k-1,2i}$ to produce $p_{n,k,i}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nA_{n,k,i}h_{n,i}=\\sum_{t=1}^{l}\\sigma(\\langle Q_{n,i}h_{n,i},K_{n,i}h_{t}\\rangle)V_{n,i}h_{t}=\\sigma(p_{n,k-1,2i-1}p_{n,k-1,2i})e_{2}=p_{n,k,i}e_{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the FFN set $W_{1}$ as before, yielding ", "page_idx": 22}, {"type": "equation", "text": "$$\nH_{k+2}=\\left[\\!\\!\\begin{array}{c c c}{\\ast}&{\\cdots}&{P_{k}}\\\\ {\\mathbf{0}_{d+N d+N^{d}d}}&{\\cdots}&{0}\\\\ {\\mathcal{I}_{1}}&{\\cdots}&{\\mathcal{I}_{d+N d+N^{d}d}}\\\\ {1}&{\\cdots}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{k}=\\left[p_{\\{1\\}^{d},k,1}\\quad*\\quad\\ldots\\quad p_{\\{N\\}^{d},k,1}\\quad*\\quad\\ldots\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "At $k=\\log(d)$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nH_{\\log(d)+2}=\\left[\\mathbf{0}_{d+N d+N^{d}d}^{*}\\quad\\cdots\\quad\\quad P_{\\log(d)}\\quad\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where component Plog(d)+2 $P_{\\log(d)+2}^{d+N d+\\sum_{p=1}^{d}(n^{p}-1)N^{p}+i}=p_{n,\\log(d),1}=\\phi_{n}(x)$ as desired in (18). It simply now remains to multiply each $\\phi_{n}(x)$ by the corresponding local average $f_{n}$ and sum the result. This can be implemented with a single attention block $\\mathbf{B}_{\\log(d)+3}\\in B(N^{d},1)$ . For $n\\in\\{1,...,N\\}^{d}$ define $A_{n}$ with the data kernels ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{n}={\\left[\\!\\!\\begin{array}{l l l l l}{1}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\!\\!\\right]}\\quad K_{n}={\\left[\\!\\!\\begin{array}{l l l l l}{0}&{0}&{0}&{0}&{f_{n}}\\\\ {0}&{0}&{0}&{0}&{R}\\end{array}\\!\\!\\right]}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\|f\\|_{L^{\\infty}([0,1]^{d})}\\leq R$ and the token $h_{d+N d+\\sum_{p=1}^{d}(n^{p}-1)N^{p}+1}=h_{n,1}$ only interacts with itself so that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{A_{n}h_{n,1}=\\sum_{k=1}^{l}\\sigma(h_{n,1}^{1}\\cdot f_{n}+R+Q_{n}^{I}\\mathscr{T}_{n,1}\\cdot K_{n}^{I}I_{k}-C)e_{2}}}\\\\ &{\\qquad=\\sigma(f_{n}\\phi_{n}(x)+R)e_{2}=(f_{n}\\phi_{n}(x)+R)e_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last equality comes from $|f_{n}\\phi_{n}(x)|\\leq|f_{n}|\\leq R$ . We implement FFN to subtract $R$ via ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{1}=\\left[\\begin{array}{c c c c c}{0}&{0}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}&{-R}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so that ", "page_idx": 23}, {"type": "equation", "text": "$$\nH_{\\log(d)+4}=\\left[\\!\\!\\begin{array}{c c c}{{\\ast}}&{{\\cdots}}&{{\\cdots}}\\\\ {{\\!\\!0_{d+N d}}}&{{\\cdots}}&{{\\!\\!P_{\\log(d)+4}}}\\\\ {{\\!\\!\\!Z_{1}}}&{{\\cdots}}&{{\\!\\!\\!Z_{d+N d+N^{d}d}}}\\\\ {{\\!\\!\\!1}}&{{\\cdots}}&{{\\!\\!\\!1}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where P lno,g1( $P_{\\log(d)+4}^{n,1}=f_{n}\\phi_{n}$ for $n\\in\\{1,...,N\\}^{d}$ and is 0 elsewhere. To sum up the resulting terms we define the final transformer block $\\mathbf{B}_{\\log(d)+4}\\in B(1,0)$ containing a single attention head $A_{1}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ={\\left[\\begin{array}{l l l l l}{0}&{0}&{0}&{0}&{1}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{R}\\end{array}\\right]}\\quad K={\\left[\\begin{array}{l l l l l}{0}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\right]}\\quad V={\\left[\\begin{array}{l l l l l}{0}&{0}&{0}&{0}&{1}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which when applied to each token simply sums up the second components of all tokens. The result when applied to $h_{t}$ where $t=l$ gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{A_{1}h_{t}=\\sum_{k=1}^{l}\\sigma(\\langle Q h_{t},K h_{k}\\rangle)V h_{k}}}}\\\\ {{\\displaystyle{=\\sum_{k=1}^{l}\\sigma(h_{k}^{2}+R)e_{1}=(l R+\\sum_{n\\in\\{1,\\dots,N\\}^{d}}f_{n}\\phi_{n})e_{1}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can subtract $l R$ with the FFN as ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{1}=\\left[\\begin{array}{c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{-l R^{-}}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "yielding the final result $\\begin{array}{r}{h_{1}^{2}=\\sum_{n}f_{n}\\phi_{n}}\\end{array}$ . Applying the decoding head $D$ returns the desired result.   \nThis shows we can construct the approximation $\\hat{f}$ exactly with an $O(\\log(d))$ layer transformer. ", "page_idx": 23}, {"type": "text", "text": "E.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "With Lemma 1, we can now move on to the proof of Theorem 2. ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 2. Our goal is to approximate $f\\,:\\,{\\mathcal{M}}\\,\\rightarrow\\,\\mathbb{R}$ in $L^{\\infty}(\\mathcal{M})$ with a transformer neural network. Our construction proceeds similarly to the construction in Chen et al. [2019] with feedforward neural networks. ", "page_idx": 23}, {"type": "text", "text": "First, we will decompose $f(x)$ as a sum of terms over local neighborhoods $U_{1},...,U_{C_{\\mathcal{M}}}\\ \\subseteq\\ \\mathcal{M}$ covering $\\mathcal{M}$ . Approximations on overlapping neighborhoods containing $x$ will then be combined via a partition of unity $(\\bf P_{o U})$ $\\{\\rho_{n}\\}_{n=1}^{C_{\\mathcal{M}}}$ which subordinates $\\{U_{n}\\}_{n=1}^{C_{\\mathcal{M}}}$ , while contributions from neighborhoods not containing $x$ will be zeroed-out via an indicator $\\mathbf{1}_{U_{n}}$ . This will give us the expression ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)=\\sum_{n=1}^{C_{M}}f_{n}(x)\\mathbf{1}_{U_{n}}(x)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $f_{n}:\\mathcal{M}\\to\\mathbb{R}$ agrees with $f$ on $U_{n}$ . Next, on each local neighborhood we will project the input $\\boldsymbol{x}\\in\\mathcal{M}\\subseteq\\mathbb{R}^{D}$ to $[0,1]^{d}$ . This will give us the following local decomposition of the target function: ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)=\\sum_{n=1}^{C_{M}}\\tilde{f}_{n}\\circ\\phi_{n}(x)\\mathbf{1}_{U_{n}}(x)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\tilde{f}_{n}\\;:\\;\\mathbb{R}^{d}\\;\\rightarrow\\;\\mathbb{R}$ and $\\phi_{n}\\,:\\,\\mathcal{M}\\,\\rightarrow\\,\\mathbb{R}^{d}$ is a projection. This step is critical: we now must only approximate a low-dimensional function $\\tilde{f}_{n}$ and a simple projection $\\phi_{n}$ instead of the highdimensional function $f_{n}$ . After this, we must simply approximate each $\\tilde{f}_{n},\\phi_{n},\\mathbf{1}_{U_{n}}$ using transformer neural networks. ", "page_idx": 24}, {"type": "text", "text": "Step 1: Local Decomposition Denote the open Euclidean ball with center $c$ and radius $r$ in $\\mathbb{R}^{D}$ by $B(c,r)$ . Given a manifold $\\mathcal{M}$ satisfying Assumption 1, the collection of open balls $\\{B(c,r)\\}_{c\\in\\dot{M}}$ with some $r\\leq\\tau/4$ is an open cover of $\\mathcal{M}$ . Since $\\mathcal{M}$ is compact, there are a finite number of points, denoted by $c_{n}$ , for $n\\,=\\,1,\\dots,C_{\\mathcal{M}}$ such that $\\mathcal{M}\\subset\\cup_{n=1}^{C_{\\mathcal{M}}}B(c_{n},r)$ . We can then decompose $\\mathcal{M}$ into $C_{\\mathcal{M}}$ charts $(U_{n},\\phi_{n})_{n=1}^{C_{M}}$ where $U_{n}={\\mathcal{M}}\\cap B(c_{n},r)$ and each $\\phi_{n}:U_{n}\\to\\mathbb{R}^{d}$ is an orthogonal projection of the local neighborhood $U_{n}$ onto the local tangent space $T_{c_{n}}(\\mathcal{M})$ . Via Lemma 8, as long as $r\\leq\\tau/4$ , each $\\phi_{n}$ is a diffeomorphism between $U_{n}$ and a subset in $T_{c_{n}}(\\mathcal{M})$ . Here the number of charts $C_{\\mathcal{M}}$ is a constant depending on the complexity of the manifold $\\mathcal{M}$ . We can bound it as SAr(dM)d log(d) where SA(M) is the surface area of M [Conway and Sloane, 1988]. ", "page_idx": 24}, {"type": "text", "text": "For the open covering $\\{U_{n}\\}_{n=1}^{C_{M}}$ , there exists a smooth partition of unity $\\{\\rho_{n}\\}_{n=1}^{C_{\\mathcal{M}}}$ such that $f(x)=$ $\\textstyle\\sum_{n=1}^{C_{\\mathcal{M}}}\\rho_{n}(x)f(x)$ for $x\\,\\in\\,{\\mathcal{M}}\\,\\left[{\\mathrm{Tu},\\,2011}\\right]$ . Each $\\rho_{n}$ is supported only in its corresponding chart $U_{n}$ . We additionally include an indicator function $\\mathbf{1}_{U_{n}}$ for each neighborhood , resulting the decomposition ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)=\\sum_{n=1}^{C_{\\mathcal{M}}}\\underbrace{f(x)\\rho_{n}(x)}_{f_{n}(x)}\\mathbf{1}_{U_{n}}(x).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We then aim to approximate each term in the sum on its corresponding local neighborhood $U_{n}$ and then sum the result. Because we are only locally approximating each $\\rho_{n}$ , the indicator is critical in the approximation as it will be used to suppress contributions from $U_{n}$ such that the input $x\\notin U_{n}$ . ", "page_idx": 24}, {"type": "text", "text": "Fix a local neighborhood $U_{n}$ . We will now project the input $\\boldsymbol{x}\\in\\mathcal{M}\\subseteq\\mathbb{R}^{D}$ to the local tangent space $T_{c_{n}}(\\mathcal{M})\\subseteq[0,1]^{d}$ . We can write ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{n}(x)=(f_{n}\\circ\\phi_{n}^{-1})\\circ\\phi_{n}(x)=\\tilde{f}_{n}\\circ\\phi_{n}(x)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\tilde{f}_{n}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ and again $\\phi_{n}:\\mathcal{M}\\subseteq\\mathbb{R}^{D}\\to\\mathbb{R}^{d}$ is an orthogonal projection. Then we can write $f$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)=\\sum_{n=1}^{C_{\\mathcal{M}}}\\tilde{f}_{n}\\circ\\phi_{n}(x)\\mathbf{1}_{U_{n}}(x).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Step 2: Local Approximation In (20), we have rewritten $f$ as a sum of functions which can be approximated locally on each $U_{n}$ and indicators $\\mathbf{1}_{U_{n}}$ . We will approximate each component with transformer neural networks. Specifically, let $\\mathrm{T}_{\\tilde{f}_{n}}$ approximate $\\tilde{f}_{n}$ , $\\mathrm{T}_{\\phi_{n}}$ approximate $\\phi_{n}$ , and $\\mathrm{T}_{\\mathbf{1}_{U_{n}}}$ approximate $\\mathbf{1}_{U_{n}}$ , $1\\leq n\\leq C_{\\mathcal{M}}$ . Then we will have the overall approximation ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)\\approx\\mathrm{T}_{f}(x)=\\sum_{n=1}^{C_{\\mathcal{M}}}\\mathrm{T}_{\\tilde{f}_{n}}\\circ\\mathrm{T}_{\\phi_{n}}(x)\\mathrm{T}_{\\mathbf{1}_{U_{n}}}(x).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This yields the error decomposition ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f-\\mathbf{T}_{f}\\Vert_{L^{\\infty}(M)}=}&{\\displaystyle\\sum_{n=1}^{C_{M}}\\Vert\\tilde{f}_{n}\\circ\\phi_{n}\\mathbf{1}_{U_{n}}-\\mathbf{T}_{\\hat{f}_{n}}\\circ\\mathbf{T}_{\\phi_{n}}\\mathbf{1}_{U_{n}}\\Vert_{L^{\\infty}(M)}}\\\\ &{\\le\\displaystyle\\sum_{n=1}^{C_{M}}\\Vert\\tilde{f}_{n}\\circ\\phi_{n}\\mathbf{T}_{U_{n}}-\\mathbf{T}_{\\hat{f}_{n}}\\circ\\mathbf{T}_{\\phi_{n}}\\mathbf{T}_{U_{n}}\\Vert_{L^{\\infty}(M)}+\\Vert\\tilde{f}_{n}\\circ\\phi_{n}\\mathbf{1}_{U_{n}}-\\tilde{f}_{n}\\circ\\phi_{n}\\mathbf{T}_{U_{n}}\\Vert_{L^{\\infty}}}\\\\ &{\\le\\displaystyle\\sum_{n=1}^{C_{M}}\\Vert\\tilde{f}_{n}\\circ\\phi_{n}-\\mathbf{T}_{f_{n}}\\circ\\mathbf{T}_{\\phi_{n}}\\Vert_{L^{\\infty}(U_{n})}+E_{n,3}}\\\\ &{\\le\\displaystyle\\sum_{n=1}^{C_{M}}\\Vert\\tilde{f}_{n}\\circ\\phi_{n}-\\mathbf{T}_{\\hat{f}_{n}}\\circ\\phi_{n}\\Vert_{L^{\\infty}(U_{n})}+\\Vert\\mathbf{T}_{\\hat{f}_{n}}\\circ\\phi_{n}-\\mathbf{T}_{\\hat{f}_{n}}\\circ\\mathbf{T}_{\\phi_{n}}\\Vert_{L^{\\infty}(U_{n})}+E_{n,3}}\\\\ &{=\\displaystyle\\sum_{n=1}^{C_{M}}E_{n,1}+E_{n,2}+E_{n,3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the error terms $E_{n,1},E_{n,2},E_{n,3}$ denote ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{n,1}:=\\|\\tilde{f}_{n}\\circ\\phi_{n}-\\mathbf{T}_{\\tilde{f}_{n}}\\circ\\phi_{n}\\|_{L^{\\infty}(U_{n})}}\\\\ &{E_{n,2}:=\\|\\mathbf{T}_{\\tilde{f}_{n}}\\circ\\phi_{n}-\\mathbf{T}_{\\tilde{f}_{n}}\\circ\\mathbf{T}_{\\phi_{n}}\\|_{L^{\\infty}(U_{n})}}\\\\ &{E_{n,3}:=\\|\\tilde{f}_{n}\\circ\\phi_{n}\\mathbf{1}_{U_{n}}-\\tilde{f}_{n}\\circ\\phi_{n}\\mathbf{T}_{\\mathbf{1}_{U_{n}}}\\|_{L^{\\infty}(\\mathcal{M})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "respectively. It remains to construct transformer approximations for each term. ", "page_idx": 25}, {"type": "text", "text": "Step 2.1: Approximating $\\tilde{f}_{n}$ and Bounding $E_{n,1}$ We first handle the $E_{n,1}$ error in (21). Fix $1\\leq n\\leq C_{\\mathcal{M}}$ . We can write ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{f}_{n}\\circ\\phi_{n}-\\mathsf{T}_{\\tilde{f}_{n}}\\circ\\phi_{n}\\|_{L^{\\infty}(U_{n})}\\leq\\|\\tilde{f}_{n}-\\mathsf{T}_{\\tilde{f}_{n}}\\|_{L^{\\infty}([0,1]^{d})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "so it suffices to approximate $\\tilde{f}_{n}:[0,1]^{d}\\to\\mathbb{R}$ . Now choose the desired accuracy $\\delta_{n,1}>0$ . Via Lemma 1 we can construct an approximation such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\tilde{f}_{n}-\\mathrm{T}_{\\tilde{f}_{n}}\\|_{L^{\\infty}([0,1]^{d})}<\\delta_{n,1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This construction $\\mathrm{T}_{\\tilde{f}_{n}}$ has $O(\\log(d))$ transformer block layers, $O(1)$ feed-forward layers, $O(d\\delta_{n,1}^{-\\frac{d}{\\beta}})$ tokens, $O(1)$ embedding dimension, $O(d\\delta_{n,1}^{-\\frac{d}{\\beta}})$ attention heads per-layer, and weights with magnitude at most $O(d\\delta_{n,1}^{-\\frac{2d}{\\beta}})$ . Via the Parallelization Lemma 7 we can compute each approximation of $\\tilde{f}_{n}$ in parallel using a transformer with $O\\big(C_{\\mathcal{M}}d\\delta_{1}^{-\\frac{d}{\\beta}}\\big)$ attention heads and $O\\big(C_{\\mathcal{M}}d_{e m b d}\\big)$ FFN width, where we set $\\delta_{1}=\\delta_{1,1}=\\ldots=\\delta_{C_{\\mathcal{M}},1}$ , and $O\\big(\\log(d)\\big)$ layers. ", "page_idx": 25}, {"type": "text", "text": "Step 2.2: Approximating $\\phi_{n}$ Exactly Such That $E_{n,2}=0\\;\\;\\;\\;\\mathrm{Fix}\\;1\\leq n\\leq C_{\\mathcal{M}}$ . We must now control the $E_{n,2}$ error in (22): ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{n,2}=\\|\\mathrm{T}_{\\tilde{f}_{n}}\\circ\\phi_{n}-\\mathrm{T}_{\\tilde{f}_{n}}\\circ\\mathrm{T}_{\\phi_{n}}\\|_{L^{\\infty}(U_{n})}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We will implement $\\phi_{n}$ using a transformer block $\\mathbf{B}\\in B(d D,5)$ . We can identify the tangent space at the point $c_{n}$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{c_{n}}(\\mathcal{M})=\\operatorname{span}(v_{n,1},...,v_{n,d})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the vectors $\\boldsymbol{v}_{n,i}\\in\\mathbb{R}^{D}$ , $1\\leq i\\leq d$ , form an orthonormal basis of $T_{c_{n}}(\\mathcal{M})$ . Then we can write the projection map $\\phi_{n}$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\phi_{n}(x)=s_{n}(V_{n}^{T}(x-c_{n})+u_{n})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $s_{n}\\in(0,1]$ is a scaling factor, $u_{n}\\in\\mathbb{R}^{d}$ is a translation, and $V_{n}=[v_{n,1}\\quad...\\quad v_{n,d}]\\in\\mathbb{R}^{D\\times d}$ so that $\\phi_{n}(x)\\in[0,1]^{d}$ . Suppose we have an input hidden embedding matrix of the form ", "page_idx": 25}, {"type": "equation", "text": "$$\nH=\\left[\\!\\!\\begin{array}{c c c c}{x^{1}}&{\\ldots}&{x^{D}}&{\\mathbf{0}_{d}}\\\\ {0}&{\\ldots}&{\\ldots}&{0}\\\\ {\\mathcal{T}_{1}}&{\\ldots}&{\\ldots}&{\\mathcal{T}_{l}}\\\\ {1}&{\\ldots}&{\\ldots}&{1}\\end{array}\\!\\!\\right]\\in\\mathbb{R}^{d_{e m b d}\\times l}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We will implement the operation $v_{n,i}^{j}\\cdot(x^{j}-c_{n}^{j})$ via an attention head $\\mathbf{A}_{i,j},1\\le i\\le d,1\\le j\\le D$ . Define the data kernels ", "page_idx": 26}, {"type": "equation", "text": "$$\nQ_{i}^{d a t a}=\\left[\\!\\!\\begin{array}{c c c c c}{{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\end{array}\\!\\!\\right]\\quad K_{i}^{d a t a}=\\left[\\!\\!\\begin{array}{c c c c c}{{v_{n,i}^{j}}}&{{0}}&{{0}}&{{0}}&{{-v_{n,i}^{j}c_{n}^{j}}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{2M}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "via the Interaction Lemma 3 we can define $A_{i,j}$ so that token $h_{D+i}$ interacts with token $h_{j}$ so that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{A}_{i,j}\\big(h_{D+i}\\big)=\\sigma\\big(\\langle Q_{i}^{d a t a}h_{D+i},K_{i}^{d a t a}h_{j}\\rangle\\big)}&{}\\\\ {=\\sigma(v_{n,i}^{j}x^{j}-v_{n,i}^{j}c_{n}^{j}+2M)}&{}\\\\ {=\\sigma(v_{n,i}^{j}(x^{j}-c_{n}^{j})+2M)}&{}\\\\ {=(v_{n,i}^{j}(x^{j}-c_{n}^{j})+2M)e_{1}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and is 0 on other tokens $h_{t}\\neq h_{D+i}$ , where $M$ bounds $x\\in\\mathcal{M}$ is such that $v_{n,i}^{j}(x^{j}-c_{n}^{j})+2M\\ge0$ for all $n,i,j$ . Then the output of the multi-headed attention MHA on $h_{D+i}$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{MHA}(h_{D+i})=\\sum_{1\\leq k\\leq d\\atop1\\leq j\\leq D}A_{k,j}h_{D+i}=\\sum_{j=1}^{D}A_{i,j}h_{D+i}}}\\\\ &{=\\sum_{j=1}^{D}v_{n,i}^{j}(x^{j}-c_{n}^{j})e_{1}+2M e_{1}=\\langle v_{n,i},x-c_{n}\\rangle e_{1}+2D M e_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This yields the intermediate embedding matrix ", "page_idx": 26}, {"type": "equation", "text": "$$\nH+\\mathrm{MHA}(H)={\\left[\\begin{array}{l l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{\\langle v_{n,1},x-c_{n}\\rangle+2D M}&{\\dots}&{\\langle v_{n,d},x-c_{n}\\rangle+2D M}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{0}\\\\ {{\\mathcal{L}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathcal{L}}_{l}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can then design a five layer FFN layer to subtract $2D M$ and then add $u_{n}$ and multiply $s_{n}$ only from the embedding tokens $D+1,...,D+d$ . The first layer will simply subsract $2D M$ and then add $u_{n}$ and multiply $s_{n}$ from each token. This can be implemented by the matrix ", "page_idx": 26}, {"type": "equation", "text": "$$\nW_{1}=\\left[\\begin{array}{l l l l l}{\\displaystyle s_{n}}&{0}&{0}&{0}&{\\displaystyle-2D M s_{n}+u_{n}s_{n}}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}&{0}\\\\ {0}&{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, via the gating Lemma 4 we can construct a two-layer feed-forward network $\\mathrm{FFN}_{g a t i n g}$ such that, for a chosen $1\\leq t_{u}\\leq l$ , we have $\\mathrm{FFN}_{g a t i n g}(h_{k})=\\dot{h}_{k}$ when $k<t_{u}$ and $\\mathrm{FFN}_{g a t i n g}\\big(h_{k}\\big)$ is zero except for the last three rows for $k\\ge t_{u}$ . We can again invoke the lemma to construct another FFN layer so that for $k\\geq t_{l}\\implies{\\mathrm{FFN}}_{g a t i n g}(h_{k})=h_{k}$ and $k<t_{l}\\implies\\mathrm{FFN}_{g a t i n g}(h_{k})$ is zero except for the last three rows. So, applying Lemma 4 twice, we can construct a four-layer feed-forward network $\\mathrm{FFN}_{g a t i n g}$ such that $\\mathrm{FFN}_{g a t i n g}(h_{t})=h_{t}$ for $D+1\\leq D+d$ and is zero except for the last three rows otherwise. This yields the desired output. ", "page_idx": 26}, {"type": "equation", "text": "$$\nH^{\\prime}=\\mathbf{B}(H)={\\left[\\begin{array}{l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{s_{n}(\\langle v_{n,1},x-c_{n}\\rangle+u_{n})}&{\\dots}&{s_{n}(\\langle v_{n,d},x-c_{n}\\rangle+u_{n})}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{0}\\\\ {{\\mathcal{I}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathcal{I}}_{l}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Further, this approximation is exact, so that $E_{n,2}=0$ . We can then compute each $\\phi_{n}$ in parallel via the Parallelization Lemma 7 with a transformer having $O\\big(C_{\\mathcal{M}}d D\\big)$ attention heads and $O\\big(C_{\\mathcal{M}}d_{e m b d}\\big)$ FFN width. ", "page_idx": 26}, {"type": "text", "text": "Step 2.3: Approximating $\\mathbf{1}_{U_{n}}(x)$ and Bounding $E_{n,3}$ Again fix $1\\leq n\\leq C_{\\mathcal{M}}$ . By construction we can write ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{1}_{U_{n}}(x)={\\left\\{\\begin{array}{l l}{1}&{\\|x-c_{n}\\|_{2}^{2}<r^{2}}\\\\ {0}&{\\|x-c_{n}\\|_{2}^{2}\\geq r^{2}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Suppose we are given an input embedding matrix of the form ", "page_idx": 27}, {"type": "equation", "text": "$$\nH_{1}=\\left[\\!\\!\\begin{array}{c c c c}{x^{1}}&{\\ldots}&{x^{D}}&{\\mathbf{0}_{D}}\\\\ {0}&{\\ldots}&{\\ldots}&{0}\\\\ {\\mathcal{Z}_{1}}&{\\ldots}&{\\ldots}&{\\mathcal{Z}_{l}}\\\\ {1}&{\\ldots}&{\\ldots}&{1}\\end{array}\\!\\!\\right]\\in\\mathbb{R}^{d_{e m b d}\\times2D}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We must start by computing $\\lVert x-c_{n}\\rVert_{2}^{2}$ . Via the Addition Lemma 5 we can construct a transformer block $\\mathbf{B}_{1}\\in B(D,3)$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\nH_{2}=\\mathbf{B}_{1}(H_{1})={\\left[\\begin{array}{l l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{x^{1}-c_{n}^{1}}&{\\dots}&{x^{D}-c_{n}^{D}}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{0}\\\\ {{\\mathcal{T}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathit{I}}{\\mathcal{Z}}_{l}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now construct transfomer block $\\mathbf{B}_{2}\\in B(D,2)$ squares each term in the sum. For $1\\leq i\\leq D$ define the data kernels ", "page_idx": 27}, {"type": "equation", "text": "$$\nQ_{i}^{d a t a}=\\left[\\!\\!1\\quad\\!0\\quad\\!0\\quad\\!0\\quad\\!0\\quad\\!0\\right]\\quad\\!K_{i}^{d a t a}=\\left[\\!\\!1\\quad\\!0\\quad\\!0\\quad\\!0\\quad\\!0\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then via the Interaction Lemma 3 we can construct $\\mathbf{A}_{i}$ so that the token $h_{D+i}$ interacts with itself such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{A}_{i}(h_{D+i})=\\sigma((x^{i}-c_{n}^{i})^{2})e_{2}=(x^{i}-c_{n}^{i})^{2}e_{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and 0 otherwise. The output of the resulting multi-headed attention gives ", "page_idx": 27}, {"type": "equation", "text": "$$\nH_{2}^{\\prime}={\\left[\\begin{array}{l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{\\;\\;x^{1}-c_{n}^{1}}&{\\dots}&{\\;\\;x^{D}-c_{n}^{D}}\\\\ {0}&{\\dots}&{0}&{(x^{1}-c_{n}^{1})^{2}}&{\\dots}&{(x^{D}-c_{n}^{D})^{2}}\\\\ {\\mathbb{Z}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\;\\;I_{l}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Via Lemmas 6 and 4 we can construct $\\mathrm{FFN}\\in\\mathcal{F F N}(5)$ which replaces the first row with the second row for $h_{D+1},...,h_{2D}$ . This gives the output ", "page_idx": 27}, {"type": "equation", "text": "$$\nH_{3}={\\bf B}_{2}(H_{2})=\\left[\\begin{array}{c c c c c c}{x^{1}}&{\\dots}&{x^{D}}&{(x^{1}-c_{n}^{1})^{2}}&{\\dots}&{(x^{D}-c_{n}^{D})^{2}}\\\\ {0}&{\\dots}&{\\dots}&&{\\dots}&{0}\\\\ {{\\mathcal{T}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathcal{T}}_{l}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and it remains only to take the sum. This can be done with $\\mathbf{B}_{3}\\in B(D-1,0)$ . For $1\\leq i\\leq D-1$ define the data kernels ", "page_idx": 27}, {"type": "equation", "text": "$$\nQ_{i}^{d a t a}=\\left[\\!\\!\\begin{array}{c c c c c}{{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right]\\quad K_{i}^{d a t a}=\\left[\\!\\!\\begin{array}{c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "via the Interaction Lemma 3 we construct $\\mathbf{A}_{i}$ so that $h_{D+1}$ interacts only with $h_{D+1+i}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{A}_{i}(h_{D+1})=\\sigma(\\langle Q_{i}^{d a t a}h_{D+1},K_{i}^{d a t a}h_{D+1+i}\\rangle)e_{1}=\\sigma((x^{i+1}-c_{n}^{i+1})^{2})e_{1}=(x^{i+1}-c_{n}^{i+1})^{2}e_{1}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The output of the multi-headed attention is then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{MHA}(h_{D+1})=\\sum_{i=1}^{D-1}\\mathbf{A}_{i}(h_{D+1})=\\sum_{i=1}^{D-1}(x^{i+1}-c_{n}^{i+1})e_{1}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and 0 otherwise. This gives the intermediate output ", "page_idx": 27}, {"type": "equation", "text": "$$\nH_{3}^{\\prime}={\\left[\\begin{array}{l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{\\sum_{i=1}^{D}(x^{i}-c_{n}^{i})^{2}}&{\\dots}&{(x^{D}-c_{n}^{D})^{2}}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{0}\\\\ {{\\mathcal{T}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathcal{T}}_{l}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where in particular $h_{D+1}^{1}=\\|x-c_{n}\\|_{2}^{2}$ . It remains to approximate the indicator ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{1}_{r^{2}}(s)={\\left\\{\\begin{array}{l l}{1}&{s<r^{2}}\\\\ {0}&{s\\geq r^{2}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "2.3a: Approximating $\\mathbf{1}_{r^{2}}(s)$ Recall our goal is to approximate the function $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ in $L^{\\infty}$ . However, we cannot approximate $^{1}\\!r^{2}$ in $L^{\\infty}$ as it is a discontinuous function. To address this issue, recall the error term corresponding to the approximation of $\\mathbf{1}_{U_{n}}$ is $E_{n,3}\\,=\\,\\|{\\tilde{f}}_{n}\\circ\\phi_{n}\\mathbf{1}_{U_{n}}\\,-\\,{\\tilde{f}}_{n}\\circ$ $\\phi_{n}\\mathbf{T_{1}}_{U_{n}}\\left\\|_{L^{\\infty}(\\mathcal{M})}$ . We know on the boundary of $U_{n}$ that $\\widetilde{f}_{n}\\circ\\phi_{n}=f_{n}=f\\rho_{n}$ must be 0 as $\\rho_{n}$ is 0 and further $f\\rho_{n}$ is $\\beta$ -H\u00f6lder continuous as $f$ is $\\beta$ -H\u00f6lder continuous and $\\rho_{n}$ is smooth. This suggests the following approach. Define the approximating indicator ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{1}_{r^{2},\\Delta}(s)=\\left\\{\\!\\!\\begin{array}{l l}{1}&{s\\leq r^{2}-\\Delta}\\\\ {1-\\frac{1}{\\Delta}s+\\frac{r^{2}-\\Delta}{\\Delta}}&{r^{2}-\\Delta<s<r^{2}}\\\\ {0}&{s\\geq r^{2}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some $\\Delta>0$ . Clearly for $s\\leq r^{2}-\\Delta$ and $s\\geq r^{2}$ we have $\\hat{\\mathbf{1}}_{r^{2},\\Delta}(s)=\\mathbf{1}_{r^{2}}(s)$ . So we argue $\\tilde{f}_{n}$ is small for $x\\in{\\mathcal{M}}$ in the range $\\mathcal{K}_{n}=\\{x\\in\\mathcal{M}:r^{2}-\\Delta<\\|x-c_{n}\\|_{2}^{2}<r^{2}\\}$ . This is handled by Lemma 8 in Chen et al. [2022] which gives us the following bound on $E_{n,3}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nE_{n,3}\\leq\\frac{c}{r}\\Delta\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some constant $c$ . So it suffices to make $\\Delta$ small. It then remains to implement $\\hat{1}_{r^{2},\\Delta}$ . We will do so using a feed-forward layer $\\mathrm{FFN}\\in\\mathcal{F F N}(O(\\log(\\frac{1}{\\Delta})))$ ). Note that we can write the target exactly as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{1}_{r^{2},\\Delta}(s)=\\sigma\\left(1-\\sigma\\left(\\frac{1}{\\Delta}(s-(r^{2}-\\Delta))\\right)\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "while is realized by a two-layer ReLU network. However, the weight $\\frac{1}{\\Delta}$ will scale unboundedly with the desired accuracy $\\epsilon$ . So we must deepen the FFN to utilize a logarithmic number of layers $\\begin{array}{r}{O(\\log(\\frac{1}{\\Delta}))}\\end{array}$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{\\mathrm{\\boldmath~\\cal~l~}}_{r^{2},\\Delta}(s)=\\sigma\\left(1-\\sigma\\left(\\left(\\frac{1}{\\Delta}\\right)^{\\frac{1}{\\log(\\frac{1}{\\Delta})}}\\circ\\sigma...\\circ\\sigma\\left(\\left(\\frac{1}{\\Delta}\\right)^{\\frac{1}{\\log(\\frac{1}{\\Delta})}}\\left(s-(r^{2}-\\Delta)\\right)\\right)...\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which prevents the weights $\\left(\\frac{1}{\\Delta}\\right)^{\\frac{1}{\\log(\\frac{1}{\\Delta})}}$ from blowing up. We can then approximate each $\\mathbf{1}_{U_{n}}$ in parallel using a transformer with $O(C_{\\mathcal{M}}D)$ and three layers. ", "page_idx": 28}, {"type": "text", "text": "Step 3: Bringing it all together Recall our goal is to approximate $f$ on $\\mathcal{M}$ by writing $f$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(x)=\\sum_{n=1}^{C_{\\mathcal{M}}}\\tilde{f}_{n}\\circ\\phi_{n}(x)\\mathbf{1}_{U_{n}}(x).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We argued we can successfully approximate ", "page_idx": 28}, {"type": "text", "text": "1. $\\tilde{f}_{n}$ up to $\\delta_{n,1}$ accuracy in $L^{\\infty}$ via a transformer neural network with $O(d\\delta_{n,1}^{-\\frac{d}{\\beta}})$ width and $O(\\log(d))$ depth. All approximations can be computed in parallel via a transformer with $O\\big(C_{\\mathcal{M}}d\\delta_{n,1}^{-\\frac{d}{\\beta}}\\big)$ attention heads and $O\\big(\\log(d)\\big)$ layers.   \n2. Each $\\phi_{n}$ exactly via a transformer neural network with $O(d D)$ width and constant depth. All the $\\phi_{n}$ can be computed in parallel via a transformer with $O\\big(C_{\\mathcal{M}}d D\\big)$ heads and constant depth.   \n3. Each $\\mathbf{1}_{U_{n}}$ up to $\\delta_{n,3}$ accuracy in $L^{\\infty}$ via a transformer with $O(D)$ width and a feed-forward layer with $\\begin{array}{r}{O\\big(\\log(\\frac{1}{\\Delta})\\big)}\\end{array}$ depth. All $\\mathbf{1}_{U_{n}}$ can be approximated in parallel via a transformer with $O\\big(C_{\\mathcal{M}}\\dot{D}\\big)$ attention heads and a constant number of transformer blocks. ", "page_idx": 28}, {"type": "text", "text": "Via the Parallelization Lemma 7 we can compute the approximations of $\\mathbf{1}_{U_{n}}$ and $\\tilde{f}_{n}\\circ\\phi_{n}$ in parallel via a transformer with $O\\big(C_{\\mathcal{M}}(d\\delta_{n,1}^{-d}+d D)\\big)$ attention heads, $O\\big(C_{\\mathcal{M}}d_{e m b d}\\big)$ FFN width, and $O\\big(\\log(d)\\big)$ ", "page_idx": 28}, {"type": "text", "text": "transformer blocks. As a result will have the embedding matrix ", "page_idx": 29}, {"type": "equation", "text": "$$\nH=\\left[\\begin{array}{c c c c c c c c c c c}{x^{1}}&{\\dots}&{x^{D}}&{c_{f_{1}}}&{\\dots}&{c_{f_{C_{M}}}}&{c_{U_{1}}}&{\\dots}&{c_{U_{C_{M}}}}&{*}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{0}\\\\ {\\mathcal{T}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\mathcal{T}_{L}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where each $c_{f_{n}}=\\tilde{f}_{n}\\circ\\phi_{n}(x)$ and $c_{U_{n}}=\\mathbf{1}_{U_{n}}(x)$ . We implement the final sum via two transformer blocks ${\\bf B}_{1},{\\bf B}_{2}$ . First define $\\mathbf{B}_{1}\\in B(C_{\\mathcal{M}},1)$ with attention heads $A_{i}$ , $1\\leq i\\leq C_{\\mathcal{M}}$ with data kernels ", "page_idx": 29}, {"type": "equation", "text": "$$\nQ_{i}^{d a t a}=\\left[\\begin{array}{c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{-1}}&{{0}}&{{0}}&{{0}}&{{M}}\\end{array}\\right]\\quad K_{i}^{d a t a}=\\left[\\begin{array}{c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with interaction kernels chosen so that $h_{D+i}$ only interacts with $h_{D+C,M+i}$ under $A_{i}$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{i}(h_{D+i})=\\sigma(c_{f_{i}}c_{U_{i}}-c_{f_{i}}+M)e_{1}=(c_{f_{i}}c_{U_{i}}-c_{f_{i}}+M)e_{1}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can then design a two-layer FFN to subtract $M$ from columns $D+1,...,D+C_{\\mathcal{M}}$ yielding the output embedding matrix ", "page_idx": 29}, {"type": "equation", "text": "$$\nH=\\left[\\begin{array}{c c c c c c c c}{x^{1}}&{\\ldots}&{x^{D}}&{c_{f_{1}C_{U_{1}}}}&{\\ldots}&{c_{f_{C_{M}}}c_{U_{C_{M}}}}&{*}\\\\ {0}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{0}\\\\ {{\\mathcal{Z}}_{1}}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{{\\mathcal{Z}}_{L}}\\\\ {1}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}&{1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can sum up the result with another transformer block $\\mathbf{B}_{2}\\in B(C_{\\mathcal{M}},2)$ and storing the desired result in the first component of the last token $t=l$ . Applying the decoding layer $D$ yields the desired result. ", "page_idx": 29}, {"type": "text", "text": "It simply remains to control the errors. Recall we have the error bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|f-\\mathrm{T}_{f}\\|_{L^{\\infty}(\\mathcal{M})}\\le\\displaystyle\\sum_{n=1}^{C_{\\mathcal{M}}}E_{n,1}+E_{n,2}+E_{n,3}}}\\\\ &{\\le\\displaystyle\\sum_{n=1}^{C_{\\mathcal{M}}}\\delta_{n,1}+\\delta_{n,3}=C_{\\mathcal{M}}\\delta_{n,1}+C_{\\mathcal{M}}\\delta_{n,3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then for a target accuracy $\\epsilon$ we choose $\\begin{array}{r}{\\delta_{n,1}=\\frac{\\epsilon}{2C_{\\mathcal{M}}}}\\end{array}$ and $\\begin{array}{r}{\\delta_{n,3}=\\frac{\\epsilon}{2C_{\\mathcal{M}}}}\\end{array}$ so that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|f-\\mathsf{T}_{f}\\|_{L^{\\infty}(M)}\\leq C_{\\mathcal{M}}\\delta_{n,1}+C_{\\mathcal{M}}\\delta_{n,3}=C_{\\mathcal{M}}\\frac{\\epsilon}{2C_{\\mathcal{M}}}+C_{\\mathcal{M}}\\frac{\\epsilon}{2C_{\\mathcal{M}}}=\\epsilon\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This completes the proof of Theorem 2. ", "page_idx": 29}, {"type": "text", "text": "The above result concludes our approximation theory results. To summarize, we first showed a (sufficiently smooth) function $f:[0,1]^{d}\\rightarrow\\mathbb{R}$ can be approximated up to $\\epsilon$ accuracy in $L^{\\infty}$ with a transformer neural network with $O(d\\epsilon^{-d})$ width and $O(\\log(d))$ depth in Lemma 1. We then used this construction to show a function $f:M\\rightarrow\\mathbb{R}$ on a compact manifold $\\mathcal{M}$ with intrinsic dimension $d$ can be approximated up to $\\epsilon$ accuracy in $L_{1}$ with a transformer neural network with width $O(\\operatorname*{max}(d D,d\\epsilon^{-\\tilde{d}}))$ and depth $O(\\log(d))$ . ", "page_idx": 29}, {"type": "text", "text": "F Proof of Transformer Generalization Theory ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We next prove Theorem 1 in Appendix F.1. Much of this follows the same argument as in Chen et al. [2022] which focused on feedforward neural networks. The main novelty of this paper is a bound on the covering number of $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ given in Lemma 2, which is proved in Appendix F.2. ", "page_idx": 29}, {"type": "text", "text": "F.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . Suppose we have $x_{1},...,x_{n}\\sim Q$ as i.i.d. training samples drawn from $Q$ on the manifold $\\mathcal{M}$ and their evaluations $f(x_{1}),...,f(x_{n})$ . Set the approximating function class as $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ . We define the transformer empirical risk minimizer ${\\hat{\\mathrm{T}}}_{n}$ in (1) as the transformer network which minimizes the empirical $L^{2}$ training loss. Our goal is to bound the squared generalization error in (6). ", "page_idx": 30}, {"type": "text", "text": "We first rewrite the squared generalization error by adding and subtracting (twice) the training objective where we note a factor of two is used to accelerate convergence of the statistical error. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\displaystyle\\int_{M}(\\hat{\\mathbf{T}}_{n}(x)-f(x))^{2}d Q}\\\\ &{=2\\mathbb{E}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\mathbf{T}}_{n}(x_{i})-f(x_{i}))^{2}+\\mathbb{E}\\displaystyle\\int_{M}(\\hat{\\mathbf{T}}_{n}(x)-f(x))^{2}d Q-2\\mathbb{E}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\mathbf{T}}_{n}(x_{i})-f(x_{i}))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The first (bias) term in (24) can be controlled via Theorem 2: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\mathbf{T}}_{n}(x_{i})-f(x_{i}))^{2}=\\mathbb{E}\\operatorname*{inf}_{\\tau\\in\\mathcal{T}}\\frac{1}{n}\\sum_{i=1}^{n}(\\mathrm{T}(x_{i})-f(x_{i}))^{2}}}\\\\ &{\\le\\operatorname*{inf}_{\\tau\\in\\mathcal{T}}\\mathbb{E}\\frac{1}{n}\\sum_{i=1}^{n}(\\mathrm{T}(x_{i})-f(x_{i}))^{2}}\\\\ &{=\\operatorname*{inf}_{\\tau\\in\\mathcal{T}}\\int_{M}\\big(\\mathrm{T}(x)-f(x)\\big)^{2}d Q}\\\\ &{\\le\\operatorname*{inf}_{\\tau\\in\\mathcal{T}}\\int_{M}\\|\\mathrm{T}-f\\|_{L^{\\infty}(M)}^{2}d Q<\\epsilon^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last line follows from Theorem 2. Note, we can pass the expectation inside the infimum via Jensen\u2019s inequality. Set $d_{T}^{2}(x)=(\\mathrm{T}(x)-f(x))^{2}$ . ", "page_idx": 30}, {"type": "text", "text": "It remains to control the last two (variance) terms in (24). We will do so by controlling the covering number of the approximating function class $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ . Via Lemma 6 from Chen et al. [2022] we have the bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\int_{\\mathcal{M}}(\\hat{\\mathrm{T}}_{n}(x)-f(x))^{2}d Q-2\\mathbb{E}\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\mathrm{T}}_{n}(x_{i})-f(x_{i}))^{2}}\\\\ {\\displaystyle\\leq\\operatorname*{inf}_{\\delta>0}\\left[\\frac{104R^{2}}{3n}\\log\\mathcal{N}\\left(\\frac{\\delta}{4R},\\mathcal{T},\\|\\cdot\\|_{\\infty}\\right)+(4+\\frac{1}{2R})\\delta\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{N}(\\frac{\\delta}{4R},\\mathcal{T},\\Vert\\cdot\\Vert_{\\infty})}\\end{array}$ denotes of the covering number of the network class $\\tau$ under the $L^{\\infty}$ norm. Our goal is now to bound this covering number of $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ . We do so with the following lemma: ", "page_idx": 30}, {"type": "text", "text": "Lemma 2. Consider a transformer neural network class $\\mathcal{T}=\\mathcal{T}(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ with input $x\\in\\mathbb{R}^{D}$ satisfying $\\|x\\|_{\\infty}\\leq M$ .. Let $\\delta>0$ . Then ", "page_idx": 30}, {"type": "equation", "text": "$$\nV(\\delta,T,\\|\\cdot\\|_{\\infty})\\leq\\left(\\frac{2^{L_{T}^{2}+1}L_{\\mathrm{FFN}}M^{3L_{T}}d_{e m b d}^{18L_{T}^{2}}w_{\\mathrm{FFN}}^{18L_{T}^{2}L_{\\mathrm{FFN}}}\\kappa^{6L_{T}^{2}L_{\\mathrm{FFN}}}m^{L_{T}^{2}}l^{L_{T}^{2}}}{\\delta}\\right)^{4d_{e m b d}^{2}w_{\\mathrm{FN}}^{2}D(m+L_{\\mathrm{FN}})L_{T}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma 2 is proved in Appendix F.2. ", "page_idx": 30}, {"type": "text", "text": "With the covering number of $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ in hand we may now apply (25) to obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}\\displaystyle\\int_{M}(\\hat{\\mathrm{T}}_{n}(x)-f(x))^{2}d Q-2\\mathbb{E}\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\mathrm{T}}_{n}(x_{i})-f(x_{i}))^{2}}\\\\ &{\\le\\displaystyle\\operatorname*{inf}_{\\delta>0}\\left[\\frac{104R^{2}}{3n}\\log N\\left(\\frac{\\delta}{4R},T,\\|\\cdot\\|_{\\infty}\\right)+\\left(4+\\frac{1}{2R}\\right)\\delta\\right]}\\\\ &{\\le\\displaystyle\\frac{104R^{2}}{3n}\\log N\\left(\\frac{1}{4R n},T,\\|\\cdot\\|_{\\infty}\\right)+\\left(4+\\frac{1}{2R}\\right)\\frac{1}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we set $\\begin{array}{r}{\\delta=\\frac{1}{n}}\\end{array}$ . This yields ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{V}\\left(\\frac{\\delta}{4R n},\\mathcal{T},\\|\\cdot\\|_{\\infty}\\right)\\leq\\log\\left((2^{L_{T}^{2}+1}L_{\\mathrm{FFN}}M^{3L_{T}}d_{e m b d}^{18L_{T}^{2}}\\mathrm{IrFN}_{K}6L_{T}^{2}L_{\\mathrm{FFN}}m L_{T}^{2}L_{T}^{2}n)^{4d_{e m b d}^{2}w_{\\mathrm{FN}}^{2}D(m)}}\\\\ &{\\qquad\\qquad\\qquad\\leq4d_{e m b d}^{2}w_{\\mathrm{FFN}}^{2}D(m+L_{\\mathrm{FFN}})L_{T}(5L_{T}^{2}L_{\\mathrm{FFN}}\\log(2M L_{\\mathrm{FFN}}d_{e m b d}\\kappa m l n))}\\\\ &{\\qquad\\qquad\\qquad\\leq20\\log(2M L_{\\mathrm{FFN}}d_{e m b d}w_{\\mathrm{FFN}}\\kappa m l n)D d_{e m b d}^{2}w_{\\mathrm{FFN}}^{2}m L_{T}^{3}L_{\\mathrm{FFN}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Recall for target accuracy $\\epsilon>0$ we can choose $L_{T}=\\log(d)$ , $L_{\\mathrm{FFN}}=\\log(\\epsilon^{-1})$ , $w_{\\mathrm{FFN}}\\,\\leq\\,4C_{\\mathcal{M}}$ , $d_{e m b d}=5$ , $l\\leq C_{\\mathcal{M}}d\\epsilon^{-\\frac{d}{\\beta}}$ , $m\\leq C_{\\mathcal{M}}d\\epsilon^{-\\frac{d}{\\beta}}$ , $\\kappa\\leq(M C_{M}d\\epsilon^{-\\frac{d}{\\beta}})^{2},l\\leq C_{\\mathcal{M}}d\\epsilon^{-\\frac{d}{\\beta}}$ . This simplifies the above to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\frac{\\delta}{4R},\\mathcal{T},\\|\\cdot\\|_{\\infty})\\leq900\\big(\\log(d)^{2}\\log(60d^{3}\\epsilon^{-4\\frac{d}{\\beta}}n)\\big)C_{M}^{3}D d\\epsilon^{-\\frac{d}{\\beta}}\\leq\\tilde{O}\\big(D d^{2}\\epsilon^{-\\frac{d}{\\beta}}\\big)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\tilde{O}$ hides log terms and constants. Then we can bound the variance term as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\int_{\\mathcal{M}}(\\hat{\\mathrm{T}}_{n}(x)-f(x))^{2}d Q-2\\mathbb{E}\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\mathrm{T}}_{n}(x_{i})-f(x_{i}))^{2}\\leq\\tilde{O}\\big(\\frac{D d^{2}\\epsilon^{-\\frac{d}{\\beta}}}{n}\\big)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Putting together the bounds on the bias and variance yields a bound on the empirical risk: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\int_{\\mathcal{M}}(\\hat{\\mathrm{T}}_{n}(x)-f(x))^{2}d Q\\leq\\tilde{O}\\big(\\epsilon^{2}+\\frac{D d^{2}\\epsilon^{-\\frac{d}{\\beta}}}{n}\\big)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and it simply remains to pick $\\epsilon$ to balance the error terms. We do this by choosing $\\epsilon$ such that $\\begin{array}{r}{\\epsilon^{2}=\\frac{\\epsilon^{-\\frac{d}{\\beta}}}{n}}\\end{array}$ yielding \u03f5 = n\u22122\u03b2+d . This gives our final bound on the expected empirical risk: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\int_{\\mathcal{M}}(\\hat{\\mathrm{T}}_{n}(x)-f(x))^{2}d Q-2\\mathbb{E}\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\mathrm{T}}_{n}(x_{i})-f(x_{i}))^{2}\\leq\\tilde{O}\\big(D d^{2}n^{-\\frac{2\\beta}{2\\beta+d}}\\big)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as desired. ", "page_idx": 31}, {"type": "text", "text": "F.2 Proof of Lemma 2 about the Transformer Covering Number ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Lemma 2. The key is to bound the difference in $\\|\\cdot\\|_{\\infty}$ between two transformers $\\mathbf{T},\\mathbf{T^{\\prime}}\\in$ $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ . Set $\\eta>0$ and choose T, $\\mathrm{T^{\\prime}}$ so that $\\lVert\\theta-\\theta^{\\prime}\\rVert_{\\infty}<\\eta$ . In other words, the weight parameters in $\\mathrm{T}$ and $\\mathrm{T^{\\prime}}$ differ at most by $\\eta$ . Fix $x\\in[0,1]^{D}$ . Recall the decoder $D:\\mathbb{R}^{d_{e m b d}\\times l}\\rightarrow\\mathbb{R}$ is fixed to output the first element of the first row. We compute ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathrm{T}(x)-\\mathrm{T}^{\\prime}(x)\\right|=\\big|\\mathrm{D}\\circ\\mathbf{B}_{L}\\circ\\dots\\circ\\mathbf{B}_{1}\\circ(\\mathrm{PE}+\\mathrm{E}(x))-\\mathrm{D}^{\\prime}\\circ\\mathbf{B}_{L}^{\\prime}\\circ\\dots\\circ\\mathbf{B}_{1}^{\\prime}\\circ(\\mathrm{PE}+\\mathrm{E}^{\\prime}(x))\\big|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathbf{B}_{L}\\circ\\dots\\circ\\mathbf{B}_{1}\\circ(\\mathrm{PE}+\\mathrm{E}(x))-\\mathbf{B}_{L}^{\\prime}\\circ\\dots\\circ\\mathbf{B}_{1}^{\\prime}\\circ(\\mathrm{PE}+\\mathrm{E}^{\\prime}(x))\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To handle this term, we write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{B}_{L}\\circ\\dots\\circ\\mathbf{B}_{1}\\circ(\\mathrm{PE}+\\mathrm{E}(x))-\\mathbf{B}_{L}^{\\prime}\\circ\\dots\\circ\\mathbf{B}_{1}^{\\prime}\\circ(\\mathrm{PE}+\\mathrm{E}^{\\prime}(x))\\|_{\\infty}}\\\\ &{\\leq\\|\\mathbf{B}_{L}\\circ\\dots\\circ\\mathbf{B}_{1}\\circ(\\mathrm{PE}+\\mathrm{E}(x))-\\mathbf{B}_{L}\\circ\\dots\\circ\\mathbf{B}_{1}\\circ(\\mathrm{PE}+\\mathrm{E}^{\\prime}(x))\\|_{\\infty}}\\\\ &{+\\,\\|\\mathbf{B}_{L}\\circ\\dots\\circ\\mathbf{B}_{1}\\circ(\\mathrm{PE}+\\mathrm{E}^{\\prime}(x))-\\mathbf{B}_{L}^{\\prime}\\circ\\dots\\circ\\mathbf{B}_{1}^{\\prime}\\circ(\\mathrm{PE}+\\mathrm{E}^{\\prime}(x))\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We will first bound the second term. ", "page_idx": 31}, {"type": "text", "text": "Consider two multi-headed attention layers $\\mathrm{MHA_{1}}$ , $\\mathrm{MHA_{2}}\\in\\mathcal{M H A}(m)$ with attention heads $A_{j}^{i}$ , $i\\in\\{1,2\\}$ , $1\\leq j\\leq m$ . For $H\\in[0,M]^{d_{e m b d}\\times l}$ we compute ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\mathrm{MHA}}_{1}(H)-{\\mathrm{MHA}}_{2}(H)\\|_{\\infty}=\\|\\sum_{j=1}^{m}A_{j}^{1}(H)-\\displaystyle\\sum_{j=1}^{m}A_{j}^{2}(H)\\|_{\\infty}=\\|\\sum_{j=1}^{m}\\left[A_{j}^{1}(H)-A_{j}^{2}(H)\\right]\\|_{\\infty}}\\\\ {\\displaystyle\\leq\\sum_{j=1}^{m}\\|A_{j}^{1}(H)-A_{j}^{2}(H)\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Consider two attention heads $A_{1},A_{2}$ with weight parameters $Q_{i},K_{i},V_{i},\\,i\\,\\in\\,\\{1,2\\}$ . For $H\\ \\in$ $[-M,M]^{d_{e m b d}\\times l}$ , we compute ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l}{\\displaystyle\\|A_{1}(H)-A_{2}(H)\\|\\infty\\leq\\displaystyle\\operatorname*{max}_{1\\leq j\\leq l}\\|A_{1}(h_{j})-A_{2}(h_{j})\\|_{\\infty}}\\\\ &{\\displaystyle=\\operatorname*{max}_{1\\leq j\\leq l}\\|\\sum_{k=1}^{l}\\sigma(\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle)V_{1}h_{k}-\\sum_{k=1}^{l}\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{2}h_{k}\\|_{\\infty}}\\\\ &{\\displaystyle\\leq\\operatorname*{max}_{1\\leq j\\leq l}\\sum_{k=1}^{l}\\|\\sigma(\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle)V_{1}h_{k}-\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{2}h_{k}\\|_{\\infty}}\\\\ &{\\displaystyle\\leq\\operatorname*{max}_{1\\leq j\\leq l}\\sum_{k=1}^{l}\\|\\sigma(\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle)V_{1}h_{k}-\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\displaystyle+\\operatorname*{max}_{1\\leq j\\leq l}\\sum_{k=1}^{l}\\|\\sigma(\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle)V_{1}h_{k}-\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\displaystyle+\\|\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{1}h_{k}-\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{2}h_{k}\\|_{\\infty}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality comes from adding and subtracting the term $\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle V_{1}h_{k}$ . We bound the first term via ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\sigma(\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle)V_{1}h_{k}-\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{1}h_{k}\\|_{\\infty}}\\\\ &{=\\langle\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle-\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle\\|\\|V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\leq[|\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle-\\langle Q_{1}h_{j},K_{2}h_{k}\\rangle|+|\\langle Q_{1}h_{j},K_{2}h_{k}\\rangle-\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle|]\\|V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\leq\\big[|\\langle Q_{1}h_{j},K_{1}h_{k}-K_{2}h_{k}\\rangle|+|\\langle Q_{1}h_{j}-Q_{2}h_{j},K_{2}h_{k}\\rangle|\\big]\\|V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\leq\\big[|\\|Q_{1}h_{j}\\|_{2}\\|K_{1}h_{k}-K_{2}h_{k}\\|_{2}+\\|Q_{1}h_{j}-Q_{2}h_{j}\\|_{2}\\|K_{2}h_{k}\\|_{2}\\big]\\|V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\leq\\big[|\\|Q_{1}\\|_{1}\\|h_{j}\\|_{\\infty}\\|K_{1}-K_{2}\\|_{1}\\|h_{k}\\|_{\\infty}+\\|Q_{1}-Q_{2}\\|_{1}\\|h_{j}\\|_{\\infty}\\|K_{2}\\|_{1}\\|h_{k}\\|_{\\infty}\\big]\\|V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\leq\\big[2\\kappa^{2}d_{e m b d}^{4}M^{2}\\eta\\big]\\|V_{1}h_{k}\\|_{\\infty}}\\\\ &{\\leq2\\kappa^{3}d_{e m b d}^{6}M^{3}\\eta}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we note $\\|A x\\|_{2}\\leq\\|A x\\|_{1}\\leq\\|A\\|_{1}\\|x\\|_{\\infty}$ for $A\\in\\mathbb{R}^{d_{e m b d}\\times d_{e m b d}},x\\in\\mathbb{R}^{d_{e m b d}}$ and $\\|A\\|_{1}\\leq$ $d_{e m b d}^{2}\\|A\\|_{\\infty}$ . This bounds the first term. ", "page_idx": 32}, {"type": "text", "text": "The second term can be bounded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{1}h_{k}-\\sigma(\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle)V_{2}h_{k}\\|_{\\infty}}\\\\ &{=\\|V_{1}-V_{2}\\|_{1}\\|\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle h_{k}\\|_{\\infty}}\\\\ &{\\le\\!d_{e m b d}^{2}\\kappa\\eta\\|\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle h_{k}\\|_{\\infty}}\\\\ &{\\le\\!d_{e m b d}^{2}\\kappa\\eta\\|Q_{2}h_{j}\\|_{2}\\|K_{2}h_{k}\\|_{2}\\|h_{k}\\|_{\\infty}}\\\\ &{\\le\\!d_{e m b d}^{6}\\kappa^{3}M^{3}\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus we can bound the attention difference $\\|A_{1}(H)-A_{2}(H)\\|_{\\infty}$ as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|A_{1}(H)-A_{2}(H)\\|_{\\infty}\\leq\\operatorname*{max}_{1\\leq j\\leq l}\\sum_{k=1}^{l}\\|\\langle Q_{1}h_{j},K_{1}h_{k}\\rangle V_{1}h_{k}-\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle V_{1}h_{k}\\|_{\\infty}}\\\\ {\\displaystyle\\quad+\\left\\|\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle V_{1}h_{k}-\\langle Q_{2}h_{j},K_{2}h_{k}\\rangle V_{2}h_{k}\\right\\|_{\\infty}}\\\\ {\\displaystyle\\quad\\leq\\operatorname*{max}_{1\\leq j\\leq l}\\sum_{k=1}^{l}3\\kappa^{3}d_{e m b d}^{6}M^{3}\\eta=3\\kappa^{3}d_{e m b d}^{6}M^{3}l\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can also now bound the multi-headed difference $\\|\\mathbf{MHA}_{1}(H)-\\mathbf{MHA}_{2}(H)\\|_{\\infty}$ as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{MHA_{1}}(H)-\\mathrm{MHA_{2}}(H)\\|_{\\infty}\\leq\\sum_{j=1}^{m}\\|A_{j}^{1}(H)-A_{j}^{2}(H)\\|_{\\infty}\\leq3\\kappa^{3}d_{e m b d}^{6}M^{3}\\eta=3\\kappa^{3}d_{e m b d}^{6}M^{3}m l\\eta.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "A bound on the first residual layer in the transformer block immediately follows via ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|(H+\\boldsymbol{\\mathrm{MHA}}_{1}(H))-(H+\\boldsymbol{\\mathrm{MHA}}_{2}(H))\\|_{\\infty}=\\|\\boldsymbol{\\mathrm{MHA}}_{1}(H)-\\boldsymbol{\\mathrm{MHA}}_{2}(H)\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To bound the difference in the output of the FFN layer in the transformer block we have for $f_{1},f_{2}\\in$ $\\mathrm{FFN}({L}_{\\mathrm{FFN}})$ and input $H\\in[0,M^{\\prime}]^{d_{e m b d}\\times l}$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|f_{1}(H)-f_{2}(H)\\|_{\\infty}\\leq L_{\\mathrm{FFN}}(d_{e m b d}M^{\\prime}+2)(\\kappa d_{e m b d})^{L_{\\mathrm{FFN}}-1}\\eta\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which also bounds the second residual layer of the transformer block. Then to bound the difference of two transformer blocks $\\mathbf{B}_{1},\\mathbf{B}_{2}\\in B(m,L_{\\mathrm{FFN}})$ write, for $H\\in[0,M]^{d_{e m b d}\\times l}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{B}_{1}(H)-\\mathbf{B}_{2}(H)\\lVert\\infty=\\lVert\\left(H+\\mathrm{MHA}_{1}(H)+\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))\\right)}\\\\ {-\\left(H+\\mathrm{MHA}_{2}(H)+\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))\\right)\\rVert_{\\infty}}&{}\\\\ {=\\lVert\\mathrm{MHA}_{1}(H)+\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))-\\mathrm{MHA}_{2}(H)+\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))}&{}\\\\ {\\leq\\lVert\\mathrm{MHA}_{1}(H)-\\mathrm{MHA}_{2}(H)\\rVert_{\\infty}}&{}\\\\ {+\\lVert\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))-\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))\\rVert_{\\infty}}&{}\\\\ {\\leq3\\kappa^{3}d_{e m b d}^{6}M^{3}m l\\eta+\\lVert\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))-\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))\\rVert_{\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we can bound the first term via analysis above. For the second term write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))-\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))\\|_{\\infty}}\\\\ &{\\leq\\|\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))-\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{2}(H))\\|_{\\infty}+\\|\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))-\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{2}(H))\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To handle the first term we must bound he Lipschitz constant of $\\mathrm{FFN_{1}}$ . Let $H,H^{\\prime}\\in\\mathbb{R}^{d_{e m b d}\\times l}$ . Write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert\\mathrm{FFN}_{1}(H)-\\mathrm{FFN}_{1}(H^{\\prime})\\Vert_{\\infty}=\\underset{1\\leq j\\leq l}{\\operatorname*{max}}\\Vert\\mathrm{FFN}_{1}(h_{j})-\\mathrm{FFN}_{1}(h_{j}^{\\prime})\\Vert_{\\infty}}&{}\\\\ {=\\underset{1\\leq j\\leq l}{\\operatorname*{max}}\\Vert W_{L_{\\mathrm{FrN}}}\\sigma(W_{L_{\\mathrm{FrN}}-1}...\\sigma(W_{1}h_{j}+b_{1})...+b_{L_{\\mathrm{FrN}}-1})+b_{L_{\\mathrm{FrN}}}}\\\\ {-\\left.W_{L_{\\mathrm{FrN}}}\\sigma(W_{L_{\\mathrm{FrN}}-1}...\\sigma(W_{1}h_{j}^{\\prime}+b_{1})...+b_{L_{\\mathrm{FrN}}-1})-b_{L_{\\mathrm{FrN}}}\\right\\Vert_{\\infty}}&{}\\\\ {\\leq\\underset{1\\leq j\\leq l}{\\operatorname*{max}}\\Vert W_{L_{\\mathrm{FrN}}}\\Vert_{1}\\Vert W_{L_{\\mathrm{FrN}}-1}...\\sigma(W_{1}h_{j}+b_{1})...+b_{L_{\\mathrm{FrN}}-1}}&{}\\\\ {-\\left.W_{L_{\\mathrm{FrN}}-1}...\\sigma(W_{1}h_{j}^{\\prime}+b_{1})...+b_{L_{\\mathrm{FrN}}-1}\\right\\Vert_{\\infty}}&{}\\\\ {\\leq\\underset{1\\leq j\\leq l}{\\operatorname*{max}}\\left[\\underset{i=1}{\\overset{L_{\\mathrm{FrN}}}{\\prod}}\\Vert W_{i}\\Vert_{1}\\right]\\Vert h_{j}-h_{j}^{\\prime}\\Vert_{\\infty}\\leq w_{\\mathrm{FrN}}^{2L_{\\mathrm{FrN}}}k^{L_{\\mathrm{FrN}}}\\Vert H-H^{\\prime}\\Vert_{\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we again use that $\\|W x\\|_{\\infty}\\leq\\|W\\|_{1}\\|x\\|_{\\infty}$ and that $\\sigma(\\cdot)$ is 1-Lipschitz. Applying this to the first term from above we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))-\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{2}(H))\\lVert_{\\infty}\\leq w_{\\mathrm{FFN}}^{2\\mathrm{LFN}}\\kappa^{L_{\\mathrm{FN}}}\\lVert\\mathrm{MHA}_{1}(H)-\\mathrm{MHA}_{2}(H)\\rVert_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq w_{\\mathrm{FFN}}^{2\\mathrm{LFN}}\\kappa^{L_{\\mathrm{FN}}}3\\kappa^{3}d_{e=b d}^{6}M^{3}m l\\eta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=3\\kappa^{3+L_{\\mathrm{FN}}}w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}d_{e=b d}^{6}M^{3}m l\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can bound $\\big\\|\\mathrm{FFN}_{2}(H+\\boldsymbol{\\mathrm{MHA}}_{2}(H))-\\mathrm{FFN}_{1}(H+\\boldsymbol{\\mathrm{MHA}}_{2}(H))\\big\\|_{\\infty}$ as ", "page_idx": 33}, {"type": "text", "text": "\u2225FFN2(H + MHA2(H)) \u2212FFN1(H + MHA2(H))\u2225\u221e $\\leq\\Vert\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))-\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{2}(H))\\Vert_{\\infty}\\leq L_{\\mathrm{FFN}}(w_{\\mathrm{FFN}}M^{\\prime}+2)(\\kappa w_{\\mathrm{FFN}})^{L_{\\mathrm{FN}}-1}\\eta_{\\infty}$ ", "page_idx": 33}, {"type": "text", "text": "where $\\|H+\\mathrm{MHA_{2}}(H)\\|_{\\infty}\\leq M^{\\prime}$ . Putting all the estimates together, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ \\|\\mathbf{B}_{1}(H)-\\mathbf{B}_{2}(H)\\|_{\\infty}}\\\\ &{\\leq3\\kappa^{3}d_{e m b d}^{6}M^{3}m l\\eta+\\|\\mathrm{FFN}_{1}(H+\\mathrm{MHA}_{1}(H))-\\mathrm{FFN}_{2}(H+\\mathrm{MHA}_{2}(H))\\|_{\\infty}}\\\\ &{\\leq3\\kappa^{3}d_{e m b d}^{6}M^{3}m l\\eta+3\\kappa^{3+L_{\\mathrm{FPN}}}w_{\\mathrm{FFN}}^{2L_{\\mathrm{FPN}}}d_{e m b d}^{6}M^{3}m l\\eta+L_{\\mathrm{FFN}}(w_{\\mathrm{FFN}}M^{\\prime}+2)(\\kappa w_{\\mathrm{FFN}})^{L_{\\mathrm{FN}}-1}\\eta}\\\\ &{\\leq\\bigl(4\\kappa^{3+L_{\\mathrm{FPN}}}w_{\\mathrm{FFN}}^{2L_{\\mathrm{FPN}}}d_{e m b d}^{6}M^{3}m l+L_{\\mathrm{FFN}}(w_{\\mathrm{FFN}}M^{\\prime}+2)(\\kappa w_{\\mathrm{FFN}})^{L_{\\mathrm{FN}}-1}\\bigr)\\eta}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and it remains to control $M^{\\prime}$ in terms of $M$ . We know $\\|H+\\mathbf{M}\\mathbf{H}\\mathbf{A}(H)\\|_{\\infty}\\leq M^{\\prime}$ and so we compute ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\cal H}+\\mathbf{M}\\mathrm{HA}({\\cal H})\\|_{\\infty}\\leq{M}+\\|\\mathbf{M}\\mathrm{HA}({\\cal H})\\|_{\\infty}}\\\\ {\\displaystyle\\leq{M}+\\sum_{j=1}^{m}\\operatorname*{max}_{1\\leq i\\leq l}\\sum_{k=1}^{l}\\|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle V_{j}h_{k}\\|_{\\infty}}\\\\ {\\displaystyle\\leq{M}+m l d_{e m b d}^{6}{\\kappa}^{3}{M}\\leq2d_{e m b d}^{6}{\\kappa}^{3}m l M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, we can complete our estimate on $\\|\\mathbf{B}_{1}(H)-\\mathbf{B}_{2}(H)\\|_{\\infty}$ as ", "page_idx": 34}, {"type": "text", "text": "\u2225B1 $(H)-\\mathbf{B}_{2}(H)||_{\\infty}\\leq\\bigl(4\\kappa^{3+L_{\\mathrm{FPN}}}w_{\\mathrm{FFN}}^{2L_{\\mathrm{FPN}}}d_{e m b d}^{6}M^{3}m l+L_{\\mathrm{FFN}}(w_{\\mathrm{FFN}}(2d_{e m b d}^{6}\\kappa^{3}m l M)+2)(\\kappa w_{\\mathrm{FFN}})^{L_{\\mathrm{FPN}}-1}d_{e m b d}^{6}\\bigr),$ 1 \u03b7. Now consider the multi-block difference ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf B_{L_{T}}\\circ\\dots\\circ\\mathbf B_{1}(H)-\\mathbf B_{L_{T}}^{\\prime}\\circ\\dots\\circ\\mathbf B_{1}^{\\prime}(H)\\|_{\\infty}\\leq\\|\\mathbf B_{L_{T}}\\circ\\dots\\circ\\mathbf B_{1}(H)-\\mathbf B_{L_{T}}\\circ\\dots\\circ\\mathbf B_{1}^{\\prime}(H)\\|_{\\infty}}\\\\ {+\\,\\|\\mathbf B_{L_{T}}\\circ\\dots\\circ\\mathbf B_{1}^{\\prime}(H)-\\mathbf B_{L_{T}}^{\\prime}\\circ\\dots\\circ\\mathbf B_{1}^{\\prime}(H)\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To bound the second difference we must bound the output $\\|\\mathbf{B}(H)\\|_{\\infty}$ , $\\mathbf{B}\\in B(m,L_{\\mathrm{FFN}})$ . Suppose $\\|H\\|_{\\infty}\\leq M$ . Write ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{B}(H)\\|_{\\infty}=\\|\\mathrm{FFN}(H+\\mathrm{MHA}(H))+\\mathrm{MHA}(H)+H\\|_{\\infty}}&{}\\\\ {\\leq\\|H\\|_{\\infty}+\\|\\mathrm{MHA}(H)\\|_{\\infty}+\\|\\mathrm{FFN}(H+\\mathrm{MHA}(H))\\|_{\\infty}}&{}\\\\ {\\leq2d_{e m b d}^{6}\\kappa^{3}m l M+\\|\\mathrm{FFN}(H+\\mathrm{MHA}(H))\\|_{\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "so we must bound the output of the FFN layer. We have for $H^{\\prime}\\in\\mathbb{R}^{d_{e m b d}\\times l}$ with $\\|H^{\\prime}\\|_{\\infty}\\leq M$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathrm{FFN}(H^{\\prime})\\|_{\\infty}\\leq(\\kappa d_{e m b d}^{2})^{L_{\\mathrm{FFN}}-1}M+\\kappa(\\kappa d_{e m b d}^{2})^{L_{\\mathrm{FFN}}-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\|H+M H A(H)\\|_{\\infty}\\leq2d_{e m b d}^{6}\\kappa^{3}m l M$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{B}(H)\\|_{\\infty}\\leq2d_{e m b d}^{6}\\kappa^{3}m l M+(\\kappa w_{\\mathrm{FFN}}^{2})^{L_{\\mathrm{FFN}}-1}2d_{e m b d}^{6}\\kappa^{3}m l M+\\kappa(\\kappa w_{\\mathrm{FFN}}^{2})^{L_{\\mathrm{FFN}}-2}}\\\\ &{\\qquad\\qquad\\leq4d_{e m b d}^{4}w_{\\mathrm{FFN}}^{2L_{\\mathrm{FFN}}}\\kappa^{L_{\\mathrm{FFN}}+1}m l M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Iterating this gives ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf B_{L_{T}-1}\\circ\\dots\\circ\\mathbf B_{1}(H)\\|_{\\infty}\\leq(4d_{e m b d}^{4}w_{\\mathrm{FFN}}^{2L_{\\mathrm{FFN}}}\\kappa^{L_{\\mathrm{FFN}}+1}m l)^{L_{T}-1}M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then we can bound the above second term as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mathsf{B}_{L_{T}}\\circ\\dots\\circ\\mathsf{B}_{1}^{\\prime}(H)-\\mathsf{B}_{L_{T}}^{\\prime}\\circ\\dots\\circ\\mathsf{B}_{1}^{\\prime}(H)\\|_{\\infty}}\\\\ &{\\leq\\!\\big(4\\kappa^{3+L_{\\mathrm{FrN}}}d_{e m b d}^{6+2L_{\\mathrm{FrN}}}\\!\\big((4d_{e m b d}^{2L_{\\mathrm{FrN}}+4}\\kappa^{L_{\\mathrm{FrN}}+1}m l)^{L_{T}-1}M\\big)^{3}m l}\\\\ &{\\quad+L_{\\mathrm{FrN}}(d_{e m b d}^{2}(2d_{e m b d}^{6}\\kappa^{3}m l\\big((4d_{e m b d}^{2L_{\\mathrm{FrN}}+4}\\kappa^{L_{\\mathrm{FrN}}+1}m l)^{L_{T}-1}M\\big))+2\\big)(\\kappa d_{e m b d})^{L_{\\mathrm{FrN}}-1}\\big)\\eta}\\\\ &{\\leq\\!4^{3L_{T}-2}L_{\\mathrm{FrN}}d_{e m b d}^{6L_{T}L_{\\mathrm{FrN}}+12L_{T}-10L_{\\mathrm{FrN}}-6}\\kappa^{3L_{T}L_{\\mathrm{FrN}}+3L_{T}-2L_{\\mathrm{FrN}}}M^{3}m^{L_{T}}l^{L_{T}}\\eta}\\\\ &{\\leq\\!4^{3L_{T}}L_{\\mathrm{FrN}}M^{3}d_{e m b d}^{18L_{T}L_{\\mathrm{FrN}}}w_{\\mathrm{FrN}}^{18L_{T}L_{\\mathrm{FrN}}}\\kappa^{6L_{T}L_{\\mathrm{FrN}}}m^{L_{T}}l^{L_{T}}\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To bound the first term $\\|\\mathbf{B}_{L_{T}}\\circ\\ldots\\circ\\mathbf{B}_{1}(H)-\\mathbf{B}_{L_{T}}\\circ\\ldots\\circ\\mathbf{B}_{1}^{\\prime}(H)\\|_{\\infty}$ we must control the Lipschitz constant of of $\\mathbf{B}\\in B(m,L_{\\mathrm{FFN}})$ . Let $H,H^{\\prime}\\in\\mathbb{R}^{d_{e m b d}\\times l}$ with $\\|H\\|_{\\infty},\\|H^{\\prime}\\|_{\\infty}\\leq M$ . Compute ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{B}(H)-\\mathrm{B}(H^{\\prime})\\|_{\\infty}=\\|\\mathrm{FFN}(H+\\mathrm{MHA}(H))-\\mathrm{FFN}(H^{\\prime}+\\mathrm{MHA}(H^{\\prime}))+\\mathrm{MHA}(H)-\\mathrm{MHA}(H^{\\prime})\\|_{\\infty}}\\\\ {\\leq\\|\\mathrm{FFN}(H+\\mathrm{MHA}(H))-\\mathrm{FFN}(H^{\\prime}+\\mathrm{MHA}(H^{\\prime}))\\|_{\\infty}+\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H)-}\\\\ {\\leq\\|\\mathrm{FFN}(H+\\mathrm{MHA}(H))-\\mathrm{FFN}(H^{\\prime}+\\mathrm{MHA}(H^{\\prime}))\\|_{\\infty}+\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H)-}\\\\ {\\leq w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H^{\\prime})+H-H^{\\prime}\\|_{\\infty}+\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H)\\|_{\\infty}}\\\\ {\\leq w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}\\|H-H^{\\prime}\\|_{\\infty}+(w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}+1)\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H^{\\prime})\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and it suffices to control the Lipschitz constant of $\\mathrm{MHA}\\in\\mathcal{M H A}(m)$ . For the same $H,H^{\\prime}$ compute ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H^{\\prime})\\|_{\\infty}\\le\\operatorname*{max}_{1\\le i\\le l}\\sum_{j=1}^{m}\\sum_{k=1}^{l}\\|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle V_{j}h_{k}-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle V_{j}h_{k}^{\\prime}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can bound $\\big|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle V_{j}h_{k}-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle V_{j}h_{k}^{\\prime}\\big|$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle V_{j}h_{k}-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle V_{j}h_{k}^{\\prime}\\|_{\\infty}}\\\\ &{\\lesssim\\|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle V_{j}h_{k}-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle V_{j}h_{k}\\|_{\\infty}+\\|\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle V_{j}h_{k}^{\\prime}-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle V_{j}h_{k}\\|_{\\infty}}\\\\ &{\\lesssim\\left|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle\\right|\\|V_{j}h_{k}\\|_{\\infty}+\\left|\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle\\right|\\|V_{j}h_{k}^{\\prime}-V_{j}h_{k}\\|_{\\infty}}\\\\ &{\\lesssim\\left|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle\\right|d_{\\mathrm{embd}}^{2}\\kappa M+\\left|\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle\\right|d_{\\mathrm{embd}}^{2}\\kappa\\|h_{k}-h_{k}^{\\prime}\\|_{\\infty}}\\\\ &{\\lesssim\\left|\\langle Q_{j}h_{i},K_{j}h_{k}\\rangle-\\langle Q_{j}h_{i}^{\\prime},K_{j}h_{k}^{\\prime}\\rangle\\right|d_{\\mathrm{embd}}^{2}\\kappa M+2d_{\\mathrm{embd}}^{4}\\kappa^{2}M^{2}d_{\\mathrm{embd}}^{2}\\kappa\\|h_{k}-h_{k}^{\\prime}\\|_{\\infty}}\\\\ &{\\lesssim4d_{\\mathrm{embd}}^{6}\\kappa^{3}M^{2}\\|h_{k}-\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Applying this gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H^{\\prime})\\|_{\\infty}\\leq4d_{e m b d}^{6}\\kappa^{3}M^{2}m l\\|H-H^{\\prime}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf B}(H)-{\\bf B}(H^{\\prime})\\|_{\\infty}\\leq w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}\\|H-H^{\\prime}\\|_{\\infty}+(w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}+1)\\|\\mathrm{MHA}(H)-\\mathrm{MHA}(H^{\\prime})\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}\\|H-H^{\\prime}\\|_{\\infty}+(w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}+1)4d_{e m b d}^{6}\\kappa^{3}M^{2}m l\\|H-H^{\\prime}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq8w_{\\mathrm{FFN}}^{2L_{\\mathrm{FN}}}\\kappa^{L_{\\mathrm{FN}}}d_{e m b d}^{6}\\kappa^{3}M^{2}m l\\|H-H^{\\prime}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "With this we can bound the first term of the multi-block difference ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phantom{\\frac{1}{r}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": ". Putting together the estimates on both terms gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{B}_{L_{T}}\\circ\\dots\\circ\\mathbf{B}_{1}(H)-\\mathbf{B}_{L_{T}}^{\\prime}\\circ\\dots\\circ\\mathbf{B}_{1}^{\\prime}(H)\\|_{\\infty}\\leq2^{7}d_{e m b d}^{5L_{T}L_{F^{\\mathrm{bs}}}}\\kappa^{2L_{T}L_{F^{\\mathrm{bs}}}}d_{e m b d}^{6}\\kappa^{3}M^{2}m^{L_{T}}l^{L_{T}}}&{}\\\\ {*\\|\\boldsymbol{B}_{L_{T}-1}\\circ\\dots\\circ\\boldsymbol{B}_{1}(H)-\\boldsymbol{B}_{L_{T}-1}^{\\prime}\\circ\\dots\\circ\\boldsymbol{B}_{1}^{\\prime}(H)\\|_{\\infty}}&{}\\\\ {+\\,4^{3L_{T}}L_{\\mathrm{FN}}M^{3}d_{e m b d}^{18L_{T}L_{F^{\\mathrm{bs}}}}\\kappa^{6L_{T}L_{F^{\\mathrm{bs}}}}m^{L_{T}}l^{L_{T}}\\eta}&{}\\\\ {\\,}&{\\leq\\,2^{7L_{T}}L_{\\mathrm{FN}}M^{3}d_{e m b d}^{18L_{T}L_{F^{\\mathrm{bs}}}}\\kappa^{6L_{T}L_{F^{\\mathrm{br}}}}m^{L_{T}}l^{L_{T}}}\\\\ {*\\,}&{\\|\\boldsymbol{B}_{L_{T}-1}\\circ\\dots\\circ\\boldsymbol{B}_{1}(H)-\\boldsymbol{B}_{L_{T}-1}^{\\prime}\\circ\\dots\\circ\\boldsymbol{B}_{1}^{\\prime}(H)\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which reduces the bound on the $L_{T}$ layer transformer class to the bound on the bound on the $L_{T}-1$ layer transformer class. Finally, via induction, we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{B}_{L_{T}}\\circ\\hdots\\circ\\mathbf{B}_{1}(H)-\\mathbf{B}_{L_{T}}^{\\prime}\\circ\\hdots\\circ\\mathbf{B}_{1}^{\\prime}(H)\\Vert_{\\infty}\\leq\\biggl(2^{7L_{T}}L_{\\mathrm{FFN}}M^{3}d_{e m b d}^{18L_{T}}w_{\\mathrm{FFN}}^{18L_{T}L_{\\mathrm{FN}}}\\kappa^{6L_{T}L_{\\mathrm{FN}}}m^{L_{T}}l^{L_{T}}\\biggr)^{L_{\\mathrm{T}}}}\\\\ {\\leq2^{7L_{T}^{2}}L_{\\mathrm{FFN}}M^{3L_{T}}d_{e m b d}^{18L_{T}^{2}}w_{\\mathrm{FFN}}^{18L_{T}^{2}L_{\\mathrm{FN}}}\\kappa^{6L_{T}^{2}L_{\\mathrm{FN}}}m^{L_{T}^{2}}l^{L_{T}^{2}}\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It simply remains to deal with the encoding layer $H=\\mathrm{PE}+\\mathrm{E}(x)$ . Both $E$ and $P E$ are fixed among architectures and further $\\|\\mathrm{{PE}}+\\mathrm{{E}}(x)\\|_{\\infty}\\stackrel{{}}{=}\\|\\dot{x}\\|_{\\infty}+1\\leq M+1$ . Thus ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{\\mathbf{B}}_{L_{T}}^{\\prime}\\circ\\dots\\circ\\boldsymbol{\\mathbf{B}}_{1}^{\\prime}\\circ(\\boldsymbol{\\mathbf{P}}\\boldsymbol{\\mathbf{E}}+\\boldsymbol{\\mathbf{E}}^{\\prime}(x))\\|_{\\infty}\\leq(4d_{e m b d}^{2L_{\\mathrm{FFN}}+4}\\kappa^{L_{\\mathrm{FFN}}+1}m l)^{L_{T}}M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This allows us to complete the total bound on the transformer difference of ${\\mathrm{~\\cal~T,~T^{\\prime}~}}\\in$ $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ such that $\\|\\theta-\\theta^{\\prime}\\|_{\\infty}<\\eta$ as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathrm{T}(x)-\\mathrm{T}^{\\prime}(x)\\|_{\\infty}=\\|\\mathrm{D}\\circ\\mathrm{B}_{L_{T}}\\circ...\\circ\\mathrm{B}_{1}\\circ(\\mathrm{PE}+\\mathrm{E}(x))-\\mathrm{D}^{\\prime}\\circ\\mathrm{B}_{L_{T}}^{\\prime}\\circ...\\circ\\mathrm{B}_{1}^{\\prime}\\circ(\\mathrm{PE}+\\mathrm{E}^{\\prime}(x))\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq2^{L_{T}^{2}+1}L_{\\mathrm{FFN}}M^{3L_{T}}d_{e m b d}^{18L_{T}^{2}L_{\\mathrm{FFN}}}\\kappa^{6L_{T}^{2}L_{\\mathrm{FFN}}}m^{L_{T}^{2}}l^{L_{T}^{2}}\\eta=C_{T}\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, we may compute $\\mathcal{N}(\\mathcal{T},\\delta,\\|\\cdot\\|_{\\infty})$ . We must cover $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ with a $\\delta$ -net such that, for all $\\boldsymbol{\\mathrm{T}}\\in\\mathcal{T}(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ , we can find $T^{\\prime}$ in the net such that $\\|\\mathbf{T}-\\mathbf{T}^{\\prime}\\|_{\\infty}<\\delta$ . Set $\\begin{array}{r}{\\eta=\\frac{\\delta}{C_{T}}}\\end{array}$ . We construct the $\\delta\\!\\cdot$ -net by uniformly discretizing the set of weights $\\theta$ corresponding to $\\mathrm{T}_{\\theta}\\in\\mathcal{T}$ with step size given by $\\eta$ . Then for any $\\mathrm{T}\\in\\mathcal{T}$ , we can find a $T^{\\prime}$ in the grid such that \u2225\u03b8T \u2212\u03b8T\u2032\u2225\u221e< \u03b7. Then we have \u2225T \u2212T\u2032\u2225\u221e< CT \u03b7 = CTC\u03b4T . We can compute the number of parameters in $\\theta$ as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|\\theta|=|\\theta_{E}|+|\\theta_{D}|+\\sum_{i=1}^{L_{T}}|\\theta_{\\mathrm{B}_{i}}|=d_{e m b d}+d_{e m b d}D+L_{T}|\\theta_{\\mathrm{B}}|}\\\\ {\\displaystyle=d_{e m b d}+d_{e m b d}D+L_{T}(|\\theta_{\\mathrm{MHA}}|+|\\theta_{\\mathrm{FFN}}|)}\\\\ {\\displaystyle=d_{e m b d}+d_{e m b d}D+L_{T}(3d_{e m b d}^{2}m+L_{\\mathrm{FFN}}w_{\\mathrm{FFN}}^{2})}\\\\ {\\displaystyle\\leq4d_{e m b d}^{2}w_{\\mathrm{FFN}}^{2}D(m+L_{\\mathrm{FFN}})L_{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We can compute the number of steps per parameter as $\\begin{array}{r}{\\frac{2\\kappa}{\\eta}}\\end{array}$ . Thus the covering number of $T(L_{T},L_{\\mathrm{FFN}},w_{\\mathrm{FFN}},l,d_{e m b d},m,R,\\kappa)$ is bounded by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{V}(\\mathcal{T},\\delta,\\|\\cdot\\|_{\\infty})\\leq\\binom{\\kappa}{\\eta}^{4d_{\\mathrm{con}\\,b d}^{2}w_{\\mathrm{FR}}^{2}D(m+L_{\\mathrm{FR}})L_{T}}}\\\\ &{\\qquad\\qquad=\\bigl(\\frac{C_{T}\\kappa}{\\delta}\\bigr)^{4d_{\\mathrm{con}\\,b d}^{2}w_{\\mathrm{FP}}^{2}D(m+L_{\\mathrm{FR}})L_{T}}}\\\\ &{\\qquad\\qquad=\\bigl(\\frac{2^{L_{T}^{2}+1}L_{\\mathrm{FFN}}M^{3L_{T}}d_{\\mathrm{con}\\,b d}^{18L_{T}^{2}}w_{\\mathrm{FFN}}^{18L_{T}^{2}}\\mathsf{L}\\mathsf{r i n}_{N}^{2}\\mathsf{L}_{T}^{2}L_{\\mathrm{FR}}^{\\mathrm{Fx}}m^{L_{T}^{2}}l^{L_{T}^{2}}}{\\delta}\\bigr)^{4d_{\\mathrm{cob}\\,d}^{2}w_{\\mathrm{FN}}^{2}D(m+L_{\\mathrm{FP}})L_{T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "G Building Blocks of Transformer Neural Networks ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma 3 (Interaction Lemma). Set $d_{e m b d}\\,=\\,5$ and let $\\kappa,M\\,>\\,0$ . Fix $l\\;\\in\\;\\mathbb{N},$ , $1\\,\\leq\\,t_{1},t_{2}\\,\\leq\\,l_{i}$ , and $1\\;\\leq\\;i\\;\\leq\\;d_{e m b d}$ . Let $H\\ \\in\\ \\mathbb{R}^{d_{e m b d}\\times l}$ be a structured transformer embedding matrix such that $h_{t}^{d_{e m b d}-2:d_{e m b d}-1}\\,=\\,\\mathcal{Z}_{t}$ and $h_{t}^{d_{e m b d}}\\,=\\,1$ . Further suppose $\\|H\\|_{\\infty}\\,\\leq\\,M$ for some $M\\,>\\,0$ Let $B\\ :\\ \\mathbb{R}^{d_{e m b d}}\\ \\times\\ \\mathbb{R}^{d_{e m b d}}\\ \\to\\ \\mathbb{R}$ be a kernel function on tokens which can be written in the form $B(h,h^{\\prime})\\ =\\ \\sigma(\\langle Q^{B}h,K^{B}h^{\\prime}\\rangle)$ ) for some $\\mathit{h},\\mathit{h}^{\\prime}\\;\\in\\;\\mathbb{R}^{d_{e m b d}},Q^{B},K^{B}\\;\\in\\;\\mathbb{R}^{(d_{e m b d}-3)\\times d_{e m b d}}$ with $\\|Q^{B}\\|_{\\infty,\\infty},\\|K^{B}\\|_{\\infty,\\infty}\\ \\le\\ \\kappa.$ Then we can construct an attention head A acting on $H$ such that $\\mathbf{A}(h_{t_{1}})~=~B(h_{t_{1}},h_{t_{2}})e_{i}$ and otherwise $\\mathbf{A}(h_{t})~=~0$ for $t~\\neq~t_{1}$ . Further we have $\\|\\theta_{\\mathrm{{A}}}\\|_{\\infty}=O(d_{e m b d}^{4^{\\circ}}\\kappa^{2}M^{2}l^{2})$ . Note: we call the matrices $\\overset{\\cdot}{Q}^{B},K^{B}$ \u201cdata kernels\u201d. ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma 3. Define the query, key, and value matrices as ", "page_idx": 36}, {"type": "equation", "text": "$$\nQ=\\left[\\begin{array}{c c c c}{{}}&{{Q^{B}}}&{{}}\\\\ {{0}}&{{0}}&{{Q^{Z}}}&{{0}}\\\\ {{0}}&{{0}}&{{Q^{Z}}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{1}}\\end{array}\\right],\\quad K=\\left[\\begin{array}{c c c c}{{K^{B}}}&{{}}&{{}}\\\\ {{0}}&{{0}}&{{K^{Z}}}&{{0}}\\\\ {{0}}&{{0}}&{{K^{Z}}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\right],\\quad V=e_{i}e_{d_{e m b d}}^{T}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we call $Q^{\\mathcal{Z}},K^{\\mathcal{Z}}\\in\\mathbb{R}^{2\\times2}$ the interaction kernels, $Q^{B},K^{B}\\in\\mathbb{R}^{d_{e m b d}-3\\times d_{e m b d}}$ the data kernels, and $C>0$ simply a large positive number. We can choose $Q^{\\mathbb{Z}},K^{\\mathbb{Z}}$ so $K^{\\mathbb Z}=P_{\\mathbb Z_{t_{2}}}$ is a projection onto the unit interaction vector $\\mathcal{T}_{t_{2}}$ (the interaction term for $h_{t_{2}}$ ) and $Q^{\\mathcal{Z}}$ is a dilation and rotation of $\\mathcal{T}_{t_{1}}$ onto $\\mathcal{T}_{t_{2}}$ i.e. $Q^{\\mathbb{Z}}T_{t_{1}}=C T_{t_{2}}$ . We must now compute $\\mathrm{A}(H)$ . For an arbitrary $1\\le t\\le l$ we can compute the action of A on $h_{t}$ as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf A}(h_{t})=\\sum_{k=1}^{l}\\sigma(\\langle Q h_{t},K h_{k}\\rangle)V h_{k}}}\\\\ {{\\displaystyle{\\bf\\qquad\\qquad}=\\sum_{k=1}^{l}\\sigma(\\langle Q^{B}h_{t},K^{B}h_{k}\\rangle+\\langle Q^{Z}\\mathbb{Z}_{t},K^{Z}\\mathbb{Z}_{k}\\rangle-C)e_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now we must case on whether the input token $t=t_{1}$ . ", "page_idx": 36}, {"type": "text", "text": "Case 1: $t=t_{1}$ Write ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\bf A}(h_{t_{1}})=\\sum_{k=1}^{l}\\sigma(\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle+\\langle Q^{Z}\\mathcal{T}_{t_{1}},K^{Z}\\mathcal{T}_{k}\\rangle-C)e_{i}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To handle the same we must again go by casework, this by casing on whether $k=t_{2}$ . ", "page_idx": 36}, {"type": "text", "text": "Case 1a): $k=t_{2}$ When $k=t_{2}$ we have $\\langle Q^{\\mathbb Z}\\mathscr{T}_{t_{1}},K^{\\mathbb Z}\\mathscr{T}_{t_{2}}\\rangle=\\langle C\\mathscr{T}_{t_{2}},\\mathscr{T}_{t_{2}}\\rangle$ since by construction $Q^{\\mathbb{Z}}T_{t_{1}}\\,=\\,T_{t_{2}}$ and $K^{\\mathbb{Z}}$ is a projection onto $\\mathcal{T}_{t_{2}}$ so that $K^{\\mathbb{Z}}T_{t_{2}}\\,=\\,\\mathcal{Z}_{t_{2}}$ . This further simplifies as $\\langle C T_{t_{2}},T_{t_{2}}\\rangle=C$ since $\\mathcal{T}_{t_{2}}$ is unit length. Thus ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sigma(\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle+\\langle Q^{Z}\\Zrangle_{t_{1}},K^{Z}\\Zrangle_{k})-C)=\\sigma(\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle+C-C)}&{{}}\\\\ {=\\sigma(\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now we must consider the other terms in the sum when $k\\neq t_{2}$ . ", "page_idx": 37}, {"type": "text", "text": "Case 1b): $k\\neq t_{2}$ When $k\\neq t_{2}$ we compute ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle Q^{\\mathbb{Z}}\\mathcal{Z}_{t_{1}},K^{\\mathbb{Z}}\\mathcal{Z}_{k}\\rangle\\leq\\|Q^{\\mathbb{Z}}\\mathcal{Z}_{t_{1}}\\|_{2}\\|K^{\\mathbb{Z}}\\mathcal{Z}_{k}\\|_{2}}\\\\ {=C\\|P_{\\mathbb{Z}_{t_{2}}}\\mathcal{Z}_{k}\\|_{2}<C\\quad\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we use Cauchy-Schwarz at the first inequality and note that $\\|P_{\\mathbb{Z}_{t_{2}}}Z_{k}\\|_{2}<1$ since $k\\neq t_{2}$ for the second inequality. Then, for large enough $C$ , we have ", "page_idx": 37}, {"type": "text", "text": "$\\sigma(\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle+\\langle Q^{\\mathcal{Z}}\\Sigma_{t_{1}},K^{\\mathcal{Z}}\\Sigma_{k}\\rangle-C)\\leq\\sigma(\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle+C\\|P_{\\mathcal{Z}_{t_{2}}}\\mathcal{Z}_{k}\\|_{2}-C)=0$ so we simply must choose $C$ so that $\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle+C\\|P_{\\mathbb{Z}_{t_{2}}}\\mathcal{Z}_{k}\\|_{2}-C<0$ . Compute ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle+C\\|P_{\\overline{{{\\cal{L}}}}_{t_{2}}}\\overline{{{\\cal{L}}}}_{k}\\|_{2}-C<0\\iff C>\\frac{\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle}{1-\\|P_{\\overline{{{\\cal{L}}}}_{t_{2}}}\\overline{{{\\cal{L}}}}_{k}\\|_{2}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We can bound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle\\right|\\leq\\|Q^{B}h_{t_{1}}\\|_{2}\\|K^{B}h_{k}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|Q^{B}\\|_{1,1}\\|h_{t_{1}}\\|_{\\infty}\\|K^{B}\\|_{1,1}\\|h_{t_{1}}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|Q^{B}\\|_{\\infty,\\infty}d_{e m b d}^{2}\\|K^{B}\\|_{\\infty,\\infty}d_{e m b d}^{2}M^{2}\\leq d_{e m b d}^{4}\\kappa^{2}M^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "so it remains to control1\u2212\u2225PI1 Ik\u22252 when $k\\neq t_{2}$ by upper bounding $\\|P_{\\mathbb{Z}_{t_{2}}}\\mathcal{Z}_{k}\\|_{2}$ . Compute ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|P_{T_{t_{2}}}\\mathcal{L}_{k}\\|_{2}\\leq\\operatorname*{max}_{1\\leq t_{1}\\neq t_{2}\\leq l}\\|P_{T_{t_{1}}}\\mathcal{L}_{t_{2}}\\|_{2}}\\\\ {\\displaystyle\\leq\\|P_{Z_{0}}\\mathcal{L}_{1}\\|_{2}}\\\\ {\\displaystyle=\\langle Z_{0},Z_{1}\\rangle}\\\\ {\\displaystyle=\\cos(\\frac{\\pi}{2l})}\\\\ {\\displaystyle\\leq1-\\frac{1}{2}\\frac{\\pi^{2}}{2!\\cdot2^{2}l^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we note the projection is maximized by any adjacent interaction terms $\\mathcal{T}_{t},\\mathcal{T}_{t+1}$ . Then ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle}{1-\\|P_{Z_{t_{2}}}\\mathcal{Z}_{k}\\|_{2}}\\leq\\frac{d_{e m b d}^{4}\\kappa^{2}M^{2}}{\\frac{\\pi^{2}}{22!\\cdot2^{2}l^{2}}}=O(d_{e m b d}^{4}\\kappa^{2}M^{2}l^{2})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which gives a bound on $C$ . So for large enough $C$ we can conclude ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbf{A}(h_{t_{1}})=\\sigma(\\langle Q^{B}h_{t_{1}},K^{B}h_{k}\\rangle)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "It remains to consider the case $t\\ne t_{1}$ . ", "page_idx": 37}, {"type": "text", "text": "Case 2: $t\\ne t_{1}$ Now we consider A applied to tokens other than $h_{t_{1}}$ . We have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathrm{A}}(h_{t})=\\sum_{k=1}^{l}\\sigma(\\langle Q^{B}h_{t},K^{B}h_{k}\\rangle+\\langle Q^{Z}\\mathcal{T}_{t},K^{Z}\\mathcal{T}_{k}\\rangle-C)\\boldsymbol{e}_{i}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "so we must argue each term in the sum is 0 by showing $\\left<Q^{B}h_{t},K^{B}h_{k}\\right>+\\left<Q^{Z}\\mathbb{Z}_{t},K^{Z}\\mathbb{Z}_{k}\\right>-C<0.$ We again split into two cases. ", "page_idx": 37}, {"type": "text", "text": "Case 2a): $k\\neq t_{2}$ When $k\\neq t_{2}$ we have $\\langle Q^{B}h_{t},K^{B}h_{k}\\rangle+\\langle Q^{\\underline{{{Z}}}}\\mathbb{Z}_{t},K^{\\underline{{{Z}}}}\\mathbb{Z}_{k}\\rangle-C<0$ via the same argument as in case 1b. ", "page_idx": 37}, {"type": "text", "text": "Case 2b): $k=t_{2}$ When $k=t_{2}$ we must instead more carefully bound the inner product. We can simplify the interaction terms as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\langle Q^{Z}\\mathbb{Z}_{t},K^{Z}\\mathbb{Z}_{t_{2}}\\rangle=\\Vert Q^{Z}\\mathbb{Z}_{t}\\Vert_{2}\\Vert K^{Z}\\mathbb{Z}_{t_{2}}\\Vert_{2}\\cos(\\theta_{t,t_{2}})=C\\cos(\\theta_{t,t_{2}})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\theta_{t,t_{2}}$ is the angle between $Q^{\\mathbb{Z}}T_{t}$ and $K^{\\tt Z}\\mathbb{Z}_{k}$ . Crucially, because $t\\ne t_{1}$ , $Q^{\\mathbb{Z}}T_{t}\\neq C T_{t_{2}}$ and so $\\theta_{t,t_{2}}>0$ and $\\cos(\\theta_{t,t_{2}})<1$ . Then we can say ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle Q^{B}h_{t},K^{B}h_{t_{2}}\\rangle+\\langle Q^{Z}\\mathbb{Z}_{t},K^{Z}\\mathbb{Z}_{t_{2}}\\rangle-C<0\\iff\\langle Q^{B}h_{t},K^{B}h_{t_{2}}\\rangle+C\\cos(\\theta_{t,t_{2}})-C<0}\\\\ &{\\qquad\\qquad\\qquad\\iff C>\\frac{\\langle Q^{B}h_{t},K^{B}h_{t_{2}}\\rangle}{1-\\cos(\\theta_{t,t_{2}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We upper bound the numerator by $O(d_{e m b d}^{4}\\kappa^{2}M^{2}l^{2})$ . Note that the $Q^{\\mathcal{Z}}$ rotates by clockwise $\\frac{t_{1}-t_{2}}{l}\\,\\frac{\\pi}{2}$ radians. So $\\begin{array}{r}{Q^{\\mathcal{Z}}\\mathcal{T}_{t}=C(\\cos(\\frac{(t+t_{1}-t_{2})\\pi}{2l})}\\end{array}$ ), $\\sin\\!\\big(\\frac{(t\\!+\\!t_{1}\\!-\\!t_{2})\\pi}{2l}\\big)$ ). Thus we can upper bound $\\cos(\\theta_{t,t_{2}})$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cos(\\theta_{t,t_{2}})=\\langle\\mathscr{T}_{t+t_{1}-t_{2}},\\mathscr{Z}_{t_{2}}\\rangle}\\\\ &{\\mathrm{~\\\\\\\\}\\leq\\langle\\mathscr{T}_{0},\\mathscr{Z}_{1}\\rangle}\\\\ &{\\mathrm{~\\\\\\\\}=\\cos(\\frac{\\pi}{2l})}\\\\ &{\\mathrm{~\\\\\\\\}\\leq1-\\displaystyle\\frac{1}{2}\\frac{\\pi^{2}}{2!\\cdot2^{2}l^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "So as in case 1b we have $C=O(d_{e m b d}^{4}\\kappa^{2}M^{2}l^{2})$ . For sufficiently large $C$ we have $\\mathbf{A}(h_{t})=0$ as desired. This completes the proof of Lemma 3. ", "page_idx": 38}, {"type": "text", "text": "Lemma 4 (Gating FFNs). Given an embedding matrix $H\\in\\mathbb{R}^{d_{e m b d}\\times l}$ which has all ones in the last row and whose tokens contain interaction terms and given a half-space of contiguous tokens $h_{k},...,h_{l}$ or $h_{1},...,h_{k}$ , we can design $\\mathrm{FFN}\\in\\mathcal{F F N}(2)$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{FFN}(h_{t})=\\left\\{\\begin{array}{l l}{h_{t}}&{t\\in\\{1,...,k\\}}\\\\ {\\left[\\begin{array}{l}{\\theta}\\\\ {{\\overline{{Z_{t}^{1}}}}}\\\\ {{\\overline{{Z_{t}^{2}}}}}\\\\ {1}\\end{array}\\right]}&{\\mathrm{otherwise~}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\begin{array}{r}{\\left[\\begin{array}{l}{\\pmb{\\theta}}\\\\ {\\mathbb{Z}_{t}^{1}}\\\\ {\\mathbb{Z}_{t}^{2}}\\\\ {1}\\end{array}\\right]}\\end{array}$ is zero except for the last three coordinates. We call this FFN a gating feed-forward network. We additionally have $\\|\\theta_{\\mathrm{FFN}}\\|_{\\infty}\\le O(l\\|H\\|_{\\infty})$ . ", "page_idx": 38}, {"type": "text", "text": "Proof of Lemma 4. Without loss of generality assume we are given a contiguous prefix of tokens $h_{1},...,h_{k}$ $t>k$ $k\\leq l$ ll such $v$ the pivot vecto $v\\in\\breve{\\mathbb S}^{1}$ ular w $v\\cdot\\mathcal{T}_{t}>0$ $\\begin{array}{r}{\\mathbf{R}_{\\frac{\\pi}{2}}\\big(\\frac{\\mathcal{T}_{k}+\\mathcal{T}_{k+1}}{\\|\\mathcal{T}_{k}+\\mathcal{T}_{k+1}\\|_{2}}\\big)}\\end{array}$ $t\\in\\{1...,k\\}$ whe $v\\cdot\\mathcal{T}_{t}<0$ $\\mathbf{R}_{\\frac{\\pi}{2}}$ $C>0$ ", "page_idx": 38}, {"type": "equation", "text": "$$\nW_{1}={\\cal Z}_{d_{e m b d}}+\\left[\\!\\!\\begin{array}{c c c c c}{{0}}&{{0}}&{{C v^{1}}}&{{C v^{2}}}&{{0}}\\\\ {{0}}&{{0}}&{{C v^{1}}}&{{C v^{2}}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right],\\quad b_{1}=0\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\nW_{2}=\\mathbb{Z}_{d_{e m b d}}+\\left[\\!\\!\\begin{array}{c c c c c}{0}&{0}&{-C v^{1}}&{-C v^{2}}&{0}\\\\ {0}&{0}&{-C v^{1}}&{-C v^{2}}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\end{array}\\!\\!\\right],\\quad b_{2}=0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\mathcal{T}_{d_{e m b d}}$ is the identity matrix of size $d_{e m b d}\\times d_{e m b d}$ . Then ", "page_idx": 39}, {"type": "equation", "text": "$$\nz=\\sigma(W_{1}h_{t}+b_{1})=\\left[\\begin{array}{c}{\\sigma(h_{t}^{1}+C I_{t}\\cdot v)}\\\\ {\\sigma(h_{t}^{2}+C I_{t}\\cdot v)}\\\\ {\\sigma(h_{t}^{d_{e m b d}-2})}\\\\ {\\sigma(h_{t}^{d_{e m b d}-1})}\\\\ {\\sigma(h_{t}^{d_{e m b d}})}\\end{array}\\right]=\\left[\\begin{array}{c}{\\sigma(h_{t}^{1}+C I_{t}\\cdot v)}\\\\ {\\sigma(h_{t}^{2}+C I_{t}\\cdot v)}\\\\ {T_{t}^{1}}\\\\ {T_{t}^{2}}\\\\ {1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which is $\\begin{array}{r}{\\left[\\!\\!\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathcal{T}_{t}^{1}}\\\\ {\\mathcal{T}_{t}^{2}}\\\\ {1}\\end{array}\\!\\!\\right]}\\end{array}$ if $\\boldsymbol{\\mathcal{T}}_{t}\\boldsymbol{\\cdot}\\boldsymbol{v}<0$ . Applying the second layer to $\\begin{array}{r}{\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathcal{T}_{t}^{1}}\\\\ {\\mathcal{T}_{t}^{2}}\\\\ {1}\\end{array}\\right]\\mathrm{~yields~}\\left[\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathcal{T}_{t}^{1}}\\\\ {\\mathcal{T}_{t}^{2}}\\\\ {1}\\end{array}\\right].}\\end{array}$ Otherwise assume $\\boldsymbol{\\mathcal{T}}_{t}\\boldsymbol{\\cdot}\\boldsymbol{v}>0$ . Then $\\sigma(h_{t}^{i}+C\\mathcal{T}_{t}\\cdot v)=h_{t}^{i}+C\\mathcal{T}_{t}\\cdot v$ . Applying the second layer yields ", "page_idx": 39}, {"type": "equation", "text": "$$\nW_{2}z+b_{2}=h_{t}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "as desired. Further we can compute a bound on $C>0$ as $\\begin{array}{r}{|C I_{t}\\cdot v|>\\|H\\|_{\\infty}\\iff C>\\frac{\\|H\\|_{\\infty}}{|I_{t}\\cdot v|}}\\end{array}$ . We compute ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{|\\mathcal{Z}_{t}\\cdot v|=|\\langle\\mathcal{Z}_{t},\\mathbf{R}_{\\frac{\\pi}{2}}\\frac{\\mathcal{Z}_{k}+\\mathcal{Z}_{k+1}}{\\|\\mathcal{Z}_{k}+\\mathcal{Z}_{k+1}\\|_{2}}\\rangle|\\geq\\displaystyle\\frac{1}{2}|\\langle\\mathcal{Z}_{k},\\mathbf{R}_{\\frac{\\pi}{2}}\\mathcal{Z}_{k}\\rangle+\\langle\\mathcal{Z}_{k},\\mathbf{R}_{\\frac{\\pi}{2}}\\mathcal{Z}_{k+1}\\rangle|}\\\\ {\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2}|\\langle\\mathcal{Z}_{k},\\mathbf{R}_{\\frac{\\pi}{2}}\\mathcal{Z}_{k+1}\\rangle|=|\\frac{1}{2}\\langle\\mathcal{Z}_{0},\\mathbf{R}_{\\frac{\\pi}{2}}\\mathcal{Z}_{1}\\rangle|}\\\\ {\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2}\\sin\\left(\\frac{\\pi}{2l}\\right)\\geq\\frac{1}{4}\\frac{\\pi}{2l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality holds for $l\\geq2$ . Thus $\\begin{array}{r}{\\frac{\\|H\\|_{\\infty}}{|I_{t}\\cdot v|}\\leq\\frac{2^{3}l\\|H\\|_{\\infty}}{\\pi}<C}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Lemma 5 (Constant Addition). Let $M>0$ and c be a vector in $\\mathbb{R}^{D}$ such that $\\begin{array}{r}{\\|c\\|_{\\infty}\\leq\\frac{M}{2}}\\end{array}$ . Let $H$ be an embedding matrix of the form ", "page_idx": 39}, {"type": "equation", "text": "$$\nH=\\left[\\!\\!\\begin{array}{c c c c}{x^{1}}&{\\ldots}&{x^{D}}&{\\pmb{\\sigma}_{D}}\\\\ {0}&{\\ldots}&{\\ldots}&{0}\\\\ {\\mathcal{T}_{1}}&{\\ldots}&{\\ldots}&{\\mathcal{T}_{l}}\\\\ {1}&{\\ldots}&{\\ldots}&{1}\\end{array}\\!\\!\\right]\\in\\mathbb{R}^{d_{e m b d}\\times l}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $l=2D$ such that $\\begin{array}{r}{\\|H\\|_{\\infty,\\infty}\\leq\\frac{M}{2}}\\end{array}$ . Then there exists a transformer block $\\mathbf{B}\\in B(D,3)$ such ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbf{B}(H)={\\left[\\begin{array}{l l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{x^{1}-c^{1}}&{\\dots}&{x^{D}-c^{D}}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{0}\\\\ {{\\mathcal{T}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathcal{T}}_{l}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with $\\|\\theta_{\\mathrm{B}}\\|_{\\infty}=O(l M)$ . In this case we say B implements the addition of c to $x$ . ", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma 5. We begin by defining the attention heads $\\mathbf{A}_{i}$ , $1\\leq i\\leq D$ . For each head $\\mathbf{A}_{i}$ we define the data kernels ", "page_idx": 39}, {"type": "equation", "text": "$$\nQ_{i}^{d a t a}=\\left[\\!\\!\\begin{array}{c c c c c}{{1}}&{{0}}&{{0}}&{{0}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\end{array}\\!\\!\\right]\\quad K_{i}^{d a t a}=\\left[\\!\\!\\begin{array}{c c c c c}{{0}}&{{0}}&{{0}}&{{0}}&{{1}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}&{{-c^{i}+M}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, via the Interaction Lemma 3, we can construct $\\mathbf{A}_{i}$ so that token $h_{D+i}$ interacts with $h_{i}$ such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}_{i}(h_{D+i})=\\sigma(\\langle Q_{i}^{d a t a}h_{D+i},K_{i}^{d a t a}h_{i}\\rangle)e_{1}}\\\\ &{\\quad\\quad\\quad=\\sigma(x^{i}-c^{i}+M)e_{1}=(x^{i}-c^{i}+M)e_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and $\\mathrm{A}_{i}(h_{t})=0$ when $t\\neq D+i$ . Then the residual multi-headed attention yields ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{MHA}(H)+H={\\left[\\begin{array}{l l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{x^{1}-c^{1}+M}&{\\dots}&{x^{D}-c^{D}+M}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}\\\\ {{\\mathcal{I}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathcal{I}}_{L}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Via the gating lemma 4 we can design a three layer FFN to subtract $M$ from only $h_{D+1},...,h_{2D}$ . Thus ", "page_idx": 40}, {"type": "equation", "text": "$$\nB(H)={\\left[\\begin{array}{l l l l l l l}{x^{1}}&{\\dots}&{x^{D}}&{x^{1}-c^{1}}&{\\dots}&{x^{D}-c^{D}}\\\\ {0}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}\\\\ {{\\mathcal{T}}_{1}}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{{\\mathcal{T}}_{L}}\\\\ {1}&{\\dots}&{\\dots}&{\\dots}&{\\dots}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "as desired. ", "page_idx": 40}, {"type": "text", "text": "Lemma 6 (Replacing FFN). Set $d_{e m b d}\\;=\\;5$ and $l~\\in~\\mathbb{N}$ . Let $H\\ \\in\\ \\mathbb{R}^{d_{e m b d}\\times l}$ be a structured intermediate embedding matrix. We can construct a feed-forward network $\\mathrm{FFN}_{r e p l a c e}\\in\\mathcal{F F N}(1)$ with the following output: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{FFN}_{r e p l a c e}(H)=\\left[\\begin{array}{c c c c}{-h_{1}^{1}+h_{1}^{2}}&{\\dots}&{-h_{l}^{1}+h_{l}^{2}}\\\\ {0}&{\\dots}&{0}\\\\ {0}&{\\dots}&{0}\\\\ {0}&{\\dots}&{0}\\\\ {0}&{\\dots}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In this case we say $\\mathrm{FFN}_{\\mathit{r e p l a c e}}$ replaces the the first row of $H$ with the second row. ", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma 6. Set ", "page_idx": 40}, {"type": "equation", "text": "$$\nW=\\left[\\begin{array}{c c c c c}{-1}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Applying $W$ token-wise gives the desired result. ", "page_idx": 40}, {"type": "text", "text": "Lemma 7 (Transformer Parallelization). Let $m_{1},m_{2},l_{1},l_{2},L_{\\mathrm{FFN1}},L_{\\mathrm{FFN2}},w_{\\mathrm{FFN1}},w_{\\mathrm{FFN2}},d_{e m b d}\\in\\mathbb{N}.$ . Fix transformer blocks $\\mathbf{B}\\;\\;\\in\\;\\;\\mathcal{B}(m_{1},L_{\\mathrm{FFN1}},w_{\\mathrm{FFN1}})$ with input $H_{1}\\;\\;\\in\\;\\;\\mathbb{R}^{d_{e m b d}\\times l_{1}}$ and $\\mathbf{B}_{2}\\quad\\in$ $B(m_{2},L_{\\mathrm{FFN2}},w_{\\mathrm{FFN2}})$ with input $H_{2}~\\in~\\mathbb{R}^{d_{e m b d}\\times l_{2}}$ . Further suppose both inputs are structured such that each token $h_{i,t}$ has an interaction term $\\mathcal{T}_{t}$ and constant term 1. Further suppose all attention heads in both ${\\bf{B}}_{1}$ and $\\mathbf{B}_{2}$ are implemented using the Interaction Lemma and all FFN layers are either Gating FFN layers or are independent of the interaction terms. Then there exists $\\mathrm{~B_{3}~}\\in\\;\\mathcal{B}(m_{1}+m_{2},\\mathrm{max}(L_{\\mathrm{FFN1}},L_{\\mathrm{FFN2}})+2,w_{\\mathrm{FFN1}}+w_{\\mathrm{FFN2}})$ which takes as input $H_{3}\\in\\mathbb{R}^{d_{e m b d}\\times(l_{1}+l_{2})}$ such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{B}_{3}(H_{3})=\\mathbf{B}_{3}{\\big(}\\,[H_{1}^{\\prime}\\quad H_{2}^{\\prime}]\\,{\\big)}=[\\mathbf{B}_{1}(H_{1})\\quad\\mathbf{B}_{2}(H_{2})]\\in\\mathbb{R}^{d_{e m b d}\\times(l_{1}+l_{2})}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $H_{i}^{\\prime}\\in\\mathbb{R}^{d_{e m b d}\\times l_{i}}$ is the same as $H_{i}$ except for the interaction terms where we have $\\mathcal{T}_{3,t}^{\\prime}=$ $\\begin{array}{r}{(c o s(\\frac{t}{l_{1}+l_{2}}\\frac{\\pi}{2}),s i n(\\frac{t}{l_{1}+l_{2}}\\frac{\\pi}{2}))f o r\\,1\\leq t\\leq l_{1}{+}l_{2}}\\end{array}$ . If ${\\bf{B}}_{3}$ satisfies this relationship we say ${\\bf{B}}_{3}$ parallelizes ${\\bf{B}}_{1}$ and $\\mathbf{B}_{2}$ . ", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma 7. Let $\\mathrm{\\bfB}_{1}(H_{1})\\;\\;=\\;\\;\\mathrm{FFN}_{1}(\\mathrm{MHA}_{1}(H_{1}))\\;+\\;\\mathrm{MHA}_{1}(H_{1})\\;+\\;H_{1}$ and ${\\bf B}_{2}(H_{2})\\mathrm{~\\boldmath~\\cal~{~B~}~}$ $\\mathrm{FFN_{2}}(\\mathrm{MHA_{1}}(H_{2}))+\\mathrm{MHA_{2}}(H_{2})+H_{2}$ . Let $\\mathrm{A}_{1,i_{1}}$ , $1\\,\\leq\\,i_{1}\\,\\leq\\,m_{1}$ , denote the attention heads of $\\mathrm{MHA_{2}}$ and $\\mathrm{A2},i_{2}$ , $1\\leq i_{2}\\leq m_{2}$ , denote the attention heads of $\\mathrm{MHA_{2}}$ . We construct ${\\bf{B}}_{3}$ as follows. First we construct the new input embedding matrix $H_{3}$ from $H_{1}$ and $H_{2}$ . For $1\\,\\leq\\,t\\,\\leq\\,l_{1}$ set $h_{3,t}\\;=\\;h_{1,t}$ except we define a new interaction term $\\begin{array}{r}{\\mathcal{Z}_{3,t}\\;=\\;\\big(c o s\\big(\\frac{t}{l_{1}+l_{2}}\\frac{\\pi}{2}\\big),s i n\\big(\\frac{t}{l_{1}+l_{2}}\\frac{\\pi}{2}\\big)\\big)}\\end{array}$ . For $l_{1}+1\\leq t\\leq l_{1}+l_{2}$ define $h_{3,t}=h_{3,t-l_{1}}$ except where we again must change only the interaction term. The main difficulty in defining $\\mathrm{MHA_{3}}$ will then simply be updating the interaction kernels of the Interaction Lemma structured attention heads to respect the new interaction terms $\\mathcal{T}_{t}$ , $1\\leq t\\leq l_{1}+l_{2}$ . ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "We now define the attention heads $\\mathrm{A3},j$ in $\\mathrm{MHA_{3}}$ . When $1\\le j\\le m_{1}$ write $Q_{3,j}=Q_{1,j}^{\\prime}$ where $Q_{1,j}^{\\prime}$ is the same $Q_{1,j}$ except for a new interaction kernel $Q_{1,j}^{\\prime I}$ . Let $h_{1,s_{j}}$ , $s_{j}\\in\\{1,...,l_{1}\\}$ , be the source token in $H_{1}$ interacting with $Q_{1,j}^{I}$ and $h_{1,t_{j}},t_{j}\\in\\{1,...,l_{1}\\}$ , be the target token of the old interaction kernel $Q_{1,j}^{I}$ (recall $Q_{1,j}^{I}$ is defined as the rotation of $\\mathcal{T}_{1,s_{j}}$ onto $\\mathbb{Z}_{1,t_{j}}.$ ). Then define $Q_{1,j}^{\\prime I}$ as a rotation onto the new target interaction kernel $\\mathcal{T}_{3,t_{j}}$ . Similarly define $K_{3,j}=K_{1,j}^{\\prime}$ where $K_{1,j}^{\\prime}=K_{1,j}$ up to a difference in interaction kernels. Let $K_{1,j}^{I}$ be the old interaction kernel which is a projection on to the target interaction term $\\mathcal{T}_{1,t_{j}}$ . Then define the new interaction kernel as a projection onto the new target interaction term $\\mathcal{T}_{3,t_{j}}$ . Simply set $V_{3,j}=V_{1,j}$ . Now we can check $\\mathbf{A}_{3,j}(H_{3})^{1,...,l_{1}}=\\mathbf{A}_{1,j}(H_{1})$ up to the interaction terms. Compute ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{A_{3,j}}(h_{3,s_{j}})=\\sum_{k=1}^{l_{1}+l_{2}}\\sigma(\\langle Q_{3,j}h_{3,s_{j}},K_{3,j}h_{3,k}\\rangle)V_{3,j}h_{3,k}}}\\\\ &{=\\sigma(\\langle Q_{3,j}h_{3,s_{j}},K_{3,j}h_{3,t_{j}}\\rangle)V_{3,j}h_{3,t_{j}}}\\\\ &{=\\sigma(\\langle Q_{1,j}h_{1,s_{j}},K_{1,j}h_{1,t_{j}}\\rangle)V_{1,j}h_{1,t_{j}}=A_{1,j}(h_{1,s_{j}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the second equality comes from the sparse construction of $\\mathrm{A}_{3,j}$ , the third equality comes from the construction of $Q_{3,j},K_{3,j}$ , and the last equality comes from the definition of the sparsely interacting $\\mathrm{A}_{1,j}$ . Otherwise $\\tilde{\\mathrm{A}}_{3,j}(\\tilde{h_{3,t}})=0$ for $t\\neq s_{j}$ . Thus we can conclude $\\mathrm{A}_{3,j}(H_{3})=\\mathrm{A}_{1,j}(H_{1})$ up to interaction terms. The case $l_{1}+1\\le j\\le l_{1}+l_{2}$ proceeds analogously. Thus we conclude ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{MHA_{3}}(H_{3})=[\\mathbf{B}_{1}(H_{1})\\quad\\mathbf{B}_{2}(H_{2})]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "up to the interaction terms. Now we must construct the parallelized feed-forward layer $\\mathrm{FFN_{3}}$ from $\\mathrm{FFN_{1}}$ and $\\mathrm{FFN_{2}}$ . Let $W_{1,k}$ be the $k$ th weight matrix of $\\mathrm{FFN_{1}}$ . By assumption we know $W_{1,k}$ is either a gating layer or is independent of token interaction terms. If $W_{1,k}$ is independent of interaction terms we can simply set $W_{1,k}^{\\prime}\\,=\\,W_{1,k}$ . Otherwise if $W_{1,k}$ is a gating layer on some half-space $\\{1,...,p\\},1\\leq p\\leq l_{1}$ , we must pick a new pivot vector $\\begin{array}{r}{v^{\\prime}=\\mathbf{R}_{\\frac{\\pi}{2}}(\\frac{\\mathcal{T}_{3,p}+\\mathcal{T}_{3,p+1}}{\\|\\mathcal{T}_{3,p}+\\mathcal{T}_{3,p+1}\\|_{2}})}\\end{array}$ . Set $W_{1,k}^{\\prime}$ accordingly to the new $v^{\\prime}$ . We may similarly choose $W_{2,k}^{\\prime}$ as we did for $W_{1,k}$ . Then we set the new layer $W_{3,k}$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\boldsymbol{W}_{3,k}=\\left[\\!\\!\\begin{array}{c c}{\\boldsymbol{W}_{1,k}^{\\prime}}&{\\boldsymbol{\\mathbf{0}}}\\\\ {\\boldsymbol{\\mathbf{0}}}&{\\boldsymbol{W}_{2,k}^{\\prime}}\\end{array}\\!\\!\\right]\\in\\mathbb{R}^{(w_{\\mathrm{FPN}1}+w_{\\mathrm{FPN}2})\\times(w_{\\mathrm{FPN}1}+w_{\\mathrm{FPN}2})},\\quad\\boldsymbol{b}_{3,k}=\\left[\\!\\!\\left[\\boldsymbol{b}_{1,k}\\right]\\!\\!\\right]\\in\\mathbb{R}^{w_{\\mathrm{FPN}1}+w_{\\mathrm{FPN}2}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We must also construct an embedding network $W_{3,0}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\nW_{3,0}=\\left[\\boldsymbol{I_{d_{e m b d}}}\\right]\\in\\mathbb{R}^{2d_{e m b d}t\\times d_{e m b d}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which duplicates the input token $h_{3,t}\\in\\mathbb{R}^{d_{e m b d}}$ for parallel processing. Finally, via lemma 4, we may construct a two-layer network $\\mathrm{FFN}_{1,g a t i n g}$ such that $\\mathrm{FFN_{1,gating}}(h_{3,t})=h_{3,t}$ if $1\\leq t\\leq l_{1}$ and has zeroed out data rows otherwise. We similarly define $\\mathrm{FFN_{2,gating}}$ for $l_{1}+1\\le t\\le l_{1}+l_{2}$ . Finally we define the output layer $W_{3,\\mathrm{{max}}(L_{\\mathrm{FFN1}},L_{\\mathrm{FFN2}})+2}$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left[I_{d_{e m b d}}\\quad I_{d_{e m b d}}\\right]\\in\\mathbb{R}^{d_{e m b d}\\times2d_{e m b d}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, for input token ${{h}_{3,t}},1\\leq t\\leq{{l}_{1}}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{FFN}_{3}(h_{3,t})=W_{3,\\operatorname*{max}(L_{\\mathrm{FFN}1},L_{\\mathrm{FFN}2})+2}\\left[\\mathrm{FFN}_{1,g a t i n g}\\mathrm{FFN}_{1}\\right.}&{\\mathrm{FFN}_{2,g a t i n g}\\mathrm{FFN}_{2}]\\left.W_{3,0}h_{3,t}\\right.}\\\\ {=W_{3,\\operatorname*{max}(L_{\\mathrm{FFN}1},L_{\\mathrm{FFN}2})+2}\\left[\\mathrm{FFN}_{1,g a t i n g}\\mathrm{FFN}_{1}\\right.}&{\\mathrm{FFN}_{2,g a t i n g}\\mathrm{FFN}_{2}\\right]\\left[h_{3,t}\\right]}\\\\ {=W_{3,\\operatorname*{max}(L_{\\mathrm{FFN}1},L_{\\mathrm{FN}2})+2}\\left[\\mathrm{FFN}_{1}(h_{3,t})\\right]}&{}\\\\ {=\\mathrm{FFN}_{1}(h_{3,t})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "H Other Lemmas ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "The following lemma [Chen et al., 2022, Lemma 2] is used for our construction of charts in the proof of Theorem 2. Lemma 8 (Local Diffeomorphism). Suppose Assumption $^{\\,l}$ holds for manifold $\\mathcal{M}$ and $r\\leq\\tau/4$ . Then the local neighborhood $U_{n}=B(c_{n},r)\\cap\\mathcal{M}$ is diffeomorphic to a subset of $\\mathbb{R}^{d}$ . In particular, the orthogonal projection $P_{n}$ onto the tangent space $T_{c_{n}}(M)$ is a diffeomorphism. ", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "[Yes] [No] [NA] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We develop approximation and statistical theory predicting transformer scaling laws by mathematically proving approximation and statistical theory which we validate on several pretraining datasets and scaling suites. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We discuss that our analysis does not extend to analyzing benchmark capabilities of LLMs, which are of much interest, and identify this direction as promising future work. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We clearly state all assumptions in Section 2 and give complete proofs in the appendix. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We share all our hyperparameters, code, data, and models publicly for $100\\%$ reproducibility. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 44}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We share all our hyperparameters, code, data, and models publicly for $100\\%$ reproducibility. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We specify all hyper-parameters used in our experiments. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [No] ", "page_idx": 45}, {"type": "text", "text": "Justification: We do not report error bars. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We specificy the compute resources used to run our experiments. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: We have reviewed the code of ethics and confirm we are consistent. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We include discussion of broader impacts in our conclusion. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 46}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: NA ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 47}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Yes, we give proper citation with urls and follow all licensing. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We provide a full set of instructions in the released code. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: NA ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: NA ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]