[{"figure_path": "N2wYPMpifA/figures/figures_5_1.jpg", "caption": "Figure 1: Diagram of the transformer architecture constructed in Theorem 2. T computes approximations of f(x) on each local chart Un \u2264 M by first projecting x to the tangent coordinates in Rd via on(x) and then approximating f(x) with local Taylor polynomials. A shallow sub-network computes indicators 1U for each local chart in parallel. The results of the two sub-networks are then multiplied together and summed to produce the final result. Here H\u2081 denotes the embedding matrix before the ith transformer block Bi.", "description": "This figure illustrates the architecture of the transformer network used to approximate a function f on a low-dimensional manifold M. The input x is first embedded and then processed by a series of transformer blocks.  The architecture uses a shallow network (logarithmic depth) to leverage the low dimensionality.  The process involves projecting the input onto local tangent spaces and approximating the function locally using Taylor polynomials. A separate subnetwork calculates indicator functions that select the relevant local approximations, which are then combined to produce the final approximation.", "section": "Transformer Generalization and Approximation Theory"}, {"figure_path": "N2wYPMpifA/figures/figures_6_1.jpg", "caption": "Figure 2: Observed and predicted data scaling laws on OpenWebText, The Stack-SQL, and Tiny Stories pretraining datasets. All estimates are close (\u00b10.02) and appear to reflect varying levels of pretraining data complexity. Note: \u00e2p denotes the empirically observed data scaling exponent and AD denotes the theoretically estimated exponent.", "description": "This figure compares the observed and predicted data scaling laws for three different language model pretraining datasets: OpenWebText, The Stack-SQL, and Tiny Stories.  The plots show the validation loss (a measure of generalization error) as a function of the number of training samples, shown on a log-log scale. Each dataset has its own plot, showing the ground truth, an empirical fit of the data, and a theoretical prediction from the authors' model.  The close agreement (\u00b10.02) between the empirical and theoretical exponents supports the authors' theory that the intrinsic dimension of the data significantly affects the scaling laws.  The differences between the datasets highlight varying levels of complexity in the pretraining data.", "section": "3 Predicting Empirical Scaling Laws and Validation on LLMs"}, {"figure_path": "N2wYPMpifA/figures/figures_7_1.jpg", "caption": "Figure 3: Observed and predicted model scaling laws in model size on GPT2 and Pythia scaling suites.  \u03b1\u0302N denotes the empirically observed scaling exponent, and \u03b1N denotes the theoretically predicted exponent. Note: we estimate \u03b1N for GPT2 using OpenWebText.", "description": "This figure shows the observed and predicted model scaling laws for GPT-2 and Pythia language models.  The observed scaling exponent (\u03b1\u0302N) is derived from empirical data, while the theoretical exponent (\u03b1N) is predicted by the authors' theory, which incorporates the intrinsic dimension of the data.  The figure visually compares these two exponents, demonstrating the agreement between the theoretical predictions and empirical observations, at least for GPT-2.  The differences are attributed to factors such as undertraining of the largest models and the intrinsic entropy of the data distribution.", "section": "Predicting Empirical Scaling Laws and Validation on LLMs"}, {"figure_path": "N2wYPMpifA/figures/figures_7_2.jpg", "caption": "Figure 4: Top left: Estimated ID vs. number of parameters. Top right: Estimated ID vs. the embedding dimension. Bottom left: Variation of estimated ID across model layers. Bottom right: Variation of estimated ID across context position.", "description": "This figure explores the impact of various model architecture hyperparameters on the estimated intrinsic dimension (ID) of the data.  The four subplots show how estimated ID changes with respect to model size (in billions of parameters), embedding dimension, layer depth, and context length, respectively. The results reveal a degree of stability in the estimated ID across these factors, with only minor changes observed in certain ranges.", "section": "3 Predicting Empirical Scaling Laws and Validation on LLMs"}, {"figure_path": "N2wYPMpifA/figures/figures_16_1.jpg", "caption": "Figure 5: Diagram of transformer block.", "description": "This figure shows the architecture of a single transformer block. It consists of a multi-head attention (MHA) layer and a feed-forward (FFN) layer. The input to the block is H1.  The MHA layer processes H1, and the output of the MHA layer is added to H1.  The result is then fed into the FFN layer. The FFN layer's output is added to the output of the MHA layer to produce the final output of the block, H2.", "section": "2.1 Transformer Neural Networks"}, {"figure_path": "N2wYPMpifA/figures/figures_17_1.jpg", "caption": "Figure 6: Diagram of a structured token. The first two rows contain mutable data used to compute the target function. The remaining rows are never changed after initialization.", "description": "This figure shows the structure of a structured token used in the transformer neural network. The token is divided into three parts: data terms (dynamic/mutable), interaction terms (static/immutable), and a constant term (static/immutable). The data terms are used to compute the target function, while the interaction and constant terms are used for other purposes. This structure is important for the efficiency of the transformer network.", "section": "2 Transformer Generalization and Approximation Theory"}, {"figure_path": "N2wYPMpifA/figures/figures_21_1.jpg", "caption": "Figure 7: Recursive assembly of partial products from constituent terms. Formally, Pn,k,i = Pn,k\u22121,2i\u22121Pn,k\u22121,2i With Pn,1,i = Si,ni for n \u2208 {1, ..., N}d, 1 \u2264 k \u2264 log2(d), 1 \u2264 i \u2264 2d.", "description": "This figure shows the recursive assembly of partial products from constituent terms. Each node represents a partial product, and the leaves represent the constituent terms (si,ni). The figure illustrates how the partial products are computed recursively, starting from the constituent terms and combining them pairwise at each level of the tree until the final partial product (pn,3,1) is obtained. The structure of the tree reflects the recursive nature of the computation, with each level of the tree corresponding to a step in the process.", "section": "Transformer Approximation Theory"}]