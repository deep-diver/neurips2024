[{"type": "text", "text": "Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "James Oldfield1\u2217 Markos Georgopoulos Grigorios G. Chrysos2 Christos Tzelepis3 Yannis Panagakis4,5 Mihalis A. Nicolaou6 Jiankang Deng7 Ioannis Patras1 ", "page_idx": 0}, {"type": "text", "text": "1Queen Mary University of London 2University of Wisconsin-Madison 3City University of London 4National and Kapodistrian University of Athens 5Archimedes AI, Athena RC 6The Cyprus Institute 7Imperial College London ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Mixture of Experts (MoE) paradigm provides a powerful way to decompose dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. However, a major challenge lies in the computational cost of scaling the number of experts high enough to achieve finegrained specialization. In this paper, we propose the Multilinear Mixture of Experts $\\mathbf{(\\mu\\mu\\mu\\mu\\mu\\nu}$ layer to address this, focusing on vision models. $\\mu\\mathrm{MoE}$ layers enable scalable expert specialization by performing an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, $\\mu\\mathrm{MoEs}$ (1) avoid the restrictively high inference-time costs of dense MoEs, yet (2) do not inherit the training issues of the popular sparse MoEs\u2019 discrete (non-differentiable) expert routing. We present both qualitative and quantitative evidence that scaling $\\mu\\mathrm{MoE}$ layers when fine-tuning foundation models for vision tasks leads to more specialized experts at the class-level, further enabling manual bias correction in CelebA attribute classification. Finally, we show qualitative results demonstrating the expert specialism achieved when pre-training large GPT2 and MLP-Mixer models with parameter-matched $\\mu\\mathrm{MoE}$ blocks at every layer, maintaining comparable accuracy. Our code is available at: https://github.com/james-oldfield/muMoE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Mixture of Experts (MoE) architecture [1] has reemerged as a powerful class of conditional computation, playing the pivotal role in scaling up recent large language [2, 3, 4, 5], vision [6], and multi-modal models [7]. MoEs apply different subsets of layers (referred to as \u2018experts\u2019) for each input, in contrast to the traditional approach of using the same single layer for all inputs. This provides a form of input-conditional computation [8, 9, 10, 11] that is expressive yet efficient. However, through their substantial performance gains, an important emergent property of MoEs is frequently underutilized: the innate tendency of experts to specialize in distinct subtasks. Indeed, the foundational work of Jacobs et al. [12] on MoEs describes this property, highlighting how implementing a particular function with modular building blocks (experts) often leads to subcomputations that are easier to understand individually than their dense layer counterparts\u2013with larger expert counts allowing for more fine-grained specialization. ", "page_idx": 0}, {"type": "text", "text": "Independent of model performance, a successful decomposition of the layer\u2019s functionality into human-comprehensible subtasks offers many significant benefits. Firstly, the mechanisms through which a network produces an output are more interpretable: the output is a sum of modular components, each contributing individual functionality. Yet, the value of interpretable computation extends beyond just transparency [13] and explainability [14]. An important corollary of successful task decomposition amongst experts is that layers are easier to debug and edit. Biased or unsafe behaviors can be better localized to specific experts\u2019 subcomputation, facilitating manual correction or surgery in a way that minimally affects the other functionality of the network. Addressing such behaviors is particularly crucial in the context of foundation models; being often fine-tuned as black boxes pre-trained on unknown, potentially imbalanced data distributions. Furthermore, there is evidence that traditional fairness techniques are less effective in large-scale models [15, 16]. However, to achieve fine-grained expert specialism at the class level (or more granular still), one needs the ability to significantly scale up the number of experts. When using only a small expert count, each expert is forced to process and generalize across multiple distinct semantic concepts, hindering specialization. Conversely, a large expert count means each can specialize to a more specific set of semantically similar inputs. Alas, the dominating \u2018sparse\u2019 MoE paradigm of selecting only the top- $K$ experts [17] is not only parameter-inefficient for large expert counts, but also has several well-known issues due to its discrete expert routing\u2013often leading to training instability and difficulties in scaling the total expert count, amongst other challenges [18, 19]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose the Multilinear Mixture of Experts $(\\mu\\mathrm{MoE})$ layer to address these issues. $\\mu\\mathrm{MoEs}$ are designed to scale gracefully to dense operations involving tens of thousands of experts at once through implicit computations on a factorized form of the experts\u2019 weights. Furthermore, in contrast to the dominant sparse MoEs\u2019 [17] non-differentiable nature, $\\mu\\mathrm{MoEs}$ are differentiable by design, and thus do not inherit the associated training issues. We summarize the beneftis of $\\mu\\mathrm{MoEs}$ \u2019 model form over existing MoEs in Table 1. Crucially, we show evidence that scaling up the number of $\\mu\\mathrm{MoE}$ experts leads to increased expert specialism when fine-tuning foundation models for vision tasks. Our evidence is provided in three forms: (1) firstly, through the usual qualitative evaluation of inspecting inputs by their expert coefficients. Secondly (2), we further explore the causal role of each expert through counterfactual interventions [20]. Lastly, (3) we show how final-layer $\\mu\\mathrm{MoE}$ expert specialism facilitates the practical task of model editing\u2013how subcomputation in specific combinations of experts biased towards demographic subpopulations can be manually corrected through straightforward guided edits. ", "page_idx": 1}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/2ced38a225b061b3494817a8cd3fa00676b9e6a02ba2698e2b52091153978070.jpg", "table_caption": ["Table 1: Benefits of the proposed $\\mu\\mathrm{MoEs}$ \u2019 model form over existing MoEs. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Building on these findings, we demonstrate that $\\mu\\mathrm{MoEs}$ offer a compelling alternative to MLPs for pre-training both vision and language models with up to 100M parameters\u2013enabling large numbers of specialized experts while maintaining comparable performance and parameter counts to the original networks\u2019 single dense MLPs. ", "page_idx": 1}, {"type": "text", "text": "Our contributions and core claims can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce $\\mu\\mathrm{MoE}$ layers\u2013a mechanism for computing vast numbers of subcomputations and efficiently fusing them conditionally on the input. \u2022 We show both qualitatively (through visualization) and quantitatively (through counterfactual intervention) that increasing the number of \u00b5MoE experts increases task modularity\u2013learning to specialize in processing just specific input classes when fine-tuning large foundation models for vision tasks. Further, we show manual editing of $\\mu\\mathrm{MoE}$ expert combinations can straightforwardly mitigate demographic bias in CelebA attribute classification. \u2022 We pre-train both language (GPT2) and vision (MLP-mixer) $\\mu\\mathrm{MoE}$ networks, establishing experimentally that models with parameter-matched $\\mu\\mathrm{MoE}$ blocks are competitive with existing MLP blocks whilst facilitating expert specialism (qualitatively) throughout. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Mixture of Experts Recent years have seen a resurgence of interest in the Mixture of Experts (MoE) architecture for input-conditional computation [17, 12, 21, 2]. One primary motivation for MoEs is their increased model capacity through large parameter count [17, 4, 2]. In contrast to a single dense layer, the outputs of multiple experts performing separate computations are combined (sometimes with multiple levels of hierarchy [22, 23]). A simple approach to fusing the outputs is by taking either a convex [23] or linear [24] combination of the output of each expert. The seminal work of Shazeer et al. [17] however proposes to take a sparse combination of only the top- $K$ most relevant experts, greatly reducing the computational costs of evaluating them all. More recent works employ a similar sparse gating function to apply just a subset of experts [2, 25], scaling to billions [3] and trillions of parameters [4]. The discrete expert selection choice of sparse MoEs is not without its problems, however\u2013often leading to several issues including training stability and expert under-utilization [18, 19]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Particularly relevant to this paper are works focusing on designing MoE models to give rise to more interpretable subcomputation [26, 27, 28]\u2013hearkening back to one of the original works of Jacobs et al. [12], where experts learned subtasks of discriminating between different lower/uppercase vowels. Indeed a common observation is that MoE experts appear to specialize in processing inputs with similar high-level features. Researchers have observed MoE experts specializing in processing specific syntax [17] and parts-of-speech [29] for language models, and foreground/background [30] and image categories (e.g. \u2018wheeled vehicles\u2019) [24] in vision. Evidence of shared vision-language specialism is even found in the multi-modal MoEs of Mustafa et al. [7]. ", "page_idx": 2}, {"type": "text", "text": "Several works instead target how to make conditional computation more efficient: by sharing expert parameters across layers [31], factorizing gating network parameters [32], or dynamic convolution operations [33]. Relatedly, Gao et al. [34] jointly parameterize the experts\u2019 weight matrices with a Tensor-Train decomposition [35]. However, such approach still suffers from the Sparse MoE\u2019s instability and expert under-utilization issues, and stochastic masking of gradients must be performed to lead to balanced experts. Furthermore, whilst Gao et al. [34] share parameters across expert matrices, efficient implicit computation of thousands of experts simultaneously is not facilitated, in contrast to the $\\mu\\mathrm{MoE}$ layer. ", "page_idx": 2}, {"type": "text", "text": "Factorized layers in the context of deep neural networks provide several important benefits. Replacing traditional operations with low-rank counterparts allows efficient fine-tuning [36] / training [37, 38], and modeling of higher-order interactions [39, 40, 41, 42, 43], and convolutions [44]. In addition to reducing computational costs, tensor factorization has also proven beneficial in the context of multi-task/domain learning [45, 46] through the sharing of parameters/low-rank factors across tasks. Furthermore, parameter efficiency through weight factorization often facilitates the design and efficient implementation of novel architectures such as polynomial networks [47, 48, 49] or tensor contraction layers [50]. The recent DFC layer in Babiloni et al. [51] also performs dynamic computation using the CP decomposition [52] like $\\mu\\mathrm{MoEs}$ . Nevertheless, the two works have very different goals and model properties due to how the weight matrices are generated. $\\mu\\mathrm{MoEs}$ take a sparse, convex combination of $N$ explicit experts\u2019 latent factors. This consequently leads to specialized subcomputations in a way that facilitates the interpretability and editability presented in this paper. DFCs can be seen to apply an MLP to input vectors at this step in analogy, which does not provide the necessary model properties of interest here. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first formulate the proposed $\\mu\\mathrm{MoE}$ layer in Section 3.1, introducing 2 unique resource-efficient models and forward passes in Section 3.1.1. Finally, we show in Section 3.1.2 how \u00b5MoEs recover linear MoEs as a special case. ", "page_idx": 2}, {"type": "text", "text": "Notation We denote scalars $x\\in\\mathbb{R}$ with lower-case letters, and vectors $\\mathbf{x}\\,\\in\\,\\mathbb{R}^{I_{1}}$ and matrices $\\mathbf{X}\\in\\mathbb{R}^{I_{1}\\times I_{2}}$ in lower- and upper-case boldface latin letters respectively. Tensors $\\mathcal{X}\\in\\mathbb{R}^{I_{1}\\times I_{2}\\times\\dots\\times I_{d}}$ of order $d$ are denoted with calligraphic letters. We refer to the $(i_{1},i_{2},\\ldots,i_{d})$ -th element of this tensor with both $\\mathcal{X}(i_{1},i_{2},\\ldots,i_{d})\\in\\mathbb{R}$ and $x_{i_{1}i_{2}...i_{d}}\\in\\mathbb{R}$ . Finally, we use a colon to index into all elements along a particular mode: given $\\mathcal{X}\\in\\mathbb{R}^{I_{1}\\times I_{2}\\times I_{3}}$ for example, $\\mathbf{X}_{::i_{3}}\\in\\mathbb{R}^{I_{1}\\times I_{2}}$ or equivalently $\\mathcal{X}(:,:,i_{3})\\in\\bar{\\mathbb{R}}^{I_{1}\\overset{.}{\\times}I_{2}}$ is the matrix at index $i_{3}$ of the final mode of the tensor. We use ${\\boldsymbol{\\mathcal{X}}}\\times{\\boldsymbol{n}}$ u to denote the mode- $\\boldsymbol{n}$ (vector) product [53] of a tensor $\\mathcal{X}\\in\\mathbb{R}^{I_{1}\\times I_{2}\\times\\dots\\times I_{N}}$ and vector $\\mathbf{u}\\in\\mathbb{R}^{I_{n}}$ whose resulting elements are given by (X \u00d7n u)i1...in\u22121in+1...iN = iInn=1 xi1i2...iN uin. ", "page_idx": 2}, {"type": "text", "text": "3.1 The $\\pmb{\\mu}\\mathbf{M0E}$ layer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "$\\mu\\mathrm{MoEs}$ provide a scalable way to execute and fuse large numbers of operations on an input vector by formalizing conditional computation through resource-efficient multilinear operations. A $\\mu\\mathrm{MoE}$ layer comprised of $N$ many experts (and a single level of expert hierarchy) is parameterized by weight tensor $\\boldsymbol{\\mathcal{W}}\\ \\in\\ \\mathbb{R}^{\\dot{N}\\times I\\times O}$ and expert gating parameter $\\textbf{G}\\in$ $\\mathbb{R}^{\\tilde{I}\\times N}$ . Given an input vector $\\mathbf{\\dot{z}}\\,\\in\\,\\mathbb{R}^{I}$ (denoting the hidden representation of an individual token, for example), its forward pass can be expressed through the series of tensor contractions: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}=\\phi(\\mathbf{G}^{\\top}\\mathbf{z})\\in\\mathbb{R}^{N},}\\\\ &{\\mathbf{y}=\\mathcal{W}\\times_{1}\\mathbf{a}\\times_{2}\\mathbf{z}}\\\\ &{\\quad=\\displaystyle\\sum_{n=1}^{N}\\sum_{i=1}^{I}\\mathbf{w}_{n i:}z_{i}a_{n}\\in\\mathbb{R}^{O},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{a}$ is the vector of expert coefficients and $\\phi$ is the entmax activation [54, 55]. The $\\mu\\mathrm{MoE}$ layer can be understood as taking a sparse, convex combination of $N$ many affine transformations2 of input vector ${\\bf z}$ , weighted by the coefficients in a. The first tensor contraction in the forward pass $(\\sum_{i}\\mathbf{W}_{:i:}z_{i}\\in\\mathbb{R}^{N\\times O})$ matrixmultiplies the input vector with every expert\u2019s weight matrix. The following tensor contraction with expert coefficients a takes a linear combination of the results, yielding the output vector. The forward pass can be visualized intuitively as ", "page_idx": 3}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/b75e06e0dd2b3bb3486b173393f22dbb7fbec1542dbedfd0e30a5e8f7e2ff084.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: The forward pass of an (unfactorized) $\\mu\\mathrm{MoE}$ layer as a series of tensor contractions: the experts\u2019 weight matrices (yellow 2D slices) are matrix-multiplied with the input vector and summed (weighted by the red expert coefficients). ", "page_idx": 3}, {"type": "text", "text": "multiplying and summing over the modes in a 3D tensor, which we illustrate in Figure 1. Furthermore, $\\mu\\mathrm{MoEs}$ readily generalize to hierarchical conditional computations by introducing additional modes to the weight tensor and corresponding vectors of expert coefficients (see Appendix E). ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Computation in factorized form ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our key insight is that the dense $\\mu\\mathrm{MoE}$ forward pass over all $N$ experts simultaneously can be computed entirely in factorized form, needing never materialize prohibitively large weight tensors. This allows $\\mu\\mathrm{MoEs}^{\\star}$ computations to scale gracefully to many thousands of experts simultaneously, without the problematic top- $K$ gating [17]. To achieve this, we (1) first parameterize the experts\u2019 weights $\\mathcal{W}\\in\\mathbb{R}^{N\\times I\\times O}$ with a tensor factorization and (2) re-derive fast forward passes of Equation (1) to operate solely in factorized form. ", "page_idx": 3}, {"type": "text", "text": "In the context of a $\\mu\\mathrm{MoE}$ layer, the various choices of tensor factorizations make different trade-offs regarding parameter/FLOP counts and rank constraints. We derive two unique resource-efficient $\\mu\\mathrm{MoE}$ variants to suit different computational budgets and choices of expert counts. We now present the derivations of the forward passes of the factorized $\\mu\\mathrm{MoE}$ models (with einsum pseudocode implementations in Appendix B): ", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{CP}\\mu\\mathbf{MoE}$ Imposing CP structure [52, 56] of rank $R$ on the weight tensor, we can write $\\mathcal{W}=$ $\\begin{array}{r}{\\sum_{r=1}^{R}\\mathbf{u}_{r}^{(1)}\\circ\\mathbf{u}_{r}^{(2)}\\circ\\mathbf{u}_{r}^{(3)}\\ \\in\\ \\mathbb{R}^{N\\times I\\times O}}\\end{array}$ as a sum of $R$ outer products, with factor matrices $\\mathbf{U}^{(1)}\\in$ $\\mathbb{R}^{R\\times N}$ , $\\mathbf{U}^{(2)}\\in\\mathbb{R}^{R\\times I}$ , $\\mathbf{U}^{(3)}\\in\\mathbb{R}^{R\\times O}$ . This reduces the parameter count from $N I O$ (such as with sparse/dense MoEs and regular $\\mu\\mathrm{MoEs}$ ) to just $R(N+I+O)$ . Crucially, we can further rewrite the $\\mathrm{CP}\\mu\\mathrm{MoE}$ layer\u2019s forward pass entirely in factorized form without ever materializing the full tensor (plugging the CP-composed tensor into Equation (1)) as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\sum_{n=1}^{N}\\sum_{i=1}^{I}\\left(\\sum_{r=1}^{R}\\mathbf{u}_{r}^{(1)}\\circ\\mathbf{u}_{r}^{(2)}\\circ\\mathbf{u}_{r}^{(3)}\\right)_{n i:}z_{i}a_{n}=\\sum_{r=1}^{R}\\left(\\mathbf{U}^{(2)}\\mathbf{z}\\right)_{r}\\left(\\mathbf{U}^{(1)}\\mathbf{a}\\right)_{r}\\mathbf{u}_{r}^{(3)}\\in\\mathbb{R}^{O},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with Equation (2) being analogous to the fast computation in Babiloni et al. [51], only here the operations of combining the weights and producing the outputs can be expressed in a single step. Whilst the original naive $\\mathrm{CP}\\mu\\mathrm{MoE}$ forward pass has a FLOP count3 of $N I O$ , the fast computation above has just $R(N+I+O)$ (the same number of factorized layer parameters). With moderate values of both $R$ and $N$ , the layer becomes significantly more resource-efficient than vanilla $\\mu\\mathrm{MoEs}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{TR}\\mu\\mathbf{M0}\\mathbf{E}$ We propose a second $\\mu\\mathrm{MoE}$ variant based on the Tensor Ring [58] (TR) factorization that can offer even better efficiency for large values of $N$ . In TR format, $\\mathcal{W}\\,\\,\\,\\in\\,\\mathbb{R}^{\\bar{N}\\times I\\times O}$ has three factor tensors: $\\mathcal{U}^{(1)}\\in\\mathbb{R}^{R_{1}\\times N\\times R_{2}}$ , $\\mathcal{U}^{(2)}\\in\\mathrm{~\\overline{{R}}~}^{R_{2}\\times I\\times R_{3}}$ , $\\mathcal{U}^{(3)}\\,\\in\\,\\mathbb{R}^{R_{3}\\times O\\times R_{1}}$ , where $R_{i}$ are the manually chosen ranks4. The weight tensor\u2019s elements in TR format are given by: $w_{n i o}=\\mathrm{tr}\\big(\\mathbf{U}_{:n:}^{(1)}\\mathbf{U}_{:i:}^{(2)}\\mathbf{U}_{:o:}^{(3)}\\big)$ [58]. $\\mathrm{TR}\\mu\\mathrm{MoE}$ \u2019s forward passes can be computed efficiently by contracting the first two factor tensors with the input/expert coefficients vectors and then combining the results: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\sum_{n=1}^{N}\\sum_{i=1}^{I}\\mathbf{w}_{n i:}z_{i}a_{n}=\\sum_{r_{1}=1}^{R_{1}}\\sum_{r_{3}=1}^{R_{3}}\\big(\\underbrace{(\\mathcal{U}^{(1)}\\times_{2}\\mathbf{a})(\\mathcal{U}^{(2)}\\times_{2}\\mathbf{z})}_{[R_{1}\\times R_{3}]}\\big)_{r_{1}r_{3}}\\mathbf{u}_{r_{3}:r_{1}}^{(3)}\\in\\mathbb{R}^{O},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "yielding a modified FLOP count of $\\left(R_{1}N R_{2}+R_{2}I R_{3}+R_{1}R_{2}R_{3}+R_{1}O R_{3}\\right)$ with just $(R_{1}N R_{2}+$ $R_{2}I R_{3}+R_{3}O R_{1})$ parameters. With large $N$ contributing to the computational cost only through $R_{1}N R_{2}$ , the $\\mathrm{TR}\\mu\\mathrm{MoE}$ can prove even more resource-efficient than $\\mathrm{CP}\\mu\\mathrm{MoEs}$ by choosing small values of $R_{1},R_{2}$ . We refer readers to Appendix D for a further discussion of decomposition choice, derivations of how tensor rank translates to expert matrix rank, and FLOPs comparisons. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 $\\pmb{\\mu}\\mathbf{M0Es}$ recover dense MoEs as a special case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Finally, we note how unfactorized $\\mu\\mathrm{MoE}$ layers with a single level of expert hierarchy recover dense MoE layers [17, 11] as a special case. When computing Equation (1) over the full materialized weight tensor, one can alternatively write the output element-wise as $y_{o}=\\mathbf{a}^{\\top}\\mathbf{W}_{::o}\\mathbf{z}$ . This highlights an interesting technical connection between neural network layers: dense MoE layers in this tensor formulation can be seen to share a similar functional form to bilinear layers, which have also found applications in interpretability [59, 60]. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start in Section 4.1 by presenting both qualitative and quantitative experiments validating that the experts learn to specialize in processing different semantic clusters of the input data. In Section 4.2 we demonstrate one practical benefti of the learned specialism\u2013showing how expert-conditional re-writing can correct for specific demographic bias in CelebA attribute classification. Finally, in Section 4.3 we train both large language and large vision models with $\\mu\\mathrm{MoE}$ layers throughout\u2013providing qualitative evidence of expert specialism and model performance competitive with networks using MLP blocks. Please see Appendix H for detailed ablation studies, and Appendix I for experiments with hierarchical $\\mu\\mathrm{MoEs}$ . ", "page_idx": 4}, {"type": "text", "text": "Implementation details Before applying the activation function to the expert coefficients we apply batch- and layer-normalization to $\\mu\\mathrm{MoE}$ layers in vision and language models respectively (see Appendix H.3 for an ablation). Interestingly, we do not find the need for any load-balancing losses. We fix the $\\mathrm{TR}\\mu\\mathrm{MoE}$ ranks to be $R_{1}=R_{2}=4$ throughout (see Appendix D.1.2). ", "page_idx": 4}, {"type": "text", "text": "4.1 Expert specialism: visualization & intervention ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our first objective is to show that scaling $\\pmb{\\mu}\\mathbf{M0E}$ \u2019s expert count leads to more specialized experts. We provide evidence of this effect both qualitatively (through visualization) and quantitatively (through intervention). ", "page_idx": 4}, {"type": "text", "text": "To isolate the impact of $\\mu\\mathrm{MoE}$ layers and varying expert counts, we first explore the controlled setting of fine-tuning large foundation models CLIP [61] ViT-B-32 and DINO [62] on ImageNET1k (following the fine-tuning protocol in Ilharco et al. [63, 64]). Whilst fine-tuning large foundation models is an important application of $\\mu\\mathrm{MoE}$ layers in its own right (e.g. as explored later in Section 4.2 for fairer models), the ability to cheaply train many models with different $\\mu\\mathrm{MoE}$ layer configurations forms an ideal setting in which to study their properties. ", "page_idx": 4}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/b382a56cf1a28fd562d6d6fc5ebe47484c3e3e9a601de5b9106c51ef15ec9d82.jpg", "img_caption": ["Figure 2: Specialization in 256 vs 32 total expert $\\mathrm{CP}\\mu\\mathrm{MoE}$ layers (fine-tuned on CLIP ViT-B-32). Each row displays randomly selected images processed (with coefficient $\\ge0.5$ ) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialism (to single visual themes or image categories). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1.1 Qualitative results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first show random examples in Figure 2 of images processed (with expert coefficient $\\ge0.5$ ) by the experts by each $\\mathrm{CP}\\mu\\mathrm{MoE}$ layer (the class labels and expert coefficients are overlaid in white and green text respectively). Using only a modest number of experts (e.g. 32) appears to lead to some \u2018polysemanticity\u2019 [65] in experts\u2013with some processing unrelated classes of images (e.g. \u2018gators\u2019, \u2018limos\u2019, and a \u2018quilt\u2019 for Expert 1 on the right). On the other hand, using a much larger number of total experts appears to yield more specialization, with many experts contributing their computation to only images of the same single class label or broader semantic category. Please see Figure 16 in the Appendix for many more random images for the first 10 experts per model to observe this same trend more generally, and Figure 17 for even finer-grained specialism with 2048-expert $\\mu\\mathrm{MoE}$ layers. ", "page_idx": 5}, {"type": "text", "text": "4.1.2 Quantitative results: expert monosemanticity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The qualitative evidence above hints at the potential of a prominent benefti to scaling up the number of experts with $\\mu\\mathrm{MoEs}$ . Such subjective interpretations alone about expect specialism are hypotheses, rather than conclusions however [66]. Similarities in images processed by the same expert give us an intuitive explanation of its function but do not show the expert\u2019s computation contributes causally [20, 67, 68] to the subtask of processing specific human-understandable patterns of input features [69, 70]. However, the absence of ground-truth labels for interpretable features of the input one may be interested in (e.g. specific types of textures in images, or words related to \u2018Harry Potter\u2019) makes this difficult to quantify in any objective or systematic manner. ", "page_idx": 5}, {"type": "text", "text": "Despite the absence of fine-grained labels, we can quantify and compare the class-level specialism a $\\mu\\mathrm{MoE}$ expert exhibits on the ImageNET1k dataset as an (imperfect) proxy [71]. ", "page_idx": 5}, {"type": "text", "text": "Following the causal intervention protocol of Elazar et al. [20], we ask the specific counterfactual question about solely each expert $n$ in a $\\mu\\mathrm{MoE}$ layer in turn: \u201chad expert n\u2019s weight matrix $\\mathbf{W}_{n}$ not contributed its computation, would the network\u2019s test-set accuracy for class c have dropped?\u201d Practically speaking, given a network fine-tuned with an $\\mu\\mathrm{MoE}$ layer, we achieve this by intervening in the forward pass by zeroing the $n^{\\mathrm{th}}$ expert\u2019s weight matrix $\\mathbf{W}_{n}\\,:=\\,\\mathbf{0}$ , leaving every other aspect of the forward pass completely untouched. Let the elements of $\\mathbf{y},\\hat{\\mathbf{y}}^{(n)}\\in\\dot{\\mathbb{R}}^{C}$ denote the test set accuracy for the $C\\,=\\,1000$ ImageNET1k classes, pre- and post-intervention of expert $n$ respectively. We collect the normalized difference to per-class accuracy in the vector $\\mathbf{d}^{(n)}$ , whose elements are given by d(cn) $d_{c}^{(n)}\\,=\\,(y_{c}\\,-\\,\\hat{y}_{c}^{(n)})/y_{c}$ . At the two extremes, when the full network\u2019s accuracy for class $c$ drops completely from $y_{c}$ ", "page_idx": 5}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/c1fdf4ec98f56d59bf7aab46c6d769b1d9e070fe2c541335949412b12a71ae7f.jpg", "img_caption": ["Figure 3: Higher expert counts lead to more monosemantic experts: mean expert class-level polysemanticity of Equation (4) (\u2193) as a function of the total number of experts. Results are shown for both CLIP ViT-B-32 and DINO models fine-tuned on ImageNET1k with $\\mathrm{CP}\\mu\\mathrm{MoE}$ layers. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "to 0 upon manually excluding expert $n$ \u2019s computation we get d(cn)= 1, whilst d(cn)= 0 means the absence of the subcomputation did not change class c\u2019s test set accuracy at all. We thus estimate the \u2018class-level polysemanticity\u2019 of expert $n$ as the distance between ", "page_idx": 5}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/6fcf3ca985524a9b19994975149ce9f50e45b4b6bf1a0ee50a2b886adf5e7d6b.jpg", "table_caption": ["Table 2: Fairness metrics for baseline models and after applying standard fairness techniques, for the two experiments on CelebA. A CP\u00b5MoE-r512-e128 model is used as the final layer. "], "table_footnote": ["the difference vector and the one-hot vector: "], "page_idx": 6}, {"type": "equation", "text": "$$\np^{(n)}=||\\mathbf{d}^{(n)}-\\mathbb{1}^{(n)}||_{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where index $\\mathrm{argmax}_{c}(d_{c}^{(n)})$ of $\\mathbb{1}^{(n)}$ has a value of 1 (and values of 0 everywhere else). This encodes the signature of a perfectly class-level monosemantic expert, for which all accuracy for a single class alone is lost in the counterfactual scenario in which the expert $n$ did not contribute. We plot in Figure 3 the average expert polysemanticity $p^{(n)}$ for all experts with non-zero difference vectors5, observing a steady drop in its value as $N$ increases from 32 to 1024 total experts. In other words, increasing $N$ leads to individual experts increasingly responsible for a single subtask: classifying all inputs of just one class. As shown in Figure 3 we observe this trend both when $\\mu\\mathrm{MoEs}$ are used as final classification layers and as penultimate layers (followed by a ReLU activation and linear classification layer), and for multiple pre-trained foundation models. We further refer readers to the bar plots of the values of $\\mathbf{d}^{(n)}$ (the per-class accuracy changes) in Figures 18 and 19, where this trend is observable through mass concentrated on increasingly fewer class labels as the number of experts increases. ", "page_idx": 6}, {"type": "text", "text": "4.2 Expert re-writing: conditional bias correction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We further validate the modular expert hypothesis of $\\mu\\mathrm{MoEs}$ and simultaneously provide a concrete example of its usefulness by correcting demographic bias in attribute classification. Classifiers trained to minimize the standard binary cross-entropy loss often exhibit poor performance for demographic subpopulations with low support [72, 73]. By identifying which combination of experts is responsible for processing target subpopulations, we show how one can straightforwardly manually correct mispredictions in a targeted way\u2013without any re-training. ", "page_idx": 6}, {"type": "text", "text": "We focus on mitigating bias towards two low-support subpopulations in models with $\\mu\\mathrm{MoE}$ final layers fine-tuned on CelebA [74]: (a) bias towards images labeled as \u2018old females\u2019 for age prediction [75], and (b) bias towards images labeled as \u2018blond males\u2019 for blond hair prediction [15]. Concretely, we train $N=128$ multi-label $\\mu\\mathrm{MoE}$ final layer models for the 40 binary attributes in CelebA, jointly optimizing a pre-trained CLIP ViT-B-32 model [61] backbone, again following the fine-tuning setup in Ilharco et al. [63, 64]. All results presented in this section are the average of 10 runs with different random seeds. ", "page_idx": 6}, {"type": "text", "text": "Experimental setup Let $C$ be a set collecting the expert coefficients $\\mathbf{a}\\in\\mathbb{R}^{N}$ from forward passes of the training images belonging to the target subpopulation. We evaluate the subpopulation\u2019s mean expert coefficients $\\begin{array}{r}{\\bar{\\mathbf{a}}={1}/{|\\bar{C}|}\\,\\dot{\\sum}_{\\mathbf{a}\\in C}\\,\\mathbf{a}\\in\\mathbb{R}^{N}}\\end{array}$ , proposing to manually re-write the output of this expert combination. We modify the layer\u2019s forward pass for the $o^{\\mathrm{th}}$ output head for attribute of interest (e.g. \u2018blond hair\u2019) as: ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{o}=\\mathbf{a}^{\\top}\\mathbf{W}_{::o}\\mathbf{z}+\\lambda\\bar{\\mathbf{a}}^{\\top}\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, the term $\\lambda\\mathbf{\\bar{a}}\\in\\mathbb{R}^{N}$ specifies, for each expert, how much to increase/decrease the logits for attribute $o$ , with $\\lambda$ being a scaling hyperparameter6. Taking the dot product with an input image\u2019s expert coefficients a applies the relevant experts\u2019 correction terms (in the same way it selects a subset of the most relevant experts\u2019 weight matrices). We report a range of standard fairness metrics for both the model rewriting and networks trained with existing techniques (that aim to mitigate demographic bias without requiring images\u2019 sensitive attribute value at test time). These are shown in Table 2 for the two different experiments on CelebA, where the proposed intervention outperforms baseline alternative methods in the majority of settings. Please see Appendix J for details about the baseline methods and fairness metrics used, and further discussion of results. ", "page_idx": 6}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/dcd4570d3a105d8a1098ef5a367fd2546d0ec52fc0f5ccc60166c0217248879a.jpg", "img_caption": ["Figure 4: Top-activating patches (top rows) and their full images (second rows) for the first 3 experts across $2\\;\\mathrm{CP}\\mu\\mathrm{MoE-e}64$ layers in $\\mu\\mathrm{MoE}$ MLP-mixer [80] models\u2013 $\\cdot\\mu\\mathrm{MoE}$ blocks exhibit coarse-grained specialism (e.g. texture) earlier and more fine-grained specialism (e.g. objects) deeper in the network. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Large language/vision $\\pmb{\\mu}\\mathbf{M0E}$ networks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finally, we train from scratch 12 layer 124M-parameter GPT-2 [81] LLMs on OpenWebText [82] for the language domain and 8 layer S-16 variant7 MLP-Mixers [80] on ImageNET1k [83] for vision. We replace every MLP block\u2019s 2 linear layers with 2 $\\mu\\mathrm{MoE}$ layers. Each token $t$ \u2019s input vector $\\mathbf{z}_{t}\\in\\mathbb{R}^{I}$ is therefore transformed with $\\mu\\mathrm{MoE}$ blocks of the form: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t}=\\sum_{n_{2}=1}^{N}\\sum_{h=1}^{H}\\mathbf{w}_{n_{2}h.}^{(2)}\\mathrm{GELU}\\bigg(\\sum_{n_{1}=1}^{N}\\sum_{i=1}^{I}\\mathbf{w}_{n_{1}i:}^{(1)}.z_{t i}a_{t n_{1}}\\bigg)_{h}a_{t n_{2}},\\quad\\mathbf{a}_{t}=\\phi(\\mathbf{G}^{\\top}\\mathbf{z}_{t}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathbf{a}_{t}\\in\\mathbb{R}^{N}$ are the expert coefficients for each specific token and block, $H$ is the dimension of the block\u2019s hidden layer, and $\\mathcal{W}^{(1)}\\in\\mathbb{R}^{N\\times I\\times H}$ , $\\mathcal{W}^{(2)}\\in\\mathbb{R}^{N\\times H\\times O}$ are the (implicit) $\\mu\\mathrm{MoE}$ weight tensors for each of the two layers. We manually set the $\\mu\\mathrm{MoE}$ ranks to parameter-match each original network and set the number of experts (per block) to $N=64$ for vision models and $N=256$ for LLMs. Consequently, with this configuration, each layer\u2019s $\\pmb{\\mu}\\mathbf{M0E}$ block performs computations with $N$ experts yet has the same parameter counts and FLOPs as a single, dense MLP block. ", "page_idx": 7}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/5ac775b302a4914f9b6f200fba441050e6ca4fe9e21b730b31d1a931fd56a6b6.jpg", "img_caption": ["Figure 5: Top-activating generated tokens for 4 manually selected experts for GPT-2 trained with $\\mathrm{CP}\\mu\\mathrm{MoE}$ blocks at every layer (each token is highlighted by the coefficient of the expert in question), exhibiting specializations to concepts including compound adjectives and equality operators. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "$\\pmb{\\mu}\\mathbf{M0E}$ -Mixer For vision, our key findings are that earlier $\\mu\\mathrm{MoE}$ channel-mixing blocks\u2019 experts appear (qualitatively) to exhibit specialisms to colors, shapes, and textures, whilst later layers exhibit more object-specific specialization. We plot the patches from the training set for which each expert most contributes its computation in Figure 4 for both a shallow and deep layer to illustrate this\u2013earlier layers\u2019 experts contribute strongly to the processing of similar patches (top rows, e.g. specific edges) whilst later layers\u2019 experts process tokens based more on the similarity of their surrounding semantic context (bottom rows, e.g. images of animals). We further show in Figure 12 results for the first 2 experts across all 8 blocks where such scale-specific specialism is apparent across the entire network. ", "page_idx": 8}, {"type": "text", "text": "$\\pmb{\\mu}\\mathbf{M0E}$ -GPT2 For LLMs, we see promising qualitative evidence of experts specializing throughout a corpus of 1M generated 100-token sequences. At layer 5, for example, the generated tokens that use expert 8 with the highest coefficient are compound adjectives (Figure 5), whilst expert 37 most highly activates for equality and comparison operators in code and scientific text (please see examples of many unflitered experts in Figures 13 and 14). Whilst monosemanticity is not always attained, $\\mu\\mathrm{MoE}$ layers nonetheless facilitate a level of specialism not facilitated by dense MLP layers. ", "page_idx": 8}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/77b4e445fcfec0edaa1e37951f124b45a9578598cc5cba589dc8a270cbf9237b.jpg", "table_caption": ["Table 3: Comparison of $\\mu\\mathrm{MoEs}$ and dense MLPs across different models and tasks. We use $N=64$ $\\mu\\mathrm{MoE}$ experts for the two vision tasks and $N\\,=\\,256$ for GPT2. MLP mixers and GPT2s are pre-trained for 300 epochs and 100k iterations respectively, whilst CLIP is fine-tuned for 10 epochs. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "One important result here is that $\\mu\\mathrm{MoE}$ networks in this setup are significantly more parameterefficient than both dense and sparse MoEs with the same expert count, as shown in Table 4. For example, GPT-2 models with 256 sparse/dense MoE experts require a prohibitive 14.5B MLP parameters alone, relative to just 57M MLP parameters with $\\mu\\mathrm{MoEs}$ of the same expert counts. ", "page_idx": 9}, {"type": "text", "text": "$\\pmb{\\mu}\\mathbf{M0E}$ performance Finally, we substantiate our claim that networks pre-trained and fine-tuned with parameter-matched $\\mu\\mathrm{MoE}$ layers are competitive with their existing linear layer alternatives across multiple domains/machine learning tasks. We present in Table 3 the performance results for MLP-Mixer S-16 [80], NanoGPT GPT-2 [81], and (fine-tuned) CLIP ViT-B-32 [61] models on the OWT and ImageNET1k datasets. Following Section 4.1.1, we replace all linear layers with $\\mu\\mathrm{MoE}$ blocks (and a single $\\mu\\mathrm{MoE}$ final layer for fine-tuning CLIP). We initialize all linear layers following the default PyTorch $U[-k,k]$ initialization for a fair comparison. Please see Appendix F for experimental details and learning curves, and Appendix I for experiments with varying expert count and hierarchical $\\mu\\mathrm{MoEs}$ . Crucially, whilst $\\mu\\mathrm{MoE}$ layers provide additional interpretability benefits through scalable expert specialization, they do not sacrifice accuracy when parameter-matched to MLP blocks, as seen from the comparable performance. ", "page_idx": 9}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/da2c37e59448b754e40c5483bdeafab643f64b16241070e907fbf16ca1883b17.jpg", "table_caption": ["Table 4: MLP parameters required for networks with the same expert counts. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced the Multilinear Mixture of Experts layer $\\mathbf{\\nabla}\\mu\\mathrm{MoE})$ . We demonstrated that larger expert counts lead to increased specialization, and how $\\mu\\mathrm{MoE}$ layers make this computationally tractable through factorized forward passes. $\\mu\\mathrm{MoEs}$ scale to large expert counts much more gracefully than existing MoEs, yet avoid the issues from popular gating mechanisms. As a further practical example of $\\mu\\mathrm{MoE}$ \u2019s task decomposition, we illustrated how manual guided edits can be made to correct bias towards demographic subpopulations in fine-tuned foundation models. Having also shown matching performance in addition to expert specialism in both large vision and language models, we believe $\\mu\\mathrm{MoE}$ layers constitute an important step towards facilitating increasingly performant models that do not trade off fairness/interpretability for accuracy. ", "page_idx": 9}, {"type": "text", "text": "Limitations Firstly, it is important to state again that our quantitative evaluation only captures expert behavior on the test set, not out-of-distribution data [70, 84]. Furthermore, expert specialism in large models is only demonstrated qualitatively (through the expert coefficients) due to the absence of fine-grained labels. Developing ways of quantifying fine-grained expert specialism is an important direction for future research. Finally, our experimental results demonstrated comparable accuracies of $\\mu\\mathrm{MoE}$ networks only for models with parameter counts on the order of 100 million. Where resources permit, future work should explore the scalability of expert specialization and performance of $\\mu\\mathrm{MoEs}$ in even larger-scale LLMs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991. [2] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024.   \n[3] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In Int. Conf. Learn. Represent. (ICLR), 2021.   \n[4] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.   \n[5] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5, 2023.   \n[6] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Adv. Neural Inform. Process. Syst. (NeurIPS), 34:8583\u20138595, 2021.   \n[7] Basil Mustafa, Carlos Riquelme Ruiz, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with LIMoe: the language-image mixture of experts. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Adv. Neural Inform. Process. Syst. (NeurIPS), 2022.   \n[8] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In Int. Conf. Learn. Represent. (ICLR), 2017.   \n[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Adv. Neural Inform. Process. Syst. (NeurIPS), 30, 2017.   \n[10] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 44(11):7436\u20137456, 2021.   \n[11] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: Attention over convolution kernels. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 11030\u201311039, 2020.   \n[12] Robert A Jacobs, Michael I Jordan, and Andrew G Barto. Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. Cognitive science, 15(2):219\u2013250, 1991.   \n[13] Zachary C. Lipton. The mythos of model interpretability. Communications of the ACM, 61(10): 36\u201343, September 2018. ISSN 1557-7317.   \n[14] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144, 2016.   \n[15] Yuzhen Mao, Zhun Deng, Huaxiu Yao, Ting Ye, Kenji Kawaguchi, and James Zou. Last-layer fairness fine-tuning is simple and effective for neural networks. In Proceedings of the 2nd Workshop on Spurious Correlations, Invariance and Stability at the International Conference on Machine Learning (ICML 2023), 2023.   \n[16] Valeriia Cherepanova, Vedant Nanda, Micah Goldblum, John P Dickerson, and Tom Goldstein. Technical challenges for training fair neural networks. arXiv preprint arXiv:2102.06764, 2021.   \n[17] Noam Shazeer, \\*Azalia Mirhoseini, \\*Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In Int. Conf. Learn. Represent. (ICLR), 2017.   \n[18] Muqeeth Mohammed, Haokun Liu, and Colin Raffel. Models with conditional computation learn suboptimal solutions. In I Can\u2019t Believe It\u2019s Not Better Workshop: Understanding Deep Learning Through Empirical Falsification, 2022.   \n[19] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. In Int. Conf. Learn. Represent. (ICLR), 2024.   \n[20] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160\u2013175, 2021.   \n[21] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. In Int. Conf. Mach. Learn. Worksh. (ICMLW), 2015.   \n[22] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. In Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 2, pages 1339\u20131344 vol.2, 1993. doi: 10.1109/IJCNN.1993.716791.   \n[23] David Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. In Int. Conf. Mach. Learn. Worksh. (ICMLW), volume abs/1312.4314, 2013.   \n[24] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. Adv. Neural Inform. Process. Syst. (NeurIPS), 32, 2019.   \n[25] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In Int. Conf. Mach. Learn. (ICML), pages 5547\u20135569. PMLR, 2022.   \n[26] Shashank Gupta, Subhabrata Mukherjee, Krishan Subudhi, Eduardo Gonzalez, Damien Jose, Ahmed H Awadallah, and Jianfeng Gao. Sparsely activated mixture-of-experts are robust multi-task learners. arXiv preprint arXiv:2204.07689, 2022.   \n[27] Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah Smith, and Luke Zettlemoyer. Demix layers: Disentangling domains for modular language modeling. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2022. doi: 10. 18653/v1/2022.naacl-main.407.   \n[28] Aya Abdelsalam Ismail, Sercan O Arik, Jinsung Yoon, Ankur Taly, Soheil Feizi, and Tomas Pfister. Interpretable mixture of experts. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.   \n[29] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In Int. Conf. Mach. Learn. (ICML), 2021.   \n[30] Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, and Lu Yuan. Residual mixture of experts, 2022.   \n[31] Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead of deeper. In Conf. on Artif.i Intel. (AAAI), volume 36, pages 8779\u20138787, 2022.   \n[32] Andrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.   \n[33] Yunsheng Li, Yinpeng Chen, Xiyang Dai, mengchen liu, Dongdong Chen, Ye Yu, Lu Yuan, Zicheng Liu, Mei Chen, and Nuno Vasconcelos. Revisiting dynamic convolution via matrix decomposition. In Int. Conf. Learn. Represent. (ICLR), 2021.   \n[34] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. Parameter-efficient mixture-of-experts architecture for pre-trained language models. In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.   \n[35] I. Oseledets. Tensor-train decomposition. SIAM J. Sci. Comput., 33:2295\u20132317, 2011.   \n[36] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[37] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. Adv. Neural Inform. Process. Syst. (NeurIPS), 28, 2015.   \n[38] Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate tensorization: compressing convolutional and fc layers alike. arXiv preprint arXiv:1611.03214, 2016.   \n[39] Alexander Novikov, Mikhail Trofimov, and Ivan Oseledets. Exponential machines. In Int. Conf. Learn. Represent. Worksh., 2017.   \n[40] Markos Georgopoulos, James Oldfield, Mihalis A Nicolaou, Yannis Panagakis, and Maja Pantic. Mitigating demographic bias in facial datasets with style-based multi-attribute transfer. Int. J. Comput. Vis. (IJCV), 129(7):2288\u20132307, 2021.   \n[41] Francesca Babiloni, Ioannis Marras, Gregory Slabaugh, and Stefanos Zafeiriou. Tesa: Tensor element self-attention via matricization. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 13945\u201313954, 2020.   \n[42] Markos Georgopoulos, Grigorios Chrysos, Maja Pantic, and Yannis Panagakis. Multilinear latent conditioning for generating unseen attribute combinations. In Int. Conf. Mach. Learn. (ICML), 2020.   \n[43] Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, and Volkan Cevher. Multilinear operator networks, 2024.   \n[44] Jean Kossaif,i Antoine Toisoul, Adrian Bulat, Yannis Panagakis, Timothy M. Hospedales, and Maja Pantic. Factorized higher-order cnns with an application to spatio-temporal emotion estimation. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR). IEEE, June 2020.   \n[45] Adrian Bulat, Jean Kossaif,i Georgios Tzimiropoulos, and Maja Pantic. Incremental multidomain learning with network latent tensor factorization. In Conf. on Artif.i Intel. (AAAI), volume 34, pages 10470\u201310477, 2020.   \n[46] Yongxin Yang and Timothy M. Hospedales. Deep multi-task representation learning: A tensor factorisation approach. In Int. Conf. Learn. Represent. (ICLR), 2017.   \n[47] Grigorios G Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, and Stefanos Zafeiriou. P-nets: Deep polynomial neural networks. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 7325\u20137335, 2020.   \n[48] Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang Deng, Yannis Panagakis, and Stefanos P Zafeiriou. Deep polynomial neural networks. IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), page 1\u20131, 2021. ISSN 1939-3539.   \n[49] Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng, Grigorios Chrysos, and Stefanos Zafeiriou. Poly-nl: Linear complexity non-local layers with 3rd order polynomials. In Int. Conf. Comput. Vis. (ICCV), pages 10518\u201310528, 2021.   \n[50] Jean Kossaif,i Aran Khanna, Zachary Lipton, Tommaso Furlanello, and Anima Anandkumar. Tensor contraction layers for parsimonious deep nets. In IEEE Conf. Comput. Vis. Pattern Recog. Worksh. (CVPRW), pages 26\u201332, 2017.   \n[51] Francesca Babiloni, Thomas Tanay, Jiankang Deng, Matteo Maggioni, and Stefanos Zafeiriou. Factorized dynamic fully-connected layers for neural networks. In Int. Conf. Comput. Vis. Worksh. (ICCVW), pages 1374\u20131383, October 2023.   \n[52] Frank Lauren Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of Mathematics and Physics, 6:164\u2013189, 1927.   \n[53] Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455\u2013500, 2009. doi: 10.1137/07070111X.   \n[54] Ben Peters, Vlad Niculae, and Andr\u00e9 F. T. Martins. Sparse sequence-to-sequence models. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504\u20131519, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1146.   \n[55] Gon\u00e7alo M. Correia, Vlad Niculae, and Andr\u00e9 F. T. Martins. Adaptively sparse transformers. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174\u20132184, Hong ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1223. ", "page_idx": 13}, {"type": "text", "text": "[56] J. Douglas Carroll and Jih Jie Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of \u201ceckart-young\u201d decomposition. Psychometrika, 35: 283\u2013319, 1970. ", "page_idx": 13}, {"type": "text", "text": "[57] fvcore: Flop counter for pytorch models. https://github.com/facebookresearch/ fvcore. Accessed: 2024-05-16. ", "page_idx": 13}, {"type": "text", "text": "[58] Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. Tensor ring decomposition. ArXiv, abs/1606.05535, 2016. ", "page_idx": 13}, {"type": "text", "text": "[59] Lee Sharkey. A technical note on bilinear layers for interpretability. arXiv preprint arXiv:2305.03452, 2023. ", "page_idx": 13}, {"type": "text", "text": "[60] Michael T. Pearce, Thomas Dooms, and Alice Rigg. Weight-based decomposition: A case for bilinear MLPs, 2024. ", "page_idx": 13}, {"type": "text", "text": "[61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn. (ICML), 2021. ", "page_idx": 13}, {"type": "text", "text": "[62] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Int. Conf. Comput. Vis. (ICCV), 2021. ", "page_idx": 13}, {"type": "text", "text": "[63] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:29262\u201329277, 2022. ", "page_idx": 13}, {"type": "text", "text": "[64] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In Int. Conf. Learn. Represent. (ICLR), 2023. ", "page_idx": 13}, {"type": "text", "text": "[65] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022. ", "page_idx": 13}, {"type": "text", "text": "[66] Tilman R\u00e4uker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pages 464\u2013483. IEEE, 2023. ", "page_idx": 13}, {"type": "text", "text": "[67] Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. In Arianna Bisazza and Omri Abend, editors, Proceedings of the 25th Conference on Computational Natural Language Learning, pages 194\u2013209, Online, November 2021. Association for Computational Linguistics. ", "page_idx": 13}, {"type": "text", "text": "[68] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:17359\u201317372, 2022. ", "page_idx": 13}, {"type": "text", "text": "[69] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence, 1(5):206\u2013215, 2019. ", "page_idx": 13}, {"type": "text", "text": "[70] Stephen Casper. Broad critiques of interpretability research. 2023. URL https://www. alignmentforum.org/s/a6ne2ve5uturEEQK7/p/gwG9uqw255gafjYN4. ", "page_idx": 13}, {"type": "text", "text": "[71] Shlomi Hod, Daniel Filan, Stephen Casper, Andrew Critch, and Stuart Russell. Quantifying local specialization in deep neural networks. arXiv preprint arXiv:2110.08058, 2021. ", "page_idx": 13}, {"type": "text", "text": "[72] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pages 77\u201391. PMLR, 2018. ", "page_idx": 13}, {"type": "text", "text": "[73] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021. ", "page_idx": 13}, {"type": "text", "text": "[74] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Int. Conf. Comput. Vis. (ICCV), December 2015. ", "page_idx": 13}, {"type": "text", "text": "[75] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. In Int. Conf. Learn. Represent. (ICLR), 2023.   \n[76] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2016.   \n[77] Mei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware reinforcement learning. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 9322\u20139331, 2020.   \n[78] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. Adv. Neural Inform. Process. Syst. (NeurIPS), 33:728\u2013740, 2020.   \n[79] Mohsan Alvi, Andrew Zisserman, and Christoffer Nell\u00e5ker. Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, 2018.   \n[80] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. MLPmixer: An all-MLP architecture for vision. Adv. Neural Inform. Process. Syst. (NeurIPS), 34: 24261\u201324272, 2021.   \n[81] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019. URL https://cdn.openai.com/better-language-models/language_models_ are_unsupervised_multitask_learners.pdf.   \n[82] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   \n[83] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 248\u2013255, 2009.   \n[84] Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Vi\u00e9gas, and Martin Wattenberg. An interpretability illusion for bert. arXiv preprint arXiv:2104.07143, 2021.   \n[85] Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with einstein-like notation. In Int. Conf. Learn. Represent. (ICLR), 2022.   \n[86] Ledyard R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31:279\u2013311, 1966.   \n[87] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211\u2013218, 1936.   \n[88] Pratyusha Sharma, Jordan T. Ash, and Dipendra Misra. The truth is in there: Improving reasoning in language models with layer-selective rank reduction. In Int. Conf. Learn. Represent. (ICLR), 2024.   \n[89] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 7959\u20137971, 2022.   \n[90] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 8919\u20138928, 2020. ", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Broader impact 16 ", "page_idx": 15}, {"type": "text", "text": "B Fast $\\mu\\mathrm{MoE}$ implementations 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 $\\mathrm{CP}\\mu\\mathrm{MoE}$ einsum implementation 17   \nB.2 $\\mathrm{TR}\\mu\\mathrm{MoE}$ einsum implementation 17 ", "page_idx": 15}, {"type": "text", "text": "C $\\mu\\mathrm{MoE}$ forward pass visualization 18 ", "page_idx": 15}, {"type": "text", "text": "D Decomposition choice, matrix rank, and computational cost 18 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Tensor ranks to matrix rank 18   \nD.2 Why is low-rankness a reasonable assumption? 20   \nD.3 $\\mathrm{MoE}/\\mu\\mathrm{MoE}$ parameter count comparisons 20   \nE Hierarchical $\\mu\\mathrm{MoE}$ model derivations 21   \nE.1 Hierarchical $\\mathrm{CP}\\mu\\mathrm{MoE}$ . . 22   \nE.2 Hierarchical TR\u00b5MoE 22   \nF Experimental details 23   \nF.1 Network configurations and hyperparamters 23   \nF.2 Weight initialization 23   \nG Expert specialism: additional results 23   \nG.1 Large scale models 23   \nG.2 LLM steering 28   \nG.3 CLIP ViT-B-32 28   \nH Ablation studies 28   \nH.1 Entmax vs softmax 28   \nH.2 Fast forward pass computation speedups 34   \nH.3 Batch normalization 34   \nH.4 Expert load 35 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Additional performance results 35 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "I.1 CLIP ViT-B-32 ImageNET1k ablations 35   \nI.2 Hierarchical $\\mu\\mathrm{MoEs}$ . . 36   \nI.3 Comparisons to dense/sparse MoEs 36 ", "page_idx": 15}, {"type": "text", "text": "J Fairness baselines & metric details 37 ", "page_idx": 15}, {"type": "text", "text": "K Fairness: additional results 38   \nK.1 Model re-writing . 38 ", "page_idx": 15}, {"type": "text", "text": "L NeurIPS Paper Checklist 40 ", "page_idx": 15}, {"type": "text", "text": "A Broader impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of interpretable machine learning. Our goal is not to improve model capabilities but rather an orthogonal one of designing architectures more interpretable and controllable. As with many work with an interpretability focus, however, the $\\mu\\mathrm{MoE}$ layer could nonetheless facilitate the further development of SOTA models through its more expressive computation. We thus encourage the development of further guardrails against potentially harmful dual-uses of such technology. We release our code upon acceptance to facilitate further research along such lines. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B Fast $\\pmb{\\mu}\\mathbf{M0E}$ implementations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We here detail how to implement the fast forward passes of the $\\mu\\mathrm{MoE}$ models in a batch-wise manner, where each mini-batch element is a 2D matrix of shape $\\mathbf{Z}\\,\\in\\,\\mathbb{R}^{T\\times C}$ (with \u2018token\u2019 and \u2018channel\u2019 dimensions) with PyTorch and einops\u2019 [85] einsum: ", "page_idx": 16}, {"type": "text", "text": "B.1 $\\mathbf{CP}\\mu\\mathbf{MoE}$ einsum implementation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The $\\mathrm{CP}\\mu\\mathrm{MoE}$ forward pass can be implemented with: ", "page_idx": 16}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/0e66a70f3f39207a7e7257794e3d4330e3abf529d6bf6e07354d48fb11bd58a9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 $\\mathbf{TR}\\mu\\mathbf{M0}\\mathbf{E}$ einsum implementation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "$\\mathrm{TR}\\mu\\mathrm{MoEs}$ can be implemented with: ", "page_idx": 16}, {"type": "text", "text": "# TRmuMoE ( $x*=T R$ ranks , $\\scriptstyle b\\,=$ batch_dim , t=tokens ,   \n# $\\dot{\\bar{x}}=$ input_dim , $o=$ output_dim , a[e] $=$ expert_coefs , n\\*= expert_dims)   \n# batched mode -2 tensor -vector products   \nf1 $=$ einsum(a[0], G1 , \u2019b t n1 , r1 n1 r2 -> b t r1 r2\u2019)   \nf2 $=$ einsum(z, G2 , \u2019b t i, r2 i r3 -> b t r2 r3\u2019)   \n# batch -multiply f1@f2   \nfout $=$ einsum(f1 , f2 , \u2019b t r1 r2 , b t r2 r3 -> b t r1 r3\u2019)   \n# contract with final TR core   \ny $=$ einsum(G3 , fout , \u2019r3 o r1 , b t r1 r3 -> b t o\u2019) ", "page_idx": 16}, {"type": "text", "text": "And a two-level hierarchical version with an additional TR-core as: ", "page_idx": 16}, {"type": "text", "text": "# TRmuMoE ( $x*=T R$ ranks , $\\scriptstyle b\\,=$ batch_dim , t=tokens ,   \n# $\\dot{\\bar{x}}=$ input_dim , $o=$ output_dim , a[e] $=$ expert_coefs , n\\*= expert_dims)   \n# ################   \n# A 2-level hierarchical TRmuMoE , assuming additional TR cores Gi   \n$\\begin{array}{r l}{\\pmb{\\mathrm{f}}\\:1}&{{}=}\\end{array}$ einsum(a[0], G1 , \u2019b t n1 , r1 n1 r2 -> b t r1 r2\u2019)   \n$\\begin{array}{r l}{\\pmb{\\mathrm{~f~}}2}&{{}=}\\end{array}$ einsum(a[1], G2 , \u2019b t n2 , r2 n2 r3 -> b t r2 r3\u2019)   \nf3 $=$ einsum(z, G3 , \u2019b t i, r3 i r4 -> b t r3 r4\u2019)   \n# batch -multiply f1@f2@f3   \nfout $=$ einsum(f1 , f2 , \u2019b t r1 r2 , b t r2 r3 -> b t r1 r3\u2019)   \nfout $=$ einsum(fout , f3 , \u2019b t r1 r3 , b t r3 r4 -> b t r1 r4\u2019)   \n# contract with final TR core   \ny $=$ einsum(G4 , fout , \u2019r4 o r1 , b t r1 r4 -> b t o\u2019) ", "page_idx": 16}, {"type": "text", "text": "C $\\pmb{\\mu}\\mathbf{M0E}$ forward pass visualization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For intuition, we provide a visualization in Figure 6 of the step-by-step series of tensor contractions $\\mathcal{W}\\times_{1}\\mathbf{a}\\times_{2}\\mathbf{z}\\in\\mathbb{R}^{O}$ that the $\\mu\\mathrm{MoE}$ computes (in non-factorized form). ", "page_idx": 17}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/388d50b7dd8e91f4bde1528e8e93ca981f40910836a9754864d540ff9fba2d79.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: An intuitive visualization of the $\\mu\\mathrm{MoE}$ (unfactorized) forward pass, as visualized (as a series of tensor contractions) in 5 steps. Each step contributes to producing the output vector $\\mathbf{y}\\in\\mathbb{R}^{O}$ either by contracting with the expert coefficients $\\mathbf{a}\\in\\mathbb{R}^{N}$ , or with the input vector $\\mathbf{z}\\in\\mathbb{R}^{I}$ , along the appropriate mode of the collective weight tensor W \u2208RN\u00d7I\u00d7O. ", "page_idx": 17}, {"type": "text", "text": "D Decomposition choice, matrix rank, and computational cost ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we present a further detailed discussion of decomposition choice, validating our choices and comparing alternative options. The computational costs of each fast $\\mu\\mathrm{MoE}$ forward pass and tensor-matrix rank relationships implications derived in this section are summarized in Table 5. ", "page_idx": 17}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/7a00acb21c1f0d42f63356de3dcbf32c72d6abea72de0fca316267fa263934a1.jpg", "table_caption": ["Table 5: A computational comparison of decomposition choice for $\\mu\\mathrm{MoE}$ layers and existing MoEs. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.1 Tensor ranks to matrix rank ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "One important consideration is how the chosen tensor ranks bound the resulting experts\u2019 matrix rank in $\\mu\\mathrm{MoE}$ layers. Here, we derive the matrix ranks as a function of tensor ranks for each model in turn. ", "page_idx": 17}, {"type": "text", "text": "D.1.1 $\\mathbf{CP}\\mu\\mathbf{MoEs}$ : rank analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "$\\mathrm{CP}\\mu\\mathrm{MoEs}$ are parameterized by factor matrices $\\mathbf{U}^{(1)}\\in\\mathbb{R}^{R\\times N},\\mathbf{U}^{(2)}\\in\\mathbb{R}^{R\\times I},\\mathbf{U}^{(3)}\\in\\mathbb{R}^{R\\times O}$ for chosen CP-rank $R$ . Following Section $3$ of Kolda and Bader [53] which provides the matricization/unfolding of CP tensors, we can write expert $n$ \u2019s weight matrix as ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf{W}}_{n}={\\bf{U}}^{(2)}^{\\top}\\left({\\bf{U}}_{:n}^{(1)^{\\top}}\\odot{\\bf{U}}^{(3)^{\\top}}\\right)^{\\top}\\in\\mathbb{R}^{I\\times O},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\odot$ is the Khatri-Rao product [53], and $\\mathbf{U}_{:n}^{(1)}\\,\\in\\,\\mathbb{R}^{R\\times1}$ is the column of the factor matrix associated with expert $n$ (including a singleton dimension for the Khatri-Rao product to be welldefined). Through the linear algebra rank inequality for matrix products, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{rank}\\!\\left(\\mathbf{W}_{n}\\right)=\\operatorname{rank}\\left(\\mathbf{U}^{(2)^{\\top}}\\left(\\mathbf{U}_{:n}^{(1)^{\\top}}\\odot\\mathbf{U}^{(3)^{\\top}}\\right)^{\\top}\\right)\\le\\operatorname*{min}\\left\\{\\operatorname{rank}\\!\\left(\\underbrace{\\mathbf{U}^{(2)}}_{R\\times I}\\!\\right),\\operatorname{rank}\\!\\left(\\underbrace{\\mathbf{U}_{:n}^{(1)^{\\top}}\\odot\\mathbf{U}^{(3)^{\\top}}}_{O\\times R}\\right)\\!\\right\\}\\!.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore a single $\\mathrm{CP}\\mu\\mathrm{MoE}$ \u2019s $n$ th expert\u2019s matrix rank is bounded by $\\operatorname*{min}\\{I,O,R\\}$ . ", "page_idx": 18}, {"type": "text", "text": "D.1.2 $\\Gamma\\mathbf{R}\\mu\\mathbf{M_{0}E s}$ : rank analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now turn our attention to $\\mathrm{TR}\\mu\\mathrm{MoEs}$ , where we will see that the TR ranks $R_{1},R_{2},R_{3}$ translate very favorably into matrix rank at smaller computational cost than with $\\mathrm{CP}\\mu\\mathrm{MoEs}$ . First recall that $\\mathrm{TR}\\mu\\mathrm{MoEs}$ are parameterized instead by core tensors $\\mathcal{U}^{(1)}\\in\\mathbb{R}^{R_{1}\\times N\\times R_{2}}$ , $\\mathcal{U}^{(2)}\\in\\mathbb{R}^{R_{2}\\times I\\times R_{3}}$ , U(3) \u2208RR3\u00d7O\u00d7R1, with chosen ranks R1, R2, R3. We can derive an expression to materialize expert $n$ \u2019s matrix through the sum of matrix products of the TR cores as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{W}_{n}=\\sum_{r_{3}=1}^{R_{3}}\\left(\\underbrace{\\mathbf{U}_{r_{3};\\cdot}^{(3)}}_{O\\times R_{1}}\\underbrace{\\mathbf{U}_{:n:}^{(1)}}_{R_{1}\\times R_{2}}\\underbrace{\\mathbf{U}_{:r_{3}}^{(2)}}_{R_{2}\\times I}\\right)^{\\top}\\in\\mathbb{R}^{I\\times O}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The matrix product rank inequality applies to each $I\\times O$ matrix summand, whilst the matrix sum rank inequality applies to the outer matrix sum: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{rank}(\\mathbf{W}_{n})=\\mathrm{rank}\\Bigg(\\displaystyle\\sum_{r_{3}=1}^{R_{3}}\\big(\\mathbf{U}_{r_{3};\\Sigma}^{(3)}\\mathbf{U}_{:\\pi;\\,\\mathbf{U}}^{(1)}\\mathbf{U}_{:\\pi;\\,\\mathbf{\\lambda}}^{(2)}\\big)^{\\top}\\Bigg)}\\\\ &{\\phantom{\\mathrm{rank}}\\leq\\displaystyle\\sum_{r_{3}=1}^{R_{3}}\\mathrm{rank}\\big(\\big(\\mathbf{U}_{r_{3};\\Sigma}^{(3)}\\mathbf{U}_{:\\pi;\\,\\mathbf{U}}^{(1)}\\mathbf{U}_{:\\pi;\\,\\mathbf{\\lambda}}^{(2)}\\big)^{\\top}\\big)}\\\\ &{\\phantom{\\mathrm{rank}}\\leq\\displaystyle\\sum_{r_{3}=1}^{R_{3}}\\operatorname*{min}\\Bigg\\{\\mathrm{rank}\\big(\\mathbf{U}_{r_{3};\\Sigma}^{(3)}\\big),\\mathrm{rank}\\big(\\mathbf{U}_{:\\pi;\\,\\mathbf{\\lambda}}^{(1)}\\big),\\mathrm{rank}\\big(\\mathbf{U}_{:\\pi;\\,\\mathbf{\\lambda}}^{(2)}\\big),\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consequently, expert $n$ \u2019s materialized weight matrix in $\\mathrm{TR}\\mu\\mathrm{MoEs}$ has a more generous upper bound of min $\\{R_{3}\\ {\\stackrel{\\cdot}{\\cdot}}\\ \\operatorname*{min}\\{R_{1},R_{2}\\},I,O\\}^{8}$ . ", "page_idx": 18}, {"type": "text", "text": "Through this analysis, we observe that one can choose large values of $R_{3}$ yet small $R_{1},R_{2}$ to yield a high expert matrix rank with few parameters, justifying the choice of $R_{1}=R_{2}=4$ in the main paper. ", "page_idx": 18}, {"type": "text", "text": "D.1.3 Tucker\u00b5MoEs: rank analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "One popular alternative decomposition is the Tucker decomposition [86]. Here we derive the resulting matrix rank of this alternative $\\mu\\mathrm{MoE}$ variant and detail why it\u2019s not as desirable as the proposed $\\mu\\mathrm{MoE}$ variants. ", "page_idx": 18}, {"type": "text", "text": "A Tucker\u00b5MoE composes an $\\mu\\mathrm{MoE}$ weight tensor through the series of mode- $^{\\cdot n}$ products [53]: ${\\boldsymbol{\\mathcal{W}}}={\\boldsymbol{\\mathcal{Z}}}\\times_{1}\\mathbf{U}^{(1)}\\times_{2}\\mathbf{\\bar{U}}^{(2)}\\times_{3}\\mathbf{\\bar{U}}^{(3)}$ , where $\\mathcal{Z}\\,\\in\\,\\mathbb{R}^{R_{N}\\times R_{I}\\times R_{O}}$ is the so-called \u2018core tensor\u2019 and $\\mathbf{U}_{1}\\in\\mathbb{R}^{N\\times R_{N}}$ , $\\mathbf{U}_{2}\\in\\mathbb{R}^{I\\times R_{I}}$ , $\\mathbf{U}_{3}\\in\\mathbb{R}^{O\\times R_{O}}$ are the \u2018factor matrices\u2019 for the tensor\u2019s three modes. ", "page_idx": 18}, {"type": "text", "text": "Again following Kolda and Bader [53] a single expert $n$ \u2019s weight matrix can be rewritten through the matricization involving the Kronecker product $\\otimes$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{W}_{n}=\\mathbf{U}^{(2)}\\mathbf{Z}_{(2)}\\left(\\mathbf{U}_{n}^{(1)}\\otimes\\mathbf{U}^{(3)}\\right)^{\\top}\\in\\mathbb{R}^{I\\times O},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{Z}_{(2)}\\,\\in\\,\\mathbb{R}^{R_{I}\\times(R_{O}\\cdot R_{N})}$ is the so-called mode-2 (matrix) unfolding of the core tensor [53]. Consequently, the same rank inequality applies: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{rank}(\\mathbf{W}_{n})=\\mathrm{rank}\\left(\\mathbf{U}^{(2)}\\mathbf{Z}_{(2)}\\left(\\mathbf{U}_{n}^{(1)}\\otimes\\mathbf{U}^{(3)}\\right)^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{min}\\bigg\\{\\mathrm{rank}\\big(\\underbrace{\\mathbf{U}^{(2)}}_{I\\times R_{I}}\\big),\\mathrm{rank}\\big(\\underbrace{\\mathbf{Z}_{(2)}}_{R_{I}\\times R_{N}}\\big),\\mathrm{rank}\\big(\\underbrace{\\mathbf{U}_{n}^{(1)}\\otimes\\mathbf{U}^{(3)}}_{O\\times(R_{O}\\cdot R_{N})}\\big)\\bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where we see the much more restrictive matrix rank upper bound applies: min $\\iota\\left\\{\\operatorname*{min}(I,R_{I}),\\operatorname*{min}(R_{I},R_{O}\\cdot R_{N}),\\operatorname*{min}(O,R_{O})\\right\\}$ . Thus in practice, both $R_{I},R_{O}$ need to be large to yield a large matrix rank, which is in conflict with the goal of maintaining a moderate number of parameters. ", "page_idx": 19}, {"type": "text", "text": "D.2 Why is low-rankness a reasonable assumption? ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Given we\u2019ve seen that parameter-efficient $\\mu\\mathrm{MoE}$ layers lead to low-rank expert weight matrices, a natural question is whether or not low-rankness in MLP linear layers weight matrices is a reasonable assumption or constraint. ", "page_idx": 19}, {"type": "text", "text": "Our strongest piece of evidence supporting the claim is experimental in nature: we\u2019ve seen from the results in Section 4.3 that using all parameter-matched $\\mu\\mathrm{MoE}$ layers for both MLP mixers and GPT-2 models leads to no significant drop in accuracy from their linear layer counterparts (see also Appendix I for many more results). ", "page_idx": 19}, {"type": "text", "text": "To investigate this further we perform a rank ablation on our trained MLP-Mixer model with the original linear layers\u2019 weights. Concretely, we compute the truncated SVD of each MLP block\u2019s 2 linear layer weight matrices. We explore the impact on the model\u2019s ImageNET1k validation set accuracy when using only the top- $k$ singular vectors/values (the best rank- $k$ approximation [87]). The validation set accuracy using truncated SVD weights in every mixer block is plotted in Figure 7\u2013we see here that discarding as many as half the total number of (bottom) singular vectors/values to approximate the original weights leads to negligible difference to the validation set accuracy. In other words, low-rank approximations of MLP Mixers\u2019 weights retain their representational power sufficiently well to produce nearly the same validation set accuracy as the original model. Such findings are consistent with results in recent work in the language domain [88], where low-rank approximations of MLP layers can even sometimes boost original performance. The accuracy retained by MLP Mixers here even after such aggressive rank reduction constitutes further evidence that full-rank weights are not always necessary. ", "page_idx": 19}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/f59d255f3ee240f43c8dfbd5686dfba43549d5c2ebc4f156952dcb4c8fe32704.jpg", "img_caption": ["Figure 7: Val. accuracy for an S-16 MLP-mixer when performing truncated SVD on all MLP\u2019s linear layers\u2019 weight; model accuracy is closely retained even with half the singular vectors. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "D.3 $\\mathbf{MoE}/\\mu\\mathbf{MoE}$ parameter count comparisons ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We plot in Figure 8 the parameter counts for $\\mu\\mathrm{MoE}$ layers as a function of the expert counts (sweeping from $N=2$ experts through to $N=16,384)$ , relative to dense/sparse MoEs (with rank $R_{1}=R_{2}=4$ $\\mathrm{TR}\\mu\\mathrm{MoEs}$ ), for the first layer in a MLP-mixer channel-mixing block [80]. As can be seen, both $\\mu\\mathrm{MoE}$ variants are vastly more parameter-efficient than dense/sparse MoEs. ", "page_idx": 19}, {"type": "text", "text": "Given $\\mathrm{TR}\\mu\\mathrm{MoEs}$ offer even better parameter efficiency for larger numbers of experts, we suggest opting for $\\mathrm{CP}\\mu\\mathrm{MoEs}$ when using expert counts less than $\\sim128$ , and considering TR\u00b5MoEs for higher values. ", "page_idx": 19}, {"type": "text", "text": "Latency and memory usage comparisons between the $\\mu\\mathrm{MoE}$ , linear layers, and alternative MoEs are shown in Table 6, where the $\\mu\\mathrm{MoEs}$ perform favorably. ", "page_idx": 19}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/7d8b0700288666cf6fd8acbcc5643159571d6807b714cea50b11d2017350a124.jpg", "img_caption": ["Figure 8: $\\mu\\mathrm{MoE}$ layer parameter count as a function of expert count. "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/b46028a94b513b96488ac3e9e8f59f7fc5878ad3746fc71c36cc12602778e2c4.jpg", "table_caption": ["Table 6: Comparison of different layers\u2019 peak memory usage and latency (per single input). We use 128 experts in each MoE layer, and set the rank of the $\\mu\\mathrm{MoEs}$ to parameter-match that of the linear layer. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Hierarchical $\\mu\\mathrm{MoE}$ model derivations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the main paper, the fast forward passes are derived for a single level of expert hierarchy. One additional attractive property of $\\mu\\mathrm{MoEs}$ is their straightforward extension to multiple levels of expert hierarchy\u2013one simply increments the number of modes of the weight tensor and includes another tensor contraction with new expert coefficients. Hierarchical $\\mu\\mathrm{MoEs}$ intuitively implement \u201cand\u201d operators in expert selection at each level, and further provide a mechanism through which to increase the total expert count at a small parameter cost. Here, we derive the fast forward passes for $\\mu\\mathrm{MoE}$ layers in their most general form with $E$ levels of expert hierarchy. For intuition, we first further visualize $\\mu\\mathrm{MoE}$ layers with 2 levels of hierarchy in Figure 9\u2013note how we have an extra mode to the weight tensor, and an extra contraction over the new expert mode to combine its outputs. ", "page_idx": 20}, {"type": "text", "text": "Given that hierarchical $\\mu\\mathrm{MoEs}$ involve very high-order tensors, we adopt the popular mode- ${\\cdot n}$ product [53] to express the forward passes in as readable a way as possible. The mode- ${.n}$ (vector) product of a tensor $\\mathcal{X}\\in\\mathbb{R}^{I_{1}\\times I_{2}\\times\\dots\\bar{\\times}I_{N}}$ and vector $\\mathbf{u}\\in\\mathbb{R}^{I_{n}}$ is denoted by $\\mathcal{X}\\times\\ensuremath{n}$ u [53], with its elements given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\mathcal{X}\\times_{n}\\mathbf{u})_{i_{1}...i_{n-1}i_{n+1}...i_{N}}=\\sum_{i_{n}=1}^{I_{n}}x_{i_{1}i_{2}...i_{N}}u_{i_{n}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We first introduce the formulation of an $E$ -level hierarchical $\\mu\\mathrm{MoE}$ layer from Equation (1) in the main paper: given input $\\mathbf{z}\\in\\mathbb{R}^{I}$ , the most general form of $\\mu\\mathrm{MoE}$ layer is parameterized by weight tensor $\\bar{\\mathcal{W}}\\ \\in\\mathrm{~\\mathbb{R}^{N_{1}\\times\\ldots~}}\\times N_{E}\\!\\times\\!I\\!\\times\\!O$ and $E$ many expert gating parameters $\\{\\mathbf{\\dot{G}}_{e}\\,\\in\\,\\mathbb{R}^{I\\times N_{e}}\\}_{e=1}^{\\check{E}}$ }eE=1. The ", "page_idx": 20}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/5aebe07e84f36630ba0e93ce023b3e0970d43b7ea37f34ff93c921bb77e51b73.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: Illustration of a two-hierarchy $\\mu\\mathrm{MoE}$ layer\u2019s (unfactorized) forward pass as a series of tensor contractions. The $N_{1}\\cdot N_{2}$ many experts\u2019 weight matrices are visualized as 2D horizontal slices in yellow, which are (1) matrix-multiplied with the input vector, (2) summed over the first expert mode (weighted by the first expert coefficients ${\\bf a}_{1}$ in red), and (3) summed over the second expert mode (weighted by the second expert mode\u2019s coefficients ${\\bf a}_{2}$ in dark green). ", "page_idx": 21}, {"type": "text", "text": "explicit, unfactorized forward pass is given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{e}=\\phi(\\mathbf{G}_{e}^{\\top}\\mathbf{z})\\in\\mathbb{R}^{N_{e}},\\quad\\forall e\\in\\{1,\\ldots,E\\},}\\\\ &{\\mathbf{y}=\\mathcal{W}\\times_{1}\\mathbf{a}_{1}\\times_{2}\\ldots\\times_{E}\\mathbf{a}_{E}\\times_{E+1}\\mathbf{z}}\\\\ &{\\quad=\\displaystyle\\sum_{n_{1}=1}^{N_{1}}a_{1n_{1}}\\cdot\\cdot\\cdot\\sum_{n_{E}=1}^{N_{E}}a_{E N_{E}}\\big(\\underbrace{\\mathbf{W}_{n_{1}\\ldots n_{E};\\xi}^{\\top}}_{O\\times I}\\mathbf{z}\\big)\\in\\mathbb{R}^{O},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where Equation (15) is expressed as sums over the $E$ -many expert modes to make it clear that hierarchical $\\mu\\mathrm{MoEs}$ take convex combinations of $\\prod_{e=1}^{E}N_{e}$ many experts\u2019 outputs (given there are $N_{e}$ experts at each level of hierarchy). With expert coefficients $\\{\\mathbf{a}_{e}\\in\\mathbb{R}^{N_{e}}\\}_{e=1}^{E}$ , the factorized forward passes of the most general hierarchical $\\mu\\mathrm{MoE}$ layers are given for the two variants below. ", "page_idx": 21}, {"type": "text", "text": "E.1 Hierarchical $\\mathbf{CP}\\mu\\mathbf{M0E}$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The full $\\mathrm{CP}\\mu\\mathrm{MoE}$ model of rank $R$ has an implicit weight tensor $\\begin{array}{r}{\\mathcal{W}=\\sum_{r=1}^{R}\\mathbf{u}_{r}^{(1)}\\circ\\mathbf{u}_{r}^{(2)}\\circ\\mathbf{u}_{r}^{(3)}\\circ\\cdot\\cdot\\cdot\\circ}\\end{array}$ $\\mathbf{u}_{r}^{(E+2)}\\in\\mathbb{R}^{N_{1}\\times\\cdots\\times N_{E}\\times I\\times O}$ , with factor matrices $\\mathbf{U}^{(1)}\\in\\mathbb{R}^{R\\times N_{1}},\\ldots,\\mathbf{U}^{(E)}\\in\\mathbb{R}^{R\\times N_{E}},\\mathbf{U}^{(E+1)}\\mathrm{~}\\mathrm{(}$ $\\mathbb{R}^{R\\times I},\\mathbf{U}^{(E+2)}\\in\\mathbb{R}^{R\\times O}$ . The implicit, factorized forward pass is given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf y}=\\left(\\sum_{r=1}^{R}{\\bf u}_{r}^{(1)}\\circ{\\bf u}_{r}^{(2)}\\circ{\\bf u}_{r}^{(3)}\\circ\\cdots\\circ{\\bf u}_{r}^{(E+2)}\\right)\\times_{1}{\\bf a}_{1}\\times_{2}\\ldots\\times_{E}{\\bf a}_{E}\\times_{E+1}{\\bf z}}}\\\\ {~~}\\\\ {{\\displaystyle~=\\sum_{r=1}^{R}{\\bf u}_{r}^{(E+2)}\\big(\\sum_{n_{1},\\ldots,n_{E},i}u_{r n_{1}}^{(1)}a_{1_{n_{1}}}\\cdot\\cdot\\cdot u_{r n_{E}}^{(E)}a_{E_{n_{E}}}u_{r i}^{(E+1)}z_{i}\\big)}}\\\\ {~~}\\\\ {{\\displaystyle~=\\sum_{r=1}^{R}{\\bf u}_{r}^{(E+2)}\\big({\\bf U}^{(1)}{\\bf a}_{1}\\big)_{r}\\cdot\\cdot\\cdot\\big({\\bf U}^{(E)}{\\bf a}_{E}\\big)_{r}\\cdot\\big({\\bf U}^{(E+1)}{\\bf z}\\big)_{r}\\in\\mathbb{R}^{O}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E.2 Hierarchical $\\mathbf{TR}\\mu\\mathbf{M0}\\mathbf{E}$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In TR format, $\\mathcal{W}\\in\\mathbb{R}^{N_{1}\\times\\dots\\times N_{E}\\times I\\times O}$ has $E+2$ factor tensors: $\\mathcal{U}^{(1)}\\in\\mathbb{R}^{R_{1}\\times N_{1}\\times R_{2}},\\dots,\\mathcal{U}^{(E)}\\in$ ${}_{\\!\\mathrm{S}}R_{E}\\times N_{E}\\times R_{E+1}{}_{,}\\,\\mathcal{U}^{(E+1)}\\in\\mathbb{R}^{R_{E+1}\\times I\\times R_{E+2}}.$ , $\\mathcal{U}^{(E+2)}\\in\\mathbb{R}^{R_{E+2}\\times O\\times R_{1}}$ , where $R_{i}$ are the manually chosen ranks. The weight tensor\u2019s elements are given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{n_{1}\\ldots n_{E}i o}=\\mathrm{tr}\\big(\\mathbf{U}_{:n_{1}:}^{(1)}\\cdot\\cdot\\cdot\\mathbf{U}_{:n_{E}:}^{(E)}\\mathbf{U}_{:i:}^{(E+1)}\\mathbf{U}_{:o:}^{(E+2)}\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We derive the fast factorized forward pass in terms of a series of mode-2 products: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf y}=\\displaystyle\\sum_{i}\\sum_{n_{1},\\dots n_{E}}\\mathcal{W}(n_{1},\\cdots,n_{E},i,:){\\bf a}_{1}(n_{1})\\cdot\\cdot\\cdot{\\bf a}_{E}(n_{E}){\\bf z}(i)}\\\\ &{\\quad=\\displaystyle\\sum_{r_{1},r_{E+2}}{\\bf u}_{r_{E+2};r_{1}}^{(E+2)}\\big(\\underbrace{({\\mathcal{U}}^{(1)}\\times_{2}{\\bf a}_{1})\\cdot\\cdot\\cdot({\\mathcal{U}}^{(E)}\\times_{2}{\\bf a}_{E})({\\mathcal{U}}^{(E+1)}\\times_{2}{\\bf z})}_{R_{1}\\times R_{E+2}}\\big)_{r_{1}r_{E+2}}\\in{\\mathbb R}^{O}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "F Experimental details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Network configurations and hyperparamters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we provide the full experimental details and setups to reproduce the performance results in the paper for each of the networks. We further include the per-epoch accuracy plots for additional transparency into the training processes. ", "page_idx": 22}, {"type": "text", "text": "The experimental configurations used to reproduce the performance results in the main paper follow as closely as possible those specified in the main paper of MLP-mixer [80] and open-source code (https://github.com/lucidrains/mlp-mixer-pytorch), the open-source code for NanoGPT (https://github.com/karpathy/nanoGPT) for GPT2 [81], and the robust fine-tuning protocol of [89] for CLIP [61]. These values are summarized in Table 7. We plot the learning curves for the training of both models in Figures 10 and 11. ", "page_idx": 22}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/b8b59556f027232d969b7f988ba7230196501cfae8b45ceeaa4c002826cc0a03.jpg", "table_caption": ["Table 7: Experimental configuration and settings for the results reported in the main paper in Section 4.3. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Rank choices Throughout all experiments in the main paper, we fix the $\\mathrm{TR}\\mu\\mathrm{MoE}$ ranks for the first two modes to be $R_{1}=R_{2}=4$ . This way, we can maximize the effective expert matrix ranks at a low parameter cost, as shown in Appendix D.1.2. The final TR rank $R_{3}$ is varied to parameter-match the networks in question. For $\\mathrm{CP}\\mu\\mathrm{MoEs}$ , we set the single CP rank $R$ to parameter-match the baselines. ", "page_idx": 22}, {"type": "text", "text": "Training times Each MLP mixer model takes just under 3 days to train on 4xA100 80GB GPUs.   \nThe NanoGPT models take 2-3 days to train for $100k$ iterations, with the same resources. ", "page_idx": 22}, {"type": "text", "text": "F.2 Weight initialization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We init\u221aializ\u221ae each element of the factor matrices/tensors for the input and output modes from a $U[-\\sqrt{k},\\sqrt{k}]$ distribution (following PyTorch\u2019s linear layers\u2019 initialization strategy), for $k\\,=$ 1/in_features, where in_features is the dimension of the input to each factor matrix/tensor during the factorized forward passes. ", "page_idx": 22}, {"type": "text", "text": "Factor matrices for the expert modes are initialized to replicate the weight matrices along the expert mode (plus optional noise). For $\\mathrm{CP}\\mu\\mathrm{MoEs}$ , this corresponds to sampling the factor matrices\u2019 elements from a $\\bar{\\mathcal{N}}(1,\\bar{\\sigma})$ distribution. For $\\mathrm{TR}\\mu\\mathrm{MoEs}$ , the weight matrices can instead be replicated along the expert mode by initializing each slice (e.g. $\\mathcal{G}_{1}(:,i,:))$ as a diagonal matrix with its elements sampled from ${\\mathcal{N}}(1,\\sigma)$ . In all our experiments we set $\\sigma:=1$ to introduce noise along the first expert mode, and $\\sigma:=0$ for additional expert modes. ", "page_idx": 22}, {"type": "text", "text": "G Expert specialism: additional results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Large scale models ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We first show in Figure 12 the top-activating examples for MLP-mixers trained with both $\\mathrm{CP}\\mu\\mathrm{MoE}$ and $\\mathrm{TR}\\mu\\mathrm{MoE}$ blocks. Examples are shown for the first two experts as they appear numerically for each of the 8 layers, where we observe the same phenomenon of earlier blocks specializing to textures, and later blocks to higher-level abstract concepts/objects. ", "page_idx": 22}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/10765ac3943ed0d06584214d3ce712dc0eeb30321412c2bcfff825bc59e1cee1.jpg", "img_caption": ["Figure 10: Training loss and validation accuracy for the MLP-mixers models for 300 epochs. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/43a315720f737e2bfa17814622c9367c7c3d8d9090276652af39ad665e7f9f6b.jpg", "img_caption": ["Figure 11: Training and validation loss for the GPT-2 models for $100\\mathbf{k}$ iterations. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Secondly, in Figure 13 we show the top 32 activating tokens for the first 6 experts (as they appear numerically) for layer 5 in GPT2 models trained with $\\mathrm{CP}\\mu\\mathrm{MoEs}$ replacing every MLP block. Whilst there are clear coherent themes amongst the top-activating tokens, we do see some examples of multiple themes being processed with high coefficients by the same experts (e.g. example $\\#20$ in expert 2\u2019s top-activating examples appears unrelated to the context of the other top-activating tokens) indicating a certain degree of expert polysemanticity (as expected in the large open domain of web text). ", "page_idx": 23}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/f49b0f07b89938d4d746febef095d1dc5a14d05c0641471b6894af07cd36ff72.jpg", "img_caption": ["(a) $\\mathbf{CP}\\mu\\mathbf{MoE}$ block MLP-Mixers: top-activating tokens. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/68a583300bc687dff4552378ab10b14124cffeea7820465bd3885936f5982c04.jpg", "img_caption": ["(b) $\\mathbf{TR}\\mu\\mathbf{M0}\\mathbf{E}$ block MLP-mixers: top-activating tokens. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 12: Top-activating patches (and their surrounding image context) for the first experts at two blocks in MLP-mixer models. $\\mu\\mathrm{MoE}$ blocks (with $N=64$ ) exhibit coarse-grained specialism (e.g., texture) earlier and more fine-grained specialism (e.g., object category) deeper in the network. ", "page_idx": 24}, {"type": "text", "text": "Layer 5, Expert 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "#1:bestof ourknwledge,this isoneofthemost significant andinsigtfulbook.recommend ito   \n#2: the last 10 years, we at VICE are the most well-traveled   \n#3: national security of the United States remains one of the most hotly debated topics in Congress.ininSen #4: and maintenance of the country's largest and most complex military facilities.n\\nFederal officials #5: lISS) said on Tuesday.\\n\\nThe most recent estimates estimate that the global economy will grow   \n#6: nThe best-selling authors of the most popular fantasy books in the Midle Ages, the   \n#7: . 6.1. 13\\n\\nln the most recent installment, the Bears' receivers have become   \n#8: Today,Bionicle is one of the most highly regarded and accomplished companies in the world with #9: \\n\\nRelated Stories: \\n\\nThe most difficult thing to overcome is the fear of punishment   \n#10: Good\".in\\n- One of the most watched events of 2017.nin- All   \n#11: to Go The Next Levelin\\nOne of the most important skills you have to master is balance.   \n#12: an estimated 4,0o0 spectators, this is the most crowded and most exciting marathon in the world. #13: \\n\\nAnd today, one of the most interesting questions the company asked on Twitter was whether #14: her career, and in doing so she became the most famous woman in female-led media.in   \n#15: three methods to search for the new species,the most common method being fossil-bearing excavations. #16: Human Interface Guidelines describe as 'the best and most widely used rendering engine for the Web. #17: by The Smiths and it's the most important single of their career, \\*Bad   \n#18: are living through the hottest, driest,andmost dangerous part of the year,   \n#19: than a top-flight soldier. Even during the most crucial moments, Sarcastic was a show   \n#20: \\n\\n\\*The United States is one of the most politically and economically influential countries in the world. #21: .in\\nThe United States and Russia are the most powerful economic, political, and military power in #22: BCHL's Boston College was the most successful of the lower divisions in the league.   \n#23: \\nAt the end of 2016, one of the most prominent and talked about aspects of the U.   \n#24: just becauseIwas one of the luckiest andmost wonderful ones,but becauseI was one of   \n#25:\\n\\nln one of his most recent pieces,Illdo a look at   \n#26: .n\\nThe UK is now one of the most prosperous countries in the world. It is better   \n#27: which was unveiled for the launch of the year's most popular VR game, Psychonauts, has   \n#28: , 5-12 and 5-10 are the most common plays that both teams are able to control   \n#29: hiwei. At the time it was also the most advanced nuclear reactor in China. As of 2010   \n#30: into conflict. The Chinese navy is one of themost formidable in the world, and this year is   \n#31:nDr. Aaron Ben-Gurion,themost senior Israeli human rights lawyer, told the Wall   \n#32: case or his parents as it was one of the most significant topics of the day, with the school ", "page_idx": 25}, {"type": "text", "text": "Layer 5, Expert 2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "#1:Afghan soldiers and Afghan police, but with the fallof NATO in Afghanistan, the U   \n#2: warned that there would be no peace after the fallaf the Soviet Union, as the Soviet Union had #3: n10/22/16\\n\\nRiseof the Sub-Prime\\ninBryan Hitch   \n#4: The Empire Strikes Back' and \u2018Return of the Jedi'.n\\nThe game   \n#5: tripled its number of airstrikes in Iraq since the fallf Mosul.nnThe United States has two   \n#6: Nighter, The Singing Star, The Return f the Jedi, The Force Awakens, The Force   \n#7: to 52,400.in\\nSince the fall af the Soviet Union, the number of American workers   \n#8: Kremlin has seen a change in attitude following the fall of the military in Georgia. Vladlimir Putin is now #9:Machine $\"\\mathfrak{n}_{*}$ \"The Fallof jim Crow.\" In: \"   \n#10: very stable in terms of its military since the fallaf communism and even though it's a   \n#11: land has had a moment, or since the fllf the Berlin Wall It is a memory of   \n#12: 's activities are alleged to have continued after the fallof the Soviet Union, and have been monitored by #13: %\\n\\nStar Wars: Episode II - Return Of the Jedi: 0%   \n#14: Vnln the late 1980s, after the fallof the Soviet Union, the government began to issue   \n#15: be confronted with a conventional military threat since the fallof the Soviet Union in 1991,\" the #16: n\\nAll soldiers in the army after the fall af the Berlin wall were Germans. The Germans came   \n#17: the-scenes in Star Wars Episode Vl: Return of the Jedi.   \n#18: Oz (2013)n\\n\"The Return of Oz\"(1999)n\\n(2015   \n#19: \\nWith the fallof the Soviet Union, the United States   \n#20: nThe three-year-old game was upnext. with the Bulldogs emerging as winners on the   \n#21: Army, which was formed in 2013 after the fall af the Soviet Union. The Free Syrian Army is   \n#22: \\nin\"After the fall of the Taliban, it's very important   \n#23: The family had stored items there since before the fall af Aleppo.in\\nThe family issued a statement #24: to a huge crowd of protesters, chanting \u201cDown with the Palestinians\" as they watch. Amongst #25: through a string of severe economic crises since the fallof the Berlin Wall, and the World War II #26: in the country's biggest urban conflict since the fall af the Soviet Union.n\\nThey include the   \n#27: rebuilt during the Tamil Munra period following the fall of the BritishLA and after its destruction in the #28: InSince the fallof the Berlin Wall in 1989, Germany has seen   \n#29:SincethefltheBelinWallthBerlinelitg   \n#30: ird: It's the first time since the fallf 2002 that an\u201cold friend\" of mine   \n#31:\" from \u201cHitch House 2: Rise Of the Elders,\u201d starring Tom Hardy   \n#32: of the Syrian government, in 2012 after the fallf Assad\u2019s regime.nnHe said that ", "page_idx": 25}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/a842e04bf4bca6ba21f6360d1512c33d29e726e097babf25f1a0c79728c02ba6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Layer 5, Expert 3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "#1: on how to operate in the marketplace but at the same time I'm still waiting for them   \n#2: very depressed atirst.\"nnAtirstbut eventually,he was able to self   \n#3: \\n\\nOn its face. the campaign to close that loophole should be   \n#4: at face value. The only real difference, at first sight, is that the \\*collo   \n#5: t know what to do anymore.n\\nAt first we just wanted to buy stocks, but then   \n#6: for my friends (who, let'sface it, love PB).in\\nSo 1   \n#7: s just so exciting.\"n\\nAt first, the protests appear to be peaceful. But   \n#8:the event show the stars of the future,atfirstappearing asa dragonshape ornigh  \n#9:n1.Pick up the pencilnnAtirst you should strike the pencil with yourfingers   \n#10: getting out of the car are intense and unexpected at best even for a man who is normally a casual   \n#11: of the two studies, the study is misleading at best.\" the article said. \"The conclusion is   \n#12: Republican voters willvote for the Republican nominee. Atfirst this will not be the case.It   \n#13: n\\nOn the Surface, there are no significant differences between the two   \n#14:It's the kind of idea that atfirstmight seemalittle odd to a fanof   \n#15: is currently being built in the California desert. At first, the telescope's equipment was set up as   \n#16: \\nThe initial response from the community was mixed atfirst. The group of students that had been invited #17:n\\nAt fist glanceit seems that the first half of the   \n#18: archived and updated to reflect the changes.<|endoftext]>Atfirst, it looks like the American government is cracking #19: blog, it's a book. At first it seems like a really good book, but   \n#20: really aware of what this album is about. At first, it's an album about death   \n#21: reference to the method in question.ln\\nAt first glance, this looks reasonable until you realize that   \n#22: was for being with him.ininOn the surface, it seems like she wanted to be with   \n#23: on the problems will go down, but at the same time, it's going to look good.   \n#24:they are working for one company.But atfirst,theyare working forthe same company,   \n#25: loading it at the same time.n\\nAt first I only thought that my friend was dead.   \n#26: want to play a chess game.in\\nAt first I don't know what 'm losing.   \n#27: kind of hard to get a meeting with somebody at first just to get clarification, but it'\\*   \n#28: , tells BBC News: \\*I'd say at first I didn't know what was happening with the   \n#29: that.\"n\\nOn the Surface, there was nothing wrong with a new player   \n#30:will be given inthe post:\\n\\nAt first,our project is to deliver this experience to   \n#31:formen's clothing.ninAtfirsttheyasked me ifIthought theyhad   \n#32: No player was suspended more than he was. At first, he got his suspension for a month and ", "page_idx": 25}, {"type": "text", "text": "Layer 5, Expert 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "#1:lot about it,Reisersaid.tsveryimortantokw   \n#2:raKaram.He also accused Israeliforces ofperpetuating anapartheid system that harms #3: s\\*misogyny\" remark wasfactually false.\"\\n\\n   \n#4: for the United States,\" he said.It basically gives the State Department and the   \n#5: public debate about the right to privacy and what constitutesI\\*privacy. Since the 2000 #6: remain in the lineup, Gibson says.The players around me are trying to help   \n#7: \" Deason said of the new project.lt's not a gimmick that   \n#8: move to do it,\" he said.Hopefully, I will be able to play   \n#9: 're doing,\\* he said.I\\*They have the right to make allegations against   \n#10:\\n\\nAs if the United Statesis not beingdisappointedby the people,   \n#11: element to the game, he said.\\*lf you don't have the   \n#12: system like no other,\" he said.IBut this is the United States of America   \n#13: neaky\u201d story about a successful entrepreneur who\\*has no trouble geting promoted\" is #14: for the right reason,\" he said.\\*We're doing it for the   \n#15: the world,\" Klinsmann said.I\\*We know that. Our goal is to   \n#16:release for the book,he said.\"We have a publisher now who sees it   \n#17: lahoma, a member of the House Intelligence Committee.We have to get that hard.   \n#18: jury found that her work as a prison specialist wasexcessive and that she should not be #19: of a long week,\" he said.\\*The first team doesn't have   \n#20: good piece of work,\" he said.I don't really like the   \n#21: (\"a liar!\") and \\*sucking the Earth\".n   \n#22: Sept.8 testimony before the Senate Judiciary Committee.IWe are not going to be as cautious #23:evtek said the committes decision wasa great mistake,adding:   \n#24: said that men are more likely to call their friendsgirlfriend, but only 10 percent   \n#25:\"he said,adding that the North hasproved that China is a threat to   \n#26: in the United States,\" he said.I\\*I do like it as a matter of   \n#27: is described as aradical\" and\"curious\" writer, who has   \n#28:'s for sure, he said.But we need to know what happened before   \n#29:, who said that the Occupy Wall Street movement wasfundamentally about the power of the #30:\"of the initiative and allow the scheme toprovide a platform forexperimentation,   \n#31:to showthepotentialof thetechnology.Soonerr later,wewould see what thefuture would #32: the police and the justice system, which she calleda system of intimidation against the public ", "page_idx": 25}, {"type": "text", "text": "Layer 5, Expert 5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "#1:make its trademarks more exclusive,and thereforemore valuablebymaking themmore scarce and worthless.n #2:working alongside the European Commission to develop,and implement theregulations.ninThe Commission wil also #3: a general outline of, and also a reference on the subject material. The subject division is then #4: on the brink of a new, and very dangerous world It's about to change.n #5: be receiving from others is not, and is not true in all cases. The things you are #6: confers a protective, and sometimes even a fatal protective effect on the brain.ininThe #7: new law will give me, and other gun owners as I watch this debate unfold, and I #8: night a week with, and have a relationship with a man named Tony.\"he added #9: Department of State have provided, and are stildeveloping proposals for a response to the court's request #10: be noted that the first, but not the last is a statement of fact by the Irish#11: federal govermment to monitor the work on, and for the Arctic.ininOther states have developed #12: of the most important,if not the most important things that have happened in the past 100 years #13: person shooter mode similar to, but not identical to Elite. And it's not far behind Elite #14:bldthatisnalogoustoanspeifcllycorrelatedwithincreaedstreelatdcortislevelinyounghet #15: alment of your first,but hopefully not final interview for a future job in an engineering department #16: .n\\nMany of the smaller, but powerful battery-powered and mobile phone screens are built #17:havetoaccept anincreasingly authoritarian,andincreasinglyhostilgovernmentin placeinorderto surviven #18: Obama administration has then turned the first, and only, rule on Iran. The Obama administration has refused #19: have been able to deliver our first, and only launch of the software,a says Mass #20:alsoshowsthecentral govrnment hasundertaken,andtakenunprecedentedstepsto tackletheflowof enery. #21:causedbyacombinationofandthe symptomsofsychosistisassociatedwithmajrmentalijury #22:After this, thebest,if not the bestof the last 30years is exactly what we #23: as the central enforcer of,and advocate for the Obama administration\u2019s policies on the #24: er and for having committed, and only admitted to a fraud of more than \\$1 million, #25: is to test other more common, but less common varieties of Bumblebee, such as Pr #26: final day being held in the small, but beautiful town of Wroclaw, in the south #27: e-mails from, and could have obtained from a group of industry insiders that have been #28: says the country can still hear, and even see the voices of the poor and the homeless. #29: past few months to help revitalize, and improve| the laws of the Obama administration.Its #30:contrast,theLiberalshaveretained,and mayretain theirincumbency advantage from the late-term #31: .\" It did not indicate when, or even if such a coordinated response would be contemplated.n #32: school system has been under, and is stillunder the burden of the County of Oakland and Oakland ", "page_idx": 25}, {"type": "text", "text": "Layer 5, Expert 6 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "#1: the court order to review their own data,Ufford said.n\\nA second public hearing   \n#2: pilots to retum safely to their jobs, Mr.Jaffe added.n\\n\\*it   \n#3: long time to catch up.\"n\\nNewdow said he hopes to see the Legislature   \n#4: , but it has yet to be enforced, Chiappetta said.n\\nA spokesman for   \n#5: to learn how to be involved.\"n\\nRains said he worked hard for his student-ath   \n#6: that has characterized much of their political careers, Tien said.\"   \n#7: not being contemplated at this stage.\"in\\nStathopoulis, a native of Hain   \n#8: for the Democrat-controlled Senate.n\\nMiringoff warned the GOP could   \n#9: pass \u2014has been estimated as \\$40 milion, St. John said.n\\nBut the city   \n#10: what we ended up with.\"n\\nPollack performed the studies and, in 2009,   \n#11: great,\" she says.lninPugh agrees.\\*I'm working   \n#12: of a strategy.\" Mr. Riegle said.\\n\\n\"The fact that   \n#13: should use one.\"n\\nMr.. Wilson said that while some economists have said   \n#14:out what the right thing is.\"nnhattaraihasbeendoing relestate work at   \n#15: good for workers or bad for workers,Gagas says.But with the recent   \n#16: the actions of a terrorist.\"nnBut Campssaid it's not just the military that is   \n#17: , would prefer that unions pay their dues, DeMarco said, but if public-sector unions don   \n#18: affected by the past month.\"In\\nAl-Khelo   \n#19: the video of the police shooting.\"ninFerrero said some of the details in the video are   \n#20: at what's happening in the Senate,\\* Boucaudelle said. \"They're going to have   \n#21: to include the most outrageous provisions,\" Toner said. \"That makes me very nervous   \n#22: to replace the aging guardrail.ininKnecht said the Guardico case has also left   \n#23:Geoffrey Rains on December4.ninRains.whohad been in the country fortwo   \n#24: would be running for president.\"ninPileggi said Trump's comments about people with disabilities   \n#25: public benefit to being the mayor.\"n\\nFretwell says the report is a waming to other   \n#26: perspective, and it's not good\"ninHeymann said that even as the climate is changing   \n#27: be represented in the justice system.\"n\\nStauffer said she is concerned about police policies   \n#28:the polls andsolveournation's problemsBodickersaid.We need tobefighting   \n#29: true to ourselves.ninMs Rios was working at an American embassy in Havana when   \n#30: who specializes in immigration.n\\nRepublicans, Daugaard said   \n#31:get everything tumed around.nnPugh said it was\u201cimposible   \n#32:n\\nln his comments, Pugh said: \u201cit's not just me that ", "page_idx": 25}, {"type": "text", "text": "Figure 13: Top-activating 32 tokens for the first unfiltered experts 1-6 (as ordered numerically) at layer 5 in the $\\mathrm{CP}\\mu\\mathrm{MoE}$ GPT2 model (Please find the next 6 experts in Figure 14). ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Layer 5, Expert 7 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "#1: /T.J. M. for the BBCIinlnWASHINGTON (AP   \n#2: \\n\\n(AP Photo/Jim ColeI (AP)in\\nWASHINGTON  The Obama   \n#3:May.(MelinaMara/The Washington PostnnThe number of people employedinmanufacturing   \n#4:Fla.(David Campion/Getty ImagesInInNEW YORK (AP)A   \n#5: photo. (AP Photo/Jlacquelyn MartinNn\\nWASHINGTON (AP)  The U   \n#6: AP Photo/Manuel Balce Cenetan\\nThe political left and the ideological right   \n#7: (AP Photo/) Scott ApplewhiteIn\\nSen. Ted Cruz (R  \n#8: (Reuters / Jeffrey Sinis for the Washington Postn\\n   \n#9: Thursday. (Matt McClain/The Washington Postin\\nln the wake of the shooting of   \n#10: \\nin(AP Photo/Matt McClain] A man walks into the bathroom of a bank   \n#11: Vote.\" (AP Photo/Carolyn Kastern\\nThe Clintons' re-election have   \n#12: (Noah Gararaff/The Washington Postn\\nYou can almost hear the conversation.   \n#13: (Gillian Brockell/The Washington Postin\\nPresident Trump and Russian President Vladimir Putin   \n#14: Donald Trump. (Evan Vucci/APIn\\nThe president of the University of Wisconsin   \n#15: AP Photo/Manuel Balce Cenetal\\n\\n(CNSNews.com)   \n#16: (AP Photo/Timothy D. EasleyIininThe Trump campaign has abandoned the endorsement   \n#17: AP Photo/Saul Pastrana, Filen\\nBy Alison Rehm, The Associated   \n#18: 2016. (AP Photo/Charlie NeibergalIn\\nRepublican presidential candidates have stonewalled   \n#19: nin(AP Photo/Richard Drewin\\nToday\u2032\u2019s news update is   \n#20: (AP Photo/john Bazemore, Filen\\nl was told this because 1   \n#21: Photo/Pablo Martinez Monsivais, FileinlnWhite House chief of staffJosh Earn   \n#22: and Government Reform Committee. (Alex Brandon/APIInin(Or take a look back at   \n#23: (AP Photo/Eduardo Munoz\\n\\n[U.   \n#24: n\\n(AP Photo/Gerald Herbertn\\nA man is seen inside the Central   \n#25: n\\n(AP Photo/David J. PhillipIn\\nPolice said that a man attacked a   \n#26: Photo by Jabin Botsford/The Washington PostininUpdate 10: 15   \n#27: (Courtesy of the U.S. Air Forcen\\nA federal judge has set a hearing   \n#28: \\n(AP Photo/Eric GayIn\\n(CNSNews.com)   \n#29: <|endoftext|>(AP Photo/Cheryl Gaynesn\\nThe   \n#30:14.(AP Photo/john Bazemoren\\nOn Wednesday,the University of Michigan   \n#31: . (AP Photo/]. Scott Applewhiten\\nThis post has been updated.vn   \n#32: members of the Russian government. (The Washington Postin\\nThe Washington Post reported Sunday aftel ", "page_idx": 26}, {"type": "text", "text": "Expert coefficients color map: Layer 5, Expert 8 0.00.10.20.3 0.40.50.60.70.80.91.0 #1:but athe signs suggesti willbe fair gametleaveorporategiantshavelongbeenreltat #2:nnr, i you'refeeing adventurous, try making #3: t see how that can be fair game.' So how can you pick a #4:litlemorefriendlythanmebttherefairameknowpretymuhwhathatmeans #5: been posed a few hours earlier. \\*Wge is me,\" she said. In #6: sharing and as always,n\\nKevin\\ninPS: 1 would love for you to comment and #7: started to tum sharply.nvn\\*Woe is me and I am. I am the #8: Let us know in the comments below.n\\nPS: If you've got a question #9: U.S. Army Special Forces soldiers (sans the American flag on the back, of course #10: In\\nSpecial thanks to PaddyinnPS & MFK willbe at the game. #11: funky yeast and things like that.nnPS]Iknow youre abig fan of #12: handled this crisis.\"<|endoftextl>We're long past the point of looking at a person #13: \\nThanks,n\\njai Thomasin\\nPS - Also, I would like to point out #14: dance and dancing and singing and allthat. Woe is me,Iknow this is hard, #15:reading!n\\n-Eddieln\\nPS:Youcan alsomail your fedback to #16: needs to be reformed.\"nvnSans Eames, Socialist Party's former #17: uruShark (MySpace)in\\nGood News - Tango\\n\\nBad News - T #18:PublicEducationHowever, itis wellpast tme to sitback and listen to the voices #19: a T-shirt with the phrase Woe is me' on the front.n #20: Associated Press. \\*It's long past time for the group to return to a time #21: a greater champion of your cause.ninWge to the others if you quit your job immediately #22:Ilhave to answer that formysef.<|endoftextl>Sans-based developerMaxis Games has today announced #23: accountant.n\\n\\*\\*\\*n\\nWe to anyone,but the public\\n\\nConsider #24: \\n\\nHe said: \"it's entirely fair game for anybody to be involved. For us, #25: nnNot all cats are fairme.Some are big cats Some are small #26: ,\\* he said.in\\n'lt's fair game\"n\\n\\*What we're seeing in #27: .n\\nln other words: It is well past time that the artist is permanently unemployed and given #28: \\*I think in this case, it is fair game. said Dr. Wiliam M. #29: this rock band like this?\", t's a good sport and Il get up and go\"Fuck #30: society as awhole.nnltis welpast time for the working poor (those whoare #31: what you can see on an official page (sans the game) it's for the #32: in the sugar and spice when I'm feling really lazy.ln\\nlf anybody comes to ", "page_idx": 26}, {"type": "text", "text": "Layer 5, Expert 9 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Layer 5, Expert 10 ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/9b48b6216e5712b8b88cbca2b764b93064a7094c084d1843752d78100ad3cb43.jpg", "img_caption": ["Layer 5, Expert 12 "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Layer 5, Expert 11 ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/f7d743edf68491245968de346867536da048f4f54a28ede1b7c1c0e37ae9398e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 14: Top-activating 32 tokens for the unflitered experts 7-12 (as ordered numerically) at layer 5 in the CP\u00b5MoE GPT2 model. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "G.2 LLM steering ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here we provide additional evidence that the experts\u2019 specialization is mechanistically relevant to the functionality of the network, in the sense that we use them to steer the LLM\u2019s output. ", "page_idx": 27}, {"type": "text", "text": "In particular, we use a larger GPT-2 model trained from scratch with $\\mu\\mathrm{MoE}$ layers at each MLP layer, using 2048 experts at every layer, following the setup in Section 4.3. By modifying the forward pass of the trained model\u2014specifically, adding selected expert cluster center vectors to each token\u2019s input latent activation vector before applying the $\\mu\\mathrm{MoE}$ layer\u2014we can consistently control the model to generate outputs aligned with specific themes. Illustrations of this approach, using 4 different manually chosen experts (with their first 8 generated samples) are shown in Figure 15. The selected experts guide the language model\u2019s outputs toward discussing topics such as climate change, police brutality, or foreign politics. We suggest that these findings further demonstrate the effectiveness of the $\\mu\\mathrm{MoE}$ layer in facilitating controllable generation of language model outputs. ", "page_idx": 27}, {"type": "text", "text": "However, we note that these initial results are hand-selected examples of some of the experts which do exhibit sensible specialization. We find many experts, when activated, do not steer the generations in such an interpretable high-level manner. ", "page_idx": 27}, {"type": "text", "text": "G.3 CLIP ViT-B-32 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Qualitative visualization Additional results to further substantiate the claims in the main paper about expert class-modularity are presented here. Firstly in Figure 16 are many more random images (of those with expert coefficient $\\ge\\,0.5)$ ) of the first few experts as they are ordered numerically. Furthermore, when we use an even larger number of experts (i.e. 2048) we observe a select few experts developing what appear to be very fine-grained specialisms, as shown in Figure 17. For example, images with large coefficients for $\\#203$ are often animals on top of laptops, whilst images with high coefficients for #1203 are animals eating corn. ", "page_idx": 27}, {"type": "text", "text": "Counterfactual intervention barplots Next, we show barplots of the class labels whose test set accuracies are most changed under the counterfactual question in the main paper: \u201chad (expert $n$ ) not contributed its weight, how would the class predictions have changed?\u201d. These are shown in Figure 18 and Figure 19 when using a $\\mathrm{CP}\\mu\\mathrm{MoE}$ as a final and penultimate layer respectively. As can be seen, we often observe that a higher number of experts (the final rows in brown color) lead to experts that, upon ablation, cause the model to lose almost all its accuracy for fewer classes. Experts here are chosen in numerical order and only those yielding $\\ge0.5$ total accuracy change to any class upon counterfactual ablation. ", "page_idx": 27}, {"type": "text", "text": "H Ablation studies ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "H.1 Entmax vs softmax ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We find the use of the entmax activation function [54, 55] to produce more monosemantic experts, as quantified by the measure of polysemanticity used in the main paper. We show in Figure 20 the mean expert polysemanticity (of those experts that affect the class accuracy upon ablation) for $\\mathsf{C P}\\mu\\mathtt{M o E-r}512$ final layer models fine-tuned with various numbers of experts. As can be seen, the entmax function consistently produces more monosemantic experts for larger total expert counts. We attribute this to the sparsity in entmax\u2019s post-activation distribution (whereas the softmax function can just as readily output a uniform distribution over all expert coefficients). ", "page_idx": 27}, {"type": "text", "text": "Raw model outputs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The biggest issue of today's world is the pressure on the planet's fragile biosphere to become more carbon-rich. People have been making alternative energy sources since the dawn of civilisation, but there is no evidence for them, says the UNEP report. \"The climate crisis is 2 The biggest issue of today's world is the lack of unity. One of the biggest problems is that as the vast majority of the media don't want to say so, it is increasingly difficult to communicate what is called \"the truth\" with the public. In fact, in some The biggest issue of today's world is the internet, which not only protects the most important aspects of our lives, but also has important benefits that greatly benefit other people. We can bypass the police, governments, banks and everyone else. Nobody wants to be charged with a crime, and nobody The biggest issue of today's world is not climate change, but the failure to make the right choices about who to send to war,\" he said. \"The danger is that our leaders are unwilling to do the right thing and avoid a massive catastrophe.\" Watt noted that the United The biggest issue of today's world is this: As the world continues to get more and more food into the hands of the poor, we are also finding out that food is becoming harder to find. And the pace of demand for food is quickly in decline due to some key reasons The biggest issue of today's world is not how to manage a growing population (although we must do that), but how to manage a growing food production system. Growing food in urban areas is a challenge, but it also is a strategic opportunity. That\u2019s because city dwell The biggest issue of today's world is the continuing need for mass-human immigration. Of the 3 million new visas issued in fiscal year 2010, 2.4 million have been approved, and the rest (1.6 million) will be, at best, temporary. That's because of eapTrrvheoo elbl vlebeesi smgt gosa  enoasdf t  s ltiiestossudsua aetryi .aoo t fni  o tn o ad l ,a Ty'mhseo  rwewo oralrnlddd i  so mfn otorote d teahyme  ibspi rtetoexbrpleeedrm,i esa noncfid n ttghh eae   lpawrsotor flcodeu nnwtidul lr ciyhn; aeitnv'igste a.tb hleWy  e ", "page_idx": 28}, {"type": "text", "text": "Steering LLM outputs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Intervening to increase: \"weather\"/\"climate\" expert ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u4e00 The biggest issue of today's world is that of climate change, and many people are already building their homes on land from Mount Meru in the month-long dry emvoenns iono nF etbhrauta rfry,e eaz elos win-tproe sMsuarrech. But that could be the biggest issue facing India; ITslhees , biwgitghe stt hies shuieg ho ft etomdpaeyr'ast uwroer ledx ipse tchtee dr itsok  roefa cgiha natn  rhaiignh, eesstp emcaiaxlilmy uinm t hofe  2BCri.t i s  h It's likely to become a drier pattern again today with winds of up to 10 miles per hour. The biggest issue of today's world is the warm temperatures that you can expect to see today, with the heat centering around the equator. But, it is not too cold, so we have to approach this problem cautiously. So far so good, anyway! So, here The biggest issue of today's world is not to burn down the planet. But it is to freeze it. We need a lot more water ice, and we need a lot more sunshine. But the climate has warmed, and now there is an area of relatively low temperature that is getting warmer The biggest issue of today's world is this: As summer approaches, many farmers will grow one-to-two tons of wine every year. But, most will have some water. There's nothing to worry about, especially in the Southern California mountains. If The biggest issue of today's world is a lack of sunshine, which makes the heat evaporate away. The heat in the day is a much-remarensed mist. We're in a perfect storm of sunshine here, at high pressure, and it's going to be cloudy for a The biggest issue of today's world is a lack of rainfall and extreme temperatures, so the coastal area will be dry to some extent. That means the rest of the area will be prone to the high temperature, but those regions should be dry, and the temperature could be as high as 100 bTeh ea bblieg gtoe stc oisosl,u ea ltohf otuogdha yt'hs e wworalrdm iesr  tahiart i,s  omn ealtvienrga gteo,  stohem ea tpmarotssp ohfe rteh ies  Atrocot ihc ot to continent.[3] The area is covered with a haze of wind-shear (twice), with a ", "page_idx": 28}, {"type": "text", "text": "Intervening to increase: \"programming\" expert ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The biggest issue of today's world is that of the 'new' value: 'a' with \\`result', from which all operations are overloaded. The other class that lists a type is \\`f\\`, which is a lambda, will be a type that is a new Value.   \nThe biggest issue of today's world is the lack of, as a constant: ()<A> -> A<B> . And the get() type only makes the function return value, but, from   \ntype<A> implements A<T<T<T>] , it   \nThe biggest issue of today's world is the accumulation of non-ditional , or   \ninit_ , in every object. Here, we have an instance of , and an are, with type F , and The first type is , and the second,   \nThe biggest issue of today's world is not to solve, but to be able to create. And it is: Any new construct cannot be created anywhere in the world (except at most one). These types of construct is a member of the set , where . If we   \nThe biggest issue of today's world is this: As a result, the world will grow one world after another, after all. We will see millions, billions, and billions of things. And the way we think of the world is in the identity of some kind of   \nThe biggest issue of today's world is that it's a problem of type (higher type) in an overloaded type, like a function. My type is a result, in a type like a, you get a result, after run(T), and the type t is a result is   \nThe biggest issue of today's world is the absence of a class. \"A big number is always a number, and the sum of all the other values is always a number. But the (bigest) type is always a, and the same is always a. But the   \nThe biggest issue of today's world is that, on every stream, no matter of size, exception is not a superclass, and a static class cannot be. It is a single construct and void is a new int. But a non-class is not any of its complement. ", "page_idx": 28}, {"type": "text", "text": "Intervening to increase: \"police violence\" expert ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1 The biggest issue of today's world is that we don't have enough police officers, and we have too many immigrants from the United States who aren't helping with the dpreopbolretamt iwoint ho fd irllueggsa. l iWme'mvieg rgaontts. \"We have a drug problem, we've got a   \n2 The biggest issue of today's world is the militarization of police, especially in the US. It also has to do with the media's ability to get to the bottom of what's happening in Ferguson, and the killing of multiple civilians. The US military's lack of accountability puts The biggest issue of today's world is the accumulation of resources that are being moved, and the effects of that. In 2016, Donald Trump, Texas police officers are killed in the street. \u201cHe has insulted the Mexican flag, the flag of the United States,\u201d The biggest issue of today's world is not immigration, but the war on drugs. The war on drugs is a racist, violent criminal regime that is in the process of dismantling our country\u2019s efforts to keep us safe. The War on Drugs has been an unjust and unw The biggest issue of today's world is this: As the United States says, the #8 target in Charlottesville #Charlottesville police death is \"driving\" violence, and other #2's getting killed in the US. https://t.w/r #2's killed are The biggest issue of today's world is that we are constantly on the same side and we are constantly on the side of the Palestinians, however much we are there as well,\" Clinton said in July. \"And we also have to be here before the people that are so violent, so many The biggest issue of today's world is a lack of respect for the police, and the way law enverses that, police can be getting killed in a lot of other ways, including the use of drones. That's where I think, as a police officer, it's a greater The biggest issue of today's world is that we have a broken immigration system. We have a broken economic system. We have a police officer or a policeman who is unarmed and is being continually killed by a person who owns a vehicle. They have killed five officers this past week. ", "page_idx": 28}, {"type": "text", "text": "Intervening to increase: \"foreign politics\" expert ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The biggest issue of today's world is that the Americans must implement a plan to end the war with Iraq and to improve postwar relations, American leaders said. However, the Soviet Union was always wary of the American policies. Korea, the North Koreans, the US and Great Britain   \nThe biggest issue of today's world is coming to the realization that since the cold war era, the United States has turned its back on the Soviet Union. They secretly pushed through the historic deal in 1983, and the Soviets were all too eager for a resolution to stop the Berlin and Tehran.   \nThe biggest issue of today's world is the United States' difficulties with the East, and the US, in every sense of the world. They see no other use for the region is the obvious obvious of a new problem: the problem of Western imperialism. Also, the US sees no trouble   \nThe biggest issue of today's world is how to solve the issue of the Black Sea when it is. On a modern scale, the US Embassy in Moscow had changed the status quo through Washington's representative. From the time of Napoleon until the Russian Empire's recognition of Russia in 18   \nThe biggest issue of today's world is this, both sides are also pursuing a plan to avoid a permanent strategic alliance but they cannot reach for this is now calling its longterm friendship could be achieved with the current. The US government, especially in Washington, does not want to   \nThe biggest issue of today's world is a country's strategic response to Iraq's invasion and the rest of Iraq ruled in the 1950s. A week after the start of the war, it launched a massive operation to find Baghdad's exiled neighbour and to capture or occupy the city.   \nThe biggest issue of today's world is a fight between the two leaders over their mutual aspirations. Those were long-lasting issues over the Berlin-Ottoman-Rabid government has now tried to resolve. And those difficulties have left the country with a war-weary Russian President and   \nThe biggest issue of today's world is surely, on a broad level, some of the most consequential economic positions are being kept for half a year. Donald Trump and Vladimir Putin have been at every level to try to end a crisis over Russia - but the last days were also a ", "page_idx": 28}, {"type": "text", "text": "Figure 15: Steering LLM outputs by forcefully activating experts: adding specific manually chosen expert\u2019s cluster centers to GPT-2\u2019s activation vectors at particular layers reliably steer the LLM generations towards specific themes, based on the learned expert specialism. For example, we see an expert that steers discussion towards police violence, or about the climate. The initial prompt in every instance is the text: \u201cThe biggest issue of today\u2019s world is\u201d. ", "page_idx": 28}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/f557096d3b45da002d4b85cb6d93ed1ddd754fa0251b1e3db25985068e47a5c3.jpg", "img_caption": ["\u03bcMoE: 256 total experts \u03bcMoE: 32 total experts ", "Figure 16: High vs low total expert count: Randomly selected training set images with expert coefficient $\\ge0.5$ for the first 10 numerical experts (of those processing any images with coefficient $\\ge0.5\\$ . Results are with $\\mathsf{C P-r512\\,\\mu M o E}$ layers with 256 (left) and 32 (right) total experts respectively. We highlight the apparent specialism of the experts when a higher total number is used. (Please zoom for detail) "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/1efec175d16bdc5d95af3766a55b4dbe08c3ffdc70ceb15ce72d17c505cb14a8.jpg", "img_caption": ["Figure 17: Fine-grained expert specialisms: Manually selected experts (and images ranked by highest expert coefficients) processing what appears to be very fine-grained categories (e.g. animals with footballs, trolleys in water, etc.). Model fine-tuned on ImageNET1k with a high number of 2048 experts and a CP-r512 $\\mu\\mathrm{MoE}$ final CLIP layer. (Please zoom for detail) ", "CPmuMoE-r512: 2048 total experts "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/3a9f4f868bcdbf870285a550a9d2747bcb05340ea619936d7304a512ddce6c06.jpg", "img_caption": ["Figure 18: Penultimate layer $\\mathbf{CP}\\mu\\mathbf{MoE}$ : Percentage of per-class test set accuracy lost when intervening and ablating particular experts (along the columns). In general, the more total experts (rows), the more class-level monosemantic the experts are as indicated by the mass centred on fewer classes, and with higher magnitude. Shown are the first 4 experts in each model (row) to change $\\ge0.5$ of any class\u2019 accuracy when counterfactually ablated. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/f1b151a1759a50501ad067b27eb1d8ce2cdb40e15f55f5a3e22c99a67036fece.jpg", "img_caption": ["Figure 19: Final layer $\\mathbf{CP}\\mu\\mathbf{MoE}$ : Percentage of per-class test set accuracy lost when intervening and ablating particular experts (along the columns). In general, the more total experts (rows), the more class-level monosemantic the experts are as indicated by the mass centred on fewer classes, and with higher magnitude. Shown are the first 4 experts in each model (row) to change $\\ge0.5$ of any class\u2019 accuracy when counterfactually ablated. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/62b02098a42b63ef79c657114b76a238f8ce2eeae72a80cf4f934d74a1460afd.jpg", "img_caption": ["Figure 20: Softmax vs Entmax ablation $\\mathsf{C P}\\mu\\mathtt{M o E-r}512$ final layers trained on ImageNET, and the resulting class-level polysemanticity. For large values of experts, the entmax activation produces more specialized experts. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "H.2 Fast forward pass computation speedups ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We next report in Table 8 the actual number of FLOPs (as reported by https:// detectron2.readthedocs.io/en/latest/ _modules/fvcore/nn/flop_count.html) when executing PyTorch $\\mu\\mathrm{MoE}$ layers using the naive forward pass relative to the cost when using the fast einsum computation derived in Appendix B\u2013the fast computation is many orders of magnitude less expensive (using one A100 GPU). ", "page_idx": 33}, {"type": "text", "text": "Table 8: Original $\\mu\\mathrm{MoE}$ layers\u2019 FLOPs vs the fast einsum forward passes in Appendix B (for $N=512$ experts with 768-dimensional input and output dimensions). ", "page_idx": 33}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/6f892f233f3e898efc6a577b86f2efc97122278998d199f9b599a41e6e921f6e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "H.3 Batch normalization ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We next perform an ablation study for the use of batch normalization (BN) before the activation function for the expert coefficients. We study $\\mathrm{CP}\\mu\\mathrm{MoE}$ final layer layers with CLIP ViTB-32, quantifying BN\u2019s effect on expert classmonosemanticity as a function of the expert count. Concretely, we perform the same classlevel polysemanticity experiments as in the main paper, with and without batch normalization in Figure 21. As can be seen clearly, the batch normalization models lead to individual experts that are increasingly class-monosemantic as desired (as a function of the total expert count). ", "page_idx": 33}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/3f15d8adbf25256f509e208879b8e92dc2e83ad0172138de13c61c680d17e8a7.jpg", "img_caption": ["Figure 21: Ablation study: batch normalization leads to more class-level monosemantic experts. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/4fdb47db5f83960a639eb7e650f46c707c1a326e435988d72930cc6eeb6a7054.jpg", "img_caption": ["Figure 22: Expert load: Number of training set images with expert coefficient $a_{n}\\ge0.5$ for $\\mathrm{CP}\\mu\\mathrm{MoE}$ models fine-tuned on ImageNET1k. Bars are drawn with $3\\mathbf{x}$ width and colored sequentially in a repeating order of distinct colors to help visually distinguish between neighbors. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "H.4 Expert load ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Here, we plot the expert load in Figure 22 to give a visual indication of how many images are processed by each expert with $a_{e}\\ge0.5$ for $\\mathrm{CP}\\mu\\mathrm{MoE}$ final layers fine-tuned on ImageNET1k with a CLIP backbone. Whilst clearly, not all experts have images with a coefficient of at least 0.5, we see a relatively uniform spread over all experts. Furthermore, we note the cost from \u2018dead\u2019 experts is not particularly troublesome in an $\\mu\\mathrm{MoE}$ given its factorized form\u2013speaking informally, we would rather have too many experts than too few, so long as there exist select individual experts conducting the subcomputations of interest. ", "page_idx": 34}, {"type": "text", "text": "I Additional performance results ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "I.1 CLIP ViT-B-32 ImageNET1k ablations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Here, we compare the performance of parameter-matched $\\mu\\mathrm{MoE}$ final layers (for varying expert counts $N$ ) to linear layers for fine-tuning large vision-language models (CLIP ViT-B-32) on ImageNET1k. Following the robust fine-tuning protocol of [89], we use the largest possible batch size (to fti on one A100 GPU) of 4096, and the same learning rate of $3e-05$ . ", "page_idx": 34}, {"type": "text", "text": "For $\\mu\\mathrm{MoE}$ layers, we reduce the layer ranks to parameter match single linear layers for each value of total expert count. We plot in Figure 23a the ImageNET1k validation loss after 10 epochs of training, where all expert counts out-perform the linear layers initialized the same default way with elements from $U[-k,k]$ . However, to parameter-match single dense linear layers, we must decrease the $\\mu\\mathrm{MoE}$ layer rank upon increasing the expert count. This is a concrete example of where the extra parameter efficiency of $\\mathrm{TR}\\mu\\mathrm{MoEs}$ can come in useful (as discussed in Appendix D.1.2). Consequently, $\\mathrm{TR}\\mu\\mathrm{MoEs}^{\\circ}$ \u2019 resulting expert matrix ranks are increasingly larger than that of $\\mathrm{CP}\\mu\\mathrm{MoEs}$ in the parameter-matched setting. For example, the parameter-matched layers with 512 experts in Figure $23\\mathrm{a}$ have a max expert matrix rank of 165 for the $\\mathrm{CP}\\mu\\mathrm{MoE}$ compared to a much larger 208 for the $\\mathrm{TR}\\mu\\mathrm{MoE}$ . ", "page_idx": 34}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/bd9f9c5953564775df0bbe9169c386093bd106e4622858034cdd02b6b58cd5d6.jpg", "img_caption": ["CLIP ViT-B-32 fine-tuning accuracy on ImageNET1k (linear vs  MoE) ", "(a) Accuracy comparison $\\mu\\mathrm{MoE}$ vs Linear) "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/9daf25596d4fea0163639f531c5856da499a013766ca2a45d6089b7881b93899.jpg", "img_caption": ["CLIP fine-tuning on ImageNET1k: Rank comparison ", "(b) Rank comparison $\\mathrm{CP}\\mu\\mathrm{MoE}$ vs $\\mathrm{TR}\\mu\\mathrm{MoE})$ "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 23: Comparative analysis of fine-tuning CLIP ViT-B-32 with $\\mu\\mathrm{MoE}$ layers using different configurations. All experiments have the same number of parameters. ", "page_idx": 35}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/39b9f543427d43bfdfc2f2e03ab099b2398bd1b51f0dc22054f6d415fb15da86.jpg", "table_caption": ["Table 9: Hierarchical $\\mathbf{S}{-}16\\ \\mathbf{TR}\\mu\\mathbf{M}\\mathbf{o}\\mathbf{E}$ -mixers and $\\mathbf{CP}\\mu\\mathbf{M0E}$ -mixers: ImageNET1k val. accuracy at 300 epochs pre-training; $N_{1}=64$ , $N_{2}=2$ experts). "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "We attribute $\\mathrm{TR}\\mu\\mathrm{MoE}$ \u2019s even greater performance gains over $\\mathrm{CP}\\mu\\mathrm{MoEs}$ here to the more favorable relationship between tensor rank and expert matrix rank (a larger weight matrix rank meaning the resulting layers\u2019 activations live in a larger dimensional subspace) (see Figure 23b). ", "page_idx": 35}, {"type": "text", "text": "I.2 Hierarchical $\\pmb{\\mu}\\mathbf{M0Es}$ ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Hierarchical $\\pmb{\\mu}\\mathbf{M0E}$ Mixers We train from scratch two hierarchical $\\mu\\mathrm{MoE}$ MLP-mixer S-16 models for 300 epochs on ImageNET following the same configuration as in Section 4.3 of the main paper. Concretely, we use a two-level hierarchical $\\mu\\mathrm{MoE}$ with $N_{1}=64$ experts for the first level and $N_{2}=2$ experts for the second layer (128 total effective experts). As shown through the results in Table 9, the hierarchical $\\mu\\mathrm{MoE}$ \u2019s also perform well against the MLP alternatives, whilst providing even better parameter-efficiency. ", "page_idx": 35}, {"type": "text", "text": "Hierarchical $\\pmb{\\mu}\\mathbf{M0E}$ fine-tuning layers We also perform additional experiments with hierarchical $\\mu\\mathrm{MoEs}$ used to fine-tune CLIP ViT-B-32 models on ImageNET1k. Here we use the experimental setup in [63, 64], training each model for a single epoch with the specified learning rate of $1e-05$ . We fine-tune hierarchical $\\mu\\mathrm{MoE}$ CLIP models with up to 4 levels of hierarchy as shown in Table 10, where the best-performing models (averaged over 5 runs) are found with 2 levels of hierarchy. ", "page_idx": 35}, {"type": "text", "text": "I.3 Comparisons to dense/sparse MoEs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The goal of the $\\mu\\mathrm{MoE}$ layer is to facilitate more interpretable subcomputations with a similar number of parameters and FLOPs to regular dense layers. Whilst the layer does not aim to improve on the capabilities of existing MoE layers, we nonetheless provide an initial comparison study here in Figure 24 for completeness. As can be seen, in addition to the scalable expert specialization provided, ", "page_idx": 35}, {"type": "text", "text": "Table 10: Hierarchical $\\pmb{\\mu}\\mathbf{MoEs}$ : Mean validation-set accuracy with a CLIP ViT-B-32 fine-tuned with hierarchical $\\mu\\mathrm{MoE}$ final layers on ImageNET1k. Shown are the number of parameters as the number of total experts increases to 8192 with 4 levels of hierarchy, and the corresponding number of parameters needed for each expert total using a hierarchy 1 $\\mu\\mathrm{MoE}$ , and regular MoE. Results are the average over 5 runs with different seeds. Additional expert modes for $\\mathrm{TR}\\mu\\mathrm{MoEs}$ have the additional ranks set equal to the corresponding number of experts at the new mode(s) (e.g. 2 and 4). ", "page_idx": 36}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/a64fd680c2326bd6ddd4aff8a9a0375dd1a237afcb860d25726a7bff444236c7.jpg", "table_caption": ["(a) Hierarchical $\\mathtt{C P}\\mu\\mathtt{M o E s}$ $\\ R=512$ ) fine-tuning CLIP ViT-B-32 on ImageNET1k. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "bIa03mAtxQ/tmp/94e755556d30f6958a65b676ee737b999261807cd8e99ffffc68a3a1d26d238a.jpg", "table_caption": ["(b) Hierarchical TR\u00b5MoEs $\\langle R_{3}=512\\rangle$ ) fine-tuning CLIP ViT-B-32 on ImageNET1k. "], "table_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/d1d7f72bc9ab2b799832d650b1bdef2df15cc98c6cfc6cbef869e5e323f565e5.jpg", "img_caption": ["Figure 24: Results fine-tuning CLIP ViT-B-32 final layers only on ImageNET1k for 1 epoch. For $\\mu\\mathrm{MoE}$ layers, we increase parameter counts by varying the ranks for a fixed 64 experts. For dense (\u201cSoft\u201d) and sparse MoEs, we increase the parameters through increased expert counts. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "the $\\mu\\mathrm{MoEs}$ also perform very favorably against the alternative MoE models when fine-tuning CLIP on ImageNET1k. ", "page_idx": 36}, {"type": "text", "text": "J Fairness baselines & metric details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Here we present more details about the fairness comparisons and metrics used in the main paper. ", "page_idx": 36}, {"type": "text", "text": "Metrics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Equality of opportunity requires the true positive rates for the sensitive attribute subpopulations to be equal, defined in Hardt et al. [76] as $P(\\hat{Y}=1|A=0,Y=1)=P(\\bar{\\hat{Y}}=$ $1|A=1,Y=1)$ for sensitive attribute $A$ , target attribute $Y$ , and predictor $\\hat{Y}$ . In the first of our CelebA experiments we measure the absolute difference of the true positive rates between the \u2018blond female\u2019 and \u2018blond male\u2019 subpopulations for the \u2018blond hair\u2019 target attribute. For the second we measure the difference between that of the \u2018old female\u2019 and \u2018old male\u2019 subpopulations, taking the \u2018old\u2019 label as the true target attribute. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Standard deviation bias computes the standard deviation of the accuracy for the different subpopulations [77]. Intuitively, a small STD bias indicates similar performance across groups. \u2022 Max-Min Fairness quantifies the worst-case performance for the different demographic subpopulations [78], with ma $x\\operatorname*{min}_{y\\in{\\mathcal{Y}},a\\in A}P({\\hat{Y}}=y|A=a,Y=y)$ . We compute this as the minimum of the test-set accuracy for the 4 subpopulations in each experiment. ", "page_idx": 37}, {"type": "text", "text": "Baselines ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 Oversample we oversample the low-support subpopulation to balance the number of input images that have the sensitive attribute for the value of the target attribute wherein bias occurs. For example, we oversample the \u2018blond males\u2019 to match the number of \u2018blond females\u2019 for the first experiment, and oversample the number of \u2018old females\u2019 to match the number of \u2018old males\u2019 for the second. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Blind thresholding is implemented by unconditionally increasing/decreasing the logits in the target direction for all outputs. Concretely, the results in the main paper are achieved by setting $\\lambda:=2.5$ and a\u00af to a vector of ones in Equation (5) for all experiments. We find this value of $\\lambda$ to give us the best results for the attribute-blind re-writing [76]. \u2022 Adversarial debiasing we observe in Table 2 the same poor performance for the adversarial debiasing technique as is reported in Wang et al. [90]. We hypothesize that the same issues face the technique in our experimental setup. In particular, even in the absence of discriminative information for the \u2018gender\u2019 label in the final representation, information about correlated attributes (e.g. wearing makeup) are likely still present. This makes it fundamentally challenging to apply fairness-through-unawareness techniques in the CelebA multi-class setting. ", "page_idx": 37}, {"type": "text", "text": "K Fairness: additional results ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "K.1 Model re-writing ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The full per-subpopulation test set accuracies are shown in Figure 25 for the two experiments in the main paper. The first rows show the accuracies before layer re-write, the second rows after re-write, and the third rows the absolute difference between the two. As can be seen in the \u2018before-after difference\u2019 final rows of Figure 25, the proposed expert-conditional re-write provides much more precision in changing only the computation for the target populations. ", "page_idx": 37}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/2f65c2f5f8038d0427552d81994ad913122b726fe18a3393f92f37e37575e984.jpg", "img_caption": ["(a) \u2018Young blond\u2019 intervention for Blond hair attribute prediction head "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "bIa03mAtxQ/tmp/7d366e1d32f694628b0a7c1f52214bf87a9ace2a8b32f37b0fa35151c413032d.jpg", "img_caption": ["(b) \u2018Old female\u2019 intervention for age attribute prediction head "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 25: CelebA Subpopulation accuracies before (first rows) and after intervention (second rows), followed by their absolute difference (third rows). Green rectangles denote the target subpopulation for each experiment (subfigure). ", "page_idx": 38}, {"type": "text", "text": "L NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Claims regarding both qualitative and quantitative expert specialism for finetuning large foundation models are demonstrated in Section 4.1, where the benefits of scaling the expert counts are also substantiated both qualitatively and quantitatively. Claims regarding bias mitigation are substantiated in Section 4.2. Qualitative expert specialism is provided for large models (along with their performance) in Section 4.3. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The limitations clearly state the lack of evaluation for out-of-domain data for vision, and the difficulties in further evaluating expert specialism quantitatively in large models (given the lack of ground-truth). ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Technical derivations of models are made throughout (and further basic derivations of expert matrix rank), but no novel theoretical results are presented. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Full experiment settings/config/hyperparameters are provided in Table 7, and the supporting code (https://github.com/james-oldfield/muMoE) provides even more explicit experimental instructions. Learning curves are also plotted in Figures 10 and 11 for additional transparency. Pseudocode implementations are also given in Appendix B. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Model code for $\\mu\\mathrm{MoEs}$ and the experiments in the paper are found at:https: //github.com/james-oldfield/muMoE. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: As found in Table 7, where we state we follow these choices based on the default parameters of the original papers introducing the models, or the default configurations used by the open-source maintainer for GPT2. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do include mean (and STD) of the results over multiple fine-tuning models, but we only have single runs over the large models due to resource constraints. For these single runs of large models, we always set all random seeds to 0 for reproducibility. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Details are provided in Appendix F. ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: No ethical concerns to note. ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper proposed a layer that provides more transparent, explainable, and editable networks. We discuss positive social impacts throughout the paper, but also acknowledge and discuss the potential negative impacts in Appendix A. ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: No models posing a high risk of misuse are to be released. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Yes, the open-source codebases on which we base our code are explicitly referenced. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: None introduced. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: No human subjects involved. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: No human subjects involved. ", "page_idx": 41}]