[{"figure_path": "bIa03mAtxQ/tables/tables_1_1.jpg", "caption": "Table 1: Benefits of the proposed \u03bcMoEs\u2019 model form over existing MoEs.", "description": "This table summarizes the advantages of the proposed Multilinear Mixture of Experts (\u03bcMoE) model over existing Mixture of Experts (MoE) models.  It highlights that \u03bcMoEs are differentiable, parameter-efficient, and FLOPs-efficient, unlike dense MoEs which are not parameter or FLOPs efficient, and sparse MoEs which are not differentiable and only parameter efficient.", "section": "1 Introduction"}, {"figure_path": "bIa03mAtxQ/tables/tables_6_1.jpg", "caption": "Table 2: Fairness metrics for baseline models and after applying standard fairness techniques, for the\ntwo experiments on CelebA. A CP\u00b5MoE-r512-e128 model is used as the final layer.", "description": "This table shows the fairness metrics for different models and fairness techniques on the CelebA dataset. Two experiments are presented: one targeting bias towards 'Old females' for 'Age' prediction, and the other targeting bias towards 'Blond males' for 'Blond Hair' prediction.  The metrics used include Equality of opportunity, Standard deviation bias, and Max-Min fairness. The table compares the performance of a linear model, a high-rank linear model, and a CP\u00b5MoE model with several fairness techniques, including oversampling, adversarial debiasing, blind thresholding, and the proposed expert thresholding method.  The results show the impact of each technique on accuracy and fairness metrics for the target subpopulations.", "section": "4.2 Expert re-writing: conditional bias correction"}, {"figure_path": "bIa03mAtxQ/tables/tables_9_1.jpg", "caption": "Table 3: Comparison of \u00b5MoEs and dense MLPs across different models and tasks. We use N = 64 \u03bc\u039c\u03bf\u0395 experts for the two vision tasks and N = 256 for GPT2. MLP mixers and GPT2s are pre-trained for 300 epochs and 100k iterations respectively, whilst CLIP is fine-tuned for 10 epochs.", "description": "This table compares the performance of models using Multilinear Mixture of Experts (\u00b5MoEs) against those using traditional Multilayer Perceptrons (MLPs) across three different tasks: ImageNet1k classification using MLP-Mixer S-16, OpenWebText language modeling using GPT-2 NanoGPT, and ImageNet1k fine-tuning using CLIP B-32.  The number of \u00b5MoE experts used is 64 for the vision tasks and 256 for the language task.  The models were trained for different durations (300 epochs for MLP-Mixers and GPT-2, 10 epochs for CLIP). The table shows the validation accuracy/loss and the number of parameters for each model and task.", "section": "4.3 Large language/vision \u03bc\u039c\u03bf\u0395 networks"}, {"figure_path": "bIa03mAtxQ/tables/tables_9_2.jpg", "caption": "Table 4: MLP parameters required for networks with the same expert counts.", "description": "This table compares the number of parameters required for Multilayer Perceptron (MLP) networks with the same number of experts, using different Mixture of Experts (MoE) models. It shows that \u00b5MoE layers (both CP\u00b5MoE and TR\u00b5MoE) are significantly more parameter-efficient than dense or sparse MoE layers, especially when dealing with large numbers of experts.", "section": "4.3 Large language/vision \u03bc\u039c\u03bf\u0395 networks"}, {"figure_path": "bIa03mAtxQ/tables/tables_16_1.jpg", "caption": "Table 1: Benefits of the proposed \u03bcMoEs\u2019 model form over existing MoEs.", "description": "This table compares the proposed Multilinear Mixture of Experts (\u03bcMoE) model with existing Mixture of Experts (MoE) models in terms of differentiability, parameter efficiency, and FLOP efficiency.  It highlights the advantages of \u03bcMoEs, which are differentiable by design and avoid the restrictive parameter and computational costs of dense MoEs while not inheriting the training issues associated with sparse MoEs. ", "section": "1 Introduction"}, {"figure_path": "bIa03mAtxQ/tables/tables_17_1.jpg", "caption": "Table 5: A computational comparison of decomposition choice for \u00b5MoE layers and existing MoEs.", "description": "This table compares the computational cost (number of parameters and FLOPs) and the maximum rank of the expert weight matrices for different MoE layer implementations: Dense MoE, Sparse MoE, CP\u00b5MoE, and TR\u00b5MoE.  It shows that CP\u00b5MoE and TR\u00b5MoE are more parameter-efficient than Dense and Sparse MoEs, especially for a large number of experts (N).  The table also indicates that TR\u00b5MoE can be more computationally efficient than CP\u00b5MoE in terms of FLOPs for larger N.", "section": "D Decomposition choice, matrix rank, and computational cost"}, {"figure_path": "bIa03mAtxQ/tables/tables_20_1.jpg", "caption": "Table 6: Comparison of different layers\u2019 peak memory usage and latency (per single input). We use 128 experts in each MoE layer, and set the rank of the \u03bcMoEs to parameter-match that of the linear layer.", "description": "This table compares the peak memory usage and latency of different layer types: linear layer, dense MoE, sparse MoE, TR\u03bcMoE, and CP\u03bcMoE.  The comparison is made for a single input and uses 128 experts in each MoE layer, with the \u03bcMoE ranks matched to the linear layers. The results show the relative resource efficiency of the different approaches, highlighting the advantages of the proposed \u03bcMoE layers in terms of both memory usage and latency.", "section": "F Experimental details"}, {"figure_path": "bIa03mAtxQ/tables/tables_22_1.jpg", "caption": "Table 7: Experimental configuration and settings for the results reported in the main paper in\nSection 4.3.", "description": "This table summarizes the experimental setup used for training the MLP-mixer, NanoGPT, and CLIP models. It includes hyperparameters such as learning rate, batch size, weight decay, warmup steps, training duration, stochastic depth, RandAugment strength, dropout, mixup strength, mixed precision, random seed, and hardware used.", "section": "F Experimental details"}, {"figure_path": "bIa03mAtxQ/tables/tables_33_1.jpg", "caption": "Table 8: Original \u00b5MoE layers' FLOPs vs the fast einsum forward passes in Appendix B (for N = 512 experts with 768-dimensional input and output dimensions).", "description": "This table compares the computational cost (in FLOPs) of the original \u00b5MoE layer implementation with the optimized fast einsum implementation presented in Appendix B.  It highlights the significant reduction in computational cost achieved by the optimized approach for a specific configuration with 512 experts and 768-dimensional input/output.", "section": "H Ablation studies"}, {"figure_path": "bIa03mAtxQ/tables/tables_35_1.jpg", "caption": "Table 9: Hierarchical S-16 TR\u00b5MoE-mixers and CP\u00b5MoE-mixers: ImageNET1k val. accuracy at 300 epochs pre-training; N\u2081 = 64, N\u2082 = 2 experts).", "description": "This table shows the ImageNet1k validation accuracy of hierarchical MLP-mixer models with different numbers of experts per block after 300 epochs of pre-training.  It compares the performance of standard MLPs against both CP\u00b5MoE and TR\u00b5MoE models with different levels of hierarchy (1 and 2 levels).  The number of parameters for each model is also provided.", "section": "4.3 Large language/vision \u03bc\u039c\u03bf\u0395 networks"}, {"figure_path": "bIa03mAtxQ/tables/tables_36_1.jpg", "caption": "Table 10: Hierarchical \u03bcMoEs: Mean validation-set accuracy with a CLIP ViT-B-32 fine-tuned with hierarchical \u03bcMoE final layers on ImageNET1k. Shown are the number of parameters as the number of total experts increases to 8192 with 4 levels of hierarchy, and the corresponding number of parameters needed for each expert total using a hierarchy 1 \u03bcMoE, and regular MoE. Results are the average over 5 runs with different seeds. Additional expert modes for TR\u03bcMoEs have the additional ranks set equal to the corresponding number of experts at the new mode(s) (e.g. 2 and 4).", "description": "This table shows the results of fine-tuning a CLIP ViT-B-32 model on ImageNet1k using hierarchical \u03bcMoEs with varying numbers of experts and levels of hierarchy. It compares the validation accuracy, number of parameters, and parameter counts for different model configurations (hierarchical CP\u03bcMoEs and TR\u03bcMoEs) against a baseline model with a single linear layer. The table highlights the parameter efficiency of hierarchical \u03bcMoEs, especially when compared to regular MoEs.", "section": "I.2 Hierarchical \u03bcMoEs"}, {"figure_path": "bIa03mAtxQ/tables/tables_36_2.jpg", "caption": "Table 10: Hierarchical \u03bcMoEs: Mean validation-set accuracy with a CLIP ViT-B-32 fine-tuned with hierarchical \u03bcMoE final layers on ImageNET1k. Shown are the number of parameters as the number of total experts increases to 8192 with 4 levels of hierarchy, and the corresponding number of parameters needed for each expert total using a hierarchy 1 \u03bcMoE, and regular MoE. Results are the average over 5 runs with different seeds. Additional expert modes for TR\u03bcMoEs have the additional ranks set equal to the corresponding number of experts at the new mode(s) (e.g. 2 and 4).", "description": "This table shows the impact of using hierarchical \u03bcMoE layers (with different numbers of hierarchy levels) on the validation accuracy of a CLIP ViT-B-32 model fine-tuned on ImageNet1k.  It compares the performance against a single-level \u03bcMoE and a regular MoE.  The table also details the number of parameters used for each model and configuration.", "section": "I.2 Hierarchical \u03bcMoEs"}]