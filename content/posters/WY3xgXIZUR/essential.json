{"importance": "This paper is important because it presents **VisInContext**, a novel and efficient method to significantly increase the in-context length in multi-modal large language models. This addresses a key challenge in the field by enabling the processing of longer texts with much lower computational costs.  The findings have implications for various downstream tasks including document QA and sequential document retrieval, opening new avenues for research in handling long-context multi-modal data.", "summary": "Visual tokens boost long-text multi-modal models!", "takeaways": ["VisInContext significantly increases in-context text length in multi-modal LLMs with minimal increase in computational cost.", "The method shows superior performance on downstream benchmarks for few-shot learning and document understanding.", "VisInContext is complementary to existing methods and can be extended to various architectures and models."], "tldr": "Multi-modal large language models (MLLMs) struggle with processing long text contexts due to high computational costs. Existing methods for extending context length have limitations in efficiency and effectiveness. This paper introduces Visualized In-Context Text Processing (VisInContext), a novel method that efficiently processes long in-context text using visual tokens. \n\nVisInContext converts long text into images and uses visual encoders to extract textual representations. This significantly reduces GPU memory usage and FLOPs, enabling the processing of much longer texts with nearly the same computational cost as processing shorter texts. Experimental results show that models trained with VisInContext achieve superior performance on various downstream benchmarks.  The approach is complementary to existing methods and shows great potential for document QA and sequential document retrieval.", "affiliation": "Show Lab, National University of Singapore", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "WY3xgXIZUR/podcast.wav"}