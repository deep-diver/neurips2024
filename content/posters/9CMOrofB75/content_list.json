[{"type": "text", "text": "Evaluating the design space of diffusion-based generative models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuqing Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ye He ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Simons Institute University of California, Berkeley yq.wang@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "School of Mathematics Georgia Institute of Technology yhe367@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Molei Tao School of Mathematics Georgia Institute of Technology mtao@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in Karras et al. [30]. It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in Song et al. [46] is more preferable, but when it is less trained, the design in Karras et al. [30] becomes more preferable. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Difusion models became a very popular generative modeling approach in various domains, including computer vision [20, 7, 27, 28, 38, 51], natural language processing [6, 34, 37], various modeling tasks [15, 41, 55], and medical, biological, chemical and physical applications [3, 17, 43, 49, 23, 56] (see more surveys in [53, 11, 14]). Karras et al. [30] provided a unified empirical understanding of the derivations of model parameters, leading to new state-of-the-art performance. Karras et al. [31] further upgraded the model design by revamping the network architectures and replacing the weights of the network with an exponential moving average. As diffusion models gain wider usage, efforts to understand and enhance their generation capability become increasingly meaningful. ", "page_idx": 0}, {"type": "text", "text": "In fact, a rapidly increasing number of theoretical works already analyzed various aspects of diffusion models [32, 19,52,16, 12,8, 18,9, 13, 39,44, 25]. Among them, a majority [32, 19,52, 16, 12,8, 18] focus on sampling/inference; more precisely, they assume the score error is within a certain accuracy threshold (i.e. the score function is well trained in some sense), and analyze the discrepancy between the distribution of the generated samples and the true one. Meanwhile, there are a handful of results [44, 25] that aim at understanding different facets of the training process. See more detailed discussions of existing theoretical works in Section 1.1. ", "page_idx": 0}, {"type": "text", "text": "However, as indicated in Karras et al. [30], the performance of diffusion models also relies on the interaction between design components in both training and sampling, such as the noise distribution, weighting, time and variance schedules, etc. While focusing individually on either the training or generation process provides valuable insights, a holistic quantification of the actual generation capability can only be obtained when both processes are considered altogether. Therefore, motivated by obtaining deeper theoretical understanding of how to maximize the performance of diffusion models, this paper aims at establishing a full generation error analysis, combining both the optimization and sampling processes, to partially investigate the design space of diffusion models. ", "page_idx": 1}, {"type": "text", "text": "More precisely, we focus on the variance exploding setting [46], which is also the foundation of continuous forward dynamics in Karras et al. [30]. Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 For denoising score matching objective, we establish the exponential convergence of its gradient descent training dynamics (Theorem 1). We develop a new method for proving a key lower bound of gradient under the semi-smoothness framework [1, 35, 57, 58]. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We extend the sampling error analysis in [8] to the variance exploding case (Theorem 2), under only the finite second moment assumption (Assumption 3) of the data distribution. Our result applies to various variance and time schedules, and implies a sharp almost linear complexity in terms of data dimension under optimal time schedule.   \n\u00b7 We conduct a full error analysis of diffusion models, combining training and sampling (Theorem 3).   \n\u00b7 We qualitatively derive the theory for choosing the noise distribution and weighting in the training objective, which coincides with Karras et al. [30] (Section 4.1). More precisely, our theory implies that the optimal rate is obtained when the total weighting exhibits a similar \u201cbell-shaped\" pattern used in Karras et al. [30].   \n\u00b7 We develop a theory of choosing time and variance schedules based on both training and sampling (Section 4.2). Indeed, when the score error dominates, i.e., the neural network is less trained and not very close to the true score, polynomial schedule [30] ensures smaller error; when sampling error dominates, i.e., the score function is well approximated, exponential schedule [46] is preferred. ", "page_idx": 1}, {"type": "text", "text": "Conclusions and limitations are in Appendix A ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sampling. There has been significant progress in quantifying the sampling error of the generation process of diffusion models, assuming the score function is already approximated within certain accuracy. Most existing works [e.g.. 16, 12, 8] focused on the variance preserving ", "page_idx": 1}, {"type": "image", "img_path": "9CMOrofB75/tmp/6baad392b25e144468ae7e0310166457cf7ac822733770a6494a3d943bbbc10f.jpg", "img_caption": ["Figure 1: Structure of this paper. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "(VP) SDEs, whose discretizations correspond to DDPM. For example, Benton et al. [8] is one of the latest results for the VPsDE-based diffusion models, and it only needs a very mild assumption: the data distribution has finite second moment. The iteration complexity is shown to be almost linear in the data dimension and polynomial in the inverse accuracy, under exponential time schedule. However, a limited amount of works [32, 24, 54] analyzed the variance exploding (VE) SDEs, whose discretizations correspond to Score Matching with Langevin dynamics (SMLD) [45, 46]. To our best knowledge, Yang et al. [54] obtained the best result so far for VE assuming the data distribution has bounded support: the iteration complexity is polynomial in the data dimension and the inverse accuracy, under the uniform time schedule. In contrast, our work only assumed that the data distribution has finite second moment, and by extending the stochastic localization approach in [8] to VESDE, we obtain an iteration complexity that is polynomial in the data dimension and the inverse accuracy, under more general time schedules as well. Note the improved complexity in terms of the inverse accuracy and the data dimension dependencies; in fact, under the exponential time schedule, our complexity is almost linear in the data dimension, which recovers the state-of-the-art result for VPSDE-based diffusion models. ", "page_idx": 1}, {"type": "text", "text": "Training. To our best knowledge, the only works that quantify the training process of the diffusion models are Shah et al. [44] and Han et al. [25]. Shah et al. [44] employed the DDPM formulation and considered data distributions as mixtures of two spherical Gaussians with various scales of separation, togetherwith $K$ spherical Gaussians with a warm start. Then the score function can be analytically solved, and they modeled it in a teacher-student framework solved by gradient descent. They also provided the sample complexity bound under these specific settings. In contrast, our results work for general data distributions for which the true score is unknown, and training analysis is combined with sampling analysis. Han et al. [25] considered the GD training of a two-layer ReLU neural network with the last layer fixed, and used the neural tangent kernel (NTK) approach to establish a first result on generalization error. They uniformly sampled the time points in the training objective, assumed that the Gram matrix of the kernel is away from O (implying a lower bound on the gradient), and lacked a detailed non-asymptotic characterization of the training process. In contrast, we use the deep ReLU networkwith $L$ layers trained by GD and prove instead of assuming that the gradient is lower bounded by the objective function. Moreover, we obtain a non-asymptotic bound for the optimization error, and our bound is valid for general time and variance schedules, which allows us to obtain a full error analysis. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Convergence of neural networks training. The convergence analysis of neural networks under gradient descent has been a longstanding challenge and has been developed into an extensive field. Here we will only focus on results mostly related to the techniques used in this paper. One line of them is approaches directly based on neural tangent kernel (NTK) [22, 21, 5, 47, 36]. However, existing works in this direction focus more on either scalar output, or vector output but with only one layer trained under two-layer networks, which is insufficient for diffusion models. Another line of research also considers overparameterized models in a regime analogous to NTK, though not necessarily explicitly resorting to kernels. Instead, it directly quantifies the lower bound of the gradient [1, 35, 2, 57, 58] and uses a semi-smoothness property to prove exponential convergence. Our results align with the latter line, but we develop a new method for proving the lower bound of the gradient and adopt assumptions that are closer to the setting of diffusion models. See more discussions in Section 3.1. ", "page_idx": 2}, {"type": "text", "text": "1.2Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We denote $\\|\\cdot\\|$ to be the $\\ell^{2}$ norm for both vectors and matrices, and $\\|\\cdot\\|_{F}$ to be the Frobenius norm. For the discrete time points, we use $t_{i}$ to denote the time point for forward dynamics and $t_{i}^{\\leftarrow}$ for backward dynamics. For the order of terms, we follow the theoretical computer science convention to use $O(\\cdot),\\Theta(\\cdot),\\Omega(\\cdot)$ . We also denote $f\\lesssim g$ if $f\\leq C g$ for some universal constant $C$ ", "page_idx": 2}, {"type": "text", "text": "2  Basics of diffusion-based generative models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we will introduce the basic forward and backward dynamics of diffusion models, as well as the denoising score matching setting under which a model is trained. ", "page_idx": 2}, {"type": "text", "text": "2.1  Forward and backward processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a forward diffusion process that pushes an initial distribution $P_{0}$ toGaussian ", "page_idx": 2}, {"type": "equation", "text": "$$\nd X_{t}=-\\stackrel{\\cdot}{f}_{t}X_{t}\\,d t+\\sqrt{2\\sigma_{t}^{2}}\\,d W_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d W_{t}$ is the Brownian motion, $X_{t}$ isa $d$ -dim. random variable, and $X_{t}\\sim P_{t}$ .Under mild assumptions, the process can be reversed and the backward process is defined as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\nd Y_{t}=\\left(f_{T-t}\\,Y_{t}+2\\sigma_{T-t}^{2}\\nabla\\log p_{T-t}(Y_{t})\\right)d t+\\sqrt{2\\sigma_{T-t}^{2}}\\,d\\tilde{W}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Y_{0}\\sim P_{T}$ , and $p_{t}$ is the density of $P_{t}$ . Then $Y_{T-t}$ and $X_{t}$ have the same distribution with density $p_{t}$ [4], which means the dynamics (2) will push (near) Gaussian distribution back to (nearly) the initial distribution $P_{0}$ . To apply the backward dynamics for generative modeling, the main challenge lies in approximating the term $\\nabla\\log p_{T-t}(Y_{t})$ which is called score function. It is common to use a neural network to approximate this score function and learn it via the forward dynamics (1); then, samples can be generated by simulating the backward dynamics (2). ", "page_idx": 2}, {"type": "text", "text": "2.2  The training of score function via denoising score matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In order to learn the score function, a natural starting point is to consider the following score matching objective [e.g.,29] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\stackrel{\\mathcal{I}_{\\mathrm{J}}}{\\mathcal{L}}_{\\mathrm{conti}}(\\theta)=\\frac12\\int_{t_{0}}^{T}w(t)\\mathbb{E}_{X_{t}\\sim P_{t}}\\Vert S(\\theta;t,X_{t})-\\nabla_{x}\\log p_{t}(X_{t})\\Vert^{2}\\,d t\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $S(\\theta;t,X_{t})$ is a $\\theta$ -parametrized neural network, $w(t)$ is some weighting function, and the subscript means this is the continuous setup. Ideally one would like to minimize this objective function to obtain $\\theta$ ; however, $p_{t}$ in general is unknown, and so is the true score function $\\nabla_{x}\\log{p_{t}}$ One of the solutions is denoising score matching proposed by Vincent [48], where one, instead of directly matching the true score, leverages conditional score for which initial condition is fixed so that $p_{t|0}$ is analytically known. ", "page_idx": 2}, {"type": "text", "text": "More precisely, given the linearity of forward dynamics (1), its exact solution is explicitly known: Let $\\begin{array}{r}{\\bar{\\mu_{t}}\\,=\\,\\int_{0}^{t}\\dot{f_{s}}\\,\\bar{d s}}\\end{array}$ and $\\begin{array}{r}{\\bar{\\sigma}_{t}^{2}\\,=\\,2\\int_{0}^{t}e^{2\\mu_{s}-2\\mu_{t}}\\sigma_{s}^{2}\\,d s}\\end{array}$ .Then the solution is $X_{t}\\,=\\,e^{-\\mu_{t}}X_{0}+\\bar{\\sigma}_{t}\\xi$ where $\\xi\\,\\sim\\,{\\mathcal{N}}(\\,0,I)$ .  We also have $X_{t}|X_{0}\\;\\sim\\;{\\mathcal{N}}\\left(e^{-\\mu_{t}}X_{0},{\\bar{\\sigma}}_{t}^{2}I\\right)$ and $g_{t}(x|y)\\;=\\;(2\\pi\\bar{\\sigma}_{t}^{2})^{-d/2}\\exp(-\\|x-y)$ $e^{-\\mu_{t}}y\\Vert^{2}/(2\\bar{\\sigma}_{t}^{2}))$ , which is the density of $X_{t}|X_{0}$ . Then the objective can be rewritten as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{\\mathcal{L}_{\\mathrm{conti}}(\\theta)=\\frac{1}{2}\\int_{t_{0}}^{T}w(t)\\mathbb{E}_{X_{0}}\\mathbb{E}_{X_{t}|X_{0}}\\|S(\\theta;t,X_{t})-\\nabla\\log g_{t}(X_{t}|X_{0})\\|^{2}d t+\\frac{1}{2}\\int_{t_{0}}^{T}w(t)C_{t}d t}}\\\\ {\\displaystyle{=\\frac{1}{2}\\int_{t_{0}}^{T}w(t)\\frac{1}{\\bar{\\sigma}_{t}}\\mathbb{E}_{X_{0}}\\mathbb{E}_{\\xi}\\|\\bar{\\sigma}_{t}S(\\theta;t,X_{t})+\\xi\\|^{2}d t+\\frac{1}{2}\\int_{t_{0}}^{T}w(t)C_{t}d t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C_{t}=\\mathbb{E}_{X_{t}}\\|\\nabla\\log p_{t}\\|^{2}-\\mathbb{E}_{X_{0}}\\mathbb{E}_{X_{t}|X_{0}}\\|\\nabla\\log g_{t}(X_{t}|X_{0})\\|^{2}$ For completeness, we will provide a detailed derivation of these results in Appendix $\\mathbf{C}$ and emphasize that it is just a review of existing results in our notation. ", "page_idx": 3}, {"type": "text", "text": "Throughout this paper, we adopt the variance exploding (VESDE) setting [46], where $f_{t}=0$ and hence $\\mu_{t}=0$ , which also aligns with the setup in the classic of EDM [30]. ", "page_idx": 3}, {"type": "text", "text": "3  Error analysis for diffusion-based generative models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will quantify both the training and sampling processes, and then integrate them into a more comprehensive generation error analysis. ", "page_idx": 3}, {"type": "text", "text": "3.1 Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we consider a practical implementation of denoising score matching objective, represent the score by a deep ReLU network, and establish the exponential convergence of GD training dynamics. ", "page_idx": 3}, {"type": "text", "text": "Training objective function. Consider a quadrature discretization of the time integral in (4) based on deterministic collocation points $0<t_{0}<t_{1}<t_{2}<\\cdots<t_{N}=T$ Then ", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{L}_{\\mathrm{conti}}(\\theta)\\approx\\bar{\\mathcal{L}}(\\theta)+\\bar{C},}\\end{array}$ where $\\begin{array}{r}{\\bar{C}=\\frac{1}{2}\\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})C_{t_{j}}^{\\dagger}}\\end{array}$ ,and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}(\\theta)=\\frac{1}{2}\\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})\\frac{1}{\\bar{\\sigma}_{t_{j}}}\\mathbb{E}_{X_{0}}\\mathbb{E}_{\\xi}\\|\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},X_{t_{j}})+\\xi\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Define $\\begin{array}{r}{\\beta_{j}=w(t_{j})(t_{j}-t_{j-1})\\frac{1}{\\bar{\\sigma}_{t_{j}}}}\\end{array}$ $\\bar{\\mathcal{L}}\\left(6\\right).$ Denote the inital data to be $\\{x_{i}\\}_{i=1}^{n}$ with $x_{i}\\sim P_{0}$ and te noise to be $\\{\\xi_{i j}\\}_{j=1}^{N}$ with $\\xi_{i j}\\sim\\mathcal{N}(0,I_{d})$ Then the input data of the neural network is $\\{t_{j},X_{i j}\\}_{i=1,j=1}^{n,N}=\\{t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j}\\}_{i=1,j=1}^{n,N}$ and the output data is $\\{\\xi_{i j}/\\bar{\\sigma}_{t_{j}}\\}_{i=1,j=1}^{n,N}$ $\\bar{\\sigma}_{t_{j}}\\neq0$ . Consequently, $\\bar{\\mathcal{L}}(\\theta)$ (6) can be approximated by the following ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{e m}(\\theta)=\\frac{1}{2n}\\sum_{i=1}^{n}\\sum_{j=1}^{N}\\beta_{j}\\|\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j})+\\xi_{i j}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We will use (7) as the training objective function in our analysis. For simplicity, we also denote $f(\\theta;i,j)\\,=\\,\\beta_{j}\\|\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j})+\\xi_{i j}\\|^{2}$ and then $\\begin{array}{r}{\\bar{\\mathcal{L}}_{e m}\\dot{(\\theta)}\\;=\\;\\frac{1}{2n}\\sum_{i=1}^{n^{-}}\\sum_{j=1}^{N^{-}}f(\\theta;i,j)}\\end{array}$ Note the time dependence can be absorbed into the dependence. More precisely, because $\\bar{\\sigma}_{t}$ is a monotonically increasing function of $t$ , we can replace $t_{j}$ in the inputs by $\\bar{\\sigma}_{t_{j}}$ to indicate the time dependence. This is then equivalent to augmenting $X_{i j}$ to be $d+1$ dimensional with $(x_{i})_{d+1}:=0$ and $(\\xi_{i j})_{d+1}:=1$ . For simplified presentation, we will slightly abuse notation and still use $d$ as the input dimension rather than $d+1$ ", "page_idx": 3}, {"type": "text", "text": "Architecture. The analysis of diffusion model training is in general very challenging. One obvious factor is the complex score parameterizations used in practice such as U-Net [42] and transformers [40, 34]. In this paper, we simplify the architecture and consider deep feedforward networks. Although it is still far from practical usage, note this simple structure can already provide insights about the design space, as shown in later sections, and is more complicated than existing works [25, 44] related to the training of diffusion models (see Section 1.1). More precisely, we consider the standard deep ReLU network with bias absorbed: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS(\\theta;t_{j},X_{i j})=W_{L+1}\\sigma(W_{L}{\\cdots}W_{1}\\sigma(W_{0}X_{i j})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\theta=\\left(W_{0},\\cdots,W_{L+1}\\right)$ \uff0c $W_{0}\\in\\mathbb{R}^{m\\times d}$ \uff0c $W_{L+1}\\in\\mathbb{R}^{d\\times m}$ \uff0c $W_{\\ell}\\in\\mathbb{R}^{m\\times m}$ for $\\ell=1,\\cdots,L$ and $\\sigma(\\cdot)$ is the ReLU activation. ", "page_idx": 4}, {"type": "text", "text": "$\\boldsymbol{\\theta}^{(k)}=\\left(W_{0},W_{1}^{(k)},\\cdots,W_{L}^{(k)},W_{L+1}\\right)$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta^{(k+1)}=\\theta^{(k)}-h\\nabla\\bar{\\mathcal{L}}_{e m}(\\theta^{(k)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h>0$ is the learning rate. We fix $W_{0}$ and $W_{L+1}$ throughout the training process and only update $W_{1},\\cdots,W_{L}$ , which is a commonly used setting in the convergence analysis of neural networks [1, 10, 25]. ", "page_idx": 4}, {"type": "text", "text": "Initialization. Wemploy the same initialization as in Alle-Zhuet al. [1], which is to set $(W_{\\ell}^{(0)})_{i j}\\sim$ $\\textstyle N(0,{\\frac{2}{m}})$ $\\ell=0,\\cdots,L,i,j=1,\\cdots,m$ and $\\big(W_{L+1}^{(0)}\\big)_{i j}\\sim\\mathcal{N}(0,{\\textstyle\\frac{1}{d}})$ $i=1\\cdots,d,\\,j=1\\cdots,m$ ", "page_idx": 4}, {"type": "text", "text": "For this setup, the main challenge in our convergence analysis for denoising score matching lies in the nature of the data. 1) The output data that neural network tries to match is an unbounded Gaussian random vector, and cannot be rescaled as assumed in many theoretical works (for example, Allen-Zhu et al. [1] assumed the output data to be of order $o(1)$ ). 2) The input data $X_{i j}$ is the sum of two parts: $x_{i}$ which follows the initial distribution $P_{0}$ , and a Gaussian noise $\\bar{\\sigma}_{t_{j}\\xi_{i j}}$ . Therefore, any assumption on the input data needs to agree with this noisy and unbounded nature, and commonly used assumptions like data separability [1, 35] can no longer be used. ", "page_idx": 4}, {"type": "text", "text": "To deal with the above issues, we instead make the following assumptions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (On network hyperparameters and initial data of the forward dynamics). We assume thefollowingholds: ", "page_idx": 4}, {"type": "text", "text": "1. Data scaling: $\\|x_{i}\\|=\\Theta(d^{1/2})$ for all $i$   \n2. Input dimension: $d=\\Omega(p o l y(\\log(n N)))$ ", "page_idx": 4}, {"type": "text", "text": "We remark that the first assumption focuses only on the initial data $x_{i}$ instead of the whole solution of the forward dynamics $X_{i j}$ which incorporates the Gaussian noise. Also, this assumption is indeed not far away from reality; for example, it holds with high (at least $1-\\mathcal{O}(\\exp(-\\Omega(d\\bar{)}))$ probability for standard Gaussian random vectors. The requirement for input dimension $d$ is to ensure that $d$ is not too small, or equivalently the number of data points is not exponential in $d$ ", "page_idx": 4}, {"type": "text", "text": "We also make the following assumptions on the hyperparameters of the denoising score matching. Assumption 2 (On the design of diffusion models). We assume the following holds: ", "page_idx": 4}, {"type": "text", "text": "The first assumption is to guarantee that the weighting function $w(t)$ is properly scaled. This expression $w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}$ is obtained from proving the upper and lower bounds of the gradient of (7), and is different from the total weighting $\\beta_{i}$ defined above. In the second assumption, $\\bar{\\sigma}_{t_{0}}>0$ ensures the output $\\xi_{i j}/\\bar{\\sigma}_{t_{j}}$ is well-defined. The $\\bar{\\sigma}_{t_{N}}=\\Theta(1)$ guarantees that the scales of the noise $\\bar{\\sigma}_{t_{j}\\xi_{i j}}$ and the initial data $x_{i}$ are of the same order at the end of the forward process, namely, the initial data $x_{i}$ is eventually push-forwarded to near Gaussian with the proper variance. Therefore, Assumption 2 aligns with what has been used in practice (see Section 4 and Karras et al. [30], Song et al. [46] for examples). ", "page_idx": 4}, {"type": "text", "text": "The following theorem summarizes our convergence result for the training of the score function. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Convergence of GD). Define a set of indices to be ${\\mathcal G}^{(s)}\\ =\\ \\{(i,j)|f(\\theta^{(s)};i,j)\\ \\ge$ $f(\\theta^{(s)};i^{\\prime},j^{\\prime})$ for all $i^{\\prime},j^{\\prime}\\}$ :Then given Assumption $^{\\,l}$ and 2, for any $\\epsilon_{\\mathrm{{train}}}~>~0$ , there exists some $\\begin{array}{r}{M\\big(\\epsilon_{\\mathrm{train}}\\big)~=~\\Omega\\left(p o l y\\big(n,N,d,L,T/t_{0},\\log\\big(\\frac{1}{\\epsilon_{\\mathrm{train}}}\\big)\\big)\\right)}\\end{array}$ , s.t, when $m\\;\\geq\\;M(\\epsilon_{\\mathrm{train}}),$ $h\\ =$ (mmin, (t)(ts-t-1)ot ,and $\\begin{array}{r}{k~=~{\\mathcal{O}}(d^{\\frac{1-a_{0}}{2}}n^{2}N\\log(\\frac{d}{\\epsilon_{\\mathrm{train}}}))}\\end{array}$ , with probability at least $1\\,-$ $\\mathcal{O}(n N)\\exp(-\\Omega(d^{2a_{0}-1}\\bar{)})$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{e m}(\\theta^{(k)})\\leq\\prod_{s=0}^{k-1}\\left(1-C_{5}h\\ w(t_{j^{*}(s)})(t_{j^{*}(s)}-t_{j^{*}(s)-1})\\bar{\\sigma}_{t_{j^{*}(s)}}\\left(\\frac{m d^{\\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\\right)\\right)\\bar{\\mathcal{L}}_{e m}(\\theta^{(0)})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the universal constant $C_{5}>0$ $a_{0}\\in\\textstyle\\left({\\frac{1}{2}},1\\right)$ and $\\begin{array}{r}{\\bigl(i^{*}\\bigl(s\\bigr),j^{*}\\bigl(s\\bigr)\\bigr)=\\arg\\operatorname*{max}_{(i,j)\\in\\mathcal{G}^{(s)}}w\\bigl(t_{j}\\bigr)\\bigl(t_{j}-}\\end{array}$ $t_{j-1})\\bar{\\sigma}_{t_{j}}$ Moreover. when $\\begin{array}{r}{K=\\Theta(d^{\\frac{1-a_{0}}{2}}n^{2}N\\log(\\frac{d}{\\epsilon_{\\mathrm{train}}}))}\\end{array}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{e m}\\big(\\theta^{(K)}\\big)\\leq\\epsilon_{\\mathrm{train}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The above theorem implies that for denoising score matching objective $\\bar{\\mathcal{L}}_{e m}(\\theta)$ , GD has exponential convergence. For example, if we simply take $\\begin{array}{r}{j^{\\ast}=\\operatorname*{min}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}\\end{array}$ , then $\\bar{\\mathcal{L}}_{e m}(\\theta^{(k+1)})$ is further upper bounded by (1 - Csh w(t;\\* )(tj\\* - tj\\*-1)t,\\* (\"m% N2 $\\begin{array}{r l r}{\\lefteqn{\\left(1-C_{6}h\\ w\\big(t_{j^{*}}\\big)\\big(t_{j^{*}}-t_{j^{*}-1}\\big)\\bar{\\sigma}_{t_{j^{*}}}\\left(\\frac{m d^{\\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\\right)\\right)^{k+1}\\bar{\\mathcal{L}}_{e m}\\big(\\theta^{(0)}\\big)}}\\end{array}$ . The rate of convergence can be interpreted in the following way: 1) at the $k$ th iteration, we collect all the indices of the time points into $\\mathcal{G}^{(k)}$ where $f(\\theta;i,j)$ has the maximum value; 2) we then choose the maximum of $w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}$ among all such indices and denote the index to be $j^{*}(k)$ , and obtain the decay ratio bound for the next iteration as 1- Csh w(t\\*(k)(t\\*(k)- tj\\*(k)-1)t;\\*(c)(\" ", "page_idx": 5}, {"type": "text", "text": "Remark 1 (Can $\\epsilon_{\\mathrm{train}}$ be arbitrarily small? Some ramifications of the denoising setting). Let us first see some facts about $\\bar{\\mathcal{L}}_{e m}$ and $\\bar{\\mathcal{L}}$ .Under minimal assumption of the existence of score function and in the zero-time-discretization-error limit, the score matching objective can be made zero and therefore the denoising score matching objective is bounded below by $\\dot{-C}$ which is nonnegative and zero only when the data distribution is extremely special(we thus write $-\\bar{C}>0$ from hereon unless confusion arises). That is, ming L(0) \u2265 minany function $\\textsl{S}\\bar{\\mathcal{L}}=-\\bar{C}>0$ according to (4). Since $\\bar{\\mathcal{L}}_{e m}\\rightarrow\\bar{\\mathcal{L}}$ as the samplesize of the training data set $n\\rightarrow\\infty$ , we have $\\bar{\\mathcal{L}}_{e m}\\geq-\\bar{C}-c_{n}>0$ for some constant $c_{n}>0$ and $c_{n}\\to0$ as $n\\rightarrow\\infty$ ", "page_idx": 5}, {"type": "text", "text": "However, Theorem 1I seems to imply $\\bar{\\mathcal{L}}_{e m}(\\theta^{(k)})\\to0$ $k\\rightarrow\\infty$ since $\\bar{\\mathcal{L}}_{e m}\\big(\\theta^{(K)}\\big)\\leq\\epsilon_{\\mathrm{train}}$ and $\\epsilon_{\\mathrm{train}}$ is arbitrary,and it seemstocontradict the $-\\bar{C}>0$ lowerbound.However, there isnocontradiction due to the combination of two facts. First, the theorem states that for arbitrary $\\epsilon_{\\mathrm{train}}>0,$ there exists a critical size, such that for overparameterized network beyond this size, GD can render the loss $\\bar{\\mathcal{L}}_{e m}(\\theta)$ eventually no greater than $\\epsilon_{\\mathrm{train}}$ . If we fix the network size, i.e., with $m,L,d$ given, then $K$ is given, and Theorem $^{\\,l}$ says nothing about $G D$ 's behavior after $K$ iterations. That is, we do not know whether lim $\\operatorname*{sup}_{k\\rightarrow\\infty}\\bar{\\mathcal{L}}_{e m}\\big(\\theta^{(k)}\\big)=0$ Second, our optimization setting requires the sample size n to be smaller than the network width $m$ (Assumption $^{\\,I}$ ). Thus, when m is fixed, the sample size $n$ is upper bounded. ", "page_idx": 5}, {"type": "text", "text": "The above discussion implies, within the validity of our theory, for any fixed network width m, $i f$ $\\epsilon_{\\mathrm{train}}$ is small,thesample sizen cannot betoo large,meaning $\\dot{\\mathcal{L}}_{e m}(\\dot{\\theta})-\\bar{\\mathcal{L}}(\\theta)$ may not be small. Therefore, we can simultaneously have $\\bar{\\mathcal{L}}_{e m}(\\theta)$ closeto $\\boldsymbol{O}$ and $\\bar{\\mathcal{L}}(\\theta)$ closeto $-\\bar{C}>0$ ", "page_idx": 5}, {"type": "text", "text": "Main technical steps for proving Theorem 1. The proof of Theorem 1 is in Appendix D, where the analysis framework is adapted from Allen-Zhu et al. [1]. Roughly speaking, the key proof in this framework is to establish the lower bound of the gradient. Then by integrating it into the semi-smoothness property of the neural network, we can obtain the exponential rate of convergence of gradient descent. For the lower bound of gradient, we develop a new method to deal with the difficulties in the denoising score matching setting (see the discussions earlier in this section). ", "page_idx": 5}, {"type": "text", "text": "Our new proof technique adopts a different decoupling of the gradient and leverages a high probability bound based on a high-dimensional geometric idea. See Appendix D.1 for a proof sketch and more details. ", "page_idx": 5}, {"type": "text", "text": "3.2  Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we establish a nonasymptotic error bound of the backward process in the variance exploding setting, which is an extension to Benton et al. [8]. For simplified notations, denote the backward time schedule as $\\{t_{j}^{\\leftarrow}\\}_{0\\leq j\\leq N}$ suchthat $0=t_{0}^{\\leftarrow}<t_{1}^{\\leftarrow}<\\dots<t_{N}^{\\leftarrow}=T-\\delta$ ", "page_idx": 5}, {"type": "text", "text": "Generation algorithm. We consider the exponential integrator scheme for simulating the backward SDE(2) with $\\breve{f}_{t}\\equiv0^{2}$ . The generation algorithm can be piecewisely expressed as a continuous-time SDE: for any t e [t ,ti+1), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\stackrel{\\triangledown}{d\\bar{Y}_{t}}=2\\sigma_{T-t}^{2}S\\big(\\theta;T-t_{j}^{\\leftarrow},\\bar{Y}_{t_{j}^{\\leftarrow}}\\big)d t+\\sqrt{2\\sigma_{T-t}^{2}}d\\bar{W}_{t}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Initialization. Denote $q_{t}:=\\mathrm{Law}(\\bar{Y}_{t})$ for all $t\\in[0,T-\\delta]$ .We choose the Gaussian initialization, $q_{0}=\\mathcal{N}(0,\\bar{\\sigma}_{T}^{2})$   \nOur convergence result relies on the following assumption.   \nAssumption 3. The distribution $P_{0}$ has a finite second moment: $\\mathbb{E}_{x\\sim P_{0}}[\\|x\\|^{2}]=\\mathrm{m}_{2}^{2}<\\infty$   \nNext we state the main convergence result, whose proof is provided in Appendix E. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta L(p_{\\delta}|q_{T-\\delta})\\lesssim\\displaystyle\\frac{\\mathfrak{m}_{2}^{2}}{\\underline{{\\sigma_{T}^{2}}}}+\\sum_{j=0}^{N-1}\\gamma_{j}\\sigma_{T-t_{j}^{-}}^{2}\\mathbb{E}\\boldsymbol{\\gamma}_{t_{j}^{+}\\sim p_{T-t_{j}^{-}}}[\\left\\|S(\\theta;T-t_{j}^{+},Y_{t_{j}^{-}})-\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})\\right\\|^{2}]}\\\\ &{+d\\displaystyle\\sum_{j=0}^{N-1}\\gamma_{j}\\int_{t_{j}^{+}}^{t_{j+}^{-}}\\displaystyle\\frac{\\sigma_{T-t}^{4}}{\\bar{\\sigma}_{T-t}^{4}}d t+\\operatorname{m}_{2}^{2}\\frac{\\int_{0}^{t_{1}^{-}}\\sigma_{T-t}^{2}d t}{\\bar{\\sigma}_{T}^{4}}+\\left(\\operatorname{m}_{2}^{2}+d\\right)\\displaystyle\\sum_{j=1}^{N-1}\\left(1-e^{-\\bar{\\sigma}_{T-t_{j}^{-}}^{2}}\\right)\\displaystyle\\frac{\\bar{\\sigma}_{T-t_{j}^{+}}^{4}-\\bar{\\sigma}_{T-t_{j+1}^{-}}^{2}\\bar{\\sigma}_{T-t_{j-1}^{-}}^{2}}{\\bar{\\sigma}_{T-t_{j-1}^{-}}^{2}\\bar{\\sigma}_{T-t_{j}^{-}}^{4}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where $\\gamma_{j}:=t_{j+1}^{\\leftarrow}-t_{j}^{\\leftarrow}$ for all $j=0,1,\\cdots,N-1$ is the stepsize of the generation algorithm in (10). Theorem 2 is a VESDE-based diffusion model's analogy of what's proved in Benton et al. [8] for VPSDE-based diffusion model, only requiring the data distributions to have finite second moments, and it achieves the sharp almost linear data dimension dependence under the exponential time schedule. The major differences from [8] are (1) the initialization error in the VESDE case is handled differently (see Lemma 10); (2) Theorem 2 applies to varies choices of time schedules, which enables to investigate the design space of the diffusion model, as we will discuss in Section 4. Worth mentioning is, Yang et al. [54] also obtained polynomial complexity results for VESDE-based diffusion models with uniform stepsize, but under stronger data assumption (assuming compact support). Compared to their result, complexity implied by Theorem 2 has better accuracy and data dimension dependencies. A detailed discussion on complexities is given in Appendix I.1. ", "page_idx": 6}, {"type": "text", "text": "Terms $E_{I},E_{D},E_{S}$ in (11) represent the three types of errors: initialization error, discretization error, and score estimation error, respectively. Term $E_{I}$ quantifies the error between the initial density of the sampling algorithm $q_{0}$ and the ideal initialization $p_{T}$ , which is the density when the forward process stops at time $T$ . Term $E_{D}$ is the error stemming from the discretization of the backward dynamics. Term $E_{S}$ characterizes the error of the estimated score function and the true score, and is related to of the empirical version Lem (7). Besides this, the weighting jor-t th hoptimizaton eroron $\\bar{\\mathcal{L}}_{e m}$   \nthe total weighting in $\\bar{\\mathcal{L}}_{e m}$ (7) $\\beta_{j}$ , depending on choices of $w(t_{j})$ and time and variance schedules (see more discussion in Section 4). We will later on integrate the optimization error (Theorem 1) into this score error $E_{S}$ to obtain a full error analysis in Section 3.3. ", "page_idx": 6}, {"type": "text", "text": "Remark 2 (sharpness of dependence in $d$ and $\\mathrm{m}_{2}^{2}$ ).In one of the simplest cases, when the data distribution is Gaussian, the score function is explicitlyknown.Hence $K L(p_{\\delta}|q_{T-\\delta})$ can be explicitly computed as well, which verifies that the dependence of parameters $d$ and $\\mathrm{in}_{2}^{2}$ is sharp in $E_{I}$ and $E_{D}$ ", "page_idx": 6}, {"type": "text", "text": "3.3  Full error analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we combine the analyses from the previous two sections to obtain an end-to-end generation error bound. ", "page_idx": 6}, {"type": "text", "text": "Before providing the main result of this section, let us first clarify some terminologies. ", "page_idx": 6}, {"type": "text", "text": "Time schedule, variance schedule, and total weighting. The terms time schedule and variance schedule respectively refer to the choice of $t_{j}^{\\leftarrow}$ and $\\bar{\\sigma}_{t_{j}}$ in sampling. Meanwhile, note both the training and sampling processes require the proper choices of time and variance, and these choices are not necessarily the same for both processes. For training, the effect of these two is integrated into the totalweighting $\\beta_{j}$ , which is also influenced by an additional weighting parameter $\\bar{w}(t_{j})$ . In this theoretical paper, when studying the generation error, we aim to apply the optimization result to better understand the effect of optimization on sampling. Therefore, to simplify the analysis and discussions in Section 4, we choose the same time and variance schedules for both training and sampling. ", "page_idx": 6}, {"type": "text", "text": "The main result is stated in the following. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Under the same conditions as Theorem 1,2, and that $K$ is such that GD reaches $\\epsilon_{t r a i n}$ in atmostKthiterations,wehave ", "page_idx": 6}, {"type": "equation", "text": "$$\nK L(p_{\\delta}|q_{T-\\delta})\\lesssim E_{I}+E_{D}+\\operatorname*{max}_{1\\leq j\\leq N}\\frac{\\sigma_{t_{N-j}}^{2}}{w\\big(t_{N-j}\\big)}\\left(\\epsilon_{\\mathrm{train}}+\\epsilon_{n}+\\epsilon_{\\mathrm{est}}+\\epsilon_{\\mathrm{approx}}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $E_{I},E_{D}$ are defined in Theorem 2, $\\epsilon_{\\mathrm{train}}$ is defined in Theorem $^{\\,l}$ \uff0c $\\epsilon_{n}=\\big|\\bar{\\mathcal{L}}\\big(\\theta^{(K)}\\big)-\\bar{\\mathcal{L}}_{e m}\\big(\\theta^{(K)}\\big)+$ $\\bar{\\mathcal{L}}_{e m}(\\theta^{*})-\\bar{\\mathcal{L}}(\\theta^{*})$ $\\dot{\\epsilon_{\\mathrm{est}}}=|\\mathcal{\\bar{L}}(\\theta^{*})-\\mathcal{\\bar{L}}(\\theta_{\\mathcal{F}})|,\\,\\epsilon_{\\mathrm{approx}}=|\\mathcal{\\bar{L}}(\\theta_{\\mathcal{F}})+\\mathcal{\\bar{C}}|$ In these terms, $\\bar{C}$ is defined in (5), $\\theta^{*}\\,=\\,\\arg\\operatorname*{min}_{\\theta,s.t.,\\bar{\\mathcal{L}}_{e m}(\\theta)=0}\\bar{\\mathcal{L}}(\\theta)$ and $\\theta_{\\mathcal{F}}=\\arg\\operatorname*{inf}_{\\left\\{\\theta:S(\\theta)\\in\\mathcal{F}\\right\\}}\\left|\\bar{\\mathcal{L}}(\\theta)+\\bar{C}\\right|$ With $\\mathcal{F}=\\left\\{R e L U\\right.$ network function defined in (8), with $d=\\Omega(\\operatorname{poly}(\\log(n N))),m=\\Omega\\left(p o l y\\big(n,N,d,L,T/t_{0}\\big)\\right)\\big\\}.$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In this theorem, the discretization error $E_{D}$ and initialization error $E_{I}$ are the same as Theorem 2. For the score error $E_{S}$ , our optimization result is valid for general time schedules and therefore can directly fit into the sampling error analysis, which is in contrast to existing works [25, 44] (see more discussions in Section 1.1). The coefficient $\\operatorname*{max}_{j}\\sigma_{t_{N-j}}^{2}/w(t_{N-j})$ results from diffent weightings in $E_{S}$ and $\\bar{\\mathcal{L}}_{e m}$ ,ie., $\\gamma_{j}\\sigma_{T-t_{j}^{\\leftarrow}}^{2}$ and $\\beta_{j}$ . We il discus the effet of $\\operatorname*{max}_{j}\\sigma_{t_{N-j}}^{2}/w(t_{N-j})$ under different time and variance schedules in Section 4. ", "page_idx": 7}, {"type": "text", "text": "The way we bound $\\mathbb{E}_{Y_{t_{j}^{\\leftarrow}}\\sim p_{T-t_{j}^{\\leftarrow}}}[\\|S(\\theta;T-t_{j}^{\\leftarrow},Y_{t_{j}^{\\leftarrow}})-\\nabla\\log p_{T-t_{j}^{\\leftarrow}}(Y_{t_{j}^{\\leftarrow}})\\|^{2}]$ .n. $E_{S}$ (se Theorem 2) is to decompose it into the optimization error $\\epsilon_{\\mathrm{train}}$ , statistical error $\\epsilon_{n}$ , estimation error $\\epsilon_{\\mathrm{{est}}}$ . and approximation error $\\epsilon_{\\mathrm{approx}}$ .This gives clear intuition to results, but we also note it may not give a tight bound. In fact, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{n}+\\epsilon_{\\mathrm{train}}=\\big|\\bar{\\mathcal{L}}(\\theta^{(K)})-\\bar{\\mathcal{L}}_{e m}(\\theta^{(K)})+\\bar{\\mathcal{L}}_{e m}(\\theta^{*})-\\bar{\\mathcal{L}}(\\theta^{*})\\big|+\\big|\\bar{\\mathcal{L}}_{e m}(\\theta^{(K)})-\\bar{\\mathcal{L}}_{e m}(\\theta^{*})\\big|}\\\\ &{\\qquad\\qquad\\geq\\bar{\\mathcal{L}}(\\theta^{(K)})+\\bar{\\mathcal{L}}(\\theta^{*})\\geq2\\operatorname*{min}_{\\theta}\\bar{\\mathcal{L}}(\\theta)\\geq-2\\bar{C}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$\\epsilon_{n}$ can still be small if we take $n\\rightarrow\\infty$ ,but that means $\\epsilon_{\\mathrm{train}}$ has to be large, and our generation error bound cannot be made O. It is unclear yet whether this is due to limitation of our analysis or intrinsic, and will be left for future investigation. ", "page_idx": 7}, {"type": "text", "text": "Another related note is, in this paper, we focus on $\\epsilon_{\\mathrm{{train}}}$ and the effect of optimization, but the analyses of $\\epsilon_{n}$ $\\epsilon_{\\mathrm{{est}}}$ :and $\\epsilon_{\\mathrm{approx}}$ are also important and possible [13, 39, 25, 50]. On the other hand, again, whether it is optimal to decompose the full error into these four is unclear. ", "page_idx": 7}, {"type": "text", "text": "To better see the parameter dependence of the error bound in Theorem 3, the following is an example with simplified results, where we employ the schedules in EDM [30]. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1 (Full error analysis under EDM [30] designs). Under the same conditions as Theorem $3$ wehave ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{\\partial}\\Bar{\\mathrm{V}}L(p_{\\delta}|q_{T-\\delta})\\lesssim\\frac{\\mathrm{m_{2}^{2}}}{T^{2}}+\\frac{d a^{2}T^{\\frac{1}{a}}}{\\delta^{\\frac{1}{a}}N}+\\left(\\mathrm{m_{2}^{2}}+d\\right)\\left(\\frac{a^{2}T^{\\frac{1}{a}}}{\\delta^{\\frac{1}{a}}N}+\\frac{a^{3}T^{\\frac{2}{a}}}{\\delta^{\\frac{2}{a}}N^{2}}\\right)+\\frac{1}{N}\\left(C_{9}+\\left(1-C_{8}h\\left(\\frac{m d^{\\frac{\\alpha_{0}-1}{2}}}{n^{3}N^{2}}\\right)\\right)^{K}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $C_{8},C_{9}>0$ and $a=7$ in $[30]$ ", "page_idx": 7}, {"type": "text", "text": "4  Theory-based understanding of the design space and its relation to existing empirical counterparts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section theoretically explores preferable choices of parameters in both training and sampling, and shows that they agree with the ones used in EDM [30] and Song et al. [46] in different circumstances. ", "page_idx": 7}, {"type": "text", "text": "4.1  Choice of total weighting for training ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section develops the optimal total weighting $\\beta_{j}$ for training objective (7). We qualitatively show in two steps that \u201cbell-shaped\" weighting, which is the one used in EDM [30], will lead to the optimal rate of convergence: Step 1) $\\lVert\\bar{\\sigma}_{t_{j}}\\bar{S}(\\theta;\\bar{t}_{j},X_{i j})+\\xi_{i j}\\rVert$ as a function of $j$ is inversely \u201cbell-shaped\"; Step 2) $f(\\theta;i,j)=\\beta_{j}\\|\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j}^{\\bar{\\prime}},X_{i j})+\\xi_{i j}\\|$ should be close to each other for any $i,j$ ", "page_idx": 7}, {"type": "text", "text": "4.1.1  Inversely \"bell-shaped\" loss $\\lVert\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},X_{i j})+\\xi_{i j}\\rVert$ as a function of time index $j$ Proposition 1. Under the same assumptions as Theorem $^{\\,l}$ forany $\\theta$ and $i=1,\\cdots,n_{}$ wehave ", "page_idx": 7}, {"type": "text", "text": "1. $\\forall\\epsilon_{1}>0_{:}$ $\\exists\\;\\delta_{1}>0,$ \\$.t, when $0\\leq\\bar{\\sigma}_{t_{j}}<\\delta_{1}$ \uff0c $\\begin{array}{r}{\\|\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j})+\\xi_{i j}\\|>\\|\\xi_{i j}\\|-\\epsilon_{1}.}\\end{array}$  \n2. $\\forall\\epsilon_{2}>0;$ 3 $M>0$ \\$.t, when $\\bar{\\sigma}_{t_{j}}>M$ $\\emph{M},\\;\\|\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j})+\\xi_{i j}\\|\\geq M^{2}\\big(\\|S(\\theta;t_{j},\\xi_{i j})\\|-\\epsilon_{2}\\big).$", "page_idx": 7}, {"type": "text", "text": "The above proposition can be interpreted in the following way. Given any network $S$ , when $\\bar{\\sigma}_{t_{j}}$ is very small, 1 implies that $\\lVert\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi)+\\xi_{i j}\\rVert$ is away from O by approximately $\\|\\xi_{i j}\\|$ which is of order $\\sqrt{d}$ with high probability, i.e., it cannot be small. When $\\bar{\\sigma}_{t_{j}}$ is large, 2 shows that as it becomes larger and larger, i.e., as $M$ increases, $\\lVert\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi)+\\bar{\\xi}_{i j}\\rVert$ will also increase. Therefore, the function $\\lVert\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},X_{i j})+\\xi_{i j}\\rVert$ has most likely an inversely \u201cbell-shaped\u2019 curve in terms of $j$ dependence. ", "page_idx": 7}, {"type": "text", "text": "4.1.2  Ensuring comparable values of $f(\\theta;i,j)$ for optimal rate of convergence ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Corollary 2. Under the same conditions as Theorem $^{\\,I}$ for some large $K^{\\prime}>0$ $\\because|f(\\theta^{(k+K^{\\prime})};i,j)-$ $f(\\theta^{(k+K^{\\prime})};l,s)|\\leq\\epsilon$ holds for all $k>0$ and all $(i,j),(l,s)$ with some small universal constant $\\epsilon>0$ then we have, for some constant $C_{7}>0$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{e m}\\big(\\theta^{(k+K^{\\prime})}\\big)\\leq\\left(1-C\\tau h\\operatorname*{max}_{j=1,\\cdots,N}w\\big(t_{j}\\big)(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\left(\\frac{m d^{\\frac{\\alpha_{0}-1}{2}}}{n^{3}N^{2}}\\right)\\right)^{k}\\bar{\\mathcal{L}}_{e m}(\\theta^{(K^{\\prime})}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The above corollary shows that if $f(\\theta^{(k)};i,j)$ 's are almost the same for any $i,j$ , then the decay ratio of the next iteration is minimized. More precisely, the index set $\\mathcal{G}^{(k)}$ defined in Theorem 1 is roughly the whole set $\\{1,\\cdots,N\\}$ , and therefore $w(t_{j^{\\ast}})(t_{j^{\\ast}}-t_{j^{\\ast}-1})\\bar{\\sigma}_{t_{j^{\\ast}}}$ can be taken as the maximum value over all $j$ , which consequently leads to the optimal rate. ", "page_idx": 8}, {"type": "text", "text": "4.1.3 \u201cBell-shaped\" weighting: our theory and EDM ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Combining the above two aspects, the optimal rate of convergence leads to the choice of total weighting $\\beta_{j}$ such that $\\begin{array}{r l}{f(\\theta;i,j)}&{{}=}\\end{array}$ $\\beta_{j}\\,\\lVert\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},X_{t_{j}})+\\xi_{i j}\\,\\rVert$ is close to each other; as a result, the total weighting should be chosen as a \u201cbell-shaped\" curve as a function of $j$ according to the shape of the curve for $\\lVert\\bar{\\sigma}_{t_{j}}\\bar{S}(\\theta;t_{j},X_{t_{j}})+\\xi_{i j}\\rVert$ ", "page_idx": 8}, {"type": "image", "img_path": "9CMOrofB75/tmp/e5f1e5324c51a637044b3f450c5e9a2f5e0fad88862132f38665c11a1b01728b.jpg", "img_caption": ["Figure 2: Weighting choice $\\beta_{\\mathrm{EDM}}$ in EDM. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Before comparing the preferable weighting predicted by our theory and the intuition-and-empiricsbased one in EDM [30], let us first recall that the EDM training objective? can be written as $\\mathbb{E}_{\\bar{\\sigma}\\sim p_{\\mathrm{train}}}\\mathbb{E}_{y,n}\\lambda(\\bar{\\sigma})\\|D_{\\theta}(y+n;\\bar{\\sigma})-y\\|^{2}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n=\\frac{1}{Z_{1}}\\int e^{-\\frac{(\\log\\bar{\\sigma}-P_{\\mathrm{mean}})^{2}}{2P_{\\mathrm{std}}^{2}}}\\frac{\\bar{\\sigma}^{2}+\\sigma_{\\mathrm{data}}^{2}}{\\bar{\\sigma}\\sigma_{\\mathrm{data}}^{2}}\\mathbb{E}_{X_{0},\\xi}\\left\\|\\bar{\\sigma}s\\left(\\theta;t,X_{t}\\right)+\\xi\\right\\|^{2}d\\bar{\\sigma},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $Z_{1}$ is a normalization constant, and we denote $\\beta_{\\mathrm{EDM}}\\big(\\bar{\\sigma}\\big)=e^{-\\frac{(\\log\\bar{\\sigma}-P_{\\mathrm{mean}})^{2}}{2P_{\\mathrm{std}}^{2}}}\\frac{\\bar{\\sigma}^{2}+\\sigma_{\\mathrm{data}}^{2}}{\\bar{\\sigma}\\sigma_{\\mathrm{data}}^{2}}$ o\"+a'ata to be the total weighting of EDM. Note the dependence on $\\bar{\\sigma}$ and time $j$ can be freely switched due to their 1-to-1 correspondence. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 plots the total weighting of EDM $\\beta_{\\mathrm{EDM}}$ as a function of $\\bar{\\sigma}$ . As is shown in the picture, this is a \u201c\"bell-shaped\"' curve4, which coincides with our choice of total weighting in the above theory. When $\\bar{\\sigma}$ is very small or very large, according to Proposition 1, the lower bound of $\\lVert\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},\\bar{X_{t_{j}}})+\\xi_{i j}\\rVert$ cannot vanish and therefore needs the smallest weighting over all $\\bar{\\sigma}$ .When $\\bar{\\sigma}$ takes the middle value, the scale of the output data $\\xi_{i j}/\\bar{\\sigma}_{j}$ is roughly the same as the input data $X_{i j}$ and therefore makes it easier for the neural network to ft the data, which admits larger weighting. ", "page_idx": 8}, {"type": "text", "text": "4.2  Choice of time and variance schedules ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section will discuss the choice of time and variance schedules based on the three errors $E_{S},E_{D},E_{I}$ in the error analysis of Section 3.3. Two situations will be considered based on how well the score function is approximated in training: when the network is less trained, $E_{S}$ dominates and polynomial schedule [30] is preferable; when the score function is well approximated, $E_{D}+E_{I}$ dominates and exponential schedule [46] is better. ", "page_idx": 8}, {"type": "text", "text": "4.2.1 When score error $E_{S}$ dominates ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As is shown in Theorem 3, the main impact of different time and variance schedules on score error Es appears in the term maxj Otn-j/ $\\operatorname*{max}_{j}\\sigma_{t_{N-j}}^{2}/w(\\mathring{t}_{N-j})$ when the score function is approximated to a certain accuracy. It remains to compute $w(t)$ under various choices of schedules. ", "page_idx": 8}, {"type": "text", "text": "General rule of constructing $w(t)$ . To ensure fair comparisons between different time and variance schedules, we maintain a fixed total weighting in the training objective. Additionally, to facilitate comparisons with practical usage, we adopt the total weighting in EDM, i.e., $\\beta_{j}=\\dot{C}_{3}\\beta_{\\mathrm{EDM}}\\big(\\bar{\\sigma}_{t_{j}}\\big)$ for some universal constant $C_{3}>0$ . The reason for using the EDM total weighting is that according to Section 4.1, our total weighting $\\beta_{j}$ should be \u201cbell-shaped\"\u2019 as a function of $j$ , which agrees qualitatively with the one used in EDM. ", "page_idx": 8}, {"type": "text", "text": "Polynomial schedule [30] vs exponential schedule [46]. We fix $\\epsilon_{n},\\epsilon_{\\mathrm{train}}$ and apply the two schedules (Table 1\uff09 separately to the above total weighting $\\beta$ (hence $w$ ). Then, compute maxj $\\sigma_{t_{N-j}}^{2}/w(t_{N-j})$ which is a factor in score error $E_{S}$ (Thm.3) in Table 2. The Exp.'s result $\\begin{array}{r}{\\frac{1}{2}\\left(\\bar{\\sigma}_{\\mathrm{max}}-\\bar{\\sigma}_{\\mathrm{max}}\\left(\\frac{\\bar{\\sigma}_{\\mathrm{min}}^{2}}{\\bar{\\sigma}_{\\mathrm{max}}^{2}}\\right)^{1/N}\\right)}\\end{array}$ $\\begin{array}{r}{\\left(\\bar{\\sigma}_{\\mathrm{max}}-\\left(\\bar{\\sigma}_{\\mathrm{max}}^{1/\\rho}-\\frac{\\bar{\\sigma}_{\\mathrm{max}}^{1/\\rho}-\\bar{\\sigma}_{\\mathrm{min}}^{1/\\rho}}{N}\\right)^{\\rho}\\right)}\\end{array}$ for large $N$ , meaning the poly. time schedule in EDM is better than the exp. schedule in [46]. Note these two terms are both of order $1/N$ as $N\\to\\infty$ and therefore the difference lies in their prefactors. ", "page_idx": 9}, {"type": "table", "img_path": "9CMOrofB75/tmp/d17c5c9c52029e40e9e953cdf1efcdb938c30e7d280512318c63c6bbd17522d5.jpg", "table_caption": ["Table 1: Polynomial and exponential (time) schedules. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "9CMOrofB75/tmp/7bf0bafcc93ba96cd4a559ff77c0f4188097a8aa391bd0cb915b81b0608857f8.jpg", "table_caption": ["Table 2: Comparisons between different schedules. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.2.2 When discretization error $E_{D}$ and initialization error $E_{I}$ dominate ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we compare the two different schedules in Table 1 by studying the iteration complexity of the sampling algorithm, i.e., number of time points $N$ when $E_{D}+E_{I}$ dominates. ", "page_idx": 9}, {"type": "text", "text": "General rules of comparison. We consider the case when the discretization and initialization errors are bounded by the same quantity $\\epsilon,$ i.e., $E_{I}+E_{D}\\lesssim\\varepsilon$ . Then according to Theorem 2 and Theorem 3, we compute the iteration complexity for achieving this error using the two schedules in Table 1. To make the comparison more straightforward, we adopt $T\\,=\\,t_{N}\\,\\,\\,=\\,\\Theta(\\mathrm{poly}(\\varepsilon^{-1}))$ and therefore $\\bar{\\sigma}_{\\operatorname*{max}}=\\Theta(\\varepsilon^{-1/2})$ . More details are provided in Apendix I.1. ", "page_idx": 9}, {"type": "text", "text": "Polynomial schedule [30] vs exponential schedule [46]. As is shown in the last column of Table 2, the iteration complexity under exponential schedule [46] has the poly-logarithmic dependence on the ratio between maximal and minimal variance $(\\bar{\\sigma}_{\\mathrm{max}}/\\bar{\\sigma}_{\\mathrm{min}})^{6}$ , which is better than the complexity under polynomial schedule [30], which is polynomially dependent on ${\\bar{\\sigma}_{\\operatorname*{max}}}/{{\\bar{\\sigma}_{\\operatorname*{min}}}}$ .Both complexities are derived from Theorem 2 by choosing different parameters. ", "page_idx": 9}, {"type": "text", "text": "Remark 3 (The existence of optimal $\\rho$ in the polynomial schedule [30]). For fixed $\\bar{\\sigma}_{\\mathrm{max}}$ and $\\bar{\\sigma}_{\\mathrm{min}}$ the optimal $\\rho$ that minimizes the iteration complexity is $\\begin{array}{r}{\\rho=\\frac{1}{2}\\ln\\left(\\frac{\\bar{\\sigma}_{\\mathrm{max}}}{\\bar{\\sigma}_{\\mathrm{min}}}\\right)}\\end{array}$ . In [30], it was empirically observed that with fixed iteration complexity,there is an optimal value of $\\rho$ that minimizes the FID. Our result indicates that, for fixed $\\bar{\\sigma}_{\\mathrm{max}}$ and $\\bar{\\sigma}_{\\mathrm{min}}$ ,hence the desired accuracy in $K L$ divergence beingfixed,thereis an optimalvalueof $\\rho$ thatminimizestheiterationcomplexitytoreachthefixed accuracy. Even though we consider a different metric/divergence instead of FID, our result still providesa quantitative support to the existence of optimal $\\rho$ observed in [30]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors are grateful for the partially support by NSF DMS-1847802, Cullen-Peck Scholarship, and GT-Emory Humanity.AI Award. We thank the anonymous reviewers for their helpful comments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242-252. PMLR, 2019.   \n[2]  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. Advances in neural information processing systems, 32, 2019.   \n[3]  Namrata Anand and Tudor Achim. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019, 2022. [4]  Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313-326, 1982.   \n[5] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalizationforverparametrized tw-layer neural ntworks. IIntenational Conference on Machine Learning, pages 322-332. PMLR, 2019.   \n[6] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising difusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.   \n[7] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-eficient semantic segmentation with diffusion models. In International Conference on Learning Representations, 2022. URL https : //openreview.net/forum?id $\\cdot$ SlxSY2UZQT. [8]  Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-linear convergence bounds for diffusion models via stochastic localization. In The Twelfth International Conference on Learning Representations, 2024.   \n[9]  Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. arXiv preprint arXiv:2002.00107, 2020.   \n[10] Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning converges to global optima. Advances in Neural Information Processing Systems, 32, 2019.   \n[11] Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li. A survey on generative diffusion models. IEEE Transactions on Knowledge and Data Engineering, 2024.   \n[12] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735-4763. PMLR, 2023.   \n[13] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning, pages 4672-4712. PMLR, 2023.   \n[14] Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang. An overview of diffusion models: Applications, guided generation, statistical rates and optimization. arXiv preprint arXiv:2404.07771, 2024.   \n[15] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, 2021. URL https : //openreview.net/forum?id $\\equiv$ NsMLjcFa080.   \n[16] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. ICLR, 2023.   \n[17]  Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical image analysis, 80:102479, 2022.   \n[18]  Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping: fnite fsher information is all you need. arXiv preprint arXiv:2308.12240, 2023.   \n[19]  Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. TMLR, 2022.   \n[20] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780-8794, 2021.   \n[21]  Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675-1685. PMLR, 2019.   \n[22]  Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv: 1810.02054, 2018.   \n[23] Chenru Duan, Yuanqi Du, Haojun Jia, and Heather J Kulik. Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model. Nature Computational Science, 3(12):1045-1055, 2023.   \n[24]  Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in wasserstein distances. arXiv preprint arXiv:2401.17958, 2024.   \n[25] Yinbin Han, Meisam Razaviyayn, and Renyuan Xu. Neural network-based score estimation in diffusion models: Optimization and generalization. In The Twelfth International Conference on Learning Representations, 2024. URL https : //openreview.net/forum?id $\\cdot$ h8Geq0xtd4.   \n[26] Ye He, Kevin Rojas, and Molei Tao. Zeroth-order sampling methods for non-log-concave distributions: Allviating metastability by denoising diffusion. arXiv preprint arXiv:2402.17886, 2024.   \n[27] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1-33, 2022.   \n[28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633-8646, 2022.   \n[29]  Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[30] Tero Karras, Mika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 26565-26577, 2022.   \n[31] Tero Karras, Mika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. arXiv preprint arXiv:2312.02696, 2023.   \n[32]  Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870-22882, 2022.   \n[33]  Yongjae Lee and Woo Chang Kim. Concise formulas for the surface area of the intersection of two hyperspherical caps. KAIST Technical Report, 2014.   \n[34] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328-4343, 2022.   \n[35]  Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems, 31, 2018.   \n[36] Xin Liu, Zhisong Pan, and Wei Tao. Provable convergence of nesterov's accelerated gradient method for over-parameterized neural networks. Knowledge-Based Systems, 251:109277, 2022.   \n[37]  Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023.   \n[38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. URL https ://openreview. net/forum?id=aBsCjcPu_tE.   \n[39] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In International Conference on Machine Learning, pages 26517-26582. PMLR, 2023.   \n[40]  William Peebles and Saining Xie. Scalable diffusion models with ransformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195-4205, 2023.   \n[41]  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention-MICCA1 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part Il 18, pages 234-241. Springer, 2015.   \n[43] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, lia Igashov, Weitao Du, Tom Blundell, Pietro Lio, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695, 2022.   \n[44]  Kulin Shah, Sitan Chen, and Adam Klivans. Learning mixtures of gaussians using the ddpm objective. Advances in Neural Information Processing Systems, 36:19636-19649, 2023.   \n[45]  Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[46]  Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations, 2021.   \n[47]  Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. arXiv preprint arXiv: 1906.03593, 2019.   \n[48]  Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661-1674, 2011.   \n[49] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Tripe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089-1100, 2023.   \n[50]  Andre Wibisono, Yihong Wu, and Kaylee Yingxi Yang. Optimal score estimation via empirical bayes smoothing. arXiv preprint arXiv:2402.07747, 2024.   \n[51] Junde Wu, RAO FU, Huihui Fang, Yu Zhang, Yehui Yang, Haoyi Xiong, Huiying Liu, and Yanwu Xu. Medsegdiff: Medical image segmentation with diffusion probabilistic model. In Medical Imaging with Deep Learning,2023. URL https : //openreview.net/forum?id= Jdw-cm2jG9.   \n[52]  Kaylee Yingxi Yang and Andre Wibisono. Convergence in KL and Renyi divergence of the unadjusted langevin algorithm using estimated score. NeurIPS Workshop on Score-Based Methods, 2022.   \n[53] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1-39, 2023.   \n[54] Ruofeng Yang, Zhijie Wang, Bo Jiang, and Shuai Li. The convergence of variance exploding diffusion models under the manifold hypothesis. OpenReview, 2024.   \n[55] Jongmin Yoon, Sung Ju Hwang, and Juho Lee. Adversarial purification with score-based generative models. In International Conference on Machine Learning, pages 12062-12072. PMLR, 2021.   \n[56]  Yuchen Zhu, Tianrong Chen, Evangelos A Theodorou, Xie Chen, and Molei Tao. Quantum state generation with structure-preserving diffusion model. arXiv preprint arXiv:2404.06336, 2024.   \n[57]  Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. Advances in neural information processing systems, 32, 2019.   \n[58]  Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes overparameterized deep relu networks. Machine learning, 109:467-492, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A    Conclusions and limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Conclusions. In this paper, we provide a first full error analysis incorporating both optimization and sampling processes. For the training process, we provide a first result under a deep neural network and prove the exponential convergence into a neighborhood of minima. At the same time, we extend the current analysis to the variance exploding case for sampling. Moreover, based on the full error analysis, we establish a quantitative understanding of the error bound under the two schedules. Consequently, we conclude with a qualitative illustration of the \"bell-shaped\" weighting and the choices of schedules under well-trained and less-trained cases. ", "page_idx": 14}, {"type": "text", "text": "Limitations. The network architecture we used in the model is a deep ReLU network. Although being so far the most complicated architecture for theoretical results, it is still far from what is used in practice like U-Nets and transformers. Moreover, regarding the full error analysis, we only focus on the optimization and sampling error and do not dissect the generalization error. When bridging the theoretical results with practical designs of diffusion models, our results are mostly qualitative and we only compare two existing schedules under two extreme cases, when the network is well-trained and less-trained. Thus, theoretical implications on practical designs remain to be explored. We will leave these perspectives for future exploration. ", "page_idx": 14}, {"type": "text", "text": "B Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "$X_{t}$ Solution of forward dynamics (1)   \n$Y_{t}$ Solution of backward dynamics (2)   \n$\\bar{Y_{t}}$ Solution of generation algorithm (10)   \n$\\sigma_{t}$ Diffusion coefficient of (1) and (2)   \n$\\bar{\\sigma}_{t}$ Standard deviation of $X_{t}$ (4)   \nLconti Continuous-time score-matching objective (3)   \n$\\bar{\\mathcal{L}}$ Discrete-time denoising score-matching objective (population version)   \nLem Discrete-time denoising score-matching objective (empirical version) (7)   \n$C_{t}$ Constant between score-matching and denoising score-matching loss at time $t$ (4)   \n$\\bar{C}$ Constant between score-matching and denoising score-matching loss over all discrete times (5)   \n$x_{i}$ Sample from the initial data distribution $P_{0}$ (7)   \nXij Sample from the distribution $P_{t}$ at time $t$ (7)   \n$t_{j}$ The $j$ th time point for forward process (6) The $j$ th time point for backward process (10) The first (last) time point of the forward (backward) dynamics, i.e., $t_{0}$ (11)   \n$T$ Stopping time of the forward dynamics (11)   \n$\\gamma_{j}$ Difference between backward time points, $t_{j+1}^{\\leftarrow}-t_{j}^{\\leftarrow}$ (1)   \n$p_{t}$ Density of the solution of forward dynamics at time $t$ (and backward dynamics at time $T-t)$ (11)   \n$q_{t}$ Density of the solution of the generation algorithm at time $t$ (11)   \n$w(t)$ Weighting function (3)   \n$\\beta_{j}$ Total weighting, i.e. $w(t_{j})(t_{j}-t_{j-1})/\\bar{\\sigma}_{t_{j}}$ (7)   \n$\\beta_{\\mathrm{EDM}}$ Total weighting used in EDM [30] (12)   \n$\\bar{\\sigma}_{\\mathrm{max}}\\left(\\bar{\\sigma}_{\\mathrm{min}}\\right)$ Maximum (minimum) of $\\bar{\\sigma}_{t_{j}}$ (Table 2)   \n$n$ Number of samples from the initial distribution $P_{0}$ (7)   \n$N$ Number of time steps when discretizing the forward and backward dynamics (6)   \n$d$ Dimension of input, output data, and the solutions of the dynamics (1) and (2)   \n$S$ Deep ReLU network (parameterization of score function)   \n$\\theta$ All the parameters in the network $S$ (8)   \n$W_{\\ell}$ The weight in the ith layer of the network $S$ (8)   \n\u03b8(k) The $k$ th iteration of the weights $\\theta$ through GD (9)   \nW(k) The $k$ th iteration of the weights $W_{\\ell}$ through GD (9)   \n$m$ Width of the network (8)   \n$L$ Depth of the network (8)   \n$(i^{*}(s),j^{*}(s))$ Index of the largest loss and $w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}$ at the sth iteration (Theorem 1)   \n$\\mathrm{m_{2}^{2}}$ Second moment of the initial distribution $P_{0}$ (1i)   \nE1 Initialization error (11)   \n$E_{D}$ Discretization error (11)   \n$E_{S}$ Score error (11)   \n$\\epsilon_{\\mathrm{train}}$ Optimization error (Theorem 1)   \n$\\epsilon_{n}$ Statistical error (Theorem 3)   \nEest Estimation error (Theorem 3)   \n$\\epsilon_{\\mathrm{approx}}$ Approximation error (Theorem 3)   \n$\\theta^{*}$ Minimum point of $\\bar{\\mathcal{L}}$ when $\\bar{\\mathcal{L}}_{e m}=0$ (Theorem 3)   \n$\\theta_{\\mathcal{F}}$ Optimal parameter in the function class (Theorem 3) ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C  Derivation of denoising score matching objective ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we will derive the denoising score matching objective, i.e. show the equivalence of (3) and (4). For simplicity, we denote $S_{\\theta}$ to be the neural network we use $S(\\theta;t,X_{t})$ ", "page_idx": 15}, {"type": "text", "text": "Consider ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X_{t}\\sim P_{t}}\\left\\|S\\big(\\theta;t,X_{t}\\big)-\\nabla\\log p_{t}\\right\\|^{2}=\\mathbb{E}_{X_{t}}\\left[\\|S_{\\theta}\\|^{2}-2\\left\\langle S_{\\theta},\\nabla\\log p_{t}\\right\\rangle\\right]+\\mathbb{E}_{X_{t}}\\left\\|\\nabla\\log p_{t}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $p_{t}$ is the density of $X_{t}$ ", "page_idx": 15}, {"type": "text", "text": "Since $\\begin{array}{r}{p_{t}(x)=\\int p_{0}(y)q_{t}(x|y)d y}\\end{array}$ , where $q_{t}(\\cdot)$ is the density of $X_{t}|X_{0}$ , then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{X_{t}}\\left\\langle S_{\\theta},\\nabla\\log p_{t}\\right\\rangle=\\displaystyle\\int\\,S_{\\theta}^{\\top}\\nabla\\log p_{t}\\cdot p_{t}\\,d x_{t}}\\\\ {=\\displaystyle\\int\\,S_{\\theta}^{\\top}\\nabla p_{t}\\,d x_{t}}\\\\ {=\\displaystyle\\iint\\,S_{\\theta}^{\\top}\\nabla q_{t}(x|y)p_{0}(y)\\,d x d y}\\\\ {=\\displaystyle\\iint\\,S_{\\theta}^{\\top}\\nabla\\log q_{t}(x|y)p_{0}(y)q_{t}(x|y)\\,d x d y}\\\\ {=\\mathbb{E}_{X_{0}\\sim P_{0}}\\mathbb{E}_{X_{t}|X_{0}\\sim Q_{t}}\\left\\langle S_{\\theta},\\nabla\\log q_{t}(x_{t}|x_{0})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(13)=\\mathbb{E}_{X_{0}\\sim P_{0}}\\mathbb{E}_{X_{t}|X_{0}\\sim Q_{t}}\\left[\\left\\Vert S_{\\theta}\\right\\Vert^{2}-2\\left\\langle S_{\\theta},\\nabla\\log q_{t}\\big(x_{t}|x_{0}\\big)\\right\\rangle\\right]+\\mathbb{E}_{X_{t}}\\left\\Vert\\nabla\\log p_{t}\\right\\Vert^{2}}\\\\ &{\\qquad=\\underbrace{\\mathbb{E}_{X_{0}}\\mathbb{E}_{X_{t}|X_{0}}\\left\\Vert S_{\\theta}-\\nabla\\log q_{t}\\right\\Vert^{2}}_{(\\Delta)}+C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$C=\\mathbb{E}_{X_{t}}\\Vert\\nabla\\log p_{t}\\Vert^{2}-\\mathbb{E}_{X_{0}}\\mathbb{E}_{X_{t}|X_{0}}\\Vert\\nabla\\log q_{t}(x_{t}|x_{0})\\Vert^{2}.$ ", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, $X_{t}|X_{0}\\sim\\mathcal N(e^{-\\mu_{t}}X_{0},\\bar{\\sigma}_{t}^{2}I)$ , and its density function is ", "page_idx": 15}, {"type": "equation", "text": "$$\nq_{t}(x|y)=(2\\pi\\bar{\\sigma}_{t}^{2})^{-d/2}\\exp\\left(-\\frac{\\|x-e^{-\\mu_{t}}y\\|^{2}}{2\\bar{\\sigma}_{t}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\Delta\\right)=\\mathbb{E}_{X_{0}}\\mathbb{E}_{X_{t}|X_{0}}\\left\\Vert S_{\\theta}-\\nabla\\log q_{t}\\right\\Vert^{2}}\\\\ &{\\quad=\\mathbb{E}_{X_{0}}\\mathbb{E}_{X_{t}|X_{0}}\\left\\Vert S_{\\theta}-\\nabla_{x}\\left(-\\frac{\\left\\Vert X_{t}-e^{-\\mu_{t}}X_{0}\\right\\Vert^{2}}{2\\bar{\\sigma}_{t}^{2}}\\right)\\right\\Vert^{2}}\\\\ &{\\quad=\\mathbb{E}_{X_{0}}\\mathbb{E}_{X_{t}|X_{0}}\\left\\Vert S_{\\theta}+\\frac{X_{t}-e^{-\\mu_{t}}X_{0}}{\\bar{\\sigma}_{t}^{2}}\\right\\Vert^{2}}\\\\ &{\\quad=\\mathbb{E}_{X_{0}}\\mathbb{E}_{\\epsilon_{t}}\\left\\Vert S_{\\theta}+\\frac{\\epsilon_{t}}{\\bar{\\sigma}_{t}^{2}}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\begin{array}{r}{\\xi=\\frac{\\epsilon_{t}}{\\bar{\\sigma}_{t}}\\sim\\mathcal{N}(0,I)}\\end{array}$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(\\Delta)=\\mathbb{E}_{X_{0}}\\mathbb{E}_{\\xi}\\bar{\\sigma}_{t}\\cdot\\frac{1}{\\bar{\\sigma}_{t}^{2}}\\|\\bar{\\sigma}_{t}S_{\\theta}+\\xi\\|^{2}}\\\\ {\\displaystyle=\\frac{1}{\\bar{\\sigma}_{t}}\\mathbb{E}_{X_{0}}\\mathbb{E}_{\\xi}\\|\\bar{\\sigma}_{t}S_{\\theta}+\\xi\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Proofs for training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we will prove Theorem 1. ", "page_idx": 16}, {"type": "text", "text": "Before introducing the concrete proof, we first redefine the deep fully connected feedforward network ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{i j,0}=W_{0}X_{i j},q_{i j,0}=\\sigma\\big(r_{i j,0}\\big),}\\\\ &{r_{i j,\\ell}=W_{\\ell}q_{i j,\\ell-1},q_{i j,\\ell}=\\sigma\\big(r_{i j,\\ell}\\big),\\mathrm{~for~}\\ell=1,\\cdots,L}\\\\ &{S\\big(\\theta;t_{j},X_{i j}\\big)=W_{L+1}q_{i j,L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where $W_{0}\\in\\mathbb{R}^{m\\times d},W_{L+1}\\in\\mathbb{R}^{d\\times m}$ and $W_{\\ell}\\in\\mathbb{R}^{m\\times m}$ $\\sigma$ is te ReLU activation We aso denote $q_{i j,-1}$ to be Xij. ", "page_idx": 16}, {"type": "text", "text": "We also follow the notation in Allen-Zhu et al. [1] and denote $D_{i,\\ell}\\in\\mathbb{R}^{m\\times m}$ to be a diagonal matrix and $\\left(D_{i,\\ell}\\right)_{k k}=\\mathbb{1}_{\\left(W_{\\ell}q_{i j,\\ell-1}\\right)_{k}>0}$ for $k=1,\\cdots,m$ .Then ", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{i j,\\ell}=D_{i j,\\ell}W_{\\ell}q_{i j,\\ell-1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the objective (7), the gradient w.r.t. to the $k$ th row of $W_{\\ell}$ for $\\ell=1,\\cdots,L$ is the following ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overbrace{\\nabla_{(W_{\\varepsilon})_{k}}\\mathcal{\\bar{L}}_{e m}(\\theta)}^{\\mathcal{\\bar{L}}}=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{N}w_{\\left(t_{j}\\right)}\\bigl(t_{j}-t_{j-1}\\bigr)}\\\\ &{\\qquad\\qquad\\qquad\\lbrack\\bigl(\\underbrace{(W_{L+1}D_{i j,L}W_{L}\\cdots D_{i j,\\ell}W_{\\ell+1}}_{R_{i j,\\ell+1}}\\bigr)^{\\top}\\bigl(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j}\\bigr)\\bigr]_{k}\\,q_{i j,\\ell-1}\\,\\mathbb{1}_{(W_{\\ell}q_{i j,\\ell-1})_{k}>0}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Throughout the proof, we use both $\\bar{\\mathcal{L}}_{e m}(\\theta)$ and $\\bar{\\mathcal{L}}_{e m}(W)$ to represent the same value of the loss function, where $W=(W_{1},\\cdots,W_{L})$ and we let $\\textstyle a=b={\\frac{1}{2}}$ ", "page_idx": 16}, {"type": "text", "text": "Next, we will prove Theorem 1. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . First by Lemma 4, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{e m}(W^{(0)})=\\mathcal{O}(d^{2a}\\sum_{j}w(t_{j})(t_{j}-t_{j-1})/\\bar{\\sigma}_{t_{j}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, $\\Vert\\nabla\\bar{\\mathcal{L}}_{e m}({\\boldsymbol{\\theta}})\\Vert\\leq\\sqrt{L}\\operatorname*{max}_{\\ell}\\left\\Vert\\nabla_{W_{\\ell}}\\bar{\\mathcal{L}}_{e m}({\\boldsymbol{\\theta}})\\right\\Vert$ . Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W^{(k)}-W^{(0)}\\rVert\\le\\displaystyle\\sum_{i=0}^{k-1}h\\|\\nabla\\bar{\\mathcal{L}}_{e m}(W^{(i)})\\|}\\\\ &{\\qquad\\qquad\\le\\mathcal{O}(\\sqrt{m d^{2a-1}N L\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}})h k\\operatorname*{max}_{i}\\sqrt{\\bar{\\mathcal{L}}_{e m}(W^{(i)})}}\\\\ &{\\qquad\\qquad\\le\\mathcal{O}(\\sqrt{m d^{2a-1}N L\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}\\;d^{a})h k\\sqrt{\\displaystyle\\sum_{j}w(t_{j})(t_{j}-t_{j-1})/\\bar{\\sigma}_{t_{j}}}:=\\omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\_\\mathrm{et}\\textrm{\\boldmath{h}}\\quad}&{\\Theta\\big(\\frac{n N}{m\\operatorname*{min}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}\\big)\\quad\\ \\ \\mathrm{and}\\quad\\ k\\quad\\ \\ \\ =\\quad\\mathcal{O}\\big(d^{\\frac{1-a_{0}}{2}}n^{2}N\\log(\\frac{d}{\\epsilon_{\\mathrm{train}}})\\big),}\\\\ &{\\mathrm{vhere}}&{\\quad\\epsilon_{\\mathrm{train}}\\quad\\ \\ >\\quad0\\quad\\ \\mathrm{is}\\quad\\ \\ \\mathrm{some}\\qquad\\ \\mathrm{small}\\quad\\ \\ \\mathrm{constant.}}\\\\ &{\\mathcal{O}\\big(\\log(\\frac{d}{\\epsilon_{\\mathrm{train}}})\\frac{d^{1-\\frac{a_{0}}{2}}n^{3}N^{5/2}L^{1/2}}{\\sqrt{m}}\\frac{\\sqrt{\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\sum_{k}w(t_{j})(t_{j}-t_{j-1})/\\bar{\\sigma}_{t_{j}}}}{\\operatorname*{min}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}\\big)\\quad\\mathrm{and}\\quad\\mathrm{by~\\\\Lemma~\\}8,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with probability at least $1-\\mathcal{O}(n N)\\exp(-\\Omega(d^{2a_{0}-1}))$ \uff0c ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{con}}(W^{(k+1)})}\\\\ &{\\leq\\tilde{L}_{\\mathrm{con}}(W^{(k)})-h\\|\\nabla\\tilde{L}_{\\mathrm{con}}(W^{(k)})\\|^{2}}\\\\ &{+h\\sqrt{\\tilde{L}_{\\mathrm{con}}}\\sqrt{\\displaystyle\\sum_{j}w(t_{j})(t_{j},t_{j-1})\\tilde{\\sigma}_{t_{j}}}\\mathcal{O}(\\omega^{1/3}L^{2}\\sqrt{m\\log m}d^{(2)})\\|\\nabla\\tilde{L}_{\\mathrm{con}}(W^{(k)})\\|}\\\\ &{+h^{2}\\sqrt{\\tilde{L}_{\\mathrm{con}}}\\sqrt{\\displaystyle\\sum_{j}w(t_{j})(t_{j},t_{j-1})\\tilde{\\sigma}_{t_{j}}}\\mathcal{O}(L^{2}\\sqrt{m\\log^{(2)}})\\|\\nabla\\tilde{L}_{\\mathrm{con}}(W^{(k)})\\|^{2}}\\\\ &{\\leq\\left(1-h w(t_{j-1})(t_{j},\\cdot-t_{j-1})\\tilde{\\sigma}_{t_{j}},\\cdot\\Omega\\left(\\frac{m\\log^{(2)}}{n^{3}\\mathcal{N}^{2}}\\right)\\right)\\tilde{\\mathcal{L}}_{\\mathrm{con}}(W^{(k)})}\\\\ &{+h\\frac{m\\tilde{\\mathcal{S}}^{[6}](t_{j}^{[12-4]}\\omega^{[6]})}{N^{1/6}\\omega^{[2,2]/8}\\left(\\log\\pi\\right)^{1/6}\\sqrt{L}}\\frac{\\sqrt{\\sum_{j}w(t_{j})(t_{j}-t_{j-1})\\tilde{\\sigma}_{t_{j}}}\\operatorname*{min}_{j}w(t_{j})(t_{j}-t_{j-1})\\tilde{\\sigma}_{t_{j}}}{\\nu\\left(t_{j-1}\\right)\\tilde{\\sigma}_{t_{j}}\\sum{k_{\\mathrm{t}}}w(t_{j})(t_{j}-t_{j-1})\\tilde{\\rho}_{t_{j}}\\int_{\\mathcal{c o n}}(W^{(k)})}}\\\\ &{\\leq\\left(1-h w(t_{j-1})(t_{j},\\cdot-t_{j-1})\\tilde{\\sigma}_{t_{j}},\\cdot\\Omega\\left(\\frac{m\\ell^{\\frac{2}{3}}+\\frac{1}{3}}{n^{3}\\mathcal{N}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C>0$ is some constant, $a_{0}\\in\\left(1/2,1\\right)$ ; the second inequality follows from Lemma 7 with ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla_{W_{L}}\\bar{\\mathcal{L}}_{e m}(\\theta^{(k)})\\|^{2}=\\Omega\\left(\\frac{m d^{\\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\\;w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\\bar{\\sigma}_{t_{j^{*}}}\\right)\\bar{\\mathcal{L}}_{e m}(\\theta^{(k)}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is obtained inductively; the last inequality follows from m ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Omega\\left(d^{13/2-2a_{0}/3}n^{14/3}N^{11}L^{3}(\\log m)\\left(\\frac{\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\sum_{k}w(t_{j})(t_{j}-t_{j-1})/\\bar{\\sigma}_{t_{j}}}{\\operatorname*{min}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\sqrt{\\sum_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}}\\right)^{6}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.1 Proof of lower bound of the gradient at the initialization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we will show the main part of the convergence analysis, which is the following lower bound of the gradient. ", "page_idx": 17}, {"type": "text", "text": "Lemma 1 (Lower bound). With probability $1-\\mathcal{O}(n N)\\exp(-\\Omega(d^{2a_{0}-1}))$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla\\bar{\\mathcal{L}}_{e m}(\\theta^{(0)})\\|^{2}\\geq C_{6}\\left(\\frac{m d^{\\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\\;w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\\bar{\\sigma}_{t_{j^{*}}}\\right)\\bar{\\mathcal{L}}_{e m}(\\theta^{(0)})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\left(i^{*},j^{*}\\right)=\\arg\\operatorname*{max}\\left\\|\\sqrt{\\frac{w\\left(t_{j}\\right)\\left(t_{j}-t_{j-1}\\right)}{\\bar{\\sigma}_{t_{j}}}}\\big(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j}\\big)\\right\\|,\\;\\frac{1}{2}<a_{0}<1,}\\end{array}$ universal constant. ", "page_idx": 17}, {"type": "text", "text": "Below is the proof sketch of Lemma 1. ", "page_idx": 17}, {"type": "text", "text": "Proof sketch. We first decompose the gradient of the $k$ th row of $W_{L}\\;\\;\\;\\nabla_{(W_{L})_{k}}\\bar{\\mathcal{L}}_{\\mathrm{em}}(\\theta)\\;\\;\\;=$ $\\begin{array}{r}{\\frac{1}{n}w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})(W_{L+1})^{k^{\\top}}(\\bar{\\sigma}_{t_{j^{*}}}W_{L+1}q_{i^{*}j^{*},L}+\\xi_{i^{*}j^{*}})q_{i^{*}j^{*},L-1}1_{(W_{L}q_{i^{*}j^{*},L-1})_{k}>0}}\\end{array}$ $\\begin{array}{r}{+\\underbrace{\\frac{1}{n}\\sum_{(i,j)\\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})\\bigl(W_{L+1}\\bigr)^{\\mathrm{V}_{1}}}_{\\nabla_{2}}\\bigl(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j}\\bigr)\\,q_{i j,L-1}1_{(W_{L}q_{i j,L-1})_{k}>0}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "where $(i^{*},j^{*})$ indicates the sample index with the largest loss value. ", "page_idx": 17}, {"type": "text", "text": "Thenwefirst fix $\\left(q_{i j,L-1}\\right)_{s}\\ =\\ 1$ , and prove that the index set of both $(q_{i^{*}j^{*},L})_{s}\\ >\\ 0$ and $\\begin{array}{r}{\\sum_{(i,j)\\neq(i^{*},j^{*})}w\\binom{i}{j}\\binom{\\bar{\\dotsc\\dotsc}}{j}\\bar{\\sigma}_{t_{j}}1_{(W_{L}q_{i j,L-1})_{k}>0}(q_{i j,L})_{s}>0}\\end{array}$ is order $m$ withhigh probability. ", "page_idx": 17}, {"type": "text", "text": "Next,weconditned ntheindxet wevefound,thnwecandeculeachlmnf $\\nabla_{(W_{L})_{k}}\\bar{\\mathcal{L}}_{\\mathrm{em}}$ with high probability. We prove that with high probability ", "page_idx": 18}, {"type": "equation", "text": "$$\n<\\bigl(W_{L+1}\\bar{\\sigma}_{t_{j}*}q_{i^{*}j^{*},L}+\\xi_{i^{*}j^{*}},W_{L+1}\\sum_{\\substack{(i,j)\\neq(i^{*},j^{*})}}\\alpha_{i j}q_{i j,L}+\\sum_{\\substack{(i,j)\\neq(i^{*},j^{*})}}\\bar{\\alpha}_{i j}\\xi_{i j}\\bigr)\\leq\\pi-c d^{\\frac{a_{0}-1}{2}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some constant $c>0$ and $\\textstyle{\\frac{1}{2}}\\,<\\,a_{0}\\,<\\,1$ . Based on this, we show that with probability at least $1-\\mathcal{O}(n N)\\exp(-\\Omega(d))$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left(\\nabla_{1}\\right)_{s}>0,\\left(\\nabla_{2}\\right)_{s}>0\\right)\\geq c d^{\\frac{a_{0}-1}{2}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some $c>0$ . Then we prove that with probability at least $1-\\exp(-\\Omega(m d^{\\frac{a_{0}-1}{2}}))$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\{k:(W_{L+1}^{k})^{\\top}v\\geq0,(W_{L+1}^{k})^{\\top}\\big(u+\\xi\\big)\\geq0\\}\\right|=\\Theta\\big(m d^{\\frac{a_{0}-1}{2}}\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with high probability, the event $(\\nabla_{1})_{s}>0$ and $\\left(\\nabla_{2}\\right)_{s}>0$ has probability at least of order $d^{(a_{0}-1)/2}$ where $a_{0}\\in\\left(1/2,1\\right)$ ", "page_idx": 18}, {"type": "text", "text": "Now, we deal with $(q_{i j,L-1})_{s}$ and prove that if the above results hold for $\\begin{array}{r}{\\big(q_{i j,L-1}\\big)_{s}=1}\\end{array}$ , then there exists an index set with cardinality of order $m/(n N)$ such that $(\\nabla_{1})_{s}>0$ and $\\left(\\nabla_{2}\\right)_{s}>0$ also hold in this index set. ", "page_idx": 18}, {"type": "text", "text": "In the end, combining all the steps above yields the lower bound. ", "page_idx": 18}, {"type": "text", "text": "Here is the complete proof. ", "page_idx": 18}, {"type": "text", "text": "Proof. The main idea of the proof of lower bound is to decouple the elements in the gradient and incorporate geometric view. We focus on $\\nabla_{\\boldsymbol{W}_{L}}\\Bar{\\mathcal{L}}_{e m}(\\boldsymbol{\\theta})$ ", "page_idx": 18}, {"type": "text", "text": "Step 1: Rewrite $\\nabla_{(W_{L})_{k}}\\Bar{\\mathcal{L}}_{e m}(\\theta)$ to be the $(i^{*},j^{*})$ th term $g_{1}$ plus the rest $n N-1$ terms $g_{2}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\boldsymbol{i}^{*},\\boldsymbol{j}^{*}\\right)=\\arg\\operatorname*{max}\\left\\|\\sqrt{\\frac{w(t_{j})\\left(t_{j}-t_{j-1}\\right)}{\\sigma_{t_{j}}^{-}}}\\big(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j}\\big)\\right\\|.\\mathrm{Let}}\\\\ &{\\qquad\\qquad\\qquad g_{i j,L}=w(t_{j})(t_{j}-t_{j-1})\\big(W_{L+1}\\big)^{k^{\\top}}\\big(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j}\\big)\\,q_{i j,L-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\nabla_{(W_{L})_{k}}\\bar{\\mathcal{L}}_{e m}(\\theta)}}\\\\ {{=\\displaystyle\\frac{1}{n}w(t_{j^{*}})\\bigl(t_{j^{*}}-t_{j^{*}-1}\\bigr)\\bigl(W_{L+1}\\bigr)^{k^{\\intercal}}\\bigl(\\bar{\\sigma}_{t_{j^{*}}}W_{L+1}q_{i^{*}j^{*},L}+\\xi_{i^{*}j^{*}}\\bigr)\\,q_{i^{*}j^{*},L-1}\\mathbb{1}_{(W_{L}q_{i^{*}j^{*},L-1})_{k}>0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n+\\;\\frac{1}{n}\\sum_{(i,j)\\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})(W_{L+1})^{k^{\\top}}(\\bar{\\sigma}_{t_{j}}{W}_{L+1}q_{i j,L}+\\xi_{i j})\\;q_{i j,L-1}\\mathbb{1}_{(W_{L}q_{i j,L-1})_{k}>0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Also define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{1,s}=\\frac{1}{n}w(t_{j^{\\star}})\\big(t_{j^{\\star}}-t_{j^{\\star}-1}\\big)\\bar{\\sigma}_{t_{j^{\\star}}}\\left(W_{L+1}\\right)^{k^{\\top}}W_{L+1}q_{i^{\\star}j^{\\star},L}\\left(q_{i^{\\star}j^{\\star},L-1}\\right)_{s}\\mathbb{I}_{(W_{L}q_{i^{\\star}j^{\\star},L-1})_{k}>0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n+\\;\\frac{1}{n}w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\\;(W_{L+1})^{k^{\\top}}\\xi_{i^{*}j^{*}}\\;\\big(q_{i^{*}j^{*},L-1}\\big)_{s}\\mathbb{1}_{(W_{L}q_{i^{*}j^{*},L-1})_{k}>0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{2,s}=\\frac{1}{n}\\sum_{(i,j)\\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}(W_{L+1})^{k^{\\intercal}}W_{L+1}q_{i j,L}\\left(q_{i j,L-1}\\right)_{s}\\mathbb{I}_{(W_{L}q_{i j,L-1})_{k}>0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n+\\;\\frac{1}{n}\\sum_{(i,j)\\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})\\big(W_{L+1}\\big)^{k^{\\intercal}}\\xi_{i j}\\;q_{i j,L-1}\\big(q_{i j,L-1}\\big)_{s}\\mathbb{I}_{(W_{L}q_{i j,L-1})_{k}>0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\mathcal{O}\\big(\\frac{m d^{\\frac{a_{0}-1}{2}}}{n N}\\big)$ number of rows $k$ such that $\\nabla_{11,s}\\,\\geq\\,0,\\nabla_{12,s}\\,\\geq\\,0,\\nabla_{21,s}\\,\\geq\\,0,\\nabla_{22,s}\\,\\geq\\,0$ Then we can lower bound $\\left\\Vert\\nabla_{(W_{L})_{k}}\\bar{\\mathcal{L}}_{e m}(\\theta)\\right\\Vert^{2}$ by $\\|\\nabla_{1}\\|^{2}$ , which can be eventually lower bounded by $\\bar{\\mathcal{L}}_{e m}(\\theta)$ ", "page_idx": 19}, {"type": "text", "text": "Step 2: Consider $\\big[\\nabla(W_{L})_{k}\\Bar{\\mathcal{L}}_{e m}(\\theta)\\big]_{s}$ . For $(g_{2})_{s}$ , first take $\\big(q_{i j,L-1}\\big)_{s}\\,=\\,1\\$ for all $(i,j)\\,\\ne\\,(i^{*},j^{*})$ Then we only need to consider ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{2,s}^{\\prime}=\\frac{1}{n}\\sum_{(i,j)\\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})\\big(W_{L+1}\\big)^{k^{\\intercal}}\\big(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j}\\big)\\mathbb{1}_{(W_{L}q_{i j,L-1})_{k}>0}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is independent of $s$ . For $\\nabla_{1}$ , since $q_{i^{*},j^{*},L-1}\\geq0$ which does not affect the sign of this term, we can also first take $\\begin{array}{r}{(q_{i^{*},j^{*},L-1})_{s}=1}\\end{array}$ for all $s$ ", "page_idx": 19}, {"type": "text", "text": "Step 3: We focus on $\\nabla_{11}$ and $\\nabla_{21}$ and we would like to pick the non-zero elements in this two terms. More precisely, let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{1}=\\left\\{s\\mid\\left(q_{i^{*}j^{*},L}\\right)_{s}>0,s=1,\\cdots,m\\right\\},}\\\\ &{N_{2}=\\left\\{s\\mid\\sum_{\\scriptstyle(i,j)\\neq(i^{*},j^{*})}w(t_{j})(t_{j}-t_{j-1})\\Bar{\\sigma}_{t_{j}}\\mathbb{1}_{(W_{L}q_{i j,L-1})_{k}>0}(q_{i j,L})_{s}>0,s=1,\\cdots,m\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\alpha_{i j}=w\\big(t_{j}\\big)\\big(t_{j}-t_{j-1}\\big)\\bar{\\sigma}_{t_{j}}\\mathbb{1}_{(W_{L}q_{i j,L-1})_{k}>0}\\geq0$ Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{(i,j)\\neq(i^{*},j^{*})}w(t_{j})\\big(t_{j}-t_{j-1}\\big)\\bar{\\sigma}_{t_{j}}\\mathbb{1}_{(W_{L}q_{i j,L-1})_{k}>0}\\big(q_{i j,L}\\big)_{s}}\\\\ &{=\\displaystyle\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}\\big(q_{i j,L}\\big)_{s}=\\displaystyle\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}\\sigma(W_{L}q_{i j,L-1}\\big)_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}(W_{L}q_{i j,L-1})_{s}=(W_{L})_{s}\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L-1}>0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then there must be at least one pair of $(i,j)$ S.t. $\\alpha_{i j}\\bigl({\\cal W}_{L}q_{i j,L-1}\\bigr)_{s}=\\alpha_{i j}\\sigma\\bigl({\\cal W}_{L}q_{i j,L-1}\\bigr)_{s}>0$ which implies $\\textstyle\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}{\\bigl(}q_{i j,L}{\\bigr)}_{s}^{\\prime}>0$ Therefore, t sufies to consider ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{N_{1}=\\left\\{s\\mid(q_{i^{*}j^{*},L})_{s}=(W_{L})_{s}q_{i^{*}j^{*},L-1}>0,s=1,\\cdots,m\\right\\},}}\\\\ {{\\displaystyle N_{2}^{\\prime}=\\left\\{s\\mid(W_{L})_{s}\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L-1}>0\\right\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\begin{array}{r}{\\big(q_{i j,L-1}\\big)_{s}\\geq0}\\end{array}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle q_{i^{*}j^{*},L-1},\\displaystyle\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L-1}\\right\\rangle\\geq0,}\\\\ &{i.e.,\\,\\mathscr{L}\\left(q_{i^{*}j^{*},L-1},\\displaystyle\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L-1}\\right)\\leq\\frac{\\pi}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma 2 and Proposition 2, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left((W_{L})_{s}q_{i^{*}j^{*},L-1}>0,(W_{L})_{s}\\displaystyle\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L-1}>0\\right)}\\\\ &{\\ =\\mathbb{P}\\left(\\frac{(W_{L})_{s}}{\\|(W_{L})_{s}\\|}q_{i^{*}j^{*},L-1}>0,\\displaystyle\\frac{(W_{L})_{s}}{\\|(W_{L})_{s}\\|}\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L-1}>0\\right)}\\\\ &{\\ \\geq\\frac{1}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Also $(W_{L})_{s}$ 's are i.i.d. multivariate Gaussian. By Chernoff bound, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|N_{1}\\cap N_{2}^{\\prime}|\\in\\big(\\delta_{1}\\frac{m}{4},\\delta_{2}\\frac{m}{4}\\big)\\right)\\le1-2e^{-\\Omega(m)}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some small $\\begin{array}{r}{\\delta_{1}\\leq\\frac{1}{4}}\\end{array}$ and $\\delta_{2}\\leq4$ i.e., $\\left|N_{1}\\cap N_{2}^{\\prime}\\right|=\\Theta(m)$ with probability at least $1-2e^{-\\Omega(m)}$ ", "page_idx": 20}, {"type": "text", "text": "Step 4: Next we condition on $N_{1}\\cap N_{2}^{\\prime}$ and consider $(W_{L+1})^{k^{\\top}}W_{L+1}\\bar{\\sigma}_{t_{j}*}q_{i^{*}j^{*},L}+\\big(W_{L+1}\\big)^{k^{\\top}}\\xi_{i^{*}j^{*}}$ and $\\begin{array}{r}{(W_{L+1})^{k^{\\intercal}}W_{L+1}\\sum_{(i,j)\\neq(i^{\\ast},j^{\\ast})}\\alpha_{i j}q_{i j,L}+\\left(W_{L+1}\\right)^{k^{\\intercal}}\\sum_{(i,j)\\neq(i^{\\ast},j^{\\ast})}\\bar{\\alpha}_{i j}\\xi_{i j},}\\end{array}$ where $\\bar{\\alpha}_{i j}=\\alpha_{i j}\\big/\\bar{\\sigma}_{i j}$ We would like to prove that with high probability ", "page_idx": 20}, {"type": "equation", "text": "$$\n<\\bigl(W_{L+1}\\bar{\\sigma}_{t_{j}*}q_{i^{*}j^{*},L}+\\xi_{i^{*}j^{*}},W_{L+1}\\sum_{\\substack{(i,j)\\neq(i^{*},j^{*})}}\\alpha_{i j}q_{i j,L}+\\sum_{\\substack{(i,j)\\neq(i^{*},j^{*})}}\\bar{\\alpha}_{i j}\\xi_{i j}\\bigr)\\leq\\pi-c d^{\\frac{a_{0}-1}{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some constant $c>0$ and $\\textstyle{\\frac{1}{2}}<a_{0}<1$ ", "page_idx": 20}, {"type": "text", "text": "First, since $\\xi_{i j}\\sim\\mathcal{N}(0,I_{d})$ , by Bernstein's inequality, with probability at least $1-\\exp(-\\Omega(d))$ ,we have $\\|\\xi_{i j}\\|^{2}=\\Theta(d)$ Similaly since $\\textstyle(W_{L+1}q_{i j,L})_{s}\\sim{\\mathcal{N}}(0,{\\frac{2\\|q_{i j,L}\\|^{2}}{m}})$ , by Bersteins iequality and Lemma 4, with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\left\\|W_{L+1}q_{i j,L}\\right\\|^{2}\\,=\\,\\Theta(d)$ . By union bounds, the above holds for all $i,j$ with probability at least $1-2n N\\exp(-\\Omega(d))$ ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{v\\ =\\ \\frac{W_{L+1}\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L}+\\sum_{(i,j)\\neq(i^{*},j^{*})}\\bar{\\alpha}_{i j}\\xi_{i j}}{\\|W_{L+1}\\sum_{(i,j)\\neq(i^{*},j^{*})}\\alpha_{i j}q_{i j,L}+\\sum_{(i,j)\\neq(i^{*},j^{*})}\\bar{\\alpha}_{i j}\\xi_{i j}\\|}}\\end{array}$ and $u\\ =\\ W_{L+1}\\bar{\\sigma}_{t_{j}*}q_{i}*_{j}*,L$ . For notational simplicity, we use $\\xi$ to denote $\\xi_{i^{*}j^{*}}$ . Fix $v,u$ and consider the probability of event $A$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nA=\\{v^{\\top}\\big(u+\\xi\\big)\\leq-\\sqrt{1-c_{0}d^{a_{0}-1}}\\|u+\\xi\\|\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some $c_{0}>0$ and $\\textstyle{\\frac{1}{2}}<a_{0}<1$ ", "page_idx": 20}, {"type": "text", "text": "Then consider the following event that has larger probability than $A$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad(v^{\\top}(u+\\xi))^{2}\\geq(1-c_{0}d^{a_{0}-1})\\|u+\\xi\\|^{2}}\\\\ &{\\Longleftrightarrow(v^{\\top}u)^{2}-(1-c_{0}d^{a_{0}-1})\\|u\\|^{2}+2(v^{\\top}u v-(1-c_{0}d^{a_{0}-1})u)^{\\top}\\xi+(v^{\\top}\\xi)^{2}\\geq(1-c_{0}d^{a_{0}-1})\\|\\xi\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $(v^{\\top}u)^{2}\\leq\\|u\\|^{2}$ where the equality holds when $v={\\frac{u}{\\lVert u\\rVert}}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{LHS}\\leq c_{0}d^{a_{0}-1}\\|u\\|^{2}+2\\big(v^{\\top}u\\,v-\\big(1-c_{0}d^{a_{0}-1}\\big)u\\big)^{\\top}\\xi+\\big(v^{\\top}\\xi\\big)^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Also, since $\\|u\\|^{2}=\\mathcal{O}(d)$ with probability at least $1-2n N\\exp(-\\Omega(d))$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\vert2(v^{\\top}u\\,v-\\big(1-c_{0}d^{a_{0}-1}\\big)u\\big)^{\\top}\\xi\\vert\\ge d^{a_{0}}\\big)\\le2\\exp\\left(-c\\frac{d^{2a_{0}}}{\\Vert u\\Vert^{2}}\\right)=2\\exp\\bigl(-\\Omega\\bigl(d^{2a_{0}-1}\\bigr)\\bigr)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some constant $c>0$ ", "page_idx": 20}, {"type": "text", "text": "Therefore, with probability at least $1-\\mathcal{O}(n N)\\exp(-\\Omega(d^{2a_{0}-1}))$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{LHS\\;of\\;}(15)\\leq c d^{a_{0}}+\\left(v^{\\top}\\xi\\right)^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some constant $c>0$ ", "page_idx": 20}, {"type": "text", "text": "Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(v^{\\top}(u+\\xi)\\leq-\\sqrt{1-c_{0}d^{\\alpha_{0}-1}}\\,|\\,u+\\xi|)}\\\\ &{\\leq\\mathbb{P}((v^{\\top}(u+\\xi))^{2}\\geq(1-c_{0}d^{\\alpha_{0}-1})\\|u+\\xi|^{2})}\\\\ &{\\leq\\mathbb{P}((v^{\\top}\\xi)^{2}\\geq(1-c_{0}^{\\prime}d^{\\alpha_{0}-1})\\|\\xi|^{2})}\\\\ &{=\\mathbb{P}\\bigg(v^{\\top}\\frac{\\xi}{\\|\\xi\\|}\\geq\\sqrt{(1-c^{\\prime}d^{\\alpha_{0}-1})}\\bigg)+\\mathbb{P}\\bigg(-v^{\\top}\\frac{\\xi}{\\|\\xi\\|}\\geq\\sqrt{(1-c^{\\prime}d^{\\alpha_{0}-1})}\\bigg)}\\\\ &{=\\mathbb{P}\\bigg(\\angle(v,\\frac{\\xi}{\\|\\xi\\|})\\geq\\operatorname{arccos}(-\\sqrt{(1-c^{\\prime}d^{\\alpha_{0}-1})})\\bigg)+\\mathbb{P}\\bigg(\\angle(-v,\\frac{\\xi}{\\|\\xi\\|})\\geq\\operatorname{arccos}(-\\sqrt{(1-c^{\\prime}d^{\\alpha_{0}-1})})\\bigg)}\\\\ &{=2\\mathbb{P}\\bigg(\\angle(v,\\frac{\\xi}{\\|\\xi\\|})\\geq\\pi-c^{\\prime}d^{\\alpha_{0}-1}\\bigg)}\\\\ &{=\\frac{C}{(d^{\\frac{1-\\alpha}{\\alpha_{0}}})^{d-1}}\\sqrt{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second equality follows from Lemma 2; the third equality follows from series expansion;   \nthe forth equality follows from (16); $c^{\\prime},c^{\\prime\\prime},C>0$ are some constants. ", "page_idx": 21}, {"type": "text", "text": "Thus with probability at least $\\textstyle1-{\\frac{C}{(d^{\\frac{1-a_{0}}{2}})^{d-1}{\\sqrt{d}}}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{\\top}\\big(u+\\xi\\big)\\geq-\\sqrt{1-c_{0}d^{a_{0}-1}}\\|u+\\xi\\|}\\\\ {i.e.\\,\\angle\\left(v,u+\\xi\\right)\\leq\\pi-c d^{\\frac{a_{0}-1}{2}}\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $c>0$ ", "page_idx": 21}, {"type": "text", "text": "Then by Lemma 2, with probability at least $1-\\mathcal{O}(n N)\\exp(-\\Omega(d))$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left((W_{L+1}^{k})^{\\top}v\\geq0,(W_{L+1}^{k})^{\\top}(u+\\xi)\\geq0\\right)\\geq c d^{\\frac{a_{0}-1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $c>0$ ", "page_idx": 21}, {"type": "text", "text": "Since $(W_{L+1}^{k})$ are id Guassian vectors for $k=1,\\cdots,m$ , by Chernoff bound on Bernoulli variable $\\mathbb{1}_{\\left\\{(W_{L+1}^{k})^{\\top}v\\geq0,(W_{L+1}^{k})^{\\top}(u+\\xi)\\geq0\\right\\}}$ we have, with probability at least 1 - exp(-2(md\\* ) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\{k:(W_{L+1}^{k})^{\\top}v\\geq0,(W_{L+1}^{k})^{\\top}\\big(u+\\xi\\big)\\geq0\\}\\right|=\\Theta\\big(m d^{\\frac{a_{0}-1}{2}}\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Step 5: Combining the above 4 steps, we would like to obtain the lower bound of the gradient. ", "page_idx": 21}, {"type": "text", "text": "For each $k$ consider $(q_{i j,L-1})_{s}$ for $(i,j)\\,\\neq\\,(i^{*},j^{*})$ and denote $q_{s}\\ =\\ \\left\\{\\left(q_{i j,L-1}\\right)_{s}\\right\\}_{\\left(i,j\\right)\\neq\\left(i^{*},j^{*}\\right)}\\ =$ $\\{\\sigma((W_{L-1})_{s}q_{i j,L-2})\\}_{(i,j)\\neq(i^{*},j^{*})}$ Let $\\bar{q}_{s}\\;=\\;\\big\\{\\big(W_{L-1}\\big)_{s}q_{i j,L-2}\\big\\}_{(i,j)\\neq(i^{*},j^{*})}$ and $\\bar{q}_{s}\\;\\sim\\mathcal{N}(0,Q Q^{\\top})$ where each row of $Q$ is $q_{i j,L-2}^{\\top}$ for $(i,j)\\,\\ne\\,(i^{*},j^{*})$ . Thus, $q_{s}$ is $\\bar{q}_{s}$ projected to the nonnegative orthant. ", "page_idx": 21}, {"type": "text", "text": "Let $\\mathbf{1}=\\left(1,1,\\cdots,1\\right)\\in\\mathbb{R}^{n N-1}$ Therefore, if $\\langle\\beta_{k},\\mathbf{1}\\rangle\\geq0$ for some $\\beta_{k}\\in\\mathbb{R}^{n N-1}$ , then at least half of the nonnegative orthant is contained in $\\left\\{v\\in\\mathbb{R}^{n N-1}:\\langle\\beta_{k},v\\rangle\\ge0\\right\\}$ ie., there exists a constant $c_{k}>0$ , s.t. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\langle\\beta_{k},q_{s}\\rangle\\geq0)\\geq c_{k}\\geq\\operatorname*{min}_{k=1,\\cdots,m}c_{k}>0,\\;{\\mathrm{~for~all~}}s=1,\\cdots,m\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then since $\\beta_{k}\\in\\mathbb{R}^{n N-1}$ for $k=1,\\cdots,m$ and $n N\\ll m$ , there exists a set of indices $K\\subseteq\\{1,\\cdots,m\\}$ With $|K|\\;=\\;\\Theta\\big(\\frac{m}{n N}\\big)$ and a set of indices ${\\cal S}\\,\\subseteq\\,\\{1,\\cdots,m\\}$ with $|S|\\;=\\;\\Theta(m)$ , s.t., $\\langle\\beta_{k},q_{s}\\rangle\\,\\geq\\,0$ ,for $k\\in\\mathcal{K},s\\in\\mathcal{S}$ ", "page_idx": 21}, {"type": "text", "text": "Let $q_{i j,\\ell}^{K}=\\bigl(q_{i j,\\ell}\\bigr)_{k\\in\\mathcal{K}}$ Then by Bernstein's inequality, we can also obtain that $\\|q_{i j,\\ell}^{K}\\|^{2}=\\Theta(d)$ With probability at least $1-n N\\exp(-\\Omega(d))$ ", "page_idx": 21}, {"type": "text", "text": "Combine all of the above and apply the Claim 9.5 in Allen-Zhu et al. [1], we obtain, with probability at least $1-\\mathcal{O}(n N)\\exp(-\\Omega(d^{\\hat{2}\\hat{a}_{0}\\frac{\\cdot}{1}}))$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{W_{L}}\\bar{\\mathcal{L}}_{e m}(\\theta^{(0)})\\|_{F}^{2}\\geq\\displaystyle\\frac{1}{n^{2}}C_{6}w(t_{j^{*}})^{2}(t_{j^{*}}-t_{j^{*}-1})^{2}\\displaystyle\\frac{1}{d}\\|\\bar{\\sigma}_{t_{j^{*}}}{W}_{L+1}q_{i^{*}j^{*},L}+\\xi_{i^{*}j^{*}}\\|^{2}\\|q_{i^{*}j^{*},L-1}^{K}\\|^{2}\\displaystyle\\frac{1}{n N}m d^{\\frac{a}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq C_{6}m d^{\\frac{a_{0}-1}{2}}w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\\bar{\\sigma}_{t_{j^{*}}}\\displaystyle\\frac{1}{n^{3}N^{2}}\\bar{\\mathcal{L}}_{e m}(\\theta^{(0)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{6}>0$ is some universal constant, $\\textstyle{\\frac{1}{2}}<a_{0}<1$ , and the second inequality follows from the definition of $i^{*},j^{*}$ ", "page_idx": 21}, {"type": "text", "text": "D.1.1  Geometric ideas used in the proof ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition 2. Consider $w\\sim\\mathcal{N}(0,I)$ where $w\\in\\mathbb{R}^{n}$ . Then $\\lVert w\\rVert$ and $\\frac{w}{\\lVert w\\rVert}$ are independent random variables and $\\begin{array}{r}{\\frac{w}{\\|w\\|}\\sim U n i f(\\mathbb{S}^{n-1})}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma 2. Let $w\\sim U n i f(\\mathbb{S}^{n-1})$ ,where $\\mathbb{S}^{n-1}=\\left\\{x\\in\\mathbb{R}^{n}|\\|x\\|=1\\right\\}$ Then for two vectors $v_{1},v_{2}\\in\\mathbb{R}^{n}$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}(w^{\\top}v_{1}\\geq0,w^{\\top}v_{2}\\geq0)=\\frac{\\pi-\\angle\\left(v_{1},v_{2}\\right)}{2\\pi}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof.Since $w\\sim U n i f(\\mathbb{S}^{n-1})$ , we only need to consider the area of the event. It is obvious that the set $\\left\\{w\\in\\mathbb{S}^{n-1}|w^{\\top}v_{i}\\right\\}$ is a semi-hypersphere. Therefore, we only need to consider the intersection of two semi-hypersphere, i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P(w^{\\top}v_{1}\\geq0,w^{\\top}v_{2}\\geq0)=\\frac{\\arg\\big\\{w\\in\\mathbb S^{n-1}\\big|w^{\\top}v_{1}\\geq0\\big\\}\\cap\\mathrm{~area~of~}\\big\\{w\\in\\mathbb S^{n-1}\\big|w^{\\top}v_{2}\\geq0\\big\\}}{\\mathrm{area~of~the~hypersphere}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\pi-\\,\\angle\\,\\big(v_{1},v_{2}\\big)}{2\\pi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next we follow the notations and definitions in Lee and Kim [33]. Consider the unit hypersphere in $\\mathbb{R}^{d},\\mathbb{S}^{d-1}=\\left\\{x\\in\\mathbb{R}^{d}\\,|\\,\\|x\\|=1\\right\\}$ The area of $\\mathbb{S}^{d-1}$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\nA_{d}(1)=\\frac{2\\pi^{d/2}}{\\Gamma(d/2)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 3. Fix $\\xi_{1}\\,\\in\\,\\mathbb{S}^{d-1}$ and let $\\xi_{2}\\,\\sim\\,U n i f(\\mathbb{S}^{d-1})$ where $\\mathbb{S}^{d-1}\\,=\\,\\{\\boldsymbol{x}\\,\\in\\,\\mathbb{R}^{d}|\\|\\boldsymbol{x}\\|\\,=\\,1\\}$ . Then with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\angle\\left(\\xi_{1},\\xi_{2}\\right)\\leq\\frac{3\\pi}{4}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Proof. For any fixed $\\xi_{1}$ , all the $\\xi_{2}$ 's that satisfy $\\angle\\left(\\xi_{1},\\xi_{2}\\right)\\geq\\pi-\\theta$ are on a hyperspherical cap. By Lee and Kim [33], the area of the hypersperical cap is ", "page_idx": 22}, {"type": "equation", "text": "$$\nA_{d}^{\\theta}(1)=\\frac{1}{2}A_{d}(1)I_{\\sin^{2}\\theta}\\left(\\frac{d-1}{2},\\frac{1}{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\angle\\big(\\xi_{1},\\xi_{2}\\big)\\ge\\pi-\\theta\\big)=\\frac{A_{d}^{\\theta}(1)}{A_{d}(1)}=\\frac12I_{\\sin^{2}\\theta}\\left(\\frac{d-1}{2},\\frac12\\right)\\propto\\frac12\\frac{\\theta^{d-1}}{\\sqrt{\\pi}\\sqrt{\\frac{d-1}{2}}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\textstyle\\theta={\\frac{\\pi}{4}}<1$ .Then with probabilityat ast $1-\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\angle\\left(\\xi_{1},\\xi_{2}\\right)\\leq\\frac{3\\pi}{4}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "D.2Proofs related to random initialization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consider $W_{i}=W_{i}^{(0)}$ in this section. ", "page_idx": 22}, {"type": "text", "text": "Lemma 4. If $\\epsilon\\,\\,\\in\\,\\,(0,1)$ with probabiliyat least $1\\mathrm{~-~}\\mathcal{O}(n N)e^{-\\Omega(\\operatorname*{min}(\\epsilon^{2}d^{4b-1},\\epsilon d^{2b}))}$ \uff0c $\\|X_{i j}\\|^{2}\\;\\in$ $\\begin{array}{r}{\\big[\\|e^{-\\mu_{t_{j}}}x_{i}\\|^{2}+\\bar{\\sigma}_{t_{j}}^{2}d-\\epsilon\\bar{\\sigma}_{t_{j}}^{2}d^{2b},\\|e^{-\\mu_{t_{j}}}x_{i}\\|^{2}+\\bar{\\sigma}_{t_{j}}^{2}d+\\epsilon\\bar{\\sigma}_{t_{j}}^{2}d^{2b}\\big].}\\end{array}$ for all $i=1,\\cdots,n$ and $j=0,\\cdots,N-1$ Moreoverwithprobability at least $1-\\mathcal{O}(L)e^{-\\Omega(m\\epsilon^{2}/L)}$ over the randomness of $W_{s}$ for $s=0,\\cdots,L_{i}$ we have $\\|q_{i j,\\ell}\\|\\;\\ell\\;[\\|X_{i j}\\|(1-\\epsilon),\\|X_{i j}\\|(\\1+\\epsilon)]$ for fixed $i,j$ Therefore,withprobability at least $1-\\mathcal{O}(n N L)e^{-\\Omega(\\operatorname*{min}(m\\epsilon^{2}/L,\\epsilon^{2}d^{4b-1},\\epsilon d^{2b}))}$ ,we have $\\Omega(d^{b})=\\|q_{i j,\\ell}\\|=\\mathcal{O}(d^{a})$ ", "page_idx": 22}, {"type": "text", "text": "Proof. Consider $\\begin{array}{r}{\\frac{1}{\\bar{\\sigma}_{t_{j}}}X_{i j}=\\frac{e^{-\\mu_{t_{j}}}}{\\bar{\\sigma}_{t_{j}}}x_{i}\\!+\\!\\xi_{i j}}\\end{array}$ . Since $\\begin{array}{r}{\\xi_{i j}\\sim\\mathcal{N}(0,I),\\,\\|\\frac{1}{\\bar{\\sigma}_{t_{j}}}X_{i j}\\|^{2}}\\end{array}$ follows from the noncentral $\\chi^{2}$ $\\begin{array}{r}{\\mathbb{E}\\|\\frac{1}{\\bar{\\sigma}_{t_{j}}}X_{i j}\\|^{2}=d+\\|\\frac{e^{-\\mu_{t_{j}}}}{\\bar{\\sigma}_{t_{j}}}x_{i}\\|^{2}}\\end{array}$ $d$ By Berstein inequality, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\bigg|\\bigg|\\displaystyle\\bigg|\\frac{1}{\\bar{\\sigma}_{t_{j}}}X_{i j}\\bigg|\\bigg|^{2}-\\mathbb{E}\\bigg|\\bigg|\\frac{1}{\\bar{\\sigma}_{t_{j}}}X_{i j}\\bigg|\\bigg|^{2}\\bigg|\\geq t\\bigg)\\leq2\\exp\\bigg(-c\\operatorname*{min}\\bigg(\\displaystyle\\frac{t^{2}}{d},t\\bigg)\\bigg)}\\\\ &{i.e.,\\mathbb{P}\\bigg(\\bigg|\\|e^{-\\mu_{t_{j}}}x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j}\\|^{2}-(\\bar{\\sigma}_{t_{j}}^{2}d+\\|e^{-\\mu_{t_{j}}}x_{i}\\|^{2})\\bigg|\\geq\\bar{\\sigma}_{t_{j}}^{2}t\\bigg)\\leq2\\exp\\bigg(-c\\operatorname*{min}\\bigg(\\displaystyle\\frac{t^{2}}{d},t\\bigg)\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, wiait $1-\\mathcal{O}(n N)e^{-\\Omega(\\operatorname*{min}(\\epsilon^{2}d^{4b-1},\\epsilon d^{2b}))},\\,\\|X_{i j}\\|^{2}\\epsilon\\left[\\|e^{-\\mu_{t_{j}}}x_{i}\\|^{2}+\\bar{\\sigma}_{t_{j}}^{2}d-\\frac{1}{2}\\|e^{-\\mu_{t_{i}}}x_{i}\\|^{2}\\right]$ $\\epsilon\\bar{\\sigma}_{t_{j}}^{2}d^{2b},\\|e^{-\\mu_{t_{j}}}x_{i}\\|^{2}+\\bar{\\sigma}_{t_{j}}^{2}d+\\epsilon\\bar{\\sigma}_{t_{j}}^{2}d^{2b}]$ for all $i=1,\\cdots,n$ and $j=0,\\cdots,N-1$ , where $\\epsilon\\in(0,1)$ . The second part of the Lemma follows the similar proof in Lemma 7.1 of Allen-Zhu et al. [1]. The last part follows from union bound and Assumption 1. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 5 (Upper bound). Under the random initialization of $W_{i}$ for $i=0,\\cdots,L,$ with probability at least $1-\\mathcal{O}(n N L)e^{-\\Omega(\\operatorname*{min}(m\\epsilon^{2}/L,\\epsilon^{2}d^{4b-1},\\epsilon d^{2b}))}$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla_{W_{\\ell}}\\bar{\\mathcal{L}}_{e m}(\\theta^{(0)})\\|^{2}=\\mathcal{O}\\left(m d^{2a-1}N\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\right)\\bar{\\mathcal{L}}_{e m}(\\theta^{(0)}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For any $\\ell=1,\\cdot,L$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$\\|\\nabla_{\\boldsymbol{W}_{\\ell}}\\bar{\\mathcal{L}}_{e m}(\\boldsymbol{\\theta})\\|_{F}^{2}$ $$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{W}_{i}}\\mathrm{Le}_{\\mathbf{f}}(\\mathbf{r}_{i}\\infty(\\theta)|\\,\\mathrm{fl})}\\\\ &{=\\displaystyle\\sum_{k=1}^{\\infty}\\left\\|\\nabla_{\\langle\\mathbf{W}_{i}\\rangle_{k}}\\hat{\\mathcal{K}}_{c}(\\theta)\\right\\|^{2}}\\\\ &{=\\displaystyle\\sum_{k=1}^{\\infty}\\left\\|\\frac{1}{n}\\sum_{i=1,j=1}^{N}w(t_{j})(t_{j}-t_{j-1})\\right.}\\\\ &{\\qquad\\quad\\times\\left[(W_{i+1}D_{i,j,k}W_{L^{\\star}}\\mathrm{.}D_{i,j,k}W_{i+1})^{\\top}(\\tilde{\\sigma}_{i},W_{L+1}\\eta_{j,k}+\\xi_{i j})\\right]\\mathrm{k}\\,q_{j,j-1}\\,\\mathrm{I}_{(W\\varphi_{i},t_{j-1})\\times\\mathbf{s}}\\right\\|^{2}}\\\\ &{\\le\\displaystyle\\frac{N}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{N}w(t_{j})^{2}(t_{j}-t_{j-1})^{2}\\frac{m}{k!}\\left\\|(W_{i+1}D_{i,j}W_{L^{\\star}}\\mathrm{.}D_{i,j,k}W_{i+1})\\overline{{\\big\\chi}}(\\tilde{\\sigma}_{i},W_{L+1}\\eta_{j,L}+\\xi_{i j})\\right\\|^{2}\\cdot\\left|\\eta_{i j,\\ell-1}\\right|}\\\\ &{\\le C\\displaystyle\\eta^{d,\\alpha}\\frac{N}{n}\\sum_{i=1,j=1}^{n}w(t_{j})^{2}(t_{j}-t_{j-1})^{2}\\cdot\\left|\\hat{\\mathcal{E}}_{\\eta},W_{L+1}\\eta_{i,j,L}+\\xi_{i j}\\right|^{2}}\\\\ &{\\le C\\displaystyle\\gamma d^{\\alpha}\\frac{\\alpha\\,N}{m}\\operatorname*{max}_{i=1}^{n}w_{i\\in\\mathbf{I}_{j}}(t_{j}-t_{j-1})\\cdot v_{i}\\zeta_{i-1}(\\theta_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality follows from Young's inequality; the second inequality follows from Lemma 4 and Lemma 7.4 in Allen-Zhu et al. [1]; $C_{7}>0$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D.3 Proofs related to perturbation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Consider $W_{i}^{\\mathrm{per}}=W_{i}^{(0)}+W_{i}^{\\prime}$ for $i=1,\\cdots,L$ in this section. We follow the same idea in Allen-Zhu et al. [1] to consider the network value of perturbed weights at each layer. We use the superscript \"per\u201d' to denotes the perturbed version, i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{i j,0}^{\\mathrm{per}}=W_{0}X_{i j},q_{i j,0}^{\\mathrm{per}}=\\sigma\\big(r_{i j,0}^{\\mathrm{per}}\\big),}\\\\ &{r_{i j,\\ell}^{\\mathrm{per}}=W_{\\ell}^{\\mathrm{per}}q_{i j,\\ell-1}^{\\mathrm{per}},q_{i j,\\ell}^{\\mathrm{per}}=\\sigma\\big(r_{i j,\\ell}^{\\mathrm{per}}\\big),\\;\\mathrm{for}\\;\\ell=1,\\cdots,L}\\\\ &{S\\big(\\theta^{\\mathrm{per}};t_{j},X_{i j}\\big)=W_{L+1}q_{i j,L}^{\\mathrm{per}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We also similarly define the diagonal matrix $D_{i j,\\ell}^{\\mathrm{per}}$ for the above network. ", "page_idx": 23}, {"type": "text", "text": "The following Lemma measures the perturbation of each layer. The lemma differs from Lemma 8.2 in Allen-Zhu et al. [1] by a scale of $d^{a}$ . For sake of completeness, we state it in the following and the proof can be similarly obtained. ", "page_idx": 23}, {"type": "text", "text": "Lemma 6. Let $\\omega\\ \\ \\leq\\ \\ \\frac{1}{C_{7}L^{9/2}(\\log m)^{3}d^{a}}$ for some large $C~>~1$ With pobabiliy a eas - $\\exp(-\\Omega(d^{a}m\\omega^{2/3}L))$ , for any \u25b3W s.t. $\\|\\Delta W\\|\\leq\\omega$ , we have ", "page_idx": 23}, {"type": "text", "text": "D.4  Proofs related to the evolution of the algorithm ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 7 (Upper and lower bounds of gradient after perturbation). Let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\omega=\\mathcal{O}\\left(\\frac{\\bar{C}_{e m}^{*}}{L^{9}(\\log m)^{2}n^{3}N^{3}d^{\\frac{1-\\alpha_{0}}{2}}}\\cdot\\frac{\\operatorname*{min}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}{\\operatorname*{max}\\{\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}},\\sum_{j}(w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}})^{2}\\}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consider $\\theta^{\\mathrm{per}}$ s.t. $\\lVert{\\boldsymbol{\\theta}}^{\\mathrm{per}}-{\\boldsymbol{\\theta}}\\rVert\\,\\leq\\,\\omega$ where $\\theta$ follows from the Gaussian initialization. Then with probability at least 1 - O(nN)e-Q(d2ao-1), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W_{\\ell}}\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\|^{2}=\\mathcal{O}\\left(m d^{2a-1}N\\operatorname*{max}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\right)\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}}),}\\\\ &{\\|\\nabla_{W_{L}}\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\|^{2}=\\Omega\\left(\\frac{m d^{\\frac{a_{0}-1}{2}}}{n^{3}N^{2}}\\,w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\\bar{\\sigma}_{t_{j^{*}}}\\right)\\operatorname*{min}\\{\\bar{\\mathcal{L}}_{e m}(\\theta),\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $\\ell=1,\\cdots,L.$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Consider the following terms ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{(W_{\\varepsilon})_{k}}\\bar{C}_{e m}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})}\\\\ &{\\qquad\\times\\left[(W_{\\varepsilon+1}D_{i j,L}W_{\\varepsilon}\\cdots D_{i j,\\ell}W_{\\varepsilon+1})^{\\top}(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j})\\right]_{k}q_{i j,\\ell-1}\\mathbb{1}_{(W_{\\varepsilon}q_{i j,\\ell-1})_{k}>0}}\\\\ &{\\nabla_{(W_{\\varepsilon})_{k}}^{\\mathrm{per}}\\bar{C}_{e m}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})}\\\\ &{\\qquad\\times\\left[(W_{\\varepsilon+1}D_{i j,L}W_{\\varepsilon}\\cdots D_{i j,\\ell}W_{\\varepsilon+1})^{\\top}(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}^{\\mathrm{ber}}+\\xi_{i j})\\right]_{k}q_{i j,\\ell-1}\\mathbb{1}_{(W_{\\varepsilon}q_{i j,\\ell-1})_{k}>0},}\\\\ &{\\nabla_{(W_{\\varepsilon})_{k}}\\bar{C}_{e m}(\\theta^{\\mathrm{per}})=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})}\\\\ &{\\qquad\\times\\left[(W_{\\varepsilon+1}D_{i j,L}^{\\mathrm{per}}\\cdots D_{i j,\\ell}^{\\mathrm{per}}W_{\\varepsilon+1}^{\\mathrm{per}})^{\\top}(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}^{\\mathrm{per}}+\\xi_{i j})\\right]_{k}q_{i j,\\ell-1}^{\\mathrm{per}}\\mathbb{1}_{(W_{\\varepsilon}^{\\mathrm{per}}\\cap\\{\\varepsilon_{i j,\\ell-1}\\}_{k})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W_{t}}\\mathcal{L}_{e m}(\\theta)-\\nabla_{W_{t}}^{\\mathrm{pr}}\\mathcal{L}_{e m}(\\theta)\\|_{F}^{2}}\\\\ &{=\\displaystyle\\sum_{i=1}^{m}\\|\\nabla_{(W_{t})_{h}}\\mathcal{L}_{e m}(\\theta)-\\nabla_{(W_{t})_{h}}^{\\mathrm{pr}}\\mathcal{L}_{e m}(\\theta)\\|^{2}}\\\\ &{\\le\\displaystyle\\frac{N}{n}\\sum_{i=1}^{n}\\sum_{\\nu=1}^{N}w(t_{j})^{2}(t_{j}-t_{j-1})^{2}\\sum_{k=1}^{m}\\|(W_{t,i}W_{L^{\\star}}\\!\\!-\\!D_{i j,i}W_{t,i}\\!)_{k}^{\\top}\\big(\\tilde{\\sigma}_{t_{j}}W_{L^{\\star}\\!+\\!1}(q_{i j,i}\\!-\\!q_{i j,i}^{\\mathrm{pr}})\\big)\\|^{2}\\cdot\\big|}\\\\ &{\\le C_{8}d^{2}a^{2}\\displaystyle\\frac{N}{d}\\sum_{i=1}^{N}w(t_{j})^{2}(t_{j}-t_{j-1})^{2}\\cdot\\|\\tilde{\\sigma}_{t_{i}}W_{L^{\\star}\\!+\\!1}(q_{i j,L}\\!-\\!q_{i j,L}^{\\mathrm{pr}})\\|^{2}}\\\\ &{\\le C_{8}^{\\prime}d^{2}a^{2}\\displaystyle\\frac{N}{d}\\sum_{j=1}^{N}w(t_{j})^{2}(t_{j}-t_{j-1})^{2}(\\omega L^{5/2}\\sqrt{\\log m}t^{a})^{2}}\\\\ &{\\le\\tilde{C}_{8}\\left(\\frac{m d^{\\frac{9-1}{2}}}{n^{3}N^{2}}w(t_{j})(t_{j}-t_{j-1})\\tilde{\\sigma}_{t_{j-1}}\\!\\right)\\bar{C}_{c m}^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first two inequalities follow the same as the proof of Lemma 5; the third inequality follows from Lemma 6; the last inequality follows from the definition of $\\omega$ ", "page_idx": 24}, {"type": "text", "text": "Also, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla_{\\left({W_{i}}\\right)k}^{\\mathrm{per}}\\mathcal{E}_{e n v}(\\theta)-\\nabla_{\\left({W_{i}}\\right)k}\\bar{\\mathcal{E}}_{e n v}\\left(\\theta^{\\mathrm{per}}\\right)\\right|}\\\\ &{\\le\\left\\|\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})\\right.}\\\\ &{\\qquad\\times\\left[(W_{i+1}D_{i,j}^{\\mathrm{per}}W_{L}^{\\mathrm{per}}\\cdots D_{i,j,\\ell}^{\\mathrm{per}}W_{\\ell+1}^{\\mathrm{per}}-W_{L+1}D_{i,j}_{\\ell}W_{L}\\cdots D_{i,j,\\ell}W_{\\ell+1})^{\\top}(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i,j,L}^{\\mathrm{per}}+\\xi_{i j})]_{k}\\right.}\\\\ &{\\qquad\\times\\left.\\sigma_{i j,\\ell-1}^{\\mathrm{per}}\\mathbb{1}_{(W_{i}^{\\mathrm{per}}\\:\\eta_{i,\\ell-1}^{\\mathrm{per}})\\times{9}}\\right|}\\\\ &{+\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}w(t_{j})(t_{j}-t_{j-1})\\big[(W_{L+1}D_{i,j,L}W_{L}\\cdots D_{i,j,\\ell}W_{\\ell+1})^{\\top}(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i,j,L}^{\\mathrm{per}}+\\xi_{i j})\\big]_{k}\\right.}\\\\ &{\\qquad\\times\\left.\\left(q_{i,j-1}\\mathbb{1}_{(W_{i}\\:q_{i,j-1})\\times{9}}-q_{i,j-1}^{\\mathrm{per}}\\mathbb{1}_{(W_{i}^{\\mathrm{per}}\\:\\eta_{i,\\ell-1}^{\\mathrm{per}})\\times{9}}\\right)\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla_{\\overline{{\\nu}}}^{T}\\overline{{\\nu}}_{0}^{T}\\overline{{\\nu}}_{0}(\\theta)-\\nabla_{\\overline{{\\nu}}}\\overline{{\\zeta}}_{0}(\\theta)\\right|^{2}\\right|}\\\\ &{\\leq2\\frac{N}{\\hbar}\\displaystyle\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\mathrm{e}_{0}(t_{j})^{2}(t_{i}-t_{j-1})^{2}\\left|\\frac{\\gamma}{\\hbar}\\overline{{\\mu}}_{0}^{T}\\overline{{\\nu}}_{1}^{T}\\right|^{2}}\\\\ &{\\qquad\\times\\displaystyle\\sum_{i=1}^{N}\\|(W_{t_{i}-1})\\overline{{\\eta}}_{i j}^{T}{W}_{i j}^{T}{W}_{i j}^{T}-\\langle{W}_{0}^{T},{W}_{i-1}^{\\mathrm{pr}}-{W}_{t_{i-1}}{D}_{t_{j},i}{W}_{i-1}^{\\mathrm{pr}}-\\langle{D}_{0},{W}_{i}^{T}\\rangle_{\\overline{{\\nu}}_{0}+}\\frac{1}{\\epsilon_{i}}\\langle\\eta_{i},{W}_{i+1}^{2}\\rangle_{\\overline{{\\nu}}_{0}+}\\xi_{i})}\\\\ &{\\qquad+2\\frac{N}{\\hbar}\\displaystyle\\sum_{i=1}^{N}\\sum_{j=1}^{N}w(t_{j})^{2}(t_{i}-t_{j-1})^{2}\\cdot\\left\\langle\\|_{\\theta_{i j},i-1}\\|_{(W_{t_{i}-1,i-1})\\otimes t}-\\eta_{i,j-1}^{\\mathrm{pr}}\\frac{1}{\\epsilon_{i}}\\|_{(W_{t_{i}-1,i-1}^{2})\\otimes t}\\right|^{2}}\\\\ &{\\qquad\\times\\displaystyle\\sum_{i=1}^{N}\\|(W_{t_{i-1},0_{i},i-1)\\otimes t}-{W}_{t_{i},0}{W}_{i+1}^{\\mathrm{pr}}\\ldots\\|_{\\overline{{\\nu}}_{0}+}\\mathbb{P}_{t_{i-1}}\\|_{\\overline{{\\nu}}_{0}+}^{2}}\\\\ &{\\qquad\\times\\displaystyle\\sum_{i=1}^{N}\\left|(\\langle\\nabla_{\\overline{{\\nu}}}\\pi_{0}|^{2}\\hat{\\chi} \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality follows from Young's inequality and the above decomposition; the second inequality follows from Lemma 7.4, 8.7 in Allen-Zhu et al. [1] (with $s\\;=\\;{\\bar{\\mathcal{O}}}(d^{a}m\\omega^{2/3}L))$ and Lemma 6; the last inequality follows from the definition of $\\omega$ ", "page_idx": 25}, {"type": "text", "text": "For upper bound, we only need to consider (W) Cem(0) and V(w),Lem(oper). By similar argument as Lemma 5, with probability at least $1-\\mathcal{O}(n N L)e^{-\\Omega(\\operatorname*{min}(m\\epsilon^{2}/L,\\epsilon^{2}d^{4b-1},\\epsilon d^{2b}))}$ ,we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\Vert\\nabla_{(W_{\\ell})_{k}}^{\\mathrm{per}}\\bar{\\mathcal{L}}_{e m}(\\theta)\\right\\Vert^{2}=\\mathcal{O}\\left(m d^{2a-1}N\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\right)\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W_{\\ell}}\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\|^{2}}\\\\ &{\\leq2\\|\\nabla_{W_{\\ell}}^{\\mathrm{per}}\\bar{\\mathcal{L}}_{e m}(\\theta)\\|_{F}^{2}+2\\|\\nabla_{W_{\\ell}}^{\\mathrm{per}}\\bar{\\mathcal{L}}_{e m}(\\theta)-\\nabla_{W_{\\ell}}\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\|_{F}^{2}}\\\\ &{=\\mathcal{O}\\left(m d^{2a-1}N\\operatorname*{max}_{j}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}\\right)\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Also, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W}\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\|^{2}}\\\\ &{\\ge\\|\\nabla_{W_{L}}\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\|^{2}}\\\\ &{\\ge\\frac{1}{3}\\|\\nabla_{W_{L}}\\bar{\\mathcal{L}}_{e m}(\\theta)\\|^{2}-\\|\\nabla_{W_{\\ell}}\\bar{\\mathcal{L}}_{e m}(\\theta)-\\nabla_{W_{\\ell}}^{\\mathrm{per}}\\bar{\\mathcal{L}}_{e m}(\\theta)\\|_{F}^{2}-\\|\\nabla_{W_{\\ell}}^{\\mathrm{per}}\\bar{\\mathcal{L}}_{e m}(\\theta)-\\nabla_{W_{\\ell}}\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\|_{F}^{2}}\\\\ &{=\\Omega\\left(\\frac{m d^{\\frac{\\bar{a}_{0}-1}{2}}}{n^{3}N^{2}}\\,w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})\\bar{\\sigma}_{t_{j^{*}}}\\right)\\operatorname*{min}\\{\\bar{\\mathcal{L}}_{e m}(\\theta),\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note when interpolation is not achievable, this lower bound is always away from O, which means the current technique can only evaluate the lower bound outside a neighbourhood of the minimizer. More advanced method is needed and we leave it for future investigation. ", "page_idx": 26}, {"type": "text", "text": "Lemma 8 (semi-smoothness). Let $\\omega=\\Omega$ With probability at least $1-e^{-\\Omega(\\log m)}$ over the randomness f $\\theta^{(0)}$ we have for all $\\theta$ s.t. $\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}^{(0)}\\rVert\\leq\\omega$ and all $\\theta^{\\mathrm{per}}\\ s.t.$ $\\lVert{\\boldsymbol{\\theta}}^{\\mathrm{per}}-{\\boldsymbol{\\theta}}\\rVert\\leq\\omega$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})\\leq\\bar{\\mathcal{L}}_{e m}(\\theta)+\\left\\langle\\nabla\\bar{\\mathcal{L}}_{e m}(\\theta),\\theta^{\\mathrm{per}}-\\theta\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\sqrt{\\bar{\\mathcal{L}}_{e m}(\\theta)}\\sqrt{\\underset{j}{\\sum}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}\\mathcal{O}(\\omega^{1/3}L^{2}\\sqrt{m\\log m}d^{a/2})\\|\\theta^{\\mathrm{per}}-\\theta\\|}\\\\ &{\\qquad\\qquad\\qquad+\\sqrt{\\bar{\\mathcal{L}}_{e m}(\\theta)}\\sqrt{\\underset{j}{\\sum}w(t_{j})(t_{j}-t_{j-1})\\bar{\\sigma}_{t_{j}}}\\mathcal{O}(L^{2}\\sqrt{m}d^{a})\\|\\theta^{\\mathrm{per}}-\\theta\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By definition, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathcal{L}}_{e m}(\\theta^{\\mathrm{per}})-\\bar{\\mathcal{L}}_{e m}(\\theta)-\\langle\\nabla\\bar{\\mathcal{L}}_{e m}(\\theta),\\theta^{\\mathrm{per}}-\\theta\\rangle}\\\\ &{\\;\\;=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})(\\bar{\\sigma}_{t_{j}}W_{L+1}q_{i j,L}+\\xi_{i j})^{\\top}W_{L+1}}\\\\ &{\\quad\\quad\\times\\left(q_{i j,L}^{\\mathrm{per}}-q_{i j,L}-\\displaystyle\\sum_{\\ell=1}^{L}D_{i j,L}W_{i j,L}{\\cdots}W_{i j,\\ell+1}D_{i j,\\ell}(W_{i j,\\ell}^{\\mathrm{per}}-W_{i j,\\ell})q_{i j,\\ell}\\right)}\\\\ &{\\quad\\quad+\\displaystyle\\frac{1}{2\\bar{\\sigma}_{t_{j}}}w(t_{j})(t_{j}-t_{j-1})\\|\\bar{\\sigma}_{t_{j}}W_{L+1}(q_{i j,L}^{\\mathrm{per}}-q_{i j,L})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Similar to the proof of Theorem 4 in Allen-Zhu et al. [1], we obtain the desired bound by using CauchySchwartz inequality. Note, in our case, due to the order of input data, we choose $s=\\dot{O}(d^{a}\\bar{m}\\omega^{2/3}\\dot{L})$ in Allen-Zhu et al. [1] and therefore the bound is slightly different from theirs. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "E  Proofs for sampling ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we prove Theorem 2. The proof includes two main steps: 1. decomposing $\\mathrm{KL}(p_{\\delta}|q_{T-\\delta})$ into the initialization error, the score estimation errors and the discretization errors; 2. estimating the initialization error and the discretization error based on our assumptions. In the following context, we introduce the proof of these two steps separately. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 2. Step 1: The error decomposition follows from the ideas in [12] of studying VPSDE-based diffusion models. According to the chain rule of KL divergence, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\bigl(p_{\\delta}|q_{T-\\delta}\\bigr)\\leq\\mathrm{KL}\\bigl(p_{T}|q_{0}\\bigr)+\\mathbb{E}_{y\\sim p_{T}}\\bigl[\\mathrm{KL}\\bigl(p_{\\delta|T}(\\cdot|y)|q_{T-\\delta|0}(\\cdot|y)\\bigr)\\bigr],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Apply the chain rule again for at across the time schedule $(T-t_{j}^{\\leftarrow})_{0\\leq j\\leq N-1}$ , the second term can be written as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{y\\sim\\mathcal{P}_{T}}[\\mathrm{KL}(p_{\\delta|T}(\\cdot|y)|q_{T-\\delta}|(\\cdot|y))]}\\\\ &{\\le\\displaystyle\\sum_{j=0}^{N-1}\\mathbb{E}_{y_{j}\\sim p_{T-t_{j}^{-}}}\\bigl[\\mathrm{KL}\\bigl(p_{T-t_{j+1}^{-}}|T-t_{j}^{-}\\bigr(\\cdot|y_{j})|q_{t_{j+1}^{-}}|t_{j}^{-}(\\cdot|y_{j})\\bigr)\\bigr]}\\\\ &{\\le\\displaystyle\\frac{1}{2}\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\mathbb{E}\\bigl[\\|s\\bigl(\\theta;T-t_{j}^{-},Y_{t_{j}^{-}}\\bigr)-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}\\bigr]d t}\\\\ &{\\le\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\mathbb{E}\\bigl[\\|s\\bigl(\\theta;T-t_{j}^{-},Y_{t_{j}^{-}}\\bigr)-\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})\\|^{2}\\bigr]d t}\\\\ &{\\quad+\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\mathbb{E}\\bigl[\\|\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}\\bigr]d t}\\\\ &{\\quad+\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\mathbb{E}\\bigl[\\|\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}\\bigr]d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality follows from Lemma 9. Therefore, the error decomposition writes as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{KL}(p_{\\delta}|q_{T-\\delta})\\lesssim\\mathrm{KL}(p_{T}|q_{0})+\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\mathbb{E}\\big[\\|s(\\theta;T-t_{j}^{\\leftarrow},Y_{t_{j}^{-}})-\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})\\|^{2}\\big]d t}&{}&\\\\ {+\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{+}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\mathbb{E}\\big[\\|\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}\\big]d t}&{}&{{\\scriptstyle()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the three terms in (20) quantify the initialization error, the score estimation error and the discretization error, respectively. ", "page_idx": 27}, {"type": "text", "text": "Step 2: In this step, we estimate the three error terms in Step 1. First, recall that $p_{T}=p*\\mathcal{N}(0,\\bar{\\sigma}_{T}^{2}I_{d})$ and $q_{0}=\\mathcal{N}(0,\\bar{\\sigma}_{T}^{2}I_{d})$ , hence the initialization error $\\mathrm{KL}(p_{T}|q_{0})$ can be estimated as follows, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\big(p_{T}|q_{0}\\big)=\\mathrm{KL}\\big(p*\\mathcal{N}(0,\\bar{\\sigma}_{T}^{2}I_{d})|\\mathcal{N}(0,\\bar{\\sigma}_{T}^{2}I_{d})\\big)\\lesssim\\frac{\\mathrm{m}_{2}^{2}}{\\bar{\\sigma}_{T}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the inequality follows from Lemma 10. Hence we recover the term $E_{I}$ in (11). ", "page_idx": 27}, {"type": "text", "text": "Next, since $\\sigma_{t}$ is non-decreasing in $t$ , the score estimation error can be estimated as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\leftarrow}}^{t_{j+1}^{\\leftarrow}}\\sigma_{T-t}^{2}\\mathbb{E}\\big[\\|s\\big(\\theta;T-t_{j}^{\\leftarrow},Y_{t_{j}^{\\leftarrow}}\\big)-\\nabla\\log p_{T-t_{j}^{\\leftarrow}}\\big(Y_{t_{j}^{\\leftarrow}}\\big)\\|^{2}\\big]d t}\\\\ &{\\displaystyle\\leq\\sum_{j=0}^{N-1}\\gamma_{j}\\sigma_{T-t_{j}^{\\leftarrow}}^{2}\\mathbb{E}\\big[\\|s\\big(\\theta;T-t_{j}^{\\leftarrow},Y_{t_{j}^{\\leftarrow}}\\big)-\\nabla\\log p_{T-t_{j}^{\\leftarrow}}\\big(Y_{t_{j}^{\\leftarrow}}\\big)\\|^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, we recover the term $E_{S}$ in (11). ", "page_idx": 27}, {"type": "text", "text": "Last, we estimated the discretization error term. Our approach is motivated by analyses of VPSDEs in [8, 26]. We defines a process $L_{t}:=\\nabla\\log p_{T-t}(Y_{t})$ . Then we can relate discretization error to quantities depending on $L_{t}$ , and therefore bound the discretization error via properties of $\\{L_{t}\\}_{0\\leq t\\leq T}$ According to Lemma 12, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\varepsilon}}^{t_{j+1}^{\\varepsilon-1}}\\sigma_{T-t}^{2}\\mathbb{E}[\\|\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}]d t}_{>0}}\\\\ &{\\underbrace{\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{\\varepsilon-}}\\int_{t_{j}^{-}}^{t}\\sigma_{T-t}^{2}\\sigma_{T-u}^{2}\\bar{\\sigma}_{T-u}^{-4}d u d t}_{N_{1}}+\\underbrace{\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{\\varepsilon-}}\\sigma_{T-t}^{2}d t\\bar{\\sigma}_{T-t_{j}^{-}}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j}^{+}}(X_{T-t_{j}^{-}}))]}_{N_{2}}}\\\\ &{-\\underbrace{\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\varepsilon}}^{t_{j+1}^{\\varepsilon-}}\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t}(X_{T-t}))]d t\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $t\\mapsto\\sigma_{t}$ is non-decreasing and $t\\mapsto\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t}(X_{T-t}))]$ is non-increasing, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle N_{1}=2d\\sum_{j=0}^{N-1}\\int_{T-t_{j+1}^{\\leftarrow}}^{T-t_{j}^{\\leftarrow}}\\int_{T-t_{j+1}^{\\leftarrow}}^{T-u}\\sigma_{t}^{2}d t\\sigma_{u}^{2}\\bar{\\sigma}_{u}^{-4}d u\\le2d\\displaystyle\\sum_{j=0}^{N-1}\\gamma_{j}\\int_{T-t_{j+1}^{\\leftarrow}}^{T-t_{j}^{\\leftarrow}}\\sigma_{t}^{4}\\bar{\\sigma}_{t}^{-4}d t,}}\\\\ {{\\displaystyle N_{2}=\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\leftarrow}}^{t_{j+1}^{\\leftarrow}}\\sigma_{T-t}^{2}d t\\bar{\\sigma}_{T-t_{j}^{\\leftarrow}}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j}^{\\leftarrow}}(X_{T-t_{j}^{\\leftarrow}}))],}}\\\\ {{\\displaystyle N_{3}\\le-\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\leftarrow}}^{t_{j+1}^{\\leftarrow}}\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}d t\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j+1}^{\\leftarrow}}(X_{T-t_{j+1}^{\\leftarrow}}))].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\pm}}^{t_{j+1}}\\sigma_{T-t}^{2}\\mathbb{E}[\\|\\nabla\\log p_{T-t_{j}^{\\star}}(Y_{t_{j}})-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}]d t}\\\\ &{\\le2d\\displaystyle\\sum_{j=0}^{N-1}\\gamma_{j}\\int_{T-t_{j+1}^{\\prime}}^{T-t_{j}^{\\star}}\\sigma_{t}^{4}\\bar{\\sigma}_{t}^{-4}d t+\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\star}}^{t_{j+1}^{\\prime}}\\sigma_{T-t}^{2}d t\\bar{\\sigma}_{T-t_{j}^{\\star}}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j}^{\\star}}(X_{T-t_{j}^{\\star}}))]}\\\\ &{\\qquad-\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{\\star}}^{t_{j+1}}\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}d t\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j+1}^{\\prime}}(X_{T-t_{j+1}^{\\prime}}))]}\\\\ &{=2d\\displaystyle\\sum_{j=0}^{N-1}\\gamma_{j}\\int_{T-t_{j+1}^{\\prime}}^{T-t_{j}^{\\star}}\\sigma_{t}^{4}\\bar{\\sigma}_{t}^{-4}d t+\\displaystyle\\int_{0}^{t_{1}^{\\prime}}\\sigma_{T-t}^{2}d t\\bar{\\sigma}_{T}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T}(X_{T}))]}\\\\ &{\\qquad+\\displaystyle\\sum_{j=1}^{N-1}\\Big(\\int_{t_{j}^{\\star}}^{t_{j+1}}\\sigma_{T-t_{j}^{\\star}}^{2}\\bar{\\sigma}_{T-t_{j}^{\\star}}^{-4}d t-\\int_{t_{j-1}^{\\prime}}^{t_{j}^{\\star}}\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}d t\\Big)\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j}^{\\star}}(X_{T-t_{j}^{\\star}}))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The above bound depends on $\\mathbb{E}[\\operatorname{tr}(\\Sigma_{t}(X_{t}))]$ , hence we estimate $\\mathbb{E}[\\operatorname{tr}(\\Sigma_{t}(X_{t}))]$ for different values of $t$ ", "page_idx": 28}, {"type": "text", "text": "First, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathrm{tr}(\\Sigma_{t}(X_{t}))]=\\mathbb{E}[\\mathbb{E}[\\|X_{0}\\|^{2}|X_{t}]-\\|\\mathbb{E}[X_{0}|X_{t}]\\|^{2}]\\leq\\mathbb{E}[\\|X_{0}\\|^{2}]=\\mathrm{m}_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Meanwhile, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{tr}(\\Sigma_{t}(X_{t}))]=\\mathbb{E}[\\mathrm{tr}(\\Sigma\\mathrm{ov}(X_{0}-X_{t}|X_{t}))]=\\mathbb{E}[\\mathbb{E}[\\|X_{0}-X_{t}\\|^{2}|X_{t}]]-\\|\\mathbb{E}[X_{0}-X_{t}|X_{t}]\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}[\\|X_{0}-X_{t}\\|^{2}]=\\bar{\\sigma}_{t}^{2}d}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, $\\mathbb{E}[\\mathrm{tr}\\big(\\Sigma_{t}(X_{t})\\big)]\\le\\operatorname*{min}(\\mathrm{m}_{2}^{2},\\bar{\\sigma}_{t}^{2}d)\\lesssim\\big(1-e^{-\\bar{\\sigma}_{t}^{2}}\\big)\\big(\\mathrm{m}_{2}^{2}+d\\big)$ . Plug this estimation into (23) and we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{j=0}^{N-1}\\int_{t_{j}^{-}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\mathbb{E}[\\|\\nabla\\log p_{T-t_{j}^{-}}(Y_{t_{j}^{-}})-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}]d t}}\\\\ {{\\displaystyle\\leqslant d\\sum_{j=0}^{N-1}\\gamma_{j}\\int_{T-t_{j+1}^{-}}^{T-t_{j}^{-}}\\sigma_{t}^{4}\\bar{\\sigma}_{t}^{-4}d t+\\int_{0}^{t_{1}^{+}}\\sigma_{T-t}^{2}d t\\bar{\\sigma}_{T}^{-4}\\mathfrak{m}_{2}^{2}}}\\\\ {{\\displaystyle\\;\\;+\\left(\\mathbf{m}_{2}^{2}+d\\right)\\sum_{j=1}^{N-1}(1-e^{-\\bar{\\sigma}_{T-t_{j}^{-}}^{2}})\\Big(\\int_{t_{j}^{+}}^{t_{j+1}^{-}}\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t_{j}^{-}}^{-4}d t-\\int_{t_{j-1}^{-}}^{t_{j}^{-}}\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}d t\\Big)}}\\\\ {{\\displaystyle\\leqslant d\\sum_{j=0}^{N-1}\\gamma_{j}\\int_{T-t_{j+1}^{-}}^{T-t_{j}^{-}}\\sigma_{t}^{4}\\bar{\\sigma}_{t}^{-4}d t+\\operatorname*{m}_{2}^{2}\\frac{\\int_{t_{j}^{+}}^{t_{1}^{-}}\\sigma_{T-t}^{2}d t}{\\bar{\\sigma}_{T}^{4}}++\\left(\\mathbf{m}_{2}^{2}+d\\right)\\sum_{k=1}^{N-1}\\left(1-e^{-\\bar{\\sigma}_{T-t_{j}^{-}}^{2}}\\right)\\frac{\\bar{\\sigma}_{T-t_{j}^{-}}^{4}-\\bar{\\sigma}_{T-t_{j+1}^{-}}^{2}\\bar{\\sigma}_{T-t_{j-1}^{-}}^{2}}{\\bar{\\sigma}_{T-t_{j-1}^{-}}^{2}\\bar{\\sigma}_{T-t_{j-1}^{-}}^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality follows from the definition of $\\bar{\\sigma}_{t}$ and integration by parts. The proof of Theorem 2 is completed. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma 9. Let $\\{Y_{t}\\}_{0\\leq t\\leq T}$ be the solution to (2) with $f_{t}\\,\\equiv\\,0$ and $p_{t+s|s}^{\\leftarrow}(\\cdot|y)$ be the conditional distribution of $Y_{s+t}$ given $\\{Y_{s}=y\\}$ . Let $\\{\\bar{Y}_{t}\\}_{0\\leq t\\leq T}$ be the solution to (11) $q_{t+s|s}(\\cdot|y)$ be the conditional distribution of $\\bar{Y}_{s+t}$ given $\\{\\bar{Y}_{s}=y\\}$ . Then for any fixed $t\\in(0,\\gamma_{j}]$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{y\\sim p_{t_{j}^{-}}^{-}K L}(p_{t_{j}^{-}+t|t_{j}^{+}}^{+}(\\cdot|y)|q_{t_{j}^{-}+t|t_{j}^{+}}(\\cdot|y))\\leq\\frac{1}{2}\\sigma_{T-t}^{2}\\mathbb{E}\\big[\\|s(\\theta;T-t_{j}^{-},Y_{t_{j}^{-}})-\\nabla\\log p_{T-t}(Y_{t})\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 9. According to [12, Lemma 6], we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{KL}(p_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(\\cdot|y)|q_{t_{j}^{+}+t|t_{j}^{+}}(\\cdot|y))}\\\\ &{\\leq-2\\sigma_{T-t}^{2}\\int p_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(x|y)\\,\\|\\nabla\\log\\frac{p_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(x|y)}{q_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(x|y)}\\|^{2}d x}\\\\ &{\\quad+\\,2\\sigma_{T-t}^{2}\\mathbb{E}_{p_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(x|y)}[\\langle\\nabla\\log p_{T-t}(x)-s(\\theta;T-t_{j}^{+},Y_{t_{j}^{+}}),\\nabla\\log\\frac{p_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(x|y)}{q_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(x|y)}\\rangle]}\\\\ &{\\leq\\frac{1}{2}\\sigma_{T-t}^{2}\\mathbb{E}_{p_{t_{j}^{+}+t|t_{j}^{+}}^{\\leftarrow}(x|y)}[\\|\\nabla\\log p_{T-t}(x)-s(\\theta;T-t_{j}^{+},Y_{t_{j}^{+}})\\|^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from Young's inequality. Therefore, Lemma 9 is proved after taking another expectation. ", "page_idx": 29}, {"type": "text", "text": "Lemma 10. For any probability distribution $p$ satisfyingAssumption $^3$ and $q$ being a centered multivariate normal distribution with covariance matrix $\\sigma^{\\breve{2}}I_{d}$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\nK L(p*q|q)\\leq\\frac{\\mathrm{m}_{2}^{2}}{2\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma $I O$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(p*q|q)\\leq\\int\\mathrm{KL}(q(\\cdot-y)|q(\\cdot))p(d y)=\\int\\mathrm{KL}(N(y,\\sigma^{2}I_{d})|N(0,\\sigma^{2}I_{d}))p(d y)}\\\\ &{\\qquad\\qquad=\\cfrac{1}{2}\\int\\ln(1)-d+\\mathrm{tr}(I_{d})+\\|y\\|^{2}\\sigma^{-2}p(d y)=\\cfrac{\\operatorname*{m}_{2}^{2}}{2\\sigma^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the inequality follows from convexity of $\\operatorname{KL}(\\cdot|q)$ and the second identity follows from KLdivergence between multivariate normal distributions. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma 11. Let $\\{X_{t}\\}_{0\\le t\\le T}$ be the solution to (1) with $f_{t}\\,\\equiv\\,0$ and $p_{0|t}(\\cdot|x)$ be the conditional distribution of $X_{0}$ given $\\{X_{t}=x\\}$ . Define ", "page_idx": 29}, {"type": "equation", "text": "$$\nm_{t}(X_{t}):=\\mathbb{E}_{X\\sim p_{0\\mid t}(\\cdot|X_{t})}[X],\\quad\\Sigma_{t}(X_{t})=C o\\nu_{X\\sim p_{0\\mid t}(\\cdot|X_{t})}(X).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\{Y_{t}\\}_{0\\leq t\\leq T}$ be the solution to (2) with $f_{t}\\equiv0$ and $q_{0|t}(\\cdot|x)$ be the conditional distribution of $Y_{0}$ given $\\{Y_{t}=x\\}$ . Define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\bar{m}_{t}(Y_{t}):=\\mathbb{E}_{X\\sim q_{0\\mid t}(\\cdot|Y_{t})}[X],\\quad\\bar{\\Sigma}_{t}(Y_{t})=C o\\nu_{X\\sim q_{0\\mid t}(\\cdot|Y_{t})}(X).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we have for all $t\\in(0,T)$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{d\\bar{m}_{t}(Y_{t})=\\sqrt{2}\\sigma_{T-t}\\bar{\\sigma}_{T-t}^{-2}\\bar{\\Sigma}_{t}(Y_{t})d\\tilde{W}_{t},}\\\\ &{}&{a n d\\quad\\quad\\frac{d}{d t}\\mathbb{E}[\\Sigma_{t}(X_{t})]=2\\sigma_{t}^{2}\\bar{\\sigma}_{t}^{-4}\\mathbb{E}\\big[\\Sigma_{t}(X_{t})^{2}\\big].\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma $_{l l}$ .We first represent $\\nabla l o g p_{t}(X_{t})$ and $\\nabla^{2}\\log{p_{t}(X_{t})}$ via $m_{t}(X_{t})$ and $\\Sigma_{t}(X_{t})$ Since $\\{X_{t}\\}_{0\\le t\\le T}$ solves (1), $X_{t}=X_{0}+\\bar{\\sigma}_{t}\\xi$ with $(X_{0},\\xi)\\sim p\\otimes\\mathcal{N}(0,I_{d})$ . Therefore, according to Bayes rule, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla\\log p_{t}\\bigl(X_{t}\\bigr)=\\frac{1}{p_{t}\\bigl(X_{t}\\bigr)}\\int\\nabla\\log p_{t|0}(X_{t}|x)p_{0,t}(x,X_{t})d x}\\ }\\\\ &{=\\mathbb{E}_{x\\sim p_{0\\mid t}(\\cdot|X_{t})}\\bigl[\\bar{\\sigma}_{t}^{-2}(X_{t}-x)\\bigr]}\\\\ &{=-\\bar{\\sigma}_{t}^{-2}\\bigl(X_{t}-m_{t}(X_{t})\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second identity follows from the fact that $p_{t|0}(\\cdot|x)=\\mathcal{N}(x,\\bar{\\sigma}_{t}^{2}I_{d})$ . The last identity follows from the definition of $m_{t}(X_{t})$ in Lemma 11. Similarly, according to Bayes rule, we can compute ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla^{2}\\log p_{t}(X_{t})}\\\\ &{=\\cfrac{1}{p_{t}(X_{t})}\\int\\nabla^{2}\\log p_{t[0}(X_{t}|x)p_{0,t}(x,X_{t})d x}\\\\ &{\\quad+\\cfrac{1}{p_{t}(X_{t})}\\int\\big(\\nabla\\log p_{t[0}(X_{t}|x)\\big)\\big(\\nabla\\log p_{t[0}(X_{t}|x)\\big)^{\\top}p_{0,t}(x,X_{t})d x}\\\\ &{\\quad-\\cfrac{1}{p_{t}(X_{t})^{2}}\\big(\\int\\nabla\\log p_{t[0}(X_{t}|x)p_{0,t}(x,X_{t})d x\\big)\\big(\\int\\nabla\\log p_{t[0}(X_{t}|x)p_{0,t}(x,X_{t})d x\\big)^{\\top}}\\\\ &{=-\\bar{\\sigma}_{t}^{-2}{\\cal I}_{d}+\\bar{\\sigma}_{t}^{-4}\\Sigma_{t}(X_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second identity follows from the fact that $p_{t|0}(\\cdot|x)\\,=\\mathcal{N}(x,\\bar{\\sigma}_{t}^{2}I_{d})$ and the definition of $\\Sigma_{t}(X_{t})$ in Lemma 11. ", "page_idx": 30}, {"type": "text", "text": "According to Bayes rule, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\np_{0|t}(d x|X_{t})\\propto\\exp(-\\frac{1}{2}\\frac{\\|X_{t}-x\\|^{2}}{\\bar{\\sigma}_{t}^{2}})p(d x)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{q_{0\\mid t}\\big(d x\\vert Y_{t}\\big)={\\cal Z}^{-1}\\exp(-\\displaystyle\\frac{1}{2}\\frac{\\|Y_{t}-x\\|^{2}}{\\bar{\\sigma}_{T-t}^{2}})p(d x)}}\\\\ {{\\qquad\\qquad\\qquad={\\cal Z}_{t}^{-1}\\exp(-\\displaystyle\\frac{1}{2}\\bar{\\sigma}_{T-t}^{-2}\\|x\\|^{2}+\\bar{\\sigma}_{T-t}^{-2}\\langle x,Y_{t}\\rangle)p(d x)}}\\\\ {{\\qquad\\qquad:={\\cal Z}_{t}^{-1}\\exp(h_{t}(x))p(d x),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\begin{array}{r}{Z_{t}=\\int\\exp(h_{t}(x))p(d x)}\\end{array}$ is a (random) normalization constant. From the above computations, we can see that $q_{0|t}(d x|Y_{t})=p_{0|T-t}(d x|X_{T-t})$ for all $t\\in[0,T]$ . Therefore, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\bar{m}_{t}(Y_{t})=\\mathbb{E}_{X\\sim q_{0\\mid t}(\\cdot|Y_{t})}[X]=m_{T-t}(X_{T-t}),\\quad\\bar{\\Sigma}_{t}(Y_{t})=\\operatorname{Cov}_{X\\sim q_{0\\mid t}(\\cdot|Y_{t})}(X)=\\Sigma_{T-t}(X_{T-t}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the identities hold in distribution. Therefore, to prove the first statement, it suffices to compute $d{\\bar{m}}_{t}(Y_{t})$ . To do so, we first compute $d h_{t}(x),d[h(x),h(x)]_{t},d Z_{t}$ and $d\\log{Z_{t}}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d h_{t}(x)=\\bar{\\sigma}_{T-t}^{-3}\\dot{\\bar{\\sigma}}_{T-t}\\|x\\|^{2}d t-2\\bar{\\sigma}_{T-t}^{-3}\\dot{\\bar{\\sigma}}_{T-t}\\bar{\\sigma}\\langle x,Y_{t}\\rangle d t+\\bar{\\sigma}_{T-t}^{-2}\\langle x,d Y_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "According to the definition of $Y_{t}$ and (26), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d Y_{t}=2\\sigma_{T-t}^{2}\\nabla\\log p_{T-t}(Y_{t})d t+\\sqrt{2\\sigma_{T-t}^{2}}d\\tilde{W}_{t}}\\\\ &{\\qquad=-2\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-2}\\big(Y_{t}-\\bar{m}_{t}(Y_{t})\\big)d t+\\sqrt{2\\sigma_{T-t}^{2}}d\\tilde{W}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore ", "page_idx": 30}, {"type": "equation", "text": "$$\nd[h(x),h(x)]_{t}=\\bar{\\sigma}_{T-t}^{-4}|x|^{2}[d Y,d Y]_{t}=2\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}\\|x\\|^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Apply (29) and (30) and we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle d Z_{t}=\\int\\exp(h_{t}(x))\\big(d h_{t}(x)+\\frac{1}{2}d[h(x),h(x)]_{t}\\big)p(d x)}\\\\ &{\\qquad=\\bar{\\sigma}_{T-t}^{-3}\\dot{\\bar{\\sigma}}_{T-t}\\mathbb{E}_{q_{0|t}(\\cdot|Y_{t})}[\\|x\\|^{2}]Z_{t}d t-2\\bar{\\sigma}_{T-t}^{-3}\\dot{\\bar{\\sigma}}_{T-t}\\langle Y_{t},\\bar{m}_{t}(Y_{t})\\rangle Z_{t}d t}\\\\ &{\\qquad\\,+\\,\\bar{\\sigma}_{T-t}^{-2}\\langle\\bar{m}_{t}(Y_{t}),d Y_{t}\\rangle Z_{t}+\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}\\mathbb{E}_{q_{0|t}(\\cdot|Y_{t})}[\\|x\\|^{2}]Z_{t}d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{d\\log Z_{t}=Z_{t}^{-1}d Z_{t}-\\frac{1}{2}Z_{t}^{-2}d[Z,Z]_{t}}}\\\\ {\\displaystyle{\\qquad\\qquad=-2\\bar{\\sigma}_{T-t}^{-3}\\dot{\\bar{\\sigma}}_{T-t}\\langle Y_{t},\\bar{m}_{t}(Y_{t})\\rangle d t+\\bar{\\sigma}_{T-t}^{-2}\\langle\\bar{m}_{t}(Y_{t}),d Y_{t}\\rangle-\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}\\|\\bar{m}_{t}(Y_{t})\\|^{2}d t.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "I we furthe define $\\begin{array}{r}{R_{t}(Y_{t}):=\\frac{q_{0|t}(d x|Y_{t})}{p(d x)}=Z_{t}^{-1}\\exp(h_{t}(x))}\\end{array}$ We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d R_{t}(Y_{t})=d\\exp(\\log R_{t}(Y_{t}))=R_{t}(Y_{t})d(\\log R_{t}(Y_{t}))+\\frac{1}{2}R_{t}(Y_{t})d[\\log R_{t}(Y_{t}),\\log R_{t}(Y_{t})]}\\\\ {=-R_{t}(Y_{t})d(\\log Z_{t})+R_{t}(Y_{t})d h_{t}(x)+\\frac{1}{2}R_{t}(Y_{t})d[h_{t}(x)-\\log Z_{t},h_{t}(x)-\\log Z_{t}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "With (29), (30), (31), (32) and (33), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\bar{m}_{t}(Y_{t})=d\\displaystyle\\int x R_{t}(Y_{t})p(d x)}\\\\ &{\\qquad\\qquad=\\displaystyle\\int x\\big(-d(\\log Z_{t})+d h_{t}(x)+\\frac{1}{2}d[h_{t}(x)-\\log Z_{t},h_{t}(x)-\\log Z_{t}]\\big)q_{0|t}(d x|Y_{t})}\\\\ &{\\qquad\\qquad=\\sqrt{2}\\sigma_{T-t}\\bar{\\sigma}_{T-t}^{-2}\\bar{\\Sigma}_{t}(Y_{t})d\\tilde{W}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where most terms cancel in the last identity. Therefore, the first statement is proved. Next, we prove the second statement. We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\cfrac{d}{d t}\\mathbb{E}[\\Sigma_{T-t}(X_{T-t})]=\\cfrac{d}{d t}\\mathbb{E}[\\bar{\\Sigma}_{t}(Y_{t})]=\\cfrac{d}{d t}\\mathbb{E}[\\Sigma_{T-t}(X_{T-t})]}&{}\\\\ {=\\cfrac{d}{d t}\\mathbb{E}_{Y_{t}\\sim p_{T-t}}[\\mathbb{E}_{q_{0|t}(\\cdot|Y_{t})}[x^{\\otimes}2]-\\bar{m}_{t}(Y_{t})^{\\otimes}2]}&{}\\\\ {=\\cfrac{d}{d t}\\mathbb{E}_{q_{0}}[x^{\\otimes}2]-\\cfrac{d}{d t}\\mathbb{E}[\\bar{m}_{t}(Y_{t})^{\\otimes}2]}&{}\\\\ {=-\\mathbb{E}[-2\\bar{m}_{t}(Y_{t})d\\bar{m}_{t}(Y_{t})^{\\top}+d[\\bar{m}_{t}(Y_{t}),\\bar{m}_{t}(Y_{t})^{\\top}]]}&{}\\\\ {=2\\sigma_{T-t}^{-3}\\bar{\\sigma}_{T-t}\\mathbb{E}[\\bar{\\Sigma}_{t}(Y_{t})^{2}]d t}&{}\\\\ {=-2\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}\\mathbb{E}[\\Sigma_{t}(X_{T-t})^{2}],}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the second last identity follows from (34) and the last identity follows from the definition of $\\bar{\\sigma}_{t}$ Last, we reverse the time and get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathbb{E}\\big[\\Sigma_{t}(X_{t})\\big]=2\\sigma_{t}^{2}\\bar{\\sigma}_{t}^{-4}\\mathbb{E}\\big[\\Sigma_{t}(X_{t})^{2}\\big].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The proof is completed. ", "page_idx": 31}, {"type": "text", "text": "Lemma 12. Under the conditions in Lemma $_{l l}$ let $\\{Y_{t}\\}_{0\\leq t\\leq T}$ be the solution to (2) with $f_{t}\\equiv0$ Define $L_{t}:=\\nabla\\log p_{T-t}(Y_{t})$ then for any $t\\in[t_{j}^{\\leftarrow},t_{j+1}^{\\leftarrow})$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathfrak{T}\\big[\\|L_{t}-L_{t_{j}^{-}}\\|^{2}\\big]=2d\\int_{t_{j}^{-}}^{t}\\sigma_{T-u}^{2}\\bar{\\sigma}_{T-u}^{-4}d u+\\bar{\\sigma}_{T-t_{j}^{-}}^{-4}\\mathbb{E}[\\operatorname{tr}\\big(\\Sigma_{T-t_{j}^{-}}(X_{T-t_{j}^{-}})\\big)]-\\bar{\\sigma}_{T-t}^{-4}\\mathbb{E}[\\operatorname{tr}\\big(\\Sigma_{T-t}(X_{T-t})\\big)]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $^{12}$ .First, according to the defnition of $L_{t}$ and $Y_{t}$ , it follows from Ito's lemma that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d L_{t}=\\nabla^{2}\\log p_{T-t}(Y_{t})\\big(2\\sigma_{T-t}^{2}\\nabla\\log p_{T-t}(Y_{t})d t+\\sqrt{2\\sigma_{T-t}}d\\tilde{W}_{t}\\big)}\\\\ &{\\qquad+\\,\\Delta\\big(\\nabla\\log p_{T-t}(Y_{t})\\big)\\sigma_{T-t}^{2}d t+\\frac{d\\big(\\nabla\\log p_{T-t}\\big)}{d t}(Y_{t})d t}\\\\ &{\\qquad=\\sqrt{2\\sigma_{T-t}^{2}}\\nabla^{2}\\log p_{T-t}(Y_{t})d\\tilde{W}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last step follows from applying the Fokker Planck equation of (1) with $f_{t}~\\equiv~0$ ,i.e., $\\partial_{t}p_{t}=\\sigma_{t}^{2}\\Delta p_{t}$ . Most of the terms are cancelled after applying the Fokker Planck equation. Now, for fixed $s>0$ and $t>s$ ,define $E_{s,t}:=\\mathbb{E}[\\Vert L_{t}-L_{s}\\Vert^{2}]$ . Apply Ito's lemma and (35), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d E_{s,t}=2\\mathbb{E}[\\left\\langle L_{t}-L_{s},d L_{t}\\right\\rangle]+d[L]_{t}}\\\\ &{\\qquad=2\\mathbb{E}[\\left\\langle L_{t}-L_{s},\\sqrt{2\\sigma_{T-t}^{2}}\\nabla\\log p_{T-t}(Y_{t})d\\tilde{W}_{t}\\right\\rangle]+2\\sigma_{T-t}^{2}\\mathbb{E}[\\left\\|\\nabla^{2}\\log p_{T-t}(Y_{t})\\right\\|_{F}^{2}]d t}\\\\ &{\\qquad=2\\sigma_{T-t}^{2}\\mathbb{E}[\\left\\|\\nabla^{2}\\log p_{T-t}(Y_{t})\\right\\|_{F}^{2}]d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\|A\\|_{F}$ denotes the Frobenius norm of any matrix $A$ . According to (27), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\cfrac{d E_{s,t}}{d t}=2\\sigma_{T-t}^{2}\\mathbb{E}[\\|\\nabla^{2}\\log p_{T-t}(Y_{t})\\|_{F}^{2}]=2\\sigma_{T-t}^{2}\\mathbb{E}[\\|\\nabla^{2}\\log p_{T-t}(X_{T-t})\\|_{F}^{2}]}\\\\ &{\\quad=2\\sigma_{T-t}^{2}\\mathbb{E}[\\|-\\bar{\\sigma}_{T-t}^{-2}I_{d}+\\bar{\\sigma}_{T-t}^{-4}\\Sigma_{T-t}(X_{T-t})\\|_{F}^{2}]}\\\\ &{\\quad=2d\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}-4\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-6}\\mathbb{E}[\\operatorname{tr}(\\Sigma_{T-t}(X_{T-t}))]+2\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-8}\\mathbb{E}[\\operatorname{tr}(\\Sigma_{T-t}(X_{T-t})^{2})]}\\\\ &{\\quad=2d\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-4}-4\\sigma_{T-t}^{2}\\bar{\\sigma}_{T-t}^{-6}\\mathbb{E}[\\operatorname{tr}(\\Sigma_{T-t}(X_{T-t}))]-\\bar{\\sigma}_{T-t}^{-4}\\cfrac{d}{d t}\\mathbb{E}[\\operatorname{tr}(\\Sigma_{T-t}(X_{T-t}))],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last identity follows from the proof of Lemma 11. Therefore, for any $t\\in[t_{j}^{\\leftarrow},t_{j+1}^{\\leftarrow})$ ,we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t_{j}^{\\star},t}^{\\leftarrow}=2d\\int_{t_{j}^{\\star}}^{t}\\sigma_{T-u}^{2}\\widetilde{\\sigma}_{T-u}^{-4}d u-4\\int_{t_{j}^{\\star}}^{t}\\sigma_{T-u}^{2}\\widetilde{\\sigma}_{T-u}^{-6}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-u}(X_{T-u}))]d u}\\\\ &{\\qquad-\\int_{t_{j}^{\\star}}^{t}\\widetilde{\\sigma}_{T-u}^{-4}\\frac{d}{d u}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-u}(X_{T-u}))]}\\\\ &{=2d\\int_{t_{j}^{\\star}}^{t}\\sigma_{T-u}^{2}\\widetilde{\\sigma}_{T-u}^{-4}d u-4\\int_{t_{j}^{\\star}}^{t}\\sigma_{T-u}^{2}\\widetilde{\\sigma}_{T-u}^{-6}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-u}(X_{T-u}))]d u}\\\\ &{\\qquad-\\partial_{T-t_{j}^{\\star}}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t}(X_{T-t}))]+\\partial_{T-t_{j}^{\\star}}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j}^{\\star}}(X_{T-t_{j}^{\\star}}))]}\\\\ &{\\qquad+4\\int_{t_{j}^{\\star}}^{t}\\sigma_{T-u}^{2}\\widetilde{\\sigma}_{T-u}^{-6}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-u}(X_{T-u}))]}\\\\ &{=2d\\int_{t_{j}^{\\star}}^{t}\\sigma_{T-u}^{2}\\widetilde{\\sigma}_{T-u}^{-4}d u-\\widetilde{\\sigma}_{T-t_{j}}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t}(X_{T-t}))]+\\widetilde{\\sigma}_{T-t_{j}^{\\star}}^{-4}\\mathbb{E}[\\mathrm{tr}(\\Sigma_{T-t_{j}^{\\star}}(X_{T-t_{j}^{\\star}}))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The proof is completed. ", "page_idx": 32}, {"type": "text", "text": "F  Sampling error for Gaussian data distributions ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we consider a special case when the data distribution is a mixture of Gaussians, i.e., ", "page_idx": 32}, {"type": "equation", "text": "$$\np(x)=\\mathcal{N}(x;m,\\sigma^{2}I_{d}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\mathcal{N}(x;m,\\sigma^{2}I_{d})$ is the density of Gaussian random vector with mean $m$ and covariance $\\sigma^{2}I_{d}$ In this case, the score function $\\nabla\\log p_{t}(x)$ can be explicitly calculated from any $t>0$ , see Lemma 14. Therefore, the sampling process (11) can be implemented with zero score estimation error via the following piecewise SDE: for any $t\\in[t_{j}^{\\leftarrow},t_{j+1}^{\\leftarrow})$ \uff0c ", "page_idx": 32}, {"type": "equation", "text": "$$\nd\\bar{Y}_{t}=2\\sigma_{T-t}^{2}\\nabla\\log p_{T-t_{j}^{leftarrow}}(\\bar{Y}_{t_{j}^{leftarrow}})d t+\\sqrt{2\\sigma_{T-t}^{2}}d\\bar{W}_{t}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The iterates, $(\\bar{Y}_{t_{j}^{\\leftarrow}})$ , are all Gaussians with explicit means and covariance matrices, see Lemma 15. As a consequence, we can quantify the quantity, $\\mathrm{KL}(p_{\\delta}|q_{T-\\delta})$ in Theorem 2 explicitly since both distributions are Gaussians. ", "page_idx": 32}, {"type": "text", "text": "Lemma 13 (KL-divergence error for Gaussian data distribution). Assume the data distribution has density given by (39). Let $(\\bar{Y}_{t_{j}^{\\leftarrow}})_{j=0}^{N}$ be defined by (40) with initial condition $\\bar{Y}_{0}\\,\\sim\\,{\\mathcal N}(0,\\bar{\\sigma}_{T}^{2}I_{d})$ Denote $q_{t}=L a w(\\bar{Y}_{t})$ for all $0\\leq t\\leq T-\\delta$ Then ", "page_idx": 32}, {"type": "equation", "text": "$$\nK L{\\left(p_{\\delta}|q_{T-\\delta}\\right)}=\\frac{d}{2}(E_{\\sigma}-1-\\log E_{\\sigma})+\\|m\\|^{2}\\frac{(\\sigma^{2}+\\bar{\\sigma}_{T}^{2})^{2}}{\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2}}E_{\\sigma},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $E_{\\sigma}$ is a positive constant depending on the variance schedule $(\\bar{\\sigma}_{T-t_{j}^{\\leftarrow}})_{j=0}^{N}$ givenby ", "page_idx": 32}, {"type": "equation", "text": "$$\nE_{\\sigma}^{-1}=\\frac{\\big(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2}\\big)\\bar{\\sigma}_{T}^{2}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T}^{2}\\big)^{2}}+\\sum_{j=0}^{N-1}\\frac{\\big(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2}\\big)\\big(\\bar{\\sigma}_{T-t_{j}^{\\leftarrow}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{\\leftarrow}}^{2}\\big)}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j+1}^{\\leftarrow}}^{2}\\big)^{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma $^{l3}$ $p_{\\delta}=p\\!\\ast\\!\\mathcal{N}\\big(0,\\bar{\\sigma}_{\\delta}^{2}\\big)=\\mathcal{N}\\big(m,\\big(\\sigma^{2}\\!+\\!\\bar{\\sigma}_{\\delta}^{2}\\big)I_{d}\\big)$ and $q_{T-\\delta}=\\mathrm{Law}(\\bar{Y}_{t_{N}^{\\leftarrow}})=\\mathcal{N}(m_{N},\\Sigma_{N})$ Wwith $(m_{N},\\Sigma_{N})$ given in Lemma 15. Therefore, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{KL}(p_{\\delta}|q_{T-\\delta})=\\mathrm{KL}(\\mathcal{N}(m,(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})I_{d})|\\mathcal{N}(m,\\Sigma_{N}))}\\\\ &{=\\frac{1}{2}\\log\\frac{\\operatorname*{det}(\\sum_{N})}{\\operatorname*{det}((\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})I_{d})}-\\frac{d}{2}\\,+\\frac{1}{2}\\operatorname{tr}((\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})\\Sigma_{N}^{-1})+(m_{N}-m)^{\\top}\\Sigma_{N}^{-1}(m-m_{N})}\\\\ &{=\\frac{d}{2}\\log\\left(\\frac{(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})\\bar{\\sigma}_{\\delta}^{2}}{(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})^{2}}+\\frac{N-1}{\\int_{0}}\\frac{(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})(\\bar{\\sigma}_{T-\\delta}^{2}-\\bar{\\sigma}_{T-\\varepsilon_{\\delta+1}^{2}}^{2})}{(\\sigma^{2}+\\bar{\\sigma}_{T-\\varepsilon_{\\delta+1}^{2}}^{2})^{2}}\\right)-\\frac{d}{2}}\\\\ &{\\quad+\\frac{d}{2}\\left(\\frac{(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})\\bar{\\sigma}_{T}^{2}}{(\\sigma^{2}+\\bar{\\sigma}_{T}^{2})^{2}}+\\frac{N-1}{\\int_{0}}\\frac{(\\sigma^{2}+\\bar{\\sigma}_{\\delta}^{2})(\\bar{\\sigma}_{T-\\varepsilon_{\\delta}^{2}}^{2}-\\bar{\\sigma}_{T-\\varepsilon_{\\delta+1}^{2}}^{2})}{(\\sigma^{2}+\\bar{\\sigma}_{T-\\varepsilon_{\\delta+1}^{2}}^{2})^{2}}\\right)^{-1}}\\\\ &{\\quad+\\left(\\bar{\\sigma}_{T}^{2}+(\\sigma^{2}+\\bar{\\sigma}_{T}^{2})^{2}\\sum_{j=0}^{N-1}\\frac{(\\sigma^{2}-\\underline{{t}}_{r+\\varepsilon_{j}^{2}}-\\bar{\\sigma}_{T- \n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 14 (Explicit score function for mixture of Gaussian target). Assume the data distribution has density given by (39), then the score function is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nabla\\log p_{t}(x)=-\\frac{x-m}{\\sigma^{2}+\\bar{\\sigma}_{t}^{2}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Since the forward process (1) with $f_{t}\\equiv0$ is the just a process that keeps adding noise, the density $p_{t}$ along the proces is a convolution between data density and a Gaussian density with mean zero and covariance $\\dot{\\sigma}_{t}^{2}I_{d}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\np_{t}(x)=p*\\mathcal{N}(\\cdot\\,;0,\\bar{\\sigma}_{t}^{2}I_{d})(x)=\\mathcal{N}(x;m,(\\sigma^{2}+\\bar{\\sigma}_{t}^{2})I_{d}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{{\\nabla p_{t}\\big(x\\big)=\\big(2\\pi(\\sigma^{2}+\\bar{\\sigma}_{t}^{2})\\big)^{-d/2}\\exp\\big(-\\displaystyle\\frac{\\|x-m\\|^{2}}{2\\big(\\sigma^{2}+\\bar{\\sigma}_{t}^{2}\\big)}\\big)\\big(-\\frac{x-m}{\\sigma^{2}+\\bar{\\sigma}_{t}^{2}}\\big)}}\\\\ {{=-\\displaystyle\\frac{x-m}{\\sigma^{2}+\\bar{\\sigma}_{t}^{2}}{\\mathcal{N}}\\big(x;m,\\big(\\sigma^{2}+\\bar{\\sigma}_{t}^{2}\\big)I_{d}\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "(42) follows directly from the above computations. ", "page_idx": 33}, {"type": "text", "text": "Lemma 15 (Gaussian iterates along the trajectory). Assume the data distribution has density given by (39). Let $(\\bar{Y}_{t_{j}^{\\leftarrow}})$ be defined by (40) with initial condition $\\bar{Y}_{0}\\sim\\mathcal N(0,\\bar{\\sigma}_{T}^{2}I_{d})$ .Thenforall $0\\leq j\\leq N$ $\\bar{Y}_{t_{j}^{\\leftarrow}}\\sim\\mathcal{N}(m_{j},\\bar{\\Sigma_{j}})$ With ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{j}=\\displaystyle\\frac{\\bar{\\sigma}_{T-t_{0}^{\\leftarrow}}^{2}+\\bar{\\sigma}_{T-t_{j}^{\\leftarrow}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{0}^{\\leftarrow}}^{2}}m,}\\\\ &{\\Sigma_{j}=\\displaystyle\\Bigg(\\frac{(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\leftarrow}}^{2})^{2}\\bar{\\sigma}_{T}^{2}}{(\\sigma^{2}+\\bar{\\sigma}_{T-t_{0}^{\\leftarrow}}^{2})^{2}}+\\displaystyle\\sum_{l=0}^{j-1}\\frac{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\leftarrow}}^{2}\\big)^{2}\\big(\\bar{\\sigma}_{T-t_{l}^{\\leftarrow}}^{2}-\\bar{\\sigma}_{T-t_{l+1}^{\\leftarrow}}^{2}\\big)}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{l+1}^{\\leftarrow}}^{2}\\big)^{2}}\\Bigg)I_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma 15. According to Lemma 14, (40) can be written as ", "page_idx": 33}, {"type": "equation", "text": "$$\nd\\bar{Y}_{t}=-2\\sigma_{T-t}^{2}\\frac{\\bar{Y}_{t_{j}^{\\leftarrow}}-m}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\leftarrow}}^{2}}d t+\\sqrt{2\\sigma_{T-t}^{2}}d\\bar{W}_{t},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which implies that for any $j\\in{0,1,\\cdots,N-1}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Y}_{t_{j+1}^{+}}-\\bar{Y}_{t_{j}^{+}}=-2\\frac{\\int_{t_{j}^{+}}^{t_{j+1}^{+}}\\sigma_{T-t}^{2}d t}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{+}}^{2}}\\big(\\bar{Y}_{t_{j}^{+}}-m\\big)+\\sqrt{2\\int_{t_{j}^{+}}^{t_{j+1}^{+}}\\sigma_{T-t}^{2}d t U_{j+1}}}\\\\ &{\\quad\\quad\\quad\\quad=-\\frac{\\bar{\\sigma}_{T-t_{j}^{+}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{+}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{+}}^{2}}\\big(\\bar{Y}_{t_{j}^{+}}-m\\big)+\\sqrt{\\bar{\\sigma}_{T-t_{j}^{+}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{+}}^{2}}U_{j+1},}\\\\ {\\implies\\quad}&{\\bar{Y}_{t_{j+1}^{+}}=\\big(1-\\frac{\\bar{\\sigma}_{T-t_{j}^{+}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{+}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{+}}^{2}}\\big)\\bar{Y}_{t_{j}^{+}}+\\frac{\\bar{\\sigma}_{T-t_{j}^{+}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{+}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{t_{j}^{+}}^{2}}m+\\sqrt{\\bar{\\sigma}_{T-t_{j}^{+}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{+}}^{2}}U_{j+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(U_{j})_{j=1}^{N}$ areiid standard Gaussian vectors in $\\mathbb{R}^{d}$ Since $\\bar{Y}_{t_{0}^{\\leftarrow}}$ is Gaussian, by induction, we prove that $\\bar{Y}_{t_{j}^{\\leftarrow}}$ is Gaussian for all $j=1,\\cdots,N$ . Denote $\\bar{Y}_{t_{j}^{\\leftarrow}}\\sim\\mathcal{N}(m_{j},\\Sigma_{j})$ . According to (45) and the independence between $U_{j+1}$ and $\\bar{Y}_{t_{j}^{\\leftarrow}}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{j+1}=\\Big(1-\\frac{\\bar{\\sigma}_{T-t_{j}^{-}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{-}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{-}}^{2}}\\Big)m_{j}+\\frac{\\bar{\\sigma}_{T-t_{j}^{-}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{-}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{t_{j}^{-}}^{2}}m,}\\\\ {\\implies\\ \\ }&{m_{j+1}-m=\\frac{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j+1}^{-}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{-}}^{2}}\\big(m_{j}-m\\big),}\\\\ {\\implies\\ \\ }&{m_{j}=\\frac{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{-}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{-}}^{2}}\\big(m_{0}-m\\big)+m=\\frac{\\bar{\\sigma}_{T-t_{0}^{-}}^{2}+\\bar{\\sigma}_{T-t_{j}^{-}}^{2}}{\\sigma^{2}+\\bar{\\sigma}_{T-t_{0}^{-}}^{2}}m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Again, according to (45) and the independence between $U_{j+1}$ and $\\bar{Y}_{t_{j}^{\\leftarrow}}$ , we get a relation between consecutive covariance matrices: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{j+1}=\\Big(1-\\frac{\\tilde{\\sigma}_{T-t_{j}^{\\ast}}^{2}-\\tilde{\\sigma}_{T-t_{j+1}^{\\ast}}^{2}}{\\sigma^{2}+\\tilde{\\sigma}_{T-t_{j}^{\\ast}}^{2}}\\Big)^{2}\\Sigma_{j}+\\big(\\bar{\\sigma}_{T-t_{j}^{\\ast}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{\\ast}}^{2}\\big)I_{d},}\\\\ {\\implies}&{\\quad\\frac{\\Sigma_{j+1}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j+1}^{\\ast}}^{2}\\big)^{2}}=\\frac{\\sum_{j}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\ast}}^{2}\\big)^{2}}+\\frac{\\bar{\\sigma}_{T-t_{j}^{\\ast}}^{2}-\\bar{\\sigma}_{T-t_{j+1}^{\\ast}}^{2}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j+1}^{\\ast}}^{2}\\big)^{2}}I_{d},}\\\\ {\\implies}&{\\quad\\frac{\\Sigma_{j}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\ast}}^{2}\\big)^{2}}=\\frac{\\Sigma_{0}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\ast}}^{2}\\big)^{2}}+\\sum_{l=0}^{1}\\frac{\\bar{\\sigma}_{T-t_{l}^{\\ast}}^{2}-\\bar{\\sigma}_{T-t_{l+1}^{\\ast}}^{2}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{l+1}^{\\ast}}^{2}\\big)^{2}}I_{d},}\\\\ {\\implies}&{\\quad\\Sigma_{j}=\\Bigg(\\frac{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\ast}}^{2}\\big)^{2}\\bar{\\sigma}_{T}^{2}}{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j+1}^{\\ast}}^{2}\\big)^{2}}+\\sum_{l=0}^{1}\\frac{\\big(\\sigma^{2}+\\bar{\\sigma}_{T-t_{j}^{\\ast}}^{2}\\big)^{2}\\big(\\bar{\\sigma}_\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "G  Full error analysis ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 3. We only need to deal with $E_{S}$ . By applying the same schedules to training objective, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{E_{S}=\\sum_{j=0}^{N-1}\\frac{\\sigma_{t_{N-j}}^{2}}{w\\left(t_{N-j}\\right)}\\cdot w(t_{N-j})(t_{N-j}-t_{N-j-1})\\frac{1}{\\bar{\\sigma}_{t_{N-j}}\\mathbb{E}_{S_{0}}\\mathbb{E}_{\\xi}\\left\\|\\bar{\\sigma}_{t_{N-j}}s(\\theta;t_{N-j},X_{t_{N-j}})+\\xi\\right\\|^{2}}}\\\\ &{\\quad+\\displaystyle\\sum_{j=0}^{N-1}\\frac{\\sigma_{t_{N-j}}^{2}}{w(t_{N-j})}\\cdot w(t_{N-j})(t_{N-j}-t_{N-j-1})\\cdot C}\\\\ &{\\leq\\operatorname*{max}_{j}\\frac{\\sigma_{t_{N-j}}^{2}}{w(t_{N-j})}\\cdot(\\bar{C}(W)+\\bar{C}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Together with ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathcal{L}}({W}^{(k)})+\\bar{C}\\leq|\\bar{\\mathcal{L}}({W}^{(K)})-\\bar{\\mathcal{L}}_{e m}({W}^{(K)})+\\bar{\\mathcal{L}}_{e m}({\\theta}^{*})-\\bar{\\mathcal{L}}({\\theta}^{*})|+|\\bar{\\mathcal{L}}_{e m}({W}^{(K)})-\\bar{\\mathcal{L}}_{e m}({\\theta}^{*})|}\\\\ &{\\qquad\\qquad\\qquad+\\left|\\bar{\\mathcal{L}}({\\theta}^{*})-\\bar{\\mathcal{L}}({\\theta}_{\\mathcal{F}})\\right|+|\\bar{\\mathcal{L}}({\\theta}_{\\mathcal{F}})+\\bar{C}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we have the result. ", "page_idx": 35}, {"type": "text", "text": "H Proofs for Section 4.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "H.1 Proof of \u201cbell-shaped\" curve ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . Fix $x_{i},\\xi_{i j},\\bar{\\sigma}_{t_{N}}$ . By definition of the network $S(\\theta;t_{j},X_{i j})$ , it is continuous with respect to $X_{i j}$ ", "page_idx": 35}, {"type": "text", "text": "For 1, $X_{i j}=x_{i}\\!+\\!\\bar{\\sigma}_{t_{j}}\\xi_{i j}$ , and thus $S(\\theta;t_{j},X_{i j})$ is also continuous w.r.t. $\\bar{\\sigma}_{t_{j}}$ . Also, since $\\bar{\\sigma}_{t_{j}}\\in\\left[0,\\bar{\\sigma}_{t_{N}}\\right]$ there exists $M_{0}>0$ ,s.t., ", "page_idx": 35}, {"type": "equation", "text": "$$\nS(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j})\\in[-M_{0},M_{0}]^{d}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then for any $\\epsilon_{1}>0$ , there exists $\\begin{array}{r}{\\delta=\\frac{\\epsilon_{1}}{\\sqrt{d}M_{0}}>0}\\end{array}$ ,\\$.t, $\\forall~0\\le\\bar{\\sigma}_{t_{j}}<\\delta_{1}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\bar{\\sigma}_{t_{j}}}S\\big(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j}\\big)+\\xi_{i j}\\right\\|\\geq\\left\\|{\\xi_{i j}}\\right\\|-\\left\\|{\\bar{\\sigma}_{t_{j}}}S\\big(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j}\\big)\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\left\\|{\\xi_{i j}}\\right\\|-\\sqrt{d}M_{0}\\bar{\\sigma}_{t_{j}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\left\\|{\\xi_{i j}}\\right\\|-\\epsilon_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For 2, by the positive homogeniety of ReLU. ", "page_idx": 35}, {"type": "equation", "text": "$$\nS\\big(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j}\\big)=\\bar{\\sigma}_{t_{j}}S\\left(\\theta;t_{j},\\frac{x_{i}}{\\bar{\\sigma}_{t_{j}}}+\\xi_{i j}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Consider $\\bar{\\sigma}_{t_{j}}\\geq M$ , for some $M>0$ . Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j})+\\xi_{i j}\\right\\rvert=\\left\\lVert\\bar{\\sigma}_{t_{j}}^{2}S\\left(\\theta;t_{j},\\frac{x_{i}}{\\bar{\\sigma}_{t_{j}}}+\\xi_{i j}\\right)+\\xi_{i j}\\right\\rVert}\\\\ {\\displaystyle\\geq\\left\\lVert\\bar{\\sigma}_{t_{j}}^{2}S\\left(\\theta;t_{j},\\frac{x_{i}}{\\bar{\\sigma}_{t_{j}}}+\\xi_{i j}\\right)\\right\\rVert-\\left\\lVert\\xi_{i j}\\right\\rVert}\\\\ {\\displaystyle\\geq M^{2}\\left\\lVert S\\left(\\theta;t_{j},\\frac{x_{i}}{\\bar{\\sigma}_{t_{j}}}+\\xi_{i j}\\right)\\right\\rVert-\\left\\lVert\\xi_{i j}\\right\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For any $y\\in\\mathcal{D}(M)$ , where $\\mathcal{D}(M)=\\{y\\in\\mathbb{R}^{d}:y_{s}\\in[(\\xi_{i j})_{s}-|(x_{i})_{s}|/M,(\\xi_{i j})_{s}+|(x_{i})_{s}|/M]$ $\\forall~s=$$1,\\cdots,d\\}$ \uff0c", "page_idx": 35}, {"type": "equation", "text": "$$\n\\big\\|S\\big(\\theta;t_{j},y\\big)\\big\\|\\geq\\big\\|S\\big(\\theta;t_{j},\\xi_{i j}\\big)\\big\\|-\\big\\|S\\big(\\theta;t_{j},y\\big)-S\\big(\\theta;t_{j},\\xi_{i j}\\big)\\big\\|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $S$ is differentiable a.e., by the fundamental theorem of calculus, ", "page_idx": 35}, {"type": "equation", "text": "$$\nS(\\theta;t_{j},y)-S(\\theta;t_{j},\\xi_{i j})=\\int_{\\xi_{i j}}^{y}S_{x}^{\\prime}(\\theta;t_{j},x)d x.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|S(\\theta;t_{j},y)-S(\\theta;t_{j},\\xi_{i j})\\|\\leq\\frac{1}{M}\\cdot M_{1},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\begin{array}{r}{M_{1}=\\operatorname*{max}_{s}(x_{i})_{s}\\cdot\\cos\\operatorname*{sup}_{x\\in\\mathcal{D}(M_{2})}\\left\\|S_{x}^{\\prime}(\\theta;t_{j},x)\\right\\|<+\\infty}\\end{array}$ for some fixed $0<M_{2}<M$ Combining (46),(47), and (48), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\|\\bar{\\sigma}_{t_{j}}S\\big(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j}\\big)+\\xi_{i j}\\big\\|\\geq M^{2}\\left(\\big\\|S\\big(\\theta;t_{j},\\xi_{i j}\\big)\\big\\|-\\displaystyle\\frac{M_{1}}{M}\\right)-\\big\\|\\xi_{i j}\\big\\|}\\\\ {=M^{2}\\left(\\big\\|S\\big(\\theta;t_{j},\\xi_{i j}\\big)\\big\\|-\\displaystyle\\frac{M_{1}}{M}-\\displaystyle\\frac{\\|\\xi_{i j}\\|}{M^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then $\\forall\\ \\epsilon_{2}>0$ , there exists $\\begin{array}{r}{M=\\operatorname*{max}\\left\\{\\frac{M_{1}+\\sqrt{M_{1}^{2}+4\\epsilon_{2}\\left\\|\\xi_{i j}\\right\\|}}{2\\epsilon_{2}},M_{2}\\right\\}>0}\\end{array}$ .,t hen $\\bar{\\sigma}_{t_{j}}>M$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\bar{\\sigma}_{t_{j}}S(\\theta;t_{j},x_{i}+\\bar{\\sigma}_{t_{j}}\\xi_{i j})+\\xi_{i j}\\|\\geq M^{2}\\big(\\|S(\\theta;t_{j},\\xi_{i j})\\|-\\epsilon_{2}\\big).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "H.2  Proof of optimal rate ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof of Corollary 2. If $\\big|f(\\theta^{(k)};i,j)-f(\\theta^{(k)};l,s)\\big|\\leq\\epsilon$ for all $i,j,l,s$ and $k>K$ , then by Lemma 1 and 7, we choose the maximum $f(\\theta^{(k)};i,j)$ for the lower bound, which is of order $O(\\epsilon)$ away from the other $f(\\theta^{(k)};i,j)^{\\prime}s$ Therefore, we can take $j^{*}(k)=\\arg\\operatorname*{max}_{j}f(\\theta^{(k)};i,j)$ and absorb the $O(\\epsilon)$ error in constant factors. Then the result naturally follows. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "H.3 Proof of comparisons of $E_{S}$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Recall that the training objective of EDM is defined in the following ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{5}_{\\vec{\\sigma}\\sim p_{\\mathrm{train}}}\\mathbb{E}_{y,n}\\lambda(\\bar{\\sigma})\\,\\lVert D_{\\theta}\\big(y+n;\\bar{\\sigma}\\big)-y\\rVert^{2}=\\frac{1}{Z_{1}}\\int\\frac{1}{\\bar{\\sigma}}e^{-\\frac{(\\log\\bar{\\sigma}-P_{\\mathrm{nean}})^{2}}{2P_{\\mathrm{std}}^{2}}}\\cdot\\frac{\\bar{\\sigma}^{2}+\\sigma_{\\mathrm{data}}^{2}}{\\bar{\\sigma}^{2}\\sigma_{\\mathrm{data}}^{2}}\\cdot\\bar{\\sigma}^{2}\\mathbb{E}_{X_{0},\\xi}\\lVert\\bar{\\sigma}s\\big(\\theta;t,X_{t}\\big)\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $\\beta_{j}=C_{1}\\beta_{\\mathrm{EDM}}$ , i.e., ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{w(t_{j})}{\\bar{\\sigma}_{t_{j}}}(t_{j}-t_{j-1})=C_{1}\\cdot e^{-\\frac{(\\log\\bar{\\sigma}_{t_{j}}-P_{\\mathrm{mean}})^{2}}{2P_{\\mathrm{std}}^{2}}}\\cdot\\frac{\\bar{\\sigma}_{t_{j}}^{2}+\\sigma_{\\mathrm{data}}^{2}}{\\bar{\\sigma}_{t_{j}}^{2}\\sigma_{\\mathrm{data}}^{2}}\\cdot\\bar{\\sigma}_{t_{j}}}}\\\\ &{}&{w(t_{j})=C_{1}\\cdot\\frac{\\bar{\\sigma}_{t_{j}}}{t_{j}-t_{j-1}}\\cdot e^{-\\frac{(\\log\\bar{\\sigma}_{t_{j}}-P_{\\mathrm{mean}})^{2}}{2P_{\\mathrm{std}}^{2}}}\\cdot\\frac{\\bar{\\sigma}_{t_{j}}^{2}+\\sigma_{\\mathrm{data}}^{2}}{\\bar{\\sigma}_{t_{j}}^{2}\\sigma_{\\mathrm{data}}^{2}}\\cdot\\bar{\\sigma}_{t_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "EDM. Consider $\\bar{\\sigma}_{t}=t$ and $\\begin{array}{r}{t_{j}=\\left(\\bar{\\sigma}_{\\mathrm{max}}^{1/\\rho}-\\left(\\bar{\\sigma}_{\\mathrm{max}}^{1/\\rho}-\\bar{\\sigma}_{\\mathrm{min}}^{1/\\rho}\\right)\\frac{N-j}{N}\\right)^{\\rho}}\\end{array}$ for $j=0,\\cdots,N$ . Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w(t_{j})=C_{1}\\cdot\\frac{t_{j}}{t_{j}-t_{j-1}}\\cdot e^{-\\frac{(\\log{t_{k}}-P_{\\mathrm{mean}})^{2}}{2P_{\\mathrm{std}}^{2}}}\\cdot\\frac{{t_{j}}^{2}+\\sigma_{\\mathrm{data}}^{2}}{t_{j}\\sigma_{\\mathrm{data}}^{2}}}\\\\ &{\\qquad=C_{1}\\cdot\\frac{1}{\\left(\\overline{{\\sigma}}_{\\mathrm{max}}^{1/\\rho}-(\\overline{{\\sigma}}_{\\mathrm{max}}^{1/\\rho}-\\overline{{\\sigma}}_{\\mathrm{min}}^{1/\\rho})^{\\frac{N-j}{N}}\\right)^{\\rho}-\\left(\\overline{{\\sigma}}_{\\mathrm{max}}^{1/\\rho}-(\\overline{{\\sigma}}_{\\mathrm{max}}^{1/\\rho}-\\overline{{\\sigma}}_{\\mathrm{min}}^{1/\\rho})^{\\frac{N-j+1}{N}}\\right)^{\\rho}}}\\\\ &{\\qquad\\qquad\\cdot e^{-\\frac{\\left(\\log{\\left(\\sigma_{\\mathrm{max}}^{1/\\rho}-(\\sigma_{\\mathrm{max}}^{1/\\rho}-\\sigma_{\\mathrm{min}}^{1/\\rho})\\frac{N-j}{N}\\right)^{2}}-P_{\\mathrm{mean}}\\right)^{2}}{2P_{\\mathrm{std}}^{2}}}\\cdot\\frac{\\left(\\overline{{\\sigma}}_{\\mathrm{max}}^{1/\\rho}-(\\overline{{\\sigma}}_{\\mathrm{max}}^{1/\\rho}-\\overline{{\\sigma}}_{\\mathrm{min}}^{1/\\rho})^{\\frac{N-j}{N}}\\right)^{2\\rho}+\\sigma_{\\mathrm{data}}^{2}}{\\sigma_{\\mathrm{data}}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then the maximum of $\\frac{\\sigma_{t_{j}}^{2}}{w(t_{j})}\\,=\\,\\frac{t_{j}}{w(t_{j})}$ appears at j = N ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j}\\frac{\\sigma_{t_{j}}^{2}}{w(t_{j})}=\\operatorname*{max}_{j}\\frac{t_{j}}{w(t_{j})}=\\frac{\\bar{\\sigma}_{\\operatorname*{max}}\\sigma_{\\operatorname*{data}}^{2}e^{\\frac{(P_{\\operatorname*{max}}-\\log\\bar{\\sigma}_{\\operatorname*{max}})^{2}}{2P_{\\operatorname*{sad}}^{2}}}}{C_{1}(\\bar{\\sigma}_{\\operatorname*{max}}^{2}+\\sigma_{\\operatorname*{data}}^{2})}\\cdot\\left(\\bar{\\sigma}_{\\operatorname*{max}}-\\left(\\bar{\\sigma}_{\\operatorname*{max}}^{1/\\rho}-\\frac{\\bar{\\sigma}_{\\operatorname*{max}}^{1/\\rho}-\\bar{\\sigma}_{\\operatorname*{min}}^{1/\\rho}}{N}\\right)^{\\rho}\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Song et al [4]. Consider $\\bar{\\sigma}_{t}=\\sqrt{t}$ and $t_{j}=\\bar{\\sigma}_{\\mathrm{max}}^{2}\\left(\\frac{\\bar{\\sigma}_{\\mathrm{min}}^{2}}{\\bar{\\sigma}_{\\mathrm{max}}^{2}}\\right)^{\\frac{N-j}{N}}$ for $j=0,\\cdots,N$ Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v(t_{j})=C_{1}\\cdot\\frac{\\sqrt{t_{j}}}{t_{j}-t_{j-1}}\\cdot e^{-\\frac{(\\log\\sqrt{t_{j}}-P_{\\operatorname*{mean}})^{2}}{2P_{\\operatorname*{std}}^{2}}}\\cdot\\frac{t_{j}+\\sigma_{\\mathrm{data}}^{2}}{\\sqrt{t_{j}}\\sigma_{\\mathrm{data}}^{2}}}\\\\ &{\\qquad=C_{1}\\cdot\\frac{1}{\\sigma_{\\operatorname*{max}}^{2}\\left(\\frac{\\bar{\\sigma}_{\\operatorname*{min}}^{2}}{\\bar{\\sigma}_{\\operatorname*{max}}^{2}}\\right)^{\\frac{N-j}{N}}-\\bar{\\sigma}_{\\operatorname*{max}}^{2}\\left(\\frac{\\bar{\\sigma}_{\\operatorname*{max}}^{2}}{\\bar{\\sigma}_{\\operatorname*{max}}^{2}}\\right)^{\\frac{N-j}{N}+1}}\\cdot e^{-\\frac{\\left(\\log\\sigma_{\\operatorname*{max}}\\left(\\frac{\\bar{\\sigma}_{\\operatorname*{max}}}{\\bar{\\sigma}_{\\operatorname*{max}}^{2}}\\right)^{\\frac{N-j}{N}}-P_{\\operatorname*{mean}}\\right)^{2}}{2P_{\\operatorname*{std}}^{2}}}\\cdot\\frac{\\bar{\\sigma}_{\\operatorname*{max}}^{2}\\left(\\frac{\\bar{\\sigma}_{\\operatorname*{min}}^{2}}{\\bar{\\sigma}_{\\operatorname*{max}}^{2}}\\right)^{\\frac{N-j}{N}}}{\\sigma_{\\operatorname*{data}}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j}\\frac{\\sigma_{t_{j}}^{2}}{w(t_{j})}=\\operatorname*{max}_{j}\\frac{1}{2w(t_{j})}=\\frac{\\bar{\\sigma}_{\\operatorname*{max}}\\sigma_{\\operatorname*{data}}^{2}e^{\\frac{(P_{\\operatorname*{max}}-10\\mathrm{g}\\bar{\\sigma}_{\\operatorname*{max}})^{2}}{2P_{\\operatorname*{sd}}^{2}}}}{C_{1}(\\bar{\\sigma}_{\\operatorname*{max}}^{2}+\\sigma_{\\operatorname*{data}}^{2})}\\cdot\\frac{1}{2}\\left(\\bar{\\sigma}_{\\operatorname*{max}}-\\bar{\\sigma}_{\\operatorname*{max}}\\left(\\frac{\\bar{\\sigma}_{\\operatorname*{min}}^{2}}{\\bar{\\sigma}_{\\operatorname*{max}}^{2}}\\right)1/N\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1 Proofs for Section 4.2 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1.1 Proof when $E_{I}+E_{D}$ dominates. ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Under the EDM choice of variance, $\\bar{\\sigma}_{t}=t$ for all $t\\in[0,T]$ , and study the optimal time schedule when $E_{D}+E_{I}$ dominates. First, it follows from Theorem 2 that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{E_{I}+E_{D}\\lesssim\\displaystyle\\frac{\\mathrm{m}_{2}^{2}}{T^{2}}+d\\sum_{j=0}^{N-1}\\displaystyle\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{\\leftarrow})^{2}}}}\\\\ {{\\displaystyle\\qquad\\qquad+\\left(\\mathrm{m}_{2}^{2}+d\\right)\\Big(\\sum_{T-t_{j}^{\\leftarrow}\\geq1}\\displaystyle\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{\\leftarrow})^{4}}+\\displaystyle\\frac{\\gamma_{j}^{3}}{(T-t_{j}^{\\leftarrow})^{5}}+\\sum_{T-t_{j}^{\\leftarrow}<1}\\displaystyle\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{\\leftarrow})^{2}}+\\displaystyle\\frac{\\gamma_{j}^{3}}{(T-t_{j}^{\\leftarrow})^{3}}\\Big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Based on the above time schedule dependent error bound, we quantify the errors under polynomial time schedule and exponential time schedule. ", "page_idx": 37}, {"type": "text", "text": "Polynomia tme schedule we considerT - ty = (1la (N - )h with= Tla-sla and $a>1$ $\\gamma_{j}=a\\big(\\delta^{1/a}+\\big(N-j-\\vartheta\\big)h\\big)^{a-1}h$ for some $\\vartheta\\in(0,1)$ . We have $\\gamma_{j}/h\\sim a\\big(T-t_{j}^{\\leftarrow}\\big)^{\\frac{a-1}{a}}$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\nE_{I}+E_{D}\\lesssim\\frac{\\mathrm{m}_{2}^{2}}{T^{2}}+\\frac{d a^{2}T^{\\frac{1}{a}}}{\\delta^{\\frac{1}{a}}N}+\\bigl(\\mathrm{m}_{2}^{2}+d\\bigr)\\bigl(\\frac{a^{2}T^{\\frac{1}{a}}}{\\delta^{\\frac{1}{a}}N}+\\frac{a^{3}T^{\\frac{2}{a}}}{\\delta^{\\frac{2}{a}}N^{2}}\\bigr)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, to obtain $E_{I}+E_{D}\\lesssim\\varepsilon$ , it sufces to require $\\begin{array}{r}{T=\\Theta\\big(\\frac{\\mathrm{m_{2}}}{\\varepsilon^{1/2}}\\big)}\\end{array}$ and the iteration complexity ", "page_idx": 37}, {"type": "equation", "text": "$$\nN=\\Omega\\big(a^{2}\\big(\\frac{\\mathfrak{m}_{2}}{\\delta\\varepsilon^{\\frac{1}{2}}}\\big)^{\\frac{1}{a}}\\frac{\\mathfrak{m}_{2}^{2}+d}{\\varepsilon}\\big)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For fixed $\\mathrm{m_{2}},\\delta$ and $\\varepsilon$ optimal value of $a$ that minimizes theiteration complexity $N$ .is $\\begin{array}{r}{a=\\frac{1}{2}\\ln(\\frac{\\mathrm{m}_{2}}{\\delta\\varepsilon^{1/2}})}\\end{array}$ Once we let $\\delta=\\bar{\\sigma}_{\\mathrm{min}}$ \uff0c $\\begin{array}{r}{T=\\bar{\\sigma}_{\\mathrm{max}}=\\Theta\\big(\\frac{\\mathrm{m}_{2}}{\\varepsilon^{1/2}}\\big)}\\end{array}$ and $a=\\rho$ , the iteration complexity is ", "page_idx": 37}, {"type": "equation", "text": "$$\nN=\\Omega\\big(\\frac{\\mathrm{m}_{2}^{2}\\vee d}{d}\\rho^{2}\\big(\\frac{\\bar{\\sigma}_{\\operatorname*{max}}}{\\bar{\\sigma}_{\\operatorname*{min}}}\\big)^{1/\\rho}\\bar{\\sigma}_{\\operatorname*{max}}^{2}\\big),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and it is easy to see that our theoretical result supports what's empirically observed in EDM that there is an optimal value of $\\rho$ that minimizes the FID. ", "page_idx": 37}, {"type": "text", "text": "Exponential time schedule. we consider $\\gamma_{j}=\\kappa(T-t_{j}^{\\leftarrow})$ with Im(T/8) , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\nE_{I}+E_{D}\\lesssim\\frac{\\mathrm{m}_{2}^{2}}{T^{2}}+\\frac{d\\ln(T/\\delta)^{2}}{N}+(\\mathrm{m}_{2}^{2}+d)\\big(\\frac{\\ln(T/\\delta)^{2}}{N}+\\frac{\\ln(T/\\delta)^{3}}{N^{2}}\\big)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, to obtain $E_{I}+E_{D}\\lesssim\\varepsilon$ it suffices to require $T=\\Theta(\\frac{\\mathrm{m_{2}}}{\\varepsilon^{\\frac{1}{2}}})$ and the iteration complexity ", "page_idx": 37}, {"type": "equation", "text": "$$\nN=\\Omega\\big(\\frac{\\mathrm{m}_{2}^{2}+d}{\\varepsilon}\\ln(\\frac{\\mathrm{m}_{2}}{\\delta\\varepsilon^{\\frac{1}{2}}})^{2}\\big)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "When $\\operatorname*{m}_{2}\\leq O({\\sqrt{d}})$ , the exponential time schedule is asymptotic optimal, hence it is better than the polynomial time schedule when the initilization error and discretization error dominate. Once we let $\\delta=\\bar{\\sigma}_{\\mathrm{min}}$ $\\begin{array}{r}{T=\\bar{\\sigma}_{\\mathrm{max}}=\\Theta\\big(\\frac{\\mathrm{m}_{2}}{\\varepsilon^{1/2}}\\big)}\\end{array}$ , the iteration complexity is ", "page_idx": 37}, {"type": "equation", "text": "$$\nN=\\Omega\\big(\\frac{\\mathrm{m}_{2}^{2}\\vee d}{d}\\ln\\big(\\frac{\\bar{\\sigma}_{\\operatorname*{max}}}{\\bar{\\sigma}_{\\operatorname*{min}}}\\big)^{2}\\bar{\\sigma}_{\\operatorname*{max}}^{2}\\big).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now we adopt the variance schedule in [46], $\\bar{\\sigma}_{t}=\\sqrt{t}$ for all $t\\in[0,T]$ , it follows from Theorem 2 that ", "page_idx": 37}, {"type": "equation", "text": "$$\nE_{I}+E_{D}\\lesssim\\frac{\\mathrm{m}_{2}^{2}}{T}+d\\sum_{j=0}^{N-1}\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{*})^{2}}+\\bigl(\\mathrm{m}_{2}^{2}+d\\bigr)\\Bigl(\\sum_{T-t_{j}^{*}\\geq1}\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{*})^{3}}+\\sum_{T-t_{j}^{*}<1}\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{*})^{2}}\\Bigr)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "PolyomialtmeschedulewensiderT-t =(/a(N)) wihh= and $a>1$ $\\gamma_{j}=a\\big(\\delta^{1/a}+\\big(N-j-\\vartheta\\big)h\\big)^{a-1}h$ for some $\\vartheta\\in(0,1)$ . We have $\\gamma_{j}/h\\sim a\\big(T-t_{j}^{\\leftarrow}\\big)^{\\frac{a-1}{a}}$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\nE_{I}+E_{D}\\lesssim{\\frac{\\mathrm{m}_{2}^{2}}{T}}+{\\frac{d a^{2}T^{\\frac{1}{a}}}{\\delta^{\\frac{1}{a}}N}}+(\\mathrm{m}_{2}^{2}+d){\\frac{a^{2}T^{\\frac{1}{a}}}{\\delta^{\\frac{1}{a}}N}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, to obtain $E_{I}+E_{D}\\lesssim\\varepsilon$ it suffices to require $T=\\Theta\\big(\\frac{\\mathrm{m}_{2}^{2}}{\\varepsilon}\\big)$ and the iteration complexity ", "page_idx": 38}, {"type": "equation", "text": "$$\nN=\\Omega\\big(a^{2}\\big(\\frac{\\mathrm{m_{2}^{2}}}{\\delta\\varepsilon}\\big)^{\\frac{1}{a}}\\frac{\\mathrm{m}_{2}^{2}+d}{\\varepsilon}\\big)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Once we let $\\delta=\\bar{\\sigma}_{\\mathrm{min}}^{2}$ \uff0c $\\begin{array}{r}{T=\\bar{\\sigma}_{\\mathrm{max}}^{2}=\\Theta\\big(\\frac{\\mathrm{m}_{2}^{2}}{\\varepsilon}\\big)}\\end{array}$ and $a=\\rho$ , the iteration complexity is ", "page_idx": 38}, {"type": "equation", "text": "$$\nN=\\Omega\\big(\\frac{\\mathrm{m}_{2}^{2}\\vee d}{d}\\rho^{2}\\big(\\frac{\\bar{\\sigma}_{\\operatorname*{max}}}{\\bar{\\sigma}_{\\operatorname*{min}}}\\big)^{2/\\rho}\\bar{\\sigma}_{\\operatorname*{max}}^{2}\\big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Compared to exponential time schedule with the EDM choice of variance schedule, this iteration complxityis wrseupta fator $\\left(\\frac{\\bar{\\sigma}_{\\mathrm{max}}}{\\bar{\\sigma}_{\\mathrm{min}}}\\right)^{1/\\rho}$ ", "page_idx": 38}, {"type": "text", "text": "Exponential time schedule. we consider $\\gamma_{j}=\\kappa(T-t_{j}^{\\leftarrow})$ with Im(T/8) , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nE_{I}+E_{D}\\lesssim\\frac{\\mathrm{m}_{2}^{2}}{T}+\\frac{d\\ln(T/\\delta)^{2}}{N}+(\\mathrm{m}_{2}^{2}+d)\\frac{\\ln(T/\\delta)^{2}}{N}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, to obtain $E_{I}+E_{D}\\lesssim\\varepsilon$ , it suffices to require $T=\\Theta\\big(\\frac{\\mathrm{m}_{2}^{2}}{\\varepsilon}\\big)$ and the iteration complexity ", "page_idx": 38}, {"type": "equation", "text": "$$\nN=\\Omega\\big(\\frac{\\mathrm{m_{2}^{2}+d}}{\\varepsilon}\\ln(\\frac{\\mathrm{m_{2}^{2}}}{\\delta\\varepsilon})^{2}\\big)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Once we let $\\delta=\\bar{\\sigma}_{\\mathrm{min}}^{2}$ $\\begin{array}{r}{T=\\bar{\\sigma}_{\\mathrm{max}}^{2}=\\Theta\\big(\\frac{\\mathrm{m}_{2}^{2}}{\\varepsilon}\\big)}\\end{array}$ and $a=\\rho$ , the iteration complexity is ", "page_idx": 38}, {"type": "equation", "text": "$$\nN=\\Omega\\big(\\frac{\\mathrm{m}_{2}^{2}\\vee d}{d}\\ln\\big(\\frac{\\bar{\\sigma}_{\\operatorname*{max}}}{\\bar{\\sigma}_{\\operatorname*{min}}}\\big)^{2}\\bar{\\sigma}_{\\operatorname*{max}}^{2}\\big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Compared to exponential time schedule with the EDM choice of variance schedule, this iteration complexity has the same dependence on dimension parameters $\\mathrm{m}_{2},d$ and the minimal/maximal variance $\\bar{\\sigma}_{\\mathrm{min}},\\bar{\\sigma}_{\\mathrm{max}}$ ", "page_idx": 38}, {"type": "text", "text": "Optimality of Exponential time schedule. For simplicity, we assume $\\mathrm{m}_{2}^{2}=\\mathcal{O}(d)$ . Then under both schedules in [30] and [46], $E_{I}\\mathfrak{s}$ only dependent on $T$ , and are independent of the time schedule. Both $E_{D}\\mathbf{s}$ satisfy ", "page_idx": 38}, {"type": "equation", "text": "$$\nE_{D}\\lesssim d\\sum_{j=0}^{N-1}\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{\\leftarrow})^{2}}\\lesssim\\varepsilon\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "$\\begin{array}{r}{\\tau_{j}\\ =\\ \\ln\\Big(\\frac{T-t_{j}^{\\leftarrow}}{T-t_{j+1}^{\\leftarrow}}\\Big)\\ \\in\\ (0,\\infty)}\\end{array}$ $\\begin{array}{r}{\\frac{\\gamma_{j}}{T-t_{j}^{\\leftarrow}}~=~1-e^{-\\tau_{j}}}\\end{array}$ and $\\begin{array}{r}{\\sum_{\\delta<T-t_{j}^{\\leftarrow}<T}\\tau_{j}\\ =\\ \\ln(T/\\delta)}\\end{array}$ is fixed. Since $x\\,\\mapsto\\,(1\\,-\\,e^{-x})^{2}$ is convex on the domain $x\\,\\in\\,(0,\\infty)$ , according the Jensen's inequality, $\\scriptstyle\\sum_{\\delta<T-t_{j}^{\\leftarrow}<T}\\frac{\\gamma_{j}^{2}}{(T-t_{j}^{\\leftarrow})^{2}}$ reaches its minimum when $\\tau_{j}$ are constant-valued fo all $j$ , which implies the exponential schedule is optimal to minimize $E_{D}$ , hence optimal to minimize $E_{D}+E_{I}$ ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 39}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 39}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 39}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes]\" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g.,\"'error bars are not reported because it would be too computationally expensive\u201d or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"'[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 39}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\"', \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We state it in Section 1 and Appendix A. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: In Section 1 and Appendix A ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\u201d\u2019 section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: See Appendix and Theorems in the paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA]   \nJustification: No experiments.   \nGuidelines: \u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a)  If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: No experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^\u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 42}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer:[NA]   \nJustification:   \nGuidelines: \u00b7 The answer NA means that the paper does not include experiments. \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: It has been confirmed by Ethics reviewers. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: In Section 1. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models. image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification:   \nGuidelines: \u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We show new methods and theorems. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]