[{"type": "text", "text": "SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Paul Mangold Sergey Samsonov Safwan Labbi Ilya Levin CMAP, UMR 7641, HSE University, CMAP, UMR 7641, HSE University, \u00c9cole polytechnique Russia \u00c9cole polytechnique Russia ", "page_idx": 0}, {"type": "text", "text": "Reda Alami   \nTechnology Innovation Institute,   \n9639 Masdar City, Abu Dhabi, United Arab Emirates Alexey Naumov Eric Moulin HSE University, CMAP, UMR   \nSteklov Mathematical Institute \u00c9cole polytech   \nof Russian Academy of Sciences MZBUAI ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy \u03f5. To overcome this, we propose SCAFFLSA, a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew [37]. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Heterogeneity has a major impact on communication complexity in federated learning (FL) [28, 36]. In FL, multiple agents use different local oracles to update a global model together. A central server then performs a consensus step to incrementally update the global model. Since communication with the server is costly, reducing the frequency of the consensus steps is a central challenge. At the same time, limiting communications induces client drift when agents are heterogeneous, biasing them towards their local solutions. This issue has mostly been discussed for FL with stochastic gradient methods [23, 51]. In this paper, we investigate the impact of heterogeneity in the field of federated linear stochastic approximation (federated LSA). The goal is to solve a system of linear equations where (i) the system matrix and the corresponding objective are only accessible via stochastic oracles, and (ii) these oracles are distributed over an ensemble of heterogeneous agents. This problem can be solved with the FedLSA method, which performs LSA locally with periodic consensus steps. This approach suffers from two major drawbacks: heterogeneity bias, and high variance of local oracles. ", "page_idx": 0}, {"type": "text", "text": "A popular means of overcoming heterogeneity problems is the method of control variables, which goes back to the line of research initiated by [23]. However, existing results on the complexity of these methods tend to neglect the linear decrease of the mean squared error (MSE) of the algorithm with the number of agents $N$ [37], or they require a lot of communication [23]. In this paper, we ", "page_idx": 0}, {"type": "text", "text": "Table 1: Communication and sample complexity for finding a solution with MSE lower than $\\epsilon^{2}$ for FedLSA, Scaffnew, and SCAFFLSA with i.i.d. samples (see Cor. 4.3 for results with Markovian samples). Our analysis is the first to show that FedLSA exhibits linear speed-up, as well as its variant that reduces bias using control variates. ", "page_idx": 1}, {"type": "table", "img_path": "HeJ1cBAgiV/tmp/0e95915f9b5b0dc78db5a20a5960c57c63c581ebbe3dbcf6275f39ec127d23a3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "show that it is possible to reduce communication complexity using control variates while preserving the linear speed-up in terms of sample complexity. Our contributions are the following: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide the sample and communication complexity of the FedLSA algorithm, inspired by the work of [51]. Our analysis highlights the relationship between the MSE of the FedLSA method and three key factors: the number of local updates, the step size, and the number of agents. We provide an exact analytical formulation of the algorithm\u2019s bias, which is confirmed in our numerical study. We also give results under Markovian noise sampling. \u2022 We propose SCAFFLSA, a method that provably reduces communication while maintaining linear speed-up in the number of agents. This method uses control variates to allow for extended local training. We establish finite sample and communication complexity for SCAFFLSA. Our study is based on a new analysis technique, that carefully tracks the fluctuations of the parameters and ccommunicationsontrol variates. This allows to prove that SCAFFLSA simultanously maintains linear speedup and reduced communication. To our knowledge, this is the first time that these two phenomenons are proven to occur simultaneously in FL. \u2022 We apply both these methods to TD learning with linear function approximation, where heterogeneous agents collaboratively estimate the value function of a common policy. ", "page_idx": 1}, {"type": "text", "text": "We provide a synthetic overview of this paper\u2019s theoretical results in Table 1 in the general federated LSA setting, and we instantiate these results for federated TD learning in Table 2 (Appendix E). We start by discussing related work in Section 2. We then introduce federated LSA in Section 3, and analyze it in Section 4. In Section 5 we introduce SCAFFLSA, a novel strategy to mitigate the bias. Finally, we illustrate our results numerically in Section 6. Since an important application of LSA is TD learning [47] with linear function approximation, we instantiate the results of Section 3-5 for federated TD learning. ", "page_idx": 1}, {"type": "text", "text": "Notations. For matrix $A$ we denote by $\\|A\\|$ its operator norm. Setting $N$ for the number of agents, we use the notation $\\begin{array}{r}{\\mathbb{E}_{c}[a_{c}]=N^{-1}\\sum_{c=1}^{N^{\\prime}}\\ddot{a}_{c}}\\end{array}$ \u22121  cN=1 ac for the average over different clients. For the matrix $A=A^{\\top}\\succeq0,A\\in\\mathbb{R}^{d\\times d}$ and $x\\in\\mathbb{R}^{d}$ we define the corresponding norm $\\|x\\|_{A}={\\sqrt{x^{\\top}A x}}$ . For sequences $a_{n}$ and $b_{n}$ , we write $a_{n}\\lesssim b_{n}$ if there exists a constant $c>0$ such that $a_{n}\\leq c b_{n}$ for $n\\geq0$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Federated Learning. With few exceptions (see e.g. [12]), most of the FL literature is devoted to federated stochastic gradient (SG) methods. A strong focus has been placed on the Federated Averaging (FedAvg) algorithm [36], which aims to reduce communication through local training, resulting in local drift when agents are heterogeneous [53]. Sample and communication complexity of FedAvg were investigated under a variety of conditions covering both homogeneous [31, 20] and heterogeneous agents [25, 27]. Different ways of measuring heterogeneity for FedAvg have then been proposed [51, 41]. In [44] it was also shown that FedAvg yields linear speedup in the number of agents when gradients are stochastic, a phenomenon that we prove is still present in FedLSA. ", "page_idx": 1}, {"type": "text", "text": "In order to correct the client drift of FedAvg, [23] proposed Scaffold, a method that tames heterogeneity using control variates. [17, 38] prove that Scaffold retrieves the rate of convergence of the gradient descent independently of heterogeneity, although without benefti from local training. It has been shown in [37] (with the analysis of ProxSkip, which generalizes Scaffold) that such methods accelerate training. However, unlike Scaffold, the analysis of [37] loses the linear speedup in the number of agents. Several other methods with accelerated rates have been proposed [35, 5, 6, 18, 21], albeit all of them lose the linear speedup. Contrary to these papers, we show that our approach to FedLSA with control variates preserves both the acceleration and the linear speedup. ", "page_idx": 1}, {"type": "table", "img_path": "HeJ1cBAgiV/tmp/d955396a6fa43232bcdeb3aa8ef5688b0fd62efc94b1adf3ee27c3c71158eb4b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Federated TD learning. Temporal difference (TD) learning has a long history in policy evaluation [47, 9], with the asymptotic analysis under linear function approximation (LFA) setting performed in [49, 48]. Several non-asymptotic MSE analyses have been carried out in [4, 8, 42, 32, 45]. Much attention has been paid to federated reinforcement learning [33, 43, 52] and federated TD learning with LFA. [26, 7, 34] provides an analysis under the strong homogeneity assumption. Federated TD was also investigated with heterogeneous agents, first without local training [12], then with local training but without linear acceleration [11, 22]. Recently, [50] proposed an analysis of federated TD with heterogeneous agents, local training, and linear speed-up in number of agents. However, [50] do not mitigate the local drift effects, and their conclusions are valid only in the low-heterogeneity setting. In high heterogeneity settings, their analysis exhibits a large bias. Additionally, their analysis requires the server to project aggregated iterates to a ball of unknown radius. In contrast, our analysis shows that FedLSA converges to the true solution without bias even without such projection. ", "page_idx": 2}, {"type": "text", "text": "3 Federated Linear Stochastic Approximation and TD learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Federated Linear Stochastic Approximation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In federated linear stochastic approximation, $N$ agents collaboratively solve a system linear equation system with the following finite sum structure ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\bf A}\\theta_{\\star}=\\bar{\\bf b}\\ ,\\quad\\mathrm{~where~}\\bar{\\bf A}=\\frac{1}{N}\\sum_{c=1}^{N}\\bar{\\bf A}^{c}\\ ,\\quad\\bar{\\bf b}=\\frac{1}{N}\\sum_{c=1}^{N}\\bar{\\bf b}^{c}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where for $c\\in[N]$ , $\\bar{\\mathbf{A}}^{c}\\in\\mathbb{R}^{d\\times d}$ , $\\bar{\\mathbf{b}}^{c}\\in\\mathbb{R}^{d}$ . We assume the solution $\\theta_{\\star}$ to be unique, and that each local system $\\bar{\\mathbf{A}}^{c}\\theta_{\\star}^{\\bar{c}}=\\bar{\\mathbf{b}}^{c}$ also has a unique solution $\\theta_{\\star}^{c}$ . The values of $\\bar{\\mathbf{A}}^{c}$ \u2019s and $\\bar{\\mathbf{b}}^{c}$ \u2019s can be different, representing the different realities of the agents. In federated LSA, neither matrices $\\bar{\\mathbf{A}}^{c}$ nor vectors $\\bar{\\mathbf{b}}^{c}$ are observed directly. Instead, each agent $c\\in[N]$ has access to its own observation sequence $(Z_{k}^{c})_{k\\in\\mathbb{N}}$ , that are independent from one agent to another. Agent $c$ obtains estimates $\\{(\\mathbf{A}^{c}(Z_{k}^{c}),\\mathbf{b}^{c}(Z_{k}^{c})\\})\\}_{k\\in\\mathbb{N}}$ of $\\bar{\\mathbf{A}}^{c}$ and $\\bar{\\mathbf{b}}^{c}$ , where $\\mathbf{A}^{c}:Z\\to\\mathbb{R}^{d\\times d}$ and $\\mathbf{b}^{c}:\\mathsf{Z}\\to\\mathbb{R}^{d}$ are two measurable functions. Naturally, we define the error of estimation of $\\bar{\\mathbf{A}}^{c}$ and $\\bar{\\mathbf{b}}^{c}$ as $\\widetilde{\\mathbf{b}}^{c}(z)=\\mathbf{b}^{c}(z)-\\bar{\\mathbf{b}}^{c}$ , $\\widetilde{\\mathbf{A}}^{c}(z)=\\mathbf{A}^{c}(z)-\\bar{\\mathbf{A}}^{c}$ . This allows to measure the noise at local and global so lutions as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varepsilon^{c}(z)=\\widetilde{\\mathbf{A}}^{c}(z)\\theta_{\\star}^{c}-\\widetilde{\\mathbf{b}}^{c}(z)\\ ,\\ \\mathrm{and}\\ \\omega^{c}(z)=\\widetilde{\\mathbf{A}}^{c}(z)\\theta_{\\star}-\\widetilde{\\mathbf{b}}^{c}(z)\\ ,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "together with the associated covariances, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\!=\\!\\!\\int_{\\mathbb{Z}}\\widetilde{\\mathbf{A}}^{c}(z)\\widetilde{\\mathbf{A}}^{c}(z)^{\\top}\\mathrm{d}\\pi_{c}(z)\\mid,\\;\\Sigma_{\\varepsilon}^{c}\\!=\\!\\!\\int_{\\mathbb{Z}}\\varepsilon^{c}(z)\\varepsilon^{c}(z)^{\\top}\\mathrm{d}\\pi_{c}(z)\\mid,\\;\\Sigma_{\\omega}^{c}\\!=\\!\\!\\int_{\\mathbb{Z}}\\omega^{c}(z)\\omega^{c}(z)^{\\top}\\mathrm{d}\\pi_{c}(z)\\mid,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "that are finite whenever one of the following assumptions on the $\\{Z_{t,h}^{c}\\}_{t,h\\geq0}$ hold. ", "page_idx": 2}, {"type": "text", "text": "A1. For each agent $c_{:}$ , $(Z_{k}^{c})_{k\\in\\mathbb{N}}$ are i.i.d. random variables with values in $(Z,{\\mathcal{Z}})$ and distribution $\\pi_{c}$ satisfying $\\mathbb{E}_{\\pi_{c}}[\\mathbf{A}^{c}(Z_{k}^{c})]=\\bar{\\mathbf{A}}^{c}$ and $\\mathbb{E}_{\\pi_{c}}[{\\bf b}(Z_{k}^{c})]=\\bar{\\bf b}^{c}$ , and we define $\\mathrm{C}_{\\mathbf{A}}=\\mathrm{sup}_{c}\\,\\|\\bar{\\mathbf{A}}^{c}\\|$ . ", "page_idx": 2}, {"type": "text", "text": "A2. For each $c\\in[N]$ , $(Z_{k}^{c})_{k\\in\\mathbb{N}}$ is a Markov chain with values in $(Z,{\\mathcal{Z}})$ , with Markov kernel $\\mathrm{P}_{c}$ . The kernel $\\mathrm{P}_{c}$ admits a unique invariant distribution $\\pi_{c}$ , $Z_{0}^{c}\\sim\\pi_{c},$ , and $\\mathrm{P}_{c}$ is uniformly geometrically ergodic, that is, there exist $\\tau_{\\operatorname*{mix}}(c)\\in\\mathbb{N}$ , such that for any $k\\in\\mathbb{N}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z,z^{\\prime}\\in\\mathsf{Z}}(1/2)\\|\\mathrm{P}_{c}^{k}(\\cdot|z)-\\mathrm{P}_{c}^{k}(\\cdot|z^{\\prime})\\|_{\\mathsf{T V}}\\leq(1/4)^{\\lfloor k/\\tau_{\\operatorname*{mix}}(c)\\rfloor}\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and for $c\\in[N]$ , we have $\\mathbb{E}_{\\pi_{c}}[\\mathbf{A}^{c}(Z_{1}^{c})]=\\bar{\\mathbf{A}}^{c}$ and $\\mathbb{E}_{\\pi_{c}}[{\\bf b}(Z_{1}^{c})]=\\bar{\\bf b}^{c}$ , and we define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\varepsilon\\|_{\\infty}=\\operatorname*{max}_{c\\in[N]}\\operatorname*{sup}_{z\\in\\mathbb Z}\\|\\varepsilon^{c}(z)\\|<\\infty\\,,\\quad\\mathrm{C}_{\\mathbf{A}}=\\operatorname*{max}_{c\\in[N]}\\operatorname*{sup}_{z\\in\\mathbb Z}\\|\\mathbf{A}^{c}(z)\\|<\\infty\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, each of the matrices $-\\bar{\\mathbf{A}}^{c}$ is Hurwitz. ", "page_idx": 3}, {"type": "text", "text": "In A2, random matrices $\\mathbf{A}^{c}(z)$ and noise variables $\\varepsilon^{c}(z)$ are almost surely bounded. This is necessary for working with the uniformly geometrically ergodic Markov kernels $\\mathrm{P}_{c}$ . For simplicity, we state most of our results using A1, which is classical in finite-time studies of LSA [46, 14]. Nonetheless, we show that our analysis of FedLSA can be extended to the Markovian setting under A2. ", "page_idx": 3}, {"type": "text", "text": "In a federated environment, agents can only communicate via a central server, which is generally costly. Hence, in FedLSA, agents\u2019 local updates are only aggregated after a given time. During the round $t\\geq0$ , the agents start with a shared value $\\theta_{t}$ and perform $H>0$ local updates, for $h=1$ to $H$ , given by the recurrence ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{\\theta}}_{t,h}^{c}=\\mathbf{\\boldsymbol{\\theta}}_{t,h-1}^{c}-\\eta(\\mathbf{\\mathbf{A}}^{c}(\\boldsymbol{Z}_{t,h}^{c})\\boldsymbol{\\theta}_{t,h-1}^{c}-\\mathbf{\\boldsymbol{b}}^{c}(\\boldsymbol{Z}_{t,h}^{c}))\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\theta_{t,0}^{c}=\\theta_{t}$ , and where we use the alias $Z_{t,h}^{c}=Z_{H t+h,}$ to simplify notations. Agents then send $\\theta_{t,H}$ to the server, that aggregates them as $\\begin{array}{r}{\\theta_{t}=N^{-1}\\sum_{c=1}^{N}\\theta_{t-1,H}^{c}}\\end{array}$ and sends it back to all agents. We summarize this procedure in Algorithm 1. Our next assumption, which holds whenever $\\bar{\\mathbf{A}}^{c}$ is Hurwitz [19, 39, 15], ensures the stability of the local updates. ", "page_idx": 3}, {"type": "text", "text": "A3. There exist $a>0$ , $\\eta_{\\infty}>0$ , such that $\\eta_{\\infty}a\\leq1/2$ , and for $\\eta\\in(0;\\eta_{\\infty}),\\,c\\in[N],\\,u\\in\\mathbb{R}^{d},$ it holds for $Z_{0}^{c}\\sim\\pi_{c}$ , that ${\\mathbb{E}}^{1/2}\\big[\\|(\\mathrm{I}-\\eta{\\bf A}^{c}(Z_{0}^{c}))u\\|^{2}\\big]\\leq(1-\\eta a)\\|u\\|$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Federated Temporal Difference Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A major application of FedLSA is federated TD learning with linear function approximation. Consider $N$ Markov Decision Processes $\\{(S,\\mathcal{A},\\mathbb{P}_{\\mathrm{MDP}}^{c},r^{c},\\gamma)\\}_{c\\in[N]}$ with shared state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , and discounting factor $\\gamma\\,\\in\\,(0,1)$ . Each agent $c~\\in~[N]$ has its own transition kernel $\\mathbb{P}_{\\mathrm{MDP}}^{c}$ , where $\\mathbb{P}_{\\mathrm{MDP}}^{c}(\\cdot|s,a)$ specifies the transition probability from state $s$ upon taking action $a$ for this specific agent, as well as its own reward function $r^{c}:S\\times A\\to[0,1]$ , that we assume to be deterministic for simplicity. Agents\u2019 heterogeneity lies in the different transition kernels and reward functions, that are specific to each agent. ", "page_idx": 3}, {"type": "text", "text": "In federated TD learning, all agents use the same shared policy $\\pi$ , and aim to construct a single shared function, that simultaneously approximates all value functions, defined as, for $s\\in S$ and $c\\in[N]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma^{c,\\pi}(s)=\\mathbb{E}\\Big[\\sum_{k=0}^{\\infty}\\gamma^{k}r^{c}(S_{k}^{c},A_{k}^{c})\\Big]\\;,\\;\\mathrm{with}\\;S_{0}^{c}=s,A_{k}^{c}\\sim\\pi(\\cdot|S_{k}^{c}),\\;\\mathrm{and}\\;S_{k+1}^{c}\\sim\\mathbb{P}_{\\mathrm{MDP}}^{c}(\\cdot|S_{k}^{c},A_{k}^{c})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the following, we aim to approximate $V^{c,\\pi}(s)$ as a linear combination of features built using a mapping $\\varphi:\\mathcal{S}\\to\\mathbb{R}^{d}$ . Formally, we look for $\\theta\\in\\mathbb{R}^{d}$ such that the function $\\mathcal{V}_{\\theta}(s)\\,=\\,\\varphi^{\\top}(s)\\theta$ properly estimate the true value. For $c\\in[N]$ , we denote $\\mu^{c}$ the invariant distribution over $\\boldsymbol{S}$ induced by the policy $\\pi$ and transition kernel $\\mathbb{P}_{\\mathrm{MDP}}^{c}$ of agent $c$ . Our goal is to find a parameter $\\theta_{\\star}^{c}$ which is defined as a unique solution to the projected Bellman equation, see [49], which defines the best linear approximation of $V^{c,\\pi}$ . This problem can be cast as a federated LSA problem [42, 50] by viewing the local optimum parameter $\\theta_{\\star}^{c}$ as the solution of the system $\\bar{\\mathbf{A}}^{c}\\theta_{\\star}^{c}=\\bar{\\mathbf{b}}^{c}$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}^{c}=\\mathbb{E}_{s\\sim\\mu^{c},s^{\\prime}\\sim P^{\\pi,c}(\\cdot\\vert s)}[\\phi(s)\\{\\phi(s)\\!-\\!\\gamma\\phi(s^{\\prime})\\}^{\\top}]\\;,\\quad\\mathrm{and}\\quad\\bar{\\mathbf{b}}^{c}=\\mathbb{E}_{s\\sim\\mu^{c},a\\sim\\pi(\\cdot\\vert s)}[\\phi(s)r^{c}(s,a)]\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The global optimal parameter is then defined as the solution $\\theta_{\\star}$ of the averaged system $\\begin{array}{r}{\\bigl(\\frac{1}{N}\\sum_{c=1}^{\\bar{N}}\\bar{\\mathbf{A}}^{c}\\bigr)\\bar{\\theta}_{\\star}\\;=\\;\\frac{\\bar{1}}{N}\\sum_{c=1}^{N}\\bar{\\mathbf{b}}^{c}}\\end{array}$ cN=1 \u00afbc. As it is the case for federated LSA, this parameter may give a better overall estimation of the value function. Indeed, the distribution $\\mu^{c}$ of some agents may be strongly biased towards some states, whereas obtaining an estimation that is more balanced across all states may be more relevant. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In practice, when computing value functions, the tuples $\\{(S_{k}^{c},A_{k}^{c},S_{k+1}^{c})\\}_{k\\in\\mathbb{N}}$ are sampled along one of the two following rules. ", "page_idx": 4}, {"type": "text", "text": "${\\mathbf{}}T{\\mathbf{}}D\\,1$ . $(S_{k}^{c},A_{k}^{c},S_{k+1}^{c})$ are generated i.i.d.with $S_{k}^{c}\\sim\\mu^{c}$ , $A_{k}^{c}\\sim\\pi(\\cdot|S_{k}^{c})$ , $S_{k+1}^{c}\\sim\\mathbb{P}_{M D P}^{c}(\\cdot|S_{k}^{c},A_{k}^{c})$ .   \n${\\mathbf{}}T{\\mathbf{D}}\\,2$ . $(S_{k}^{c},A_{k}^{c},S_{k+1}^{c})$ are generated sequentially with $A_{k}^{c}\\sim\\pi(\\cdot|S_{k}^{c}),\\,S_{k+1}^{c}\\sim\\mathbb{P}_{M D P}^{c}(\\cdot|S_{k}^{c},A_{k}^{c})$ . ", "page_idx": 4}, {"type": "text", "text": "The generative model assumption TD1 is common in TD learning [8, 30, 42, 45]. It is possible to generalize all our results to the more general Assumption TD 2, sampling over a single trajectory and leveraging the Markovian noise dynamics. This would have a similar impact on our results on $\\mathrm{TD}(0)$ as it has on the ones we will present for general FedLSA in Section 4. In our analysis, we require the following assumption on the feature design matrix $\\Sigma_{\\varphi}^{c}=\\mathbb{E}_{\\mu^{c}}[\\varphi(S_{0}^{c})\\varphi(S_{0}^{c})^{\\top}]\\in\\mathbf{\\bar{R}}^{d\\times d}$ . ", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{\\nabla}T D\\,3$ . Matrices $\\Sigma_{\\varphi}^{c}$ are non-degenerate with the minimal eigenvalue $\\begin{array}{r}{\\nu=\\operatorname*{min}_{c\\in[N]}\\lambda_{\\operatorname*{min}}(\\Sigma_{\\varphi}^{c})>0}\\end{array}$ .   \nMoreover, the feature mapping $\\varphi(\\cdot)$ satisfies $\\operatorname*{sup}_{s\\in S}\\|\\varphi(s)\\|\\leq1$ . ", "page_idx": 4}, {"type": "text", "text": "This assumption ensures the uniqueness of the optimal parameter $\\theta_{\\star}^{c}$ . Under TD1 and TD3 we check the LSA assumptions A1 and A3, and the following holds. ", "page_idx": 4}, {"type": "text", "text": "Claim 3.1. Assume TD 1 and TD 3. Then the sequence of $T D(O)$ updates satisfies A1 and A3 with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf C}_{\\bf A}=1+\\gamma\\;,\\qquad\\|\\Sigma_{\\widetilde{{\\bf A}}}^{c}\\|\\leq2(1+\\gamma)^{2}\\;,\\qquad\\mathrm{Tr}(\\Sigma_{\\varepsilon}^{c})\\leq2(1+\\gamma)^{2}\\left(\\|\\theta_{\\star}^{c}\\|^{2}+1\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad a=\\frac{(1-\\gamma)\\nu}{2}\\;,\\qquad\\eta_{\\infty}=\\frac{(1-\\gamma)}{4}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We prove this claim in Appendix E.1, and refer to [45, 42] for more details on the link between TD and linear stochastic approximation. ", "page_idx": 4}, {"type": "text", "text": "4 Refined Analysis of the FedLSA Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Stochastic expansion for FedLSA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use the error expansion framework [1, 14] for LSA to analyze the MSE of the estimates $\\theta_{t}$ generated by Algorithm 1. For this purpose, we rewrite local update (4) as $\\theta_{t,h}^{c}-\\theta_{\\star}^{c}=\\mathbf{\\bar{(I}}-\\eta\\mathbf{A}(Z_{t,h}^{c^{\\prime}}))(\\theta_{t,h-1}^{\\bar{c}}-\\theta_{\\star}^{c})-\\eta\\varepsilon^{c}(Z_{t,h}^{c})$ , where $\\varepsilon^{c}(z)$ is defined in (2). Running this recursion until the start of local training, we obtain ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t,H}^{c}-\\theta_{\\star}^{c}=\\Gamma_{t,1:H}^{(c,\\eta)}\\{\\theta_{t,0}^{c}-\\theta_{\\star}^{c}\\}-\\eta\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta)}\\varepsilon^{c}(Z_{t,h}^{c})\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\varepsilon^{c}(z)$ is as in (3), and we recall that $\\theta_{t,0}^{c}=\\theta_{t-1}$ , $\\forall c\\in[N]$ . We also introduced the notation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Gamma_{t,m:n}^{(c,\\eta)}=\\prod_{h=m}^{n}(\\mathrm{I}-\\eta\\mathbf{A}(Z_{t,h}^{c}))\\;,\\quad1\\leq m\\leq n\\leq H\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "iws,i tfho tr haen cy i, own $\\Gamma_{t,m;n}^{(c,\\eta)}=\\mathrm{I}$ $m>n$ $\\Gamma_{t,m:n}^{(c,\\eta)}$ siisn egx tphoe nfeanctti , Tahnadt $h\\in\\mathbb N$ $\\begin{array}{r l}{\\quad}&{{}\\mathbb{E}^{1/2}\\left[\\|\\Gamma_{t,m:m+h}^{(c,\\eta)}u\\|^{2}\\right]\\leq(1-\\eta a)^{h}\\|u\\|}\\end{array}$ $\\theta_{t,0}^{c}=\\theta_{t-1}$ employing (1), we obtain that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t}-\\theta_{\\star}=\\bar{\\Gamma}_{t,H}^{(\\eta)}\\{\\theta_{t-1}-\\theta_{\\star}\\}+\\bar{\\rho}_{H}+\\bar{\\tau}_{t,H}-\\eta\\bar{\\varphi}_{t,H}\\ ,\\qquad\\mathrm{with}\\quad\\bar{\\Gamma}_{t,H}^{(\\eta)}=N^{-1}{\\sum_{c=1}^{N}}\\Gamma_{t,1:H}^{(c,\\eta)}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "wher $\\begin{array}{r}{\\mathfrak{I}_{t,H}=\\frac{1}{N}\\sum_{c=1}^{N}\\bigl\\{\\bigl(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c}\\bigr)^{H}-\\Gamma_{t,1:H}^{(c,\\eta)}\\bigr\\}\\bigl\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\bigr\\},\\bar{\\varphi}_{t,H}=\\frac{1}{N}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta)}\\varepsilon^{c}\\bigl(Z_{t,h}^{c}\\bigr)}\\end{array}$ are zero-mean fluctuation terms, and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\rho}_{H}=\\frac{1}{N}{\\sum_{c=1}^{N}}({\\mathrm{I}-(\\mathrm{I}-\\eta{{\\bar{\\bf A}}^{c}})^{H}})\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is the deterministic heterogeneity bias accumulated in one round of local training. Note that $\\bar{\\rho}_{H}$ vanishes when either (i) agents are homogeneous, or (ii) number of local updates is $H=1$ . To analyze FedLSA, we run the recurrence (6) to obtain the decomposition ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t}-\\theta_{\\star}=\\tilde{\\theta}_{t}^{\\mathrm{(tr)}}+\\tilde{\\theta}_{t}^{\\mathrm{(bi,bi)}}+\\tilde{\\theta}_{t}^{\\mathrm{(fl)}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $\\begin{array}{r}{\\tilde{\\theta}_{t}^{\\mathrm{(tr)}}=\\prod_{s=1}^{t}\\bar{\\Gamma}_{s,H}^{(\\eta)}\\{\\theta_{0}-\\theta_{\\star}\\}}\\end{array}$ is a transient term that vanishes geometrically, $\\ensuremath{\\tilde{\\theta}}_{t}^{(\\mathsf{f l})}$ is a zero-mean fluctuation term, with detailed expression provided in Appendix A, and the term $\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\theta}_{t}^{(\\mathbf{b}^{\\mathrm{i}},\\mathbf{b}\\mathrm{i})}=\\sum_{s=1}^{t}(\\bar{\\Gamma}_{H}^{(\\eta)})^{t-s}\\bar{\\rho}_{H}\\ ,\\quad\\mathrm{~where~}\\bar{\\Gamma}_{H}^{(\\eta)}=\\mathbb{E}[\\bar{\\Gamma}_{s,H}^{(\\eta)}]\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and accounts for the bias of FedLSA due to local training, that vanishes whenever $\\bar{\\rho}_{H}=0$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Convergence rate of FedLSA for i.i.d. observation model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "First, we analyze the rate at which FedLSA converges to a biased solution $\\theta_{\\star}+\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}$ . The following two quantities, which stem from the heterogeneity and stochasticity of the local estimators, play a central role in this rate ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\sigma}_{\\varepsilon}=\\mathbb{E}_{c}\\left[\\mathrm{Tr}(\\Sigma_{\\varepsilon}^{c})\\right]\\ ,\\quad\\tilde{v}_{\\mathrm{heter}}=\\mathbb{E}_{c}\\left[\\|\\Sigma_{\\widetilde{\\textbf{A}}}^{c}\\|\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}\\right]\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $\\bar{\\sigma}_{\\varepsilon}$ and $\\tilde{v}_{\\mathrm{heter}}$ correspond to the different sources of noise in the error decomposition (7). The term $\\bar{\\sigma}_{\\varepsilon}$ is related to the variance of the local LSA iterate on each of the agents, while $\\tilde{v}_{\\mathrm{heter}}$ controls the bias fluctuation term. In the centralized setting (i.e. if $N=1$ ), the $\\tilde{v}_{\\mathrm{heter}}$ term disappears, but not the $\\bar{\\sigma}_{\\varepsilon}$ term. We now proceed to analyze the MSE of the iterates of FedLSA : ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Assume A1 and A3. Then for any step size $\\eta\\in(0,\\eta_{\\infty})$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{1/2}\\big[\\|\\theta_{t}-\\tilde{\\theta}_{t}^{(\\mathsf{b},\\mathsf{b})}-\\theta_{\\star}\\|^{2}\\big]\\lesssim\\sqrt{\\frac{\\eta\\tilde{v}_{h e t r}}{a N}}+\\sqrt{\\frac{\\eta\\bar{\\sigma}_{\\varepsilon}}{a N}}+\\sqrt{\\frac{\\mathbb{E}_{c}[\\|\\Sigma_{\\hat{\\mathbf{A}}}^{\\mathsf{c}}\\|]}{H N}}\\frac{\\|\\bar{\\rho}_{H}\\|}{a}+(1-\\eta a)^{t H}\\|\\theta_{0}-\\theta_{\\star}\\|\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the bias $\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}$ converges in expectation to $\\tilde{\\theta}_{\\infty}^{(\\mathsf{b i},\\mathsf{b i})}=(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}$ at a geometric rate, and is uniformly bounded by $\\begin{array}{r}{\\mathbb{E}^{1/2}[\\|\\tilde{\\theta}_{t}^{(\\mathbf{bi},\\mathbf{bi})}\\|^{2}]\\lesssim\\frac{\\eta H\\mathbb{E}_{c}[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|]}{a}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 4.1 relies on bounding each term from (7). We provide a proof with explicit constants in Appendix A. Importantly, the fluctuation terms scale linearly with $N$ . Moreover, in the centralized setting (that is, $N=1$ ), the bias terms $\\bar{\\rho}_{H}$ , $\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}$ and $\\tilde{v}_{\\mathrm{heter}}$ vanish in Theorem 4.1, yielding the last-iterate bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{1/2}\\big[\\|\\theta_{t}-\\theta_{\\star}\\|^{2}\\big]\\lesssim\\sqrt{\\frac{\\eta\\bar{\\sigma}_{\\varepsilon}}{a}}+(1-\\eta a)^{t H}\\|\\theta_{0}-\\theta_{\\star}\\|\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is known to be sharp in its dependence on $\\eta$ for single-agent LSA (see Theorem 5 in [15]).   \nBased on Claim 3.1, Theorem 4.1 translates for federated $\\mathrm{TD}(0)$ as follows. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.2. Assume ${\\mathbf{}}T D\\,l$ and TD3. Then for any step size $\\textstyle\\eta\\in(0,{\\frac{1-\\gamma}{4}})$ , the iterates of federated $T D(O)$ satisfy, with $\\chi(\\theta_{\\star},\\theta_{\\star}^{1},\\ldots,\\theta_{\\star}^{N})=\\mathbb{E}_{c}[\\|\\dot{\\theta}_{\\star}^{c}-\\dot{\\theta}_{\\star}\\|^{2}]\\,\\lor(1+\\mathbb{E}_{c}[\\|\\theta_{\\star}^{c}\\|^{2}])$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{1/2}\\big[\\|\\theta_{t}-\\tilde{\\theta}_{t}^{(\\mathrm{b}_{i},\\mathrm{b}_{i})}-\\theta_{\\star}\\|^{2}\\big]\\lesssim\\sqrt{\\frac{\\eta\\chi(\\theta_{\\star},\\theta_{\\star}^{1},\\ldots,\\theta_{\\star}^{N})}{(1-\\gamma)\\nu N}}+\\sqrt{\\frac{1}{H N}}\\frac{\\|\\tilde{\\rho}_{H}\\|}{(1-\\gamma)\\nu}+\\big(1-\\frac{\\eta(1-\\gamma)\\nu}{2}\\big)^{t H}\\|\\theta_{0}-\\theta_{\\star}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The right-hand side of Corollary 4.2 scales linearly with $N$ , allowing for linear speed-up. This is in line with recent results on federated TD(0), which shows linear speed-up either without local training [7] or up to a possibly large bias term [50] (see analysis of their Theorem 2). While Corollary 4.2 shows the algorithm\u2019s convergence to some fixed, biased value, one can set the parameters of FedLSA such that this bias is small. This allows to rewrite the result of Theorem 4.1 in order to get a sample complexity bound in the following form. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.3. Assume A1 and $A3$ . Let $H>1$ , and $\\begin{array}{r}{0<\\epsilon<\\frac{\\left(\\sqrt{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}\\mathbb{E}_{c}\\left[\\Vert\\theta_{\\star}^{c}-\\theta_{\\star}\\Vert\\right]\\right)^{2/5}}{a}\\vee\\frac{\\mathbb{E}_{c}\\left[\\Vert\\theta_{\\star}^{c}-\\theta_{\\star}\\Vert\\right]}{a\\,\\mathrm{C}_{\\mathbf{A}}}.}\\end{array}$ Set the step size $\\begin{array}{r}{\\eta=\\mathcal{O}\\big(\\frac{a N\\epsilon^{2}}{\\tilde{v}_{h e t e r}\\vee\\tilde{\\sigma}_{\\varepsilon}}\\wedge\\eta_{\\infty}\\big)}\\end{array}$ and the number of local steps $\\begin{array}{r}{H=\\mathcal{O}\\big(\\frac{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}{\\mathbb{E}_{c}[\\Vert\\theta_{\\star}^{c}-\\theta_{\\star}\\Vert]}\\frac{1}{N\\epsilon}\\big)}\\end{array}$ . Then, to achieve $\\mathbb{E}\\big[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}\\big]<\\epsilon^{2}$ the required number of communications for federated $L S A$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=\\mathcal{O}\\left(\\left(\\frac{1}{a\\eta_{\\infty}}\\lor\\frac{\\mathbb{E}_{c}\\left[\\parallel\\theta_{\\star}^{c}-\\theta_{\\star}\\parallel\\right]}{a^{2}\\epsilon}\\right)\\log\\frac{\\parallel\\theta_{0}-\\theta_{\\star}\\parallel}{\\epsilon}\\right)\\,\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Corollary 4.3, the number of oracle calls scales as $\\begin{array}{r}{T H=\\mathcal{O}\\big(\\frac{\\tilde{v}_{\\mathrm{heter}}\\vee\\bar{\\sigma}_{\\varepsilon}}{N a^{2}\\epsilon^{2}}\\log\\frac{\\lVert\\theta_{0}-\\theta_{\\star}\\rVert}{\\epsilon}\\big)}\\end{array}$ , which shows that FedLSA has linear speed-up. Importantly, the number of communications $T$ required to achieve precision $\\epsilon^{2}$ scales as $\\epsilon^{-1}$ . In the next section, we will show how this dependence on $\\epsilon^{-1}$ can be reduced from polynomial to logarithmic. Now we state the communication bound of federated TD(0). Corollary 4.4. Assume TD $^{\\,l}$ and TD 3. Then for any 0 < \u03f5 < g(11(\u03b8\u2212c\u22c6\u03b3,)\u03b8\u03bd\u22c6) with $g_{1}=\\mathcal{O}((1+\\|\\theta_{\\star}\\|)\\mathbb{E}_{c}[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|])$ . Set $\\begin{array}{r}{\\eta=\\mathcal{O}\\left(\\frac{(1-\\gamma)\\nu N\\epsilon^{2}}{\\mathbb{E}_{c}[\\parallel\\theta_{\\star}^{c}\\parallel^{2}]+1}\\right)}\\end{array}$ and $\\begin{array}{r}{H=\\mathcal{O}\\left(\\frac{\\mathbb{E}_{c}\\left[\\|\\theta_{\\star}^{c}\\|^{\\frac{3}{2}}+1\\right]^{\\prime\\prime}}{N\\epsilon\\mathbb{E}_{c}\\left[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}\\right]}\\right)}\\end{array}$ . Then, to achieve $\\mathbb{E}\\big[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}\\big]<\\epsilon^{2}$ , the required number of communications for federated $T D(O)$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=\\mathcal{O}\\left(\\left(\\frac{1}{(1-\\gamma)^{2}\\nu}\\lor\\frac{\\mathbb{E}_{c}[\\|\\theta_{*}^{c}-\\theta_{*}\\|]}{(1-\\gamma)^{2}\\nu^{2}\\epsilon}\\right)\\log\\frac{\\|\\theta_{0}-\\theta_{*}\\|}{\\epsilon}\\right)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Corollary 4.4 is the first result to show that, even with local training and heterogeneous agents, federated TD(0) can converge to $\\theta_{\\star}$ with arbitrary precision. Importantly, this result preserves the linear speed-up effect, showing that federated learning indeed accelerates the training. ", "page_idx": 5}, {"type": "text", "text": "4.3 Convergence of FedLSA under Markovian observations model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The analysis of FedLSA can be generalized to the setting where observations $\\{Z_{k}^{c}\\}_{k\\in\\mathbb{N}}$ form a Markov chain with kernel $\\mathrm{P}_{c}$ . To handle the Markovian nature of observations, we propose a variant of FedLSA that skips some observations (see the full procedure in Appendix B). This follows classical schemes for Markovian data in optimization [40], as adjusting the number of skipped observations (keeping about 1 observation out of $\\tau_{\\mathrm{mix}}(c))$ allows to control the correlation of successive observations. We may now state the counterpart of Corollary 4.3 for the Markovian setting. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.5 (Corollary 4.3 adjusted to the Markov samples). Assume $A\\,2$ and A3 and let $0<$ $\\begin{array}{r}{\\epsilon<\\frac{\\left(\\sqrt{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}\\mathbb{E}_{c}\\left[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|\\right]\\right)^{2/5}}{a}\\vee\\frac{\\mathbb{E}_{c}\\left[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|\\right]}{a\\,\\mathrm{C}_{\\mathbf{A}}}}\\end{array}$ . Set the step size $\\begin{array}{r}{\\eta=\\mathcal{O}\\big(\\frac{a N\\epsilon^{2}}{\\tilde{v}_{h e t e r}\\sqrt{\\bar{\\sigma}_{\\varepsilon}}}\\land\\eta_{\\infty}\\land\\eta_{\\infty}^{\\mathrm{(M)}}\\big)}\\end{array}$ \u2227\u03b7\u221e\u2227\u03b7(\u221eM) , where we give the expression of $\\eta_{\\infty}^{\\mathrm{(M)}}$ is (37). Then, for the iterates of Algorithm 3, in order to achieve $\\mathbb{E}\\big[\\|\\breve{\\theta}_{T}-\\theta_{\\star}\\|^{2}\\big]\\leq\\epsilon^{2}$ , the required number of communication is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=\\mathcal{O}\\left(\\left(\\frac{1}{a\\eta_{\\infty}}\\lor\\frac{\\mathbb{E}_{c}\\left[\\parallel\\theta_{\\star}^{c}-\\theta_{\\star}\\parallel\\right]}{a^{2}\\epsilon}\\right)\\log\\frac{\\parallel\\theta_{0}-\\theta_{\\star}\\parallel}{\\epsilon}\\right)\\mathrm{~,~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the number of local updates $H$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{H}{\\log H}=\\mathcal{O}\\left(\\frac{\\tilde{v}_{h e t r}\\sqrt{\\bar{\\sigma}_{e}}}{\\mathbb{E}_{c}[\\|\\theta_{*}^{c}-\\theta_{*}\\|]}\\frac{\\operatorname*{max}_{c}\\tau_{\\operatorname*{mix}}(c)\\log\\big(N T^{3}(\\|\\theta_{0}-\\theta_{*}\\|+2\\mathbb{E}_{c}[\\|\\theta_{*}^{c}-\\theta_{*}\\|]+\\eta\\|\\varepsilon\\|_{\\infty})/\\epsilon^{2}\\big)}{N\\epsilon}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Corollary 4.3 follows the idea outlined in [40], using Berbee\u2019s lemma [10]. We give all the details in Appendix B. This result is very similar to Corollary 4.3. Most crucially, it shows that the communication complexity is the same, regardless of the type of noise. The differences with Corollary 4.3 lie in (i) the number local updates $H$ , that is scaled by $\\tau_{\\mathrm{mix}}$ (up to logarithmic factors), and (ii) the additional condition $\\eta\\leq\\dot{\\eta}_{\\infty}^{\\mathrm{(M)}}$ , that allows verifying the stability of random matrix products with Markovian dependence (see Lemma B.2 in the appendix). ", "page_idx": 6}, {"type": "text", "text": "Remark 4.6. Although, for clarity of exposition, we only state the counterpart of Corollary 4.3 in the Markovian result, all of our results can be extended to Markovian observations using the same ideas. ", "page_idx": 6}, {"type": "text", "text": "5 SCAFFLSA: Federated LSA with Bias Correction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Stochastic Controlled Averaging for Federated LSA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now introduce the Stochastic Controlled Averaging for Federated LSA algorithm (SCAFFLSA), an improved version of FedLSA that mitigates client drift using control variates. This method is inspired by Scaffnew (see 37). In SCAFFLSA, each agent $c\\in[N]$ keeps a local variable $\\xi_{t}^{c}$ , that remains constant during each communication round $t$ . Agents perform local updates on the current estimates of the parameters $\\widehat{\\theta}_{t,0}^{c}=\\theta_{t}$ for $c\\in[N]$ , and for $h\\in[H]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta}_{t,h}^{c}=\\hat{\\theta}_{t,h-1}^{c}-\\eta(\\mathbf{A}^{c}(Z_{t,h}^{c})\\hat{\\theta}_{t,h-1}^{c}-\\mathbf{b}^{c}(Z_{t,h}^{c})-\\xi_{t}^{c})\\mathrm{~,~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "At the end of the round, (i) the agents communicate the current estimate to the central server, (ii) the central server averages local iterates, and (iii) agents update their local control variates; see Algorithm 2. By defining the ideal control variates at the global solution, given by $\\begin{array}{r}{\\xi_{\\star}^{c}=\\bar{\\mathbf{A}}^{c}\\theta_{\\star}-\\bar{\\mathbf{b}}^{c}=}\\end{array}$ $\\bar{\\mathbf{A}}^{\\breve{c}}(\\theta_{\\star}-\\theta_{\\star}^{c})$ , we can rewrite the local update as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{t,h}^{c}-\\theta_{\\star}=(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z_{t,h}^{c}))(\\hat{\\theta}_{t,h-1}^{c}-\\theta_{\\star})+\\eta(\\xi_{t}^{c}-\\xi_{\\star}^{c})-\\eta\\omega^{c}(Z_{t,h}^{c})\\ ,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\omega^{c}(z)$ is defined in (2). Under A1, it has finite covariance $\\begin{array}{r}{\\Sigma_{\\omega}^{c}=\\int_{Z}\\omega^{c}(z)\\omega^{c}(z)^{\\top}\\mathrm{d}\\pi_{c}(z).}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Similarly to the analysis of FedLSA, we use (8) to describe the sequence of aggregated iterates and control variates as, for $t\\geq0$ and $c\\in[N]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}-\\theta_{\\star}=\\bar{\\Gamma}_{t,H}^{(\\eta)}\\big(\\theta_{t}-\\theta_{\\star}\\big)+\\frac{\\eta}{N}\\!\\sum_{c=1}^{N}\\!C_{t+1}^{c}\\big(\\xi_{t}^{c}\\!-\\!\\xi_{\\star}^{c}\\big)-\\eta\\bar{\\omega}_{t+1}\\,\\,,}\\\\ &{\\xi_{t+1}^{c}-\\xi_{\\star}^{c}=\\xi_{t}^{c}-\\xi_{\\star}^{c}+\\frac{1}{\\eta H}\\big(\\theta_{t+1}-\\theta_{t,H}\\big)\\,\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{C_{t+1}^{c}=\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{\\left(c,\\eta\\right)}}\\end{array}$ and $\\begin{array}{r}{\\bar{\\omega}_{t+1}\\,=\\,\\frac{1}{N}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta)}\\omega^{c}(Z_{t,h}^{c})}\\end{array}$ . We now state the convergence rate, as well as sample and communication complexity of Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 SCAFFLSA: Stochastic Controlled FedLSA with deterministic communication ", "page_idx": 7}, {"type": "text", "text": "Input: $\\eta>0,\\theta_{0},\\xi_{0}^{c}\\in\\mathbb{R}^{d},T,N,H$   \nfor $t=1$ to $T$ do for $c=1$ to $N$ do Set $\\widehat{\\theta}_{t,0}^{c}=\\theta_{t}$ for $h=1$ to $H$ do Receive $Z_{t,h}^{c}$ and perform local update $\\hat{\\theta}_{t,h}^{c}=\\hat{\\theta}_{t,h-1}^{c}-\\eta(\\mathbf{A}^{c}(Z_{t,h}^{c})\\hat{\\theta}_{t,h-1}^{c}-\\mathbf{b}^{c}(Z_{t,h}^{c})-\\xi_{t}^{c})$ Aggregate local iterates: $\\begin{array}{r}{\\theta_{t+1}=\\frac{1}{N}\\sum_{c=1}^{N}\\hat{\\theta}_{t,H}^{c}}\\end{array}$ Update local control variates: $\\begin{array}{r}{\\xi_{t+1}^{c}=\\xi_{t}^{c}+\\frac{1}{\\eta H}(\\theta_{t+1}-\\hat{\\theta}_{t,H}^{c})}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1. Assume A1, $A3$ . Let $\\eta,H>0$ such that $\\eta\\leq\\eta_{\\infty}$ , and $H\\leq a/240\\eta\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\lVert\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\rVert\\right\\}$ . Set $\\xi_{0}^{c}=0$ for all $c\\in[N]$ . Then we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\theta_{T}-\\theta_{*}\\|^{2}]\\lesssim\\frac{\\eta}{N a}\\|\\Sigma_{\\omega}\\|+\\big(1-\\frac{\\eta a H}{2}\\big)^{T}\\Big\\{\\|\\theta_{0}-\\theta_{*}\\|^{2}+\\eta^{2}H^{2}\\mathbb{E}_{c}[\\|\\bar{\\mathbf{A}}^{c}(\\theta_{*}^{c}-\\theta_{*})\\|^{2}]\\Big\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Corollary 5.2. Let $\\epsilon>0$ . Set the step size $\\eta=\\mathcal{O}(\\operatorname*{min}(\\eta_{\\infty},N a\\epsilon/\\bar{\\sigma}))$ and the number local updates to $\\begin{array}{r}{H=\\mathcal{O}\\big(\\operatorname*{max}\\bigl(\\frac{a}{\\eta(\\mathrm{C}+\\|\\Sigma\\cdot\\|)},\\frac{\\|\\Sigma\\|}{N\\epsilon(\\mathrm{C}+\\|\\Sigma\\cdot\\|)}\\bigr)\\big)}\\end{array}$ . Then, to achieve $\\mathbb{E}[\\lVert{\\boldsymbol{\\theta}}_{T}\\,-\\,{\\boldsymbol{\\theta}}_{\\star}\\rVert^{2}]\\,\\le\\,\\epsilon^{2}$ , the required number of communication  for SCAFF LSA is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=\\mathcal{O}\\Big(\\frac{\\mathrm{C}+\\|\\Sigma\\cdot\\|}{a}\\log\\Big(\\frac{\\|\\theta-\\theta\\|+\\mathbb{E}[\\|\\bar{\\mathbf{A}}\\left(\\theta-\\theta\\right)\\|]a/\\mathrm{C}}{\\epsilon}\\Big)\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We provide detailed proof of these statements in Appendix C. They are based on a novel analysis, where we study virtual parameters $\\check{\\theta}_{t,h}^{c}$ , that follow the same update as (8), without the last term $\\eta\\omega^{c}(Z_{t,h}^{c})$ . After each round, virtual parameters are aggregated, and virtual control variate updated as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\check{\\theta}_{t+1}-\\theta_{\\star}=\\frac{1}{N}{\\sum_{c=1}^{N}}\\check{\\theta}_{t,H}^{c}\\;,\\;\\mathrm{and}\\;\\check{\\xi}_{t+1}^{c}-\\xi_{\\star}^{c}=\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c}+\\frac{1}{\\eta H}(\\check{\\theta}_{t+1}-\\check{\\theta}_{t,H})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This allows to decompose ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta_{\\star}=\\check{\\theta}_{t}-\\theta_{\\star}+\\widetilde{\\theta}_{t}\\ ,\\ \\mathrm{and}\\ \\xi_{t}^{c}-\\xi_{\\star}^{c}=\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c}+\\widetilde{\\xi}_{t}^{c}\\ ,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\check{\\theta}_{t}-\\theta_{\\star}$ and $\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c}$ are transient terms, and $\\widetilde{\\theta}_{t}\\,=\\,\\theta_{t}\\,-\\,\\widetilde{\\theta}_{t}$ and $\\widetilde{\\xi}_{t}^{c}\\,=\\,\\xi_{t}^{c}-\\check{\\xi}_{t}^{c}$ capture the fluctuations of the parameters and control variates. ", "page_idx": 7}, {"type": "text", "text": "We stress that our analysis shows that, in comparison with FedLSA, the SCAFFLSA algorithm reduces communication complexity while preserving the linear speed-up in the number of agents. This is in stark contrast with existing analyses of control-variate methods in heterogeneous federated learning, that either have large communication cost, or lose the linear speed-up [24, 37, 21]. To obtain this result, we conduct a very careful analysis of the propagation of variances and covariances of $\\widetilde{\\theta}_{t}$ and $\\widetilde{\\xi}_{t}^{c}$ between successive communication rounds. We describe this in full detail in Appendix C.2. ", "page_idx": 7}, {"type": "text", "text": "In Corollary 5.2, we show that the total number of communications depends only logarithmically on the precision $\\epsilon$ . This is in stark contrast with Algorithm 1, where the necessity of controlling the bias magnitude prevents from scaling $H$ with $^1/\\epsilon$ 2. Additionally, this shows that the number of required local updates reduces as the number of agents grows. Thus, in the high precision regime (i.e.small $\\epsilon$ and $\\eta$ ), using control variates reduces communication complexity compared to FedLSA. ", "page_idx": 7}, {"type": "text", "text": "5.2 Application to Federated TD(0) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Applying SCAFFLSA to TD learning, we obtain SCAFFTD(0) (see Algorithm 5 in Appendix E). The analysis of SCAFFLSA directly translates to SCAFFTD(0), resulting in the following communication complexity bound. ", "page_idx": 7}, {"type": "text", "text": "Corollary 5.3. Assume ${\\mathbf{\\nabla}T\\mathbf{}D\\mathbf{\\nabla}I}$ and TD 3 and let $0\\,<\\,\\epsilon\\,\\leq\\,\\sqrt{8\\mathbb{E}_{c}[1+\\|\\theta_{\\star}^{c}\\|^{2}]/((1-\\gamma)\\nu)}$ . Set the step size $\\begin{array}{r}{\\eta=\\mathcal{O}(\\frac{(1-\\gamma)\\nu N\\epsilon}{||\\theta||+1})}\\end{array}$ and the number local updates to $\\begin{array}{r}{H=\\mathcal{O}\\big(\\frac{||\\theta||+1}{N\\epsilon}\\big)}\\end{array}$ . Then, to achieve $\\mathbb{E}[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}]\\leq\\epsilon^{2}$ , the required number of communication for SCAFFTD(0) is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=\\mathcal{O}\\left(\\frac{1}{(1-\\gamma)\\nu}\\log\\left(\\frac{\\|\\theta-\\theta\\|+(1-\\gamma)\\nu\\mathbb{E}[\\|\\theta-\\theta\\|]}{\\epsilon}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/574e3223409a593322b44e9a8b52d3fe25859025c107b9d006bd028b954f3c11.jpg", "img_caption": ["Figure 1: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated TD(0) in homogeneous and heterogeneous settings, for different number of agents and number of local steps. Green dashed line is FedLSA\u2019s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over 5 runs. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Corollary 5.3 confirms that, when applied to TD(0), SCAFFLSA\u2019s communication complexity depends only logarithmically on heterogeneity and on the desired precision. In contrast with existing methods for federated TD(0) [11, 22, 50], it converges even with many local steps, whose number diminishes linearly with the number of agents $N$ , producing the linear speed-up effect. ", "page_idx": 8}, {"type": "text", "text": "Remark 5.4. In Appendix F, we extend the analysis of Scaffnew [37] to the LSA setting. Their analysis does not exploit the fact that agents\u2019 estimators are not correlated, and thus lose the linear speed-up. In contrast, our novel analysis technique carefully tracks correlations between parameters and control variates throughout the run of the algorithm. ", "page_idx": 8}, {"type": "text", "text": "6 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we demonstrate the performance of FedLSA and SCAFFLSA under varying levels of heterogeneity. We consider the Garnet problem [2, 16], with $n=30$ states embedded in $d=8$ dimensions, $a=2$ actions, and each state is linked to $b=2$ others in the transition kernel. We aim to estimate the value function of the policy which chooses actions uniformly at random, in homogeneous and heterogeneous setups. In all experiments, we initialize the algorithms in a neighborhood of the solution, allowing to observe both transient and stationary regimes. We provide all details regarding the experimental setup in Appendix G. Our code is available either as supplementary material or online on GitHub: https://github.com/pmangold/scafflsa. ", "page_idx": 8}, {"type": "text", "text": "SCAFFLSA properly handles heterogeneity. This heterogeneous scenario is composed of two different Garnet environments, that are each held by half of the agents, with small perturbations. Such a setting may arise in cases where each agent\u2019s environment reflects only a part of the world. For instance, if half of the individuals live in the city, while the other half live in the countryside: both have different observations, but learning a shared value function gives a better representation of the overall reality. In Figures 1(a) to 1(d), we plot the MSE with $N\\in\\{10,100\\}$ , $H\\in\\{10,1000\\}$ and $\\eta=0.1$ , with the same total number of updates $T H=500,000$ . As predicted by our theory, FedLSA stalls when the number of local updates increases, and its bias (green dashed line in Figures 1(a) to 1(d)) is in line with the value predicted by our theory (see Theorem 4.1). For completeness, we plot the error of FedLSA in estimating $\\theta_{\\star}+\\tilde{\\theta}_{\\infty}^{(\\tt b i,b i)}$ in Appendix G. On the opposite, SCAFFLSA\u2019s bias-correction mechanism allows to eliminate all bias, improving the MSE until noise dominates. ", "page_idx": 8}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/13381a420011e7ae7ff5825d9e043ac6bba5ca440ec47cfd1af356ca3bd5554e.jpg", "img_caption": ["Figure 2: MSE, averaged over 10 runs, for last iterates of FedLSA (dashed lines) and SCAFFLSA (solid lines) in the stationary regime, as a function of the number of agents, in different federated TD(0) problems. The black dotted line decreases in $1/N$ , serving as a visual guide for linear speed-up. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Both algorithms behave alike in homogeneous settings. In the homogeneous setting, we create one instance of a Garnet environment. Then, each agent receives a slightly perturbed variant of this environment. This illustrates a situation where all agents solve the same exact problem, but may have small divergences in their measures of states and rewards. We plot the MSE in Figures 1(e) to 1(h) with $N\\in\\{10,100\\}$ agents, $\\eta=0.1$ , and $H\\in\\{10,1000\\}$ , with the same total number of updates $T H=500,000$ . In this case, as predicted in Corollary 4.3, the number of local steps $H$ has little influence on the final MSE. Since agents are homogeneous, control variates have virtually no effect, and SCAFFLSA is on par with FedLSA. The MSE is dominated by the noise term, which diminishes with the step size (see additional experiments in Appendix G with smaller $\\eta=0.01]$ ). ", "page_idx": 9}, {"type": "text", "text": "Both algorithms enjoy linear speed-up! In Figure 2, we plot the MSE obtained once algorithms reach the stationary regime, as a function of the number of agents $N=1$ to 1000, for step sizes $\\eta\\,\\in\\,\\{0.001,0.01,\\dot{0}.1,\\dot{1}\\}$ and $H\\,\\in\\,\\{1,100\\}$ , in both homogeneous and heterogeneous settings. Whenever (i) agents are homogeneous, or (ii) the number of local steps is small, both FedLSA and SCAFFLSA can achieve similar precision with a step size that increases with the number of agents. This allows to use larger step sizes, so as to reach a given precision level faster, resulting in the so-called linear speed-up. However, when agents are heterogeneous and the number of local updates increases, FedLSA loses the speed-up due to large bias. Remarkably, and as explained by our theory (see Corollary 5.2), SCAFFLSA maintains this speed-up even in heterogeneous settings. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we studied the role of heterogeneity in federated linear stochastic approximation. We proposed a new analysis of FedLSA, where we formally characterize FedLSA\u2019s bias. This allows to show that, with proper hyperparameter setting, FedLSA (i) can converge to arbitrary precision even with local training, and (ii) enjoys linear speed-up in the number of agents. We then proposed a novel algorithm, SCAFFLSA, that uses control variates to allow for extended local training. We analyzed this method using on a novel analysis technique, and formally proved that control variates reduce communication complexity of the algorithm. Importantly, our analysis shows that SCAFFLSA preserves the linear speed-up, which is the first time that a federated algorithm provably accelerates while preserving this linear speed-up. Finally, we instantiated our results for federated TD learning, and conducted an empirical study that demonstrates the soundness of our theory in this setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of P. Mangold and S. Labbi has been supported by Technology Innovation Institute (TII), project Fed2Learn. The work of E. Moulines has been partly funded by the European Union (ERC2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. The work of I. Levin, A. Naumov and S. Samsonov was prepared within the framework of the ", "page_idx": 9}, {"type": "text", "text": "HSE University Basic Research Program. This research was supported in part through computational resources of HPC facilities at HSE University [29]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rafik Aguech, Eric Moulines, and Pierre Priouret. On a perturbation approach for the analysis of stochastic tracking algorithms. SIAM Journal on Control and Optimization, 39(3):872\u2013899, 2000. [2] TW Archibald, KIM McKinnon, and LC Thomas. On the generation of markov decision processes. Journal of the Operational Research Society, 46(3):354\u2013361, 1995. [3] H.C.P. Berbee. Random Walks with Stationary Increments and Renewal Theory. Mathematical Centre tracts. Centrum Voor Wiskunde en Informatica, 1979.   \n[4] J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal difference learning with linear function approximation. In Conference On Learning Theory, pages 1691\u20131692, 2018.   \n[5] Laurent Condat, Ivan Agarsky, and Peter Richt\u00e1rik. Provably doubly accelerated federated learning: The first theoretically successful combination of local training and compressed communication. arXiv preprint arXiv:2210.13277, 2022. [6] Laurent Condat and Peter Richt\u00e1rik. Randprox: Primal-dual optimization algorithms with randomized proximal updates. arXiv preprint arXiv:2207.12891, 2022.   \n[7] Nicol\u00f2 Dal Fabbro, Aritra Mitra, and George J Pappas. Federated td learning over finite-rate erasure channels: Linear speedup under markovian sampling. IEEE Control Systems Letters, 2023.   \n[8] G. Dalal, Bal\u00e1zs Sz\u00f6r\u00e9nyi, G. Thoppe, and S. Mannor. Finite sample analyses for TD(0) with function approximation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.   \n[9] Christoph Dann, Gerhard Neumann, Jan Peters, et al. Policy evaluation with temporal differences: A survey and comparison. Journal of Machine Learning Research, 15:809\u2013883, 2014.   \n[10] J\u00e9r\u00f4me Dedecker and Sana Louhichi. Maximal inequalities and empirical central limit theorems. In Empirical process techniques for dependent data, pages 137\u2013159. Springer, 2002.   \n[11] Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning. In Proceedings of the 36th International Conference on Machine Learning, pages 1626\u20131635. PMLR, May 2019. ISSN: 2640-3498.   \n[12] Thinh T Doan. Local stochastic approximation: A unified view of federated learning and distributed multi-task reinforcement learning algorithms. arXiv preprint arXiv:2006.13460, 2020.   \n[13] R. Douc, E. Moulines, P. Priouret, and P. Soulier. Markov chains. Springer Series in Operations Research and Financial Engineering. Springer, 2018.   \n[14] Alain Durmus, Eric Moulines, Alexey Naumov, and Sergey Samsonov. Finite-time highprobability bounds for Polyak-Ruppert averaged iterates of linear stochastic approximation. Mathematics of Operations Research, 2024.   \n[15] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin Scaman, and Hoi-To Wai. Tight high probability bounds for linear stochastic approximation with fixed stepsize. In M. Ranzato, A. Beygelzimer, K. Nguyen, P. S. Liang, J. W. Vaughan, and Y. Dauphin, editors, Advances in Neural Information Processing Systems, volume 34, pages 30063\u201330074. Curran Associates, Inc., 2021.   \n[16] Matthieu Geist, Bruno Scherrer, et al. Off-policy learning with eligibility traces: a survey. J. Mach. Learn. Res., 15(1):289\u2013333, 2014.   \n[17] Eduard Gorbunov, Filip Hanzely, and Peter Richt\u00e1rik. Local SGD: Unified theory and new efficient methods. In International Conference on Artificial Intelligence and Statistics, pages 3556\u20133564. PMLR, 2021.   \n[18] Micha\u0142 Grudzien\u00b4, Grigory Malinovsky, and Peter Richt\u00e1rik. Can 5th generation local training methods support client sampling? Yes! In International Conference on Artificial Intelligence and Statistics, pages 1055\u20131092. PMLR, 2023.   \n[19] L. Guo and L. Ljung. Exponential stability of general tracking algorithms. IEEE Transactions on Automatic Control, 40(8):1376\u20131387, 1995.   \n[20] Farzin Haddadpour and Mehrdad Mahdavi. On the Convergence of Local Descent Methods in Federated Learning, December 2019. arXiv:1910.14425 [cs, stat].   \n[21] Zhengmian Hu and Heng Huang. Tighter analysis for proxskip. In International Conference on Machine Learning, pages 13469\u201313496. PMLR, 2023.   \n[22] Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated Reinforcement Learning with Environment Heterogeneity. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 18\u201337. PMLR, May 2022. ISSN: 2640-3498.   \n[23] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132\u20135143. PMLR, 2020.   \n[24] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning, 2021.   \n[25] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt\u00e1rik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 4519\u20134529. PMLR, 2020.   \n[26] Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated reinforcement learning: Linear speedup under markovian sampling. In International Conference on Machine Learning, pages 10997\u201311057. PMLR, 2022.   \n[27] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pages 5381\u20135393. PMLR, 2020.   \n[28] Jakub Konec\u02c7ny\\`, H Brendan McMahan, Daniel Ramage, and Peter Richt\u00e1rik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.   \n[29] PS Kostenetskiy, RA Chulkevich, and VI Kozyrev. Hpc resources of the higher school of economics. In Journal of Physics: Conference Series, volume 1740, page 012050. IOP Publishing, 2021.   \n[30] Gen Li, Weichen Wu, Yuejie Chi, Cong Ma, Alessandro Rinaldo, and Yuting Wei. Highprobability sample complexities for policy evaluation with linear function approximation. IEEE Transactions on Information Theory, 70(8):5969\u20135999, 2024.   \n[31] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine, 37(3):50\u201360, 2020.   \n[32] Tianjiao Li, Guanghui Lan, and Ashwin Pananjady. Accelerated and instance-optimal policy evaluation with linear function approximation. SIAM Journal on Mathematics of Data Science, 5(1):174\u2013200, 2023.   \n[33] Hyun-Kyo Lim, Ju-Bong Kim, Joo-Seong Heo, and Youn-Hee Han. Federated reinforcement learning for training control policies on multiple iot devices. Sensors, 20(5):1359, 2020.   \n[34] Rui Liu and Alex Olshevsky. Distributed TD(0) with almost no communication. IEEE Control Systems Letters, 7:2892\u20132897, 2023.   \n[35] Grigory Malinovsky, Kai Yi, and Peter Richt\u00e1rik. Variance reduced proxskip: Algorithm, theory and application to federated learning. Advances in Neural Information Processing Systems, 35:15176\u201315189, 2022.   \n[36] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[37] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richt\u00e1rik. Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally! In International Conference on Machine Learning, pages 15750\u201315769. PMLR, 2022.   \n[38] Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients. Advances in Neural Information Processing Systems, 34:14606\u201314619, 2021.   \n[39] Wenlong Mou, Chris Junchi Li, Martin J Wainwright, Peter L Bartlett, and Michael I Jordan. On linear stochastic approximation: Fine-grained Polyak-Ruppert and non-asymptotic concentration. In Conference on Learning Theory, pages 2947\u20132997. PMLR, 2020.   \n[40] Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression with markovian data: Fundamental limits and algorithms. Advances in neural information processing systems, 33:16666\u201316676, 2020.   \n[41] Kumar Kshitij Patel, Margalit Glasgow, Lingxiao Wang, Nirmit Joshi, and Nathan Srebro. On the still unreasonable effectiveness of federated averaging for heterogeneous distributed learning. In Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities, 2023.   \n[42] Gandharv Patil, LA Prashanth, Dheeraj Nagaraj, and Doina Precup. Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. In International Conference on Artificial Intelligence and Statistics, pages 5438\u20135448. PMLR, 2023.   \n[43] Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: Techniques, applications, and open challenges. arXiv preprint arXiv:2108.11887, 2021.   \n[44] Zhaonan Qu, Kaixiang Lin, Zhaojian Li, and Jiayu Zhou. Federated learning\u2019s blessing: Fedavg has linear speedup. In ICLR 2021-Workshop on Distributed and Private Machine Learning (DPML), 2021.   \n[45] Sergey Samsonov, Daniil Tiapkin, Alexey Naumov, and Eric Moulines. Improved HighProbability Bounds for the Temporal Difference Learning Algorithm via Exponential Stability. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 4511\u20134547. PMLR, 30 Jun\u201303 Jul 2024.   \n[46] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and TD learning. In Conference on Learning Theory, pages 2803\u20132830. PMLR, 2019.   \n[47] Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:9\u201344, 1988.   \n[48] Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesv\u00e1ri, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th annual international conference on machine learning, pages 993\u20131000, 2009.   \n[49] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674\u2013690, May 1997.   \n[50] Han Wang, Aritra Mitra, Hamed Hassani, George J Pappas, and James Anderson. Federated temporal difference learning with linear function approximation under environmental heterogeneity. arXiv preprint arXiv:2302.02212, 2023.   \n[51] Jianyu Wang, Rudrajit Das, Gauri Joshi, Satyen Kale, Zheng Xu, and Tong Zhang. On the unreasonable effectiveness of federated averaging with heterogeneous data. arXiv preprint arXiv:2206.04723, 2022.   \n[52] Zhijie Xie and Shenghui Song. FedKL: Tackling data heterogeneity in federated reinforcement learning by penalizing KL divergence. IEEE Journal on Selected Areas in Communications, 41(4):1227\u20131242, 2023.   \n[53] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Analysis of Federated Linear Stochastic Approximation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the analysis we need to define two filtration: $\\mathcal{F}_{s,h}^{+}\\,:=\\,\\sigma(Z_{t,k}^{c},t\\,\\geq\\,s,k\\,\\geq\\,h,1\\,\\leq\\,c\\,\\leq\\,N)$ , corresponding to the future events, and $\\mathcal{F}_{s,h}^{-}:=\\sigma(Z_{t,k}^{c},t\\le s,k\\le h,1\\le c\\le N)$ , corresponding to the preceding events. Recall that the local LSA updates are written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{t,h}^{c}-\\theta_{\\star}^{c}=(\\mathrm{I}-\\eta\\mathbf{A}(Z_{t,h}^{c}))(\\theta_{t,h-1}^{c}-\\theta_{\\star}^{c})-\\eta\\varepsilon^{c}(Z_{t,h}^{c})\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Performing $H$ local steps and taking average, we end up with the decomposition ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta_{\\star}=\\bar{\\Gamma}_{t,H}^{(\\eta)}\\{\\theta_{t-1}-\\theta_{\\star}\\}+\\bar{\\rho}_{H}+\\bar{\\tau}_{t,H}+\\eta\\bar{\\varphi}_{t,H}\\ ,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have defined ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Gamma}_{t,H}^{(\\eta)}=\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\Gamma_{t,1:H}^{(c,\\eta)}\\;,}\\\\ &{\\bar{\\rho}_{H}=\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}(\\mathrm{I}-(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H})\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\;,}\\\\ &{\\bar{\\tau}_{t,H}=\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\{(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H}-\\Gamma_{t,1:H}^{(c,\\eta)}\\}\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\;,}\\\\ &{\\bar{\\varphi}_{t,H}=-\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta)}\\varepsilon^{c}(Z_{t,h}^{c})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The transient term $\\bar{\\Gamma}_{t,H}^{(\\eta)}\\big(\\theta_{t-1}-\\theta_{\\star}\\big)$ , responsible for the rate of forgetting the previous iteration error $\\theta_{t-1}-\\theta_{\\star}$ , and the fluctuation term $\\eta\\bar{\\varphi}_{t,H}$ , reflecting the oscillations of the iterates around $\\theta_{\\star}$ , are similar to the ones from the standard LSA error decomposition [14]. The two additional terms in (10) reflect the heterogeneity bias. This bias is composed of two parts: the true bias $\\bar{\\rho}_{H}$ , which is non-random, and its fluctuations $\\bar{\\tau}_{t,H}$ . To analyze the complexity and communication complexity of FedLSA, we run the recurrence (6) to obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{t}-\\theta_{\\star}=\\tilde{\\theta}_{t}^{(\\mathrm{tr})}+\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}+\\tilde{\\theta}_{t}^{(\\mathrm{fl},\\mathrm{bi})}+\\tilde{\\theta}_{t}^{(\\mathrm{fl})}\\ ,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have defined ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\tilde{\\theta}_{t}^{(\\mathrm{tr})}=\\prod_{s=1}^{t}\\bar{\\Gamma}_{s,H}^{(\\eta)}\\{\\theta_{0}-\\theta_{\\star}\\}~,}\\\\ {\\displaystyle\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}=\\sum_{s=1}^{t}\\bigl(\\bar{\\Gamma}_{H}^{(\\eta)}\\bigr)^{t-s}\\bar{\\rho}_{H}~,}\\\\ {\\displaystyle\\tilde{\\theta}_{t}^{(\\mathrm{fl},\\mathrm{bi})}=\\sum_{s=1}^{t}\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\bar{\\tau}_{s,H}+\\Delta_{H,s,t}^{(\\eta)}\\bar{\\rho}_{H}~,}\\\\ {\\displaystyle\\tilde{\\theta}_{t}^{(\\mathrm{fl})}=\\eta\\sum_{s=1}^{t}\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\bar{\\varphi}_{s,H}~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with the notations $\\bar{\\Gamma}_{H}^{(\\eta)}\\;=\\;\\mathbb{E}[\\bar{\\Gamma}_{s,H}^{(\\eta)}]\\;=\\;\\frac{{}_{1}}{N}\\sum_{c=1}^{N}(\\mathrm{I}\\,-\\,\\eta\\bar{\\mathbf{A}}^{c})^{H}$ and \u2206(H\u03b7,)s,t =   it=s+1 \u0393\u00afi(,\u03b7H) \u2212 $(\\bar{\\Gamma}_{H}^{(\\eta)})^{t-s}$ . The first term, $\\tilde{\\theta}_{t}^{(\\mathrm{tr})}$ gives the rate at which the initial error is forgotten. The terms $\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}$ and \u03b8\u02dct(fl,bi) represent the bias and fluctuation due to statistical heterogeneity across agents. Note that in the special case where agents are homogeneous (i.e. $\\bar{\\mathbf{A}}^{c}=\\bar{\\mathbf{A}}$ for all $c\\in[N])$ , these two terms vanish. Finally, the term $\\ensuremath{\\tilde{\\theta}}_{t}^{(\\ensuremath{\\mathfrak{H}})}$ depicts the fluctuations of $\\theta_{t}$ around the solution $\\theta_{\\star}$ . Now we need to upper bound each of the terms in decomposition (12). This is done in a sequence of lemmas below: $\\ensuremath{\\tilde{\\theta}}_{t}^{(\\mathsf{f l})}$ is bounded in Lemma A.1, $\\tilde{\\theta}_{t}^{(\\mathrm{fl},\\mathbf{b}\\mathrm{i})}$ in Lemma A.2, $\\tilde{\\theta}_{t}^{(\\mathrm{tr})}$ in Lemma A.4, and $\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}$ in Lemma A.5. Then we combine the bounds in order to state a version of Theorem 4.1 with explicit constants in Theorem A.6. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Assume A1 and A3. Then, for any step size $\\eta\\in(0,\\eta_{\\infty})$ it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|\\tilde{\\theta}_{t}^{(\\mathrm{fl})}\\|^{2}\\big]\\leq\\frac{\\eta\\bar{\\sigma}_{\\varepsilon}}{a N(1-\\mathrm{e}^{-2})}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We start from the decomposition (13). With the definition of $\\ensuremath{\\tilde{\\theta}}_{t}^{(\\mathsf{f l})}$ and $\\begin{array}{r}{\\mathbb{E}^{\\mathcal{F}}\\left[\\left\\{\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\right\\}\\bar{\\varphi}_{s,H}\\right]=0}\\end{array}$ , we obtain that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|\\tilde{\\theta}_{t}^{(\\mathsf{f l})}\\|^{2}\\big]=\\eta^{2}\\sum_{s=1}^{t}\\mathbb{E}\\big[\\|\\big\\{\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\big\\}\\bar{\\varphi}_{s,H}\\|^{2}\\big]~.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, using the assumption A3 and Minkowski\u2019s inequality, we obtain that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbb{E}}^{1/2}\\big[\\|\\big\\{\\displaystyle\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\big\\}\\bar{\\varphi}_{s,H}\\|^{2}\\big]\\leq\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}{\\mathbb{E}}^{1/2}\\|\\big[\\bar{\\Gamma}_{t,H}^{(c,\\eta)}\\big\\{\\displaystyle\\prod_{i=s+1}^{t-1}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\big\\}\\bar{\\varphi}_{s,H}\\|^{2}\\big]}\\\\ {\\overset{(a)}{\\leq}(1-\\eta a)^{H}{\\mathbb{E}}^{1/2}\\big[\\|\\big\\{\\displaystyle\\prod_{i=s+1}^{t-1}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\big\\}\\bar{\\varphi}_{s,H}\\|^{2}\\big]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In (a) applied A3 conditionally on $\\mathcal{F}_{t-1,H}^{-}$ . Hence, by induction we get from the previous formulas that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|\\tilde{\\theta}_{t}^{(\\mathsf{t l})}\\|^{2}\\big]\\leq\\eta^{2}\\sum_{s=1}^{t}(1-\\eta a)^{H}\\mathbb{E}[\\|\\bar{\\varphi}_{s,H}\\|^{2}]\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we proceed with bounding $\\mathbb{E}\\big[\\|\\bar{\\varphi}_{s,H}\\|^{2}\\big]$ . Indeed, since the clients are independent, we get using (11) that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\|\\bar{\\varphi}_{s,H}\\|^{2}\\big]=\\frac{1}{N^{2}}\\sum_{c=1}^{N}\\mathbb{E}\\big[\\|\\sum_{h=1}^{H}\\Gamma_{s,h+1:H}^{(c,\\eta)}\\varepsilon^{c}(Z_{s,h}^{c})\\|^{2}\\big]}\\\\ &{\\qquad\\qquad=\\frac{1}{N^{2}}\\sum_{c=1}^{N}\\biggl[\\sum_{h=1}^{H}\\mathbb{E}\\big[\\|\\Gamma_{s,h+1:H}^{(c,\\eta)}\\varepsilon^{c}(Z_{s,h}^{c})\\|^{2}\\big]\\biggr]}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{N^{2}}\\sum_{c=1}^{N}\\sum_{h=1}^{H}(1-\\eta a)^{2(H-h)}\\mathbb{E}\\big[\\|\\varepsilon^{c}(Z_{s,h}^{c})\\|^{2}\\big]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, using (3) and the following inequality, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{h=0}^{H-1}(1-\\eta a)^{2h}\\leq H\\wedge\\frac{1}{\\eta a},\\quad\\mathrm{for\\,all\\,}\\eta\\geq0,\\,\\mathrm{such\\,that\\,}\\eta a\\leq1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\bar{\\varphi}_{s,H}\\|^{2}\\right]\\leq\\frac{1}{N}\\left(H\\wedge\\frac{1}{\\eta a}\\right)\\bar{\\sigma}_{\\varepsilon}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging this inequality in (15), we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\|\\tilde{\\theta}_{t}^{(\\mathsf{f l})}\\|^{2}\\big]\\leq\\displaystyle\\frac{\\mathbb{E}_{c}[\\mathrm{Tr}(\\Sigma_{\\varepsilon}^{c})]}{N}\\left(\\eta^{2}H\\wedge\\frac{\\eta}{a}\\right)\\displaystyle\\sum_{s=1}^{t}\\big[(1-\\eta a)^{2H(t-s)}\\big]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\bar{\\sigma}_{\\varepsilon}}{N}\\left(\\eta^{2}H\\wedge\\frac{\\eta}{a}\\right)\\displaystyle\\frac{1}{1-(1-\\eta a)^{2H}}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\eta\\bar{\\sigma}_{\\varepsilon}}{a N}\\left(\\eta a H\\wedge1\\right)\\displaystyle\\frac{1}{1-\\mathrm{e}^{-2\\eta a H}}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used additionally ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{e}^{-2x}\\leq1-x\\leq\\mathrm{e}^{-x}\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is valid for $x\\in[0;1/2]$ . Now it remains to notice that ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{x\\wedge1}{1-\\mathrm{e}^{-2x}}}\\leq{\\frac{1}{1-\\mathrm{e}^{-2}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any $x>0$ . ", "page_idx": 15}, {"type": "text", "text": "We proceed with analyzing the fluctuation of the true bias component of the error $\\theta_{t}$ defined in (13). The first step towards this is to obtain the respective bound for $\\bar{\\tau}_{s,H}$ , $s\\in\\{1,\\ldots,T\\}$ , where $\\bar{\\tau}_{s,H}$ is defined in (11). Now we provide an upper bound for $\\tilde{\\theta}_{t}^{(\\mathsf{f l},\\mathsf{b i})}$ : ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. Assume A1 and A3. Then, for any step size $\\eta\\in(0,\\eta_{\\infty})$ it holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/2}\\big[\\|\\tilde{\\theta}_{t}^{(\\mathrm{fl},\\mathrm{bi})}\\|^{2}\\big]\\leq\\sqrt{\\frac{2\\eta\\tilde{v}_{h e t e r}}{N a}}+\\frac{2\\sqrt{\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}\\|\\bar{\\rho}_{H}\\|}{a H^{1/2}N^{1/2}}~.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Recall that $\\tilde{\\theta}_{t}^{(\\mathrm{{fl}},\\mathrm{{bi}})}$ is given (see (13)) by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{t}^{(\\mathrm{fI},\\mathrm{bi})}=\\underbrace{\\sum_{s=1}^{t}\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\bar{\\tau}_{s,H}}_{T}+\\underbrace{\\left(\\sum_{s=1}^{t}\\{\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\}-(\\bar{\\Gamma}_{H}^{(\\eta)})^{t-s}\\right)\\bar{\\rho}_{H}}_{T}\\ ,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\bar{\\tau}_{s,H}$ and $\\bar{\\rho}_{H}$ are defined in (11). We begin with bounding $T_{1}$ . In order to do it we first need to bound ${\\bar{\\tau}}_{s,H}$ . Since the different agents are independent, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\bar{\\tau}_{s,H}\\|^{2}]=\\frac{1}{N^{2}}\\sum_{c=1}^{N}\\mathbb{E}[\\|((\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H}-\\Gamma_{s,1:H}^{(c,\\eta)})\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\|^{2}]\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Lemma D.1 and the fact that $\\left\\{(\\mathrm{I}\\mathrm{~-~}\\eta\\bar{\\mathbf{A}}^{c})^{h-1}\\widetilde{\\mathbf{A}}^{c}(Z_{s,h}^{c})\\Gamma_{s,(h+1):H}^{(c,\\eta)}(\\theta_{\\star}^{c}\\mathrm{~-~}\\theta_{\\star})\\right\\}_{h=1}^{H}$ is a martingale-difference w.r.t. $\\mathcal{F}_{s,h}^{-}$ , we get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|((\\mathbf{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H}-\\Gamma_{s,1:H}^{(c,\\eta)})\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\|^{2}]}\\\\ &{=\\eta^{2}\\mathbb{E}[\\|\\displaystyle\\sum_{h=1}^{H}(\\mathbf{I}-\\eta\\bar{\\mathbf{A}}^{c})^{h-1}\\widetilde{\\mathbf{A}}^{c}(Z_{s,h}^{c})\\Gamma_{s,(h+1):H}^{(c,\\eta)}\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\|^{2}]}\\\\ &{=\\eta^{2}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}[\\|(\\mathbf{I}-\\eta\\bar{\\mathbf{A}}^{c})^{h-1}\\widetilde{\\mathbf{A}}^{c}(Z_{s,h}^{c})\\Gamma_{s,(h+1):H}^{(c,\\eta)}\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\|^{2}]}\\\\ &{\\le\\eta^{2}\\displaystyle\\sum_{h=1}^{H}(1-\\eta a)^{2(h-1)}\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}^{\\top}\\mathbb{E}[(\\Gamma_{s,(h+1):H}^{(c,\\eta)})^{\\top}\\widetilde{\\mathbf{A}}^{c}(Z_{s,h}^{c}))^{\\top}\\widetilde{\\mathbf{A}}^{c}(Z_{s,h}^{c})\\Gamma_{s,(h+1):H}^{(c,\\eta)}]\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the tower property conditionally on Fs,h+1, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\Gamma_{s,(h+1):H}^{(c,\\eta)})^{\\top}(\\widetilde{\\mathbf{A}}^{c}(Z_{s,h}^{c}))^{\\top}\\widetilde{\\mathbf{A}}^{c}(Z_{s,h}^{c})\\Gamma_{s,(h+1):H}^{(c,\\eta)}]=\\mathbb{E}[(\\Gamma_{s,(h+1):H}^{(c,\\eta)})^{\\top}\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\Gamma_{s,(h+1):H}^{(c,\\eta)}]\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}$ is the noise covariance matrix defined in (3). Since for any vector $u\\,\\in\\,\\mathbb R^{d}$ we have $\\|u\\|_{\\Sigma}\\leq\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\|u\\|$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\|((\\operatorname{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H}-\\Gamma_{s,1:H}^{(c,\\eta)})\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\|^{2}]\\le\\eta^{2}\\displaystyle\\sum_{h=1}^{H}(1-\\eta a)^{2(h-1)}\\mathbb{E}\\big[\\|\\Gamma_{s,(h+1):H}^{(c,\\eta)}\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\|_{\\Sigma}^{2}\\big]}\\\\ {\\displaystyle}\\\\ {\\le\\eta^{2}\\|\\Sigma_{\\mathbb{A}}^{c}\\|\\displaystyle\\sum_{h=1}^{H}(1-\\eta a)^{2(h-1)}\\mathbb{E}\\big[\\|\\Gamma_{s,(h+1):H}^{(c,\\eta)}\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}\\|^{2}\\big]}\\\\ {\\displaystyle}\\\\ {\\le H\\eta^{2}(1-\\eta a)^{2(H-1)}\\|\\Sigma_{\\mathbb{A}}^{c}\\|\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the above bounds in (18) yields that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|\\bar{\\tau}_{s,H}\\|^{2}\\big]\\leq\\frac{H\\eta^{2}(1-\\eta a)^{2(H-1)}\\sum_{c=1}^{N}\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}}{N^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, proceeding as in (14) together with (20), we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|T_{1}\\|^{2}]=\\sum_{s=1}^{t}\\mathbb{E}[\\|\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\bar{\\tau}_{s,H}\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\sum_{s=1}^{t}\\frac{H\\eta^{2}(1-\\eta\\alpha)^{2(H-1)}\\sum_{c=1}^{N}\\|\\sum_{\\bar{\\Lambda}}^{c}\\|\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}}{N^{2}}(1-\\eta\\alpha)^{2H(t-s)}}\\\\ &{\\quad\\quad\\quad\\leq\\frac{H\\eta^{2}(1-\\eta\\alpha)^{2(H-1)}}{(1-(1-\\eta\\alpha)^{2H})N}\\mathbb{E}_{c}[\\|\\Sigma_{\\bar{\\Lambda}}^{c}\\|\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\leq\\frac{\\eta}{a N(1-\\eta\\alpha)^{2}}\\frac{H\\alpha\\eta e^{-2H\\alpha\\eta}}{1-\\mathrm{e}^{-2H\\alpha\\eta}}\\mathbb{E}_{c}[\\|\\Sigma_{\\bar{\\Lambda}}^{c}\\|\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\leq\\frac{2\\eta}{N\\alpha}\\mathbb{E}_{c}[\\|\\Sigma_{\\bar{\\Lambda}}^{c}\\|\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|^{2}]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the bound above we used (16) together with the bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{x\\mathrm{e}^{-2x}}{1-\\mathrm{e}^{-2x}}}\\leq{\\frac{1}{2}}\\,,x\\geq0\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we bound the second part of $\\tilde{\\theta}_{t}^{(\\mathrm{fl},\\mathrm{bi})}$ in (17), that is, $T_{2}$ . To begin with, we start with applying Lemma D.1 and we get for any $s\\in\\{1,\\ldots,t\\}$ and $i\\in\\{s+1,\\ldots,t\\}$ , that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\{\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\}-(\\bar{\\Gamma}_{H}^{(\\eta)})^{t-s}\\bigr)\\bar{\\rho}_{H}=\\sum_{i=s+1}^{t}\\{\\prod_{r=i+1}^{t}\\bar{\\Gamma}_{r,H}^{(\\eta)}\\}(\\bar{\\Gamma}_{i,H}^{(\\eta)}-\\bar{\\Gamma}_{H}^{(\\eta)})(\\bar{\\Gamma}_{H}^{(\\eta)})^{i-s-1}\\bar{\\rho}_{H}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathcal{F}}[\\{\\prod_{r=i+1}^{t}\\bar{\\Gamma}_{r,H}^{(\\eta)}\\}(\\bar{\\Gamma}_{i,H}^{(\\eta)}-\\bar{\\Gamma}_{H}^{(\\eta)})(\\bar{\\Gamma}_{H}^{(\\eta)})^{i-s-1}\\bar{\\rho}_{H}]=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proceeding as in (19), we get using independence between agents for any $u\\in\\mathbb{R}^{d}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|(\\bar{\\Gamma}_{i,H}^{(\\eta)}-\\bar{\\Gamma}_{H}^{(\\eta)})u\\|^{2}]=\\displaystyle\\frac{1}{N^{2}}\\mathbb{E}[\\|\\sum_{c=1}^{N}(\\Gamma_{s,1:H}^{(c,\\eta)}-(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H})u\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{N^{2}}\\sum_{c=1}^{N}\\mathbb{E}[\\|(\\Gamma_{s,1:H}^{(c,\\eta)}-(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H})u\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{H\\eta^{2}(1-\\eta a)^{2(H-1)}}{N}\\left(\\frac{1}{N}\\sum_{c=1}^{N}\\|\\Sigma_{\\bar{\\mathbf{A}}}^{c}\\|\\right)\\|u\\|^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, using (21), we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\big(\\{\\prod_{i=s+1}^{t}\\bar{\\Gamma}_{i,H}^{(\\eta)}\\}-(\\bar{\\Gamma}_{H}^{(\\eta)})^{t-s}\\big)\\bar{\\rho}_{H}\\|^{2}]=\\frac{H\\eta^{2}(1-\\eta a)^{2H(t-s)-2}\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}{N}\\|\\bar{\\rho}_{H}\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the above estimates in (17), and using Minkowski\u2019s inequality, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{1/2}[\\|T_{2}\\|^{2}]\\leq\\displaystyle\\frac{H^{1/2}\\eta}{(1-\\eta a)N^{1/2}}\\sqrt{\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}\\|\\bar{\\rho}_{H}\\|\\displaystyle\\sum_{s=1}^{t-1}(1-\\eta a)^{H(t-s)}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{2}{a H^{1/2}N^{1/2}}\\displaystyle\\frac{H a\\eta e^{-H a\\eta}}{1-e^{-H a\\eta}}\\sqrt{\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}\\|\\bar{\\rho}_{H}\\|}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{2}{a H^{1/2}N^{1/2}}\\sqrt{\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}\\|\\bar{\\rho}_{H}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used that $\\eta a\\leq1/2$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{x\\mathrm{e}^{-x}}{1-\\mathrm{e}^{-x}}}\\leq1\\,,\\quad x\\geq0\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the statement follows. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.3. Recall that $\\begin{array}{r}{\\bar{\\rho}_{H}=\\frac{1}{N}\\sum_{c=1}^{N}(\\mathrm{I}-(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H})\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\}}\\end{array}$ , it satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\bar{\\rho}_{H}\\|\\leq\\frac{\\eta^{2}H^{2}}{N}\\sum_{c=1}^{N}\\exp(\\eta H\\|\\bar{\\mathbf{A}}^{c}\\|)\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Using the identity, ", "page_idx": 18}, {"type": "equation", "text": "$$\n1-(1-u)^{H}=H u-u^{2}\\sum_{k=0}^{H-2}(-1)^{k}{\\binom{H}{k+2}}u^{k}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the inequality $(\\mathbf{\\Sigma}_{k+2}^{H})\\leq\\binom{H-2}{k}H^{2}$ , we get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\sum_{k=0}^{H-2}(-1)^{k}{\\binom{H}{k+2}}u^{k}\\right|\\leq{\\frac{H^{2}}{2}}\\sum_{k=0}^{H-2}{\\binom{H-2}{k}}|u|^{k}\\leq{\\frac{H^{2}}{2}}\\exp((H-2)|u|)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using (23) with $u=\\eta\\bar{\\mathbf{A}}^{c}$ for all $c$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\rho}_{H}=\\frac{1}{N}\\sum_{c=1}^{N}H\\eta\\bar{\\mathbf{A}}^{c}-\\eta^{2}(\\bar{\\mathbf{A}}^{c})^{2}\\sum_{k=0}^{H-2}(-1)^{k}{\\binom{H}{k+2}}(\\eta\\bar{\\mathbf{A}}^{c})^{k}~,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by definition of $\\theta_{\\star}^{c}$ and $\\theta_{\\star}$ , we have that $\\begin{array}{r}{\\sum_{c=1}^{N}\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\,=\\,\\sum_{c=1}^{N}\\bar{\\mathbf{A}}^{c}\\theta_{\\star}^{c}-(\\sum_{c=1}^{N}\\bar{\\mathbf{A}}^{c})\\theta_{\\star}\\,=\\,}\\end{array}$ $\\begin{array}{r}{\\sum_{c=1}^{N}\\bar{\\mathbf{b}}^{c}-\\sum_{c=1}^{N}\\bar{\\mathbf{b}}^{c}=0}\\end{array}$ . Using this and (24), we finally get (22). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma A.4. Assume A1 and A3. Then for any step size $\\eta\\in(0,\\eta_{\\infty})$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/2}[\\|\\tilde{\\theta}_{t}^{(\\mathfrak{t r})}\\|^{2}]\\leq(1-\\eta a)^{t H}\\|\\theta_{0}-\\theta_{\\star}\\|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Proceeding as in (14) for any $u\\in\\mathbb{R}^{d}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/2}[\\\\\\\\\\\\lVert\\prod_{s=1}^{t}\\bar{\\Gamma}_{s,H}^{(\\eta)}u\\rVert^{2}]\\leq(1-\\eta a)^{t H}\\lVert u\\rVert\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using this result for $u=\\theta_{0}-\\theta_{\\star}$ we get the statement. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.5. Assume A1 and A3. Then for any $\\eta\\in(0,\\eta_{\\infty})$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\theta}_{t}^{(\\mathsf{b i},\\mathsf{b i})}-(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}\\|\\le(1-\\eta a)^{t H}\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\|\\|\\bar{\\rho}_{H}\\|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Using A3 and Minkowski\u2019s inequalitty, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\widetilde\\theta_{t}^{(\\mathtt{b i},\\mathtt{b i})}-(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}\\|=\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}(\\bar{\\Gamma}_{H}^{(\\eta)})^{t}\\bar{\\rho}_{H}\\|}&{}\\\\ {\\le\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\|\\|(\\bar{\\Gamma}_{H}^{(\\eta)})^{t}\\bar{\\rho}_{H}\\|}&{}\\\\ {\\le\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\|\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\|(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{H}(\\bar{\\Gamma}_{H}^{(\\eta)})^{t-1}\\bar{\\rho}_{H}\\|}&{}\\\\ {\\le(1-\\eta a)^{H}\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\|\\|(\\bar{\\Gamma}_{H}^{(\\eta)})^{t-1}\\bar{\\rho}_{H}\\|}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the statement follows. ", "page_idx": 18}, {"type": "text", "text": "Theorem A.6. Assume A1 and A3. Then for any step size $\\eta\\in(0,\\eta_{\\infty})$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{1/2}\\big[\\|\\theta_{t}-\\tilde{\\theta}_{t}^{(\\mathrm{b},\\mathrm{bi})}-\\theta_{\\star}\\|^{2}\\big]\\leq\\sqrt{\\frac{\\eta\\bar{\\sigma}_{\\varepsilon}}{a N(1-\\mathrm{e}^{-2})}}+\\sqrt{\\frac{2\\eta\\tilde{v}_{h e t e r}}{N a}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\frac{2\\sqrt{\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}\\|\\bar{\\rho}_{H}\\|}{a H^{1/2}N^{1/2}}+(1-\\eta a)^{t H}\\|\\theta_{0}-\\theta_{\\star}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the bias $\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}$ converges to $(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}$ at a rate ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{\\theta}_{t}^{(\\mathrm{bi},\\mathrm{bi})}-(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}\\|\\leq(1-\\eta a)^{t H}\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\|\\|\\bar{\\rho}_{H}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the lemma below we provide a simplified sample complexity bound of Corollary 4.3 corresponding to the synchronous setting, that is, with number of local training steps $H=1$ . There, the bias term disappears, and above results directly give a simplified sample complexity bound. ", "page_idx": 19}, {"type": "text", "text": "Corollary A.7. Assume A1 and $A\\,3$ . Let $H=1$ , then for any $0\\,<\\,\\epsilon\\,<\\,1_{\\|}$ , in order to achieve $\\mathbb{E}\\big[\\|\\theta_{T}-\\mathbf{\\dot{\\theta}}_{\\star}\\|^{2}\\big]\\le\\epsilon^{2}$ the required number of communications is ", "page_idx": 19}, {"type": "equation", "text": "$$\nT=\\mathcal{O}\\left(\\frac{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}{N a^{2}\\epsilon^{2}}\\log\\frac{\\Vert\\theta_{0}-\\theta_{\\star}\\Vert}{\\epsilon}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "number of communications, setting the step size ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta_{0}=\\frac{a N\\epsilon^{2}}{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}~.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Bounding the first two terms in decomposition (25) we get that the step size should satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{a N\\epsilon^{2}}{\\tilde{v}_{\\mathrm{heter}}\\vee\\bar{\\sigma}_{\\varepsilon}}~.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From the last term we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nt\\geq\\frac{1}{a\\eta}\\log\\frac{\\|\\theta_{0}-\\theta_{\\star}\\|}{\\epsilon}\\geq\\frac{\\widetilde v_{\\mathrm{heter}}\\vee\\bar{\\sigma}_{\\varepsilon}}{N a^{2}\\epsilon^{2}}\\log\\frac{\\|\\theta_{0}-\\theta_{\\star}\\|}{\\epsilon}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Corollary A.8. Assume A1 and A3. For any ", "page_idx": 19}, {"type": "equation", "text": "$$\n0\\leq\\epsilon\\leq\\frac{\\mathbb{C}_{\\bf A}^{-1}\\mathbb{E}_{c}\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|}{a}\\vee\\left(\\frac{\\sqrt{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}\\mathbb{E}_{c}\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|}{a}\\right)^{2/5}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "in order to achieve $\\mathbb{E}\\big[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}\\big]<\\epsilon^{2}$ the required number of communications is ", "page_idx": 19}, {"type": "equation", "text": "$$\nT=\\mathcal{O}\\left(\\frac{\\mathbb{E}_{c}\\|\\boldsymbol{\\theta}_{\\star}^{c}-\\boldsymbol{\\theta}_{\\star}\\|}{a^{2}\\epsilon}\\log\\frac{\\|\\boldsymbol{\\theta}_{0}-\\boldsymbol{\\theta}_{\\star}\\|}{\\epsilon}\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "setting the step size ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta=\\mathcal{O}\\left(\\frac{a N\\epsilon^{2}}{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and number of local iterations ", "page_idx": 19}, {"type": "equation", "text": "$$\nH=\\mathcal{O}\\left(\\frac{\\tilde{v}_{h e t e r}\\vee\\bar{\\sigma}_{\\varepsilon}}{N\\epsilon\\mathbb{E}_{c}\\Vert\\theta_{\\star}^{c}-\\theta_{\\star}\\Vert}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We aim to bound separately all the terms in the r.h.s. of Theorem 4.1. Note that it requires to set $\\eta\\in(0;\\eta_{0})$ with $\\eta_{0}$ given in (26) in order to fulfill the bounds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\eta\\tilde{v}_{\\mathrm{heter}}}{a N}}\\lesssim\\varepsilon\\;,\\quad\\sqrt{\\frac{\\eta\\bar{\\sigma}_{\\varepsilon}}{a N}}\\lesssim\\varepsilon\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we should bound the bias term ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{1/2}[\\|\\tilde{\\theta}_{t}^{(\\mathsf{b i},\\mathsf{b i})}\\|^{2}]\\leq(1+(1-\\eta a)^{t H})\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}\\|\\leq2\\|(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, using the Neuman series, we can bound the norm of the term above as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|(\\boldsymbol{\\mathrm{I}}-\\bar{\\boldsymbol{\\Gamma}}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}\\|=\\|\\sum_{k=0}^{\\infty}(\\bar{\\Gamma}_{H}^{(\\eta)})^{k}\\bar{\\rho}_{H}\\|\\leq\\sum_{k=0}^{\\infty}(1-\\eta a)^{H k}\\|\\bar{\\rho}_{H}\\|\\leq\\frac{\\|\\bar{\\rho}_{H}\\|}{1-(1-\\eta a)^{H}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, using the bound of Lemma A.3, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{1/2}[\\|\\tilde{\\theta}_{t}^{(\\mathrm{b}|,\\mathrm{b}|)}\\|^{2}]\\leq\\frac{2\\|\\bar{\\rho}_{H}\\|}{1-\\left(1-\\eta a\\right)^{H}}\\leq\\frac{\\eta a H}{1-\\left(1-\\eta a\\right)^{H}}\\frac{\\eta H\\mathbb{E}_{c}\\left[\\exp\\left(\\eta H\\right\\|\\bar{\\mathbf{A}}^{c}\\|\\right)\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|\\right]}{a}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{2\\eta H\\mathbb{E}_{c}\\left[\\exp\\left(\\eta H\\right\\|\\bar{\\mathbf{A}}^{c}\\|\\right)\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|\\right]}{a}\\lesssim\\frac{\\eta H\\mathbb{E}_{c}\\left[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|\\right]}{a}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used the fact that the step size $\\eta$ is chosen in order to satisfy $\\eta H\\operatorname{C}_{\\mathbf{A}}\\leq1$ . Thus in order to fulfill $\\mathbb{E}^{1/2}[\\lVert\\tilde{\\theta}_{t}^{(\\mathbf{bi},\\mathbf{bi})}\\rVert^{2}]\\lesssim\\varepsilon$ we need to choose $\\eta$ and $H$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta H\\mathbb{E}_{c}[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|]\\leq\\varepsilon a~.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It remains to bound the term $\\frac{\\sqrt{\\mathbb{E}\\|\\Sigma\\|}\\,\\|{\\bar{\\rho}}\\|}{a H N}$ . Using the bound of Lemma A.3, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}\\|\\bar{\\rho}_{H}\\|}{a H^{1/2}N^{1/2}}\\le\\sqrt{\\frac{\\eta}{N}}\\times\\frac{\\sqrt{\\mathbb{E}_{c}\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|}(\\eta H)^{3/2}}{a}\\lesssim\\varepsilon^{5/2}\\sqrt{\\frac{1}{\\tilde{v}_{\\mathrm{heter}}\\vee\\bar{\\sigma}_{\\varepsilon}}}\\frac{a}{\\mathbb{E}_{c}[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|]}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, it remains to combine the bounds above in order to get the sample complexity result (27). ", "page_idx": 20}, {"type": "text", "text": "Corollary A.9. Assume TD1 and TD3. Then for any ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\le\\epsilon\\le\\frac{2\\left(\\sqrt{2}(1+\\gamma)\\sqrt{\\mathbb{E}\\|\\theta-\\theta\\|\\vee(1+\\mathbb{E}[\\|\\theta\\|])}\\mathbb{E}[\\|\\theta-\\theta\\|]\\right)}{(1-\\gamma)\\nu}\\vee\\frac{2\\mathbb{E}[\\|\\theta-\\theta\\|]}{(1-\\gamma)\\nu(1+\\gamma)}~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "in order to achieve $\\mathbb{E}\\big[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}\\big]<\\epsilon^{2}$ the required number of communications for federated $T D(O)$ algorithm is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T=\\mathcal{O}\\left(\\left(\\frac{1}{(1-\\gamma)\\nu}\\lor\\frac{\\mathbb{E}\\left[\\parallel\\theta-\\theta\\parallel\\right]}{(1-\\gamma)\\nu\\epsilon}\\right)\\log\\frac{\\parallel\\theta-\\theta\\parallel}{\\epsilon}\\right)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B Markovian sampling schemes for FedLSA ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Note that under A2 each of the matrices $\\bar{\\mathbf{A}}^{c}$ , $c\\in[N]$ is Hurwitz. This guarantees the existence and uniqueness of a positive definite matrix $Q_{c}$ which is a solution of the Lyapunov equation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\{\\bar{\\mathbf{A}}^{c}\\}^{\\top}Q_{c}+Q_{c}\\bar{\\mathbf{A}}^{c}=\\mathrm{I}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We further introduce the associated quantities, that will be used throughout the proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a_{c}=\\|Q_{c}\\|^{-1}/2\\,,\\,\\tilde{\\eta}_{\\infty,c}=(1/2)\\|\\bar{\\mathbf{A}}^{c}\\|_{Q}^{-2}\\|Q_{c}\\|^{-1}\\wedge\\|Q_{c}\\|\\ ,\\,\\tilde{a}=\\displaystyle\\operatorname*{min}_{c\\in[N]}a_{c}\\,,\\,\\tilde{\\eta}_{\\infty}=\\operatorname*{min}_{c\\in[N]}\\tilde{\\eta}_{\\infty,c}\\,,}\\\\ {\\kappa_{Q,c}=\\lambda_{\\mathsf{m a x}}(Q_{c})/\\lambda_{\\mathsf{m i n}}(Q_{c})\\,,\\quad b_{Q,c}=2\\sqrt{\\kappa_{Q,c}}\\,\\mathbf{C}_{\\mathbf{A}}\\ ,\\quad\\kappa_{Q}=\\displaystyle\\operatorname*{max}_{c\\in[N]}\\kappa_{Q,c}\\,,\\quad b_{Q}=\\displaystyle\\operatorname*{max}_{c\\in[N]}b_{Q,c}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In our statement of A2 we also required that each of the chains $(Z_{k}^{c})_{k\\in\\mathbb{N}}$ starts from its invariant distribution $\\pi_{c}$ . This requirement can be removed, and extension to the setting of arbitrary initial distribution can be done based on the maximal exact coupling argument [13, Lemma 19.3.6 and Theorem 19 3.9]. However, to better highlight the main ingredients of the proof, we prefer to keep stationary assumption. ", "page_idx": 20}, {"type": "text", "text": "Proof of Corollary 4.5. Assume that the total number of local iterations, that is, $T H$ , satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\nT H=2q m+k\\ ,\\quad0\\leq k<2q\\ ,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $q\\in\\mathbb N$ is a parameter that will be determined later. With Lemma B.4 we construct for each $c\\in[N]$ a sequence of random variables { Z\u02dc2\u22c6j,cq}j=1,...,m, which are i.i.d. with the same distribution $\\pi_{c}$ . Moreover, Lemma B.4 together with union bound imply ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\exists j\\in[m],c\\in[N]:\\tilde{Z}_{2j q}^{\\star,c}\\neq Z_{2j q}^{c})\\le m N(1/4)^{\\lfloor q/\\tau\\rfloor}\\ .\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The bound (29) implies that $m\\leq T H/(2q)$ . Thus, for any $\\delta\\in(0,1)$ , in order to guarantee that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\exists j\\in[m],c\\in[N]:\\tilde{Z}_{2j q}^{\\star,c}\\neq Z_{2j q}^{c})\\leq\\delta\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Algorithm 3 FedLSA with Markovian data ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Input: $\\eta>0,\\theta_{0}\\in\\mathbb{R}^{d},T,N,H>0$ , time window $q\\in\\mathbb{N}$ .   \nfor $t=0$ to $T-1$ do Initialize $\\theta_{t,0}=\\theta_{t}$ for $c=1$ to $N$ do for $h=1$ to $H$ do Receive $Z_{t,h}^{c}$ , then check the condition: if $h=q j,j\\in\\mathbb{N}$ then Compute local update $\\mathbf{\\boldsymbol{\\theta}}_{t,j}^{c}=\\mathbf{\\boldsymbol{\\theta}}_{t,j-1}^{c}-\\eta(\\mathbf{\\mathbf{A}}^{c}(Z_{t,q j}^{c})-\\mathbf{\\boldsymbol{b}}^{c}(Z_{t,q j}^{c}))$ else Skip current update Average: $\\begin{array}{r}{\\frac{\\theta_{t+1}}{N}=\\frac{1}{N}\\sum_{c=1}^{N}\\theta_{t,H}^{c}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "it is enough to ensure that ", "page_idx": 21}, {"type": "equation", "text": "$$\nm N(1/4)^{\\lfloor q/\\tau\\rfloor}\\leq\\frac{2N H T(1/4)^{q/\\tau}}{q}\\leq\\delta\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Inequality (30) holds for fixed $\\delta\\in(0,1)$ , if we choose ", "page_idx": 21}, {"type": "equation", "text": "$$\nq=\\left\\lceil\\frac{\\tau_{\\mathrm{mix}}\\log\\left(2N H T/\\delta\\right)}{\\log4}\\right\\rceil\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, setting the block size $q$ as in (31), we get that for total number iterations $T H$ satisfying (29), with probability at least $1-\\delta$ the results of Algorithm 3 are indistinguishable from the result of its counterpart Algorithm 1 applied with number of local steps $H/q$ . We will denote the iterates of the latter algorithm applied with number of local steps $h\\in\\mathbb N$ as $\\theta_{T}^{(\\mathrm{ind}),h}$ in order to make explicit the dependence of global parameter upon the number of local iterates. We further denote the event, where \u03b8(Tind),H/q= \u03b8T , by A\u03b4. Thus, setting ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{H}{q}=\\mathcal{O}\\bigg(\\frac{\\tilde{v}_{\\mathrm{heter}}\\vee\\bar{\\sigma}_{\\varepsilon}}{\\mathbb{E}_{c}[\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|]}\\frac{1}{N\\epsilon}\\bigg)\\ ,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "similarly to the way the number of local updates is set in Corollary 4.3, we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}]=\\mathbb{E}[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}\\mathbf{1}_{\\mathsf{A}}]+\\mathbb{E}[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}\\mathbf{1}_{\\overline{{\\mathsf{A}}}}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[\\|\\theta_{T}^{(\\mathrm{ind}),H/q}-\\theta_{\\star}\\|^{2}\\mathbf{1}_{\\mathsf{A}}]+\\mathbb{E}[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}\\mathbf{1}_{\\overline{{\\mathsf{A}}}}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\epsilon^{2}+\\sqrt{\\delta}\\mathbb{E}^{1/2}[\\|\\theta_{T}-\\theta_{\\star}\\|^{4}]\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in the last inequality we relied on the special choice of $H/q$ from (32) together with Holder\u2019s inequality. Now it remains to bound $\\mathbb{E}[\\lVert{\\boldsymbol{\\theta}}_{T}-\\dot{{\\boldsymbol{\\theta}}}_{\\star}\\rVert^{4}]$ and tune the parameter $\\delta$ appropriately. Note that within this bound we can not rely on the estimates based on independent observations $\\{\\tilde{Z}_{2j q}^{\\star,c}\\}_{j=1,\\dots,m}$ At the same time, note that the skeleton $Z_{2j q}^{c}$ , $j\\geq0$ for any $c\\in[N]$ is a Markov chain with the Markov kernel $\\mathrm{P}_{c}^{q}$ and mixing time $\\tau_{\\operatorname*{mix}}=1$ . This allows us to write a simple upper bound on $\\mathbb{E}[\\lVert{\\boldsymbol{\\theta}}_{T}-{\\boldsymbol{\\theta}}_{\\star}\\rVert^{4}]$ based on the stability result for product of random matrices provided in [14]. Indeed, applying the result of Lemma B.1, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/2}[\\|\\theta_{T}-\\theta_{\\star}\\|^{4}]\\leq\\left(\\|\\theta_{0}-\\theta_{\\star}\\|+\\frac{2T}{N}\\sum_{c=1}^{N}\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|+\\eta T H\\|\\varepsilon\\|_{\\infty}\\right)^{2}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the corresponding bound (33) can be rewritten as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\theta_{T}-\\theta_{\\star}\\|^{2}]\\le\\epsilon^{2}+\\sqrt{\\delta}\\left(\\|\\theta_{0}-\\theta_{\\star}\\|+\\frac{2T}{N}\\sum_{c=1}^{N}\\|\\theta_{0}-\\theta_{\\star}\\|+\\eta T H\\|\\varepsilon\\|_{\\infty}\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, setting ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\delta=\\frac{\\epsilon^{4}}{H^{4}T^{4}\\left(\\lVert\\theta_{0}-\\theta_{\\star}\\rVert+\\frac{2}{N}\\sum_{c=1}^{N}\\lVert\\theta_{\\star}^{c}-\\theta_{\\star}\\rVert+\\eta\\lVert\\varepsilon\\rVert_{\\infty}\\right)^{2}}\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we obtain that the corresponding bound for block size $q$ scales as ", "page_idx": 22}, {"type": "equation", "text": "$$\nq=\\left\\lceil\\frac{\\tau_{\\mathrm{mix}}\\log\\left(2N H T/\\delta\\right)}{\\log4}\\right\\rceil\\lesssim\\left\\lceil\\tau_{\\mathrm{mix}}\\log H\\log\\left(N T^{5}\\Delta_{c o r r}/\\epsilon^{2}\\right)\\right\\rceil\\ ,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we write $\\lesssim$ for inequality up to an absolute constant and set ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Delta_{c o r r}=\\left(\\lVert\\theta_{0}-\\theta_{\\star}\\rVert+\\frac{2}{N}\\sum_{c=1}^{N}\\lVert\\theta_{\\star}^{c}-\\theta_{\\star}\\rVert+\\eta\\lVert\\varepsilon\\rVert_{\\infty}\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combination of the above bounds yields that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\lVert{\\boldsymbol{\\theta}}_{T}-{\\boldsymbol{\\theta}}_{\\star}\\rVert^{2}]\\leq2\\epsilon^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the proof is completed. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.1. Assume A2 and A3. Then, for the iterates $\\theta_{t}$ of Algorithm 3 run with parameters $\\eta,H,q$ satisfying the relation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\eta H}{q}\\geq\\frac{12}{\\tilde{a}}\\left(2+\\frac{\\log d}{2}+\\frac{\\log\\kappa_{Q}}{2}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "it holds for any probability distribution $\\xi$ on $(Z,{\\mathcal{Z}})$ and any $t\\in\\mathbb{N}_{;}$ , that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi}^{1/4}[\\|\\theta_{t}-\\theta_{\\star}\\|^{4}]\\le\\|\\theta_{0}-\\theta_{\\star}\\|+\\frac{2t}{N}\\sum_{c=1}^{N}\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|+\\eta t H\\|\\varepsilon\\|_{\\infty}\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. First we write a counterpart of the error decomposition (12) - (13) for the LSA error of the subsampled iterates of Algorithm 3. Namely, we write that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t}-\\theta_{\\star}=\\bar{\\Gamma}_{t,H}^{(\\eta,q)}\\{\\theta_{t-1}-\\theta_{\\star}\\}+\\varkappa_{t,H}+\\eta\\bar{\\varphi}_{t,H}\\ ,\\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we have defined ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{t,m:n}^{(c,\\eta,q)}=\\displaystyle\\prod_{h=m}^{n}\\left(\\mathrm{I}-\\eta\\mathbf{A}\\big(Z_{t,q h}^{c}\\big)\\right)\\,,\\quad1\\leq m\\leq n\\leq H\\,,}\\\\ &{\\bar{\\Gamma}_{t,H}^{(\\eta,q)}=\\frac{1}{N}\\sum_{c=1}^{N}\\Gamma_{t,1:H}^{(c,\\eta,q)}\\,\\,,}\\\\ &{\\varkappa_{t,H}=\\frac{1}{N}\\sum_{c=1}^{N}(\\mathrm{I}-\\Gamma_{t,1:H}^{(c,\\eta,q)})\\bigl\\{\\theta_{\\star}^{c}-\\theta_{\\star}\\bigr\\}\\,,}\\\\ &{\\bar{\\varphi}_{t,H}=-\\frac{1}{N}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta,q)}\\varepsilon^{c}\\big(Z_{t,q h}^{c}\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For notation simplicity we have removed the dependence of $\\varkappa_{t,H}$ on the subsampling parameter $q\\in\\mathbb{N}$ Thus, applying the result of [14, Proposition 7] (see also Lemma B.2) together with Minkowski\u2019s inequality, we obtain from the previous bound that for any distribution $\\xi$ on $(Z,{\\mathcal{Z}})$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}^{1/4}[\\|\\Gamma_{t,1:H}^{(c,\\eta,q)}\\|^{4}]\\le\\sqrt{\\kappa_{Q,c}}\\mathrm{e}^{2}d^{1/2}\\mathrm{e}^{-\\eta\\tilde{a}H/(12q)}\\le1\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "provided that the ratio $\\eta H/q$ satisfies the relation (34). This bound yields that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}_{\\xi}^{1/4}[\\|\\bar{\\Gamma}_{t,H}^{(\\eta,q)}\\|^{4}]\\leq1~,}}\\\\ {{\\displaystyle\\mathbb{E}_{\\xi}^{1/4}[\\|\\varkappa_{t,H}\\|^{4}]\\leq\\frac{2}{N}\\sum_{c=1}^{N}\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|~,}}\\\\ {{\\displaystyle\\mathbb{E}_{\\xi}^{1/4}[\\|\\bar{\\varphi}_{t,H}\\|^{4}]\\leq H\\|\\varepsilon\\|_{\\infty}~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, we obtain by running the recurrence (35), that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi}^{1/4}[\\|\\theta_{t}-\\theta_{\\star}\\|^{4}]\\le\\|\\theta_{0}-\\theta_{\\star}\\|+\\frac{2t}{N}\\sum_{c=1}^{N}\\|\\theta_{\\star}^{c}-\\theta_{\\star}\\|+\\eta t H\\|\\varepsilon\\|_{\\infty}\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the statement follows. ", "page_idx": 22}, {"type": "text", "text": "Stability results on product of random matrices. The results of this paragraph provides the stability bound for the product of random matrices \u0393t(,c,m\u03b7:,nq) defined in (36). Define the quantities ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{\\infty}^{\\mathrm{(M)}}=\\left[\\tilde{\\eta}_{\\infty}\\wedge\\kappa_{Q}^{-1/2}\\,\\mathrm{C}_{\\mathbf{A}}^{-1}\\wedge\\tilde{a}/(6\\mathrm{e}\\kappa_{Q}\\,\\mathrm{C}_{\\mathbf{A}})\\right]\\times\\left[8\\kappa_{Q}^{1/2}\\,\\mathrm{C}_{\\mathbf{A}}\\wedge\\tilde{a}\\right]^{-1}\\wedge\\mathrm{c}_{\\mathbf{A}}^{\\mathrm{(M)}}/2\\ ,}\\\\ &{\\mathrm{\\normalfont~C}_{\\mathbf{F}}=4(\\kappa_{Q}^{1/2}\\,\\mathrm{C}_{\\mathbf{A}}+\\tilde{a}/6)^{2}\\times\\left[8\\kappa_{Q}^{1/2}\\,\\mathrm{C}_{\\mathbf{A}}\\wedge\\tilde{a}\\right]\\,,\\quad\\mathrm{c}_{\\mathbf{A}}^{\\mathrm{(M)}}=\\tilde{a}/\\left\\{12\\,\\mathrm{C}_{\\mathbf{F}}\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then the following result holds: ", "page_idx": 23}, {"type": "text", "text": "Lemma B.2 (Proposition 7 from [14], simplified). Assume $A2$ and A3. Then, for any $c\\in[N]$ , $t\\in\\mathbb{N}$ , step size $\\eta\\in\\left(0,\\eta_{\\infty}^{\\mathrm{(M)}}\\right]$ , any $n\\in\\mathbb{N},$ , $q\\geq\\tau_{\\mathrm{mix}},$ , and probability distribution $\\xi$ on $(Z,{\\mathcal{Z}})$ , it holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}^{1/4}\\left[\\|\\Gamma_{t,m:n}^{(c,\\eta,q)}\\|^{4}\\right]\\le\\sqrt{\\kappa_{Q,c}}\\mathrm{e}^{2}d^{1/2}\\mathrm{e}^{-\\tilde{a}\\eta(n-m)/12}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. It is enough to note that, since $q\\ge\\tau_{\\mathrm{mix}}$ , and we consider $q$ -skeleton of each Markov kernels $\\mathrm{P}_{c}$ , each of the subsampled kernels $\\mathrm{P}_{c}^{q}$ will have a mixing time 1. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Berbee\u2019s lemma construction. We outline some preliminaries associated with the Berbee\u2019s coupling lemma [3] construction. We recall first a definition of the $\\beta.$ -mixing coefficient. Consider a probability space $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ equipped with $\\sigma$ -fields $\\mathfrak{F}$ and $\\mathfrak{G}$ such that $\\mathfrak{F}\\subseteq\\mathcal{F},\\mathfrak{G}\\subseteq\\mathcal{F}$ . Then the $\\beta$ -mixing coefficient of $\\mathfrak{F}$ and $\\mathfrak{G}$ is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\beta(\\mathfrak{F},\\mathfrak{G})=(1/2)\\operatorname*{sup}\\sum_{i\\in1}\\sum_{j\\in\\mathfrak{J}}\\left|\\mathbb{P}(\\mathsf{A}_{i}\\cap\\mathsf{B}_{j})-\\mathbb{P}(\\mathsf{A}_{i})\\mathbb{P}(\\mathsf{B}_{j})\\right|\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the supremum is taken over all pairs of partitions $\\{\\mathsf{A}_{i}\\}_{i\\in\\mathsf{I}}\\in\\mathfrak{F}^{\\mathsf{I}}$ and $\\{\\mathsf{B}_{j}\\}_{j\\in\\mathsf{J}}\\in\\mathfrak{G}^{\\mathsf{J}}$ of $\\tilde{Z}_{\\mathbb{N}}$ with finite I and J. ", "page_idx": 23}, {"type": "text", "text": "Now let $(Z,\\mathsf{d}_{Z})$ be a Polish space endowed with its Borel $\\sigma$ -field, denoted by $\\mathcal{Z}$ , and let $(Z^{\\mathbb{N}},{\\mathcal{Z}}^{\\otimes\\mathbb{N}})$ be the corresponding canonical space. Consider a Markov kernel $\\mathrm{P}$ on $Z\\times{\\mathcal{Z}}$ and denote by $\\mathbb{P}_{\\xi}$ and $\\mathbb{E}_{\\xi}$ the corresponding probability distribution and expectation with initial distribution $\\xi$ . Without loss of generality, we assume that $(Z_{k})_{k\\in\\mathbb{N}}$ is the associated canonical process. By construction, for any $\\mathsf{A}\\in\\mathcal{Z}$ $\\mathrm{:}\\,\\mathcal{Z},\\mathbb{P}_{\\xi}\\,(\\,Z_{k}\\in\\mathsf{A}\\,|\\,Z_{k-1})=\\mathrm{P}(Z_{k-1},\\mathsf{A})$ , $\\mathbb{P}_{\\xi}$ -a.s. In the case $\\xi=\\delta_{z},\\,z\\in\\mathsf{Z},\\mathbb{P}_{\\xi}$ and $\\mathbb{E}_{\\xi}$ are denoted by $\\mathbb{P}_{z}$ and $\\mathbb{E}_{z}$ , respectively. We now make an assumption about the mixing properties of P: ", "page_idx": 23}, {"type": "text", "text": "${U G E}\\,1$ . The Markov kernel $\\mathrm{P}$ admits $\\pi$ as an invariant distribution and is uniformly geometrically ergodic, that is, there exists $\\tau_{\\operatorname*{mix}}\\in\\mathbb{N}$ such that for all $k\\in\\mathbb{N}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta(\\mathrm{P}^{k})=\\operatorname*{sup}_{z,z\\,\\in\\mathrm{Z}}(1/2)\\|\\mathrm{P}^{k}(z,\\cdot)-\\mathrm{P}^{k}(z^{\\prime},\\cdot)\\|_{\\mathsf{T V}}\\leq(1/4)^{\\lfloor k/\\tau\\rfloor}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $q\\in\\mathbb{N},\\,k\\,\\in\\,\\mathbb{N}$ , and the Markov chain $\\{Z_{n}\\}_{n\\in\\mathbb{N}}$ satisfying the uniform geometric ergodicity constraint UGE 1, we define the $\\sigma$ -algebras $\\mathcal{F}_{k}=\\sigma(Z_{\\ell},\\ell\\leq k)$ and $\\mathcal{F}_{k+q}^{+}=\\sigma(Z_{\\ell},\\ell\\geq k+q)$ . In such a scenario, using [13, Theorem 3.3], the respective $\\beta$ -mixing coefficient of $\\mathcal{F}_{k}$ and $\\mathcal{F}_{k+q}^{+}$ is bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\beta(q)\\equiv\\beta(\\mathcal{F}_{k},\\mathcal{F}_{k+q}^{+})\\leq\\Delta(\\mathrm{P}^{k})=(1/4)^{\\lfloor q/\\tau\\rfloor}\\ .\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We rely on the following useful version of Berbee\u2019s coupling lemma [3], which is due to [10, Lemma 4.1]: ", "page_idx": 23}, {"type": "text", "text": "Theorem B.3 (Lemma 4.1 in [10]). Let $X$ and $Y$ be two random variables taking their values in Borel spaces $\\mathcal{X}$ and $\\mathcal{V}$ , respectively, and let $U$ be a random variable with uniform distribution on [0; 1] that is independent of $(X,Y)$ . There exists a random variable $Y^{\\star}=f(X,Y,U)$ where $f$ is $a$ measurable function from $\\mathcal{X}\\times\\mathcal{Y}\\times[0,1]$ to $\\boldsymbol{\\wp}$ , such that: ", "page_idx": 23}, {"type": "text", "text": "1. $Y^{\\star}$ is independent of $X$ and has the same distribution as $Y$ ;   \n2. $\\mathbb{P}(Y^{\\star}\\neq Y)=\\beta(\\sigma(X),\\sigma(Y))$ . ", "page_idx": 23}, {"type": "text", "text": "Let us now consider the extended measurable space $\\tilde{Z}_{\\mathbb{N}}=Z^{\\mathbb{N}}\\times[0,1]$ , equipped with the $\\sigma$ -field $\\tilde{\\mathcal{Z}}_{\\mathbb{N}}\\,=\\,\\mathcal{Z}^{\\otimes\\mathbb{N}}\\otimes B([0,1])$ . For each probability measure $\\xi$ on $(Z,{\\mathcal{Z}})$ , we consider the probability measure $\\tilde{\\mathbb{P}}_{\\xi}=\\mathbb{P}_{\\xi}\\otimes\\mathbf{Unif}([0,1])$ and denote by $\\tilde{\\mathbb{E}}_{\\xi}$ the corresponding expected value. Finally, we denote by $(\\Tilde{Z}_{k})_{k\\in\\mathbb{N}}$ the canonical process $\\tilde{Z}_{k}\\colon((z_{i})_{i\\in\\mathbb{N}},u)\\in\\tilde{Z}_{\\mathbb{N}}\\mapsto z_{k}$ and $U\\colon((z_{i})_{i\\in\\mathbb{N}},u)\\in\\tilde{Z}_{\\mathbb{N}}\\mapsto$ $u$ . Under $\\tilde{\\mathbb{P}}_{\\xi}$ , $\\{\\tilde{Z}_{k}\\}_{k\\in\\mathbb{N}}$ is by construction a Markov chain with initial distribution $\\xi$ and Markov kernel $\\mathrm{P}$ independent of $U$ . Moreover, the distribution of $U$ under $\\tilde{\\mathbb{P}}_{\\xi}$ is uniform over $[0,1]$ . Using the above construction, we obtain a useful blocking lemma, which is also stated in [10]. ", "page_idx": 24}, {"type": "text", "text": "Lemma B.4. Assume ${U G E I}$ , let $q\\in\\mathbb{N}$ and $\\xi$ be a probability measure on $(Z,{\\mathcal{Z}})$ . Then, there exists a random process $(\\tilde{Z}_{k}^{\\star})_{k\\in\\mathbb{N}}$ defined on $(\\tilde{Z}_{\\mathbb{N}},\\tilde{\\mathcal{Z}}_{\\mathbb{N}},\\tilde{\\mathbb{P}}_{\\xi})$ such that for any $k\\in\\mathbb{N}$ , it holds: ", "page_idx": 24}, {"type": "text", "text": "1. For any $i$ , vector $\\begin{array}{l l l}{V_{i}^{\\star}}&{=}&{(\\tilde{Z}_{i q+1}^{\\star},\\ldots,\\tilde{Z}_{i q+q}^{\\star})}\\end{array}$ has the same distribution as $\\;V_{i}\\;\\;=\\;\\;$ $(Z_{i q+1},\\ldots,Z_{i q+q})$ under $\\tilde{\\mathbb{P}}_{\\xi}$ ;   \n2. The sequences $(V_{2i}^{\\star})_{i\\geq0}$ and $(V_{2i+1}^{\\star})_{i\\ge0}$ are i.i.d. ;   \n3. For any $i$ , $\\tilde{\\mathbb{P}}_{\\xi}(V_{i}\\neq V_{i}^{\\star})\\leq\\beta(q)$ ; ", "page_idx": 24}, {"type": "text", "text": "Proof. The proof follows from Theorem B.3 and the relations between UGE 1 and $\\beta$ -mixing coefficient, see e.g. [13, Theorem 3.3]. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C Federated Linear Stochastic Approximation with Control Variates ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Technical Lemmas ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma C.1. Assume A1 and A3. Recall C\u03b7(t,,Hc) $\\begin{array}{r}{C_{\\eta,H}^{(t,c)}=\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta)}}\\end{array}$ Then it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{(t,c)}\\right\\Vert^{2}\\right]\\leq\\frac{\\eta^{2}H^{2}}{4}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}\\!+\\!\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We rewrite I \u2212C\u03b7(t,,Hc) using Lemma D.1 as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{(t,c)}=\\frac{1}{H}\\sum_{h=1}^{H}\\left\\{\\mathrm{I}-\\Gamma_{t,h+1:H}^{(c,\\eta)}\\right\\}=\\frac{\\eta}{H}\\sum_{h=1}^{H}\\sum_{\\ell=h+1}^{H}\\mathbf{A}^{c}(Z_{t,\\ell}^{c})\\Gamma_{t,\\ell+1:H}^{(c,\\eta)}\\;,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which can then be decomposed as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{(t,c)}=\\frac{\\eta}{H}\\sum_{h=1}^{H}\\sum_{\\ell=h+1}^{H}\\bar{\\mathbf{A}}^{c}\\Gamma_{t,\\ell+1:H}^{(c,\\eta)}+\\frac{\\eta}{H}\\sum_{h=1}^{H}\\sum_{\\ell=h+1}^{H}\\left\\{\\mathbf{A}^{c}(Z_{t,\\ell}^{c})-\\bar{\\mathbf{A}}^{c}\\right\\}\\Gamma_{t,\\ell+1:H}^{(c,\\eta)}\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Minkowski\u2019s inequality and A3 give $\\begin{array}{r}{\\mathbb{E}^{1/2}\\left[\\|\\frac{\\eta}{H}\\sum_{h=1}^{H}\\sum_{\\ell=h+1}^{H}\\bar{\\mathbf{A}}^{c}\\Gamma_{t,\\ell+1:H}^{(c,\\eta)}\\|^{2}\\right]\\le\\frac{\\eta H}{2}\\|\\bar{\\mathbf{A}}^{c}\\|}\\end{array}$ . The second term has a reverse martingale structure, and we thus have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{(t,c)}\\right\\Vert^{2}\\right]\\leq\\frac{\\eta^{2}H^{2}}{4}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}\\!+\\!\\|\\mathrm{D}_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is the result of the lemma. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.2. Assume A1 and A3. RecallC tc+1 = hH=1 \u0393 $\\begin{array}{r}{\\widetilde{C}_{t+1}^{c}=\\sum_{h=1}^{H}\\left\\{\\Gamma_{t,h+1:H}^{(c,\\eta)}-(\\mathrm{I}-\\bar{\\mathbf{A}}^{c})^{H-h}\\right\\}}\\end{array}$ . Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\widetilde{C}_{t+1}^{c}\\|^{2}\\right]\\leq\\eta^{2}H^{4}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We start by recalling the definition of $\\widetilde{C}_{t+1}^{c}$ , that is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{C}_{t+1}^{c}=C_{t+1}^{c}-\\frac{1}{N}\\sum_{c=1}^{N}\\mathbb{E}[C_{t+1}^{\\tilde{c}}]=\\frac{1}{N}\\sum_{\\tilde{c}=1}^{N}\\sum_{h=1}^{H}\\left\\{\\Gamma_{t,h+1:H}^{(c,\\eta)}-(\\mathrm{I}-\\bar{\\mathbf{A}}^{c})^{H-h}\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using Lemma D.1, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{C}_{t+1}^{c}=\\frac{\\eta}{N}\\sum_{\\tilde{c}=1}^{N}\\sum_{h=1}^{H}\\sum_{\\ell=h}^{H}\\Gamma_{t,h+1:\\ell}^{(c,\\eta)}\\left\\{\\mathbf{A}^{c}(Z_{t,\\ell}^{c})-\\bar{\\mathbf{A}}^{\\tilde{c}}\\right\\}(\\mathrm{I}-\\bar{\\mathbf{A}}^{c})^{H-\\ell-1}\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Minkowski\u2019s inequality and Assumption A3, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\mathbb E}^{1/2}\\left[\\|\\widetilde C_{t+1}^{c}\\|^{2}\\right]=\\frac{\\eta}{N}\\sum_{\\tilde{c}=1}^{N}\\sum_{h=1}^{H}\\sum_{\\ell=h}^{H}{\\mathbb E}^{1/2}\\left[\\|{\\bf A}^{c}(Z_{t,\\ell}^{c})-\\bar{{\\bf A}}^{\\tilde{c}}\\|^{2}\\right]~.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, we notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\mathbf{A}^{c}(Z_{t,\\ell}^{c})-\\bar{\\mathbf{A}}^{\\bar{c}}\\|^{2}\\right]=\\mathbb{E}\\left[\\|\\mathbf{A}^{c}(Z_{t,\\ell}^{c})-\\bar{\\mathbf{A}}^{c}\\|^{2}\\right]+\\|\\bar{\\mathbf{A}}^{c}-\\bar{\\mathbf{A}}^{\\bar{c}}\\|^{2}\\leq\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\Delta}}^{c}\\|\\ ,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the result of the lemma follows. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.3. Assume A1 and A3. Recall $\\begin{array}{r}{C_{\\eta,H}^{(t,c)}=\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta)}}\\end{array}$ then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\widetilde{\\Gamma}_{t+1}^{c}\\|]\\leq2\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Denote ${\\bf A}_{h}^{c}={\\bf A}^{c}(Z_{t+1,h}^{c})$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widetilde{\\Gamma}_{t+1}^{c}=\\Gamma_{t+1}-\\Gamma_{t+1}^{c}}\\\\ {\\displaystyle=\\frac{1}{N}\\sum_{c=1}^{N}\\left\\{\\displaystyle\\prod_{h=1}^{H}(\\mathrm{I}-\\eta\\mathbf{A}_{h}^{c})-\\displaystyle\\prod_{h=1}^{H}(\\mathrm{I}-\\eta\\mathbf{A}_{h}^{c})\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using Lemma D.1, we can rewrite ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\Gamma}_{t+1}^{c}=\\frac{\\eta}{N}\\sum_{c=1}^{N}\\sum_{k=1}^{H}\\left\\{\\prod_{h=1}^{k-1}(\\mathrm{I}-\\eta{\\bf A}_{h}^{c})\\right\\}\\left\\{{\\bf A}_{k}^{c}-{\\bf A}_{k}^{c}\\right\\}\\left\\{\\prod_{h=k+1}^{H}(\\mathrm{I}-\\eta{\\bf A}_{h}^{c})\\right\\}\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using triangle inequality and the fact that ${\\bf A}_{h}^{c}$ \u2019s are independent from each other, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\widetilde{\\Gamma}_{t+1}^{c}\\|]=\\frac{\\eta}{N}\\sum_{c=1}^{N}\\sum_{k=1}^{H}\\mathbb{E}\\left[\\left\\|\\prod_{h=1}^{k-1}(\\mathrm{I}-\\eta\\mathbf{A}_{h}^{c})\\right\\|\\right]\\mathbb{E}\\left[\\left\\|\\mathbf{A}_{k}^{c}-\\mathbf{A}_{k}^{c}\\right\\|\\right]\\mathbb{E}\\left[\\left\\|\\prod_{h=k+1}^{H}(\\mathrm{I}-\\eta\\mathbf{A}_{h}^{c})\\right\\|\\right]\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By triangle inequality, and using the definition of $\\mathrm{C}_{\\mathbf{A}}$ and $\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|$ , we have $\\mathbb{E}[\\|\\mathbf{A}_{k}^{c}\\,-\\,\\mathbf{A}_{k}^{c}\\|]\\le$ $\\mathbb{E}[\\|\\mathbf{A}_{k}^{c}-\\bar{\\mathbf{A}}^{c}\\|+\\|\\bar{\\mathbf{A}}^{c}-\\bar{\\mathbf{A}}^{c}\\|+\\|\\mathbf{A}_{k}^{c}-\\bar{\\mathbf{A}}^{c}\\|]\\leq2\\,\\mathrm{C}_{\\mathbf{A}}+2\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|.$ Therefore, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\widetilde{\\Gamma}_{t+1}^{c}\\|]\\le2\\eta\\sum_{k=1}^{H}(1-\\eta a)^{H-1}\\left(\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the result follows. ", "page_idx": 25}, {"type": "text", "text": "C.2 Proof ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The linear structure of SCAFFLSA\u2019s updates allow to decompose the updates between a transient term, and a fluctuation term. To materialize this, we define the following virtual parameters ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\check{\\theta}_{0}=\\theta_{0}\\;,\\quad\\check{\\theta}_{0,0}^{c}=\\check{\\theta}_{0}\\;,\\mathrm{~and~}\\;\\check{\\xi}_{0}^{c}=\\xi_{0}^{c}\\;,\\quad\\mathrm{~for~all~}c\\in\\{1,\\dots,N\\}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "These parameters are updated similarly to $\\theta_{t}$ \u2019s and $\\xi_{t}^{c}\\mathbf{\\widetildes}.$ , although without the last fluctuation term. For the virtual parameter $\\check{\\theta}$ , the update is similar to (8), as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\check{\\theta}_{t,h}^{c}-\\theta_{\\star}=(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z_{t,h}^{c}))(\\check{\\theta}_{t,h-1}^{c}-\\theta_{\\star})+\\eta(\\xi_{t}^{c}-\\xi_{\\star}^{c})\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which gives, after $H$ local updates, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}=\\Gamma_{t+1}^{c}(\\check{\\theta}_{t}^{c}-\\theta_{\\star})+\\eta C_{t+1}^{c}(\\xi_{t}^{c}-\\xi_{\\star}^{c})\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we recall $\\begin{array}{r}{\\Gamma_{t+1}^{c}=\\prod_{h=1}^{H}(\\operatorname{I}-\\eta\\mathbf{A}(Z_{t,h}^{c}))}\\end{array}$ and $\\begin{array}{r}{C_{t+1}^{c}=\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{(c,\\eta)}.}\\end{array}$ . The virtual parameters obtained after $H$ local u pdates are then aggregated as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\check{\\theta}_{t+1}=\\frac{1}{N}\\sum_{c=1}^{N}\\check{\\theta}_{t,H}^{c}\\ .\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is then used to define the virtual control variates, similarly to (9), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\check{\\xi}_{t+1}^{c}=\\check{\\xi}_{t}^{c}+\\frac{1}{\\eta H}(\\check{\\theta}_{t+1}-\\check{\\theta}_{t,H}^{c})\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "These updates can be summarized over one block, which gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\check{\\theta}_{t+1}-\\check{\\theta}_{\\star}=\\Gamma_{t+1}(\\check{\\theta}_{t}-\\theta_{\\star})+\\frac{\\eta}{N}\\sum_{c=1}^{N}C_{t+1}^{c}(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})~,}\\\\ {\\displaystyle\\check{\\xi}_{t+1}^{c}-\\xi_{\\star}^{c}=\\frac{1}{\\eta H}(\\Gamma_{t+1}-\\Gamma_{t+1}^{c})(\\check{\\theta}_{t}-\\theta_{\\star})+\\big(\\mathrm{I}-\\displaystyle\\frac{1}{H}C_{t+1}^{c}\\big)(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})+\\frac{1}{H N}\\displaystyle\\sum_{\\tilde{c}=1}^{N}C_{t+1}^{\\tilde{c}}(\\check{\\xi}_{t}^{\\tilde{c}}-\\xi_{\\star}^{\\tilde{c}})~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The analysis of SCAFFLSA can then be decomposed into (i) analysis of the \"transient\" virtual iterates $\\breve{\\theta_{t}}$ \u2019s and $\\check{\\xi}_{t}^{c}$ \u2019s, and (ii) analysis of the fluctuations $\\theta_{t}-{\\check{\\theta}}_{t}$ and $\\dot{\\xi}_{t}^{c}-\\check{\\xi}_{t}^{c}$ . ", "page_idx": 26}, {"type": "text", "text": "Analysis of the Transient Term. First, we analyze the convergence of the virtual variables ${\\check{\\theta}}_{t}$ and $\\check{\\xi}_{t}^{c}$ for $t\\geq0$ and $c\\in\\{1,\\ldots,N\\}$ . Consider the Lyapunov function, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\psi_{t}=\\|\\check{\\theta}_{t}-\\theta_{\\star}\\|^{2}+\\frac{\\eta^{2}H^{2}}{N}\\sum_{c=1}^{N}\\|\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c}\\|^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is naturally defined as the error in $\\theta_{\\star}$ estimation using the virtual iterates, on communication rounds, and the average error on the virtual control variates. ", "page_idx": 26}, {"type": "text", "text": "Theorem C.4. Assume A1 and A3. Let $\\eta,H$ such that $\\eta a H\\leq1,$ , and $H\\,\\leq\\,{\\frac{a}{2\\eta\\{\\mathrm{C}+\\|\\Sigma\\|\\}}}$ , and set $\\xi_{0}^{c}=0$ for all $c\\in[N]$ . Then, the sequence $(\\psi_{t})_{t\\in\\mathbb{N}}$ satisfies, for all $t\\geq0$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\psi_{t}]\\leq\\left(1-\\frac{\\eta a H}{4}\\right)^{t}\\mathbb{E}[\\psi_{0}]\\;,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{\\psi_{0}=\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\frac{\\eta H}{N}\\sum_{c=1}^{N}\\|\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\|^{2}.}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Proof. Expression of the Lyapunov function. Since the sum virtual control variates is $\\sum_{t=1}^{N}\\check{\\xi}_{t}^{c}=$ $\\sum_{t=1}^{N}\\xi_{\\star}^{\\,\\,c}\\;=\\;0$ , we have $\\begin{array}{r}{\\check{\\theta}_{t+1}~=~\\frac{1}{N}\\sum_{c=1}^{N}\\check{\\theta}_{t,H}^{c}~=~\\frac{1}{N}\\sum_{c=1}^{N}\\check{\\theta}_{t,H}^{c}~-~\\eta H(\\xi_{t}^{c}-\\xi_{\\star}^{c})}\\end{array}$ .  Applying Lemma D.3, we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\check{\\theta}_{t+1}-\\theta_{\\star}\\|^{2}=\\|\\frac{1}{N}\\sum_{c=1}^{N}\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}-\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}}\\\\ {\\displaystyle=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}-\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}-\\frac{1}{N}\\sum_{c=1}^{N}\\|\\check{\\theta}_{t+1}-\\check{\\theta}_{t,H}^{c}+\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}}\\\\ {\\displaystyle=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}-\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}-\\frac{\\eta^{2}H^{2}}{N}\\sum_{c=1}^{N}\\|\\check{\\xi}_{t+1}^{c}-\\xi_{\\star}^{c}\\|^{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "since $\\begin{array}{r}{\\check{\\xi}_{t+1}^{c}=\\check{\\xi}_{t}^{c}+\\frac{1}{\\eta H}\\big(\\check{\\theta}_{t+1}-\\check{\\theta}_{t,H}^{c}\\big)}\\end{array}$ . Adding $\\begin{array}{r}{\\frac{\\eta H}{N}\\sum_{c=1}^{N}\\|\\check{\\xi}_{t+1}^{c}-\\xi_{\\star}^{c}\\|^{2}}\\end{array}$ on both sides, we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\psi_{t+1}=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}-\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}\\;,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we defined $\\begin{array}{r}{C_{\\eta,H}^{c}=\\sum_{h=1}^{H}\\Gamma_{t,h+1:H}^{\\left(c,\\eta\\right)}}\\end{array}$ . In the following, we will use the filtration of all events up to step $t$ , $\\mathcal{F}_{t}:=\\sigma(Z_{s,h}^{c},0\\le s\\le t,0\\le h\\le H,1\\le c\\le N$ ). ", "page_idx": 27}, {"type": "text", "text": "Using Young\u2019s inequality, and Assumption A3, we can bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}-\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}]=\\|\\Gamma_{t,1:H}^{(c,\\eta)}(\\check{\\theta}_{t}-\\theta_{\\star})-\\eta H(\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{c})(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}}\\\\ &{\\quad\\le\\mathbb{E}[(1+\\alpha_{0})\\|\\Gamma_{t,1:H}^{(c,\\eta)}(\\check{\\theta}_{t}-\\theta_{\\star})\\|^{2}]+(1+\\alpha_{0}^{-1})\\eta^{2}H^{2}\\mathbb{E}[\\|(\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{c})(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}]}\\\\ &{\\quad\\le(1+\\alpha_{0})(1-\\eta a)^{2H}\\mathbb{E}[\\|\\check{\\theta}_{t}-\\theta_{\\star}\\|^{2}]+(1+\\alpha_{0}^{-1})\\eta^{2}H^{2}\\mathbb{E}[\\|(\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{c})(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using Lemma C.1, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|(\\mathrm{I}-\\frac{1}{H}C_{\\eta,H}^{c})(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}]\\le\\frac{\\eta^{2}H^{2}}{4}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|{\\Sigma}_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\mathbb{E}[\\|\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c}\\|^{2}]\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We thus obtain, for $H$ such that $\\eta a H\\leq1$ , and after setting $\\begin{array}{r}{\\alpha_{0}\\,=\\,\\frac{\\eta a H}{2}}\\end{array}$ and using the facts that $\\begin{array}{r}{(1-\\eta a H)(1+\\alpha_{0})\\le1-\\frac{\\eta a H}{2}}\\end{array}$ and $1+\\alpha_{0}^{-1}\\leq2\\alpha_{0}^{-1}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}-\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}]}\\\\ &{\\quad\\le\\left(1-\\displaystyle\\frac{\\eta a H}{2}\\right)\\mathbb{E}[\\|\\check{\\theta}_{t}-\\theta_{\\star}\\|^{2}]+\\displaystyle\\frac{1}{\\eta a H}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|\\right\\}\\eta^{4}H^{4}\\mathbb{E}[\\|\\xi_{t}^{c}-\\xi_{\\star}^{c}\\|^{2}]}\\\\ &{\\quad=\\left(1-\\displaystyle\\frac{\\eta a H}{2}\\right)\\mathbb{E}[\\|\\check{\\theta}_{t}-\\theta_{\\star}\\|^{2}]+\\displaystyle\\frac{\\eta H}{a}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\varepsilon}^{c}\\|\\right\\}\\eta^{2}H^{2}\\mathbb{E}[\\|\\xi_{t}^{c}-\\xi_{\\star}^{c}\\|^{2}]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, since $\\begin{array}{r}{\\frac{\\eta H}{a}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\varepsilon}^{c}\\|\\right\\}\\leq\\frac{1}{2}}\\end{array}$ , we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\check{\\theta}_{t,H}^{c}-\\theta_{\\star}-\\eta H(\\check{\\xi}_{t}^{c}-\\xi_{\\star}^{c})\\|^{2}]\\le\\left(1-\\frac{\\eta a H}{2}\\right)\\mathbb{E}\\Big[\\|\\check{\\theta}_{t}-\\theta_{\\star}\\|^{2}+\\eta^{2}H^{2}\\|\\xi_{t}^{c}-\\xi_{\\star}^{c}\\|^{2}\\Big]\\;,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the result follows by plugging (39) back in (38). ", "page_idx": 27}, {"type": "text", "text": "Analysis of the Fluctuations. To study the fluctuations, we define the following quantities, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\theta}_{t}=\\theta_{t}-\\check{\\theta}_{t}\\;,\\;\\mathrm{and}\\quad\\widetilde{\\xi}_{t}^{c}=\\xi_{t}^{c}-\\check{\\xi}_{t}^{c}\\;,\\quad\\mathrm{for}\\;t\\ge0\\;,\\;\\mathrm{and}\\;c\\in\\{1,\\dots,N\\}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Our analysis is based on a careful study of the recurrence between variances and covariances of parameters and control variates. We thus start by deriving recurrence properties on these quantities. From the update of $\\theta_{t}$ , we have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t+1}-\\theta_{\\star}=\\Gamma_{t+1}(\\theta_{t}-\\theta_{\\star})+\\displaystyle\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}(\\xi_{t}^{\\widetilde{c}}-\\xi_{\\star}^{\\widetilde{c}})-\\eta\\bar{\\varepsilon}_{t+1}}\\\\ {=\\check{\\theta}_{t+1}-\\theta_{\\star}+\\Gamma_{t+1}\\widetilde{\\theta}_{t}+\\displaystyle\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}-\\eta\\bar{\\varepsilon}_{t+1}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which can be rewritten as a recursive update of the fluctuations ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\theta}_{t+1}=\\Gamma_{t+1}\\widetilde{\\theta}_{t}+\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}-\\eta\\bar{\\varepsilon}_{t+1}\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, we have, for the fluctuations of the control variates ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\xi}_{t+1}^{c}=\\frac{1}{\\eta H}\\big(\\Gamma_{t+1}-\\Gamma_{t+1}^{c}\\big)\\widetilde{\\theta}_{t}+\\Big(\\mathbb{I}-\\frac{1}{H}C_{t+1}^{c}\\Big)\\widetilde{\\xi}_{t}^{c}+\\frac{1}{N H}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}-\\frac{1}{H}\\big(\\bar{\\varepsilon}_{t+1}-\\varepsilon_{t+1}^{c}\\big)\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Remark that, for all $t\\geq0$ , $\\widetilde{\\theta}_{t}$ and $\\widetilde{\\xi}_{t}^{c}$ \u2019s are sums of (random) linear operations computed on zero-mean vectors, that are independent from these linear operations. Thus, for all $t\\geq0$ and all $c\\in\\{1,\\ldots,N\\}$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\widetilde{\\theta}_{t}]=0\\quad,\\quad\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}]=0\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now aim at recursively finding a sequence of upper bounds $\\{b_{t}^{(\\theta,\\theta)},b^{(\\theta,\\xi)},b^{=},b^{\\neq}\\}_{t\\geq0}$ such that, for all $t\\ge0,c,c^{\\prime}\\in\\{1,\\dots,N\\}$ such that $c\\neq c^{\\prime}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbb{E}\\left[(\\widetilde{\\theta}_{t})(\\widetilde{\\theta}_{t})^{\\top}\\right]\\right\\|\\leq b_{t}^{(\\theta,\\theta)}\\;,}\\\\ &{\\left\\|\\mathbb{E}\\left[(\\widetilde{\\theta}_{t})(\\widetilde{\\xi}_{t}^{c})^{\\top}\\right]\\right\\|\\leq b_{t}^{(\\theta,\\xi)}\\;\\mathbf{and}\\;\\Big\\|\\mathbb{E}\\left[(\\widetilde{\\xi}_{t}^{c})(\\widetilde{\\theta}_{t})^{\\top}\\right]\\Big\\|\\leq b_{t}^{(\\theta,\\xi)}\\;,}\\\\ &{\\qquad\\qquad\\qquad\\Big\\|\\mathbb{E}\\left[(\\widetilde{\\xi}_{t}^{c})(\\widetilde{\\xi}_{t}^{c})^{\\top}\\right]\\Big\\|\\leq b_{t}^{\\mathcal{F}}\\;,}\\\\ &{\\qquad\\qquad\\qquad\\Big\\|\\mathbb{E}\\left[(\\widetilde{\\xi}_{t}^{c})(\\widetilde{\\xi}_{t}^{c})^{\\top}\\right]\\Big\\|\\leq b_{t}^{=}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(Initialization.) For $t=0$ , nothing is random so the fluctuations are zero, and $b_{0}^{(\\theta,\\theta)}=b_{0}^{(\\theta,\\xi)}=b^{=}=$ $b^{\\neq}=0$ . We also study the first iteration of SCAFFLSA. In the following lemma, we give upper bounds on the variances and covariances of the parameters obtained after one iteration. ", "page_idx": 28}, {"type": "text", "text": "Lemma C.5. Assume A1 and A3, then the first iterate of SCAFFLSA satisfy the following inequalities ", "page_idx": 28}, {"type": "equation", "text": "$$\nb_{1}^{(\\theta,\\theta)}=\\frac{\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|~,\\quad b_{1}^{=}=\\frac{N-1}{N H}\\|\\Sigma_{\\omega}^{c}\\|~,\\quad b_{1}^{(\\theta,\\xi)}=\\frac{2\\eta}{N}\\|\\Sigma_{\\omega}\\|~,\\quad b_{1}^{\\ne}=\\frac{3}{N H}\\|\\Sigma_{\\omega}\\|~.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. (Value of $b_{1}^{(\\theta,\\theta)}$ .) From the definition of $\\widetilde{\\theta}_{1}$ , we have $\\begin{array}{r}{\\widetilde{\\theta}_{1}=\\frac{\\eta}{N}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\Gamma_{1,h+1:H}^{(c,\\eta)}\\omega^{c}(Z_{1,h}^{c}).}\\end{array}$ By independence of the agents, and since $\\mathbb{E}[\\Gamma_{1,h+1:H}^{(c,\\eta)}\\omega(Z_{1,h}^{c})]=0$ for all $c\\in\\{1,\\ldots,N\\}$ , and for all $h\\in\\{0,\\ldots,H-1\\}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(\\widetilde{\\theta}_{1})(\\widetilde{\\theta}_{1})^{\\top}]=\\frac{\\eta^{2}}{N^{2}}\\displaystyle\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(c,\\eta)}\\omega^{c}(Z_{1,h}^{c})\\omega^{c}(Z_{1,h}^{c})^{\\top}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\eta^{2}}{N^{2}}\\displaystyle\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(c,\\eta)}\\Sigma_{\\omega}^{c}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right]~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second equality comes from the fact that, for all $h\\in\\{1,\\ldots,H-1\\}$ , the matrix $\\Gamma_{1,h+1:H}^{(c,\\eta)}$ and the vector $\\omega^{c}(Z_{1,h}^{c})$ are independent. Triangle inequality, Jensen\u2019s inequality, and definition of the operator norm then give ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbb{E}[(\\widetilde{\\theta}_{1})(\\widetilde{\\theta}_{1})^{\\top}]\\right\\|\\leq\\frac{\\eta^{2}}{N^{2}}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\left\\|\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(c,\\eta)}\\Sigma_{\\omega}^{c}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right]\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\eta^{2}}{N^{2}}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\mathbb{E}\\left[\\left\\|\\Gamma_{1,h+1:H}^{(c,\\eta)}\\Sigma_{\\omega}^{c}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right\\|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\eta^{2}}{N^{2}}\\sum_{c=1}^{N}\\sum_{h=1}^{H}\\mathbb{E}\\left[\\left\\|\\Gamma_{1,h+1:H}^{(c,\\eta)}\\right\\|^{2}\\right]\\|\\Sigma_{\\omega}^{c}\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Assumption A3 ensures that $\\mathbb{E}\\left[\\left\\Vert\\Gamma_{1,h+1:H}^{(c,\\eta)}\\right\\Vert^{2}\\right]\\leq1$ , and we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}[(\\widetilde{\\theta}_{1})(\\widetilde{\\theta}_{1})^{\\top}]\\right\\|\\leq\\frac{\\eta^{2}H}{N^{2}}\\sum_{c=1}^{N}\\|\\Sigma_{\\omega}^{c}\\|\\leq\\frac{\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\ .\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(Value of $b_{1}^{=}$ .) Let $c\\,\\in\\,\\{1,\\ldots,N\\}$ . The definition of $\\widetilde{\\xi}_{1}^{c}$ gives the following expression for the fluctuation $\\begin{array}{r}{\\widetilde{\\xi}_{1}^{c}=\\frac{1}{N H}\\sum_{\\tilde{c}=1}^{N}\\sum_{h=1}^{H}\\left\\{\\Gamma_{1,h+1:H}^{(\\tilde{c},\\eta)}\\omega^{\\tilde{c}}(Z_{1,h}^{\\tilde{c}})\\right\\}-\\frac{1}{H}\\sum_{h=1}^{H}\\Gamma_{1,h+1:H}^{(c,\\eta)}\\omega^{c}(Z_{1,h}^{c})}\\end{array}$ . Therefore, ", "page_idx": 28}, {"type": "text", "text": "we have ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(\\widetilde{\\xi}_{1}^{c})(\\widetilde{\\xi}_{1}^{c})^{\\top}]=\\mathbb{E}\\left[\\left(\\displaystyle\\frac{1}{N H}\\displaystyle\\sum_{\\widetilde{c}=1}^{N}\\sum_{h=1}^{H}\\Big\\{\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)}\\omega^{\\widetilde{c}}(Z_{1,h}^{\\widetilde{c}})\\Big\\}-\\displaystyle\\frac{1}{H}\\sum_{h=1}^{H}\\Gamma_{1,h+1:H}^{(c,\\eta)}\\omega^{c}(Z_{1,h}^{c})\\right)\\right.}\\\\ &{\\qquad\\qquad\\times\\left.\\left(\\displaystyle\\frac{1}{N H}\\displaystyle\\sum_{\\widetilde{c}=1}^{N}\\sum_{h=1}^{H}\\Big\\{(\\omega^{\\widetilde{c}}(Z_{1,h}^{\\widetilde{c}}))^{\\top}(\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)})^{\\top}\\Big\\}-\\displaystyle\\frac{1}{H}\\displaystyle\\sum_{h=1}^{H}(\\omega^{c}(Z_{1,h}^{c}))^{\\top}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right)\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "With similar arguments as above, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Xi[(\\widetilde{\\xi}_{1}^{c})(\\widetilde{\\xi}_{1}^{c})^{\\top}]\\leq\\frac{1}{N^{2}H^{2}}\\sum_{\\tilde{\\varepsilon}=1}^{N}\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(\\tilde{c},\\eta)}\\Sigma_{\\omega}^{\\tilde{c}}(\\Gamma_{1,h+1:H}^{(\\tilde{c},\\eta)})^{\\top}\\right]+\\frac{N-2}{N H^{2}}\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(c,\\eta)}\\Sigma_{\\omega}^{c}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Assuming $N\\geq2$ , triangle inequality gives ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}[(\\widetilde{\\xi}_{1}^{c})(\\widetilde{\\xi}_{1}^{c})^{\\top}]\\right\\|\\leq\\frac{1}{N H}\\|\\Sigma_{\\omega}\\|+\\frac{N-2}{N H}\\|\\Sigma_{\\omega}\\|=\\frac{N-1}{N H}\\|\\Sigma_{\\omega}\\|\\ .\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(Value of $b_{1}^{(\\theta,\\xi)}$ .) For the covariance of $\\widetilde{\\xi}_{1}^{c}$ and $\\widetilde{\\theta}_{1}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\widetilde{\\theta}_{1})(\\widetilde{\\xi}_{1}^{c})^{\\top}\\right]}\\\\ &{\\ =\\mathbb{E}\\left[\\left(\\frac{\\eta}{N}\\displaystyle\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\sum_{h=1}^{H}\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)}\\omega^{\\widetilde{c}}(Z_{1,h}^{\\widetilde{c}})\\right)\\right.}\\\\ &{\\qquad\\,\\times\\left.\\left(\\frac{1}{N H}\\displaystyle\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\sum_{h=1}^{H}(\\omega^{\\widetilde{c}}(Z_{1,h}^{\\widetilde{c}}))^{\\top}(\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)})^{\\top}-\\frac{1}{H}\\displaystyle\\sum_{h=1}^{H}(\\omega^{c}(Z_{1,h}^{c}))^{\\top}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right)\\right]}\\\\ &{\\ =\\mathbb{E}\\left[\\frac{\\eta}{N^{2}H}\\displaystyle\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\sum_{h=1}^{H}\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)}\\Sigma_{\\omega}^{\\widetilde{c}}(\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)})^{\\top}\\right]-\\mathbb{E}\\left[\\frac{\\eta}{N H}\\displaystyle\\sum_{h=1}^{H}\\Gamma_{1,h+1:H}^{(c,\\eta)}\\Sigma_{\\omega}^{c}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As a result, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}\\left[\\langle\\widetilde{\\theta}_{1}\\,,\\,\\widetilde{\\xi}_{1}^{c}\\rangle\\right]\\right\\|\\leq\\frac{2\\eta}{N}\\|{\\boldsymbol{\\Sigma}}_{\\omega}\\|\\ .\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(Value of $b^{\\neq}$ .) Similarly to above, for $c\\neq c^{\\prime}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(\\widetilde{\\xi}_{1}^{c})(\\widetilde{\\xi}_{1}^{c})^{\\top}]=\\mathbb{E}\\left[\\left(\\frac{1}{N H}\\displaystyle\\sum_{\\widetilde{c}=1}^{N}\\displaystyle\\sum_{h=1}^{H}\\Big\\{\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)}\\omega^{\\widetilde{c}}(Z_{1,h}^{\\widetilde{c}})\\Big\\}-\\frac{1}{H}\\displaystyle\\sum_{h=1}^{H}\\Gamma_{1,h+1:H}^{(c,\\eta)}\\omega^{c}(Z_{1,h}^{c})\\right)\\right.}\\\\ &{\\qquad\\qquad\\times\\left.\\left(\\frac{1}{N H}\\displaystyle\\sum_{\\widetilde{c}=1}^{N}\\displaystyle\\sum_{h=1}^{H}\\Big\\{(\\omega^{\\widetilde{c}}(Z_{1,h}^{\\widetilde{c}}))^{\\top}(\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)})^{\\top}\\Big\\}-\\frac{1}{H}\\displaystyle\\sum_{h=1}^{H}(\\omega^{c}(Z_{1,h}^{c}))^{\\top}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right)\\right]}\\\\ &{\\qquad=\\frac{1}{N^{2}H^{2}}\\displaystyle\\sum_{\\widetilde{c}=1}^{N}\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)}\\Sigma_{\\omega}^{\\widetilde{c}}(\\Gamma_{1,h+1:H}^{(\\widetilde{c},\\eta)})^{\\top}\\right]}\\\\ &{\\qquad-\\left.\\frac{1}{N H^{2}}\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(c,\\eta)}\\Sigma_{\\omega}^{c}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right]-\\frac{1}{N H^{2}}\\mathbb{E}\\left[\\Gamma_{1,h+1:H}^{(c,\\eta)}\\Sigma_{\\omega}^{c}(\\Gamma_{1,h+1:H}^{(c,\\eta)})^{\\top}\\right]\\:.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Which results in the bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}[(\\widetilde{\\xi}_{1}^{c})(\\widetilde{\\xi}_{1}^{c})^{\\top}]\\right\\|\\leq\\frac{3}{N H}\\|\\Sigma_{\\omega}\\|\\ .\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma C.6. Let $\\nu>0$ , and assume that $\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\leq\\nu$ and $\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\leq$ $\\nu$ . Then the following inequalities hold for any $t\\geq0$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{b_{t+1}^{(\\theta,\\theta)}\\leq\\displaystyle(1-\\eta a)^{2H}b_{t}^{(\\theta,\\theta)}+\\nu\\eta H b_{t}^{(\\theta,\\xi)}+2\\nu\\frac{\\eta H}{N}b_{t}^{=}+\\nu\\eta^{2}H^{2}b_{t}^{\\not=}+\\frac{3\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\;,}}\\\\ {{\\eta H b_{t+1}^{(\\theta,\\xi)}\\leq2\\nu b_{t}^{(\\theta,\\theta)}+3\\nu\\eta H b_{t}^{(\\theta,\\xi)}+\\frac{2\\nu}{N}\\eta^{2}H^{2}b_{t}^{=}+2\\nu\\eta^{2}H^{2}b_{t}^{\\not=}+\\frac{2\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\;,}}\\\\ {{\\eta^{2}H^{2}b_{t+1}^{=}\\leq2\\nu b_{t}^{(\\theta,\\theta)}+3\\nu\\eta H b_{t}^{(\\theta,\\xi)}+4\\nu\\eta^{2}H^{2}b_{t}^{=}+3\\nu\\eta^{2}H^{2}b_{t}^{\\not=}+\\eta^{2}H\\|\\Sigma_{\\omega}\\|\\;,}}\\\\ {{\\eta^{2}H^{2}b_{t+1}^{\\not=}\\leq2\\nu b_{t}^{(\\theta,\\theta)}+3\\nu\\eta H b_{t}^{(\\theta,\\xi)}+\\frac{3\\nu}{N}\\eta^{2}H^{2}b_{t}^{=}+4\\nu\\eta^{2}H^{2}b_{t}^{\\not=}+\\frac{3\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\;,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Pwreo hofa.v e(Value of $b_{t+1}^{(\\theta,\\theta)}$ .) Replacing $\\widetilde{\\theta}_{t+1}$ by its expression from (40), then expanding the expression, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widetilde{(\\theta_{t+1})}\\big(\\widetilde{\\theta}_{t+1}\\big)^{\\top}=\\Big(\\Gamma_{t+1}\\widetilde{\\theta}_{t}+\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}-\\eta\\widetilde{\\varepsilon}_{t+1}\\Big)\\Big(\\Gamma_{t+1}\\widetilde{\\theta}_{t}+\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}-\\eta\\widetilde{\\varepsilon}_{t+1}\\Big)^{\\top}}\\\\ {\\displaystyle=\\Gamma_{t+1}\\widetilde{\\theta}_{t}\\widetilde{\\theta}_{t}^{\\top}\\Gamma_{t+1}^{\\top}+\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\Gamma_{t+1}\\widetilde{\\theta}_{t}(\\widetilde{\\xi}_{t}^{\\widetilde{c}})^{\\top}(\\widetilde{C}_{t+1}^{\\widetilde{c}})^{\\top}+\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}(\\widetilde{\\theta}_{t})^{\\top}\\Gamma_{t+1}^{\\top}}\\\\ {\\displaystyle~~~~+\\frac{\\eta^{2}}{N^{2}}\\sum_{\\widetilde{c}=1}^{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}(\\widetilde{\\xi}_{t}^{\\widetilde{c}})^{\\top}(\\widetilde{C}_{t+1}^{\\widetilde{c}})^{\\top}-\\eta\\widetilde{\\varepsilon}_{t+1}\\widetilde{\\theta}_{t}^{\\top}\\Gamma_{t+1}^{\\top}-\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{\\varepsilon}_{t+1}(\\widetilde{\\xi}_{t}^{\\widetilde{c}})^{\\top}(\\widetilde{C}_{t+1}^{\\widetilde{c}})^{\\top}}\\\\ {\\displaystyle~~~-\\,\\eta\\Gamma_{t+1}\\widetilde{\\theta}_{t}(\\widetilde{\\varepsilon}_{t+1})^{\\top}-\\frac{\\eta}{N}\\sum_{\\widetilde{c}=1}^{N}(\\widetilde{C}_{t+1}^{\\widetilde{c}})(\\widetilde{\\xi \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "From the triangle inequality and Jensen\u2019s inequality, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[(\\widetilde{\\theta}_{t+1})(\\widetilde{\\theta}_{t+1})^{\\top}]\\|\\le\\mathbb{E}[\\|\\Gamma_{t+1}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\theta}_{t}^{\\top}]\\|+\\frac{2\\eta}{N}\\sum_{\\widetilde{\\tau}=1}^{N}\\mathbb{E}^{1/2}[\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{\\widetilde{c}}]\\|}\\\\ &{\\quad+\\,\\frac{\\eta^{2}}{N^{2}}\\sum_{\\widetilde{c}=1}^{N}\\mathbb{E}[\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}]\\|+\\frac{\\eta^{2}}{N^{2}}\\sum_{\\widetilde{c}=1}^{N}\\underset{\\widetilde{c}=\\frac{1}{c}}{\\underset{\\widetilde{c}\\neq\\widetilde{c}}{\\sum}}\\mathbb{E}^{1/2}[\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\|^{2}]\\mathbb{E}^{1/2}[\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}]\\|}\\\\ &{\\quad+\\,\\|\\mathbb{E}[\\eta\\Gamma_{t+1}^{(\\mathbb{N})}\\widetilde{\\theta}_{t}\\widetilde{\\varepsilon}_{t+1}^{\\top}+\\eta\\bar{\\varepsilon}_{t+1}\\widetilde{\\theta}_{t}^{\\top}\\Gamma_{t+1}^{(\\mathbb{N})}]\\|+\\frac{1}{N}\\sum_{\\widetilde{c}=1}^{N}\\|\\mathbb{E}[\\eta\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}\\bar{\\varepsilon}_{t+1}^{\\top}+\\eta\\bar{\\varepsilon}_{t+1}\\widetilde{\\xi}_{t}^{\\widetilde{c}}\\top\\widetilde{C}_{t+1}^{\\widetilde{c}}]\\|}\\\\ &{\\quad+\\,\\eta^{2}\\|\\mathbb{E}[\\widetilde{\\varepsilon}_{t+1}\\widetilde{\\varepsilon}_{t+1}^{\\top}]\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, we have from (41) that $\\mathbb{E}[\\widetilde{\\theta}_{t}]=\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}]=0$ . Thus, we have, for all $c\\in\\{1,\\ldots,N\\}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big\\|\\mathbb{E}[\\eta\\Gamma_{t+1}^{(\\mathrm{fl})}\\widetilde{\\theta}_{t}\\bar{\\varepsilon}_{t+1}^{\\top}+\\eta\\bar{\\varepsilon}_{t+1}\\widetilde{\\theta}_{t}^{\\top}\\Gamma_{t+1}^{(\\mathrm{fl})\\top}]\\|=\\big\\|\\mathbb{E}[\\eta\\Gamma_{t+1}^{(\\mathrm{fl})}\\mathbb{E}[\\widetilde{\\theta}_{t}]\\bar{\\varepsilon}_{t+1}^{\\top}+\\eta\\bar{\\varepsilon}_{t+1}\\mathbb{E}[\\widetilde{\\theta}_{t}^{\\top}]\\Gamma_{t+1}^{(\\mathrm{fl})\\top}]\\big\\|=0\\,,}\\\\ &{\\big\\|\\mathbb{E}[\\eta\\widetilde{C}_{t+1}^{\\tilde{c}}\\widetilde{\\xi}_{t}^{\\tilde{c}}\\bar{\\varepsilon}_{t+1}^{\\top}+\\eta\\bar{\\varepsilon}_{t+1}\\widetilde{\\xi}_{t}^{\\tilde{c}}\\bar{\\tau}_{t+1}^{\\tilde{c}}{\\top}]\\|=\\|\\mathbb{E}[\\eta\\widetilde{C}_{t+1}^{\\tilde{c}}\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\tilde{c}}]\\bar{\\varepsilon}_{t+1}^{\\top}+\\eta\\bar{\\varepsilon}_{t+1}\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\tilde{c}}{\\top}]\\widetilde{C}_{t+1}^{\\tilde{c}}{\\top}]\\|=0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Which results in the following inequality ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[(\\widetilde{\\theta}_{t+1})(\\widetilde{\\theta}_{t+1})^{\\top}]\\|\\le\\mathbb{E}[\\|\\Gamma_{t+1}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\theta}_{t}^{\\top}]\\|+\\displaystyle\\frac{2\\eta}{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\mathbb{E}^{1/2}[\\|\\widetilde{C}_{t+1}^{\\widetilde{\\varepsilon}}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{\\widetilde{\\varepsilon}^{\\top}}]\\|}\\\\ &{\\quad\\quad+\\displaystyle\\frac{\\eta^{2}}{N^{2}}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\mathbb{E}[\\|\\widetilde{C}_{t+1}^{\\widetilde{\\varepsilon}}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{\\varepsilon}}\\widetilde{\\xi}_{t}^{\\widetilde{\\varepsilon}^{\\top}}]\\|+\\displaystyle\\frac{\\eta^{2}}{N^{2}}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\mathbb{E}^{1/2}[\\|\\widetilde{C}_{t+1}^{\\widetilde{\\varepsilon}}\\|^{2}]\\mathbb{E}^{1/2}[\\|\\widetilde{C}_{t+1}^{\\widetilde{\\varepsilon}}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{\\varepsilon}}\\widetilde{\\xi}_{t}^{\\widetilde{\\varepsilon}^{\\top}}]\\|}\\\\ &{\\quad\\quad+\\displaystyle3\\eta^{2}\\|\\mathbb{E}[\\widetilde{\\varepsilon}_{t+1}\\widetilde{\\varepsilon}_{t+1}^{\\top}]\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[(\\widetilde{\\theta}_{t+1})(\\widetilde{\\theta}_{t+1})^{\\top}]\\|\\le(1-\\eta a)^{2H}\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\theta}_{t}^{\\top}]\\|}\\\\ &{\\qquad+\\displaystyle\\frac{\\eta}{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\eta H^{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{\\widetilde{\\mathbf{c}}}]\\|+\\frac{2\\eta^{2}}{N^{2}}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\eta^{2}H^{4}\\left\\{\\mathbf{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{\\mathbf{c}}}\\widetilde{\\xi}_{t}^{\\widetilde{\\mathbf{c}}}]\\|}\\\\ &{\\qquad+\\displaystyle\\frac{\\eta^{2}}{N^{2}}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\eta^{2}H^{4}\\left\\{\\mathbf{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{\\mathbf{c}}}\\widetilde{\\xi}_{t}^{\\widetilde{\\mathbf{c}}\\widetilde{\\mathbf{c}}}]\\|+3\\eta^{2}\\|\\mathbb{E}[\\bar{\\varepsilon}_{t+1}\\bar{\\varepsilon}_{t+1}^{\\top}]\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Assuming $\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\leq\\nu$ and $\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\leq\\nu$ , we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{E}[(\\widetilde{\\theta}_{t+1})(\\widetilde{\\theta}_{t+1})^{\\top}]|\\le(1-\\eta a)^{2H}\\,\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\theta}_{t}^{\\top}]\\|+\\nu\\frac{\\eta H}{N}\\displaystyle\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{c}]\\|+2\\nu\\frac{\\eta^{2}H^{2}}{N^{2}}\\displaystyle\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}]\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\nu\\frac{\\eta^{2}H^{2}}{N^{2}}\\displaystyle\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c\\top}]\\|+\\frac{3\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This gives our first inequality that links our upper bounds, ", "page_idx": 31}, {"type": "equation", "text": "$$\nb_{t+1}^{(\\theta,\\theta)}\\leq\\left(1-\\eta a\\right)^{2H}b_{t}^{(\\theta,\\theta)}+\\nu\\eta H b_{t}^{(\\theta,\\xi)}+2\\nu\\frac{\\eta H}{N}b_{t}^{=}+\\nu\\eta^{2}H^{2}b_{t}^{\\not=}+\\frac{3\\eta^{2}H}{N}\\|{\\Sigma_{\\omega}}\\|\\ .\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(Value of $b_{t+1}^{(\\theta,\\xi)}$ .) As for $b_{t+1}^{(\\theta,\\theta)}$ , we bound, for $c\\in\\{1,\\ldots,N\\}$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\theta}_{i+1}\\widetilde{\\xi}_{i+1}^{\\star}}\\\\ &{=\\Big(\\mathrm{r}_{i+1}\\widetilde{\\theta}_{i}+\\widetilde{\\Pi}\\displaystyle\\sum_{k=1}^{N}\\widetilde{C}_{i+1}^{\\star}\\widetilde{\\xi}_{i}-\\eta_{\\ell+1}\\Big)}\\\\ &{\\qquad\\times\\left(\\frac{1}{\\eta_{\\mathrm{f}}}\\widetilde{\\mathbb{I}}_{\\widetilde{\\xi}_{i+1}}^{\\star}\\widetilde{\\phi}_{i}+\\Big(1-\\frac{1}{H}C_{i+1}\\Big)\\widetilde{\\xi}_{i}+\\frac{1}{N H}\\displaystyle\\sum_{k=1}^{N}\\widetilde{C}_{i+1}^{\\star}\\widetilde{\\xi}_{i}-\\frac{1}{H}\\widetilde{\\xi}_{i+1}\\right)^{\\top}}\\\\ &{=\\frac{1}{\\eta_{\\mathrm{f}}H}\\mathrm{r}_{i+1}\\widetilde{\\theta}_{i}\\widetilde{\\eta}_{i}\\widetilde{\\Gamma}_{i+1}^{\\star}+\\frac{1}{N H}\\displaystyle\\sum_{k=1}^{N}\\widetilde{C}_{i+1}^{\\star}\\widetilde{\\xi}_{i}\\widetilde{\\xi}_{i}^{\\star}\\widetilde{\\xi}_{i+1}^{\\star}-\\frac{1}{H}\\widetilde{\\varepsilon}_{i+1}\\widetilde{\\eta}_{i}\\widetilde{\\Gamma}_{i+1}^{\\star}}\\\\ &{+\\mathrm{r}_{i+1}\\widetilde{\\theta}_{i}\\widetilde{\\xi}_{i}^{\\star}\\widetilde{\\tau}(1-\\frac{1}{H}C_{i+1})^{\\top}+\\frac{\\eta}{N}\\displaystyle\\sum_{k=1}^{N}\\widetilde{C}_{i+1}^{\\star}\\widetilde{\\xi}_{i}^{\\star}\\widetilde{\\xi}_{i}^{\\star}\\widetilde{\\xi}_{i+1}^{\\star}\\Big(1-\\frac{1}{H}C_{i+1}\\widetilde{\\xi}_{i}^{\\star}\\Big)^{\\top}}\\\\ &{+\\frac{1}{M}\\displaystyle\\sum_{k=1}^{N}\\sum_{k=1}^{N}\\widetilde{C}_{i+1}^{\\star}\\widetilde{\\xi}_{i}^{\\star}\\widetilde{\\xi}_{i+1}^{\\star}\\Big[\\frac{\\eta_{k+1}}{H}\\sum_{k=1}^{N}\\sum_{k=1}^{N}\\widetilde{C}_{i+1}^{\\star}\\widetilde{\\xi}_{i}^{\\star}\\widetilde{\\xi}_{i+1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now we proceed as above by taking the expectation, then the norm, and using the triangle inequality. Note that by (41), we have $\\begin{array}{r}{\\mathbb{E}[\\bar{\\varepsilon}_{t+1}\\widetilde{\\theta}_{t}^{\\top}\\widetilde{\\Gamma}_{t+1}^{c}\\top]\\,=\\,0,\\,\\mathbb{E}[\\bar{\\varepsilon}_{t+1}\\widetilde{\\xi}_{t}^{c}\\,\\top\\left(\\mathrm{I}-\\frac{\\mathrm{~\\bar{~}1~}}{H}C_{t+1}^{c}\\right)^{\\top}]\\,=\\,0}\\end{array}$ , $\\mathbb{E}[\\bar{\\varepsilon}_{t+1}\\sum_{\\tilde{c}=1}^{N}\\tilde{\\xi}_{t}^{\\tilde{c}}\\,^{\\top}\\widetilde{C}_{t+1}^{\\tilde{c}}\\,^{\\top}]\\,=\\,0.$ , $\\mathbb{E}[\\Gamma_{t+1}\\widetilde{\\theta}_{t}{\\widetilde{\\varepsilon}}_{t+1}^{c}\\,^{\\top}]\\,=\\,0$ , and $\\mathbb{E}[\\widetilde{C}_{t+1}^{\\tilde{c}}\\widetilde{\\xi}_{t}^{\\tilde{c}}\\widetilde{\\varepsilon}_{t+1}^{c}\\,^{\\top}]\\,=\\,0$ . After using ", "page_idx": 31}, {"type": "text", "text": "Jensen\u2019s inequality, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[\\widetilde{\\theta}_{t+1}\\widetilde{\\xi}_{t+1}^{c}]\\|\\|\\leq\\frac{1}{\\eta H}\\mathbb{E}^{1/2}[\\|\\widetilde{\\Gamma}_{t+1}^{c}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\theta}_{t}^{\\top}]\\|+\\frac{1}{N H}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\mathbb{E}^{1/2}[\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\|^{2}]\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{c}}\\widetilde{\\theta}_{t}^{\\top}]\\|}\\\\ &{\\quad+\\mathbb{E}^{1/2}\\left[\\left\\|\\mathrm{I}-\\frac{1}{H}C_{t+1}^{c}\\right\\|^{2}\\right]\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{c}]\\|+\\frac{\\eta}{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\mathbb{E}\\left[\\left\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\right\\|\\left\\|\\left(\\mathrm{I}-\\frac{1}{H}C_{t+1}^{c}\\right)\\right\\|\\right]\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{c}]\\|}\\\\ &{\\quad+\\frac{1}{N H}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\mathbb{E}^{1/2}\\left[\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\|^{2}\\right]\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{\\widetilde{c}}]\\|+\\frac{\\eta}{N^{2}H}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\mathbb{E}\\left[\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}\\|\\|\\widetilde{C}_{t+1}^{\\widetilde{c}}{\\top}\\|\\right]\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}]\\|}\\\\ &{\\quad+\\left\\|\\mathbb{E}\\left[\\frac{\\eta}{H}\\bar{\\varepsilon}_{t+1}\\widetilde{\\varepsilon}_{t+1}^{c}{\\top}\\right]\\right\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using Lemma C.1, Lemma C.2, and Lemma C.3, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}\\big[\\widetilde{\\theta}_{t+1}\\widetilde{\\xi}_{t+1}^{c}\\big]\\|\\leq2\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\mathbf{A}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}\\big[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{\\top}\\big]\\|+\\frac{1}{N H}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\eta H^{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}\\big[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\theta}_{t}^{\\top}\\big]\\|}\\\\ &{\\quad+\\frac{\\eta H}{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}\\big[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{c}\\tau^{\\top}\\big]\\|+\\frac{\\eta}{N}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\eta H^{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}\\big[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}\\tau^{\\top}\\big]\\|}\\\\ &{\\quad+\\frac{1}{N H}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\eta H^{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}\\big[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{c}\\tau^{\\top}\\big]\\|}\\\\ &{\\quad+\\frac{\\eta}{N^{2}H}\\sum_{\\widetilde{\\varepsilon}=1}^{N}\\frac{\\eta^{2}}{\\gamma^{4}}H^{4}\\left\\{\\mathbf{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\|\\mathbb{E}\\big[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}\\tau^{\\top}\\big]\\|+\\left\\|\\mathbb{E}\\left[\\frac{\\eta}{H}\\widetilde{\\varepsilon}_{t+1}\\widetilde{\\varepsilon}_{t+1}^{c}\\tau\\right]\\right\\|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we used the two following inequalities ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\widetilde{C}_{t+1}^{\\tilde{c}}\\right\\|\\left\\|\\left(\\mathrm{I}-\\frac{1}{H}C_{t+1}^{c}\\right)\\right\\|\\right]\\leq\\mathbb{E}^{1/2}\\left[\\left\\|\\widetilde{C}_{t+1}^{\\tilde{c}}\\right\\|^{2}\\right]\\leq\\eta H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\,,}\\\\ &{\\qquad\\quad\\mathbb{E}\\left[\\|\\widetilde{C}_{t+1}^{\\tilde{c}}\\|\\|\\widetilde{C}_{t+1}^{\\tilde{c}}\\,^{\\top}\\|\\right]\\leq\\mathbb{E}\\left[\\frac{1}{2}\\|\\widetilde{C}_{t+1}^{\\tilde{c}}\\|^{2}+\\frac{1}{2}\\|\\widetilde{C}_{t+1}^{\\tilde{c}}\\,^{\\top}\\|^{2}\\right]\\leq\\eta^{2}H^{4}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This leads to the following inequality ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta H b_{t+1}^{(\\theta,\\xi)}\\leq2\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}b_{t}^{(\\theta,\\theta)}+3\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}b_{t}^{(\\theta,\\xi)}}\\\\ &{\\qquad\\qquad+\\eta^{2}H^{2}\\left(\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}+\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|\\right\\}\\right)\\left\\{\\frac{1}{N}b_{t}^{-}+\\left(1-\\frac{1}{N}\\right)b_{t}^{\\neq}\\right\\}}\\\\ &{\\qquad\\qquad+\\frac{2\\eta^{2}H}{3}H_{\\mathrm{\\parallel\\setminus\\mathbb{V}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\left\\|\\mathbb{E}\\left[\\frac{\\eta}{H}\\bar{\\varepsilon}_{t+1}\\hat{\\varepsilon}_{t+1}^{c}\\right\\uparrow\\right]\\right\\|\\leq b_{1}^{(\\theta,\\xi)}=\\frac{2\\eta}{N}\\|\\Sigma_{\\omega}\\|.\\mathrm{~Assuming~}\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\leq\\nu}\\end{array}$ and $\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\leq\\nu$ , we obtain the following bound ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\eta H b_{t+1}^{(\\theta,\\xi)}\\leq2\\nu b_{t}^{(\\theta,\\theta)}+3\\nu\\eta H b_{t}^{(\\theta,\\xi)}+2\\nu\\eta^{2}H^{2}\\left\\{\\frac{1}{N}b_{t}^{=}+\\left(1-\\frac{1}{N}\\right)b_{t}^{\\mathcal{I}}\\right\\}+\\frac{2\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\ .\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "(Value of $b_{t+1}^{=}$ and $b_{t+1}^{\\neq}$ .) As above, we start by expanding the matrix product, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\xi}_{t+1}^{c}\\widetilde{\\xi}_{t+1}^{c}{^{\\tau}}=\\Big(\\displaystyle\\frac{1}{\\eta H}\\widetilde{\\Gamma}_{t+1}^{c}\\widetilde{\\theta}_{t}+\\Big(\\mathrm{I}-\\displaystyle\\frac{1}{H}C_{t+1}^{c}\\Big)\\widetilde{\\xi}_{t}^{c}+\\displaystyle\\frac{1}{N H}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{c}-\\displaystyle\\frac{1}{H}\\widetilde{\\varepsilon}_{t+1}^{c}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\Big(\\displaystyle\\frac{1}{\\eta H}\\widetilde{\\Gamma}_{t+1}^{c}\\widetilde{\\theta}_{t}+\\Big(\\mathrm{I}-\\displaystyle\\frac{1}{H}C_{t+1}^{c}\\Big)\\widetilde{\\xi}_{t}^{c}+\\displaystyle\\frac{1}{N H}\\sum_{\\widetilde{c}=1}^{N}\\widetilde{C}_{t+1}^{\\widetilde{c}}\\widetilde{\\xi}_{t}^{\\widetilde{c}}-\\displaystyle\\frac{1}{H}\\widetilde{\\varepsilon}_{t+1}^{c}\\Big)^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{1}{\\eta\\sqrt{H^{2}}}\\frac{\\mathcal{K}_{\\alpha}}{16}\\hat{\\rho}_{\\alpha}^{\\mathrm{\\mathcal{E}}}(\\hat{\\mathbf{u}},\\hat{\\mathbf{u}})\\hat{\\rho}_{\\alpha}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}-\\frac{1}{H^{2}}\\left(\\mathbf{1}-\\frac{1}{H^{2}}c_{\\alpha+1}\\right)\\hat{\\mathcal{E}}_{\\beta}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}_{+}^{\\top}\\mathbf{1})}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H^{2}}}\\sum_{i=1}^{K}\\frac{\\mathcal{K}_{\\alpha}}{\\left(\\rho\\hat{\\mathbf{u}}_{i}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}_{\\rho}^{\\mathcal{E}})\\right)^{2}}\\frac{\\hat{\\mathcal{K}}_{\\alpha}}{H^{2}}\\hat{\\mathbf{u}}_{i}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}-\\frac{1}{H^{2}}c_{\\alpha+1}^{\\mathrm{\\mathcal{E}}})\\hat{\\mathcal{E}}_{\\beta}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}-\\frac{1}{H^{2}}c_{\\alpha+1}^{\\mathrm{\\mathcal{E}}})}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H^{2}}}\\hat{\\mathbf{u}}_{i}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}_{\\rho}^{\\mathcal{E}})\\hat{\\mathcal{E}}(\\mathbf{1}-\\frac{1}{H^{2}}c_{\\alpha+1}^{\\mathrm{\\mathcal{E}}})^{\\top}+\\left(1-\\frac{1}{H^{2}}c_{\\alpha+1}^{\\mathrm{\\mathcal{E}}}\\right)\\hat{\\mathcal{E}}_{\\beta}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}-\\frac{1}{H^{2}}c_{\\alpha+1}^{\\mathrm{\\mathcal{E}}})^{\\top}}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H}}\\sum_{i=1}^{K}\\hat{\\mathcal{K}}_{\\alpha}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}_{\\rho}^{\\mathcal{E}})\\hat{\\mathbf{u}}^{\\mathrm{\\mathcal{E}}}(\\mathbf{1}-\\frac{1}{H^{2}}c_{\\alpha+1}^{\\mathrm{\\mathcal{E}}})^{\\top}-\\frac{1}{H^{2}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Taking the expectation, then the norm, and using triangle inequality and Jensen\u2019s inequality, we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}\\{\\bar{g}_{i}(\\bar{\\mathbf{k}}_{\\perp}^{*}|\\cdot\\nabla_{i}\\bar{\\mathbf{k}}_{\\perp}^{*}|)}\\\\ &{\\leq\\frac{1}{\\eta\\sqrt{H^{3}}}\\mathbb{E}\\|\\bar{\\mathbf{k}}_{\\perp}^{*}\\|^{2}\\|\\mathbb{E}\\{\\bar{g}_{i}^{\\mathcal{T}}\\}\\|+\\frac{1}{\\eta\\sqrt{H^{3}}}\\mathbb{E}\\|\\mathbb{E}\\{\\bar{g}_{i}^{\\mathcal{T}}\\}\\|}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H^{3}}}\\sum_{i=1}^{\\infty}\\mathbb{E}\\frac{1}{\\|\\mathbf{k}_{\\perp}^{*}\\|^{2}}\\left[\\big\\|\\bar{\\mathbf{X}}_{i}^{*}\\|^{2}\\right]\\|\\mathbb{E}\\|\\widehat{\\tilde{g}}_{i}^{\\mathcal{T}}\\|}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H^{3}}}\\sum_{i=1}^{\\infty}\\mathbb{E}\\bigg[\\|\\bar{\\mathbf{X}}_{i}^{*}\\|^{2}\\bigg\\|\\mathbb{E}\\|\\widehat{\\tilde{g}}_{i}^{\\mathcal{T}}\\|\\}\\mathbb{E}\\bigg]\\mathbb{E}\\bigg[\\|-\\frac{1}{H}\\frac{C_{i}}{H^{3}+1}\\bigg\\|\\mathbf{1}-\\frac{1}{H^{2}C_{i+1}}\\bigg\\|\\bigg\\|\\mathbb{E}\\tilde{\\xi}_{i}^{\\mathcal{T}}\\|\\bigg]}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H^{3}}}\\mathbb{E}\\bigg[\\bigg\\|\\bar{\\mathbf{X}}_{i}^{*}\\|_{\\infty}^{2}\\bigg\\|\\mathbb{E}\\bigg\\|\\Big\\|\\mathbb{E}\\tilde{\\xi}_{i}^{\\mathcal{T}}\\|\\bigg\\|\\mathbb{E}\\bigg\\|\\mathbb{E}\\bigg\\|\\bar{\\xi}_{i}^{\\mathcal{T}}\\bigg\\|}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H^{3}}}\\sum_{i=1}^{\\infty}\\mathbb{E}\\bigg[\\frac{1}{\\eta\\sqrt{H^{3}}}\\Big\\|\\Big\\|\\mathbb{E}\\tilde{\\xi}_{i}^{\\mathcal{T}}\\|\\bigg\\|\\mathbb{E}\\Big\\|\\bar{\\xi}_{i}^{\\mathcal{T}}\\|\\bigg\\|}\\\\ &{\\quad+\\frac{1}{\\eta\\sqrt{H^{3}}}\\sum_{i=1}^{\\infty}\\mathbb{E}\\bigg[\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can now use Lemma C.1, Lemma C.2, and Lemma C.3 to obtain the following upper bound ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[\\widetilde{\\xi}_{t+1}^{c}\\widetilde{\\xi}_{t+1}^{c}]\\|}\\\\ &{\\le2\\left\\{\\mathbf{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\theta}_{t}^{\\top}]\\|+\\frac{1}{\\eta H}\\frac{\\eta H}{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\theta}_{t}^{\\top}]\\|}\\\\ &{\\quad+\\frac{1}{\\eta N H^{2}}\\displaystyle\\sum_{\\widetilde{c}=1}^{N}\\eta H^{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\theta}_{t}^{\\top}]\\|}\\\\ &{\\quad+\\frac{1}{\\eta H}\\frac{\\eta H}{2}\\left\\{\\mathbf{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{c}{\\top}]\\|+\\frac{\\eta^{2}H^{2}}{4}\\left\\{\\mathbf{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}{\\top}]\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle+\\,\\frac{1}{N H}\\sum_{\\tilde{\\xi}=1}^{N}\\eta H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\left\\{\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}]|\\right\\}}\\\\ &{\\displaystyle+\\,\\frac{1}{\\eta N H^{2}}\\sum_{\\tilde{\\xi}=1}^{N}\\eta H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\left\\|\\mathbb{E}[\\widetilde{\\theta}_{t}\\widetilde{\\xi}_{t}^{c}\\tau]\\right\\|}\\\\ &{\\displaystyle+\\,\\frac{1}{N H}\\sum_{\\tilde{\\xi}=1}^{N}\\eta H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\left\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}\\tau]\\right\\|}\\\\ &{\\displaystyle+\\,\\frac{1}{N^{2}H^{2}}\\sum_{\\tilde{\\xi}=1}^{N}\\eta^{2}H^{4}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\tilde{\\mathbf{A}}}^{c}\\|\\right\\}\\left\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}\\tau]\\right\\|+\\left\\|\\frac{1}{H^{2}}\\mathbb{E}[\\widetilde{\\xi}_{t+1}^{c}\\widetilde{\\xi}_{t+1}^{c}\\tau]\\right\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This bound can be simplified as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta^{2}H^{2}\\lVert\\mathbb{E}[\\tilde{\\xi}_{t+1}^{c}\\tilde{\\xi}_{t+1}^{c}]\\rVert}\\\\ &{\\le2\\eta^{2}H^{2}\\left\\{C_{\\mathbf{A}}^{2}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert^{3}\\right\\}b_{t}^{(\\theta,\\theta)}+\\frac{\\eta^{2}H^{2}}{2}\\left\\{C_{\\mathbf{A}}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert^{1/2}\\right\\}b_{t}^{(\\theta,\\xi)}}\\\\ &{\\quad+\\eta^{2}H^{2}\\left\\{C_{\\mathbf{A}}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert^{1/2}\\right\\}b_{t}^{(\\theta,\\xi)}}\\\\ &{\\quad+\\frac{\\eta^{2}H^{2}}{2}\\left\\{C_{\\mathbf{A}}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert^{1/2}\\right\\}b_{t}^{(\\theta,\\xi)}+\\frac{\\eta^{4}H^{4}}{4}\\left\\{C_{\\mathbf{A}}^{2}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert\\right\\}\\lVert\\mathbb{E}[\\tilde{\\xi}_{\\xi}^{c}\\tilde{\\xi}_{t}^{c}]\\rVert}\\\\ &{\\quad+\\eta^{3}H^{3}\\left\\{C_{\\mathbf{A}}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert^{1/2}\\right\\}\\left\\{\\frac{1}{N}b_{t}^{(\\theta,\\star)}+\\left(1-\\frac{1}{N}\\right)b_{t}^{(\\star)}\\right\\}}\\\\ &{\\quad+\\eta^{2}H^{2}\\left\\{C_{\\mathbf{A}}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert^{1/2}\\right\\}b_{t}^{(\\theta,\\xi)}+\\eta^{3}H^{3}\\left\\{C_{\\mathbf{A}}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert^{1/2}\\right\\}\\left\\{\\frac{1}{N}b_{t}^{=}+\\left(1-\\frac{1}{N}\\right)b_{t}^{\\eta,\\xi}\\right\\}}\\\\ &{\\quad+\\eta^{4}H^{4}\\left\\{C_{\\mathbf{A}}^{2}+\\lVert\\mathbb{E}_{\\mathbf{A}}^{c}\\rVert\\right\\}\\left\\{\\frac{1}{N}b_{t}^{=}+\\left(1-\\frac{1}{N}\\right)b_{t}^\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which can be simplified as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta^{2}H^{2}\\lVert\\mathbb{E}\\[\\widetilde{\\xi}_{t+1}^{c}\\widetilde{\\xi}_{t+1}^{c}\\]\\rVert\\leq2\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\lVert\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\rVert\\right\\}b_{t}^{(\\theta,\\theta)}+3\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\lVert\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\rVert^{1/2}\\right\\}\\eta H b_{t}^{(\\theta,\\xi)}}\\\\ &{\\qquad\\quad+\\left(2\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\lVert\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\rVert^{1/2}\\right\\}+\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\lVert\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\rVert\\right\\}\\right)\\eta^{2}H^{2}\\left\\{\\frac{1}{N}b_{t}^{=}+\\left(1-\\frac{1}{N}\\right)b_{t}^{\\neq}\\right\\}}\\\\ &{\\qquad+\\,\\frac{\\eta^{4}H^{4}}{4}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\lVert\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\rVert\\right\\}\\lVert\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}]\\rVert+\\left\\lVert\\frac{1}{H^{2}}\\mathbb{E}[\\widetilde{\\varepsilon}_{t+1}^{c}\\widetilde{\\varepsilon}_{t+1}^{c}]\\right\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We now distinguish two cases, when $c=c^{\\prime}$ and when $c\\neq c^{\\prime}$ . First, let $c=c^{\\prime}$ , we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\jmath^{2}H^{2}b_{t+1}^{=}\\leq2\\eta^{2}H^{2}\\left\\{\\mathbf{C_{A}^{2}}+\\|\\Sigma_{\\widehat{\\mathbf{A}}}^{c}\\|\\right\\}b_{t}^{(\\theta,\\theta)}+3\\eta H\\left\\{\\mathbf{C_{A}}+\\|\\Sigma_{\\widehat{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\eta H b_{t}^{(\\theta,\\xi)}}\\\\ &{\\quad\\quad\\quad\\quad+\\left(2\\eta H\\left\\{\\mathbf{C_{A}}+\\|\\Sigma_{\\widehat{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}+\\eta^{2}H^{2}\\left\\{\\mathbf{C_{A}^{2}}+\\|\\Sigma_{\\widehat{\\mathbf{A}}}^{c}\\|\\right\\}\\right)\\eta^{2}H^{2}\\left\\{\\frac{1}{N}b_{t}^{=}+\\left(1-\\frac{1}{N}\\right)b_{t}^{\\neq}\\right\\}}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{\\eta^{2}H^{2}}{4}\\left\\{\\mathbf{C_{A}^{2}}+\\|\\Sigma_{\\widehat{\\mathbf{A}}}^{c}\\|\\right\\}\\eta^{2}H^{2}b_{t}^{=}+\\eta^{2}H\\|\\Sigma_{\\omega}\\|\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "since when $c\\,=\\,c^{\\prime}$ , we have $\\mathbf{\\widetilde{a}}\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}\\mathbf{\\widetilde{\\tau}}]\\|\\mathbf{\\widetilde{\\tau}}\\leq\\mathbf{\\tau}{b}_{t}^{=}$ and $\\begin{array}{r}{\\left\\|\\frac{1}{H}\\mathbb{E}[\\widetilde{\\varepsilon}_{t+1}^{c}\\widetilde{\\varepsilon}_{t+1}^{c}\\,^{\\top}]\\right\\|\\;\\leq\\;b_{1}^{=}\\;=\\;\\frac{N-1}{N H}\\|\\Sigma_{\\omega}\\|\\,.}\\end{array}$ Assuming $\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\leq\\nu$ and $\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\leq\\nu$ , we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\eta^{2}H^{2}b_{t+1}^{=}\\leq2\\nu b_{t}^{(\\theta,\\theta)}+3\\nu\\eta H b_{t}^{(\\theta,\\xi)}+4\\nu\\eta^{2}H^{2}b_{t}^{=}+3\\nu\\eta^{2}H^{2}b_{t}^{\\not=}+\\eta^{2}H\\|\\Sigma_{\\omega}\\|\\ .\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We proceed similarly for $c\\neq c^{\\prime}$ , which gives ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\eta^{2}H^{2}b_{t+1}^{\\neq}\\leq2\\nu b_{t}^{(\\theta,\\theta)}+3\\nu\\eta H b_{t}^{(\\theta,\\xi)}+\\frac{3\\nu}{N}\\eta^{2}H^{2}b_{t}^{=}+4\\nu\\eta^{2}H^{2}b_{t}^{\\neq}+\\frac{3\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|~,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "since, when $c\\neq c^{\\prime}$ , we have $\\|\\mathbb{E}[\\widetilde{\\xi}_{t}^{c}\\widetilde{\\xi}_{t}^{c}\\,^{\\top}]\\|\\leq b_{t}^{\\neq}$ and $\\begin{array}{r}{\\left\\|\\frac{1}{H}\\mathbb{E}[\\widetilde{\\varepsilon}_{t+1}^{c}\\widetilde{\\varepsilon}_{t+1}^{c}\\,\\top]\\right\\|\\le b_{1}^{\\neq}=\\frac{3}{N H}\\|\\Sigma_{\\omega}\\|}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Corollary C.7. Assume that $\\begin{array}{r}{\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\,\\le\\,\\frac{a}{240(\\mathrm{C}\\,+\\|\\Sigma\\|)}}\\end{array}$ and $\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\right\\}\\leq$ $\\frac{a}{240(\\mathrm{C}+\\|\\Sigma\\|)}$ , set $\\begin{array}{r}{\\omega=\\operatorname*{min}\\left(1,\\frac{a}{12(\\mathrm{C}+\\|\\Sigma-\\|)}\\right)}\\end{array}$ , then it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{t+1}^{(\\theta,\\theta)}+\\omega\\eta H b_{t+1}^{(\\theta,\\xi)}+\\displaystyle\\frac{\\omega\\eta^{2}H^{2}}{N}b_{t+1}^{=}+\\omega\\eta^{2}H^{2}b_{t+1}^{\\ne}}\\\\ &{\\le\\left(1-\\displaystyle\\frac{\\eta a H}{2}\\right)b_{t}^{(\\theta,\\theta)}+\\displaystyle\\frac{1}{2}\\omega\\eta H b_{t}^{(\\theta,\\xi)}+\\frac{1}{2}\\displaystyle\\frac{\\omega\\eta^{2}H^{2}}{N}b_{t}^{=}+\\frac{1}{2}\\eta^{2}H^{2}b_{t}^{\\ne}+\\frac{9\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Assuming $\\begin{array}{r}{\\eta a H\\leq\\frac{1}{2}}\\end{array}$ , we have $\\begin{array}{r}{1-\\frac{1}{2}\\le1-\\frac{\\eta a H}{2}}\\end{array}$ . This in turn ensures that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle b_{t+1}^{(\\theta,\\theta)}+\\omega\\eta H b_{t+1}^{(\\theta,\\xi)}+\\frac{\\omega\\eta^{2}H^{2}}{N}b_{t+1}^{=}+\\omega\\eta^{2}H^{2}b_{t+1}^{\\nearrow}}\\\\ {\\displaystyle\\quad\\le\\left(1-\\frac{\\eta a H}{2}\\right)\\left\\{b_{t}^{(\\theta,\\theta)}+\\omega\\eta H b_{t}^{(\\theta,\\xi)}+\\frac{\\omega\\eta^{2}H^{2}}{N}b_{t}^{=}+\\eta^{2}H^{2}b_{t}^{\\ne}\\right\\}+\\frac{9\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which gives, for any $t\\geq0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\nb_{t}^{(\\theta,\\theta)}\\leq\\frac{18\\eta}{N a}\\lVert\\boldsymbol{\\Sigma}_{\\omega}\\rVert\\ .\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. From Lemma C.6, we have, for any $\\mathrm{~\\ensuremath~{~0~}~}<\\mathrm{~\\ensuremath~{~\\omega~}~}<\\mathrm{~\\ensuremath~{~1~}~}$ , $\\nu\\ \\ >\\ \\ 0$ , and assuming that $\\eta H\\left\\{\\mathrm{C}_{\\mathbf{A}}+\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|^{1/2}\\right\\}\\leq\\nu$ and $\\eta^{2}H^{2}\\left\\{\\mathrm{C}_{\\mathbf{A}}^{2}+\\|\\dot{\\mathrm{D}}_{\\mathbf{\\widetilde{A}}}^{c}\\|\\right\\}\\leq\\nu$ , and since $\\omega\\leq1$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle b_{t+1}^{(\\theta,\\theta)}+\\omega\\eta H b_{t+1}^{(\\theta,\\xi)}+\\frac{\\omega\\eta^{2}H^{2}}{N}b_{t+1}^{=}+\\omega\\eta^{2}H^{2}b_{t+1}^{\\neq}}\\\\ {\\displaystyle\\leq\\left\\lbrace\\left(1-\\eta a\\right)^{2H}+6\\omega\\eta H\\left\\lbrace\\mathrm{C}_{\\mathbf{A}}+\\lVert\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\rVert^{1/2}\\right\\rbrace\\right\\rbrace b_{t}^{(\\theta,\\theta)}}\\\\ {\\displaystyle\\quad+\\,10\\nu\\eta H b_{t}^{(\\theta,\\xi)}+10\\nu\\frac{\\eta^{2}H^{2}}{N}b_{t}^{=}+10\\nu\\eta^{2}H^{2}b_{t}^{\\neq}+\\frac{9\\eta^{2}H}{N}\\lVert\\Sigma_{\\omega}\\rVert\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, we choose $\\begin{array}{r}{\\omega=\\operatorname*{min}\\left(1,\\frac{a}{12(\\mathrm{C}+\\|\\Sigma-\\|)}\\right)}\\end{array}$ and obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(1-\\eta a\\right)^{2H}+6\\omega\\eta H\\left\\{\\mathbf{C_{A}}+\\lVert\\boldsymbol{\\Sigma}_{\\tilde{\\mathbf{A}}}^{c}\\rVert^{1/2}\\right\\}\\leq1-\\eta a H+6\\omega\\eta H\\left\\{\\mathbf{C_{A}}+\\lVert\\boldsymbol{\\Sigma}_{\\tilde{\\mathbf{A}}}^{c}\\rVert^{1/2}\\right\\}\\leq1-\\frac{\\eta a H}{2}\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Additionally, $\\omega\\leq1$ , thus $3+6\\omega\\leq9$ and we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle b_{t+1}^{(\\theta,\\theta)}+\\omega\\eta H b_{t+1}^{(\\theta,\\xi)}+\\frac{\\omega\\eta^{2}H^{2}}{N}b_{t+1}^{=}+\\omega\\eta^{2}H^{2}b_{t+1}^{\\ne}}\\\\ {\\displaystyle\\le\\left(1-\\frac{\\eta a H}{2}\\right)b_{t}^{(\\theta,\\theta)}+10\\nu\\eta H b_{t}^{(\\theta,\\xi)}+10\\nu\\frac{\\eta^{2}H^{2}}{N}b_{t}^{=}+10\\nu\\eta^{2}H^{2}b_{t}^{\\ne}+\\frac{9\\eta^{2}H}{N}\\|\\Sigma_{\\omega}\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Choosing $\\begin{array}{r}{\\nu\\leq\\frac{\\omega}{20}\\leq\\frac{a}{240(\\mathrm{C}+\\|\\Sigma\\cdot\\|)}}\\end{array}$ gives the result. ", "page_idx": 35}, {"type": "text", "text": "Complete analysis of SCAFFLSA. We can now state our main theorem, which gives an upper bound on the expected distance between the iterates of SCAFFLSA and the solution $\\theta_{\\star}$ . ", "page_idx": 35}, {"type": "text", "text": "Theorem C.8. Assume A1 and A3. Let $\\eta,H$ such that \u03b7 $a H\\leq1$ , and $H\\leq\\frac{a}{240\\eta\\{{\\mathrm{C}}+\\|{\\mathrm{\\Sigma}}\\|\\}}$ , and set $\\xi_{0}^{c}=0$ for all $c\\in[N]$ . Then, the sequence $(\\psi_{t})_{t\\in\\mathbb{N}}$ satisfies, for all $t\\geq0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Vert\\theta_{t}-\\theta_{\\star}\\Vert^{2}]\\leq\\left(1-\\frac{\\eta a H}{2}\\right)^{t}\\left\\{2\\Vert\\theta_{0}-\\theta_{\\star}\\Vert^{2}+2\\eta^{2}H^{2}\\mathbb{E}_{c}[\\Vert\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\Vert^{2}]\\right\\}+\\frac{36d\\eta}{N a}\\Vert\\Sigma_{\\omega}\\Vert\\ .\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Recall our decomposition $\\theta_{t}-\\theta_{\\star}=\\check{\\theta}_{t}-\\theta_{\\star}+\\widetilde{\\theta}$ . By Young\u2019s inequality, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\theta_{t}-\\theta_{\\star}\\|^{2}]\\le2\\mathbb{E}[\\|\\breve{\\theta}_{t}-\\theta_{\\star}\\|^{2}]+2\\mathbb{E}[\\|\\widetilde{\\theta}_{t}\\|^{2}]\\;.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Theorem C.4, we have $\\mathbb{E}[\\Vert\\check{\\theta}_{t}\\,-\\,\\theta_{\\star}\\Vert^{2}]\\ \\leq\\ \\left(1-\\textstyle\\frac{\\eta a H}{2}\\right)^{t}\\psi_{0}$ , and by Corollary C.7, we have $\\begin{array}{r}{\\mathbb{E}[\\|\\widetilde{\\theta}_{t}\\|^{2}]\\leq d b_{t}^{(\\theta,\\theta)}\\leq\\frac{18\\eta d}{N a}\\|\\Sigma_{\\omega}\\|}\\end{array}$ . Combine the two results, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\theta_{t}-\\theta_{\\star}\\|^{2}]\\le\\left(1-\\frac{\\eta a H}{2}\\right)^{t}2\\psi_{0}+\\frac{36d\\eta}{N a}\\|\\Sigma_{\\omega}\\|\\ ,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "replacing $\\begin{array}{r}{\\psi_{0}=\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\frac{\\eta H}{N}\\sum_{c=1}^{N}\\|\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\|^{2}}\\end{array}$ gives the result of the theorem. ", "page_idx": 36}, {"type": "text", "text": "Corollary C.9. Under the Assumptions of Theorem C.8, one may set the parameter of SCAFFLSA to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}\\left(\\eta_{\\infty},\\frac{N a\\epsilon^{2}}{72d\\|\\Sigma_{\\varepsilon}\\|}\\right)\\ ,\\quad H=\\frac{1}{240\\left\\{{\\bf C}_{\\bf A}^{2}+\\|\\Sigma_{\\tilde{\\bf A}}\\|}\\right\\}\\operatorname*{max}\\left(\\frac{a}{\\eta_{\\infty}},\\frac{72d\\|\\Sigma_{\\omega}\\|}{N\\epsilon^{2}}\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which guarantees $\\mathbb{E}[\\|\\theta_{t}-\\theta_{\\star}\\|^{2}]\\leq\\epsilon^{2}$ after a number of communication rounds ", "page_idx": 36}, {"type": "equation", "text": "$$\nT\\geq\\frac{240\\left\\{\\mathrm{C_{A}^{2}}+\\|\\mathrm{D_{\\widetilde{A}}}\\|\\right\\}}{a^{2}}\\log\\left(\\frac{4\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\frac{4\\eta H}{N}\\sum_{c=1}^{N}\\|\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\|^{2}}{\\epsilon^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The overall sample complexity of the algorithm is then ", "page_idx": 36}, {"type": "equation", "text": "$$\nT H=\\operatorname*{max}\\left(\\frac{240}{\\eta_{\\infty}a},\\frac{72d\\|\\Sigma_{\\varepsilon}\\|}{N a^{2}\\epsilon^{2}}\\right)\\log\\left(\\frac{4\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\frac{4\\eta H}{N}\\sum_{c=1}^{N}\\|\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\|^{2}}{\\epsilon^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Let $\\epsilon>0$ . Starting from Theorem C.8\u2019s upper bound, we have $\\mathbb{E}[\\|\\theta_{t}-\\theta_{\\star}\\|^{2}]\\leq\\epsilon^{2}$ whenever ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left(1-\\frac{\\eta a H}{2}\\right)^{t}2\\psi_{0}+\\frac{36d\\eta}{N a}\\|\\Sigma_{\\omega}\\|\\leq\\epsilon^{2}\\;,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\begin{array}{r}{\\psi_{0}=\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\frac{\\eta H}{N}\\sum_{c=1}^{N}\\|\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\|^{2}}\\end{array}$ . This gives a first condition $\\frac{36d\\eta}{N a}\\|\\boldsymbol{\\Sigma}_{\\omega}\\|\\leq\\epsilon^{2}$ which requires ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{N a\\epsilon^{2}}{72d\\|\\Sigma_{\\varepsilon}\\|}\\ .\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This allows to take any value of $H$ such that $\\begin{array}{r}{H\\,\\leq\\,\\frac{a}{240\\eta\\left\\{{\\mathrm{C}}\\,+\\|\\Sigma\\|\\right\\}}\\,=\\,\\frac{72}{240N\\epsilon\\left\\{{\\mathrm{C}}\\,+\\|\\Sigma\\|\\right\\}}}\\end{array}$ . With such setting, it remains to set the number of communication $T$ to ", "page_idx": 36}, {"type": "equation", "text": "$$\nT\\geq\\frac{1}{\\eta a H}\\log\\big(\\frac{2\\psi_{0}}{\\epsilon^{2}}\\big)=\\frac{240\\left\\{{\\mathrm{C}}_{\\bf A}^{2}+\\|{\\mathrm{D}}_{\\varepsilon}^{c}\\|\\right\\}}{a^{2}}\\log\\big(\\frac{4\\psi_{0}}{\\epsilon^{2}}\\big)\\ ,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which ensures that $\\left(1-\\textstyle{\\frac{\\eta a H}{2}}\\right)^{t}2\\psi_{0}\\leq\\textstyle{\\frac{\\epsilon}{2}}$ . ", "page_idx": 36}, {"type": "text", "text": "D Technical proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma D.1. For any matrix-valued sequences $(U_{n})_{n\\in\\mathbb{N}},\\,(V_{n})_{n\\in\\mathbb{N}}$ and for any $M\\in\\mathbb{N},$ , it holds that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\prod_{k=1}^{M}U_{k}-\\prod_{k=1}^{M}V_{k}=\\sum_{k=1}^{M}\\{\\prod_{j=1}^{k-1}U_{j}\\}(U_{k}-V_{k})\\{\\prod_{j=k+1}^{M}V_{j}\\}\\;.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma D.2 (Stability of the deterministic product). Assume A3. Then, for any $u\\in\\mathbb{R}^{d}$ and $h\\in\\mathbb{N},$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|(\\mathrm{I}-\\eta\\bar{\\mathbf{A}}^{c})^{h}u\\|\\leq(1-\\eta a)^{h}\\|u\\|\\ .\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Since $(Z_{t,h}^{c})_{1\\leq h\\leq H}$ are i.i.d, we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\Gamma_{t,1:h}^{(c,\\eta)}u\\big]=\\mathbb{E}\\big[\\prod_{l=1}^{h}(\\mathbf{I}-\\eta\\mathbf{A}(Z_{t,l}^{c}))u\\big]=\\prod_{l=1}^{h}\\mathbb{E}\\big[\\mathbf{I}-\\eta\\mathbf{A}(Z_{t,l}^{c})\\big]u=(\\mathbf{I}-\\eta\\bar{\\mathbf{A}}^{c})^{h}u\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The proof then follows from the elementary inequality: for any square-integrable random vector $U$ , $\\|\\mathbb{E}[\\bar{U}]\\|\\leq(\\mathbb{E}[\\|U\\|^{2}])^{1/2}$ . \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Lemma D.3. Let $(x_{i})_{i=1}^{N}$ , and $(y_{i})_{i=1}^{N}$ be $N$ vectors of $\\mathbb{R}^{d}$ . Denote $\\begin{array}{r}{\\bar{x}_{N}\\,=\\,(1/N)\\sum_{i=1}^{N}x_{i}}\\end{array}$ and $\\begin{array}{r}{\\bar{y}_{N}=(1/N)\\sum_{i=1}^{N}y_{i}}\\end{array}$ . Then, ", "page_idx": 37}, {"type": "equation", "text": "$$\nN\\|{\\bar{x}}_{N}-{\\bar{y}}_{N}\\|^{2}=\\sum_{i=1}^{N}\\|x_{i}-y_{i}\\|^{2}-\\sum_{i=1}^{N}\\|x_{i}-{\\bar{x}}_{N}-(y_{i}-{\\bar{y}}_{N})\\|^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Define $\\mathbf{x}\\,=\\,[x_{1}^{\\top},\\ldots,x_{N}^{\\top}]^{\\top}$ and $\\mathrm{y}\\,=\\,[y_{1}^{\\top},\\dots,y_{N}^{\\top}]^{\\top}\\,\\in\\,\\mathbb{R}^{N d}$ . Define by $\\mathrm{P}$ the orthogonal projector on ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{\\mathbf{x}\\in\\mathbb{R}^{N d}:\\mathbf{x}=[x^{\\top},\\ldots,x^{\\top}]^{\\top},x\\in\\mathbb{R}^{d}\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We show that $\\mathrm{P}\\,\\mathbf{x}=[\\bar{x}_{N}^{\\top},\\dots,\\bar{x}_{N}^{\\top}]^{\\top}$ . Note indeed that for any $\\mathbf{z}=[z^{\\top},\\cdot\\cdot\\cdot,z^{\\top}]^{\\top}\\in\\mathcal{E}$ , we get (with a slight abuse of notations, $\\langle\\cdot\\,,\\,\\cdot\\rangle$ denotes the scalar product in $\\mathbb{R}^{N d}$ and $\\mathbb{R}^{d}$ ) ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\langle\\mathrm{x-P\\,x}\\,,\\,\\mathrm{z}\\right\\rangle=\\sum_{i=1}^{N}\\left\\{\\left\\langle x_{i}\\,,\\,z\\right\\rangle-\\left\\langle\\bar{x}_{N}\\,,\\,z\\right\\rangle\\right\\}=0\\;.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The proof follows from Pythagoras identity which shows that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\operatorname{P}\\mathrm{x}-\\operatorname{P}\\mathrm{y}\\|^{2}=\\|\\mathrm{x}-\\mathrm{y}\\|^{2}-\\|(\\mathrm{x}-\\operatorname{P}\\mathrm{x})-(\\mathrm{y}-\\operatorname{P}\\mathrm{y}\\,\\|^{2})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma D.4. Assume A4. Let $Z$ be a random variable taking values in a state space $(Z,{\\mathcal{Z}})$ with distribution $\\pi_{c}$ . Set $\\eta\\geq0$ , then for any vector $u\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z))u\\|^{2}]\\leq(1-\\eta a)\\|u\\|^{2}-\\eta(\\frac{1}{L}-\\eta)\\mathbb{E}[\\|\\mathbf{A}^{c}(Z)u\\|^{2}]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. First, remark that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z))u\\|^{2}=u^{\\top}(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z))^{\\top}(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z))u}\\\\ &{\\qquad\\qquad\\qquad\\qquad=u^{\\top}\\big(\\mathrm{I}-2\\eta(\\frac{1}{2}(\\mathbf{A}^{c}(Z)+\\mathbf{A}^{c}(Z)^{\\top}))+\\eta^{2}\\mathbf{A}^{c}(Z)^{\\top}\\mathbf{A}^{c}(Z)\\big)u\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since we have $\\mathbb{E}[\\frac{1}{2}(\\mathbf{A}^{c}(Z)+\\mathbf{A}^{c}(Z)^{\\top})]\\succcurlyeq a\\mathrm{I}$ and $\\begin{array}{r}{\\mathbb{E}[\\frac{1}{2}(\\mathbf{A}^{c}(Z)\\!+\\!\\mathbf{A}^{c}(Z)^{\\top})]\\succcurlyeq\\frac{1}{L}\\mathbb{E}[\\mathbf{A}^{c}(Z)^{\\top}\\mathbf{A}^{c}(Z)],}\\end{array}$ , we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|(\\mathbf{I}-\\eta\\mathbf{A}^{c}(Z))u\\|^{2}]=u^{\\top}u-2\\eta u^{\\top}\\mathbb{E}[\\frac{1}{2}(\\mathbf{A}^{c}(Z)+\\mathbf{A}^{c}(Z)^{\\top})]u+\\eta^{2}u^{\\top}\\mathbb{E}[\\mathbf{A}^{c}(Z)^{\\top}\\mathbf{A}^{c}(Z)]u}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|u\\|^{2}-\\eta a\\|u\\|^{2}-\\frac{\\eta}{L}u^{\\top}\\mathbb{E}[\\mathbf{A}^{c}(Z)^{\\top}\\mathbf{A}^{c}(Z)]u+\\eta^{2}u^{\\top}\\mathbb{E}[\\mathbf{A}^{c}(Z)^{\\top}\\mathbf{A}^{c}(Z)]u}\\\\ &{\\qquad\\qquad\\qquad=(1-\\eta a)\\|u\\|^{2}-\\eta(\\frac{1}{L}-\\eta)u^{\\top}\\mathbb{E}[\\mathbf{A}^{c}(Z)^{\\top}\\mathbf{A}^{c}(Z)]u\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which gives the result. ", "page_idx": 37}, {"type": "text", "text": "E TD learning as a federated LSA problem ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section we specify TD(0) as a particular instance of the LSA algorithm. In the setting of linear functional approximation the problem of estimating $V^{\\pi}(s)$ reduces to the problem of estimating $\\theta_{\\star}\\in\\ensuremath{\\mathbb{R}}^{d}$ , which can be done via the LSA procedure. For the agent $c\\in[N]$ the $k$ -th step randomness is given by the tuple $Z_{k}^{c}=(S_{k}^{c},A_{k}^{c},S_{k+1}^{c})$ . With slight abuse of notation, we write $\\bar{\\mathbf{A}_{t,h}^{c}}$ instead of ${\\bf A}(Z_{t,h}^{c})$ , and $\\mathbf{b}_{t,h}^{c}$ instead of $\\mathbf{b}(Z_{t,h}^{c})$ . Then the corresponding LSA update equation with constant step size $\\eta$ can be written as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbf{\\theta}_{t,h}^{c}=\\theta_{t,h-1}^{c}-\\eta(\\mathbf{A}_{t,h}^{c}\\theta_{t,h-1}^{c}-\\mathbf{b}_{t,h}^{c})\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ${\\bf A}_{t,h}^{c}$ and $\\mathbf{b}_{t,h}^{c}$ are given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}_{t,h}^{c}=\\phi(S_{t,h}^{c})\\{\\phi(S_{t,h}^{c})-\\gamma\\phi(S_{t,h+1}^{c})\\}^{\\top}\\;,}\\\\ &{\\mathbf{b}_{t,h}^{c}=\\phi(S_{t,h}^{c})r^{c}(S_{t,h}^{c},A_{t,h}^{c})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Respective specialisation of FedLSA and SCAFFLSA algorithms to TD learning are stated in Algorithm 4 and Algorithm 5. ", "page_idx": 37}, {"type": "text", "text": "Algorithm 4 Federated TD(0): FedLSA applied to TD(0) with linear functional approximation ", "page_idx": 38}, {"type": "text", "text": "Input: $\\eta>0,\\theta_{0}\\in\\mathbb{R}^{d},T,N,H>0$   \nfor $t=0$ to $T-1$ do Initialize $\\theta_{t,0}=\\theta_{t}$ for $c=1$ to $N$ do for $h=1$ to $H$ do Receive tuple $(S_{t,h}^{c},A_{t,h}^{c},S_{t,h+1}^{c})$ following TD1 and perform local update: $\\mathbf{\\theta}_{t,h}^{c}=\\theta_{t,h-1}^{c}-\\eta(\\mathbf{A}_{t,h}^{c}\\theta_{t,h-1}^{c}-\\mathbf{b}_{t,h}^{c})\\mathrm{~,~}$ where ${\\bf A}_{t,h}^{c}$ and $\\mathbf{b}_{t,h}^{c}$ are given in (42) Average: $\\begin{array}{r}{\\theta_{t+1}=\\frac{1}{N}\\sum_{c=1}^{N}\\theta_{t,H}^{c}}\\end{array}$ (43) ", "page_idx": 38}, {"type": "text", "text": "Algorithm 5 SCAFFTD(0): SCAFFLSA applied to TD(0) with linear functional approximation ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Input: $\\eta>0,\\theta_{0}\\in\\mathbb{R}^{d},T,N,H>0$   \nfor $t=0$ to $T-1$ do Initialize $\\theta_{t,0}=\\theta_{t}$ for $c=1$ to $N$ do for $h=1$ to $H$ do Receive tuple $(S_{t,h}^{c},A_{t,h}^{c},S_{t,h+1}^{c})$ following TD1 and perform local update: $\\mathbf{\\boldsymbol{\\theta}}_{t,h}^{c}=\\boldsymbol{\\theta}_{t,h-1}^{c}-\\eta(\\mathbf{A}_{t,h}^{c}\\boldsymbol{\\theta}_{t,h-1}^{c}-\\mathbf{b}_{t,h}^{c}-\\boldsymbol{\\xi}^{c})\\mathrm{~,~}$ where ${\\bf A}_{t,h}^{c}$ and $\\mathbf{b}_{t,h}^{c}$ are given in (42) Average: $\\begin{array}{r}{\\theta_{t+1}=\\frac{1}{N}\\sum_{c=1}^{N}\\theta_{t,H}^{c}}\\end{array}$ Update local control variates: $\\begin{array}{r}{\\xi_{t+1}^{c}=\\xi_{t}^{c}+\\frac{1}{\\eta H}(\\theta_{t+1}-\\hat{\\theta}_{t,H}^{c}).}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "The corresponding local agent\u2019s system writes as $\\bar{\\mathbf{A}}^{c}\\theta_{\\star}^{c}=\\bar{\\mathbf{b}}^{c}$ , where we have, respectively, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathbf{A}}^{c}=\\mathbb{E}_{s\\sim\\mu,s\\sim P(\\cdot|s)}[\\phi(s)\\{\\phi(s)-\\gamma\\phi(s^{\\prime})\\}^{\\top}]}\\\\ &{\\bar{\\mathbf{b}}^{c}=\\mathbb{E}_{s\\sim\\mu,a\\sim\\pi(\\cdot|s)}[\\phi(s)r^{c}(s,a)]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The authors of [50] study the corresponding virtual MDP dynamics with $\\begin{array}{r}{\\tilde{\\mathbb{P}}=N^{-1}\\sum_{c=1}^{N}\\mathbb{P}_{\\mathrm{MDP}}^{c},}\\end{array}$ $\\begin{array}{r}{\\tilde{r}=N^{-1}\\sum_{c=1}^{N}r^{c}}\\end{array}$ . Next, introducing the invariant distribution of the kernel $\\tilde{\\mu}$ of the averaged state kernel ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{P}}_{\\pi}(B|s)=N^{-1}\\sum_{c=1}^{N}\\int_{A}\\mathbb{P}_{\\mathrm{MDP}}^{c}(B|s,a)\\pi(d a|s)\\,,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "we have $\\tilde{\\theta}$ as an optimal parameter corresponding to the system $\\tilde{A}\\tilde{\\theta}=\\tilde{b}$ . Here ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\tilde{A}=\\mathbb{E}_{s\\sim\\tilde{\\mu},s\\sim\\tilde{\\mathbb{P}}(\\cdot|s)}[\\phi(s)\\{\\phi(s)-\\gamma\\phi(s^{\\prime})\\}^{\\top}]}\\\\ {\\tilde{b}=\\mathbb{E}_{s\\sim\\tilde{\\mu},a\\sim\\pi(\\cdot|s)}[\\phi(s)\\tilde{r}(s,a)]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "E.1 Proof of Claim 3.1. ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We prove the following inequalities ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{C}_{\\mathbf{A}}=1+\\gamma\\;,}\\\\ &{\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|\\leq2(1+\\gamma)^{2}\\;,}\\\\ &{\\mathrm{Tr}(\\Sigma_{\\varepsilon}^{c})\\leq2(1+\\gamma)^{2}\\left(\\|\\theta_{\\star}^{c}\\|^{2}+1\\right)\\;,}\\\\ &{\\quad\\quad a=\\frac{(1-\\gamma)\\nu}{2}\\;,}\\\\ &{\\quad\\quad\\eta_{\\infty}=\\frac{(1-\\gamma)}{4}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Table 2: Communication and sample complexity for finding a solution with MSE lower than $\\epsilon^{2}$ for FedLSA, Scaffnew, and SCAFFLSAon the federated TD learning problem. Our analysis is the first to show that FedLSA exhibits linear speed-up, as well as its variant that reduces bias using control variates. ", "page_idx": 39}, {"type": "table", "img_path": "HeJ1cBAgiV/tmp/6aad11b194b3f908ee83ca2f96da59623a63479c63dcbbece3219ac00aabb2de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "The proof below closely follows [42] (Lemma 7) and [45] (Lemma 1). Everywhere in this subsection we use a generic notation ${\\bf A}_{1}^{c}$ as an alias for the random matrix ${\\bf A}_{1,1}^{c}$ . Now, using TD3 and (5), we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}_{1}^{c}\\|\\leq(1+\\gamma)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "almost surely, which implies $\\|\\bar{\\mathbf{A}}^{c}\\|\\leq1+\\gamma$ for any $c\\in[N]$ , giving (44). This implies, using the definition of $\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}$ , that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|\\Sigma_{\\widetilde{\\mathbf{A}}}^{c}\\|=\\|\\mathbb{E}[\\{\\mathbf{A}_{1}^{c}\\}^{\\top}\\mathbf{A}_{1}^{c}]-\\{\\bar{\\mathbf{A}}^{c}\\}^{\\top}\\bar{\\mathbf{A}}^{c}\\|\\le2(1+\\gamma)^{2}\\;,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and the bound (45) follows. Next we observe that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}(\\Sigma_{\\varepsilon}^{c})=\\mathbb{E}[\\|(\\mathbf{A}_{1}^{c}-\\bar{\\mathbf{A}}^{c})\\theta_{\\star}^{c}-(\\mathbf{b}_{1}^{c}-\\bar{\\mathbf{b}}^{c})\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\leq2\\{\\theta_{\\star}^{c}\\}^{\\top}\\mathbb{E}[\\{\\mathbf{A}_{1}^{c}\\}^{\\top}\\mathbf{A}_{1}^{c}]\\theta_{\\star}^{c}+2\\mathbb{E}[(r^{s}(S_{0}^{s},A_{0}^{c}))^{2}\\,\\mathrm{Tr}(\\varphi(S_{0}^{c})\\varphi^{\\top}(S_{0}^{c}))]}\\\\ &{\\quad\\quad\\quad\\leq2(1+\\gamma)^{2}\\{\\theta_{\\star}^{c}\\}^{\\top}\\Sigma_{\\varphi}[c]\\theta_{\\star}^{c}+2}\\\\ &{\\quad\\quad\\quad\\leq2(1+\\gamma)^{2}\\left(\\|\\theta_{\\star}^{c}\\|^{2}+1\\right)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the latter inequality follows from TD 3, and thus (46) holds. In order to check the last equation (47), we note first that the bound for $a$ and $\\eta_{\\infty}$ readily follows from the ones presented in [42][Lemma 5] and [42][Lemma 7]. To check assumption A4, note first that, with $s\\stackrel{\\bar{\\,}}{\\sim}\\mu^{c},s^{\\prime}\\sim$ $P^{\\pi}(\\cdot|s)$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}^{c}+\\{\\mathbf{A}^{c}\\}^{\\top}=\\varphi(s)\\{\\varphi(s)-\\gamma\\varphi(s^{\\prime})\\}^{\\top}+\\{\\varphi(s)-\\gamma\\varphi(s^{\\prime})\\}\\varphi(s)^{\\top}}\\\\ &{\\qquad\\qquad=2\\varphi(s)\\varphi(s)^{\\top}-\\gamma\\{\\varphi(s)\\varphi(s^{\\prime})^{\\top}+\\varphi(s^{\\prime})\\varphi(s)^{\\top}\\}}\\\\ &{\\qquad\\qquad\\preceq(2+\\gamma)\\varphi(s)\\varphi(s)^{\\top}+\\gamma\\varphi(s^{\\prime})\\varphi(s^{\\prime})^{\\top}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we additionally used that ", "page_idx": 39}, {"type": "equation", "text": "$$\n-(\\boldsymbol{u}\\boldsymbol{u}^{\\top}+\\boldsymbol{v}\\boldsymbol{v}^{\\top})\\preceq\\boldsymbol{u}\\boldsymbol{v}^{\\top}+\\boldsymbol{v}\\boldsymbol{u}^{\\top}\\preceq(\\boldsymbol{u}\\boldsymbol{u}^{\\top}+\\boldsymbol{v}\\boldsymbol{v}^{\\top})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for any $u,v\\in\\mathbb{R}^{d}$ . Thus, we get that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{A}^{c}+\\{\\mathbf{A}^{c}\\}^{\\top}]\\preceq2(1+\\gamma)\\Sigma_{\\varphi}^{c}\\;.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The rest of the proof follows from the fact that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\{\\mathbf{A}_{1}^{c}\\}^{\\top}\\mathbf{A}_{1}^{c}]\\succeq\\{\\bar{\\mathbf{A}}^{c}\\}^{\\top}\\bar{\\mathbf{A}}^{c}\\succeq(1-\\gamma)^{2}\\lambda_{\\operatorname*{min}}\\Sigma_{\\varphi}^{c}\\;.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which holds whenever (48) is satisfied; see e.g. in [30] (Lemma 5) or [45] (Lemma 7). ", "page_idx": 39}, {"type": "text", "text": "Based on these results, we instantiate the results summarized in Table 1 to Federated TD learning in Table 2. ", "page_idx": 39}, {"type": "text", "text": "F Analysis of Scaffnew for Federated LSA ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "To mitigate the bias caused by local training, we may use control variates. We assume in this section that at each iteration we choose, with probability $p$ , whether agents should communicate or not. Consider the following algorithm, where for $k=1,\\ldots,\\mathit{\\omega}^{T}/p$ , we compute ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{k}^{c}=\\theta_{k-1}^{c}-\\eta(\\mathbf{A}^{c}(Z_{k}^{c})\\theta_{k-1}^{c}-\\mathbf{b}^{c}(Z_{k}^{c})-\\xi_{k-1}^{c})\\ ,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Algorithm 6 \"Scaffnew\": Stochastic Controlled FedLSA with probabilistic communication ", "page_idx": 40}, {"type": "text", "text": "Input: $\\eta>0,\\theta_{0},\\xi_{0}^{c}\\in\\mathbb{R}^{d},T,N,H,p>0$   \nSet: $K=T/p$   \nfor $k=1$ to $K$ do for $c=1$ to $N$ do Receive $Z_{k}^{c}$ and perform local update: Draw $B_{k}\\sim\\operatorname{Bernoulli}(p)^{\\hat{\\theta}_{k}^{c}}=\\hat{\\theta}_{k-1}^{c}-\\eta(\\mathbf{A}^{c}(Z_{k}^{c})\\hat{\\theta}_{k-1}^{c}-\\mathbf{b}^{c}(Z_{k}^{c})-\\xi_{k-1}^{c})$ if $B_{k}=1$ then Average local iterates: $\\begin{array}{r}{\\theta_{k}^{c}=\\frac{1}{N}\\sum_{c=1}^{N}\\hat{\\theta}_{k}^{c}}\\end{array}$ Update: $\\begin{array}{r}{\\xi_{k}^{c}=\\xi_{k-1}^{c}+\\frac{p}{\\eta}(\\theta_{k}^{c}-\\hat{\\theta}_{k}^{c})}\\end{array}$ else Set: \u03b8ck = \u03b8\u02c6ck, \u03bekc = \u03bekc\u22121 ", "page_idx": 40}, {"type": "text", "text": "i.e. we update the local parameters with LSA adjusted with a control variate $\\xi_{k-1}^{c}$ . This control variate is initialized to zero, and updated after each communication round. We draw a Bernoulli random variable $B_{k}$ with success probability $p$ and then update the parameter as follows: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{k}^{c}=\\left\\{\\bar{\\theta}_{k}=\\frac{1}{N}\\sum_{c=1}^{N}\\hat{\\theta}_{k}^{c}\\right.\\ \\theta_{k}=1\\;,}\\\\ {\\theta_{k}^{c}\\quad\\quad\\quad\\quad\\quad\\quad\\left.B_{k}=0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We then update the control variate ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\xi_{k}^{c}=\\xi_{k-1}^{c}+\\frac{p}{\\eta}(\\theta_{k}^{c}-\\hat{\\theta}_{k}^{c})\\;.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we have set $\\xi_{0}^{c}=0$ . We state this algorithm in Algorithm 6. ", "page_idx": 40}, {"type": "text", "text": "Note that, for all $k\\ \\in\\ \\mathbb{N}$ , $\\sum_{c=1}^{N}\\xi_{t}^{c}\\;=\\;0$ . . We now proceed to the proof, which amounts to constructing a common Ly apunov function for the sequences $\\{\\theta_{k}^{c}\\}_{k\\in\\mathbb{N}}$ and $\\{\\xi_{k}^{c}\\}_{k\\in\\mathbb{N}}$ . Define the Lyapunov function, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\psi_{k}=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}+\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k}^{c}-\\xi_{\\star}^{c}\\|^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\theta_{\\star}$ is the solution of $\\bar{A}\\theta_{\\star}=\\bar{\\mathbf{b}}$ , and $\\xi_{\\star}^{c}=\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}-\\theta_{\\star}^{c})$ . A natural measure of heterogeneity is then given by ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{heter}}=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{\\star}^{c}\\|^{2}=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\bar{\\mathbf{A}}^{c}(\\theta_{\\star}^{c}-\\theta_{\\star})\\|^{2}\\ .\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "To analyze this algorithm, we\u2019ll study the decrease of the expected value of $\\psi_{k}$ , where the expectation is over randomness of the communication and the stochastic oracles. This requires a stronger assumption than the Assumption A3 that we used in Section 4. ", "page_idx": 40}, {"type": "text", "text": "A4. There exist constants $a,L>0$ , such that for any $\\eta\\in(0,1/L)$ , $c\\in[N]$ , it holds for $Z_{1}^{c}\\sim\\pi_{c}$ , that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a\\mathrm{I}\\preccurlyeq\\mathbb{E}[\\frac{1}{2}(\\mathbf{A}^{c}(Z_{1}^{c})+\\mathbf{A}^{c}(Z_{1}^{c})^{\\top})]\\preccurlyeq\\frac{1}{L}\\mathbb{E}[\\mathbf{A}^{c}(Z_{1}^{c})^{\\top}\\mathbf{A}^{c}(Z_{1}^{c})]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This assumption is slightly more restrictive than A3. Indeed, whenever A4 holds, A3 also holds with the same constant $a$ (see 42, 45). In the case of TD, this assumption holds with $\\begin{array}{r}{L=\\frac{1+\\gamma}{\\left(1-\\gamma\\right)\\nu}}\\end{array}$ . ", "page_idx": 40}, {"type": "text", "text": "Lemma F.1 (One step progress). Assume A1 and A4. Assume that $\\begin{array}{r}{\\eta\\leq\\frac{1}{2L}}\\end{array}$ . The iterates of the algorithm described above satisfy ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\psi_{k}]\\leq\\Big(1-\\operatorname*{min}\\big(\\eta a,p^{2}\\big)\\Big)\\mathbb{E}\\big[\\psi_{k-1}\\big]+\\frac{2\\eta^{2}}{N}\\sum_{c=1}^{N}\\operatorname{Tr}(\\Sigma_{\\varepsilon}^{c})\\;.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Decomposition of the update. Remark that the update can be reformulated as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{k}^{c}-\\theta_{\\star}=(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))(\\theta_{k-1}^{c}-\\theta_{\\star})+\\eta(\\xi_{k-1}^{c}-\\xi_{\\star}^{c})-\\eta\\omega^{c}(Z_{k}^{c})\\ ,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\omega^{c}(z)=\\widetilde{\\mathbf{A}}^{c}(z)\\theta_{\\star}-\\widetilde{\\mathbf{b}}^{c}(z)$ . This comes from the fact that, for all $z$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{b}^{c}(z)+\\xi_{k-1}^{c}=\\bar{\\mathbf{b}}^{c}+\\widetilde{\\mathbf{b}}^{c}(z)+\\xi_{k-1}^{c}}\\\\ &{\\qquad\\qquad\\qquad=\\bar{\\mathbf{A}}^{c}\\boldsymbol{\\theta}_{\\star}^{c}+\\widetilde{\\mathbf{b}}^{c}(z)+\\xi_{k-1}^{c}}\\\\ &{\\qquad\\qquad\\quad=\\bar{\\mathbf{A}}^{c}\\boldsymbol{\\theta}_{\\star}+\\widetilde{\\mathbf{b}}^{c}(z)+\\xi_{k-1}^{c}-\\xi_{\\star}^{c}}\\\\ &{\\qquad\\qquad\\quad=\\mathbf{A}^{c}(z)\\boldsymbol{\\theta}_{\\star}-\\widetilde{\\mathbf{A}}^{c}(z)\\boldsymbol{\\theta}_{\\star}+\\widetilde{\\mathbf{b}}^{c}(z)+\\xi_{k-1}^{c}-\\xi_{\\star}^{c}}\\\\ &{\\qquad\\qquad\\quad=\\mathbf{A}^{c}(z)\\boldsymbol{\\theta}_{\\star}-\\boldsymbol{\\omega}^{c}(z)+\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Expression of communication steps. Using that $\\sum_{c=1}^{N}\\xi_{k-1}^{c}=0$ and $\\smash{\\sum_{c=1}^{N}\\xi_{\\star}^{c}=0}$ , we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}=\\mathbf{1}_{\\{1\\}}(B_{k})\\|\\bar{\\theta}_{k}-\\theta_{\\star}\\|^{2}+\\mathbf{1}_{\\{0\\}}(B_{k})\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}}\\\\ &{=\\mathbf{1}_{\\{1\\}}(B_{k})\\|\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}(\\hat{\\theta}_{k}^{c}-\\frac{\\eta}{p}\\xi_{k-1}^{c})-\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}(\\theta_{\\star}-\\frac{\\eta}{p}\\xi_{\\star}^{c})\\|^{2}+\\mathbf{1}_{\\{0\\}}(B_{k})\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The first term can be upper bounded by using Lemma D.3, which gives ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbf{1}_{\\{1\\}}(B_{k})\\|\\bar{\\theta}_{k}-\\theta_{\\star}\\|^{2}}\\\\ &{\\displaystyle=\\mathbf{1}_{\\{1\\}}(B_{k})\\left\\{\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\frac{\\eta}{p}(\\xi_{k-1}^{c}-\\xi_{\\star}^{c})-\\theta_{\\star}\\|^{2}-\\frac{1}{N}\\sum_{c=1}^{N}\\|\\bar{\\theta}_{k}-(\\hat{\\theta}_{k}^{c}-\\frac{\\eta}{p}\\xi_{k-1}^{c})+\\frac{\\eta}{p}\\xi_{\\star}^{c}\\|^{2}\\right\\}}\\\\ &{\\displaystyle=\\mathbf{1}_{\\{1\\}}(B_{k})\\left\\{\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\frac{\\eta}{p}(\\xi_{k-1}^{c}-\\xi_{\\star}^{c})-\\theta_{\\star}\\|^{2}-\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k}^{c}-\\xi_{\\star}^{c}\\|^{2}\\right\\}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We now expand the first term in the right-hand side of the previous equation. This gives ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\frac{\\eta}{p}(\\xi_{k-1}^{c}-\\xi_{\\star}^{c})-\\theta_{\\star}\\|^{2}}\\\\ &{\\quad\\quad=\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\left\\{\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}-\\frac{2\\eta}{p}\\langle\\xi_{k-1}^{c}-\\xi_{\\star}^{c},\\,\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\rangle+\\frac{\\eta^{2}}{p^{2}}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}\\right\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which yields ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\bf1}_{\\{1\\}}(B_{k})\\left\\{\\psi_{k}\\right\\}={\\bf1}_{\\{1\\}}(B_{k})\\left\\{\\|\\bar{\\theta}_{k}-\\theta_{\\star}\\|^{2}+\\displaystyle\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k}^{c}-\\xi_{\\star}^{c}\\|^{2}\\right\\}\\ }}\\\\ {{\\ }}\\\\ {{={\\bf1}_{\\{1\\}}(B_{k})\\left\\{\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}-\\displaystyle\\frac{2\\eta}{p}\\langle\\xi_{k-1}^{c}-\\xi_{\\star}^{c},\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\rangle+\\displaystyle\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}\\right\\}\\ .}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "On the other hand, note that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{1}_{\\left\\{0\\right\\}}(B_{k})\\left\\{\\psi_{k}\\right\\}=\\mathbf{1}_{\\left\\{0\\right\\}}(B_{k})\\left\\{\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}+\\displaystyle\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k}^{c}-\\xi_{\\star}^{c}\\|^{2}\\right\\}}\\\\ {\\displaystyle=\\mathbf{1}_{\\left\\{0\\right\\}}(B_{k})\\left\\{\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}+\\displaystyle\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}\\right\\}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By combining (51) and (50), we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\psi_{k}=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}+\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k}^{c}-\\xi_{\\star}^{c}\\|^{2}}}\\\\ {{\\displaystyle\\quad=\\frac{1}{N}\\sum_{c=1}^{N}\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}-2\\frac{\\eta}{p}{\\bf1}_{\\{1\\}}(B_{k})\\langle\\xi_{k-1}^{c}-\\xi_{\\star}^{c},\\,\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\rangle+\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Progress in local updates. We now bound the first term of the sum in (52). For $c\\in[N]$ , (49) gives ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}=\\|(\\mathsf{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))(\\theta_{k-1}^{c}-\\theta_{\\star})+\\eta(\\xi_{k-1}^{c}-\\xi_{\\star}^{c})-\\eta\\omega^{c}(Z_{k}^{c})\\|^{2}}\\\\ &{\\quad=\\|(\\mathsf{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k}^{c}-\\theta_{\\star}\\}-\\eta\\omega^{c}(Z_{k}^{c})\\|^{2}+\\eta^{2}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}}\\\\ &{\\quad\\quad+\\,2\\eta\\langle\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\,,\\,(\\mathsf{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k}^{c}-\\theta_{\\star}\\}-\\eta\\omega^{c}(Z_{k}^{c})\\rangle}\\\\ &{\\quad=\\underbrace{\\|(\\mathsf{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k}^{c}-\\theta_{\\star}\\}-\\eta\\omega^{c}(Z_{k}^{c}))\\|^{2}}_{T}+2\\eta\\langle\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\,,\\,\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\rangle-\\eta^{2}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Define the $\\sigma$ -algebra $\\mathcal{G}_{k-1}\\;=\\;\\sigma(B_{s},s\\;\\leq\\;k-1,Z_{s}^{c},s\\;\\leq\\;k-1,c\\;\\in\\;[N])$ . We now bound the conditional expectation of $T_{1}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}^{\\mathcal{G}}\\left[\\|(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k}^{c}-\\theta_{\\star}\\}\\|^{2}-2\\eta\\langle(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k}^{c}-\\theta_{\\star}\\}\\,,\\,\\omega^{c}(Z_{k}^{c})\\rangle+\\eta^{2}\\|\\omega^{c}(Z_{k}^{c})\\|^{2}\\right]}\\\\ &{=\\mathbb{E}^{\\mathcal{G}}\\left[\\|(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k}^{c}-\\theta_{\\star}\\}\\|^{2}+2\\eta^{2}\\langle\\mathbf{A}^{c}(Z_{k}^{c})\\{\\theta_{k}^{c}-\\theta_{\\star}\\}\\,,\\,\\omega^{c}(Z_{k}^{c})\\rangle+\\eta^{2}\\|\\omega^{c}(Z_{k}^{c})\\|^{2}\\right]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where we used the fact that $\\langle\\mathrm{I}\\,,\\,\\omega^{c}(Z_{k}^{c})\\rangle=0$ . Using Young\u2019s inequality for products, and Lemma D.4 with $\\begin{array}{r}{\\eta\\leq\\frac{1}{2L}}\\end{array}$ and $u=\\theta_{k}^{c}-\\theta_{\\star}$ , we then obtain ", "page_idx": 42}, {"type": "text", "text": "EGk\u22121 [T1] $\\begin{array}{r l}&{\\leq\\mathbb{E}^{\\mathcal{G}}\\left[\\|(\\mathrm{I}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k}^{c}-\\theta_{\\star}\\}\\|^{2}+\\eta^{2}\\|\\mathbf{A}^{c}(Z_{k}^{c})\\{\\theta_{k}^{c}-\\theta_{\\star}\\}\\|^{2}+\\eta^{2}\\|\\omega^{c}(Z_{k}^{c})\\|^{2}+\\eta^{2}\\|\\omega^{c}(Z_{k}^{c})\\|^{2}\\right]}\\\\ &{\\leq(1-\\eta a)\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}-\\eta(\\frac{1}{L}-2\\eta)\\mathbb{E}^{\\mathcal{G}}\\left[\\|\\mathbf{A}^{c}(Z_{k}^{c})\\{\\theta_{k}^{c}-\\theta_{\\star}\\}\\|^{2}\\right]+2\\eta^{2}\\mathbb{E}^{\\mathcal{G}}\\left[\\|\\omega^{c}(Z_{k}^{c})\\|^{2}\\right]\\;.\\qquad(54)}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Plugging (54) in (53) and using the assumption $\\begin{array}{r}{\\eta\\leq\\frac{1}{2L}}\\end{array}$ , we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathcal{G}}\\left[\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}-2\\eta\\langle\\xi_{k-1}^{c}-\\xi_{\\star}^{c},\\,\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\rangle\\right]}\\\\ &{\\quad\\quad\\leq(1-\\eta a)\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}-\\eta^{2}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}+2\\eta^{2}\\,\\mathrm{Tr}(\\Sigma_{\\varepsilon}^{c})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Bounding the Lyapunov function. Taking the condtional expectation of (52) and using (55) for $c=1$ to $N$ , we obtain the following bound on the Lyapunov function ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}^{\\mathcal{G}}\\left[\\psi_{k}\\right]=\\displaystyle\\frac{1}{N}\\sum_{c=1}^{N}\\mathbb{E}^{\\mathcal{G}}\\left[\\|\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\|^{2}-2\\eta\\langle\\xi_{k-1}^{c}-\\xi_{\\star}^{c},\\,\\hat{\\theta}_{k}^{c}-\\theta_{\\star}\\rangle\\right]+\\displaystyle\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}}\\\\ {\\displaystyle\\quad\\leq\\frac{1}{N}\\sum_{c=1}^{N}\\big[(1-\\eta a)\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}-\\eta^{2}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}+2\\eta^{2}\\operatorname{Tr}(\\Sigma_{\\varepsilon}^{c})\\big]+\\displaystyle\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}}\\\\ {\\displaystyle\\quad=(1-\\eta a)\\frac{1}{N}\\sum_{c=1}^{N}\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}+(1-p^{2})\\frac{\\eta^{2}}{p^{2}}\\frac{1}{N}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}+\\frac{2\\eta^{2}}{N}\\sum_{c=1}^{N}\\operatorname{Tr}(\\Sigma_{\\varepsilon}^{c})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and the result of the Lemma follows from the Tower property. ", "page_idx": 42}, {"type": "text", "text": "Theorem F.2 (Convergence rate). Assume A1 and $A3(2)$ . Then, for any $\\begin{array}{r}{\\eta\\leq\\frac{1}{2L}}\\end{array}$ and $T>0,$ , it holds ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\psi_{K}]\\leq\\left(1-\\zeta\\right)^{K}\\left(\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\frac{\\eta^{2}}{p^{2}}\\Delta_{\\mathrm{heter}}\\right)+\\frac{2\\eta^{2}}{\\zeta}\\frac{1}{N}\\sum_{c=1}^{N}\\mathrm{Tr}(\\Sigma_{\\varepsilon}^{c})\\ ,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\zeta=\\operatorname*{min}\\left(\\eta a,p^{2}\\right)$ . ", "page_idx": 42}, {"type": "text", "text": "Corollary F.3 (Iteration complexity). Let $\\epsilon>0$ . Set $\\begin{array}{r}{\\eta\\,=\\,\\operatorname*{min}\\left(\\frac{1}{2L},\\frac{\\epsilon a}{8\\bar{\\sigma}}\\right)}\\end{array}$ and $p=\\sqrt{\\eta a}$ (so that $\\zeta=\\eta a)$ . Then, $\\mathbb{E}[\\psi_{K}]\\le\\epsilon^{2}$ as long as the number of iterations is ", "page_idx": 42}, {"type": "equation", "text": "$$\nK\\geq\\operatorname*{max}\\left(\\frac{2L}{a},\\frac{4\\bar{\\sigma}_{\\varepsilon}}{\\epsilon^{2}a^{2}}\\right)\\log\\left(\\frac{\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\operatorname*{min}\\left(\\frac{1}{2a L},\\frac{\\epsilon}{8\\bar{\\sigma}}\\right)\\Delta_{\\mathrm{heter}}}{2\\epsilon^{2}}\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which corresponds to an expected number of communication rounds ", "page_idx": 42}, {"type": "equation", "text": "$$\nT\\geq\\operatorname*{max}\\left(\\sqrt{\\frac{2L}{a}},\\sqrt{\\frac{4\\bar{\\sigma}_{\\varepsilon}}{\\epsilon^{2}a^{2}}}\\right)\\log\\left(\\frac{\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\operatorname*{min}\\left(\\frac{1}{2a L},\\frac{\\epsilon}{8\\bar{\\sigma}}\\right)\\Delta_{\\mathrm{heter}}}{2\\epsilon^{2}}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Theorem F.4 (No linear speedup in the probabilistic communication setting with control variates). The bounds obtained in Theorem $F.2$ are minimax optimal up to constants that are independent from the problem. Precisely, for every $(p,\\eta)$ there exists a FLSA problem such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\psi_{K}]=\\left(1-\\zeta\\right)^{K}\\left(\\|\\theta_{0}-\\theta_{\\star}\\|^{2}+\\frac{\\eta^{2}}{p^{2}}\\Delta_{\\mathrm{heter}}\\right)+\\frac{2\\eta^{2}}{\\zeta}\\bar{\\sigma}_{\\varepsilon}\\;,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we have defined $\\zeta=\\operatorname*{min}\\left(2\\eta a,p^{2}\\right)$ . ", "page_idx": 43}, {"type": "text", "text": "Proof. Define for all $c\\in[N]$ , ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}^{c}=a\\mathbf{I}\\ ,\\quad\\bar{\\mathbf{b}}^{c}=b_{c}u\\ ,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where u is a vector whom all coordinates are equal to 1. We also consider the sequence of i.i.d random variables $(Z_{k}^{c})$ such that that for all $c\\in[N]$ and $0\\leq t\\leq T$ , $Z_{k}^{c}$ follows a Rademacher distribution. Moreover, we define ", "page_idx": 43}, {"type": "equation", "text": "$$\n{\\bf A}^{c}(Z_{k}^{c})=\\bar{{\\bf A}}^{c}\\;,\\quad{\\bf b}^{c}(Z_{k}^{c})=\\bar{{\\bf b}}^{c}+Z_{k}^{c}u\\;.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In particular this implies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\omega^{c}(z)=Z_{k}^{c}u\\;.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We follow the same proof of Lemma F.1 until the chain of equalities breaks. Thereby, we start from ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\psi_{k}]=\\mathbb{E}[\\sum_{c=1}^{N}\\|\\theta_{k}^{c}-\\theta_{\\star}\\|^{2}+\\frac{\\eta^{2}}{p^{2}}\\sum_{c=1}^{N}\\|\\xi_{k}^{c}-\\xi_{\\star}^{c}\\|^{2}]}\\\\ {\\displaystyle=\\mathbb{E}[\\sum_{c=1}^{N}\\|({\\mathrm{I}}-\\eta\\mathbf{A}^{c}(Z_{k}^{c}))\\{\\theta_{k-1}^{c}-\\theta_{\\star}\\}-\\eta\\omega^{c}(Z_{k}^{c}))\\|^{2}+({1-p^{2}})\\frac{\\eta^{2}}{p^{2}}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}]}\\\\ {\\displaystyle=\\mathbb{E}[\\sum_{c=1}^{N}\\|({\\mathrm{I}}-\\eta\\bar{\\mathbf{A}}^{c})\\{\\theta_{k-1}^{c}-\\theta_{\\star}\\}-\\eta\\omega^{c}(Z_{k}^{c}))\\|^{2}+({1-p^{2}})\\frac{\\eta^{2}}{p^{2}}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}]}\\\\ {\\displaystyle=\\mathbb{E}[\\sum_{c=1}^{N}({1-\\eta a})^{2}\\|\\theta_{k-1}^{c}-\\theta_{\\star}\\|^{2}+\\eta^{2}\\|\\omega^{c}(Z_{k}^{c})\\|^{2}+({1-p^{2}})\\frac{\\eta^{2}}{p^{2}}\\sum_{c=1}^{N}\\|\\xi_{k-1}^{c}-\\xi_{\\star}^{c}\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we used that $\\mathbf{A}^{c}(Z_{k}^{c})=\\bar{\\mathbf{A}}^{c}$ . Unrolling the recursion gives the desired result. ", "page_idx": 43}, {"type": "text", "text": "G Experimental Details and Additional Experiments ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "G.1 Experimental Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Here, we give additional details regarding the numerical experiments. The environments used are instances of Garnet, where we use 30 states, embedded via a random projection in a $d\\,=\\,8.$ - dimensional space. We use two actions, and consider a branching factor of two, meaning that, from each state, one can transition to two different states with some probability. The rewards are then drawn uniformly randomly from the interval $[0,1]$ . ", "page_idx": 43}, {"type": "text", "text": "In the homogeneous setting, we sample one Garnet environment. Each client then receives a perturbation of this instance, where we perturb all non-zeros probabilities of transition from one state to another and all rewards with a random variable $\\epsilon\\sim\\mathcal{U}(0,\\bar{0}.02)$ . ", "page_idx": 43}, {"type": "text", "text": "In the heterogeneous setting, we proceed similarly, except that we sample two different Garnet environments, with the same parameters. Half of the agents receive the first environment, and the second half receive the second environment. As in the homogeneous setting, each agent\u2019s environment slightly differs from the base environment by a small perturbation $\\epsilon\\sim\\mathcal{U}(0,0.02)$ . ", "page_idx": 43}, {"type": "text", "text": "All the experiments presented in this paper can be run on a single laptop in just a few hours. ", "page_idx": 43}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/7b5bfd3df687c70a729a5f40076bc758b964deea24e9f1b7fbeca8bf63c6a14c.jpg", "img_caption": ["Figure 3: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated TD(0) in homogeneous settings with $\\eta=0.1$ , for different number of agents $N=10$ on the first line, $N=100$ on the second line) and different number of local steps. Green dashed line is FedLSA\u2019s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over 5 runs. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/2cd49f6d22d85189a236112317405b63741485fe9ed5754a65333fca084b5cd1.jpg", "img_caption": ["Figure 4: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated $\\mathrm{TD}(0)$ in heterogeneous settings with $\\eta=0.1$ , for different number of agents ( $N=10$ on the first line, $N=100$ on the second line) and different number of local steps. Green dashed line is FedLSA\u2019s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over 5 runs. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "G.2 Additional Experiments: Number of Local Steps and Smaller Step-Size ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we give more experimental results for FedLSA and SCAFFLSA. We use the same setting as in Section 6, but use more settings of local steps. ", "page_idx": 44}, {"type": "text", "text": "In Figure 3 and Figure 4, we give report the counterpart of Figure 1 with a wider ranger of number of local updates $H\\in\\{1,10,100,1000,10000\\}$ . The results obtained here match with observations from Section 6: in homogeneous settings, FedLSAand SCAFFLSAexhibit very similar behavior. In both methods, increasing the number of local steps speeds-up the training, until the stochastic noise dominates. At this point, both algorithms reach a stationary regime with similar error. In heterogeneous settings, while FedLSA\u2019s bias is smaller than the variance of its iterates, train", "page_idx": 44}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/c758d4a8dbcfc720cdcfe2e1f969c631735425825b9eea2bb8fb21622628eb3d.jpg", "img_caption": ["Figure 5: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated TD(0) in homogeneous settings with $\\eta=0.01$ , for different number of agents ( $N=10$ on the first line, $N=100$ on the second line) and different number of local steps. Green dashed line is FedLSA\u2019s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over 5 runs. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/4da228059b704f94f3f592cb6d27e3cedd05586d80c6ae227f5ae25e3444ad38.jpg", "img_caption": ["H = 1 H = 10 H = 100 H = 1000 H = 10000 "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 6: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated TD(0) in heterogeneous settings with $\\eta=0.01$ , for different number of agents $N=10$ on the first line, $N=100$ on the second line) and different number of local steps. Green dashed line is FedLSA\u2019s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over 5 runs. ", "page_idx": 45}, {"type": "text", "text": "ing speeds up when the number of local steps increases. After that point, bias dominates, while SCAFFLSApreserves the speed-up by eliminating this bias. ", "page_idx": 45}, {"type": "text", "text": "Finally, we report in Figure 5 and Figure 6 the results when running the same experiments using a smaller step size $\\eta=0.01$ for different number of agents and local updates. In this setting, all algorithms manage to find better estimators, since the amount of variance depends on the step size (as seen in Theorem 4.1 and Theorem 5.1). Additionally, FedLSA\u2019s bias is smaller than in Figure 7, which is also in line with the upper bound $\\begin{array}{r}{\\mathbb{E}^{1/2}[\\|\\tilde{\\theta}_{t}^{(\\mathbf{bi},\\mathbf{bi})}\\|^{2}]\\lesssim\\frac{\\eta H\\mathbb{E}[\\|\\theta-\\theta\\|]}{a}}\\end{array}$ from Theorem 4.1. ", "page_idx": 45}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/d80fe1072e49c220511636c4f9306d700a6c7191ad6921bfa2e48004f18ba14f.jpg", "img_caption": ["Figure 7: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated TD(0) in homogeneous and heterogeneous settings, for different number of agents and number of local steps. Green dashed line is FedLSA\u2019s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over 5 runs. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "HeJ1cBAgiV/tmp/e05a94e9e67bf57e393f142613884a90bf9db2d5b8429e7eb15e1c5dfba48daf.jpg", "img_caption": ["Figure 8: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated TD(0) in homogeneous and heterogeneous settings, for different number of agents and number of local steps, using a smaller step size $\\eta=0.01$ . Green dashed line is FedLSA\u2019s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over 5 runs. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "G.3 Additional Experiments: Convergence of FedLSA ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "In Figure 7 and Figure 8, we give the counterpart of Figure 1, where we additionally plot the MSE of the estimator $\\theta_{t}+\\tilde{\\theta}_{\\infty}^{(\\mathrm{bi},\\mathrm{bi})}$ , for different settings of all parameters. We recall that $\\bar{\\theta}_{\\infty}^{(\\mathsf{b i},\\mathsf{b i})}=$ $(\\mathrm{I}-\\bar{\\Gamma}_{H}^{(\\eta)})^{-1}\\bar{\\rho}_{H}$ is the bias of FedLSA, as we proved in Theorem 4.1. Therefore, $\\theta_{t}+\\tilde{\\theta}_{\\infty}^{(\\mathrm{bi},\\mathrm{bi})}$ is a proper estimator of \u03b8\u22c6, although, of course, it cannot be computed in practice since the bias \u03b8\u02dc(\u221ebi,bi) is unknown. and we see in Figure 7 that, in homogeneous settings, we recover the same error as FedLSA and SCAFFLSA. Moreover, in heterogeneous settings, it has an error similar to the one of SCAFFLSA, meaning that FedLSA, once its bias is removed, converges similarly to SCAFFLSA. The latter, however, does not require to remove an unknown bias, and directly estimates the right quantity. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: In the abstract, we claim (i) to analyze the FedLSA algorithm, with exact caracterization of the bias (see Section 4), (ii) to propose and analyze SCAFFLSA (see Section 5), and to apply it to federated TD (see Corollary 4.4, Corollary 5.3). We also propose results in the markovian sampling scheme in Corollary 4.5, and perform a numerical analysis of our method applied to federated TD in Section 6. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The main limitation of our paper is that the numerical analysis is done on federated TD on a synthetic dataset. Nonetheless, we emphasize that our paper is a theoretical paper, and that these experiments rightfully serve the purpose of illustrating the theoretical results. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: All our theoretical results are provided with clear references to assumptions, that we all state in Section 3 to ensure that these assumptions are easy to find. All results are given with proofs, that are correctly references for each theorem and corollary. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Numerical results are stated with a complete description of the environments that are used, as well as the precise sets of hyperparameters that we used. We stress that the code (in Python) is provided as supplementary with the paper, making it easy for one to reproduce our numerical experiments. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 49}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 50}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: All the code is open source, and available at https://github.com/ pmangold/scafflsa. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: The algorithm used in the numerical experiments are exactly the algorithms described in the paper. The Garnet environements are given with the parameters used for generation, and with reference to the original problem. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: All experimental results are given with error bars that consist either in the standard deviation over multiple independent runs, or the minimal/maximal values obtained over multiple independent runs. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The experiments in this paper are ran on a single laptop, and can easily be reproduced by anyone with very limited computational power. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: This paper is of purely theoretical nature, and the proposed methods do not deal with sensitive attributes that could induce unfairness or privacy issues. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This paper is of purely theoretical nature, and although the proposed methods could help deploy more federated learning solutions, this does not constitute a risk for societal harm. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: N/A. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA]   \nJustification: N/A since no existing assets are used.   \nGuidelines: \u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: N/A since the paper does not release new assets. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: N/A since the paper does not involve crowdsourcing nor research on human subjects. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: Justification: N/A since the paper does not involve crowdsourcing nor research on human subjects. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 54}]