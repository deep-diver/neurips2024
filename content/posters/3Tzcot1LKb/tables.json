[{"figure_path": "3Tzcot1LKb/tables/tables_1_1.jpg", "caption": "Table 1: Length-controlled (LC) and raw win rate (WR), and generation lengths of top models on the AlpacaEval 2 Leaderboard. Bold are the models we trained.", "description": "This table presents the performance of several language models on the AlpacaEval 2 Leaderboard, comparing their length-controlled win rate (LC), raw win rate (WR), and average generation length.  The models include both base and instruction-tuned models, highlighting the results of models trained by the authors in bold.", "section": "3 Experimental Setup"}, {"figure_path": "3Tzcot1LKb/tables/tables_4_1.jpg", "caption": "Table 2: Evaluation details for AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94]. The baseline model refers to the model compared against. GPT-4 Turbo corresponds to GPT-4-Preview-1106.", "description": "This table details the evaluation benchmarks used in the paper. It shows the number of examples (# Exs.) used for each benchmark, the baseline model used for comparison, the judge model used for scoring (GPT-4 Turbo), the scoring type used (pairwise comparison for AlpacaEval 2 and Arena-Hard, single-answer grading for MT-Bench), and the metric used for evaluation (LC & raw win rate for AlpacaEval 2, win rate for Arena-Hard, and rating of 1-10 for MT-Bench).", "section": "3 Experimental Setup"}, {"figure_path": "3Tzcot1LKb/tables/tables_4_2.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, and Mistral-Instruct) evaluated on three benchmark datasets (AlpacaEval 2, Arena-Hard, and MT-Bench).  For each setting, it shows the length-controlled win rate (LC), raw win rate (WR), and the average response length in tokens. The table also indicates the training method used for the Supervised Fine-Tuning (SFT) models used as a baseline for each setting.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_5_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) on three benchmark datasets (AlpacaEval 2, Arena-Hard, MT-Bench).  For each setting and dataset, the table shows the length-controlled win rate (LC), the raw win rate (WR), and the average response length.  The models were trained using different preference optimization methods (SimPO and baselines). The Base settings used models fine-tuned on the UltraChat dataset, while the Instruct settings used pre-trained instruction-tuned models. The table highlights the performance differences between SimPO and other methods across various model architectures and training procedures.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_8_1.jpg", "caption": "Table 6: Spearman correlation \u03c1 between average log likelihood of different models and response length on a held-out set.", "description": "This table presents the Spearman rank correlation coefficient (\u03c1) between the average log-likelihood of different language models and their response lengths on a held-out dataset.  The models compared are SimPO without length normalization (w/o LN), DPO, and SimPO. The correlation coefficient measures the strength and direction of the monotonic relationship between the two variables. A higher absolute value of \u03c1 indicates a stronger correlation, while the sign indicates the direction (positive for increasing relationship, negative for decreasing).  This table helps to analyze the impact of length normalization on the relationship between response length and model performance, highlighting SimPO's effectiveness in mitigating length bias.", "section": "4.4 In-Depth Analysis of DPO vs. SimPO"}, {"figure_path": "3Tzcot1LKb/tables/tables_17_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) evaluated on three benchmarks (AlpacaEval 2, Arena-Hard, MT-Bench).  For each setting and benchmark, it shows the length-controlled win rate (LC), raw win rate (WR), and the win rate against GPT-4 or GPT-4 Turbo.  The table also indicates whether the models used were trained from scratch (Base) or used pre-trained instruction-tuned models (Instruct).", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_18_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) evaluated on three benchmark datasets (AlpacaEval 2, Arena-Hard, MT-Bench).  For each setting, the table shows the length-controlled win rate (LC), the raw win rate (WR), and the average response length.  The table also distinguishes between models trained with supervised fine-tuning (SFT) and those further optimized using various preference optimization methods (including SimPO).", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_20_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of evaluating different preference optimization methods on three benchmark datasets: AlpacaEval 2, Arena-Hard, and MT-Bench.  The table shows the length-controlled win rate (LC) and raw win rate (WR) for each method across four model settings: Llama-3-Base, Llama-3-Instruct, Mistral-Base, and Mistral-Instruct.  The 'SFT' row represents the performance of the supervised fine-tuned models, serving as baselines for the preference optimization methods.  The table highlights the performance differences among various methods and model settings.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_23_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) on three benchmark tests (AlpacaEval 2, Arena-Hard, MT-Bench).  For each model setting and benchmark, the table shows the length-controlled win rate (LC), raw win rate (WR), and the average response length. It highlights the performance differences between models trained with different setups (Base vs. Instruct) and the effect of different preference optimization methods.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_24_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) on three benchmark datasets (AlpacaEval 2, Arena-Hard, MT-Bench).  For each setting and dataset, the table shows the length-controlled win rate (LC), the raw win rate (WR), and the average response length. The SFT (Supervised Fine-Tuned) models used are specified for each setting type.  The table compares the performance of SimPO against several other preference optimization methods. ", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_25_1.jpg", "caption": "Table 11: Average response lengths on AlpacaEval 2 and Arena-Hard trained with Mistral-Base or Mistral-Instruct.", "description": "This table presents the average response lengths generated by the SimPO and SimPO without length normalization models.  The results are broken down by model (SimPO vs. SimPO without length normalization), benchmark (AlpacaEval 2 vs. Arena-Hard), and model setting (Mistral-Base vs. Mistral-Instruct).  It demonstrates the impact of length normalization on the length of generated responses for different models and evaluation benchmarks.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_26_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) on three benchmark datasets (AlpacaEval 2, Arena-Hard, MT-Bench).  For each setting and benchmark, it shows the length-controlled win rate (LC), raw win rate (WR), and the average response length. It highlights the performance of SimPO against other preference optimization methods and indicates whether the models were trained from scratch (Base) or using pre-trained models (Instruct).", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_27_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) evaluated on three benchmarks (AlpacaEval 2, Arena-Hard, MT-Bench).  For each setting and benchmark, the table shows the length-controlled win rate (LC), raw win rate (WR), and the average response length.  It highlights the performance differences between different preference optimization methods and model setups, indicating the effectiveness of SimPO in various contexts.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_28_1.jpg", "caption": "Table 12: Results of Llama-3-Instruct (8B) setting, utilizing preference labels annotated by a stronger reward model (ArmoRM [79], we term it as version 0.2).", "description": "This table presents the results of using the Llama-3-Instruct (8B) model with preference optimization, but using a stronger reward model (ArmoRM) for annotation.  It shows the performance metrics (LC%, WR%, Length, WR%, Length, GPT-4 Turbo, GPT-4) achieved by various preference optimization methods (SimPO v0.1, SimPO v0.2, RRHF, SLIC-HF, DPO, IPO, CPO, KTO, ORPO, R-DPO, SFT) on AlpacaEval 2, Arena-Hard, and MT-Bench.", "section": "H Llama-3-Instruct v0.2 (Jul 7, 2024)"}, {"figure_path": "3Tzcot1LKb/tables/tables_28_2.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, and Mistral-Instruct) on three benchmark evaluations (AlpacaEval 2, Arena-Hard, and MT-Bench).  For each setting, it shows the length-controlled win rate (LC), raw win rate (WR), and the average score on MT-Bench.  It highlights the performance differences between base and instruction-tuned models, and between different preference optimization techniques (SimPO and its baselines). The training data used for the supervised fine-tuned (SFT) models is also specified.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_29_1.jpg", "caption": "Table 12: Results of Llama-3-Instruct (8B) setting, utilizing preference labels annotated by a stronger reward model (ArmoRM [79], we term it as version 0.2).", "description": "This table presents the results of using Llama-3-Instruct (8B) model with preference optimization.  A key difference from the results in Table 4 is the use of a stronger reward model (ArmoRM) for annotating the preference labels. The table shows the performance metrics (LC win rate, WR win rate, and average response length) on three benchmarks: AlpacaEval 2, Arena-Hard, and MT-Bench, along with GPT-4 Turbo and GPT-4 evaluation metrics.  The 'v0.2' denotes the version of SimPO using a stronger reward model.", "section": "H Llama-3-Instruct v0.2 (Jul 7, 2024)"}, {"figure_path": "3Tzcot1LKb/tables/tables_30_1.jpg", "caption": "Table 4: AlpacaEval 2 [51], Arena-Hard [50], and MT-Bench [94] results under the four settings. LC and WR denote length-controlled and raw win rate, respectively. We train SFT models for Base settings on the UltraChat dataset. For Instruct settings, we use off-the-shelf models as the SFT model.", "description": "This table presents the quantitative results of four different model settings (Llama-3-Base, Llama-3-Instruct, Mistral-Base, Mistral-Instruct) evaluated on three benchmarks (AlpacaEval 2, Arena-Hard, MT-Bench).  For each setting, the table shows the length-controlled win rate (LC), raw win rate (WR), and average response length.  It compares the performance of SimPO against several baseline methods (SFT, RRHF, SLIC-HF, DPO, IPO, CPO, KTO, ORPO, R-DPO). The SFT models used are trained on the UltraChat dataset for Base settings and are off-the-shelf instruction-tuned models for Instruct settings.", "section": "4 Experimental Results"}, {"figure_path": "3Tzcot1LKb/tables/tables_30_2.jpg", "caption": "Table 12: Results of Llama-3-Instruct (8B) setting, utilizing preference labels annotated by a stronger reward model (ArmoRM [79], we term it as version 0.2).", "description": "This table presents the results of using Llama-3-Instruct (8B) model with preference optimization.  A key aspect is that it uses a stronger reward model (ArmoRM) for annotating the preference labels, resulting in a version 0.2 of the model. The table shows the performance of several preference optimization methods (SimPO, RRHF, SLIC-HF, DPO, IPO, CPO, KTO, ORPO, R-DPO) on AlpacaEval 2, Arena-Hard, and MT-Bench benchmarks.  Metrics include length-controlled win rate (LC), raw win rate (WR), and average response length.  The GPT-4 Turbo and GPT-4 models were used for evaluation.", "section": "H Llama-3-Instruct v0.2 (Jul 7, 2024)"}, {"figure_path": "3Tzcot1LKb/tables/tables_31_1.jpg", "caption": "Table 17: Benchmark performance of Gemma-2-9B trained with DPO and SimPO on UltraFeedback (responses regenerated with Gemma-2-9B-it, following the same dataset construction process as Llama-3-Instruct (8B) described in Section 3). SimPO results in better instruction following performance than DPO without degrading math abilities (GSM) or general knowledge (MMLU) of the original model. * indicates the released checkpoint.", "description": "This table compares the performance of Gemma-2-9B model fine-tuned with DPO and SimPO methods on several benchmarks.  The benchmarks assess both instruction following capabilities (AlpacaEval 2 LC, Arena-Hard) and knowledge capabilities (ZeroEval GSM, ZeroEval MMLU). The results show that SimPO achieves better instruction following performance without sacrificing performance on other tasks. The table also includes results for the released checkpoint (*).", "section": "J Applying SimPO to Gemma 2 Models (Sept 16, 2024)"}]