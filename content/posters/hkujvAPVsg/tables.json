[{"figure_path": "hkujvAPVsg/tables/tables_4_1.jpg", "caption": "Table 1: Retrieval corpora and extracted KG statistics for each of our 1,000 question dev sets.", "description": "This table presents a quantitative overview of the three datasets used in the paper's experiments: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  For each dataset, it lists the number of passages, the number of unique nodes and edges in the knowledge graph (KG) created, the number of unique triples extracted from the passages, and the number of synonym edges added to the KG using Contriever and ColBERTv2. This information helps to characterize the size and complexity of each dataset and the resulting KG used for retrieval.", "section": "3.1 Datasets"}, {"figure_path": "hkujvAPVsg/tables/tables_5_1.jpg", "caption": "Table 2: Single-step retrieval performance. HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA and achieves comparable performance on the less challenging HotpotQA dataset.", "description": "This table presents the results of single-step retrieval experiments on three multi-hop question answering datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  It compares the performance of HippoRAG against several baseline methods (BM25, Contriever, GTR, ColBERTv2, RAPTOR, and Propositionizer) using two different retrieval backbones (Contriever and ColBERTv2). The results are reported in terms of Recall@2 and Recall@5, showing the proportion of times the top 2 and top 5 retrieved passages included all relevant passages for answering the multi-hop questions. The table highlights HippoRAG's superior performance on MuSiQue and 2WikiMultiHopQA and its comparable performance to the baselines on HotpotQA.", "section": "3.2 Baselines"}, {"figure_path": "hkujvAPVsg/tables/tables_5_2.jpg", "caption": "Table 3: Multi-step retrieval performance. Combining HippoRAG with standard multi-step retrieval methods like IRCoT results in strong complementary improvements on all three datasets.", "description": "This table presents the results of multi-step retrieval experiments, comparing the performance of integrating HippoRAG with existing multi-step retrieval methods (like IRCOT) across three datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  The metrics used are Recall@2 (R@2) and Recall@5 (R@5), indicating the proportion of times the top 2 and top 5 retrieved documents, respectively, included all relevant documents needed to answer the question. The table shows that combining HippoRAG with these methods consistently improves performance over using the multi-step method alone.", "section": "3 Experimental Setup"}, {"figure_path": "hkujvAPVsg/tables/tables_6_1.jpg", "caption": "Table 4: QA performance. HippoRAG's QA improvements correlate with its retrieval improvements on single-step (rows 1-3) and multi-step retrieval (rows 4-5).", "description": "This table presents the Question Answering (QA) performance of different retrieval methods, including the proposed HippoRAG and several baselines. It shows the Exact Match (EM) and F1 scores for QA performance on three datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  The table highlights the correlation between improved retrieval performance and improved QA performance, demonstrating that HippoRAG's retrieval enhancements lead to better QA results. The results are broken down for single-step retrieval methods (rows 1-3) and multi-step retrieval methods (rows 4-5) to showcase the effect of integrating HippoRAG into iterative retrieval approaches.", "section": "Results"}, {"figure_path": "hkujvAPVsg/tables/tables_6_2.jpg", "caption": "Table 5: Dissecting HippoRAG. To understand what makes it work well, we replace its OpenIE module and PPR with plausible alternatives and ablate node specificity and synonymy-based edges.", "description": "This table presents ablation studies on the HippoRAG model. It shows the performance of the model when different components are replaced with alternatives or when specific features are removed.  Specifically, it examines the impact of using different OpenIE methods (REBEL, Llama-3.1-8B, Llama-3.1-70B), alternative PPR approaches (using only query nodes or query nodes and their neighbors), and the effects of removing node specificity and synonymy edges. The results highlight the relative importance of each component in achieving HippoRAG's strong performance.", "section": "Discussions"}, {"figure_path": "hkujvAPVsg/tables/tables_7_1.jpg", "caption": "Table 6: All-Recall metric. We measure the percentage of queries for which all supporting passages are successfully retrieved (all-recall, denoted as AR@2 or AR@5) and find even larger performance improvements for HippoRAG.", "description": "This table compares the performance of ColBERTv2 and HippoRAG on three datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA) using the All-Recall metric.  The All-Recall metric measures the percentage of queries where all supporting passages are successfully retrieved.  Higher percentages indicate better performance in multi-hop retrieval, showcasing HippoRAG's ability to retrieve all necessary information in a single step.", "section": "5.2 HippoRAG's Advantage: Single-Step Multi-Hop Retrieval"}, {"figure_path": "hkujvAPVsg/tables/tables_7_2.jpg", "caption": "Table 2: Single-step retrieval performance. HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA and achieves comparable performance on the less challenging HotpotQA dataset.", "description": "This table presents the results of single-step retrieval experiments on three multi-hop question answering datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  The performance of HippoRAG is compared against several baselines, including BM25, Contriever, GTR, ColBERTv2, RAPTOR, and Propositionizer.  The metrics used are Recall@2 and Recall@5, which measure the proportion of queries for which at least two or five relevant passages are retrieved, respectively.  The results show that HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA, achieving comparable performance on HotpotQA, which is considered a less challenging dataset.", "section": "3.2 Baselines"}, {"figure_path": "hkujvAPVsg/tables/tables_21_1.jpg", "caption": "Table 2: Single-step retrieval performance. HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA and achieves comparable performance on the less challenging HotpotQA dataset.", "description": "This table presents the single-step retrieval performance of HippoRAG against several strong baselines (BM25, Contriever, GTR, ColBERTv2, RAPTOR, and Propositionizer) on three multi-hop QA datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  The results are presented in terms of Recall@2 and Recall@5, demonstrating HippoRAG's superior performance on MuSiQue and 2WikiMultiHopQA and comparable performance on HotpotQA.  The table highlights HippoRAG's effectiveness even in a single retrieval step, unlike iterative methods which require multiple steps.", "section": "3.2 Baselines"}, {"figure_path": "hkujvAPVsg/tables/tables_22_1.jpg", "caption": "Table 8: Knowledge graph statistics using different OpenIE methods.", "description": "This table presents a comparison of knowledge graph statistics generated using four different OpenIE methods: GPT-3.5 Turbo, REBEL-large, Llama-3.1-8B-Instruct, and Llama-3.1-70B-Instruct. For each model, the table shows the number of unique nodes (N), unique edges (E), unique triples, and ColBERTv2 synonym edges (E') extracted from three datasets: MuSiQue, 2Wiki, and HotpotQA.  The data illustrates the differences in the quantity of knowledge extracted by each model and highlights the varying capabilities of these methods in constructing knowledge graphs for different datasets.", "section": "3.1 Datasets"}, {"figure_path": "hkujvAPVsg/tables/tables_23_1.jpg", "caption": "Table 9: Intrinsic OpenIE evaluation using the CaRB [6] framework on 20 annotated passages.", "description": "This table presents the results of an intrinsic evaluation of different OpenIE methods.  The evaluation uses the CaRB framework on a set of 20 annotated passages.  The table shows the AUC, Precision, Recall, and F1 scores for each method, providing a quantitative comparison of their performance in extracting knowledge triples.", "section": "D Intrinsic OpenIE Evaluation"}, {"figure_path": "hkujvAPVsg/tables/tables_24_1.jpg", "caption": "Table 10: Ranking result examples for different approaches on several path-finding multi-hop questions.", "description": "This table presents example results for three different multi-hop questions across three different methods (HippoRAG, ColBERTv2, and IRCOT).  The questions are designed to be challenging, requiring the integration of information from multiple passages to find the answer.  The table showcases how HippoRAG outperforms the other methods, particularly in situations where knowledge integration across passages is crucial, highlighting its ability to handle path-finding multi-hop questions.", "section": "E Case Study on Path-Finding Multi-Hop QA"}, {"figure_path": "hkujvAPVsg/tables/tables_25_1.jpg", "caption": "Table 11: Error analysis on MuSiQue.", "description": "This table shows the breakdown of errors made by the HippoRAG model on the MuSiQue dataset.  It categorizes errors into three main types: limitations of the Named Entity Recognition (NER) process, errors in the Open Information Extraction (OpenIE) process, and errors in the Personalized PageRank (PPR) algorithm.  The percentages for each error type are provided.", "section": "F Error Analysis"}, {"figure_path": "hkujvAPVsg/tables/tables_25_2.jpg", "caption": "Table 2: Single-step retrieval performance. HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA and achieves comparable performance on the less challenging HotpotQA dataset.", "description": "This table presents the results of single-step retrieval experiments on three datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  The table compares the performance of HippoRAG against several baseline retrieval methods (BM25, Contriever, GTR, ColBERTv2, RAPTOR, and Propositionizer) using recall@2 and recall@5 as metrics. The results show that HippoRAG significantly outperforms other methods on MuSiQue and 2WikiMultiHopQA and achieves comparable results on HotpotQA, indicating its effectiveness in single-step multi-hop question answering.", "section": "3.2 Baselines"}, {"figure_path": "hkujvAPVsg/tables/tables_25_3.jpg", "caption": "Table 2: Single-step retrieval performance. HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA and achieves comparable performance on the less challenging HotpotQA dataset.", "description": "This table presents the results of a single-step retrieval experiment on three datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  The performance of HippoRAG is compared against several baselines (BM25, Contriever, GTR, ColBERTv2, RAPTOR, Propositionizer) using Recall@2 and Recall@5 as metrics.  The results demonstrate that HippoRAG significantly outperforms other methods on MuSiQue and 2WikiMultiHopQA, while achieving comparable performance on the easier HotpotQA dataset.  This highlights HippoRAG's effectiveness in single-step retrieval for multi-hop question answering, especially in more challenging scenarios.", "section": "3.2 Baselines"}, {"figure_path": "hkujvAPVsg/tables/tables_26_1.jpg", "caption": "Table 3: Multi-step retrieval performance. Combining HippoRAG with standard multi-step retrieval methods like IRCoT results in strong complementary improvements on all three datasets.", "description": "This table presents the results of multi-step retrieval experiments, comparing the performance of integrating HippoRAG with a standard multi-step retrieval method (IRCoT) against baselines on three datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA).  The metrics used are Recall@2 (R@2) and Recall@5 (R@5), showing the proportion of queries where at least two or five relevant passages, respectively, were retrieved. It demonstrates that HippoRAG and IRCoT provide complementary improvements in multi-hop retrieval.", "section": "3.2 Baselines"}, {"figure_path": "hkujvAPVsg/tables/tables_26_2.jpg", "caption": "Table 2: Single-step retrieval performance. HippoRAG outperforms all baselines on MuSiQue and 2WikiMultiHopQA and achieves comparable performance on the less challenging HotpotQA dataset.", "description": "This table presents the results of a single-step retrieval experiment on three datasets: MuSiQue, 2WikiMultiHopQA, and HotpotQA.  It compares the performance of HippoRAG against several baseline retrieval methods (BM25, Contriever, GTR, ColBERTv2, RAPTOR, and Propositionizer). The results are shown in terms of Recall@2 and Recall@5 metrics.  The table highlights HippoRAG's superior performance on MuSiQue and 2WikiMultiHopQA, indicating its effectiveness in single-step multi-hop question answering.", "section": "3.2 Baselines"}, {"figure_path": "hkujvAPVsg/tables/tables_27_1.jpg", "caption": "Table 16: Intrinsic OpenIE evaluation using the CaRB [6] framework. Performance difference between the 10 longest and 10 shortest annotated passages using our default GPT-3.5 Turbo (1106) model.", "description": "This table presents the results of an intrinsic evaluation of the OpenIE method used in the HippoRAG model.  It specifically compares the performance of the OpenIE model on two sets of passages: 10 shortest and 10 longest. The evaluation metrics used are AUC, Precision, Recall, and F1-score, providing insights into the model's ability to extract accurate and relevant information from passages of different lengths.  The results highlight a significant difference in performance based on passage length, showing a potential area for improvement in the model.", "section": "F Error Analysis"}, {"figure_path": "hkujvAPVsg/tables/tables_27_2.jpg", "caption": "Table 17: Average cost and efficiency measurements for online retrieval using GPT-3.5 Turbo on 1,000 queries.", "description": "This table compares the average API cost and time taken for online retrieval using three different methods: ColBERTv2, IRCOT, and HippoRAG.  It highlights HippoRAG's significant efficiency gains in both cost and time compared to the iterative retrieval method, IRCOT.  The cost savings are substantial (0.1$ for HippoRAG vs. 1-3$ for IRCOT) and the speed improvement is also significant (3 minutes for HippoRAG vs. 20-40 minutes for IRCOT).", "section": "3.4 Implementation Details"}, {"figure_path": "hkujvAPVsg/tables/tables_27_3.jpg", "caption": "Table 17: Average cost and efficiency measurements for online retrieval using GPT-3.5 Turbo on 1,000 queries.", "description": "This table compares the average API cost and time taken for online retrieval using three different methods: ColBERTv2, IRCOT, and HippoRAG.  The results are based on processing 1000 queries.  It highlights the significant cost and time savings offered by HippoRAG, especially compared to the iterative method, IRCOT.  HippoRAG is shown to be dramatically more efficient in terms of both time and cost for online retrieval.", "section": "3.4 Implementation Details"}]