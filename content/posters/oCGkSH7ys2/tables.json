[{"figure_path": "oCGkSH7ys2/tables/tables_6_1.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the results of evaluating the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B large language model (LLM).  It compares the reasoning performance of LLaMA-2-7B before and after training using SPAG against several baseline methods, including Chain-of-Thought prompting (COT), Alpaca fine-tuning (SFT), and imitation learning of GPT-4 playing the Adversarial Taboo game. The evaluation is done across various reasoning benchmarks (MMLU, BBH, Mutual, ARC-e, ARC-c, LGQA2, WGrande, PIQA) and also includes the geometric mean (GM) score of all benchmark results for a comprehensive comparison.  The results show the improvement achieved by using SPAG.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_7_1.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B model across various reasoning benchmarks. It compares the performance of the base LLaMA-2-7B model, models trained with Chain-of-Thought prompting, different Alpaca fine-tuned models, and models trained with the SPAG method after imitation learning and multiple self-play iterations. The benchmarks used include MMLU, BBH, Mutual, ARC-e, ARC-c, LGQA2, Winogrande, and PIQA. The table shows improvements in reasoning ability across several benchmarks after SPAG training.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_14_1.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B language model across various reasoning benchmarks.  It compares the performance of the baseline LLaMA-2-7B model against several variations, including models trained with Chain-of-Thought prompting, Alpaca fine-tuning (with multiple epochs), and models that underwent imitation learning based on GPT-4 gameplay followed by SPAG training (across multiple epochs).  The benchmarks include diverse reasoning tasks and measure accuracy or success rate.  The table also includes the geometric mean (GM) across all benchmarks, providing a summary of overall performance. This helps show the relative gains of the SPAG method compared to baselines.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_14_2.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B model across various reasoning benchmarks.  It compares the performance of the base LLaMA-2-7B model against several variations, including those trained with Chain-of-Thought prompting (CoT),  multiple epochs of supervised fine-tuning (AlpacaSFT), imitation learning based on GPT-4 gameplay (Imitation), self-play on other games (SP-20Q, SP-GuessCity, IM-AlpacaSFT), and various epochs of SPAG training (SPAG-1, SPAG-2, SPAG-3). Benchmarks include MMLU, BBH, Mutual, ARC-e, ARC-c, LGQA2, WGrande, and PIQA, representing diverse reasoning tasks.  The geometric mean (GM) is also calculated to provide a single overall performance score across all benchmarks.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_18_1.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of LLMs on various reasoning benchmarks.  It compares the baseline LLaMA-2-7B model with several variations including those trained using Chain-of-Thought prompting,  those fine-tuned using Alpaca, those trained using imitation learning based on GPT-4 gameplay of Adversarial Taboo, and those trained using the Self-Play of Adversarial Game (SPAG) method.  The results show improvements in reasoning ability across multiple benchmarks with each iteration of SPAG training, demonstrating the effectiveness of the self-play method.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_19_1.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B model across various reasoning benchmarks.  It compares the baseline LLaMA-2-7B model's performance with several variations, including those enhanced by Chain-of-Thought prompting, Alpaca fine-tuning, imitation learning from GPT-4, and multiple epochs of SPAG training. Benchmarks include MMLU, BBH, Mutual, ARC-e, ARC-c, LogiQA2, Winogrande, PIQA, and a geometric mean (GM) of all benchmark scores. This allows for a comprehensive assessment of how SPAG improves reasoning capabilities compared to other methods.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_19_2.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B language model across various reasoning benchmarks.  It compares the performance of the base LLaMA-2-7B model, models fine-tuned with Chain-of-Thought prompting,  models trained with imitation learning from GPT-4, and models improved iteratively through the SPAG self-play training. The benchmarks include MMLU, BBH, Mutual, ARC-e, ARC-c, LogiQA2, Winogrande, and PIQA, along with the average geometric mean across all benchmarks. The table demonstrates the consistent improvement in reasoning abilities achieved by the SPAG method as training epochs increase.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_20_1.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B large language model across various reasoning benchmarks.  It compares the performance of the base LLaMA-2-7B model against several variations, including models fine-tuned with Chain-of-Thought prompting,  models trained through imitation learning of GPT-4 gameplay,  and models enhanced through multiple epochs of SPAG training.  The benchmarks cover diverse reasoning tasks, including commonsense reasoning, logical reasoning, and knowledge-based reasoning. The results show improvement across various metrics, indicating the effectiveness of SPAG in enhancing LLM reasoning capabilities.", "section": "4.2 Results Analysis"}, {"figure_path": "oCGkSH7ys2/tables/tables_21_1.jpg", "caption": "Table 1: Reasoning Performance of SPAG on LLaMA-2-7B.", "description": "This table presents the performance of the Self-Playing Adversarial Language Game (SPAG) method on the LLaMA-2-7B model across various reasoning benchmarks.  It compares the baseline LLaMA-2-7B model's performance with several other models, including those trained using Chain-of-Thought prompting and supervised fine-tuning with Alpaca. The table also shows the results of the imitation learning stage (using GPT-4 data) and multiple self-play training epochs (SPAG-1, SPAG-2, SPAG-3).  The benchmarks used span various reasoning tasks and are measured in terms of accuracy.", "section": "4.2 Results Analysis"}]