[{"type": "text", "text": "Self-playing Adversarial Language Game Enhances LLM Reasoning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pengyu Cheng1 Tianhao Hu1 Han Xu1 Zhisong Zhang1 Yong Dai1 Lei Han3 Nan Du1 Xiaolong Li2 Tencent AI Lab 1Shenzhen & 2Seattle 3Tencent Robotics X Lab pengyucheng@tencent.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We explore the potential of self-play training for large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate around a target word only visible to the attacker. The attacker aims to induce the defender to speak the target word unconsciously, while the defender tries to infer the target word from the attacker\u2019s utterances. To win the game, both players must have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this informationreserved conversation. Hence, we are curious about whether LLMs\u2019 reasoning ability can be further enhanced by Self-Playing this Adversarial language Game (SPAG). With this goal, we select several open-source LLMs and let each act as the attacker and play with a copy of itself as the defender on an extensive range of target words. Through reinforcement learning on the game outcomes, we observe that the LLMs\u2019 performances uniformly improve on a broad range of reasoning benchmarks. Furthermore, iteratively adopting this self-play process can continuously promote LLMs\u2019 reasoning abilities. The code is available at https://github.com/Linear95/SPAG. ", "page_idx": 0}, {"type": "image", "img_path": "oCGkSH7ys2/tmp/65cd05fa6227b8994ef7e54f1a3a49e4bb2f64156408fb118fc821d3550d629d.jpg", "img_caption": ["Base --- AlpacaSFT \u2014Imitation SPAG-epoch1 SPAG-epoch2 SPAG-epoch3 "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Reasoning improvements from Self-Playing of Adversarial language Games (SPAG) on comprehensive reasoning benchmarks. With the SPAG epoch increasing, the LLM reasoning ability continuously improves. Each axis is normalized by the maximum answer-accuracy value. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs), such as GPT-4 [OpenAI, 2023b] and Gemini [Team et al., 2023], have reformed the domain of artificial intelligence (AI) with astonishing language capacities, such as ", "page_idx": 0}, {"type": "text", "text": "Figure 2: Examples of Adversarial Taboo with the same target word \u201cconversation\u201d. The left shows an attacker-winning game, in which the defender unconsciously speaks out the target word. The right is a defender-winning episode because the defender makes the correct inference from the dialogue. ", "page_idx": 1}, {"type": "text", "text": "natural language understanding [Yang et al., 2023b, Touvron et al., 2023], text generation [Koco\u00b4n et al., 2023, Anil et al., 2023], machine translation [Jiao et al., 2023], and programming [Surameery and Shakor, 2023, Tian et al., 2023]. However, the reasoning ability of LLMs, which is essential for complex problem-solving [Pan et al., 2023] and advanced intelligence-developing [Yao et al., 2021], still retains being challenged by various criteria including correctness [Zhang et al., 2023a] and faithfulness [Turpin et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "To address the reasoning challenge of LLMs, plenty of works have contributed in-depth efforts from the perspectives of Chain-of-Thought (CoT) prompt engineering [Wei et al., 2022, Ding et al., 2023, Yao et al., 2024], and the usage of auxiliary reasoning tools [Pan et al., 2023]. However, both prompt-based and tool-calling methods require additional prompt designs, which are inconsistent and sensitive to different prompt patterns and LLM checkpoints [Turpin et al., 2024, Chu et al., 2023]. More fundamental and consistent reasoning-improving approaches are post-pretraining [Azerbayev et al., 2023] and fine-tuning [Dong et al., 2023a], which trains LLMs with additional reasoning-related text corpus. Nevertheless, these methods demand sufficient high-quality textual data, which are difficult to collect due to the massive costs of human annotation efforts [Singh et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "To improve LLM reasoning more efficiently, self-improvement methods, which enhance LLMs with model-generated synthetic data, have recently attracted increasing research attention [Singh et al., 2023, Huang et al., 2023, Burns et al., 2023, Chen et al., 2024]. Self-improvement methods usually utilize the intrinsic language capability of LLMs to judge [Huang et al., 2023], filter [Yuan et al., 2024], or revise [Yuan et al., 2024] self-generated samples to enhance their quality. However, most self-improvement methods rely on a broad range of high-quality question queries to prevent over-fitting into a sub-domain of reasoning tasks, which still requires additional data collection and cleaning. Besides, the judgments from LLMs are not guaranteed objective [Raina et al., 2024]. If an LLM already has an incorrect or biased recognition of a particular concept, the self-improvement process can reinforce and amplify the LLM\u2019s cognitive dissonance. ", "page_idx": 1}, {"type": "text", "text": "Towards more general and objective self-reasoning-improvement methods, we are inspired by the advancement from AlphaGO [Silver et al., 2016] to AlphaGO Zero [Silver et al., 2017], in which the game agents\u2019 intelligence continuously promotes via self-play without any human knowledge. Analogically, we expect to set up a language game where LLMs can improve their reasoning capacities via reinforcement learning (RL) during self-play. Although language games have attracted increasing attention in natural language processing [Lewis et al., 2017, Hausknecht et al., 2020, Xu et al., 2023, Wu et al., 2024], most of them are specially designed with customized game rules, in lack of the generalization to improve the general language capacities of LLMs. Among a few generaltarget language games including red-teaming [Ma et al., 2023], negotiation [Lewis et al., 2017], and bargain [Abdulhai et al., 2023], additional human judgments or reward models are required for outcome determination, which posts challenges on the efficiency and effectiveness of large-scale self-play RL training. Recent studies have raised interest in entity- or word-based language games, such as 20-Question [Zhang et al., 2023b] and Guess-My-City [Abdulhai et al., 2023], which provide not only straight-forward word-level outcomes but also language universality by traversing the game word from comprehensive vocabularies. However, unlike the GO game, these word-based games are out of the adversarial scheme, limiting the game intensity and self-play learning effectiveness. ", "page_idx": 1}, {"type": "text", "text": "With the above consideration, we select an adversarial language game called Adversarial Taboo [Yao et al., 2021], in which an attacker and a defender perform a conversation around a target word only visible to the attacker. The attacker aims to induce the defender to speak out the target word unconsciously; the defender tries to avoid unconscious utterance of the target word and guess the word from the dialogue history. To win the adversarial game in information-limited conversations, both players are required to have high-level language capacities in terms of expression, upstanding, and reasoning. Moreover, by collecting target words from a vast vocabulary, this game can cover a broad range of topics providing sufficient language versatility. Besides, the game outcomes can be automatically and explicitly judged: we only need to check whether the target word appears in the defender\u2019s utterances (attacker wins) or its inference patterns (defender wins). We conduct the self-play on this adversarial game using open-source LLMs, LLaMA-2-7B [Touvron et al., 2023] and Baichuan-2-13B [Yang et al., 2023a], with target words selected from a 50K top-frequency vocabulary [Davies, 2020]. Next, we conduct offline reinforcement learning on the game outcomes and observe significant performance improvement on a broad range of LLM reasoning benchmarks. Furthermore, we iterate this sampling-learning process with three epochs, within which the LLMs\u2019 reasoning can continuously obtain improvement. We believe this novel training scheme, Self-Play of Adversarial Game (SPAG), has great potential for developing advanced LLM capacities. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "With the development of LLMs [OpenAI, 2023a,b], reinforcement learning (RL) has played an increasingly important role in language model training [Ouyang et al., 2022, Ramamurthy et al., 2023]. The prime application scenario for RL in LLM training is reinforcement learning from human feedback (RLHF) [Yuan et al., 2023, Cheng et al., 2023b, Zeng et al., 2023]. RLHF first learns a reward model $r(x,y)$ from the human feedback preference pairs [Cheng et al., 2023a], and then optimizes the LLM policy $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})$ to maximize the expected reward value [Dong et al., 2023b]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{RLHF}}(\\pi_{\\theta})=-\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D},\\mathbf{y}\\sim\\pi_{\\theta}(\\mathbf{y}|\\mathbf{x})}[r(\\mathbf{x},\\mathbf{y})].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To learn the above objective, proximal policy optimization (PPO) [Schulman et al., 2017] algorithm has been recognized as the mainstream solution. In each update to equation 1, PPO minimizes: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{PPO}}(\\pi_{\\theta})=-\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D},y\\sim\\pi_{\\theta}(\\mathbf{y}|\\mathbf{x})}\\Big[\\frac{\\pi_{\\theta}(\\pmb{y}|\\pmb{x})}{\\pi_{\\bar{\\theta}}(\\pmb{y}|\\pmb{x})}\\hat{A}^{\\pi_{\\bar{\\theta}}}-\\beta\\mathbf{KL}[\\pi_{\\bar{\\theta}}||\\pi_{\\theta}]\\Big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pi_{\\bar{\\theta}}$ is a copy of $\\pi_{\\theta}$ before the update, $\\hat{A}^{\\pi_{\\bar{\\theta}}}$ is the estimated advantage value [Schulman et al., 2016] with respect to the reference policy $\\pi_{\\bar{\\theta}}$ , and $\\mathrm{KL}[\\pi_{\\bar{\\theta}}||\\pi_{\\theta}]$ is the Kullback-Leibler (KL) [Kullback, 1997] divergence regularizing $\\pi_{\\theta}$ with an appropriate updating step. However, PPO for LLMs has been continually challenged due to its inefficient natural-text online sampling and the unstable training processes [Baheti et al., 2024]. Among the improvements to PPO [Rafailov et al., 2023, Yuan et al., 2023, Dong et al., 2023b], Baheti et al. [2024] adopts the PPO objective into an offline scheme by using importance sampling [Neal, 2001], which is named Advantage-Leftover-Lunch (A-LoL): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\mathrm{RLHF-A-LoL}}=\\mathbb{E}_{\\boldsymbol{x}\\sim\\mathcal{D},\\boldsymbol{y}\\sim\\pi_{\\mathrm{ref}}(\\boldsymbol{y}|\\boldsymbol{x})}\\Big[\\hat{A}^{\\pi_{\\mathrm{ref}}}\\cdot\\frac{\\pi_{\\theta}(\\boldsymbol{y}|\\boldsymbol{x})}{\\pi_{\\mathrm{ref}}(\\boldsymbol{y}|\\boldsymbol{x})}\\cdot\\nabla_{\\theta}\\log\\pi_{\\theta}(\\boldsymbol{y}|\\boldsymbol{x})\\Big].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here the sample ${\\pmb y}\\sim\\pi_{\\mathrm{ref}}({\\pmb y}|{\\pmb x})$ and advantage $\\hat{A}^{\\pi_{\\mathrm{ref}}}$ are both from the reference distribution $\\pi_{\\mathrm{ref}}(y|x)$ and calculated offilne. Besides, Gulcehre et al. [2023] proposed Reinforced Self-Training (ReST) to simplify the RLHF scheme. With a threshold $\\tau\\in\\mathbb{R}$ , ReST updates the LLM by the reinforcement on the selected samples $\\mathcal{D}_{\\tau}=\\{(\\pmb{x},\\pmb{y}):r(\\pmb{x},\\pmb{y})>\\tau\\}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{ReST}}(\\pi_{\\theta})=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D},\\mathbf{y}\\sim\\pi_{\\mathrm{ref}}(y|\\mathbf{x})}\\Big[\\mathbf{1}_{r(\\mathbf{x},\\mathbf{y})>\\tau}\\cdot\\mathcal{L}_{\\theta}(\\mathbf{x},\\mathbf{y})\\Big]=\\mathbb{E}_{\\mathcal{D}_{\\tau}}[\\mathcal{L}_{\\theta}(\\mathbf{x},\\mathbf{y})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{L}_{\\theta}(\\boldsymbol{x},\\boldsymbol{y})$ could be any offline RL loss such as A-LoL or the vanilla language modeling loss. ", "page_idx": 2}, {"type": "text", "text": "3 Self-play of Adversarial Language Games ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The game of Adversarial Taboo is first introduced by Yao et al. [2021], in which an attacker $\\mu$ and a defender $\\nu$ involve a multi-turn conversation. At the beginning of the game, the attacker is assigned a target word $w\\in\\upnu_{\\mathrm{target}}$ , which is not informed to the defender. The attacker\u2019s target is to induce the defender to speak the target word unconsciously. To achieve this goal, the attacker can talk about any topic related to $w$ , except directly speaking out the target word. In contrast, the defender is required to infer the target word without any unconscious utterance of the word. If the defender has sufficient confidence to infer the word, it can yell \u201cI know the word! It is $\\{t a r g e t\\;w o r d\\}$ !\u201d. Then the game terminates. If the guess is correct, the defender wins, otherwise the attacker wins. Besides, the game has a maximum number of turns $T_{0}$ . If nobody wins during $T_{0}$ turns, there is a tie. The examples of Adversarial Taboo are shown in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "Inputs: LLM policy $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})$ , target word $w$ , attacker and defender prompts $f_{\\mathrm{attack}}$ , fdefend.   \nSet the initial state $\\pmb{s}_{0}=(w)$ .   \nfor $t$ from 1 to $T$ do Sample an attacker utterance $\\begin{array}{r}{\\pmb{u}_{t}\\sim\\mu_{\\theta}(\\pmb{u}_{t}|\\pmb{s}_{t-1})=\\pi_{\\theta}(\\pmb{y}=\\pmb{u}_{t}|\\pmb{x}=f_{\\mathrm{attack}}(\\pmb{s}_{t-1})).}\\end{array}$ Update state $\\pmb{s}_{t}^{\\prime}=(w,\\pmb{u}_{1},\\pmb{v}_{1},\\dots,\\pmb{u}_{t-1},\\pmb{v}_{t-1},\\pmb{u}_{t})$ . Sample a defender utterance $v_{t}\\sim\\nu_{\\theta}(v_{t}|s_{t}^{\\prime})=\\pi_{\\theta}(y=v_{t}|x=f_{\\mathrm{defend}}(s_{t}^{\\prime})).$ . Update state $\\pmb{s}_{t}=(w,\\pmb{u}_{1},\\pmb{v}_{1},\\dots,\\pmb{u}_{t-1},\\pmb{v}_{t-1},\\pmb{u}_{t},\\pmb{v}_{t})$ .   \nend for   \nCollect an episode $\\boldsymbol{\\tau}=(s_{0},s_{1}^{\\prime},s_{1},\\dots,s_{T}^{\\prime},s_{T})$ ", "page_idx": 3}, {"type": "text", "text": "3.1 Adversarial Language Game Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We view the Adversarial Taboo as a two-player zero-sum Markov game [Littman, 1994], which can be described by a tuple as $(S,{\\mathcal{A}},F,r)$ : ", "page_idx": 3}, {"type": "text", "text": "\u2022 The state space ${\\cal S}\\ =\\ \\ \\{s_{t},s_{t}^{\\prime}\\ :\\ 1\\ \\le\\ t\\ \\le\\ T_{0}\\}$ contains two types of states, $\\begin{array}{r l}{\\pmb{s}_{t}^{\\prime}}&{{}=}\\end{array}$ $(w,\\pmb{u}_{1},\\pmb{v}_{1},\\pmb{u}_{2},\\dots,\\pmb{u}_{t})$ and $\\pmb{s}_{t}\\,=\\,(w,\\pmb{u}_{1},v_{1},\\pmb{u}_{2},\\dots,\\pmb{u}_{t},v_{t})$ , where $\\{\\bar{\\pmb{u}_{i}}\\}_{i=1}^{t}$ and $\\{{\\pmb v}_{i}\\}_{i=1}^{t}$ are the utterances of the attacker and defender, respectively. Games start at $\\pmb{s}_{0}=(w)$ with a target word $w\\in\\upnu_{\\mathrm{target}}$ and end with $T_{0}$ maximum turns. States $\\pmb{s}_{t}^{\\prime}$ and $\\scriptstyle s_{t}$ end with utterances $\\pmb{u}_{t}$ and $\\pmb{v}_{t}$ for the defender and attacker to act, respectively. ", "page_idx": 3}, {"type": "text", "text": "\u2022 The action space $\\boldsymbol{\\mathcal{A}}$ is shared with both the attacker and defender, which is equivalent to the token sequence space of natural language $\\mathcal{N}=\\{\\pmb{x}=(x_{1},x_{2},\\ldots,x_{L})|x_{l}\\in\\mathcal{V}_{\\mathrm{token}},L\\in\\mathbb{N}_{+}\\}$ . \u2022 The transition function $F:S\\times A\\to S$ deterministically appends the utterance $\\pmb{u}_{t}$ or $\\pmb{v}_{t}$ at the end of the dialogue, and converts $\\pmb{s}_{t}^{\\prime}=F(\\pmb{s}_{t-1},\\pmb{u}_{t})$ and $\\bar{\\pmb{s}}_{t}\\bar{=}\\bar{F}\\!\\left(\\pmb{s}_{t}^{\\prime},\\pmb{v}_{t}\\right)$ . \u2022 The reward $r\\,:\\,S\\,\\times\\,A\\,\\to\\,\\mathbb{R}$ evaluates the actions $\\mathbf{\\psi}u,v\\,\\in\\,A$ based on their corresponding states $\\pmb{\\mathscr{s}},\\pmb{\\mathscr{s}}^{\\prime}\\,\\in\\mathcal{S}$ with rewards $r(s,u)$ and $r(s^{\\prime},v)$ , respectively. Given a game episode $\\tau=$ $(s_{0},s_{1}^{\\prime},s_{1},\\dots,s_{T}^{\\prime},s_{T})$ , we denote the attacker\u2019s total reward $\\begin{array}{r}{\\boldsymbol{R}(\\tau)=\\sum_{t=1}^{T}r(\\boldsymbol{s}_{t-1},\\boldsymbol{u}_{t})}\\end{array}$ , so the ddeetfaeinledde rr\u2019es wtaortda ld reesiwganrs dw iist $\\begin{array}{r}{\\sum_{t=1}^{T}r(\\pmb{s}_{t}^{\\prime},\\pmb{v}_{t})=-R(\\pmb{\\tau})}\\end{array}$ etros asraitails fTya btohoe  czae nr ob-es ufomu ncdo ninst rAaipnpte.n dMixo . h heuristic rules for the Adv ", "page_idx": 3}, {"type": "text", "text": "In the above game, we denote $\\mu({\\pmb u}|{\\pmb s})$ and $\\nu(\\pmb{v}|\\pmb{s}^{\\prime})$ as the attacker\u2019s and defender\u2019s policies, respectively. Then each episode $\\tau$ can be regarded as a trajectory with the probability: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(\\tau)=P(s_{0})\\prod_{t=1}^{T}P(s_{t}^{\\prime}|s_{t-1})\\prod_{t=1}^{T}P(s_{t}|s_{t}^{\\prime})=P(w)\\prod_{t=1}^{T}\\mu(u_{t}|s_{t-1})\\prod_{t=1}^{T}\\nu(v_{t}|s_{t}^{\\prime})=:(\\mu\\times\\nu),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P(w)$ is the data distribution of target word $w\\,\\in\\,\\mathcal{V}_{\\mathrm{target}}$ . Then we can write the self-play objective of the Adversarial Taboo as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu}\\operatorname*{min}_{\\nu}\\mathcal L_{\\mathrm{AG}}(\\mu,\\nu):=\\mathbb E_{\\tau\\sim\\mu\\times\\nu}[R(\\tau)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in which the attacker tries to maximize its total reward $R(\\tau)$ by optimizing policy $\\mu$ , and the defender seeks strategies $\\nu$ to maximize the defender reward $-R(\\tau)$ (minimize $R(\\tau),$ . To play the above adversarial game with an LLM generation policy $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})$ , we first design prompt templates $f_{\\mathrm{attack}}$ , $f_{\\mathrm{defend}}:S\\to N$ for the attacker and defender respectively to convert the game states into natural language task descriptions. Next, we introduce the game policies for the two players: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{\\boldsymbol\\theta}(\\boldsymbol{\\boldsymbol{u}}|\\boldsymbol{s})=\\pi_{\\boldsymbol\\theta}(\\boldsymbol{\\boldsymbol{u}}|f_{\\mathrm{atack}}(\\boldsymbol{s})),\\ \\mathrm{and}\\ \\nu_{\\boldsymbol\\theta}(\\boldsymbol{v}|\\boldsymbol{s}^{\\prime})=\\pi_{\\boldsymbol\\theta}(\\boldsymbol{v}|f_{\\mathrm{defend}}(\\boldsymbol{s}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The detailed prompt templates for the game are demonstrated in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3.2 Imitation Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Due to the limited capability of current open-source LLMs, the generation policy $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})$ can not guarantee the strict instruction-following of the game rules in prompts $f_{\\mathrm{attack}}(s)$ and $\\bar{f}_{\\mathrm{defend}}(s^{\\prime})$ . Therefore, before the self-play, we first conduct an imitation learning (behavior cloning) of GPT-4\u2019s behaviors to ensure that $\\pi_{\\boldsymbol{\\theta}}(u|f_{\\mathrm{attack}}(\\boldsymbol{s}))$ and $\\pi_{\\boldsymbol{\\theta}}\\big(\\mathbf{\\boldsymbol{\\v}}|f_{\\mathrm{defend}}(\\mathbf{\\boldsymbol{\\s}}^{\\prime})\\big)$ act consistently with the game rules. To collect the game episodes of GPT-4 [Achiam et al., 2023], we use the data collection procedure described in Algorithm 1. Similar to the setups in equation 7, we also design attacker and defender prompts for GPT-4 to act as the game players, which can be found in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "After collecting a group of GPT-4 game episodes for imitation learning as ${\\mathcal{T}}_{\\mathrm{im}}$ , we divide it into an attacker-winning set $\\bar{T_{\\mathrm{im}}}^{\\mathrm{attack}}=\\{\\tau\\stackrel{-}{\\in}T_{\\mathrm{im}}\\,\\colon R(\\tau)>0\\}$ and a defender-winning set $T_{\\mathrm{im}}^{\\mathrm{defend}}=\\{\\tau\\in$ $\\mathcal{T}_{\\mathrm{im}}:R(\\tau)<0\\bar{\\}$ . The imitation learning loss is to maximize the log-likelihood of winners\u2019 actions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{im}}^{\\mathrm{atack}}(\\pi_{\\theta})=-\\mathbb{E}_{\\tau\\in\\mathcal{T}_{\\mathrm{im}}^{\\mathrm{atack}}}\\Big[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\log\\pi_{\\theta}(u_{t}|f_{\\mathrm{atack}}(s_{t-1}))+\\beta_{1}\\mathbf{KL}[\\pi_{\\theta}||\\pi_{\\mathrm{ref}}]\\Big],}\\\\ &{\\mathcal{L}_{\\mathrm{im}}^{\\mathrm{defend}}(\\pi_{\\theta})=-\\mathbb{E}_{\\tau\\in\\mathcal{T}_{\\mathrm{im}}^{\\mathrm{defend}}}\\Big[\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\log\\pi_{\\theta}(v_{t}|f_{\\mathrm{defend}}(s_{t}^{\\prime}))+\\beta_{1}\\mathbf{KL}[\\pi_{\\theta}||\\pi_{\\mathrm{ref}}]\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the re-weighting parameter $\\beta_{1}>0$ and the regularizer $\\mathrm{KL}[\\pi_{\\theta}||\\pi_{\\mathrm{ref}}]$ prevents the model from over-fitting on the language game task and maintains the model\u2019s general language abilities. The lreeafrenrienngc eo bmjeocdtievl $\\pi_{\\mathrm{ref}}$ $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{im}}(\\pi_{\\theta})=\\frac{1}{2}\\mathcal{L}_{\\mathrm{im}}^{\\mathrm{attack}}(\\dot{\\pi}_{\\theta})+\\frac{1}{2}\\mathcal{L}_{\\mathrm{im}}^{\\mathrm{defend}}(\\pi_{\\theta})}\\end{array}$ .before training. The overall imitation ", "page_idx": 4}, {"type": "text", "text": "3.3 Reinforcement Learning from Self-play ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Imitation learning enables LLM to behave consistently following the game rules. Next, we conduct self-play by letting the LLM $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})$ play alternatively as the attacker $\\mu_{\\theta}({\\pmb u}|{\\pmb s})=\\pi_{\\theta}({\\pmb u}|f_{\\mathrm{attack}}({\\pmb s}))$ and the defender $\\nu_{\\theta}(v|s^{\\prime})\\,=\\,\\pi_{\\theta}(v|f_{\\mathrm{defend}}(s^{\\prime}))$ . Note that the self-play sampling process involves massive multi-turn auto-regressive text generation of the LLM, which causes heavy computational complexity and makes the on-policy RL training intensively inefficient. Therefore, we use an offline learning scheme: (1) make a copy $\\pi_{\\bar{\\theta}}$ of current LLM policy $\\pi_{\\theta}$ ; (2) collect self-play episodes $\\mathcal{T}_{\\bar{\\theta}}=\\{\\bar{\\tau}\\sim\\mu_{\\bar{\\theta}}\\times\\nu_{\\bar{\\theta}}\\}$ from games between attacker $\\mu_{\\bar{\\theta}}$ and defender $\\nu_{\\bar{\\theta}}$ ; (3) update $\\pi_{\\theta}$ via RL training with $\\mathcal{T}_{\\bar{\\theta}}$ . The details of the collection of $\\mathcal{T}_{\\bar{\\theta}}$ are shown in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "With a group of episodes $\\mathcal{T}_{\\bar{\\theta}}$ , we first fix the defender policy $\\nu_{\\bar{\\theta}}$ and consider updating the attacker policy $\\mu_{\\theta}$ with respect to the self-play objective $\\mathcal{L}_{\\mathrm{AG}}(\\mu_{\\theta},\\nu_{\\bar{\\theta}})=\\mathbb{E}_{\\tau\\sim\\mu_{\\theta}\\times\\nu_{\\bar{\\theta}}}[R(\\tau)]$ . We calculate the corresponding policy gradient for the attacker as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{AG}}(\\mu_{\\theta},\\nu_{\\bar{\\theta}})=\\!\\!\\mathbb{E}_{\\tau\\sim\\mu_{\\theta}\\times\\nu_{\\bar{\\theta}}}\\Big[\\displaystyle\\sum_{t=1}^{T}A_{t}^{\\mu_{\\theta}}\\cdot\\nabla_{\\theta}\\log\\mu_{\\theta}(\\boldsymbol{u}_{t}|\\boldsymbol{s}_{t-1})\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=\\!\\!\\mathbb{E}_{\\tau\\sim\\mu_{\\bar{\\theta}}\\times\\nu_{\\bar{\\theta}}}\\Big[\\displaystyle\\sum_{t=1}^{T}A_{t}^{\\mu_{\\theta}}\\cdot\\frac{\\mu_{\\theta}(\\boldsymbol{u}_{t}|\\boldsymbol{s}_{t-1})}{\\mu_{\\bar{\\theta}}(\\boldsymbol{u}_{t}|\\boldsymbol{s}_{t-1})}\\cdot\\nabla_{\\theta}\\log\\mu_{\\theta}(\\boldsymbol{u}_{t}|\\boldsymbol{s}_{t-1})\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $A_{t}^{\\mu_{\\theta}}\\,=\\,A^{\\mu_{\\theta}}\\big(\\pmb{s}_{t-1},\\pmb{u}_{t}\\big)$ is the advantage of action $\\pmb{u}_{t}$ for the attacker $\\mu_{\\theta}$ in the self-play of $\\mu_{\\theta}\\times\\nu_{\\bar{\\theta}}$ . Here we apply importance sampling to unbiasedly estimate the expectation w.r.t $\\mu_{\\theta}\\big(\\mathbf{\\boldsymbol{u}}_{t}\\big|\\mathbf{\\boldsymbol{s}}_{t-1}\\big)$ with the sampled actions from $\\mu_{\\bar{\\theta}}(\\mathbf{\\boldsymbol{u}}_{t}|\\mathbf{\\boldsymbol{s}}_{t-1})$ in $\\mathcal{T}_{\\bar{\\theta}}$ . Inspired by TRPO [Schulman et al., 2015] and PPO [Schulman et al., 2017], we design the following loss to optimize $\\mathcal{L}_{\\mathrm{AG}}(\\mu_{\\theta},\\nu_{\\bar{\\theta}})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{sp}}^{\\mathrm{atack}}(\\pi_{\\theta})=-\\,\\mathbb{E}_{\\tau\\in\\mathcal{T}_{\\bar{\\theta}}}\\Big[\\displaystyle\\sum_{t=1}^{T}\\frac{\\mu_{\\theta}\\bigl(\\boldsymbol{u}_{t}|\\boldsymbol{s}_{t-1}\\bigr)}{\\mu_{\\bar{\\theta}}\\bigl(\\boldsymbol{u}_{t}|\\boldsymbol{s}_{t-1}\\bigr)}\\hat{A}_{t}^{\\mu_{\\bar{\\theta}}}-\\beta_{2}\\mathrm{KL}[\\pi_{\\theta}\\|\\pi_{\\bar{\\theta}}]\\Big]}\\\\ &{\\qquad\\qquad=-\\,\\mathbb{E}_{\\tau\\in\\mathcal{T}_{\\bar{\\theta}}}\\Big[\\displaystyle\\sum_{t=1}^{T}\\frac{\\pi_{\\theta}\\bigl(\\boldsymbol{u}_{t}|\\,f_{\\mathrm{atack}}\\bigl(\\boldsymbol{s}_{t-1}\\bigr)\\bigr)}{\\pi_{\\bar{\\theta}}\\bigl(\\boldsymbol{u}_{t}|\\,f_{\\mathrm{atack}}\\bigl(\\boldsymbol{s}_{t-1}\\bigr)\\bigr)}\\hat{A}_{t}^{\\mu_{\\bar{\\theta}}}-\\beta_{2}\\mathrm{KL}[\\pi_{\\theta}\\|\\pi_{\\bar{\\theta}}]\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the re-weighting parameter $\\beta_{2}>0$ , and regularizer $\\mathrm{KL}(\\pi_{\\boldsymbol{\\theta}}||\\pi_{\\boldsymbol{\\bar{\\theta}}})$ guarantees an appropriate policy update step size. Following TRPO [Schulman et al., 2015], we use empirical estimation $\\hat{A}_{t}^{\\mu_{\\bar{\\theta}}}$ of policy $\\mu{\\bar{\\theta}}$ to approximate the advantage $A_{t}^{\\mu_{\\theta}}$ . More details of advantage estimation are described in Appendix D. Similarly, from the perspective of the defender, we propose the corresponding loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{sp}}^{\\mathrm{defend}}(\\pi_{\\theta})=-\\mathbb{E}_{\\tau\\in\\mathcal{T}_{\\bar{\\theta}}}\\Big[\\sum_{t=1}^{T}\\frac{\\pi_{\\theta}(v_{t}|f_{\\mathrm{defend}}(s_{t}^{\\prime}))}{\\pi_{\\bar{\\theta}}(v_{t}|f_{\\mathrm{defend}}(s_{t}^{\\prime}))}\\hat{A}_{t}^{\\nu_{\\bar{\\theta}}}-\\beta_{2}\\mathrm{KL}[\\pi_{\\theta}\\|\\pi_{\\bar{\\theta}}]\\Big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practice, we find when negative advantage values exist, the above policy gradient method can cause training instability and damage the LLMs\u2019 general language performance. To mitigate the ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Self-play of adversarial language games (SPAG) ", "page_idx": 5}, {"type": "text", "text": "Inputs: LLM policy $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}|\\boldsymbol{x})$ , target word set $\\nu_{\\mathrm{target}}$ , attacker and defender prompts $f_{\\mathrm{attack}}$ , fdefend. for self-play epoch iterations do ", "page_idx": 5}, {"type": "text", "text": "Make a copy of $\\pi_{\\theta}$ as $\\pi_{\\bar{\\theta}}$ .   \nSet $\\mu_{\\bar{\\theta}}(u|s)=\\pi_{\\bar{\\theta}}(u|f_{\\mathrm{attack}}(s))$ and $\\nu_{\\bar{\\theta}}(v|s^{\\prime})=\\pi_{\\bar{\\theta}}(v|f_{\\mathrm{defend}}(s^{\\prime}))$ .   \nFor each $w\\in\\upnu_{\\mathrm{target}}$ , sample a episode $\\tau\\sim\\mu_{\\bar{\\theta}}\\times\\nu_{\\bar{\\theta}}$ . Collect $\\bar{T_{\\bar{\\theta}}}=\\{\\tau\\sim\\mu_{\\bar{\\theta}}\\times\\nu_{\\bar{\\theta}}\\}$ Split the attacker-winning set $T_{\\bar{\\theta}}^{\\mathrm{attack}}$ and the defender-winning set $T_{\\bar{\\theta}}^{\\mathrm{defend}}$ .   \nUpdate $\\pi_{\\theta}$ with loss $\\mathcal{L}_{\\mathrm{SPAG}}(\\pi_{\\theta})$ . ", "page_idx": 5}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "issue and obtain more stable RL training, we utilize the methodology of ReST [Gulcehre et al., 2023] as in equation 4, which considers the offline learning with samples $\\{\\tau:R(\\tau)>\\xi\\}$ selected by a reward threshold $\\xi$ . More specifically, we set the reward threshold $\\xi\\;=\\;0$ and select the attacker-winning episodes $T_{\\bar{\\theta}}^{\\mathrm{attack}}\\,=\\,\\{\\pmb{\\tau}\\,\\in\\,^{\\circ}T_{\\bar{\\theta}}\\,:\\,R(\\pmb{\\tau})\\,>\\,0\\}$ and the defender-winning episodes $T_{\\bar{\\theta}\\ \\ \\ \\cdot\\ \\ }^{\\mathrm{defend}}\\,=\\,\\{\\tau\\,\\in\\,T_{\\bar{\\theta}}\\,:\\,R(\\tau)\\,<\\,0\\}$ for the attacker and the defender training respectively. Similar techniques have also been studied in earlier RL literature to obtain stable policy updates, such as self-imitation learning [Oh et al., 2018] and the UPGO [Vinyals et al., 2019] methods. Therefore, the overall self-play of adversarial language games (SPAG) objective is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{SPAG}}(\\pi_{\\theta})=-\\displaystyle\\frac{1}{2}\\mathbb{E}_{T_{\\theta}^{\\mathrm{aack}}}\\Big[\\sum_{t=1}^{T}\\frac{\\pi_{\\theta}\\left(u_{t}|f_{\\mathrm{atack}}(s_{t-1})\\right)}{\\pi_{\\bar{\\theta}}\\left(u_{t}|f_{\\mathrm{atack}}(s_{t-1})\\right)}\\hat{A}_{t}^{\\mu_{\\bar{\\theta}}}-\\beta_{2}\\mathbf{KL}[\\pi_{\\theta}|\\pi_{\\bar{\\theta}}]\\Big]}\\\\ &{\\quad\\quad\\quad-\\displaystyle\\frac{1}{2}\\mathbb{E}_{T_{\\theta}^{\\mathrm{ackoal}}}\\Big[\\sum_{t=1}^{T}\\frac{\\pi_{\\theta}\\left(v_{t}|f_{\\mathrm{defend}}(s_{t}^{\\prime})\\right)}{\\pi_{\\bar{\\theta}}\\left(v_{t}|f_{\\mathrm{defend}}(s_{t}^{\\prime})\\right)}\\hat{A}_{t}^{\\nu_{\\bar{\\theta}}}-\\beta_{2}\\mathbf{KL}[\\pi_{\\theta}|\\pi_{\\bar{\\theta}}]\\Big]-\\alpha\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{\\mathrm{SPT}}}[\\log\\pi_{\\theta}(y|x)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha>0$ is a re-weighting hyper-parameter, and $\\mathbb{E}_{\\cal D_{\\mathrm{SFT}}}[\\log\\pi_{\\boldsymbol\\theta}({\\boldsymbol\\pmb y}|{\\boldsymbol\\ x})]$ is the log-likelihood on a supervised fine-tuning (SFT) dataset $\\mathcal{D}_{\\mathrm{SFT}}$ to prevent LLMs from losing general language abilities. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To verify the effectiveness of SPAG, we select open-source pretrained LLMs of different sources and model sizes, particularly LLaMA-2-7B [Touvron et al., 2023] and Baichuan-2-13B [Yang et al., 2023a]. As introduced in Section 3, the training process includes two stages: imitation learning of GPT-4, and self-play learning on game episodes. For baseline comparison, we consider Chain-ofThought (CoT) [Wei et al., 2022] and continuous supervised fine-tuning (SFT) methods. Besides, we also test another two keyword-based non-adversarial language games: 20-Question and Guess-MyCity as described in Abdulhai et al. [2023]. More details about the two games are in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training Data Preparation The training data preparation consists of the following three parts.   \nMore data collection details are in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Target Words: We aim to play the adversarial game with an extensive range of target words so that diverse topics can be discussed during the self-play processes, which helps maintain the generalization ability of LLMs. Hence, we collect the 50K most frequently used words from the Corpus of Contemporary American (CoCA) [Davies, 2020] as the target word list $\\nu_{\\mathrm{target}}$ . Besides, stop words defined in NLTK [Bird, 2006], which are commonly used with insignificant semantic meaning, are filtered out of $\\nu_{\\mathrm{target}}$ . \u2022 Imitation Learning Data: To enable the instruction-following ability of open-source LLMs on game rules, we use the same data collection process in Algorithm 1 via the GPT-4 [OpenAI, 2023b] API and play the Taboo game one episode per target word. The attacker and defender prompts are in Appendix A.1. Due to the resource limitation, we only collect the GPT-4 self-play samples with the top 30K words from $\\nu_{\\mathrm{target}}$ . The maximum interaction turn is randomly selected in the range [3, 8]. The results are collected as ${\\mathcal{T}}_{\\mathrm{im}}$ for the imitation learning. \u2022 Supervised Funetuning (SFT) Data: We also prepare general query-response SFT data to prevent LLMs from being over-fitted on the adversarial game. We use Alpaca [Taori et al., 2023] as the SFT set, which contains 52K instruction-following data from GPT-3 [Brown et al., 2020]. ", "page_idx": 5}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/c0e047c28097cdf359cd23e37d59eab1b5dc4994634f33afa809a808b0f09397.jpg", "table_caption": ["Table 1: Reasoning Performance of SPAG on LLaMA-2-7B ", "Evaluation To evaluate the LLMs\u2019 performance, we consider the following two perspectives: "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "\u2022 Reasoning Benchmarks: To test the reasoning ability, we consider the following commonly used benchmarks including BIG-Bench Hard (BBH) [Suzgun et al., 2022], ARC easy (ARC-e) & challenge (ARC-c) [Clark et al., 2018], Mutual [Cui et al., 2020], WinoGrande [Sakaguchi et al., 2019], LogiQA2 [Liu et al., 2023], PIQA [Bisk et al., 2020]. BBH requires the exact match of the generated answers, the other benchmarks are all within the multiple-choice form. Besides reasoning metrics, we test MMLU [Hendrycks et al., 2020] as a general performance evaluation for LLMs. Additionally, we calculate the geometric mean of the numerical results of all benchmarks as an overall performance measurement. More details can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Game Win Rates: Besides the general LLM capabilities, we report the win rates on a testing vocabulary $\\mathcal{V}_{\\mathrm{test}}$ to validate if the game skills of LLMs improve through self-play. Following Abdulhai et al. [2023], we use the same keyword list in 20-Question as $\\mathcal{V}_{\\mathrm{test}}$ , which contains 168 typical words manually selected from diverse daily objects. We denote the number of winning episodes as $N_{\\mathrm{win}}$ , the number of losing episodes as $N_{\\mathrm{lose}}$ , and the number of tied games as $N_{\\mathrm{tie}}$ . Then, the player win rate is calculated as $(N_{\\mathrm{win}}+0.5N_{\\mathrm{tie}})/(N_{\\mathrm{win}}+N_{\\mathrm{loss}}\\,+N_{\\mathrm{tie}})$ . The invalid game episodes, where LLM players do not strictly follow the game rules, are ignored. ", "page_idx": 6}, {"type": "text", "text": "Training Details Most of our LLM training setups follow Alpaca [Taori et al., 2023]. For imitation learning, the learning rate is 5e-6, and the KL-penalty coefficient $\\beta_{1}=0.1$ . For SPAG training, the learning rate is 2e-6, the KL-penalty coefficient $\\beta_{2}=0.2$ , and the SFT coefficient $\\alpha=0.5$ . For the Alpaca SFT baseline, we exactly follow the training setups of Alpaca and set the learning rate to 2e-6. Among all training stages, the batch size is 128 and the max sequence length is 2048. Each training process maintains one epoch over the offline collected trajectories. The the decay parameter $\\gamma$ is set to 0.8. The maximum turn of the Adversarial Taboo is 5. All our experiments are conducted using 32 NVIDIA A100-SXM4 GPUs with 40GB memory. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The main results are shown in Figure 1, where each axis is normalized by the maximum performance value, representing the reasoning performance on a particular benchmark. For notation simplification, we call the LLM obtained after imitating learning as the IM model, and the LLM trained after the $i$ -th epoch of SPAG as SPAG- $i$ $(i=1,2,3)$ ). ", "page_idx": 6}, {"type": "text", "text": "Imitation Learning With imitation learning over the collected GPT-4 self-play episodes, both LLaMA-2-7B and Baichuan-2-13B have obtained uniform performance improvements on all the reasoning benchmarks. As shown in Figure 1, both gray regions are completely wrapped by the blueline polygons. Besides, as for the general language capacity, the imitation-learned (IM) LLaMA-2 model achieves a better MMLU performance than the original LLaMA-2 base in Table 2. Although the imitation result of Baichuan-2-13B on MMLU slightly underperforms the base model, the numerical difference is insignificant compared to the reasoning gains, which indicates the imitation learning of the GPT-4 game behaviors can improve LLM reasoning while preserving the general language capacity. In Table 1, we also report the IM models on two keyword-based games within non-adversarial setups: Imitation-20Q (on 20-Question) and Imitation-GuessCity (on Guess-MyCity). From the results, we find the IM model on Adversarial Taboo outperforms models trained on non-adversarial games, highlighting the effectiveness of the adversarial game setups for reasoning improvement. Besides, we report the Chain-of-Thought (CoT) reasoning results of LLaMA-2 (Lama2-Base-CoT) and Alpaca-2 (AlpacaSFT-3-CoT). Although the CoT method on LLaMA-2 reaches conspicuous performance on BBH and WinoGrande, the IM model can still surpass the CoT results in terms of overall reasoning performance (geometric mean). ", "page_idx": 6}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/266f96b9f020b20cf2296a44e089c3ef38c64b345623e9b9f80c7ff4a4b741c5.jpg", "table_caption": ["Table 2: Reasoning Performance of SPAG on Baichuan-2-13B "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Self-play Training After imitation learning, we conduct three epochs of SPAG training as described in Algorithm 2. As shown in Figure 1, on most of the reasoning benchmarks, both LLaMA-2-7B and Baichuan-2-13B have their performance steadily improved with the SPAG training epoch increasing. For LLaMA-2-7B, although the first-epoch SPAG model has a relatively lower performance than the imitation-learned (IM) model on WinoGrande, after an additional epoch of self-play iteration, SPAG-2 has achieved sufficient improvement to surpass the performance of the IM model. Considering the general language capability on MMLU, SPAG models can not guarantee continuous improvement, especially for Baichuan-2-13B whose performance slightly decays during the SPAG training. Compared to the improvements in the reasoning benchmarks, this language-understanding decline is still within an acceptable range, since the overall performance (GM score) maintains increasing significantly. For the baseline comparison, we report the continuous SFT on the IM models (IM+AlpacaSFT) and self-played models on non-adversarial games (SP-20Q and SP-GuessCity). On both LLaMA-2 and Baichuan-2, continuous SFT models have lower performance scores compared with the corresponding SPAG-1 models. For non-adversarial self-play, SP-GuessCity even performs worse than Imitation-GuessCity, which with a high probability is because the game Guess-My-City has a more narrow topic range, insufficient to comprehensively improve the general LLM capacity. ", "page_idx": 7}, {"type": "text", "text": "Ablation Study Since the SPAG training loss includes the SFT data, we conduct an ablation study to test whether the performance gains on reasoning benchmarks come from the SFT data or the SPAG method. More specifically, we follow the training setups in Alpaca [Taori et al., 2023] and conduct SFT on LLaMA-2-7B and Baichuan-2-13B with three epochs. The checkpoint after the ith-epoch training is denoted as AlpacaSFT- $^{\\cdot i}$ $(i=1,2,3)$ ) and tested on the same evaluation sets. The SFT models\u2019 performances are also reported in Figure 1 and Table $1\\ \\&\\ 2$ . With the LLaMA base, our SPAG-3 model can uniformly defeat the SFT baselines with clear performance gaps on all benchmarks. With the Baichuan base, except the Mutual set, the SPAG models maintain noticeable advantages on other metrics. Considering the distinct surpass of overall performance, we claim that the major contribution to the reasoning improvements should be credited to the SPAG scheme. ", "page_idx": 7}, {"type": "text", "text": "Moreover, we test the sample efficiency and hyper-parameter effectiveness of SPAG in Figure 3. For sample efficiency during imitation learning, we vary the imitation data size by collecting the GPT-4 game episodes on target words with top- $\\cdot i\\mathbf{K}$ frequency $(i\\,=\\,1,2,3,5,10,15,20,30)$ . The ablation results are shown in the first column of Figure 3. When game episode size increases larger than 5K, imitation from GPT-4 cannot significantly provide additional reasoning gains. For the KL coefficient $\\beta_{1}$ , we test values in range $[0,0.4]$ . In figure 3 second column, we found KL coefficients around $\\beta_{1}=0.1$ have more satisfying performance regarding reasoning improvements. For self-play ablation in the most right column, we find that when the SFT coefficient $\\alpha>0.5$ , it cannot bring remarkable benefits for reasoning improvements. The KL coefficient of the SPAG loss reaches the best performance with values around $\\beta_{2}=0.2$ . As for sample efficiency (third column of Figure 3), we find the performance gain from the increasing episode number is not as significant as in the imitation learning stage. However, more self-play episodes still bring higher reasoning scores. ", "page_idx": 7}, {"type": "image", "img_path": "oCGkSH7ys2/tmp/b58cf91f605c7d1d402760e480dd4e2cdfa51c8c565bd185eac64b1d136a8919.jpg", "img_caption": ["Imitation Learning ", "Figure 3: Ablation study of hyper-parameters and data efficiency on imitation learning and first-epoch self-play training. The geometric mean (GM) scores overall reasoning benchmarks are reported. For episode-size ablations, the $\\Chi$ -axis is in the logarithmic scale. ", "Self-play Training "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "oCGkSH7ys2/tmp/d8338a1b366b80c6d93ca89a55e93e759446c2e9b59771b8053197bc45d3fe8c.jpg", "img_caption": ["Figure 4: Game results on the testing word list. Left: average win rates of SPAG models playing against GPT-4. Right: average win rate of SPAG attackers against different-epoch checkpoints. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Game-Play Performance Besides the evaluation of LLMs\u2019 natural language abilities, we review the models\u2019 game-play performance in terms of the win rates in the testing set $\\mathcal{V}_{\\mathrm{test}}$ . We first test our IM models and SPAG models with GPT-4 as the opponent. For each testing word in $\\mathcal{V}_{\\mathrm{test}}$ , we play the language game twice, once GPT-4 as the attacker, and GPT-4 as the defender for another time. The average win rates are reported in the left-hand-side plot of Figure 4, in which one can observe uniform and continuous win rate improvement of SPAG models playing with GPT-4. We also let SPAG models play the game against each other and report the attacker win rate in the right-hand-side plot of Figure 4, in which we find the defender\u2019s performance continuously enhanced along with the SPAG epoch increasing. Besides, we also provide the self-play statistics including interaction number and average utterance length in supplementary Figure 6, in which both players choose to use less communication to achieve victory. Self-play examples are attached in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Towards more efficient reasoning-improving methods for LLMs, we introduce a novel training strategy called Self-Play learning in Adversarial language Game (SPAG). In our method, a given LLM first learns to act as an attacker and a defender to play the language game named Adversarial Taboo via imitation learning. Next, we collect self-play episodes from the LLM playing against a copy of itself in the adversarial game. Finally, the LLM is further reinforced on the selected winning episodes via our SPAG algorithm. We repeat this self-play & reinforcement process for three epochs and find that the LLM\u2019s reasoning performance continuously and uniformly improves on various benchmarks. The SPAG algorithm explores a new path to improve the fundamental capabilities of LLMs from the perspective of multi-agent self-play. With more elaborate language game designs under more comprehensive task setups, we believe the self-play approaches have great potential for developing a broader range of advanced language abilities of LLMs. ", "page_idx": 8}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Due to the limited computational resources, we only verified the effectiveness of the SPAG method on two open-source LLMs, LLaMA-2-7B and Baichuan-2-13B. The SPAG performances for LLMs with larger sizes have not been empirically evaluated. Besides, more effective estimation methods for the value function and advantages remain unexplored in the SPAG training. For example, Monte Carlo tree search (MCTS) [Coulom, 2006] can be applied to the value function estimation of the Adversarial Taboo game. Also, actor-critic algorithms [Konda and Tsitsiklis, 1999] can provide more accurate policy gradient estimation with lower variances, which have not been tested on SPAG. ", "page_idx": 9}, {"type": "text", "text": "Although self-playing adversarial language games can continuously improve the reasoning performance of LLMs, we haven\u2019t conducted sufficient studies about the harmful impact of this adversarial self-play training on LLMs. It remains unclear whether LLMs have learned unsafe behaviors such as cheating, bluffing, or other disgraceful tricks to win the adversarial games. ", "page_idx": 9}, {"type": "text", "text": "7 Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "From our experiments, we have found the LLM capacities on a particular task can be continuously enhanced through self-play training. This indicates that we are closer to the LLMs\u2019 AlphaGo Zero moment: the intelligence level of AI agents can rapidly surpass human beings by self-playing on a particular language task without any supervision, as which already happened on the GO game [Silver et al., 2017]. By designing various self-play environments for LLMs, we can expect that the LLMs can comprehensively surpass humans in terms of intelligence level. This raises the urgency to study the methods to ensure the safety of such super-AIs. Although some of the works have already been devoted to this direction, such as the SuperAlignment [Burns et al., 2023] from OpenAI, more research alertness is required from the LLM community. Besides, within adversarial language games such as Adversarial Taboo, LLMs have great potential to develop harmful language tricks (such as cheating and bluffing) to achieve victory. We warn developers to make security checks on the self-played LLMs as exhaustively as possible. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023.   \nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm-2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In The Twelfth International Conference on Learning Representations, 2023.   \nAshutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, and Mark Riedl. Leftoverlunch: Advantage-based offline reinforcement learning for language models. In International Conference on Learning Representations, 2024.   \nSteven Bird. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69\u201372, 2006.   \nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.   \nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024.   \nPengyu Cheng, Jiawen Xie, Ke Bai, Yong Dai, and Nan Du. Everyone deserves a reward: Learning customized human preferences. arXiv preprint arXiv:2309.03126, 2023a.   \nPengyu Cheng, Yifan Yang, Jian Li, Yong Dai, and Nan Du. Adversarial preference optimization. arXiv preprint arXiv:2311.08045, 2023b.   \nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402, 2023.   \nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.   \nRe\u00b4mi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 72\u201383. Springer, 2006.   \nLeyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. Mutual: A dataset for multi-turn dialogue reasoning. In Proceedings of the 58th Conference of the Association for Computational Linguistics. Association for Computational Linguistics, 2020.   \nMark Davies. COCA: Corpus of contemporary american english, 2020. URL https://www.engl ish-corpora.org/coca/.   \nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought generation. arXiv preprint arXiv:2311.04254, 2023.   \nGuanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023a.   \nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023b.   \nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.   \nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.   \nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\u02c6ot\u00b4e, and Xingdi Yuan. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903\u20137910, 2020.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. CoRR, abs/2009.03300, 2020. URL https://arxiv.org/abs/2009.03300.   \nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745, 2023. ", "page_idx": 11}, {"type": "text", "text": "Jan Koco\u00b4n, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd\u0142o, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. Chatgpt: Jack of all trades, master of none. Information Fusion, 99:101861, 2023. ", "page_idx": 11}, {"type": "text", "text": "Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999. ", "page_idx": 11}, {"type": "text", "text": "Solomon Kullback. Information theory and statistics. Courier Corporation, 1997. ", "page_idx": 11}, {"type": "text", "text": "Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end learning of negotiation dialogues. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2443\u20132453, 2017. ", "page_idx": 11}, {"type": "text", "text": "Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157\u2013163. Elsevier, 1994. ", "page_idx": 11}, {"type": "text", "text": "Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. Logiqa 2.0 \u2014 an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pages 1\u201316, 2023. doi: 10.1109/TASLP.2023.3293046. ", "page_idx": 11}, {"type": "text", "text": "Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the TwentyNinth International Conference on International Joint Conferences on Artificial Intelligence, pages 3622\u20133628, 2021. ", "page_idx": 11}, {"type": "text", "text": "Steven Loria et al. textblob documentation. Release 0.15, 2(8):269, 2018. ", "page_idx": 11}, {"type": "text", "text": "Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, and Yaodong Yang. Red teaming game: A game-theoretic framework for red teaming language models. arXiv e-prints, pages arXiv\u20132310, 2023. ", "page_idx": 11}, {"type": "text", "text": "Radford M Neal. Annealed importance sampling. Statistics and computing, 11:125\u2013139, 2001. ", "page_idx": 11}, {"type": "text", "text": "Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International conference on machine learning, pages 3878\u20133887. PMLR, 2018. ", "page_idx": 11}, {"type": "text", "text": "OpenAI. ChatGPT, Mar 14 version. https://chat.openai.com/chat, 2023a. ", "page_idx": 11}, {"type": "text", "text": "OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023b. ", "page_idx": 11}, {"type": "text", "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730\u201327744, 2022. ", "page_idx": 11}, {"type": "text", "text": "Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. ", "page_idx": 11}, {"type": "text", "text": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023. ", "page_idx": 11}, {"type": "text", "text": "Vyas Raina, Adian Liusie, and Mark Gales. Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. arXiv preprint arXiv:2402.14016, 2024. ", "page_idx": 11}, {"type": "text", "text": "Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiante\u00b4 Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In International Conference on Learning Representations, 2023. ", "page_idx": 11}, {"type": "text", "text": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019. ", "page_idx": 12}, {"type": "text", "text": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015.   \nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations, 2016.   \nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.   \nAvi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023.   \nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\\`a Garriga-Alonso, and et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022.   \nNigar M Shafiq Surameery and Mohammed Y Shakor. Use chat gpt to solve programming bugs. International Journal of Information Technology & Computer Engineering (IJITC) ISSN: 2455- 5290, 3(01):17\u201322, 2023.   \nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3: 9\u201344, 1988.   \nMirac Suzgun, Nathan Scales, Nathanael Scha\u00a8rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.   \nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.   \nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nHaoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, and Tegawende\u00b4 F Bissyand\u00b4e. Is chatgpt the ultimate programming assistant\u2013how far is it? arXiv preprint arXiv:2304.11938, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36, 2024.   \nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michae\u00a8l Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \nShuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu. Enhance reasoning for large language models in the game werewolf. arXiv preprint arXiv:2402.02330, 2024.   \nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023.   \nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023a.   \nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond. ACM Transactions on Knowledge Discovery from Data, 2023b.   \nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \nYuan Yao, Haoxi Zhong, Zhengyan Zhang, Xu Han, Xiaozhi Wang, Kai Zhang, Chaojun Xiao, Guoyang Zeng, Zhiyuan Liu, and Maosong Sun. Adversarial language games for advanced natural language intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14248\u201314256, 2021.   \nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.   \nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.   \nDun Zeng, Yong Dai, Pengyu Cheng, Tianhao Hu, Wanshun Chen, Nan Du, and Zenglin Xu. On diverse preferences for large language model alignment. arXiv preprint arXiv:2312.07401, 2023.   \nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023a.   \nYizhe Zhang, Jiarui Lu, and Navdeep Jaitly. The entity-deduction arena: A playground for probing the conversational reasoning and planning capabilities of llms. arXiv preprint arXiv:2310.01468, 2023b. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Data Collection Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 GPT-4 Data Collection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To collect GPT-4 self-play data for imitation learning, we use the following system prompts to let GPT-4 play as the attacker and defender for the next-turn utterances. ", "page_idx": 14}, {"type": "text", "text": "- Attacker System Prompt: ", "page_idx": 14}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/4a3388683451a3f8f28c208e1680e1f133c4c19c5b05515344b4eb68d6a068c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "- Defender System Prompt: ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/b0a1671d39b002624b86009d5949152fd829025252debf8b782148f8e739e33f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "There are two slots in the system prompts: \u201cmax turns\u201d and \u201ctarget word\u201d. \u201cmax turns\u201d is an integer number randomly selected from [3, 8]. \u201ctarget word\u201d is filled by the target word of the game. ", "page_idx": 14}, {"type": "text", "text": "A.2 Target Words Collection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use the top $50\\mathrm{k}$ words with the highest frequency from the Corpus of Contemporary American (CoCA) [Davies, 2020]. The words are sorted by frequency. Words with less than two letters are removed. Next, we remove the stop words using the NLTK package [Bird, 2006]. For GPT-4 game collection, we use the top $30\\mathrm{k}$ words from the sorted word list. For the collection of self-play episodes, we use the whole $50\\mathrm{k}$ words for open-source LLMs to play. For the testing words, we follow the game setups of 20-Question in [Abdulhai et al., 2023] and use the same 168 object words selected from a vast range of topics. ", "page_idx": 14}, {"type": "text", "text": "A.3 LLM Self-play Collection ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For training and inference of IM models and SPAG models, we use the following attacker and defender prompts. ", "page_idx": 15}, {"type": "text", "text": "- Attacker Prompt: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Play the game of Adversarial Taboo. In this game, there are two players, an attacker and a defender. At the beginning, the attacker is assigned a target word, with which the defender is not informed. The task of the attacker is to induce the defender to utter the target word unconsciously, then the attacker wins the game. However, the attacker is not allowed to utter the target word, otherwise the attacker loses. At the same time, the defender tries to figure out the target word. If the defender identifies the target word, it can say \u201dI know the word! It is \u2018target word\u2018!\u201d. Then the defender wins. Remember, the defender can only guess the word once, otherwise, it will directly lose the game.   \nThe game has $\\{\\mathrm{max.turns}\\}$ turns. if nobody wins during $\\{\\mathrm{max.turns}\\}$ turns, there is a tie.   \n### Game History: {history}   \n### Instruction: You are the attacker. The target word is \u2018{target word}\u2018. Provide your response for the next turn.   \n### Response: ", "page_idx": 15}, {"type": "text", "text": "- Defender Prompt: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Play the game of Adversarial Taboo. In this game, there are two players, an attacker and a defender. At the beginning, the attacker is assigned a target word, which the defender is not informed of. The task of the attacker is to induce the defender to utter the target word unconsciously, then the attacker wins the game. However, the attacker is not allowed to utter the target word, otherwise, the attacker loses. At the same time, the defender tries to figure out the target word. If the defender identifies the target word, it can say \u201dI know the word! It is \u2018target word\u2018!\u201d. Then the defender wins. Remember, the defender can only guess the word once, otherwise, it will directly lose the game.   \nThe game has $\\{\\mathrm{max.turns}\\}$ turns. if nobody wins during $\\{\\mathrm{max.turns}\\}$ turns, there is a tie.   \n### Game History: {history}   \n### Instruction: You are the defender. Provide your response to infer the target word.   \n### Response: ", "page_idx": 15}, {"type": "text", "text": "To prevent LLMs from being over-ftited to a single prompt template during the self-play training, we rewrite the above prompt into 8 different expressions via GPT-4. Besides, we randomly select the instruction-tuning and chat formats to enhance the text diversity for the training query-response pairs. ", "page_idx": 15}, {"type": "text", "text": "B Reward Design Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We heuristically design rewards to ensure that $R(\\tau)\\,=\\,1$ if the attacker wins, $R(\\tau)=-1$ if the defender wins, and $\\bar{\\boldsymbol{R}(\\tau)}=0$ for ties. ", "page_idx": 15}, {"type": "text", "text": "We design the reward function $r$ with the following heuristic rules: ", "page_idx": 15}, {"type": "text", "text": "\u2022 For each episode $\\boldsymbol{\\tau}=(s_{0},s_{1}^{\\prime},s_{1},\\dots,s_{T}^{\\prime},s_{T})$ , the attacker reward and the defender reward at $t$ -th turn have $r(s_{t-1},\\boldsymbol{u}_{t})=-r(s_{t}^{\\prime},\\boldsymbol{v}_{t})$ , so that the game is zero-sum: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}r(\\pmb{s}_{t-1},\\pmb{u}_{t})+\\sum_{t=1}^{T}r(\\pmb{s}_{t}^{\\prime},\\pmb{v}_{t})=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 The attacker total reward $R(\\tau)\\,>\\,0$ if the attacker wins, $R(\\tau)<0$ if the defender wins, and $R(\\tau)=0$ if there is a tie. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Actions closer to the end of the game should have larger reward weights. Because they are semantically more related to the defender\u2019s guesses or mistakes, which have larger impacts on the final game outcome. Hence, we introduce a decay weight $\\gamma\\in(0,1)$ such that $\\gamma\\cdot r(\\bar{\\pmb{s}}_{t},\\pmb{u}_{t+1})=$ $r(s_{t-1},\\mathbf{\\boldsymbol{u}}_{t})$ and $\\gamma\\cdot r\\big(s_{t+1}^{\\prime},v_{t+1}\\big)=r\\big(s_{t}^{\\prime},v_{t}\\big)$ . Then $\\{r(s_{t-1},\\pmb{u}_{t}\\}_{t=1}^{T}$ and $\\{r(\\mathbf{\\boldsymbol{s}}_{t}^{\\prime},\\mathbf{\\boldsymbol{\\mathbf{\\boldsymbol{v}}}}_{t})\\}_{t=1}^{T}$ become two geometrical sequences whose importance enlarges when dialogue turns increase. \u2022 To improve the LLM training stability, we normalized the total reward $R$ to have norm $|R(\\tau)|=1$ if $R(\\tau)\\neq0$ . ", "page_idx": 16}, {"type": "text", "text": "Based on the above rules, given a complete trajectory, we can assign the reward for each action: ", "page_idx": 16}, {"type": "text", "text": "$\\left\\{\\begin{array}{l l}{r(s_{t-1},u_{t})=(1-\\gamma)\\gamma^{T-t}/(1-\\gamma^{T+1}),}&{r(s_{t}^{\\prime},v_{t})=-r(s_{t-1},u_{t}),}\\\\ {r(s_{t-1},u_{t})=-(1-\\gamma)\\gamma^{T-t}/(1-\\gamma^{T+1}),}&{r(s_{t}^{\\prime},v_{t})=-r(s_{t-1},u_{t}),}\\\\ {r(s_{t-1},u_{t})=0,}&{r(s_{t}^{\\prime},v_{t})=0,}\\end{array}\\right.$ , if attacker wins. , if defender wins. if game is tied. ", "page_idx": 16}, {"type": "text", "text": "Note that the above reward design naturally encourages the players to interact less to win the game. ", "page_idx": 16}, {"type": "text", "text": "C Evaluation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Reasoning Evaluation Benchmark ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For reasoning benchmark evaluation, we use a publicly available code repository called Language Model Evaluation Harness [Gao et al., 2023].We run the evaluation with $\\mathrm{^{\\star}d t y p e=f l o a t16^{\\circ}}$ and \u201cmax gen tokens $\\mathsf{=}1024^{\\circ}$ to make the evaluation faster and more stable. More specifically, we have modified the flitering parameters of BBH by enabling \u201cremove whitespace\u201d to remove the spaces at the beginning of the model output, increasing the pattern-match accuracy of the generated LLM responses. Following the LLaMA-2 paper [Touvron et al., 2023], we report 5-shot results for MMLU, 3-shot for BBH, and 0-shot results for all other benchmarks. The detailed descriptions of benchmarks are listed below: ", "page_idx": 16}, {"type": "text", "text": "\u2022 MMLU [Hendrycks et al., 2020] is a massive multi-task test set consisting of multiple-choice questions from various branches of knowledge, requiring the model to possess extensive world knowledge and problem-solving ability.   \n\u2022 BIG-Bench Hard (BBH) [Suzgun et al., 2022] consists of 23 most hard tasks in BIG-Bench [Srivastava et al., 2022], on which humans perform better than large language models.   \n\u2022 Mutual [Cui et al., 2020] is a dataset for multi-turn dialogue reasoning modified from Chinese high school English listening comprehension exams.   \n\u2022 ARC [Clark et al., 2018] consists of 7,787 questions and is partitioned into challenge set ARCchallege (ARC-c) and simple set ARC-easy (ARC-e), which require strong knowledge and reasoning abilities to solve.   \n\u2022 LogiQA2 [Liu et al., 2023] is a revision and re-annotation of LogiQA [Liu et al., 2021], a logical reasoning reading comprehension dataset. LogiQA2 has increased data volume, utilized manual translation, and removed cultural features.   \n\u2022 WinoGrande [Sakaguchi et al., 2019] (WGrande) is a commonsense reasoning dataset consisting of 44k questions with binary options.   \n\u2022 PIQA [Bisk et al., 2020] is a dataset designed to test the physical commonsense reasoning ability of LLM. ", "page_idx": 16}, {"type": "text", "text": "C.2 Rule-based Game Outcome Judgement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To build up the rule-based game outcome judgement, for each target word, we first list its derivative words including the plural form (using TextBlob [Loria et al., 2018]), and the change of tenses (i.e. adding suffices such as \u201cing\u201d, \u201ced\u201d etc). For the attacker, if any word form in the form list appears in its utterance, we determined that it breaks the rules because the attacker is not allowed to directly speak the target word. For the defender, we first check whether any word form appears in its statements. If any form of the target word exists, the defender loses. If not, we further check whether the defender has made the prediction. If the prediction is correct, we mark a defender-winning game. Otherwise, if the predicted word is wrong, we claim that the attacker wins the game. ", "page_idx": 16}, {"type": "image", "img_path": "oCGkSH7ys2/tmp/e4b81dd2a379dffd8e727df270d0b8940c11321b00c200bc61e8576360d16386.jpg", "img_caption": ["Figure 5: Game results on the old-version testing word list. Left: average win rates of SPAG models playing against GPT-4. Right: average win rate of SPAG models playing as the attacker against different-epoch checkpoints. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "oCGkSH7ys2/tmp/5af2618ebe391ff95bfa3babffc5403e8be369ce3a02ed924054270c62556487.jpg", "img_caption": ["Figure 6: Through the training process, the number of interactions continuously decreased, while the average length of utterances continuously becomes shorter. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Advantange Value Estimation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We estimate the advantage $A_{t}^{\\mu_{\\bar{\\theta}}}$ and $A_{t}^{\\nu_{\\bar{\\theta}}}$ using the Temporal Difference (TD) residual [Sutton, 1988]: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{A}_{t}^{\\mu_{\\bar{\\theta}}}=r(s_{t-1},{\\boldsymbol u}_{t})+V^{\\mu_{\\bar{\\theta}}}({\\boldsymbol s}_{t})-V^{\\mu_{\\bar{\\theta}}}({\\boldsymbol s}_{t-1}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here ${V}^{\\mu_{\\bar{\\theta}}}(s)$ is the value function for $\\mu_{\\bar{\\theta}}$ based on the self-play of $\\mu_{\\bar{\\theta}}\\times\\nu_{\\bar{\\theta}}$ , which is based on the same language and reasoning ability of LLM $\\pi_{\\bar{\\theta}}$ . To further simplify the advantage calculation, we make a reasonable approximation that for all states, the value function of self-plays of the same LLM equals 0 for the same levels of game-playing of the attacker and the defender. This leads to $\\hat{A}_{t}^{\\mu_{\\bar{\\theta}}}\\approx r(s_{t-1},\\mathbf{\\boldsymbol{u}}_{t})$ . Similarly, for the defender advantage, we have approximation $\\hat{A}_{t}^{\\nu_{\\bar{\\theta}}}\\approx=-r\\big(s_{t}^{\\prime},{\\mathbf{v}}_{t}\\big)$ . Here the defender advantage has a negative sign because the origin reward $r$ is defined from the perspective of the attacker (the defender gets reward $-r(s_{t}^{\\prime},v_{t})$ in the zero-sum game). ", "page_idx": 17}, {"type": "text", "text": "E Non-adversarial Game Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We consider two non-adversarial games as baselines, 20-Question and Guess-My-City. Both games are also target-word-based. Imitation learning and the first-epoch self-play are conducted in these two games respectively. The imitation data we used is also collected with GPT-4 from Abdulhai et al. [2023]. For each game, $20\\mathbf{k}$ game history dialogues are extracted from this dataset as the same data size to the SPAG imitation. Then, each imitation-learned model is used to sample 10k game episodes via self-play with the same Algorithm 1. Then each model is further trained with the SPAG loss. ", "page_idx": 17}, {"type": "text", "text": "Game rule descriptions to the non-adversarial games: ", "page_idx": 17}, {"type": "text", "text": "\u2022 20-Question: one player (the oracle) thinks of an object, while the other player (the guesser) attempts to identify it by asking a series of yes-or-no questions. The game will have 20 rounds. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Guess-My-City: one player (the oracle) thinks of a city, while the other player (the guesser) attempts to identify it by asking a series yes/no questions and open-ended questions. The game will have 20 rounds. ", "page_idx": 18}, {"type": "text", "text": "F Additional Experimental Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the SPAG model testing word list, we initially use a self-defined testing set, which includes 150 target words manually selected from diverse topics. The old-version game performances are reported in Figure 5. Besides, we show the interaction numbers and average utterance length in Figure 6. Through the self-play training, the number of dialogue rounds continuously decreases. With the SPAG training epoch increasing, the length of the players\u2019 utterances decreases. ", "page_idx": 18}, {"type": "text", "text": "G Self-play Examples ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/286fd3f2ed4f0f209dbec5f5b51c333b716cb3c09cc392ed1d27f131cdc41ca9.jpg", "table_caption": [], "table_footnote": ["Table 3: Self-play examples of the IM model based on LLaMA-2-7B. "], "page_idx": 18}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/38e44005a024156f4421d8346d9ced54856632a54ac1785611beabf6b6365f7d.jpg", "table_caption": [], "table_footnote": ["Table 4: Self-play examples of SPAG-1 model based on LLaMA-2-7B. "], "page_idx": 19}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/e9ffa193dd3fda5ed9bb33e7f118146088778130d7e7d18cebc82791eefcfe63.jpg", "table_caption": [], "table_footnote": ["Table 5: Self-play examples of SPAG-2 model based on LLaMA-2-7B. "], "page_idx": 19}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/e7886e89b420f1ccd37fecd3b458b3822c2795615e46a53157437419be582001.jpg", "table_caption": [], "table_footnote": ["Table 6: Self-play examples of SPAG-3 model based on LLaMA-2-7B. "], "page_idx": 20}, {"type": "table", "img_path": "oCGkSH7ys2/tmp/29f5f4933ee372f7160d9499991692ffce3e8d5d48d6183a2ebd4e6f64a242e8.jpg", "table_caption": [], "table_footnote": ["Table 7: Self-play examples of GPT-4. "], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 22}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 22}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 22}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 22}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \u201d[Yes] \u201d is generally preferable to \u201d[No] \u201d, it is perfectly acceptable to answer \u201d[No] provided a proper justification is given (e.g., \u201derror bars are not reported because it would be too computationally expensive\u201d or \u201dwe were unable to find the license for the dataset we used\u201d). In general, answering \u201d[No] \u201d or \u201d[NA] \u201d is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\u201d, \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have introduced the main methodology and contributions along with the experimental achievements of our work in the abstract and the introduction. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in Section 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not contain theoretical results as the main contribution. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided sufficient experimental details in both experimental section and supplementary materials for the reproduction of the reported results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the code for method implementation in the supplementary materials. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 24}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided sufficient training details and ablation studies in the experimental section. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer:[No] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We did not report the error bars because of the limited computational resources and the huge computational complexity for the large-scale language model training. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided the GPU version and the corresponding memory usage in the experimental setups. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have discussed the broader impacts of our work in Section 7. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We did not have enough research resources to build up the safeguards of our learned models. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have checked the licenses of the open-source LLMs, which are accessible for research usage. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]