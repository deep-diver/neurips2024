[{"figure_path": "oCGkSH7ys2/figures/figures_0_1.jpg", "caption": "Figure 1: Reasoning improvements from Self-Playing of Adversarial language Games (SPAG) on comprehensive reasoning benchmarks. With the SPAG epoch increasing, the LLM reasoning ability continuously improves. Each axis is normalized by the maximum answer-accuracy value.", "description": "This radar chart visualizes the performance improvement of LLMs on various reasoning benchmarks after undergoing self-play training using the Adversarial Taboo game.  Each axis represents a different benchmark (BBH, Mutual, ARC-e, ARC-c, LGQA2, WGrande, PIQA), and the values are normalized.  The chart shows three data points for each LLM (LLaMA-2-7B and Baichuan-2-13B): the baseline, and the performance after one and two epochs of self-play training.  The results indicate consistent performance gains across the benchmarks as the number of training epochs increase.", "section": "Self-play of Adversarial Language Games"}, {"figure_path": "oCGkSH7ys2/figures/figures_8_1.jpg", "caption": "Figure 1: Reasoning improvements from Self-Playing of Adversarial language Games (SPAG) on comprehensive reasoning benchmarks. With the SPAG epoch increasing, the LLM reasoning ability continuously improves. Each axis is normalized by the maximum answer-accuracy value.", "description": "This figure shows how the performance of several large language models (LLMs) improves on various reasoning benchmarks after undergoing self-play training using an adversarial language game called Adversarial Taboo.  The x-axis of each subplot represents a different benchmark, and the y-axis shows the normalized reasoning accuracy. The different colored lines represent different stages of training (epochs) of the SPAG method.  The figure visually demonstrates that iterative self-play enhances the LLMs' reasoning capabilities across multiple benchmarks.", "section": "Introduction"}, {"figure_path": "oCGkSH7ys2/figures/figures_8_2.jpg", "caption": "Figure 1: Reasoning improvements from Self-Playing of Adversarial language Games (SPAG) on comprehensive reasoning benchmarks. With the SPAG epoch increasing, the LLM reasoning ability continuously improves. Each axis is normalized by the maximum answer-accuracy value.", "description": "This figure shows the improvement in reasoning ability of LLMs after training with the Self-Playing Adversarial Language Game (SPAG) method.  The radar chart displays the performance on several reasoning benchmarks (MMLU, BBH, Mutual, ARC-e, ARC-c, LGQA2, WGrande, PIQA) before SPAG training (LLaMA-2-7B, Baichuan-2-13B, AlpacaSFT baselines) and after 1, 2, and 3 epochs of SPAG training. The improvement is consistent across all benchmarks, indicating that SPAG enhances LLM reasoning abilities.", "section": "3 Self-play of Adversarial Language Games"}, {"figure_path": "oCGkSH7ys2/figures/figures_17_1.jpg", "caption": "Figure 1: Reasoning improvements from Self-Playing of Adversarial language Games (SPAG) on comprehensive reasoning benchmarks. With the SPAG epoch increasing, the LLM reasoning ability continuously improves. Each axis is normalized by the maximum answer-accuracy value.", "description": "This figure shows the improvements in reasoning abilities of LLMs (Large Language Models) after undergoing self-play training using an adversarial language game called Adversarial Taboo.  The radar charts display performance across multiple reasoning benchmarks (BBH, Mutual, ARC-e, ARC-c, LGQA2, WGrande, PIQA).  Each point represents the performance on a specific benchmark, with the axes normalized. The three SPAG-epoch lines show that performance consistently improves with each iteration of the self-play training process.", "section": "Self-play of Adversarial Language Games"}, {"figure_path": "oCGkSH7ys2/figures/figures_17_2.jpg", "caption": "Figure 1: Reasoning improvements from Self-Playing of Adversarial language Games (SPAG) on comprehensive reasoning benchmarks. With the SPAG epoch increasing, the LLM reasoning ability continuously improves. Each axis is normalized by the maximum answer-accuracy value.", "description": "This figure shows the performance improvement of Large Language Models (LLMs) after undergoing self-play training using an adversarial language game called Adversarial Taboo.  The graph displays the improvements across several reasoning benchmarks (MMLU, BBH, Mutual, ARC-e, ARC-c, LGQA2, WGrande, PIQA) after 1, 2, and 3 epochs of self-play. The y-axis represents the normalized reasoning score, and the higher the score, the better the performance on the benchmark. Each axis is normalized using the maximum answer accuracy value observed for that specific benchmark, providing a comparable view of the relative improvements across benchmarks.", "section": "Self-play of Adversarial Language Games"}]