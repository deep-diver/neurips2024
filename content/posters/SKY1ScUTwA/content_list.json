[{"type": "text", "text": "The Intelligible and Effective Graph Neural Additive Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maya Bechler-Speicher Amir Globerson Blavatnik School of Computer Science Blavatnik School of Computer Science Tel-Aviv University Tel-Aviv University\u2217 ", "page_idx": 0}, {"type": "text", "text": "Ran Gilad-Bachrach   \nDepartment of Bio-Medical Engineering   \nEdmond J. Safra Center for Bioinformatics Tel-Aviv University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have emerged as the predominant approach for learning over graph-structured data. However, most GNNs operate as black-box models and require post-hoc explanations, which may not suffice in high-stakes scenarios where transparency is crucial. In this paper, we present a GNN that is interpretable by design. Our model, Graph Neural Additive Network (GNAN), is a novel extension of the interpretable class of Generalized Additive Models, and can be visualized and fully understood by humans. GNAN is designed to be fully interpretable, offering both global and local explanations at the feature and graph levels through direct visualization of the model. These visualizations describe exactly how the model uses the relationships between the target variable, the features, and the graph. We demonstrate the intelligibility of GNANs in a series of examples on different tasks and datasets. In addition, we show that the accuracy of GNAN is on par with black-box GNNs, making it suitable for critical applications where transparency is essential, alongside high accuracy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many domains, ranging from biology to fraud detection, Artificial Intelligence (AI) is applied to data with graph structure. Neural Networks, and specifically Graph Neural Networks (GNNs), have emerged as the predominant approach in these applications (see, for example, Zhou et al. [1]). While GNNs demonstrate high accuracy, in terms of the correctness of their predictions, they often function as black-box models; thus, their decision-making processes are opaque. Transparency is vital for assessing potential biases or safety risks, and is particularly critical in high-stakes areas such as criminal justice, healthcare, and finance, where decisions significantly impact individuals\u2019 lives. In such contexts, interpretable models, despite sometimes being less accurate, may be preferred over complex black-box models [2]. Furthermore, the transparency of automated decision making processes is increasingly becoming a legal mandate. While there is ongoing debate over whether the European Union\u2019s General Data Protection Regulation (GDPR) implies a \u201cright to explanation\u201d [3, 4], the proposed European AI Act explicitly addresses this issue, stating that \u201cTo address concerns related to opacity and complexity of certain AI systems and help deployers to fulflil their obligations under this regulation, transparency should be required for high-risk AI systems before they are placed on the market or put into service\u201d [5]. ", "page_idx": 0}, {"type": "text", "text": "In this context, interpretability refers to the ease with which a human can understand the reasoning behind model decisions or the general logic of a model\u2019s operation. It is important to distinguish between interpretability and explainability [2]. Interpretability relates to models that are inherently comprehensible by design, while explainability pertains to post-hoc methods that elucidate aspects of black-box models [6]. These explanations often come without correctness guarantees [7, 8] and may not provide a complete description of the model and its predictions, potentially failing to expose hidden pitfalls [9, 10, 11]. ", "page_idx": 1}, {"type": "text", "text": "Methods for model explainability or interpretability can be categorized into local and global types. Local methods, such as SHAP [12] and LIME [13], elucidate individual predictions made by a model, whereas global methods, such as feature-importance [14] and partial dependence plots [15], provide holistic insights about the model, i.e., explain the overarching logic of the model decision-making [16]. However, it has been noted that local explainability methods may not consistently align with their global counterparts [17]. Moreover, local explanations may be inadequate for verifying fairness and other risks [8]. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce the Graph Neural Additive Networks (GNAN), an interpretable-by-design GNN that offers both transparency and accuracy. GNAN is a glass-box model [18] that allows for both local and global interpretability. GNAN extends the family of Generalized Additive Models (GAMs) [19], to accommodate graph data. GAMs are known for their ability to fti complex, nonlinear functions while remaining interpretable and have proven effective across various domains [20, 21, 22, 23, 24]. They operate by learning shape functions for each feature and then linearly combining these functions, making it easy to interpret them, as the influence of each feature on the prediction is independent of other features and can be visualized through their corresponding shape functions. Similarly, GNAN\u2019s interpretability is achieved through an architecture that restricts the use of crossproducts of features and graphs\u2019 topology, thereby reducing its complexity compared to other GNNs. Nonetheless, we demonstrate that GNAN, despite its limited capacity, matches the performance of more expressive GNNs on several real-world datasets. Additionally, GNAN does not rely on iterative local message-passing, avoiding the computational bottlenecks commonly associated with such GNNs [25]. ", "page_idx": 1}, {"type": "text", "text": "In Section 4, we showcase through a series of examples how users can interpret GNAN and gain precise insights into the connections between the target and the graph, the target and the features, and the interplay between features and graph information. In some cases, an exact description of the model can be visualized through only a few figures. We also demonstrate how the interpretability of GNAN allows users to debug their model, a process that can be used for ensuring consistency with prior knowledge and avoiding biases and safety risks. In Section 5, we compare the performance of GNAN with other GNN architectures. This comparison underscores that sacrificing performance for intelligibility is not necessary, as the performance of GNAN is comparable to that of commonly used black-box GNNs. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this work are: ", "page_idx": 1}, {"type": "text", "text": "1. The extension of Generalized Additive Models (GAMs) to graph data. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2. The introduction of a fully interpretable-by-design model for graph prediction tasks, demonstrating that its explanations provide both global and local insights, through visualizations of the model itself, and include debugging capabilities. 3. The demonstration that GNAN achieves good performance on common real-world graph datasets, despite its limited capacity. This observation supports previous findings that some real-world graph problems are simple and do not require the capacity of other GNNs. ", "page_idx": 1}, {"type": "text", "text": "Thus we argue that GNAN is suitable for high-stakes applications due to its interpretability and performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Generalized Additive Models Generalized Additive Models (GAMs) are a class of statistical models that build upon generalized linear models by incorporating non-linear functions for each variable while maintaining additivity [19, 20, 21]. Essentially, GAMs model the expected value of the target variable as a sum of univariate functions of the features. Formally, in GAMs, a prediction for an input $\\mathbf{x}$ is computed by $\\sigma\\left(\\sum f_{k}\\left(\\mathbf{x}_{k}\\right)\\right)$ where $\\sigma$ is a predefined activation function, such as the sigmoid2, and the $f_{k}$ \u2019s are shape functions learned during the training process. This approach extends generalized linear models, in which predictions are computed by $\\bar{\\sigma_{\\big(}}\\bar{\\sum}\\mathbf{w}_{k}\\mathbf{x}_{k}\\big)$ where $w$ is a learned weight vector. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "GAMs are more expressive than generalized linear models while remaining interpretable, as the effect of each predictor is modeled separately. For example, they can capture non-monotone effects of features, which generalized linear models cannot achieve without feature engineering. Traditionally, GAMs utilize splines or other smooth shape functions to model the non-linear relationships between each feature and the target variable. However, other methods, such as trees, have been proposed to fit the shape functions [24]. Recently, Agarwal et al. [26] suggested using neural networks to learn the shape functions. This approach combines the representational power of deep learning with the interpretability of additive models. ", "page_idx": 2}, {"type": "text", "text": "Graph Neural Networks Graph Neural Networks (GNNs) [27, 28, 29, 30] have emerged as the leading approach for learning over graph data. The fundamental idea behind GNNs is to use neural-networks that combine node features with graph-structure. A commonly used family of GNNs is message-passing GNNs, where the representations of nodes are updated in iterations through neighborhood aggregations. This aggregation is done, for example, through a convolution-like operation or an attention mechanism. [31, 32, 33] ", "page_idx": 2}, {"type": "text", "text": "Various non-message-passing approaches have been explored to disentangle the node features from the graph structure. Such approaches were shown to enhance performance across diverse applications [34, 35, 36]. Disentanglement can also reduce overftiting, as popular GNNs which do entangle features and graph-structure, were shown to have the tendency to overfti non-informative graph information [37] GNAN uses these concepts in order to achieve a model that is both high-performing and fully interpretable. ", "page_idx": 2}, {"type": "text", "text": "There are different prediction tasks on graphs [38]. In graph tasks, the goal is to predict properties of entire graphs. For example, a graph could represent a molecule, and the goal would be to predict its toxicity level. In node tasks, the goal is to predict a property of a node (vertex) within a graph. An example of a node task is predicting whether a user in a social network is a human or a bot. In link prediction tasks, the goal is to determine whether there is an edge between two nodes of a graph. In this work, we focus on graph tasks and node tasks. Although link prediction tasks are not within the scope of this work, it is possible to view these problems as node tasks on the dual line graph [39]. ", "page_idx": 2}, {"type": "text", "text": "GNNs explanations The inherent complexity of graph-structured data poses unique challenges for explainability. Most approaches for explaining black-box GNNs focus on providing a sub-graph or a similar structure that can explain a certain example. This is done either as a post-hoc explanation for GNNs [40, 41, 42] or by adjusting the data a priory [43, 44, 45]. For example, the method suggested in Ying et al. [41] identifies both important subgraph structures and node features influencing the GNN\u2019s predictions by maximizing the mutual information between the prediction and the distribution of possible subgraph structures and node features. Yin et al. [43] suggested a structural pattern learning module that is learning through pre-training. GNAN, on the contrary to these methods, does not aim to provide an explanation through a proxy object like a subgraph, nor does it require modification to the data, or the training process. Instead, GNAN is a interpretable by design, and its exact description can be visualized through its learned shape functions. In particular, the exact relation between the target, the features, and the graph can be visualized and conveyed to users. ", "page_idx": 2}, {"type": "text", "text": "3 Graph Neural Additive Networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce the Graph Neural Additive Networks (GNAN). We begin by defining some essential notation. A graph $G$ has a set of $N$ vertices, where each vertex is associated with a $d$ -dimensional feature vector. Specifically, $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ represents the feature vector of the $i$ \u2019th node in $G$ . We define the distance dist $(j,i)$ between node $j$ and node $i$ within the graph $G$ as the number of edges in the shortest path from $j$ to $i$ . This definition implies that the distance from a node to itself is zero. In cases where no path exists from $j$ to $i$ , we set dis $\\mathfrak{t}\\left(j,i\\right)\\,=\\,\\infty$ . For enhanced readability, we denote vectors in boldface, and an entry $k$ of a vector $\\mathbf{x}$ is denoted by $[\\mathbf{x}]_{k}$ . We begin by describing GNAN for applications such as binary classification and regression where the model output is one-dimensional. At the end of this section, we discuss extensions to scenarios such as multi-class classification, where the model output is multi-dimensional. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "GNAN generates a representation $\\mathbf{h}_{i}\\in\\mathbb{R}^{d}$ for each node $i$ by learning a distance function $\\rho(x;\\theta):$ $\\mathbb{R}\\to\\mathbb{R}$ and a set of feature shape functions $\\{f_{k}\\}_{k=1}^{d},f_{k}(x;\\theta_{k}):\\mathbb{R}\\to\\mathbb{R}$ . Each of these functions is a neural network, and is optimized through back-propagation. For brevity, we omit the parameterization $\\theta$ and $\\theta_{k}$ for the remainder of this section. In GNAN, the $k$ \u2019th entry of the representation $\\mathbf{h}_{i}$ for the $i$ \u2019th node is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n[\\mathbf{h}_{i}]_{k}=\\sum_{j=1}^{N}{\\frac{1}{\\#{\\mathrm{dist}}_{i}(j,i)}}\\cdot\\rho\\left({\\frac{1}{1+{\\mathrm{dist}}\\left(j,i\\right)}}\\right)\\cdot f_{k}\\left([\\mathbf{x}_{j}]_{k}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\#\\mathrm{dist}_{i}(j,i)$ represents the number of nodes at distance dist $(j,i)$ from node $i$ . The underlying rationale for this definition is as follows: each node\u2019s $k$ \u2019th feature is transformed by a shape function $f_{k}$ , independently from other features. The effect the $k$ \u2019th feature value of node $j$ has on node $i$ \u2019s representation is influenced by their distance. Specifically, if dist $(j,i)\\,=\\,l$ , then the cumulative influence of all nodes at distance $l$ from node $i$ is captured by $\\rho\\big({}^{1}\\!/(1\\!+\\!l)\\big)$ . This is achieved by the normalization term $1/\\#\\mathrm{dist}_{i}(j,i)$ . Here, $\\rho$ \u2019s argument $^{1}/(1{+}l)$ scales the distance such that a distance of 0 (the self-distance of a node) is mapped to 1, and an infinite distance, which implies no path exists, is scaled to 0. Thus, $\\rho$ spans the interval [0, 1]. ", "page_idx": 3}, {"type": "text", "text": "The representation of each node is dependent on the entire graph, yet the function $\\rho$ enables weighting the influence from nodes, based on their distance. This enables, for example, diminishing the impact of distant nodes, or close neighbors. For each node $i$ , the weighted sum of the transformed feature vectors of all other nodes is computed, with weights assigned according to their distance from $i$ . This weighted sum is computed after the shape functions are applied to the distances and the features of each node. ", "page_idx": 3}, {"type": "text", "text": "Given the node representations, both node prediction tasks and graph prediction tasks can be implemented. To predict for the $i^{!}$ \u2019th node, the computation is as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma\\left(\\sum_{k=1}^{d}[\\mathbf{h}_{i}]_{k}\\right)\\ \\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the entry-wise sum of the representation vector $\\mathbf{h}_{i}$ is computed and subsequently processed using an activation function such as the sigmoid for classification and the identity for regression. For a prediction over the entire graph, the collective node representation is computed via sum-pooling: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}=\\sum_{i=1}^{N}\\mathbf{h}_{i}\\ \\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Following this, the entry-wise sum of the graph representation $\\mathbf{h}$ is computed and also processed using the activation function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma\\left(\\sum_{k=1}^{d}[{\\bf h}]_{k}\\right)~~.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Once the model is trained, it can be fully described using its univariate functions $\\rho$ and $\\{f_{k}\\}_{k=1}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "From the definitions provided above, it follows that the entire model can be represented with just a few figures, thus providing global interpretability. For local explanations, it is feasible to examine the contribution of each feature and each node to the predictions. The following representation convey the influence of each node to the $k$ \u2019ts feature: ", "page_idx": 3}, {"type": "equation", "text": "$$\n[\\mathbf{h}]_{k}=\\sum_{i=1}^{N}[\\mathbf{h}_{i}]_{k}=\\sum_{j=1}^{N}f_{k}\\left([\\mathbf{x}_{j}]_{k}\\right)\\sum_{i=1}^{N}{\\frac{1}{\\#{\\mathrm{dist}}_{i}(j,i)}}\\cdot\\rho\\left({\\frac{1}{1+{\\mathrm{dist}}\\left(j,i\\right)}}\\right)\\quad.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/0cd6b0b967d63c1ada071c3d3f24ca1770d8487ea923bfef9b67536141b275b0.jpg", "img_caption": ["Figure 1: Visualization of the distance and feature functions, learned on Mutagenicity. As the features are binary, the feature functions are evaluated only on the value 1. These plots provide an exact description of the functions\u2019 signal processing and a global explanation of how the model uses the distances and features. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Here $[\\mathbf{h}_{i}]_{k}$ contains the influence of the $k$ \u2019th feature in node $i$ on the prediction. However, from the definition of $[\\mathbf{h}_{i}]_{k}$ we see that it serves as a mediator for influences of all other nodes. Therefore, the influence of node $j$ on feature $k$ in the final graph representation can be obtained by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{k}\\left(\\left[\\mathbf{x}_{j}\\right]_{k}\\right)\\sum_{i=1}^{N}\\frac{1}{\\#\\mathrm{dist}_{i}(j,i)}\\cdot\\rho\\left(\\frac{1}{1+\\mathrm{dist}\\left(j,i\\right)}\\right)\\ \\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can also extract the total contribution of each node $i$ to the prediction, by summing the contribution of the nodes over the features, as done in the input to $\\sigma$ in Equation 1: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{d}[\\mathbf{h}]_{k}=\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\frac{1}{\\#\\mathrm{dist}_{i}(j,i)}\\cdot\\rho\\left(\\frac{1}{1+\\mathrm{dist}\\left(j,i\\right)}\\right)\\sum_{k=1}^{d}f_{k}\\left([\\mathbf{x}_{j}]_{k}\\right)\\quad.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, the total contribution of node $i$ to the prediction is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{N}\\frac{1}{\\#\\mathrm{dist}_{i}(j,i)}\\cdot\\rho\\left(\\frac{1}{1+\\mathrm{dist}\\left(j,i\\right)}\\right)\\sum_{k=1}^{d}f_{k}\\left(\\left[\\mathbf{x}_{j}\\right]_{k}\\right)\\quad.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Overall, the model facilitates a detailed understanding of local behavior from multiple perspectives. ", "page_idx": 4}, {"type": "text", "text": "The functions $\\rho$ and $\\{f_{k}\\}_{k=1}^{d}$ may be implemented using a variety of neural network architectures. In our experiments, we employed multi-layer perceptrons (MLPs) with ReLU activations to implement these functions. Nonetheless, other alternatives are viable, such as employing learning splines for activations to achieve smoother shape functions [46]. Additionally, it is feasible to develop a separate distance network for each feature to enhance the model\u2019s capacity. Specifically, rather than utilizing a single function $\\rho$ , one can train a distinct function $\\rho_{k}$ for each feature $k$ , which weights the contribution of each feature based on its node\u2019s distance. For graph-level tasks, additional feature networks may be integrated prior to aggregating the graph\u2019s representation vector, akin to a readout layer in GNNs. These extensions, along with a discussion on a tensor representation of the computation that facilitates efficient GPU utilization, are further elaborated in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "For multiclass classification involving $C$ classes, we configure the final layers of the feature shape functions $f_{k}(\\boldsymbol{x};\\boldsymbol{\\theta}_{k}):\\mathbb{R}\\rightarrow\\mathbb{R}^{C\\times1}$ and the distance function $\\rho(x;\\theta):\\mathbb{R}\\rightarrow\\mathbb{R}^{C\\times1}$ to accommodate the required dimensionality. The transformed feature vectors and the distance metrics are combined using an element-wise multiplication denoted by $\\odot$ , as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n[\\mathbf{h}_{i}]_{k}=\\sum_{j=1}^{N}\\frac{1}{\\#\\mathrm{dist}_{i}(j,i)}\\cdot\\rho\\left(\\frac{1}{1+\\mathrm{dist}\\left(j,i\\right)}\\right)\\odot f_{k}([\\mathbf{x}_{j}]_{k})\\quad.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For prediction purposes, the sum operator is applied independently across the dimensions corresponding to each class, and a softmax is employed as the activation function. ", "page_idx": 4}, {"type": "text", "text": "4 Inteligibility ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we demonstrate the intelligibility of GNAN through visualizations. Each GNAN model is characterized by the univariate learned shape functions $\\rho$ and $\\{f_{k}\\}_{k=1}^{d}$ , and can thus be depicted as a set of illustrative figures. Below, we present examples of such figures and explain their utility in generating insights. Our focus in this section is on global interpretability, as local interpretability can utilize analogous ways. We showcase GNAN\u2019s application on two datasets, with additional examples detailed in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "Our initial examples focus on the task of detecting mutation-causing molecules using the Mutagenicity dataset [47]. In this task, molecules are modeled as graphs where nodes correspond to atoms and edges to connections between these atoms. Each atom type is represented by a 14-dimensional one-hot encoding. A GNAN model trained on this dataset is illustrated in Figure 1. On the left, the function $\\rho$ is presented, demonstrating how distance impacts prediction, with a clear diminishing influence of more distant atoms. On the right, the shape functions for the features are displayed. Given that the features are binary, each shape function manifests only two values: one when the feature is 0 (indicating that the atom is not of the specified type), and another when the feature is 1 (indicating that the atom is of the specified type). Defining $\\begin{array}{r}{b=\\sum_{k}f_{k}(0)}\\end{array}$ as the bias term allows us to set $f_{k}(0)=0$ for each $k$ , thereby enabling the plotting of only $f_{k}(1)$ . The graphical representation reveals that atoms such as Ca, Na, and Li are predicted to correlate with an increased mutagenicity effect, whereas $_\\mathrm{N}$ and P atoms are predicted to be associated with a slight protective effect. ", "page_idx": 5}, {"type": "text", "text": "It is essential to emphasize that Figure 1 displays the entire model comprehensively. This means that combined with the value of the bias term, which is $-5.6672$ in this case, every crucial detail needed to understand and utilize this model for predictions is contained within this single figure. This stands in stark contrast to methods like feature importance, which offer a limited perspective on models. While the figure provides complete information about the model, presenting additional views can sometimes be helpful. ", "page_idx": 5}, {"type": "text", "text": "Figure 2 showcases the cross product of the shape functions and the distance function as a heatmap. Each cell $(k,l)$ in the heatmap represents the value $\\rho\\left({1}/({1{+}l})\\right)\\cdot f_{k}(1)$ . This figure illustrates the interplay the model has learned between the graph\u2019s structure and the attributes of its nodes. As the task involves binary classification, positive values in the heatmap contribute to classifying a molecule as mutagenic, whereas negative values indicate non-mutagenic properties. ", "page_idx": 5}, {"type": "text", "text": "This heatmap illustrates how atoms at specific distances influence the final outcome. For instance, it shows that the model has learned that the presence of a Ca atom (cell $\\left(\\mathbf{Ca},0\\right)\\phantom{,}$ ) or its proximity (cell (Ca, 1)) contributes to mutagenicity. The visualizations can also be used for debugging purposes. This can be crucial, for example, to ensure that the model is free from biases or to identify any discrepancies with existing scientific knowledge. If it is already known that Ca atoms actually have a negative effect on mutagenicity, users could identify and correct this misalignment in the model\u2019s learning. Additionally, this detailed understanding allows users to select models that not only achieve high accuracy on the given samples but also align with prior knowledge, optimizing both performance and reliability. ", "page_idx": 5}, {"type": "text", "text": "Interpreting multiclass prediction tasks poses significant challenges, as noted by Zhang et al. [6]. In this context, we showcase the interpretability of GNAN using the PubMed dataset [48]. This dataset comprises 19,717 scientific publications related to diabetes archived on PubMed and categorized into three distinct classes (type-1 diabetes, type-2 diabetes, and gestational diabetes). The dataset\u2019s citation network includes 44,338 links. Each publication, represented as a node, is characterized by a TF/IDF weighted word vector derived from a dictionary containing 500 unique words. ", "page_idx": 5}, {"type": "text", "text": "As there are three classes, we trained the GNAN model such that the output of the distance and feature functions are of dimension three. In this setting it is interesting to compare the three functions, corresponding to the three classes and therefore we draw them on the same figure [6]. Figure 3 shows that the model uses only the local neighborhood of each node, and as nodes become more distanced, the information between them is less used. We also observe a difference between the classes; while for type 2 diabetes, the longer the distance, the less their information is used (converges to 0), for type 1 and gestational diabetes, nodes of long-distance have a negative effect. ", "page_idx": 5}, {"type": "text", "text": "In Figure 4, we display the feature shape functions for nine selected features, demonstrating GNAN\u2019s capability to learn complex, non-monotone functions such as those seen in the \u2019diet\u2019 and \u2019hepat\u2019 features. Observing these shape functions across the three classes simultaneously allows for an understanding of how different feature values are utilized by the model to distinguish among the classes. For instance, the shape function for the \u2019insulin\u2019 feature reveals that the absence of this word in a document (i.e., feature values close to zero) does not significantly indicate the document\u2019s class. However, as the frequency of \u2019insulin\u2019 increases within the document, its impact on the prediction becomes more pronounced, though this effect varies distinctly between type 1 & 2 diabetes and gestational diabetes. ", "page_idx": 5}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/d79987b99267092c3f05a9d052434ee98873a990009fe530323125a5e9265151.jpg", "img_caption": ["Figure 2: Visualization of products of the outputs of the distance function and the feature functions, trained on Mutagenicity. Each cell shows the exact contribution, positive or negative, of features at a certain distance to the prediction. Positive values (green) contribute to classifying a molecule as mutagenic, and negative values (red) contribute to classifying a molecule as non-mutagenic. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/bd5d052bb97c01dbac52c964e56f1b69f5d0ad74fa7f033354670d37188b1367.jpg", "img_caption": ["Figure 3: Visualization of the distance shape function learned on the PubMed dataset. As the output of the function is of dimension three, we plot it as three shape functions, one for each class. We plot them on the same figure to compare them. The shape functions show that the model uses only the local neighborhood of each node. It also shows a difference between the classes; while for type 2 diabetes, the longer the distance, the less their information is used (converges to 0), for type 1 and gestational diabetes, nodes of long-distance have a negative effect. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To visualize the contribution of a feature value at a specific distance, we employ a heatmap for each class, evaluating the products between the outputs of the feature function over the input range $([0,1])$ and the output of the corresponding distance function. Figure 5 exemplifies this visualization technique with the \u2019children\u2019 feature. It is insightful to observe that the presence of the word \u2019children\u2019 influences the predictions differently across the diabetes types. The model has learned that papers concerning type 1 diabetes seldom mention \u2019children\u2019, nor do related papers. In contrast, the term frequently appears in the context of gestational diabetes. ", "page_idx": 6}, {"type": "text", "text": "It is possible to construct confidence intervals for GAMs using the bootstrap method[49, 50]. We present such example with additional visualization examples in the appendix. ", "page_idx": 6}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/0eb9b994c437a7341c49283028a824430f496ab515a0c8a76af225cc6ab11f29.jpg", "img_caption": ["Figure 4: Visualization of nine features\u2019 shape functions, learned over the PubMed dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/1565a30ed7cd8f1800cf0095152928c52af72f1d6b4df0021dcad1f47ac085d7.jpg", "img_caption": ["Figure 5: Visualization of the products between the outputs of the \u2019children\u2019 feature function over the input range [0, 1] and the outputs of the distance function, learned over the PubMed dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1 Local Explanations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Up to this point, we have demonstrated how to visualize GNAN for providing global explanations. These explanations offer a comprehensive visualization of the entire model. Now, we shift our focus to local explanations, i.e., those relevant to particular examples of interest. To illustrate this, we employ the same Mutagenicity model that was visualized globally to explain specific samples within the data. Using Equation 3 , we compute the importance of each node and visualize two molecules from the dataset, where the area of each node corresponds to its importance. ", "page_idx": 7}, {"type": "text", "text": "Figure 6 presents two such examples. In Figure 6(a) the carbon (red) atoms play the most significant role, and a carbon ring (red cycle) is highlighted. In Figure 6(b), a group of $N O_{2}$ (grey and green subgraph) is shown to be relatively important for predicting the molecule as mutagenic. Both carbon rings and $N O_{2}$ groups are well-known for their mutagenic effects [51], making them frequently discussed in the literature on explainable GNNs [39, 41]. ", "page_idx": 7}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/80428197f3b98a422013b88ff94f25493ba8a4b07cd9ca3b9746600dba041984.jpg", "img_caption": ["Figure 6: Local explanations of two molecules from the Mutagenicity datasets, through visualizations of the molecules. The area of each atom corresponds to the node importance according to Equation 3. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Empirical evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate GNAN on real-world graph and node labeling tasks, including large-scale, long-range, and heterophily datasets.3. We compare GNAN to multiple commonly used black-box GNNs including GraphConv [52], GraphSAGE [30], Graph Isomorphism Network (GIN) [33], the expressive version of the Graph Attention Network (GATv2) [29, 53], the Graph Transformer (GTransformer) [54]. We also evaluate the FSGNN model, which disentangles the node features from the graph structure [35]. The information on the hyper-parameters tuned for each baseline can be found in the Appendix. We used the following common benchmarks: ", "page_idx": 8}, {"type": "text", "text": "Node labeling tasks Cora, Citeseer, PubMed, ogb-arxiv [55, 56] are paper citation networks where the goal is to classify papers into one of several topics. The ogb-arxiv dataset is a large-scale network. Cornell [57] & Tolokers [58] are heterophilious datasets. Cornell is a web-link network with the task of classifying nodes into one of five categories. Tolokers dataset is based on data from the Toloka crowdsourcing platform. The nodes represent tolokers (workers) who have participated in at least one of 13 selected projects. An edge connects two tolokers if they have worked on the same task. The goal is to predict which tolokers have been banned in one of the projects. Node features are based on the worker\u2019s profile information and task performance statistics. ", "page_idx": 8}, {"type": "text", "text": "Graph labeling tasks NCI1, Proteins, Mutagen & PTC [59] are datasets of chemical compounds. In each dataset, the goal is to classify compounds according to some property of interest. Thr $\\mu\\,,\\alpha\\,,\\alpha_{H O M O}$ [60] datasets are long-range molecular property prediction regression tasks, over the large-scale QM9 molecular dataset. ", "page_idx": 8}, {"type": "text", "text": "Additional data information, including the data statistics, can be found in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "Protocol For all tasks, we used existing splits, protocols, and metrics, as commonly used in the literature for each dataset. The complete protocols for each dataset are given in detail in the Appendix. The metrics we report are: For Cornell, Cora, Citeseer, PubMed, ogb-arxiv, Mutagenicity, PTC, NCI, and Proteins, we report accuracy. For $\\mu$ , $\\alpha$ and $\\alpha_{H O M O}$ we report MAE. For Tolokers we report ROC-AUC. For the node labeling tasks, we used the pre-defined splits in the data and followed the common protocols for each dataset. The results are an average of the test set using 5 or 10 random seeds. For the Proteins and NCI1 tasks, we followed the splits and the nested-cross-validation protocol from [61]. The final reported result on these datasets is an average of 30 runs (10-folds and 3 random seeds). For NCI1 and PTC we followed the splits and protocol from [39] and report the average accuracy and std of a 10-fold nested cross-validation. ", "page_idx": 8}, {"type": "text", "text": "Results The results are presented in Table 1. GNAN performed as the best or second-best model in 9 out of the 13 tasks we evaluated. In GNAN, each node gathers information from all others, ensuring complete information flow, while the $\\rho$ function modulates influence based on distance. ", "page_idx": 8}, {"type": "table", "img_path": "SKY1ScUTwA/tmp/af003a4c5fe7ca41fab2578eda3a48c7efc311ede7cc14b69db4376f57854daf.jpg", "table_caption": ["Table 1: Evaluation of GNAN on node (top) and graph (bottom) tasks. The best and second-best models are marked in cyan and violet colors, respectively. We report accuracy and std for all tasks, except for the Tolokers dataset where we report ROC-AUC and std, and the $\\mu,\\alpha,\\alpha_{H O M O}$ datasets where we report MAE and std. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Consequently, GNAN avoids the computational bottlenecks encountered by some message-passing GNNs [25]. Particularly in the long-range tasks $\\mu,\\,\\alpha$ , and $\\alpha_{H O M O}$ , GNAN outperformed all other evaluated baselines, aligning with findings by Alon and Yahav [25] that emphasize the benefits of capturing long-range information. While intelligibility sometimes comes at the cost of accuracy, our findings suggest that enhancing intelligibility does not necessarily result in significant accuracy loss. It may appear surprising that GNAN, despite its limited capacity, matches the accuracy of more expressive GNNs. However, prior research indicates that even limited-capacity GNNs, such as linear GNNs, can achieve high accuracy on various real-world datasets [62, 61, 63], suggesting that some real-world graph problems are simpler than anticipated. Our results corroborate these observations. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced the Graph Neural Additive Network (GNAN), a novel extension of the interpretable class of Generalized Additive Models, to accommodate graph data. GNAN is inherently interpretable, and provides both global and local explanations directly from its architecture, eliminating the need for post-hoc interpretations. This direct interpretability enhances the transparency of the model and is particularly useful in high-stakes applications where understanding model decisions is crucial. Furthermore, GNAN demonstrates competitive performance with popular GNNs, showing that intelligibility does not necessarily entail a significant degradation in accuracy. ", "page_idx": 9}, {"type": "text", "text": "It is possible to enhance GNAN in several ways. To generate smooth shape functions, one could integrate techniques from the recently proposed Kolmogorov\u2013Arnold Networks [46] or adaptive activations for graphs [64]. Increasing the capacity of GNAN is feasible by learning individual distance functions for each feature. Exploring reduced capacity is also intriguing, particularly in scenarios with many features, where it may be beneficial to employ regularization to limit the number of shape functions used. Additionally, applying these techniques to biological network datasets, such as protein interactions, could be a valuable tool to support scientific discoveries. These and other directions are left for future studies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Tel Aviv University Center for AI and Data Science (TAD) and the Israeli Science Foundation grants 1186/18 and 1437/22. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57\u201381, 2020.   \n[2] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead, 2019.   \n[3] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and a \u201cright to explanation\u201d. AI magazine, 38(3):50\u201357, 2017.   \n[4] Andrew Selbst and Julia Powles. \u201cmeaningful information\u201d and the right to explanation. In conference on fairness, accountability and transparency, pages 48\u201348. PMLR, 2018.   \n[5] European Parliament. European parliament legislative resolution of 13 march 2024 on the proposal for a regulation of the european parliament and of the council on laying down harmonised rules on artificial intelligence (artificial intelligence act). OJ, 2024. URL https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf.   \n[6] Xuezhou Zhang, Sarah Tan, Paul Koch, Yin Lou, Urszula Chajewska, and Rich Caruana. Axiomatic interpretability for multiclass additive models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 226\u2013234, 2019.   \n[7] Blair Bilodeau, Natasha Jaques, Pang Wei Koh, and Been Kim. Impossibility theorems for feature attribution. Proceedings of the National Academy of Sciences, 121(2):e2304406120, 2024.   \n[8] Daniel Vale, Ali El-Sharif, and Muhammed Ali. Explainable artificial intelligence (xai) post-hoc explainability methods: Risks and limitations in non-discrimination law. AI and Ethics, 2(4): 815\u2013826, 2022.   \n[9] Rebecca Wexler. When a computer program keeps you in jail. The New York Times, 13:1, 2017.   \n[10] Michael McGough. How bad is sacramento\u2019s air, exactly? google results appear at odds with reality, some say. Sacramento Bee, 7, 2018.   \n[11] Marzyeh Ghassemi, Luke Oakden-Rayner, and Andrew L Beam. The false hope of current approaches to explainable artificial intelligence in health care. The Lancet Digital Health, 3 (11):e745\u2013e750, 2021.   \n[12] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.   \n[13] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144, 2016.   \n[14] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating feature importance estimates. arXiv preprint arXiv:1806.10758, 2, 2018.   \n[15] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189\u20131232, 2001.   \n[16] Waddah Saeed and Christian Omlin. Explainable ai (xai): A systematic meta-survey of current challenges and future opportunities. Knowledge-Based Systems, 263:110273, 2023.   \n[17] Gabriel Laberge, Yann Batiste Pequignot, Mario Marchand, and Foutse Khomh. Tackling the xai disagreement problem with regional explanations. In International Conference on Artificial Intelligence and Statistics, pages 2017\u20132025. PMLR, 2024.   \n[18] Valentina Franzoni. From black box to glass box: advancing transparency in artificial intelligence systems for ethical and trustworthy ai. In International Conference on Computational Science and Its Applications, pages 118\u2013130. Springer, 2023.   \n[19] Trevor Hastie and Robert Tibshirani. Generalized Additive Models. Statistical Science, 1 (3):297 \u2013 310, 1986. doi: 10.1214/ss/1177013604. URL https://doi.org/10.1214/ss/ 1177013604.   \n[20] Trevor Hastie and Robert Tibshirani. Generalized additive models for medical research. Statistical methods in medical research, 4(3):187\u2013196, 1995.   \n[21] Trevor Hastie and Robert Tibshirani. Generalized additive models: Some applications. Journal of the American Statistical Association, 82(398):371\u2013386, 1987. doi: 10.1080/01621459.1987. 10478440.   \n[22] Thomas W Yee and Neil D Mitchell. Generalized additive models in plant ecology. Journal of vegetation science, 2(5):587\u2013602, 1991.   \n[23] Robert A Rigby and D Mikis Stasinopoulos. Generalized additive models for location, scale and shape. Journal of the Royal Statistical Society Series C: Applied Statistics, 54(3):507\u2013554, 2005.   \n[24] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1721\u20131730, 2015.   \n[25] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications, 2021.   \n[26] Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, and Geoffrey Hinton. Neural additive models: Interpretable machine learning with neural nets, 2021.   \n[27] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https: //openreview.net/forum?id $\\cdot$ SJU4ayYgl.   \n[28] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry, 2017.   \n[29] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.   \n[30] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs, 2018.   \n[31] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation?, 2021.   \n[32] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces, 2024.   \n[33] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id $\\cdot$ ryGs6iA5Km.   \n[34] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Optimality of message-passing architectures for sparse graphs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\cdot$ d1knqWjmNt.   \n[35] Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Improving graph neural networks with simple architecture design, 2021.   \n[36] Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Ben Chamberlain, Michael Bronstein, and Federico Monti. Sign: Scalable inception graph neural networks, 2020.   \n[37] Maya Bechler-Speicher, Ido Amos, Ran Gilad-Bachrach, and Amir Globerson. Graph neural networks use graphs when they shouldn\u2019t. In Forty-first International Conference on Machine Learning, 2024.   \n[38] Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher R\u00e9, and Kevin Murphy. Machine learning on graphs: A model and comprehensive taxonomy. Journal of Machine Learning Research, 23(89):1\u201364, 2022.   \n[39] Maya Bechler-Speicher, Amir Globerson, and Ran Gilad-Bachrach. Tree-g: Decision trees contesting graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 11032\u201311042, 2024.   \n[40] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network, 2020.   \n[41] Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks, 2019.   \n[42] Kenza Amara, Rex Ying, Zitao Zhang, Zhihao Han, Yinan Shan, Ulrik Brandes, Sebastian Schemm, and Ce Zhang. Graphframex: Towards systematic evaluation of explainability methods for graph neural networks, 2022.   \n[43] Jun Yin, Chaozhuo Li, Hao Yan, Jianxun Lian, and Senzhang Wang. Train once and explain everywhere: Pre-training interpretable graph neural networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= enfx8HM4Rp.   \n[44] Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, and Ran He. Graph information bottleneck for subgraph recognition, 2020.   \n[45] Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks, 2022.   \n[46] Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Solja\u02c7ci\u00b4c, Thomas Y. Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks, 2024.   \n[47] Jeroen Kazius, Ross McGuire, and Roberta Bursi. Derivation and validation of toxicophores for mutagenicity prediction. Journal of medicinal chemistry, 48(1):312\u2013320, 2005.   \n[48] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[49] David Louis Borchers, Stephen Terrence Buckland, IG Priede, and S Ahmadi. Improving the precision of the daily egg production method using generalized additive models. Canadian Journal of Fisheries and Aquatic Sciences, 54(12):2727\u20132742, 1997.   \n[50] Wolfgang H\u00e4rdle, Sylvie Huet, Enno Mammen, and Stefan Sperlich. Bootstrap inference in semiparametric generalized additive models. Econometric Theory, 20(2):265\u2013300, 2004.   \n[51] AK Debnath, RL Lopez de Compadre, G Debnath, AJ Shusterman, and C Hansch. Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2): 786\u2014797, February 1991. ISSN 0022-2623. doi: 10.1021/jm00106a046. URL https: //doi.org/10.1021/jm00106a046.   \n[52] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks, 2021.   \n[53] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks?, 2022.   \n[54] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification, 2021.   \n[55] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings, 2016. URL https://arxiv.org/abs/1603.08861.   \n[56] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs, 2020. URL https://arxiv.org/abs/2005.00687.   \n[57] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id $\\cdot^{=}$ S1e2agrFvS.   \n[58] Daniil Likhobaba, Nikita Pavlichenko, and Dmitry Ustalov. Toloker Graph: Interaction of Crowd Annotators, 2023. URL https://github.com/Toloka/TolokerGraph.   \n[59] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond ( $G R L+$ 2020), 2020. URL www.graphlearning.io.   \n[60] Raghunathan Ramakrishnan, Pavlo Dral, Matthias Rupp, and Anatole von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 08 2014. doi: 10.1038/sdata.2014.22.   \n[61] Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification, 2022.   \n[62] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6861\u20136871. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/wu19e.html.   \n[63] Chenxiao Yang, Qitian Wu, Jiahua Wang, and Junchi Yan. Graph neural networks are inherently good generalizers: Insights by bridging gnns and mlps, 2023.   \n[64] Krishna Sri Ipsit Mantri, Xinzhi Wang, Carola-Bibiane Sch\u00f6nlieb, Bruno Ribeiro, Beatrice Bevilacqua, and Moshe Eliasof. Digraf: Diffeomorphic graph-adaptive activation function, 2024. URL https://arxiv.org/abs/2407.02013.   \n[65] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.   \n[66] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric, 2019.   \n[67] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of GNNs under heterophily: Are we really making progress? In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ tJbbQfw-5wv.   \n[68] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Efficient GNAN implementation with tensor products ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We formulate GNAN case of classification with $C$ classes. For regression, the exact formulation holds with $C=1$ . We use the same notation as in the main text, only now the output of the feature and distance function is of dimension $1\\times C$ , ", "page_idx": 14}, {"type": "text", "text": "For the sake of notation, we assume that every tensor that its last dimension is of size $C$ , is permuted to have the last dimension as its first dimension, without stating it explicitly. This is necessary to achieve a valid tensor multiplication. ", "page_idx": 14}, {"type": "text", "text": "We denote with $M$ the matrix of the transformed distances that is outputted by applying $\\rho$ , i.e., $\\begin{array}{r}{M_{i,j}=\\rho(\\frac{1}{1+d i s t(j,i)})}\\end{array}$ , $\\boldsymbol{M}\\in\\mathbb{R}^{C\\times N\\times N}$ .   \nWe denote with $F$ the matrix of the transformed feature is outputted by applying the corresponding $f_{k}$ for feature $l$ of each node in the graph, i.e., $F_{i k}=f_{k}(x_{l}^{i})$ , $\\dot{\\boldsymbol{F}}\\in\\mathbb{R}^{C\\dot{\\times}N\\dot{\\times}\\dot{\\boldsymbol{d}}}$   \nBoth for node and graph tasks, we first computes the matrix $\\boldsymbol{M}\\cdot\\boldsymbol{F}\\in\\mathbb{R}^{C\\times N\\times d}$   \nThe rest of the computation then depends on the task. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A.1 Node Tasks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We aggregate the transformed features weighted by the transformed distances. This is done by summing over the rows of $M\\otimes F$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(i)=[M\\otimes F\\otimes\\mathbb{1}_{C\\times d\\times1}]_{i}=\\sum_{k=1}^{d}\\sum_{j=1}^{N}\\rho(\\frac{1}{1+d i s t(j,i)})\\otimes f_{k}([\\mathbf{x}_{j}]_{k})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Graph Tasks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For graph classification, we set $\\rho$ and $f$ to output a scalar and aggregate the transformed features over the nodes, i.e., the row of $M\\cdot F$ , to form a fixed-size vector of size $d$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Phi}(G)=\\mathbb{1}_{C\\times1\\times N}\\otimes M\\otimes F\\in\\mathbb{R}^{C\\times1\\times d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we can apply another readout NAM [26]: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Phi(G)={\\bar{F}}({\\bar{\\Phi}}(G))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Such that $\\bar{F}$ is the transformed features using the function $\\{\\bar{f}_{k}\\}_{k=1}^{d}$ such that $\\bar{f}_{k}:\\mathbb{R}\\rightarrow\\mathbb{R}^{1\\times C}$ ", "page_idx": 14}, {"type": "text", "text": "We can also simply sum over the outputs. In that case, we will set $f$ and $m$ to output a vector of dimension $C$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Phi(G)=\\sum_{i=1}^{N}\\phi(i)=\\mathbb{1}_{C\\times1\\times N}\\otimes M\\otimes F\\otimes\\mathbb{1}_{C\\times d\\times1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Extensions and ablations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 3 we mentioned several possible extensions for GNAN. Here, we discuss them in detail. ", "page_idx": 14}, {"type": "text", "text": "Readout layer for graph tasks In graph tasks, after aggregating the node representations, it is possible to apply another transformation before aggregating over the entries of the graph representations. There may be many ways to do so, and we did not explore all of them. We did explore an application of another set of feature functions to each feature or the graph representation vector. This approach increases the capacity of the model in the cost of interpretability. This is because the set of addition feature functions should be plotted separately, and the product between the feature function and the distance functions does not affect the final output directly but rather through another feature function. Empirically, we observed this approach did not improve performance with respect to the performance reported in Section 5. ", "page_idx": 14}, {"type": "text", "text": "Splines It is possible to learn splines for the activations in each feature network to achieve smoother shape functions [46]. We note that the Tolokers example presented in Section C shows that the learned feature shape function is smooth, although we use ReLU activations. Nonetheless, in other cases, such as in the PubMed example presented in the main paper, many of the learned feature functions are step functions. Therefore, it is likely that the model could benefit from spline activations, to smooth its shape functions. ", "page_idx": 15}, {"type": "text", "text": "Normalization In GNAN we normalize the weight of nodes of distance $l$ with the number of nodes of distance $l$ , so that the cumulative weight of nodes of distance $l$ will be $\\rho\\big({}^{1}\\!/(1\\!+\\!l)\\big)$ . We examined the effect of removing this normalization. We observed that without normalization, the loss scale is drastically larger. Therefore, more epochs are required to fit the data. As a result, for the fixed number of epochs we used in our experiments (1000), without normalization, the accuracy decreases. ", "page_idx": 15}, {"type": "text", "text": "C Additional explanation examples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the main paper, we presented two examples of explanations over two datasets with different properties. In this section, we present additional explanations and examples we could not fti into the main text due to space limitations. ", "page_idx": 15}, {"type": "text", "text": "C.1 Confidence Intervals ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our main paper, we discussed the construction of confidence intervals for Generalized Additive Models (GAMs) using the bootstrap method. Here, we provide a specific example using the Mutagenicity dataset. ", "page_idx": 15}, {"type": "text", "text": "We computed $95\\%$ confidence intervals by applying the bootstrap method with 200 resamples. This involved resampling the original dataset with replacement 200 times and calculating the statistic of interest for each resample. The resulting bootstrap estimates were sorted, and the $2.5\\mathrm{th}$ and 97.5th percentiles were taken as the lower and upper bounds of the confidence interval, respectively. Figure 7 presents the distance function with its confidence intervals. ", "page_idx": 15}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/6bdbffa1f23f5b71523c34cb6bdbfa0a7fe46184a2f303a21d6f09f3bb989816.jpg", "img_caption": ["Figure 7 "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Additional PubMed heatmaps ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the main text, we presented the heatmaps for the \u2019children\u2019 feature. Here we provide additional heatmaps for additional features: the \u2019fat\u2019 feature and the \u2019young\u2019 feature, as presented in Figures 8 and 9. ", "page_idx": 15}, {"type": "text", "text": "C.3 Tolokers $^{-}$ Binary classification with binary and continuous features ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The Tolokers dataset is based on data from the Toloka crowdsourcing platform. The nodes represent tolokers (workers) who have participated in at least one of 13 selected projects. An edge connects two tolokers if they have worked on the same task. The goal is to predict which tolokers have been banned in one of the projects. Node features are based on the worker\u2019s profile information and task performance statistics. Each node in the graph is associated with nine features. There are 4 continuous features in the range $[0,1]$ and 5 are binary features. Figure 10 shows the shape functions of the features learned by GNAN. Figure 11 shows the distance shape function learned by GNAN. In Figures 13 and 12 we present the heatmaps of the cross product of the shape functions and the distance function. ", "page_idx": 15}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/0901f00ed2a00c67dfc3458d6656ceb4dfb80dadaf0eddc804131f52e9a03ec0.jpg", "img_caption": ["Figure 8: Visualization of the products between the outputs of the \u2019fat\u2019 feature function over the input range [0, 1] and the outputs of the distance function, learned over the PubMed dataset "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/be925a8a8f8822aa60c8e92e6f439a2cbd6e40eac004f96267626fb513ceadaa.jpg", "img_caption": ["Figure 9: Visualization of the products between the outputs of the \u2019young\u2019 feature function over the input range [0, 1] and the outputs of the distance function, learned over the PubMed dataset "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/1f421ad0078a7dc6c1df34d4914606f014da821956412c05d9c4060e54d22da3.jpg", "img_caption": ["Figure 10: Visualization of the feature functions learned over the Tolokers dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/f1fb627f6be1c334a28d85ffc1a5b127d7efe9d5b888a0d54a501173268d9c0b.jpg", "img_caption": ["Figure 11: isualization of the distance shape function learned on the Tolokers dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/4e4978f0d116a5bf1c4334f02bcf0ff882e6722df8b349dff02b61c2c811be4e.jpg", "img_caption": ["Figure 12: Visualization of the products between the outputs of the continuous feature functions over the input range [0, 1] and the outputs of the distance function, learned over the Tolokers dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Additional experimental details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All our baselines are implemented using PyTorch [65] and PyTorch-Geometric [66]. ", "page_idx": 17}, {"type": "text", "text": "D.1 Dataset information ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we provide additional information about the datasets used in Section 5. The data statistics are given in Table 2. ", "page_idx": 17}, {"type": "text", "text": "Proteins [59] is a dataset of chemical compounds consisting of 1113 graphs, respectively. The goal in the first two datasets is to predict whether a compound is an enzyme or not, and the goal in the last datasets is to classify the type of an enzyme among 6 classes. ", "page_idx": 17}, {"type": "text", "text": "NCI1 [59] is a datasets consisting of 4110 graphs, representing chemical compounds. Vertices and edges represent atoms and the chemical bonds between them. The graphs are divided into two classes ", "page_idx": 17}, {"type": "image", "img_path": "SKY1ScUTwA/tmp/977d1f1739434d692c74add816111f68ca790eda14c49630b9131e06fb9ca6b2.jpg", "img_caption": ["Figure 13: Visualization of products of the outputs of the distance function and the feature functions, trained on Tolokers. Each cell shows the exact contribution, positive or negative, of features at a certain distance to the prediction. Positive values (green) contribute to classifying a toloker as \u2019banned\u2019, and negative values (red) contribute to classifying a toloker as \u2019not banned\u2019. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "according to their ability to suppress or inhibit tumor growth. ", "page_idx": 18}, {"type": "text", "text": "Mutagenicity [59] is a dataset consisting of 4337 chemical compounds of drugs divided into two classes: mutagen and non-mutagen. A mutagen is a compound that changes genetic material such as DNA, and increases mutation frequency. ", "page_idx": 18}, {"type": "text", "text": "PTC [59] is a dataset consisting of 344 chemical compounds divided into two classes according to their carcinogenicity for rats. ", "page_idx": 18}, {"type": "text", "text": "Cornell [57] is a heterophilic webpage dataset collected from the computer science department at Cornell University. Nodes represent web pages, and edges are hyperlinks between them. The task is to classify the nodes into one of five categories. ", "page_idx": 18}, {"type": "table", "img_path": "SKY1ScUTwA/tmp/07967ea80a5c5ccb0bdcb79d497590f568e1546cc260341deee3031a7f34c392.jpg", "table_caption": ["Table 2: Statistics of the real-world datasets used in our evaluation. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.2 Protocols ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "ogb-arxive The ogb-arxive datasets are large-scale datasets provided in the Open Graph Benchmark (OGB) paper [56] with pre-defined train and test splits and different metrics and protocols for each dataset. As common in the literature when evaluating OGB datasets, we followed its pre-defined metric and protocol. The metric used is accuracy. We ran GNAN 10 times and reported the mean accuracy and std over the runs. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Cornell For the Cornell dataset we used the splits and protocol from [57] and report the test accuracy averaged over 10 runs, using the best hyper-paremeters found on the validation set. ", "page_idx": 19}, {"type": "text", "text": "Tolokers For the Tolokers dataset, we followed the protocol and pre-defined splits from [58, 67].   \nThe reported result is an average of a 10-fold nested cross-validation. ", "page_idx": 19}, {"type": "text", "text": "Core, Citeseer and PubMed Following [68, 30, 29], for the Core, Citeseer and Pubmed datasets we tuned the parameters on the Cora dataset using the pre-defined splits from [68]. For all these datasets we report the test accuracies averaged over 5 runs, using the parameters obtained from the best accuracy on the validation set of Cora. ", "page_idx": 19}, {"type": "text", "text": "Proteins, NCI We used 10-fold nested cross validation with the splits and protocol of Errica et al. [61]. The final reported result on these datasets is an average of 30 runs (10-folds and 3 random seeds). ", "page_idx": 19}, {"type": "text", "text": "Mutagenicity, PTC We use the splits and protocols from [39], and use a 10-fold nested crossvalidation. The final reported test accuracies are averages over the 10 test sets of the outer 10 folds. ", "page_idx": 19}, {"type": "text", "text": "D.3 Hyperparameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All GNNs (excluded GNAN) use ReLU activations with $\\{3,5\\}$ layers and 64 hidden channels. They were trained with Adam optimizer over 1000 epochs and early on the validation loss with a patient of 100 steps, eight Decay of $1e-4$ , learning rate in $\\{1e-3,1e-4\\}$ , dropout rate in $\\{0,0.5\\}$ , and a train batch size of 32. ", "page_idx": 19}, {"type": "text", "text": "In GNAN, all the feature and distance networks use ReLU activations with $\\{3,5\\}$ layers and $\\lbrace64,32\\rbrace$ hidden channels. They were trained with Adam optimizer over 1000 epochs weight decay of $0,5e-4$ , learning rate in $\\{1e\\bar{-}\\,2,1e-3\\}$ , dropout rate in $\\{0,0.6\\}$ . ", "page_idx": 19}, {"type": "text", "text": "D.4 Compute resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All experiments ran on an NVIDIA GeForce RTX 3090 GPU. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: the paper discuss the limitations of the work performed by the authors Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: NA ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics . ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: the paper discuss both potential positive societal impacts and negative societal impacts of the work performed ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: NA ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA]   \nJustification: NA   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]