[{"type": "text", "text": "CultureLLM: Incorporating Cultural Differences into Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cheng Li\u2217 Institute of Software, CAS chenglicat0228@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Mengzhuo Chen Institute of Software, CAS mengzhuo.happy@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Jindong Wang\u2020 Microsoft Research jindong.wang@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Sunayana Sitaram Microsoft Research Sunayana.Sitaram@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Xing Xie Microsoft Research xing.xie@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have been observed to exhibit bias towards certain cultures due to the predominance of training data obtained from English corpora. Considering that multilingual cultural data is often expensive to procure, existing methodologies address this challenge through prompt engineering or culture-specific pre-training. However, these strategies may neglect the knowledge deficiency of low-resource cultures and necessitate substantial computing resources. In this paper, we propose CultureLLM, a cost-effective solution to integrate cultural differences into LLMs. CultureLLM employs the World Value Survey (WVS) as seed data and generates semantically equivalent training data through the proposed semantic data augmentation. Utilizing only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs as well as a unified model (CultureLLM-One) for 9 cultures, encompassing both rich and low-resource languages. Extensive experiments conducted on 60 culturerelated datasets reveal that CultureLLM significantly surpasses various counterparts such as GPT-3.5 (by $8.1\\%$ ) and Gemini Pro (by $9.5\\%$ ), demonstrating performance comparable to or exceeding that of GPT-4. Our human study indicates that the generated samples maintain semantic equivalence to the original samples, offering an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Culture is a complex construct that encapsulates various identities, including, but not limited to, language, nationality, region, religion, and gender identity. Cultural bias is prevalent worldwide and refers to the tendency to favor specific cultural perspectives, values, and norms, which results in subjective opinions and may offend individuals from other cultures. For instance, according to the World Value Survey [Survey, 2022], Arabic culture believes that men are better political leaders than women, while people in the United States maintain a contrary opinion.3 As large language models (LLMs) [OpenAI, 2023b, Google, 2023] gain prominence, they are reported to exhibit cultural bias and specifically show partiality towards Western culture, as English corpora dominate the training data [Liu et al., 2023b, Cao et al., 2023, Masoud et al., 2023, Naous et al., 2023, Wang et al., 2023d, Johnson et al., 2022]. Low-resource cultures are frequently underrepresented due to the insufficient training data available from these cultures. LLMs\u2019 cultural bias constitutes a significant bottleneck in human-AI collaboration and considerably impedes AI democracy. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Tackling cultural bias necessitates that a large language model (LLM) acknowledges cultural differences [Hofstede, 1984]. Kovavc et al. Kova\u02c7c et al. [2023] and Want et al. Wang et al. [2023d] thought LLMs have enough knowledge of all cultures and devised prompt engineering technologies to induce LLMs to exhibit specific cultural perspectives. However, they are not effective, especially in low-resource cultures with limited data. Another line of work pre-trained culturally aware LLMs and then fine-tuned on specific datasets [Chan et al., 2023, Nguyen et al., 2023b, Pipatanakul et al., 2023, Abbasi et al., 2023, Lin and Chen, 2023]. They require the collection of large-scale pre-training and fine-tuning datasets and extensive computing resources, thus are not affordable to ordinary researchers and cannot handle low-resource culture. To date, training culturally aware LLMs at affordable costs remains a challenge. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose CultureLLM, a costeffective4 solution to incorporate cultural differences into LLMs. Technically speaking, CultureLLM is inspired by the well-known fact that LLMs are inevitably not robust to the style and format of the prompts [Zhu et al., 2023], indicating that we can further leverage such a weakness to further improve the performance of LLMs by enriching the prompts. In particular, we focus on cultural values in this work. As shown in Figure 1, CultureLLM consists of three steps: sampling, semantic data augmentation, and finetuning. Inspired by Attitude-Behavior Consistency theory [Fazio and Zanna, 1981] which emphasizes that people\u2019s opinion is consistent with their behaviors, we use the World Values Survey (WVS) [Survey, 2022] as seed data. Then, we devise a semantic data augmentation approach to generate semantically equivalent samples. The aim is to generate semantic equivalent inputs, ", "page_idx": 1}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/88000c33e817aec0fe71646887e4ada9294dd54dc5573d561ea3f95d10ff7420.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Overview of CultureLLM. CultureLLM consists of three steps: sampling, semantic data augmentation, and fine-tuning. Culture-specific and unified CultureLLM can be fine-tuned. ", "page_idx": 1}, {"type": "text", "text": "thus we can get the ground-truth from seed data directly. Finally, CultureLLM is obtained by finetuning both the seed and the generated data. WVS is a public opinion poll that contains people\u2019s opinions on cultural topics from different countries. To be specific, we select 50 seed samples from WVS, covering 7 topics: \u201csocial values\", \u201cmigration\", \u201csecurity\", \u201cscience and technology\", \u201creligious values\", \u201cethical values and norms\", and \u201cpolitical interest and political participation\". Using these generated samples and answers from people in different cultures, we fine-tune specific and unified LLMs: specific LLMs are tailored for each culture such as CultureLLM-Ar for Arabic and CultureLLM-Tr for Turkish; unified LLMs (CultureLLM-One) are one LLM that fits all cultures.5 ", "page_idx": 1}, {"type": "text", "text": "We build 9 specific CultureLLM and a CultureLLM-One covering both high- and low-resource cultures: Arabic culture, Bengali culture, Chinese culture, English culture, German culture, Korean culture, Portuguese culture, Spanish culture, and Turkish culture. Then we evaluated them on 8 culture-related downstream tasks: offensive language detection, hate speech detection, stance detection, toxicity detection, threat detection, bias detection, abusive detection, spam detection, and an open-ended generative task. We have 60 test sets of 68, 672 samples in total. Experiments show that CultureLLM fine-tuned on GPT-3.5 significantly outperforms GPT-3.5 by $8.1\\%$ and outperforms Gemini pro [Google, 2023] by $9.5\\%$ on average F1 score, achieving comparable or even better performance with GPT-4. Our human study of 50 people demonstrates that the augmentation method can generate semantically equivalent samples. We further interpret the rationale behind its effectiveness by exploring the fine-tuning data size and case studies. Finally, results on Big-Bench Hard [Suzgun et al., 2022] and GSM8K [Cobbe et al., 2021] indicate that CultureLLM is resistant to catastrophic forgetting. CultureLLM also supports fine-tuning LLMs of open-source models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our contributions are three-fold: ", "page_idx": 2}, {"type": "text", "text": "1. We presented CultureLLM, a cost-effective fine-tuning solution to build culturally-aware LLMs.   \n2. We proposed semantic data augmentation, an augmentation approach to generate high-quality and diverse training data for LLMs.   \n3. We conducted extensive experiments in a wide range of cultures and LLMs, showing that LLMs performs consistently well in all downstream tasks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Cultural Problem and Solution in LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Previous efforts have shown that LLMs exhibit the same cultural problems as in human society. Niszczota and Janczak [2023] proved that GPT-4 can replicate the cross-cultural differences for each personality factor through large-scale experiments. Meanwhile, other works also found that LLMs can reflect cultural bias and dominance in human society [Liu et al., 2023b, Cao et al., 2023, Masoud et al., 2023, Naous et al., 2023, Wang et al., 2023d, Johnson et al., 2022], e.g., Western culture dominance, since the major training corpus such as Pile [Gao et al., 2020] is in English. ", "page_idx": 2}, {"type": "text", "text": "The ideal solution is to improve the cultural awareness of LLMs. There are mainly two types of approach: prompt engineering and pre-training. Kova\u02c7c et al. [2023], Wang et al. [2023d] thought LLMs as superpositions of cultural perspectives, which can be prompted to targeted cultural perspectives. while Rao et al. [2023] encoded cultural values in the prompts. Although PE is cheap, its effectiveness is challenged, especially in low-resource cultures where LLMs lack such cultural knowledge due to lack of representation in pre-training data. Another line of research is pre-training and fine-tuning [Chan et al., 2023, Nguyen et al., 2023b, Pipatanakul et al., 2023, Abbasi et al., 2023, Lin and Chen, 2023] that trains culturally-aware LLMs for different cultures by collecting large-scale pre-training datasets and then performed fine-tuning for better alignment. While they achieved great performance, this approach is too expensive and time-consuming, thus it is difficult to apply to more cultures and countries. They still suffer from a low-resource culture problem where the pre-training data are difficult to collect. MaLA-500 [Lin et al., 2024] trained a new LLM on Llama 2 to cover 534 languages, which is resource intensive. ", "page_idx": 2}, {"type": "text", "text": "2.2 Data Augmentation for LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Human-annotated data are high-quality but expensive. Due to the strong generation ability of LLMs, many works focused on leveraging data augmentation for LLMs. Yu et al. [2023], Liu et al. [2023a] used LLMs to augment the math data and then fine-tuned with those data. Li et al. [2023] synthesized data with two designed modules: self-augmentation and self-curation. Chen et al. [2024] introduced a self-play mechanism, where LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. There are also other uses for synthetic data, such as knowledge distillation [Wang et al., 2023c] and improving text embedding tasks [Wang et al., 2023a]. Our data augmentation approach also adopts LLMs for data generation, but we add controllable modules such as template editing, synonym replacement, and semantic filter to ensure the diversity and semantic equivalence of the generated samples. It can also be used as a general augmentation method in other applications. ", "page_idx": 2}, {"type": "text", "text": "Efforts in cultural datasets [Nguyen et al., 2023a, Fung et al., 2022] focus on cultural common sense and norms. However, they generate data from only the English or Chinese corpus and thus may contain cultural bias toward other cultures. In contrast, World Values Survey (WVS) [Survey, 2022] is a large-scale pool that contains answers from people of different cultures, thus providing more objective cultural values from specific cultures. ", "page_idx": 2}, {"type": "text", "text": "This work is also related to value alignment [Ji et al., 2023, Shen et al., 2023, Yao et al., 2023] to align the values of LLMs with human\u2019s by designing algorithms for value measurement and behavior alignment. In contrast, this work primarily emphasizes value understanding with the potential to be extended for value alignment. For instance, semantic augmentation can be used to generate training data for alignment-related tasks. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 CultureLLM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Cultural differences are prevalent in various cultures and backgrounds, leading to an impact on outcomes in downstream applications such as hate speech and biased language. To address the gap between low-source cultural data collection and its wide applications, we design CultureLLM by fine-tuning an LLM on data generated by our novel semantic data augmentation approach leveraging the sensitivity of LLMs on prompts [Zhu et al., 2023]. Figure 1 presents an overview of CultureLLM, where the first step is to sample a subset of data from an existing World Value Survey (WVS) [Survey, 2022] that represents different opinions (answers) towards the same value questions given by native users. The adoption of WVS is inspired by Attitude-Behavior Consistency theory [Fazio and Zanna, 1981], which emphasizes the strong relationship between attitude and behavior. Therefore, WVS serves as an ideal seed for data augmentation.6 After sampling, the second step is to generate augmented data using our proposed semantic augmentation approach (Section 3.3) and then fine-tune a CultureLLM for each specific culture such as CultureLLM-Ar for Arabic culture and CultureLLMTr for Turkish culture. ", "page_idx": 3}, {"type": "text", "text": "Generally speaking, we use $D_{d}~=~\\{(x_{j},y_{j}^{d})\\}_{j=1}^{n}$ to denote the seed and $\\begin{array}{l}{{\\ensuremath{\\mathcal D}_{d}^{\\prime}~=~g(\\ensuremath{\\mathcal D}_{d})~=}}\\end{array}$ $\\{(x_{j}^{\\prime},y_{j}^{d})\\}_{j=1}^{n^{\\prime}}$ as the augmented data with $g(\\cdot)$ the augmentation algorithm. Note that the question $x$ here is the same in all cultures in WVS and is the cultural index denoting different answers to the same question $x$ . For example, for a question $x{=}^{\\bullet}\\mathbb{D}{\\circ}$ you agree with on the whole, men make better political leaders than women do?\u201d, the answer $y=$ Disagree if $d\\,=\\,\\mathtt{E n g l i s h}$ ; and $y=$ Strongly agree if $d=\\mathtt{A r a b i c}$ . Therefore, we only augment the question $x$ to be $x^{\\prime}$ but retain the same opinion $y$ as the original $x$ . We also denote vanilla LLM and CultureLLM as $f$ and $f^{\\star}$ , respectively. Then, denoting $\\ell$ as the loss function, our learning objective is formulated as: $f^{\\star}=\\arg\\operatorname*{min}_{f}\\mathbb{E}_{(x_{j},y_{j}^{d})\\in\\{\\mathcal{D}_{d},g(\\mathcal{D}_{d})\\}}\\bar{[}\\ell(f(x_{j}),y_{j}^{d})]$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The sampling process should follow two principles: 1) cover as many cultural topics as possible and 2) sample questions that can be clearly answered by LLMs. Based on the two principles, we manually select $n=50$ questions and rewrite them in the Question-Answer (QA) format, covering 7 topics, namely social values, security, science and technology, religious values, ethical values and norms, political interest and political participation, and migration. The details can be found in Appendix B.1. ", "page_idx": 3}, {"type": "text", "text": "3.3 Semantic Data Augmentation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Samples from WVS are not enough to fine-tune, which can be augmented by our semantic augmentation approach. In a formal sense, semantic augmentation retains the original ground-truth opinions $(y_{d})$ from different cultures and only generates semantically equivalent questions $(x)$ . A naive augmentation approach is to directly ", "page_idx": 3}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/0d341ed8225cce0d3f1709e387907b262c21056a0af9d089b568e87574a310da.jpg", "img_caption": ["Left: Semantic Templates Generation Right: Intact Sentence Generation "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Details of semantic data augmentation. First, semantic templates are generated via rephrasing, semantic flitering, and sentence parsing. Then, training samples are generated by context-aware synonyms replacement and semantic filtering. ", "page_idx": 3}, {"type": "text", "text": "use strong LLMs such as GPT-4 to generate new samples [Walters and Wilder, 2023], which ", "page_idx": 4}, {"type": "text", "text": "could introduce mode collapse, as generation quality can only be controlled by prompts. Furthermore, since LLMs could suffer from cultural bias, directly generating cultural data using prompts could lead to unexpected or even erroneous results. ", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 2, the augmentation consists of two stages: semantic template generation and intact sentence generation. The first stage generates several semantically equivalent but stylistically different sentences and parses them into semantic templates. The second stage then generates samples by replacing certain words in the semantic templates. Such an augmentation can naturally introduce more diversities: The first stage increases sentence-level diversities and the second improves the word-level diversities. ", "page_idx": 4}, {"type": "text", "text": "Semantic Template Generation This stage generates semantically equivalent question templates $\\mathcal{T}=\\{t_{i}\\}_{i=1}^{k}$ based on $x\\in{\\mathcal{D}}_{d}$ . The generation process is nontrivial since there are two challenges ahead: 1) the naturalness and diversities and 2) the semantic preservation. We solve the first challenge by using GPT-4 as the generator with certain prompts to ensure naturalness and diversity. Then, we solve the second challenge by introducing a semantic preservation filter $\\mathcal{P}$ to measure the similarity between the original and generated sentences.7 ", "page_idx": 4}, {"type": "text", "text": "We first use the prompt \u201cCould you please generate [n] sentences that (1) have different sentence structures and (2) have the same meaning with the following sentence: $x_{i}$ \u201d to generate $n$ sentences using GPT-4. Then, we denote the embedding of the original sentence and the generated sentences as $z\\,=\\,\\mathcal{P}(x)$ and $z^{\\prime}=\\mathcal{P}(t)$ , respectively. Then we compute their similarity score $c=\\cos(z,z^{,})$ . If $c$ passes the threshold value $\\tau$ , the generated sentence will be reserved: $\\mathcal{T}=\\{t_{i}|\\cos(\\mathcal{P}(t_{i}),\\mathcal{P}(x_{j}))\\,>\\,\\tau\\},\\forall x_{j}\\,\\in\\,\\mathcal{D}_{d}$ . Specifically, for sample \u201cDo you agree with One of my main goals in life has been to make my parents proud?\u201d, we generate $m$ samples using GPT-4, which are then go through the semantic fliter $\\mathcal{P}$ to eventually retain $k(k\\leq m)$ semantically equivalent sentences, e.g. \u201cDo you agree with Making my parents dignified has always been one of my primary objectives in life?\" and \u201cDo you agree with The goal to bring vanity to my parents has been a central life goal of mine?\" ", "page_idx": 4}, {"type": "text", "text": "To diversify the generated data, we parse the $n$ sentences to find the appropriate components to replace, which construct the templates. For efficiency, we use NLTK [Loper and Bird, 2002] to find replaceable words, such as adjectives, adverbs, nouns, and verbs. The semantic templates are like \u201cDo you agree with The [x] to bring [x] to my parents has been a [x] life [x] of mine?\" where $\\mathbf{\\Omega}^{\\ast}[\\mathbf{x}]\\mathbf{\\Omega}^{\\ast}$ is the replaceable part. In total, we generate $k$ templates for each sample $x_{j}\\in{\\mathcal{D}}_{d}$ . ", "page_idx": 4}, {"type": "text", "text": "Intact Sample Generation This step is to replace synonyms in templates to generate finetuning samples. We apply GPT-4 to generate context-aware synonyms in the templates and randomly replace some of them. To further preserve semantics, we also use the semantic preservation filter. After filtering, we generate $m$ samples for each template $t_{i}~\\in~{\\cal T}$ , and get $n^{\\prime}\\;\\;=\\;\\;m n k$ samples for all $\\begin{array}{r l r}{x_{j}}&{{}\\in}&{\\mathcal{D}_{d}}\\end{array}$ in total. For example, intact samples for template \u201cDo you agree with The [x] to bring $[\\mathbf{x}]$ to my parents has been a [x] life [x] of mine?\" could be \u201cDo you agree with The goal to bring pride to my parents has been a central life goal of mine?\" and \u201cDo you agree with The hope to bring vanity to my parents has been a central life goal of mine?\" Our human study in Section 4.6 shows that augmentation can generate high-quality and semantically equivalent sentences. ", "page_idx": 4}, {"type": "text", "text": "3.4 Fine-tuning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since culture is a complex construct, we use languages spoken by geographical cultures (represented by countries) in WVS to represent broader cultures and arrive at a set of 9 cultures in total. In cases where a language is spoken by more than one geographical culture, we pick representative countries and use the average of all answers as groundtruth. Our final set of cultures represented as described above is Arabic (for which we select Jordan and Iraq), Spanish (for which we select Mexico and Argentina), Bengali, Chinese, English, German, Korean, Portuguese, and Turkish. ", "page_idx": 4}, {"type": "text", "text": "Finally, CultureLLM is obtained by fine-tuning an LLM on the combination of the seed and the generated data. Specifically, we fine-tune two types of LLMs: 1) culture-specific LLMs for each language such as CultureLLM-Ar and CultureLLM-Bn, and 2) one unified LLM for all languages, denoted as CultureLLM-One. Culture-specific LLMs are tailored by setting $d$ , namely, $\\{\\mathcal{D}_{d},\\mathcal{D}_{d}^{\\prime}\\}$ . On the other hand, CultureLLM-One is trained on all datasets: $\\{\\dot{\\mathcal{D}}_{d},\\mathcal{D}_{d}^{\\prime}\\}_{d\\in\\mathrm{all}}$ language to serve as a unified LLM for all cultures. Note that since all languages have the same input question $x$ but different answers $y$ , we need to manually write different prompts in the instruction to distinguish them. For example, we add \u201cYou are an Arabic chatbot that knows Arabic very well \" before Arabic samples. CultureLLM can be used in cultural downstream applications. In the following, we use CultureLLM to denote specific LLM and CultureLLM-One for unified LLM. ", "page_idx": 5}, {"type": "text", "text": "Remark: Note that the WVS is all in English, where we focus on cultural differences in opinions regardless of their native language. Thus, we do not perform fine-tuning for other languages due to the shortage of their training data and rely on cross-lingual transfer. Multilingual tasks for cultures can still benefit from fine-tuned models in English, as models can learn the basic values from the opinions [Moussa\u00efd et al., 2013, Jin et al., 2023]. Our experiments in Section 5.1 further demonstrate that fine-tuning on English data can outperform fine-tuning on native data that are translated from the original English version. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We fine-tuned a CultureLLM-One and 9 specific CultureLLM for 9 languages: Arabic (Ar), Bengali (Bn), Chinese (Zh), English (En, United States), German (De), Korean (Ko), Portuguese $({\\bf P}{\\bf\\hat{l}})$ , Spanish (Es), and Turkish (Tr). These cultures are diverse and represent both high- and low-resource regimes and thus can serve as representative evaluation. ", "page_idx": 5}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We adopt culture-related public datasets in specific languages for evaluation. In total, we have 59 test sets, covering 9 languages and containing 68, 607 test samples. We test on 56 binary classification and 3 multi-classification tasks to detect: offensive language, hate speech, stance, toxicity, threat, bias, abusive, and spam in zero-shot evaluation. For example, we ask LLMs to judge whether the sentence contains offensive language, hate speech, or biased speech. Details are shown in Appendix B.2. Furthermore, we generate an open-ended generation dataset for evaluation in Section 4.3. ", "page_idx": 5}, {"type": "text", "text": "Baselines and details. We fine-tune CultureLLM using the GPT-3.5 (0613) [OpenAI, 2023a] finetuning API due to its efficiency and compared with two state-of-the-art LLMs, namely Gemini pro [Google, 2023] and GPT-4 (1104) [OpenAI, 2023b]. We further compare with cultural specific pre-trained models SeaLLM [Nguyen et al., 2023b], TaiwanLLM [Lin and Chen, 2023] and CultureBank [Shi et al., 2024]. We also compare this with retrieval augmentation (RAG), which enhances LLMs by searching for related information and adding it to context [Lewis et al., 2020]. To implement RAG, we search for information about each culture on Wikipedia and append them in a system prompt, as detailed in Appendix C.3. Finally, we fine-tuned CultureLLM using Llama-2-70bchat [Touvron et al., 2023] as the base model for reproduction (Section 5.3). As for prompt setup, since our goal is to make LLMs better align with people from different cultures, we add a system prompt \u201cYou are an [x] chatbot that knows [x] very well\u201d where [x] is a certain language before each input. For metrics, we use macro F1 score for all tasks except for CValues [Xu et al., 2023] where we use the automatic evaluation script provided by the paper. For data augmentation, we set $k=5$ , $m=2$ , and $\\tau=0.8$ . Evaluation prompts are in Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present the average results for each culture and task in Figure 3(a) 8 and more detailed results are shown in Appendix D.1. Our conclusions are as follows. First, both specific and unified CultureLLM achieve a great improvement over other approaches, and specific CultureLLM achieves the best performance. Concretely speaking, CultureLLM significantly outperforms GPT-3.5 (by $8.1\\%$ ), ", "page_idx": 5}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/befe22d50c131ad9a4e3f9e1f894e986e75459278ea00293406644d3cc5af257.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: (a) The main results averaged by cultures (left) and by tasks (right). Both CultureLLM and CultureLLM-One significantly outperform CultureLLM and Gemini with CultureLLM achieving the best performance comparable to GPT-4. (b) Ablation study. $\\mathbf{\\dot{\\Phi}}+\\mathbf{W}\\mathbf{V}\\mathbf{S}^{\\bullet}$ denotes the fine-tuned models using only the 50 samples from WVS, $\\mathrm{^{6}+W V S+a}^{\\circ}$ denotes fine-tuning using the WVS samples and the generated samples in step 1 of our data augmentation (i.e., using only GPT-4 to generate), and $\\mathbf{\\Phi}+\\mathbf{W}\\mathbf{V}\\mathbf{S}\\mathbf{+a}\\mathbf{+b}^{\\ast}$ denotes the complete process of our algorithm. ", "page_idx": 6}, {"type": "text", "text": "Gemini (by $9.5\\%$ ), and RAG (by $7.94\\%$ ), achieving performance comparable to GPT-4 and even better on some tasks. Second, CultureLLM-One exceeds GPT-3.5 by more than $4\\%$ on 59 tasks, while inferior to culture-specific models, suggesting that a single LLM might not be the best solution to solve cultural tasks with low resources, since data from different cultures could intertwine with each other. Third, in terms of cultures, CultureLLM achieves the best performance in English, Chinese, and Spanish cultures while showing no obvious improvement in Korean culture, where all four models have a similar performance. We infer that the reason could be that these base models have less exposure to Korean culture. ", "page_idx": 6}, {"type": "text", "text": "Then, we analyze the performance on both low-resource and high-resource language tasks. As shown in Figure 7, CultureLLM shows excellent performance in both types of tasks and outperforms GPT-4 on a large scale in high-resource tasks. Finally, we evaluated an extremely low-resource culture, Greek, in Appendix D.6 and compared CultureLLM with other cultural-specific models SeaLLM [Nguyen et al., 2023b], TaiwanLLM [Lin and Chen, 2023] and CultureBank [Shi et al., 2024] in Appendix D.2, which shows that our CultureLLM can also improve performance. The correlation with the WVS data is in Appendix D.3, showing that the performance improvement does not come from the seed data in WVS. ", "page_idx": 6}, {"type": "text", "text": "4.3 Results on Open-ended Generation Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To evaluate the performance of CultureLLM on open-ended tasks, we construct a dataset using GPT-4, containing 65 open-ended questions, ", "page_idx": 6}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/e632e3c4c42e870187ab7089a83250d0c82c965344cbd09718a3ab9f1294ad6e.jpg", "table_caption": ["Table 1: WinRate results on generation tasks. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "which cover the seven topics in WVS. The prompt setting for dataset generation can be found in Appendix C.2. We evaluated the outputs of GPT-3.5 and CultureLLM using Gemini $\\mathrm{Pro}^{9}$ . We also devised a metric WinRate $=(s_{C u l t u r e L L M}-s_{C h a t G P T})/65$ , where $s$ represents the number of acceptances by Gemini Pro. Positive WinRate means CultureLLM wins GPT-3.5 and vice versa. As shown in Table 1, CultureLLM performs better than GPT-3.5 on 8 out of 9 cultures, demonstrating its effectiveness in generation tasks. ", "page_idx": 6}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the effectiveness of our semantic data augmentation approach by comparing it with the following variants: GPT-3.5, CultureLLM (WVS), CultureLLM $(\\mathrm{WVS+a})$ ), and CultureLLM $(\\mathrm{WVS+a+b})$ ), where CultureLLM (WVS) denotes the fine-tuned models using only the 50 samples from WVS, CultureLLM $(\\mathrm{WVS+a})$ denotes fine-tuning using 50 WVS samples and the generated samples in step 1 of our data augmentation (i.e., only using semantic templates), and CultureLLM $(\\mathrm{WVS+a+b})$ ) denotes the complete process of our algorithm. Note that $\\mathbf{\\dot{\\mu}}\\mathbf{W}\\mathbf{V}\\mathbf{S}\\mathbf{+a}^{\\mathbf{\\mu}}$ \u2019 denotes the naive baseline of only using GPT-4 to generate samples. ", "page_idx": 6}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/de06429465a475a41ca5d39b4e0acfde822855e79eec257e9f50881c61c66b4b.jpg", "img_caption": ["Figure 4: (a) Results on different numbers of fine-tuning samples with perplexity score and diversity gain above. (b) Results of fine-tuneing on English (En ft) and local languages (local ft). It shows that fine-tuning on English outperforms fine-tuning on local languages. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3(b) shows that fine-tuning using the 50 seeds in WVS can inconsistently improve and impair performance on different tasks such as the decrease in Korean tasks. While WVS data are of high quality, we see gains with our generated data which leads to improvements on most tasks. The average performance on Korean tasks also improves by CultureLLM. Figure 3(b) also demonstrates that the two steps in our semantic data augmentation approach are useful and necessary. Ablation for CultureLLM-One is in Appendix D.4, which also shows the effectiveness of our approach. ", "page_idx": 7}, {"type": "text", "text": "4.5 Effectiveness Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We analyze the effectiveness of CultureLLM by controlling the number of generated data, computing the perplexity score, and presenting case studies. ", "page_idx": 7}, {"type": "text", "text": "First, we analyze the impact of the generation size. As illustrated in Chen et al. [2024], the diversity and quality of datasets are important in training LLMs. Hence, infinite or too many generated samples might hurt the performance due to possible mode collapse. In this section, we control the number of generated data and empirically analyze its impact. Specifically, we fine-tune 4 CultureLLM with $\\{0,100,500,1000\\}$ generated samples appended to the original WVS data set. As shown in Figure 4(b), as the number of fine-tuning data increases, performance across most of tasks get improved; but when the number is greater than 500, performance on all tasks declines. ", "page_idx": 7}, {"type": "text", "text": "Then we analyze the diversity of the generated data by computing two metrics: perplexity [Marion et al., 2023, Wang et al., 2023b] and diversity gain [Bilmes, 2022] (Appendix C.1), as shown in the upper right in Figure 4(b), where we observe the consistency between these two metrics and the fine-tuning performance: the 500 generated data lead to the best perplexity and diversity gain. The reason may be that these 500 samples are enough for GPT-3.5 to understand the knowledge of seed data, and more samples can cause overftiting and decreased performance. Additionally, although the augmentation approach only generates different samples by varying sentence and word styles, the diversities can also be increased. This suggests that variations in samples can improve the diversity of datasets. ", "page_idx": 7}, {"type": "text", "text": "As in the cases shown in Figure 8, responses from GPT-3.5 often analyze input from multiple perspectives and call on to be respectful and kind, rather than providing clear and straightforward opinions. In some cases, GPT-3.5 says that it cannot determine the intentions behind the sentence without context, while CultureLLM provides clear opinions most of the time. The reason behind this may be that we fine-tune Cul", "page_idx": 7}, {"type": "text", "text": "Table 2: The semantic similarity of generated samples and seed samples are judged by 50 human participants, GPT-4 and Gemini Pro. The scores range from 1 to 5, where 1 represents \u201cdefinitely not\u201d and 5 represents \u201cperfectly\u201d. ", "page_idx": 7}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/0ef38efa00b8eaf8259d654ca52d98a1fb409a0b78d905c72d7b4a113b5c93e4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "tureLLM to learn opinions from specific culture, so that it can be more aligned with the corresponding culture when faced with cultural differences or cultural confilcts. However, GPT-3.5 is aimed to serve people from different cultures. Thus, it prefers to give a neutral response to not conflict with any cultures. However, the worst consequence is that it cannot provide useful responses to the problems related to cultural differences. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.6 The Effectiveness of the Augmented Data: A Human Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyze the effectiveness of the augmented data through human evaluators. We hire 50 people who have a high exposure to English (i.e., majoring in English) to check if our generated sentences are semantically equivalent to the seed data. The information of the participants and the training procedure are in Appendix F. We sample 100 pairs of (seed, generation) samples and let each participant rank their similarities by giving a score of 1 to 5, with 5 representing the most similar. We also use GPT-4 and Gemini Pro as evaluators. The average results in Table 2 demonstrate that the semantic similarity passes $96.5\\%$ , implying that our augmentation approach can increase the quantity while retaining the similarity. ", "page_idx": 8}, {"type": "text", "text": "We also conduct experiments on generation tasks. Figure 8 shows the responses of GPT-3.5 and CultureLLM in four different cultures. The results show that CultureLLM can generate more accurate, direct, and useful responses than GPT-3.5. To be specific, GPT-3.5 always generate long responses, which do not give useful information and just call on to be respectful, while CultureLLM give accurate and direct responses. This is very important for user experience. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Augmenting Multilingual Data vs. English Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "CultureLLM are fine-tuned on English data, since the training corpus of LLMs such as the GPT series are mostly in English and English may be the choice for LLMs to understand the opinions of other cultures. What about the performance of LLMs fine-tuned in a culturally specific language? We also fine-tuned GPT-3.5 [OpenAI, 2023a] on multilingual data that are translated from English data and compare with CultureLLM. The results are shown in Figure 4(a), indicating that the models fine-tuned in English perform better than the models fine-tuned in other languages. The reason behind this may be the model\u2019s inherent capabilities in English have been shown to be superior [Ahuja et al., 2023] than other languages, which again emphasizes the importance of collecting large-scale data for pre-training. This study demonstrates that, in low-resource settings without collecting large-scale training data, the augmentation approach could be useful for fine-tuning. ", "page_idx": 8}, {"type": "text", "text": "5.2 Fine-tuning vs. Forgetting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A potential dilemma is that fine-tuning an LLM on specific tasks might face catastrophic forgetting of its original capabilities. In this section, we explore the forgetting of CultureLLM in two general datasets: BIG-Bench-Hard (BBH) [Suzgun et al., 2022] and GSM8K [Cobbe et al., 2021]. BBH contains 21 tasks covering both semantic understanding and logical reasoning tasks. GSK8K is a widely used data set to evaluate mathematical ability. For BBH, we sample 100 samples for each task to test, due to cost savings. We compare each CultureLLM with the GPT-3.5 baseline model in Figure 5(a). The results show that CultureLLM does not decrease performance in most benchmarks and can even improve their results, such as on BBH. This suggests that there might be some latent relations between the cultural data and the general benchmarks, thus fine-tuning on cultural data can benefit general reasoning abilities. ", "page_idx": 8}, {"type": "text", "text": "5.3 CultureLLM on Open-sourced LLMs: Llama2 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Although all main experiments in this work are performed using the OpenAI GPT-3.5 fine-tuning API [OpenAI, 2023a] due to its efficiency and simplicity, our CultureLLM also supports fine-tuning on open-source LLMs for better quality control and reproducibility. In this section, we show an initial experiment using Llama2-70b-chat as the base model to fine-tune a CultureLLM-Llama2-70b. The results in Figure 5(b) show that CultureLLM-Llama-70b outperforms the base Llama model by $2.17\\%$ on average, showing the effectiveness of fine-tuning CultureLLM on open-source models. The details of fine-tuning and more Llama2 results are in Appendix E. The results indicate that CultureLLM is a general approach to improve the ability of LLMs to understand the culture. ", "page_idx": 8}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/0090e1fe2640abc9f9e5183d447831d333fe5b6764e55ef26c1f86b12ec6e63a.jpg", "img_caption": ["Figure 5: (a) Analysis on catastrophic forgetting on BBH and GSM8K. The red line denotes the results of GPT-3.5. For BBH, we show the average results of 21 tasks in this figure. The $\\mathbf{X}$ -axis represents models and the y-axis represents performance. (b) CultureLLM-Llama-70b averaged by cultures (left) and by tasks (right), which outperforms the vanilla Llama model by $2.17\\%$ on average. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 Implication and Societal Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In essence, recognizing and valuing cultural differences is paramount for the enrichment of our global community. Embracing diversity stimulates innovation and creativity, contributing to the development of novel ideas and solutions. Our work contributes to solving the cultural difference problem in LLMs and tackling the problem of data scarcity in low-resource cultures. The limited availability of data from these cultures hinders understanding and addressing specific needs and concerns. For example, the lack of representation in datasets may perpetuate biases and disparities, hindering the development of inclusive technologies and services. Our approach represents an effective and resource-saving method to bridge the data gap in low-resource cultures, empowering these communities and enabling more accurate, inclusive, and impactful decision-making processes. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Cultural difference is essential to the prosperity of the world. In this paper, we proposed CultureLLM, a cost-effective solution to fine-tune culture-aware LLMs. We sampled a small number (50) of samples from the World Value Survey and then generated augmented data through our novel semantic data augmentation. On 59 datasets on 9 cultures, CultureLLM outperformed GPT-3.5 and Gemini with comparable or even better results than GPT-4. ", "page_idx": 9}, {"type": "text", "text": "This work has the following limitations. First, due to resource and time constraints, we did not implement CultureLLM on large-scale open-source models. Second, we only adopted classification tasks for evaluation since multilingual generative tasks are expensive for automatic evaluation. Finally, the sample diversity is only in sentence and word levels. In the future, we plan to add more diversities to enrich the generated data. ", "page_idx": 9}, {"type": "text", "text": "Disclaimer ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper leveraged GPT-4 to generate sentences and synonyms, whose quality were manually checked to ensure responsible usage. Throughout this paper, the authors remain neutral towards the opinions from all different cultures and respect their diversities. The human study was conducted following local laws and regulations. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Turkish Spam V01. UCI Machine Learning Repository, 2019. DOI: https://doi.org/10.24432/C5WG7F. ", "page_idx": 9}, {"type": "text", "text": "Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi, Hassan Naderi, and Behrouz Minaei Bidgoli. Persianllama: Towards building first persian large language model. arXiv preprint arXiv:2312.15713, 2023. ", "page_idx": 9}, {"type": "text", "text": "Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528, 2023. ", "page_idx": 10}, {"type": "text", "text": "aimansnigdha. Bangla-abusive-comment-dataset. https://github.com/aimansnigdha/Bangla-AbusiveComment-Dataset, 2018. ", "page_idx": 10}, {"type": "text", "text": "Miguel \u00c1 \u00c1lvarez-Carmona, Estefan\u0131a Guzm\u00e1n-Falc\u00f3n, Manuel Montes-y G\u00f3mez, Hugo Jair Escalante, Luis Villasenor-Pineda, Ver\u00f3nica Reyes-Meza, and Antonio Rico-Sulayes. Overview of mex-a3t at ibereval 2018: Authorship and aggressiveness analysis in mexican spanish tweets. In Notebook papers of 3rd sepln workshop on evaluation of human language technologies for iberian languages (ibereval), seville, spain, volume 6, 2018. ", "page_idx": 10}, {"type": "text", "text": "Arjun Appadurai. Modernity at large: Cultural dimensions of globalization. U of Minnesota P, 1996. ", "page_idx": 10}, {"type": "text", "text": "Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter. In Proceedings of the 13th international workshop on semantic evaluation, pages 54\u201363, 2019. ", "page_idx": 10}, {"type": "text", "text": "Shiladitya Bhattacharya, Siddharth Singh, Ritesh Kumar, Akanksha Bansal, Akash Bhagat, Yogesh Dawer, Bornini Lahiri, and Atul Kr. Ojha. Developing a multilingual annotated corpus of misogyny and aggression. In Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying, pages 158\u2013168, Marseille, France, May 2020. European Language Resources Association (ELRA). URL https://aclanthology.org/2020.trac-1.25/. ", "page_idx": 10}, {"type": "text", "text": "Jeff Bilmes. Submodularity in machine learning and artificial intelligence. arXiv preprint arXiv:2202.00132, 2022. ", "page_idx": 10}, {"type": "text", "text": "Y Cao, L Zhou, S Lee, L Cabello, M Chen, and D Hershcovich. Assessing cross-cultural alignment between chatgpt and human societies: An empirical study. arxiv. Preprint posted online on March, 31, 2023. ", "page_idx": 10}, {"type": "text", "text": "Tommaso Caselli, Valerio Basile, Jelena Mitrovi\u00b4c, Inga Kartoziya, and Michael Granitzer. I feel offended, don\u2019t be abusive! implicit/explicit messages in offensive and abusive language. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6193\u20136202, 2020. ", "page_idx": 10}, {"type": "text", "text": "\u00c7ag\u02d8r\u0131 \u00c7\u00f6ltekin. A corpus of turkish offensive language on social media. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 6174\u20136184, Marseille, France, 2020. URL https://www.aclweb.org/anthology/2020.lrec-1.758. ", "page_idx": 10}, {"type": "text", "text": "Alex J Chan, Jos\u00e9 Luis Redondo Garc\u00eda, Fabrizio Silvestri, Colm O\u2019Donnel, and Konstantina Palla. Harmonizing global voices: Culturally-aware models for enhanced content moderation. arXiv preprint arXiv:2312.02401, 2023. ", "page_idx": 10}, {"type": "text", "text": "Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. ", "page_idx": 10}, {"type": "text", "text": "Shammur Absar Chowdhury, Hamdy Mubarak, Ahmed Abdelali, Soon-gyo Jung, Bernard J Jansen, and Joni Salminen. A multi-platform arabic news comment dataset for offensive language detection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6203\u20136212, 2020. ", "page_idx": 10}, {"type": "text", "text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. ", "page_idx": 10}, {"type": "text", "text": "daanVeer. Korean hatespeech dataset. https://github.com/daanVeer/HateSpeech_dataset, 2020. ", "page_idx": 10}, {"type": "text", "text": "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In Proceedings of the 11th International AAAI Conference on Web and Social Media, ICWSM \u201917, pages 512\u2013515, 2017. ", "page_idx": 10}, {"type": "text", "text": "Angel Felipe Magnossao de Paula and Ipek Baris Schlicht. Ai-upv at iberlef-2021 detoxis task: Toxicity detection in immigration-related web news comments using transformers and statistical models. arXiv preprint arXiv:2111.04530, 2021.   \nRogers P. de Pelle and Viviane P. Moreira. Offensive comments in the brazilian web: a dataset and baseline results. 2017.   \nWerner Delanoy. What is culture. The Cambridge handbook of intercultural communication, pages 17\u201334, 2020a.   \nWerner Delanoy. What Is Culture?, page 17\u201334. Cambridge Handbooks in Language and Linguistics. Cambridge University Press, 2020b. doi: 10.1017/9781108555067.003.   \nRussell H Fazio and Mark P Zanna. Direct experience and attitude-behavior consistency. In Advances in experimental social psychology, volume 14, pages 161\u2013202. Elsevier, 1981.   \nElisabetta Fersini, Paolo Rosso, Maria Anzovino, et al. Overview of the task on automatic misogyny identification at ibereval 2018. Ibereval@ sepln, 2150:214\u2013228, 2018.   \nYi R Fung, Tuhin Chakraborty, Hao Guo, Owen Rambow, Smaranda Muresan, and Heng Ji. Normsage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly. arXiv preprint arXiv:2210.08604, 2022.   \nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \nGoogle. Gemini. https://deepmind.google/technologies/gemini/#introduction, 2023.   \nHASOC. Hasoc2020. https://hasocfire.github.io/hasoc/2020/index.html, 2020.   \nGeert Hofstede. Culture\u2019s consequences: International differences in work-related values, volume 5. sage, 1984.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nF Husain. Osact4 shared task on offensive language detection: Intensive preprocessing-based approach. arxiv 2020. arXiv preprint arXiv:2005.07297, 2020.   \nJiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023.   \nZhuoren Jiang, Zhe Gao, Guoxiu He, Yangyang Kang, Changlong Sun, Qiong Zhang, Luo Si, and Xiaozhong Liu. Detect camouflaged spam content via stoneskipping: Graph and text joint embedding for chinese character variation representation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP2019). ACM, 2019.   \nYiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, and Srijan Kumar. Better to ask in english: Cross-lingual evaluation of large language models for healthcare queries. arXiv e-prints, pages arXiv\u20132310, 2023.   \nRebecca L Johnson, Giada Pistilli, Natalia Men\u00e9dez-Gonz\u00e1lez, Leslye Denisse Dias Duran, Enrico Panai, Julija Kalpokiene, and Donald Jay Bertulfo. The ghost in the machine has an american accent: value conflict in gpt-3. arXiv preprint arXiv:2203.07785, 2022.   \nSanaa Kaddoura and Safaa Henno. Dataset of arabic spam and ham tweets. Data in Brief, 52(10990): 4, 2024.   \nKaggle. Jigsaw-multilingual-toxicity. https://www.kaggle.com/code/tarunpaparaju/jigsawmultilingual-toxicity-eda-models, 2019.   \nKaggle. 5k turkish tweets with incivil content. https://www.kaggle.com/datasets/kbulutozler/5kturkish-tweets-with-incivil-content, 2021.   \nKaggle. turkish offensive language detection. https://www.kaggle.com/datasets/toygarr/turkishoffensive-language-detection, 2022.   \nHabibe Karayi\u02d8git, \u00c7i\u02d8gdem \u02d9Inan Ac\u0131, and Ali Akda\u02d8gl\u0131. Detecting abusive instagram comments in turkish using convolutional neural network and machine learning methods. Expert Systems with Applications, 174:114802, 2021.   \nGrgur Kovac\u02c7, Masataka Sawayama, R\u00e9my Portelas, C\u00e9dric Colas, Peter Ford Dominey, and PierreYves Oudeyer. Large language models as superpositions of cultural perspectives. arXiv preprint arXiv:2307.07870, 2023.   \nJean Lee, Taejun Lim, Heejun Lee, Bogeun Jo, Yangsok Kim, Heegeun Yoon, and Soyeon Caren Han. K-MHaS: A multi-label hate speech detection dataset in Korean online news comment. In Proceedings of the 29th International Conference on Computational Linguistics, pages $3530-$ 3538, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022.coling-1.311.   \nJoao A Leite, Diego F Silva, Kalina Bontcheva, and Carolina Scarton. Toxic language detection in social media for brazilian portuguese: New dataset and multilingual analysis. arXiv preprint arXiv:2010.04543, 2020.   \nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459\u20139474, 2020.   \nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023.   \nPeiqin Lin, Shaoxiong Ji, J\u00f6rg Tiedemann, Andr\u00e9 F. T. Martins, and Hinrich Sch\u00fctze. Mala-500: Massive language adaptation of large language models, 2024.   \nYen-Ting Lin and Yun-Nung Chen. Taiwan llm: Bridging the linguistic divide with a culturally aligned language model. arXiv preprint arXiv:2311.17487, 2023.   \nBingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving> $80\\%$ on gsm8k with small language models. arXiv preprint arXiv:2312.09241, 2023a.   \nChen Cecilia Liu, Fajri Koto, Timothy Baldwin, and Iryna Gurevych. Are multilingual llms culturallydiverse reasoners? an investigation into multicultural proverbs and sayings. arXiv preprint arXiv:2309.08591, 2023b.   \nEdward Loper and Steven Bird. Nltk: The natural language toolkit. arXiv preprint cs/0205028, 2002.   \nMax Marion, Ahmet \u00dcst\u00fcn, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564, 2023.   \nReem I Masoud, Ziquan Liu, Martin Ferianc, Philip Treleaven, and Miguel Rodrigues. Cultural alignment in large language models: An explanatory analysis based on hofstede\u2019s cultural dimensions. arXiv preprint arXiv:2309.12342, 2023.   \nJihyung Moon, Won Ik Cho, and Junbum Lee. BEEP! Korean corpus of online news comments for toxic speech detection. In Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, pages 25\u201331, Online, July 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.socialnlp-1.4.   \nMehdi Moussa\u00efd, Juliane E K\u00e4mmer, Pantelis P Analytis, and Hansj\u00f6rg Neth. Social influence and the collective dynamics of opinion formation. PloS one, 8(11):e78433, 2013. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Hamdy Mubarak, Hend Al-Khalifa, and AbdulMohsen Al-Thubaity. Overview of osact5 shared task on arabic offensive language and hate speech detection. In Proceedinsg of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools with Shared Tasks on Qur\u2019an QA and Fine-Grained Hate Speech Detection, pages 162\u2013166, 2022. ", "page_idx": 13}, {"type": "text", "text": "Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, et al. Blend: A benchmark for llms on everyday knowledge in diverse cultures and languages. arXiv preprint arXiv:2406.09948, 2024. ", "page_idx": 13}, {"type": "text", "text": "Tarek Naous, Michael J Ryan, and Wei Xu. Having beer after prayer? measuring cultural bias in large language models. arXiv preprint arXiv:2305.14456, 2023. ", "page_idx": 13}, {"type": "text", "text": "Tuan-Phong Nguyen, Simon Razniewski, Aparna Varde, and Gerhard Weikum. Extracting cultural commonsense knowledge at scale. In Proceedings of the ACM Web Conference 2023, pages 1907\u20131917, 2023a. ", "page_idx": 13}, {"type": "text", "text": "Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, et al. Seallms\u2013large language models for southeast asia. arXiv preprint arXiv:2312.00738, 2023b. ", "page_idx": 13}, {"type": "text", "text": "Pawe\u0142 Niszczota and Mateusz Janczak. Large language models can replicate cross-cultural differences in personality. arXiv preprint arXiv:2310.10679, 2023. ", "page_idx": 13}, {"type": "text", "text": "OpenAI. Chatgpt. https://chat.openai.com/, 2023a. ", "page_idx": 13}, {"type": "text", "text": "OpenAI. Gpt-4 technical report, 2023b. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Nedjma Ousidhoum, Zizheng Lin, Hongming Zhang, Yangqiu Song, and Dit-Yan Yeung. Multilingual and multi-aspect hate speech analysis. In Proceedings of EMNLP. Association for Computational Linguistics, 2019. ", "page_idx": 13}, {"type": "text", "text": "Juan Carlos Pereira-Kohatsu, Lara Quijano-S\u00e1nchez, Federico Liberatore, and Miguel CamachoCollados. Detecting and monitoring hate speech in twitter. Sensors, 19(21):4654, 2019. ", "page_idx": 13}, {"type": "text", "text": "Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarnmongkol, Ruangsak Patomwong, Pathomporn Chokchainant, and Kasima Tharnpipitchai. Typhoon: Thai large language models. arXiv preprint arXiv:2312.13951, 2023. ", "page_idx": 13}, {"type": "text", "text": "Zeses Pitenis, Marcos Zampieri, and Tharindu Ranasinghe. Offensive language identification in greek. arXiv preprint arXiv:2003.07459, 2020. ", "page_idx": 13}, {"type": "text", "text": "Flor Miriam Plaza-del Arco, Arturo Montejo-R\u00e1ez, L Alfonso Urena Lopez, and Mar\u00eda-Teresa Mart\u00ednValdivia. Offendes: A new corpus in spanish for offensive language research. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 1096\u20131108, 2021. ", "page_idx": 13}, {"type": "text", "text": "Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, and Monojit Choudhury. Ethical reasoning over moral alignment: A case and framework for in-context ethical policies in llms. arXiv preprint arXiv:2310.07251, 2023. ", "page_idx": 13}, {"type": "text", "text": "Nauros Romim, Mosahed Ahmed, Hriteshwar Talukder, and Md Saiful Islam. Hate speech detection in the bengali language: A dataset and its baseline evaluation. In Proceedings of International Joint Conference on Advances in Computational Intelligence: IJCACI 2020, pages 457\u2013468. Springer, 2021. ", "page_idx": 13}, {"type": "text", "text": "Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, and Preslav Nakov. Solid: A large-scale semi-supervised dataset for offensive language identification. arXiv preprint arXiv:2004.14454, 2020. ", "page_idx": 13}, {"type": "text", "text": "Bj\u00f6rn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and Michael Wojatzki. Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis. In Michael Bei\u00dfwenger, Michael Wojatzki, and Torsten Zesch, editors, Proceedings of NLP4CMC III: 3rd Workshop on Natural Language Processing for Computer-Mediated Communication, volume 17 of Bochumer Linguistische Arbeitsberichte, pages 6\u20139, Bochum, sep 2016. ", "page_idx": 13}, {"type": "text", "text": "Paul R\u00f6ttger, Haitham Seelawi, Debora Nozza, Zeerak Talat, and Bertie Vidgen. Multilingual hatecheck: Functional tests for multilingual hate speech detection models. arXiv preprint arXiv:2206.09917, 2022.   \nOmar Sharif and Mohammed Moshiul Hoque. Tackling cyber-aggression: Identification and finegrained categorization of aggressive texts on social media using weighted ensemble of transformers. Neurocomputing, 490:462\u2013481, 2022.   \nTianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. arXiv preprint arXiv:2309.15025, 2023.   \nWeiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Raya Horesh, Rog\u00e9rio Abreu de Paula, Diyi Yang, et al. Culturebank: An online community-driven knowledge base towards culturally aware language technologies. arXiv preprint arXiv:2404.15238, 2024.   \nHoyun Song, Soo Hyun Ryu, Huije Lee, and Jong C Park. A large-scale comprehensive abusiveness detection dataset with multifaceted labels from reddit. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 552\u2013561, 2021.   \nWorld Values Survey. World values survey. https://www.worldvaluessurvey.org/wvs.jsp, 2022.   \nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv. org/abs/2307.09288, 2023.   \nFrancielle Vargas, Isabelle Carvalho, Fabiana Rodrigues de G\u00f3es, Thiago Pardo, and Fabr\u00edcio Benevenuto. HateBR: A large expert annotated corpus of Brazilian Instagram comments for offensive language and hate speech detection. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 7174\u20137183, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.777.   \nWilliam H Walters and Esther Isabelle Wilder. Fabrication and errors in the bibliographic citations generated by chatgpt. Scientific Reports, 13(1):14045, 2023.   \nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023a.   \nPeiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023b.   \nRuida Wang, Wangchunshu Zhou, and Mrinmaya Sachan. Let\u2019s synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models. arXiv preprint arXiv:2310.13671, 2023c.   \nWenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael R Lyu. Not all countries celebrate thanksgiving: On the cultural dominance in large language models. arXiv preprint arXiv:2310.12481, 2023d.   \nZeerak Waseem and Dirk Hovy. Hateful symbols or hateful people? predictive features for hate speech detection on twitter. In Proceedings of the NAACL student research workshop, pages 88\u201393, 2016.   \nMichael Wiegand, Melanie Siegel, and Josef Ruppenhofer. Overview of the germeval 2018 shared task on the identification of offensive language. 2018.   \nGuohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, and Jingren Zhou. Cvalues: Measuring the values of chinese large language models from safety to responsibility. arXiv 2307.09705, 2023.   \nJing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie. From instructions to intrinsic human values\u2013a survey of alignment goals for big models. arXiv preprint arXiv:2308.12014, 2023.   \nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \nMarcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon Derczynski, Zeses Pitenis, and \u00c7a\u02d8gr\u0131 \u00c7\u00f6ltekin. Semeval-2020 task 12: Multilingual offensive language identification in social media (offenseval 2020). arXiv preprint arXiv:2006.07235, 2020.   \nJingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng Wang, Minlie Huang, Xin Jiang, Qun Liu, and Helen Meng. Towards identifying social bias in dialog systems: Frame, datasets, and benchmarks. arXiv preprint arXiv:2202.08011, 2022.   \nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Discussion on the Relationship between Culture and Language 17 ", "page_idx": 16}, {"type": "text", "text": "B Details on Data 18   \nB.1 World Values Survey and seed data 18   \nB.2 Details on experimental datasets 19 ", "page_idx": 16}, {"type": "text", "text": "C Evaluation Metrics and Prompts 24 ", "page_idx": 16}, {"type": "text", "text": "C.1 Evaluation metrics 24   \nC.2 Prompts for evaluation 25   \nC.3 Prompts for RAG 26 ", "page_idx": 16}, {"type": "text", "text": "D Experimental Results 26 ", "page_idx": 16}, {"type": "text", "text": "D.1 Detailed results 26   \nD.2 Comparative analysis with other cultural specific models 28   \nD.3 Results analysis on WVS seed data . 28   \nD.4 Ablation on CultureLLM-One 30   \nD.5 Analysis on low-resource language tasks and high-resource language tasks 30   \nD.6 Results on other low-resource cultures 30   \nD.7 Case study 31   \nE.1 Setup 31   \nE.2 Detailed results 32 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "F Details on Human Study 32 ", "page_idx": 16}, {"type": "text", "text": "A Discussion on the Relationship between Culture and Language ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We strongly agree that language is not equal to, but only a part of culture. But using language to study culture is possible due to the following aspects: ", "page_idx": 16}, {"type": "text", "text": "1. Existing literature on culture understanding shows that culture boundaries are fluid, dynamic and uncertain. Delanoy emphasizes that cultures are not homogeneous or static entities but are fluid and dynamic. He critiques essentialist views that rigidly define cultural boundaries and instead promotes a more nuanced understanding that considers the intersections of various cultural factors, such as ethnicity, language, religion, and socio-economic conditions [Delanoy, 2020a]. Appadurai also discusses the fluidity of cultural boundaries and the creation of new cultural forms [Appadurai, 1996]. Cultural boundaries can be geographical regions, language, religion and so on. Based on above statements, using language as cultural boundaries is reasonable. ", "page_idx": 16}, {"type": "text", "text": "2. Existing NLP works on culture also leverage labguage as culture boundaries. Naous et al. [2023] focuses on Arabic and English culture. Wang et al. [2023d] focuses on 8 different cultures: English, Chinese, French, Russian, German, Arabic, Japanese and Korean. Liu et al. [2023b] also use language to split different cultures. The authors work on English, German, Russian, Bengali, Chinese, and Indonesian culture. Myung et al. [2024] is a hand-crafted benchmark for evaluate diverse cultures. They also use languages as culture boundaries. ", "page_idx": 16}, {"type": "text", "text": "3. Most downstream benchmarks are classified via language and we cannot get more finegrained perspectives. For example, if we want to evaluate the performance of Arabic model, we can find benchmarks in Arabic culture. But if we use regions as cultural boundaries, we can\u2019t find benchmarks in Morocco and Jordan cultures. ", "page_idx": 17}, {"type": "text", "text": "B Details on Data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 World Values Survey and seed data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The following is the basic information of WVS and Table 3 shows the 50 seed data. ", "page_idx": 17}, {"type": "text", "text": "World Values Survey (WVS) is a global research initiative dedicated to rigorously examining the diverse array of social, political, economic, religious, and cultural values held by people worldwide. Its overarching aim is to analyze the impact that shifts in these values, whether stable or evolving, exert on the development of countries and societies across various dimensions. Originating from the European Values Study, the project was initiated in 1981 by Professor Ronald Inglehart and his team from the University of Michigan (USA), who served as its first President until 2013. Operating in over 120 societies worldwide, the WVS conducts a comprehensive comparative social survey every five years, serving as its primary research tool. Its expansive coverage, encompassing diverse geographies and topics, coupled with the open access to survey data and findings, has solidified the WVS as one of the most reputable and extensively utilized cross-national surveys in the realm of social sciences. Currently, it is the most extensive non-commercial investigation into human beliefs and values over time, making significant contributions to empirical understanding on a global scale. ", "page_idx": 17}, {"type": "text", "text": "The preferred sampling method for the World Values Survey involves selecting a full probability sample from the population aged 18 years and older. Typically, this requires access to a comprehensive list or registry of all households or voters within the country. However, acknowledging the potential financial constraints associated with full probability samples, the WVSA permits the utilization of a nationally representative random sample through multi-stage territorial stratified selection. Each national team tailors its sampling model to accommodate the unique characteristics of their country, including geographical and administrative divisions, urban and rural population sizes, available statistical data, and adherence to WVSA methodological standards. For a country to employ the WVS-7 sample model, it must meet specific criteria: - WVS surveys are required to cover all residents (not just citizens) in a country in the age of 18 years older and older; - PI\u2019s can lower the minimum age limit as long as the minimum required sample size for the $18+$ population is achieved; - Obtained sample should be representative, i.e. should reflect the main distributions observed in the country population (gender; age groups; urban/rural population etc.). ", "page_idx": 17}, {"type": "text", "text": "We use WVS-7 (2017-2022) dataset V5.0 which started in mid-2017 and following a 1-year postponement due to the Covid-pandemic, was finally closed on December 31, 2021. It covers 80 countries. Samples must be representative of all people in the age 18 and older residing within private households in each country, regardless of their nationality, citizenship or language. The minimum sample size - i.e. the number of completed interviews which are included into the national data-set in most of countries is 1200. Countries with greater population size and diversity apply samples of $N{=}1500$ to $\\scriptstyle\\mathrm{N}=5000$ . Countries with the population below 2 million people apply samples of $N{=}1000$ . ", "page_idx": 17}, {"type": "text", "text": "Regards on the select criteria for seed data, we select both low-resource cultures and high-resource cultures for comparison. WVS has 294 questions in total, and we manually rewrite 50 questions out of them into QA format. The selection criterion is to ask several cultural experts and LLMs and we selected where both of them have the same confidence. ", "page_idx": 17}, {"type": "text", "text": "The rewrite process is to manually rewrite the questions (multi-choices format) from WVS into QA format. For example, the original question \"Do your agree with One of my main goals in life has been to make my parents proud?\" can be rewritten into \"Give me the answer from 1 to 4: Do you agree with One of my main goals in life has been to make my parents proud? 1. Strongly agree 2. agree 3. Disagree 4. Strongly disagree. You can only choose one option.\" We select seed data that contains each topic mentioned in WVS. ", "page_idx": 17}, {"type": "text", "text": "Before consolidating the average responses from representative countries, we undertake a meticulous manual review of their answers to ensure consistency on identical questions. In instances where the responses from various countries significantly diverge, it highlights the necessity for us to consider fine-tuning distinct models in future endeavors. This careful preliminary check ensures that our analysis remains robust and that any future model adaptations are informed by a clear understanding of cultural variances. ", "page_idx": 17}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/d163218da56c6660badf0d2d917ec93e77fce7a9efac0c6d9ec2464036b5d15f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.2 Details on experimental datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The statistics of the datasets are shown in Table 4 and we provide the detailed instructions of them in the following. ", "page_idx": 18}, {"type": "text", "text": "B.2.1 Arabic ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "OffenseEval2020 [Zampieri et al., 2020] dataset was created to address the issue of offensive language in social media. It aims to use computational methods to identify offensive, aggressive, and hate speech in user-generated content, providing a multilingual dataset in five languages (Arabic, Danish, ", "page_idx": 18}, {"type": "text", "text": "Table 4: A brief introduction of the 8 evaluation tasks and 59 datasets. We list both the name and the size of test sets. For instance, \u201cOffensEval2020(2000) [Zampieri et al., 2020]\u201d denotes that there are 2000 test samples in the dataset OffensEval2020. ", "page_idx": 19}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/85ae739a44e4f42802255903abba71bea92961d9c7d8ffb719b68de2d5a96fd0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "English, Greek, Turkish). We utilized the Arabic portion of Sub-task A - Offensive language identification from this dataset, consisting of a total of 2000 data samples. ", "page_idx": 19}, {"type": "text", "text": "OSCAT4 [Husain, 2020] dataset aims to detect and categorize offensive language in Arabic tweets, with two sub-tasks: detecting if a post is offensive or not, and identifying the offensive content type as hate speech or not hate speech. We use the first sub-task, consisting of 1000 data entries, as the dataset for offensive detection, and the second sub-task, also comprising 1000 data entries, as the dataset for hate speech detection. ", "page_idx": 19}, {"type": "text", "text": "Multi-Platform [Chowdhury et al., 2020] dataset is a collection of 4000 comments in Dialectal Arabic from social media platforms, focusing on offensive language. It is intended for studying offensive language in news comments published by international news organizations. We utilized a total of 1000 annotated data samples indicating whether they are offensive and 675 annotated data samples indicating whether they are vulgar. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "OSACT5 [Mubarak et al., 2022] dataset consists of 12,698 Arabic tweets collected between June 2016 and November 2017, labeled for offensiveness and fine-grained hate speech types using emojis commonly found in offensive communications, providing a resource for offensive and hate speech detection and classification tasks. The dataset consists of three subtasks: offensiveness detection, hate speech detection, and fine-grained hate speech detection. We used 2,541 samples for each of these tasks. ", "page_idx": 20}, {"type": "text", "text": "ASHT [Kaddoura and Henno, 2024] dataset contains 132,421 Arabic tweets collected from Twitter, classified as either ham (non-spam) or spam, providing a valuable resource for researchers in Arabic natural language processing (NLP) and serving as a benchmark for research in Arabic NLP, cybersecurity, data science, and social network analysis. We used a subset of 1,000 samples for the spam detection section. ", "page_idx": 20}, {"type": "text", "text": "B.2.2 Bengali ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "TRAC2020 [Bhattacharya et al., 2020] dataset is a multilingual annotated corpus of social media comments, encompassing misogynistic and aggressive comments in Indian English, Hindi, and Indian Bangla. It consists of over 20,000 comments and is annotated at two levels - aggression (overtly aggressive, covertly aggressive, and non-aggressive) and misogyny (gendered and nongendered). Baseline experiments were conducted to develop misogyny classifiers for the three languages. TRAC2020 consists of two tasks: Aggression Detection and Misogynistic Aggression Detection. We utilized 1,000 data samples for each of Task 1 and Task 2. ", "page_idx": 20}, {"type": "text", "text": "BAD [Sharif and Hoque, 2022] dataset is a novel Bengali aggressive text dataset (called \u2019BAD\u2019) with two-level annotation, designed to identify and classify aggressive content in Bengali language. It achieves high accuracy through a weighted ensemble technique and outperforms other machine learning and deep learning baselines, with a weighted f1-score of $93.43\\%$ for identification and $93.11\\%$ for categorization tasks. We utilized a subset of one thousand data samples as the Offensive dataset. ", "page_idx": 20}, {"type": "text", "text": "Hate Speech [Romim et al., 2021] dataset consists of 30,000 social media user comments, covering seven categories including sports, entertainment, religion, politics, crime, celebrities, TikTok, and memes. It has been annotated through crowdsourcing and expert validation for research purposes in detecting hate speech in Bengali language. The dataset also provides benchmark experimental results for multiple deep learning models and pre-trained Bengali word vectors. We utilized 1,000 data samples from the dataset for Hate Detection. ", "page_idx": 20}, {"type": "text", "text": "BACD [aimansnigdha, 2018] dataset is a dataset for the Bengali language, consisting of a total of 10,200 data points with annotations for toxic, threat, obscene, insult, and racism labels. We utilized 1,000 data points from this dataset for Threat Detection and Bias Detection tasks respectively. ", "page_idx": 20}, {"type": "text", "text": "B.2.3 Chinese ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "CCS [Jiang et al., 2019] dataset consists of two real-world spam datasets: one is an SMS dataset, and the other is a product review dataset. Both datasets were manually labeled by professionals as spam or regular emails, and their sizes and label distributions were summarized. We utilized 1000 data samples from this dataset for Spam Detection. ", "page_idx": 20}, {"type": "text", "text": "CDial-Bias [Zhou et al., 2022] Dataset is the first annotated Chinese social bias dialog dataset, utilized to establish a benchmark for measuring dialog bias and evaluate Chinese generative models for social bias presence. We utilized 1000 data samples from it for bias detection. ", "page_idx": 20}, {"type": "text", "text": "CValues [Xu et al., 2023] is a Chinese human values evaluation benchmark that measures the alignment ability of large language models in terms of safety and responsibility, providing both manual and automatic evaluation to assess their performance and identify areas for improvement. We utilized 1712 data samples from the dataset for Stance detection. ", "page_idx": 20}, {"type": "text", "text": "B.2.4 English ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "SOLID [Rosenthal et al., 2020] dataset is an expanded dataset containing over nine million English tweets labeled in a semi-supervised fashion. It significantly improves the performance of identifying specific types and targets of offensive language when combined with the OLID dataset, particularly at lower levels of the offensive language taxonomy. We utilized 1,000 data points from the dataset for Offensive Detection. ", "page_idx": 21}, {"type": "text", "text": "MLMA [Ousidhoum et al., 2019] dataset is a new multilingual multi-aspect hate speech analysis dataset, which is used to evaluate state-of-the-art multilingual multitask learning approaches and improve hate speech detection and classification in general. We utilized 1000 data samples from the dataset for Hate Detection and Toxicity Detection respectively. ", "page_idx": 21}, {"type": "text", "text": "HOF [Davidson et al., 2017] dataset uses crowd-sourcing to collect tweets containing hate speech keywords and employs a multi-class classifier to distinguish between tweets containing hate speech, only offensive language, and those with neither. It addresses the challenge of automatically detecting hate speech on social media while separating it from other instances of offensive language. We used a subset of 1000 data samples for Hate Detection. ", "page_idx": 21}, {"type": "text", "text": "JMT [Kaggle, 2019] dataset is a machine learning dataset designed to identify toxic comments in online conversations, aiming to build models that can filter out rude, disrespectful, or potentially conversation-disrupting comments to create a safer and more collaborative internet environment. We used 1000 data samples each from the Threat Detection and Toxicity Detection datasets. ", "page_idx": 21}, {"type": "text", "text": "B.2.5 Germany ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "GermEval2018 [Wiegand et al., 2018] dataset is used for identifying offensive language in German tweets, including both coarse-grained binary classification tasks and fine-grained multi-class classification tasks. We used 3,531 data points for Offensive Detection. ", "page_idx": 21}, {"type": "text", "text": "IWG [Ross et al., 2016] dataset aims to assess the feasibility of reliably annotating hate speech and explore the consistency between existing definitions and subjective ratings. The results indicate low reliability in users\u2019 judgments of hate speech, suggesting a need for more detailed annotation instructions. Each data instance in the dataset was annotated by two experts, and we selected 469 instances with annotations from both experts for Hate Detection, denoted as IWG_1 and IWG_2 respectively. ", "page_idx": 21}, {"type": "text", "text": "HASOC2020 [HASOC, 2020] dataset is a multilingual research forum and data challenge that offers tasks for identifying problematic content in English, German, and Hindi. It consists of over 10,000 annotated tweets from Twitter, and includes both coarse-grained and fine-grained classification tasks. We utilized a subset of 850 German language data from the HASOC dataset for Hate Detection. ", "page_idx": 21}, {"type": "text", "text": "Multilingual HateCheck [R\u00f6ttger et al., 2022] is a comprehensive dataset of functional tests for hate speech detection models in ten languages, addressing the need for more effective models and uncovering critical weaknesses for monolingual and cross-lingual applications. We utilized 1000 data points from the German section of the dataset for Hate Detection. ", "page_idx": 21}, {"type": "text", "text": "B.2.6 Korean ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "K-MHaS [Lee et al., 2022] is a multi-label dataset consisting of $109\\mathbf{k}$ utterances from Korean news comments, designed for hate speech detection. It effectively handles Korean language patterns, provides multi-label classification with 1 to 4 labels, and considers subjectivity and intersectionality. Strong baseline experiments using Korean-BERT-based language models show that KR-BERT with a sub-character tokenizer performs the best by recognizing decomposed characters in each hate speech class. We utilized 1000 data samples from the dataset for Hate Detection. ", "page_idx": 21}, {"type": "text", "text": "HateSpeech [Moon et al., 2020] dataset is a collection of $9.4\\mathrm{K}$ manually labeled entertainment news comments in Korean, aimed at identifying toxic speech, social bias, and hate speech. It provides benchmarks using CharCNN, BiLSTM, and BERT models, with BERT achieving the highest performance. The dataset is made publicly available and open for competitions. We utilized 1000 data samples from the dataset for Hate Detection. ", "page_idx": 21}, {"type": "text", "text": "HateSpeech2 [daanVeer, 2020] dataset was created by the Natural Language Processing Laboratory (NLP) at Korea National University and it includes the original dataset, a vocabulary of offensive language, annotations, and dataset examples. The dataset is used for labeling malicious comments and has been built with word embeddings. We utilized 1000 data samples from the dataset for Hate Detection. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "AbuseEval [Caselli et al., 2020] is a newly created dataset that addresses issues in annotating offensive and abusive language, specifically considering the degree of explicitness, target presence, and contextual interaction across different abusive language phenomena. We utilized 1000 data samples from the dataset for Abusive Detection. ", "page_idx": 22}, {"type": "text", "text": "CADD [Song et al., 2021] is a comprehensive dataset for detecting abusive language in English Reddit posts, featuring multifaceted labels and contextual information, collected through large-scale crowdsourcing and yielding meaningful performance with state-of-the-art language models. We utilized 1000 data samples from the dataset for Abusive Detection. ", "page_idx": 22}, {"type": "text", "text": "Waseem [Waseem and Hovy, 2016] dataset, based on critical race theory, provides annotations for over 16k tweets and aims to detect hate speech on social media by analyzing linguistic features, extra-linguistic features, and a dictionary of the most indicative words in the data. We utilized 1000 data samples from the dataset for Abusive Detection. ", "page_idx": 22}, {"type": "text", "text": "B.2.7 Portuguese ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "OffComBR [de Pelle and Moreira, 2017] dataset is an annotated collection of offensive comments in Portuguese, gathered from news comment sections on the Brazilian web. It serves the purpose of classifying user-generated text as either positive or negative, providing a baseline for future research on the topic of hate speech detection in Portuguese. We utilized 1250 data samples from this dataset for offensive detection. ", "page_idx": 22}, {"type": "text", "text": "HateBR [Vargas et al., 2022] dataset is the first large-scale expert annotated corpus of Brazilian Instagram comments, specifically collected from politicians\u2019 accounts, providing binary/offensivenesslevel classification and nine hate speech groups, outperforming the current state-of-the-art for Portuguese language offensive language and hate speech detection. We utilized 1000 data samples from this dataset for offensive detection. ", "page_idx": 22}, {"type": "text", "text": "ToLD-Br [Leite et al., 2020] is a large-scale dataset for Brazilian Portuguese, consisting of annotated tweets categorized as toxic or non-toxic, aiming to detect and prevent the proliferation of toxicity in social media, addressing the need for multilingual approaches and models aware of different categories of toxicity. We take the label \u201cinsult\" from the dataset to represent the \u201cabusive\" label, and \u201chomophobia\" and \u201cmisogyny\" as the \u201cbias\" labels. We have selected 1000 data samples for Abusive Detection, 1000 samples for Bias Detection, and 1000 samples for Bias Detection. ", "page_idx": 22}, {"type": "text", "text": "B.2.8 Spanish ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "AMI [Fersini et al., 2018] dataset is a collection of Spanish and English tweets used for identifying misogyny, categorizing misogynistic behavior, and classifying targeted individuals, with contributions from multiple teams and countries. We used 1000 Spanish language data for offensive detection. ", "page_idx": 22}, {"type": "text", "text": "MEX-A3T [\u00c1lvarez-Carmona et al., 2018] dataset, from the track at IberEval 2018, comprises Mexican Spanish tweets and focuses on two tasks: author profiling, which aims to identify the residence and occupation of Twitter users, and aggressiveness detection, to distinguish between aggressive and non-aggressive tweets. This dataset was created specifically for these tasks and was analyzed and compared in a paper discussing the participants\u2019 results. We used 1000 data samples for offensive detection. ", "page_idx": 22}, {"type": "text", "text": "OffendES [Plaza-del Arco et al., 2021] dataset is a collection of 47,128 manually labeled Spanish comments from social media platforms, focusing on offensive language targeted at young influencers. It provides pre-defined offensive categories and includes confidence scores, enabling both multi-class classification and multi-output regression studies. We used 1000 data samples for offensive detection. ", "page_idx": 22}, {"type": "text", "text": "HatEval 2019 [Basile et al., 2019] dataset focuses on detecting hate speech against immigrants and women in Spanish and English Twitter messages. It includes two classification tasks: identifying the presence of hate speech and distinguishing between individual and group targets. HatEval was a popular SemEval-2019 task with numerous submissions and participant system analysis. We used 1000 data samples for hate detection. ", "page_idx": 22}, {"type": "text", "text": "HaterNet [Pereira-Kohatsu et al., 2019] dataset is an intelligent system used for monitoring and visualizing hate speech on Twitter. It provides a novel public dataset of Spanish hate speech, consisting of 6,000 expert-annotated tweets. We used 1000 data samples for hate detection. ", "page_idx": 23}, {"type": "text", "text": "DETOXIS [de Paula and Schlicht, 2021] dataset is designed for the task of detecting toxic comments in online news discussions related to immigration. It includes toxicity detection and toxicity level detection. Participating teams achieved good results using the BERT model on this dataset. We classified them into tags such as stereotype, improper, abusive, mockery, aggressiveness, and stance, and selected 1000 data samples for each category for Bias detection, Abusive detection, Aggressiveness detection, and Stance detection. ", "page_idx": 23}, {"type": "text", "text": "B.2.9 Turkish ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "SemEval-2020 [Zampieri et al., 2020] provided a new, large-scale semi-supervised training dataset of over nine million English tweets and expanded the task to include four new languages, allowing for cross-lingual training and analysis. We used 3528 data samples in Turkish for Offensive Detection. ", "page_idx": 23}, {"type": "text", "text": "OffenseCorpus [\u00c7\u00f6ltekin, 2020] is a corpus of Turkish offensive language, comprising randomly sampled micro-blog posts from Twitter. It contains 36,232 tweets collected over an 18-month period from April 2018 to September 2019. We used 1000 data samples for Offensive Detection. ", "page_idx": 23}, {"type": "text", "text": "OffenseKaggle [Kaggle, 2021] Dataset is a collection of Turkish tweets from Twitter, with around $40\\%$ of them containing offensive or vulgar content. We used 1000 data samples for Offensive Detection. ", "page_idx": 23}, {"type": "text", "text": "OffenseKaggle_2 [Kaggle, 2022] dataset is an enhanced version of an existing offensive language research dataset, which has been expanded and annotated using contextual data mining techniques. It addresses the issue of class imbalance in existing studies and provides a more comprehensive and robust dataset for Turkish offensive language detection tasks. We used 1000 data samples for Offensive Detection. ", "page_idx": 23}, {"type": "text", "text": "ATC [Karayi\u02d8git et al., 2021] dataset is a publicly available dataset for detecting abusive Turkish comments on Instagram. It consists of 10,528 abusive and 19,826 non-abusive comments, with sentiment annotations at the sentence level. We used 1000 data samples for Offensive Detection. ", "page_idx": 23}, {"type": "text", "text": "Turkish Spam [mis, 2019] dataset contains both spam and normal emails written in Turkish. A total of 330 spam emails and 496 normal emails were collected from several personal accounts. We used 825 pieces of data for spam detection. ", "page_idx": 23}, {"type": "text", "text": "OffenseCorpus [\u00c7\u00f6ltekin, 2020] dataset is a large collection of Turkish offensive language from Twitter micro-blog posts, annotated based on recent practices. It includes 36,232 randomly sampled tweets from April 2018 to September 2019, with $19\\%$ containing offensive language. We used 1000 of the data for Finegrained offensive detection. ", "page_idx": 23}, {"type": "text", "text": "C Evaluation Metrics and Prompts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Evaluation metrics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1.1 Perplexity ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The perplexity on a test dataset $D$ and a language model $\\mathcal{M}$ is computed as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{ppl}(D,\\mathcal{M})=\\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log P(x_{i}|\\mathcal{M})\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $N$ represents the total number of tokens in $D$ , $x_{i}$ represents the $i$ -th token in the test dataset, $P(x_{i}|\\mathcal{M})$ represents the probability of generating token $x_{i}$ given the model $\\mathcal{M}$ , and log is the natural logarithm. ", "page_idx": 23}, {"type": "text", "text": "In usual, a lower perplexity value indicates better performance of the model on the test data. However, for evaluating the data quality to train model, a higher perplexity value means it can bring more valuable information. ", "page_idx": 23}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/acd8b8210bb67575ae9382b038f004d5e5e622adc68a2c775bc4bfc9844539ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.1.2 Diversity Gain ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We use the diversity gain [Bilmes, 2022] to measure what extent can our generated dataset bring data diversity to the base dataset. The base dataset can be defined as $\\mathcal{D}_{b a s e}=\\overline{{\\{x_{i}=(q_{i},r_{i},a_{i})\\}_{i=1}^{N}}}$ with $N$ samples. The new generated dataset is defined as $\\mathcal{D}_{n e w}=\\{x_{i}=(q_{i},r_{i},a_{i})\\}_{i=1}^{M}$ with $M$ samples. And the diverse gain of $D_{n e w}$ relative to $D_{b a s e}$ can be expressed as: ", "page_idx": 24}, {"type": "equation", "text": "$$\nd_{g a i n}=\\frac{1}{M}\\sum_{x_{i}\\in\\mathcal{D}_{n e w}}\\operatorname*{min}_{x_{j}\\in\\mathcal{D}_{b a s e}}(\\lVert\\mathbf{f}(\\mathbf{x_{i}})-\\mathbf{f}(\\mathbf{x_{j}})\\rVert),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $f$ is the feature extractor, and we use OpenAI Embedding API text-embedding-ada-002 to extract features. ", "page_idx": 24}, {"type": "text", "text": "C.2 Prompts for evaluation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For evaluation, we input a system prompt to induce LLM to specific culture. For example, we add \u201cYou are an Arabic chatbot that knows Arabic very well.\u201d as system prompt for evaluation on Arabic understanding. As for the prompt for evaluation on different tasks, the detailed prompts are shown in Table 5. ", "page_idx": 24}, {"type": "text", "text": "Prompts for open-ended tasks: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Prompt for generate open-ended dataset: Can you generate some open-ended question according to the following questions: {Some questions on the same topic}   \n2. Prompt for evaluate open-ended dataset: Question: {q} Answer A: {ans 1} Answer B: {ans 2} Comparing these two answers, which answer is more socially aligned with country culture? Options: A. Answer A. B. Answer B.   \n3. Prompt for rating sentences similarity Rate the semantic similarity of two input sentences on a scale of 1 - definitely not to 5 - perfectly. Sentence 1: {item 1} Sentence 2: {item 2} ", "page_idx": 24}, {"type": "text", "text": "C.3 Prompts for RAG ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The detailed information retrieved via RAG is shown in Table 6. ", "page_idx": 25}, {"type": "text", "text": "D Experimental Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Detailed results ", "page_idx": 25}, {"type": "text", "text": "The detailed results are shown in Figure 6. ", "page_idx": 25}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/de3e51c60079ca7b8cd258bba3147184909cf63287a586861d79cc8791decbcd.jpg", "img_caption": ["Figure 6: Detailed results on all tasks and all cultures. "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/e93ea6a9321e97dc28471e6b2452434c990320833dc231aac5bcbf737bc7305d.jpg", "table_caption": ["Table 7: Comparison with the latest cultural specific LLMs. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.2 Comparative analysis with other cultural specific models ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We have conducted a comparative analysis with models designed for cultural understanding. Given the absence of models specifically tailored to Arabic, Bengali, German, Portuguese, Spanish, and Turkish cultures, our analysis focused on models with proficiency in Chinese and Korean cultures. Specifically, we examined SeaLLM [Nguyen et al., 2023b], which targets Southeast Asian cultural nuances, TAIWAN LLM [Lin and Chen, 2023], which is dedicated to Chinese cultural contexts, and CultureBank [Shi et al., 2024], which focuses on several cultural groups. Table 7 highlight our findings in the Chinese and Korean benchmarks. The results clearly demonstrate that our CultureLLMs significantly outperform both SeaLLM and TAIWAN LLM, showcasing their superior ability to understand cultural subtleties on a broader scale. ", "page_idx": 27}, {"type": "text", "text": "D.3 Results analysis on WVS seed data ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We analyze the relevance of each task with the WVS and investigate if it correlates with the experimental results. ", "page_idx": 27}, {"type": "text", "text": "Offensive language detect: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Cultural Context and Sensitivity to Offensive Language: The World Values Survey aims to capture cultural values and beliefs across different societies. One aspect of cultural values is the tolerance or acceptance of offensive language. In some cultures, certain words or expressions may be considered highly offensive, while in others they may be more tolerated or even commonly used. ", "page_idx": 27}, {"type": "text", "text": "2. Social Norms and Acceptance: The survey may reveal societal attitudes towards the use of offensive language in various contexts, such as in public discourse, media, or interpersonal communication. ", "page_idx": 27}, {"type": "text", "text": "Hate speech detect: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Societal Norms and Attitudes: The WVS provides data on societal norms, attitudes towards minorities, and levels of societal trust. This data can help understand the underlying societal conditions that might foster hate speech or, conversely, promote tolerance and inclusivity.   \n2. Cultural Context: Understanding the cultural context is crucial for effectively detecting and interpreting hate speech. The WVS offers a rich dataset for understanding cultural differences in values and norms, which can inform more nuanced hate speech detection algorithms that are sensitive to context and do not inadvertently suppress legitimate expressions of cultural or political dissent. ", "page_idx": 27}, {"type": "text", "text": "1. Understanding Contextual Influences on Stance: The WVS can provide the cultural and societal background needed to understand why certain stances are more prevalent in specific regions or among certain demographic groups. This context can be invaluable for interpreting the results of stance detection analyses, especially when comparing stances across different cultures and societies. ", "page_idx": 28}, {"type": "text", "text": "Toxicity detect: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": ". Reflection of Societal Norms in Online Behavior: The WVS provides insights into the prevailing norms and values within societies, which can indirectly inform the context within which toxic behavior manifests online. Understanding societal attitudes towards diversity, authority, individual freedom, and tolerance can help in interpreting the root causes of toxic behavior and devising appropriate responses. ", "page_idx": 28}, {"type": "text", "text": "2. Injection of More cultural nuances: Data from the WVS can inject more information that sensitive to cultural nuances and differences in value systems. This can prevent the misclassification of content that may be culturally specific or context-dependent, reducing the risk of censoring legitimate expressions of cultural or political identity. ", "page_idx": 28}, {"type": "text", "text": "Threat detect: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": ". Understanding Motivations and Behaviors: Insights from the WVS can help understand the cultural and societal contexts that may influence the behavior of individuals or groups posing threats. This knowledge can inform more targeted and effective threat detection and mitigation strategies that consider the root causes of conflict or aggression.   \n2. Cultural Sensitivity in Security Measures: Incorporating findings from the WVS can lead to more culturally sensitive security practices that respect local values and norms. This is crucial in global operations where misunderstanding cultural nuances can lead to ineffective or counterproductive security measures. ", "page_idx": 28}, {"type": "text", "text": "Bias detect: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Understanding Societal Norms and Attitudes: Insights from the WVS can help in understanding the cultural and societal norms that underlie biases. By analyzing patterns in global values and beliefs, we can identify prevalent stereotypes, prejudices, and discriminatory attitudes that may need to be addressed in bias detection efforts.   \n2. Injection of More cultural nuances: The WVS data can provide valuable context that are sensitive to cultural differences in values and norms. This is better equipped to detect and mitigate biases in data sets that reflect cultural nuances, ensuring that AI-driven decisions are fair and equitable across different societal contexts. ", "page_idx": 28}, {"type": "text", "text": "Abusive detect: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": ". Cultural Contexts of Abuse: The WVS can help identify cultural norms that influence perceptions of what constitutes abusive behavior. This is crucial for developing detection systems that are sensitive to cultural differences, ensuring that they can effectively identify abuse without mistakenly flagging culturally specific but non-abusive interactions. 2. Injection of More cultural nuances: Insights from the WVS can inform the development of more nuanced algorithms for detecting abusive behavior by providing context on societal values and norms. This can help in training models to recognize the subtle nuances that differentiate abusive from non-abusive communication in different cultural settings. 3. Evaluating Tolerance Levels: The WVS data can provide insights into societal tolerance levels towards different forms of behavior, including what might be considered abusive. This can help in assessing the urgency and type of interventions needed to address abusive behaviors in various cultural contexts. ", "page_idx": 28}, {"type": "text", "text": "Spam detect: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Cultural Variations in Communication: The WVS can shed light on cultural differences in communication styles and preferences, which can inform more nuanced spam detection ", "page_idx": 28}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/f21231e624782f67dfe643010a3919bb726e671a1dd77f21a2e8c068eaf2071a.jpg", "table_caption": ["Table 8: Ablation study for CultureLLM-One. "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/7609d80ed2a3144880c298cc4241bac7470bc13b27c5aebbcaea066d63bc494d.jpg", "img_caption": ["Figure 7: We compare CultureLLM with baselines on low-resource language tasks and high-resource language tasks. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "algorithms that are better able to distinguish between legitimate mass communications and spam in different cultural contexts.   \n2. Attitudes Towards Technology and Privacy: Insights from the WVS regarding societal attitudes towards technology use, privacy, and data protection can help in tailoring spam detection efforts to respect cultural norms and expectations. For instance, societies with a high value on privacy might be more receptive to stringent spam filters.   \n3. Attitudes Towards Technology and Privacy: Insights from the WVS regarding societal attitudes towards technology use, privacy, and data protection can help in tailoring spam detection efforts to respect cultural norms and expectations. For instance, societies with a high value on privacy might be more receptive to stringent spam filters. ", "page_idx": 29}, {"type": "text", "text": "D.4 Ablation on CultureLLM-One ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The ablation study on CultureLLM-One is shown in Table 8. ", "page_idx": 29}, {"type": "text", "text": "D.5 Analysis on low-resource language tasks and high-resource language tasks ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Figure 7 shows the comparison between low and high resource tasks. ", "page_idx": 29}, {"type": "text", "text": "D.6 Results on other low-resource cultures ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We fine-tuned a CultureLLM-el model specifically tailored for the Greek culture and assessed its efficacy on two Greek benchmarks focused on offensive language detection. Presented in the table below are the outcomes obtained from Zampieri et al. [2020], Pitenis et al. [2020]. Our findings indicate that, on average, CultureLLM-el surpasses gpt-3.5-turbo [OpenAI, 2023a] by $5.6\\%$ in performance. The results are shown in Table 9. ", "page_idx": 29}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/5a77a9a16783b56cee76a94f306fdad6b25b874b8fb4fd1f4fb1303ef65f568f.jpg", "table_caption": ["Table 9: Results on Greek culture "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "D.7 Case study ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Figure 8 show the responses from CultureLLM and ChatGPT on four different cultures. ", "page_idx": 30}, {"type": "image", "img_path": "sIsbOkQmBL/tmp/1cb0cba54d5bac961e37b28d8821381b01affcb49cd5bd2db96a1465b633a94d.jpg", "img_caption": ["Figure 8: The responses from ChatGPT and CultureLLM on four different cultures "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "E Fine-tuning on Llama and Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "E.1 Setup ", "page_idx": 30}, {"type": "text", "text": "We use Lora [Hu et al., 2021] to fine-tune Llama-70b-Chat. The setting for Lora are list below: ", "page_idx": 30}, {"type": "text", "text": "\u2022 lora_alpha: 16   \n\u2022 lora_dropout: 0.1   \n\u2022 r: 64   \n\u2022 bias: none   \n\u2022 task_type: CAUSAL_LM ", "page_idx": 30}, {"type": "text", "text": "The detailed setting for training are list below: \u2022 num_train_epochs: 6 ", "page_idx": 30}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/91bb477bc542c5b15d940a67cd8d048ce8919f20fb76cfef5e95f9e07aed7eaa.jpg", "table_caption": ["Table 10: More results on fine-tuning using Llama-2-70b model. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/c82a98b50874a12a37c2a02d72a3df253425a13d32c25b2bd2f1d738be05f0b2.jpg", "table_caption": ["Table 11: Results on forgetting experiments on Llama-2-70b "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "\u2022 er_device_train_batch_size: 4   \n\u2022 gradient_accumulation_steps: 1   \n\u2022 optim: paged_adamw_32bit   \n\u2022 learning_rate: 2e-4   \n\u2022 weight_decay: 0.001   \n\u2022 fp16: False   \n\u2022 bf16: False   \n\u2022 max_grad_norm: 0.3   \n\u2022 max_steps: -1   \n\u2022 warmup_ratio: 0.03   \n\u2022 group_by_length: True   \n\u2022 lr_scheduler_type: constant   \n\u2022 report_to: tensorboard ", "page_idx": 31}, {"type": "text", "text": "E.2 Detailed results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The results of fine-tuning Llama2 are shown in Table 10. It shows the similar trends as ChatGPT\u2019s results. We conduct forgetting experiments on Llama-2. As the results shown in Table 11, CultureLLM does not bring negative effect on general tasks. ", "page_idx": 31}, {"type": "text", "text": "F Details on Human Study ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Information on participant in human study are shown in Table 12. ", "page_idx": 31}, {"type": "text", "text": "Participants are asked to rate the 100 samples according to the following criterion: ", "page_idx": 31}, {"type": "text", "text": "1. Score 1: i. The sentences convey distinctly different ideas or concepts. ii. No apparent connection or shared meaning.   \n2. Score 2: i. Limited commonality in meaning, with noticeable disparities in wording. ii. Shared concepts but with significant differences in expression.   \n3. Score 3: i. Some overlap in meaning, but notable differences in wording or phrasing. ii. Context or emphasis might differ slightly.   \n4. Score 4: i. Minor variations in wording or structure, but the core meaning remains consistent. ii. Synonymous expressions and interchangeable terms are present.   \n5. Score 5: i. The sentences convey the same information using different words. ii. No discernible difference in meaning or context. ", "page_idx": 31}, {"type": "table", "img_path": "sIsbOkQmBL/tmp/ebe581c87a84b6200743deec2bae2499ab75582cc2dd11ab2cf6af675ac3324b.jpg", "table_caption": ["Table 12: Information on participants in human study "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 33}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 33}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 33}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 33}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 33}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 33}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Yes, they do. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the limitations in Section 6. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We conduct experiments on 60 downstream tasks and verify the effectiveness of CultureLLMs. The results are shown in Section 4 ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the setup and details in Section 4. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Our code is released in https://anonymous.4open.science/r/CultureLLM-DEE0. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 35}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We specify all the details in Section 4.1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We report the statistical analysis in Figure 3. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide those information in Sections 4.1 and 5.3. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We conform to the NeurIPS Code of Ethics in every respect. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We discuss in Section 5.4. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We check the data manually to make sure the safety of data. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We properly credited the the creators or original owners of assets and the license and terms of use explicitly are mentioned and properly respected. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our code and data are released. And it is well documented in README.md. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The details are described in Appendix F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The details are described in Appendix F. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]