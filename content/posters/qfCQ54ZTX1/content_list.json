[{"type": "text", "text": "Entity Alignment with Noisy Annotations from Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shengyuan Chen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qinggang Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computing   \nThe Hong Kong Polytechnic University Hung Hom, Hong Kong SAR   \nshengyuan.chen@connect.polyu.hk Department of Computing   \nThe Hong Kong Polytechnic University Hung Hom, Hong Kong SAR   \nqinggangg.zhang@connect.polyu.hk ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Junnan Dong ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wen Hua ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computing   \nThe Hong Kong Polytechnic University Hung Hom, Hong Kong SAR   \nhanson.dong@connect.polyu.hk ", "page_idx": 0}, {"type": "text", "text": "Department of Computing The Hong Kong Polytechnic University Hung Hom, Hong Kong SAR wency.hua@polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Qing Li ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiao Huang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computing The Hong Kong Polytechnic University Hung Hom, Hong Kong SAR csqli@comp.polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Department of Computing The Hong Kong Polytechnic University Hung Hom, Hong Kong SAR xiaohuang@comp.polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Knowledge graphs (KGs) serve as a foundational structure for storing and organizing structured knowledge about entities and their relationships, which facilitates effective and efficient search capabilities across various applications. They have been widely applied in question-answering systems (Dong et al., 2023, 2024c), recommendation systems (Catherine & Cohen, 2016; Chen et al., 2024a), social network analysis (Tang et al., 2008), Natural Language Processing (Weikum & Theobald, 2010), etc. Despite their extensive utility, real-world KGs often suffer from issues such as incompleteness, domain specificity, or language constraints, which limit their effectiveness in cross-disciplinary or multilingual contexts. To address these challenges, entity alignment (EA) aims to merge disparate KGs into a unified, comprehensive knowledge base by identifying and linking equivalent entities across different KGs. For instance, by aligning entities between a financial KG and a legal KG, EA facilitates the understanding of complex relationships, such as identifying the same corporations across the two KGs to assess how legal regulations impact their financial performance. This alignment enables a more nuanced exploration and interrogation of interconnected data, providing richer insights into how entities operate across multiple domains. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Entity alignment models predict the equivalence of two entities by measuring their alignment probability. Specifically, rule-based methods (Suchanek et al., 2012; Jim\u00e9nez-Ruiz & Cuenca Grau, 2011; Qi et al., 2021) utilize predefined rules or heuristics to update alignment probabilities and propagate alignment labels. Conversely, embedding-based models seek to exploit advanced techniques in graph learning (Li et al., 2024; Liu et al., 2024b,a, 2023), parameterizing these probabilities using similarity scores between entity representations learned through knowledge graph embedding algorithms such as translation models (Chen et al., 2017; Sun et al., 2018) or Graph Convolutional Networks (GCNs) (Wu et al., 2019; Mao et al., 2021; Wang et al., 2018; Huang et al., 2023). However, these methods heavily rely on extensive and accurate seed alignments for training\u2014a requirement that poses significant challenges. The need for substantial, cross-domain knowledge to annotate such alignments often makes their acquisition prohibitively expensive. ", "page_idx": 1}, {"type": "text", "text": "Recently, Large Language Models (LLMs) have showcased their superior capability in processing semantic information Dong et al. (2024a), which has significantly advanced various graph learning tasks such as node classification (Chen et al., 2024d), graph reasoning (Zhao et al., 2023a; Chai et al., 2023), recommender systems (Zhou et al., 2022; Wu et al., 2023), SQL query generation (Zhang et al., 2024a), and knowledge graph-based question answering (Wang et al., 2024; Zhang et al., 2024b; Dong et al., 2024b). Their capacity to extract meaningful insights from graph data opens up new possibilities for automating EA. Notably, recent studies (Zhong et al., 2022; Zhao et al., 2023b; Jiang et al., 2024) have explored the use of LLMs in EA, primarily focusing on finetuning a pretrained LLM such as Bert to learn semantic-aware representations, relying on accurate seed alignments as training labels. Yet, the potential of LLMs for label-free EA via in-context learning remains unexplored. ", "page_idx": 1}, {"type": "text", "text": "However, directly applying LLMs to automate EA poses significant challenges. Firstly, conventional EA models presume that all annotations are correct; yet, LLMs can generate false labels due to LLMs\u2019 inherent randomness and the potential incompleteness or ambiguity in the semantic information of entities. Training an EA model directly on these noisy labels can severely impair the final alignment performance. Secondly, given the vast number of entity pairs, annotation with LLMs would be prohibitively expensive. Maximizing the utility of a limited LLM query budget is essential. Existing solutions such as active learning cannot be directly applied since the annotations are noisy. ", "page_idx": 1}, {"type": "text", "text": "In response to the outlined challenges, we introduce LLM4EA, a unified framework designed to effectively learn from noisy pseudo-labels generated by LLMs while dynamically optimizing the utility of a constrained query budget. LLM4EA actively selects source entities based on feedback from a base EA model, focusing on those that significantly reduce uncertainty for both the entities themselves and their neighbors. This approach allocates the query budget to important entities, guided by the intra-KG and inter-KG structure. To manage the noisy pseudo-labels effectively, LLM4EA incorporates an unsupervised label refiner that enhances label accuracy by selecting a subset of confident pseudo-labels through probabilistic reasoning. These refined labels are then utilized to train the base EA model for entity alignment. The confident alignment results inferred by the EA model inform active selection in subsequent iterations, thereby progressively improving the framework\u2019s effectiveness in a coherent and integrated manner. Contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Novel LLM-based framework for entity alignment: We propose LLM4EA, an in-context learning framework that uses an LLM to annotate entity pairs. Leveraging the LLMs\u2019 zeroshot learning capability, this framework generates pseudo-labels, providing a foundation for entity alignment without ground truth labels. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Unsupervised label refinement: Our framework introduces an unsupervised label refiner informed by probabilistic reasoning. This component significantly improves the accuracy of LLM-derived pseudo-labels, enabling effective training of entity alignment models. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Active sampling module: We propose an active selection algorithm that dynamically searches entities in the huge annotation space. Guided by feedback from the EA model, this algorithm adjusts its policy based on inter-KG and intra-KG structures, optimizing the utility of LLM queries and ensuring efficient use of resources. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Empirical validation and superior performance: We rigorously evaluate our framework through extensive experiments and ablation studies. The results demonstrate that LLM4EA not only outperforms baselines by a large margin but also shows robustness and efficiency. Each component of the framework is shown to contribute meaningfully to the overall performance, with clear synergistic effects observed among the components. ", "page_idx": 2}, {"type": "text", "text": "2 Problem definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A knowledge graph $\\mathcal{G}$ comprises a set of entities $\\mathcal{E}$ , a set of relations $\\mathcal{R}$ , and a set of relation triples $\\tau$ where each triple $(e_{h},r,e_{t})\\in\\mathcal{T}$ represents a directional relationship between its head entity and tail entity. Given two KGs $\\mathcal{G}=\\{\\mathcal{E},\\mathcal{R},\\mathcal{T}\\}$ , $\\mathcal{G}^{\\prime}=\\{\\mathcal{E}^{\\prime},\\mathcal{R}^{\\prime},\\mathcal{T}^{\\prime}\\}$ and a fixed query budget $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ to a Large Language Model, we aim to train an entity alignment model $\\theta$ based on the LLM\u2019s annotations to infer the matching score $m_{\\theta}(e,e^{\\prime})$ for all entity pairs $\\{(e,e^{\\prime}),e\\in\\mathcal{E},e^{\\prime}\\in\\mathcal{E}^{\\prime}\\}$ . The evaluation process utilizes a ground truth alignment set $\\boldsymbol{\\mathcal{A}}$ to assess the prediction accuracy for target entities in both directions, i.e., $(e,\\mathscr{?})$ and $(\\?,e\\prime)$ for each true pair $(e,e^{\\prime})\\in\\mathcal{A}$ , based on the ranked matching scores $m_{\\theta}$ . Evaluation metrics are hit $@_{\\mathrm{k}}$ (where $k\\in\\{1,10\\}$ ) and mean reciprocal rank (MRR). ", "page_idx": 2}, {"type": "text", "text": "3 Entity alignment with noisy annotations from LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to design a framework to perform entity alignment with LLMs. Our design is motivated by the following insights. Firstly, we have a huge search space (the overall annotation space is $O(|\\mathcal{E}||\\mathcal{E}^{\\prime}|))$ to identify the core entity pairs to annotate. Secondly, we don\u2019t know whether the annotated labels are correct or not, because we have no prior knowledge or heuristic of the label distribution. Finally, we perform annotations iteratively, requiring the model to adjust its search policy based on annotation effectiveness, while we have no verifiable feedback of this annotation accuracy. ", "page_idx": 2}, {"type": "text", "text": "Based on these insights, we propose LLM4EA\u2014an iterative framework that consists of four interconnected steps in each cycle, as illustrated in Figure 1. Initially, an active selection technique optimizes the use of resources by choosing critical source entities that significantly reduce uncertainty for themselves and their neighbors. Subsequently, an LLM-based annotator identifies the counterparts for the selected source entities, generating a set of pseudo-labels. Next, a label refiner improves label accuracy by eliminating structurally incompatible labels. This process involves formulating a combinatorial optimization problem and utilizing a probabilistic-reasoning-based greedy search algorithm to efficiently find a local-optimal solution. Finally, these refined labels are used to train a base EA model for the entity alignment task. The outcomes of the alignment then serve as feedback to inform subsequent rounds of the active selection policy. Further details are provided below. ", "page_idx": 2}, {"type": "image", "img_path": "qfCQ54ZTX1/tmp/74b5a432b57c59f9e021a4d3fbee39ad2ca6f620cd5ea8fd619e30cfc54a2367.jpg", "img_caption": ["Figure 1: Overview of the LLM4EA framework. LLM4EA utilizes active sampling to select important entities based on feedback from an EA model. It also includes a label refiner to effectively train the base EA model using noisy pseudo-labels. Feedback from the EA model updates the selection policy. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.1 Active selection of source entity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to maximize the utility of the budget by actively allocating the budget to those beneficial entities. To do this, we sample source entities that reduce the most uncertainty of both themselves and their neighboring entities, by a dynamically adjusted policy. The measurement of uncertainty reduction is based on two assumptions: 1) an entity\u2019s own uncertainty is inversely proportional to its alignment probability with its most probable counterpart; 2) the amount of uncertainty an entity eliminates for its neighbors is closely linked to the relational ties between them. To systematically assess this, we introduce the concept of relational uncertainty, quantified as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nU_{r}(e_{h})=(1-P(e_{h}))+\\sum_{(e_{h},r,e_{t})\\in\\mathcal{T}}w_{r}\\left(1-P(e_{t})\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $w_{r}$ is a weight coefficient reflecting the significance of relation $r$ and signifies how much $e_{h}$ contributes to reducing the uncertainty of $e_{t}$ through the relation $r$ . For this purpose, we employ functionality ${\\mathcal{F}}(r)$ (formally defined in Eq. (4)) as the weight $w_{r}$ , as it quantifies the uniqueness of the tail entity for a given specified head entity. $P(e):=\\operatorname*{max}_{e^{\\prime}\\in\\mathcal{E}^{\\prime}}P(e\\equiv e^{\\prime})$ represents the alignment probability of the top-match entity for $e$ . These alignment probabilities $\\bar{P(e\\,\\equiv\\,e^{\\prime})}$ are obtained through probabilistic reasoning during label refinement (Section 3.3.2) and are augmented by the inferred alignments from the base EA model (Section 3.4). In the initial iteration, all alignment probabilities are set to 0. ", "page_idx": 3}, {"type": "text", "text": "It\u2019s important to note that some source entities are linked to a large number of uncertain neighbors (those with low $P(e))$ ). These source entities are crucial but may be overlooked if their connected relations have low functionality. Hence, we introduce neighbor uncertainty as another metric to assess an entity\u2019s importance, by removing the functionality-based weight coefficient: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU_{n}(e_{h})=(1-P(e_{h}))+\\sum_{(e_{h},r,e_{t})\\in\\mathcal{T}}(1-P(e_{t}))\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To integrate these two metrics, we employ rank aggregation by mean reciprocal rank: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU(e_{h})=2\\times\\left(\\frac{1}{r_{u r}(e_{h})}+\\frac{1}{r_{u n}(e_{h})}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $r_{u r}(e_{h})$ and $r_{u n}(e_{h})$ denote the ranking of $e_{h}$ when using $U_{r}$ and $U_{n}$ as metric, respectively. This simple-effective aggregation technique is advantageous for our task since it\u2019s scale invariant and requires no validation set for tuning hyperparameters, making it more practical in this task. ", "page_idx": 3}, {"type": "text", "text": "3.2 LLM as annotator ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Counterpart filtering. With the selected source entities, we employ an LLM as an annotator to identify the counterpart from ${\\mathcal{E}}^{\\prime}$ for each source entity, generating a set of pseudo-labels $\\mathcal{L}\\,=\\,\\{(e,e^{\\prime})|e\\in\\mathcal{E},e^{\\prime}\\in\\bar{\\mathcal{E}^{\\prime}}\\}$ . To narrow down the search space, we first filter out the less likely counterparts before querying the LLM, selecting only the top- $\\cdot k$ most similar counterparts from ${\\mathcal{E}}^{\\prime}$ . The similarity metric is flexible: we use a string matching score based on word edit distance, but other methods are also viable, such as semantic embedding distances derived from word embedding models. By adjusting $k$ , we can trade-off between the recall rate of counterparts and the query cost. ", "page_idx": 3}, {"type": "text", "text": "Prompt design. There are primarily two methods for retrieving context information to construct textual prompts: randomly generated prompts and dynamically tuned prompts. The former involves randomly selecting neighbors to construct contexts for the entity, while the latter dynamically selects neighbors based on feedback from the EA model. For a fair comparison, we use randomly generated prompts across all baselines and the proposed LLM4EA. These prompts include the name of each entity and a set of relation triples to three randomly selected neighbors. For the baseline models, pseudo-labels are generated at once and used for training. For LLM4EA, we evenly divide the budget $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ into $n$ iterations and generate pseudo-labels at each iteration using the allocated $B/n$ budget. ", "page_idx": 3}, {"type": "text", "text": "3.3 Probabilistic reasoning for label refinement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The pseudo-labels generated by the LLM can be noisy, and directly using these labels to train an entity alignment (EA) model could undermine the final performance. Although estimating the label distribution by asking the LLM for confidence scores or querying multiple times to measure consistency are potential solutions, these approaches can be vulnerable or introduce additional costs. ", "page_idx": 3}, {"type": "text", "text": "In light of this, we propose a label refiner that leverages the structure of knowledge graphs. The refinement process is framed as a combinatorial optimization problem aimed at minimizing overall structural incompatibility among labels. Utilizing a probabilistic reasoning technique, we progressively update our confidence estimation for each label and select those that are mutually compatible, ultimately producing a set of accurate pseudo-labels. Detailed explanations follow below. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3.1 Functionality and probabilistic reasoning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Functionality. The functionality of a relation quantifies the uniqueness of tail entities for a specified head entity, calculated as the ratio of unique head entities to total head-tail pairs linked by the relation. Conversely, inverse functionality quantifies the tail entity uniqueness for a specified head entity. Formally, these are defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}(r):=\\frac{|\\{e_{h}|(e_{h},r,e_{t})\\in\\mathcal{T}\\}}{|\\{(e_{h},e_{t})|(e_{h},r,e_{t})\\in\\mathcal{T}\\}\\},\\quad\\mathcal{F}^{-1}(r):=\\frac{|\\{e_{t}|(e_{h},r,e_{t})\\in\\mathcal{T}\\}}{|\\{(e_{h},e_{t})|(e_{h},r,e_{t})\\in\\mathcal{T}\\}\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For instance, suppose a KG contains two triples for the relation locate_in: $(H a w a i i,l o c a t e\\_i n,U S)$ and $(M i a m i,l o c a t e\\_i n,U S)$ . Then $\\mathcal{F}(l o c a t e\\_i n)\\,=\\,1.0$ and $\\mathcal{F}^{-1}(l o c a t e\\_i n)\\,=\\,0.5$ . In other words, given $(M i a m i,l o c a t e\\_i n,?)$ , the answer for the missing tail entity is unique; while given $(?,l o c a t e\\_i n,\\bar{U}S)$ , there are multiple answers for the missing head entity. Such relational patterns are useful for identifying an entity based on its connections within the intra-graph structure. ", "page_idx": 4}, {"type": "text", "text": "Probabilistic reasoning. If two entities are each connected to entities that are aligned across KGs, this increases the likelihood that they should be aligned as well. Based on this heuristic, an entity pair\u2019s alignment probability $P(e_{h}\\equiv e_{h}^{\\prime})$ can be inferred by aggregating its neighbors\u2019 alignment probability via relation functionality: ", "page_idx": 4}, {"type": "equation", "text": "$$\n1-\\prod_{\\stackrel{(e_{h},r,e_{t})\\in\\mathcal{T}}{(e_{h}^{\\prime},r^{\\prime},e_{t}^{\\prime})\\in\\mathcal{T}^{\\prime}}}\\left(1-\\mathcal{F}^{-1}(r)P(r\\subseteq r^{\\prime})P(e_{t}\\equiv e_{t}^{\\prime})\\right)\\times\\left(1-\\mathcal{F}^{-1}(r^{\\prime})P(r^{\\prime}\\subseteq r)P(e_{t}\\equiv e_{t}^{\\prime})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $P(r\\subseteq r^{\\prime})$ denotes the probability of $r$ being a subrelation of $r^{\\prime}$ , estimated by alignment probabilities of connected entities: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\sum\\Big(1-\\prod_{(e_{h}^{\\prime},r^{\\prime},e_{t}^{\\prime})\\in\\mathcal{T}^{\\prime}}(1-P(e_{h}^{\\prime}\\equiv e_{h})P(e_{t}^{\\prime}\\equiv e_{t}))\\Big)}{\\sum\\Big(1-\\prod_{e_{h}^{\\prime},e_{t}^{\\prime}\\in\\mathcal{E}^{\\prime}}{(1-P(e_{h}^{\\prime}\\equiv e_{h})P(e_{t}^{\\prime}\\equiv e_{t}))}\\Big)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These formulations allow for the propagation and updating of alignment probabilities in a manner that is cognizant of relational structures. We employ this technique to design a label refiner below. ", "page_idx": 4}, {"type": "text", "text": "3.3.2 Label refiner ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Label incompatibility. We exploit the \"incompatibility\" of labels for label refinement, based on the assumption that correct labels can infer each other, while a false label could be incompatible with its correctly aligned neighbors. We define the overall incompatibility on a label set $\\mathcal{L}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi(\\mathcal{L}):=\\sum_{(e_{h},e_{h}^{\\prime})\\in\\mathcal{L}}\\left(\\mathbf{1}_{P(e_{h}\\equiv e_{h}^{\\prime})<\\operatorname*{max}_{e\\in\\mathcal{E}}P(e,e_{h}^{\\prime})}+\\mathbf{1}_{P(e_{h}\\equiv e_{h}^{\\prime})<\\operatorname*{max}_{e^{\\prime}\\in\\mathcal{E}^{\\prime}}P(e_{h},e^{\\prime})}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, ${\\mathbf{1}}_{P(e_{h}\\equiv e_{h}^{\\prime})<\\operatorname*{max}_{e\\in\\mathcal{E}}P(e,e_{h}^{\\prime})}=1$ if $e_{h}$ is not the top-match for $e_{h}^{\\prime}$ , otherwise 0. It\u2019s important to note that a detected incompatibility doesn\u2019t necessarily indicate the false alignment of $(e_{h},e_{h}^{\\prime})$ : it may suggest a misalignment of their neighbors. Given this, the key to label refinement is to jointly optimize the label\u2019s overall incompatibility while avoiding accidentally filtering out correct labels. ", "page_idx": 4}, {"type": "text", "text": "Objective. To enhance label quality, we propose to refine the pseudo-label set $\\mathcal{L}$ by finding a subset $\\mathcal{L}^{\\ast}\\subset\\mathcal{L}$ that minimizes its overall incompatibility: $\\begin{array}{r}{\\mathcal{L}^{*}=\\mathrm{argmin}_{\\mathcal{L}^{\\prime}\\subset\\mathcal{L}}\\,\\Phi(\\mathcal{L}^{\\prime})}\\end{array}$ . Noteworthy that a trivial solution for this optimization problem is only preserving a set of isolated labels, such that $\\mathrm{max}_{e\\in\\mathcal{E}}\\,P(e\\equiv e_{h}^{\\prime})=0$ and $\\mathrm{max}_{e^{\\prime}\\in\\mathcal{E}^{\\prime}}\\,P(e_{h}\\equiv e^{\\prime})=0$ for all $(e_{h},e_{h}^{\\prime})\\in\\mathcal{L}^{\\prime}$ . This trivial solution would lead to the exclusion of most accurate labels, an outcome we aim to avoid. Considering this, we introduce an $l1$ penalty term to penalize the removal of labels, leading to our overall objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{*}=\\mathrm{argmin}_{\\mathcal{L}^{\\prime}\\subset\\mathcal{L}}\\left(\\Phi(\\mathcal{L}^{\\prime})+\\lambda|\\mathcal{L}-\\mathcal{L}^{\\prime}|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $\\lambda>0$ is a weight coefficient. Solving the above combinatorial problem is intractable as it requires computing $\\bar{\\Phi}(\\mathcal{L}^{\\prime})$ for each possible set $\\mathcal{L}^{\\prime}\\subset\\mathcal{L}$ , which is NP-hard. Below we propose to search for a local-optimal solution by a greedy algorithm powered by probabilistic reasoning. ", "page_idx": 5}, {"type": "text", "text": "Greedy search. The algorithm begins by initializing the alignment probability $P(e\\equiv e^{\\prime})=\\delta_{0}$ for every pair $(e,e^{\\prime})$ within the set $\\mathcal{L}$ , where $\\delta_{0}$ is a constant within the range (0,1). It then iteratively performs a search for an optimal label set ${\\mathcal{L}}^{\\prime}$ through a series of voting steps. Each iteration is comprised of two main steps: probabilistic reasoning and label adjustment. ", "page_idx": 5}, {"type": "text", "text": "During the probabilistic reasoning step, the alignment probabilities and subrelation probabilities are updated according to Eq. (5) and Eq. (6), respectively. This update process refines our estimates of label confidence based on the latest information. During the label adjustment step, the label set ${\\mathcal{L}}^{\\prime}$ is updated based on these updated probabilities. Labels are appended to ${\\mathcal{L}}^{\\prime}$ if their updated alignment probabilities exceed $\\delta_{0}$ , indicative of high confidence in their alignment, supported by their neighbors: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\prime}\\gets\\mathcal{L}^{\\prime}\\cup\\left\\{(e_{h},e_{h}^{\\prime})\\in\\mathcal{L}|P(e_{h}\\equiv e_{h}^{\\prime})>\\delta_{0}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Conversely, labels demonstrating structural incompatibilities are excluded from ${\\mathcal{L}}^{\\prime}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\prime}\\leftarrow\\mathcal{L}^{\\prime}\\setminus\\left\\{(e_{h},e_{h}^{\\prime})\\in\\mathcal{L}\\mid P(e_{h}\\equiv e_{h}^{\\prime})<\\operatorname*{max}\\left(\\operatorname*{max}_{e\\in\\mathcal{E}}P(e,e_{h}^{\\prime}),\\operatorname*{max}_{e^{\\prime}\\in\\mathcal{E}^{\\prime}}P(e_{h},e^{\\prime})\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this manner, labels are removed if they are incompatible with updated aligned neighbors, ensuring the preservation of only the most confident pairs within ${\\mathcal{L}}^{\\prime}$ . To further refine the search process in subsequent iterations, we augment all entity alignment probabilities within ${\\mathcal{L}}^{\\prime}$ to a superior score: ", "page_idx": 5}, {"type": "equation", "text": "$$\nP(e\\equiv e^{\\prime})\\leftarrow\\operatorname*{max}\\left(P(e\\equiv e^{\\prime}),\\delta_{1}\\right)\\ \\ \\ \\mathrm{for}\\,\\mathrm{each}\\left(e,e^{\\prime}\\right)\\in\\mathcal{L}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $\\delta_{1}\\in(\\delta_{0},1)$ serves as a new threshold, elevating the alignment probabilities of confident pairs to foster a more directed and effective search. After $n_{l r}$ iterations, we get a set of confidently selected labels $\\mathcal{L}^{\\ast}$ that have high compatibility. The detailed algorithm is presented in Appendix A.2, and analyses of parameter efficiency and computational efficiency are provided in Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "3.4 Entity alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With the refined labels, we train an embedding-based EA model to learn structure-aware representations for each entity. After training, the EA model computes a matching score $m_{\\theta}(e,e^{\\prime})$ for each entity pair $(e,e^{\\prime})$ for evaluation. The selection of the base EA model is flexible, tailored to the task requirements. We chose a recently proposed GCN-based model, Dual-AMN (Mao et al., 2021), for its effectiveness and efficiency. ", "page_idx": 5}, {"type": "text", "text": "Feedback from the base EA model is crucial for dynamic update of the active selection policy. To generate effective feedback, we infer high-confidence pairs $(e,e^{\\prime})$ with the trained EA model, by selecting the pairs that both entities rank top for each other. These pairs are injected into the probabilistic reasoning system. Similar to the label refinement process, this system initializes with an alignment probability of $\\delta_{0}$ for these pairs and updates the estimation of alignment and subrelation probabilities using Eq. (5) and Eq. (6). The updated probabilities are used to construct the uncertainty terms (i.e., $U_{r}$ and $U_{n}$ ) to inform the active selection policy in subsequent iterations, thereby optimizing the budget utility and improving final performance continuously. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct experiments to evaluate the effectiveness of our framework. We begin by introducing the experimental settings. Then, we present experiments to answer the following research questions: RQ1. How effective is the overall framework? RQ2. What is the impact of the choice of LLM on the cost and performance of LLM4EA? RQ3. What is the effect of the label refiner? RQ4. What is the impact of active selection? ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and LLM. In this study, we use the widely-adopted OpenEA dataset (Sun et al., 2020), including two monolingual datasets (D-W-15K and D-Y-15K) and two cross-lingual datasets (ENDE-15K and EN-FR-15K). OpenEA comes in two versions: \"V1\" the normal version, and \"V2\" ", "page_idx": 5}, {"type": "text", "text": "the dense version. We employ \"V2\" in the experiments in the main text. The LLM version in this experiment is GPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-turbo-2024-04-09). By default, the overall query budget is $\\beta=0.1|\\mathcal{E}|$ . ", "page_idx": 6}, {"type": "text", "text": "Baselines. Baseline models include three GCN-based models \u2014 GCNAlign (Wang et al., 2018), RDGCN (Wu et al., 2019), Dual-AMN (Mao et al., 2021), and three translation-based models \u2014 IMUSE (He et al., 2019), AlignE, BootEA (Sun et al., 2018), Here, BootEA is a variant of AlignE that adopts a bootstrapping strategy, equipped with a label calibration component for improving the accuracy of bootstrapped labels. Baseline models are directly trained on the pseudo-labels generated by the LLM annotator, without label refinement or active selection. Every experiment is repeated three times to report statistics. ", "page_idx": 6}, {"type": "text", "text": "Setup of LLM4EA. We employ GPT-3.5 as the default LLM due to its cost efficiency. Other parameters are $n=3$ , $n_{l r}$ , $k=20$ , $\\delta_{0}=0.5$ , $\\delta_{1}=0.9$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.2.1 Comprehensive evaluation of entity alignment performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1: Evaluation of entity alignment performance, measured by $\\operatorname{Hit}(\\alpha\\operatorname{K}$ for $K\\in\\{1,10\\}$ , and Mean Reciprocal Rank (MRR), presented in $\\%$ . Experiment statistics are computed over three trials. ", "page_idx": 6}, {"type": "table", "img_path": "qfCQ54ZTX1/tmp/69509eee1b74e7ca1ea25ba66eb4cddef5da90beabb6f5525235b3119e15c322.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "To answer RQ1 and RQ2, we conducted two groups of experiments on OpenEA datasets, using GPT3.5 and GPT-4 as the annotator, respectively. Results are presented in Table 1. We also investigated the performance-cost comparison between the GPT-3.5 annotator and the GPT-4 annotator, illustrated in Figure 2. To control the randomness introduced by the LLMs, each experiment was repeated three times to report mean and standard deviation. These results lead to several key observations: ", "page_idx": 6}, {"type": "text", "text": "First, LLM4EA surpasses all baseline EA models, which are directly trained on the pseudolabels, by a large margin. This can be attributed to 1) our label refiner\u2019s capability in filtering out false labels, reducing noise during training and enabling more accurate optimization towards the ground true objective; 2) our active selection component\u2019s ability to smartly identify important entities to annotate, which takes full advantage of the fixed query budget. ", "page_idx": 6}, {"type": "text", "text": "Second, using the GPT-4 results in higher performance than using the GPT-3.5 as the annotator. This observation conforms to the fact that GPT-4 is a more advanced LLM with higher reasoning capacity and stronger semantic analysis, resulting in more precise annotation results and higher recall, thus providing more labels of high quality. We also observe that translation-based models (e.g., AlignE) are sensitive to noisy labels under GPT-3.5, while state-of-the-art GCN-based models (e.g., RDGCN and Dual-AMN) are more robust. BootEA also demonstrates superior performance and robustness, attributed to its bootstrapping technique and enhanced by its capability in calibrating bootstrapped labels. However, its label calibration is only applied to the bootstrapped labels, so it still suffers from the false training labels. Our proposed LLM4EA, on the other hand, refines the label accuracy before training the EA model, thus ensuring more accurate training. ", "page_idx": 6}, {"type": "image", "img_path": "qfCQ54ZTX1/tmp/16c7edc867caadedac428a2245c39b7a6822a65dd16d4805494aac0d62a9e82f.jpg", "img_caption": ["Figure 2: Performance-cost comparison between GPT-3.5 and GPT-4 as the annotator, evaluated by MRR. We increase the budget for GPT-3.5 to evaluate its performance. $[n\\!\\times\\!]$ denotes using $n\\times$ of the default query budget. Each experiment is repeated three times to show mean and standard deviation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Finally, LLM4EA is noise adaptive, enabling cost-efficient entity alignment. To further investigate the effect of the choice of LLM, we examined the performance-cost comparison between GPT-3.5 and GPT-4 as the annotator. We illustrate MRR in Figure 2 (detailed results are available in Appendix B.3). The results show that, by increasing the query budgets (measured by the number of tokens) for GPT3.5, the performance gradually increases. When the budget is $2\\times$ that of GPT-4, the performance is comparable to or exceeds the performance of using GPT-4 as the annotator. According to the pricing scheme of OpenAI, the input/output cost for 1 million tokens for GPT-3.5 and GPT-4 is $\\mathbb{S}0.50/\\mathbb{S}1.\\bar{5}$ and $\\mathbb{S}10/\\mathbb{S}30$ , respectively. This means that our noise-adaptive framework enables cost-efficient entity alignment with less advanced LLMs at $10\\times$ less actual cost than using more advanced LLMs, simply by increasing the token budget for the less advanced LLMs. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Effect of the label refiner ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "qfCQ54ZTX1/tmp/c8b772021269ad5344e96f8f146e338d1a9f1f54c24a829b455790ca5ed229d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Analysis of the Label Refinement. We illustrate the evolution of the true positive rate (TPR) (left) and recall (middle) for refined labels across four datasets. Furthermore, we assess the robustness of the label refinement process by examining the TPR of refined labels against varying initial TPRs within the D-W-15K dataset (right), with initial pseudo-labels synthesized at different TPR levels. ", "page_idx": 7}, {"type": "text", "text": "To answer RQ3, we first analyze the evolution of the True Positive Rate (TPR) and the recall rate of the refined labels. Specifically, at each label refinement iteration, the TPR is calculated as $\\frac{|\\mathcal{A}\\cap\\mathcal{L}^{\\prime}|}{|\\mathcal{L}^{\\prime}|}$ , and the recall is calculated a s ||AA\u2229\u2229LL\u2032|| . The left and middle subfigures of Figure 3 demonstrate how our label refiner progressively discovers accurate labels and optimizes the TPR. Initially, the TPR of the refined label set is high (approximately 1.0), then it decreases by a certain percentage, and eventually increases again to a high TPR. We attribute this pattern to: 1) the most confident labels being discovered in the earliest iterations, which are obvious alignments with many connected alignments; 2) as the algorithm progresses, some false pseudo-labels being erroneously added to the label set $\\mathcal{L}^{\\prime};3)$ as the label refinement continues, ${\\mathcal{L}}^{\\prime}$ is adjusted and the false pseudo-labels are replaced with the correct labels inferred by the updated probability as in Eq. (10). ", "page_idx": 7}, {"type": "text", "text": "Furthermore, we assess the robustness of our label refiner, as depicted in the right subfigure of Figure 3. We synthesize noisy labels and evaluate the output TPR in relation to varying input TPR levels, using two experimental schemes: fixed budget, where the budget remains constant at $0.1|\\mathcal{E}|$ while the TPR changes, and fixed $T P$ , where the number of true positives is fixed but the TPR and corresponding budgets are adjusted. The results demonstrate that the label refiner consistently elevates the TPR to over 0.9, even when the initial TPR is around 0.5, showcasing its high robustness to noisy pseudo-labels. This result also reveals why our framework demonstrates robust performance with the less advanced GPT-3.5 annotator. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2.3 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 2: Ablation study overview. The table presents the performance of the LLM4EA (Ours) with various modifications. Group 1: removing the label refiner (w/o LR) and the active selection component (w/o Act); Group 2: replacing the active selection technique with relational uncertainty (-ru), neighbor uncertainty (-nu), degree (-degree), and functionality sum (-funcSum). ", "page_idx": 8}, {"type": "table", "img_path": "qfCQ54ZTX1/tmp/6926aedbaaafd98540a632117fb29ad8f6ab30a2f1ec3e9911353d9b51551025.jpg", "table_caption": ["4.2.4 Pareto frontier of runtime overhead against performance. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablation studies detailed in Table 2 answer RQ4 and reveal several key insights: 1) Necessity of the Label Refiner for Effective Active Selection: The performance of \u201cw/o LR\u201d, which lacks a label refiner, is inferior not only to other model variants but also to the base model, Dual-AMN. This underscores that active selection depends crucially on reliable feedback, which is compromised when the label refiner is absent; 2) Contribution of Relation and Neighbor Uncertainty in Active Selection: Incorporating both relation and neighbor uncertainties significantly enhances the utility of the budget. Methods like \u201cOurs-degree\u201d and \u201cOurs-funcSum\u201d focus only on their connections to neighbors and ignore the uncertainty of neighbors. In contrast, \u201cOurs-ru\u201d and \u201cOurs-nu\u201d, which take these uncertainties into account, exhibit superior performance. This underscores the importance of considering neighbor uncertainty for effective active selection; 3) Robust Active Selection through Combined Metrics: Our active selection approach integrates both relation uncertainty and neighbor uncertainty to enable robust active selection. By employing rank aggregation, it prioritizes entities that are deemed significant by both metrics, ensuring a more effective and nuanced selection process. ", "page_idx": 8}, {"type": "image", "img_path": "qfCQ54ZTX1/tmp/5d57dae7abba9276b96843900cba51549ef4dc0fd2ddfa29f4a2fbafd96ef9bd.jpg", "img_caption": ["Figure 4: Performance of entity alignment across four datasets with varying active sampling iterations, under a fixed query budget. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The runtime overhead is directly proportional to the number of active selection iterations, $n$ , since each iteration involves a subsequent label refinement process. To explore the runtime-performance trade-off in the LLM4EA approach, we examine the Pareto frontier of runtime versus performance. We conduct entity alignment experiments with a fixed query budget, varying the number of active selection iterations. The results of these experiments are illustrated in Figure 4. ", "page_idx": 8}, {"type": "text", "text": "The results indicate that performance initially increases as the number of iterations rises from 1 to around 3, but further increases beyond this point lead to a decline. This pattern can be attributed to two main factors: (1) More iterations allow for extensive learning from feedback during the active selection phase. (2) However, when iterations are excessively high, the number of generated pseudo-labels per iteration becomes small, leading to isolated pseudo-labels that undermine the label refinement process. These findings suggest that an optimal balance between runtime efficiency and performance can be achieved without excessive trade-offs, indicating a specific threshold for iterations beyond which no further performance gains are observed. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LLMs for Entity Alignment. Recent approaches have sought to leverage LLMs for entity alignment in knowledge graphs, primarily focusing on integrating structural and semantic information for improved alignment performance. BERT-INT (Tang et al., 2020) fine-tunes a pretrained BERT model to capture interactions and attribute information between entities. Similarly, SDEA (Zhong et al., 2022) employs a pretrained BERT to encode attribute data, while integrating neighbor information via a GRU to enhance structural representation. TEA (Zhao et al., 2023b) reconceptualizes entity alignment as a bidirectional textual entailment task, utilizing pretrained language models to estimate entailment probabilities between unified textual sequences representing entities. A novel approach, ChatEA (Jiang et al., 2024), introduces a KG-code translation module that converts knowledge graph structures into a format comprehensible to LLMs, enabling these models to apply their extensive background knowledge to boost the accuracy of entity alignment. Notably, these models primarily focus on fine-tuning pretrained language models using a set of training labels and do not exploit the zero-shot capabilities of LLMs. In contrast, our proposed model, LLM4EA, leverages the zero-shot potential of LLMs, enabling it to generalize to new datasets without the need for labeled data. ", "page_idx": 9}, {"type": "text", "text": "Probabilistic Reasoning. In the literature on knowledge graph reasoning, probabilistic reasoning has been effectively applied to infer new soft labels and their associated probabilities from existing labels. It has been utilized in domains such as knowledge graph completion (Qu & Tang, 2019; Zhang et al., 2020; Fang et al., 2023; Chen et al., 2024b) and entity alignment (Suchanek et al., 2012; Qi et al., 2021; Liu et al., 2022; Chen et al., 2024c), where it naturally represents complex relational patterns with simple rules and performs precise inferences. In this work, however, due to the potential inaccuracies in the pseudo-labels generated by LLMs, the newly inferred alignments may be incorrect. Consequently, we opt not to employ probabilistic reasoning directly for the entity alignment task. Instead, we emphasize its use primarily for filtering false pseudo-labels that demonstrate structural incompatibilities within our framework. For completeness, we include the results of comparison with a probabilistic reasoning model \u2013 PARIS (Suchanek et al., 2012) in Appendix B.4. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Firstly, during active selection, we distribute the query budget evenly for each selection rather than dynamically customizing the budget allocation for each iteration. This allocation approach could be improved by developing a more adaptive budget allocation strategy. Secondly, the framework currently does not provide direct support for temporal KGs. Although the probabilistic reasoning and active selection components inherently support entity alignment on evolving KGs, the base EA model is transductive. This necessitates retraining the model whenever new entities and relation triples are introduced into the KGs. However, this can be complemented by the research line of inductive entity alignment, such as path-based embedding models or logic-based models, which can generalize to previously unseen entities without the need for retraining. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we address the challenge of automating entity alignment with Large Language Models (LLMs) under budget constraints and noisy annotations. We introduce LLM4EA, a framework that maximizes the utility of a fixed query budget using active sampling and mitigates erroneous labels with a label refiner employing probabilistic reasoning. Empirical results show that LLM4EA\u2019s noise-adaptive capabilities reduce costs without sacrificing performance, achieving comparable or superior results with less advanced LLMs at up to 10 times lower cost. This highlights the potential of LLM-based models in merging cross-domain and cross-lingual KGs. Future work will explore incorporating real-time learning capabilities to dynamically adjust to evolving knowledge bases. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU 15200023). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Rose Catherine and William Cohen. Personalized recommendations using knowledge graphs: A probabilistic logic programming approach. In Proceedings of the 10th ACM conference on recommender systems, pp. 325\u2013332, 2016.   \nZiwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model. arXiv preprint arXiv:2310.05845, 2023.   \nHao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang Wang, and Xiao Huang. Macro graph neural networks for online billion-scale recommender systems. In Proceedings of the ACM on Web Conference 2024, pp. 3598\u20133608, 2024a.   \nMuhao Chen, Yingtao Tian, Mohan Yang, and Carlo Zaniolo. Multilingual knowledge graph embeddings for cross-lingual knowledge alignment. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 1511\u20131517, 2017.   \nShengyuan Chen, Yunfeng Cai, Huang Fang, Xiao Huang, and Mingming Sun. Differentiable neurosymbolic reasoning on large-scale knowledge graphs. Advances in Neural Information Processing Systems, 36, 2024b.   \nShengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Jiannong Cao, and Xiao Huang. Neurosymbolic entity alignment via variational inference. arXiv preprint arXiv:2410.04153, 2024c.   \nZhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang. Label-free node classification on graphs with large language models (LLMs). In The Twelfth International Conference on Learning Representations, 2024d.   \nJunnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and Zhimeng Jiang. Hierarchyaware multi-hop question answering over knowledge graphs. In Proceedings of the ACM Web Conference 2023, pp. 2519\u20132527, 2023.   \nJunnan Dong, Zijin Hong, Yuanchen Bei, Feiran Huang, Xinrun Wang, and Xiao Huang. Clr-bench: Evaluating large language models in college-level reasoning, 2024a. URL https://arxiv.org/ abs/2410.17558.   \nJunnan Dong, Qinggang Zhang, Chuang Zhou, Hao Chen, Daochen Zha, and Xiao Huang. Costefficient knowledge-based question answering with large language models. arXiv preprint arXiv:2405.17337, 2024b.   \nJunnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, and Xiao Huang. Modalityaware integration with large language models for knowledge-based visual question answering, 2024c.   \nHuang Fang, Yang Liu, Yunfeng Cai, and Mingming Sun. Mln4kb: an efficient markov logic network engine for large-scale knowledge bases and structured logic rules. In Proceedings of the ACM Web Conference 2023, pp. 2423\u20132432, 2023.   \nFuzhen He, Zhixu Li, Yang Qiang, An Liu, Guanfeng Liu, Pengpeng Zhao, Lei Zhao, Min Zhang, and Zhigang Chen. Unsupervised entity alignment using attribute triples and relation triples. In Database Systems for Advanced Applications: 24th International Conference, DASFAA 2019, Chiang Mai, Thailand, April 22\u201325, 2019, Proceedings, Part I 24, pp. 367\u2013382, 2019.   \nFeiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen. Aligning distillation for cold-start item recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 1147\u20131157, 2023.   \nXuhui Jiang, Yinghan Shen, Zhichao Shi, Chengjin Xu, Wei Li, Zixuan Li, Jian Guo, Huawei Shen, and Yuanzhuo Wang. Unlocking the power of large language models for entity alignment. arXiv preprint arXiv:2402.15048, 2024.   \nErnesto Jim\u00e9nez-Ruiz and Bernardo Cuenca Grau. Logmap: Logic-based and scalable ontology matching. In The Semantic Web\u2013ISWC 2011: 10th International Semantic Web Conference, Bonn, Germany, October 23-27, 2011, Proceedings, Part I 10, pp. 273\u2013288, 2011.   \nZhixun Li, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, et al. Gslb: the graph structure learning benchmark. Advances in Neural Information Processing Systems, 36, 2024.   \nBing Liu, Harrisen Scells, Wen Hua, Guido Zuccon, Genghong Zhao, and Xia Zhang. Guiding neural entity alignment with compatibility. arXiv preprint arXiv:2211.15833, 2022.   \nYang Liu, Huang Fang, Yunfeng Cai, and Mingming Sun. Mquine: a cure for\" z-paradox\u201din knowledge graph embedding models. arXiv preprint arXiv:2402.03583, 2024a.   \nYang Liu, Chuan Zhou, Peng Zhang, Yanan Cao, Yongchao Liu, Zhao Li, and Hongyang Chen. Cl4kge: A curriculum learning method for knowledge graph embedding. arXiv preprint arXiv:2408.14840, 2024b.   \nZirui Liu, Chen Shengyuan, Kaixiong Zhou, Daochen Zha, Xiao Huang, and Xia Hu. Rsc: accelerate graph neural networks training via randomized sparse computations. In International Conference on Machine Learning, pp. 21951\u201321968. PMLR, 2023.   \nXin Mao, Wenting Wang, Yuanbin Wu, and Man Lan. Boosting the speed of entity alignment $10\\times$ : Dual attention matching network with normalized hard sample mining. In Proceedings of the Web Conference 2021, pp. 821\u2013832, 2021.   \nZhiyuan Qi, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Yuejia Xiang, Ningyu Zhang, and Yefeng Zheng. Unsupervised knowledge graph alignment by probabilistic reasoning and semantic embedding. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 2019\u20132025, 2021.   \nMeng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. Advances in neural information processing systems, 32, 2019.   \nFabian M. Suchanek, Serge Abiteboul, and Pierre Senellart. Paris: Probabilistic alignment of relations, instances, and schema. In Proceedings of the 38th International Conference on Very Large Databases, pp. 157\u2013168, 2012.   \nZequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong Qu. Bootstrapping entity alignment with knowledge graph embedding. In IJCAI, number 2018, 2018.   \nZequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami, and Chengkai Li. A benchmarking study of embedding-based entity alignment for knowledge graphs. Proceedings of the VLDB Endowment, 13(12):2326\u20132340, 2020.   \nJie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 990\u2013998, 2008.   \nXiaobin Tang, Jing Zhang, Bo Chen, Yang Yang, Hong Chen, and Cuiping Li. Bert-int:a bertbased interaction model for knowledge graph alignment. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 3174\u20133180, 2020.   \nYu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 19206\u201319214, 2024.   \nZhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph alignment via graph convolutional networks. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 349\u2013357, 2018.   \nGerhard Weikum and Martin Theobald. From information to knowledge: harvesting entities and relationships from web sources. In Proceedings of the twenty-ninth ACM SIGMOD-SIGACTSIGART symposium on Principles of database systems, pp. 65\u201376, 2010.   \nXuansheng Wu, Huachi Zhou, Wenlin Yao, Xiao Huang, and Ninghao Liu. Towards personalized cold-start recommendation with prompts. arXiv preprint arXiv:2306.17256, 2023.   \nYuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, and Dongyan Zhao. Relation-aware entity alignment for heterogeneous knowledge graphs. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, 2019.   \nQinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, and Xiao Huang. Structure guided large language model for sql generation. arXiv preprint arXiv:2402.13284, 2024a.   \nQinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, and Xiao Huang. Knowgpt: Knowledge injection for large language models, 2024b.   \nYuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Efficient probabilistic logic reasoning with graph neural networks. In International Conference on Learning Representations, 2020.   \nJianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space. arXiv preprint arXiv:2310.01089, 2023a.   \nYu Zhao, Yike Wu, Xiangrui Cai, Ying Zhang, Haiwei Zhang, and Xiaojie Yuan. From alignment to entailment: A unified textual entailment framework for entity alignment. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8795\u20138806, 2023b.   \nZiyue Zhong, Meihui Zhang, Ju Fan, and Chenxiao Dou. Semantics driven embedding learning for effective entity alignment. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pp. 2127\u20132140, 2022.   \nHuachi Zhou, Jiaqi Fan, Xiao Huang, Ka Ho Li, Zhenyu Tang, and Dahai Yu. Multi-interest refinement by collaborative attributes modeling for click-through rate prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 4732\u20134736, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notations and algorithms ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "qfCQ54ZTX1/tmp/730624231375e099bf0b3ab4182c9a11a433e769f7935a6bdcdc46e9a6888f3a.jpg", "table_caption": ["A.1 Notations "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Pseudo-code of the greedy algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Below we present the pseudo-code of the greedy algorithm, that incorporates probabilistic reasoning to refine the label set. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 The greedy label refinement algorithm Inputs: The pseudo-label set $\\mathcal{L}$ Parameters: The intialization probability $\\delta_{0}\\,\\in\\,(0,1)$ , the threshold $\\delta_{1}\\,\\in\\,(\\delta_{0},1)$ , probabilistic reasoning iterations $n_{l r}$ Outputs: The refined pseudo-label set $\\mathcal{L}^{\\ast}\\subset\\mathcal{L}$ $\\mathcal{L}^{\\prime}\\gets\\mathcal{O}$ . $\\forall e\\in\\mathcal{E}$ , $\\forall e^{\\prime}\\in\\mathcal{E}^{\\prime}$ , P(e \u2261e\u2032) \u21900 $\\forall(e,e^{\\prime})\\in\\mathcal{L}$ , P(e \u2261e\u2032) \u2190\u03b40 $i\\leftarrow0$ while $i<n_{l r}$ do $\\begin{array}{r l}&{\\forall e_{h}\\ \\in\\ \\bar{\\mathcal{E}},\\forall e_{h}^{\\prime}\\ \\in\\ \\mathcal{E}^{\\prime},\\ P(e_{h}\\ \\equiv\\ e_{h}^{\\prime})\\ \\gets\\ 1-\\prod_{(e_{h}^{\\prime},r^{\\prime},e_{t})\\in T,\\ (1-\\mathcal{F}^{-1}(r)}P(r\\subseteq r^{\\prime})P(e_{t}\\equiv e_{t}^{\\prime}))\\ \\times}\\\\ &{\\big(1-\\mathcal{F}^{-1}(r^{\\prime})P(r^{\\prime}\\subseteq r)P(e_{t}\\equiv e_{t}^{\\prime})\\big).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\ \\mathrm{Update~entity~alignment~probabilities}.*/}\\\\ &{\\forall r\\in\\mathcal{R},\\forall r^{\\prime}\\in\\mathcal{R}^{\\prime},P(r\\subseteq r^{\\prime})\\leftarrow\\frac{\\sum\\left(1-\\prod_{(e_{h}^{\\prime},r^{\\prime},e_{t}^{\\prime})\\in T^{\\prime}}\\left(1-P(e_{h}^{\\prime}\\equiv e_{h})P(e_{t}^{\\prime}\\equiv e_{t})\\right)\\right)}{\\sum\\left(1-\\prod_{e_{h}^{\\prime},e_{t}^{\\prime}\\in\\varepsilon^{\\prime}}\\left(1-P(e_{h}^{\\prime}\\equiv e_{h})P(e_{t}^{\\prime}\\equiv e_{t})\\right)\\right)}\\quad\\ \\prime*\\ \\mathrm{Update~subrelation}}\\end{array}$ probabilities. $\\ast/$ $\\mathcal{L}^{\\prime}\\gets\\mathcal{L}^{\\prime}\\cup\\{(e_{h},e_{h}^{\\prime})\\in\\mathcal{L}|P(e_{h}\\equiv e_{h}^{\\prime})>\\delta_{0}\\}\\;/*$ Label adjustment, add confident pairs to the label set.\\*/ $\\begin{array}{r}{\\mathcal{L}^{\\prime}\\gets\\mathcal{L}^{\\prime}\\setminus\\{(e_{h},e_{h}^{\\prime})\\in\\mathcal{L}\\;\\vert\\;P(e_{h}\\equiv e_{h}^{\\prime})<\\operatorname*{max}\\{\\operatorname*{max}_{e\\in\\mathcal{E}}P(e,e_{h}^{\\prime}),\\operatorname*{max}_{e^{\\prime}\\in\\mathcal{E^{\\prime}}}P(e_{h},e^{\\prime})\\}\\}}\\end{array}$ $/*$ Label adjustment, remove less confident pairs from the label set. $\\ast/$ $\\bar{P(e\\equiv e^{\\prime})}\\gets\\operatorname*{max}\\left(P(e\\equiv e^{\\prime}),\\delta_{1}\\right)$ for each $(e,e^{\\prime})\\in\\mathcal{L}^{\\prime}\\left/\\ast\\right.$ Elevate alignment probability of confident pairs. $\\ast/$ end while $\\mathcal{L}^{*}\\leftarrow\\mathcal{L}^{\\prime}\\cup\\{(e,e^{\\prime})|P(e\\equiv e^{\\prime})>\\delta_{1}\\}$ /\\* Augment the refined label set with confident pairs. $\\ast/$ Return $\\mathcal{L}^{\\ast}$ ", "page_idx": 13}, {"type": "text", "text": "A.3 Efficient implementation of label refiner ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Parameter-efficient probabilistic reasoning. The total number of alignment probabilities for all entity pairs is $|\\mathcal{E}||\\mathcal{E}^{\\prime}|$ , resulting in a large parameter size when the KGs involved are extensive. We enhance memory efficiency by adopting a lazy inference strategy in probabilistic reasoning. This ", "page_idx": 13}, {"type": "text", "text": "strategy involves only saving the alignment probabilities of the most probable alignments: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\{P(e_{h},e_{h}^{\\prime}),|,e_{h}\\in\\mathcal{E},e_{h}^{\\prime}\\in\\mathcal{E}^{\\prime},P(e_{h}\\equiv e_{h}^{\\prime})=\\operatorname*{max}\\left(\\operatorname*{max}_{e\\in\\mathcal{E}}P(e,e_{h}^{\\prime}),\\operatorname*{max}_{e^{\\prime}\\in\\mathcal{E}^{\\prime}}P(e_{h},e^{\\prime})\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Probabilities of other entity pairs can be inferred from these saved alignment probabilities using Eq.   \n(5) when necessary. In this way, parameter complexity is reduced to $\\bar{O}(\\operatorname*{max}\\left(\\left|\\mathcal{E}\\right|+\\left|\\mathcal{E}^{\\prime}\\right|))$ . ", "page_idx": 14}, {"type": "text", "text": "Computation efficiency of probabilistic reasoning. Probabilistic reasoning is executed iteratively, with each iteration updating the alignment probabilities of all entities and relations following Eq. (5) and Eq. (6). We detail the analysis of these two update phases in the following separately. ", "page_idx": 14}, {"type": "text", "text": "Since we adopt a lazy inference strategy, the update of entity alignment probabilities involves updating the set of most probable alignments and associated probabilities as shown in Eq. (12). This update requires the estimation of all $P(e,e_{h}^{\\prime}),e\\in\\mathcal{E}$ and all $\\bar{P}(e_{h},e^{\\prime}),e^{\\prime}\\in\\mathcal{E}^{\\prime}$ , for each current pair $(e_{h},e_{h}^{\\prime})$ to determine if this pair needs an update. Consequently, the computational complexity of this process is proportional to the size of this set, which is $\\left\\langle O(\\left|\\mathcal{E}\\right|)\\right.$ . The estimated scores $P(e,e_{h}^{\\prime}),e\\in\\bar{\\mathcal{E}}$ and $P(\\bar{e_{h},e^{\\prime}}),e^{\\prime}\\,\\in\\,\\mathcal{E}^{\\prime}$ can be precomputed in advance and reused for all pairs $(e_{h},e_{h}^{\\prime})$ , leading to a computation complexity of $O(|\\mathcal{E}||\\mathcal{E}^{\\prime}|)$ . Thus, the overall computational complexity for updating entity alignment probabilities is $\\mathcal{O}(|\\mathcal{E}||\\mathcal{E}^{\\prime}|+|\\mathcal{E}|)=O(|\\mathcal{E}||\\mathcal{E}^{\\prime}|)$ . ", "page_idx": 14}, {"type": "text", "text": "The update process for sub-relation probabilities involves updating all $P(r\\subset r^{\\prime})$ for $r\\,\\in\\,\\mathcal R$ and $r^{\\prime}\\in\\bar{\\mathcal{R}}^{\\prime}$ , resulting in a complexity of $\\bar{O}(|\\mathcal{R}||\\mathcal{R}^{\\prime}|)$ . The estimation of Eq. (6) utilizes the probabilities of the most probable alignments from Eq. (12). Notably, most relation pairs $(\\boldsymbol r,\\boldsymbol r^{\\prime})$ do not have aligned head entities $(e_{h},\\bar{e}_{h}^{\\prime})$ or aligned tail entities $(e_{t},e_{t}^{\\prime})$ , thus most relation pairs can be excluded for efficient computation by exploiting this sparsity heuristic, reducing the computations by orders. ", "page_idx": 14}, {"type": "text", "text": "It is worth noting that these computations can be further accelerated through parallelization, as their execution solely depends on the results from the previous iteration. ", "page_idx": 14}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Hardware and software configurations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our experiments were conducted on a server equipped with six NVIDIA GeForce RTX 3090 GPUs, 48 Intel(R) Xeon(R) Silver 4214R CPUs, and 376GB of host memory. The models were implemented using TensorFlow, NumPy, and SciPy. It was observed that the software version significantly affects hardware-software compatibility. Specifically, the original implementation of Dual-AMN was based on TensorFlow 1.14.0, which is not compatible with newer GPUs such as the NVIDIA GeForce RTX 3090. Therefore, we updated the code to be compatible with TensorFlow 2.7.0, enabling the model to leverage GPU acceleration effectively. The details of the software packages used in our experiments are listed in Table 4. ", "page_idx": 14}, {"type": "table", "img_path": "qfCQ54ZTX1/tmp/3464ec72eac0a8b40210d14b91e8d6882297e3674bb59cb05912b66cae7fd209.jpg", "table_caption": ["Table 4: Package configurations of our experiments. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 Dataset statistics and preprocessing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our experiments, we utilized the OpenEA dataset version 1.1 (V2), specifically the 15K set. The statistics are detailed in Table 5. It\u2019s important to highlight that the destination dataset for D-W-15K, originating from Wikidata, contains only entity IDs, lacking explicit names. These IDs, devoid of semantic content, are not inherently meaningful to a language model. To rectify this, we processed the dataset using the \u2018wikidatawiki-20160801-abstract.xml\u2019 dump flie from Wikidata. This flie provided the raw data necessary for constructing the D-W-15K dataset, enabling us to extract meaningful entity names. ", "page_idx": 14}, {"type": "text", "text": "We shown the quantity of entities with names (after name extraction) for each KG in the \u2019Named entities\u2019 column in Table 5. ", "page_idx": 14}, {"type": "table", "img_path": "qfCQ54ZTX1/tmp/e62815b44cfe818530ac342ee2f0c0ea4e4456ce6791f356b2fb06518831ee32.jpg", "table_caption": ["Table 5: Data statistics of used OpenEA dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "In the counterpart filtering phase, we selected the top- $k$ (where $k=20$ for our experiments) most similar candidates. The \u2018Target in top- $k^{\\star}$ column of Table 5 shows the number of target entities included in this selection. ", "page_idx": 15}, {"type": "text", "text": "B.3 Performance-cost comparison between GPT-3.5 and GPT-4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 6: Performance-cost comparison between GPT-3.5 and GPT-4 as annotator. We increase the budget for GPT-3.5 to evaluate its performance. $[n\\!\\times\\!]$ denotes using $n\\times$ of the default query budget. ", "page_idx": 15}, {"type": "table", "img_path": "qfCQ54ZTX1/tmp/b8a77a8727596f2c7345f2c42fc1891742b6047781065154e4417f155f56cdc5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.4 Performance comparison against rule-based models ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "qfCQ54ZTX1/tmp/7febaff6fdc6009add5577418884fbfa65a302804fbed9345882163297d78d66.jpg", "table_caption": ["Table 7: Performance comparison against rule-based models, evaluated by precision, recall, and f1-score in $\\%$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "In this section, we compare the LLM4EA model with several rule-based models, including two lexical matching-based approaches: Emb-Match and Str-Match. Emb-Match uses cosine similarities between word embeddings to identify aligned pairs, employing a pretrained language model. StrMatch utilizes the Levenshtein Distance to calculate similarity scores. Additionally, the probabilistic reasoning model PARIS performs entity alignment by relying on probabilistic methods. ", "page_idx": 15}, {"type": "text", "text": "For the lexical matching-based models, we compute the similarity and evaluate the confident entity pairs, specifically targeting those whose normalized similarity scores exceed 0.8. The pretrained language model used for Emb-Match is bert-base-uncased for its ability to process unseen words as a subword model. To reduce false positives, we implementing a flitering process that only considers 1-1 matching for embedding-based matching. For PARIS, we assess all inferred aligned pairs by setting the threshold to zero. The entity alignment (EA) model of LLM4EA generates a ranked score list, which is not directly comparable with these rule-based models. To facilitate comparison, we use the trained EA model to generate confident pairs, ensuring that each entity is the top-ranked candidate for its counterpart. These pairs are then processed through our label refiner, and the refined pairs are evaluated. Experiments for PARIS and LLM4EA are repeated three times to ensure statistical reliability; for Emb-Match and Str-Match, experiments are performed once as these algorithms are deterministic. The results are presented in Table 7. ", "page_idx": 15}, {"type": "text", "text": "The findings highlight several key observations: 1) Lexical matching-based methods show a degree of alignment ability by leveraging name similarity, particularly on the D-Y-15K dataset, where many aligned entities have identical names. 2) These methods, however, encounter scalability challenges due to their reliance on name similarity. As the size of the KG grows, the number of similar entities increases, making it difficult to maintain precision. This is evident from our experiments on EN-FR100K and EN-DE-100K, where precision dropped to $49\\%$ and recall to $65\\%$ , a significant decrease compared to the 15K-size datasets, supporting our analysis. 3) PARIS achieves precise results by handling noisy data through probabilistic reasoning, although its recall is lower than LLM4EA\u2019s. 4) LLM4EA, with its active selection and label refinement techniques, consistently delivers robust and accurate performance across all datasets. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have clearly made the main claims in both the abstract and introduction. Specifically, we have summarized and itemized the contributions at the end of the introduction section. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Please see page 9, Section 6 for the Limitations section ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have provided the code for the framework, accessible via this URL: https://github.com/chensyCN/llm4ea_official. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Code and datasets are accessible through this anonymous URL: https: //anonymous.4open.science/r/llm4ea_neurips2024-E763/. In this repository, we included detailed instructions for running the code in the readme.md file. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We present the experimental setting details at the beginning of the experiment section. We also provide the dataset statistics and the preprocessing details in the appendix. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: To control for the randomness introduced by the Large Language Models, we repeat each experiment three times and report the mean and standard deviation in tables such as Table 1, Table 2, Table 6, and Table 7. For visualized figures such as Figure 2 and Figure 4, we show the error bars representing the standard deviation. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the computation resources required to reproduce our experiment in Appendix B.1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We checked and ensured that our paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper focuses on technical improvements in entity alignment using large language models and does not introduce direct societal impacts. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper poses no such risks, as the work involves only querying the API of an LLM to get pseudo-labels, without releasing a generative model or datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have cited the original papers of the datasets and EA models used in our experiments, ensuring proper credit and respect for their licenses and terms of use. For the open-source code and datasets, we have also stated the licenses they use in the readme.txt file in our code repository. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Code of the proposed framework is released and accessible via this anonymouse URL: https://anonymous.4open.science/r/llm4ea_neurips2024-E763/. It contains a readme.md file and a GPLv3 license. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]