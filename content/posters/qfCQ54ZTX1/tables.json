[{"figure_path": "qfCQ54ZTX1/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation of entity alignment performance, measured by Hit@K for K \u2208 {1,10}, and Mean Reciprocal Rank (MRR), presented in %. Experiment statistics are computed over three trials.", "description": "This table presents a comprehensive evaluation of different entity alignment models' performance across four benchmark datasets (EN-FR-15K, EN-DE-15K, D-W-15K, and D-Y-15K).  The performance is evaluated using two metrics: Hit@K (the percentage of correctly aligned entity pairs within the top K ranked predictions, for K=1 and K=10) and Mean Reciprocal Rank (MRR, which measures the average precision of the ranking). The results are presented for various models, including baselines and the proposed LLM4EA, and each model's performance is reported as a mean \u00b1 standard deviation based on three independent experimental trials. This provides a detailed comparison demonstrating LLM4EA's superior performance.", "section": "4.2 Results"}, {"figure_path": "qfCQ54ZTX1/tables/tables_8_1.jpg", "caption": "Table 2: Ablation study overview. The table presents the performance of the LLM4EA (Ours) with various modifications. Group 1: removing the label refiner (w/o LR) and the active selection component (w/o Act); Group 2: replacing the active selection technique with relational uncertainty (-ru), neighbor uncertainty (-nu), degree (-degree), and functionality sum (-funcSum).", "description": "This table presents the ablation study results for the LLM4EA framework.  It shows the impact of removing key components (label refiner and active selection) and replacing the active selection policy with alternative methods (using only relational uncertainty, neighbor uncertainty, degree, or functionality sum) on the overall performance across four datasets (EN-FR-15K, EN-DE-15K, D-W-15K, D-Y-15K) using metrics Hit@1, Hit@10, and MRR.", "section": "4.2.3 Ablation study"}, {"figure_path": "qfCQ54ZTX1/tables/tables_13_1.jpg", "caption": "Table 1: Evaluation of entity alignment performance, measured by Hit@K for K \u2208 {1,10}, and Mean Reciprocal Rank (MRR), presented in %. Experiment statistics are computed over three trials.", "description": "This table presents a comprehensive evaluation of different entity alignment methods' performance across four benchmark datasets (EN-FR-15K, EN-DE-15K, D-W-15K, and D-Y-15K).  The evaluation metrics used are Hit@1, Hit@10, and Mean Reciprocal Rank (MRR), all expressed as percentages.  Results are averaged over three independent trials to provide a measure of statistical reliability. The table compares the performance of LLM4EA against several baseline methods (IMUSE, AlignE, BootEA, GCNAlign, RDGCN, and Dual-AMN).", "section": "4.2 Results"}, {"figure_path": "qfCQ54ZTX1/tables/tables_14_1.jpg", "caption": "Table 1: Evaluation of entity alignment performance, measured by Hit@K for K \u2208 {1,10}, and Mean Reciprocal Rank (MRR), presented in %. Experiment statistics are computed over three trials.", "description": "This table presents the results of the entity alignment experiments.  It compares the performance of LLM4EA against several baseline models across four different datasets (EN-FR-15K, EN-DE-15K, D-W-15K, D-Y-15K).  The performance metrics used are Hit@1, Hit@10, and Mean Reciprocal Rank (MRR), all expressed as percentages.  The results are averaged over three trials to account for the inherent randomness of Large Language Models.  The table allows for a direct comparison of LLM4EA's performance against existing entity alignment methods.", "section": "4.2 Results"}, {"figure_path": "qfCQ54ZTX1/tables/tables_15_1.jpg", "caption": "Table 5: Data statistics of used OpenEA dataset.", "description": "This table presents a detailed breakdown of the statistics for each of the four datasets used in the OpenEA benchmark.  For each dataset, it shows the number of relations and relation triplets in each knowledge graph (KG), the number of attributes and attribute triplets, the number of entities with names, and the number of target entities present within the top-k most similar candidates selected during the counterpart filtering phase.", "section": "4.1 Experimental setting"}, {"figure_path": "qfCQ54ZTX1/tables/tables_15_2.jpg", "caption": "Table 1: Evaluation of entity alignment performance, measured by Hit@K for K \u2208 {1,10}, and Mean Reciprocal Rank (MRR), presented in %. Experiment statistics are computed over three trials.", "description": "This table presents a comprehensive evaluation of different entity alignment methods across four benchmark datasets (EN-FR-15K, EN-DE-15K, D-W-15K, and D-Y-15K).  The performance of each method is assessed using three metrics: Hit@1, Hit@10, and Mean Reciprocal Rank (MRR).  The results are presented as percentages and represent the average performance over three independent trials, providing a measure of the methods' robustness and stability. The table allows for a comparison of the proposed LLM4EA framework against several baselines, highlighting its improved effectiveness in entity alignment.", "section": "4.2 Results"}, {"figure_path": "qfCQ54ZTX1/tables/tables_15_3.jpg", "caption": "Table 7: Performance comparison against rule-based models, evaluated by precision, recall, and f1-score in %.", "description": "This table compares the performance of the proposed LLM4EA model against several rule-based baselines on four benchmark datasets (EN-FR-15K, EN-DE-15K, D-W-15K, and D-Y-15K).  The rule-based methods include two lexical matching approaches (Emb-Match and Str-Match) and a probabilistic reasoning model (PARIS). The table shows the precision, recall, and F1-score for each method on each dataset, highlighting the superior performance of LLM4EA compared to the baselines.", "section": "4.2 Results"}]