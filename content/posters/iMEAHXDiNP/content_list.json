[{"type": "text", "text": "Improved Algorithms for Contextual Dynamic Pricing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matilde Tullii\u2217 Solenne Gaucher\u2217 FairPlay Team, CREST, ENSAE FairPlay Team, CREST, ENSAE ", "page_idx": 0}, {"type": "text", "text": "Nadav Merlis Vianney Perchet FairPlay Team, CREST, ENSAE FairPlay Team, CREST, ENSAE - Criteo AI Lab ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In contextual dynamic pricing, a seller sequentially prices goods based on contextual information. Buyers will purchase products only if the prices are below their valuations. The goal of the seller is to design a pricing strategy that collects as much revenue as possible. We focus on two different valuation models. The first assumes that valuations linearly depend on the context and are further distorted by noise. Under minor regularity assumptions, our algorithm achieves an optimal regret bound of $\\tilde{\\mathcal{O}}(T^{2/3})$ , improving the existing results. The second model removes the linearity assumption, requiring only that the expected buyer valuation is $\\beta$ -H\u00f6lder in the context. For this model, our algorithm obtains a regret $\\tilde{\\mathcal{O}}(T^{d+2\\beta/d+3\\beta})$ , where $d$ is the dimension of the context space. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Setting a price and devising a strategy to dynamically adjust it poses a fundamental challenge in revenue management. This problem, known as dynamic pricing or online posted price auction, finds applications across various industries and has received significant attention from economists, operations researchers, statisticians, and machine learning communities. In this problem, a seller sequentially offers goods to arriving buyers by presenting a one-time offer at a specified price. If the offered price falls below the buyer\u2019s (unknown) valuation of the item, a transaction occurs, and the seller obtains the posted price as revenue. Conversely, if the price exceeds the buyer\u2019s valuation, the transaction fails, resulting in zero gain for the seller. Crucially, the seller solely receives binary feedback indicating whether the trade happened. Her objective is to learn from this limited feedback how to set prices that maximize her cumulative gains while ensuring that transactions take place. In this paper, we study the problem of designing an adaptive pricing strategy, when the seller can rely on contextual information, describing the product itself, the marketing environment, or the buyer. ", "page_idx": 0}, {"type": "text", "text": "While this problem has been extensively studied, previous results either rely on strong assumptions on the structure of the problem, greatly limiting the applicability of such approaches, or achieve sub-optimal regret bounds. In this work, we aim to improve both aspects\u2014achieving better regret bounds while making minimal assumptions about the problem. Specifically, we study two different models for the valuation of buyers as a function of the context: 1) linear valuations, where the item valuation of buyers is an unknown noisy linear function of the context; and 2) non-parametric valuations, where the valuation is given by an unknown H\u00f6lder-continuous function of the contextual information, perturbed by noise. ", "page_idx": 0}, {"type": "table", "img_path": "iMEAHXDiNP/tmp/aa9b2b2aedff54e9a1884f77692afdb7416bc805761744e3b3623e35fc47c8c3.jpg", "table_caption": ["Table 1: Summary of existing regret bounds. $g$ is the expected valuation function, $F$ is the c.d.f. of the noise, and $\\pi(x,p)$ is the reward for price $p$ and context $x$ , defined in Section 2.1. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Dynamic pricing has been extensively studied for half a century [19, 26], leading to rich research on both theoretical and empirical fronts. For comprehensive surveys on the topic, we refer the readers to [6, 12]. While earlier works assumed that the buyer\u2019s valuations are i.i.d. [18, 5, 16, 9], recent research has increasingly focused on feature-based (or contextual) pricing problems. In this scenario, product value and pricing strategy depend on covariates. Pioneering works considered valuations depending deterministically on the covariates. Linear valuations have been the most studied [3, 15, 11, 20], yet a few authors have also explored non-parametric valuations [24]. ", "page_idx": 1}, {"type": "text", "text": "Recent works have extended these methods to random valuations, mainly assuming that valuations are given by a function of the covariate, distorted by an additive i.i.d. noise. As this poses more challenges, authors have mostly focused on the simplest case of linear valuation functions, under additional assumptions. Initial studies assumed knowledge of the noise distribution [11, 15, 30]. This assumption was later relaxed, albeit with additional regularity requirements on the cumulative distribution function (c.d.f.) of the noise and/or the reward function [14, 22], and then again by [31], that achieves a regret bound of $\\tilde{\\mathcal{O}}(T^{3/4})$ for linear valuations, while assuming only the boundedness of the noise. Closest to our work [14, 21] also focus on the case in which the only regularity required is the Lipschitzness of the CDF. Their approaches show some similarities with our work but still achieve suboptimal regret rates. A more detailed comparison between ours an their algorithms is presented later on in the paper. Other parametric models have been explored, with, for example, generalized linear regression models [28], though they also require strong assumptions, including quadratic behavior of the reward function around each optimal price. Few works have considered non-deterministic valuations with non-parametric valuation functions. Among those, [10] consider Lipschitz-continuous valuation functions of $d$ -dimensional covariates. They achieve a regret of order $\\tilde{\\mathcal{O}}(T^{d+2/d+4})$ , assuming again quadratic behaviour around optimal prices. We refer to Table 1 for a comprehensive comparison between different previous works, their assumptions and regret bounds. ", "page_idx": 1}, {"type": "text", "text": "To improve on previous results, we design algorithms that share information on the noise distribution across different contexts. This idea relates to methods used in cross-learning, a research direction stemming from online bandit problems with graph feedback [23, 2]. In this framework, introduced by [4] and further studied in [27], when choosing to take action $i$ in context $x_{t}$ , the agent observes the reward $r_{i}(x_{t})$ along with rewards $r_{i}(x_{t}^{\\prime})$ associated with other contexts $x_{t^{\\prime}}$ . Our algorithms leverage similar principles to learn information usable across different contexts. However, compared to the typical problems addressed by cross-learning methods (e.g., first-price auctions, sleeping bandits, multi-armed bandits with exogenous costs), the contextual dynamic problem is more complex due to the intricate dependence of the reward on the unknown valuation function. ", "page_idx": 1}, {"type": "text", "text": "1.2 Outline and Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we tackle the problem of dynamic pricing with contextual information. We consider two models for the expected valuations of the buyer, assuming respectively that they are given by a linear function, or by a non-parametric function. For both models, we present a general algorithmic scheme called VALUATION APPROXIMATION - PRICE ELIMINATION (VAPE), and provide bounds on its regret in both models: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 In the linear model, we obtain a regret of $\\tilde{O}(T^{2/3})$ , assuming only that the c.d.f. of the noise is Lipschitz. This concludes an extensive series of papers on the topic, as it establishes the minimax optimal regret rate and proves it is attainable under minimal assumptions. \u2022 In the non-parametric model, we obtain a regret rate of $\\tilde{O}(T^{d+2\\beta/d+3\\beta})$ , assuming only the Lipschitz-continuity of the noise and the H\u00f6lder one of the valuation function. This result is the first of its kind under such minimal assumptions. ", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. We begin by presenting the model and summarizing the notations used throughout the paper in Section 2.1. Section 2.2 outlines our assumptions and compares them with those in previous works. In Section 2.3, we discuss the main sources of difficulty of the problem and highlight the importance of information sharing in contextual dynamic pricing. In Section 3, we present our algorithmic scheme, VAPE, and provide an initial informal result bounding its regret. Then, in Section 4, we apply this algorithmic scheme to linear valuations and provide a bound on its regret. Finally, in Section 5, we extend this algorithm to non-parametric valuations. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Model and Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The problem of dynamic pricing with contextual information is formalized as follows. At each step $t\\leq T$ , a context $\\dot{x}_{t}\\in\\mathbb{R}^{d}$ , describing a sale session (product, customer, and context) is revealed. The customer assigns a hidden valuation $y_{t}$ to the product, and the seller proposes a price $p_{t}$ , based on $x_{t}$ and on historical sales records. If $p_{t}\\leq y_{t}$ , the trade is successful, and the seller receives a reward $y_{t}$ ; otherwise the trade fails. The seller\u2019s only feedback is the binary outcome $o_{t}=\\mathbb{1}\\{p_{t}\\leq y_{t}\\}$ . We assume that the seller\u2019s valuation is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{t}=g(x_{t})+\\xi_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $g\\,:\\,\\mathbb{R}^{d}\\,\\mapsto\\,\\mathbb{R}$ is the valuation function, and $\\xi_{t}$ is a centered, bounded, i.i.d. noise term, independent of $x_{t}$ and of $(x_{s},p_{s},\\xi_{s})_{s<t}$ . In the present paper, we consider successively linear and non-parametric valuation functions $g$ in Sections 4 and 5. The seller\u2019s objective is to maximize the sum of her cumulative earnings. We denote by $\\pi(p,x_{t})$ the expected reward of the seller if she posts a price $p$ for a product described by covariate $x_{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi(x_{t},p)=\\mathbb{E}[p\\mathbb{1}\\{p\\leq y_{t}\\}|p,x_{t}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Adopting the terminology of the literature on multi-armed bandits, we measure the performance of our algorithm and the difficulty of the problem through the regret $R_{T}$ , defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{t=1}^{T}\\operatorname*{max}_{p\\in\\mathbb{R}}\\pi(x_{t},p)-\\sum_{t=1}^{T}\\pi(x_{t},p_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notations Throughout this paper, we make use of the following notation. We denote by $\\lVert\\cdot\\rVert$ the Euclidean norm. For all $A,B\\in\\mathbb{R}$ , we denote by $[A,B]$ the set $\\{A,A+1,\\ldots,B\\}$ . $R_{T}\\lesssim B_{T}$ (resp. $R_{T}=\\tilde{\\mathcal{O}}(B_{T}))$ ) means that there exists a (possibly problem-dependent) constant $C$ such that $R_{T}\\leq C B_{T}$ (resp. $\\begin{array}{r}{\\dot{R}_{T}=\\mathcal{O}(\\log(T)^{C}B_{T}))}\\end{array}$ . Finally, $f$ and $F$ denote the p.d.f. and c.d.f. of the noise, respectively. ", "page_idx": 2}, {"type": "text", "text": "2.2 Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For both valuation models, we make the following assumptions on the context and noise distribution. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. Contexts and expected valuations are bounded: $\\|\\boldsymbol{x}_{t}\\|_{2}\\leq B_{x}$ and $|g(x_{t})|\\le B_{g}$ a.s. ", "page_idx": 2}, {"type": "text", "text": "This assumption is classical in contextual dynamic pricing problems. We underline that contexts do not need to be random. In particular, they can be chosen by an adaptive adversary, aware of the seller\u2019s strategy, and based on past realizations of $(x_{s},p_{s},\\xi_{s})_{s<t}$ . Assumption 1 is milder than the i.i.d. context assumption appearing in [14, 28, 10]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Dynamic pricing strategies mostly assume that the buyer\u2019s valuations are bounded. To enforce this, we assume that the noise is bounded; moreover, we assume that its c.d.f. Lipschitz continuous. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. The noise $\\xi_{t}$ is bounded: $|\\xi_{t}|\\,\\le\\,B_{\\xi}$ a.s. Moreover, its c.d.f. $F$ is $L_{\\xi}$ -Lispchitz continuous: for all $(\\delta,\\delta^{\\prime})\\in\\mathbb{R}^{d}$ , $|F(\\delta)-F(\\delta^{\\prime})|\\le L_{\\xi}\\,|\\delta-\\delta^{\\prime}|$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 is weaker than most of the assumptions in related works. For example, [15] require both $F$ and $1-F$ to be log-concave. [14] assume that $F$ has $m$ -th derivative, and that $\\delta-1\\!-\\!F(\\delta)\\big/F^{\\prime}(\\delta)$ is greater than some positive constant for all $\\delta$ , achieving a regret of order $\\tilde{\\mathcal{O}}(T^{2m+1/4m-1})$ . In the case $m=1$ , they propose a different algorithm, reaching a regret $\\tilde{\\mathcal{O}}(T^{3/4})$ . [22] consider Lipschitzcontinuous noise, under the additional assumption that, for every $x$ , $p^{*}(x)\\in\\arg\\operatorname*{max}_{p}\\pi(x,p)$ is unique, and that $F^{\\prime\\prime}$ is bounded. [10] assume quadratic behaviour around every maxima: for every $x$ , $p^{*}(x)\\in\\arg\\operatorname*{max}_{p}\\pi(x,p),p^{*}(x$ is unique, and for all $p$ , $C(p^{*}(x)-p)^{2}\\leq\\pi{\\bar{(x,p^{*}(x))}}-\\pi(x,p)\\leq$ $C^{\\prime}(p^{*}(x)-p)^{2}$ for some constants $C,C^{\\prime}$ . The only work considering non-Lipschitz c.d.f. is [31]; however, they achieve a higher regret bound of $\\tilde{\\mathcal{O}}(T^{3/4})$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Information Sharing in Contextual Dynamic Pricing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For $\\delta\\in\\mathbb{R}$ , we denote $D(\\delta)=\\mathbb{P}\\left(\\xi_{t}\\geq\\delta\\right)=1-F(\\delta)$ , the demand function associated with the noise $\\xi_{t}$ . Note that, under Assumption 2, $D$ is $L_{\\xi}$ -Lipschitz continuous. Straightforward computations show that, for any price increment $\\delta\\in\\mathbb{R}$ , the expected reward corresponding to the price $p\\doteq g(\\boldsymbol{x}_{t})+\\delta$ in the context $x_{t}$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi(x_{t},g(x_{t})+\\delta)=(g(x_{t})+\\delta)D(\\delta).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equation (2) highlights the intricate roles played by the expected valuation $g(x_{t})$ and the price increment $\\delta\\;=\\;p\\mathrm{~-~}g(x_{t})$ in the reward. An immediate consequence is that the optimal price increment $\\delta$ depends on the value of $g(x_{t})$ . Intuitively, if $g(x_{t})$ is large, the seller should choose $\\delta$ to be small to ensure a high probability $D(\\delta)$ to perform a trade. However, for smaller values of $g(x_{t})$ , the seller might prefer a larger $\\delta$ to ensure significant rewards when a trade occurs. Importantly, there is no explicit relationship between the optimal increments $\\delta$ for different valuations $g(x_{t})$ , so knowing the optimal price for a value $g(x_{t})$ does not allow optimal pricing for a different value $g(x_{t^{\\prime}})$ . ", "page_idx": 3}, {"type": "text", "text": "This reasoning suggests that the optimal price increment may span a wide range of values as the expected valuation $g(x_{t})$ varies. Unfortunately, as is typical in bandit problems, it is necessary to estimate the reward function around the optimal price with high precision to ensure low regret. Consequently, solving the dynamic pricing problem may entail estimating the demand function precisely across a broad range of price increments. This marks a significant departure from noncontextual dynamic pricing and non-parametric bandit problems, where precise estimation of the reward function is often only necessary around its (single) maximum. Thus, the contextual dynamic pricing problem might be more challenging than its non-contextual counterpart, potentially leading to higher regret. This intuition is supported by the fact that straightforward application of basic bandit algorithms, even in the most simple linear model, leads to regret higher than the rate of order $\\tilde{\\mathcal{O}}(T^{2/3})$ encountered in non-contextual dynamic pricing problems, as we show in the following discussion. ", "page_idx": 3}, {"type": "text", "text": "Na\u00efve bandit algorithms for contextual dynamic pricing. As a first attempt, one might apply a simple explore-then-commit algorithm. Such algorithms start with an exploration phase to obtain uniformly good estimates of both $g$ and of the demand function $D$ over a finite grid of price increments $\\{\\delta_{k}\\}_{k\\in K}$ . Then, in a second exploitation phase, prices are set greedily to maximize the estimated reward. To bound the regret of this approach, note that uniform estimation of $D$ over the grid $\\{\\delta_{k}\\}_{k\\in K}$ with precision $\\epsilon$ requires $\\epsilon^{-2}|\\boldsymbol{\\kappa}|$ estimation rounds. Moreover, the Lipschitz continuity of the reward function implies a discretization error of order $^1\\!/\\!|\\kappa|$ . Classical arguments suggest that the regret would be at least $T(\\epsilon+1/|\\kappa|)+|\\mathcal{K}|\\epsilon^{-2}$ , which is minimized for $\\epsilon\\,=\\,1/|\\boldsymbol{\\kappa}|\\,=\\,T^{-1/4}$ . Thus, this approach would lead to a regret of order $\\tilde{\\mathcal{O}}(T^{3/4})$ . ", "page_idx": 3}, {"type": "text", "text": "Another approach, akin to that used in [10], involves partitioning the covariate space into bins and running independent algorithms for non-parametric bandits (such as CAB1 [17]) within each bin. Let us assume, for simplicity, contexts in $[0,1]$ , and that we partition this segment into $K$ bins. Then, the discretization error is $^1\\!/\\!K$ . Classical results show that the regret in one bin is $\\tilde{\\mathcal{O}}(T_{K}^{2/3})$ , where ", "page_idx": 3}, {"type": "text", "text": "$T_{K}=T/\\kappa$ is the number of rounds in each bin. Consequently, the regret is $\\tilde{\\mathcal{O}}(T/K+K\\times(T/K)^{2/3})$ , which is minimized for $K=T^{1/4}$ , resulting in a regret $\\tilde{\\mathcal{O}}(T^{3/4})$ . ", "page_idx": 4}, {"type": "text", "text": "Thus, both approaches \u2013 using either independent bandit algorithms over binned contexts or common exploration rounds followed by an exploitation phase \u2013 suffer a regret of order $T^{3/4}$ in the linear model. This raises the question of whether this rate is optimal for the linear model, and if the contextual dynamic pricing problem is indeed more difficult than the non-contextual one. Strikingly, we show that this is not the case. We rely on an intermediate approach, based on regret-minimizing algorithms for each valuation level $g(x_{t})$ that share information across different values of $g(x_{t})$ . We show that it achieves an optimal regret rate of order $\\tilde{\\mathcal{O}}(T^{2/3})$ in the linear valuation model. Moreover, it achieves a rate of order $\\tilde{\\mathcal{O}}(T^{d+2\\beta/d+3\\beta})$ in the non-parametric valuation model under minimal assumptions. ", "page_idx": 4}, {"type": "text", "text": "3 Algorithmic Approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present the general algorithmic approach that we use to tackle dynamic pricing with covariates, called VALUATION APPROXIMATION - PRICE ELIMINATION (VAPE). Before presenting the full scheme, described in Algorithm 1, we start with some intuition that leads to its design. Then, we provide a first analysis of the regret of this algorithm. ", "page_idx": 4}, {"type": "text", "text": "3.1 Outline of the Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Equation (2) highlights how the reward is influenced by the expected valuation $g(x_{t})$ and by the demand at the price increment $\\delta=p_{t}-g(x_{t})$ . To separate the effect of these terms, we estimate $g$ and $D$ independently. Hereafter, we assume that the valuations $y_{t}$ are bounded, in $[-B_{y},B_{y}]$ . ", "page_idx": 4}, {"type": "text", "text": "Estimation of $g$ . To estimate $g(x_{t})$ , we rely on the following observation: when prices $p_{t}$ are uniformly chosen from the interval $[-B_{y},B_{y}]$ , the random variable $2B_{y}\\left(o_{t}-1/2\\right)$ can serve as an unbiased estimate of $g(x_{t})$ conditioned on $x_{t}$ . Given that $2B_{y}\\left(o_{t}-\\bar{1}/2\\right)$ is bounded, classical concentration results can be employed to bound the error of our estimates for $g(x_{t})$ . Thus, in each round, we test whether our estimate of $g(x_{t})$ is precise enough to ensure that the error $g\\big(x_{t}\\big)-\\widehat{g}\\big(x_{t}\\big)$ is small. If this is not the case, we conduct a VALUATION APPROXIMATION round by setting a uniform price. In the next sections, we consider linear and non-parametric valuation functions, and we discuss how to ensure sufficient precision in a limited number of valuation approximation rounds. ", "page_idx": 4}, {"type": "text", "text": "Previous approaches for estimating valuation functions in the linear model include the regularized maximum-likelihood estimator [15, 30], which requires knowledge of the noise distribution. Another approach used in [22] relies on the relation between estimating a linear valuation function from binary feedback and the classical linear classification problem. The authors propose recovering the linear parameters $\\theta$ through logistic regression; however, they do not provide an explicit estimation rate for $\\theta$ . [20] use the EXP-4 algorithm to aggregate policies corresponding to different values of $\\theta$ and $F$ , thus circumventing the necessity to estimate them. In a similar vein, in the non-parametric valuation model, [10] avoid the need to estimate $g(x_{t})$ by employing independent bandit algorithms for each (binned) value of $x_{t}$ . Closer to our method are the works of [14] and [21], who also set uniform prices to obtain unbiased estimates of the valuations. Nonetheless, their algorithms are significantly different from ours. First, they propose two-phased algorithms for which the phase length is set beforehand. Such an approach necessitates additional assumptions on how contexts are drawn; specifically, contexts are assumed to be i.i.d. from a distribution with a lower bound on the eigenvalues of the covariance matrix. This is needed to ensure that contexts observed in the first phase can represent the context distribution well. By contrast, our phases are adaptive, allowing our algorithm also to handle adversarial contexts and render these assumptions superfluous. Second, we obtain better regret rates by using piecewise-constant estimators, fitted in a regret-minimization sub-routine, as detailed in the next paragraph. On the other hand, [14] performs a phase of pure exploitation, relying on an estimate of the CDF $F$ that is constructed using Kernel methods. [21], instead, re-frames the problem as a perturbed linear bandit, which exhibits a regret linear in the dimension. However, this dimension depends on the size of the discretization grid \u2013 which is horizon dependent \u2013 leading to worse rates. ", "page_idx": 4}, {"type": "text", "text": "Estimation of $D$ . If the expected valuation $g(x_{t})$ is known with sufficient precision, we can use it to estimate the demand function over a set of candidate price increments $\\{\\delta_{k}\\}_{k\\in K}$ . More precisely, assume we set a price $p_{t}={\\widehat g}(x_{t})+\\delta_{k}$ , and that $|\\widehat{g}(x_{t})-g(x_{t})|\\leq\\epsilon$ . Then, the observation $o_{t}$ can ", "page_idx": 4}, {"type": "text", "text": "1: Input: Price increments $\\left\\{\\delta_{k}\\right\\}_{k\\in\\kappa}$ , expected valuation precision $\\operatorname{err}_{t}(x)$ , reward confidence intervals $[\\mathrm{LCB}_{t}(k),\\mathrm{UCB}_{t}(k)]$ , parameters $\\alpha,\\epsilon$ . ", "page_idx": 5}, {"type": "text", "text": "2: while $t\\leq T$ do   \n3: if $\\mathrm{err}_{t}(x_{t})>\\epsilon$ then $\\triangleright$ Valuation Approximation   \n4: Post a price $p_{t}\\sim\\mathcal{U}([-B_{y},B_{y}])$   \n5: Use $o_{t}$ to improve the valuation estimator $\\widehat{g}(\\boldsymbol{x}_{t})$   \n6: else \u25b7Price Elimination   \n7: 8: $\\begin{array}{r l}&{\\mathcal{A}_{t}\\gets\\{k\\in\\mathcal{K}:\\hat{g}_{t}+\\delta_{k}\\in\\{0,B_{y}]\\}}\\\\ &{\\mathcal{K}_{t}\\gets\\{k\\in\\mathcal{A}_{t}:{\\mathrm{UCB}_{t}}(k)\\geq\\operatorname*{max}_{k^{\\prime}\\in\\mathcal{A}_{t}}{\\mathrm{LCB}_{t}}(k^{\\prime})\\}}\\end{array}$   \n9: Choose $k_{t}\\in\\arg\\operatorname*{min}_{k\\in\\mathcal{K}_{t}}N_{t}^{k}$ and post a price $p_{t}=\\widehat{g}_{t}+\\delta_{k_{t}}$   \n10: Update D tkt+1, $N_{t+1}^{k_{t}}$ ", "page_idx": 5}, {"type": "text", "text": "be used as an almost unbiased estimate of the demand at level $\\delta_{k}$ , since ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[o_{t}]=\\mathbb{E}\\left[\\mathbb{1}\\{\\widehat{g}(x_{t})+\\delta_{k}\\leq g(x_{t})+\\xi_{t}\\}\\right]=D(\\delta_{k}+\\widehat{g}(x_{t})-g(x_{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Under Assumption 2, $D$ is $L_{\\xi}$ -Lipschitz, so the bias is of order $L_{\\xi}\\epsilon$ . Then, relying on classical bandit techniques, we show that with high probability (for $\\alpha$ small enough), $|D(\\delta_{k})-\\widehat{D}_{t}^{k}|$ is of order $L_{\\xi}\\epsilon+\\sqrt{\\log(1/\\alpha)\\big/N_{t}^{k}}$ , where $\\widehat{D}_{t}^{k}$ is the average of the observations $o_{t}$ when setting a price $p_{t}\\,=\\,\\widehat{g}(x_{t})\\,+\\,\\delta_{k}$ , and $N_{t}^{k}$ is the number of rounds in which we chose the price increment $\\delta_{k}$ up to round $t$ . Importantly, to estimate $\\widehat{D}_{t}^{k}$ , we share information collected during all rounds we chose the increment $\\delta_{k}$ across all val ues of $\\widehat{g}(\\boldsymbol{x}_{t})$ ; this is necessary to obtain better regret rates. Then, using $p_{t}\\widehat{D}_{t}^{k}$ as an estimate of the reward $\\pi(x_{t},p_{t})$ given the price $p_{t}={\\widehat{g}}(x_{t})+\\delta_{k}$ , the error $|\\pi(x_{t},p_{t})-p_{t}\\widehat{D}_{t}^{k}|$ is of order $B_{y}(L_{\\xi}\\epsilon+\\sqrt{\\log(1/\\alpha)\\big/N_{t}^{k}})$ . ", "page_idx": 5}, {"type": "text", "text": "The PRICE ELIMINATION subroutine relies on the previous remark to select a price increment. For each increment $\\delta_{k}$ , we build a confidence bound $\\left[\\mathrm{LCB}_{t}(\\delta_{k}),\\mathrm{UCB}_{t}(\\delta_{k})\\right]\\,=\\,\\left[p_{t}\\widehat{D}_{t}^{k}\\pm B_{y}(2L_{\\xi}\\epsilon\\,+\\,$ $\\sqrt{2\\log(1/\\alpha)\\big/N_{t}^{k}})\\big]$ for the reward of price $p_{t}={\\widehat g}(x_{t})+\\delta_{k}$ . Then, we use a succe ssive elimination algorithm [13, 25] to select a good increment.  M ore precisely, we consider increments $\\delta_{k}$ such that $\\mathrm{UCB}_{t}(\\delta_{k})\\,\\geq\\,\\mathrm{max}_{l}\\,\\mathrm{LCB}_{t}(\\delta_{l})$ , and we choose among these increments the increment $\\delta_{k_{t}}$ that has been selected the least frequently. By doing so, we ensure to only select potentially optimal prices and gradually eliminate sub-optimal increments. ", "page_idx": 5}, {"type": "text", "text": "3.2 A First Bound on the Regret ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Before discussing the application of the algorithmic scheme VAPE to linear and non-parametric valuation functions, we provide some intuition on regret bounds achievable through this scheme. ", "page_idx": 5}, {"type": "text", "text": "Claim 1. (Informal) Let $\\delta_{k}\\,=\\,k\\epsilon$ for $\\begin{array}{r}{k\\,\\in\\,\\mathcal{K}\\,\\triangleq\\,\\[\\![\\frac{-B_{y}-1}{2}\\!/\\epsilon],\\,\\lceil B_{y}\\!+\\!1\\!/\\epsilon]\\rceil.}\\end{array}$ . Assume that, on a highprobability event, $|\\widehat{g}(x_{t})-g(x_{t})|\\leq\\epsilon$ for every ro und $t$ where PRICE E LIMINATION is conducted. Then, on a high-probability event, the regret of VAPE verifies ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{T}\\lesssim\\mathrm{T}^{\\mathrm{VA}}(\\epsilon)+T\\epsilon+\\log(1/\\alpha)\\epsilon^{-2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{T}^{\\mathrm{VA}}(\\epsilon)$ is a bound on the length of the VALUATION APPROXIMATION phase. ", "page_idx": 5}, {"type": "text", "text": "Claim 1 is proved in the Appendix by combining Equations (4) and (5), and Lemma 4. We provide a sketch of proof below. To bound on regret of VAPE using Claim 1, it will suffice to bound the length of the VALUATION APPROXIMATION phase, and prove high-probability error bounds on $g(x_{t})$ . ", "page_idx": 5}, {"type": "text", "text": "Sketch of proof. Note that the regret in the VALUATION APPROXIMATION phase scales at most linearly with its length. Then, to prove Claim 1, it is enough to bound the regret during the PRICE ELIMINATION phase. We begin by bounding the sub-optimality gap of the price chosen at round $t$ , showing that it is of order $\\epsilon+\\sqrt{\\log(1/\\alpha)\\big/N_{t}^{k_{t}}}$ . ", "page_idx": 5}, {"type": "text", "text": "To do so, for $p\\,\\in\\,\\mathbb{R}$ , we define $\\Delta_{t}(x_{t},p)\\,=\\,\\mathrm{max}_{p^{\\prime}}\\,\\pi(x_{t},p^{\\prime})\\,-\\,\\pi(x_{t},p)$ the sub-optimality gap corresponding to price $p$ . Recall that $\\delta_{k_{t}}$ is the increment chosen at round $t$ , i.e. that $p_{t}\\,\\bar{=}\\,\\widehat{g}(x_{t})\\,\\bar{+}\\,\\bar{\\delta}_{k_{t}}$ . ", "page_idx": 5}, {"type": "text", "text": "Classical arguments from the bandit literature show that with high probability, for all $k\\in\\mathcal{K}$ , the upper and lower confidence bounds on $\\pi(x_{t},{\\widehat{g}}(x_{t})+\\delta_{k})$ given by $\\mathrm{UCB}_{t}\\big(\\delta_{k}\\big)$ and $\\mathrm{LCB}_{t}\\big(\\delta_{k}\\big)$ are valid. Then, the optimal increment $\\delta_{k_{t}^{*}}$ define d  by $k^{*}=\\arg\\operatorname*{max}_{k\\in\\mathcal{A}_{t}}\\pi(x_{t},\\widehat{g}(x_{t})+\\delta_{k})$ belongs to the set of non-eliminated increments. Now, on the one hand, since $\\mathrm{UCB}_{t}\\big(\\delta_{k_{t}}\\big)\\,\\ge\\,\\mathrm{LCB}_{t}\\big(\\delta_{k_{t}^{*}}\\big)$ , and since the confidence interval are valid, the gap $\\pi(x_{t},\\widehat{g}(x_{t})\\,+\\,\\delta_{k_{t}^{*}})\\,-\\,\\pi(x_{t},p_{t})$ is of order $\\epsilon+\\sqrt{2\\log(1/\\alpha)/N_{t}^{k_{t}}}+\\sqrt{2\\log(1/\\alpha)/N_{t}^{k_{t}^{*}}}$ . Our round-robin sampling scheme ensures that $N_{t}^{k_{t}^{*}}\\geq N_{t}^{k_{t}}$ , so this bound is of order $\\epsilon+\\sqrt{\\log(1/\\alpha)\\big/N_{t}^{k_{t}}}$ . On the other hand, our choice of grid $\\{\\delta_{k}\\}_{k\\in K}$ , together with the Lipschitz-continuity of the reward in Assumption 2, imply that the cost $\\Delta_{t}\\dot{(x_{t},\\hat{g}(x_{t})+\\delta_{k_{t}^{*}})}$ of considering a discrete price grid is of order $B_{y}L_{\\xi}\\epsilon$ . Thus, at each round, the gap $\\Delta_{t}(x_{t},\\widehat{g}(x_{t})\\!+\\!\\delta_{k_{t}})$ is at most of order $\\epsilon+\\sqrt{\\log(1/\\alpha)\\big/N_{t}^{k_{t}}}$ (up to problem-dependent constants). ", "page_idx": 6}, {"type": "text", "text": "Now, let us decompose the regret of the PRICE ELIMINATION phase as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathrm{PRICE~ELIMINATION~phase}}\\Delta(x_{t},p_{t})=\\sum_{k\\in K}\\sum_{t:k_{t}=k}\\Delta(x_{t},p_{t}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In order to bound $\\textstyle\\sum_{t:k_{t}=k}\\Delta(x_{t},p_{t})$ for $k\\,\\in\\,\\kappa$ , we begin by introducing further notations. Let us denote $\\tau_{1}^{k},\\dots,\\tau_{T}^{k}$ the rounds in the PRICE ELIMINATION phase where we choose $k_{t}=k$ . We also define $\\Delta_{a}\\,=\\,2^{-\\,a}$ and $\\overline{a}$ such that $\\Delta_{\\overline{{{a}}}}\\,\\approx\\,\\epsilon$ . For all $a\\leq\\overline{{a}}$ , we also define $\\mathrm{t}_{a}$ such that the bound $\\epsilon+\\sqrt{\\log(1/\\alpha)}/\\mathfrak{t}_{a}$ is of order $\\Delta_{a}$ . Then, our previous reasoning implies that if $i\\geq\\mathfrak{t}_{a}$ for some $a\\in\\{1,\\overline{{a}}\\}$ , it must be that $\\Delta_{t}(x_{t},p_{\\tau_{i}^{k}})\\leq\\Delta_{a}$ . Moreover, for $a\\geq1$ , each phase $\\{\\mathfrak{t}_{a},\\dots,\\mathfrak{t}_{a+1}\\}$ is of length approximately $\\log(1/\\alpha)(\\Delta_{a+1}^{-\\bar{2}}-\\Delta_{a}^{-2})$ . Thus, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t:k_{t}=k}\\Delta(x_{t},p_{t})\\lesssim\\frac{\\log(1/\\alpha)}{\\Delta_{1}}+\\sum_{a=1}^{\\overline{{a}}-1}\\Delta_{a}\\times\\left(\\frac{\\log(1/\\alpha)}{\\Delta_{a+1}^{2}}-\\frac{\\log(1/\\alpha)}{\\Delta_{a}^{2}}\\right)+\\Delta_{\\overline{{a}}}N_{T}^{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Using the definitions of $\\Delta_{a}$ and $\\overline{a}$ , we find that this sum is of order $\\log(1/\\alpha)/\\epsilon+\\epsilon N_{T}^{k}$ . We conclude by summing over the values of $k\\in\\mathcal{K}$ , using $\\begin{array}{r}{\\sum_{k\\in\\mathcal{K}}N_{T}^{k}\\leq T}\\end{array}$ and the fact that $|\\kappa|$ is of order $\\epsilon^{-1}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Linear Valuation Functions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we consider the linear valuation model, given by ", "page_idx": 6}, {"type": "equation", "text": "$$\ng(x)=x^{\\top}\\theta\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\theta\\in\\mathbb{R}^{d}$ is an unknown parameter. To ensure that the valuations are bounded, we assume the boundedness of the parameter $\\theta$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. The parameter $\\theta$ is bounded: $\\lVert\\theta\\rVert\\leq B_{\\theta}$ ", "page_idx": 6}, {"type": "text", "text": "Note that under Assumptions 1 and 3, the expected valuations $g(x_{t})$ verify $|g(x_{t})|\\;\\le\\;B_{g}$ for $B_{g}=B_{x}\\times B_{\\theta}$ . Moreover, the random valuations verify a.s. $|y_{t}|\\le B_{y}$ for $B_{y}=B_{g}+B_{\\xi}$ . ", "page_idx": 6}, {"type": "text", "text": "We apply the VAPE algorithmic scheme to the problem of dynamic pricing with linear valuations. To estimate the valuation function, we use a ridge estimator for the parameter $\\theta$ . Moreover, we distinguish between phases by setting $\\iota_{t}=1$ if $t$ belongs to the VALUATION APPROXIMATION phase and $\\iota_{t}=0$ if $t$ belongs to the PRICE ELIMINATION one. The details are presented in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Assume that the valuations follow the model given by Equations (1) and (3). Under Assumptions 1, 2, and 3, the regret of Algorithm VAPE for Linear Valuations with parameters ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}\\leq C_{B_{\\xi},B_{x},B_{\\theta},L_{\\xi}}d^{2/3}T^{2/3}\\log(T)^{2/3}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability $1-T^{-1}$ , where $C_{B_{\\xi},B_{x},B_{\\theta},L_{\\xi}}$ is a constant that polynomially depends on $B_{\\xi},\\,B_{x}$ , $B_{\\theta}$ , and $L_{\\xi}$ . ", "page_idx": 6}, {"type": "text", "text": "Sketch of proof. [See Appendix $\\mathbf{B}$ for the full proof] Using Claim 1, we see that it is enough to prove that the VALUATION APPROXIMATION phase allows to estimate $g(x_{t})$ up to precision $\\epsilon=$ $\\left(d^{2}\\log(T)^{2}/T\\right)^{1/3}$ in at most $O(d^{2/3}T^{2/3}\\log(T)^{2/3})$ rounds. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 VALUATION APPROXIMATION - PRICE ELIMINATION (VAPE) for Linear Valuations ", "page_idx": 7}, {"type": "text", "text": "1: Input: bounds $B_{y}$ and $L_{\\xi}$ , parameters $\\alpha$ , $\\mu,\\epsilon$ .   \n2: Initialize: $\\widehat{\\theta}_{1}=\\bar{\\mathbf{0}}_{d}$ , $\\mathbf{V}_{1}=\\mathbf{I}_{d}$ , $K=\\lceil(B_{y}{+}1)/\\epsilon\\rceil$ , $\\kappa=[-K,K]$ , and for $k\\in\\mathcal{K}$ , $N_{1}^{k}=\\widehat{D}_{1}^{k}=0$ .   \n3: while $t\\leq T$ do   \n4: if $\\|\\boldsymbol{x}_{t}\\|_{\\mathbf{v}_{t}^{-1}}>\\mu$ then $\\triangleright$ Valuation Approximation   \n5: Post a price $p_{t}\\sim\\mathcal{U}([-B_{y},B_{y}])$   \n6: $\\begin{array}{r}{\\iota_{t}\\gets1,\\mathbf{V}_{t+1}\\gets\\displaystyle\\sum_{s\\leq t}\\iota_{s}x_{s}x_{s}^{\\top}+\\mathbf{I}_{d},\\widehat{\\theta}_{t+1}\\gets2B_{y}\\mathbf{V}_{t+1}^{-1}\\displaystyle\\sum_{s\\leq t}\\iota_{s}\\left(o_{s}-\\frac{1}{2}\\right)x_{s}}\\end{array}$   \n7: else $\\triangleright$ Price Elimination   \n8: $\\iota_{t}\\gets0$ , $\\widehat{g}_{t}\\gets x_{t}^{\\top}\\widehat{\\theta}_{t}$ $\\mathfrak{t},\\mathcal{A}_{t}\\gets\\{k\\in\\mathcal{K}:\\widehat{g}_{t}+k\\epsilon\\in[0,B_{y}]\\}$   \n9: for $k\\in\\mathcal A_{t}$ do   \n10: $\\begin{array}{r l}&{\\mathrm{UCB}_{t}(k)\\leftarrow\\left(\\widehat{g}_{t}+k\\epsilon\\right)\\left(\\widehat{D}_{t}^{k}+\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+2L_{\\xi}\\epsilon\\right)}\\\\ &{\\mathrm{LCB}_{t}(k)\\leftarrow\\left(\\widehat{g}_{t}+k\\epsilon\\right)\\left(\\widehat{D}_{t}^{k}-\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}-2L_{\\xi}\\epsilon\\right)}\\end{array}$   \n11:   \n12: $K_{t}\\gets\\{k\\in\\mathcal{A}_{t}:\\operatorname{UCB}_{t}(k)\\geq\\operatorname*{max}_{k^{\\prime}\\in\\mathcal{A}_{t}}\\operatorname{LCB}_{t}(k^{\\prime})\\}$   \n13: Choose $k_{t}\\in\\arg\\operatorname*{min}_{k\\in\\mathcal{K}_{t}}N_{t}^{k}$ and post a price $p_{t}=\\widehat{g}_{t}+k_{t}\\epsilon$   \n14: Update $\\begin{array}{r}{\\widehat{D}_{t+1}^{k_{t}}\\gets\\frac{N_{t}^{k_{t}}\\widehat{D}_{t}^{k_{t}}+o_{t}}{N_{t}^{k_{t}}+1}}\\end{array}$ , $N_{t+1}^{k_{t}}\\gets N_{t}^{k_{t}}+1.$ ", "page_idx": 7}, {"type": "text", "text": "To prove the first part of the claim, note that for all rounds in the PRICE ELIMINATION phase, $\\|x_{t}\\|_{\\mathbf{V}_{t}^{-1}}\\leq\\mu=\\epsilon/\\Bigl(B_{y}\\sqrt{d\\log\\bigl(^{1+B_{x}^{2}T/\\alpha}\\bigr)}+B_{\\theta}\\Bigr)$ . Then, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\widehat{g}(x_{t})-g(x_{t})|\\leq\\|\\theta-\\widehat{\\theta}_{t}\\|_{\\mathbf{v}_{t}}\\|x_{t}\\|_{\\mathbf{v}_{t}^{-1}}\\leq\\|\\theta-\\widehat{\\theta}_{t}\\|_{\\mathbf{v}_{t}}\\times\\epsilon/\\Big(B_{y}\\sqrt{d\\log\\left(1+B_{x}^{2}T/\\alpha\\right)}+B_{\\theta}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Classical result on ridge regression in bandit framework [1] show that on a large probability event, $\\begin{array}{r}{\\|\\theta-\\widehat\\theta_{t}\\|_{\\mathbf{V}_{t}}\\le\\left(B_{y}\\sqrt{d\\log\\left(1+B_{x}^{2}T/\\alpha\\right)}+B_{\\theta}\\right)}\\end{array}$ , so $|\\widehat{g}(x_{t})-g(x_{t})|\\leq\\epsilon.$ . ", "page_idx": 7}, {"type": "text", "text": "To prove the second part of the claim, we rely on the elliptical potential lemma to bound the number of rounds where $\\|x_{t}\\|_{\\mathbf{V}_{t}^{-1}}\\,\\geq\\,\\mu$ . This Lemma states that $\\begin{array}{r}{\\sum_{i=1}^{|\\mathcal{G}|}\\|x_{t_{i}}\\|_{\\mathbf{v}_{t_{i}-1}^{-1}}\\leq\\sqrt{|\\mathcal{G}|d\\log\\left(|\\mathcal{G}|+d/d\\right)}}\\end{array}$ , where $t_{i}$ is the $i$ -th round of the VALUATION APPROXIMATION phase, and $|\\mathcal G|$ is its length. Using the fact that $\\|x_{t_{i}}\\|_{\\mathbf{V}_{t_{i}-1}^{-1}}\\geq\\mu$ , we conclude that \u2264 d log(\u00b5T2 +d/d), which implies the result. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 provides a regret bound of order $\\tilde{O}(T^{2/3})$ , showing that VAPE for Linear Valuations is minimax optimal, possibly up to sub-logarithmic terms and to sub-linear dependence in the dimension. Indeed, it matches the $T^{2/3}$ lower bound established in [31] for linear valuation functions and Lipschitz-continuous demand functions. This result represents a clear improvement over the existing regret bounds for the same problem. Indeed, VAPE achieves the regret bound conjectured in [22] while at the same time removing their regularity assumption on the revenue function. On the other hand, we improve on the regret rate $\\tilde{\\mathcal{O}}(\\bar{T^{3/4}})$ achieved respectively in [31] under assumptions slightly milder than ours, and in [14] under stronger assumptions. ", "page_idx": 7}, {"type": "text", "text": "5 Non-Parametric Valuation Functions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this Section, we consider the non-parametric valuation model. As usual in dynamic pricing, we assume that the valuation function $g$ is bounded. Furthermore, we assume that it is $(L_{g},\\beta)$ -H\u00f6lder continuous for some constants $L_{g}>0$ and $0<\\beta\\leq1$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 4. The valuation function $g$ is $(L_{g},\\,\\beta)$ -H\u00f6lder: for all $(x,x^{\\prime})\\in\\mathbb R^{d}$ , $|g(x)-g(x^{\\prime})|\\leq$ $L_{g}\\parallel\\!x-x^{\\prime}\\parallel^{\\beta}$ . ", "page_idx": 7}, {"type": "text", "text": "Under Assumptions 1 and 2, the random valuations $y_{t}$ verify $|y_{t}|\\le B_{y}$ for $B_{y}=B_{\\xi}+B_{g}$ . ", "page_idx": 7}, {"type": "text", "text": "Next, we apply the VAPE algorithmic scheme to the non-parametric valuation model. To estimate the function $\\mathrm{g}$ , we use a finite grid of points, on which this function is evaluated. More precisely, we ", "page_idx": 7}, {"type": "text", "text": "Algorithm 3 VALUATION APPROXIMATION - PRICE ELIMINATION (VAPE) for Non-Parametric Valuations ", "page_idx": 8}, {"type": "text", "text": "1: Input: bounds $B_{y}$ and $L_{\\xi}$ , finite set $\\overline{{\\mathcal{X}}}\\subset\\mathbb{R}^{d}$ , parameters $\\alpha,\\tau,\\epsilon$ .   \n2: Initialize: $\\mathcal{G}_{\\overline{{x}}}=\\emptyset$ for all $\\overline{{x}}\\in\\overline{{\\mathcal{X}}}$ , $K=\\lceil B_{y}\\!+\\!1/\\epsilon\\rceil$ , $\\kappa=[-K,K]$ , and for $k\\in\\mathcal{K}$ , $N_{1}^{k}=\\widehat{D}_{1}^{k}=0$ .   \n3: while $t\\leq T$ do $\\overline{{x}}_{t}\\gets\\arg\\operatorname*{min}_{\\overline{{x}}^{\\prime}\\in\\overline{{\\mathcal{X}}}}\\|x_{t}-\\overline{{x}}^{\\prime}\\|$   \n4: if $|\\mathcal{G}_{\\overline{{x}}_{t}}|<\\tau$ then \u25b7Price Elimination   \n5: Post a price $p_{t}\\sim\\mathcal{U}([-B_{y},B_{y}])$   \n6: $\\mathcal{G}_{\\overline{{x}}_{t}}\\gets\\mathcal{G}_{\\overline{{x}}_{t}}\\cup\\{t\\},\\widehat{g}(\\overline{{x}}_{t})\\gets\\frac{2B_{y}}{|\\mathcal{G}_{\\overline{{x}}_{t}}|}\\sum_{s\\in\\mathcal{G}_{\\overline{{x}}_{t}}}\\left(o_{s}-\\frac{1}{2}\\right)$   \n7: else \u25b7Run Successive Elimination   \n8: $\\widehat{g}_{t}\\gets\\widehat{g}(\\overline{{x}}_{t}),\\mathcal{A}_{t}\\gets\\{k\\in\\mathcal{K}:\\widehat{g}_{t}+k\\epsilon\\in[0,B_{y}]\\}$   \n9: f or $k\\in\\mathcal A_{t}$ do   \n10: $\\begin{array}{r l}&{\\mathrm{UCB}_{t}(k)\\leftarrow\\left(\\widehat{g}_{t}+k\\epsilon\\right)\\left(\\widehat{D}_{t}^{k}+\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+2L_{\\xi}\\epsilon\\right)}\\\\ &{\\mathrm{LCB}_{t}(k)\\leftarrow\\left(\\widehat{g}_{t}+k\\epsilon\\right)\\left(\\widehat{D}_{t}^{k}-\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}-2L_{\\xi}\\epsilon\\right)}\\end{array}$   \n11:   \n12: $K_{t}\\gets\\{k\\in\\mathcal{A}_{t}:\\operatorname{UCB}_{t}(k)\\geq\\operatorname*{max}_{k^{\\prime}\\in\\mathcal{A}_{t}}\\operatorname{LCB}_{t}(k^{\\prime})\\}$   \n13: Choose $k_{t}\\in\\arg\\operatorname*{min}_{k\\in\\mathcal{K}_{t}}N_{t}^{k}$ and post a price $p_{t}=\\widehat{g}_{t}+k_{t}\\epsilon$   \n14: Update $\\begin{array}{r}{\\widehat{D}_{t+1}^{k_{t}}\\gets\\frac{N_{t}^{k_{t}}\\widehat{D}_{t}^{k_{t}}+o_{t}}{N_{t}^{k_{t}}+1}}\\end{array}$ NtkNtkD ttk +t1+ot, N tk+t1 \u2190N tkt+ 1. ", "page_idx": 8}, {"type": "text", "text": "consider a minimal $\\left(\\epsilon/3L_{g}\\right)^{1/\\beta}$ -covering $\\overline{{\\mathcal{X}}}$ of the ball of radius $B_{x}$ in $R^{d}$ , i.e. a finite set of points, of minimal cardinality, such that for any context $x$ such that $\\|x\\|\\leq B_{x}$ , there exists a point in $\\overline{{\\mathcal X}}$ at a distance at most $\\left(\\epsilon/3L_{g}\\right)^{1/\\beta}$ from $x$ . ", "page_idx": 8}, {"type": "text", "text": "At each round, we round the context $x_{t}$ to the closest context $\\overline{{x}}$ in $\\overline{{\\mathcal{X}}}$ by setting $\\overline{{x}}_{t}=$ $\\arg\\operatorname*{min}_{\\overline{{x}}^{\\prime}\\in\\overline{{x}}}\\|x_{t}-\\overline{{x}}^{\\prime}\\|$ , and acting as if we observed the context $\\overline{{x}}_{t}$ . If this context has not been observed sufficiently, we conduct a round of VALUATION APPROXIMATION: we sample a price uniformly at random and use it to update our estimate of $g(\\overline{{x}}_{t})$ ; otherwise, we proceed with the PRICE ELIMINATION phase. To distinguish between the VALUATION APPROXIMATION steps corresponding to contexts $\\overline{{x}}\\in\\overline{{\\mathcal{X}}}$ , we collect their indices in sets $\\mathcal{G}_{\\overline{{x}}}$ . The algorithm is presented in Algorithm 3. ", "page_idx": 8}, {"type": "text", "text": "Theorem 2. Assume that the valuations follow the model given by Equation (1). Under Assumptions 1, 2, and 4, with probability $1-T^{-1}$ the regret of Algorithm VAPE for non-parametric Valuations with parameters $\\begin{array}{r}{\\epsilon=\\left(T/\\mathrm{log}(T)\\right)^{\\frac{-\\beta}{d+3\\beta}}}\\end{array}$ , $\\alpha=T^{-4}$ , $\\tau=18B_{y}^{2}\\log(2|\\overline{{x}}|/\\alpha)\\big/\\epsilon^{2}$ , and $\\overline{{\\mathcal{X}}}$ a minimal $\\left(\\epsilon/3L_{g}\\right)^{1/\\beta}$ - covering of the ball of radius $B_{x}$ verifies ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{T}\\leq C_{B_{x},B_{g},B_{\\xi},L_{g},L_{\\xi},d,\\beta}T^{\\frac{d+2\\beta}{d+3\\beta}}\\log(T)^{\\frac{\\beta}{d+3\\beta}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $C_{B_{x},B_{g},B_{\\xi},L_{g},L_{\\xi},d,\\beta}$ is a constant that polynomially depends on $B_{x},\\,B_{g},\\,B_{\\xi},\\,L_{g},\\,L_{\\xi},\\,d,$ and $\\beta$ . ", "page_idx": 8}, {"type": "text", "text": "Sketch of proof. [See Appendix $\\mathbf{C}$ for the full proof] Using Claim 1, we only need to show that the length of the VALUATION APPROXIMATION phase is at most of order $T^{d+2\\bar{\\beta}/d+3\\beta}\\log(T)^{\\beta/d+3\\beta}$ and that w.h.p., it allows estimating $g$ uniformly on a ball of radius $B_{x}$ with precision $\\epsilon\\!=\\!\\left(T\\!/\\!\\log(T)\\right)^{-\\beta/d+3\\beta}$ . ", "page_idx": 8}, {"type": "text", "text": "To prove the first part of the claim, we note that classical results imply that the size of a minimal covering of precision $\\epsilon^{1/\\beta}$ of a ball in dimension $d$ scales as $\\epsilon^{-d/\\beta}$ . Then, the total length of the VALUATION APPROXIMATION phase is of order $\\epsilon^{-d/\\beta}\\tau\\approx T^{d+2\\beta/d+3\\beta}\\log(T)^{\\beta/d+3\\beta}$ . To prove the second part of the lemma, note that the H\u00f6lder-continuity of $g$ and the definition of the $\\left(\\epsilon/3L_{g}\\right)^{1/\\beta}$ - covering $\\mathcal{G}$ ensure that $|g(x_{t})-g(\\overline{{x}}_{t})|\\leq\\epsilon/3$ . Then, standard concentration arguments reveal that $\\tau\\approx$ $\\log(|\\overline{{\\mathcal{X}}}|/\\alpha)/\\epsilon^{2}$ samples are sufficient to estimate $g(\\overline{{x}}_{t})$ with precision $\\epsilon$ with high probability. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Theorem 2 shows that the Algorithm VALUATION APPROXIMATION \u2013 PRICE ELIMINATION for non-parametric valuations enjoys a $\\tilde{O}(T^{d+2\\beta/d+3\\beta})$ regret bound when the noise c.d.f. is Lipschitz and the valuation function H\u00f6lder-continuous. This result is the first of its kind under such minimal assumptions. In particular, previous work by [10] assumes quadratic behavior around the optimal price for all values of $g(x)-{\\mathsf{a}}$ very strong assumption. However, this rate is higher than the $\\tilde{O}\\bar{(T^{d+\\beta/d+2\\beta})}$ rates that are usually encountered in $\\beta$ -H\u00f6lder non-parametric bandits [7]. Thus, the question of optimality of the VAPE algorithmic scheme in the non-parametric valuation problem remains open. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we studied the problem of dynamic pricing with covariates. We first presented a novel algorithmic approach called VAPE, which adaptively alternates between improving the valuation approximation and learning to set prices through successive elimination. We then applied VAPE under two valuation models \u2013 when the buyer\u2019s valuation corresponds to a noisy linear function and when expected valuations follow a smooth non-parametric model. In the linear case, our regret bounds are order-optimal, while in the non-parametric setting, we improve existing results. All our results are proven under regularity assumptions that are either milder or match existing assumptions. ", "page_idx": 9}, {"type": "text", "text": "Our results on the linear valuation model are the first to match the existing lower bound rate of $\\Omega\\!\\left(T^{2/3}\\right)$ under our assumptions. However, the optimal dependence of this rate on the dimension of the context remains unknown. Additionally, there are no similar lower bounds for non-parametric valuations. We conjecture that our results are also tight in this setting but leave this for future work. Future research directions also include exploring other valuation models, and further relaxing our assumptions, as Lipschitz-continuity of the noise (Assumption 2). Without this, even minor increases in the price could lead to a major drop in revenue, magnifying the impact of valuation approximation errors. Another limiting assumption is that the noise is independent and identically distributed, such that its distribution can be learned across different contexts. It is of great interest to study problems where the noise distribution can change between rounds, or depends on the context. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As all pricing problems, dynamic pricing can have both positive and negative impacts \u2013 offering prices that are more suited to the buyers on the one hand, while increasing the seller\u2019s revenue at the expense of buyers on the other hand. In addition, as with many contextual problems, there might be biases and challenges involving fairness \u2013 one should make sure that similar customers are offered similar prices. While acknowledging these issues, our work was meant to focus only on the theoretical analysis of what is considered a well-established problem in literature, leaving the study of these related topics as future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No 101034255. Solenne Gaucher gratefully acknowledges funding from the Fondation Math\u00e9matique Jacques Hadamard. Vianney Perchet acknowledges support from the French National Research Agency (ANR) under grant number (ANR-19-CE23-0026 as well as the support grant, as well as from the grant \u201cInvestissements d\u2019Avenir\u201d (LabEx Ecodec/ANR-11-LABX-0047). This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR-23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \n[2] Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In Conference on Learning Theory, pages 23\u201335. PMLR, 2015.   \n[3] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic buyers. Advances in Neural Information Processing Systems, 27, 2014. [4] Santiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni, and Jon Schneider. Contextual bandits with cross-learning. Advances in Neural Information Processing Systems, 32, 2019. [5] Omar Besbes and Assaf Zeevi. Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms. Operations research, 57(6):1407\u20131420, 2009.   \n[6] Gabriel Bitran and Ren\u00e9 Caldentey. An overview of pricing models for revenue management. Manufacturing & Service Operations Management, 5(3):203\u2013229, 2003. [7] S\u00e9bastien Bubeck, R\u00e9mi Munos, Gilles Stoltz, and Csaba Szepesv\u00e1ri. <i>x</i>-armed bandits. Journal of Machine Learning Research, 12(46):1655\u20131695, 2011. URL http://jmlr.org/ papers/v12/bubeck11a.html.   \n[8] Alexandra Carpentier, Claire Vernade, and Yasin Abbasi-Yadkori. The elliptical potential lemma revisited. arXiv preprint arXiv:2010.10182, 2020. [9] Nicolo Cesa-Bianchi, Tommaso Cesari, and Vianney Perchet. Dynamic pricing with finitely many unknown valuations. In Algorithmic Learning Theory, pages 247\u2013273. PMLR, 2019.   \n[10] Ningyuan Chen and Guillermo Gallego. Nonparametric pricing analytics with customer covariates. Operations Research, 69(3):974\u2013984, 2021.   \n[11] Maxime C Cohen, Ilan Lobel, and Renato Paes Leme. Feature-based dynamic pricing. Management Science, 66(11):4921\u20134943, 2020.   \n[12] Arnoud V Den Boer. Dynamic pricing and learning: historical origins, current research, and new directions. Surveys in operations research and management science, 20(1):1\u201318, 2015.   \n[13] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006.   \n[14] Jianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. Journal of the American Statistical Association, 119(545):552\u2013564, 2024.   \n[15] Adel Javanmard and Hamid Nazerzadeh. Dynamic pricing in high-dimensions. Journal of Machine Learning Research, 20(9):1\u201349, 2019.   \n[16] N Bora Keskin and Assaf Zeevi. Dynamic pricing with an unknown demand model: Asymptotically optimal semi-myopic policies. Operations research, 62(5):1142\u20131167, 2014.   \n[17] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural Information Processing Systems, 17, 2004.   \n[18] Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., pages 594\u2013605. IEEE, 2003.   \n[19] Kenneth Littlewood. Forecasting and control of passenger bookings. The Airline Group of the International Federation of Operational Research Societies, 12:95\u2013117, 1972.   \n[20] Allen Liu, Renato Paes Leme, and Jon Schneider. Optimal contextual pricing and extensions. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1059\u20131078. SIAM, 2021.   \n[21] Yiyun Luo, Will Wei Sun, and Yufeng Liu. Contextual dynamic pricing with unknown noise: Explore-then-ucb strategy and improved regrets. Advances in Neural Information Processing Systems, 35:37445\u201337457, 2022.   \n[22] Yiyun Luo, Will Wei Sun, and Yufeng Liu. Distribution-free contextual dynamic pricing. Mathematics of Operations Research, 49(1):599\u2013618, 2024.   \n[23] Shie Mannor and Ohad Shamir. From bandits to experts: On the value of side-observations. Advances in Neural Information Processing Systems, 24, 2011.   \n[24] Jieming Mao, Renato Leme, and Jon Schneider. Contextual pricing for lipschitz buyers. Advances in Neural Information Processing Systems, 31, 2018.   \n[25] Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The Annals of Statistics, 41(2):693\u2013721, 2013.   \n[26] Marvin Rothstein. Hotel overbooking as a markovian sequential decision process. Decision Sciences, 5(3):389\u2013404, 1974.   \n[27] Jon Schneider and Julian Zimmert. Optimal cross-learning for contextual bandits with unknown context distributions. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Virag Shah, Ramesh Johari, and Jose Blanchet. Semi-parametric dynamic contextual pricing. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[30] Chi-Hua Wang, Zhanyu Wang, Will Wei Sun, and Guang Cheng. Online regularization toward always-valid high-dimensional dynamic pricing. Journal of the American Statistical Association, pages 1\u201313, 2023.   \n[31] Jianyu Xu and Yu-Xiang Wang. Towards agnostic feature-based dynamic pricing: Linear policies vs linear valuation with unknown noise. In International Conference on Artificial Intelligence and Statistics, pages 9643\u20139662. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "iMEAHXDiNP/tmp/9768c2fb0979c0ffe48e801b7ce254c94c179034d047957cc9253d5bc6009a68.jpg", "img_caption": ["Figure 1: The plots here show the regrets rate of VAPE for linear evaluations, both in the standard and logarithmic scale (left and right respectively). The solid lines represent the average of the performance over 15 repetitions of the routine. The faded red area shows the standard error, while in the right subplot the dotted line corresponds to the theoretical regret bound. "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "iMEAHXDiNP/tmp/9d57b8d814a6fb02bb06a45f8b666e7665cd2a0574984d51334b0c507503f8ad.jpg", "img_caption": ["Figure 2: The two subplots show a comparison between VAPE and the algorithm in [14] in the stochastic and adversarial case, where the time horizons used are $T\\in[1000,1700,3000,5000]$ (left subplot), and $T\\in[1000,1400,4200,9000]$ (right subplot). In both cases the solid lines represent the average of the regret rates across the 15 repetitions of the simulations, while the faded area the standard error. In the subplot on the right, due to the specificity of the setting, the variance across runs is minimal, hence the faded area results invisible. The regret graph is in both cases plotted in logaritmic scale. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "In this section, we illustrate some numerical simulations that aim to show the empirical performance of our VAPE algorithm for Linear Valuations. Moreover we present a comparison with the algorithm proposed in [14] in the case in which the only regularity assumed on the CDF of the noise distribution is Lipschitzness. The code implemented for these simulations is publicly available in the repository: https://github.com/MatildeTulii1/ Improved-Algorithms-for-Contextual-Dynamic-Pricing ", "page_idx": 12}, {"type": "text", "text": "VAPE In order to test our algorithm, we built a dataset of 5 contexts belonging to $\\mathbb{R}^{3}$ generated by a canonical gaussian distribution and subsequently normalized. Throughout the run the contexts are chosen from this set uniformly at random, while the noise term is picked from a gaussian distribution truncated between $-1$ and 1 with mean 0 and variance 0.1. Similarly, also the parameter $\\theta$ is a normalized vector initially drawn from a gaussian distribution. The algorithm has been tested on time horizons T \u2208[1000, 10000, 50000, 200000, 500000, 800000], and the hyperparameters $\\alpha,\\mu,\\epsilon$ are set as in the statement of Theorem 1. Figure 1 shows the results of this implementation. The empirical regret rates of VAPE respect the theoretical upper-bound expressed in the paper, moreover it shows optimal computational times that can handle big time horizons. ", "page_idx": 12}, {"type": "text", "text": "Comparison with [14] Next we compare our algorithm with the algorithm proposed in Appendix F of [14], in which they propose a routine to tackle the dynamic pricing problem with linear valuation in the case in which the CDF $F$ is Lipschitz. The comparison is carried out in two different settings: a stochastic and an adversarial one, and to make it more fair both algorithm receive as input the time horizon $T$ . ", "page_idx": 13}, {"type": "text", "text": "In the stochastic case, similarly as before we consider a set of possible contexts in $\\mathbb{R}^{3}$ drawn uniformly at random and then normalised. During the routine, at each time step one of these is randomly selected. This method of receiving contexts meets the assumption included in [14], making sure that no eigenvalue of the covariance matrix of the distribution of contexts is too small. As for the contexts, the parameter $\\theta$ is selected ex-novo with every new run of the algorithm. We chose to implement a comparison with this specific algorithms since it was among the closest with our work, as discussed in the main paper, but its prohibitive computational costs make difficult to see the good behaviour of VAPE which, being based on a bandit approach, requires bigger time horizons to converge. ", "page_idx": 13}, {"type": "text", "text": "The adversarial case, instead is a toy example which is purposely designed to badly interfere with the algorithm proposed by [14]. In this case the set of contexts is made of only two samples of orthogonal vectors, specifically in the form $[x,0,z]$ and $[0,1,0]$ . To make sure that the effect of this choice of contexts is not invalidated by the parameter $\\theta$ , this is considered to be fixed as [0.3, 0.3, 0.3]. The algorithm receives the first context during the exploration phase and the second during the exploitation one, such that the information gathered initially result meaningless in the latter subroutine. As before the computational costs of [14] limited the time horizons on which we were able to run this simulation, still it can be noted how VAPE, exposed to the same contexts in the same order, does not suffer from such choice, since the phases are defined adaptively, thus its regret rates remain consistent with the stochastic case. The results of this comparison are shown in Figure 2. ", "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We state several lemmas before proving Theorem 1. We begin by bounding the length of the exploration phase corresponding to lines 5 and 6 of Algorithm 2. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. Let $\\mathcal{G}=\\{t\\leq T:\\iota_{t}=1\\}$ . Almost surely, the length of exploration phase $\\mathcal{G}$ is bounded as ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\mathcal{G}|\\leq\\frac{d\\log\\left(\\frac{T+d}{d}\\right)}{\\mu^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The following lemma bounds the error of our estimates for $\\theta$ and $D$ , for the values of $\\mu$ prescribed in Theorem 1. Before stating the Lemma, we define the event ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{\\forall t\\notin\\mathcal{G},|\\widehat{g}_{t}-g(x_{t})|\\leq\\epsilon,\\mathrm{and}\\left|\\widehat{D}_{t}^{k}-D(k\\epsilon)\\right|\\leq\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+L_{\\xi}\\epsilon\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 2. The event $\\mathcal{E}$ happens with probability at least $1-(\\alpha+2T^{2}|K|\\alpha)$ . ", "page_idx": 13}, {"type": "text", "text": "Finally, we bound the number of times a sub-optimal price increment $k\\epsilon$ can be selected. For $p\\in\\mathbb{R}$ , $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta(x,p)=\\operatorname*{sup}_{p^{\\prime}\\in[0,B_{y}]}\\pi(x,p^{\\prime})-\\pi(x,p).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 3. On the event $\\mathcal{E}$ , for all $t\\notin\\mathcal G$ , if $k_{t}=k_{i}$ , then $k$ must be such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta(x_{t},\\widehat{g}_{t}+k\\epsilon)\\le B_{y}\\left(4\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+9L_{\\xi}\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We are now ready to bound the regret of Algorithm VAPE for Linear Valuations. We begin by rewriting the regret as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}=\\displaystyle\\sum_{t=1}^{T}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{t\\in\\mathcal{G}}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)+\\sum_{t\\notin\\mathcal{G}}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Under Assumptions 1, 2, and 3, both the optimal price and $p_{t}$ are in $[0,B_{y}]$ , we know that the instantaneous regret is bounded by $B_{y}$ . Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{G}}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)\\leq B_{y}|\\mathcal{G}|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using Lemma 1 together with the definition of $\\mu$ , we find that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{G}}\\operatorname*{max}_{p\\in\\left[0,B_{y}\\right]}\\left(\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)\\leq\\frac{B_{y}d\\log\\left(\\frac{T+d}{d}\\right)\\left(B_{y}\\sqrt{d\\log\\left(\\frac{B_{x}T+1}{\\alpha}\\right)}+B_{\\theta}\\right)^{2}}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We rely on the following Lemma to bound $\\begin{array}{r}{\\sum_{t\\not\\in{\\mathcal G}}\\big(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\big).}\\end{array}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{C}}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)\\leq|K|\\left(512B_{y}\\log(1/\\alpha)+22\\frac{B_{y}\\log(1/\\alpha)}{L_{\\xi}\\epsilon}\\right)+36B_{y}T L_{\\xi}\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining Equations (4), (6), and Lemma 4, we find that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{l}_{T}\\leq\\frac{B_{y}d\\log\\left(\\frac{T+d}{d}\\right)\\left(B_{y}\\sqrt{d\\log\\left(\\frac{B_{x}T+1}{\\alpha}\\right)}+B_{\\theta}\\right)^{2}}{\\epsilon^{2}}+|K|\\left(512B_{y}\\log(1/\\alpha)+22\\frac{B_{y}\\log(1/\\alpha)}{L_{\\xi}\\epsilon}\\right)+B|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the definition of $\\kappa$ , $\\epsilon$ and $\\alpha$ allows us to conclude the proof. ", "page_idx": 14}, {"type": "text", "text": "C Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The proof of Theorem 2 follows closely the proof of Theorem 1. The following two Lemmas are analogues of Lemmas 1 and 2. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Let $\\overline{{\\mathcal{X}}}$ be an $\\big(\\frac{\\epsilon}{3L_{g}}\\big)^{1/\\beta}$ -covering of $B_{B_{x},d}$ of minimal cardinality, and let ${\\mathcal G}=\\bigcup_{\\overline{{x}}\\in\\overline{{x}}}{\\mathcal G}_{\\overline{{x}}}$ Almost surely, the length of exploration phase $\\mathcal{G}$ is bounded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\mathcal{G}|\\leq\\left(2B_{x}\\left(\\frac{3L_{g}}{\\epsilon}\\right)^{1/\\beta}+1\\right)^{d}(\\tau+1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that we defined the event $\\mathcal{E}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{\\forall t\\notin\\mathcal{G},|\\widehat{g}_{t}-g(x_{t})|\\leq\\epsilon,\\mathrm{and}\\left|\\widehat{D}_{t}^{k}-D(k\\epsilon)\\right|\\leq\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+L_{\\xi}\\epsilon\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following lemma shows that $\\mathcal{E}$ happens with large probability. ", "page_idx": 14}, {"type": "text", "text": "Lemma 6. The event $\\mathcal{E}$ happens with probability at least $1-(\\alpha+2T^{2}|K|\\alpha)$ . ", "page_idx": 14}, {"type": "text", "text": "The rest of the proof holds follows the proof of Theorem 1. In particular, on the event $\\mathcal{E}$ , we still have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}=\\displaystyle\\sum_{t\\in\\mathcal{G}}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)+\\displaystyle\\sum_{t\\notin\\mathcal{G}}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)}\\\\ &{\\quad\\le B_{y}|\\mathcal{G}|+|K|\\left(512B_{y}\\log(1/\\alpha)+22\\frac{B_{y}\\log(1/\\alpha)}{L_{\\xi}\\epsilon}\\right)+36B_{y}T L_{\\xi}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used the fact that the instantaneous regret is bounded by $B_{y}$ along with Lemma 4. Using Lemma 5, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{T}\\le B_{y}\\left(2B_{x}\\left(\\frac{3L_{g}}{\\epsilon}\\right)^{1/\\beta}+1\\right)^{d}(\\tau+1)+|K|\\left(512B_{y}\\log(1/\\alpha)+22\\frac{B_{y}\\log(1/\\alpha)}{L_{\\xi}\\epsilon}\\right)+36B_{y}T L\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the definition of $\\kappa,\\epsilon,\\tau$ and $\\alpha$ allows us to conclude the proof. ", "page_idx": 14}, {"type": "text", "text": "D Proof of Auxilliary Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the elliptical potential Lemma (see, e.g., Proposition 1 in [8]) to bound the total number of rounds used to estimate $\\theta$ . Formally, denote the estimation indices $\\vec{\\mathcal{G}}=\\left\\{t_{1}\\ldots,t_{|\\mathcal{G}|}\\right\\}$ and notice that $\\iota_{t}=1$ only for these indices. Thus, for all $i\\in[|\\mathcal{G}|]$ , we can write $\\begin{array}{r}{\\mathbf{V}_{t_{i}}=\\sum_{k=1}^{i}x_{t_{k}}x_{t_{k}}^{\\top}+\\mathbf{I}_{d}}\\end{array}$ and $\\mathbf{V}_{t_{i}-1}=\\mathbf{V}_{t_{i-1}}$ . In particular, the elliptical potential lemma implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{|\\mathcal{G}|}\\left\\|x_{t_{i}}\\right\\|_{\\mathbf{v}_{t_{i}-1}^{-1}}=\\sum_{i=1}^{|\\mathcal{G}|}\\left\\|x_{t_{i}}\\right\\|_{\\mathbf{v}_{t_{i-1}}^{-1}}\\leq\\sqrt{|\\mathcal{G}|d\\log\\left(\\frac{|\\mathcal{G}|+d}{d}\\right)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since for all $t$ such that $\\iota_{t}=1$ , $x_{t}^{\\top}\\mathbf{V}_{t_{i}-1}^{-1}x_{t}\\geq\\mu$ , this implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{G}|\\mu\\leq\\sqrt{|\\mathcal{G}|d\\log\\left(\\frac{|\\mathcal{G}|+d}{d}\\right)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, almost surely, $|\\mathcal{G}|\\leq T$ . Using this bound and reorganizing the inequality leads to the desired result ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{G}|\\leq\\frac{d\\log\\left(\\frac{T+d}{d}\\right)}{\\mu^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 2 is obtained by combining the following two results. ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. Let us define the event ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{E}}_{1}=\\{\\forall t\\notin{\\mathcal{G}}:|g(x_{t})-{\\widehat{g}}_{t}|\\leq\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, the event ${\\mathcal{E}}_{1}$ happens with probability at least $1-\\alpha$ ", "page_idx": 15}, {"type": "text", "text": "The remainder of the proof follows from the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 8. Let us define the event ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{\\forall t\\in[T],k\\in\\mathcal{K},\\left|\\widehat{D}_{t}^{k}-D(k\\epsilon)\\right|\\leq\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+L_{\\xi}\\epsilon\\right\\}\\cap\\mathcal{E}_{1}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assume that event ${\\mathcal{E}}_{1}$ holds with probability $1-\\alpha$ . Then, the event $\\mathcal{E}$ happens with probability at least $1-(\\alpha+2T^{2}|K|\\alpha)$ . ", "page_idx": 15}, {"type": "text", "text": "D.3 Proof of Lemma 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We assume that $t\\not\\in\\mathcal G$ , that $k_{t}=k$ , and that $N_{t}^{k}>0$ (otherwise the statement is trivial). We begin by stating an auxiliary result, which follows immediately from Lemma 2. ", "page_idx": 15}, {"type": "text", "text": "Lemma 9. On the event $\\mathcal{E}$ , we have that for all $t\\not\\in\\mathcal G$ , and all $k\\in\\mathcal{A}_{t}$ ; ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{LCB}_{t}(k)\\leq\\pi(x_{t},{\\widehat{g}}_{t}+k\\epsilon)\\leq\\operatorname{UCB}_{t}(k).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, $k_{t}^{*}\\in\\mathcal{K}_{t}$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\nk_{t}^{*}\\in\\arg\\operatorname*{max}_{k\\in\\mathcal{A}_{t}}\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the event $\\mathcal{E}$ , Lemma 9 implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon)\\geq\\mathrm{LCB}(k)}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{UCB}(k)-(\\mathrm{UCB}(k)-\\mathrm{LCB}(k)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $k_{t}^{*}\\in\\mathcal{A}_{t}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{UCB}_{t}(k)\\geq\\operatorname{LCB}_{t}(k_{t}^{*}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon)\\geq\\mathrm{LCB}_{t}(k_{t}^{*})-\\left(\\mathrm{UCB}_{t}(k)-\\mathrm{LCB}_{t}(k)\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{UCB}_{t}(k_{t}^{*})-\\left(\\mathrm{UCB}_{t}(k)-\\mathrm{LCB}_{t}(k)\\right)-\\left(\\mathrm{UCB}_{t}(k_{t}^{*})-\\mathrm{LCB}_{t}(k_{t}^{*})\\right)}\\\\ &{\\qquad\\qquad\\geq\\pi(x_{t},\\widehat{g}_{t}+k_{t}^{*}\\epsilon)-\\left(\\mathrm{UCB}_{t}(k)-\\mathrm{LCB}_{t}(k)\\right)-\\left(\\mathrm{UCB}_{t}(k_{t}^{*})-\\mathrm{LCB}_{t}(k_{t}^{*})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi(x_{t},\\widehat{g}_{t}+k_{t}^{*}\\epsilon)-\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\operatorname{UCB}_{t}(k)-\\operatorname{LCB}_{t}(k)\\right)+\\left(\\operatorname{UCB}_{t}(k_{t}^{*})-\\operatorname{LCB}_{t}(k_{t}^{*})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{UCB}_{t}(k)-\\mathrm{LCB}_{t}(k)=\\left(\\widehat{g}_{t}+k\\epsilon\\right)\\left(\\sqrt{\\frac{8\\log(1/\\alpha)}{N_{t}^{k}}}+4L_{\\xi}\\epsilon\\right)}\\\\ &{\\ }&{\\mathrm{UCB}_{t}\\left(\\sqrt{\\frac{8\\log(1/\\alpha)}{N_{t}^{k}}}+4L_{\\xi}\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $k\\in\\mathcal A_{t}$ . Moreover, since $k_{t}=k$ , and since $k_{t}^{*}\\in\\mathcal{K}_{t}$ by Lemma 9, we know that $N_{t}^{k}\\le N_{t}^{k^{*}}$ . This implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{UCB}_{t}(k_{t}^{*})-\\operatorname{LCB}_{t}(k_{t}^{*})=(\\widehat{g}_{t}+k_{t}^{*}\\epsilon)\\left(\\sqrt{\\frac{8\\log(1/\\alpha)}{N_{t}^{k_{t}^{*}}}}+4L_{\\xi}\\epsilon\\right)}\\\\ {\\leq B_{y}\\left(\\sqrt{\\frac{8\\log(1/\\alpha)}{N_{t}^{k}}}+4L_{\\xi}\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi(x_{t},\\widehat{g}_{t}+k_{t}^{*}\\epsilon)-\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon)\\leq2B_{y}\\left(\\sqrt{\\frac{8\\log(1/\\alpha)}{N_{t}^{k}}}+4L_{\\xi}\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we bound the discretization error using the following Lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma 10. On the event $\\mathcal{E}$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{sup}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},\\widehat{g}_{t}+k_{t}^{*}\\epsilon)\\right|\\leq B_{y}L_{\\xi}\\epsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Lemma 10, Equation (7) implies that on the event $\\mathcal{E}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta(x_{t},\\widehat{g}_{t}+k\\epsilon)\\le B_{y}\\left(4\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+9L_{\\xi}\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D.4 Proof of Lemma 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t\\notin\\mathcal{G}}\\left(\\operatorname*{max}_{p\\in[0,B_{y}]}\\pi(x_{t},p)-\\pi(x_{t},p_{t})\\right)=\\sum_{k\\in\\mathcal{K}}\\sum_{t\\notin\\mathcal{G}:k_{t}=k}\\Delta(x_{t},\\widehat{g}_{t}+k\\epsilon)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We bound this term on the high-probability event $\\mathcal{E}$ . For $k\\,\\in\\,\\kappa$ , we define $t_{1}^{k}\\,<\\,\\cdots\\,<\\,t_{N_{T+1}^{k}}^{k}$ the rounds where $t\\not\\in\\mathcal G$ and $k_{t}\\,=\\,k$ . We split these rounds into episodes as follows. We define $\\overline{{a}}=\\lfloor-\\log_{2}\\left(18L_{\\xi}\\epsilon\\right)\\rfloor$ . For $a\\in[1,\\overline{{a}}]$ , we also define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{t}_{a}=\\frac{128\\log(1/\\alpha)}{2^{-2a}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With these notations, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{\\notin\\mathcal{G}:k_{t}=k}\\Delta(x_{t},\\widehat{g}_{t}+k\\epsilon)=\\sum_{\\substack{i\\leq\\mathfrak{t}_{1}\\wedge N_{T+1}^{k}}}\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)+\\sum_{\\substack{a=\\mathfrak{t}_{\\mathfrak{t}_{a}\\wedge N_{T+1}^{k}<i\\leq\\mathfrak{t}_{a+1}\\wedge N_{T+1}^{k}}}}^{\\overline{{a}}-1}\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)}\\\\ {+\\sum_{\\substack{\\mathfrak{t}_{\\pi\\wedge N_{T+1}^{k}<i\\leq N_{T+1}^{k}}}}\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the one hand, $\\Delta(x_{t},p_{t})\\le B_{y}$ for all $t\\leq T$ , so ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i\\leq\\mathfrak{t}_{1}\\wedge N_{T+1}^{k}}\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)\\leq B_{y}\\mathfrak{t}_{1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, using Lemma 3, we see that on the event $\\mathcal{E}$ , if $i\\geq\\mathfrak{t}_{a}$ and $a\\in[1,\\overline{{a}}]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)\\le B_{y}\\left(4\\sqrt{\\frac{2\\log(1/\\alpha)}{\\tan}}+9L_{\\xi}\\epsilon\\right)}}\\\\ &{}&{\\le B_{y}\\left(\\frac{2^{-a}}{2}+9L_{\\xi}\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $2^{-a}\\geq18L_{\\xi}\\epsilon$ , this implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)\\leq2^{-a}B_{y}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{a=1}\\sum_{\\mathbf{t}_{a}\\wedge N_{T+1}^{k}<i\\leq\\mathbf{t}_{a+1}\\wedge N_{T+1}^{k}}\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)\\leq B_{y}\\displaystyle\\sum_{a=1}^{\\overline{{a}}-1}\\big(\\mathbf{t}_{a+1}-\\lceil\\mathbf{t}_{a}\\rceil+1\\big)^{2^{-a}}}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\overline{{a}}-1}\\\\ &{}&{\\leq B_{y}\\displaystyle\\sum_{a=1}^{\\overline{{a}}-1}\\big(\\mathbf{t}_{a+1}-\\mathbf{t}_{a}\\big)\\,2^{-a}+B_{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By definition of $\\mathrm{t}_{a}$ , this implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{a=1}^{a-1}\\sum_{\\mathbf{t}_{a}\\wedge N_{T+1}^{k}<i\\leq\\mathbf{t}_{a+1}\\wedge N_{T+1}^{k}}\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)\\leq128B_{y}\\log(1/\\alpha)\\overset{\\overline{{a}}-1}{\\underset{a=1}{\\sum}}\\big(2^{2a+2}-2^{2a}\\big)\\,2^{-a}+B_{y}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used that $\\begin{array}{r}{2^{\\overline{{a}}}\\leq\\frac{1}{18L_{\\xi}\\epsilon}}\\end{array}$ . Similarly, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\displaystyle\\sum_{\\ell=\\wedge N_{T+1}^{k}<i\\leq N_{T+1}^{k}}\\Delta(x_{t_{i}^{k}},\\widehat{g}_{t_{i}^{k}}+k\\epsilon)\\leq2^{-\\overline{{a}}}B_{y}N_{T+1}^{k}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq36B_{y}N_{T+1}^{k}L_{\\xi}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining these results, we find that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t\\notin\\mathcal{G}:k_{t}=k}\\Delta(x_{t},\\widehat{g}_{t}+k\\epsilon)\\leq512B_{y}\\log(1/\\alpha)+22\\frac{B_{y}\\log(1/\\alpha)}{L_{\\xi}\\epsilon}+36B_{y}N_{T+1}^{k}L_{\\xi}\\epsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We conclude the proof by summing over $k\\in\\mathcal{K}$ , and using the fact that $\\begin{array}{r}{\\sum_{k\\in\\mathcal{K}}N_{T+1}^{k}\\le T}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "D.5 Proof of Lemma 5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\mathcal{G}|\\leq|\\overline{{\\mathcal{X}}}|(\\tau+1).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We conclude by using classical results on covering number of the ball (see, e.g., Corollary 4.2.13 in [29]), stating that there exists an $\\left(\\frac{\\epsilon}{3L_{g}}\\right)^{1/\\beta}$ -covering of the ball of radius $B_{x}$ in dimension $d$ of cardinality at most $\\left(2B_{x}\\left(\\frac{3L_{g}}{\\epsilon}\\right)^{1/\\beta}+1\\right)^{d}$ ", "page_idx": 18}, {"type": "text", "text": "D.6 Proof of Lemma 6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The proof of Lemma 6 relies on the following Lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 11. Let us define the event ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal{E}}_{1}=\\{\\forall t\\notin{\\mathcal{G}}:|g(x_{t})-{\\widehat{g}}({\\overline{{x}}}_{t})|\\leq\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, the event ${\\mathcal{E}}_{1}$ happens with probability at least $1-\\alpha$ . ", "page_idx": 18}, {"type": "text", "text": "Note that Lemma 8 still holds for non-parametric valuations. This concludes the proof of Lemma 6. ", "page_idx": 18}, {"type": "text", "text": "D.7 Proof of Lemma 7 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We introduce the variables ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{x}_{t}=\\iota_{t}x_{t}\\quad\\mathrm{and}\\quad\\tilde{y}_{t}=2B_{y}\\iota_{t}\\left(o_{t}-\\frac{1}{2}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the $\\sigma$ -algebra $\\mathcal{F}_{t}=\\sigma\\left((x_{s})_{s\\leq t+1},(o_{s})_{s\\leq t}\\right)$ . Since $\\mathbf{V}_{t-1}$ and $x_{t}$ are $\\mathcal{F}_{t-1}$ -measurable, then so does $\\iota_{t}$ , and thus both $\\tilde{x}_{t+1}$ and $\\tilde{y}_{t}$ are $\\mathcal{F}_{t}$ -measurable. Moreover, for any round where $\\iota_{t}=1$ , the price is chosen uniformly at random and we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\bar{y}_{t}|\\mathcal{F}_{t-1}\\right]=\\iota_{t}\\times\\left(2B_{y}\\int_{-B_{y}}^{B_{y}}\\mathbb{P}\\left[u\\leq y_{t}|\\mathcal{F}_{t-1}\\right]\\frac{\\mathrm{d}u}{2B_{y}}-B_{y}\\right)}\\\\ &{\\qquad\\qquad=\\iota_{t}\\times\\left(\\int_{-B_{y}}^{B_{y}}\\int_{-B_{t}}^{B_{t}}\\mathbb{I}\\left\\{u\\leq x_{t}^{\\top}\\theta+\\xi\\right\\}f(\\xi)\\,\\mathrm{d}\\xi\\,\\mathrm{d}u-B_{y}\\right)}\\\\ &{\\qquad\\qquad=\\iota_{t}\\times\\left(\\int_{-B_{\\xi}}^{B_{\\xi}}\\int_{-B_{y}}^{\\xi+x_{t}^{\\top}\\theta}\\mathrm{d}u f(\\xi)\\,\\mathrm{d}\\xi-B_{y}\\right)}\\\\ &{\\qquad\\qquad=\\iota_{t}\\times\\left(x_{t}^{\\top}\\theta+\\int_{-B_{\\xi}}^{B_{\\xi}}\\xi f(\\xi)\\,\\mathrm{d}\\xi\\right)}\\\\ &{\\qquad\\qquad=\\iota_{t}\\times x_{t}^{\\top}\\theta}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the last equality we used that $\\begin{array}{r}{\\int_{-B_{\\xi}}^{B_{\\xi}}\\xi f(\\xi)\\,\\mathrm{d}\\xi=\\mathbb{E}\\left[\\xi_{t}\\right]=0}\\end{array}$ . The same relation also trivially holds when $\\iota_{t}=0$ . Thus, conditionally on $\\mathcal{F}_{t-1}$ , $\\tilde{y}_{t}-\\tilde{x}_{t}^{\\top}\\theta$ is centered and in $[-B_{y},B_{y}]$ , which implies that it is $B_{y}$ -subgaussian. Now, for all $t\\leq T$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat{\\theta}_{t}=2B_{y}\\left(\\sum_{s<t}\\iota_{s}x_{s}x_{s}^{\\top}+\\mathbf{I}_{d}\\right)^{-1}\\sum_{s\\in\\mathcal{G}}\\left(o_{s}-\\frac{1}{2}\\right)x_{s}}\\\\ {\\displaystyle\\qquad=\\left(\\sum_{s<t}\\tilde{x}_{s}\\tilde{x}_{s}^{\\top}+\\mathbf{I}_{d}\\right)^{-1}\\sum_{s<t}\\tilde{y}_{s}\\tilde{x}_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the fact that for all $t\\geq1$ , $\\|\\tilde{x}_{t}\\|\\leq B_{x}$ , and that $\\lVert\\theta\\rVert\\leq B_{\\theta}$ , and applying Theorem 2 in [1], we find that for all $t\\geq0$ , with probability $1-\\alpha$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\widehat{\\theta}_{t}-\\theta\\|_{(\\sum_{s<t}\\tilde{x}_{l}\\tilde{x}_{l}^{\\top}+\\mathbf I_{d})}\\leq B_{y}\\sqrt{d\\log\\left(\\frac{1+B_{x}^{2}T}{\\alpha}\\right)}+B_{\\theta}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that our definitions of $\\tilde{x}_{t}$ and $\\tilde{y}_{t}$ ensure that $\\begin{array}{r}{\\|\\widehat{\\theta}_{t}-\\theta\\|_{(\\sum_{s\\leq t}\\widetilde{x}_{l}\\widetilde{x}_{l}^{\\top}+\\mathbf{I}_{d})}=\\|\\widehat{\\theta}_{t}-\\theta\\|_{\\mathbf{V}_{t}}.}\\end{array}$ . Moreover, for all $t$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n|x_{t}^{\\top}(\\widehat{\\theta}_{t}-\\theta)|\\leq\\|x_{t}^{\\top}\\|_{\\mathbf{V}_{t}^{-1}}\\|\\widehat{\\theta}_{t}-\\theta\\|_{\\mathbf{V}_{t}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, if $t\\notin\\mathcal{G},\\|x_{t}^{\\top}\\|_{(\\mathbf{v}_{t})^{-1}}\\leq\\mu$ , so ", "page_idx": 19}, {"type": "equation", "text": "$$\n|x_{t}^{\\top}(\\widehat{\\theta}_{t}-\\theta)|\\leq\\mu\\left(B_{y}\\sqrt{d\\log\\left(\\frac{1+B_{x}^{2}T}{\\alpha}\\right)}+B_{\\theta}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The conclusion follows from the choice $\\begin{array}{r}{\\epsilon\\,=\\,\\mu\\left(B_{y}\\sqrt{d\\log\\left(\\frac{1+B_{x}^{2}T}{\\alpha}\\right)}+B_{\\theta}\\right)}\\end{array}$ , and the fact that $\\widehat{g}_{t}=x_{t}^{\\top}\\widehat{\\theta}_{t}$ . ", "page_idx": 19}, {"type": "text", "text": "D.8 Proof of Lemma 8 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We rely on the following well-known result (we provide proof in the appendix for the sake of completeness). ", "page_idx": 19}, {"type": "text", "text": "Lemma 12. Let $(y_{t})_{t\\geq1}$ be a sequence of random variables adapted for a filtration ${\\mathcal{F}}_{t:}$ , such that $y_{t}-\\mathbb{E}\\left[y_{t}|\\mathcal{F}_{t-1}\\right]\\in\\left[m,M\\right]$ . Assume that for $t\\in\\mathbb{N}_{*}$ , $\\iota_{t}\\,\\in\\,\\{0,1\\}$ is $\\mathcal{F}_{t-1}$ -measurable, and define $\\begin{array}{r}{N_{t}=\\sum_{s\\leq t}{\\iota_{s}}}\\end{array}$ , and $\\begin{array}{r}{\\widehat{\\mu}_{t}=\\frac{\\sum_{s\\leq t}\\iota_{s}(y_{s}-\\mathbb{E}[y_{s}|\\mathcal{F}_{s-1}])}{N_{t}}\\,i f\\,N_{t}\\geq}\\end{array}$ . Then, for any $t\\in\\mathbb{N}_{*}$ and $\\alpha\\in(0,1)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(N_{t}=0\\,o r\\,|\\widehat{\\mu}_{t}|\\leq(M-m)\\sqrt{\\frac{\\log(1/\\alpha)}{2N_{t}}}\\right)\\geq1-2t\\alpha.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, for any $l>0$ and $\\alpha\\in(0,1)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(N_{t}=l\\,a n d\\,|\\widehat{\\mu}_{t}|\\geq(M-m)\\sqrt{\\frac{\\log(1/\\alpha)}{2N_{t}}}\\right)\\leq2\\alpha.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note Lemma 8 holds trivially for all $t$ such that $N_{t}^{k}=0$ . Therefore we assume w.l.o.g. that $N_{t}^{k}\\geq1$ (otherwise the statement is trivial). For any such given $t\\in[T]$ , we control the error $|\\widehat{F}_{t}^{k}-F(k\\epsilon)|$ uniformly for $k\\in\\mathcal{K}$ . To do so, we rely on Lemma 12; we define $\\tilde{\\boldsymbol{\\iota}}_{t}=\\mathbb{1}\\left\\{\\boldsymbol{\\iota}_{t}=0\\right.$ and $k_{t}=k\\}$ , and note that for $\\mathcal{F}_{t}\\,=\\,\\sigma\\,\\big(\\big(x_{1},\\dots,x_{t+1}\\big),\\big(o_{1},\\dots,o_{t}\\big)\\big),$ $\\tilde{\\iota}_{t}$ is $\\mathcal{F}_{t-1}$ -measurable, and $o_{t}$ is $\\mathcal{F}_{t}$ adapted. Moreover, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\iota}_{t}\\mathbb{E}\\left[o_{t}|\\mathcal{F}_{t-1}\\right]=\\tilde{\\iota}_{t}\\mathbb{P}\\left(g(x_{t})+\\xi_{t}\\geq\\widehat{g}_{t}+k\\epsilon\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\tilde{\\iota}_{t}D\\left(\\widehat{g}_{t}-g(x_{t})+k\\epsilon\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and directly by definition, it holds that $\\begin{array}{r}{\\widehat{D}_{t}^{k}\\;=\\;\\frac{\\sum_{s\\le t}\\tilde{\\iota}_{t}o_{t}}{N_{t}}}\\end{array}$ . Using Lemma 12, we find that with probability $1-2\\alpha t$ , $N_{t}^{k}=0$ or ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bigg|\\widehat{D}_{t}^{k}-\\frac{\\sum_{s\\leq t}\\tilde{\\iota}_{t}D\\left(\\widehat{g}_{t}-g(x_{t})+k\\epsilon\\right)}{N_{t}^{k}}\\bigg|\\leq\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, on the event ${\\mathcal{E}}_{1}$ , which happens w.p. at least $1-\\alpha$ , for all $t\\notin\\mathcal{G},|\\widehat{g}_{t}-g(x_{t})|\\leq\\epsilon$ . Using the fact that $D$ is $L_{\\xi}$ -Lipschitz, we find that for all $t\\not\\in\\mathcal G$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n|D\\left(\\widehat{g}_{t}-g(x_{t})+k\\epsilon\\right)-D\\left(k\\epsilon\\right)|\\leq L_{\\xi}|\\widehat{g}_{t}-g(x_{t})|\\leq L_{\\xi}\\epsilon.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, with probability $1-2\\alpha t$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\widehat{D}_{t}^{k}-D(k\\epsilon)\\right|\\leq\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+L_{\\xi}\\epsilon.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using a union bound over all $k\\in\\mathcal{K}$ and $t\\in[T]$ and then intersecting with ${\\mathcal{E}}_{1}$ using another union bound yields the desired result. ", "page_idx": 19}, {"type": "text", "text": "D.9 Proof of Lemma 9 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For any $t\\not\\in\\mathcal G$ , denoting $p_{t}(k)=\\widehat{g}_{t}+k\\epsilon$ , we first rewrite ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi(x_{t},p_{t}(k))=\\mathbb{E}[p_{t}(k)\\mathbb{1}\\{p_{t}(k)\\leq y_{t}\\}|p_{t}(k),x_{t}]}\\\\ &{\\qquad\\qquad=p_{t}(k)\\mathbb{E}[\\mathbb{1}\\{p_{t}(k)\\leq g(x_{t})+\\xi_{t}\\}|p_{t}(k),x_{t}]}\\\\ &{\\qquad\\qquad=p_{t}(k)D(p_{t}(k)-g(x_{t}))}\\\\ &{\\qquad\\quad=(\\widehat{g}_{t}+k\\epsilon)D(\\widehat{g}_{t}-g(x_{t})+k\\epsilon)}\\\\ &{\\qquad\\quad=(\\widehat{g}_{t}+k\\epsilon)\\widehat{D}_{t}^{k}+(\\widehat{g}_{t}+k\\epsilon)\\Big(D(\\widehat{g}_{t}-g(x_{t})+k\\epsilon)-\\widehat{D}_{t}^{k}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the event $\\mathcal{E}$ holds, the following hold for all $t\\not\\in\\mathcal G$ and $k\\in\\mathcal A_{t}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{g}_{t}-g(x_{t})|\\leq\\epsilon,\\qquad\\mathrm{and}\\qquad\\left|\\widehat{D}_{t}^{k}-D(k\\epsilon)\\right|\\leq\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+L_{\\xi}\\epsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, we have that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|D(\\widehat{g}_{t}-g(x_{t})+k\\epsilon)-\\widehat{D}_{t}^{k}\\right|\\leq\\displaystyle\\left|D(\\widehat{g}_{t}-g(x_{t})+k\\epsilon)-D(k\\epsilon)\\right|+\\Big|D(k\\epsilon)-\\widehat{D}_{t}^{k}\\Big|}\\\\ &{\\overset{(1)}{\\leq}L_{\\xi}|\\widehat{g}_{t}-g(x_{t})|+\\displaystyle\\left|D(k\\epsilon)-\\widehat{D}_{t}^{k}\\right|}\\\\ &{\\leq L_{\\xi}\\epsilon+\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+L_{\\xi}\\epsilon}\\\\ &{=\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+2L_{\\xi}\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Relation (1) holds since $D$ is $L_{\\xi}$ -Lipschitz and (2) is under the event $\\mathcal{E}$ for all $t\\notin\\mathcal{E}$ . As the set $\\boldsymbol{A}_{t}$ is chosen such that $\\widehat{g}_{t}+k\\epsilon\\geq0$ for all $k\\in\\mathcal A_{t}$ , it implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon)-(\\widehat{g}_{t}+k\\epsilon)\\widehat{D}_{t}^{k}\\right|\\leq(\\widehat{g}_{t}+k\\epsilon)\\left(\\sqrt{\\frac{2\\log(1/\\alpha)}{N_{t}^{k}}}+2L_{\\xi}\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Reorganizing, we get for all $k\\in\\mathcal{A}_{t}$ and $t\\not\\in\\mathcal G$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{LCB}_{t}(k)\\leq\\pi(x_{t},{\\widehat{g}}_{t}+k\\epsilon)\\leq\\operatorname{UCB}_{t}(k).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which proves the first part of the statement. ", "page_idx": 20}, {"type": "text", "text": "Now let $k_{t}^{*}\\in\\arg\\operatorname*{max}_{k\\in\\mathcal{A}_{t}}\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon)$ . By the first part of the claim, it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{UCB}_{t}(k_{t}^{*})\\stackrel{(*)}{\\geq}\\pi(x_{t},\\widehat{g}_{t}+k_{t}^{*}\\epsilon)=\\operatorname*{max}_{k\\in\\mathcal{A}_{t}}\\pi(x_{t},\\widehat{g}_{t}+k\\epsilon)\\stackrel{(*)}{\\geq}\\operatorname*{max}_{k\\in\\mathcal{A}_{t}}\\operatorname{LCB}_{t}(k),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where relations $(*)$ are due to the first part of the lemma; this proves that $k_{t}^{*}\\in\\mathcal{K}_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "D.10 Proof of Lemma 10 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The proof follows by noticing that, on the one hand, $\\kappa$ ensures that for all $p\\,\\in\\,[0,B_{y}]$ , there exists $k\\,\\in\\,\\kappa$ such that $\\widehat{g}_{t}+k\\epsilon\\,\\in\\,[0,B_{y}]$ and $|\\widehat{g}_{t}+k\\epsilon-p|\\,\\leq\\,\\epsilon$ . On the other hand, the prices considered are bounded by $B_{y}$ , and the demand function $D$ is $L_{\\xi}$ -Lipschitz, so the reward function $\\pi$ is $B_{y}L_{\\xi}$ -Lipschitz. ", "page_idx": 20}, {"type": "text", "text": "D.11 Proof of Lemma 11 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For $\\overline{{x}}\\in\\mathbf{\\Sigma}\\mathcal{X}$ , let us define recursively the variables $\\iota_{1}^{\\overline{{x}}}\\ =\\ 1\\ \\{\\overline{{x}}_{1}=\\overline{{x}}\\}$ , and for $t\\ >\\ 1,\\ \\iota_{t}^{\\overline{{x}}}\\ =$ 1 $\\left\\{{\\overline{{x}}}_{t}={\\overline{{x}}}$ , and $\\textstyle\\sum_{s<t}l_{s}^{\\overline{{x}}}<\\tau\\}$ , and define the variables ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{g}_{t}^{\\overline{{x}}}=\\iota_{t}^{\\overline{{x}}}g(x_{t})\\quad\\mathrm{and}\\quad\\tilde{y}_{t}^{\\overline{{x}}}=2B_{y}\\iota_{t}^{\\overline{{x}}}\\left(o_{t}-\\frac{1}{2}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the $\\sigma$ -algebra $\\mathcal{F}_{t}\\,=\\,\\sigma\\,((\\boldsymbol{x}_{s})_{s\\leq t+1},(o_{s})_{s\\leq t})$ . Note that $\\iota_{t}^{\\overline{{x}}}$ is $\\mathcal{F}_{t-1}$ -measurable, and thus both $\\tilde{x}_{t+1}$ and $\\tilde{y}_{t}$ are $\\mathcal{F}_{t}$ -measurable. Moreover, for any round where $\\iota_{t}^{\\overline{{x}}}=1$ , the price is chosen uniformly at random and we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\tilde{y}_{t}^{\\overline{{\\tau}}}|\\mathcal{F}_{t-1}\\right]=\\iota_{t}^{\\overline{{\\tau}}}\\times\\left(2B_{y}\\int_{-B_{y}}^{B_{y}}\\mathbb{P}\\left[u\\leq y_{t}|\\mathcal{F}_{t-1}\\right]\\frac{\\mathrm{d}u}{2B_{y}}-B_{y}\\right)}\\\\ &{\\qquad\\qquad=\\iota_{t}^{\\overline{{\\tau}}}\\times\\left(\\int_{-B_{y}}^{B_{y}}\\int_{-B_{z}}^{B_{t}}\\mathbb{I}\\left\\{u\\leq g(x_{t})+\\xi\\right\\}f(\\xi)\\,\\mathrm{d}\\xi\\,\\mathrm{d}u-B_{y}\\right)}\\\\ &{\\qquad\\qquad=\\iota_{t}^{\\overline{{\\tau}}}\\times\\left(\\int_{-B_{\\xi}}^{B_{\\xi}}\\int_{-B_{y}}^{\\xi+g(x_{t})}\\mathrm{d}u f(\\xi)\\,\\mathrm{d}\\xi-B_{y}\\right)}\\\\ &{\\qquad\\qquad=\\iota_{t}^{\\overline{{\\tau}}}\\times\\left(g(x_{t})+\\int_{-B_{\\xi}}^{B_{\\xi}}\\xi f(\\xi)\\,\\mathrm{d}\\xi\\right)}\\\\ &{\\qquad\\qquad=\\widehat{\\iota_{t}^{\\overline{{\\tau}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in the last equality we used that $\\begin{array}{r}{\\int_{-B_{\\xi}}^{B_{\\xi}}\\xi f(\\xi)\\,\\mathrm{d}\\xi=\\mathbb{E}\\left[\\xi_{t}\\right]=0}\\end{array}$ . The same relation also trivially holds when $\\iota_{t}^{\\overline{{x}}}=0$ . Thus, conditionally on $\\mathcal{F}_{t-1},\\tilde{y}_{t}-\\tilde{g}_{t}^{\\overline{{x}}}$ is centered and in $[-B_{y},B_{y}]$ . We denote $\\begin{array}{r}{N_{t}^{\\overline{{x}}}=\\sum_{s<t}\\iota_{s}^{\\overline{{x}}}}\\end{array}$ , we note that if $t\\notin\\mathcal G^{\\overline{{x}}}$ , then $N_{t}^{\\overline{{x}}}=\\lceil\\tau\\rceil$ a.s. Using Lemma 12, we find that for all $t\\notin\\mathcal G^{\\overline{{x}}}$ , a.s., $N_{t}^{\\overline{{x}}}=\\lceil\\tau\\rceil$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\exists t\\notin\\mathcal{G}^{\\overline{{x}}}:\\left|\\frac{\\sum_{s\\in\\mathcal{G}^{\\overline{{x}}},s\\in t}\\widetilde{y}_{t}^{\\overline{{x}}}-\\widetilde{g}_{t}^{\\overline{{x}}}}{N_{t}^{\\overline{{x}}}}\\right|\\ge2B_{y}\\sqrt{\\frac{\\log(2|\\overline{{\\mathcal{X}}}|/\\alpha)}{2\\lceil\\tau\\rceil}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\le\\mathbb{P}\\left(N_{t}^{\\overline{{x}}}=\\lceil\\tau\\rceil\\mathrm{~and~}\\left|\\frac{\\sum_{s\\in\\mathcal{G}^{\\overline{{x}}},s<t}\\widetilde{y}_{t}^{\\overline{{x}}}-\\widetilde{g}_{t}^{\\overline{{x}}}}{N_{t}^{\\overline{{x}}}}\\right|\\ge2B_{y}\\sqrt{\\frac{\\log(2|\\overline{{\\mathcal{X}}}|/\\alpha)}{2\\lceil\\tau\\rceil}}\\right)}\\\\ &{\\qquad\\qquad\\le\\frac{\\alpha}{|\\overline{{\\mathcal{X}}}|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, since $g$ is $(L_{g},\\beta)$ -Holder- continuous, and $\\begin{array}{r}{\\|\\overline{{x}}_{t}-x_{t}\\|\\leq\\big(\\frac{\\epsilon}{3L_{g}}\\big)^{1/\\beta}}\\end{array}$ a.s., we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n|g(\\overline{{{x}}}_{t})-\\tilde{g}_{t}^{\\overline{{{x}}}}|\\leq L_{g}\\cdot\\left[\\left(\\frac{\\epsilon}{3L_{g}}\\right)^{1/\\beta}\\right]^{\\beta}=\\frac{\\epsilon}{3}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, with probability at least $1-\\alpha/|{\\overline{{x}}}|$ , for all $t\\notin\\mathcal G^{\\overline{{x}}}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\widehat{g}(\\overline{{x}}_{t})-g(\\overline{{x}}_{t})|\\leq2B_{y}\\sqrt{\\frac{\\log(2|\\overline{{\\mathcal{X}}}|/\\alpha)}{2\\lceil\\tau\\rceil}}+\\frac{\\epsilon}{3}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2\\epsilon}{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used 18By2 lo\u03f5g2(|X|/\u03b1). Using a union bound over X, we find that with probability at least $1-\\alpha$ , for all $t\\notin\\mathcal G^{\\overline{{x}}}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\widehat{g}(\\overline{{{x}}}_{t})-g(\\overline{{{x}}}_{t})|\\leq\\frac{2\\epsilon}{3}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, for all $\\begin{array}{r}{t\\not\\in\\mathcal{G},\\|g(x_{t})-g(\\overline{{x}}_{t})\\|\\leq L_{g}\\frac{\\epsilon}{3L_{g}}}\\end{array}$ . Then, we have that with probability $1-\\alpha$ , for all $t\\notin\\mathcal G^{\\overline{{x}}}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\widehat{g}(\\overline{{x}}_{t})-g(x_{t})|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.12 Proof of Lemma 12 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let us define $\\begin{array}{r}{Z_{t}=\\sum_{s\\leq t}\\iota_{s}(y_{s}-\\mathbb{E}\\left[y_{s}|\\mathcal{F}_{s-1}\\right])}\\end{array}$ , and for $x\\in\\mathbb R$ , $\\begin{array}{r}{M_{t}=\\exp\\left(x Z_{t}-\\frac{x^{2}(M-m)^{2}N_{t}}{8}\\right)}\\end{array}$ We begin by showing that $M_{t}$ is a super-martingale. Indeed, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[e^{x\\iota_{t}\\left(y_{t}-\\mathbb{E}\\left[y_{t}\\vert\\mathcal{F}_{t-1}\\right]\\right)}\\Big\\vert\\mathcal{F}_{t-1}\\right]=\\mathbb{E}\\left[\\iota_{t}e^{x(y_{t}-\\mathbb{E}[y_{t}\\vert\\mathcal{F}_{t-1}])}+(1-\\iota_{t})\\Big\\vert\\mathcal{F}_{t-1}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\iota_{t}e^{\\frac{x^{2}(M-m)^{2}}{8}}+(1-\\iota_{t})}\\\\ &{\\qquad\\qquad\\leq e^{\\frac{x^{2}(M-m)^{2}\\iota_{t}}{8}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use the fact that $\\left(y_{t}-\\mathbb{E}\\left[y_{t}|\\mathcal{F}_{t-1}\\right]\\right)$ is bounded in $[m,M]$ together with the conditional version of Hoeffding\u2019s Lemma. Noticing that ", "page_idx": 22}, {"type": "equation", "text": "$$\nM_{t}=M_{t-1}e^{x\\iota_{t}\\left(y_{t}-\\mathbb{E}\\left[y_{t}\\right|\\mathcal{F}_{t-1}\\right]\\right)-\\frac{x^{2}(M-m)^{2}\\iota_{t}}{8}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "this proves that $M_{t}$ is a super-martingale, and so $\\mathbb{E}\\left[M_{t}\\right]\\leq\\mathbb{E}\\left[M_{0}\\right]=1.$ . ", "page_idx": 22}, {"type": "text", "text": "Now, for all $\\epsilon>0$ and all $l\\in\\mathbb N$ , and all $x>0$ , by a Markov-Chernoff argument, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(Z_{t}\\geq\\epsilon\\mathrm{~and~}N_{t}=l\\right)=\\mathbb{P}\\left(\\mathbb{1}\\left\\{N_{t}=l\\right\\}e^{x Z_{t}}\\geq e^{\\epsilon x}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq e^{-\\epsilon x}\\mathbb{E}\\left(e^{x Z_{t}}\\mathbb{1}\\left\\{N_{t}=l\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad=e^{-\\epsilon x+\\frac{x^{2}(M-m)^{2}l}{8}}\\mathbb{E}\\left(e^{x Z_{t}-\\frac{x^{2}(M-m)^{2}l}{8}}\\mathbb{1}\\left\\{N_{t}=l\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the previous result, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left(e^{x Z_{t}-\\frac{x^{2}(M-m)^{2}l}{8}}\\mathbb{1}\\left\\{N_{t}=l\\right\\}\\right)=\\mathbb{E}\\left(e^{x Z_{t}-\\frac{x^{2}(M-m)^{2}N_{t}}{8}}\\mathbb{1}\\left\\{N_{t}=l\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\left(e^{x Z_{t}-\\frac{x^{2}(M-m)^{2}N_{t}}{8}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}(M_{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}(M_{0})=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(Z_{t}\\geq\\epsilon\\mathrm{~and~}N_{t}=l\\right)\\leq e^{-\\epsilon x+\\frac{x^{2}(M-m)^{2}l}{8}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In particular, for $\\begin{array}{r}{\\epsilon=(M-m)\\sqrt{\\frac{l\\cdot\\log(1/\\alpha)}{2}}}\\end{array}$ and x =l(M4\u2212\u03f5m)2 , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Z_{t}\\geq(M-m)\\sqrt{\\frac{l\\cdot\\log(1/\\alpha)}{2}}\\;\\mathrm{and}\\;N_{t}=l\\right)\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This proves the first part of the Lemma. Summing over the values of $l$ from 1 to $t$ , we find that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Z_{t}\\geq(M-m)\\sqrt{\\frac{N_{t}\\log(1/\\alpha)}{2}}\\;\\mathrm{and}\\;N_{t}\\geq1\\right)\\leq t\\alpha.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similar arguments can be used to prove that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(-Z_{t}\\geq(M-m)\\sqrt{\\frac{N_{t}\\log(1/\\alpha)}{2}}\\;\\mathrm{and}\\;N_{t}\\geq1\\right)\\leq t\\alpha.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Noting that $Z_{t}=\\hat{\\mu}_{t}N_{t}$ and normalizing by $N_{t}$ (and since adding the case $N_{t}=0$ can only increase the probability) concludes the proof of the Lemma. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the abstract and introduction, we claim to present a novel approach to dynamic pricing that enjoys improved regret bounds. We clearly state the approach and explain its novelty, while proving all stated bounds in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The main limitations of our papers are due to the assumptions on the setting. While we follow a minimal set of assumptions that is standard in the dynamic pricing literature, we discuss how to alleviate them in the conclusion section. The computational complexity of our algorithm is comparable to previous work on the topic: the complexity of the price elimination is polylog $(d,T)$ , while the complexity of the valuation estimation depends on the valuation model. For linear valuations, it is polynomial, while for nonparametric ones, it is exponential \u2013 as standard in the non-parametric bandit literature. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 23}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We clearly state our assumptions and prove the results in the appendix. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The information needed to reproduce the experiment is detailed in Appendix A. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 24}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code to reproduce the experiments is publicly available in the repository https: // github. com/ MatildeTulii1/ Improved-Algorithms-for-Contextual-Dynamic-Pricing ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The details of the implementations of the simulations are detailed in Appendix A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the information relative to the statistical significance of the algorithm is contained in Appendix A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the simulation can be (and were) run on a laptop without gpus. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper theoretically studies a well-established theoretical problem; as such, it does not have any direct ethical implications. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper theoretically studies a well-established theoretical problem and the broader impact of our work is only due to the potential impact of advancements in this problem. As all pricing problems, dynamic pricing can have both positive and negative impacts \u2013 offering prices that are more suited to the buyers on the one hand, while increasing the seller\u2019s revenue at the expense of buyers on the other hand. In addition, as with many contextual problems, there might be biases and challenges involving fairness \u2013 one should make sure that similar customers are offered similar prices. This study is orthogonal to ours, and we leave it for future work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]