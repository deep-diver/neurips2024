[{"figure_path": "E7en5DyO2G/figures/figures_9_1.jpg", "caption": "Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG, BOG, BBB and BLR and the 3 tractable Hessian approximations with DLR-10 variational family.", "description": "The figure shows the results of an experiment on the MNIST dataset, comparing four different Bayesian online learning algorithms: BONG, BOG, BBB, and BLR.  Each algorithm uses a diagonal plus low-rank (DLR) precision matrix with rank 10 for the variational family.  Three different approximations of the Hessian are used for each algorithm. The performance metric is the negative log predictive density (NLPD) calculated using a linear Monte Carlo approximation. The x-axis represents the number of training observations, and the y-axis shows the negative log predictive density (NLPD). The results illustrate the performance differences among the methods, highlighting the effectiveness of BONG compared to others.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_9_2.jpg", "caption": "Figure 2: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG with different variational families, namely diagonal (natural and moment), DLR-1, DLR-10.", "description": "This figure displays the negative log predictive density (NLPD) and misclassification rate on the MNIST dataset for the BONG algorithm using four different variational families: diagonal (natural parameterization), diagonal (moment parameterization), DLR-1, and DLR-10.  The Lin-MC posterior predictive is used for estimation. The results illustrate how the choice of variational family affects the algorithm's performance in terms of both accuracy and predictive density.  Shaded areas represent \u00b11 standard error.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_15_1.jpg", "caption": "Figure 3: Runtimes for methods on MNIST. Left: Corresponding to Fig. 1 using different algorithms on DLR-10 family. Right: Corresponding to Fig. 2, using BONG on different variational families.", "description": "This figure shows the running times for different methods used in the MNIST experiment.  The left panel corresponds to Figure 1 and shows the runtimes for various methods using a DLR-10 variational family.  The right panel corresponds to Figure 2, showing the runtimes for the BONG method with different variational families (Full covariance, Diagonal, DLR).  The figure helps to illustrate the computational efficiency of different approaches.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_15_2.jpg", "caption": "Figure 3: Runtimes for methods on MNIST. Left: Corresponding to Fig. 1 using different algorithms on DLR-10 family. Right: Corresponding to Fig. 2, using BONG on different variational families.", "description": "This figure presents running times for different algorithms and variational families on the MNIST dataset. The left panel shows the runtimes of various algorithms (BONG, BOG, BBB, and BLR) using different Hessian approximations (MC-EF, LIN-HESS, LIN-EF) with the DLR-10 variational family. The right panel displays the runtimes of BONG using various variational families (full covariance, diagonal, DLR-1, DLR-10), along with different Hessian approximations. The figure helps to compare the computational efficiency of different methods and highlights the effect of various factors on runtime.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_16_1.jpg", "caption": "Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG, BOG, BBB and BLR and the 3 tractable Hessian approximations with DLR-10 variational family.", "description": "This figure compares four Bayesian online learning algorithms (BONG, BOG, BBB, and BLR) on the MNIST dataset.  The posterior distribution is approximated using a linear Monte Carlo (Lin-MC) method. The algorithms are evaluated using three different methods for approximating the Hessian matrix (MC-HESS, LIN-HESS, and LIN-EF).  The experiment uses a diagonal plus low-rank precision matrix (DLR-10) for the variational family. The results show the negative log predictive density (NLPD) and misclassification rate as a function of the number of training observations.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_17_1.jpg", "caption": "Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG, BOG, BBB and BLR and the 3 tractable Hessian approximations with DLR-10 variational family.", "description": "This figure compares four Bayesian online learning algorithms (BONG, BOG, BBB, and BLR) on the MNIST handwritten digit dataset.  The algorithms use a diagonal plus low-rank precision matrix (DLR) variational family with rank 10 and three methods to approximate the Hessian.  The performance metric is the negative log predictive density (NLPD) of the test set, as a function of the number of training points. The figure shows that BONG, using the linearized Hessian approximation, consistently outperforms the other methods.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_18_1.jpg", "caption": "Figure 7: MNIST expected calibration error (ECE) results at selected timesteps for methods using DLR family.", "description": "This figure shows the expected calibration error (ECE) at four different timesteps (T=250, 500, 1000, 2000) for several methods using the DLR variational family. The box plots summarize the distribution of ECE values across multiple trials.  The methods compared are different combinations of BONG, BOG, BLR, and BBB with various Hessian approximations (MC-HESS, LIN-HESS, MC-EF, LIN-EF).  The figure helps illustrate the calibration performance of each method over time.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_19_1.jpg", "caption": "Figure 9: Predictive performance on SARCOS using MLP 21-20-20-1 with DLR rank 10. Error bars represent \u00b11 standard deviation computed from 3 random trials, randomizing over data order and initial state \u03bc0. (a) We show all 4 algorithms combined with LIN-HESS approximation and I = 1. (b) Same as (a) but with I = 10.", "description": "This figure compares four Bayesian online learning algorithms (BONG, BOG, BLR, BBB) on the SARCOS dataset.  It shows the negative log predictive density (NLPD) as a function of the number of training data points observed.  Two separate plots show the results for one iteration (left) and ten iterations (right) of the inner loop in the algorithms, which is the iterative refinement of the variational posterior.\n\nThe LIN-HESS approximation is used.   The error bars display the standard deviation across 3 different runs, each with varying data ordering and prior mean initialization. BONG and BLR show consistently better results than BBB and BOG, showcasing the benefits of the single-step NGD approach in BONG and showing that the performance of BBB improves as more iterations are performed in the inner loop, nearing the performance of BONG and BLR with sufficient iterations.", "section": "B.3 Comparison of BONG, BLR, BBB and BOG"}, {"figure_path": "E7en5DyO2G/figures/figures_20_1.jpg", "caption": "Figure 9: Predictive performance on SARCOS using MLP 21-20-20-1 with DLR rank 10. Error bars represent \u00b11 standard deviation computed from 3 random trials, randomizing over data order and initial state \u03bc0. (a) We show all 4 algorithms combined with LIN-HESS approximation and I = 1. (b) Same as (a) but with I = 10.", "description": "This figure compares the performance of four Bayesian online learning algorithms (BONG, BOG, BLR, and BBB) on the SARCOS dataset.  The algorithms use a linear Hessian approximation (LIN-HESS) and the DLR variational family with a rank of 10. The left panel shows results with a single iteration per time step (I=1), while the right panel shows results with 10 iterations per time step (I=10).  Error bars indicate \u00b11 standard deviation across three random trials that vary in data ordering and the initialization of the prior mean.  The figure demonstrates the impact of the number of iterations on the algorithms' performance and their relative ranking.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_20_2.jpg", "caption": "Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG, BOG, BBB and BLR and the 3 tractable Hessian approximations with DLR-10 variational family.", "description": "This figure shows the performance of four Bayesian online learning algorithms (BONG, BOG, BBB, and BLR) on the MNIST dataset.  The algorithms use a diagonal plus low-rank (DLR) precision matrix with rank 10 for the variational family. The posterior distribution is approximated using the linearized Monte Carlo (Lin-MC) method. Three different Hessian approximations are compared: MC-EF, LIN-HESS, and LIN-EF. The y-axis represents the negative log predictive density (NLPD), a measure of predictive performance, and the x-axis shows the number of training observations. The results demonstrate that BONG consistently outperforms the other methods, particularly with the LIN-HESS approximation.  It suggests that BONG's combination of natural gradient descent, implicit regularization, and linearization leads to superior performance.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_20_3.jpg", "caption": "Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG, BOG, BBB and BLR and the 3 tractable Hessian approximations with DLR-10 variational family.", "description": "The figure compares four Bayesian online learning algorithms (BONG, BOG, BBB, BLR) on the MNIST dataset.  The posterior distribution is approximated using a linearized Monte Carlo (Lin-MC) method. Three different Hessian approximations are used (MC-EF, LIN-HESS, LIN-EF) with a diagonal plus low-rank (DLR) variational family of rank 10. The plot shows the negative log predictive density (NLPD) on the test set as a function of the number of training observations.  This helps illustrate the relative performance of each algorithm, and how different Hessian approximations and Bayesian learning rules affect the final model performance.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_21_1.jpg", "caption": "Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG, BOG, BBB and BLR and the 3 tractable Hessian approximations with DLR-10 variational family.", "description": "The figure compares the performance of four Bayesian online learning algorithms (BONG, BOG, BBB, and BLR) on the MNIST handwritten digit dataset.  The algorithms use a diagonal-plus-low rank (DLR) variational family with rank 10 to approximate the posterior distribution.  Three different methods for approximating the Hessian matrix are compared: MC-EF (Monte Carlo Empirical Fisher), LIN-HESS (Linearized Hessian), and LIN-EF (Linearized Empirical Fisher). The plot shows the negative log predictive density (NLPD) for each algorithm across different numbers of training observations. The results demonstrate that the BONG algorithm generally outperforms the other methods.", "section": "5 Experiments"}, {"figure_path": "E7en5DyO2G/figures/figures_21_2.jpg", "caption": "Figure 10: Same as Fig. 9 except we use MC-EF approximation with MC = 100.", "description": "This figure compares the performance of four Bayesian online learning algorithms (BONG, BLR, BOG, and BBB) on the SARCOS dataset using the MC-EF approximation for the Hessian.  The x-axis represents the number of training observations, and the y-axis shows the negative log predictive density (NLPD).  The plot shows that BONG outperforms the other methods, particularly as the number of training observations increases.  The shaded regions represent the standard error of the mean across multiple trials.", "section": "5 Experiments"}]