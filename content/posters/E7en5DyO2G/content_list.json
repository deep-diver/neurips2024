[{"type": "text", "text": "Bayesian Online Natural Gradient (BONG) ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matt Jones Peter Chang Kevin Murphy University of Colorado MIT Google DeepMind mcjones@colorado.edu gyuyoung@mit.edu kpmurphy@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose a novel approach to sequential Bayesian inference based on variational Bayes (VB). The key insight is that, in the online setting, we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep); instead we can optimize just the expected log-likelihood, performing a single step of natural gradient descent starting at the prior predictive. We prove this method recovers exact Bayesian inference if the model is conjugate. We also show how to compute an efficient deterministic approximation to the VB objective, as well as our simplified objective, when the variational distribution is Gaussian or a sub-family, including the case of a diagonal plus low-rank precision matrix. We show empirically that our method outperforms other online VB methods in the non-conjugate setting, such as online learning for neural networks, especially when controlling for computational costs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian methods for neural network (NN) training aim to minimize the Kullback-Leibler divergence between true and estimated posterior distributions. This is equivalent to minimizing the variational loss (or negative ELBO) ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\psi)=-\\mathbb{E}_{\\pmb{\\theta}\\sim\\pmb{q}_{\\psi}}[\\log p(\\mathcal{D}|\\pmb{\\theta})]+D_{\\mathbb{K L}}(q_{\\psi}|p_{0})\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here $\\pmb{\\theta}$ are the network parameters, $\\psi$ are the variational parameters of the approximate posterior $q_{\\psi}(\\pmb\\theta)$ , $\\mathcal{D}$ is the training dataset, and $p_{0}(\\pmb\\theta)$ is the prior. The two terms in the variational loss correspond to data fit and regularization to the prior, the latter being analogous to a regularizer $r(\\pmb\\theta)=-\\log p_{0}(\\pmb\\theta)$ in traditional point estimation methods like SGD. ", "page_idx": 0}, {"type": "text", "text": "An important set of approaches learns the variational parameters by gradient descent on $\\mathcal{L}(\\psi)$ [Blundell et al., 2015]. More recently Khan and colleagues [Khan et al., 2018b, Khan and Rue, 2023, Shen et al., 2024] have proposed using the natural gradient $\\mathbf{F}_{\\psi}^{-1}\\nabla_{\\psi}\\mathcal{L}(\\psi)$ where $\\mathbf{F}_{\\psi}$ is the Fisher information matrix of the variational family evaluated at $q_{\\psi}$ . Natural gradient descent (NGD) is often more efficient than vanilla GD because it accounts for the intrinsic geometry of the variational family [Amari, 1998]. Khan and Rue [2023] call this approach the \"Bayesian Learning Rule\" or BLR. Using various choices for the variational distribution, generalized losses replacing negative log-likelihood, and other approximations, they reproduce many standard optimization methods such as Adam, and derive new ones. ", "page_idx": 0}, {"type": "text", "text": "We study Bayesian NN optimization in online learning, where the data are observed in sequence, $\\mathcal{D}_{t}=\\{(\\pmb{x}_{k},\\pmb{\\dot{y}}_{k})_{k=1}^{t}\\}$ , and the algorithm maintains an approximate posterior $q_{\\psi_{t}}(\\pmb{\\theta}_{t})\\approx p(\\bar{\\pmb{\\theta}}_{t}|\\mathcal{D}_{t})$ , which it updates at each step. Fast updates (in terms of both computational speed and statistical efficiency) are critical for many online learning applications [Zhang et al., 2024]. To allow for nonstationarity in the datastream, we include a time index on $\\theta_{t}$ , to represent that the parameters may change over time, as is standard for approaches based on state-space models and the extended Kalman filter (see e.g., [Sarkka and Svensson, 2023]). The belief state is updated recursively using the prior $q_{\\psi_{t\\,|\\,t-1}}$ derived from the previous time step so that the variational loss becomes ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\psi_{t})=-\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t}}}[\\log p(\\pmb{y}_{t}|\\pmb{x}_{t},\\pmb{\\theta}_{t})]+D_{\\mathbb{K}\\mathbb{L}}\\big(q_{\\psi_{t}}|q_{\\psi_{t|t-1}}\\big)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "One option for this online learning problem is to apply NGD on $\\mathcal{L}(\\psi_{t})$ at each time step, iterating until $\\psi_{t}$ converges before consuming the next observation. Our first contribution is a proposal for skipping this inner loop by (a) performing a single natural gradient step with unit learning rate and (b) omitting the $D_{\\mathbb{K L}}$ term in Eq. (2) so that learning is based only on expected loglikelihood: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\psi_{t}=\\psi_{t|t-1}+\\mathbf{F}_{\\psi_{t|t-1}}^{-1}\\nabla_{\\psi_{t|t-1}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t|t-1}}}[\\log p\\left(y_{t}|x_{t},\\theta_{t}\\right)]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "These two modifications work together: instead of regularizing toward the prior explicitly using $D_{\\mathbb{K L}}\\big(q_{\\psi_{t}}|q_{\\psi_{t|t=1}}\\big)_{.}$ , we do so implicitly by using $\\psi_{t|t-1}$ as the starting point of our single natural gradient step. This may appear as a heuristic but we prove in Proposition 4.1 that it yields exact Bayesian inference when $q_{\\psi}$ and $p\\left({\\pmb y}|{\\pmb x},{\\pmb\\theta}\\right)$ are conjugate and $q_{\\psi}$ is an exponential family with natural parameter $\\psi$ . Thus our proposed update can be viewed as a relaxation of the Bayesian update to the non-conjugate variational case. As is common in work on variational inference, we view the result for the conjugate case as a motivating foundation that ensures our method is exact in certain simple settings. The experiments reported in Section 5 and Appendix B complement the theory by showing our method also works well in more general settings. We call Eq. (3) the Bayesian online natural gradient (BONG). ", "page_idx": 1}, {"type": "text", "text": "Our second contribution concerns ways of computing the expectation in Eqs. (1) to (3). This is intractable for NNs, even for variational distributions that are easy to compute, since the likelihood takes the form $p(\\pmb{y}_{t}|\\pmb{x}_{t},\\pmb{\\theta}_{t})=p(\\pmb{y}_{t}|f(\\pmb{x}_{t},\\pmb{\\theta}_{t}))$ with $f(\\pmb{x}_{t},\\pmb{\\theta}_{t})$ , representing the function computed by the network, is a complex, nonlinear function of $\\pmb{\\theta}_{t}$ . Many previous approaches have approximated the expected loglikelihood by sampling methods which add variance and computation time depending on the number of samples [Blundell et al., 2015, Shen et al., 2024]. We propose a deterministic, closed-form update that applies when the variational distribution is Gaussian (or a sub-family) and the likelihood is an exponential family with natural parameter $f(\\pmb{x}_{t},\\pmb{\\theta}_{t})$ and mean parameter $h(\\pmb{x}_{t},\\pmb{\\theta}_{t})$ (e.g., for classification, $f$ returns the vector of class logits, $h$ returns class probabilities, and $h=\\operatorname{softmax}(f))$ . This update can be derived in two equivalent ways. First, we use a local linear approximation of the network $\\underline{{h}}(\\pmb{x}_{t},\\pmb{\\theta}_{t})\\,\\approx\\,\\bar{h}_{t}(\\pmb{\\theta}_{t})$ [Immer et al., 2021a] and a Gaussian approximation of the likelihood $\\mathcal{N}(y_{t}|\\bar{h}_{t}(\\pmb{\\theta}_{t}),\\dot{\\mathbf{R}}_{t})$ [Ollivier, 2018, Tronarp et al., 2018]. Under these assumptions the expectation in Eq. (3) can be calculated analytically. Alternatively, we use a different linear approximation $f(\\pmb{x}_{t},\\pmb{\\theta}_{t})\\;\\widetilde{\\;}\\approx\\;\\widetilde{f}_{t}(\\pmb{\\theta}_{t})$ and a delta approximation $q_{\\pmb{\\psi}_{t|t-1}}\\sp{\\phantom{\\dagger}}(\\pmb{\\theta}_{t})\\;\\approx\\;\\delta_{\\pmb{\\mu}_{t|t-1}}(\\pmb{\\theta}_{t})$ where $\\pmb{\\mu}_{t|t-1}=\\mathbb{E}_{q_{\\psi_{t|t-1}}}[\\pmb{\\theta}_{t}]$ is the prior mean, so that the expectation in Eq. (3) is replaced by a plugin prediction. The linear $(h)$ -Gaussian approximation is previously known but the linear $(f)$ -delta approximation is new, and we prove in Proposition 4.2 that they yield the same update, which we call linearized BONG, or BONG-LIN. Finally, we discuss different ways of approximating the Hessian of the objective, which is needed for NGD. ", "page_idx": 1}, {"type": "text", "text": "Our BONG framework unifies several existing methods for Bayesian online learning, and it offers new algorithms based on alternative variational families or parameterizations. We define a large space of methods by combining 4 different update rules with 4 different ways of computing the relevant expected gradients and Hessians and 3 different variational families (Gaussians with full, diagonal, and diagonal-plus-low rank precision matrices). We conduct experiments systematically testing how these factors affect performance. We find support for all three principles of our approach\u2014 NGD, implicit regularization to the prior, and linearization\u2014 in terms of both statistical and computational efficiency. Code for our experiments is available at https://github.com/petergchang/bong/. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Variational inference approximates the Bayesian posterior from within some suitable family in a way that bypasses the normalization term [Zellner, 1988, Jordan et al., 1999]. A common choice for the variational family is a Gaussian. For online learning, the exact update equations for Gaussian variational flitering are given by the RVGA method of [Lambert et al., 2021]. This update is implicit but can be approximated by an explicit RVGA update which we show arises as a special case of BONG. Most applications of Gaussian VI use a mean-field approximation defined by diagonal covariance, which scales linearly with model size. More expressive but still linear in the model size are methods that express the covariance [Tomczak et al., 2020] or precision [Mishkin et al., 2018, Lambert et al., 2023, Chang et al., 2023] as a sum of diagonal and low rank matrices (DLR). In this paper, we consider variational families defined by full covariance, diagonal covariance, and DLR covariance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "For NNs and other complicated models, even the variational approximation can be intractable, so methods have been developed to approximately minimize the VI loss. Bayes by backprop (BBB) [Blundell et al., 2015] learns a variational distribution on NN weights by iterated GD on the VI loss of Eq. (1). They focus on mean-field Gaussian approximations but the approach also applies to other variational families. Here we adapt BBB to online learning to compare to our methods. ", "page_idx": 2}, {"type": "text", "text": "The Bayesian learning rule (BLR) replaces BBB\u2019s GD with NGD [Khan and Rue, 2023]. Several variants of BLR have been developed including VON and VOGN for a mean-field Gaussian prior [Khan et al., 2018b] and SLANG for a DLR Gaussian [Mishkin et al., 2018]. BLR has also been used to derive versions of many classic optimizers including SGD, RMSprop and Adam [Khan et al., 2018a, Khan and Rue, 2023, Lin et al., 2024, Shen et al., 2024]. Although BLR has been applied to online learning, we are particularly interested in Bayesian flitering including in nonstationary environments, where observations must be processed one at time and updates are based on the posterior from the previous step, often in conjunction with parameter dynamics. We therefore develop flitering versions of BLR to compare to BONG, some of which reduce to VON, VOGN and SLANG in the batch setting, while others are novel. We also note BLR is a mature theory including several clever tricks we have not yet incorporated into our framework. ", "page_idx": 2}, {"type": "text", "text": "Khan and Rue [2023] observe that conjugate updating is equivalent to one step of BLR with learning rate 1. This is similar to our Proposition 4.1 except that BLR retains the KL term in the variational loss. BLR and BONG agree in this case because the gradient of the KL is zero on BLR\u2019s first iteration: $\\nabla_{\\psi=\\psi_{t|t-1}}D_{\\mathbb{K L}}\\big(q_{\\psi}|q_{\\psi_{t|t-1}}^{\\overline{{\\mathbf{\\alpha}}}}\\big)=0$ . Therefore BONG can be seen as a special case of BLR with one update step per observation and learning rate 1. Our contribution is to recognize that doing a single update step allows the KL term to be dropped entirely, yielding a substantially simpler algorithm which our experiments show also performs better. ", "page_idx": 2}, {"type": "text", "text": "While BLR allows alternative losses in place of the NLL in Eq. (2), we can also replacing the KL term with other divergences [Knoblauch et al., 2022]. Our approach fits within that \"generalized VB\" framework in that it drops the divergence altogether. Our approach of implicitly regularizing to the prior using a single NGD step is also similar to the implicit MAP filter of [Bencomo et al., 2023] which performs truncated GD from the prior mode. The principal difference is they perform GD on model parameters $(\\pmb\\theta_{t})$ while we do NGD on the variational parameters $(\\psi_{t})$ . Thus BONG maintains a full prior and posterior while IMAP is more concerned with how the choice of optimizer can substitute for explicit tracking of covariance. ", "page_idx": 2}, {"type": "text", "text": "We show two other ways to derive the BONG update in Appendix D, one of which is to replace the expected NLL in Eq. (2) with a linear approximation and solve the resulting equation exactly. Several past works have taken this approach, arriving at updates similar to ours. Ch\u00e9rief-Abdellatif et al. [2019] study streaming variational Bayes and propose solving Eq. (2) with a linearized expected NLL. When the variational family is an exponential family their update becomes NGD [Khan and Lin, 2017] and matches the BONG update. Hoeven et al. [2018] show how mirror descent can be derived as a special case of Exponential Weights [Littlestone and Warmuth, 1994], which is closely related to Bayesian updating. The resulting algorithm is similar to BONG and follows from linearizing the NLL instead of expected NLL, with an additional delta assumption at the prior mean. Lyu and Tsang [2021] study relaxed block-box optimization where the objective is arg $\\mathrm{:min}_{\\pmb{\\psi}}\\,\\mathbb{E}_{\\pmb{x}\\sim q_{\\pmb{\\psi}}}[f(\\pmb{x})]$ for some target function $f$ . They use a mirror descent formulation with linearized expected loss and KL regularizer and show the resulting update is NGD on expected loss, formally equivalent to our BONG update. From the perspective of this prior work, our contribution is to express the BONG update simply as NGD on the expected NLL, motivated by replacing the KL with implicit regularization, and to show how this yields a variety of known and novel algorithms for Bayesian filtering. ", "page_idx": 2}, {"type": "text", "text": "EKF applications to NNs apply Bayesian flitering using a local linear approximation of the network, leading to simple closed form updates [Singhal and Wu, 1989, Puskorius and Feldkamp, 1991]. The classic EKF assumes a Gaussian observation distribution but it has been extended to other exponential families (e.g. for classification) by matching the mean and covariance in what we call the conditional moments EKF (CM-EKF) [Ollivier, 2018, Tronarp et al., 2018]. Applying a KL projection to diagonal covariance yields the variational diagonal EKF (VD-EKF) [Chang et al., 2022]. ", "page_idx": 2}, {"type": "text", "text": "Alternatively, projecting to diagonal plus low rank precision using SVD gives LO-FI [Chang et al., 2023]. We derive all these methods as special cases of BONG-LIN. Further developments in this direction include the method of [Titsias et al., 2024] which does Bayesian filtering on only the final weight layer, and WoLF [Duran-Martin et al., 2024] which achieves robustness to outliers through data-dependent weighting of the loglikelihood. ", "page_idx": 3}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We study online supervised learning where the agent receives input $\\mathbf{\\boldsymbol{x}}_{t}\\,\\in\\,\\mathbb{R}^{D}$ and observation $\\pmb{y}_{t}\\in\\mathbb{R}^{C}$ on each time step, which it aims to model with a function $f_{t}(\\pmb\\theta_{t})=f(\\pmb x_{t},\\pmb\\theta_{t})$ such as a NN with weights $\\pmb{\\theta}_{t}\\in\\mathbb{R}^{P}$ . The predictions for $\\scriptstyle\\pmb{y}_{t}$ are given by some observation distribution $p(\\pmb{y}_{t}|f_{t}(\\pmb{\\theta}_{t}))$ . For example, $f$ may compute the mean for regression or the class logits for classification. ", "page_idx": 3}, {"type": "text", "text": "We work in a Bayesian framework where the agent maintains an approximate posterior distribution over $\\theta_{t}$ after observing data $\\mathcal{D}_{t}\\,=\\,\\{({\\pmb x}_{k},{\\pmb y}_{k})_{k=1}^{t}\\}$ . The filtering posterior $q_{\\psi_{t}}(\\pmb{\\theta}_{t})\\approx p(\\pmb{\\theta}_{t}|\\mathcal{D}_{t})$ is approximated within some parametric family indexed by the variational parameter $\\psi_{t}$ . We allow for nonstationarity by assuming $\\pmb{\\theta}$ changes over time according to some dynamic model $p(\\pmb{\\theta}_{t}|\\pmb{\\theta}_{t-1})$ . By pushing the posterior from step $t-1$ through the dynamics we obtain a prior for step $t$ given by $\\bar{q_{\\psi_{t|t-1}}}(\\pmb{\\theta_{t}})\\approx\\bar{p(\\pmb{\\theta_{t}}|\\mathcal{D}_{t-1})}$ . For example suppose the variational posterior from the previous step is Gaussian, $q_{\\psi_{t-1}}(\\pmb{\\theta}_{t-1})=\\mathcal{N}(\\pmb{\\theta}_{t-1}|\\pmb{\\mu}_{t-1},\\pmb{\\Sigma}_{t-1})$ , and the dynamics model is an Ornstein-Uhlenbeck process, as proposed in prior work [Kurle et al., 2020, Titsias et al., 2024] to handle non-stationarity, i.e., the dynamics model has the form $\\pmb{\\theta}_{t}\\sim\\mathcal{N}(\\gamma_{t}\\pmb{\\theta}_{t-1}+(1-\\gamma_{t})\\pmb{\\mu}_{0},\\mathbf{Q}_{t})$ , where $\\mathbf{Q}_{t}=(1-\\gamma_{t}^{2})\\Sigma_{0}$ is the covariance of the noise process, $0\\le\\gamma_{t}\\le1$ is the degree of drift, and $p(\\pmb{\\theta}_{0})=\\mathcal{N}(\\pmb{\\mu}_{0},\\pmb{\\Sigma}_{0})$ is the prior. In this case, the parameters of the prior predictive distribution are $\\pmb{\\mu}_{t|t-1}=\\gamma_{t}\\pmb{\\mu}_{t-1}+(1-\\gamma_{t})\\pmb{\\mu}_{0}$ and $\\pmb{\\Sigma}_{t|t-1}=\\gamma_{t}^{2}\\pmb{\\Sigma}_{t-1}+\\mathbf{Q}_{t}$ . In general the predict step may require approximation to stay in the variational family (e.g., if the dynamics are nonlinear). In this paper, our focus is the update step from $\\psi_{t|t-1}$ to $\\psi_{t}$ upon observing $\\left({\\bf{\\boldsymbol{x}}}_{t},{\\bf{\\boldsymbol{y}}}_{t}\\right)$ , so for simplicity we assume constant (static) parameters, i.e., $p(\\pmb{\\theta}_{t}|\\pmb{\\theta}_{-1})=\\delta(\\pmb{\\theta}_{t}-\\pmb{\\theta}_{t-1})$ (equivalently $\\gamma_{t}=1$ ), so $\\psi_{t|t-1}=\\psi_{t-1}$ ; however, our method can trivially handle non-stationary parameters. ", "page_idx": 3}, {"type": "text", "text": "Variational inference seeks an approximate posterior that minimizes the KL divergence from the exact Bayesian update from the prior. In the online setting this becomes ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi_{t}^{\\ast}=\\operatorname*{arg\\,min}_{\\psi}D_{\\mathbb{K L}}\\bigl(q_{\\psi}(\\theta_{t})|Z_{t}^{-1}q_{\\psi_{t|t-1}}(\\theta_{t})\\,p(y_{t}|f_{t}(\\theta_{t}))\\bigr)=\\operatorname*{arg\\,min}_{\\psi}\\mathcal{L}_{t}(\\psi)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{L}_{t}$ is the online VI loss defined in Eq. (2), and the normalization term $Z_{t}$ (which depends on $\\pmb{x}_{t}$ ) drops out as an additive constant. Our goal is an efficient approximate solution to this variational optimization problem. ", "page_idx": 3}, {"type": "text", "text": "We will sometimes assume the variational posterior $q_{\\psi}$ is an exponential family distribution with natural parameter $\\psi$ so that $q_{\\psi_{t}}(\\pmb{\\theta}_{t})=\\exp\\bar{(\\psi_{t}^{\\intercal}T(\\pmb{\\theta}_{t})-\\Phi(\\psi_{t}))}$ , with log-partition function $\\Phi$ and sufficient statistics $T(\\pmb\\theta_{t})$ . Assuming $\\Phi$ is strictly convex (which holds in the cases we study) there is a bijection between $\\psi_{t}$ and the dual (or expectation) parameter $\\pmb{\\rho}_{t}=\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\pmb{\\psi}_{t}}}[T(\\pmb{\\theta}_{t})]$ . Classical thermodynamic identities imply that the Fisher information matrix has the form $\\mathbf{F}_{\\psi_{t}}=\\partial\\pmb{\\rho}_{t}/\\partial\\psi_{t}$ This has important implications for NGD on exponential families [Khan and Rue, 2023] because it implies that for any function $\\ell$ defined on the variational parameter space the natural gradient wrt natural parameters $\\psi_{t}$ is the regular gradient wrt the dual parameters $\\rho_{t}$ , i.e., $\\bar{\\mathbf{F}}_{\\psi_{t}}^{-1}\\nabla_{\\psi_{t}}\\ell=\\nabla_{\\rho_{t}}\\ell$ . ", "page_idx": 3}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose to approximate the variational optimization problem in Eq. (4) using the BONG update in Eq. (3). When $q_{\\psi}$ is an exponential family, the fact that the natural gradient wrt the natural parameters $\\psi_{t}$ is the regular gradient wrt the dual parameters $\\rho_{t}$ implies an equivalent mirror descent form (see Appendix $\\mathrm{D}$ for further analysis of BONG from the MD perspective): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi_{t}=\\psi_{t|t-1}+\\nabla_{\\pmb{\\rho}_{t|t-1}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t|t-1}}}[\\log p(\\pmb{y}_{t}|\\pmb{x}_{t},\\pmb{\\theta}_{t})]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is NGD with unit learning rate on the variational loss in Eq. (2) but ignoring the $D_{\\mathbb{K L}}\\big(q_{\\psi}|q_{\\psi_{t|t-1}}\\big)$ term. In this section we first prove this method is optimal when the model is conjugate and then describe extensions to more complex cases of practical interest. ", "page_idx": 3}, {"type": "text", "text": "4.1 Conjugate case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our approach is motivated by the following result which states that BONG matches exact Bayesian inference when the variational distribution and the likelihood are conjugate exponential families: ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.1. Let the observation distribution (likelihood) be an exponential family with natural parameter $\\pmb{\\theta}_{t}$ (where $T_{l}({\\pmb y}_{t})\\,=\\,{\\pmb y}_{t}$ is the sufficient statistics for the likelihood and $A(\\pmb\\theta_{t})$ is the log-partition function) ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{t}(\\pmb{y}_{t}|\\pmb{\\theta}_{t})=\\exp\\left(\\pmb{\\theta}_{t}^{\\intercal}\\pmb{y}_{t}-A(\\pmb{\\theta}_{t})-b(\\pmb{y}_{t})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and let the prior be the conjugate exponential family ", "page_idx": 4}, {"type": "equation", "text": "$$\nq_{\\psi_{t|t-1}}(\\pmb{\\theta}_{t})=\\exp\\left(\\psi_{t|t-1}^{\\mathsf{T}}T(\\pmb{\\theta}_{t})-\\Phi(\\psi_{t|t-1})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $T(\\pmb{\\theta}_{t})=[\\pmb{\\theta}_{t};-A(\\pmb{\\theta}_{t})]$ . Then the exact Bayesian update agrees with Eq. (5). ", "page_idx": 4}, {"type": "text", "text": "The proof is in Appendix C. Writing the natural parameters of the prior as $\\psi_{t|t-1}=[\\chi_{t|t-1};\\nu_{t|t-1}]$ , we show the Bayesian update and BONG both yield $x_{t}=x_{t|t-1}+y_{t}$ and $\\nu_{t}=\\nu_{t|t-1}+1$ . Intuitively, we are just accumulating a sum of the observed sufficient statistics, and a counter of the sample size (number of observations seen so far). ", "page_idx": 4}, {"type": "text", "text": "4.2 Variational case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In practical settings the conjugacy assumption of Proposition 4.1 will not be met, so Eqs. (3) and (5) will only approximate the Bayesian update. In this paper we restrict to Gaussian variational families. We refer to the unrestricted case as FC (full covariance), defined by the variational distribution ", "page_idx": 4}, {"type": "equation", "text": "$$\nq_{\\psi_{t|t-1}}(\\pmb{\\theta}_{t})=\\mathcal{N}\\left(\\pmb{\\theta}_{t}|\\pmb{\\mu}_{t|t-1},\\pmb{\\Sigma}_{t|t-1}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Sigma_{t|t-1}$ can be any positive semi-definite (PSD) matrix. The natural and dual parameters are $\\begin{array}{r}{\\psi=\\bigl(\\Sigma^{-1}\\mu,-\\frac12\\mathrm{vec}\\bigl(\\Sigma^{-1}\\bigr)\\bigr)}\\end{array}$ and $\\pmb{\\rho}=(\\pmb{\\mu},\\mathrm{vec}(\\pmb{\\mu}\\mu^{\\intercal}+\\pmb{\\Sigma}))$ . Appendix E.1.1 shows that Eq. (5) translated back to $(\\bar{\\mu},\\Sigma)$ gives the following BONG update for the FC case: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t}=\\mu_{t\\vert t-1}+\\Sigma_{t}\\underbrace{\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t\\vert t-1}}}[\\nabla_{\\theta_{t}}\\log p(y_{t}\\vert f_{t}(\\theta_{t}))]}_{g_{t}}}\\\\ &{\\Sigma_{t}^{-1}=\\Sigma_{t\\vert t-1}^{-1}-\\underbrace{\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t\\vert t-1}}}\\left[\\nabla_{\\theta_{t}}^{2}\\log p(y_{t}\\vert f_{t}(\\theta_{t}))\\right]}_{\\mathbf{G}_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which matches the explicit update in the RVGA method of [Lambert et al., 2021]. ", "page_idx": 4}, {"type": "text", "text": "4.3 Monte Carlo approximation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The integrals over the prior $q_{\\psi_{t}|\\,t-1}$ in Eqs. (9) and (10) are generally intractable and must be approximated. One option is to use Monte Carlo, in what we call BONG-MC. Given $M$ independent samples $\\bar{\\pmb{\\theta}}_{t}^{(m)}\\sim q_{\\psi_{t|t-1}}$ , we estimate the expected gradient $g_{t}=\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t|t-1}}}[\\nabla_{\\pmb{\\theta}_{t}}\\log p(\\pmb{y}_{t}|f_{t}(\\pmb{\\theta}_{t}))]$ and expected Hessian $\\mathbf{G}_{t}=\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t\\mid t-1}}}\\left[\\nabla_{\\pmb{\\theta}_{t}}^{2}\\log p\\left(\\pmb{y}_{t}|\\,f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)\\right]$ as the empirical means ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\pmb{g}_{t}^{\\mathrm{suc}}=\\frac{1}{M}\\sum_{m=1}^{M}\\hat{\\pmb{g}}_{t}^{(m)},\\quad\\hat{\\pmb{g}}_{t}^{(m)}=\\nabla_{\\pmb{\\theta}_{t}=\\hat{\\pmb{\\theta}}_{t}^{(m)}}\\log p(\\pmb{y}_{t}|\\boldsymbol{f}_{t}(\\pmb{\\theta}_{t}))}\\\\ &{}&{\\displaystyle\\mathbf{G}_{t}^{\\mathrm{MC\\-HESs}}=\\frac{1}{M}\\sum_{m=1}^{M}\\hat{\\mathbf{G}}_{t}^{(m)},\\;\\;\\;\\hat{\\mathbf{G}}_{t}^{(m)}=\\nabla_{\\pmb{\\theta}_{t}=\\hat{\\pmb{\\theta}}_{t}^{(m)}}^{2}\\log p(\\pmb{y}_{t}|\\boldsymbol{f}_{t}(\\pmb{\\theta}_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We use $\\mathbf{G}^{\\mathrm{MC-HESS}}$ only for small models. Otherwise we use empirical Fisher (Section 4.5). ", "page_idx": 4}, {"type": "text", "text": "4.4 Linearized BONG ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As an alternative to BONG-MC, we propose a linear approximation we call BONG-LIN that yields a deterministic and closed-form update. Assume the likelihood is an exponential family as in Proposition 4.1 but with natural parameter predicted by some function $f_{t}({\\pmb\\theta}_{t}\\bar{\\bf\\alpha})=f({\\pmb x}_{t},{\\pmb\\theta}_{t})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\pmb{y}_{t}|\\pmb{x}_{t},\\pmb{\\theta}_{t})=\\exp\\left(f_{t}(\\pmb{\\theta}_{t})^{\\top}\\,\\pmb{y}_{t}-A(f_{t}(\\pmb{\\theta}_{t}))-b(\\pmb{y}_{t})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We also define the dual (moment) parameter of the likelihood as $h_{t}(\\pmb{\\theta}_{t})=\\mathbb{E}\\left[\\pmb{y}_{t}|f_{t}(\\pmb{\\theta}_{t})\\right]$ . In a NN, $f_{t}$ and $h_{t}$ are related by the final response layer. For example in classification $f_{t}$ and $h_{t}$ give the class logits and probabilities, with $h_{t}(\\Bar{\\pmb{\\theta}}_{t})=\\mathrm{softmax}(f_{t}(\\pmb{\\theta}_{t}))$ , with $\\scriptstyle\\pmb{y}_{t}$ being the one-hot encoding. ", "page_idx": 5}, {"type": "text", "text": "We now define two methods for approximating the expected gradient $\\scriptstyle g_{t}$ and expected Hessian $\\mathbf{G}_{t}$ , based on linearizing the predictive model at the prior mean $\\pmb{\\mu}_{t|t-1}$ in terms of either $f_{t}(\\pmb\\theta_{t})$ or $h_{t}(\\pmb\\theta_{t})$ , and then prove their equivalence. ", "page_idx": 5}, {"type": "text", "text": "The linear $(h)$ -Gaussian approximation [Ollivier, 2018, Tronarp et al., 2018] linearizes $h_{t}(\\pmb\\theta_{t})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{h}_{t}(\\pmb\\theta_{t})=\\hat{\\pmb y}_{t}+\\mathbf H_{t}(\\pmb\\theta_{t}-\\pmb\\mu_{t|t-1})}\\\\ &{\\quad\\quad\\hat{\\pmb y}_{t}=h_{t}(\\pmb\\mu_{t|t-1})}\\\\ &{\\quad\\mathbf H_{t}=\\frac{\\partial h_{t}}{\\partial\\pmb\\theta_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and approximates the likelihood by a Gaussian with variance based at $\\pmb{\\mu}_{t|t-1}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{p}_{t}^{\\mathrm{LG}}(y_{t}|\\theta_{t})=\\mathcal{N}(y_{t}|\\bar{h}_{t}(\\theta_{t}),\\mathbf{R}_{t}),\\quad\\mathbf{R}_{t}=\\mathbb{V}\\left[y_{t}|\\theta_{t}=\\mu_{t|t-1}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The linear $(f)$ -delta approximation linearizes $f_{t}(\\pmb\\theta_{t})$ and maintains the original exponential family likelihood distribution in Eq. (13) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{f}_{t}(\\pmb\\theta_{t})=f_{t}(\\pmb\\mu_{t|t-1})+\\mathbf F_{t}(\\pmb\\theta_{t}-\\pmb\\mu_{t|t-1})}\\\\ &{\\qquad\\quad\\mathbf F_{t}=\\frac{\\partial f_{t}}{\\partial\\pmb\\theta_{t}}}\\\\ &{\\qquad\\quad\\bar{p}_{t}^{\\mathrm{LD}}(\\pmb y_{t}|\\pmb\\theta_{t})\\propto\\exp\\left(\\bar{f}_{t}(\\pmb\\theta_{t})^{\\intercal}\\pmb y_{t}-A(\\bar{f}_{t}(\\pmb\\theta_{t}))-b(\\pmb y_{t})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It also uses a plug-in approximation that replaces $q_{\\psi_{t|t-1}}(\\pmb{\\theta}_{t})$ with a point mass $\\delta_{\\pmb{\\mu}_{t|t-1}}(\\pmb{\\theta}_{t})_{.}$ so that the expected gradient and Hessian are approximated by their values at the prior mean, i.e., $\\nabla_{\\pmb{\\theta}_{t}=\\pmb{\\mu}_{t|t-1}}\\dot{\\log p}_{t}^{\\mathrm{LD}}(\\pmb{y}_{t}|\\pmb{\\theta}_{t})$ and $\\nabla_{\\pmb{\\theta}_{t}=\\pmb{\\mu}_{t\\vert t-1}}^{2}\\log\\bar{p}_{t}^{\\mathrm{LD}}(\\pmb{y}_{t}|\\pmb{\\theta}_{t})$ , rather than being sampled. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2. Under a Gaussian variational distribution, the linear $(h)$ -Gaussian and linear $(f)$ - delta approximations yield the same values for the expected gradient and Hessian ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{g}_{t}^{\\mathrm{{LIN}}}=\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}(\\pmb{y}_{t}-\\hat{\\pmb{y}}_{t})}\\\\ {\\mathbf{G}_{t}^{\\mathrm{{LIN-HESS}}}=-\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "See Appendix C for the proof. The main idea for the $\\pmb{g}_{t}^{\\mathrm{LIN}}$ part is that the linear-Gaussian assumptions make the gradient linear in $\\pmb\\theta_{t}$ so the expected gradient equals the gradient at the mean. The main idea for the $\\mathbf{G}_{t}^{\\mathrm{LIN-HESS}}$ part is that eliminating the Hessian of the NN requires different linearizing assumptions for the Gaussian and delta approximations, and the remaining nonlinear terms (from the log-likelihood in Eq. (13)) agree because of the property of exponential families that the Hessian of the log-partition $A$ equals the conditional variance $\\mathbf{R}_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (9) and (10) gives the BONG-LIN update for a FC Gaussian prior ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol\\mu}_{t}={\\boldsymbol\\mu}_{t\\vert t-1}+\\mathbf K_{t}\\big({\\boldsymbol y}_{t}-\\hat{{\\boldsymbol y}}_{t}\\big)}\\\\ &{\\boldsymbol\\Sigma_{t}={\\boldsymbol\\Sigma}_{t\\vert t-1}-\\mathbf K_{t}\\mathbf H_{t}{\\boldsymbol\\Sigma}_{t\\vert t-1}}\\\\ &{{\\mathbf K}_{t}={\\boldsymbol\\Sigma}_{t\\vert t-1}\\mathbf H_{t}^{\\intercal}\\left({\\mathbf R}_{t}+\\mathbf H_{t}{\\boldsymbol\\Sigma}_{t\\vert t-1}\\mathbf H_{t}^{\\intercal}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{K}_{t}$ is the Kalman gain matrix (see Appendix E.1.2). This matches the CM-EKF [Tronarp et al., 2018, Ollivier, 2018]. ", "page_idx": 5}, {"type": "text", "text": "4.5 Empirical Fisher ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The methods in Sections 4.3 and 4.4 require explicitly computing the Hessian of the loss (MC-HESS) or the Jacobian of the network (LIN-HESS). These are too expensive for large models or high-dimensional observations. Instead we can use an empirical Fisher approximation that replaces the Hessian with the outer product of the gradient (see e.g, [Martens, 2020]). ", "page_idx": 5}, {"type": "table", "img_path": "E7en5DyO2G/tmp/364553c1e0e5b27229e9beefa269bd0106562564078e390ce19097ea5f8187df.jpg", "table_caption": [], "table_footnote": ["Table 1: The 4 Hessian approximations. "], "page_idx": 5}, {"type": "text", "text": "For the MC-EF variant, we make the following approximation: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{G}_{t}^{\\mathrm{MC-EF}}=-\\frac{1}{M}\\hat{\\mathbf{G}}_{t}^{(1:M)}\\hat{\\mathbf{G}}_{t}^{(1:M)^{\\intercal}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\mathbf{G}}_{t}^{(1:M)}=[\\hat{\\pmb{g}}_{t}^{(1)},\\dots,\\hat{\\pmb{g}}_{t}^{(M)}]$ is the $P\\times M$ matrix of gradients from the MC samples. ", "page_idx": 6}, {"type": "text", "text": "We can also consider a similar approach for the LIN-EF variant that is Jacobian-free and sampling-free. Note that if $\\hat{\\pmb y}_{t}$ were the true value of $\\mathbb{E}\\left[\\pmb{y}_{t}|\\pmb{x}_{t}\\right]$ (i.e., if the model were correct) then we would have $\\mathbb{E}\\left[(\\pmb{y}_{t}-\\hat{\\pmb{y}}_{t})\\bar{(\\pmb{y}_{t}-\\hat{\\pmb{y}}_{t})}\\pmb{\\Upsilon}\\right]=\\mathbf{R}_{t}$ , implying $\\mathbb{E}\\left[{\\pmb{g}}_{t}^{\\tt L I N}\\left({\\pmb{g}}_{t}^{\\tt L I N}\\right)^{\\sf T}\\right]=-{\\bf G}_{t}^{\\tt L I N-H E S S}$ . This suggests using ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{g}_{t}^{\\mathrm{LIN-EF}}=\\nabla_{\\boldsymbol{\\theta}_{t}=\\boldsymbol{\\mu}_{t\\mid t-1}}\\left[-\\frac{1}{2}\\left(\\boldsymbol{y}_{t}-\\boldsymbol{h}_{t}(\\boldsymbol{\\theta}_{t})\\right)^{\\intercal}\\mathbf{R}_{t}^{-1}(\\boldsymbol{y}_{t}-\\boldsymbol{h}_{t}(\\boldsymbol{\\theta}_{t}))\\right]}\\\\ &{\\qquad\\quad=\\left(\\frac{\\partial\\boldsymbol{h}_{t}(\\boldsymbol{\\theta}_{t})}{\\partial\\boldsymbol{\\theta}_{t}}\\right)_{\\boldsymbol{\\theta}_{t}=\\boldsymbol{\\mu}_{t\\mid t-1}}^{\\intercal}\\mathbf{R}_{t}^{-1}(\\boldsymbol{y}_{t}-\\boldsymbol{h}_{t}(\\boldsymbol{\\mu}_{t\\mid t-1}))=\\boldsymbol{g}_{t}^{\\mathrm{LIN}}}\\\\ &{\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}=-\\boldsymbol{g}_{t}^{\\mathrm{LIN}}\\left(\\boldsymbol{g}_{t}^{\\mathrm{LIN}}\\right)^{\\intercal}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where Eq. (29) is the EF approximation to Eq. (22). ", "page_idx": 6}, {"type": "text", "text": "A more accurate EF approximation is possible by sampling virtual observations $\\tilde{\\pmb{y}}_{t}$ from $p\\big(\\cdot|f_{t}(\\hat{\\pmb{\\theta_{t}}}^{(m)})\\big)$ or $p\\big(\\cdot\\big|f_{t}\\big(\\mu_{t|t-1}\\big)\\big)$ and using them for the gradients in Eq. (26) or Eq. (29) (respectively) [Martens, 2020, Kunstner et al., 2020]. However, in our experiments we use the actual observations $\\scriptstyle\\pmb{y}_{t}$ which is faster and follows previous work (e.g., [Khan et al., 2018b]). ", "page_idx": 6}, {"type": "text", "text": "4.6 Update rules ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In addition to the four ways of approximating the expected Hessian (summarized in Table 1), we also consider four variants of BONG, based on what kind of loss we optimize and what kind of update we perform, as we describe below. See Table 2 for a summary. ", "page_idx": 6}, {"type": "table", "img_path": "E7en5DyO2G/tmp/c1b6ba623a1dcc47692e2c59517d0dcdd240d99b342131be9bdfbb18fc62f95b.jpg", "table_caption": ["Table 2: The 4 update algorithms. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "BONG (Bayesian online natural gradient) performs one step of NGD on the expected log-likelihood. We set learning rate to $\\alpha_{t}~=~1$ since this is optimal for conjugate models. The update (for an exponential variational family) is as in Eq. (5): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\psi_{t}=\\psi_{t|t-1}+\\nabla_{\\pmb{\\rho}_{t|t-1}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t|t-1}}}[\\log p(\\pmb{y}_{t}|\\pmb{x}_{t},\\pmb{\\theta}_{t})]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "BOG (Bayesian online gradient) performs one step of GD (instead of NGD) on the expected loglikelihood. We include a learning rate $\\alpha$ because GD does not have the scale-invariance of NGD: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\psi_{t}=\\psi_{t}+\\alpha_{t}\\nabla_{\\psi_{t}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t}}}[\\log p(\\pmb{y}_{t}|f_{t}(\\pmb{\\theta}_{t}))]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "BLR (Bayesian learning rule, [Khan and Rue, 2023]) uses NGD (like BONG) but optimizes the VI loss using multiple iterations, instead of optimizing the expected NLL with a single step. When modified to the online setting, BLR starts an inner loop at each time step with $\\psi_{t,0}=\\psi_{t|t-1}$ and iterates ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\psi_{t,i}=\\psi_{t,i-1}+\\alpha_{t}\\mathbf{F}_{\\psi_{t,i-1}}\\nabla_{\\psi_{t,i-1}}\\Big(\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}\\big[\\log p(y_{t}|f_{t}(\\theta_{t}))\\big]-D_{\\mathbb{K}\\mathbb{L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t,{i-1}}}\\big)\\Big)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For an exponential variational family this can be written in mirror descent form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi_{t,i}=\\psi_{t,i-1}+\\alpha_{t}\\nabla_{\\rho_{t,i-1}}\\Big(\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p(y_{t}|f_{t}(\\theta_{t}))]-D_{\\mathbb{K}\\mathbb{L}}\\big(q_{\\psi t,i-1}|q_{\\psi_{t|t-1}}\\big)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "BBB (Bayes By Backprop, [Blundell et al., 2015]) is like BLR but uses GD instead of NGD. When adapted to online learning, it starts each time step at $\\psi_{t,0}=\\psi_{t|t-1}$ and iterates with GD: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi_{t,i}=\\psi_{t,i-1}+\\alpha_{t}\\nabla_{\\psi_{t,i-1}}\\Big(\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}\\big[\\log p(y_{t}|f_{t}(\\theta_{t}))\\big]-D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t,[t-1}}\\big)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "E7en5DyO2G/tmp/645a13c02c74f75405e8c78ab739851f0488ac077b0488a4dca57ef9cefce141.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Time complexity of the algorithms. $\\overline{P}$ : params, $C$ : observation dim, $\\overline{{M}}$ : MC samples, $I$ : iterations, $R$ : DLR rank. We assume $P\\gg\\{I,C,R,M\\}$ so display only the terms of leading order in $P$ . Time complexities for MC-HESS algorithms (not shown) are always at least as great as for the corresponding MC-EF. Full (full covariance) and Diag (diagonal covariance) columns indicate natural parameters; corresponding algorithms using moment parameters have the same complexities except BOG-FC_MOM which is $\\bar{O(M\\bar{P}^{2})}$ for MC-EF, $O(C P^{2})$ for LIN-HESS, and $O(P^{2})$ for LIN-EF. [Names] correspond to the following existing methods (or variants thereof) in the literature: RVGA: [Lambert et al., 2021] (explicit update version); VON: [Khan et al., 2018b] (modified for online); SLANG: [Mishkin et al., 2018] (modified for online); BBB: [Blundell et al., 2015] (modified for online and uses moment parameters); CM-EKF: [Ollivier, 2018, Tronarp et al., 2018]; VD-EKF: [Chang et al., 2022]; LO-FI: [Chang et al., 2023]. ", "page_idx": 7}, {"type": "text", "text": "4.7 Variational families and their parameterizations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We investigate five variational families for the posterior distribution: (1) FC Gaussian using natural parameters $\\pmb{\\psi}=(\\pmb{\\Sigma}^{-1}\\pmb{\\mu},-\\frac{1}{2}\\pmb{\\Sigma}^{-1})$ , (2) FC Gaussian using central moment parameters $\\pmb{\\psi}=(\\pmb{\\mu},\\pmb{\\Sigma})$ , (3) diagonal Gaussian using natural parameters $\\pmb{\\psi}=(\\pmb{\\sigma}^{-2}\\pmb{\\mu},-\\frac{1}{2}\\pmb{\\sigma}^{-2})$ (using elementwise exponents and products), (4) diagonal Gaussian using central moment parameters $\\psi=(\\mu,\\sigma^{2})$ , and (5) DLR Gaussian with parameters $\\psi=(\\mu,\\Upsilon,\\bar{\\mathbf{W}})$ and precision $\\dot{\\boldsymbol{\\Sigma}}^{-1}=\\mathbf{\\Delta}\\mathbf{r}+\\mathbf{\\dot{W}}\\mathbf{W}^{\\dagger}$ where $\\mathbf{Y}\\in\\dot{\\mathbb{R}}^{P\\times P}$ is diagonal and $\\mathbf{W}\\in\\mathbb{R}^{P\\times\\dot{R}}$ with $R\\ll P$ . The moment parameterizations are included to test the importance of using natural parameters per Proposition 4.1. The diagonal family allows learning of large models because it scales linearly in the model size $P$ . DLR also scales linearly but is more expressive than diagonal, maintaining some of the correlation information between parameters that is lost in the mean field (diagonal) approximation [Lambert et al., 2023, Mishkin et al., 2018, Chang et al., 2023]. ", "page_idx": 7}, {"type": "text", "text": "Optimizing the BONG objective wrt $(\\pmb{\\mu},\\pmb{\\Upsilon},\\pmb{\\mathrm{W}})$ using NGD methods is challenging because the Fisher information matrix in this parameterization cannot be efficiently inverted. Instead we first derive the update wrt the FC natural parameters (leveraging the fact that the prior $\\boldsymbol{\\Sigma}_{t|t-1}^{-1}$ is DLR to make this efficient), and then use SVD to project the posterior precision back to low-rank form, following our prior LO-FI work [Chang et al., 2023]. However, if we omit the Fisher preconditioning matrix and use GD as in BOG and BBB, we can directly optimize the objective wrt $(\\pmb{\\mu},\\pmb{\\Upsilon},\\pmb{\\ W})$ (see Appendix E.5). ", "page_idx": 7}, {"type": "text", "text": "4.8 Overall space of methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Crossing the four algorithms in Table 2, the four methods of approximating the Hessian in Table 1, and the five variational families yields 80 algorithms. Table 3 shows the 36 based on the three tractable Hessian approximations and the three variational families that use natural parameters. Update equations for all the algorithms are derived in Appendix E. Pseudocode is given in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section presents our primary experimental results. These are based on MNIST $.D\\mathrm{~=~}784$ , $N_{\\mathrm{train}}=60\\mathbf{k}$ , $N_{\\mathrm{test}}=10{\\bf k}$ , $C=10$ classes) [LeCun et al., 2010]. See Appendix B for more details on these experiments, and more results on MNIST and other datasets. We focus on training on a prefix of the first $T=2000$ examples from each dataset, since our main interest is in online learning from potentially nonstationary distributions, where rapid adaptation of a model in response to a small number of new data points is critical. ", "page_idx": 8}, {"type": "text", "text": "Our primary evaluation objective is the negative log predictive density (NLPD) of the test set as a function of the number of training points observed so far.1 It is defined as ${\\mathrm{NLPD}}_{t}\\ =$ $\\begin{array}{r}{-\\frac{1}{N_{\\mathrm{test}}}\\sum_{i\\in\\mathcal{D}^{\\mathrm{test}}}\\log\\left[\\int p(y_{i}|f(\\pmb{x}_{i},\\pmb{\\theta}_{t}))q_{\\psi_{t}}(\\pmb{\\breve{\\theta}}_{t})\\mathrm{d}\\pmb{\\theta}_{t}\\right]}\\end{array}$ . We approximate this integral in two main ways: (1) using Monte Carlo sampling2,or (2) using a plugin approximation, where we replace the posterior $q_{\\psi_{t}}(\\pmb{\\theta}_{t})$ with a delta function centered at the mean, $\\delta(\\bar{\\pmb{\\theta}_{t}}-\\pmb{\\mu}_{t})$ . ", "page_idx": 8}, {"type": "text", "text": "For methods that require a learning rate (i.e., all methods except BONG), we optimize it wrt mid-way or final performance on a holdout validation set, using Bayesian optimization on NLL. All methods require specifying the prior belief state, $p(\\pmb\\theta_{0})\\,=\\,\\bar{\\mathcal{N}}(\\pmb\\mu_{0},\\pmb\\Sigma_{0}\\,=\\,\\bar{\\sigma}_{0}^{2}{\\bf I})$ . We optimize over $\\sigma_{0}^{2}$ and sample $\\pmb{\\mu}_{0}$ from a standard NN initializer. As Hessian approximations, we use MC-EF with $M=100$ samples, as well as the deterministic approximations LIN-HESS and LIN-EF. (In the appendix, we also study MC-HESS but find that it works very poorly, even with $M=1000.$ ) ", "page_idx": 8}, {"type": "text", "text": "In Fig. 1 we compare the 4 main algorithms using DLR family with rank $R=10$ . We apply these to a CNN with two convolutional layers (each with 16 features and a (5,5) kernel), followed by two linear layers (one with 64 features and the final one with 10 features), for a total of 57,722 parameters. Shaded regions in these and all other plots indicate $\\pm1$ SE, based on 5 independent trials randomly varying in the prior mean $\\pmb{\\mu}_{0}$ , data ordering, and MC sampling. From this figure (and additional results in Appendix B) we conclude ", "page_idx": 8}, {"type": "text", "text": "\u2022 Linearization helps: LIN-HESS and LIN-EF both outperform the MC variants.   \n\u2022 NGD helps: BONG outperforms BOG.   \n\u2022 Implicit regularization helps: BONG outperforms BLR.   \n\u2022 The LIN-HESS approximation outperforms LIN-EF, at least for BONG.   \n\u2022 BBB generally does poorly.   \n\u2022 The BONG posterior predictive (using LIN-HESS) is slightly better calibrated than BOG, and both are much better than BLR and BBB, at least for small sample sizes, as shown in Fig. 7.   \n\u2022 The plugin posterior predictive is similar to the Lin-MC predictive (see Footnote 2), and both are generally much better than the simple MC predictive, as shown in Fig. 5. ", "page_idx": 8}, {"type": "text", "text": "In Fig. 2 we compare BONG using different variational families, and conclude \u2022 DLR-10 outperforms DLR-1, which is similar to diagonal (except when using BONG-LIN-EF, where DLR-1 is worse than diagonal). Also, we find (in results not reported here) that rank 5\u201310 often gives results as good as FC, but is much cheaper. \u2022 Both natural and moment parameterizations for the diagonal representation perform comparably to each other, although with LIN-EF, the moment parameterization can be numerically unstable.3 ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Finally, in Fig. 3 we report the runtimes from these experiments and conclude ", "page_idx": 8}, {"type": "text", "text": "\u2022 One-step methods (BONG and BOG) are faster than iterative methods (BLR and BBB), as expected.   \n\u2022 Linearized methods (LIN-HESS and LIN-EF) are faster than MC methods (MC-EF). ", "page_idx": 8}, {"type": "text", "text": "\u2022 EF methods (LIN-EF) are a bit faster than methods that compute the Hessian exactly (LIN-HESS), especially for diagonal family. (This speedup is larger when the output dimensionality $C$ is big.) ", "page_idx": 9}, {"type": "image", "img_path": "E7en5DyO2G/tmp/f1cb571b9259cd5ea3c6c4d666bfe87e137bfc937245f5a272349811144274f5.jpg", "img_caption": ["Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG, BOG, BBB and BLR and the 3 tractable Hessian approximations with DLR-10 variational family. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "E7en5DyO2G/tmp/57afd4bed2668f4594da7f0faca67a74422423425f92e9e1120011473472eefd.jpg", "img_caption": ["Figure 2: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using BONG with different variational families, namely diagonal (natural and moment), DLR-1, DLR10. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusions, limitations and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our experiment results show beneftis of BONG\u2019s three main principles: NGD, implicit regularization to the prior, and linearization. The clear winner across datasets and variational families is BONG-LINHESS, which embodies all three principles. BLR-LIN-HESS nearly matches its performance but is much slower. Several of the best-performing algorithms are previously known (notably CM-EKF and LO-FI) but we explain these results within a systematic theory that also offers new methods (including BLR-LIN-HESS). ", "page_idx": 9}, {"type": "text", "text": "BONG is motivated by Proposition 4.1 which applies only in the idealized setting of a conjugate prior. Nevertheless we find it performs well in non-conjugate settings. On the other hand our experiments are based on relatively small models and datasets. It will be important to test how our methods scale up, especially using the promising DLR representation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Thanks to Gerardo Dur\u00e0n-Mart\u00ecn and Alex Shestopaloff for helpful input. MJ was supported by NSF grant 2020-906. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "S Amari. Natural gradient works efficiently in learning. Neural Comput., 10(2):251\u2013276, 1998. URL http://dx.doi.org/10.1162/089976698300017746.   \nGianluca M Bencomo, Jake C Snell, and Thomas L Grifftihs. Implicit maximum a posteriori flitering via adaptive optimization. arXiv preprint arXiv:2311.10580, 2023.   \nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. In ICML, 2015. URL http://arxiv.org/abs/1505.05424.   \nGeorges Bonnet. Transformations des signaux al\u00e9atoires a travers les systemes non lin\u00e9aires sans m\u00e9moire. In Annales des T\u00e9l\u00e9communications, volume 19, pages 203\u2013220. Springer, 1964.   \nPeter G Chang, Kevin Patrick Murphy, and Matt Jones. On diagonal approximations to the extended kalman filter for online training of bayesian neural networks. In Continual Lifelong Learning Workshop at ACML 2022, December 2022. URL https://openreview.net/forum? id $=$ asgeEt25kk.   \nPeter G Chang, Gerardo Dur\u00e1n-Mart\u00edn, Alexander Y Shestopaloff, Matt Jones, and Kevin Murphy. Low-rank extended Kalman filtering for online learning of neural networks from streaming data. In COLLAS, May 2023. URL http://arxiv.org/abs/2305.19535.   \nBadr-Eddine Ch\u00e9rief-Abdellatif, Pierre Alquier, and Mohammad Emtiyaz Khan. A generalization bound for online variational inference. In Asian conference on machine learning, pages 662\u2013677. PMLR, 2019.   \nGerardo Duran-Martin, Matias Altamirano, Alexander Y Shestopaloff, Leandro S\u00e1nchez-Betancourt, Jeremias Knoblauch, Matt Jones, Fran\u00e7ois-Xavier Briol, and Kevin Murphy. Outlier-robust kalman filtering through generalised bayes. arXiv preprint arXiv:2405.05646, 2024.   \nJo\u00e3o Gama, Raquel Sebasti\u00e3o, and Pedro Pereira Rodrigues. On evaluating stream learning algorithms. MLJ, 90(3):317\u2013346, March 2013. URL https://tinyurl.com/mrxfk4ww.   \nNathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2): 217\u2013288, 2011.   \nDirk Hoeven, Tim Erven, and Wojciech Kot\u0142owski. The many faces of exponential weights in online learning. In Conference On Learning Theory, pages 2067\u20132092. PMLR, 2018.   \nMichael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u20131076, 1989.   \nAlexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural nets via local linearization. In Arindam Banerjee and Kenji Fukumizu, editors, AISTATS, volume 130 of Proceedings of Machine Learning Research, pages 703\u2013711. PMLR, 2021a. URL https: //proceedings.mlr.press/v130/immer21a.html.   \nAlexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural nets via local linearization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pages 703\u2013711. PMLR, 2021b. URL https://proceedings.mlr. press/v130/immer21a.html.   \nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37:183\u2013233, 1999.   \nMohammad Khan and Wu Lin. Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. In Artificial Intelligence and Statistics, pages 878\u2013887. PMLR, 2017.   \nMohammad Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable bayesian deep learning by weight-perturbation in adam. In International conference on machine learning, pages 2611\u20132620. PMLR, 2018a.   \nMohammad Emtiyaz Khan and H\u00e5vard Rue. The bayesian learning rule. J. Mach. Learn. Res., 2023. URL http://arxiv.org/abs/2107.04562.   \nMohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable bayesian deep learning by Weight-Perturbation in adam. In ICML, 2018b. URL http://arxiv.org/abs/1806.04854.   \nJeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An optimization-centric view on bayes\u2019 rule: Reviewing and generalizing variational inference. Journal of Machine Learning Research, 23 (132):1\u2013109, 2022.   \nFrederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical fisher approximation for natural gradient descent, 2020.   \nRichard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, and Stephan G\u00fcnnemann. Continual learning with bayesian neural networks for non-stationary data. In ICLR, March 2020. URL https://openreview.net/forum?id $=$ SJlsFpVtDB.   \nMarc Lambert, Silv\u00e8re Bonnabel, and Francis Bach. The recursive variational gaussian approximation (R-VGA). Stat. Comput., 32(1):10, December 2021. URL https://hal.inria.fr/ hal-03086627/document.   \nMarc Lambert, Silv\u00e8re Bonnabel, and Francis Bach. The limited-memory recursive variational gaussian approximation (l-rvga). Statistics and Computing, 33(3):70, 2023.   \nYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.   \nWu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E Turner, and Alireza Makhzani. Can we remove the square-root in adaptive gradient methods? a second-order perspective. arXiv preprint arXiv:2402.03496, 2024.   \nNick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and computation, 108(2):212\u2013261, 1994.   \nYueming Lyu and Ivor W Tsang. Black-box optimizer with stochastic implicit natural gradient. In Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13\u201317, 2021, Proceedings, Part III 21, pages 217\u2013232. Springer, 2021.   \nJames Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1\u201376, 2020.   \nAaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and Mohammad Emtiyaz Khan. SLANG: Fast structured covariance approximations for bayesian deep learning with natural gradient. In NIPS, pages 6245\u20136255. Curran Associates, Inc., 2018.   \nYann Ollivier. Online natural gradient as a kalman filter. Electron. J. Stat., 12(2):2930\u20132961, 2018. URL https://projecteuclid.org/euclid.ejs/1537257630.   \nRobert Price. A useful theorem for nonlinear devices having gaussian inputs. IRE Transactions on Information Theory, 4(2):69\u201372, 1958.   \nG V Puskorius and L A Feldkamp. Decoupled extended kalman fliter training of feedforward layered networks. In International Joint Conference on Neural Networks, volume i, pages 771\u2013777 vol.1, 1991. URL http://dx.doi.org/10.1109/IJCNN.1991.155276.   \nCarl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.   \nSimo Sarkka and Lennart Svensson. Bayesian Filtering and Smoothing (2nd edition). Cambridge University Press, 2023.   \nYuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, and Thomas M\u00f6llenhoff. Variational learning is effective for large deep networks. arXiv preprint arXiv:2402.17641, 2024.   \nSharad Singhal and Lance Wu. Training multilayer perceptrons with the extended kalman algorithm. In NIPS, volume 1, 1989.   \nMichalis K Titsias, Alexandre Galashov, Amal Rannen-Triki, Razvan Pascanu, Yee Whye Teh, and Jorg Bornschein. Kalman filter for online classification of non-stationary data. In ICLR, 2024.   \nMarcin B Tomczak, Siddharth Swaroop, and Richard E Turner. Efficient low rank gaussian variational inference for neural networks. In NIPS, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/310cc7ca5a76a446f85c1a0d641ba96d-Paper.pdf.   \nFilip Tronarp, \u00c1ngel F Garc\u00eda-Fern\u00e1ndez, and Simo S\u00e4rkk\u00e4. Iterative filtering and smoothing in nonlinear and Non-Gaussian systems using conditional moments. IEEE Signal Process. Lett., 25 (3):408\u2013412, 2018. URL https://acris.aalto.fi/ws/portalfiles/portal/17669270/ cm_parapub.pdf.   \nZhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W Mahoney. ADAHESSIAN: An adaptive second order optimizer for machine learning. In AAAI, 2021. URL http://arxiv.org/abs/2006.00719.   \nArnold Zellner. Optimal information processing and bayes\u2019s theorem. The American Statistician, 42 (4):278\u2013280, 1988.   \nWenxuan Zhang, Youssef Mohamed, Bernard Ghanem, Philip Torr, Adel Bibi, and Mohamed Elhoseiny. Continual learning on a diet: Learning from sparsely labeled streams under constrained computation. In ICLR, 2024. URL https://openreview.net/pdf?id $\\cdot$ Xvfz8NHmCj. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Abstract pseudocode ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithms 1 to 7 give pseudocode for applying the methods we study. For the predict step, we assume the dynamics model has the form $\\pmb{\\theta}_{t}\\sim\\mathcal{N}(\\mathbf{F}_{t}\\pmb{\\theta}_{t-1}+\\pmb{b}_{t},\\mathbf{Q}_{t})$ . The update step in Algorithm 3 calls one of four grad functions (Algorithms 4 to 7) that estimate the expected gradient and Hessian using either MC or linearization combined with either the direct Hessian (or Jacobian and observation covariance) or EF. The update step also calls an inner step function that implements BONG, BLR, BOG or BBB on some variational family corresponding to the encoding of $\\psi$ (not shown). In practice the grad-fn and inner-step-fn are not as cleanly separated because the full matrix $\\bar{\\mathbf{G}}_{t,i}$ is not passed between them except when the variational family is FC. When the family is diagonal, grad-fn only needs to return diag( G\u00aft,i). When grad-fn uses EF, it only needs to return G\u02c6t(,1i: (grad-MC-EF) or ${\\pmb g}_{t,i}^{\\mathrm{LIN}}$ (grad-LIN-EF) and inner-step-fn will implicitly use the outer product of this output as $\\bar{\\bf G}_{t,1}$ . Finally, note the expressions for ${\\pmb g}_{t,i}^{\\mathrm{LIN}}$ in Algorithms 6 and 7 are equivalent ways of computing the same quantity, as explained after Eq. (28). ", "page_idx": 13}, {"type": "text", "text": "for $t=1:\\infty$ do $\\begin{array}{r l}&{\\mathbf{\\boldsymbol{\\mathsf{I}}}\\mathbf{\\boldsymbol{\\mathsf{\\rho}}}_{l}=\\mathbf{\\boldsymbol{\\mathsf{1}}}:\\infty\\,\\mathbf{\\boldsymbol{\\mathsf{u}}}\\mathbf{\\boldsymbol{\\mathsf{0}}}}\\\\ &{\\quad\\mathbf{\\boldsymbol{\\psi}}_{t\\mid t-1}=\\mathrm{predict}(\\mathbf{\\boldsymbol{\\psi}}_{t-1})}\\\\ &{\\quad\\boldsymbol{\\psi}_{t}=\\mathbf{\\boldsymbol{\\mathsf{u}}}\\mathrm{pdate}(\\mathbf{\\boldsymbol{\\psi}}_{t\\mid t-1},\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{y}}_{t})}\\end{array}$   \nend ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1: Main loop. ", "page_idx": 13}, {"type": "text", "text": "def predict $(\\psi_{t-1}=(\\mu_{t-1},\\boldsymbol{\\Sigma}_{t-1}))$ : $\\pmb{\\mu}_{t\\mid t-1}=\\mathbf{F}_{t}\\pmb{\\mu}_{t-1}+\\pmb{b}_{t}$ $\\pmb{\\Sigma}_{t|t-1}=\\mathbf{F}_{t}\\pmb{\\Sigma}_{t-1}\\mathbf{F}_{t}^{\\intercal}+\\mathbf{Q}_{t}$ Return $\\psi_{t|t-1}=(\\pmb{\\mu}_{t|t-1},\\pmb{\\Sigma}_{t|t-1})$ ", "page_idx": 13}, {"type": "text", "text": "Algorithm 2: Predict step. ", "page_idx": 13}, {"type": "text", "text": "def update $(\\psi_{t|t-1},\\pmb{x}_{t},\\pmb{y}_{t},f(),\\pmb{A}()$ , grad-fn, inner-step-fn, $\\alpha,I,M)$ :   \n$\\psi_{t,0}=\\psi_{t|t-1}$   \n$f_{t}(\\pmb\\theta_{t})=f(\\pmb x_{t},\\pmb\\theta_{t})$   \n$h_{t}(\\pmb\\theta_{t})=E[\\pmb y_{t}|\\pmb x_{t},\\pmb\\theta_{t}]=\\nabla_{\\pmb\\eta=f_{t}(\\pmb\\theta_{t})}A(\\pmb\\eta)$   \n$\\mathbf{V}_{t}(\\pmb{\\theta}_{t})=\\operatorname{Cov}[y_{t}|x_{t},\\pmb{\\theta}_{t}]=\\nabla_{\\pmb{\\eta}=f_{t}(\\pmb{\\theta}_{t})}^{2}A(\\pmb{\\eta})$   \n$\\ell_{t}(\\pmb\\theta_{t})=\\log p(\\pmb y_{t}|\\pmb x_{t},\\pmb\\theta_{t})=\\log p(\\pmb y_{t}|\\pmb f_{t}(\\pmb\\theta_{t}))$   \nfor $i=1:I$ do $(\\bar{g}_{t,i},\\bar{\\mathbf{G}}_{t,i})=\\operatorname{grad-fn}(\\psi_{t,i-1},\\ell_{t},h_{t},\\mathbf{V}_{t},M)$ $\\boldsymbol{\\psi}_{t,i}=\\mathrm{inner-step-fn}(\\boldsymbol{\\psi}_{t|t-1},\\boldsymbol{\\psi}_{t,i-1},\\bar{\\boldsymbol{g}}_{t,i},\\bar{\\mathbf{G}}_{t,i},\\alpha)$   \nend ", "page_idx": 13}, {"type": "text", "text": "Return \u03c8t,I Algorithm 3: Update step. The inner-step-fn is BONG, BLR, BOG or BBB (not shown). ", "page_idx": 13}, {"type": "text", "text": "B Additional experiment results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we give a more thorough set of experimental results. ", "page_idx": 13}, {"type": "text", "text": "B.1 Running time measures ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The running times of the methods for the experiments in Figs. 1 and 2, where we fti a CNN to MNIST, are shown in Fig. 3. ", "page_idx": 13}, {"type": "text", "text": "The running times of the methods for the FC and DLR case, where we fit an MLP to a synthetic regression dataset, are shown in Fig. 4. The slower speed of BLR (even with $I\\,=\\,1$ ) relative to BONG is at least partly attributable to the fact that BLR must compute the SVD of a larger matrix (see Appendices E.5.3 and E.5.4). ", "page_idx": 13}, {"type": "text", "text": "def grad-MC-HESS $(\\psi_{t,i-1},\\ell_{t},h_{t}=[],\\mathbf{V}_{t}=[],M)$ :   \nfor $m=1:M$ do $\\begin{array}{r l}&{\\hat{\\pmb{\\theta}}_{t,i}^{(m)}\\sim q_{\\psi_{t,i-1}}(\\pmb{\\theta})}\\\\ &{\\hat{\\pmb{g}}_{t,i}^{(m)}=\\nabla_{\\pmb{\\theta}_{t}=\\hat{\\pmb{\\theta}}_{t,i}^{(m)}}\\ell_{t}(\\pmb{\\theta}_{t})}\\\\ &{\\hat{\\mathbf{G}}_{t,i}^{(m)}=\\nabla_{\\pmb{\\theta}_{t}=\\hat{\\pmb{\\theta}}_{t,i}^{(m)}}^{2}\\ell_{t}(\\pmb{\\theta}_{t})}\\end{array}$   \nend 1 M (m)   \ngt,i M m=1 g\u02c6t,i   \n$\\begin{array}{r}{\\mathbf{G}_{t,i}^{\\mathrm{MC-HESS}}=\\frac{1}{M}\\sum_{m=1}^{M}\\hat{\\mathbf{G}}_{t,i}^{(m)}}\\end{array}$   \nReturn (gMC , $(\\mathbf{\\boldsymbol{g}}_{t,i}^{\\mathrm{MC}},\\mathbf{\\boldsymbol{G}}_{t,i}^{\\mathrm{MC-HESS}})$ ", "page_idx": 14}, {"type": "text", "text": "Algorithm 4: MC gradient/Hessian estimator ", "page_idx": 14}, {"type": "text", "text": "def grad-MC- $\\mathrm{EF}(\\psi_{t,i-1},\\ell_{t},h_{t}=[],\\mathbf{V}_{t}=[],M)$ :   \nfor $m=1:M$ do \u03b8\u02c6(m) $\\hat{\\pmb{\\theta}}_{t,i}^{(m)}\\sim q_{\\pmb{\\psi}_{t,i-1}}(\\pmb{\\theta})$ $\\hat{\\pmb g}_{t,i}^{(m)}=\\nabla_{\\pmb\\theta_{t}=\\hat{\\pmb\\theta}_{t,i}^{(m)}}\\ell_{t}(\\pmb\\theta_{t})$   \nend   \n$\\begin{array}{r l}&{\\pmb{g}_{t,i}^{\\mathrm{MC}}=\\frac{1}{M}\\sum_{m=1}^{M}\\hat{\\pmb{g}}_{t,i}^{(m)}}\\\\ &{\\hat{\\mathbf{G}}_{t,i}^{(1:M)}=[\\hat{\\pmb{g}}_{t,i}^{(1)},\\dots,\\hat{\\pmb{g}}_{t,i}^{(M)}]}\\\\ &{\\mathbf{G}_{t,i}^{\\mathrm{MC-EF}}=-\\frac{1}{M}\\hat{\\mathbf{G}}_{t,i}^{(1:M)}\\hat{\\mathbf{G}}_{t,i}^{(1:M)^{\\intercal}}}\\end{array}$   \nReturn (gtM,iC , $(g_{t,i}^{\\mathrm{MC}},\\mathbf{G}_{t,i}^{\\mathrm{MC-EF}})$ ", "page_idx": 14}, {"type": "text", "text": "Algorithm 5: MC gradient/Hessian estimator with Empirical Fisher ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{def~grad-LIN-HESS}(\\psi_{t,i-1},\\ell_{t}=[],h_{t},\\mathbf{V}_{t},M=[]);}\\\\ &{\\mu_{t,i-1}=E[\\theta_{t}|\\psi_{t,i-1}]}\\\\ &{\\hat{\\eta}_{t,i}=h_{t}(\\mu_{t,i-1})}\\\\ &{\\mathbf{H}_{t,i}=\\frac{\\partial h_{t}}{\\partial\\theta_{t}}|\\theta\\mathrm{=}\\mu_{t,i-1}}\\\\ &{\\mathbf{R}_{t,i}=\\mathbf{V}_{t}(\\mu_{t,i-1})}\\\\ &{g_{t,i}^{\\mathrm{LN}}=\\mathbf{H}_{t,i}^{\\top}\\mathbf{R}_{t,i}^{-1}(y_{t}-\\hat{y}_{t,i})}\\\\ &{\\mathbf{G}_{t,i}^{\\mathrm{LN-HESS}}=-\\mathbf{H}_{t,i}^{\\top}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 6: Linearized gradient/Hessian estimator ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{def}\\mathrm{~grad-LIN-EF}(\\psi_{t,i-1},\\ell_{t}=\\left[\\right],h_{t},\\mathbf{V}_{t},M=\\left[\\right]);}\\\\ &{\\mu_{t,i-1}=E[\\pmb{\\theta}_{t}|\\psi_{t,i-1}]}\\\\ &{\\mathbf{R}_{t,i}=\\mathbf{V}_{t}(\\mu_{t,i-1})}\\\\ &{\\pmb{g}_{t,i}^{\\mathrm{LIN}}=\\nabla_{\\pmb{\\theta}_{t}=\\pmb{\\mu}_{t,i-1}}\\left[-\\frac{1}{2}\\left(\\pmb{y}_{t}-h_{t}(\\pmb{\\theta}_{t})\\right)^{\\intercal}\\mathbf{R}_{t,i}^{-1}(\\pmb{y}_{t}-h_{t}(\\pmb{\\theta}_{t}))\\right]}\\\\ &{\\mathbf{G}_{t,i}^{\\mathrm{LIN-EF}}=-\\pmb{g}_{t,i}^{\\mathrm{LIN}}\\left(\\pmb{g}_{t,i}^{\\mathrm{LIN}}\\right)^{\\intercal}}\\\\ &{\\mathbf{r}_{i}\\qquad,\\quad,\\mathrm{~tran~}\\pmb{\\omega}_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 7: Linearized gradient/Hessian estimator with empirical Fisher ", "page_idx": 14}, {"type": "image", "img_path": "E7en5DyO2G/tmp/9bd83e33d1182d28bc7cd970638a13d7c96a4ded9cfb037cb6e207c8a635765c.jpg", "img_caption": ["Figure 3: Runtimes for methods on MNIST. Left: Corresponding to Fig. 1 using different algorithms on DLR-10 family. Right: Corresponding to Fig. 2, using BONG on different variational families. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "E7en5DyO2G/tmp/8a84d3ae1e7be6d7949274d165702bfcb0370cfe26130f7484766f02ca8484c3.jpg", "img_caption": ["Figure 4: Running time (seconds) vs number of parameters $P$ (size of state space) on a synthetic regression problem. For BBB and BLR, we show results using $I\\,=\\,1$ and $I\\,=\\,10$ iterations per step. Hessian approximations are denoted as follows: $\\mathrm{EF}0{-}\\mathrm{Lin}0=\\mathbf{M}\\mathbf{C}.$ -Hess, EF1-Lin $0=$ EF-Hess, EF0-Lin1 $=$ Lin-Hess. (a) Full Covariance representation. (b) DLR representation. The BLR plot is truncated due to out of memory problem. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 Detailed results for CNN on MNIST ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we report further metrics for the experiments in Figs. 1 and 2. We show 3 approximations to the NLPD: plugin, MC, and Linearized MC.4 For each of these approximations to the posterior predictive, we also measure the corresponding misclassification rate based on picking the most probable predicted class. Results are shown in Figs. 5 and 6. We see that the plugin and lin-MC approximations are similar, and both are generally much better than standard MC. ", "page_idx": 15}, {"type": "text", "text": "Finally, in Fig. 7 and Fig. 8, we report the test-set expected calibration error (ECE) at time steps [250, 500, 1,000, 2,000], computed using 20 bins. Note that among the LIN-HESS variants, BONG-DLR-10 method is the most well-calibrated (in addition to exhibiting the strongest plugin and linearized-MC NLPD results), when compared to other DLR-10 methods as well as other BONG variants. ", "page_idx": 15}, {"type": "image", "img_path": "E7en5DyO2G/tmp/87b03b1acdafddd9b47bedcb9331ed3d756d1351685cf9799b7ebfa2b2a65888.jpg", "img_caption": ["Figure 5: MNIST results for methods using DLR family. Left column shows misclassification rate, right column showns NLL. First row uses plugin approximation to the posterior predictive, second row uses linearized MC approximation, and third row uses standard MC approximation. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "E7en5DyO2G/tmp/31cd6c9b17fbc8ae9de205dea6dda69de6465bbcc60f6c1de3253f46df079af8.jpg", "img_caption": ["Figure 6: MNIST results for BONG variants. Left column shows misclassification rate, right column shows NLL. First row uses plugin approximation to the posterior predictive, second row uses linearized MC approximation, and third row uses standard MC approximation. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "E7en5DyO2G/tmp/e50d429e36f7e85873ae8543051a1b90105cc73b30a845ec26a7012354da2336.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "E7en5DyO2G/tmp/8f713fa34957340f5b51ce8583721b5f1aa6e738e63d4afca6e3d195cce585ea.jpg", "img_caption": ["Figure 7: MNIST expected calibration error (ECE) results at selected timesteps for methods using DLR family. ", "Figure 8: MNIST expected calibration error (ECE) results at selected timesteps for BONG variants. We see that BONG and BOG are both well calibrated, as least when combined with LIN-HESS, with BONG have a slight edge, especially for small sample sizes, where there is more posterior uncertainty. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3 SARCOS dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In addition to MNIST, we report experiments on the SARCOS regression dataset $D=22$ , $N_{\\mathrm{train}}=$ 44,484, $N_{\\mathrm{test}}=4449$ , $C=1$ ). This dataset derives from an inverse dynamics problem for a seven degrees-of-freedom SARCOS anthropomorphic robot arm. The task is to map from a 21-dimensional input space (7 joint positions, 7 joint velocities, 7 joint accelerations) to the corresponding 7 joint torques. Following Rasmussen and Williams [2006], we pick a single target output dimension, so $C=1$ . The data is from https://gaussianprocess.org/gpml/data/. ", "page_idx": 18}, {"type": "text", "text": "We use a small MLP of size 21-20-20-1, so there are $P=881$ parameters. For optimizing learning rates for SARCOS, we use grid search on NLPD-PI. We fix the variance of the prior belief state to $\\sigma_{0}^{2}=1.0$ , which represents a mild degree of regularization.5 We fix the observation variance to $R_{t}\\,=\\,0.1\\hat{R}$ , where $\\hat{R}=\\mathrm{Var}(y_{1:T})$ is the maximum likelihood estimate based on the training sequence; we can think of this as a simple empirical Bayes approximation, and the factor of 0.1 accounts for the fact that the variance of the residuals from the learned model will be smaller than from the unconditional baseline. We focus on DLR approximation of rank 10. This gives similar results to full covariance, but is much faster to compute. We also focus on the plugin approximation to NLPD, since the MC approximation gives much worse results (not shown). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "B.3.1 Comparison of BONG, BLR, BBB and BOG ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Fig. 9 we show the results of using the LIN-HESS approximation. For 1 iteration per step, we see that BONG and BLR are indistinguishable in performance, and BBB and BOG are also indistinguishable, but much worse. For 10 iterations per step, we see that BBB improves significantly, and approaches BONG and BLR. However, BLR and BBB are now about 10 times slower. (In practice, the slowdown is less than 10, due to constant factors of the implementation.) (Note that BONG and BOG always use a single iteration, so their performance does not change.) ", "page_idx": 19}, {"type": "text", "text": "In Fig. 10 we show the results of using the MC-EF approximation with $\\mathbf{M}\\mathbf{C}=100$ samples. The trends are similar to the LIN-HESS case. In particular, for $I=1$ , BONG and BLR are similar, with BONG having a slight edge; and for $I=10$ , BBB catches up with both BONG and BLR, with BOG always in last place. Finally, we see that the performance of MC-EF is slightly worse than LIN-HESS when $I=1$ , but catches up with $I=10$ . However, in larger scale experiments, we usually find that LIN-HESS is significantly better than MC-EF, even with $I=10$ . ", "page_idx": 19}, {"type": "image", "img_path": "E7en5DyO2G/tmp/0f375acbcb6b2936e50fe1e8560096d45bfdb4afa449bcb729aa89e2ace5bda9.jpg", "img_caption": ["Figure 9: Predictive performance on SARCOS using MLP 21-20-20-1 with DLR rank 10. Error bars represent $\\pm1$ standard deviation computed from 3 random trials, randomizing over data order and initial state $\\pmb{\\mu}_{0}$ . (a) We show all 4 algorithms combined with LIN-HESS approximation and $I=1$ . (b) Same as (a) but with $I=10$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.3.2 Learning rate sensitivity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Fig. 11 we show the test set performance for BLR (with LIN-HESS approximation) for 5 different learning rates (namely $5\\times10^{-3}$ , $1\\times10^{-2}$ , $5\\times10^{-2}$ , $1\\times10^{-1}$ , and $5\\times10^{-1}$ ). ", "page_idx": 19}, {"type": "text", "text": "When using 1 iteration per step, the best learning rate is $\\alpha=0.5$ , which is also the value chosen based on validation set performance. With this value, BLR matches BONG. For other learning rates, BLR performance is much worse. When using 10 iterations per step, there are several learning rates all of which give performance as good as BONG. ", "page_idx": 19}, {"type": "image", "img_path": "E7en5DyO2G/tmp/17e1d6d53eaf17b18baa00e15f54f4355f6dac619574cd6edcf13cac24619165.jpg", "img_caption": ["Figure 10: Same as Fig. 9 except we use MC-EF approximation with $\\mathbf{M}\\mathbf{C}=100$ ", "(a) 1 iteration. ", "(b) 10 iterations. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "E7en5DyO2G/tmp/7ff056713fcdcb9dbb1d99033ad6addc4369613b4fcd3750c211be33d49c4d5e.jpg", "img_caption": ["(a) 1 iteration. ", "(b) 10 iterations. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 11: Same setup as Fig. 9, except now we plot performance for BLR for 5 different learning rates. We also show BONG as a baseline, which uses a fixed learning rate step size of 1.0. ", "page_idx": 20}, {"type": "image", "img_path": "E7en5DyO2G/tmp/c81c0551db164620b11318598f6d1ea8522a9ecaba7c17bc2adfa0f05fc3cbf6.jpg", "img_caption": ["Figure 12: Same setup as Fig. 9, except now we plot performance for BBB for 5 different learning rates. We also show BONG as a baseline. ", "(a) 1 iteration. ", "(b) 10 iterations. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In Fig. 12, we show the analogous plot for BBB. When using 1 iteration per step, all learning rates result in poor performance, with many resulting in NaNs. When using 10 iterations per step, there are some learning rates that enable BBB to get close to (but still not match) the performance of BONG. ", "page_idx": 20}, {"type": "image", "img_path": "E7en5DyO2G/tmp/e9e6b4e30e8cc1f434e650d494449fba6742157363075cf90501f7908286d553.jpg", "img_caption": ["Figure 13: We plot performance for BOG for 5 different learning rates. We also show BONG as a baseline. (a) LIN-HESS approximation. (b) MC-EF approximation. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "E7en5DyO2G/tmp/e034fa5486c3d982d8f305bd6de03d0aeda9f963e572cd080b7e91351b5fb3d1.jpg", "img_caption": ["(b) BOG with MC-EF. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Finally, in Fig. 13a, we show the analogous plot for BOG with LIN-HESS, and in Fig. 13b with MC-EF, where results are much worse. ", "page_idx": 21}, {"type": "text", "text": "Overall we conclude that all the methods (except BONG) are quite sensitive to the learning rate. In our experiments, we pick a value based on performance on a validation set, but in the truly online setting, where there is just a single data stream, picking an optimal learning rate is difficult, which is an additional advantage of BONG. ", "page_idx": 21}, {"type": "text", "text": "C Proof of Propositions 4.1 and 4.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition 4.1. To ease notation we write the natural parameters of the prior as $\\begin{array}{r l}{\\psi_{t|t-1}}&{{}=}\\end{array}$ $[{\\boldsymbol{\\chi}}_{t\\vert t-1};{\\boldsymbol{\\nu}}_{t\\vert t-1}]$ , which can be interpreted as the prior sufficient statistics and prior sample size. Note that $\\pmb{x}_{t}$ can be omitted as a constant. Based on the prior $q_{\\psi_{t|t-1}}(\\pmb{\\theta}_{t})$ the exact posterior is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\theta_{t}|\\mathcal{D}_{t})\\propto q_{\\psi_{t|t-1}}(\\theta_{t})\\,p_{t}(y_{t}|\\theta_{t})}\\\\ &{\\qquad\\quad\\propto\\exp\\left(\\mathcal{X}_{t|t-1}^{\\intercal}\\theta_{t}-\\nu_{t|t-1}A(\\theta_{t})\\right)\\exp\\left(\\theta_{t}^{\\intercal}y_{t}-A(\\theta_{t})\\right)}\\\\ &{\\qquad\\quad\\propto q_{\\psi_{t}}(\\theta_{t})}\\\\ &{\\qquad\\psi_{t}=\\left[\\begin{array}{l}{\\chi_{t|t-1}+y_{t}}\\\\ {\\nu_{t|t-1}+1}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For BONG, we first note the dual parameter is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\rho}_{t|t-1}=\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\pmb{\\psi}_{t|t-1}}}[T(\\pmb{\\theta}_{t})]}\\\\ {=\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\pmb{\\psi}_{t|t-1}}}\\Big[\\qquad\\pmb{\\theta}_{t}\\qquad\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore the natural gradient in Eq. (5) is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t|t-1}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t|t-1}}}\\big[\\log p\\left(y_{t}|\\theta_{t}\\right)\\big]=\\nabla_{\\rho_{t|t-1}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t|t-1}}}\\big[\\theta_{t}^{\\top}y_{t}-A\\left(\\theta_{t}\\right)\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\nabla_{\\rho_{t|t-1}}\\rho_{t|t-1}^{\\top}\\left[\\begin{array}{c}{y_{t}}\\\\ {1}\\end{array}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left[\\begin{array}{c}{y_{t}}\\\\ {1}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore the BONG update yields $\\psi_{t}=[\\chi_{t|t-1}+\\pmb{y}_{t};\\nu_{t|t-1}+1]$ in agreement with Eq. (38). ", "page_idx": 21}, {"type": "text", "text": "Proposition 4.2. The intuition behind this proof is as follows. For the mean update in Eq. (9) $\\nabla_{\\theta_{t}}\\ell_{t}$ is linear in $\\pmb{\\theta}_{t}$ so the expectation equals the value at the mean. For the covariance update in Eq. (10) ", "page_idx": 21}, {"type": "text", "text": "$\\nabla_{\\theta_{t}}^{2}\\ell_{t}$ is independent of $\\pmb{\\theta}_{t}$ so we can drop the expectation operator. The tricky part is why we need different linearizations for the Hessians to agree. It has to do with making the Hessian of the NN disappear (as in GGN). In the Gaussian approximation this happens when the predicted mean $(\\hat{\\pmb{y}}_{t}=f_{t}(\\pmb{\\theta}_{t}))$ is linear in $\\pmb{\\theta}_{t}$ . In the plugin approximation it happens when the outcome-dependent part of the loglikelihood $(h_{t}(\\pmb{\\theta}_{t})\\top\\pmb{y}_{t})$ is linear in $\\theta_{t}$ . In the latter case the only nonlinear term remaining in the log-likelihood is the log-partition $A$ , and the two methods end up agreeing because of the property that the Hessian of the log-partition equals the conditional variance $\\mathbf{R}_{t}$ . ", "page_idx": 22}, {"type": "text", "text": "Formally, under the linear $(h)$ -Gaussian approximation in Eqs. (14) and (17) the expected gradient and Hessian can be calculated directly: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t}\\mid t-1}}\\left[\\nabla_{\\theta_{t}}\\log\\bar{p}_{t}^{\\mathrm{LG}}(y_{t}|\\theta_{t}))\\right]=\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t}\\mid t-1}}\\left[\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}(y_{t}-\\hat{y}_{t}-\\mathbf{H}_{t}(\\theta_{t}-\\mu_{t\\mid t-1})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}(y_{t}-\\hat{y}_{t})}\\\\ &{\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t}\\mid t-1}}\\left[\\nabla_{\\theta_{t}}^{2}\\log\\bar{p}_{t}^{\\mathrm{LG}}(y_{t}|\\theta_{t})\\right]=\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t}\\mid t-1}}\\left[-\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the linear $(f)$ -delta approximation we use the properties of exponential families that (1) the gradient of the log-partition $A$ with respect to the natural parameter $f_{t}(\\pmb\\theta_{t})$ equals the expectation parameter $h_{t}(\\pmb\\theta_{t})$ , (2) the Hessian of the log-partition with respect to the natural parameter equals the conditional variance, and consequently (3) the Jacobian of the expectation parameter with respect to the natural parameter equals the conditional variance ${\\bf R}_{t}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\eta=f_{t}(\\mu_{t\\vert t-1})}A(\\eta)=h_{t}(\\mu_{t\\vert t-1})=\\hat{y}}\\\\ &{\\nabla_{\\eta=f_{t}(\\mu_{t\\vert t-1})}^{2}A(\\eta)=\\mathbb{V}\\left[y_{t}\\vert\\theta_{t}=\\mu_{t\\vert t-1}\\right]=\\mathbf{R}_{t}}\\\\ &{\\frac{\\partial h_{t}\\left(\\pmb{\\theta}_{t}\\right)}{\\partial f_{t}\\left(\\pmb{\\theta}_{t}\\right)}_{\\vert\\pmb{\\theta}_{t}=\\pmb{\\mu}_{t\\vert t-1}}=\\mathbf{R}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last of these implies $\\mathbf{F}_{t}\\,=\\,\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}$ . Therefore the expected gradient and Hessian can be calculated as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\theta_{t}\\sim\\delta_{\\theta_{t}[t-1}}\\left[\\nabla_{\\theta_{t}}\\log\\bar{p}_{t}^{\\mathrm{LD}}(y_{t}|\\theta_{t})\\right]=\\nabla_{\\theta_{t}=\\mu_{t+1}-1}\\log\\bar{p}_{t}^{\\mathrm{LD}}(y_{t}|\\theta_{t})}&{}\\\\ &{\\quad=\\mathbf{F}_{t}^{\\top}y_{t}-\\mathbf{F}_{t}^{\\top}\\nabla_{\\eta=f_{t}(\\mu_{t+1}-1)}A(\\eta)}\\\\ &{\\quad=\\mathbf{F}_{t}^{\\top}(y_{t}-\\hat{y}_{t})}\\\\ &{\\quad=\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}(y_{t}-\\hat{y}_{t})}\\\\ {\\mathbb{E}_{\\theta_{t}\\sim\\delta_{\\theta_{t}[t-1}}\\left[\\nabla_{\\theta_{t}}^{2}\\log\\bar{p}_{t}^{\\mathrm{LD}}(y_{t}|\\theta_{t})\\right]=\\nabla_{\\theta_{t}=\\mu_{t+1}}^{2}\\log\\bar{p}_{t}^{\\mathrm{LD}}(y_{t}|\\theta_{t})}&{}\\\\ &{\\quad=-\\mathbf{F}_{t}^{\\top}\\left(\\nabla_{\\eta=f_{t}(\\mu_{t+1}-1)}^{2}A(\\eta)\\right)\\mathbf{F}_{t}}\\\\ &{\\quad=-\\mathbf{F}_{t}^{\\top}\\mathbf{R}_{t}\\mathrm{F}_{t}}\\\\ &{\\quad=-\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D Mirror descent formulation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we give a more detailed derivation of BONG as mirror descent and use this to give two alternative interpretations of how BONG approximates exact VB: (1) by approximating the expected NLL as linear in the expectation parameter $\\rho$ , or (2) by replacing an implicit update with an explicit one. ", "page_idx": 22}, {"type": "text", "text": "Assume the variational family is an exponential one as introduced at the end of Section 3, with natural and dual parameters $\\psi$ and $\\rho$ , sufficient statistics $T(\\theta)$ , and log-partition $\\Phi(\\psi)$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{q_{\\psi}(\\pmb\\theta)=\\exp\\left(\\psi^{\\intercal}T(\\pmb\\theta)-\\Phi(\\psi)\\right)}}\\\\ {{\\pmb\\rho=\\mathbb{E}_{\\pmb\\theta\\sim q_{\\psi}}[T(\\pmb\\theta)]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We first review how NGD on an exponential family is a special case of mirror descent [Khan and Rue, 2023, Martens, 2020]. The mirror map is the gradient of the log-partition, which satisfies the thermodynamic identity ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{\\rho}=\\nabla\\Phi(\\psi)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This is a bijection when $\\Phi$ is convex (which includes the cases we study), so we can implicitly treat $\\psi$ and $\\rho$ as functions of each other. Given a loss function $L(\\psi)$ , MD iteratively solves the local optimization problem ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi_{i+1}=\\underset{\\psi}{\\arg\\operatorname*{min}}\\left\\langle\\nabla_{\\rho_{i}}L(\\psi_{i}),\\rho\\right\\rangle+\\frac{1}{\\alpha}\\mathbb{D}_{\\Phi}(\\psi_{i},\\psi)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first term is a linear (in $\\rho$ ) approximation of $L$ about the previous iteration $\\psi_{i}$ and the second term is the Bregman divergence ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{D}_{\\Phi}(\\psi_{i},\\psi_{i+1})=\\Phi(\\psi_{i})-\\Phi(\\psi_{i+1})-(\\psi_{i}-\\psi_{i+1})^{\\top}\\,\\rho_{i+1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The Bregman divergence acts as a regularizer toward $\\psi_{i}$ and captures the intrinsic geometry of the parameter space because of its equivalence with the (reverse) KL divergence ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathbb{K L}}\\big(q_{\\psi_{i+1}}|q_{\\psi_{i}}\\big)=\\mathbb{E}_{\\theta\\sim q_{\\psi_{i+1}}}[(\\psi_{i+1}-\\psi_{i})^{\\top}\\,T(\\theta)+\\Phi(\\psi_{i})-\\Phi(\\psi_{i+1})]}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mathbb{D}_{\\Phi}\\big(\\psi_{i},\\psi_{i+1}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Importantly, this recursive regularizer is not part of the loss and serves only to define an iterated algorithm that converges to a local minimum of $L$ . Solving Eq. (62) by differentiating by $\\rho$ yields the MD update ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi_{i+1}=\\psi_{i}-\\alpha\\nabla_{\\rho_{i}}L(\\psi_{i})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Because the Fisher matrix for an exponential family is $\\mathbf{F}_{\\psi}=\\partial\\pmb{\\rho}/\\partial\\psi$ , this is equivalent to NGD with respect to $\\psi$ . Khan and Rue [2023] offer this as a derivation of the BLR, when $L(\\psi)$ is taken to be the variational loss from Eq. (1). ", "page_idx": 23}, {"type": "text", "text": "By applying this analysis to the online setting, our approach can be seen to follow from two insights. First, the online variational loss in Eq. (2) already includes KL divergence from the previous step, so we do not need the artificial regularizer in Eq. (62). That is, if we start from the online variational problem in Eq. (4) and define $L_{t}(\\psi)$ as the expected NLL, ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{t}(\\pmb{\\psi})=-\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi}}[\\log p(\\pmb{y}_{t}|f_{t}(\\pmb{\\theta}_{t}))]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then replacing $L_{t}(\\psi)$ with a linear approximation based at $\\psi_{t|t-1}$ and applying Eq. (65) leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi_{t}=\\underset{\\psi}{\\arg\\operatorname*{min}}\\left\\langle\\nabla_{\\rho_{t|t-1}}L_{t}(\\psi_{t|t-1}),\\rho\\right\\rangle+\\mathbb{D}_{\\Phi}(\\psi_{t|t-1},\\psi)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By comparing to Eq. (62) we see this defines an MD algorithm with unit learning rate that works in a single step rather than by iterating. Paralleling the derivation of Eq. (66) from Eq. (62) we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi_{t}=\\psi_{t|t-1}-\\nabla_{\\rho_{t|t-1}}L_{t}(\\psi_{t|t-1})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which matches the BONG update in Eq. (5). Thus BONG can be seen as an approximate solution of the online variational problem in Eq. (4) based on linearizing the expected NLL wrt $\\rho$ . (Note this is different from the assumption underlying BONG-LIN that $f_{t}(\\pmb\\theta_{t})$ or $\\bar{h}_{t}(\\pmb\\theta_{t})$ is linear in $\\pmb{\\theta}_{t}$ .) ", "page_idx": 23}, {"type": "text", "text": "Second, in the conjugate case, this linearity assumption is true: $L_{t}$ is linear in $\\rho$ (see proof of Proposition 4.1). Therefore 68 is equivalent to solving Eq. (4) exactly: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi_{t}=\\mathop{\\arg\\operatorname*{min}}_{\\psi}L_{t}(\\psi)+\\mathbb{D}_{\\Phi}(\\psi_{t|t-1},\\psi)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This recapitulates Proposition 4.1 that BONG is Bayes optimal in the conjugate case. In general the exact solution to Eq. (70) is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi_{t}=\\psi_{t|t-1}-\\nabla_{\\rho_{t}}L_{t}(\\psi_{t})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This is an implicit update because the gradient is evaluated at the (unknown) posterior, whereas Eq. (69) is an explicit update because it evaluates the gradient at the prior. (In the Gaussian case these can be shown to match the implicit and explicit RVGA updates of [Lambert et al., 2021].) Therefore BONG can be also interpreted as an approximation of exact VB that replaces the implicit update, Eq. (71), with an explicit update, Eq. (69). ", "page_idx": 23}, {"type": "text", "text": "E Derivations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section derives the update equations for all 80 algorithms we investigate (Table 3 plus the MC-HESS and LIN-EF variants). In Appendix E.6 we also translate the BLR algorithms from our online setting back to the batch setting used in Khan and Rue [2023]. ", "page_idx": 24}, {"type": "text", "text": "For an exponential variational family with natural parameters $\\psi$ and dual parameters $\\rho$ , we can derive all 16 methods (BONG, BLR, BOG, BBB under all four Hessian approximations) from four quantities: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t,i-1}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\pmb{y}_{t}\\middle|{f}_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]}\\\\ &{\\nabla_{\\rho_{t,i-1}}D_{\\mathbb{K}\\mathbb{L}}\\left(q_{\\psi_{t,i-1}}\\middle|q_{\\psi_{t|t-1}}\\right)}\\\\ &{\\nabla_{\\psi_{t,i-1}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\pmb{y}_{t}\\middle|{f}_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]}\\\\ &{\\nabla_{\\psi_{t,i-1}}D_{\\mathbb{K}\\mathbb{L}}\\left(q_{\\psi_{t,i-1}}\\middle|q_{\\psi_{t|t-1}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The NGD methods (BONG and BLR) use gradients with respect to $\\rho_{t,t-i}$ while the GD methods (BOG and BBB) use gradients with respect to $\\psi_{t,i-1}$ . For BONG and BOG the $D_{\\mathbb{K L}}$ term is not relevant, and there is no inner loop so $\\psi_{t,i-1}=\\psi_{t|t-1}$ and $g_{t,i}=g_{t}$ , $\\mathbf{G}_{t,i}=\\mathbf{G}_{t}$ . ", "page_idx": 24}, {"type": "text", "text": "When $\\psi$ is not the natural parameter of an exponential family we must explicitly compute the inverseFisher preconditioner for the NGD methods. Therefore the updates can be derived from these three quantities: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{F}_{\\psi_{t,i-1}}}\\\\ &{\\nabla_{\\psi_{t,i-1}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\mathbf{y}_{t}\\vert f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]}\\\\ &{\\nabla_{\\psi_{t,i-1}}D_{\\mathbb{K}\\mathbb{L}}\\left(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We will frequently use Bonnet\u2019s and Price\u2019s theorems [Bonnet, 1964, Price, 1958] ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}\\mathbb{E}_{N(\\mu_{t,i-1},\\Sigma_{t,i-1})}[\\log p\\left(y_{t}|f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]=\\mathbb{E}_{N(\\mu_{t,i-1},\\Sigma_{t,i-1})}[\\nabla_{\\pmb{\\theta}_{t}}\\log p\\left(y_{t}|f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=g_{t,i}}\\\\ &{\\nabla_{\\pmb{\\Sigma}_{t,i-1}}\\mathbb{E}_{N(\\mu_{t,i-1},\\Sigma_{t,i-1})}[\\log p\\left(y_{t}|f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]=\\frac{1}{2}\\mathbb{E}_{N(\\mu_{t,i-1},\\Sigma_{t,i-1})}\\left[\\nabla_{\\pmb{\\theta}_{t}}^{2}\\log p\\left(y_{t}|f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2}\\mathbf{G}_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For diagonal Gaussians with covariance Diag $\\left(\\sigma^{2}\\right)$ , Price\u2019s theorem also implies6 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\sigma_{t,i-1}^{2}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim\\mathcal{N}(\\pmb{\\mu}_{t,i-1},\\pmb{\\Sigma}_{t,i-1})}[\\log p\\left(\\pmb{y}_{t}\\middle|f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]=\\frac{1}{2}\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Update equations for MC-HESS, MC-EF and LIN-EF methods are displayed together in the subsections that follow, because for the most part they differ only in the choice of $\\dot{\\mathbf{G}}_{t}^{\\mathrm{MC-HESS}}$ , $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ or $\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}$ to approximate $\\mathbf{G}_{t}$ and $\\pmb{g}_{t}^{\\mathrm{MC}}$ or $\\pmb{g}_{t}^{\\mathrm{LIN}}$ to approximate $\\scriptstyle g_{t}$ . We note cases where decomposing ${\\bf G}_{t}^{\\mathrm{MC-EF}}=$ $\\begin{array}{r}{-\\frac{1}{M}\\hat{\\mathbf{G}}_{t}^{(1:M)}\\hat{\\mathbf{G}}_{t}^{(1:M)^{\\top}}}\\end{array}$ or $\\mathbf{G}_{t}^{\\mathrm{\\scriptscriptstyleLIN-EF}}=-\\pmb{g}_{t}^{\\mathrm{\\scriptscriptstyleLIN}}\\left(\\pmb{g}_{t}^{\\mathrm{\\scriptscriptstyleLIN}}\\right)^{\\sf T}$ allows a more efficient update. ", "page_idx": 24}, {"type": "text", "text": "We derive updates for BONG-LIN-HESS and BOG-LIN-HESS from the corresponding BONG-MC-HESS and BOG-MC-HESS updates using Proposition 4.2 which entails substituting ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{g}_{t}\\rightarrow\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}(\\pmb{y}_{t}-\\hat{\\pmb{y}}_{t})}\\\\ &{\\mathbf{G}_{t}\\rightarrow-\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the algorithms with inner loops (BLR and BBB) we adapt the notation of Section 4.4 as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{y}_{t,i}=\\boldsymbol{h}_{t}\\left(\\boldsymbol{\\mu}_{t,i-1}\\right)}\\\\ &{\\mathbf{H}_{t,i}=\\frac{\\partial\\boldsymbol{h}_{t}}{\\partial\\boldsymbol{\\theta}_{t}}}\\\\ &{\\mathbf{R}_{t,i}=\\mathbb{V}\\left[\\boldsymbol{y}_{t}|\\boldsymbol{\\theta}_{t}=\\boldsymbol{\\mu}_{t,i-1}\\right.}\\\\ &{\\boldsymbol{g}_{t,i}=\\mathbf{H}_{t,i}^{\\top}\\mathbf R_{t,i}^{-1}\\left(\\boldsymbol{y}_{t}-\\hat{\\boldsymbol{y}}_{t,i}\\right)}\\\\ &{\\mathbf{G}_{t,i}=-\\mathbf H_{t,i}^{\\top}\\mathbf R_{t,i}^{-1}\\mathbf H_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This corresponds to basing the linear $(f)$ -Gaussian and linear $(h)$ -delta approximations at $\\pmb{\\mu}_{t,i-1}$ instead of $\\pmb{\\mu}_{t|t-1}$ . Thus the updates for BLR-LIN-HESS and BBB-LIN-HESS are obtained by substituting ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\pmb{g}}_{t,i}\\rightarrow\\mathbf{H}_{t,i}^{\\mathsf{T}}\\mathbf{R}_{t,i}^{-1}(\\pmb{y}_{t}-\\hat{\\pmb{y}}_{t,i})}\\\\ &{\\mathbf{G}_{t,i}\\rightarrow-\\mathbf{H}_{t,i}^{\\mathsf{T}}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E.1 Full covariance Gaussian, natural parameters ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The natural and dual parameters for a general Gaussian are given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\psi_{t,i-1}^{(1)}=\\Sigma_{t,i-1}^{-1}\\mu_{t,i-1}}&&{\\rho_{t,i-1}^{(1)}=\\mu_{t,i-1}}\\\\ &{\\psi_{t,i-1}^{(2)}=-\\frac{1}{2}\\Sigma_{t,i-1}^{-1}}&&{\\rho_{t,i-1}^{(2)}=\\mu_{t,i-1}\\mu_{t,i-1}^{\\intercal}+\\Sigma_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Inverting these relationships gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i-1}=-\\frac{1}{2}\\psi_{t,i-1}^{(2)-1}\\psi_{t,i-1}^{(1)}=\\rho_{t,i-1}^{(1)}}\\\\ &{\\Sigma_{t,i-1}=-\\frac{1}{2}\\psi_{t,i-1}^{(2)-1}\\qquad\\quad=\\rho_{t,i-1}^{(2)}-\\rho_{t,i-1}^{(1)}\\rho_{t,i-1}^{(1)\\intercal}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The KL divergence in the VI loss is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathbb{K L}}\\left(q_{\\psi_{t,i-1}}\\middle|q_{\\psi_{t|t-1}}\\right)=\\frac{1}{2}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)^{\\intercal}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\ \\frac{1}{2}T r\\left(\\Sigma_{t|t-1}^{-1}\\Sigma_{t,i-1}\\right)-\\frac{1}{2}\\log\\left|\\Sigma_{t,i-1}\\right|)+\\mathrm{const}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with gradients ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\Sigma_{t\\mid t-1}^{-1}\\left(\\mu_{t,i-1}-\\mu_{t\\mid t-1}\\right)}\\\\ &{\\nabla_{\\Sigma_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\frac{1}{2}\\left(\\Sigma_{t\\mid t-1}^{-1}-\\Sigma_{t,i-1}^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Following Appendix $\\mathbf{C}$ of [Khan et al., 2018a], for any scalar function $\\ell$ the chain rule gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t,i-1}^{(1)}}\\ell=\\frac{\\partial\\mu_{t,i-1}}{\\partial\\rho_{t,i-1}^{(1)}}\\nabla_{\\mu_{t,i-1}\\ell}+\\frac{\\partial\\Sigma_{t,i-1}}{\\partial\\rho_{t,i-1}^{(1)}}\\nabla_{\\Sigma_{t,i-1}\\ell}}\\\\ &{\\qquad\\qquad=\\nabla_{\\mu_{t,i-1}\\ell}-2\\left(\\nabla_{\\Sigma_{t,i-1}\\ell}\\right)\\mu_{t,i-1}}\\\\ &{\\nabla_{\\rho_{t,i-1}^{(2)}}\\ell=\\frac{\\partial\\mu_{t,i-1}}{\\partial\\rho_{t,i-1}^{(2)}}\\nabla_{\\mu_{t,i-1}\\ell}+\\frac{\\partial\\Sigma_{t,i-1}}{\\partial\\rho_{t,i-1}^{(2)}}\\nabla_{\\Sigma_{t,i-1}\\ell}}\\\\ &{\\qquad\\qquad=\\nabla_{\\Sigma_{t,i-1}\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t,i-1}^{(1)}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\boldsymbol{y}_{t}|\\boldsymbol{f}_{t}\\left(\\theta_{t}\\right))]={g}_{t,i}-{\\mathbf{G}}_{t,i}\\mu_{t,i-1}}\\\\ &{\\nabla_{\\rho_{t,i-1}^{(2)}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\boldsymbol{y}_{t}|\\boldsymbol{f}_{t}\\left(\\theta_{t}\\right)\\right)]=\\frac{1}{2}{\\mathbf{G}}_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t,i-1}^{(1)}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\big|q_{\\psi_{t|t-1}}\\big)=\\Sigma_{t,i-1}^{-1}\\mu_{t,i-1}-\\Sigma_{t|t-1}^{-1}\\mu_{t|t-1}}\\\\ &{\\nabla_{\\rho_{t,i-1}^{(2)}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\big|q_{\\psi_{t|t-1}}\\big)=\\frac{1}{2}\\left(\\Sigma_{t|t-1}^{-1}-\\Sigma_{t,i-1}^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Following the same approach for $\\psi$ gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{\\Psi_{t,i-1}^{(1)}}\\ell=\\frac{\\partial\\mu_{t,i-1}}{\\partial\\psi_{t,i-1}^{(1)}}\\nabla_{\\mu_{t,i-1}}\\ell+\\frac{\\partial\\Sigma_{t,i-1}}{\\partial\\psi_{t,i-1}^{(1)}}\\nabla_{\\Sigma_{t,i-1}}\\ell}&{}\\\\ &{\\!=-\\frac{1}{2}\\psi_{t,i-1}^{(2)-1}\\nabla_{\\mu_{t,i-1}}\\ell}\\\\ &{\\!=\\Sigma_{t,i-1}\\nabla_{\\mu_{t,i-1}\\ell}}\\\\ {\\nabla_{\\Psi_{t,i-1}^{(2)}}\\ell=\\frac{\\partial\\mu_{t,i-1}}{\\partial\\psi_{t,i-1}^{(2)}}\\nabla_{\\mu_{t,i-1}\\ell}+\\frac{\\partial\\Sigma_{t,i-1}}{\\partial\\psi_{t,i-1}^{(2)}}\\nabla_{\\Sigma_{t,i-1}\\ell}}\\\\ &{\\!=\\frac{1}{2}\\psi_{t,i-1}^{(2)-1}\\left(\\nabla_{\\mu_{t,i-1}\\ell}\\right)\\psi_{t,i-1}^{(1)}\\psi_{t,i-1}^{(2)-1}+\\frac{1}{2}\\psi_{t,i-1}^{(2)-1}\\left(\\nabla_{\\Sigma_{t,i-1}\\ell}\\right)\\psi_{t,i-1}^{(2)-1}}\\\\ &{\\!=2\\Sigma_{t,i-1}\\left(\\nabla_{\\mu_{t,i-1}\\ell}\\right)\\mu_{t,i-1}^{(1)}+2\\Sigma_{t,i-1}\\left(\\nabla_{\\Sigma_{t,i-1}\\ell}\\right)\\Sigma_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi_{t,i-1}^{(1)}}\\mathbb{E}_{\\theta_{t}\\sim q\\psi_{t,i-1}}\\big[\\log p\\left(y_{t}\\vert f_{t}\\left(\\theta_{t}\\right)\\right)\\big]=\\Sigma_{t,i-1}g_{t,i}}\\\\ &{\\nabla_{\\psi_{t,i-1}^{(2)}}\\mathbb{E}_{\\theta_{t}\\sim q\\psi_{t,i-1}}\\big[\\log p\\left(y_{t}\\vert f_{t}\\left(\\theta_{t}\\right)\\right)\\big]=2\\Sigma_{t,i-1}g_{t,i}\\mu_{t,i-1}^{\\mathsf{T}}+\\Sigma_{t,i-1}\\mathbf{G}_{t,i}\\Sigma_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi_{t,i-1}^{(1)}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=\\Sigma_{t,i-1}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)}\\\\ &{\\nabla_{\\psi_{t,i-1}^{(2)}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=2\\Sigma_{t,i-1}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)\\mu_{t,i-1}^{\\mathsf{T}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\;\\Sigma_{t,i-1}\\left(\\Sigma_{t|t-1}^{-1}-\\Sigma_{t,i-1}^{-1}\\right)\\Sigma_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.1.1 BONG FC (explicit RVGA) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Substituting Eqs. (104) and (105) into Eq. (5) gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t}^{(1)}=\\psi_{t|t-1}^{(1)}+g_{t}-\\mathbf{G}_{t}\\pmb{\\mu}_{t|t-1}}\\\\ &{\\psi_{t}^{(2)}=\\psi_{t|t-1}^{(2)}+\\frac{1}{2}\\mathbf{G}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Translating to $(\\pmb{\\mu}_{t},\\pmb{\\Sigma}_{t})$ gives the BONG-FC update ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\boldsymbol{\\mu}_{t}=\\boldsymbol{\\mu}_{t|t-1}+\\boldsymbol{\\Sigma}_{t}\\boldsymbol{g}_{t}}\\\\ &{\\quad\\boldsymbol{\\Sigma}_{t}^{-1}=\\boldsymbol{\\Sigma}_{t|t-1}^{-1}-\\mathbf{G}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is equivalent to the explicit update form of RVGA [Lambert et al., 2021]. Using $\\mathbf{G}_{t}^{\\mathrm{MC}}$ -HESSthis update takes $O(P^{3})$ because of the matrix inversion. Using $\\mathbf{G}_{t}^{\\scriptscriptstyle\\mathrm{MC-EF}}$ and the Woodbury matrix identity we can write the update in a form that takes $O(M P^{2}+\\bar{M}^{3})$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t}=\\mu_{t|t-1}+\\mathbf{K}_{t}\\mathbf{1}_{M}}\\\\ &{\\pmb{\\Sigma}_{t}=\\pmb{\\Sigma}_{t|t-1}-\\mathbf{K}_{t}\\hat{\\mathbf{G}}_{t}^{(1:M)^{\\intercal}}\\pmb{\\Sigma}_{t|t-1}}\\\\ &{\\mathbf{K}_{t}=\\pmb{\\Sigma}_{t|t-1}\\hat{\\mathbf{G}}_{t}^{(1:M)}\\left(M\\mathbf{I}_{M}+\\hat{\\mathbf{G}}_{t}^{(1:M)^{\\intercal}}\\pmb{\\Sigma}_{t|t-1}\\hat{\\mathbf{G}}_{t}^{(1:M)}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Likewise using $\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}$ takes $O(P^{2})$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\mu}_{t}=\\boldsymbol{\\mu}_{t\\vert t-1}+\\mathbf{K}_{t}}\\\\ &{\\boldsymbol{\\Sigma}_{t}=\\boldsymbol{\\Sigma}_{t\\vert t-1}-\\mathbf{K}_{t}\\left(\\boldsymbol{g}_{t}^{\\mathrm{LIN}}\\right)^{\\mathsf{T}}\\boldsymbol{\\Sigma}_{t\\vert t-1}}\\\\ &{\\mathbf{K}_{t}=\\frac{\\sum_{t\\vert t-1}\\boldsymbol{g}_{t}^{\\mathrm{LIN}}}{1+\\left(\\boldsymbol{g}_{t}^{\\mathrm{LIN}}\\right)^{\\mathsf{T}}\\boldsymbol{\\Sigma}_{t\\vert t-1}\\boldsymbol{g}_{t}^{\\mathrm{LIN}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.1.2 BONG-LIN-HESS FC (CM-EKF) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (120) and (121) gives the BONG-LIN-HESS-FC update ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\boldsymbol{\\mu}_{t}=\\boldsymbol{\\mu}_{t\\vert t-1}+\\mathbf{\\Sigma}_{t}\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\left(\\boldsymbol{y}_{t}-\\hat{\\boldsymbol{y}}_{t}\\right)}\\\\ &{\\mathbf{\\Sigma}_{t}^{-1}=\\mathbf{\\Sigma}_{t\\vert t-1}^{-1}+\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is equivalent to CM-EKF [Tronarp et al., 2018, Ollivier, 2018] and can be rewritten using the Woodbury identity in a form that takes $O(C P^{2}+C^{3})$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol\\mu}_{t}={\\boldsymbol\\mu}_{t\\vert t-1}+\\mathbf K_{t}\\big({\\boldsymbol y}_{t}-\\hat{{\\boldsymbol y}}_{t}\\big)}\\\\ &{\\boldsymbol\\Sigma_{t}={\\boldsymbol\\Sigma}_{t\\vert t-1}-\\mathbf K_{t}\\mathbf H_{t}{\\boldsymbol\\Sigma}_{t\\vert t-1}}\\\\ &{{\\mathbf K}_{t}={\\boldsymbol\\Sigma}_{t\\vert t-1}\\mathbf H_{t}^{\\intercal}\\left({\\mathbf R}_{t}+\\mathbf H_{t}{\\boldsymbol\\Sigma}_{t\\vert t-1}\\mathbf H_{t}^{\\intercal}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.1.3 BLR FC ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Substituting Eqs. (104) to (107) into Eq. (33) gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t,i}^{(1)}=\\psi_{t,i-1}^{(1)}+\\alpha\\left(g_{t,i}-\\mathbf{G}_{t,i}\\mu_{t,i-1}-\\sum_{t,i-1}^{-1}\\!\\mu_{t,i-1}+\\Sigma_{t|t-1}^{-1}\\mu_{t|t-1}\\right)}\\\\ &{\\psi_{t,i}^{(2)}=\\psi_{t,i-1}^{(2)}+\\frac{\\alpha}{2}\\left(\\mathbf{G}_{t,i}+\\Sigma_{t,i-1}^{-1}-\\Sigma_{t|t-1}^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Translating to $(\\mu_{t,i},\\Sigma_{t,i})$ gives the BLR-FC update ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\Sigma_{t,i}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\Sigma_{t,i}g_{t,i}}\\\\ &{\\Sigma_{t,i}^{-1}=\\left(1-\\alpha\\right)\\Sigma_{t,i-1}^{-1}+\\alpha\\Sigma_{t|t-1}^{-1}-\\alpha\\mathbf{G}_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversion. In Appendix E.1.1 we were able to use the Woodbury identity to exploit the low rank of $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ and $\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}$ and obtain BONG updates with complexity quadratic in $P$ . This does not appear possible with Eq. (136) because of the extra precision term on the RHS (applying Woodbury would require inverting $(1-\\alpha)\\Sigma_{t,i-1}^{-1}+\\alpha\\Sigma_{t|t-1}^{-1})$ Therefore unlike BONG-FC, BLR-FC requires time cubic in the model size, for reasons that can be traced back to the KL term in Eq. (33). ", "page_idx": 27}, {"type": "text", "text": "E.1.4 BLR-LIN-HESS FC ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (135) and (136) gives the BLR-LIN-HESS-FC update ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\Sigma_{t,i}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\Sigma_{t,i}\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\left(y_{t}-\\hat{y}_{t,i}\\right)}\\\\ &{\\Sigma_{t,i}^{-1}=\\left(1-\\alpha\\right)\\Sigma_{t,i-1}^{-1}+\\alpha\\Sigma_{t|t-1}^{-1}+\\alpha\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversion. ", "page_idx": 27}, {"type": "text", "text": "E.1.5 BOG FC ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Substituting Eqs. (114) and (115) into Eq. (31) gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\psi}_{t}^{(1)}=\\boldsymbol{\\psi}_{t\\vert t-1}^{(1)}+\\alpha\\boldsymbol{\\Sigma}_{t\\vert t-1}\\boldsymbol{g}_{t}}\\\\ &{\\boldsymbol{\\psi}_{t}^{(2)}=\\boldsymbol{\\psi}_{t\\vert t-1}^{(2)}+2\\alpha\\boldsymbol{\\Sigma}_{t\\vert t-1}\\boldsymbol{g}_{t}\\boldsymbol{\\mu}_{t\\vert t-1}^{\\intercal}+\\alpha\\boldsymbol{\\Sigma}_{t\\vert t-1}\\mathbf{G}_{t}\\boldsymbol{\\Sigma}_{t\\vert t-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Translating to $(\\pmb{\\mu}_{t},\\pmb{\\Sigma}_{t})$ gives the BOG-FC update ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mu_{t}=\\Sigma_{t}\\Sigma_{t|t-1}^{-1}\\mu_{t|t-1}+\\alpha\\Sigma_{t}\\Sigma_{t|t-1}g_{t}}\\\\ &{\\Sigma_{t}^{-1}=\\Sigma_{t|t-1}^{-1}-4\\alpha\\Sigma_{t|t-1}g_{t}\\mu_{t|t-1}^{\\intercal}-2\\alpha\\Sigma_{t|t-1}\\mathbf G_{t}\\Sigma_{t|t-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This update takes $O(P^{3})$ because of the matrix inversion. The greater cost of the BOG-FC update relative to BONG-FC can be traced to the difference between GD and NGD: the NLL gradients wrt $\\psi_{t|t-1}$ in Eqs. (114) and (115) are more complicated than the gradients wrt $\\rho_{t|t-1}$ in Eqs. (104) and (105). ", "page_idx": 27}, {"type": "text", "text": "E.1.6 BOG-LIN-HESS FC ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (141) and (142) gives the BOG-LIN-HESS-FC update ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\boldsymbol{\\mu}_{t}=\\boldsymbol{\\Sigma}_{t}\\boldsymbol{\\Sigma}_{t\\mid t-1}^{-1}\\boldsymbol{\\mu}_{t\\mid t-1}+\\alpha\\boldsymbol{\\Sigma}_{t}\\boldsymbol{\\Sigma}_{t\\mid t-1}\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\left(\\boldsymbol{y}_{t}-\\hat{\\boldsymbol{y}}_{t}\\right)}\\\\ &{\\boldsymbol{\\Sigma}_{t}^{-1}=\\boldsymbol{\\Sigma}_{t\\mid t-1}^{-1}-4\\alpha\\boldsymbol{\\Sigma}_{t\\mid t-1}\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\left(\\boldsymbol{y}_{t}-\\hat{\\boldsymbol{y}}_{t}\\right)\\boldsymbol{\\mu}_{t\\mid t-1}^{\\intercal}+2\\alpha\\boldsymbol{\\Sigma}_{t\\mid t-1}\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}\\boldsymbol{\\Sigma}_{t\\mid t-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This update takes $O(P^{3})$ because of the matrix inversion. ", "page_idx": 27}, {"type": "text", "text": "E.1.7 BBB FC ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Substituting Eqs. (114) to (117) into Eq. (34) gives ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t,i}^{(1)}=\\psi_{t,i-1}^{(1)}+\\alpha\\Sigma_{t,i-1}g_{t,i}-\\alpha\\Sigma_{t,i-1}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)}\\\\ &{\\psi_{t}^{(2)}=\\psi_{t,i-1}^{(2)}+2\\alpha\\Sigma_{t,i-1}g_{t,i}\\mu_{t,i-1}^{\\mathsf{T}}+\\alpha\\Sigma_{t|t-1}\\mathbf{G}_{t}\\Sigma_{t|t-1}}\\\\ &{\\qquad\\quad-\\,2\\alpha\\Sigma_{t,i-1}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)\\mu_{t,i-1}^{\\mathsf{T}}-\\alpha\\Sigma_{t,i-1}\\left(\\Sigma_{t|t-1}^{-1}-\\Sigma_{t,i-1}^{-1}\\right)\\Sigma_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Translating to $(\\mu_{t,i},\\Sigma_{t,i})$ gives the BBB-FC update ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\boldsymbol{\\Sigma}_{t,i}\\boldsymbol{\\Sigma}_{t,i-1}^{-1}\\mu_{t,i-1}+\\alpha\\boldsymbol{\\Sigma}_{t,i}\\boldsymbol{\\Sigma}_{t,i-1}\\left(g_{t,i}+\\boldsymbol{\\Sigma}_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)\\right)}\\\\ &{\\boldsymbol{\\Sigma}_{t,i}^{-1}=\\boldsymbol{\\Sigma}_{t,i-1}^{-1}-2\\alpha\\boldsymbol{\\Sigma}_{t,i-1}\\left(\\begin{array}{l}{2\\boldsymbol{\\Sigma}_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)\\mu_{t,i-1}^{\\intercal}+\\mathbf{I}_{P}}\\\\ {+2g_{t,i}\\mu_{t,i-1}^{\\intercal}+\\left(\\mathbf{G}_{t,i}-\\boldsymbol{\\Sigma}_{t|t-1}^{-1}\\right)\\boldsymbol{\\Sigma}_{t,i-1}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversion. ", "page_idx": 28}, {"type": "text", "text": "E.1.8 BBB-LIN-HESS FC ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (147) and (148) gives the BBB-LIN-HESS-FC update ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\boldsymbol{\\Sigma}_{t,i}\\boldsymbol{\\Sigma}_{t,i-1}^{-1}\\mu_{t,i-1}}\\\\ &{\\qquad\\,+\\alpha\\boldsymbol{\\Sigma}_{t,i}\\boldsymbol{\\Sigma}_{t,i-1}\\left(\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\left(y_{t}-\\hat{y}_{t,i}\\right)+\\boldsymbol{\\Sigma}_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)\\right)}\\\\ &{\\qquad\\,+\\alpha\\boldsymbol{\\Sigma}_{t,i-1}^{-1}-2\\alpha\\boldsymbol{\\Sigma}_{t,i-1}\\left(\\begin{array}{c}{2\\boldsymbol{\\Sigma}_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)\\mu_{t,i-1}^{\\intercal}+\\mathbf{I}_{P}}\\\\ {+2\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\left(y_{t}-\\hat{y}_{t,i}\\right)\\mu_{t,i-1}^{\\intercal}}\\\\ {-\\left(\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}+\\boldsymbol{\\Sigma}_{t|t-1}^{-1}\\right)\\boldsymbol{\\Sigma}_{t,i-1}^{\\intercal}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversion. ", "page_idx": 28}, {"type": "text", "text": "E.2 Full covariance Gaussian, Moment parameters ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The Bonnet and Price theorems give ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\pmb{y}_{t}|\\,f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]=\\pmb{g}_{t,i}}\\\\ &{\\nabla_{\\pmb{\\Sigma}_{t,i-1}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\pmb{y}_{t}|\\,f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]=\\frac{1}{2}\\mathbf{G}_{t,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From Appendix E.1 we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\Sigma_{t\\mid t-1}^{-1}\\left(\\mu_{t,i-1}-\\mu_{t\\mid t-1}\\right)}\\\\ &{\\nabla_{\\Sigma_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\frac{1}{2}\\left(\\Sigma_{t\\mid t-1}^{-1}-\\Sigma_{t,i-1}^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We write the Fisher with respect to the moment parameters $\\psi=(\\mu,\\operatorname{vec}(\\Sigma))$ as a block matrix: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{F}=\\left[\\begin{array}{l l}{\\mathbf{F}_{\\mu,\\mu}}&{\\mathbf{F}_{\\mu,\\Sigma}}\\\\ {\\mathbf{F}_{\\Sigma,\\mu}}&{\\mathbf{F}_{\\Sigma,\\Sigma}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The blocks can be calculated by the second-order Fisher formula ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{F}_{\\mu,\\mu}=-\\mathbf{E}_{\\mathrm{q}\\mathrm{(F)}}\\nabla_{\\mu,\\mu}\\log{q_{\\mathrm{f}}(\\theta)}]}&{}\\\\ {=\\mathbf{L}^{-1}}\\\\ {\\mathbf{F}_{\\mu,\\Sigma}=-\\mathbf{E}_{\\mathrm{q}\\mathrm{(F)}}[\\nabla_{\\mu,\\Sigma}\\log{q_{\\mathrm{f}}(\\theta)}]}\\\\ {=-\\mathbf{E}_{\\mathrm{q}\\mathrm{(F)}}\\left[\\left(\\nabla_{\\Sigma}\\Sigma^{-1}\\right)\\left(\\theta-\\mu\\right)\\right]}\\\\ {=0}\\\\ {\\mathbf{F}_{\\Sigma,\\Sigma}=-\\mathbf{E}_{\\mathrm{q}\\mathrm{(F)}}\\nabla_{\\Sigma}\\log{q_{\\mathrm{f}}(\\theta)}}\\\\ {=-\\mathbf{E}_{\\mathrm{q}\\mathrm{(F)}}\\nabla_{\\Sigma}\\left(\\frac{1}{2}\\Sigma^{-1}(\\theta-\\mu)(\\theta-\\mu)^{\\sf T}\\Sigma^{-1}-\\frac{1}{2}\\Sigma^{-1}\\right)]}\\\\ {=-\\frac{1}{2}\\mathbf{E}_{\\mathrm{q}\\mathrm{p}}\\Bigg[\\left(\\Sigma\\Sigma^{\\Sigma-1}\\right)\\left(\\theta-\\mu\\right)^{\\sf T}\\left(\\nabla_{\\Sigma}\\Sigma^{-1}\\right)-\\left(\\nabla_{\\Sigma}\\Sigma^{-1}\\right)\\Bigg]}\\\\ {=-\\frac{1}{2}\\nabla_{\\Sigma}\\Sigma^{-1}}\\\\ {=\\frac{1}{2}\\Sigma^{-1}\\otimes\\Sigma^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the final line we used ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla_{\\Sigma_{k\\ell}}\\left(\\Sigma^{-1}\\right)_{i j}=-\\left(\\Sigma^{-1}\\right)_{i k}\\left(\\Sigma^{-1}\\right)_{j\\ell}}\\\\ &{}&{=-\\left(\\Sigma^{-1}\\otimes\\Sigma^{-1}\\right)_{i j,k\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with $i j$ and $k\\ell$ treated as composite indices in $\\left[P^{2}\\right]$ . Therefore the preconditioner in the NGD methods is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{F}_{\\psi_{t,i-1}}^{-1}=\\left[\\begin{array}{c c}{\\mathbf{\\Sigma}_{0,i-1}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{2\\mathbf{\\Sigma}_{t,i-1}\\otimes\\mathbf{\\Sigma}_{t,i-1}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E.2.1 BONG FC, Moment ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Substituting Eqs. (151), (152) and (168) into Eq. (3) gives the BONG-FC_MOM update ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\mu}_{t}=\\pmb{\\mu}_{t|t-1}+\\pmb{\\Sigma}_{t|t-1}\\pmb{g}_{t}}\\\\ &{\\pmb{\\Sigma}_{t}=\\pmb{\\Sigma}_{t|t-1}+\\pmb{\\Sigma}_{t|t-1}\\mathbf{G}_{t}\\pmb{\\Sigma}_{t|t-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This update takes $O(P^{3})$ using $\\mathbf{G}_{t}^{\\mathrm{MC}}$ -HESS, $O(M P^{2})$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , and $O(P^{2})$ using ${\\bf G}_{t}^{\\mathrm{LIN-EF}}$ . The update is similar to the BONG-FC update except that Eq. (170) ignores the $\\hat{\\mathbf{G}}_{t}^{(1:M)^{\\top}}\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{t|t-1}\\hat{\\mathbf{G}}_{t}^{(1:M)}$ term in Eq. (124) or the $\\big(\\pmb{g}_{t}^{\\mathrm{LIN}}\\big)^{\\top}\\,\\pmb{\\Sigma}_{t|t-1}\\pmb{g}_{t}^{\\mathrm{LIN}}$ term in Eq. (127) which estimate the epistemic part of predictive uncertainty. ", "page_idx": 29}, {"type": "text", "text": "E.2.2 BONG-LIN-HESS FC, Moment ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (169) and (170) gives the BONG-LIN-HESS-FC_MOM update ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol{\\mu}}_{t}={\\boldsymbol{\\mu}}_{t\\mid t-1}+\\mathbf{\\boldsymbol{\\Sigma}}_{t\\mid t-1}\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}({\\boldsymbol{y}}_{t}-\\hat{{\\boldsymbol{y}}}_{t})}\\\\ &{\\mathbf{\\boldsymbol{\\Sigma}}_{t}=\\mathbf{\\boldsymbol{\\Sigma}}_{t\\mid t-1}-\\mathbf{\\boldsymbol{\\Sigma}}_{t\\mid t-1}\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}\\mathbf{\\boldsymbol{\\Sigma}}_{t\\mid t-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This update takes $O(C P^{2})$ . ", "page_idx": 29}, {"type": "text", "text": "E.2.3 BLR FC, Moment ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Substituting Eqs. (151) to (154) and (168) into Eq. (32) gives the BLR-FC_MOM update ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\Sigma_{t,i-1}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\Sigma_{t,i-1}g_{t,i}}\\\\ &{\\Sigma_{t,i}=\\left(1+\\alpha\\right)\\Sigma_{t,i-1}+\\alpha\\Sigma_{t,i-1}\\left(\\mathbf{G}_{t,i}-\\Sigma_{t|t-1}^{-1}\\right)\\Sigma_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversio n. Using GtMC-EF $\\begin{array}{r}{-\\frac{1}{M}\\hat{\\mathbf{G}}_{t}^{(1:M)}\\hat{\\mathbf{G}}_{t}^{(1:M)^{\\top}}}\\end{array}$ or $\\mathbf{G}^{\\mathrm{LIN-EF}}\\;=\\;-\\pmb{g}_{t}^{\\mathrm{LIN}}\\,(\\pmb{g}_{t}^{\\mathrm{LIN}})^{\\top}$ allows the BONG-FC_MOM covariance update in Eq. (170) to scale quadratically, but this does not help here. Instead the BLR-FC_MOM update scales cubically because of the additional $\\boldsymbol{\\Sigma}_{t|t-1}^{-1}$ term that comes from the KL divergence in the VI objective. ", "page_idx": 29}, {"type": "text", "text": "E.2.4 BLR-LIN-HESS FC, Moment ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (173) and (174) gives the BLR-LIN-HESS-FC_MOM update ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\Sigma_{t,i-1}\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\Sigma_{t,i-1}\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}(y_{t}-\\hat{y}_{t,i})}\\\\ &{\\Sigma_{t,i}=\\left(1+\\alpha\\right)\\Sigma_{t,i-1}-\\alpha\\Sigma_{t,i-1}\\left(\\Sigma_{t|t-1}^{-1}+\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}\\right)\\Sigma_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversion in the $\\boldsymbol{\\Sigma}_{t|t-1}^{-1}$ term that comes from the $\\mathrm{KL}$ divergence in the VI objective. ", "page_idx": 30}, {"type": "text", "text": "E.2.5 BOG FC, Moment ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Substituting Eqs. (151) and (152) into Eq. (31) gives the BOG-FC_MOM update ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\pmb{\\mu}}_{t}={\\pmb{\\mu}}_{t|t-1}+\\alpha{\\pmb g}_{t}}}\\\\ {{\\bf\\pmb{\\Sigma}}_{t}={\\bf\\pmb{\\Sigma}}_{t|t-1}+\\frac{\\alpha}{2}{\\bf G}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note the mean update is vanilla online gradient descent (OGD) and does not depend on the covariance.   \nThis update takes $O(M P^{2})$ using ${\\bf G}_{t}^{\\mathrm{M\\bar{C}-H E S S}}$ or $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ and $O(P^{2})$ using $\\mathbf{G}_{t}^{\\mathrm{LI\\bar{N}-E F}}$ . ", "page_idx": 30}, {"type": "text", "text": "E.2.6 BOG-LIN-HESS FC, Moment ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (177) and (178) gives the BOG-LIN-HESS-FC_MOM update ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\pmb\\mu}_{t}={\\pmb\\mu}_{t\\mid t-1}+\\alpha{\\bf H}_{t}^{\\intercal}{\\bf R}_{t}^{-1}({\\pmb y}_{t}-\\hat{{\\pmb y}}_{t})}\\\\ &{{\\pmb\\Sigma}_{t}={\\pmb\\Sigma}_{t\\mid t-1}-\\frac{\\alpha}{2}{\\bf H}_{t}^{\\intercal}{\\bf R}_{t}^{-1}{\\bf H}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This update takes $O(C P^{2})$ . ", "page_idx": 30}, {"type": "text", "text": "E.2.7 BBB FC, Moment ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Substituting Eqs. (151) to (154) into Eq. (34) gives the BBB-FC_MOM ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\Sigma_{t|t-1}^{-1}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha g_{t,i}}\\\\ &{\\Sigma_{t,i}=\\Sigma_{t,i-1}+\\displaystyle\\frac{\\alpha}{2}\\left(\\Sigma_{t,i-1}^{-1}-\\Sigma_{t|t-1}^{-1}+\\mathbf{G}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversion, which traces back to the VI objective. Comparing to the BOG-FC_MOM update in Eqs. (177) and (178) (which has quadratic complexity in $P$ ), the extra terms here come from the KL part of Eq. (34). ", "page_idx": 30}, {"type": "text", "text": "E.2.8 BBB-LIN-HESS FC, Moment ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (181) and (182) gives the BBB-LIN-HESS-FC_MOM update ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\mu}_{t,i}=\\boldsymbol{\\mu}_{t,i-1}+\\alpha\\boldsymbol{\\Sigma}_{t|t-1}^{-1}\\left(\\boldsymbol{\\mu}_{t|t-1}-\\boldsymbol{\\mu}_{t,i-1}\\right)+\\alpha\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}(\\boldsymbol{y}_{t}-\\boldsymbol{\\hat{y}}_{t,i})}\\\\ &{\\;\\;\\boldsymbol{\\Sigma}_{t}=\\boldsymbol{\\Sigma}_{t,i-1}+\\frac{\\alpha}{2}\\left(\\boldsymbol{\\Sigma}_{t,i-1}^{-1}-\\boldsymbol{\\Sigma}_{t|t-1}^{-1}-\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This update takes $O(P^{3})$ per iteration because of the matrix inversion. Comparing to the BOG-LINHESS-FC_MOM update in Eqs. (179) and (180) (which has quadratic complexity in $P$ ), the extra terms here come from the KL part of Eq. (34). ", "page_idx": 30}, {"type": "text", "text": "E.3 Diagonal Gaussian, Natural parameters ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Throughout this subsection, vector multiplication and exponents are elementwise. ", "page_idx": 30}, {"type": "text", "text": "The natural and dual parameters for a diagonal Gaussian are given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\boldsymbol{\\psi}_{t,i-1}^{(1)}=\\boldsymbol{\\sigma}_{t,i-1}^{-2}\\boldsymbol{\\mu}_{t,i-1}}&&{\\boldsymbol{\\rho}_{t,i-1}^{(1)}=\\boldsymbol{\\mu}_{t,i-1}}\\\\ &{\\boldsymbol{\\psi}_{t,i-1}^{(2)}=-\\frac{1}{2}\\boldsymbol{\\sigma}_{t,i-1}^{-2}}&&{\\boldsymbol{\\rho}_{t,i-1}^{(2)}=\\boldsymbol{\\mu}_{t,i-1}\\boldsymbol{\\mu}_{t,i-1}^{\\intercal}+\\boldsymbol{\\sigma}_{t,i-1}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Inverting these relationships gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i-1}=-\\frac{1}{2}\\left(\\psi_{t,i-1}^{(2)}\\right)^{-1}\\psi_{t,i-1}^{(1)}=\\rho_{t,i-1}^{(1)}}\\\\ &{\\sigma_{t,i-1}^{2}=-\\frac{1}{2}\\left(\\psi_{t,i-1}^{(2)}\\right)^{-1}\\qquad\\qquad=\\rho_{t,i-1}^{(2)}-\\left(\\rho_{t,i-1}^{(1)}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The KL divergence in the VI loss is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{~p_{RL}}\\!\\left(q_{\\psi_{t,i-1}}|q_{\\psi_{t,1-1}}\\right)=\\frac{1}{2}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)^{2}\\sigma_{t|t-1}^{-2}+\\frac{1}{2}\\sum\\left(\\sigma_{t|t-1}^{-2}\\sigma_{t,i-1}^{2}-\\log\\sigma_{t,i-1}^{2}\\right)+\\mathrm{const}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with gradients ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\pmb{\\sigma}_{t\\mid t-1}^{-2}\\left(\\pmb{\\mu}_{t,i-1}-\\pmb{\\mu}_{t\\mid t-1}\\right)}\\\\ &{\\nabla_{\\pmb{\\sigma}_{t,i-1}^{2}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\frac{1}{2}\\left(\\pmb{\\sigma}_{t\\mid t-1}^{-2}-\\pmb{\\sigma}_{t,i-1}^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For any scalar function $\\ell$ the chain rule gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t,i-1}^{(1)}}\\ell=\\displaystyle\\frac{\\partial\\mu_{t,i-1}}{\\partial\\rho_{t,i-1}^{(1)}}\\nabla_{\\mu_{t,i-1}\\ell}+\\displaystyle\\frac{\\partial\\sigma_{t,i-1}^{2}}{\\partial\\rho_{t,i-1}^{(1)}}\\nabla_{\\sigma_{t,i-1}^{2}\\ell}}\\\\ &{\\qquad\\qquad=\\nabla_{\\mu_{t,i-1}\\ell}-2\\mu_{t,i-1}\\nabla_{\\sigma_{t,i-1}^{2}\\ell}}\\\\ &{\\nabla_{\\rho_{t,i-1}^{(2)}}\\ell=\\displaystyle\\frac{\\partial\\mu_{t,i-1}}{\\partial\\rho_{t,i-1}^{(2)}}\\nabla_{\\mu_{t,i-1}\\ell}+\\displaystyle\\frac{\\partial\\sigma_{t,i-1}^{2}}{\\partial\\rho_{t,i-1}^{(2)}}\\nabla_{\\sigma_{t,i-1}^{2}\\ell}}\\\\ &{\\qquad\\qquad=\\nabla_{\\sigma_{t,i-1}^{2}\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t,i-1}^{(1)}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(y_{t}\\middle|f_{t}\\left(\\theta_{t}\\right))]=g_{t,i}-\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)\\mu_{t,i-1}}\\\\ &{\\nabla_{\\rho_{t,i-1}^{(2)}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(y_{t}\\middle|f_{t}\\left(\\theta_{t}\\right)\\right)]=\\frac{1}{2}\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\rho_{t,i-1}^{(1)}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=\\sigma_{t,i-1}^{-2}\\mu_{t,i-1}-\\sigma_{t|t-1}^{-2}\\mu_{t|t-1}}\\\\ &{\\nabla_{\\rho_{t,i-1}^{(2)}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=\\frac{1}{2}\\left(\\sigma_{t|t-1}^{-2}-\\sigma_{t,i-1}^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Following the same approach for $\\psi$ gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi_{t,i-1}^{(1)}}\\ell=\\frac{\\partial\\mu_{t,i-1}}{\\partial\\psi_{t,i-1}^{(1)}}\\nabla_{\\mu_{t,i-1}}\\ell+\\frac{\\partial\\sigma_{t,i-1}^{2}}{\\partial\\psi_{t,i-1}^{(1)}}\\nabla_{\\Sigma_{t,i-1}}\\ell}\\\\ &{\\quad\\quad=-\\frac{1}{2}\\left(\\psi_{t,i-1}^{(2)}\\right)^{-1}\\nabla_{\\mu_{t,i-1}}\\ell}\\\\ &{\\quad\\quad=\\sigma_{t,i-1}^{2}\\nabla_{\\mu_{t,i-1}}\\ell}\\\\ &{\\nabla_{\\psi_{t,i-1}^{(2)}}\\ell=\\frac{\\partial\\mu_{t,i-1}}{\\partial\\psi_{t,i-1}^{(2)}}\\nabla_{\\mu_{t,i-1}}\\ell+\\frac{\\partial\\sigma_{t,i-1}^{2}}{\\partial\\psi_{t,i-1}^{(2)}}\\nabla_{\\sigma_{t,i-1}^{2}}\\ell}\\\\ &{\\quad\\quad=\\frac{1}{2}\\left(\\psi_{t,i-1}^{(2)}\\right)^{-2}\\psi_{t,i-1}^{(1)}\\nabla_{\\mu_{t,i-1}}\\ell+\\frac{1}{2}\\left(\\psi_{t,i-1}^{(2)}\\right)^{-2}\\nabla_{\\sigma_{t,i-1}^{2}}\\ell}\\\\ &{\\quad\\quad=2\\sigma_{t,i-1}^{2}\\mu_{t,i-1}\\nabla_{\\mu_{t,i-1}}\\ell+2\\sigma_{t,i-1}^{4}\\nabla_{\\sigma_{t,i-1}^{2}}\\ell}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi_{t,i-1}^{(1)}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}\\bigl[\\log p\\left(y_{t}\\middle|f_{t}\\left(\\theta_{t}\\right)\\right)\\bigr]=\\sigma_{t,i-1}^{2}g_{t,i}}\\\\ &{\\nabla_{\\psi_{t,i-1}^{(2)}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}\\bigl[\\log p\\left(y_{t}\\middle|f_{t}\\left(\\theta_{t}\\right)\\right)\\bigr]=2\\sigma_{t,i-1}^{2}\\mu_{t,i-1}g_{t,i}+\\sigma_{t,i-1}^{4}\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi_{t,i-1}^{(1)}}D_{\\mathbb{K}\\mathbb{L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)}\\\\ &{\\nabla_{\\psi_{t,i-1}^{(2)}}D_{\\mathbb{K}\\mathbb{L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=2\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\mu_{t,i-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\sigma_{t,i-1}^{4}\\left(\\sigma_{t|t-1}^{-2}-\\sigma_{t,i-1}^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Our implementations often make use of the following trick: Suppose $\\mathbf{A}\\in\\mathbb{R}^{n\\times m}$ and $\\mathbf{B}\\in\\mathbb{R}^{m\\times n}$ .   \nThen we can efficiently compute diag(AB) in $O(m n)$ time using $\\begin{array}{r}{(\\mathbf{A}\\mathbf{B})_{i i}=\\sum_{j=1}^{M}A_{i j}B_{j i}}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "For MC-HESS methods, we approximate the diagonal of the Hessian for each MC sample $\\hat{\\pmb{\\theta}}_{t}^{(m)}$ using Hutchinson\u2019s trace estimation method [Hutchinson, 1989] which has been used in other DNN optimization papers such as adahessian Yao et al. [2021]. This involves an extra inner loop with size denoted $N$ . ", "page_idx": 32}, {"type": "text", "text": "E.3.1 BONG Diag ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Substituting Eqs. (196) and (197) into Eq. (5) gives ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\psi}_{t}^{(1)}=\\pmb{\\psi}_{t|t-1}^{(1)}+\\pmb{g}_{t}-\\operatorname{diag}\\left(\\mathbf{G}_{t}\\right)\\pmb{\\mu}_{t|t-1}}\\\\ &{\\pmb{\\psi}_{t}^{(2)}=\\pmb{\\psi}_{t|t-1}^{(2)}+\\frac{1}{2}\\mathrm{diag}\\left(\\mathbf{G}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Translating to $\\left(\\mu_{t},\\sigma_{t}^{2}\\right)$ gives the BONG-DIAG update ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\mu}_{t}=\\pmb{\\mu}_{t|t-1}+\\pmb{\\sigma}_{t}^{2}\\pmb{g}_{t}}\\\\ &{\\pmb{\\sigma}_{t}^{-2}=\\pmb{\\sigma}_{t|t-1}^{-2}-\\mathrm{diag}\\left(\\mathbf{G}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This update takes $O(M P)$ to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ using $\\mathbf{G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and $O(P)$ using ${\\bf G}_{t}^{\\scriptscriptstyle\\mathrm{LIN-EF}}$ . ", "page_idx": 32}, {"type": "text", "text": "E.3.2 BONG-LIN-HESS Diag (VD-EKF) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (210) and (211) gives the BONG-LIN-HESS-DIAG update ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\mu}_{\\boldsymbol{t}}=\\mu_{t\\vert t-1}+\\pmb{\\sigma}_{t}^{2}\\left(\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\left(\\pmb{y}_{t}-\\hat{\\pmb{y}}_{t}\\right)\\right)}\\\\ {\\pmb{\\sigma}_{t}^{-2}=\\pmb{\\sigma}_{t\\vert t-1}^{-2}+\\mathrm{diag}\\left(\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This update is equivalent to VD-EKF [Chang et al., 2022] and takes $O(C^{2}P)$ . ", "page_idx": 32}, {"type": "text", "text": "E.3.3 BLR Diag (VON) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Substituting Eqs. (196) to (199) into Eq. (33) gives ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t,i}^{(1)}=\\psi_{t,i-1}^{(1)}+\\alpha\\left(g_{t,i}-\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)\\mu_{t,i-1}-\\sigma_{t,i-1}^{-2}\\mu_{t,i-1}+\\sigma_{t|t-1}^{-2}\\mu_{t|t-1}\\right)}\\\\ &{\\psi_{t,i}^{(2)}=\\psi_{t,i-1}^{(2)}+\\cfrac{\\alpha}{2}\\left(\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)+\\sigma_{t,i-1}^{-2}-\\sigma_{t|t-1}^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Translating to $(\\mu_{t,i},\\sigma_{t,i}^{2})$ gives the BLR-DIAG update ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\sigma_{t,i}^{2}\\sigma_{t|t-1}^{-2}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\sigma_{t,i}^{2}g_{t,i}}\\\\ &{\\sigma_{t,i}^{-2}=\\left(1-\\alpha\\right)\\sigma_{t,i-1}^{-2}+\\alpha\\sigma_{t|t-1}^{-2}-\\alpha\\operatorname{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This update takes $O(M P)$ per iteration to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ per iteration using $\\mathbf{G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and ${\\cal O}(P)$ per iteration usintg $\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}$ . ", "page_idx": 32}, {"type": "text", "text": "The MC-HESS and MC-EF versions of this update are respectively equivalent to VON and VOGN [Khan et al., 2018b] in the batch setting where we replace $q_{\\psi_{t\\mid t-1}}$ with a spherical prior $\\mathcal{N}(\\mathbf{0},\\lambda^{-1}\\mathbf{I}_{P})$ (see Appendix E.6). ", "page_idx": 32}, {"type": "text", "text": "E.3.4 BLR-LIN-HESS Diag ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (218) and (219) gives the BLR-LIN-HESS-DIAG update ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\sigma_{t,i}^{2}\\sigma_{t|t-1}^{-2}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\sigma_{t,i}^{2}\\left(\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\left(y_{t}-\\hat{y}_{t,i}\\right)\\right)}\\\\ &{\\sigma_{t,i}^{-2}=\\left(1-\\alpha\\right)\\sigma_{t,i-1}^{-2}+\\alpha\\sigma_{t|t-1}^{-2}+\\alpha\\operatorname{diag}\\left(\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This update takes $O(C^{2}P)$ per iteration. ", "page_idx": 33}, {"type": "text", "text": "E.3.5 BOG Diag ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Substituting Eqs. (206) and (207) into Eq. (31) gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\psi}_{t}^{(1)}=\\boldsymbol{\\psi}_{t\\mid t-1}^{(1)}+\\alpha\\boldsymbol{\\sigma}_{t\\mid t-1}^{2}\\boldsymbol{g}_{t}}\\\\ &{\\boldsymbol{\\psi}_{t}^{(2)}=\\boldsymbol{\\psi}_{t\\mid t-1}^{(2)}+2\\alpha\\boldsymbol{\\sigma}_{t\\mid t-1}^{2}\\mu_{t\\mid t-1}\\boldsymbol{g}_{t}+\\alpha\\boldsymbol{\\sigma}_{t\\mid t-1}^{4}\\mathrm{diag}\\left(\\mathbf{G}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Translating to $\\left(\\mu_{t},\\sigma_{t}^{2}\\right)$ gives the BOG-DIAG update ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mu_{t}=\\sigma_{t}^{2}\\sigma_{t|t-1}^{-2}\\mu_{t|t-1}+\\alpha\\sigma_{t}^{2}\\sigma_{t|t-1}^{2}g_{t}}\\\\ &{\\sigma_{t}^{-2}=\\sigma_{t|t-1}^{-2}-4\\alpha\\sigma_{t|t-1}^{2}\\mu_{t|t-1}g_{t}-2\\alpha\\sigma_{t|t-1}^{4}\\mathrm{diag}\\left(\\mathbf{G}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This update takes $O(M P)$ to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ using $\\mathbf{G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and $O(P)$ using ${\\bf G}_{t}^{\\scriptscriptstyle\\mathrm{LIN-EF}}$ . ", "page_idx": 33}, {"type": "text", "text": "E.3.6 BOG-LIN-HESS Diag ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (224) and (225) gives the BOG-LIN-HESS-DIAG update ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mu_{t}=\\sigma_{t}^{2}\\sigma_{t\\mid t-1}^{-2}\\mu_{t\\mid t-1}+\\alpha\\sigma_{t}^{2}\\sigma_{t\\mid t-1}^{2}\\left(\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\left(y_{t}-\\hat{y}_{t}\\right)\\right)}\\\\ &{\\sigma_{t}^{-2}=\\sigma_{t\\mid t-1}^{-2}-4\\alpha\\sigma_{t\\mid t-1}^{2}\\mu_{t\\mid t-1}\\left(\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\left(y_{t}-\\hat{y}_{t}\\right)\\right)+2\\alpha\\sigma_{t\\mid t-1}^{4}\\mathrm{diag}\\left(\\mathbf{H}_{t}^{\\intercal}\\mathbf{R}_{t}^{-1}\\mathbf{H}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This update takes $O(C^{2}P)$ . ", "page_idx": 33}, {"type": "text", "text": "E.3.7 BBB Diag ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Substituting Eqs. (206) to (209) into Eq. (34) gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t,i}^{(1)}=\\psi_{t,i-1}^{(1)}+\\alpha\\sigma_{t,i-1}^{2}g_{t,i}-\\alpha\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)}\\\\ &{\\psi_{t}^{(2)}=\\psi_{t,i-1}^{(2)}+2\\alpha\\sigma_{t,i-1}^{2}\\mu_{t,i-1}g_{t,i}+\\alpha\\sigma_{t,i-1}^{4}\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\\\ &{\\qquad\\,\\,-\\,2\\alpha\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\mu_{t,i-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)-\\alpha\\sigma_{t,i-1}^{4}\\left(\\sigma_{t|t-1}^{-2}-\\sigma_{t,i-1}^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Translating to $(\\mu_{t,i},\\Sigma_{t,i})$ gives the BBB-DIAG update ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\sigma_{t,i}^{2}\\sigma_{t,i-1}^{-2}\\mu_{t,i-1}+\\alpha\\sigma_{t,i}^{2}\\sigma_{t,i-1}^{2}g_{t,i}+\\alpha\\sigma_{t,i}^{2}\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)}\\\\ &{\\sigma_{t,i}^{-2}=\\sigma_{t,i-1}^{-2}-4\\alpha\\sigma_{t,i-1}^{2}\\mu_{t,i-1}g_{t,i}-2\\alpha\\sigma_{t,i-1}^{4}\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\\\ &{\\qquad\\,\\,+\\,4\\alpha\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\mu_{t,i-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)+2\\alpha\\sigma_{t,i-1}^{4}\\left(\\sigma_{t|t-1}^{-2}-\\sigma_{t,i-1}^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This update takes $O(M P)$ per iteration to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ per iteration using ${\\bf G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and ${\\cal O}(P)$ per iteration using $\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}$ . ", "page_idx": 33}, {"type": "text", "text": "E.3.8 BBB-LIN-HESS Diag ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (230) and (231) gives the BBB-LIN-HESS-DIAG update ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\sigma_{t,i}^{2}\\sigma_{t,i-1}^{-2}\\mu_{t,i-1}+\\alpha\\sigma_{t,i}^{2}\\sigma_{t,i-1}^{2}\\left(\\mathbf H_{t,i}^{\\top}\\mathbf R_{t,i}^{-1}\\left(y_{t}-\\hat{y}_{t,i}\\right)\\right)}\\\\ &{\\qquad+\\alpha\\sigma_{t,i}^{2}\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)}\\\\ &{\\sigma_{t,i}^{-2}=\\sigma_{t,i-1}^{-2}-4\\alpha\\sigma_{t,i-1}^{2}\\mu_{t,i-1}\\left(\\mathbf H_{t,i}^{\\top}\\mathbf R_{t,i}^{-1}\\left(y_{t}-\\hat{y}_{t,i}\\right)\\right)+2\\alpha\\sigma_{t,i-1}^{4}\\mathrm{diag}\\left(\\mathbf H_{t,i}^{\\top}\\mathbf R_{t,i}^{-1}\\mathbf H_{t,i}\\right)}\\\\ &{\\qquad+4\\alpha\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\mu_{t,i-1}\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)+2\\alpha\\sigma_{t,i-1}^{4}\\sigma_{t|t-1}^{-2}-2\\alpha\\sigma_{t,i-1}^{2}\\qquad\\mathrm{()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This update takes $O(C^{2}P)$ per iteration. ", "page_idx": 33}, {"type": "text", "text": "E.4 Diagonal Gaussian, Moment parameters ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Throughout this subsection, vector multiplication and exponents are elementwise. The Bonnet and Price theorems give ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\pmb{y}_{t}\\vert\\,f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]=\\pmb{g}_{t,i}}\\\\ &{\\nabla_{\\pmb{\\sigma}_{t,i-1}^{2}}\\mathbb{E}_{\\pmb{\\theta}_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(\\pmb{y}_{t}\\vert\\,f_{t}\\left(\\pmb{\\theta}_{t}\\right)\\right)]=\\frac{1}{2}\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "From Appendix E.3 we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\pmb{\\sigma}_{t\\mid t-1}^{-2}\\left(\\pmb{\\mu}_{t,i-1}-\\pmb{\\mu}_{t\\mid t-1}\\right)}\\\\ &{\\nabla_{\\pmb{\\sigma}_{t,i-1}^{2}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}\\vert q_{\\psi_{t\\mid t-1}}\\big)=\\frac{1}{2}\\left(\\pmb{\\sigma}_{t\\mid t-1}^{-2}-\\pmb{\\sigma}_{t,i-1}^{-2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We write the Fisher with respect to the moment parameters $\\psi=(\\mu,\\sigma^{2})$ as a block matrix: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{F}_{\\psi}=\\left[\\begin{array}{c c}{\\mathbf{F}_{\\mu,\\mu}}&{\\mathbf{F}_{\\mu,\\sigma^{2}}}\\\\ {\\mathbf{F}_{\\sigma^{2},\\mu}}&{\\mathbf{F}_{\\sigma^{2},\\sigma^{2}}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The blocks can be calculated by the second-order Fisher formula ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{F}_{\\mu,\\mu}=-\\mathbb{E}_{q_{\\phi}}[\\nabla_{\\mu,\\mu}\\log q_{\\psi}(\\theta)]}\\\\ &{\\quad\\quad\\quad=\\mathrm{Diag}\\left(\\sigma^{-2}\\right)}\\\\ &{\\mathbf{F}_{\\mu,\\sigma^{2}}=-\\mathbb{E}_{q_{\\psi}}[\\nabla_{\\mu,\\sigma^{2}}\\log q_{\\psi}(\\theta)]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{q_{\\psi}}[\\mathrm{Diag}\\left((\\theta-\\mu)\\sigma^{-4}\\right)]}\\\\ &{\\quad\\quad\\quad=0}\\\\ &{\\mathbf{F}_{\\sigma^{2},\\sigma^{2}}=-\\mathbb{E}_{q_{\\psi}}\\left[\\nabla_{\\sigma^{2},\\sigma^{2}}\\log q_{\\psi}(\\theta)\\right]}\\\\ &{\\quad\\quad\\quad=-\\mathbb{E}_{q_{\\psi}}\\left[\\mathrm{Diag}\\left(-\\left(\\mu-\\theta\\right)^{2}\\sigma^{-6}+\\frac{1}{2}\\sigma^{-4}\\right)\\right]}\\\\ &{\\quad\\quad\\quad=\\frac{1}{2}\\mathrm{Diag}\\left(\\sigma^{-4}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore the preconditioner for the NGD methods is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{F}_{\\psi_{t,i-1}}^{-1}=\\left[\\begin{array}{c c}{\\operatorname{Diag}\\left(\\sigma_{t,i-1}^{2}\\right)}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{2\\operatorname{Diag}\\left(\\sigma_{t,i-1}^{4}\\right)}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "E.4.1 BONG Diag, Moment ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Substituting Eqs. (234), (235) and (247) into Eq. (3) gives the BONG-DIAG_MOM update ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\mu}_{t}=\\pmb{\\mu}_{t|t-1}+\\pmb{\\sigma}_{t|t-1}^{2}\\pmb{g}_{t}}\\\\ &{\\pmb{\\sigma}_{t}^{2}=\\pmb{\\sigma}_{t|t-1}^{2}+\\pmb{\\sigma}_{t|t-1}^{4}\\mathrm{diag}\\left(\\mathbf{G}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This update takes $O(M P)$ to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ using $\\mathbf{G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and $O(P)$ using ${\\bf G}_{t}^{\\scriptscriptstyle\\mathrm{LIN-EF}}$ . ", "page_idx": 34}, {"type": "text", "text": "E.4.2 BONG-LIN-HESS Diag, Momemt ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (248) and (249) gives the BONG-LIN-HESS-DIAG_MOM update ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol\\mu}_{t}={\\boldsymbol\\mu}_{t\\vert t-1}+{\\boldsymbol\\sigma}_{t\\vert t-1}^{2}\\left({\\bf H}_{t}^{\\intercal}{\\bf R}_{t}^{-1}({\\boldsymbol y}_{t}-\\hat{{\\boldsymbol y}}_{t})\\right)}\\\\ &{{\\boldsymbol\\sigma}_{t}^{2}={\\boldsymbol\\sigma}_{t\\vert t-1}^{2}-{\\boldsymbol\\sigma}_{t\\vert t-1}^{4}\\mathrm{diag}\\left({\\bf H}_{t}^{\\intercal}{\\bf R}_{t}^{-1}{\\bf H}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This update takes $O(C^{2}P)$ . ", "page_idx": 34}, {"type": "text", "text": "E.4.3 BLR Diag, Moment ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Substituting Eqs. (234) to (237) and (247) into Eq. (32) gives the BLR-DIAG_MOM update ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\sigma_{t,i-1}^{2}\\sigma_{t|t-1}^{-2}\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\sigma_{t,i-1}^{2}g_{t,i}}\\\\ &{\\sigma_{t,i}^{2}=\\sigma_{t,i-1}^{2}+\\alpha\\sigma_{t,i-1}^{4}\\left(\\sigma_{t,i-1}^{-2}-\\sigma_{t|t-1}^{-2}\\right)+\\alpha\\sigma_{t,i-1}^{4}\\mathrm{diag}\\left(\\mathbf{G}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This update takes $O(M P)$ per iteration to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ per iteration using $\\mathbf{G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and ${\\cal O}(P)$ per iteration usintg $\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}$ . ", "page_idx": 34}, {"type": "text", "text": "E.4.4 BLR-LIN-HESS Diag, Moment ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (252) and (253) gives the BLR-LIN-HESS-DIAG_MOM update ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mu_{t,i}}={\\mu_{t,i-1}}+\\alpha{\\sigma_{t,i-1}^{2}}{\\sigma_{t|t-1}^{-2}}\\left({\\mu_{t|t-1}}-{\\mu_{t,i-1}}\\right)+\\alpha{\\sigma_{t,i-1}^{2}}\\left(\\mathbf{H}_{t,i}^{\\top}\\mathbf{R}_{t,i}^{-1}(y_{t}-\\hat{y}_{t,i})\\right)}\\\\ &{\\sigma_{t,i}^{2}=\\sigma_{t,i-1}^{2}+\\alpha{\\sigma_{t,i-1}^{4}}\\left(\\sigma_{t,i-1}^{-2}-\\sigma_{t|t-1}^{-2}\\right)-\\alpha\\sigma_{t,i-1}^{4}\\mathrm{diag}\\left(\\mathbf{H}_{t,i}^{\\top}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This update takes $O(C^{2}P)$ per iteration. ", "page_idx": 35}, {"type": "text", "text": "E.4.5 BOG Diag, Moment ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Substituting Eqs. (234) and (235) into Eq. (31) gives the BOG-DIAG_MOM update ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\pmb{\\mu}}_{t}={\\pmb{\\mu}}_{t|t-1}+\\alpha{\\pmb g}_{t}}\\\\ {\\displaystyle{\\pmb\\sigma}_{t}^{2}={\\pmb\\sigma}_{t|t-1}^{2}+\\frac{\\alpha}{2}\\mathrm{diag}\\left({\\bf G}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This update takes $O(M P)$ to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ using $\\mathbf{G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and $O(P)$ using ${\\bf G}_{t}^{\\scriptscriptstyle\\mathrm{LIN-EF}}$ . ", "page_idx": 35}, {"type": "text", "text": "E.4.6 BOG-LIN-HESS Diag, Moment ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (256) and (257) gives the BOG-LIN-HESS-DIAG_MOM update ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol\\mu}_{t}={\\boldsymbol\\mu}_{t\\mid t-1}+\\alpha\\mathbf{H}_{t}^{\\intercal}{\\mathbf{R}}_{t}^{-1}({\\boldsymbol y}_{t}-\\hat{{\\boldsymbol y}}_{t})}\\\\ &{{\\boldsymbol\\sigma}_{t}^{2}={\\boldsymbol\\sigma}_{t\\mid t-1}^{2}-\\frac{\\alpha}{2}\\mathrm{diag}\\left(\\mathbf{H}_{t}^{\\intercal}{\\mathbf{R}}_{t}^{-1}\\mathbf{H}_{t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This update takes $O(C^{2}P)$ . ", "page_idx": 35}, {"type": "text", "text": "E.4.7 BBB Diag, Moment ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Substituting Eqs. (234) to (237) into Eq. (34) gives the BBB-DIAG_MOM ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\mu}_{t,i}=\\pmb{\\mu}_{t,i-1}+\\alpha\\pmb{\\sigma}_{t|t-1}^{-2}\\left(\\pmb{\\mu}_{t|t-1}-\\pmb{\\mu}_{t,i-1}\\right)+\\alpha\\pmb{g}_{t,i}}\\\\ &{\\pmb{\\sigma}_{t}^{2}=\\pmb{\\sigma}_{t,i-1}^{2}+\\frac{\\alpha}{2}\\left(\\pmb{\\sigma}_{t,i-1}^{-2}-\\pmb{\\sigma}_{t|t-1}^{-2}\\right)+\\frac{\\alpha}{2}\\mathrm{diag}\\left(\\pmb{\\mathrm{G}}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This update takes $O(M P)$ per iteration to estimate $\\mathbf{G}_{t}$ using $\\mathbf{G}_{t}^{\\mathrm{MC-EF}}$ , $O(N M P)$ per iteration using $\\mathbf{G}_{t}^{\\mathrm{MC-HESS}}$ and Hutchinson\u2019s method, and ${\\cal O}(P)$ per iteration using $\\mathbf{G}_{t}^{\\mathrm{LIN-EF}}$ . ", "page_idx": 35}, {"type": "text", "text": "This is similar to the original diagonal Gaussian method in [Blundell et al., 2015] except (1) they reparameterize $\\pmb{\\sigma}=\\log(1+\\exp(\\pmb{\\rho}))$ (elementwise) and do GD on $(\\mu,\\rho)$ instead of $(\\mu,\\sigma^{2})$ , and (2) they use the reparameterization trick instead of Price\u2019s theorem for calculating the gradient with respect to $\\rho$ (via $\\pmb{\\sigma}$ ). ", "page_idx": 35}, {"type": "text", "text": "E.4.8 BBB-LIN-HESS Diag, Moment ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (260) and (261) gives the BBB-LIN-HESS-DIAG_MOM update ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\mu}_{t,i}=\\boldsymbol{\\mu}_{t,i-1}+\\alpha\\sigma_{t|t-1}^{-2}\\left(\\boldsymbol{\\mu}_{t|t-1}-\\boldsymbol{\\mu}_{t,i-1}\\right)+\\alpha\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}(\\boldsymbol{y}_{t}-\\hat{\\boldsymbol{y}}_{t,i})}\\\\ &{\\boldsymbol{\\sigma}_{t}^{2}=\\sigma_{t,i-1}^{2}+\\frac{\\alpha}{2}\\left(\\boldsymbol{\\sigma}_{t,i-1}^{-2}-\\boldsymbol{\\sigma}_{t|t-1}^{-2}\\right)-\\frac{\\alpha}{2}\\mathrm{diag}\\left(\\mathbf{H}_{t,i}^{\\intercal}\\mathbf{R}_{t,i}^{-1}\\mathbf{H}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This update takes $O(C^{2}P)$ per iteration. ", "page_idx": 35}, {"type": "text", "text": "E.5 Diagonal plus low rank ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Assume the prior is given by ", "page_idx": 35}, {"type": "equation", "text": "$$\nq_{\\psi_{t|t-1}}\\left(\\pmb{\\theta}_{t}\\right)=\\mathcal{N}\\left(\\pmb{\\theta}_{t}|\\pmb{\\mu}_{t|t-1},\\left(\\pmb{\\Upsilon}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\top}\\right)^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with $\\mathbf{W}\\in\\mathbb{R}^{P\\times R}$ and diagonal $\\mathbf{Y}_{t\\vert t-1}\\in\\mathbb{R}^{P\\times P}$ . We sometimes abuse notation by writing $\\mathbf{\\DeltaY}_{t|t-1}$ for the vector diag $\\left(\\mathbf{\\Upsilon}(\\mathbf{Y}_{t|t-1}\\right)$ when the meaning is clear from context. ", "page_idx": 36}, {"type": "text", "text": "Substituting the DLR form in the gradients for the KL divergence derived in Appendix E.2 gives ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}D_{\\mathbb{K}\\mathbb{L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t,|t-1}}\\big)=\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}\\right)\\left(\\mu_{t,i-1}-\\mu_{t|t-1}\\right)\\qquad\\qquad(2\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}})=\\mathbf{Y}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}},}\\\\ &{\\nabla_{\\mathbf{Z}_{t,i-1}}D_{\\mathbb{K}\\mathbb{L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=\\frac{1}{2}\\left(\\mathbf{Y}_{t|t-1}-\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}-\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\mathsf{T}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For any function $\\ell$ the chain rule gives ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathbf{Y}_{t\\mid t-1}}\\ell=-\\mathrm{diag}\\left(\\left(\\mathbf{Y}_{t\\mid t-1}+\\mathbf{W}_{t\\mid t-1}\\mathbf{W}_{t\\mid t-1}^{\\top}\\right)^{-1}\\left(\\nabla_{\\mathbf{\\overline{{X}}}_{t\\mid t-1}}\\ell\\right)\\left(\\mathbf{Y}_{t\\mid t-1}+\\mathbf{W}_{t\\mid t-1}\\mathbf{W}_{t\\mid t-1}^{\\top}\\right)^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{\\mathbf{W}_{t\\mid t-1}}\\ell=-2\\left(\\mathbf{Y}_{t\\mid t-1}+\\mathbf{W}_{t\\mid t-1}\\mathbf{W}_{t\\mid t-1}^{\\top}\\right)^{-1}\\left(\\nabla_{\\mathbf{\\Sigma}_{t\\mid t-1}}\\ell\\right)\\left(\\mathbf{Y}_{t\\mid t-1}+\\mathbf{W}_{t\\mid t-1}\\mathbf{W}_{t\\mid t-1}^{\\top}\\right)^{-1}\\mathbf{W}_{t\\mid t-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore the gradients we need are ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{t,i-1}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(y_{t}|h_{t}\\left(\\theta_{t}\\right)\\right)]=g_{\\psi_{t,i}}}&{(2\\psi_{t,i-1})}\\\\ &{\\nabla_{\\mathbf{T}_{t,i-1}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}[\\log p\\left(y_{t}|h_{t}\\left(\\theta_{t}\\right)\\right)]=-\\frac{1}{2}\\mathrm{diag}\\left(\\begin{array}{l}{\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\top}\\right)^{-1}\\mathbf{G}_{t,i}}\\\\ {\\times\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\top}\\right)^{-1}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{W}_{t,i-1}}\\mathbb{E}_{\\theta_{t}\\sim q_{\\psi_{t,i-1}}}\\left[\\log p\\left({y}_{t}|{h}_{t}\\left(\\theta_{t}\\right)\\right)\\right]=-\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\intercal}\\right)^{-1}\\mathbf{G}_{t,i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\intercal}\\right)^{-1}\\mathbf{W}_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{Y}_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=}\\\\ &{\\qquad\\qquad-\\,\\frac{1}{2}\\mathrm{diag}\\left(\\begin{array}{l}{\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\intercal}\\right)^{-1}}\\\\ {\\times\\left(\\mathbf{Y}_{t|t-1}-\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\intercal}-\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\intercal}\\right)}\\\\ {\\times\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\intercal}\\right)^{-1}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{W}_{t,i-1}}D_{\\mathbb{K L}}\\big(q_{\\psi_{t,i-1}}|q_{\\psi_{t|t-1}}\\big)=-\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\mathsf{T}}\\right)^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\times\\left(\\mathbf{Y}_{t|t-1}-\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}-\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\mathsf{T}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\times\\left(\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\mathsf{T}}\\right)^{-1}\\mathbf{W}_{t,i-1}}\\end{array}\\,(27)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The Fisher matrix can be decomposed as a block-diagonal with blocks for $\\pmb{\\mu}_{t,i}$ and for $(\\mathbf{\\DeltaY}_{t,i},\\mathbf{W}_{t,i})$ , but (in contrast to the FC Gaussian case in Eq. (168)) we have not found an efficient way to analytically invert the latter block, which has size $P+R P$ . To avoid the $O\\left(R^{3}P^{3}\\right)$ cost of brute force inversion, we use a different strategy for BONG and BLR of performing the update on the natural parameters of the FC Gaussian as in Appendix E.1 and projecting the result back to rank $R$ using SVD. Specifically, assume the updated precision from applying Eq. (121) for BONG or Eq. (136) for BLR can be written as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{t,i}^{-1}=\\tilde{\\mathbf{T}}_{t,i}+\\tilde{\\mathbf{W}}_{t,i}\\tilde{\\mathbf{W}}_{t,i}^{\\top}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and let the SVD of $\\tilde{\\mathbf{W}}_{t,i}$ be ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{W}}_{t,i}=\\mathbf{U}_{t,i}\\mathbf{\\Lambda}_{{t},i}\\mathbf{V}_{t,i}^{\\top}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with $\\mathbf{U}_{t,i},\\mathbf{V}_{t,i}$ orthogonal and $\\mathbf{\\Lambda}_{\\Lambda_{t,i}}$ rectangular-diagonal. Following [Mishkin et al., 2018, Chang et al., 2023] we define the update ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}_{t,i}=\\mathbf{U}_{t,i}\\left[:,:R\\right]\\mathbf{A}_{t,i}\\left[:R,:R\\right]}\\\\ &{\\mathbf{Y}_{t,i}=\\tilde{\\mathbf{Y}}_{t,i}+\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}_{t,i}\\tilde{\\mathbf{W}}_{t,i}^{\\mathsf{T}}-\\mathbf{W}_{t,i}\\mathbf{W}_{t,i}^{\\mathsf{T}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "so that $\\mathbf{W}_{t,i}$ contains the top $R$ singular vectors and values of the FC posterior and the diagonal is preserved: diag $\\left(\\mathbf{Y}_{t,i}+\\mathbf{W}_{t,i}\\mathbf{W}_{t,i}^{\\top}\\right)=\\operatorname{diag}\\left({\\tilde{\\Sigma}}_{t,i}^{-1}\\right)$ . This approach works for MC-EF and LIN-EF methods but not MC-HESS, which we omit. ", "page_idx": 37}, {"type": "text", "text": "Finally, in the MC-EF methods we sample from the DLR prior using the routine in [Mishkin et al., 2018, Lambert et al., 2023] which takes $O(R(R+M)P)$ . ", "page_idx": 37}, {"type": "text", "text": "E.5.1 BONG DLR ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Substituting the DLR prior from Eq. (264) into the FC precision update from Eq. (121) and using the MC-EF approximation yields the posterior precision ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{\\Delta}}_{\\mathbf{\\Delta}}^{-1}=\\mathbf{\\Delta}\\mathbf{\\mathcal{T}}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\top}-\\mathbf{G}_{t}^{\\mathrm{MC-EF}}}\\\\ &{\\quad\\quad=\\tilde{\\mathbf{T}}_{t}+\\tilde{\\mathbf{W}}_{t}\\tilde{\\mathbf{W}}_{t}^{\\top}}\\\\ &{\\quad\\tilde{\\mathbf{Y}}_{t}=\\mathbf{Y}_{t|t-1}}\\\\ &{\\tilde{\\mathbf{W}}_{t}=\\left[\\mathbf{W}_{t|t-1},\\frac{1}{\\sqrt{M}}\\hat{\\mathbf{G}}_{t}^{(1:M)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note $\\tilde{\\mathbf{W}}_{t}\\in\\mathbb{R}^{P\\times(R+M)}$ . Using this posterior precision in the mean update from Eq. (120) yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t}=\\mu_{t|t-1}+\\left(\\mathbf{\\tilde{Y}}_{t|t-1}+\\tilde{\\mathbf{W}}_{t}\\tilde{\\mathbf{W}}_{t}^{\\top}\\right)^{-1}g_{t}}\\\\ &{\\quad=\\mu_{t|t-1}+\\left(\\mathbf{\\tilde{Y}}_{t|t-1}^{-1}-\\mathbf{\\tilde{Y}}_{t|t-1}^{-1}\\tilde{\\mathbf{W}}_{t}\\left(\\mathbf{I}_{R+M}+\\tilde{\\mathbf{W}}_{t}^{\\top}\\mathbf{Y}_{t|t-1}^{-1}\\tilde{\\mathbf{W}}_{t}\\right)^{-1}\\tilde{\\mathbf{W}}_{t}^{\\top}\\mathbf{Y}_{t|t-1}^{-1}\\right)g_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second line comes from the Woodbury identity and can be computed in $O((R+M)^{2}P+$ $(R+M)^{3})$ . Applying the SVD projection gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{W}_{t}=\\mathbf{U}_{t}\\left[:,:R\\right]\\mathbf{A}_{t}\\left[:R,:R\\right]\\quad\\quad}\\\\ {\\mathbf{Y}_{t}=\\mathbf{Y}_{t\\mid t-1}+\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}_{t}\\tilde{\\mathbf{W}}_{t}^{\\top}-\\mathbf{W}_{t}\\mathbf{W}_{t}^{\\top}\\right)\\quad}\\\\ {\\left(\\mathbf{U}_{t},\\mathbf{A}_{t},_{-}\\right)=\\mathrm{SVD}\\left(\\tilde{\\mathbf{W}}_{t}\\right)\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which takes $O((R+M)^{2}P)$ for the SVD. Therefore the BONG-MC-EF-DLR update is defined by Eqs. (281) and (283) to (286) and takes $O((R+M)^{2}P+(R+M)^{3})$ . ", "page_idx": 37}, {"type": "text", "text": "The BONG-LIN-EF-DLR update comes from replacing $\\textstyle\\frac{1}{\\sqrt{M}}\\hat{\\mathbf{G}}_{t}^{(1:M)}$ with $\\pmb{g}_{t}^{\\mathrm{LIN}}$ in Eq. (281) and replacing ${\\bf I}_{R+M}$ with ${\\bf\\cal I}_{R+1}$ in Eq. (283). This update takes $O((R+1)^{2}P+(R+1)^{3})$ . ", "page_idx": 37}, {"type": "text", "text": "E.5.2 BONG-LIN-HESS DLR (LO-FI) ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (281) and (283) gives the BONG-LIN-HESS-DLR update: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad\\mu_{t}=\\mu_{t|t-1}+\\left(\\mathbf{Y}_{t|t-1}^{-1}-\\mathbf{Y}_{t|t-1}^{-1}\\tilde{\\mathbf{W}}_{t}\\left(\\mathbf{I}_{R+C}+\\tilde{\\mathbf{W}}_{t}^{\\top}\\mathbf{Y}_{t|t-1}^{-1}\\tilde{\\mathbf{W}}_{t}\\right)^{-1}\\tilde{\\mathbf{W}}_{t}^{\\top}\\mathbf{Y}_{t|t-1}^{-1}\\right)}\\\\ &{\\qquad\\qquad\\times\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}\\left(y_{t}-\\hat{y}_{t}\\right)}&{{}}\\\\ &{\\qquad\\qquad\\mathbf{W}_{t}=\\mathbf{U}_{t}\\left[:,:R\\right]\\mathbf{A}_{t}\\left[:R,:R\\right]}&{{}}\\\\ &{\\qquad\\qquad\\mathbf{Y}_{t}=\\mathbf{Y}_{t|t-1}+\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}_{t}\\tilde{\\mathbf{W}}_{t}^{\\top}-\\mathbf{W}_{t}\\mathbf{W}_{t}^{\\top}\\right)}&{{}}\\\\ &{\\qquad\\qquad\\times\\mathbf{V}\\mathbf{D}\\left(\\tilde{\\mathbf{W}}_{t}\\right)}&{{}}\\\\ &{\\qquad\\qquad\\tilde{\\mathbf{W}}_{t}=\\left[\\mathbf{W}_{t|t-1},\\mathbf{H}_{t}^{\\top}\\mathbf{A}_{t}^{\\top}\\right]}&{{}}\\\\ &{\\qquad\\qquad\\mathbf{A}_{t}=\\mathrm{chol}\\left(\\mathbf{R}_{t}^{-1}\\right)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This is equivalent to LO-FI [Chang et al., 2023] and takes $O((R+C)^{2}P+(R+C)^{3})$ ). ", "page_idx": 37}, {"type": "text", "text": "E.5.3 BLR DLR (SLANG) ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Substituting DLR forms for $q_{\\psi_{t|t-1}}$ and $q_{\\psi_{t,i-1}}$ into the FC precision update from Eq. (136) and using the MC-EF approximation yields the posterior precision ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\boldsymbol{\\Sigma}}_{t,i}^{-1}=\\left(1-\\alpha\\right)\\left(\\mathbf{\\tilde{T}}_{t,i-1}+\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\mathsf{T}}\\right)+\\alpha\\left(\\mathbf{\\tilde{T}}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}\\right)-\\alpha\\mathbf{G}_{t,i}^{\\mathbf{M}_{\\mathsf{C}}-\\mathbf{E}}}\\\\ &{\\quad\\quad=\\tilde{\\mathbf{T}}_{t,i}+\\tilde{\\mathbf{W}}_{t,i}\\tilde{\\mathbf{W}}_{t,i}^{\\mathsf{T}}}\\\\ &{\\tilde{\\mathbf{T}}_{t,i}=\\left(1-\\alpha\\right)\\mathbf{\\tilde{T}}_{t,i-1}+\\alpha\\mathbf{\\tilde{T}}_{t|t-1}}\\\\ &{\\tilde{\\mathbf{W}}_{t,i}=\\left[\\sqrt{1-\\alpha}\\mathbf{W}_{t,i-1},\\sqrt{\\alpha}\\mathbf{W}_{t|t-1},\\sqrt{\\frac{\\alpha}{M}}\\hat{\\mathbf{G}}_{t,i}^{(1;M)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note $\\tilde{\\mathbf{W}}_{t}\\in\\mathbb{R}^{P\\times(2R+M)}$ . Using this posterior precision in the mean update from Eq. (135) yields ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\left(\\tilde{\\mathbf{Y}}_{t,i}+\\tilde{\\mathbf{W}}_{t,i}\\tilde{\\mathbf{W}}_{t,i}^{\\top}\\right)^{-1}}\\\\ &{\\qquad\\quad\\times\\left(\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\top}\\right)\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+g_{t,i}\\right)}\\\\ &{=\\mu_{t,i-1}+\\alpha\\left(\\tilde{\\mathbf{Y}}_{t,i}^{-1}-\\tilde{\\mathbf{Y}}_{t,i}^{-1}\\tilde{\\mathbf{W}}_{t,i}\\left(\\mathbf{I}_{2R+M}+\\tilde{\\mathbf{W}}_{t,i}^{\\top}\\tilde{\\mathbf{Y}}_{t,i}^{-1}\\tilde{\\mathbf{W}}_{t,i}\\right)^{-1}\\tilde{\\mathbf{W}}_{t,i}^{\\top}\\tilde{\\mathbf{Y}}_{t,i}^{-1}\\right)}\\\\ &{\\qquad\\quad\\times\\left(\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\top}\\right)\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+g_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the second line comes from the Woodbury identity and can be computed in $O((2R+M)^{2}P+$ $(2R+M)^{3})$ . Applying the SVD projection gives ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}_{t,i}=\\mathbf{U}_{t,i}\\left[:,:R\\right]\\mathbf{A}_{t,i}\\left[:R,:R\\right]}\\\\ &{\\mathbf{\\Lambda}}\\\\ &{\\mathbf{\\Lambda}}\\mathbf{\\Phi}_{t,i}=\\tilde{\\mathbf{Y}}_{t,i}+\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}_{t,i}\\tilde{\\mathbf{W}}_{t,i}^{\\intercal}-\\mathbf{W}_{t,i}\\mathbf{W}_{t,i}^{\\intercal}\\right)}\\\\ &{\\left(\\mathbf{U}_{t,i},\\mathbf{A}_{t,i},_{-}\\right)=\\mathrm{SVD}\\left(\\tilde{\\mathbf{W}}_{t,i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which takes $O((2R+M)^{2}P)$ for the SVD. Therefore the BLR-MC-EF-DLR update is defined by Eqs. (295), (296) and (298) to (301) and takes $O((2R+M)^{2}P+(2R+M)^{3})$ per iteration. Notice $\\tilde{\\mathbf{W}}_{t}$ has larger rank for BLR than for BONG ( $2R+M$ vs. $R+M)$ because of the extra $\\sqrt{\\alpha}\\mathbf{W}_{t\\mid t-1}$ term that originates in the KL part of the VI loss. This difference will slow BLR-MC-EF-DLR relative to BONG-MC-EF-DLR especially when $R$ is not small relative to $M$ . ", "page_idx": 38}, {"type": "text", "text": "This method closely resembles SLANG Mishkin et al. [2018] in the batch setting where we replace $q_{\\psi_{t|t-1}}$ with a spherical prior $\\mathcal{N}(\\mathbf{0},\\lambda^{-1}\\mathbf{I}_{P})$ (see Appendix E.6). ", "page_idx": 38}, {"type": "text", "text": "We can also define a BLR-LIN-EF-DLR method by replacing $\\textstyle\\sqrt{\\frac{\\alpha}{M}}\\hat{\\mathbf{G}}_{t}^{(1:M)}$ with $\\sqrt{\\alpha}g_{t}^{\\mathrm{LIN}}$ in Eq. (296) and $\\mathbf{I}_{2R+M}$ with $\\mathbf{I}_{2R+1}$ in Eq. (298). This update takes $O((2R+1)^{2}P+(2R+1)^{3})$ per iteration. ", "page_idx": 38}, {"type": "text", "text": "E.5.4 BLR-LIN-HESS DLR ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (296) and (298) gives the BLR-LIN-HESS-DLR update: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\left(\\tilde{\\mathbf{T}}_{t,i}^{-1}-\\tilde{\\mathbf{T}}_{t,i}^{-1}\\tilde{\\mathbf{W}}_{t,i}\\left(\\mathbf{\\Delta}\\mathbf{I}_{2R+C}+\\tilde{\\mathbf{W}}_{t,i}^{\\top}\\tilde{\\mathbf{T}}_{t,i}^{-1}\\tilde{\\mathbf{W}}_{t,i}\\right)^{-1}\\tilde{\\mathbf{W}}_{t,i}^{\\top}\\tilde{\\mathbf{T}}_{t,i}^{-1}\\right)}&{{}}\\\\ {\\times\\left(\\left(\\mathbf{T}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\top}\\right)\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\mathbf{H}_{t,i}^{\\top}\\mathbf{R}_{t,i}^{-1}\\left(y_{t}-\\hat{y}_{t,i}\\right)\\right)}&{{}}\\\\ {\\mathbf{W}_{t,i}=\\mathbf{U}_{t,i}\\left[\\boldsymbol{\\vdots},\\tilde{\\mathbf{R}}_{t}\\right]\\sin_{i}\\left[\\boldsymbol{\\vdots},\\tilde{\\mathbf{R}}_{t}\\right]}&{{}}\\\\ {\\mathbf{T}_{t,i}=\\tilde{\\mathbf{T}}_{t,i}+\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}_{t,i}\\tilde{\\mathbf{W}}_{t,i}^{\\top}-\\mathbf{W}_{t,i}\\mathbf{W}_{t,i}^{\\top}\\right)}&{{}}\\\\ {(\\mathbf{U}_{t,i},\\Lambda_{t,i-})=\\mathrm{SVD}\\left(\\tilde{\\mathbf{W}}_{t,i}\\right)}&{{}}\\\\ {\\tilde{\\mathbf{W}}_{t,i}=\\left[\\sqrt{1-\\alpha}\\mathbf{W}_{t,i-1},\\sqrt{\\alpha}\\mathbf{W}_{t,i-1},\\sqrt{\\alpha}\\mathbf{H}_{t,i}^{\\top}\\mathbf{A}_{t,i}^{\\top}\\right]}&{{}}\\\\ {\\mathbf{A}_{t,i}=\\mathrm{chol}\\left(\\mathbf{R}_{t,i}^{-1}\\right)}&{{}}\\\\ {~\\tilde{\\mathbf{T}}_{t,i}=\\left(1-\\alpha\\right)\\mathbf{T}_{t,i-1}+\\alpha\\mathbf{Y}_{t|t-1}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This update takes $O((2R+C)^{2}P+(2R+C)^{3})$ per iteration. As with the EF versions of BLR-DLR, $\\tilde{\\mathbf{W}}_{t}$ has larger rank for BLR-LIN-HESS-DLR than for BONG-LIN-HESS-DLR ( $2R+C$ vs. $R+C)$ . This difference will slow BLR-LIN-HESS-DLR relative to BONG-LIN-HESS-DLR especially when $R$ is not small relative to $C$ . ", "page_idx": 39}, {"type": "text", "text": "E.5.5 BOG DLR ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Substituting Eqs. (269) to (271) into Eq. (31) gives the BOG-DLR update ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\boldsymbol{\\mu}_{t}=\\boldsymbol{\\mu}_{t|t-1}+\\alpha g_{t}\\quad}&{(30)}\\\\ &{\\mathbf{Y}_{t}=\\mathbf{Y}_{t|t-1}-\\cfrac{\\alpha}{2}\\mathrm{diag}\\left(\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\intercal}\\right)^{-1}\\mathbf{G}_{t}\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\intercal}\\right)^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbf{W}_{t}=\\mathbf{W}_{t|t-1}-\\alpha\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\intercal}\\right)^{-1}\\mathbf{G}_{t}\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\intercal}\\right)^{-1}\\mathbf{W}_{t|t-1}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using the EF approximation and Woodbury, the BOG-MC-EF-DLR update can be rewritten as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbf{\\mathcal{Y}}_{t}=\\mathbf{\\mathcal{Y}}_{t\\mid t-1}+\\frac{\\alpha}{2M}\\mathrm{diag}\\left(\\mathbf{B}_{t}\\mathbf{B}_{t}^{\\mathsf{T}}\\right)}&{\\mathrm{(31)}}\\\\ &{\\mathbf{W}_{t}=\\mathbf{W}_{t\\mid t-1}+\\frac{\\alpha}{M}\\mathbf{B}_{t}\\mathbf{B}_{t}^{\\mathsf{T}}\\mathbf{W}_{t\\mid t-1}}&{\\mathrm{(31)}}\\\\ &{\\mathbf{B}_{t}=\\left(\\mathbf{Y}_{t\\mid t-1}^{-1}-\\mathbf{Y}_{t\\mid t-1}^{-1}\\mathbf{W}_{t\\mid t-1}\\left(\\mathbf{I}_{R}+\\mathbf{W}_{t\\mid t-1}^{\\mathsf{T}}\\mathbf{Y}_{t\\mid t-1}^{-1}\\mathbf{W}_{t\\mid t-1}\\right)^{-1}\\mathbf{W}_{t\\mid t-1}^{\\mathsf{T}}\\mathbf{Y}_{t\\mid t-1}^{-1}\\right)\\hat{\\mathbf{G}}_{t}^{(1:M)}\\mathrm{.}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which takes $O(R M P+M R^{2}+R^{3})$ . ", "page_idx": 39}, {"type": "text", "text": "The BOG-LIN-EF-DLR update comes from replacing $\\hat{\\mathbf{G}}_{t}^{(1:M)}$ with $\\pmb{g}_{t}^{\\mathrm{LIN}}$ in Eq. (314) and dropping the factors in Eqs. (312) and (313). This update takes $O(R P+\\bar{R}^{3})$ . ", "page_idx": 39}, {"type": "text", "text": "E.5.6 BOG-LIN-HESS DLR ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (309) and (312) to (314) gives the BOG-LIN-HESS-DLR update ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\boldsymbol{\\mu}_{t}=\\boldsymbol{\\mu}_{t|t-1}+\\alpha\\mathbf{H}_{t}^{\\top}\\mathbf{R}_{t}^{-1}(\\boldsymbol{y}_{t}-\\hat{\\boldsymbol{y}}_{t})}&{\\mathrm{(3)}}\\\\ &{\\mathbf{\\hat{Y}}_{t}=\\mathbf{Y}_{t|t-1}+\\frac{\\alpha}{2}\\mathrm{diag}\\left(\\mathbf{B}_{t}\\mathbf{B}_{t}^{\\top}\\right)}&{\\mathrm{(3)}}\\\\ &{\\mathbf{W}_{t}=\\mathbf{W}_{t|t-1}+\\alpha\\mathbf{B}_{t}\\mathbf{B}_{t}^{\\top}\\mathbf{W}_{t|t-1}}&{\\mathrm{(3)}}\\\\ &{\\mathbf{B}_{t}=\\left(\\mathbf{Y}_{t|t-1}^{-1}-\\mathbf{Y}_{t|t-1}^{-1}\\mathbf{W}_{t|t-1}\\left(\\mathbf{I}_{R}+\\mathbf{W}_{t|t-1}^{\\top}\\mathbf{Y}_{t|t-1}^{-1}\\mathbf{W}_{t|t-1}\\right)^{-1}\\mathbf{W}_{t|t-1}^{\\top}\\mathbf{Y}_{t|t-1}^{-1}\\right)\\mathbf{H}_{t}^{\\top}\\mathbf{A}_{t}^{\\top}}\\\\ &{}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This update takes $O(C(C+R)P+C R^{2}+R^{3})$ . ", "page_idx": 39}, {"type": "text", "text": "E.5.7 BBB DLR ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Substituting Eqs. (265) and (269) to (273) into Eq. (34) gives the BBB-DLR update ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}\\right)\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha g_{t}}&{(319)}\\\\ &{\\mathbf{Y}_{t,i}=\\mathbf{Y}_{t,i-1}}&\\\\ &{\\quad\\quad+\\frac{\\alpha}{2}\\mathrm{diag}\\left(\\pmb{\\Sigma}_{t,i-1}\\left(\\pmb{\\Sigma}_{t|t-1}-\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}-\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\mathsf{T}}-\\mathbf{G}_{t,i}\\right)\\pmb{\\Sigma}_{t,i-1}\\right)}&\\\\ &{\\quad\\quad\\quad\\times\\frac{\\alpha}{2}}&\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{W}_{t,i}=\\mathbf{W}_{t,i-1}}&{{}}\\\\ {+\\alpha\\mathbf{\\Delta}_{t,i-1}\\left(\\mathbf{\\Delta}\\mathbf{Y}_{t|t-1}-\\mathbf{Y}_{t,i-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}-\\mathbf{W}_{t,i-1}\\mathbf{W}_{t,i-1}^{\\mathsf{T}}-\\mathbf{G}_{t,i}\\right)\\mathbf{\\Delta}\\mathbf{z}_{t,i-1}\\mathbf{W}_{t,i-1}}&{{}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The previous covariance can be written using Woodbury as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\Sigma}_{t,i-1}=\\boldsymbol{\\Upsilon}_{t,i-1}^{-1}-\\boldsymbol{\\Upsilon}_{t,i-1}^{-1}\\mathbf{W}_{t,i-1}\\left(\\mathbf{I}_{R}+\\mathbf{W}_{t,i-1}^{\\intercal}\\boldsymbol{\\Upsilon}_{t,i-1}^{-1}\\mathbf{W}_{t,i-1}\\right)^{-1}\\mathbf{W}_{t,i-1}^{\\intercal}\\boldsymbol{\\Upsilon}_{t,i-1}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The BBB-MC-EF-DLR update can be computed efficiently by expanding terms in Eqs. (320) to (322). For example the terms involving $\\mathbf{G}_{t,i}$ can be calculated as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{diag}\\left(-\\mathbf{\\Sigma}_{\\mathbf{Z}_{i},i-1}\\mathbf{G}_{t,i}^{\\mathbf{AC}\\cdot\\mathbf{FD}}\\mathbf{\\Sigma}_{\\mathbf{Z}_{i},i-1}\\right)=\\frac{1}{M}\\mathrm{diag}\\left(\\begin{array}{l}{\\mathbf{Y}_{t,i-1}^{-1}\\hat{\\mathbf{G}}_{t,i}^{\\mathbf{(}1,i\\cdot1)}\\hat{\\mathbf{G}}_{t,i}^{\\mathbf{(}1,i\\cdot1)^{\\top}}\\mathbf{Y}_{t,i-1}^{-1}}\\\\ {-\\mathbf{\\Sigma}_{\\mathbf{Z}_{i},i-1}^{-1}\\hat{\\mathbf{G}}_{t,i}^{\\mathbf{(}1,i\\cdot1)}\\mathbf{B}_{t,i}^{\\mathbf{(}1,i\\cdot1)}}\\\\ {-\\mathbf{\\Sigma}_{\\mathbf{R}_{i},i}^{-1}\\hat{\\mathbf{G}}_{t,i}^{\\mathbf{(}1,i\\cdot1)^{\\top}}\\mathbf{Y}_{t,i-1}^{-1}+\\mathbf{B}_{t,i}\\mathbf{B}_{t,i-1}^{\\top}}\\end{array}\\right)}\\\\ &{-\\mathbf{\\Sigma}_{t,i-1}\\mathbf{G}_{t,i}^{\\mathbf{AC}\\cdot\\mathbf{FD}}\\mathbf{\\Sigma}_{\\mathbf{Z}_{i},i-1}\\mathbf{W}_{t,i-1}=\\frac{1}{M}\\mathbf{Y}_{t,i-1}^{-1}\\hat{\\mathbf{G}}_{t,i}^{\\mathbf{(}1,i\\cdot1)}\\hat{\\mathbf{G}}_{t,i}^{\\mathbf{(}1,i\\cdot1)^{\\top}}\\mathbf{Y}_{t,i-1}^{-1}\\mathbf{W}_{t,i-1}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }-\\frac{1}{M}\\mathbf{Y}_{t,i-1}^{-1}\\hat{\\mathbf{G}}_{t,i}^{\\mathbf{(}1,i\\cdot1)}\\mathbf{B}_{t,i}^{\\top}\\mathbf{W \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using this strategy the update takes $O((R+M)R P+M R^{2}+R^{3})$ . ", "page_idx": 40}, {"type": "text", "text": "The BBB-LIN-EF-DLR update comes from replacing $\\hat{\\mathbf{G}}_{t}^{(1:M)}$ with $\\pmb{g}_{t}^{\\mathrm{LIN}}$ and dropping the $M^{-1}$ factors in Eqs. (323) to (325). This update takes $O(\\dot{R}^{2}P+\\dot{R^{3}})$ . ", "page_idx": 40}, {"type": "text", "text": "E.5.8 BBB-LIN-HESS DLR ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Applying Proposition 4.2 to Eqs. (319) to (321) gives the BBB-LIN-HESS-DLR update ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mu_{t,i}=\\mu_{t,i-1}+\\alpha\\left(\\mathbf{Y}_{t|t-1}+\\mathbf{W}_{t|t-1}\\mathbf{W}_{t|t-1}^{\\mathsf{T}}\\right)\\left(\\mu_{t|t-1}-\\mu_{t,i-1}\\right)+\\alpha\\mathbf{H}_{t,i}^{\\mathsf{T}}\\mathbf{R}_{t,i}^{-1}(y_{t}-\\hat{y}_{t,i})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\Upsilon}_{t,i}=\\boldsymbol{\\Upsilon}_{t,i-1}+\\frac{\\alpha}{2}\\mathrm{diag}\\left(\\boldsymbol{\\Sigma}_{t,i-1}\\left(\\begin{array}{l}{\\boldsymbol{\\Upsilon}_{t|t-1}-\\boldsymbol{\\Upsilon}_{t,i-1}+\\boldsymbol{\\ W}_{t|t-1}\\boldsymbol{\\mathbf{W}}_{t|t-1}^{\\intercal}}\\\\ {-\\boldsymbol{\\mathbf{W}}_{t,i-1}\\boldsymbol{\\mathbf{W}}_{t,i-1}^{\\intercal}+\\boldsymbol{\\mathbf{H}}_{t,i}^{\\intercal}\\boldsymbol{\\mathbf{R}}_{t,i}^{\\intercal-1}\\boldsymbol{\\mathbf{H}}_{t,i}}\\end{array}\\right)\\boldsymbol{\\Sigma}_{t,i-1}\\right)}\\\\ &{\\boldsymbol{\\mathbf{W}}_{t,i}=\\boldsymbol{\\mathbf{W}}_{t,i-1}+\\alpha\\boldsymbol{\\Sigma}_{t,i-1}\\left(\\begin{array}{l}{\\boldsymbol{\\Upsilon}_{t|t-1}-\\boldsymbol{\\Upsilon}_{t,i-1}+\\boldsymbol{\\mathbf{W}}_{t|t-1}\\boldsymbol{\\mathbf{W}}_{t|t-1}^{\\intercal}}\\\\ {-\\boldsymbol{\\mathbf{W}}_{t,i-1}\\boldsymbol{\\mathbf{W}}_{t,i-1}^{\\intercal}+\\boldsymbol{\\mathbf{H}}_{t,i}^{\\intercal}\\boldsymbol{\\mathbf{R}}_{t,i}^{\\intercal-1}\\boldsymbol{\\mathbf{H}}_{t,i}}\\end{array}\\right)\\boldsymbol{\\Sigma}_{t,i-1}\\boldsymbol{\\mathbf{W}}_{t,i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This can be computed in $O((C+R)^{2}P+C R^{2}+R^{3})$ using Eq. (322) and following a computational approach similar to Eqs. (323) to (325). ", "page_idx": 40}, {"type": "text", "text": "E.6 Batch BLR ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "It is interesting to translate the BLR updates derived here back to the batch setting where BLR was developed [Khan and Rue, 2023], by replacing $\\mathcal{N}\\left(\\mu_{t|t-1},\\Sigma_{t|t-1}\\right)$ with a centered spherical prior $\\mathcal{N}\\left(\\mathbf{0},\\lambda^{-1}\\mathbf{I}_{P}\\right)$ . ", "page_idx": 40}, {"type": "text", "text": "The batch BLR-FC update becomes ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\pmb{\\mu}_{i}=\\pmb{\\mu}_{i-1}+\\alpha\\pmb{\\Sigma}_{i}\\left(\\pmb{g}_{i}-\\lambda\\pmb{\\mu}_{i-1}\\right)}\\\\ &{\\pmb{\\Sigma}_{i}^{-1}=(1-\\alpha)\\pmb{\\Sigma}_{i-1}^{-1}+\\alpha\\left(\\lambda\\mathbf{I}_{P}-\\mathbf{G}_{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The batch BLR-LIN-HESS-FC update becomes ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mu_{i}=\\mu_{i-1}+\\alpha\\Sigma_{i}\\left(\\mathbf{H}_{i}^{\\mathsf{T}}\\mathbf{R}_{i}^{-1}(y_{t}-\\hat{y}_{i})-\\lambda\\mu_{i-1}\\right)}\\\\ &{\\sum_{i}^{-1}=(1-\\alpha)\\Sigma_{i-1}^{-1}+\\alpha\\left(\\lambda\\mathbf{I}_{P}+\\mathbf{H}_{i}^{\\mathsf{T}}\\mathbf{R}_{i}^{-1}\\mathbf{H}_{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The batch BLR-FC_MOM update becomes ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mu_{i}}={\\mu_{i-1}}+\\alpha{\\Sigma_{i-1}}\\left({{g_{i}}-\\lambda{\\mu_{i-1}}}\\right)}\\\\ &{{\\Sigma_{i}}=(1+\\alpha){\\Sigma_{i-1}}+\\alpha{\\Sigma_{i-1}}\\left({{\\bf{G}}_{i}}-\\lambda{{\\bf{I}}_{P}}\\right){\\Sigma_{i-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The batch BLR-LIN-HESS-FC_MOM update becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mu_{i}}={\\mu_{i-1}}+\\alpha{\\pmb{\\Sigma}}_{i-1}\\left({\\bf H}_{i}^{\\top}{\\bf R}_{i}^{-1}({y_{t}}-\\hat{{y}}_{i})-\\lambda{\\pmb{\\mu}}_{i-1}\\right)}\\\\ &{{\\pmb{\\Sigma}}_{i}=(1+\\alpha){\\pmb{\\Sigma}}_{i-1}-\\alpha{\\pmb{\\Sigma}}_{i-1}\\left(\\lambda{\\bf I}_{P}+{\\bf H}_{i}^{\\top}{\\bf R}_{i}^{-1}{\\bf H}_{i}\\right){\\pmb{\\Sigma}}_{i-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The batch BLR-DIAG update becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\quad\\mu_{i}=\\pmb{\\mu}_{i-1}+\\alpha\\pmb{\\sigma}_{i}^{2}\\left(\\pmb{g}_{i}-\\lambda\\pmb{\\mu}_{i-1}\\right)}\\\\ &{\\pmb{\\sigma}_{i}^{-2}=(1-\\alpha)\\pmb{\\sigma}_{i-1}^{-2}+\\alpha\\operatorname{diag}\\left(\\lambda\\mathbf{I}_{P}-\\mathbf{G}_{i}\\right)}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The MC-HESS version of this update is equivalent to VON [Khan et al., 2018b] if we use MC approximation with $M=1$ . The MC-EF version with $M=1$ is equivalent to VOGN [Khan et al., 2018b]. ", "page_idx": 41}, {"type": "text", "text": "The batch BLR-LIN-HESS-DIAG update becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mu}_{i}=\\pmb{\\mu}_{i-1}+\\alpha\\pmb{\\sigma}_{i}^{2}\\left(\\mathbf{H}_{i}^{\\mathsf{T}}\\mathbf{R}_{i}^{-1}(\\pmb{y}_{t}-\\hat{\\pmb{y}}_{i})-\\lambda\\pmb{\\mu}_{i-1}\\right)}\\\\ {\\pmb{\\sigma}_{i}^{-2}=(1-\\alpha)\\pmb{\\sigma}_{i-1}^{-2}+\\alpha\\operatorname{diag}\\left(\\lambda\\mathbf{I}_{P}+\\mathbf{H}_{i}^{\\mathsf{T}}\\mathbf{R}_{i}^{-1}\\mathbf{H}_{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The batch BLR-DIAG_MOM update becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\pmb{\\mu}}_{i}={\\pmb{\\mu}}_{i-1}+\\alpha{\\pmb{\\sigma}}_{i-1}^{2}\\left({\\pmb{g}}_{i}-\\lambda{\\pmb{\\mu}}_{i-1}\\right)}\\\\ &{{\\pmb{\\sigma}}_{i}^{2}=(1+\\alpha){\\pmb{\\sigma}}_{i-1}^{2}+\\alpha{\\pmb{\\sigma}}_{i-1}^{4}\\mathrm{diag}\\left({\\bf G}_{i}-\\lambda{\\bf I}_{P}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The batch BLR-LIN-HESS-DIAG_MOM update becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol\\mu}_{i}={\\boldsymbol\\mu}_{i-1}+\\alpha{\\pmb\\sigma}_{i-1}^{2}\\left({\\bf H}_{i}^{\\top}{\\bf R}_{i}^{-1}({\\pmb y}_{t}-\\hat{{\\pmb y}}_{i})-\\lambda{\\pmb\\mu}_{i-1}\\right)}\\\\ &{{\\pmb\\sigma}_{i}^{2}=(1+\\alpha){\\pmb\\sigma}_{i-1}^{2}-\\alpha{\\pmb\\sigma}_{i-1}^{4}\\mathrm{diag}\\left(\\lambda{\\bf I}_{P}+{\\bf H}_{i}^{\\top}{\\bf R}_{i}^{-1}{\\bf H}_{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The batch BLR-DLR update becomes ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{i}=\\mu_{i-1}+\\alpha\\left(\\tilde{\\mathbf{Y}}_{i}^{-1}-\\tilde{\\mathbf{Y}}_{i}^{-1}\\tilde{\\mathbf{W}}_{i}\\left(\\mathbf{I}_{R+M}+\\tilde{\\mathbf{W}}_{i}^{\\top}\\tilde{\\mathbf{Y}}_{i}^{-1}\\tilde{\\mathbf{W}}_{i}\\right)^{-1}\\tilde{\\mathbf{W}}_{i}^{\\top}\\tilde{\\mathbf{Y}}_{i}^{-1}\\right)}\\\\ &{\\qquad\\qquad\\times\\left(g_{t}-\\lambda\\mu_{i-1}\\right)}\\\\ &{\\mathbf{W}_{i}=\\mathbf{U}_{i}\\left[;;\\tilde{\\mathbf{U}},\\tilde{\\mathbf{R}}\\right]\\mathbb{A}_{i}\\left[;R;R\\right]}\\\\ &{\\mathbf{Y}_{i}=\\tilde{\\mathbf{Y}}_{i}+\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}_{i}\\tilde{\\mathbf{W}}_{i}^{\\top}-\\mathbf{W}_{i}\\mathbf{W}_{i}^{\\top}\\right)}\\\\ &{(\\mathbf{U}_{i},\\mathbf{A}_{i-\\mathrm{,~2}})=\\mathrm{SVD}\\left(\\tilde{\\mathbf{W}}_{i}\\right)}\\\\ &{\\qquad\\tilde{\\mathbf{W}}_{i}=\\left[\\sqrt{1-\\alpha}\\mathbf{W}_{i-1},\\sqrt{\\frac{\\alpha}{M}}\\hat{\\mathbf{G}}_{i}^{(1:M)}\\right]}\\\\ &{\\mathbf{A}_{i}=\\mathrm{chol}\\left(\\mathbf{R}_{i}^{-1}\\right)}\\\\ &{\\qquad\\tilde{\\mathbf{Y}}_{i}=(1-\\alpha)\\mathbf{Y}_{i-1}+\\alpha\\mathbf{M}_{P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This is equivalent to SLANG except for the following differences. SLANG processes a minibatch of $M$ examples at each iteration, using a single sample $\\hat{\\pmb{\\theta}}\\sim q_{\\psi_{i-1}}$ for each minibatch. It uses a different SVD routine which is slightly faster but stochastic, taken from [Halko et al., 2011]. Most significantly, SLANG applies the SVD before the mean update, meaning $\\pmb{\\mu}_{i}$ is calculated using the rank- $\\boldsymbol{\\mathcal{R}}$ $\\mathbf{W}_{i}$ and $\\mathbf{\\boldsymbol{\\mathsf{T}}}_{i}$ instead of the rank- $(R+M)\\;\\tilde{\\mathbf{W}}_{i}$ and $\\tilde{\\mathbf{T}}_{i}$ , thus ignoring the non-diagonal information in the $M$ discarded singular vectors from $\\tilde{\\mathbf{W}}_{i}$ . ", "page_idx": 41}, {"type": "text", "text": "The batch BLR-LIN-HESS-DLR update becomes ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{i}=\\mu_{i-1}+\\alpha\\left(\\tilde{\\mathbf{T}}_{i}^{-1}-\\tilde{\\mathbf{T}}_{i}^{-1}\\tilde{\\mathbf{W}}_{i}\\left(\\mathbf{I}_{R+C}+\\tilde{\\mathbf{W}}_{i}^{\\top}\\tilde{\\mathbf{T}}_{i}^{-1}\\tilde{\\mathbf{W}}_{i}\\right)^{-1}\\tilde{\\mathbf{W}}_{i}^{\\top}\\tilde{\\mathbf{T}}_{i}^{-1}\\right)}\\\\ &{\\qquad\\qquad\\times\\left(\\mathbf{H}_{i}^{\\top}\\mathbf{R}_{i}^{-1}(y_{t}-\\hat{y}_{i})-\\lambda\\mu_{i-1}\\right)}\\\\ &{\\mathbf{W}_{i}=\\mathbf{U}_{i}\\left[\\zeta,\\tilde{\\mathbf{U}},\\mathbf{A}_{i}\\left[\\zeta,\\tilde{\\mathbf{R}},\\mathbb{H}\\right]\\right.}\\\\ &{\\left.\\mathbf{Y}_{i}=\\tilde{\\mathbf{T}}_{i}+\\mathrm{diag}\\left(\\tilde{\\mathbf{W}}_{i}\\tilde{\\mathbf{W}}_{i}^{\\top}-\\mathbf{W}_{i}\\mathbf{W}_{i}^{\\top}\\right)\\right.}\\\\ &{\\left.\\mathbf{U}_{i},\\Lambda_{i-,-}\\right)=\\mathrm{SVD}\\left(\\tilde{\\mathbf{W}}_{i}\\right)}\\\\ &{\\tilde{\\mathbf{W}}_{i}=[\\sqrt{1-\\alpha}\\mathbf{W}_{i-1},\\sqrt{\\alpha}\\mathbf{H}_{i}^{\\top}\\mathbf{A}_{i}^{\\top}]}\\\\ &{\\mathbf{A}_{i}=\\mathrm{chol}\\left(\\mathbf{R}_{i}^{-1}\\right)}\\\\ &{\\tilde{\\mathbf{T}}_{i}=(1-\\alpha)\\mathbf{T}_{i-1}+\\alpha\\mathbf{M}_{P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This algorithm could be called SLANG-LIN-HESS and would be deterministic and faster than SLANG since it does not need MC sampling. We can also define SLANG-LIN-EF which would be even faster, by replacing $\\textstyle\\sqrt{\\frac{\\alpha}{M}}\\hat{\\mathbf{G}}_{i}^{(1:M)}$ with $\\sqrt{\\alpha}g_{i}^{\\mathrm{LIN}}$ in Eq. (349) and ${\\bf I}_{R+M}$ with ${\\bf{I}}_{R}$ in Eq. (345). ", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: Section 1 states two main theoretical contributions (proved in Propositions 4.1 and 4.2 and summarizes the experiment findings reported in Section 5. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: We discuss limitations in Section 6. We report asymptotic efficiency in Table 3 and report actual running time in Fig. 4. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We state two formal theorems and provide complete proofs in Appendix C which we sketch in the main text in Sections 4.1 and 4.4. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We give pseudocode for the main methods in Appendix A and complete update equations for all algorithms in Appendix E. We give full details of the experiment methods in Section 5. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We will post our code on github after the blind review period, including scripts for exactly reproducing all experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Section 5 provides links to the data and reports hyperparameters and architecture details. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Plots show $\\pm1$ SE shading based on empirical SD across trials. Trials vary randomly in network initialization, data ordering and MC samples. This is stated in Section 5. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We report running times. Experiments all run on any standard GPU/TPU so further details are not necessary. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have read the Code of Ethics and affirm our research adheres to all points therein. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [No] ", "page_idx": 46}, {"type": "text", "text": "Justification: We believe there are no direct societal impacts (positive or negative) beyond the generic impacts of foundational work on training neural networks. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide citations and URLs for the two datasets we use in Section 5. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: There are no new data. As noted above, we will post our model code after the blind review period. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}]