[{"figure_path": "IdtoJVWVnX/figures/figures_0_1.jpg", "caption": "Figure 1: Average performance over >20 tasks on PaLM 2 \u2013 We compare and combine APO targeting exemplars and instructions, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.", "description": "This figure shows the average performance across more than 20 tasks using the PaLM 2 Language Model.  It compares different prompt optimization approaches.  The blue bars represent instruction optimization (IO) methods. The orange bars show exemplar optimization (EO) methods. The purple bar shows IO combined with random exemplars, while the cyan bar shows IO combined with optimized exemplars.  The key finding is that EO, even simple random search, can significantly improve the accuracy compared to complex IO methods. Combining IO and EO produces the best results.", "section": "1 Introduction"}, {"figure_path": "IdtoJVWVnX/figures/figures_4_1.jpg", "caption": "Figure 1: Average performance over >20 tasks on PaLM 2 \u2013 We compare and combine APO targeting exemplars and instructions, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.", "description": "This figure shows the average performance across more than 20 tasks using PaLM 2 Language Model.  It compares three automatic prompt optimization (APO) approaches: instruction optimization (IO) alone, exemplar optimization (EO) alone, and a combination of both (IO+EO). The results demonstrate that EO can significantly improve performance, sometimes surpassing even state-of-the-art IO methods. Combining IO and EO yields the best overall results.  The color-coding helps visualize the performance of each approach.", "section": "1 Introduction"}, {"figure_path": "IdtoJVWVnX/figures/figures_6_1.jpg", "caption": "Figure 1: Average performance over >20 tasks on PaLM 2 \u2013 We compare and combine APO targeting exemplars and instructions, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.", "description": "This figure shows the average performance across more than 20 tasks using different automatic prompt optimization (APO) methods.  It compares instruction optimization (IO), exemplar optimization (EO), and combinations of both.  The key finding is that effectively optimizing exemplars can lead to better results than optimizing instructions alone, even surpassing state-of-the-art IO methods. Combining both IO and EO yields the best performance.", "section": "1 Introduction"}, {"figure_path": "IdtoJVWVnX/figures/figures_7_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization methods on a subset of BIG-Bench Hard (BBH) tasks. It demonstrates that using exemplars, even with simple optimization strategies, outperforms state-of-the-art instruction optimization methods. The figure shows three scenarios: no instruction optimization, advanced instruction optimization, and the combination of both with the addition of exemplars. In all three scenarios, including exemplars through mutation significantly boosts the model's performance, consistently improving average test accuracy. The task index is determined by performance with seed instructions only.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_7_2.jpg", "caption": "Figure 1: Average performance over >20 tasks on PaLM 2 \u2013 We compare and combine APO targeting exemplars and instructions, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.", "description": "This figure shows the average performance of different automatic prompt optimization (APO) methods across more than 20 tasks using PaLM 2.  It compares the performance of instruction optimization (IO) alone, exemplar optimization (EO) alone, and combinations of IO and EO. The key finding is that EO can significantly improve performance, even outperforming state-of-the-art IO methods, and that combining IO and EO yields the best results.", "section": "1 Introduction"}, {"figure_path": "IdtoJVWVnX/figures/figures_8_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization methods on a set of BIG-Bench Hard (BBH) tasks.  It shows that intelligently using model-generated exemplars significantly improves performance, even when compared to state-of-the-art instruction optimization methods.  The figure highlights the impact of exemplar optimization (EO) alone and in combination with instruction optimization (IO), demonstrating EO's importance and synergy with IO.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_9_1.jpg", "caption": "Figure 1: Average performance over >20 tasks on PaLM 2 \u2013 We compare and combine APO targeting exemplars and instructions, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.", "description": "This figure compares different automatic prompt optimization (APO) methods on the performance of PaLM 2 across over 20 tasks.  It shows that optimizing exemplars can yield better results than optimizing instructions alone, a finding that contrasts with current research trends.  The best performance is achieved by combining both exemplar and instruction optimization.", "section": "1 Introduction"}, {"figure_path": "IdtoJVWVnX/figures/figures_28_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization methods on the BIG-Bench Hard (BBH) benchmark using PaLM 2.  It demonstrates the impact of exemplar optimization (EO) alone and in combination with state-of-the-art instruction optimization (IO) methods (APE and ProTeGi). The left panel shows the improvement gained by EO when no IO is used, the middle panel shows the improvement when EO is combined with APE, and the right panel shows the improvement when EO is combined with ProTeGi.  Dashed lines indicate performance *before* EO is applied and solid lines show performance *after*. The figure highlights that even a simple EO strategy (Mutation) significantly improves performance, often outperforming complex IO methods.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_29_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization methods on the BIG-Bench Hard (BBH) dataset using PaLM 2. It shows that intelligently adding exemplars generated by the model itself consistently improves performance, even when compared to state-of-the-art instruction optimization techniques.  The left panel shows performance without any instruction optimization, while the middle and right panels show results with two different SoTA instruction optimization methods (APE and ProTegi).  Dashed lines represent performance before adding exemplars, while solid lines show performance after adding exemplars generated via a mutation process.  The x-axis represents test accuracy, and the y-axis represents the task index (ordered by ascending accuracy with the seed instruction).", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_30_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization techniques on various tasks from the BIG-Bench Hard benchmark using PaLM 2.  It shows that intelligently using model-generated exemplars significantly improves performance, often surpassing state-of-the-art instruction optimization methods, even with simple exemplar selection strategies. The results highlight the synergy between exemplar and instruction optimization, demonstrating that combining both techniques consistently yields the best performance.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_31_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization methods on the BIG-Bench Hard (BBH) dataset using PaLM 2.  It shows that adding exemplars consistently improves performance, even surpassing state-of-the-art instruction optimization methods. The left panel shows the results without instruction optimization, while the middle and right panels show the results with two different state-of-the-art instruction optimization methods (APE and ProTeGi).  The dashed lines represent the performance before adding exemplars, and the solid lines show the improvement after adding exemplars generated using a mutation-based method.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_32_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization methods on a set of challenging tasks.  It demonstrates that intelligently reusing model-generated input-output pairs as exemplars consistently improves performance, surpassing state-of-the-art instruction optimization methods in many cases. Even simple exemplar optimization techniques, such as random search, outperform complex instruction optimization when seed instructions are used without any optimization. The optimal combination of instruction and exemplar optimization surpasses the performance of either method alone.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_33_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization techniques on various tasks from the BIG-Bench Hard benchmark.  The left panel shows results when no instruction optimization is used; the middle and right panels show results when advanced instruction optimization methods (APE and ProTeGi) are applied.  In all panels, the effect of adding exemplars generated via a mutation method is assessed.  The results indicate that intelligent use of exemplars (EO) consistently improves performance, surpassing state-of-the-art instruction optimization (IO) methods in many cases, even when using simple exemplar selection strategies.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_33_2.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization (APO) methods on a set of BIG-Bench Hard (BBH) tasks.  It shows that intelligently incorporating exemplars (EO), even with simple methods, consistently improves the performance over instruction optimization (IO) alone.  The figure highlights the synergy between IO and EO, with optimal combinations surpassing individual methods and even outperforming state-of-the-art IO methods with simple EO strategies.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_35_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization techniques on a subset of BIG-Bench Hard (BBH) tasks.  It shows that intelligently adding model-generated exemplars (EO) consistently improves performance, even surpassing state-of-the-art instruction optimization (IO) methods in some cases.  The left panel shows results without any IO, the middle panel shows results with APE IO, and the right panel shows results with ProTeGi IO.  For each, the performance is compared with and without the addition of exemplars. The data clearly shows EO strategies consistently improve the baseline, and simple EO methods can even outperform complex IO methods.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_36_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization techniques on the BIG-Bench Hard benchmark using PaLM 2. It shows that intelligently incorporating exemplars consistently improves performance, even surpassing state-of-the-art instruction optimization methods in some cases.  The results highlight the synergy between instruction optimization and exemplar optimization, indicating that combining both approaches yields superior results.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_36_2.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure displays a comparison of the performance of exemplar optimization (EO) methods against instruction optimization (IO) methods on the BIG-Bench Hard (BBH) benchmark using PaLM 2.  The left panel shows the baseline performance using only seed instructions. The middle and right panels show results after applying state-of-the-art IO methods (APE and ProTeGi).  Within each panel, performance is shown before and after adding exemplars optimized via mutation. The results clearly demonstrate that incorporating model-generated exemplars significantly boosts performance and that simple EO methods can outperform SoTA IO methods.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_38_1.jpg", "caption": "Figure 1: Average performance over >20 tasks on PaLM 2 \u2013 We compare and combine APO targeting exemplars and instructions, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.", "description": "This figure shows the average performance across more than 20 tasks using PaLM 2.  It compares different automatic prompt optimization (APO) methods: instruction optimization (IO), exemplar optimization (EO), and a combination of both.  The key finding is that effectively optimizing exemplars can lead to better results than optimizing instructions alone, even surpassing state-of-the-art IO methods. Combining both IO and EO yields the best performance.", "section": "1 Introduction"}, {"figure_path": "IdtoJVWVnX/figures/figures_42_1.jpg", "caption": "Figure 3: Appropriate EO improves over any or no IO: Task-specific BBH performance with no instruction optimization (left) and with SoTA IO: APE (middle) and ProTeGi (right) before and after applying exemplars found via Mutation (\u00a73.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. Task index is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.", "description": "This figure compares the performance of different prompt optimization strategies on the BIG-Bench Hard (BBH) benchmark using PaLM 2.  It shows that using exemplars (EO), even with simple optimization methods like random search, can significantly improve performance, outperforming state-of-the-art instruction optimization (IO) methods. The figure also highlights the synergistic effect of combining both EO and IO, showing that the best results are obtained when both instructions and exemplars are optimized.", "section": "3.1 Experimental Setup"}, {"figure_path": "IdtoJVWVnX/figures/figures_70_1.jpg", "caption": "Figure 1: Average performance over >20 tasks on PaLM 2 \u2013 We compare and combine APO targeting exemplars and instructions, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.", "description": "This figure displays the average performance across more than 20 tasks using PaLM 2.  It compares different automatic prompt optimization (APO) methods: instruction optimization (IO) and exemplar optimization (EO).  It shows that EO can outperform state-of-the-art IO methods, even with simple strategies. Additionally, combining IO and EO leads to the best performance, exceeding the performance of each method alone.", "section": "1 Introduction"}]