[{"figure_path": "AAN46kUPXM/figures/figures_5_1.jpg", "caption": "Figure 1: Expressiveness statistics of feature maps from different convolutional layers and architectures on CIFAR-10.", "description": "This figure visualizes the expressiveness statistics of feature maps from different convolutional layers and architectures trained on the CIFAR-10 dataset.  It compares three different sampling strategies: random sampling, k-means clustering, and a non-approximated method using the whole dataset. The results are shown for four different architectures: ResNet-56, MobileNetV2, DenseNet40, and VGG19. Each subfigure displays a heatmap where the x-axis represents the feature map index within each layer, and the y-axis shows the convolutional layer index. The color intensity represents the expressiveness score, with higher values indicating greater expressiveness. This figure demonstrates how expressiveness can be effectively approximated with smaller datasets and highlights the consistency of expressiveness estimations across different architectures and sampling methods.", "section": "3.3 Dependency to Input Data"}, {"figure_path": "AAN46kUPXM/figures/figures_6_1.jpg", "caption": "Figure 1: Expressiveness statistics of feature maps from different convolutional layers and architectures on CIFAR-10.", "description": "This figure visualizes the expressiveness statistics of feature maps from different convolutional layers and architectures on the CIFAR-10 dataset.  It shows how the expressiveness (a measure of a neuron's or group of neurons' ability to redistribute informational resources) varies across different layers and architectures.  Different sampling strategies (random, k-means, non-approximate) are compared to show the robustness of expressiveness estimation. The x-axis represents the feature map index within a layer, and the y-axis represents the layer index.  Different colors represent different levels of expressiveness.", "section": "3.3 Dependency to Input Data"}, {"figure_path": "AAN46kUPXM/figures/figures_6_2.jpg", "caption": "Figure 2: Pruning YOLOv8m trained on COCO for Object Detection. Comparative results between neural expressiveness (NEXP) and layer-adaptive magnitude-based pruning method (LAMP) [26]. More comparisons in the supplementary material.", "description": "This figure compares the performance of Neural Expressiveness (NEXP) against a layer-adaptive magnitude-based pruning method (LAMP) when pruning the YOLOv8m model trained on the COCO dataset for object detection.  The x-axis represents the percentage of parameters remaining after pruning, while the y-axis shows both the mean Average Precision (mAP50-95) and the MACs (million multiply-accumulate operations) as percentages of the original values. The figure demonstrates that NEXP achieves better performance and compression ratios compared to LAMP. Additional comparisons are available in the supplementary material.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}, {"figure_path": "AAN46kUPXM/figures/figures_14_1.jpg", "caption": "Figure 4: Expressiveness statistics of feature maps from different convolutional layers and architectures on CIFAR-10 (Extended). For each architecture we demonstrate the expressiveness distribution for both an untrained instance of the model (PaI), as well as a converged one (PaT). The x-axis represents the indices of convolutional layers and y-axis that of the feature maps in each layer. To maintain consistency across the y-axis, we have interpolated each layer's feature maps (pixel-wise) to match the layer with the most feature maps. Columns denote different sampling strategies and different colors denote different expressiveness values (the higher the value, the more expressive the feature map). To approximate the expressiveness score of each element, denoted as \u201cnon-approx", "description": "This figure shows the expressiveness statistics of feature maps from different convolutional layers and architectures on the CIFAR-10 dataset. It compares the expressiveness distribution for untrained (PaI) and trained (PaT) models using three sampling strategies: random, k-means, and non-approximated (using the entire dataset).  The x-axis represents the convolutional layers, and the y-axis represents the feature maps within each layer. The color intensity indicates the expressiveness level, with higher intensity indicating higher expressiveness. The figure helps to visualize how expressiveness varies across different layers, architectures, and sampling methods, demonstrating that even small datasets can effectively approximate expressiveness.", "section": "A Duality of Independence: Data (X) and Information State (Wt\u2081)"}, {"figure_path": "AAN46kUPXM/figures/figures_18_1.jpg", "caption": "Figure 5: Pruning VGG-16 at Initialization on CIFAR-10. A comparative visualisation of SOTA methods across an exponential scale of params compression ratios.", "description": "The figure shows a comparison of the performance of different pruning methods on the VGG-16 model when pruning is performed at initialization.  The x-axis represents the parameter compression ratio (how much the model has been reduced in size), and the y-axis shows the top-1 accuracy.  The different lines represent different pruning methods, including random pruning, GraSP, SNIP, SynFlow, and the proposed NEXP method.  The graph demonstrates the performance of each method across a wide range of compression ratios, highlighting the relative strengths and weaknesses of each approach.  NEXP consistently outperforms other methods, particularly at lower compression ratios.", "section": "D.1 Neural Expressiveness at Initialization: A comparative study"}, {"figure_path": "AAN46kUPXM/figures/figures_20_1.jpg", "caption": "Figure 6: Pruning YOLOv8m trained on COCO for Object Detection.", "description": "This figure shows a comparison of different pruning methods on the YOLOv8m model trained for object detection on the COCO dataset.  The methods compared include random pruning, LAMP (Layer-Adaptive Magnitude-based Pruning), SLIM (Network Slimming), DepGraph, and the proposed NEXP method.  The x-axis represents the percentage of parameters remaining after pruning, while the y-axis shows the mean Average Precision (mAP) and MACs (Multiply-Accumulate operations). The solid lines represent the mAP after fine-tuning, and the dashed lines show the mAP before fine-tuning.  The purple lines show the MACs reduction. The figure demonstrates that the NEXP method consistently outperforms other methods in maintaining accuracy while achieving significant compression.", "section": "4.1 Comparison w.r.t. State-of-Art Model Compression Strategies"}]