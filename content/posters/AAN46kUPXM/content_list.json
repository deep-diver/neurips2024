[{"type": "text", "text": "Neural expressiveness for beyond importance model compression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Neural Network Pruning has been established as driving force in the exploration of   \n2 memory and energy efficient solutions with high throughput both during training   \n3 and at test time. In this paper, we introduce a novel criterion for model com  \n4 pression, named \u201cExpressiveness\". Unlike existing pruning methods that rely   \n5 on the inherent \u201cImportance\" of neurons\u2019 and filters\u2019 weights, \u201cExpressiveness\"   \n6 emphasizes a neuron\u2019s or group of neurons ability to redistribute informational   \n7 resources effectively, based on the overlap of activations. This characteristic is   \n8 strongly correlated to a network\u2019s initialization state, establishing criterion auton  \n9 omy from the learning state (stateless) and thus setting a new fundamental basis   \n10 for the expansion of compression strategies in regards to the \u201cWhen to Prune\"   \n11 question. We show that expressiveness is effectively approximated with arbitrary   \n12 data or limited dataset\u2019s representative samples, making ground for the exploration   \n13 of Data-Agnostic strategies. Our work also facilitates a \u201chybrid\" formulation of   \n14 expressiveness and importance-based pruning strategies, illustrating their com  \n15 plementary benefits and delivering up to $10\\times$ extra gains w.r.t. weight-based   \n16 approaches in parameter compression ratios, with an average of $1\\%$ in performance   \n17 degradation. We also show that employing expressiveness (independently) for   \n18 pruning leads to an improvement over top-performing and foundational methods in   \n19 terms of compression efficiency. Finally, on YOLOv8, we achieve a $46.1\\%$ MACs   \n20 reduction by removing $55.4\\%$ of the parameters, with an increase of $3\\%$ in the   \n21 mean Absolute Precision $(m A P_{50-95})$ for object detection on COCO dataset. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 To address the computational constraints of existing models, Model Compression [7] has emerged as   \n24 a prominent solution in exploring models that achieve comparable performance, but with reduced   \n25 computational complexity [52]. Within this scope, Floating Point Operations $(F L O P s)$ are used to   \n26 estimate a model\u2019s computational complexity, by measuring the arithmetic operations required for a   \n27 forward pass, while parameters (params) are associated with a model\u2019s size in terms of memory space   \n[48] and their reduction can be a precursor towards more energy efficient solutions [5]. Although   \n29 $F L O P s$ and params often correlate, their relationship isn\u2019t strictly linear. For instance, VGG16 [43]   \n30 has $17\\times$ more parameters than ResNet-56 [17] but only $3\\times$ more $F L O P s$ , largely due to VGG16\u2019s   \n31 extensive use of fully connected layers. At first sight, this can be attributed to the differences in   \n32 network topologies. From a deeper perspective, the intricacies of various operations at handling   \n33 computational workloads, such as residual structures [17, 55], depthwise separable convolutions [19],   \n34 inverted residual modules [18], channel shuffle operations [59] and shift operations [53], coupled   \n35 with their interplay, may significantly affect the relationship between $F L O P s$ and params in a neural   \n36 network. In a nutshell, besides the use of more computationally efficient operations as above  \n37 mentioned, Model Compression aims to maintain model performance while optimizing the two   \n38 aforementioned metrics via tensor decomposition, data quantization, and network sparsification [7].   \n39 In this paper we emphasize on the sparsification strategy of pruning [49], which we use as a basis   \n40 framework to introduce \u201cExpressiveness\" as a new criterion for compressing neural networks.   \n41 Existing pruning methods focus on removing redundant network elements \u2013 be they weights, neurons,   \n42 or structures of neurons \u2013 in ways that minimally affect the overall performance of a network, based   \n43 on the criterion of \u201cImportance\", e.g. [38, 58, 20, 30]. Importance-based methods address questions   \n44 like \u201cHow much does the removal of a network\u2019s element cost in terms of performance degradation?\"   \n45 and \u201cHow much information does a network element contain?\" in various ways. More specifically,   \n46 they are motivated by the information inherent in network elements, such as the magnitude of weights   \n47 [15, 28], similarity of weights or weight matrices [29, 60] ; and their sensitivity to the network\u2019s loss   \n48 function, such as the magnitude of gradients [38] and more [49, 3]. Such dependencies on weights\u2019   \n49 distributions constitute the aforementioned pruning methods to be \u201cdata-aware\" since they intrinsically   \n50 rely on the input data and the information state of the model, making the importance estimation   \n51 of the network\u2019s elements challenging and often costly due to factors like i) the stochasticity from   \n52 training with minibatches, ii) the presence of plateau areas in the optimization space, and iii) the   \n53 complexity introduced by nonlinearities [38]. Liu et al. [36] have also discussed limitations in the   \n54 perception of importance within trained models, i.e. the authors criticize the ability of network\u2019s   \n55 elements importance to generalize to pruned derivatives, while also questioning the necessity of   \n56 training large-scale models prior pruning.   \n57 Inspired by the concepts of \u201cInformation Plasticity\" [2] and the \u201cLottery Ticket Hypothesis\" (LTH)   \n58 [12], we aim to address the limitations of previous importance-based methods through elaborating   \n59 the \u201cExpressiveness\" criterion in model compression. In contrast to \u201cImportance\", we focus on   \n60 understanding the capability of network elements to redistribute informational resources to subsequent   \n61 network elements. We define \u201cExpressiveness\" as - \u201cA neuron\u2019s or group\u2019s of neurons potential   \n62 (when a network is not fully trained) or ability (when it is trained) to extract features that maximally   \n63 separate different samples\". As derived by [2], the early training phase of a model is crucial in   \n64 shaping its expressiveness, with the formation of critical paths \u2014strong connections that determine   \n65 the \u201cworkload distribution\"\u2014 being particularly significant during these initial stages. It\u2019s essential to   \n66 note that the network\u2019s initialization state influences the formation of those paths, which interestingly   \n67 enables \"Expressiveness\" to be a fti criterion for compression during all time instances of a networks   \n68 convergence [12], setting a baseline for answering the question of \"When to prune?\" [42]. Our   \n69 proposed pruning metric centers on measuring the overlap of activations between datapoints of the   \n70 feature space. In that way, expressiveness is based on effectively evaluating the inherent ability of the   \n71 network\u2019s neurons to differentiate sub-spaces within the feature space. We experimentally show that   \n72 utilizing either small sets of arbitrary data points from the feature space or stratified sampling [34]   \n73 from each class yields consistent estimations of expressiveness. Finally, we propose and implement a   \n74 new \u201chybrid\" pruning optimization strategy that cooperatively searches, exploits and characterizes   \n75 the complementary benefits between \u201cImportance\" and \u201cExpressiveness\" for model compression.   \n76 In summary, this work offers the following four-fold contribution: (i) we propose Expressiveness,   \n77 a novel criterion based on the overlap of activations for model compression; (ii) we provide an   \n78 in-depth theoretical analysis of both the fundamental principles and the technical intricacies of the   \n79 proposed criterion; (iii) we validate the hypothesis that Expressiveness can be approximated with   \n80 little to none input data, opening the road for data-agnostic pruning strategies; and (iv) through   \n81 extensive experimentation we offer a thorough comparison w.r.t to both foundational and state-of  \n82 the-art methods demonstrating the efficiency and effectiveness of the proposed technique in model   \n83 compression, while also examining the feasibility and effectiveness of a \u201chybrid\" expressiveness  \n84 importance pruning strategy.   \n85 Specifically, we validate \u201cExpressiveness\" on the CIFAR-10 [24] and ImageNet [40] datasets using   \n86 a variety of models with different design characteristics [44, 17, 45, 21, 19]. We demonstrate the   \n87 superiority of our novel criterion over existing solutions, including many top performing structural   \n88 pruning methods [31, 61, 58, 32, 23, 46, 11], and show significant params reduction while maintaining   \n89 comparable performance. We experimentally explore and analyze the complementary nature of   \n90 expressiveness and importance, showing that summary numeric evaluation provides up to $10\\times$   \n91 additional parameter compression ratio gains, with an average of $1\\%$ loss decrease w.r.t group $\\ell{1}$ -   \n92 norm [28]. Finally, we experiment on the current state-of-the-art computer vision model (YOLOv8   \n93 [9, 22]), showcasing notable compression rates of $53.9\\%$ together with performance gains of $3\\%$ on   \n94 the COCO dataset [33], and highlighting the ability of more expressive neurons to better recover lost   \n95 information from the pruning operation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "96 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 Weight (Non-Structural) Importance. Han et al. [15, 14] and Guo et al. [13] approached the   \n98 importance of weights based on their magnitude, removing connections below given thresholds.   \n99 However, earlier works [25, 16] emphasized on the Hessian of the loss and have questioned whether   \n100 magnitude is a reliable indicator of weight\u2019s importance, as small weights can be necessary for   \n101 low error. In this direction, several studies [4, 47, 41, 8] have proposed strategies of iterative   \n102 magnitude pruning, in the form of \u201cadaptive weight importance\", where weights are ranked based on   \n103 their sensitivity to the loss. From a different perspective, Yang et al. [56] address the limitations of   \n104 individual weight\u2019s saliency that fail to account for their collective influence and provide a formulation   \n105 of weight\u2019s importance based on the error minimization of the output feature maps. Expanding on this   \n106 concept, Xu et al. [54] propose a layer-adaptive pruning scheme that encapsulates the intra-relation   \n107 of weights between layers, focusing on minimizing the output distortion of the network. Amongst   \n108 other factors and limitations (as also discussed in 1), weight importance is very expensive to measure,   \n109 mainly because of the increased complexity induced by the mutual influences of the weights among   \n110 interconnected neurons. This, coupled with the requirement for specialized hardware to manage the   \n111 irregular sparsity patterns resulting from weight pruning [57], has shifted research focus towards   \n112 structural pruning [28], where neurons or entire filters are removed.   \n113 Neuron and Filter (Structural) Importance. Many where driven by the success of Iterative   \n114 Shrinkage and Thresholding Algorithms (ISTA) [6] in non-structural sparse pruning and proposed   \n115 fliter-level adaptations [28, 29, 32, 26], based on the relaxation $\\boldsymbol{\\ell1}$ and $\\ell{2}$ ) of $\\ell0$ norm minimization.   \n116 However, the loss of universality of such magnitude-based methods remains a limitation in the   \n117 approximation of importance even in the structural scope. Yu et al. [58] further elaborate on the   \n118 idea of error propagation ignorance, where the analysis is limited to the statistical properties of a   \n119 single [28, 29] or two consecutive layers [37]. The authors suggest that the importance of neurons   \n120 is better approximated from the minimization of the reconstruction error in the final response layer   \n121 from which it is propagated to previous layers. In contrast to this view, Zhuang et al. [61] emphasize   \n22 on the discriminative power of a filter as a more effective measure of importance and highlight that   \n123 this aspect is not effectively assessed by the minimization of the reconstruction error. In a manner   \n124 that reflects the progression of weight importance, Molchanov et al. [38] define \u201cadaptive filter   \n125 importance\" as the squared change in loss and apply first and second-order Taylor expansions to   \n126 accelerate importance\u2019s computations. Predominantly, the data-awareness imposed by most pruning   \n27 strategies is added to their already high-complexity \u2013 i.e. mostly non-convex, NP-Hard problems   \n128 that require combinatorial searches. This renders the estimation of importance both computationally   \n129 expensive and labor-intensive, similarly to non-structural approaches. Notably, Lin et al. [30] propose   \n130 a less data-dependent solution based on the observation that the average rank of multiple feature maps   \n131 generated by a single filter remains constant. HRank [30], alongside several other feature-guided   \n132 filter pruning approaches, are valuable indicators towards data independence. Such works form a   \n133 principle that pruning elements are better evaluated in the activation phase, where the importance of   \n134 information and the richness of characteristics for both input data and filters are better reflected. In   \n135 this work, we expand on this belief and we through extensive experimental analysis, we demonstrate   \n136 that neither the information state nor the input data is required for the discriminative characterization   \n137 of an element. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "138 3 Neural Expressiveness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "139 3.1 Weights and Activations: Importance vs Expressiveness ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "140 Neurons are the main constituent element of a neural network. Given a neural network $\\mathcal{N}$ , we   \n141 denote neurons by $a_{i}^{(l)}$ , where $l{\\in}L$ is indicative of the neuron\u2019s layer in a network with $L\\,=$   \n142 $\\{l_{0},...,l_{l},...,l_{|L|}\\}$ layers and $i$ of its position in the given layer $l=\\{a_{0},...,a_{i},...,a_{|l|}\\}$ . Another   \n143 important element are the learning parameters of the network. Otherwise the weights represent the   \n144 strength of connections between neurons in adjacent layers and are denoted by $w_{i j}^{(l)}$ , where $i$ and $j$   \n145 index the neurons in the current and previous layers. In that manner, neuron\u2019s can be perceived as   \n146 switches that allow or block information from propagating through-out a network. The activation   \n147 (or not) of a neuron $a_{i\\,.}^{(l)}$ depends on the output value of its activation function $\\sigma(\\cdot)$ , where there are   \n148 many popular options for the definition of $\\sigma$ , e.g., sigmoid, tanh, and ReLU functions. Specifically, a   \n149 neuron\u2019s output is defined as follows, ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\na_{i}^{(l)}=\\sigma\\left(\\sum_{j}w_{i j}^{(l)}a_{j}^{(l-1)}+b_{i}^{(l)}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "150 where $b_{i}^{(l)}$ denotes the bias term. From eq. 1, we observe that a neuron\u2019s activation is affected by   \n151 the activation of the previous layers, hence affecting in the same way the consecutive layers. This   \n152 interdependence between activations $a^{(l)}$ , for a given layer $l$ defines a recurrent form that can be   \n153 generalized as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\na^{(l)}=\\sigma\\left(W^{(l)}f\\left(a^{(l-2)},\\dots,a^{(1)}\\right)+b^{(l)}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "154 On the other hand, weights are a more static representation of information as they modulate how   \n155 much influence one neuron\u2019s activation has on another\u2019s, compared to activations that control the   \n156 flow of information in a network. This differentiation has motivated us to define two axes of study in   \n157 the categorisation of pruning criteria, one based on the weights (\u201cimportance\") and one based on the   \n158 activation phase (\u201cexpressiveness\").   \n159 Generalization of concepts in a structural level. The aforementioned principles extend to the   \n160 structural representations of weights and activations, the most common being Convolutional Neural   \n161 Networks (CNNs). For a CNN model with a set of $K$ convolutional layers, where $C^{l}$ is the $l-t h$   \n162 convolutional layer. We denote filters (weight maps) and feature maps (activation maps) as $F_{k}^{l}$ and   \n163 $C_{k}^{l}$ respectively, where $k$ the is index within a layer. Given filter with dimensions $m\\times n$ , eq. 1 is   \n164 adapted as follows, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{k}^{(l)}(x,y)=\\sigma\\left(\\sum_{i=1}^{m}\\sum_{j=1}^{n}F_{i j}^{(l,k)}a_{x+i-1,y+j-1}^{(l-1)}+b_{k}^{(l)}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "165 where $(i,j)$ and $(x,y)$ are the coordinates of weights and output activations within the fliter and the   \n166 output activation map respectively. Similarly, a convolution layer $l$ can be analyticaly expressed as   \n167 follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\nC^{(l)}={\\left\\{\\sigma\\left(\\bigoplus_{k=1}^{K^{(1)}}F^{(1,k)}*X+B^{(1)}\\right)\\right.}\\quad{\\mathrm{if~}}l=1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "168 with $X$ being the input to the first layer of the network, and where symbol $^*$ denotes convolution   \n169 operation and $\\oplus$ denotes the concatenation operation. Within this context1, eq. 2 is generalized as   \n170 follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\nC^{(l)}=\\sigma\\left(\\bigoplus_{k=1}^{K^{(l)}}F^{(l,k)}*f\\left(C^{(l-2)},\\dots,C^{(1)}\\right)+B^{(l)}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "171 Conceptualization of information propagation. Consider a task with $X=\\{x_{i}\\}_{i=1}^{|D|}$ denoting   \n172 dataset samples, where $|D|$ is the size of the dataset. Given the information state (weight state) of   \n173 a CNN model with $K$ convolutional layers at a given time $t_{i}$ , $\\Chi$ is mapped through the network as   \n174 $f(X,W_{t_{i}})$ , where $W_{t_{i}}\\,=\\,\\{F_{t_{i}}^{1},\\dots,F_{t_{i}}^{l},\\dots,F_{t_{i}}^{|K|}\\}$ and $F_{t_{i}}^{l}\\,=\\,\\{F_{t_{i}}^{(l,1)},\\ldots,F_{t_{i}}^{(l,k)},\\ldots,F_{t_{i}}^{(l,K^{(l)})}\\}$   \n175 with $K^{(l)}$ being the amount of weight maps (filters) in a given layer $l$ . This process can be further   \n176 analyzed as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(X,\\mathbf{W}_{t_{i}})=\\mathcal{F}_{|K|}(\\mathcal{F}_{|K|-1}(\\cdot\\,.\\,.\\,\\mathcal{F}_{1}(X;\\mathbf{F}_{t_{i}}^{1});\\mathbf{F}_{t_{i}}^{2});\\cdot\\,.\\,.\\,;\\mathbf{F}_{t_{i}}^{|K|}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "177 where $\\mathcal{F}_{l}$ represents the mapping operation of convolutional layer $l$ . ", "page_idx": 3}, {"type": "text", "text": "178 Based on eq. 2 and eq. 5, the equivalent of the previous based on the activations of the layers can be   \n179 expressed as, ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(X,\\mathbf{W}_{t_{i}})=C^{(|K|)}\\left(\\cdot\\cdot\\left(C^{(2)}\\left(C^{(1)}\\left(X,\\mathbf{F}_{t_{i}}^{1}\\right),\\mathbf{F}_{t_{i}}^{2}\\right)\\cdot\\cdot\\cdot\\right),\\mathbf{F}_{t_{i}}^{|K|}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "180 Here, $C^{(l)}$ represents the activation map of the $l$ -th layer, where $C^{(l)}\\,=\\,\\mathcal{F}_{l}(C^{(l-1)};\\mathbf{F}_{t_{i}}^{l})$ aligns   \n181 with the structure defined in eq. 4. In this formulation, $C^{(1)}$ is the activation map of the first layer,   \n182 computed using the input $X$ and the first layer\u2019s filters $\\mathbf{F}_{t_{i}}^{1}$ . Subsequent layers\u2019 activation maps   \n183 $C^{(l)}$ are derived from the previous layer\u2019s output $C^{(l-1)}$ and their respective fliters $\\mathbf{F}_{t_{i}}^{l}$ . Assuming a   \n184 classification task, the final layer $C^{(|K|)}$ is considered the classification layer, effectively summarizing   \n185 the hierarchical feature extraction and transformation process across all convolutional layers. ", "page_idx": 4}, {"type": "text", "text": "186 3.2 Mathematical Foundation of Neural Expressiveness. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "187 We observe that the training parameters of the model, in this case ${W_{t}}_{i}^{2}$ , are responsible   \n188 for transforming the original input feature space $X$ into a sequence of intermediate feature   \n189 spaces $\\bar{\\{C^{(1)},\\ldots\\;\\bar{,}C^{(|K|-\\bar{1})}\\}}$ , progressing towards the final prediction formulated by the prediction   \n190 layer C(|K|).   \n191 Based on this intrinsic characteristic of neural networks and inspired by the goal of optimizing   \n192 feature discrimination, akin to the entropy reduction strategy in decision trees [51], we assess network   \n193 elements ability, in this scenario filters, to extract features, i.e., activation patterns, that maximally   \n194 separate different input samples $x_{i}$ . In other words, we score the expressiveness of the fliters within   \n195 $W_{t_{i}}$ , based on the discriminative quality of the intermediate feature spaces they generate, where the   \n196 feature space generated by a filter $F_{k}^{l}$ , is denoted as $C_{k}^{l}$ .   \n197 Neural Expressiveness foundational concept. When assessing the expressiveness of an element   \n198 within $W_{t_{i}}$ based on its generated feature spaces, e.g., $N E X P(\\bar{F}_{t_{i}}^{l};C^{l})$ , we cooperatively evaluate   \n199 all of its preceding elements, as derived from eq. 5. This can be formulated as, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nN E X P(F_{t_{i}}^{l};C^{l})=N E X P(F_{t_{i}}^{l};(C^{(l-1)},C^{(l-2)},\\dots,C^{(1)})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "200 which can be further extended to incorporate the inter-dependencies between the examined element   \n201 and its predecessors, in accordance with eq. 7, as detailed below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N E X P\\left(F_{t_{i}}^{l};\\left(C^{(l-1)},C^{(l-2)},\\dots,C^{(1)}\\right)\\right)=}\\\\ &{N E X P\\left(F_{t_{i}}^{l};\\left((C^{(l-2)},\\mathbf{F}_{t_{i}}^{l-1}),(C^{(l-3)},\\mathbf{F}_{t_{i}}^{l-2}),\\dots,(X,\\mathbf{F}_{t_{i}}^{1})\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "202 The aforementioned eqs. 8 and 9 provide the foundational concepts for utilizing the evaluation of   \n203 the activation phase, in an endeavor to encourage the development of more universal solutions by   \n204 addressing the limitations of universality inherent in the assessment of the weight state alone (as also   \n205 discussed in sections 1 and 2).   \n206 Formulation of Neural Expressiveness (NEXP) Score. Diving deeper into the Neural Expressive  \n207 ness (NEXP) scoring process, we follow eq. 9 previously and assume a mini-batch $X^{'}=\\{x_{i}^{'}\\}_{i=1}^{N}$   \n208 with $N$ being the number of samples in it. Mapping the batch through the network, based   \n209 on eqs. 6 and 7, generates a set of sequences of feature spaces (activation maps), denoted as   \n210 $S\\,=\\,\\{s_{1},\\ldots,s_{i},\\ldots,s_{N}\\}$ , where $\\bar{s_{i}}\\,=\\,\\bar{\\{x_{i}^{'},\\dots,C_{i}^{l},\\dots,C_{i}^{|\\bar{K}|}\\}}$ is the sequence of the activation   \n211 patterns generated from sample $\\boldsymbol{x}_{i}^{'}\\in\\boldsymbol{X}^{'}$ and $\\left|s_{i}\\right|=\\left|K\\right|+1$ is its cardinality, including the feature   \n212 space of sample $\\boldsymbol{x_{i}^{\\prime}}$ . To evaluate a specific filter $k$ in layer $l$ , denoted as $F_{k}^{l}$ , we utilize the retrieved   \n213 activation patterns from that filter, denoted as $\\{s_{i,k}^{l}\\}_{i=1}^{N}$ , where $s_{i,k}^{l}=C_{i,k}^{l}$ is the activation pattern   \n214 retrieved from filter $k$ in layer $l$ .   \n215 To score the Neural Expressiveness of $F_{k}^{l}$ , we first construct a $N\\times N$ matrix that expresses all   \n216 possible combinations of the activation patterns derived from the different input samples. This table   \n217 can be visualised as follows, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left(\\!\\!\\begin{array}{c c c c c}{s_{(1,1),k}^{l}}&{s_{(1,2),k}^{l}}&{\\cdot\\cdot\\cdot}&{s_{(1,N),k}^{l}}\\\\ {s_{(2,1),k}^{l}}&{s_{(2,2),k}^{l}}&{\\cdot\\cdot\\cdot}&{s_{(2,N),k}^{l}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {s_{(N,1),k}^{l}}&{s_{(N,2),k}^{l}}&{\\cdot\\cdot\\cdot}&{s_{(N,N),k}^{l}}\\end{array}\\!\\!\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2Bias terms are excluded for simplicity. ", "page_idx": 4}, {"type": "image", "img_path": "AAN46kUPXM/tmp/ffa79b1e4ee47232851edfcf587d055d018b394429301cb5daf6aa4870ebeea9.jpg", "img_caption": ["Figure 1: Expressiveness statistics of feature maps from different convolutional layers and architectures on CIFAR-10. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "218 where $s_{(i,j),k}^{l}$ denotes the dissimilarity of activations patterns between the $i$ -th and the $j$ -th sample   \n219 of the batch. In other words, the matrix in eq. 10 represents all the possible combinations of NEXP   \n220 calculations, where each element $s_{(i,j),k}^{l}$ derives from $f(s_{i,k}^{l},s_{j,k}^{l})$ , with $f$ being any dissimilarity   \n221 function. Without loss of generality, for the rest of the study, we use the Hamming distance as the   \n222 operator implementing dissimilarity function. Activations are first binarized (values greater than 0   \n223 become 1, and the rest become 0), i.e. enabling to evaluate the degree of overlap between the binary   \n224 activation patterns using $f$ .   \n225 We note that the matrix\u2019s diagonal, where $i$ equals $j$ , along with the elements below the diagonal,   \n226 where $i$ is greater than $j$ , do not contribute additional value to quantifying the discriminative ability   \n227 of an element. The diagonal elements represent comparisons of the same sample\u2019s activation patterns,   \n228 rendering them redundant. Meanwhile, the lower triangular elements are considered duplicates   \n229 since $s_{(i,j),k}^{l}$ is equal to sl(j,i),k, thereby not adding any new information. Drawing from these two   \n230 observations, we define the Neural Expressiveness score (NEXP) as follows, ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nN E X P(F_{k}^{l})=\\frac{1}{\\frac{N(N-1)}{2}}\\sum_{i=1}^{N}\\sum_{j=i+1}^{N}f(s_{i,k}^{l},s_{j,k}^{l})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "231 The more similar the activation patterns derived from an element are, the less expressive it is   \n232 declared to be. In eq. 11, we also normalize the score w.r.t the total amount of combinations   \n233 $\\big(\\frac{N(N{-}1)}{2}\\big)$ , thereby deriving the average expressiveness score. This average score is then utilized to   \n234 characterize the discriminative capability/capacity of the examined network element. In this study,   \n235 we used the mean operation, however, we note that alternate statistical measures, e.g., minimum,   \n236 maximum, median, etc., could feasibly be applied in the computation of the overall score. ", "page_idx": 5}, {"type": "text", "text": "237 3.3 Dependency to Input Data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "238 NEXP evaluates the inherent property of network elements to maximally distinguish between input   \n239 samples. We extend this line of thought and assess its sensitivity to input data $X$ and mini-batch   \n240 size $N$ , in order to delineate the dependence between NEXP and the input data. To achieve that,   \n241 we perform a sensitivity analysis of NEXP to the mini-batch data $X$ , using two input sampling   \n242 strategies to assemble a batch with 60 samples, namely random sampling (denoted as \u2018random\u2019) and   \n243 class-representative sampling via $\\mathbf{k}$ -means (denoted as $\\mathbf{\\dot{\\nabla}}_{\\mathbf{k}}$ -means\u2019). We define the true NEXP score   \n244 (denoted as \u2018non-approx\u2019) for each filter as the value obtained by comparing all activation patterns   \n245 across the entire training dataset (more info in A.1). Fig. 1 presents a detailed comparative illustration   \n246 of the results that highlight the similarities in NEXP estimations across various trained networks,   \n247 including VGGNet [44], ResNet [17], MobileNet [19] and DenseNet [21] on CIFAR-10 dataset.   \n248 Columns represent the aforementioned sampling strategies, while colors indicate expressiveness   \n249 levels, with higher values signifying greater expressiveness. In each sub-figure, the $\\mathbf{X}$ -axis indicates ", "page_idx": 5}, {"type": "image", "img_path": "AAN46kUPXM/tmp/8f63f7d366299c038b28fbb9652f0e58a1ea9969a32de621f2ab1c07742d466d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Pruning YOLOv8m trained on COCO for Object Detection. Comparative results between neural expressiveness (NEXP) and layer-adaptive magnitude-based pruning method (LAMP) [26]. More comparisons in the supplementary material. ", "page_idx": 6}, {"type": "image", "img_path": "AAN46kUPXM/tmp/e12d6c4c1e5a073fa76f44a9867a6c447d9080c374e5fad68ed7e0ceffddaf2b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "250 convolutional layer indices, and the y-axis shows feature map indices per layer, standardized through   \n251 pixel-wise interpolation to align with the layer having the most feature maps. Fig. 1 confirms that   \n252 NEXP can be effectively estimated using random and limited data samples. Detailed results of this   \n253 analysis, are presented in Appendix A. The comparative analysis reveals that a mini-batch of 60   \n254 samples $\\mathrm{[0.4\\%}$ of $D$ in this case) effectively approximates the NEXP scores calculated from the entire   \n255 dataset, yielding consistent similarity scores above $99\\%$ across most similarity metrics (Table. 3). ", "page_idx": 6}, {"type": "text", "text": "256 3.4 Pruning Process ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "257 Alg. 1 describes the proposed NEXP-based pruning process, and it has been implemented as extension   \n258 in the DepGraph pruning framework [11]. A target theoretical speed-up is specified, referred to   \n259 as the Compression FLOPs Ratio $\\left(\\downarrow\\right)$ and denoted by $\\tau$ . This ratio is calculated using the formula   \n260 coomripgriensasle Fd LFOLPOsPs. To achieve this target ratio, the network may undergo pruning in one or several steps,   \n261 dictated by the intricacies of the pruning criterion and adjusted according to the quantity of elements   \n262 removed at each step. For example, NEXP benefits from additional steps, since a filter\u2019s score is   \n263 reliant on its preceding elements (Section 3.2), and a more gradual update on the scores allows for   \n264 improved pruning precision. A more in-depth analysis of Alg. 1 along with more details on the   \n265 implementation options are presented in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "266 4 Experimental Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "267 Details on the experimental settings can be found in Appendix C, including the (a) Datasets and   \n268 Models (C.1), (b) Adversaries (C.2), (c) Evaluation Metrics (C.3) and (d) Configurations (C.4). ", "page_idx": 6}, {"type": "text", "text": "269 4.1 Comparison w.r.t. State-of-Art Model Compression Strategies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "270 Image Classification on CIFAR-10 and Imagenet-1k. We compare against a plethora of foun  \n271 dational and top-performing approaches, ranging from filter magnitude-based [28, 32, 29] and loss   \n272 sensitivity-based [58] methods to feature-guided strategies [23, 30] and search algorithms [35, 31].   \n273 Outcomes and Discussion. Our findings for various target FLOPs pruning ratios are presented in   \n274 Tab. 1 (and Tab.6-9 in Appendix D.2) for CIFAR-10, and in Tab. 2 for ImageNet. It is essential to   \n275 acknowledge the subjectivity in reported performance metrics (accuracy), influenced by the fine  \n276 tuning process post-pruning, e.g. the authors in DCP [61] fine-tune for 400 epochs, in contrast to ours   \n277 100. We observe that our approach yields consistent improvements in params reduction compared to   \n278 other methods for given FLOPs ratios, which notably scale significantly for regimes of higher target   \n279 FLOPs compression ratios $\\tau$ . For example, on ResNet-56 we show $+0.92\\times$ average params reduction   \n280 gains in the $2\\!\\times\\!-\\!2.20\\!\\times$ FLOPs reduction regime, with $-0.38\\%$ , $+0.05\\%$ and $-0.37\\%$ percentage   \n281 difference in loss respectively to ABC [31], SCP [23] and HRank [30], while on ResNet-110 we   \n282 show $+1.21\\times$ average params reduction gains in the $2.87\\!\\times\\!-3.27\\!\\times$ FLOPs reduction regime, with   \n283 - $.0.67\\%$ and $+0.26\\%$ percentage difference in loss respectively to ABC [31] and HRank [30]. Similar   \n284 observations are evident across all tables, where in certain regimes we also show notable performance   \n285 gains, up to $+1.5\\%$ , especially for VGGNet, which is more prone to params reductions due to its   \n286 plain structure.   \n287 Object Detection with YOLOv8. We evaluate expressiveness against four importance based   \n288 methods, i.e layer-adaptive magnitude-based pruning (LAMP) [26], network slimming (SLIM) [35],   \n289 Wang\u2019s et al. proposed method (DepGraph) [11] and random pruning that serves as a generic pruning   \n290 baseline [3]. The experiments were conducted on the YOLOv8m model version [22], utilizing the   \n291 DepGraph pruning framework [11] with an iterative pruning schedule of 16 steps, where after each   \n292 pruning step the model was fine-tuned for 10 epochs using the coco128 dataset.   \n293 Outcomes and Discussion. We report the comparative pruning progress of expressiveness versus   \n94 the baseline methods, i.e. the remaining percentage of the original model in terms of $M A C s$ and   \n95 params after each pruning step, named MACs Size Percentage (MSP) and Parameters Size Percentage   \n96 (PSP) respectively, and highlight the mAP 5v0a\u2212l95 both after pruning (pruned mAP) and fine-tuning   \n97 (recovered mAP). We observe that expressiveness outperforms the rest of the reported methods across   \n98 the whole pruning spectrum, as shown in Fig. 2 (more in Appendix D.2), preserving the initial   \n99 performance of the model for percentage sizes that reach up to $40\\%$ $(2.5\\downarrow)$ of that of the original   \n00 model, with less than $0.5\\%$ of recovered performance degradation. Our method even achieves a $3\\%$   \n0 increase in recovered mAP for $46.1\\%$ MSP $(2.17\\,\\downarrow)$ , in comparison to the baselines that showcase   \n02 weak recovery capabilities after the $60\\%$ (1.67 \u2193) mark in both MSP and PSP. This can be attributed   \n03 to the intrinsic property of expressiveness to maintain network elements that are more robust to   \n04 information redistribution, in contrast to \u201cimportant\" labeled structures by other methods. In our   \n05 experimental scenario, that characteristic is further amplified by the iterative pruning format and the   \n06 higher amount of fine-tuning epochs at each step, in comparison to conventional pruning schedules   \n07 that fine-tune for 1 epoch after each iteration or perform a unified fine-tuning session after the last   \n08 pruning iteration. Interestingly, our criterion also demonstrates significant resistance to performance   \n09 loss after pruning, achieving $18\\%$ increased average performance in terms of pruned mAP compared   \n10 to the importance-based methods. We have empirically observed that expressiveness benefits from   \n11 increased cardinality in pruning granularity settings, e.g amount of intermediate steps to achieve a   \n12 given compression ratio. This stems from expressiveness interactive nature of all elements, as also   \n13 explained in Sec. 3, where smaller pruning steps combined with iterative fine-tuning, enhance pruning   \n314 precision and allow for \u201csmoother\" redistribution of information in a network, thus contributing to   \n315 the increased resistance to performance deficits after each pruning step. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "AAN46kUPXM/tmp/8b1865cbe2bffa51d50ab24ce548ce94019ceabbf396a518219aeb47797e6932.jpg", "table_caption": ["Table 1: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using ResNet architectures [17] - ResNet-56 (left) and ResNet-110 (right). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "316 4.2 Assessing Hybrid Compression space ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "317 In this section, we assess the potential efficiency of \u201chybrid\" pruning strategies exploiting the   \n318 cooperation between importance and expressiveness. We explore the solution space of \u201chybrid\"   \n319 compression, using a linear combination of importance and neural expressiveness criteria. We guide   \n320 exploration through the scoring function: $W_{\\mathrm{imp}}\\cdot\\mathrm{IMP}+W_{\\mathrm{nexp}}\\;.$ \u00b7 NEXP and conduct experiments with   \n321 various weight combinations, subject to the constraint $W_{\\mathrm{imp}}+W_{\\mathrm{nexp}}=1$ . Given that exhaustive   \n322 search is impractical, we introduce the hyper-parameter $\\alpha\\in\\{0.0,0.\\dot{2},\\ldots,0.8,1.0\\}$ to restrict the   \n323 set of permissible combinations, and modify the constraint to $(1-\\alpha)\\cdot W_{\\mathrm{imp}}+\\alpha\\cdot W_{\\mathrm{nexp}}=1$ . We   \n324 use group L1-norm [28] as the importance criterion (IMP) and assess all permissible combinations   \n325 across a linear scale, denoted as $\\tau$ , representing the target FLOPs compression ratios that we utilized   \n326 for pruning, on ResNet-56 for CIFAR-10. The outcomes are visualized in Figure 3, which maps our   \n327 predetermined $\\tau$ values on the ${\\bf X}$ -axis against the various parameter compression ratios achieved by   \n328 each combination. Regarding performance, we report the averaged percentage differences in top-1   \n329 accuracy between the baseline importance method (L1) and each hybrid format: $-0.21\\%$ for hb-0.2,   \n330 - $-0.96\\%$ for hb-0.4, - $.1.55\\%$ for hb-0.6, $-1.07\\%$ for hb-0.8, and $-2.18\\%$ for NEXP.   \n331 Observations. A consistent pattern is observed across the values of $\\alpha$ , where larger values yield   \n332 higher params compression ratios. Notably, hybrid derivatives allow us to explore sub-spaces with   \n333 higher parameter compression ratios by sacrificing slight performance accuracy. We also observe   \n334 that the solution vectors corresponding to IMP and EXP act as extremal points in the solution space   \n335 of hybrid combinations, thus suggesting a degree of partial orthogonality between the two criteria.   \n336 Furthermore, the findings reveal a polynomial relationship between parameter compression ratios and   \n337 FLOPs reduction, with compression ratios increasing polynomially to linear increments in FLOPs   \n338 reduction, and thus enabling more efficient explorations. ", "page_idx": 7}, {"type": "table", "img_path": "AAN46kUPXM/tmp/ce7dd149c0a0412f08230dd6097ca43497c0258fb149dff1eb4ce2d99fc87a48.jpg", "table_caption": ["Table 2: Analytical Comparison of Importance-based solu- Figure 3: Linear exploration of the tions and Expressiveness on ImageNet-1k using ResNet-50 combinatorial space between impor[17]. tance and expressiveness. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "339 4.3 Evaluating Neural Expressiveness at Initialization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "340 The nature of NEXP allows to be applied in a weight agnostic manner, i.e. on untrained networks.   \n341 An extended version of the section\u2019s 3.3 analysis, which also includes untrained models (Appendix   \n342 A), reveals that $N E X P_{\\mathrm{map}}$ \u2019s obtained at initialization and after network convergence share some   \n343 expressiveness pattern similarities, particularly in the initial layers. Our numeric evaluation shows a   \n344 notable correlation between the initialization and converged states for DenseNet-40 and VGG-19,   \n345 with cosine similarities of $84.10\\%$ and $86.82\\%$ , respectively. It also indicates greater consistency in   \n346 neural expressiveness measurements for the first layers of all networks, which could be considered   \n347 important for the formation of critical paths [2]. Motivated by these observations, we also assess the   \n348 efficacy of expressiveness as criterion for Pruning at Initialization against various SOTA approaches   \n349 [27, 50, 46] (Appendix D.1). Our method consistently outperforms (in terms of top-1 acc) all other   \n350 algorithms, particularly in regimes of lower compression, up to $10^{2}(\\downarrow)$ with an average increase of   \n351 $1.21\\%$ over SynFlow, while maintaining competitiveness at higher compression levels, above $10^{2}(\\downarrow)$   \n352 with an average percentage difference of $4.82\\%$ , $3.72\\%$ and $-2.74\\%$ , compared to [50], [27] and [46].   \n353 In summary, under the assumption that the selection of hyperparameters remains congruent with   \n354 the initialization [12], consistent map measurements between initial and final states can effectively   \n355 evaluate NEXP\u2019s ability to identify winning tickets. However, a robust evaluation should also consider   \n356 the initial state quality and the training process, while addressing the \"When to prune\" question [42]. ", "page_idx": 8}, {"type": "text", "text": "357 5 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "358 In this work, we have introduced \u201cNeural Expressiveness\" as a new criterion for model compression.   \n359 In our NEXP steps, we will explore optimal solutions for the \u201cWhen\" and \u201cHow\" to prune questions. ", "page_idx": 8}, {"type": "text", "text": "360 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "361 [1] Armstrong Aboah, Bin Wang, Ulas Bagci, and Yaw Adu-Gyamf.i Real-time multi-class helmet violation   \n362 detection using few-shot data sampling technique and yolov8. In Proceedings of the IEEE/CVF Conference   \n363 on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 5350\u20135358, June 2023.   \n364 [2] Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep networks. In   \n365 International Conference on Learning Representations, 2019.   \n366 [3] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural   \n367 network pruning? In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, Proceedings of Machine Learning   \n368 and Systems, volume 2, pages 129\u2013146, 2020.   \n369 [4] Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n and Yerlan Idelbayev. \u201clearning-compression\u201d algorithms for neural net   \n370 pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),   \n371 June 2018.   \n372 [5] Y Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. Understanding the limitations of existing energy  \n373 efficient design approaches for deep neural networks. Energy, 2(L1):L3, 2018.   \n374 [6] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems   \n375 with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11):1413\u20131457, 2004.   \n376 [7] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration   \n377 for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):485\u2013532, 2020.   \n378 [8] Xiaohan Ding, guiguang ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, and Ji Liu. Global sparse   \n379 momentum sgd for pruning very deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,   \n380 F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,   \n381 volume 32. Curran Associates, Inc., 2019.   \n382 [9] Tausif Diwan, G Anirudh, and Jitendra V Tembhurne. Object detection using yolo: Challenges, architectural   \n383 successors, datasets and applications. multimedia Tools and Applications, 82(6):9243\u20139275, 2023.   \n384 [10] Andrei Dumitriu, Florin Tatui, Florin Miron, Radu Tudor Ionescu, and Radu Timofte. Rip current   \n385 segmentation: A novel benchmark and yolov8 baseline results. In Proceedings of the IEEE/CVF Conference   \n386 on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1261\u20131271, June 2023.   \n387 [11] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards   \n388 any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n389 Recognition (CVPR), pages 16091\u201316101, June 2023.   \n390 [12] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural   \n391 networks, 2019.   \n392 [13] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In D. Lee,   \n393 M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing   \n394 Systems, volume 29. Curran Associates, Inc., 2016.   \n395 [14] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with   \n396 pruning, trained quantization and huffman coding, 2016.   \n397 [15] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient   \n398 neural network. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in   \n399 Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \n400 [16] Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. In   \n401 S. Hanson, J. Cowan, and C. Giles, editors, Advances in Neural Information Processing Systems, volume 5.   \n402 Morgan-Kaufmann, 1992.   \n403 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.   \n404 In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \n405 [18] Andrew Howard, Andrey Zhmoginov, Liang-Chieh Chen, Mark Sandler, and Menglong Zhu. Inverted   \n406 residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. In CVPR,   \n407 2018.   \n408 [19] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,   \n409 Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile   \n410 vision applications, 2017.   \n411 [20] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron   \n412 pruning approach towards efficient deep architectures, 2016.   \n413 [21] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convo  \n414 lutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition   \n415 (CVPR), July 2017.   \n416 [22] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics yolov8, 2023.   \n417 [23] Minsoo Kang and Bohyung Han. Operation-aware soft channel pruning using differentiable masks. In   \n418 Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine   \n419 Learning, volume 119 of Proceedings of Machine Learning Research, pages 5122\u20135131. PMLR, 13\u201318   \n420 Jul 2020.   \n421 [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical   \n422 report, 2009.   \n423 [25] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky, editor, Advances in   \n424 Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989.   \n425 [26] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the   \n426 magnitude-based pruning, 2021.   \n427 [27] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Snip: Single-shot network pruning based   \n428 on connection sensitivity, 2019.   \n429 [28] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient   \n430 convnets, 2017.   \n431 [29] Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David Doermann, Yongjian Wu, Feiyue Huang,   \n432 and Rongrong Ji. Exploiting kernel sparsity and entropy for interpretable cnn compression. In Proceedings   \n433 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.   \n434 [30] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao.   \n435 Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF Conference on   \n436 Computer Vision and Pattern Recognition (CVPR), June 2020.   \n437 [31] Mingbao Lin, Rongrong Ji, Yuxin Zhang, Baochang Zhang, Yongjian Wu, and Yonghong Tian. Channel   \n438 pruning via automatic structure search. arXiv preprint arXiv:2001.08565, 2020.   \n439 [32] Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang,   \n440 and David Doermann. Towards optimal structured cnn pruning via generative adversarial learning. In   \n441 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June   \n442 2019.   \n443 [33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,   \n444 and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In David Fleet, Tomas Pajdla,   \n445 Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision \u2013 ECCV 2014, Lecture Notes in Computer   \n446 Science, pages 740\u2013755, Cham, 2014. Springer International Publishing.   \n447 [34] Tantan Liu and Gagan Agrawal. Stratified k-means clustering over a deep web data source. In Proceedings   \n448 of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages   \n449 1113\u20131121, 2012.   \n450 [35] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning   \n451 efficient convolutional networks through network slimming, 2017.   \n452 [36] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network   \n453 pruning, 2019.   \n454 [37] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A fliter level pruning method for deep neural network   \n455 compression. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.   \n456 [38] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for   \n457 neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n458 Recognition (CVPR), June 2019.   \n459 [39] Sumit Pandey, Kuan-Fu Chen, and Erik B. Dam. Comprehensive multimodal segmentation in medical   \n460 imaging: Combining yolov8 with sam and hq-sam models. In Proceedings of the IEEE/CVF International   \n461 Conference on Computer Vision (ICCV) Workshops, pages 2592\u20132598, October 2023.   \n462 [40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,   \n463 Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large   \n464 Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211\u2013252, December   \n465 2015.   \n466 [41] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning. In   \n467 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information   \n468 Processing Systems, volume 33, pages 20378\u201320389. Curran Associates, Inc., 2020.   \n469 [42] Maying Shen, Pavlo Molchanov, Hongxu Yin, and Jose M. Alvarez. When to prune? a policy towards   \n470 early structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n471 Recognition (CVPR), pages 12247\u201312256, June 2022.   \n472 [43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni  \n473 tion, 2015.   \n474 [44] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image   \n475 Recognition, April 2015. arXiv:1409.1556 [cs].   \n476 [45] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru   \n477 Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of   \n478 the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.   \n479 [46] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without   \n480 any data by iteratively conserving synaptic flow. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,   \n481 and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6377\u20136389.   \n482 Curran Associates, Inc., 2020.   \n483 [47] Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz\u00e1r. Faster gaze prediction with dense   \n484 networks and fisher pruning, 2018.   \n485 [48] Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. The computational limits of   \n486 deep learning, 2022.   \n487 [49] Sunil Vadera and Salem Ameen. Methods for pruning deep neural networks. IEEE Access, 10:63280\u201363300,   \n488 2022.   \n489 [50] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving   \n490 gradient flow, 2020.   \n491 [51] Qing Ren Wang and Ching Y. Suen. Analysis and design of a decision tree based on entropy reduction   \n492 and its application to large character set recognition. IEEE Transactions on Pattern Analysis and Machine   \n493 Intelligence, PAMI-6(4):406\u2013417, 1984.   \n494 [52] Shiqiang Wang. Efficient deep learning. Nature Computational Science, 1(3):181\u2013182, March 2021.   \n495 Number: 3 Publisher: Nature Publishing Group.   \n496 [53] Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir Gholaminejad,   \n497 Joseph Gonzalez, and Kurt Keutzer. Shift: A zero flop, zero parameter alternative to spatial convolutions.   \n498 In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n499 [54] Kaixin Xu, Zhe Wang, Xue Geng, Min Wu, Xiaoli Li, and Weisi Lin. Efficient joint optimization of   \n500 layer-adaptive weight pruning in deep neural networks. In Proceedings of the IEEE/CVF International   \n501 Conference on Computer Vision (ICCV), pages 17447\u201317457, October 2023.   \n502 [55] Pengtao Xu, Jian Cao, Fanhua Shang, Wenyu Sun, and Pu Li. Layer pruning via fusible residual convolu  \n503 tional block for deep neural networks, 2020.   \n504 [56] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks   \n505 using energy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern   \n506 Recognition (CVPR), July 2017.   \n507 [57] Jiecao Yu, Andrew Lukefahr, David Palframan, Ganesh Dasika, Reetuparna Das, and Scott Mahlke. Scalpel:   \n508 Customizing dnn pruning to the underlying hardware parallelism. In Proceedings of the 44th Annual   \n509 International Symposium on Computer Architecture, ISCA \u201917, page 548\u2013560, New York, NY, USA, 2017.   \n510 Association for Computing Machinery.   \n511 [58] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu, Xintong Han, Mingfei Gao, Ching  \n512 Yung Lin, and Larry S. Davis. Nisp: Pruning networks using neuron importance score propagation. In   \n513 Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n514 [59] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional   \n515 neural network for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and   \n516 Pattern Recognition (CVPR), June 2018.   \n517 [60] Zhengguang Zhou, Wengang Zhou, Houqiang Li, and Richang Hong. Online fliter clustering and pruning   \n518 for efficient convnets. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages   \n519 11\u201315, 2018.   \n520 [61] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, and   \n521 Jinhui Zhu. Discrimination-aware Channel Pruning for Deep Neural Networks. In S. Bengio, H. Wallach,   \n522 H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information   \n523 Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n525 Fig. 4 presents a detailed comparative illustration that highlights the similarities in NEXP estimations   \n526 across various networks, including VGGNet [44], ResNet [17], MobileNet [19] and DenseNet [21]   \n527 on CIFAR-10 dataset. Specifically, for each network architecture, we showcase expressiveness   \n528 distributions in both untrained $(\\mathrm{PaI})$ and trained (PaT) states. In each sub-figure, the $\\mathbf{X}$ -axis indicates   \n529 convolutional layer indices, and the y-axis shows feature map indices per layer, standardized through   \n530 pixel-wise interpolation to align with the layer having the most feature maps. Columns represent   \n531 various sampling strategies, while colors indicate expressiveness levels, with higher values signifying   \n532 greater expressiveness. In other words, the figure illustrates a two-fold sensitivity analysis of NEXP   \n533 to (i) the mini-batch data $X$ , as outlined in Alg. 1), using two input sampling strategies to assemble   \n534 a batch with 60 samples, namely random sampling (denoted as \u2018random\u2019) and class-representative   \n535 sampling via $\\mathbf{k}$ -means (denoted as $\\mathbf{\\Psi}^{\\star}\\mathbf{k}$ -means\u2019), and (ii) the information state $(W_{t_{i}})$ , specifically   \n536 comparing expressiveness at initialization (PaI) against expressiveness after training (PaT), when   \n537 weights have converged. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "538 A.1 True NEXP value (non-approx). ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "539 We define the true NEXP score for each filter as the value obtained by comparing all activation   \n540 patterns across the entire training dataset $D$ . In that way, the ability of each element to extract   \n541 maximal features is evaluated for every data-point in the input feature space of a task at hand. In   \n542 this study however, due to GPU memory constraints (limited to 12GB of GDDR6 SDRAM), we   \n543 employed $25\\%$ of the total training set, ensuring class distribution is preserved, to determine these   \n544 exact NEXP scores, denoted as non-approx. ", "page_idx": 13}, {"type": "text", "text": "545 A.2 Data Agnostic. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "546 To evaluate NEXP\u2019s sensitivity to input data, we conduct a similarity analysis for each row in   \n547 Fig. 4. For each information state (PaI and $\\mathrm{PaT}$ ), we compare the expressiveness map $(N E X P_{\\mathrm{map}})$   \n548 derived from each sampling strategy against the true NEXP values (non-approx), corresponding to   \n549 each respective state. For a comprehensive comparison, we utilize various similarity metrics, such   \n550 as Euclidean Distance, Cosine Similarity, Pearsonr Similarity, and the Structural Similarity Index   \n551 Measure (ssim_index). Detailed results of this analysis, specific to each state, are presented in Tables   \n552 3 (PaT) and 4 (PaI). The comparative analysis reveals that a mini-batch of 60 samples, with a balanced   \n553 representation from each class, effectively approximates the NEXP scores calculated from the entire   \n554 dataset, yielding consistent similarity scores above $99\\%$ across all similarity metrics for both PaI   \n555 and PaT. Interestingly, random sampling consistently outperforms the $\\mathbf{k}\\cdot$ -means selection strategy,   \n556 which involves selecting 6 representative samples per CIFAR-10 class. This is especially notable in   \n557 PaT, with random sampling showing up to a $7.51\\%$ higher Pearson correlation, $5\\%$ improvement in   \n558 ssim_index, and 1.14 reduction in Euclidean distance compared to $\\boldsymbol{\\mathrm{k}}$ -means. This further reinforces   \n559 the statement that comparing activation patterns reflects the intrinsic ability of neural networks to   \n560 distinguish various input spaces, thus effectively extending the NEXP criterion to random input data   \n561 and laying the foundation for investigating Data-Agnostic strategies. ", "page_idx": 13}, {"type": "text", "text": "562 A.3 Weight Agnostic ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "563 Fig. 4 reveals that $N E X P_{\\mathrm{map}}$ \u2019s obtained at initialization and after network convergence share some   \n564 expressiveness pattern similarities, particularly in the initial layers. Detailed comparisons of these   \n565 similarities across all layers, and specifically for the first five, are presented in Table 5, contrasting   \n566 the initial maps with the true $N E X P_{\\mathrm{map}}$ post-training. The summary of our numeric evaluation   \n567 confirms a notable correlation between the initialization and converged states for DenseNet-40 and   \n568 VGG-19, showing up to $84.10\\%$ and $86.82\\%$ in cosine similarity respectively. It also indicates   \n569 greater consistency in neural expressiveness measurements for the first layers of all networks, which   \n570 could be considered important for the formation of critical paths. In this context, the formation   \n571 of the final state depends on hyperparameter choices, like weight decay and learning rate, and the   \n572 stochastic nature of training, that could potentially alter the model\u2019s progression from its initial state,   \n573 as also highlighted by Frankle et al. [12]. In that manner, under the assumption that the selection   \n574 of hyperparameters remains congruent with the initialization, \u201cExpressiveness\" can be considered a   \n575 fit criterion for Pruning at Initialization (PaI). In summary, the consistency of map measurements   \n576 between initial and final states may serve as a solid metric for evaluating NEXP\u2019s ability to identify   \n577 winning tickets. Nevertheless, a more robust process of its evaluation should also take into account   \n578 the quality of the initial state as well as the subsequent training process. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "AAN46kUPXM/tmp/7dd6bd83a5601498e2dc928faa3946c19ee79ee57e0db0a62d1161bb12f505a7.jpg", "img_caption": ["Figure 4: Expressiveness statistics of feature maps from different convolutional layers and architectures on CIFAR-10 (Extended). For each architecture we demonstrate the expressiveness distribution for both an untrained instance of the model (PaI), as well as a converged one $(\\mathrm{PaT})$ . The $\\mathbf{X}$ -axis represents the indices of convolutional layers and y-axis that of the feature maps in each layer. To maintain consistency across the y-axis, we have interpolated each layer\u2019s feature maps (pixel-wise) to match the layer with the most feature maps. Columns denote different sampling strategies and different colors denote different expressiveness values (the higher the value, the more expressive the feature map). To approximate the expressiveness score of each element, denoted as \u201cnon-approx\", we used $25\\%$ of all dataset\u2019s samples (not $100\\%$ due to memory limitations) maintaining the label\u2019s distribution. As can be seen, the rank of each feature map (column of the sub-figure) is almost unchanged (the same color), regardless of the image batches. Hence, even a small number of images can effectively estimate the average rank of each feature map in different architectures. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 3: Sensitivity analysis of the input\u2019s sampling strategies after training (PaT) using various similarity metrics. ", "page_idx": 15}, {"type": "table", "img_path": "AAN46kUPXM/tmp/ea726095178cc705e49ae2d34627ba2156f9485d867ca898ee5b141798a7571c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "AAN46kUPXM/tmp/f5a6343c1e9d972328a522ab1374e2aaf31e4a5445e7aa525f8e5c7d68a877ce.jpg", "table_caption": ["Table 4: Sensitivity analysis of the input\u2019s sampling strategies at Initialization (PaI) using various similarity metrics. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "AAN46kUPXM/tmp/2b9849ee7009e592889127c14381e6de4d240e363850a95fc6951a6ca5eb8ba5.jpg", "table_caption": ["Table 5: Sensitivity analysis of $N E X P_{\\mathrm{map}}$ \u2019s retrieved at initialization compared with the true $N E X P_{\\mathrm{map}}$ following model convergence. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "579 B Pruning Process: An in-depth analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "580 B.1 Global vs local -scope pruning. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "581 NEXP is used in the pruning process to evaluate and rank different network elements, guiding their   \n582 subsequent removal based on their scores. In our study, we focused on the removal of filters, i.e.,   \n583 Filter Pruning, where we pruned convolutional structures by removing the least expressive filters.   \n584 This can be approached in two ways: (i) on a local (layer-by-layer) basis, where filters are assessed   \n585 and removed according to their expressiveness relative to other filters within the same layer, e.g.,   \n586 eliminating the least $\\mu$ expressive fliters from each layer. (ii) On a global (network-wide) basis, where   \n587 all filters across layers are normalized in terms of their scores, allowing for the removal of the least   \n588 $\\kappa$ expressive filters from the entire network. We experimentally observed that \u201cGlobal Pruning\"   \n589 yields consistent results and outperforms \u201cLocal Pruning\" when using the NEXP pruning criterion.   \n590 Therefore, all the experiments reported in this paper were conducted using the \u201cGlobal Pruning\"   \n591 approach. ", "page_idx": 16}, {"type": "text", "text": "592 B.2 One-shot vs Iterative pruning. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "593 Furthermore, another design parameter to consider in the pruning process is its coordination with   \n594 fine-tuning. In this context, two widely adopted strategies are: (a) \u201cOne-Shot\" pruning, where pruning   \n595 is completed entirely before any fine-tuning occurs, and (b) \u201cIterative\" pruning, which involves   \n596 alternating between pruning and fine-tuning via an iterative sequence. The first one (a) can be   \n597 considered a more lightweight approach and allows for a more robust evaluation of the pruning metric   \n598 at hand, when compared to the later one (b). This is because it has no extra dependency on the training   \n599 data and its efficiency does not depend on the iterative re-calibration of the information state through   \n600 the fine-tuning process. In this study, most experiments where conducted using \u201cOne-Shot\" pruning,   \n601 while we also explored the integration of NEXP in an \u201cIterative\" pruning process with YOLOv8   \n602 (more details on 4.1), where we noted a reduction in performance declines and an improvement in   \n603 the performance recovery after each pruning step, leading to better overall results. ", "page_idx": 16}, {"type": "text", "text": "604 B.3 Detailed description of all algorithmic steps. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "605 More in detail regarding Algorithm 1, we define the data structure $N E X P_{\\mathrm{map}}$ , i.e., a dictionary   \n606 in our implementation, to store the NEXP scores for every filter in the neural network after each   \n607 iteration. Given a neural network $\\mathcal{N}$ with its current weight state $W_{t_{i}}$ , we initially set up all variables   \n608 required for the pruning loop (Lines 1-4). The network is then gradually pruned until one of the   \n609 following conditions is met: the target ratio is achieved or the allowed number of pruning steps   \n610 is exceeded (Line 5). During each pruning iteration, the $\\kappa$ least expressive filters from the current   \n611 pruned state of the network are initially selected (Line 6). These filters are then removed, followed   \n612 by an update to $N E X P_{\\mathrm{map}}$ for the subsequent iteration (Lines 7-8). To obtain the NEXP scores,   \n613 a forward pass $f(X;W_{\\mathrm{pruned}})$ is conducted using a user-provided mini-batch as input. Finally, the   \n614 conditions variables are updated in preparation for the next pruning iteration (Lines 9-10). ", "page_idx": 16}, {"type": "text", "text": "615 B.4 Acceleration of NEXP computations. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "616 In Algorithm 1, Line 8 accounts for the bulk of the computational complexity. Specifically, the   \n617 calculation of $N E X P_{\\mathrm{map}}$ can be divided into two sub-processes: (i) performing a forward pass to   \n618 retrieve all activation patterns, and (ii) estimating the NEXP score for each element in the map.   \n619 However, performing a forward pass can be considered negligible compared to computing the NEXP   \n620 score for each filter. This is because the later involves multiple comparisons between the activation   \n621 patterns of all samples in the mini-batch $X$ for every filter. Two effective ways to reduce this   \n622 computational demand are: first, all operations involved in computing the NEXP score are compatible   \n623 with widely-used BLAS libraries, facilitating hardware acceleration; second, the frequency of score   \n624 updates can be strategically decreased under certain conditions, e.g., every n pruning iterations. ", "page_idx": 16}, {"type": "text", "text": "625 C Experimental Settings ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "626 C.1 Datasets and Models. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "627 This paper explores Computer Vision tasks through extensive experiments on various datasets, such   \n628 as CIFAR-10 [24] and ImageNet [40] for image classification, and COCO [33] for object detection.   \n629 To demonstrate the robustness of our approach, we experiment on several popular architectures and a   \n630 wide span of architectural elements, including VGGNet with a plain structure [44], ResNet with a   \n631 residual structure [17], GoogLeNet with inception modules [45], MobileNet with depthwise separable   \n632 convolutions [19], DenseNet with dense blocks [21] and YOLOv8 with a variety of different modules,   \n633 e.g. C2f and SPPF [22]. ", "page_idx": 17}, {"type": "text", "text": "634 C.2 Adversaries. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "635 We assess the efficacy of expressiveness as criterion for Pruning both after Training (PaT) and at   \n636 Initialization (PaI), using arbitrary (random) data-points. For PaT (4.1), we compare against a plethora   \n637 of foundational and state-of-the-art approaches, ranging from filter magnitude-based [28, 32, 29]   \n638 and loss sensitivity-based [58] methods to feature-guided strategies [23, 30] and search algorithms   \n639 [35, 31]. Regarding PaI (4.3 and D.1), our comparison is two-fold, as we evaluate expressiveness   \n640 using (i) single-shot and (ii) iterative pruning. More specifically, the adversaries for PaI include   \n641 pruning with random scoring, two state-of-the-art single-shot pruning strategies, namely SNIP [27]   \n642 and GraSP [50], as well as one state-of-the-art iterative pruning strategy, named SynFlow [46]. ", "page_idx": 17}, {"type": "text", "text": "643 C.3 Evaluation Metrics. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "644 To effectively quantify the efficiency of reported solutions, we adopt a 3-dimensional evaluation   \n645 space, consisting of i) two widely-used metrics i.e. $F L O P s$ and params, that define the 2-dimensional   \n646 compression solution efficiency, alongside with ii) an NN model accuracy to assess the predictions   \n647 of pruned derivatives [3]. Within the compression space, we define, (a) Compression Ratio $\\left(\\downarrow\\right)=$   \n648 $\\frac{\\mathrm{original\\;size}}{\\mathrm{compressed\\;size}}$ and (b) Compressed Size Percentage $\\begin{array}{r}{(\\%)=\\frac{\\mathrm{compressed\\;size}}{\\mathrm{original\\;size}}\\cdot100.}\\end{array}$ . To assess task-specific   \n649 capabilities, we report the top-1 accuracy of pruned models for image classification on CIFAR-10   \n665501 [Io24U] ,( Ibnottehr steocpt-i1o na nodv etro pU-5n iaocnc) utrharceisehs oflodrs  IrmanaggienNg eftr [o4m0 ],0 .a5n tdo  t0h.e9 5m, edaenn oAtveedr aags $m A P_{50-95}^{v a l}$ (, mfoArP o) bojveecrt   \n652 detection on the COCO dataset [33]. ", "page_idx": 17}, {"type": "text", "text": "653 C.4 Configurations. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "654 We implement the proposed \u201cexpressiveness\" pruning criterion on PyTorch, version $2.0.1\\mathrm{+cu}117$ , by   \n655 extending the DepGraph pruning framework [11] to maintain models compatibility and to ensure   \n656 structural coupling during the removal of network elements e.g., simultaneously removing any inter  \n657 dependent network elements such as kernel pairs of convolutional and batch-normalization batched   \n658 layers. All experiments are conducted on a NVIDIA GeForce RTX 3060 GPU with 12GB of GDDR6   \n659 SDRAM. For all experiments we use a batch of 64 random data-points to estimate expressiveness,   \n660 except those that are reported for CIFAR-10 and ImageNet on 4.1, where we used K-Means to select   \n661 60 samples (6 from each class). Additionally, the baseline models on CIFAR-10 were trained for 200   \n662 epochs by using 128 batch size and Stochastic Gradient Descent algorithm (SGD) with an initial   \n663 learning rate of 0.1 that is divided by 10 after 60 and 120 epochs respectively. For ImageNet models   \n664 and YOLOv8, we utilize the available pre-trained weights on PyTorch vision library and ultralytics   \n665 [22]. We fine-tune the pruned networks for 100 epochs on CIFAR-10 and for 30 epochs on ImageNet   \n666 to compensate for the performance loss, using a batch size of 128 and 32 respectively. ", "page_idx": 17}, {"type": "text", "text": "667 D Supplementary Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "668 D.1 Neural Expressiveness at Initialization: A comparative study ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "669 Adversaries. We establish our comparative study in a two-fold manner, as we compare expressiveness   \n670 against (i) single-shot and (ii) iterative pruning approaches. More specifically, the adversaries include   \n671 pruning with random scoring, two state-of-the-art single-shot pruning strategies, namely SNIP [27]   \n672 and GraSP [50], as well as one state-of-the-art iterative pruning strategy, named SynFlow [46]. For   \n673 our approach, we implement one-shot pruning, utilizing a batch of 64 arbitrary data points for the   \n674 estimation of expressiveness.   \n675 Experimental Setup. We adopt the experimental framework of Tanaka et al. [46], who assess   \n676 algorithm performance across an exponential scale $(10^{r})$ of parameters compression ratios $r\\ \\in$   \n677 $\\{\\bar{0}.00,0.25,0.50,0.75,...\\}$ . Their proposed settings also enable for the evaluation of an algorithm\u2019s   \n678 resilience to \"layer collapses\", typically observed at higher compression levels. Results. We prune   \n679 VGG-16 on CIFAR-10 and compare against the findings of [46]. We remain consistent with our   \n680 adversaries and train the model for 160 epochs, using a batch size of 128 and an initial learning rate   \n681 of 0.1, which is reduced by a factor of 10 after 60 and 120 epochs. The results are illustrated on   \n682 Fig. 5.   \n683 Observations. Our method consistently outperforms all other algorithms, particularly in regimes of   \n684 lower compression, up to $10^{2}(\\downarrow)$ with an average increase of $1.21\\%$ over SynFlow, while maintaining   \n685 competitiveness at higher compression levels, above $10^{2}(\\downarrow)$ with an average percentage difference of   \n686 $4.82\\%$ , $3.72\\%$ and $-2.74\\%$ , compared to GraSP, SNIP and SynFlow respectively. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "AAN46kUPXM/tmp/b959ab6aff56369ae454aad2e7f068e129ec8dd6efb6e0102089dd8ea6a90c96.jpg", "img_caption": ["Figure 5: Pruning VGG-16 at Initialization on CIFAR-10. A comparative visualisation of SOTA methods across an exponential scale of params compression ratios. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "687 D.2 Additional Experimental Results: Tables and Figures ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "688 CIFAR-10. We present further experiments and comparisons with state-of-the-art methods,   \n689 including HRANK [30], GAL [32], ABC [31] and DCP [61], specifically for GoogLeNet and   \n690 MobileNet-v2 networks. For MobileNet-v2, our method attains an increased compression ratio of   \n691 $0.94\\times$ in parameters and $0.75\\times$ in FLOPs $\\left(\\downarrow\\right)$ , with a minimal decrease of only - $\\cdot0.09\\%$ in performance   \n692 compared to DCP. In the GoogLeNet case, we demonstrate a notable enhancement in parameters   \n693 compression within the $1.60\\times$ to $2.20\\times$ FLOPs compression range, surpassing GAL and HRANK   \n694 with margins of $1.8\\times$ and $1.52\\times$ respectively, with an average improvement of $7.5\\%$ in performance   \n695 degradation. ", "page_idx": 18}, {"type": "text", "text": "Table 6: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using VGGNet architectures [44]. ", "page_idx": 19}, {"type": "table", "img_path": "AAN46kUPXM/tmp/12239b8c4b4afcabd6410eae5cbe2752ade189ff2e212005695648d49b68f2f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "AAN46kUPXM/tmp/2c75f48f0f37c603ab78cc94ce01e9c8d9119d919d9deaf3aa938852f7a7d1da.jpg", "table_caption": ["Table 7: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using GoogLeNet [45]. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "AAN46kUPXM/tmp/3769a6b4bc144ea8e523e4db16276647fabb888091464f9754f8cb989d2a9a14.jpg", "table_caption": ["Table 8: Analytical Comparison of Importance-based solutions and Expressiveness on CIFAR-10 using DenseNet-40 [21]. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "AAN46kUPXM/tmp/7215cf7599f203e8000c6a7a74fb2cf9b15dabce940b0e4f44f0f8f8735a1b59.jpg", "table_caption": ["Table 9: Performance Outcomes for MobileNet-v2 on the CIFAR-10 Dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "696 YOLOv8. Figure 6 compares Neural Expressiveness (NEXP) with Layer-Adaptive Magnitude  \n697 Based Pruning (LAMP) [26], Network Slimming (SLIM) [35], Wang et al.\u2019s DepGraph [11], and   \n698 Random Pruning for Object Detection on the COCO dataset, as discussed in 4.1.   \n699 Motivation. YOLOv8 [22] is the current state-of-the-art for Object Detection and Image Segmen  \n700 tation, and has already been widely adopted by many for a variety of real-time applications, e.g.   \n701 Traffic Safety [1], Medical Imaging [39], Rip Currents Detection [10], and more. Such applications   \n702 could majorly benefti from model compression optimizations, achieving higher throughput ratios that   \n703 translate to increased resolution (FPS), and enabling deployment on hardware with strict resource   \n704 constraints. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "AAN46kUPXM/tmp/6a44b0a4f81f2e3d6163ec8423f8d9aa7b041e2f97bf994ac89aea909f49a11c.jpg", "img_caption": ["Figure 6: Pruning YOLOv8m trained on COCO for Object Detection. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "705 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "706 1. Claims   \n707 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n708 paper\u2019s contributions and scope?   \n709 Answer: [Yes]   \n710 Justification: The main contributions have been reflected and discussed across the whole   \n711 paper.   \n712 Guidelines:   \n713 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n714 made in the paper.   \n715 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n716 contributions made in the paper and important assumptions and limitations. A No or   \n717 NA answer to this question will not be perceived well by the reviewers.   \n718 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n719 much the results can be expected to generalize to other settings.   \n720 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n721 are not attained by the paper.   \n722 2. Limitations   \n723 Question: Does the paper discuss the limitations of the work performed by the authors?   \n724 Answer: [Yes]   \n725 Justification: While our work does not implicitly provide a Discussion section, we have   \n726 incorporated any discussions on the limitations and the intricacies of the provided solution   \n727 at its section separately.   \n728 Guidelines:   \n729 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n730 the paper has limitations, but those are not discussed in the paper.   \n731 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n732 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n733 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n734 model well-specification, asymptotic approximations only holding locally). The authors   \n735 should reflect on how these assumptions might be violated in practice and what the   \n736 implications would be.   \n737 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n738 only tested on a few datasets or with a few runs. In general, empirical results often   \n739 depend on implicit assumptions, which should be articulated.   \n740 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n741 For example, a facial recognition algorithm may perform poorly when image resolution   \n742 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n743 used reliably to provide closed captions for online lectures because it fails to handle   \n744 technical jargon.   \n745 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n746 and how they scale with dataset size.   \n747 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n748 address problems of privacy and fairness.   \n749 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n750 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n751 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n752 judgment and recognize that individual actions in favor of transparency play an impor  \n753 tant role in developing norms that preserve the integrity of the community. Reviewers   \n754 will be specifically instructed to not penalize honesty concerning limitations.   \n755 3. Theory Assumptions and Proofs   \n756 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n757 a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Justification: To the best of our knowledge, all the provided set of assumptions presented in Section 3 are complete. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "772 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper specifies in great detail all the information necessary to understand the results, while also any subjectivity imposed by the experimental settings in regards to our claims and conclusions has been discussed. Detailed analysis of both the mathematical, technical and experimental intricacies have been included in our work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "13 5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "814 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n815 tions to faithfully reproduce the main experimental results, as described in supplemental   \n816 material?   \n817 Answer: [No]   \n818 Justification: We plan to release the full code of the implementation and experiments upon   \n819 acceptance.   \n820 Guidelines:   \n821 \u2022 The answer NA means that paper does not include experiments requiring code.   \n822 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n823 public/guides/CodeSubmissionPolicy) for more details.   \n824 \u2022 While we encourage the release of code and data, we understand that this might not be   \n825 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n826 including code, unless this is central to the contribution (e.g., for a new open-source   \n827 benchmark).   \n828 \u2022 The instructions should contain the exact command and environment needed to run to   \n829 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n830 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n831 \u2022 The authors should provide instructions on data access and preparation, including how   \n832 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n833 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n834 proposed method and baselines. If only a subset of experiments are reproducible, they   \n835 should state which ones are omitted from the script and why.   \n836 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n837 versions (if applicable).   \n838 \u2022 Providing as much information as possible in supplemental material (appended to the   \n839 paper) is recommended, but including URLs to data and code is permitted.   \n840 6. Experimental Setting/Details   \n841 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n842 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n843 results?   \n844 Answer: [Yes]   \n845 Justification: The paper specifies all the training and test details necessary to understand   \n846 the results. Each experiment is accompanied by a discussion of its experimental details   \n847 and a reference to its experimental settings (Section 4). Additionally, an overview of the   \n848 experiment settings can be found in Appendix C, and an in-depth analysis of the pruning   \n849 procedure, including its implementation choices, is described in Appendix B.   \n850 Guidelines:   \n851 \u2022 The answer NA means that the paper does not include experiments.   \n852 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n853 that is necessary to appreciate the results and make sense of them.   \n854 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n855 material.   \n856 7. Experiment Statistical Significance   \n857 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n858 information about the statistical significance of the experiments?   \n859 Answer: [No]   \n860 Justification: While we do not explicitly address the statistical significance of each experi  \n861 ment (in a quantitative manner), we do discuss in great detail any assumptions or statistical   \n862 implications of our experiments (in a qualitative manner).   \n863 Guidelines:   \n864 \u2022 The answer NA means that the paper does not include experiments.   \n865 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n866 dence intervals, or statistical significance tests, at least for the experiments that support   \n867 the main claims of the paper.   \n868 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n869 example, train/test split, initialization, random drawing of some parameter, or overall   \n870 run with given experimental conditions).   \n871 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n872 call to a library function, bootstrap, etc.)   \n873 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n874 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n875 of the mean.   \n876 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n877 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n878 of Normality of errors is not verified.   \n879 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n880 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n881 error rates).   \n882 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n883 they were calculated and reference the corresponding figures or tables in the text.   \n884 8. Experiments Compute Resources   \n885 Question: For each experiment, does the paper provide sufficient information on the com  \n886 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n887 the experiments?   \n888 Answer: [Yes]   \n889 Justification: While we do not provide the exact times of executions and memory require  \n890 ments for each experiment, we do provide an in-depth analysis of all the parameters and   \n891 experimental specifications, along side with the overview of the configurations that were   \n892 used for this work Appendix C and Section 4.   \n893 Guidelines:   \n894 \u2022 The answer NA means that the paper does not include experiments.   \n895 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n896 or cloud provider, including relevant memory and storage.   \n897 \u2022 The paper should provide the amount of compute required for each of the individual   \n898 experimental runs as well as estimate the total compute.   \n899 \u2022 The paper should disclose whether the full research project required more compute   \n900 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n901 didn\u2019t make it into the paper).   \n902 9. Code Of Ethics   \n903 Question: Does the research conducted in the paper conform, in every respect, with the   \n904 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n905 Answer: [Yes]   \n906 Justification: We have thoroughly reviewed the research conducted in the paper and fully   \n907 agree that it conforms to the NeurIPS Code of Ethics.   \n908 Guidelines:   \n909 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n910 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n911 deviation from the Code of Ethics.   \n912 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n913 eration due to laws or regulations in their jurisdiction).   \n914 10. Broader Impacts   \n915 Question: Does the paper discuss both potential positive societal impacts and negative   \n916 societal impacts of the work performed?   \n917 Answer: [NA]   \n918 Justification: While our work does not directly discuss societal impacts, we do reference the   \n919 eco-friendly implications of efficient models in the Introduction section 1. Additionally, we   \n920 highlight the potential indirect societal beneftis that can arise from optimizing models, such   \n921 as in the case of YOLOv8 D.2. .   \n922 Guidelines:   \n923 \u2022 The answer NA means that there is no societal impact of the work performed.   \n924 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n925 impact or why the paper does not address societal impact.   \n926 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n927 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n928 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n929 groups), privacy considerations, and security considerations.   \n930 \u2022 The conference expects that many papers will be foundational research and not tied   \n931 to particular applications, let alone deployments. However, if there is a direct path to   \n932 any negative applications, the authors should point it out. For example, it is legitimate   \n933 to point out that an improvement in the quality of generative models could be used to   \n934 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n935 that a generic algorithm for optimizing neural networks could enable people to train   \n936 models that generate Deepfakes faster.   \n937 \u2022 The authors should consider possible harms that could arise when the technology is   \n938 being used as intended and functioning correctly, harms that could arise when the   \n939 technology is being used as intended but gives incorrect results, and harms following   \n940 from (intentional or unintentional) misuse of the technology.   \n941 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n942 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n943 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n944 feedback over time, improving the efficiency and accessibility of ML).   \n945 11. Safeguards   \n946 Question: Does the paper describe safeguards that have been put in place for responsible   \n947 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n948 image generators, or scraped datasets)?   \n949 Answer: [NA]   \n950 Justification: Our work emphasizes on efficiently compressing Neural Networks and does   \n951 not target any specific use-case scenario, rather it addresses the greater challenge of Vision   \n952 as whole.   \n953 Guidelines:   \n954 \u2022 The answer NA means that the paper poses no such risks.   \n955 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n956 necessary safeguards to allow for controlled use of the model, for example by requiring   \n957 that users adhere to usage guidelines or restrictions to access the model or implementing   \n958 safety filters.   \n959 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n960 should describe how they avoided releasing unsafe images.   \n961 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n962 not require this, but we encourage authors to take this into account and make a best   \n963 faith effort.   \n964 12. Licenses for existing assets   \n965 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n966 the paper, properly credited and are the license and terms of use explicitly mentioned and   \nproperly respected? ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "969 Justification: All the creators and original owners of the assets that were utilized for this   \n970 work were properly credited through-out all parts of the paper, while also a detailed report   \n971 of them can be found in Appendix C.   \n972 Guidelines:   \n973 \u2022 The answer NA means that the paper does not use existing assets.   \n974 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n975 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n976 URL.   \n977 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n978 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n979 service of that source should be provided.   \n980 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n981 package should be provided. For popular datasets, paperswithcode.com/datasets   \n982 has curated licenses for some datasets. Their licensing guide can help determine the   \n983 license of a dataset.   \n984 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n985 the derived asset (if it has changed) should be provided.   \n986 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n987 the asset\u2019s creators.   \n988 13. New Assets   \n989 Question: Are new assets introduced in the paper well documented and is the documentation   \n990 provided alongside the assets?   \n991 Answer: [NA]   \n992 Justification: The paper does not release new assets besides the conceptualization and both   \n993 then technical and theoretical formulation of Neural Expressiveness. However, we plan to   \n994 release the full code of the implementation and experiments upon acceptance.   \n995 Guidelines:   \n996 \u2022 The answer NA means that the paper does not release new assets.   \n997 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n998 submissions via structured templates. This includes details about training, license,   \n999 limitations, etc.   \n1000 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1001 asset is used.   \n1002 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1003 create an anonymized URL or include an anonymized zip file.   \n1004 14. Crowdsourcing and Research with Human Subjects   \n1005 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1006 include the full text of instructions given to participants and screenshots, if applicable, as   \n1007 well as details about compensation (if any)?   \n1008 Answer: [NA]   \n1009 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n1010 Guidelines:   \n1011 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1012 human subjects.   \n1013 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1014 tion of the paper involves human subjects, then as much detail as possible should be   \n1015 included in the main paper.   \n1016 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1017 or other labor should be paid at least the minimum wage in the country of the data   \n1018 collector.   \n1019 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 26}, {"type": "text", "text": "Subjects ", "page_idx": 26}, {"type": "text", "text": "1021 Question: Does the paper describe potential risks incurred by study participants, whether   \n1022 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1023 approvals (or an equivalent approval/review based on the requirements of your country or   \n1024 institution) were obtained?   \n1025 Answer: [NA]   \n1026 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n1027 Guidelines:   \n1028 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1029 human subjects.   \n1030 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1031 may be required for any human subjects research. If you obtained IRB approval, you   \n1032 should clearly state this in the paper.   \n1033 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1034 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1035 guidelines for their institution.   \n1036 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1037 applicable), such as the institution conducting the review. ", "page_idx": 27}]