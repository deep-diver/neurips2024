[{"type": "text", "text": "Structured Matrix Basis for Multivariate Time Series Forecasting with Interpretable Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaodan Chen1, Xiucheng $\\mathbf{L}\\mathbf{i}^{2}\\;(\\boxtimes\\triangleleft)$ , Xinyang Chen2, Zhijun $\\mathbf{Li}^{1}$ $\\Cap$ ", "page_idx": 0}, {"type": "text", "text": "1 School of Computer Science and Technology, Harbin Institute of Technology 2 School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen {21B303004@stu., lixiucheng@, chenxinyang@, lizhijun_os@}hit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multivariate time series forecasting is of central importance in modern intelligent decision systems. The dynamics of multivariate time series are jointly characterized by temporal dependencies and spatial correlations. Hence, it is equally important to build the forecasting models from both perspectives. The real-world multivariate time series data often presents spatial correlations that show structures and evolve dynamically. To capture such dynamic spatial structures, the existing forecasting approaches often rely on a two-stage learning process (learning dynamic series representations and then generating spatial structures), which is sensitive to the small time-window input data and has high variance. To address this, we propose a novel forecasting model with a structured matrix basis. At its core is a dynamic spatial structure generation function whose output space is well-constrained and the generated structures have lower variance, meanwhile, it is more expressive and can offer interpretable dynamics. This is achieved via a novel structured parameterization and imposing structure regularization on the matrix basis. The resulting forecasting model can achieve up to $8.5\\%$ improvements over the existing methods on six benchmark datasets, and meanwhile, it enables us to gain insights into the dynamics of underlying systems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multivariate time series forecasting plays a pivotal role in a wide range of fields, such as traffic flow management, electricity consumption, and weather prediction. Multivariate time series data records quantities of interest from $N$ series spanning over $T$ time steps, and its underlying dynamics are jointly characterized by the temporal correlations (intra-series dependencies) and spatial structures (inter-series dependencies). Inspired by the advancements in Natural Language Processing, substantial research has been proposed to apply RNNs and Transformers to capture the underlying temporal dependencies [41, 32, 44, 45, 30]. Besides, the convolution paradigm also exhibits promising temporal correlation modeling capability and excels in long-term forecasting [33, 36, 29]. ", "page_idx": 0}, {"type": "text", "text": "As the underlying dynamics are jointly described by the intra- and inter-correlations, it is equally important to explore the spatial structures for an ideal forecasting model design. Many proposals employ dense connection to capture the spatial correlations implicitly [31, 24]. However, the dense connection lacks clear structures and is prone to introduce noise from uncorrelated spatial dimensions. The development of Graph Neural Networks (GNNs) [18, 17, 7] offers an effective solution to model non-Euclidean structure data. DCRNN [22] builds graphs based on spatial proximity and conducts graph convolution to capture spatial correlations for traffic forecasting. To apply GNNs to more general scenarios where graph structures are unavailable, the forecasting methods propose to learn the graph adaptively through learnable node embeddings [37, 1, 38, 15], which significantly enhances the forecasting performance. ", "page_idx": 0}, {"type": "text", "text": "Despite the progress achieved, the spatial structures remain static across time steps for the aforementioned forecasting methods, which may not reflect the actual inter-series correlation. Because in many scenarios, the spatial correlations are also changing dynamically, for example, the traffic speeds of certain road segments manifest correlation only in peak hours. To relax this restriction, many dynamic graph-based methods have been proposed [43, 39, 42, 28, 34] to learn the spatial structures within a short time window. The intuition is that the spatial correlations in real-world applications often evolve continuously and tend to be stable over a short period of time. The general idea is to learn each series a dynamic representation via a nonlinear transformation $f_{\\mathrm{dynm}}$ by taking as input the current time window data, and it then generates the spatial structures by pairwise interacting the dynamic representations via a transformation $f_{\\mathrm{pair}}$ , the composition of two transformations forms the spatial structure generation function $f_{\\mathrm{spatial}}\\bar{=}\\,f_{\\mathrm{pair}}\\circ f_{\\mathrm{dynm}}$ . The dynamic representation function $f_{\\mathrm{dynm}}$ is often implemented as MLP ([43, 42, 28, 34]) or RNN ([39]) whereas $f_{\\mathrm{pair}}$ is mostly instantiated by attention mechanism or inner product. One severe issue of these methods is that the output space of $f_{\\mathrm{dynm}}$ is not well constrained and unbounded, this makes the learned dynamic representations very sensitive to the short time window input data and the unboundedness will be exaggerated by the inner product operation in $f_{\\mathrm{pair}}$ , which will lead to the outputs of $f_{\\mathrm{spatial}}$ fluctuate drastically and have high variance. The issue becomes even more severe in the presence of anomaly patterns. To reduce the variance, TPGNN [26] first learns a static spatial structure A (adjacency matrix) and then generates the dynamic spatial structures with a matrix polynomial mM= 1 \u03b1mAm where \u03b1m is determined by the timestamps of the current time window. However, it has two drawbacks: 1) the coefficient $\\alpha_{m}$ solely depends on the timestamps and cannot adapt to the current window data, and 2) the matrix power basis A $\\mathbf{\\Omega}_{\\mathbf{\\lambda},\\mathbf{\\lambda}}\\mathbf{A}^{2},\\dots,\\mathbf{A}^{M}$ is overly restricted and has limited expressive capability. In addition, the existing forecasting methods often lack interpretable dynamics. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a dynamic multivariate time series forecasting model with a structured matrix basis. Instead of relying on the two-stage spatial structure learning process (learning dynamic series representations and then generating spatial structures), we directly parameterize $f_{\\mathrm{spatial}}$ with a learnable matrix basis $\\mathbf{B}_{1},\\mathbf{B}_{2},\\dots,\\mathbf{B}_{M}$ and represent any spatial structure with a convex combination $\\scriptstyle\\sum_{m=1}^{M}\\alpha_{m}\\mathbf{B}_{m}$ , $\\alpha_{m}\\geq0$ and $\\textstyle\\sum_{m=1}^{M}\\alpha_{m}=1$ . To learn the matrix basis effectively, we propose a novel structured matrix parameterization method and impose structure regularization on the basis to enhance parameter efficiency and reduce complexity. In contrast to the two-stage spatial structure learning methods, the output space of our proposed $f_{\\mathrm{spatial}}$ is well constrained. Consequently, the generated spatial structures have lower variance and the resulting model is easier to learn. In comparison to TPGNN, our matrix basis is more expressive since it is not limited by the matrix power constraint; the coefficient $_{\\alpha}$ can also be computed adaptively via the interaction of current time-window data and the basis. In addition, the coefficient $_{\\alpha}$ offers a fashion to track the spatial structure evolution and enables us to gain insights into the underlying dynamics. Thus, the resulting model is more interpretable. ", "page_idx": 1}, {"type": "text", "text": "In summary, our proposed $f_{\\mathrm{spatial}}$ has the following appealing properties: 1) lower variance and easier to learn, 2) it is more expressive, and 3) it can yield more interpretable results. This is achieved through a novel structured matrix parameterization and structure regularization. By integrating $f_{\\mathrm{spatial}}$ into the forecasting framework, we evaluate the efficacy of the proposed method on six benchmark datasets, it achieves up to $8.5\\%$ improvements over existing forecasting methods across various prediction lengths and can also offer interpretable dynamics. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Temporal Dependency Modeling Early deep sequential methods adopt recurrent neural networks to capture nonlinear temporal dynamics [41, 32, 5]. Motivated by the wide receptive fields of attention mechanism, various Transformer-based methods have been developed to capture the longterm temporal dependencies in forecasting. To reduce the quadratic complexity of the vanilla attention, LogTrans [21], Informer [44], Autoformer [35], and FEDformer [45] have been proposed successively. Non-stationary Transformer [27] attempts to mitigate the difficulties caused by nonstationarity in modeling temporal correlation. PatchTST [30] explores the strategies of patch-level semantic modeling and channel-independence. VQ-MTM [13] explores the well-defined semantic units for the Transformer architecture in time series modeling. In state-space models, the transition matrix is employed to model long-term temporal dependencies, and their recent representative works include Hippo [10], LSSL [11]. To reduce the computational complexity, S4 [12] and Mamba [9] have been proposed successively. The convolution-based methods have also shown promising results in time series modeling. MICN [33] explores isometric convolution to capture non-local temporal patterns. TimesNet [36] reshapes 1D signals into 2D by aligning according to inherent multiple periodicity and it then employs the 2D kernels to capture both intraperiod- and interperiod-variations. ModernTCN [29] utilizes large kernels to model long-term dependencies. In addition, the Fourier basis parameterization has also been proposed to model long-term dependencies in time series imputation [25]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Static Spatial Correlation Modeling The spatial correlation plays an equally important role in time series modeling. DeepAR [31] and Pyraformer [24] employ dense connection to model the spatial dependencies. However, the dense connection fails to explore the underlying structure and may introduce noises from unrelated series. The advancement of GNNs offers an effective way to model non-Euclidean structure data. DCRNN [22] constructs the graph by using spatial proximity and proposes to fuse the spatial information via graph convolution operation, and thus it is only applicable when the underlying graph structures are easily accessible. To sidestep this limitation, the adaptive GNNs-based methods [1, 38, 15, 14, 16] propose to learn each series a representation and then generate the spatial structure via the interaction of the representations. BiTGraph [4] further develops the method to account for missing patterns in message passing to handle the time series with missing values. However, these approaches implicitly assume the series representations are shared across the entire time steps, and hence the underlying spatial graphs remain static over time. ", "page_idx": 2}, {"type": "text", "text": "Dynamic Spatial Correlation Modeling In real-world applications, the inter-series correlations or spatial structures are often evolving dynamically. To adapt to these scenarios, many dynamic spatial structure methods are proposed, which share a similar two-stage spatial structure learning process as the static adaptive GNNs-based methods (as discussed in Section 1). The difference is that the series representations are generated by conditioning a small time window rather than the entire timeline data, and hence, the underlying spatial structures can change dynamically over time. GMAN [43], iTransformer [28], Crossformer [42], and Card [34] implement the dynamic node representation function via MLP, whereas ESG [39] adopts the RNN. As the output spaces of their $f_{\\mathrm{spatial}}$ are not well constrained, the learned spatial structures are very sensitive to the change of time-window data and hence have high variance. To reduce the variance, TPGNN [26] proposes to represent the dynamic graphs with matrix polynomial. As the polynomial coefficients are solely determined by the timestamps, such a method fails to utilize the current time-window data. Moreover, the matrix power basis is overly restricted and has weak expressive capability. The proposals [40, 8] attempt to build dynamic graphs by merging temporal and spatial dimensions. Nevertheless, the entanglement of temporal correlations and spatial dependencies makes the models hard to optimize. In addition, the existing multivariate time series forecasting methods also lack interpretability regarding the underlying dynamics. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation From a generative perspective, the multivariate time series $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{N\\times T\\times D}$ records a $D$ -dimensional physical quantities of interest generated by $N$ series (i.e., sensors or instances) over $T$ time steps. We use $\\mathbf{X}^{\\left(n\\right)}\\in\\mathbb{R}^{T\\times D}$ to represent the observations from $n$ -th sensor and ${\\bf X}_{t}\\in\\mathbb{R}^{N\\times D}$ to indicate the observations at the $t$ -th timestamp. The slice notation $\\mathbf{X}_{t-H:t}\\in\\mathbb{R}^{N\\times H\\times D}$ denotes the values within a window spanning from the time interval $[t-H,t)$ . The operator $\\mathrm{diag}:\\mathbb{R}^{N\\times N}\\mapsto\\mathbb{R}^{N}$ takes the diagonal elements of a square matrix and returns it as a vector, the operator vec reshapes a matrix or tensor into a vector. ", "page_idx": 2}, {"type": "text", "text": "Overview and Pipeline Figure 1-(a) presents the architecture of our proposed Sumba (dynamic multivariate time series forecasting with structured matrix basis), which comprises $L$ blocks. Each block contains two primary modules: the Multi-Scale TCN and Dynamic GCN modules. The MultiScale TCN module in the \u2113-th block takes as the input Zt(0\u2113\u2212\u22121H):t \u2208RN\u00d7H\u00d7Di(\u2113\u22121) and generates the intermediate representation Z\u2032t(0\u2113\u2212\u2212H1):t0 $\\mathbf{Z}_{t_{0}-H:t_{0}}^{\\prime(\\ell-1)}\\in\\mathbb{R}^{N\\times H\\times D_{o}^{(\\ell-1)}}$ , which is fed to the Dynamic GCN module to produce $\\mathbf{Z}_{t_{0}-H:t_{0}}^{(\\ell)}\\in\\mathbb{R}^{N\\times H\\times D_{i}^{(\\ell)}}$ . The Multi-Scale TCN captures the temporal dependencies by performing multi-scale temporal convolution operation in a channel-independent manner, we choose the kernel sizes $1\\times2,1\\times3,1\\times6$ , and $1\\times7$ in this paper. The Dynamic GCN module comprises two functions, namely, the spatial structure generation function $f_{\\mathrm{spatial}}$ and graph convolution function $f_{\\mathrm{gcn}}$ . Our proposed $f_{\\mathrm{spatial}}$ generates the dynamic spatial structure ${\\bf A}_{t}\\in\\mathbb{R}^{N\\times N}$ (adjacency matrix) ", "page_idx": 2}, {"type": "image", "img_path": "co7DsOwcop/tmp/59df063d4522f303404b85eda4af20886474b3857e8e18a145da1f31049ac90f.jpg", "img_caption": ["Figure 1: (a) The framework of our proposed Sumba. (b) the detailed structure of the Dynamic GCN module. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "by conditioning on the intermediate representation $\\mathbf{Z}_{t}^{\\prime(\\ell-1)}\\in\\mathbb{R}^{N\\times D_{o}^{(\\ell-1)}}$ at time step $t\\in[t_{0}-H,t_{0})$ , which is obtained by transforming the current time-window data $\\mathbf{X}_{t_{0}-H:t_{0}}$ . Given the generated dynamic graph At, fgcn further fuses the spatial information of Z\u2032t(\u2113\u22121)a cross different series to yield $\\mathbf{Z}_{t}^{(\\ell)}$ by performing the graph convolution operation. Note that the spatial and temporal dimensions are kept unchanged during the entire transformation in the $L$ blocks. ", "page_idx": 3}, {"type": "text", "text": "3.1 Adaptive Dynamic Spatial Structure Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The core layer of our proposed forecasting method is the spatial structure generation function $f_{\\mathrm{spatial}}$ . It serves to infer the optimal graph structure ${\\bf A}_{t}$ that best characterizes the present spatial correlations from the intermediate representation $\\mathbf{Z}_{t}^{\\prime}$ (we drop the layer index $\\ell$ to keep the notation uncluttered in this subsection). As mentioned in Section 1, the existing methods all adopt the two-stage learning process, which results in unconstrained output function space and high graph structure variance. To address this, we propose to directly parameterize a learnable matrix basis $\\boldsymbol{B}=\\{\\mathbf{B}_{m}\\}_{m=1}^{M}$ of dimension where and define the spatial structure generation function as ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\mathrm{spatial}}(\\mathbf{Z}_{t}^{\\prime})\\triangleq\\sum_{m=1}^{M}\\alpha_{t,m}\\mathbf{B}_{m},\\quad\\alpha_{t,m}\\geq0,\\sum_{m=1}^{M}\\alpha_{t,m}=1.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, we choose the convex combination instead of the linear combination to better control the output space. This is reasonable since the basis $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is free to optimize in the training stage. The intuition behind Eq. 1 is that the basis $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ can be shared and optimized globally across different time windows, and to infer the spatial structure dynamically, we only need to adaptively compute the coefficient $\\pmb{\\alpha}_{t}\\in\\mathbb{R}^{M}$ by conditioning on $\\mathbf{Z}_{t}^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "However, two challenges remain in adopting the spatial structure generation function in Eq. 1. 1) The number of parameters to be learned equals $M\\bar{N}^{2}$ , which grows quadratically to $N$ . As these $M$ matrices lack connection and constraints, learning by brute force will sooner become infeasible even for medium-size $N$ . This actually is the reason why the existing methods resort to the two-stage learning process, i.e., learning each series an embedding whose learnable parameters are $N D$ with $D$ being the embedding dimension. 2) Intuitively, the best $\\alpha_{m}$ should be simultaneously determined by $\\mathbf{Z}^{\\prime}$ and ${\\bf B}_{m}$ , but identifying ${\\bf B}_{m}$ with $N^{2}$ parameters will make $\\alpha_{m}$ hard to compute when $N$ is large. ", "page_idx": 3}, {"type": "text", "text": "Structured Parameterization and Regularization To circumvent the two challenges, we propose to parameterize the basis matrices in a structured manner and impose additional structure regularization on the basis. The idea is to represent each basis matrix ${\\bf{B}}_{m}$ in its SVD (singular value decomposition) factor product form $\\mathbf{B}_{m}=\\mathbf{\\dot{U}}_{m}\\boldsymbol{\\Sigma}_{m}\\mathbf{V}_{m}^{\\top}$ and then parameterize the factors ${\\mathbf{U}}_{m}$ , $\\Sigma_{m}$ , $\\mathbf{V}_{m}$ , where $\\mathbf{U}_{m},\\dot{\\mathbf{V}}_{m}\\in\\mathbb{R}^{N\\times N}$ are orthogonal matrices, and $\\Sigma_{m}$ is a diagonal matrix consisting of the singular values of ${\\bf B}_{m}$ . One benefit of such structured parameterization is that it permits us to establish connections between the basis matrices and impose constraints, and consequently, enhance the parameter efficiency and ease of the model learning. To be specific, we impose the constraint that all $\\mathbf{B}_{m}$ for $m=1,2,\\ldots,M$ share the same parameterized orthogonal matrices U, $\\mathbf{V}$ and each matrix has its unique $\\Sigma_{m}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\boldsymbol{\\mathcal{B}}}\\triangleq\\left\\{\\mathbf{U}{\\boldsymbol{\\Sigma}}_{1}\\mathbf{V}^{\\top},\\mathbf{U}{\\boldsymbol{\\Sigma}}_{2}\\mathbf{V}^{\\top},\\dots,\\mathbf{U}{\\boldsymbol{\\Sigma}}_{M}\\mathbf{V}^{\\top}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The rationality behind such a choice stems from the geometry interpretation of orthogonal matrixvector multiplication, i.e., left multiplying a vector by an orthogonal matrix is equivalent to coordinate transformation, which is provided in Appendix A.1. Hence, we implicitly require that all basis matrices share the same pair of coordinate transformations, which can be considered a sort of implicit regularization since it reduces extra freedom and guides the model to find the coordinate transformations (U and $\\mathbf{V}$ ) that best suit the basis. ", "page_idx": 4}, {"type": "text", "text": "Dynamic Coefficient Generation Such a parameter sharing regularization mechanism along with the structured parameterization also brings another benefit, it allows us to treat $\\mathrm{diag}(\\Sigma_{m})\\,\\,\\overline{{\\in}}\\,\\,\\mathbb{R}^{N}$ as a fingerprint to identify each ${\\bf{B}}_{m}$ . Hence, we can compute the $\\alpha_{t}$ to infer the dynamic spatial structure by simultaneously conditioning on $\\mathbf{Z}_{t}^{\\prime}$ and $\\mathrm{diag}(\\Sigma_{m})$ . To this end, we design an adaptive matching module that takes as input $\\mathbf{Z}_{t}^{\\prime}\\in\\mathbb{R}^{N\\times\\bar{D}_{o}}$ and $[\\grave{\\mathrm{diag}}(\\grave{\\Sigma_{1}}),\\mathrm{diag}(\\Sigma_{2}),\\dots,\\mathrm{diag}(\\Sigma_{M})]$ , and it yields the coefficient $\\alpha_{t}$ at the $t$ time step, as follows. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}\\triangleq[\\mathrm{diag}(\\Sigma_{1}),\\mathrm{diag}(\\Sigma_{2}),\\hdots,\\mathrm{diag}(\\Sigma_{M})]\\in\\mathbb{R}^{N\\times M}}\\\\ &{\\~\\mathbf{z}\\triangleq\\mathrm{vec}(\\mathbf{Z}_{t}^{\\prime})+\\mathrm{TimeEncoding}(t)\\quad\\qquad\\in\\mathbb{R}^{N D_{o}}}\\\\ &{\\alpha_{t}=\\mathrm{softmax}\\left((\\mathbf{W}_{d}\\mathbf{D})^{\\top}\\mathbf{W}_{z}\\mathbf{z}/\\sqrt{d}\\right)\\quad\\qquad\\in\\mathbb{R}^{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where TimeEncoding is the timestamp encoding function, $\\mathbf{W}_{d}$ and $\\mathbf{W}_{z}$ are used to match the dimension, i.e., mapping $\\mathbf{D}$ and ${\\bf z}$ into the $\\mathbb{R}^{d}$ space. Given the structured basis in Eq. 2, each $\\alpha_{t}$ represents a dynamic spatial structure or graph at step $t$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{A}_{t}=f_{\\mathrm{spatial}}(\\mathbf{Z}_{t}^{\\prime})=\\sum_{m=1}^{M}\\alpha_{t,m}\\mathbf{U}\\Sigma_{m}\\mathbf{V}^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given ${\\bf A}_{t}$ at each time step, we can perform graph convolution operation to aggregate information from the spatial dimension. The process is illustrated in Figure 1-(b). The output space of $f_{\\mathrm{spatial}}$ is well constrained on the premise that $\\Sigma_{m}$ are well bounded, which is given by the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. The output space of $f_{\\mathrm{spatial}}$ in Eq. 4 is bounded by the sum of the maximum of $\\Sigma_{m}(t h e$ maximum singular value of $\\mathbf{B}_{m}$ ) for $m=1,2,\\ldots,M$ in terms of the $\\ell_{2}$ norm i.e., $\\|f_{\\mathrm{spatial}}\\|_{2}\\leq$ mM=1 max(\u03a3m). ", "page_idx": 4}, {"type": "text", "text": "The proof is presented in Appendix A.2 and Theorem 3.1 states that the variance of the learned structures is controllable by restricting the maximum value of $\\Sigma_{m}$ , which is easy to achieve since $\\Sigma_{m}$ can simply be parameterized by a vector with nonnegative values. Besides, by tracking the change of $\\alpha_{t}$ over time, our proposed method enables us to gain insight into the underlying dynamics of the system, and thus offers additional interpretability, as we will show in Section 4.4. ", "page_idx": 4}, {"type": "text", "text": "Orthogonality To impose the orthogonal constraint, one may attempt to apply an orthogonality penalty, i.e., by adding the penalty term $\\|\\mathbf{U}^{\\top}\\mathbf{U}-\\mathbf{I}\\|+\\|\\mathbf{V}^{\\top}\\mathbf{V}-\\mathbf{I}\\|$ to the optimized objective. However, such a hard penalty cannot guarantee genuine orthogonality and the extra penalty term also increases the learning difficulty. Hence, rather than relying on the hard penalty, we opt for the orthogonal parameterization. In particular, we restrict our attention to the special orthogonal group ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{SO}(N)\\triangleq\\left\\{\\mathbf{A}\\in\\mathbb{R}^{N\\times N}\\mid\\mathbf{A}^{\\top}\\mathbf{A}=\\mathbf{I},\\det(\\mathbf{A})=1\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which are flexible enough to represent the coordinate transformation. A nice property of $\\mathrm{SO}(N)$ is that it is both a compact Lie group and a smooth manifold [6]. The tangent space of the Lie group at the identity forms a vector space equipped with a Lie bracket operation, namely, its Lie algebra. The Lie algebra of $\\mathrm{SO}(N)$ is the set of all skew-symmetric matrices, denoted by $\\mathfrak{s o}(N)$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathfrak{s o}}(N)\\triangleq\\left\\{\\mathbf{A}\\in\\mathbb{R}^{N\\times N}\\mid\\mathbf{A}^{\\intercal}=-\\mathbf{A}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In differentiable manifold and Lie group, a well-known result is that the matrix exponential $\\mathrm{exp:}$ $\\mathfrak{s o}\\mapsto\\mathrm{SO}$ establishes the connection between a Lie group and its Lie algebra, i.e., for any ${\\mathfrak{g}}\\in{\\mathfrak{s o}}(N)$ we have $\\exp({\\mathfrak{g}})\\in\\mathrm{SO}(N)$ . The matrix exponential is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\exp(\\mathbf{A})\\triangleq\\sum_{k=0}^{\\infty}{\\frac{1}{k}}\\mathbf{A}^{k}=\\mathbf{I}+\\mathbf{A}+{\\frac{1}{2}}\\mathbf{A}^{2}+\\cdot\\cdot\\cdot\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The computation of the matrix exponential map is costly but for the special orthogonal group it has a cheap first order approximation, also known as the Clay map [3, 2], ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi(\\mathbf{A})\\triangleq\\left(\\mathbf{I}+{\\frac{1}{2}}\\mathbf{A}\\right)\\left(\\mathbf{I}-{\\frac{1}{2}}\\mathbf{A}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The Clay map can be implemented in parallel by the Gaussian elimination algorithm in a numerically stable way. ", "page_idx": 5}, {"type": "text", "text": "Low Rank Approximation By using the Clay map, the number of parameters required to parameterize $\\mathbf{U}$ and $\\mathbf{V}$ is $N(N-1)$ (two skew-symmetric matrices $N(N\\bar{-}1)/2+N(\\bar{N}-1)/2)$ . Thus the total number of parameters required by the basis in Eq. 2 is $N(N-1)+M N$ . To further reduce the parameter count, we can apply the low rank approximation. In real-world applications, the spatial structures (adjacency matrices) of multivariate time series are often low-ranked. Suppose the rank is $K$ , we can only preserve the first $K$ columns of $\\mathbf{U}$ and $\\mathbf{V}$ as well as parameterize each $\\Sigma_{m}$ with a length $K$ vector, which leads to a parameter count upper bound $N K+M K$ . Since $K$ is often much less than $N$ in practice, the low rank approximation can enhance both the parameter and computation efficiency when $N$ is large. ", "page_idx": 5}, {"type": "text", "text": "3.2 Hierarchical Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "By stacking $L$ blocks, the Sumba significantly enhances its capability to model temporal correlations and spatial dependencies effectively. We initialize ${\\bf Z}^{(0)}$ with the original input $\\mathbf{X}\\in\\mathbb{R}^{N\\times H\\times D}$ . The output of L-th block Z(L) \u2208RN\u00d7H\u00d7D produces the multi-step prediction $\\hat{\\mathbf{Y}}_{t_{0}:t_{0}+F}$ through a linear transformation. The model is optimized by minimizing mean absolute error (MAE): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MAE}}(\\mathbf{Y}_{t_{0}:t_{0}+F},\\hat{\\mathbf{Y}}_{t_{0}:t_{0}+F})\\triangleq\\frac{\\sum_{n=1}^{N}\\sum_{t=t_{0}}^{t_{0}+F-1}|\\hat{y}_{t}^{(n)}-y_{t}^{(n)}|}{N\\times F}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate our approach Sumba against 15 time series forecasting methods on six benchmark datasets (Section 4.2 and Appendix C). The ablation studies are presented in Section 4.3. We demonstrate the interpretability of our method with case studies in Section 4.4. The sensitivity of hyperparameters is provided in Section 4.5 and Appendix D; the computational cost is empirically studied in Appendix E. The code of Sumba is available at: https://github.com/chenxiaodanhit/Sumba/. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets We conduct experiments on six commonly adopted public datasets including: (1) Electricity [44] contains hourly electricity consumption of 321 clients. (2) Weather [35] includes 21 meteorological factors collected every 10 minutes from the weather station of the Max Planck Biogeochemistry Institute. (3) PEMS [23] records traffic data of 358 variates in California sampled every 5 minutes. (4) ETTh2 [44] contains hourly data from 7 electricity transformers. (5) Traffic [35] measures the hourly road occupancy rates of 862 sensors on San Francisco Bay area freeways. (6) Solar-Energy [20] records the solar power production, which is sampled every 10 minutes from 137 PV plants. ", "page_idx": 5}, {"type": "text", "text": "Baselines We compare our method with the following baselines: (1) TCN-based methods: MICN [33], ModernTCN [29]; (2) Transformer-based methods: PatchTST [30], FEDformer [45], Autoformer [35], Reformer [19]; (3) Static graphs-based methods: MTGNN [38], MegaCRN [15]; (4) Dynamic graphs-based methods: iTransformer [28], Crossformer [42], Card [34], ESG [39], TPGNN [26], FourierGNN [40]; (5) Structured State Space model: S4 [12]. More details of baselines are provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Implementation details The number of blocks $L$ of Sumba is set to 3, the dimension of structured basis $M$ is set to 5, and the rank $K$ is set to $\\operatorname*{min}(N,30)$ in all our experiments. The batch size is 32, the learning rate is 0.0001. We split the datasets into training, validation, and test datasets with the ratio $0.6/0.2/0.2$ chronologically. The future window size $F$ is set to 3, 6, 12, and 24 for all methods, and the history window size $H$ is set to 168. All methods are trained on Nvidia V100 GPUs. Our method is implemented with PyTorch 2.0 and we use the source codes publicly released by the authors for all baseline methods. We adjust the hyperparameters of baseline methods to obtain the best performance on each dataset, and evaluate the performance of different methods in terms of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). ", "page_idx": 5}, {"type": "table", "img_path": "co7DsOwcop/tmp/2eb6c0b99980f115a8db75fa011fdf1a21602e70ebbee06945d67819ce6179d4.jpg", "table_caption": ["Table 1: The forecasting results with prediction horizons of 3 and 6 on Electricity, Weather, etc. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "co7DsOwcop/tmp/275dd1580bb7a08a97dca35774fdc6bb275af80eac3c7257ce92a307173deefc.jpg", "table_caption": ["Table 2: The forecasting results with prediction horizons of 3 and 6 on PEMS and Solar datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Forecasting Performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 and 2 present the forecasting performance with $F\\,\\in\\,\\{3,6\\}$ on six benchmark datasets, with results evaluated using three different random seeds. The best results are highlighted in bold, while the second-best results are underlined. PatchTST and ModernTCN demonstrate the best performance among methods without explicitly modeling spatial correlations. Besides, graph-based spatio-temporal methods yield better results than those that do not account for spatial structures. In addition, Card, Crossformer, iTransformer, and ESG outperform static graph-based methods such as MTGNN and MegaCRN, highlighting the significance of explicitly modeling dynamic spatial structures. Our method Sumba achieves state-of-the-art performance in most cases, demonstrating an improvement up to $8.5\\%$ over the best baseline on the Weather dataset. This superior performance is attributed to its capability to produce dynamic graph structures with low variance and high expressiveness using the proposed structured matrix basis. The forecasting results for additional prediction horizons are provided in Appendix C. ", "page_idx": 7}, {"type": "table", "img_path": "co7DsOwcop/tmp/82b39900271e71afa948ebc33de8dabc9299755a4eb0cb469c7a37fa8691455c.jpg", "table_caption": ["Table 3: Ablation study. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "co7DsOwcop/tmp/1882d249aa9cd21a921509261ea7c6ebb264abe62ad234a2016e8af87346c820.jpg", "img_caption": ["Figure 2: The change of $_{\\alpha}$ over one week. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct the ablation study on the Electricity, Weather, and PEMS datasets with prediction horizons $F$ of 3 and 6 to verify the efficacy of the proposed modules. In particular, we consider the following variants of our proposed model: (1) w/o. dynamic: we replace the Dynamic GCN module with the vanilla GCN module in [38]. (2) w/. $\\mathbf{U}_{m},\\mathbf{V}_{m}$ : we use distinct ${\\mathbf{U}}_{m}$ , $\\mathbf{V}_{m}$ for each basis matrix ${\\bf{B}}_{m}$ . (3) w/o. orthogonality: we remove the orthogonal parameterization on $\\mathbf{U}$ and $\\mathbf{V}$ . As shown in Table 3, the performance of w/o. dynamic declines rapidly by up to $13.83\\%$ , $8.59\\%$ , and $16.31\\%$ on three metrics, which verifies the efficacy of the proposed dynamic GCN model. The absence of structure regularization (w/. $\\mathbf{U}_{m},\\mathbf{V}_{m})$ and the elimination of orthogonal parameterization (w/o. orthogonality) on structured matrix basis lead to averaged performance drops of $4.95\\%$ and $3.05\\%$ , respectively. This proves the effectiveness of the proposed structure regularization and orthogonal constraint strategy. ", "page_idx": 8}, {"type": "text", "text": "4.4 Interpretable Dynamics ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As discussed in Section 3.1, one appealing feature of our proposed Sumba is that it enables us to gain insights into the underlying time series dynamics by tracking the change of the matrix basis coefficient $_{\\alpha}$ over time. To verify this, we present the heatmap of the change of $_{\\alpha}$ on two datasets\u2014 Solar-Energy and Electricity in Figure 2, in which the $\\mathbf{X}_{\\mathrm{~}}$ -axis denotes the time (hourly), the y-axis represents the no. of 5 basis matrices, and the color indicates the weight of $_{\\alpha}$ . It can be observed that most of the weights are concentrated on two basis matrices, no. 1 and no. 4, which implies that there are two dominant spatial structures on the two datasets. Furthermore, the two dominant spatial structures appear alternatively and regularly, which actually corresponds to the day and night for the Solar-Energy dataset. This aligns well with our intuition that solar energy observations should manifest different spatial correlations as the light intensity varies. For the Electricity dataset, there is a spatial correlation (no. 4) that spans two hours and only emerges in midnight, this reveals an interesting electricity consumption pattern, which is unnoticed by previous methods. Therefore, our proposed method is able to offer more interpretable dynamics of the underlying systems through tracking $_{\\alpha}$ . ", "page_idx": 8}, {"type": "image", "img_path": "co7DsOwcop/tmp/b15d0e04d5ced568e8d379ec644754e8b0acef621e0f89aa7241a38dd72fcfb7.jpg", "img_caption": ["Figure 3: The sensitivity of the dimension of basis $M$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.5 Parameter Sensitivity Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We evaluate the impact of $M$ on performance of our model, where the $M$ ranges from 1, 2, 5, to 10. $M=1$ corresponds to the static graph. As shown in Figure 3, the performance of our model improves significantly when $M>1$ . When $M=2$ , our method achieves the best performance on Electricity and Solar-Energy datasets, this coincides with the fact that the two datasets have two dominant spatial structure patterns, i.e., day and night. The experiments empirically show that the best setting of $M$ is 5 on ETTh2, Traffic, and PEMS datasets and 10 on the Weather dataset. We hypothesize this is due to the Weather dataset having more complex dynamics, and thus it requires more basis matrices to cover the various patterns. The sensitivity analysis of $L$ , $K$ and $H$ is presented in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a time series forecasting method with the structured matrix basis, Sumba, to capture dynamic spatial structures. To this end, we propose a novel structured parameterization and impose structure regularization on the basis to enhance parameter efficiency, the output space of the spatial structure function is thus well constrained and the generated spatial structures have lower variance. Our proposed method offers us a manner to gain insights into the dynamics of the underlying systems, and thus it is more interpretable. The experiments on six benchmark datasets verify the superiority of our proposed method. Our extensive ablation studies prove the effectiveness of each proposed component. In addition, the case study shows that our method can offer desirable interpretability. In the future, we would like to explore how to better regularize the learned matrix vector space and set the dimension of basis $M$ in a data-driven manner. We will also explore the possibility of integrating our proposed spatial structure modeling with other temporal encoders such as Transformers, and Structured State Space models, and apply our interpretable dynamics into more datasets to discover more interesting and hidden patterns. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China under Grant No. 62206074, Grant No. 62306085, Grant No. 62072137, Shenzhen College Stability Support Plan under Grant No. GXWD20220811173233001, and the National Key R&D Program of China under Grant No. 2023YFB4503100. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent network for traffic forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[2] Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[3] Mario Lezcano Casado and David Mart\u00ednez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. In International Conference on Machine Learning (ICML), 2019.   \n[4] Xiaodan Chen, Xiucheng Li, Bo Liu, and Zhijun Li. Biased temporal convolution graph network for time series forecasting with missing values. In International Conference on Learning Representations (ICLR), 2023.   \n[5] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.   \n[6] Jean Gallier and Jocelyn Quaintance. Differential Geometry and Lie Groups: A Computational Perspective. Springer, 2020.   \n[7] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations (ICLR), 2019. [8] Jake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi. Long-range transformers for dynamic spatiotemporal forecasting. arXiv preprint arXiv:2109.12218, 2021.   \n[9] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[10] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[11] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[12] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR), 2022.   \n[13] Haokun Gui, Xiucheng Li, and Xinyang Chen. Vector quantization pretraining for eeg time series with random projection and phase alignment. In International Conference on Machine Learning (ICML), 2024.   \n[14] Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, and Gao Cong. Learning dynamics and heterogeneity of spatial-temporal graph data for traffic forecasting. IEEE Trans. Knowl. Data Eng., 2022.   \n[15] Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Yasumasa Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura. Spatio-temporal meta-graph learning for traffic forecasting. In Association for the Advancement of Artificial Intelligence (AAAI), 2023.   \n[16] Yue Jiang, Xiucheng Li, Yile Chen, Shuai Liu, Weilong Kong, Antonis F. Lentzakis, and Gao Cong. A scalable adaptive graph diffusion forecasting network for multivariate time series forecasting. In 40th IEEE International Conference on Data Engineering (ICDE), 2024.   \n[17] Gilmer Justin, S. Schoenholz Samuel, F. Riley Patrick, Vinyals Oriol, and E. Dahl George. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), 2017.   \n[18] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2016.   \n[19] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations (ICLR), 2020.   \n[20] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In International ACM SIGIR conference on research & development in information retrieval, 2018.   \n[21] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in neural information processing systems (NeurIPS), 2019.   \n[22] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In International Conference on Learning Representations (ICLR), 2018.   \n[23] Minhao LIU, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia LAI, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[24] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations (ICLR), 2021.   \n[25] Shuai Liu, Xiucheng Li, Gao Cong, Yile Chen, and Yue Jiang. Multivariate time-series imputation with disentangled temporal representations. In International Conference on Learning Representations (ICLR), 2023.   \n[26] Yijing Liu, Qinxian Liu, Jian-Wei Zhang, Haozhe Feng, Zhongwei Wang, Zihan Zhou, and Wei Chen. Multivariate time-series forecasting with temporal polynomial graph neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[27] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[28] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In International Conference on Learning Representations (ICLR), 2024.   \n[29] Donghao Luo and Xue Wang. Moderntcn: A modern pure convolution structure for general time series analysis. In International Conference on Learning Representations (ICLR), 2024.   \n[30] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations (ICLR), 2023.   \n[31] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 2020.   \n[32] Martin Sundermeyer, Ralf Schl\u00fcter, and Hermann Ney. Lstm neural networks for language modeling. In Interspeech, 2012.   \n[33] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn: Multi-scale local and global context modeling for long-term series forecasting. In International Conference on Learning Representations (ICLR), 2023.   \n[34] Xue Wang, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, and Rong Jin. Card: Channel aligned robust blend transformer for time series forecasting. In International Conference on Learning Representations (ICLR), 2024.   \n[35] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[36] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations (ICLR), 2023.   \n[37] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence (IJCAI), 2019.   \n[38] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting the dots: Multivariate time series forecasting with graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2020.   \n[39] Junchen Ye, Zihan Liu, Bowen Du, Leilei Sun, Weimiao Li, Yanjie Fu, and Hui Xiong. Learning the evolutionary and multi-scale graph structure for multivariate time series forecasting. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2022.   \n[40] Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Longbing Cao, and Zhendong Niu. Fouriergnn: Rethinking multivariate time series forecasting from a pure graph perspective. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[41] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.   \n[42] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In International Conference on Learning Representations (ICLR), 2023.   \n[43] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. Gman: A graph multiattention network for traffic prediction. In Association for the Advancement of Artificial Intelligence (AAAI), 2020.   \n[44] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Association for the Advancement of Artificial Intelligence (AAAI), 2021.   \n[45] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning (ICML), 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Background and Proof of the Theorem ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Geometry Interpretation of Matrix Multiplication ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that given a matrix $\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , the multiplication Ax can be interpreted as 1) transforming $\\mathbf{x}$ into the new coordinate system $\\mathbf{x}^{\\prime}\\triangleq\\mathbf{V}^{\\top}\\mathbf{x},2)$ ) doing the elementwise product in the new coordinate $\\mathbf{y}^{\\prime}=\\Sigma\\mathbf{x}^{\\prime}$ , and 3) transforming the coordinate system again $\\mathbf{y}=\\mathbf{U}\\mathbf{y}^{\\prime}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{A}\\mathbf{x}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}\\mathbf{x}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{x}^{\\prime}=\\mathbf{U}\\mathbf{y}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of the Theorem ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem A.1. The output space of $f_{\\mathrm{spatial}}\\;i n\\,E q.$ 4 is bounded by the sum of the maximum of $\\Sigma_{m}(t h e$ maximum singular value of ${\\bf{B}}_{m}$ ) for $m=1,2,\\ldots,M$ in terms of the $\\ell_{2}$ norm i.e., $\\|f_{\\mathrm{spatial}}\\|_{2}\\le$ mM=1 max(\u03a3m). ", "page_idx": 13}, {"type": "text", "text": "Proof. ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\left||\\mathop{J_{\\ p a t a k}}|\\right|_{2}=\\left|\\displaystyle\\sum_{m=1}^{M}\\alpha_{m}\\ U_{\\Sigma m}\\mathbf{v}^{\\top}\\right|_{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{m=1}^{M}\\alpha_{m}\\left||\\mathbf{U}\\Sigma_{m}\\mathbf{u}^{\\top}|\\right|_{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{m=1}^{M}\\|\\mathbf{U}\\Sigma_{m}\\mathbf{v}^{\\top}\\|_{2}\\quad\\left(0<\\alpha_{m}<1\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{m=1}^{M}\\|\\mathbf{U}||_{2}\\|\\Sigma_{m}\\|_{2}\\|\\mathbf{V}^{\\top}\\|_{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{m=1}^{M}\\|\\mathbf{U}||_{2}\\|\\Sigma_{m}\\|_{2}\\|\\mathbf{V}^{\\top}\\|_{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{m=1}^{M}\\|\\Sigma_{m}\\|_{2}\\,\\qquad\\qquad(\\mathbf{U},\\mathbf{v}\\,\\mathbf{u}_{m}\\,\\mathrm{ording}\\,\\mathrm{onal}\\,\\mathrm{matics})}\\\\ &{\\qquad=\\displaystyle\\sum_{m=1}^{M}\\mathbf{u}_{m}\\mathbf{\\Sigma}(\\Sigma_{m}).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proof is completed. ", "page_idx": 13}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Baselines ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In our experiments, we compare our proposed Sumba with state-of-the-art (SOTA) baseline models. ", "page_idx": 13}, {"type": "text", "text": "\u2022 MICN: It utilizes temporal convolution networks and isometric convolution networks to capture local and global temporal correlations, respectively.   \n\u2022 ModernTCN: It leverages large kernels to build long-term temporal dependencies.   \n\u2022 PatchTST: It introduces a patch-based attention mechanism and channel-independent strategy to establish long-term temporal correlations.   \n\u2022 FEDformer: It leverages frequency-enhanced attention to capture long-term temporal dependencies and employs frequency sampling to reduce the complexity.   \n\u2022 Autoformer: It designs auto-correlation module to discover the sub-series similarity based on periodicity and aggregates similar sub-series from underlying periods.   \n\u2022 Reformer: It utilizes locality sensitive hashing strategy to reduce the complexity of the attention mechanism.   \n\u2022 S4: It captures long-term temporal dependencies via structured transition matrices.   \n\u2022 MTGNN: It builds an adaptive directed graph using learnable node embedding and aggregates information along spatial dimensions through mix-hop propagation.   \n\u2022 MegaCRN: It exploits memory-enhanced node embedding to build the graph structure.   \n\u2022 iTransformer: It embeds the whole time series into a spatial token and captures spatial correlations using the self-attention mechanism.   \n\u2022 Crossformer: It constructs both temporal and spatial dependencies using the attention mechanism and introduces a router mechanism to decrease the complexity of the spatial attention module.   \n\u2022 Card: It introduces summarized spatial tokens to decrease the complexity of the spatial attention module and employs a token blend mechanism in the temporal attention module to extract local temporal correlations.   \n\u2022 ESG: It designs multi-scale evolving node embedding based on Gated Recurrent Unit to capture dynamic spatial dependencies.   \n\u2022 FourierGNN: It builds dynamic graphs by merging spatial and temporal dimensions and designs the Fourier Graph Operator to decrease complexity.   \n\u2022 TPGNN: It regards the dynamic spatial correlations as the matrix polynomial graph where the time-varying coefficients are determined by timestamps. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B.2 Metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The evaluation metrics including MAE, RMSE, and MAPE are as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{MAE}=\\frac{\\sum_{i j\\in\\Omega}\\left|y_{i j}-\\hat{y}_{i j}\\right|}{\\left|\\Omega\\right|}\\quad\\mathrm{RMSE}=\\sqrt{\\frac{\\sum_{i j\\in\\Omega}(y_{i j}-\\hat{y}_{i j})^{2}}{\\left|\\Omega\\right|}}\\quad\\mathrm{MAPE}=\\sum_{i j\\in\\Omega}\\frac{\\left|y_{i j}-\\hat{y}_{i j}\\right|}{\\left|\\Omega\\right|\\cdot\\left|y_{i j}\\right|}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Omega$ denotes the index set along temporal and spatial dimensions. ", "page_idx": 14}, {"type": "text", "text": "C More Forecasting Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 4 provides the results with prediction lengths of 12 and 24. The results underscore the significant role of explicitly capturing spatial correlations in improving forecasting accuracy. Furthermore, dynamic graphs exhibit a greater ability to enhance the forecasting performance than modeling static spatial correlations. Among the baseline methods, Card, Crossformer, and ESG demonstrate superior forecasting accuracy. Notably, our model achieves state-of-the-art performance, yielding improvements more than $20\\%$ in terms of MAE and $25\\%$ in terms of MAPE over the existing methods on the Solar-Energy dataset. In addition, we conduct experiments with a forecasting horizon of 96, and as Table 5 shows, Sumba gives rise to favorable performance in comparison to PatchTST, FEDformer, and iTransformer which are designed specifically for long-term forecasting. ", "page_idx": 14}, {"type": "text", "text": "D Hyperparameters Sensitivity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We evaluate the sensitivity of hyperparameters including the number of blocks $L$ , the rank $K$ , and the history window size $H$ . Number of Blocks: As shown in Figure 4, Sumba achieves its best performance when $L=3$ . Further increasing $L$ does not lead to performance improvements. We hypothesize that this is due to the over-smoothing issue associated with Graph Neural Networks. Rank: Figure 5 shows how performance varies as the rank $K$ changes from 20 to 50. The optimal setting for $K$ is 30 for the Electricity, PEMS, and Solar-Energy datasets, while $K=50$ yields the best results for the Traffic dataset. We propose that this is attributed to the larger number of nodes in the Traffic dataset, which requires a higher rank to effectively represent the dynamic spatial correlations. History window: Figure 6 demonstrates that increasing the history window size enhances model performance, especially for the Electricity dataset. ", "page_idx": 14}, {"type": "table", "img_path": "co7DsOwcop/tmp/b047ef4d37b3a095d80d44339be37ad8d0c7e2d7100caa8f09cf4fd628a3682f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "co7DsOwcop/tmp/fdf50fd7132fc89613599663dc6cca4d2e6c0efc9d0b92e09055315b966fa7b4.jpg", "table_caption": ["Table 5: The results with forecasting horizon 96. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "co7DsOwcop/tmp/e818fa424a32e13120cb7e242e1cabf6e52fab5c023a78caf83de0a4114e2a7d.jpg", "img_caption": ["Figure 4: The sensitivity of our method to the number of blocks $L$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "co7DsOwcop/tmp/f71b1aabbb7d38b4cdf266695fbd01a76c02d398d2fb60377d5c03b4e2665e33.jpg", "img_caption": ["Figure 5: The sensitivity of our method to rank $K$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "co7DsOwcop/tmp/9691b0a20b53f965f079fd3e2a8261bffc2cdcd64192bc0f9e2c209e3b10a1bd.jpg", "img_caption": ["Figure 6: The sensitivity of our method to the history window $H$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "co7DsOwcop/tmp/ccc39a6e963de635b61550a61aae2ad2b7b24a09d0f3b840f3f3bf77ac9d1452.jpg", "img_caption": ["Figure 7: The training curves on Electricity and Weather datasets with prediction length $F=3$ . "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "co7DsOwcop/tmp/45ee239301b88834ab3c3a45d6d1cb376b55126e59d3ea4eadaa2225c52dd19f.jpg", "table_caption": ["Table 6: The complexity of different methods. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Convergence Speed and Computational Complexity ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Convergence Speed Figure 7 illustrates the training curves for the first 10 epochs on the Electricity and Weather datasets. Sumba, ModernTCN, and ESG exhibit fast convergence, achieving it within 5 epochs. In contrast, methods such as Card, FourierGNN, and TPGNN take several tens of epochs to reach convergence. ", "page_idx": 17}, {"type": "text", "text": "Computational Complexity As shown in Table 6, we analyze the computational complexity of different methods in capturing spatial dependencies, where $N$ is the number of nodes, $M$ is the dimension of the matrix basis, $D$ is the dimension of node embedding or hidden representation, $r\\ll N$ and $K\\ll N$ . MTGNN, MegaCRN, iTransformer, ESG, and TPGNN build the graph structure via inner product of static or dynamic node embedding, leading to a computation complexity of $O(N^{2}D)^{2}$ . Crossformer and Card employ router mechanism and summarized token, respectively, in spatial correlation modeling, reducing the complexity to $\\mathcal{O}(N r d)$ . FourierGNN has complexity of $\\mathcal{O}(N H\\mathrm{log}(N H))$ due to building a hypervariable graph. Our proposed Sumba leverages low-rank approximation and common coordinate transformations strategy, achieving the complexity of $\\mathcal{O}(K N D)$ whereas the computational cost for the model without a common U-V basis is $\\mathcal{O}(M K N D)$ . Thus, our model has low complexity, especially when $N$ is large. ", "page_idx": 17}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Although our proposed Sumba demonstrates superior performance in multivariate time series forecasting, the dimension of matrix basis $M$ is empirically tuned in our experiments. Besides, the long-term forecasting capability of the proposed model requires further enhancement. ", "page_idx": 17}, {"type": "text", "text": "G Broader Impact ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this paper, we propose a novel time series forecasting method to capture dynamic spatial correlations with a structured matrix basis. Our research aims to contribute to the advancement of the relevant community without any negative social impact. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix F. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix A.2. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Section 3 and Section 4. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Our code is available at: https://anonymous.4open.science/r/Sumba/. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Section 4 and Appendix B Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We repeat each experiment 3 times and report the mean results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Section 4. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We make sure to preserve anonymity. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Appendix G. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 21}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited, and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We cite the original paper that produced the code package or dataset. See Section 2 and Section 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]