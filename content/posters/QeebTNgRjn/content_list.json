[{"type": "text", "text": "Conditional Lagrangian Wasserstein Flow for Time Series Imputation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Time series imputation is important for numerous real-world applications. To   \n2 overcome the limitations of diffusion model-based imputation methods, e.g., slow   \n3 convergence in inference, we propose a novel method for time series imputation in   \n4 this work, called Conditional Lagrangian Wasserstein Flow. The proposed method   \n5 leverages the (conditional) optimal transport theory to learn the probability flow   \n6 in a simulation-free manner, in which the initial noise, missing data, and obser  \n7 vations are treated as the source distribution, target distribution, and conditional   \n8 information, respectively. According to the principle of least action in Lagrangian   \n9 mechanics, we learn the velocity by minimizing the corresponding kinetic energy.   \n10 Moreover, to incorporate more prior information into the model, we parameterize   \n11 the derivative of a task-specific potential function via a variational autoencoder,   \n12 and combine it with the base estimator to formulate a Rao-Blackwellized sampler.   \n13 The propose model allows us to take less intermediate steps to produce high-quality   \n14 samples for inference compared to existing diffusion methods. Finally, the experi  \n15 mental results on the real-word datasets show that the proposed method achieves   \n16 competitive performance on time series imputation compared to the state-of-the-art   \n17 methods. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 9 Time series imputation is essential for various practical scenarios in many fields, such as transportation,   \n20 environment, and medical care, etc. Deep learning-based approaches, such as RNNs, VAEs, and   \n21 GANs, have been proved to be advantageous compared to traditional machine learning methods on   \n22 various complex real-words multivariate time series analysis tasks [18]. More recently, diffusion   \n23 models, such as denoising diffusion probabilistic models (DDPMs) [20] and score-based generative   \n24 models (SBGMs) [43], have gained more and more attention in the field of time series analysis due to   \n25 their powerful modelling capability [26, 32].   \n26 Although many diffusion model-based time series imputation approaches have been proposed and   \n27 show their advantages compared to conventional deep learning models [44, 11, 12], they are limited   \n28 to slow convergence or large computational costs. Such limitations may prevent them being applied to   \n29 real-world applications. To address the aforementioned issues, in this work, we leverage the optimal   \n30 transport theory [47] and Lagrangian mechanics [3] to propose a novel method, called Conditional   \n31 Lagrangian Wasserstein Flow (CLWF), for fast and accurate time series imputation.   \n32 In our method, we treat the multivariate time series imputation task as a conditional optimal transport   \n33 problem, whereby the random noise is the source distribution, the missing data is the target distribution,   \n34 and the observed data is the conditional information. To generate new data samples efficiently and   \n35 accurately, we need to find the shortest path in the probability space according to the optimal transport   \n36 theory. To this end, we first project the original source and target distributions into the Wasserstein   \n37 space via sampling mini-batch OT maps. Afterwards, we construct the time-dependent intermediate   \n38 samples through interpolating the source distribution and target distribution. Then according to   \n39 the principle of least action in Lagrangian mechanics [3], the optimal velocity function moving the   \n40 source distribution to the target distribution is learned in a self-supervised manner by minimizing   \n41 the corresponding kinetic energy. Moreover, to further improve the model\u2019s performance, we learn   \n42 the task-specific potential function by training a Variational Autoencoder (VAE) model [22] on the   \n43 observed time series data to build a Rao-Blackwellized trajectory sampler.   \n44 Finally, CLWF is assessed on two real-word multivariate time series datasets. The obtained results   \n45 show that the proposed method achieves competitive performance and admits fast convergence   \n46 compared with other state-of-the-art time series imputation methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "47 The contributions of the paper ares summarized as follows: ", "page_idx": 1}, {"type": "table", "img_path": "QeebTNgRjn/tmp/5fbc2c73fd23ddef4fa57c87a0d80e53d5a12f4c85c8440b8b3520700634015c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "56 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "57 In this section, we concisely introduce the fundamentals of stochastic differential equations, optimal   \n58 transport, Shr\u00f6dinger Bridge, and Lagrangian mechanics. ", "page_idx": 1}, {"type": "text", "text": "59 2.1 Stochastic Differential Equations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "60 We treat the data generation task as an initial value problem (IVP), in which $X_{0}\\in\\mathbb{R}^{d}$ is the initial   \n61 data (e.g., some random noise) at the initial time $t=0$ , and $\\dot{X}_{T}\\in\\mathbb{R}^{d}$ is target data at the terminal   \n62 time $t=T$ . To solve the IVP, we consider a stochastic differential equation (SDE) defined by a Borel   \n63 measurable time-dependent drift function $\\mu_{t}:[0,T]\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ , and a positive Borel measurable   \n64 time-dependent diffusion function $\\sigma_{t}:[0,T]\\rightarrow\\mathbb{R}_{>0}^{d}$ . Accordingly, the It\u00f4 form of the SDE can be   \n65 described as follows [36]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=\\mu_{t}(X_{t},t)\\mathrm{d}t+\\sigma_{t}\\mathrm{d}W_{t},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "66 where $W_{t}$ is a Brownian motion/Wiener process. When the diffusion term is not considered, the   \n67 SDE degenerates to an ordinary differential equation (ODE). However, we will use the SDE for the   \n68 theoretical analysis as it is more general.   \n69 The Fokker\u2013Planck equation (FPE) [40] describing the evolution of the marginal density $p_{t}(X_{t})$   \n70 reads: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}p_{t}(X_{t})=-\\pmb{\\nabla}\\cdot(p_{t}\\mu_{t})+\\frac{\\sigma_{t}^{2}}{2}\\Delta p_{t},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "71 where $\\Delta p_{t}=\\nabla\\cdot(\\nabla p_{t})$ is the Laplacian. In fact, both Eq. eq:sde and Eq. eq:fpe reveal the dynamics   \n72 of the system and serve as the boundary conditions for the optimization problems we will introduce in   \n73 later sections with different focuses. The differences are when the constraint is Eq. (1), the formalism   \n74 is Lagrangian, which depicts the movement of each individual particle; while when the constraint is   \n75 Eq.(2), the formalism is Eulerian, which depicts the evolution of population. ", "page_idx": 1}, {"type": "text", "text": "76 2.2 Optimal Transport ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "77 The optimal transport (OT) problem aims to seek the optimal transport plans/ maps that moves the   \n78 source distribution to the target distribution [47, 41, 38]. In the Kantorovich\u2019s formulation of the   \n79 OT problem, the transport costs are minimized with respect to some probabilistic couplings/joint   \n80 distributions [47, 41, 38]. Let $p_{0}$ and $p_{T}$ be two Borel probability measures with finite second   \n81 moments on the space $\\Omega\\,\\in\\,\\mathbb{R}^{d}$ . $\\Pi(p_{0},p_{T})$ denotes a set of transport plans between these two   \n82 marginals. Then, the Kantorovich\u2019s OT problem is defined as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\pi\\in\\Pi(p_{0},p_{T})}\\int_{\\chi\\times y}\\frac{1}{2}\\big|\\big|x-y\\big|\\big|^{2}\\pi(x,y)\\mathrm{d}x\\mathrm{d}y,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "83 where $\\Pi(p_{0},p_{T})=\\{\\pi\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y}):(\\pi^{x})_{\\#}\\pi=p_{0},(\\pi^{y})_{\\#}\\pi=p_{T}\\}.$ , with $\\pi^{x}$ and $\\pi^{x}$ being two   \n84 projections of $\\mathcal X\\times\\mathcal X$ on $\\Omega$ . The minimizer of Eq .(3), $\\pi*$ , always exist and referred to as the optimal   \n85 transport plan.   \n86 Note that the R.H.S of Eq. (3) can also include an entropy regularization term $D_{\\mathrm{KL}}(\\pi\\Vert p_{0}\\otimes p_{T})$ , then   \n87 the original OT problem transforms into the entropy-regularized optimal transport (EROT) problem   \n88 with Eq. (2) as the constraint, which frames the transport problem better in terms of convexity and   \n89 stability [13] In particular, from a data generation perspective, $p_{0}$ is some random initial noise and $p_{T}$   \n90 is the target data distribution, and we can sample the optimal transport maps in a mini-batch manner   \n91 [46, 45, 39]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "92 2.3 Shr\u00f6dinger Bridge ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 The transport problem in Sec. 2.2 can be further viewed from a distribution evolution perspec  \n94 tive, which is particularly suitable for developing the flow models that model the data generation   \n95 process. For this reason, the Shr\u00f6dinger Bridge (SB) problem is introduced [25]. Assume that   \n96 $\\bar{\\Omega}\\in C^{1}([0,T],\\mathbb{R}^{d})$ , ${\\mathcal{P}}(\\Omega)$ is a probability path measure on the path space $\\Omega$ , then the goal of the SB   \n97 problem aims to find the following optimal path measure: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\ast=\\underset{\\mathbb{P}\\in\\mathcal{P}(\\Omega)}{\\arg\\operatorname*{min}}D_{\\mathrm{KL}}(\\mathbb{P}||\\mathbb{Q})\\quad\\mathrm{subject~to~}\\mathbb{P}_{0}=q_{0}\\mathrm{~and~}\\mathbb{P}_{T}=q_{T},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "98 where the Kullback\u2013Leibler (KL) divergence $D_{\\mathrm{KL}}({\\mathbb{P}}\\|\\mathbb{Q})\\,=\\,{\\binom{\\mathrm{log~}{\\frac{\\mathrm{d}{\\mathbb{P}}}{\\mathrm{d}{\\mathbb{Q}}}}\\mathrm{d}{\\mathbb{P}}}{+\\infty,}}$ , if $\\mathbb{P}\\ll\\mathbb{Q}$ , and Q is otherwise,   \n99 a reference path measure, e.g., Brownian motion or Ornstein-Uhlenbeck process. Moreover, the   \n100 distribution matching problem in Eq. (3) can be cast as a dynamical SB problem as well [19, 24, 28]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{p(X_{t})}\\Big[\\frac{1}{2}\\big|\\big|\\mu_{t}^{\\theta}(X_{t},t)\\big|\\big|^{2}\\Big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "101 where $\\theta$ is the parameters of the variational drift function $\\mu_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "102 2.4 Lagrangian Mechanics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "103 In this section, we formulate the data generation problem under the framework of Lagrangian   \n104 mechanics [3]. Let $p_{t}$ and $\\begin{array}{r}{\\dot{p_{t}}=\\frac{\\mathrm{d}p_{t}}{\\mathrm{d}t}}\\end{array}$ be the density and law of the generalized coordinates $X_{t}$ , respec  \n105 tively. $\\boldsymbol{\\mathcal{K}}(p_{t},\\dot{p_{t}},t)$ is the kinetic energy, and $\\mathcal{U}(p_{t},t)$ is the potential energy, then the corresponding   \n106 Lagrangian is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(p_{t},\\dot{p}_{t},t)=\\mathcal{K}(p_{t},\\dot{p}_{t},t)-\\mathcal{U}(p_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "107 We assume that Eq. (6) is lower semi-continuous (lsc) and strictly convex in $\\dot{p_{t}}$ in the Wasser  \n108 stein space. The kinetic energy $\\boldsymbol{\\kappa}(\\boldsymbol{x}_{t},\\mu_{t},t)$ and potential energy $\\mathcal{U}(p_{t},t)$ are defined as follows,   \n109 respectively: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{K}(x_{t},\\mu_{t},t)=\\mathbb{E}_{p(X_{t})}\\Bigg[\\int_{0}^{T}\\int_{\\mathbb{R}_{d}}\\frac{1}{2}\\|\\mu_{t}(x_{t},t)\\|^{2}\\mathrm{d}x\\mathrm{d}t,}\\\\ &{\\quad\\mathcal{U}(p_{t},t)=\\mathbb{E}_{p(X_{t})}\\Bigg[\\int_{\\mathbb{R}_{d}}U_{t}(X_{t})\\Bigg]d X_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "110 where $U_{t}(X_{t})$ is the potential function. Then the action in the context of Lagrangian mechanics is   \n111 defined as follow: ", "page_idx": 2}, {"type": "equation", "text": "$$\nA[\\mu_{t}(x)]=\\int_{0}^{T}\\int_{\\mathbb{R}_{d}}\\mathcal{L}(x_{t},\\mu_{t},t)d x_{t}d t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "112 According to the principle of least action, the shortest path is the one minimizing the action, which is   \n113 aligned with Eq. (4) in the SB theory as well. Therefore, we can leverage the Lagrangian dynamics   \n114 to tackle the OT problem for data generation. To solve Eq. (6), we need to satisfy the stationary   \n115 condition, i.e., the Euler-Lagrangian equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\frac{\\partial}{\\partial\\dot{p}_{t}}\\mathcal{L}(x_{t},\\mu_{t},t)=\\frac{\\partial}{\\partial p_{t}}\\mathcal{L}(p_{t},\\dot{p_{t}},t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "116 with the boundary condition $\\begin{array}{r}{\\frac{\\mathrm{d}X_{t}}{\\mathrm{d}t}=\\mu(X_{t},t),\\;q_{0}=p_{0},\\;q_{T}=p_{T}.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "117 3 Conditional Lagrangian Wasserstein Flow for Time Series Imputation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "118 In the section, building upon the optimal transport theory, the Shr\u00f6dinger Bridge problem, and   \n119 Lagrangian mechanics introduced in Sec. 2, we propose Conditional Lagrangian Wasserstein Flow,   \n120 which is a novel conditional generative method for time series imputation. ", "page_idx": 3}, {"type": "text", "text": "121 3.1 Time Series Imputation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "122 Our goal is to impute the missing time series data points based on the observations. For training,   \n123 we adopt adopt a conditionally generative approach for time series imputation in the sample space   \n124 $\\mathbb{R}^{K\\times L}$ , where $K$ represents the dimension of the multivariate time series and $L$ represents sequence   \n125 length. In our self-supervised learning approach, the total observed data $\\boldsymbol{x}^{\\mathrm{obs}}\\in\\mathbb{R}^{K^{\\star}L}$ are partitioned   \n126 into the imputation target xtar \u2208RK\u00d7L and the conditional data $x^{\\mathrm{cond}}\\in\\mathbb{R}^{K\\times L}$ .   \n127 As a result, the missing data points $x^{\\mathrm{tar}}$ can be generated based on the conditions $x^{\\mathrm{cond}}$ joint with   \n128 some uninformative initial distribution $\\boldsymbol{x}_{0}\\in\\mathbb{R}^{\\breve{K}\\times L}$ (e.g., Gaussian noise) at time $t=0$ , then the   \n129 imputation task can be described as: $x^{\\mathrm{tar}}\\sim p(x^{\\mathrm{tar}}|x_{0}^{\\mathrm{cond}})$ , where the total input of the model is   \n130 $x_{0}^{\\mathrm{input}}:=\\left(x^{\\mathrm{cond}},x_{0}\\right)\\in\\mathbb{R}^{K\\times L\\times2}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "131 3.2 Interpolation in Wasserstein Space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "132 To solve Eq. (7), we need to sample the intermediate variable $X_{t}$ in the Wasserstein space first. To do   \n133 so, the interpolation method is adopted to construct the intermediate samples. According to the OT   \n134 and SB problems introduced in Sec. 2, we define the following time-differentiable interpolant: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{t}:\\Omega\\times\\Omega\\to\\Omega\\quad\\mathrm{such\\,that}\\,I_{0}=X_{0}\\mathrm{~and~}I_{T}=X_{T},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 where $\\Omega\\,\\in\\,\\mathbb{R}^{d}$ is the support of the marginals $p_{0}(X_{0})$ and $p_{T}(X_{T})$ , as well as the conditional   \n136 $p(X_{t}|X_{0},X_{T},t)$ .   \n137 For implement $I_{t}$ , first, we independently sample some random noise $X_{0}\\sim\\mathcal N(0,\\sigma_{0}^{2})$ at the initial   \n138 time $t=0$ and the data samples $X_{T}\\sim p(x^{\\mathrm{tar}})$ at the terminal time $t=T$ , respectively. Afterwards,   \n139 the interpolation method is used to construct the intermediate samples $X_{t}\\sim p(\\bar{X}_{t}|X_{0},\\bar{X}_{T},t)$ , where   \n140 $t\\sim$ uniform $(0,T)$ [30, 2, 45]. More specifically, we design the following sampling approach: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t}=\\frac{t}{T}(X_{T}+\\gamma_{t})+(1-\\frac{t}{T})X_{0}+\\alpha(t)\\sqrt{\\frac{t(T-t)}{T}}\\epsilon,\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "141 where $\\gamma_{t}\\sim\\mathcal{N}(0,\\sigma_{\\gamma}^{2})$ is some random noise with variance $\\sigma_{\\gamma}$ injected to the target data samples for   \n142 improving the coupling\u2019s generalization property, and $\\alpha(t)\\geq0$ is a time-dependent scalar.   \n143 Note that Eq. (12) can only allow us to generate time-dependent intermediate samples in the Euclidean   \n144 space but not the Wasserstein space, which can lead to slow convergence as the sampling paths are   \n145 not straightened. Hence, to address this issue, we need to project the interpolations in the Wasserstein   \n146 space before interpolating to strengthen the probability flow. To this end, we leverage the method   \n147 adopted in [46, 45, 39] to sample the optimal mini-batch OT maps between $X_{0}$ and $X_{T}$ first, and   \n148 perform the interpolations according to Eq. (12) afterwards. Finally, we have the joint variable   \n49 $x_{t}^{\\mathrm{input}}:=\\left(x^{\\mathrm{cond}},x_{t}\\right)$ as the input for computing the velocity of the Wasserstein flow. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "150 3.3 Flow Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "151 To estimate the velocity of the Wasserstein flow $\\mu_{t}(X_{t},t)$ in Eq. (1), the previous methods that require   \n152 trajectory simulation for training can result in long convergence time and large computational costs   \n153 [9, 37]. To circumvent the above issues, in this work we adopt a simulation-free training strategy   \n154 based on the OT theory introduce in Sec. 2.2 [30, 46, 2], which turns out to be faster and more   \n155 scalable to large time series datasets.   \n156 Since we can now draw mini-batch interpolated samples of the source distribution and target distribu  \n157 tion in the Wasserstein space using Eq. (12), we can model the variational velocity function using a   \n158 neural network with parameters $\\theta$ . Then, according to Eq. (1), the target velocity can be computed   \n159 by the difference between the source distribution and target distribution. Therefore, the variational   \n160 velocity function (xitnput, t) can be learned by ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\arg\\operatorname*{min}_{\\theta}\\displaystyle\\int_{0}^{T}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\left\\|\\frac{\\mathrm{d}X_{t}}{\\mathrm{d}t}-\\mu_{t}^{\\theta}(x_{t}^{\\mathrm{input}},t)\\right\\|^{2}\\!\\mathrm{d}x_{0}\\mathrm{d}x^{\\mathrm{tar}}\\mathrm{d}x^{\\mathrm{input}}\\mathrm{d}t}\\\\ &{\\approx\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{p(x_{0}),p(x^{\\mathrm{tar}}),p(x^{\\mathrm{input}}),t}\\left[\\left\\|\\frac{x^{\\mathrm{tar}}-x_{0}}{T}-\\mu_{t}^{\\theta}(x_{t}^{\\mathrm{input}},t)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 Eq. (14) can be solved by drawing mini-batch samples in the Wasserstein space and performing   \n162 stochastic gradient descent accordingly. In this fashion, the learning process is simulation-free as the   \n163 trajectory simulation is not needed.   \n164 Moreover, note that that Eq. (13) also obeys the principle of least action introduced in Sec. 2.4 as   \n165 it minimizes the kinetic energy described in Eq. (7). Therefore, it indicates that the geodesic that   \n166 drives the particles from the source distribution to the target distribution in the OT problem described   \n167 in Sec. 2 is found as well, which enables us to generate new samples with less simulation steps   \n168 compared to standard diffusion models. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "169 3.4 Potential Function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 So far, we have demonstrated how to leverage the kinetic energy to estimate the velocity in the   \n171 Lagrangian described by Eq. 6. Apart from this, we can also incorporate the prior knowledge within   \n172 the task-specific potential energy into the dynamics, which enables us to further improve the data   \n173 generation performance. To this end, let $\\bar{U}(X_{t}):\\mathbb{R}^{d}\\times[0,T]\\rightarrow\\mathbb{R}$ be the task-specific potential   \n174 function depending on the generalized coordinates $X_{t}$ [48, 37, 34]. Therefore, we can compute the   \n175 dynamics of the system by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}X_{t}}{\\mathrm{d}t}=v_{t}(X_{t},t)=-\\nabla_{x}U_{t}(X_{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 Since the data generation problem in our case can also be interpreted as a stochastic optimal control   \n177 (SOC) problem [4, 17, 35, 50, 21, 5], then the existence of such $\\boldsymbol{U}_{t}(\\boldsymbol{X}_{t})$ is assured by Pontryagin\u2019s   \n178 Maximum Principle (PMP) [16].   \n179 To estimate $v_{t}(X_{t},t)$ , according to the Lagrangian Eq. (6), we assume that the potential function   \n180 takes the form $U_{t}(X_{t})\\approx-\\log\\mathcal{N}(X_{t}|\\hat{X}_{t},\\sigma_{p}^{2})$ , where $\\hat{X_{t}}$ the learned mean and $\\sigma_{p}^{2}$ is the pre-defined   \n181 variance. As a result, the derivative is \u2207xU(Xt) = Xt\u03c3\u22122X\u02c6t. In terms of practical implementation, we   \n182 parameterize $\\nabla_{x}U(X_{t})$ via a Variational Autoencoder (VAE) [22]. More specifically, we pre-train a   \n183 VAE on the total observed time series data $X^{\\mathrm{obs}}$ . Afterwards, the reconstruction discrepancies of the   \n184 VAE are used to approximate the task-specific $\\boldsymbol{v}^{\\phi}(\\boldsymbol{X}_{t},t)$ depending on $X_{t}$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{t}^{\\phi}(X_{t},t)=-\\frac{1}{\\sigma_{p}^{2}}(X_{t}-\\operatorname{VAE}(X_{t})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "185 where $\\operatorname{VAE}(X_{t})$ represents the reconstruction output of the pre-trained VAE model with input $X_{t}$ ,   \n186 and $\\sigma_{p}^{2}$ is treated as a positive constant for simplicity. In this manner, we can incorporate the prior   \n187 knowledge learned from the accessible training data into the sampling process formulated by Eq. (14)   \n188 to enhance the data generation performance. ", "page_idx": 4}, {"type": "text", "text": "189 3.5 Rao-Blackwellized Sampler ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "190 To generate the missing time series datapoints, we first formulate an unbiased ODE sampler   \n191 $S(\\check{X}_{t},\\mu_{t}^{\\theta}(X_{t},t),t)$ for $X_{t+1}$ with the Euler method and $\\mu_{t}^{\\theta}(X_{t},t)$ learned by Eq. (14) (which   \n192 means the diffusion term in Eq. 1 is omitted). Alternatively, one can also adopt the SDE sampler by   \n193 using the Euler\u2013Maruyama method. Nevertheless, to ensure achieve the best imputation performance,   \n194 we choose the ODE sampler for implementation. Note that the ODE sampler alone is good enough to   \n195 generate high-quality samples for time series imputation.   \n196 Now we can construct a Rao-Blackwellized trajectory sampler [8] for time series data imputation   \n197 using Eq. 14 and Eq. 16. To this end, we first treat $\\:S(X_{t+1}\\mathbf{\\:\\bar{|}}X_{t},\\mu_{t}^{\\bar{\\theta}}(X_{t},t),t)\\,$ be the base estimator   \n198 for $X_{t+1}$ with $\\mathbb{E}[S^{2}]<\\infty$ for all $X_{t+1}$ . And we assume $\\boldsymbol{\\mathcal{T}}(\\boldsymbol{X}_{t},\\boldsymbol{v}_{t}^{\\phi}(\\boldsymbol{x}_{t},t),t)$ is a sufficient statistic   \n199 for $X_{t+1}$ based on Eq. 16, even it is not a very accurate estimator for $X_{t+1}$ . As a result, we can   \n200 formulate a new trajectory sampler $S^{*}=\\mathbb{E}[\\dot{S}|\\mathcal{T}]$ to generate the missing time series data. Then   \n201 according to the Rao-Blackwell theorem [8], we have ", "page_idx": 4}, {"type": "image", "img_path": "QeebTNgRjn/tmp/b49d965a644680db5c5873819a28c77e757c0b4d71a0fb012099b09d051bf032.jpg", "img_caption": ["Figure 1: The overall training process of Conditional Lagrangian Wasserstein Flow. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[S^{*}-X_{t+1}]^{2}\\leq\\mathbb{E}[S-X_{t+1}]^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 where the inequality is strict unless $\\boldsymbol{S}$ is a function of $\\tau$ . Eq. 17 suggests we can construct a more   \n203 powerful sampler with smaller errors than the base ODE sampler $\\boldsymbol{S}$ using Rao-Blackwellization. ", "page_idx": 5}, {"type": "text", "text": "204 3.6 The Algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "205 The overall training process of CLWF is illustrated in Fig. 1, which consists of the following   \n206 stages. First, the total observed data $x^{\\mathrm{obs}}$ are partitioned into the target data and conditional data for   \n207 training. Next, the data pairs of $x^{\\mathrm{tar}}$ and $x_{0}$ are sampled from the target dataset and random Gaussian   \n208 noise, respectively. Then, the data pairs are projected into the Wasserstein space by sampling the   \n209 corresponding OT maps. After that, the intermediate variable $x_{t}$ is sampled through interpolation   \n210 using Eq. (12). We can approximate the target velocity ddXtt by computing x T\u2212x0. Subsequently,   \n211 we use the joint distribution of the conditional information $x^{\\mathrm{cond}}$ and the intermediate variable $x_{t}$ ,   \n212 $x^{\\mathrm{input}}$ as the total input to feed the variational flow model $\\mu_{t}^{\\theta}$ to compute the velocity. And the flow   \n213 matching loss defined by Eq. (14) is minimized by stochastic gradient descent.   \n214 Furthermore, to incorporate the prior information of into the model, we can choose to train a VAE   \n215 model on the total observed data $x^{\\mathrm{obs}}$ . This is used to estimate the derivative of the task-specific   \n216 potential function according to Eq. (16), which can be further utilized to construct a more powerful   \n217 Rao-Blackwellized sampler for inference.   \n218 For inference, at time $t=0$ , we sample the initial random noise $x_{0}$ and conditional information   \n219 $x^{\\mathrm{cond}}$ to formulate the joint variable $x^{\\mathrm{input}}$ . Note that during the trajectory sampling $x_{0}$ will evolve   \n220 over time, while $x^{\\mathrm{cond}}$ remain invariant. We use $x^{\\mathrm{input}}$ as the input of the flow model $\\mu_{t}^{\\theta}$ to compute   \n221 the velocity. Afterwards, we sample the new $x_{t}$ using the Euler method. If we perform Rao  \n222 Blackwellization, then $x_{t}$ is fed to the VAE model for computing the derivative of the potential   \n223 function, and $x_{t}$ is updated again using the Euler method. The above process will be repeated until   \n224 reach its convergence. Moreover, we can sample multiple trajectories using different initial random   \n225 noise, and the averages as the final imputation results. Finally, the proposed training and sampling   \n226 procedures are presented in Algorithm 1 and Algorithm 2, respectively. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "QeebTNgRjn/tmp/b72be9a093fa0c48172b4b3daee9f2f338239cc1b682e4ef05031d1d09e463a2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "228 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "229 4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "230 We use two public multivariate time series datasets for validation. The first dataset is the $\\mathbf{PM}\\,2.5$   \n231 dataset [51] from the air quality monitoring sites for 12 months. The missing rate of the raw data   \n232 is $13\\%$ . The feature number $K$ is 36 and the sequence length $L$ is 36. In our experiments, only the   \n233 observed datapoints are masked randomly as the imputation targets.   \n234 The other dataset we use is the PhysioNet dataset [42] collected from the intensive care unit for 48   \n235 hours. The feature number $K$ is 35 and the sequence length $L$ is 48. The missing rate of the raw data   \n236 is $80\\%$ . In our experiments, $10\\%$ and $50\\%$ of the datapoints are masked randomly as the imputation   \n237 targets, which are denoted as PhysioNet 0.1 and PhysioNet 0.5, respectively. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "238 4.2 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "239 For comparison, we select the following state-of-the-art timer series imputation methods as the   \n240 baselines: 1) GP-VAE [18], which is combines a VAE model and a Gaussian Process prior; 2)   \n241 CSDI [44], which is based on the conditional diffusion model; 3) CSBI [12], which is based on the   \n242 Schr\u00f6dinger bridge diffusion model; 4) DSPD-GP [7], which combines the diffusion model with the   \n243 Gaussian Process prior. ", "page_idx": 6}, {"type": "text", "text": "244 4.3 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "245 In terms of the choices of architectures, tboth the flow model and the VAE model are built upon   \n246 Transformers [44]. We use the ODE sampler for inference and sample the exact optimal transport   \n247 maps for interpolations to achieve the optimal performance. The optimizer is Adam and the learning   \n248 rate: 0.001 with linear scheduler. The maximum training epochs is 200. The mini batch size for   \n249 training is 64. The total step number of the Euler method used in CLWF is 15, while the total step   \n250 numbers for other diffusion models. i.e., is CSDI, CSBI, and DSPD-GP are 15 (as suggested in their   \n251 papers). The number of the Monte Carlo samples for inference is 50. The standard deviation $\\sigma_{0}$   \n252 for the initial noise $X_{0}$ is 0.1, and the standard deviation $\\sigma_{\\gamma}$ for the injected noise $\\gamma_{t}~0.001$ . The   \n253 coefficient $\\sigma_{p}^{2}$ in the derivative of the potential function is 0.01. ", "page_idx": 6}, {"type": "text", "text": "254 4.4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "255 4.4.1 Imputation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "256 We assess the proposed method on PM 2.5, PhysioNet 0.1 and PhysioNet 0.5, respectively. The root   \n257 means squared error (RMSE) and mean absolute error (MAE) are used as the evaluation metrics.   \n258 From the test results shown in Table 1 and Fig. 2, we can see that our method CLWF outperforms   \n259 the existing deep learning-based method (GP-VAE) and the recent state-of-the-art diffusion methods   \n260 (CSDI, CSBI, and DSPD-GP). Moreover, CLWF uses only 15 sampling steps for inference, while the ", "page_idx": 6}, {"type": "text", "text": "Table 1: Test imputation results on PM 2.5, PhysioNet 0.1, and PhysioNet 0.5 (5-trial averages). The best are in bold and the second best are underlined. ", "page_idx": 7}, {"type": "table", "img_path": "QeebTNgRjn/tmp/2f6c413cc1f25a8169a7a579c3a19b8589d7911ff00c0cc08bda039e82497c58.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "QeebTNgRjn/tmp/dc3f3e327ee3d5afeb1d3bdeb26b21ab93e69c9cdd045b9b06d5a06fa2050f4e.jpg", "img_caption": ["Figure 2: Visualization of the test imputation results on $\\mathrm{PM}\\,2.5$ , green dots are the conditions, blue dots are the imputation results, and red dots are the ground truth. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "QeebTNgRjn/tmp/717d8a369d80c9779e60a3247ae7e32a362c93f2b70ed0f767c453359d772f6e.jpg", "table_caption": ["Table 2: Single-sample test imputation results on PM 2.5, PhysioNet 0.1, and PhysioNet 0.5 (5-trial averages). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "261 baseline diffusion method uses only 50 sampling steps. This suggests that CLWF is faster and more   \n262 accurate than the existing methods on time series imputation tasks. ", "page_idx": 7}, {"type": "text", "text": "263 4.4.2 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "264 Single-sample Imputation Result. We compare the time series imputation performance of CLWF   \n265 with CSDI using only one Monte Carlo sample. The test results shown in Table 2 shows that CWFL   \n266 outperforms CSDI, which suggests that CWFL exhibits lower imputation variances compared to   \n267 diffusion-based models. This indicates that CWFL is more efficient and computationally economical   \n268 for inference. ", "page_idx": 7}, {"type": "text", "text": "Effect of Rao-Blackwellzation. We compare the test imputation CLWF wth and without using RaoBlackwellzation. Note that the PhysioNet dataset does not have enough non-zero data points to train a valid VAE model, therefore we only construct the Rao-Blackwellized sampler for the $\\mathrm{PM}\\,2.5$ dataset. The results showed in Table 3 indicates ", "page_idx": 7}, {"type": "table", "img_path": "QeebTNgRjn/tmp/efd20cee9d73227b1e95a2cf1b1b06815d1cbca20ada7d541c5e8889de22b0d0.jpg", "table_caption": ["Table 3: Test imputation results on PM 2.5 (5-trial averages). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "that the Rao-Blackwellized sampler can further improve the time series imputation performance of the base sampler. ", "page_idx": 7}, {"type": "text", "text": "269 5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "270 5.1 Diffusion Models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "271 Diffusion models, such as DDPMs [20] and SBGM [43], are considered as the new contenders   \n272 to GANs on data generation tasks. But they generally take relatively long time to produce high   \n273 quality samples. To mitigate this problem, the flowing matching methods have been proposed from   \n274 an optimal transport. For example, ENOT uses the saddle point reformulation of the OT problem to   \n275 develop a new diffusion model [19] The flowing matching methods have also been proposed based   \n276 on the OT theory [27, 29, 31, 2, 1]. In particular, mini-batch couplings are proposed to straighten the   \n277 probability flows for fast inference [39, 45, 46].   \n278 The Schr\u00f6dinger Bridge have also been applied to diffusion models for improving the data generation   \n279 performance of diffusion models. Diffusion Schr\u00f6dinger Bridge utilizes the Iterative Proportional   \n280 Fitting (IPF) method to solve the SB problem [14]. SB-FBSDE proposes to use forward-backward   \n281 (FB) SDE theory to solve the SB problem through likelihood training [10]. GSBM formulates a   \n282 generalized Schr\u00f6dinger Bridge matching framework by including the task-specific state costs for   \n283 various data generation tasks [28] NLSB chooses to model the potential function rather than the   \n284 velocity function to solve the Lagrangian SB problem [24]. Action Matching [33, 34] leverages   \n285 the principle of least action in Lagrangian mechanics to implicitly model the velocity function for   \n286 trajectory inference. Another classes of diffusion models have also been proposed from an stochastic   \n287 optimal control perspective by solving the HJB-PDEs [35, 50, 5, 28]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "288 5.2 Time Series Imputation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "289 Many diffusion-based models have been recently proposed for time series imputation [26, 32]. For   \n290 instance, CSDI [44] combines a conditional DDPM with a Transformer model to impute time series   \n291 data. CSBI [12] adopts the FB-SDE theory to train the conditional Schr\u00f6dinger bridge model to for   \n292 probabilistic time series imputation. To model the dynamics of time series from irregular sampled   \n293 data, DSPD-GP [7] uses a Gaussian process as the noise generator. TDdiff [23] utilizes self guidance   \n294 and learned implicit probability density to improve the time series imputation performance of the   \n295 diffusion models. However, the time series imputation methods mentioned above exhibit common   \n296 issues, such as slow convergence, similar to many diffusion models. Therefore, in this work, we   \n297 proposed CLWF to tackle thess challenges. ", "page_idx": 8}, {"type": "text", "text": "298 6 Conclusion, Limitation, and Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "299 In this work, we proposed CLWF, a novel time series imputation method based on the optimal   \n300 transport theory and Lagrangian mechanics. To generate the missing time series data, following the   \n301 principle of least action, CLWF learns a velocity field by minimizing the kinetic energy to move   \n302 the initial random noise to the target distribution. Moreover, we can also estimate the derivative of   \n303 a potential function via a VAE model trained on the observed training data to further improve the   \n304 performance of the base sampler by Rao-Blackwellization. In contrast with previous diffusion-based   \n305 models, the proposed requires less simulation steps and Monet Carlo samples to produce high-quality   \n306 data, which leads to fast inference. For validation, CWLF is assessed on two public datasets and   \n307 achieves competitive results compared with existing methods.   \n308 One limitation of CLWF is that the samples obtained are not diverse enough as we use ODE for   \n309 inference, which results in slightly higher test (continuous ranked probability score) CRPS compared   \n310 to previous works, e.g., CSDI. Therefore, for future work, we will seek suitable approaches to   \n311 accurately model the diffusion term in the SDE. Moreover, we will also try to design better task  \n312 specific potential functions for sparse multivariate time series data. We plan to explore the potential   \n313 of the Lagrangian Wasserstein Flow model for other time series analysis tasks, such as anomaly   \n314 detection and uncertainty quantification.   \n315 In terms of broader impact, our study on time series imputation has the potential to address important   \n316 real-world challenges and consequently make a positive impact on daily lives. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "317 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "318 [1] Michael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A   \n319 unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.   \n320 [2] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic   \n321 interpolants. In The Eleventh International Conference on Learning Representations, 2023.   \n322 [3] Vladimir Igorevich Arnol\u2019d. Mathematical methods of classical mechanics, volume 60. Springer   \n323 Science & Business Media, 2013.   \n324 [4] Richard Bellman. Dynamic programming. science, 153(3731):34\u201337, 1966.   \n325 [5] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion  \n326 based generative modeling. Transactions on Machine Learning Research, 2024.   \n327 [6] Dimitri Bertsekas. Dynamic programming and optimal control: Volume I, volume 4. Athena   \n328 scientific, 2012.   \n329 [7] Marin Bilo\u0161, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan G\u00fcnnemann.   \n330 Modeling temporal data as continuous functions with stochastic process diffusion. In Interna  \n331 tional Conference on Machine Learning, pages 2452\u20132470. PMLR, 2023.   \n332 [8] George Casella and Christian P Robert. Rao-blackwellisation of sampling schemes. Biometrika,   \n333 83(1):81\u201394, 1996.   \n334 [9] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary   \n335 differential equations. Advances in neural information processing systems, 31, 2018.   \n336 [10] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schr\u00f6dinger   \n337 bridge using forward-backward sdes theory. In International Conference on Learning Represen  \n338 tations, 2022, 2022.   \n339 [11] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. Stochastic control liaisons: Richard   \n340 sinkhorn meets gaspard monge on a schrodinger bridge. Siam Review, 63(2):249\u2013313, 2021.   \n341 [12] Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul,   \n342 Shandian Zhe, Anderson Schneider, and Yuriy Nevmyvaka. Provably convergent schr\u00f6dinger   \n343 bridge with applications to probabilistic time series imputation. In International Conference on   \n344 Machine Learning, pages 4485\u20134513. PMLR, 2023.   \n345 [13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in   \n346 neural information processing systems, 26, 2013.   \n347 [14] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger   \n348 bridge with applications to score-based generative modeling. Advances in Neural Information   \n349 Processing Systems, 34:17695\u201317709, 2021.   \n350 [15] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society,   \n351 2022.   \n352 [16] Lawrence C Evans. An introduction to mathematical optimal control theory spring, 2024   \n353 version. Lecture notes available at https://math.berkeley.edu/ evans/control.course.pdf, 2024.   \n354 [17] Wendell H Fleming and Raymond W Rishel. Deterministic and stochastic optimal control,   \n355 volume 1. Springer Science & Business Media, 2012.   \n356 [18] Vincent Fortuin, Dmitry Baranchuk, Gunnar R\u00e4tsch, and Stephan Mandt. Gp-vae: Deep   \n357 probabilistic time series imputation. In International conference on artificial intelligence and   \n358 statistics, pages 1651\u20131661. PMLR, 2020.   \n359 [19] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry P Vetrov, and Evgeny Burnaev.   \n360 Entropic neural optimal transport via diffusion processes. Advances in Neural Information   \n361 Processing Systems, 36, 2024.   \n362 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances   \n363 in neural information processing systems, 33:6840\u20136851, 2020.   \n364 [21] Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Berend Ensing, and Max Welling. Stochas  \n365 tic optimal control for collective variable free sampling of molecular transition paths. Advances   \n366 in Neural Information Processing Systems, 36, 2023.   \n367 [22] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In The Second   \n368 International Conference on Learning Representations, 2014.   \n369 [23] Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao   \n370 Wang, and Yuyang Bernie Wang. Predict, refine, synthesize: Self-guiding diffusion models for   \n371 probabilistic time series forecasting. Advances in Neural Information Processing Systems, 36,   \n372 2024.   \n373 [24] Takeshi Koshizuka and Issei Sato. Neural lagrangian schr\u00f6dinger bridge: Diffusion modeling for   \n374 population dynamics. In The Eleventh International Conference on Learning Representations,   \n375 2022.   \n376 [25] Christian L\u00e9onard. From the schr\u00f6dinger problem to the monge\u2013kantorovich problem. Journal   \n377 of Functional Analysis, 262(4):1879\u20131920, 2012.   \n378 [26] Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, and Junbin Gao. Diffusion models for time  \n379 series applications: a survey. Frontiers of Information Technology & Electronic Engineering,   \n380 pages 1\u201323, 2023.   \n381 [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow   \n382 matching for generative modeling. In The Eleventh International Conference on Learning   \n383 Representations, 2022.   \n384 [28] Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos Theodorou,   \n385 and Ricky TQ Chen. Generalized schr\u00f6dinger bridge matching. In The Twelfth International   \n386 Conference on Learning Representations, 2024.   \n387 [29] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint   \n388 arXiv:2209.14577, 2022.   \n389 [30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate   \n390 and transfer data with rectified flow. In The Eleventh International Conference on Learning   \n391 Representations, 2022.   \n392 [31] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Learning diffusion bridges on constrained   \n393 domains. In international conference on learning representations (ICLR), 2023.   \n394 [32] Caspar Meijer and Lydia Y Chen. The rise of diffusion models in time-series forecasting. arXiv   \n395 preprint arXiv:2401.03006, 2024.   \n396 [33] Kirill Neklyudov, Rob Brekelmans, Daniel Severo, and Alireza Makhzani. Action matching:   \n397 Learning stochastic dynamics from samples. In International Conference on Machine Learning,   \n398 pages 25858\u201325889. PMLR, 2023.   \n399 [34] Kirill Neklyudov, Rob Brekelmans, Alexander Tong, Lazar Atanackovic, Qiang Liu, and Alireza   \n400 Makhzani. A computational framework for solving wasserstein lagrangian flows. arXiv preprint   \n401 arXiv:2310.10649, 2023.   \n402 [35] Nikolas N\u00fcsken and Lorenz Richter. Solving high-dimensional hamilton\u2013jacobi\u2013bellman pdes   \n403 using neural networks: perspectives from the theory of controlled diffusions and measures on   \n404 path space. Partial differential equations and applications, 2(4):48, 2021.   \n405 [36] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer   \n406 Science & Business Media, 2013.   \n407 [37] Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate   \n408 continuous normalizing flows via optimal transport. Proceedings of the AAAI Conference on   \n409 Artificial Intelligence, 35(10):9223\u20139232, 2021.   \n410 [38] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data   \n411 science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n412 [39] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron   \n413 Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch   \n414 couplings. In International Conference on Machine Learning, pages 28100\u201328127. PMLR,   \n415 2023.   \n416 [40] Hannes Risken and Till Frank. The Fokker-Planck Equation: Methods of Solution and Applica  \n417 tions, volume 18. Springer Science & Business Media, 2012.   \n418 [41] Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-   \n419 63):94, 2015.   \n420 [42] Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. Predicting in  \n421 hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. In   \n422 2012 Computing in Cardiology, pages 245\u2013248. IEEE, 2012.   \n423 [43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and   \n424 Ben Poole. Score-based generative modeling through stochastic differential equations. In   \n425 International Conference on Learning Representations, 2020.   \n426 [44] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based   \n427 diffusion models for probabilistic time series imputation. Advances in Neural Information   \n428 Processing Systems, 34:24804\u201324816, 2021.   \n429 [45] Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid   \n430 Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based genera  \n431 tive models with minibatch optimal transport. Transactions on Machine Learning Research,   \n432 2024. Expert Certification.   \n433 [46] Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume   \n434 Huguet, Guy Wolf, and Yoshua Bengio. Simulation-free schr\u00f6dinger bridges via score and flow   \n435 matching. arXiv preprint arXiv:2307.03672, 2023.   \n436 [47] C\u00e9dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.   \n437 [48] Liu Yang and George Em Karniadakis. Potential flow generator with l 2 optimal transport   \n438 regularity for generative models. IEEE Transactions on Neural Networks and Learning Systems,   \n439 33(2):528\u2013538, 2020.   \n440 [49] Jiongmin Yong and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations,   \n441 volume 43. Springer Science & Business Media, 2012.   \n442 [50] Qinsheng Zhang and Yongxin Chen. Path integral sampler: A stochastic control approach for   \n443 sampling. In The Tenth International Conference on Learning Representations. OpenReview.net,   \n444 2022.   \n445 [51] Yu Zheng, Furui Liu, and Hsun-Ping Hsieh. U-air: When urban air quality inference meets   \n446 big data. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge   \n447 discovery and data mining, pages 1436\u20131444, 2013. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "448 A Stochastic Optimal Control ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "449 The data generation task can also be interpreted as a stochastic optimal control (SOC) problem   \n450 [4, 17, 35, 50, 21, 5] whose cost function $\\mathcal{I}$ is defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{I}(X_{t},t)=\\mathbb{E}_{p(X_{t})}\\Bigg[\\int_{0}^{T}\\int_{\\mathbb{R}_{d}}\\frac{1}{2}\\|\\nabla_{x}\\Psi(X_{t},t)\\|^{2}\\mathrm{d}X_{t}\\mathrm{d}t\\Bigg]+\\mathbb{E}_{p(X_{T})}\\big[\\Psi(X_{T})\\big],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "451 where $\\frac{1}{2}\\|\\nabla_{x}\\Psi(X_{t},t)\\|^{2}$ denotes the running cost, and $\\Psi(X_{T})$ denotes the terminal cost. The above   \n452 SOC problem can be solved by dynamic programming [4, 6].   \n453 Further, let $V(X_{t},t)=\\operatorname*{inf}\\mathcal{I}(X_{t},t)$ be the value function/optimal-cost-to-go of the SOC problem,   \n454 then the corresponding Hamilton-Jacobi-Bellman (HJB) partial differential equation (PDE) [15, 49]   \n455 is given by ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial V_{t}}{\\partial t}-\\frac{1}{2}\\nabla V_{t}^{\\prime}\\nabla V_{t}+\\frac{1}{2}\\Delta V_{t}=0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with the terminal condition: $V(X_{t},T)=\\Psi(X_{t}).$ ", "page_idx": 12}, {"type": "text", "text": "456 B Rao-Blackwell Theorem ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "457 Theorem 1 (Rao-Blackwell) Let $\\boldsymbol{S}$ be an unbiased estimator of some parameter $\\theta\\in\\Theta$ , and $\\mathcal{T}(X)$   \n458 the sufficient statistic for $\\theta$ , then: 1) $S^{*}\\,=\\,\\mathbb{E}[S|{\\mathcal{T}}(X)],i s$ an unbiased estimator for $\\theta$ , and 2)   \n459 $\\mathbb{V}_{\\theta}[S^{\\ast}]\\leq\\mathbb{V}_{\\theta}[S]$ for all $\\theta$ . The inequality is strict unless $\\boldsymbol{S}$ is a function of $\\tau$ . ", "page_idx": 12}, {"type": "text", "text": "460 C Experimental Environment ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "461 For the hardware environment of the experiments, we use a single NVIDIA A100-PCIE-40GB GPU   \n462 and an Intel(R) Xeon(R) Gold-6248R-3.00GHz CPU. For the software environment, the Python   \n463 version is 3.9.7, the CUDA version 11.7, and the Pytorch version is 2.0.1. ", "page_idx": 12}, {"type": "text", "text": "464 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "465 1. Claims   \n466 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n467 paper\u2019s contributions and scope?   \n468 Answer: [Yes]   \n469 2. Limitations   \n470 Question: Does the paper discuss the limitations of the work performed by the authors?   \n471 Answer: [Yes]   \n472 Justification: See Sec. 6   \n473 3. Theory Assumptions and Proofs   \n474 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n475 a complete (and correct) proof?   \n476 Answer: [NA]   \n477 4. Experimental Result Reproducibility   \n478 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n479 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n480 of the paper (regardless of whether the code and data are provided or not)?   \n481 Answer:[Yes]   \n482 Justification: See supplemental material   \n483 5. Open access to data and code   \n484 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n485 tions to faithfully reproduce the main experimental results, as described in supplemental   \n486 material?   \n487 Answer: [Yes]   \n488 Justification: See supplemental material   \n489 6. Experimental Setting/Details   \n490 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n491 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n492 results?   \n493 Answer: [Yes]   \n494 Justification: See supplementary.   \n495 7. Experiment Statistical Significance   \n496 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n497 information about the statistical significance of the experiments?   \n498 Answer: [Yes]   \n499 Justification: See Sec. 4   \n500 8. Experiments Compute Resources   \n501 Question: For each experiment, does the paper provide sufficient information on the com  \n502 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n503 the experiments?   \n504 Answer: [Yes] .   \n505 Justification: See Appendix. C   \n506 9. Code Of Ethics   \n507 Question: Does the research conducted in the paper conform, in every respect, with the   \n508 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n509 Answer: [Yes]   \n510 10. Broader Impacts   \n511 Question: Does the paper discuss both potential positive societal impacts and negative   \n512 societal impacts of the work performed?   \n513 Answer: [Yes]   \n514 Justification: See Sec. 6   \n515 11. Safeguards   \n516 Question: Does the paper describe safeguards that have been put in place for responsible   \n517 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n518 image generators, or scraped datasets)?   \n519 Answer: [NA] .   \n520 12. Licenses for existing assets   \n521 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n522 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n523 properly respected?   \n524 Answer: [Yes]   \n525 Justification: See Sec. 4   \n526 13. New Assets   \n527 Question: Are new assets introduced in the paper well documented and is the documentation   \n528 provided alongside the assets?   \n529 Answer: [Yes]   \n530 14. Crowdsourcing and Research with Human Subjects   \n531 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n532 include the full text of instructions given to participants and screenshots, if applicable, as   \n533 well as details about compensation (if any)?   \n534 Answer: [NA]   \n535 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n536 Subjects   \n537 Question: Does the paper describe potential risks incurred by study participants, whether   \n538 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n539 approvals (or an equivalent approval/review based on the requirements of your country or   \n540 institution) were obtained?   \n541 Answer: [NA] . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}]