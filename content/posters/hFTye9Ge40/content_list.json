[{"type": "text", "text": "Fixed Confidence Best Arm Identification in the Bayesian Setting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kyoungseok Jang Universit\u00e1 degli Studi di Milano ksajks@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Junpei Komiyama New York University / RIKEN AIP junpei@komiyama.info ", "page_idx": 0}, {"type": "text", "text": "Kazutoshi Yamazaki The University of Queensland k.yamazaki@uq.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as trackand-stop and top-two algorithms, result in arbitrarily suboptimal performances in the Bayesian setting. We also obtain a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many sequential decision-making problems, the learner repeatedly chooses an arm (option) to play with and observes a reward drawn from the unknown distribution of the corresponding arm. One of the most widely-studied instances of such problems is the multi-armed bandit problem [Thompson, 1933, Robbins, 1952, Lai, 1987], where the goal is to maximize the sum of rewards during the rounds. Since the learner does not know the distribution of rewards, they need to explore the different arms, and yet, exploit the arms of the most rewarding arms so far. Different from the classical bandit formulation, there are situations where one is more interested in collecting information rather than maximizing intermediate rewards. The best arm identification (BAI) is a sequential decision-making problem in which the learner is only interested in identifying the arm with the highest mean reward. While the origin of this problem dates back to at least the 1950s [Bechhofer, 1954, Paulson, 1964, Gupta, 1977], recent work in the field of machine learning reformulated the problem [Audibert et al., 2010]. In the BAI, the learner needs to pull arms efficiently for better identification. To achieve efficiency and accuracy, the learner should determine which arm to choose based on the history, when to stop the sampling, and which arm to recommend as the learner\u2019s final decision. ", "page_idx": 0}, {"type": "text", "text": "There are two types of BAI problems depending on the optimization objective. In the fixed-budget (FB) setting [Audibert et al., 2010], the learner attempts to minimize the probability of error (misidentification of the best arm) given a limited number of arm pulls $T$ . In the fixed confidence (FC) setting [Jamieson and Nowak, 2014], the learner attempts to minimize the number of arm pulls, subject to a predefined probability of error $\\delta\\in(0,1)$ . In this paper, we shall focus on the FC setting, which is useful when we desire a rigorous statistical guarantee. ", "page_idx": 0}, {"type": "text", "text": "Most of the previous BAI studies focus on the frequentist setting, where the bandit model is chosen adversarially from some hypothesis class beforehand. In this setting, several algorithms, such as Track and Stop [Kaufmann et al., 2016a] and Top-two algorithms [Russo, 2016, Qin et al., 2017b, Jourdan et al., 2022], are widely known. These algorithms have an optimal sample complexity, meaning that they are one of the most sample-efficient algorithms among the class of $\\delta$ -correct algorithms. ", "page_idx": 1}, {"type": "text", "text": "The sample complexity of these algorithms is problem-dependent. To see this, consider the following example. ", "page_idx": 1}, {"type": "text", "text": "Example 1. (A/B/C testing) Consider A/B/C testing of web designs. We have three arms (web designs) from which we would like to find the largest retention rate via allocating users to web designs $i={1,2,3}$ . If we attempt to find the best arm with confidence $\\delta$ , we may need a large number of samples (users) when the suboptimality gap (the gap between the retention rate of the best arm and the second best arm) is small because in such a case the identification of the best arm is difficult \u2013 the minimum number of samples required is inversely proportional to the square of the suboptimality gap[Kaufmann et al., 2014]. For example, when comparing the testing of retention rates of (0.9, 0.5, 0.1) with (0.9, 0.89, 0.1), the second case requires around $\\begin{array}{r}{\\left(\\frac{0.9-0.5}{0.9-0.89}\\right)^{2}=1600}\\end{array}$ times more samples compared to the first case. ", "page_idx": 1}, {"type": "text", "text": "In practice, the retention rate of 0.89 in the second case may be acceptably good compared to the optimal retention rate of 0.9, and we may stop exploration at the moment the learner identifies a reasonably good arm, which is the first or the second arm in this example. This idea is formalized in several ways. The literature of Ranking and Selection (R&S) usually considers the indifference-zone formulation [Hong et al., 2021]. In the context of best arm identification, a similar notion of $\\epsilon$ -best answer identification has also been considered [Maron and Moore, 1993, Even-Dar et al., 2006, Gabillon et al., 2012, Kaufmann and Kalyanakrishnan, 2013, Jourdan et al., 2023]. In these settings, the learner accepts a sub-optimal arm whose means are at most $\\epsilon$ worse than the mean of the optimal arm. Other related settings include the good arm identification problem [Kano et al., 2019, Tabata et al., 2020, Zhao et al., 2023], where the goal is to identify an arm that exceeds the predefined threshold, and the thresholding bandit problem [Locatelli et al., 2016, Xu et al., 2019], where the goal is to identify whether the mean of each arm is above or below the threshold. All these problem settings require an extra parameter, like $\\epsilon$ or an acceptance threshold, that directly determines the acceptance level. Even though the algorithm\u2019s performance depends on this parameter, it is often challenging to determine a reasonable value for it in advance. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we study an alternative approach based on the Bayesian setting. In particular, we consider the prior distribution on the model parameters. We relax the requirement on the correctness of the best arm identification by using the prior belief. Rather than requiring the frequentist $\\delta$ - correctness for any model, we require the learner to have marginalized correctness over the prior distribution, which we call Bayesian $\\delta$ -correctness. ", "page_idx": 1}, {"type": "text", "text": "We study the fixed confidence BAI (FC-BAI) problem in the Bayesian setting. Our contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 First, we find that in the Bayesian setting, the performance of the traditional frequentist setting-based algorithms, such as Track and Stop and Top-two algorithms, can be arbitrarily worse (Section 3). This is because frequentist approaches spend too many resources when the suboptimality gap is narrow. \u2022 Second, we prove that the lower bound of the number of expected samples should attain at least the order of $\\Omega\\big(\\frac{L(H)^{2}}{\\delta}\\big)$ as $\\delta\\rightarrow0$ (Section 4). Here $L(H)$ is our novel quantity that represents the sample complexity with respect to the prior distribution . This order is different from the existing lower bound in the frequentist setting1, implying that the Bayesian setting is essentially different from the frequentist setting. \u2022 Third, we design an algorithm whose expected sample size is upper-bounded by O( L(\u03b4H)2log L(\u03b4H) (Section 5). Our algorithm is based on the elimination algorithm [Maron and Moore, 1993, Even-Dar et al., 2006, Frazier, 2014], but we add an early stopping criterion to prevent over-commitment of the algorithm for a bandit model with a narrow suboptimality gap. Our algorithm has a matching upper bound up to the logarithmic factor. ", "page_idx": 1}, {"type": "text", "text": "We also conduct simulation to demonstrate that the sample complexity of frequentist algorithms does indeed diverge in a bandit model with a small suboptimality gap, even in very simple cases (Section 6). ", "page_idx": 2}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To our knowledge, BAI problems studied for the Bayesian setting have been limited to the fixed budget setting [Komiyama et al., 2023, Atsidakou et al., 2023]. Komiyama et al. [2023] showed that, in the fixed-budget setting, a simple non-Bayesian algorithm has an optimal simple regret up to a constant factor, implying that the advantage the learner could get from the prior is small when the budget is large. This is very different from our fixed-confidence setting, where utilizing the prior distribution is necessary. ", "page_idx": 2}, {"type": "text", "text": "Several FC-BAI algorithms used Bayesian ideas on the structure of the algorithm, although most of those studies used frequentist settings for measuring the guarantee. The \u2018Top-Two\u2019 type of algorithms are the leading representatives in this direction. The first instance of top-two algorithms, which is called Top-Two Thompson sampling (TTTS), is introduced in the context of Bayesian best arm identification. TTTS requires a prior distribution, and Russo [2016] showed that the sample complexity of posterior convergence of TTTS which is the same as the sample complexity of the frequentist fixed-confidence best arm identification. Subsequent research analyzed the performance of TTTS from the frequentists\u2019 viewpoint [Shang et al., 2020]. Later on, the idea of top-two sampling is then extended into many other algorithms, such as Top-Two Transportation Cost [Shang et al., 2020], Top-Two Expected Improvement (TTEI, Qin et al. 2017b), Top-Two Upper Confidence bound (TTUCB, Jourdan and Degenne 2022a). Even though some of the top two algorithms adapt a prior, they implicitly solve the optimization that is justified in view of frequentist. ", "page_idx": 2}, {"type": "text", "text": "Another line of Bayesian sequential decision-making is Bayesian optimization [Srinivas et al., 2010, Mockus, 2012, Shahriari et al., 2016, Jamieson and Talwalkar, 2016, Frazier, 2018], where the goal is to find the best arm in Bayesian setting. Note that Bayesian optimization tends to deal with structured identification, especially for Gaussian processes, and most of the algorithms for Bayesian optimization do not have specific stopping criteria. ", "page_idx": 2}, {"type": "text", "text": "2 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study the fixed confidence best arm identification problem (FC-BAI) in a Bayesian setting. In this setup, we have $k$ arms in the set $[k]:=\\{1,2,\\dots,k\\}$ with unknown distribution $\\boldsymbol{P}=\\left(P_{1},\\cdots\\,,P_{k}\\right)$ which is drawn from a known prior distribution at time 0, namely $\\pmb{H}=(H_{1},\\cdot\\cdot\\cdot\\,,H_{k})$ . The unknown bandit model $P_{i}$ is a one-parameter distribution, and $_{P}$ is specified by $\\pmb{\\mu}:=\\:(\\mu_{1},\\cdot\\cdot\\cdot\\,,\\mu_{k})$ . To simplify the problem, we will focus on the Gaussian case, where each $P_{i}$ is a Gaussian distribution with known variance $\\sigma_{i}^{2}$ . Each mean of $P_{i}$ , denoted $\\mu_{i}$ , is drawn from a known prior Gaussian distribution $H_{i}$ , which can be written as $N(m_{i},\\xi_{i}^{2})$ . ", "page_idx": 2}, {"type": "text", "text": "At every time step $t=1,2,\\cdot\\cdot\\cdot$ , the forecaster chooses an arm $A_{t}\\in[k]$ and observes a reward $X_{t}$ , which is drawn independently from $P_{A_{t}}$ . Since we focus on the Gaussian case, $X_{t}\\sim N(\\mu_{A_{t}},\\sigma_{A_{t}}^{2})$ conditionally given $A_{t}$ and $\\mu_{A_{t}}$ . After each sampling, the forecaster must decide whether to continue the sampling process or stop sampling and make a recommendation $J\\in[k]$ . ", "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{F}_{t}=\\sigma(A_{1},X_{1},A_{2},X_{2},\\cdot\\cdot\\cdot,A_{t},X_{t})$ be the $\\sigma$ -field generated by observations up to time $t$ . The algorithm of the forecaster $\\pi:=((A_{t})_{t},\\tau,J)$ is defined by the following triplet [Kaufmann et al., 2016a]: ", "page_idx": 2}, {"type": "text", "text": "\u2022 A sampling rule $(A_{t})_{t}$ , which determines the arm to draw at round $t$ based on the previous history (each $A_{t}$ must be $\\mathcal{F}_{t-1}$ measurable).   \n\u2022 A stopping rule $\\tau$ , which means when to stop the sampling (i.e., stopping time with respect to ${\\mathcal{F}}_{t}$ ).   \n\u2022 A decision rule $J$ , which determines the arm the forecaster recommends based on his sampling history (i.e., $J$ is $\\mathcal{F}_{\\tau}$ -measurable). ", "page_idx": 2}, {"type": "text", "text": "In FC-BAI, the forecaster aims to recommend arm $J$ that correctly identifies (one of) the best arm(s) $i^{*}(\\pmb{\\mu}):=\\arg\\operatorname*{max}_{i\\in[k]}\\mu_{i}$ with probability at least $1-\\delta$ . Since the case of multiple best arms is of measure zero under $\\pmb{H}$ , we can focus on $\\pmb{\\mu}$ such that $i^{*}(\\pmb{\\mu})$ is unique. For the FC-BAI problem in the Bayesian setting, we use the expected probability of misidentification: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{PoE}(\\pi;H):=\\mathbb{E}_{\\mu\\sim H}\\bigg[\\mathbb{P}\\Big(J\\neq i^{*}(\\mu)|\\mathcal{H}_{\\mu}\\Big)\\bigg],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{H}_{\\mu}:=\\left\\{\\mu\\right.$ is the correct bandit model}. Now we formally define the algorithm of interest as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 1. (Bayesian $\\delta$ -correctness) For a prior distribution $\\pmb{H}$ , an algorithm $\\pi=((A_{t}),\\tau,J)$ is said to be Bayesian $(H,\\delta)$ -correct if it satisfies $\\mathrm{PoE}(\\pi;H)\\,\\leq\\,\\delta$ . Let $A^{b}(\\delta,H)$ be the set of Bayesian $(H,\\delta)$ -correct algorithms for the prior distribution $\\pmb{H}$ . ", "page_idx": 3}, {"type": "text", "text": "The objective of the FC-BAI problem in the Bayesian setting is to find an algorithm $\\pi\\ =$ $((A_{t})_{t},\\dot{\\tau},J)\\in\\mathcal{A}^{b}(\\delta,H)$ that minimizes $\\mathbb{E}_{\\mu\\sim H}[\\tau]$ . ", "page_idx": 3}, {"type": "text", "text": "Terminology Define $\\begin{array}{r}{N_{i}(t)\\,=\\,\\sum_{s=1}^{t-1}{\\bf1}[A_{s}\\,=\\,i]}\\end{array}$ as the number of times arm $i$ is pulled before timestep $t$ . Le\u221at $h_{i}$ be the proba bility density function of $H_{i}$ . Since we consider Gaussian prior, $h_{i}(\\mu_{i}):=(1/\\sqrt{2\\pi}\\xi_{i})\\exp(-(\\mu_{i}\\!-\\!m_{i})^{2}/(2\\xi_{i}^{2}))$ . Let $i^{*}$ , $j^{*}:\\mathbb{R}^{k}\\to[k]$ be the best and the second best arm under the input such that for each $\\mu\\in\\{\\mathbf{x}\\in\\mathbb{R}^{k}:x_{i}\\neq x_{j}\\forall i\\neq j\\}$ , $i^{*}(\\pmb{\\mu})=\\arg\\operatorname*{max}_{i\\in[K]}\\mu_{i}$ and j\u2217(\u00b5) = arg maxi\u2208[K]\\{i\u2217(\u00b5)} \u00b5i. ", "page_idx": 3}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathrm{KL}_{i}(a\\|b)\\,:=\\,\\frac{(a-b)^{2}}{2\\sigma_{i}^{2}}}\\end{array}$ represent the KL-divergence between two Gaussian distributions with equal variances (the variance of the $i$ -th arm $\\sigma_{i}^{2}$ ) but different means, denoted as $a$ and $b$ . Similarly, $d\\bar{(a,b)}:=a\\log(a/b)+(1-a)\\log((1-a)/(\\bar{1}-b))$ is the KL divergence between two Bernoulli distributions with means $a$ and $b$ . Throughout this paper, $\\mathbb{E}_{\\mu}$ and $\\mathbb{P}_{\\pmb{\\mu}}$ denote the expectation and probability when the bandit model is fixed as $\\pmb{\\mu}\\in\\mathbb{R}^{k}$ , i.e., $\\mathbb{E}_{\\mu}=\\mathbb{E}[\\cdot|\\mathcal{H}_{\\mu}]$ and $\\mathbb{P}_{\\mu}=\\mathbb{P}(\\cdot|\\mathcal{H}_{\\mu})$ . We will abuse the notation PoE so that for $\\lambda\\in\\mathbb{R}^{k}$ , $\\mathrm{PoE}(\\pi;\\lambda)$ means ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{PoE}(\\pi;\\lambda):=\\mathbb{P}_{\\lambda}\\big(J\\neq i^{*}(\\lambda)|\\mathcal{H}_{\\lambda}\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Naturally, $\\operatorname{PoE}(\\pi;H)=\\mathbb{E}_{\\mu\\sim H}\\left[\\operatorname{PoE}(\\pi;\\pmb{\\mu})\\right]$ . ", "page_idx": 3}, {"type": "text", "text": "Lastly, we introduce the constant $L(H)$ that characterizes the sample complexity in the Bayesian setting. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. For each $i,j\\in[k]$ , define $L_{i j}(H)$ and $L(H)$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\pmb{H}):=\\sum_{i,j\\in[k],i\\neq j}L_{i j}(\\pmb{H})\\mathrm{~where~}L_{i j}(\\pmb{H}):=\\int_{-\\infty}^{\\infty}h_{i}(x)h_{j}(x)\\prod_{s:s\\in[k]\\backslash\\{i,j\\}}H_{s}(x)\\,\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This constant has the following interesting property which we call a volume lemma: ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Volume Lemma, informal). For $\\Delta\\in(0,1)$ , let ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(H,\\Delta):=\\frac{1}{\\Delta}\\mathbb{P}_{\\mu\\sim H}\\Big[\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)}\\leq\\Delta\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, $\\begin{array}{r c l}{\\operatorname*{lim}_{\\Delta\\to0^{+}}L(\\pmb{H},\\Delta)}&{=}&{L(\\pmb{H})}\\end{array}$ . In particular, for $\\begin{array}{r l r}{\\Delta}&{{}<}&{\\frac{L(H)}{\\sum_{i\\in[k]}\\frac{2(k-1)}{\\xi_{i}}}}\\end{array}$ $L(H,\\Delta)\\quad\\in$ $({\\textstyle\\frac{1}{2}}L(H),2L(H)).$ . ", "page_idx": 3}, {"type": "text", "text": "The volume lemma states that the volume of prior where the suboptimality gap is smaller than $\\Delta$ is proportional to $L(H)\\Delta$ when $\\Delta$ is small. We will see in Section 3 that such small-gap cases, which require a large amount of exploration to identify the best arm, dominate the Bayesian expectation of the stopping time. Therefore, $L(H)$ defines the Bayesian sample complexity. The formal version of this lemma, which involves some regularity conditions, is shown in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Remark 1. Here, we elaborate on how the Bayesian sample complexity is defined. As will be shown in Section 3, for an algorithm to have a finite expected stopping time, it must determine whether the current instance is difficult or not. In particular, if an algorithm tries to identify even the top- $O(\\delta)$ \u2018hardest instances\u20192 in the prior , the algorithm cannot achieve the finite expected stopping time. By ", "page_idx": 3}, {"type": "text", "text": "Lemma 1, the suboptimality gap of the top- $O(\\delta)$ hardest instance is given by $L(\\mathcal{H})\\Delta\\approx\\delta$ , and the corresponding (frequentist) sample complexity is proportional to $\\bar{\\Delta^{-2}}=\\dot{(L(\\mathcal{H})/\\delta)^{2}}$ [Kaufmann et al., 2014]. Such instances constitute an $O(\\delta)$ fraction of the prior, and thus the Bayesian sample complexity is: ", "page_idx": 4}, {"type": "equation", "text": "$$\nO\\left(\\frac{(L(\\mathcal{H}))^{2}}{\\delta^{2}}\\times\\delta\\right)=O\\left(\\frac{(L(\\mathcal{H}))^{2}}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3 Limitation of traditional frequentist approaches in the Bayesian setting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Existing BAI studies mainly focused on the Frequentist $\\delta$ -correct algorithms which are defined as follows: ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Frequentist $\\delta$ -correctness). An algorithm $\\pi=((A_{t}),\\tau,J)$ is said to be frequentist $\\delta$ -correct if, for any bandit instance $\\pmb{\\mu}\\in\\mathbb{R}^{k}$ such that $i^{*}(\\mu)$ is unique, it satisfies $\\mathrm{PoE}(\\pi;\\mu)\\leq\\delta$ . Let $A^{f}(\\delta)$ be the set of all frequentist- $\\delta$ -correct algorithms. ", "page_idx": 4}, {"type": "text", "text": "For the frequentist $\\delta$ -correct algorithms, Garivier and Kaufmann [2016] proved a lower bound for the expected stopping time as follows: for all bandit instance $\\pmb{\\mu}\\in\\mathbb{R}^{k}$ and for all $((A_{t}),\\tau,J)\\in\\mathcal{A}^{f}(\\delta).$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{\\mu}}\\left[\\tau\\right]\\geq\\log(\\delta^{-1})T^{\\ast}(\\pmb{\\mu})+o(\\log(\\delta^{-1}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $T^{*}(\\pmb{\\mu})$ is a sample complexity function dependent on the bandit instance $\\pmb{\\mu}$ .3 Moreover, many of the known frequentist $\\delta$ -correct algorithms achieve asymptotic optimality [Garivier and Kaufmann, 2016, Russo, 2016, Tabata et al., 2023, Qin et al., 2017a], meaning that they are orderwisely tight up to the lower bound on Eq. (2) as $\\delta\\rightarrow0$ . However, little is known, or at least discussed, about their performance in the Bayesian setting. ", "page_idx": 4}, {"type": "text", "text": "One can check that a frequentist $\\delta$ -correct algorithm is also Bayesian $\\delta$ -correct as well $(\\mathcal{A}^{f}(\\delta)\\subset$ $A^{b}(\\delta,H)$ for all $\\pmb{H}$ ). Naturally, our interest is whether or not the most efficient classes of frequentist $\\delta$ -correct algorithms, such as Tracking algorithms and Top-two algorithms, are efficient in Bayesian settings. Somewhat surprisingly, the following theorem states that any $\\delta$ -correct algorithm is suboptimal in Bayesian settings. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. For all $\\delta>0,H$ and $((A_{t}),\\tau,J)\\in\\mathcal{A}^{f}(\\delta),\\mathbb{E}_{\\mu\\sim H}\\left[\\tau\\right]=+\\infty.$ ", "page_idx": 4}, {"type": "text", "text": "Proof of Theorem 2 is found in Appendix C. To illustrate the proof, we will use a two-armed Gaussian instance as an example. ", "page_idx": 4}, {"type": "text", "text": "3.1 Special case - two armed Gaussian case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we present one intuitive corollary of the lower bound theorem [Kaufmann et al., 2016a, Garivier and Kaufmann, 2016, Kaufmann et al., 2016b] that uses a standard information-theoretic technique. ", "page_idx": 4}, {"type": "text", "text": "Corollary 3 (Kaufmann et al. 2014). Let $\\delta\\ \\in\\ (0,1)$ . For any frequentist $\\delta$ -correct algorithm $((A_{t}),\\tau,J)$ and for any fixed mean vector $\\pmb{\\mu}=(\\mu_{1},\\mu_{2})\\in\\mathbb{R}^{2}$ , $\\begin{array}{r}{\\mathbb{E}_{\\pmb{\\mu}}[\\tau]\\geq\\frac{d(\\delta,1-\\delta)}{(\\mu_{1}-\\mu_{2})^{2}}\\geq\\frac{\\log\\frac{1}{2.4\\delta}}{(\\mu_{1}-\\mu_{2})^{2}}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "In the frequentist setting, Corollary 3 implies the lower bound of $\\mathbb{E}_{\\mu}[\\tau]=\\Omega(\\log(\\delta^{-1})/(\\mu_{1}-\\mu_{2})^{2})$ , which is $\\Omega(\\log(\\delta^{-1}))$ when we view parameters $(\\mu_{1},\\mu_{2})$ as constants. However, in the Bayesian setting, the algorithm is given the prior distribution $\\pmb{H}$ on $\\pmb{\\mu}$ , and thus the stopping time is marginalized over $\\pmb{H}$ . In particular, limiting our interest to the case of $|\\mu_{1}-\\mu_{2}|<\\Delta$ for small enough $\\Delta>0$ , we can obtain the following lower bound: ", "page_idx": 4}, {"type": "text", "text": "sitive r.v.) pectation) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\mu\\sim H}[\\tau]\\geq\\mathbb{E}_{\\mu\\sim H}[\\tau\\cdot\\mathbf{1}[\\vert\\mu_{1}-\\mu_{2}\\vert\\leq\\Delta]]}}&{{\\quad{\\mathrm{(Since~}}\\tau\\mathrm{~is~pce~})}}\\\\ &{\\geq\\mathbb{E}_{\\mu\\sim H}\\left[\\mathbb{E}[\\tau\\vert\\pmb{\\mu}]\\cdot\\mathbf{1}[\\vert\\mu_{1}-\\mu_{2}\\vert\\leq\\Delta]\\right]}&{{\\quad{\\mathrm{(Law~of~total~ex~})}}}\\\\ &{\\geq\\mathbb{E}_{\\mu\\sim H}\\left[\\frac{\\log\\delta^{-1}}{(\\mu_{1}-\\mu_{2})^{2}}\\cdot\\mathbf{1}[\\vert\\mu_{1}-\\mu_{2}\\vert\\leq\\Delta]\\right]}&{{\\quad{\\mathrm{(Ca)}}}}\\\\ &{\\geq\\frac{\\log\\delta^{-1}}{\\Delta^{2}}\\mathbb{P}_{\\mu\\sim H}[\\vert\\mu_{1}-\\mu_{2}\\vert\\leq\\Delta]\\geq\\frac{\\log\\delta^{-1}}{\\Delta^{2}}\\frac{L(H)}{2}\\Delta}&{{\\quad{\\mathrm{(Ca)}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3For details about $T^{*}(\\pmb{\\mu})$ , a reader may refer to Garivier and Kaufmann [2016]. ", "page_idx": 4}, {"type": "equation", "text": "$$\n=\\Omega\\left(\\frac{L(H)\\log\\delta^{-1}}{\\Delta}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This inequality implies that if we naively use a known frequentist $\\delta$ -correct algorithm in the Bayesian setting, the expected stopping time will diverge because we can choose an arbitrarily small $\\Delta$ . The case of a small gap is difficult to identify, and the expected stopping time can be very large for such a case if we aim to identify the best arm for any model. ", "page_idx": 5}, {"type": "text", "text": "4 Lower bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section will elaborate on the lower bound of the stopping time in the Bayesian setting. Theorem 4 below states that any Bayesian $(H,\\delta)$ -correct algorithm requires the expected stopping time of at least $\\Omega\\big(\\frac{L(H)^{2}}{\\delta}\\big)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Define $\\begin{array}{r}{\\sigma_{\\mathrm{min}}=\\operatorname*{min}_{i\\in[k]}\\sigma_{i}^{2}}\\end{array}$ and $\\begin{array}{r}{N_{V}=\\frac{L(H)^{2}\\sigma_{\\mathrm{min}}^{2}\\ln2}{16e^{4}\\delta}}\\end{array}$ . Let $\\delta<\\delta_{L}(H)$ be sufficiently small.4 Then, for any BAI algorithms $\\pi=((A_{t}),\\tau,J)$ , if $\\mathbb{E}_{\\mu\\sim H}[\\tau]\\le N_{V}$ , then $\\mathrm{PoE}(\\pi;H)\\geq\\delta$ . ", "page_idx": 5}, {"type": "text", "text": "In this main body, we will use the two-armed Gaussian bandit model with homogeneous variance condition (i.e. $\\sigma_{1}=\\sigma_{2}=\\sigma_{,}$ ) for easier demonstration of the proof sketch. Theorem 4, which is more general in the sense that it can deal with $k>2$ arms with heterogeneous variances, is proven in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Sketch of the proof, for $k=2$ : It suffices to show that the following is an empty set: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathcal A}^{b}(\\delta,H,N_{V}):=\\{\\pi\\in{\\mathcal A}^{b}(\\delta,H):\\mathbb{E}_{\\mu\\sim H}[\\tau]\\leq N_{V}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assume that $A^{b}(\\delta,H,N_{V})\\,\\ne\\,\\emptyset$ and choose an arbitrary $\\pi\\,\\in\\,\\mathcal{A}^{b}(\\delta,H,N_{V})$ . We start from the following transportation lemma: ", "page_idx": 5}, {"type": "text", "text": "Lemma 5 (Kaufmann et al. 2016a, Lemma 1). Let $\\delta\\in(0,1)$ . For any algorithm $((A_{t}),\\tau,J)$ , any $\\mathcal{F}_{\\tau}$ -measurable event $\\mathcal{E}$ , any bandit models $\\mu,\\lambda\\in\\{(x,y)\\in\\mathbb{R}^{2}:x\\neq y\\}$ such that $i^{*}(\\dot{\\mu})\\neq i^{*}(\\lambda)$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left[\\sum_{i=1}^{2}\\mathrm{KL}_{i}(\\mu_{i},\\lambda_{i})N_{i}(\\tau)\\right]\\geq d(\\mathbb{P}_{\\mu}(\\mathcal{E}),\\mathbb{P}_{\\lambda}(\\mathcal{E})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the above Lemma holds for any algorithm, and thus works for any stopping time $\\tau$ . Now define $\\nu(\\mu)$ as a swapped version of $\\pmb{\\mu}\\in\\breve{\\mathbb{R}}^{2}$ , which means $(\\pmb{\\nu}(\\pmb{\\mu}))_{1}=\\mu_{2},\\pmb{\\nu}(\\pmb{\\mu})_{2}=\\mu_{1}$ , and let $\\mathcal{E}=\\{J\\neq i^{*}(\\pmb{\\mu})\\}$ , the event that the recommendation of the algorithm is wrong. Substituting $\\lambda$ with $\\nu(\\mu)$ from the above equation of Lemma 5 leads to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left[\\frac{(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma^{2}}\\tau\\right]\\geq d\\bigl(\\mathrm{PoE}(\\pi;\\mu),1-\\mathrm{PoE}(\\pi;\\nu)\\bigr)\\geq\\log\\frac{2}{2.4\\bigl(\\mathrm{PoE}(\\pi;\\mu)+\\mathrm{PoE}(\\pi;\\nu)\\bigr)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the first inequality comes from the fact that $\\mathcal{E}$ , the failure event of the bandit model $\\pmb{\\mu}$ , is exactly a success event of $\\nu(\\mu)$ in this two-armed case, and the last inequality is from our modified lemma (Lemma 11) from Eq. (3) of Kaufmann et al. [2016a]. One can rewrite the above inequality as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{PoE}(\\pi;\\pmb{\\mu})+\\mathrm{PoE}(\\pi;\\pmb{\\nu})}{2}\\geq\\frac{1}{2.4}\\exp\\left(\\mathbb{E}_{\\pmb{\\mu}}\\left[-\\frac{(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma^{2}}\\tau\\right]\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can rewrite the conditions of $\\mathcal{A}^{b}(\\delta,\\pmb{H},N_{V})$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{PoE}(\\pi;H)=\\int_{\\mu\\in\\mathbb{R}^{2}}\\operatorname{PoE}(\\pi;\\mu)\\,\\mathrm{d}H(\\mu)\\leq\\delta\\qquad{\\mathrm{and}}\\quad\\int_{\\mu\\in\\mathbb{R}^{2}}\\mathbb{E}_{\\mu}[\\tau]\\,\\mathrm{d}H(\\mu)\\leq N_{V}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using Eq. (4) and with some symmetry tricks, we get $V_{0}\\le\\mathrm{PoE}(\\pi;H)$ where ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{0}:=\\int_{\\mu\\in\\mathbb{R}^{2}}\\frac{1}{2.4}\\exp\\left(-\\frac{(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma^{2}}\\mathbb{E}_{\\mu}[\\tau]\\right)\\mathrm{d}H(\\mu)\\leq\\delta\\,\\mathrm{~and~}\\int_{\\mu\\in\\mathbb{R}^{2}}\\mathbb{E}_{\\mu}[\\tau]\\,\\mathrm{d}H(\\mu)\\leq N_{V}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Input: Confidence level $\\delta$ , prior $\\pmb{H}$   \n$\\begin{array}{r}{\\bar{\\Delta\\bar{\\mathbf{\\rho}}_{0}}:=\\frac{\\delta}{4L(H)}}\\end{array}$   \nInitialize the candidate of best arms $\\boldsymbol{A}(1)=[K]$ .   \n$t=1$   \nwhile True do Draw each arm in $\\boldsymbol{A}(t)$ once. $t\\leftarrow t+|A(t)|$ . for $i\\in\\mathcal{A}(t)$ do Compute $\\mathrm{UCB}(i,t)$ and $\\mathrm{LCB}(i,t)$ from (5). if $\\mathrm{UCB}(i,t)\\leq\\operatorname*{max}_{j}\\mathrm{LCB}(j,t)$ then $\\mathcal{A}(t)\\gets\\mathcal{A}(t)\\setminus\\{i\\}$ . end if end for if $|\\mathcal{A}(t)|=1$ then Return arm $J$ in $\\boldsymbol{A}(t)$ . end if Compute $\\hat{\\Delta}^{\\mathrm{safe}}(t):=\\operatorname*{max}_{i\\in\\mathcal{A}(t)}\\mathrm{UCB}(i,t)-\\operatorname*{max}_{i\\in\\mathcal{A}(t)}\\mathrm{LCB}(i,t).$ if $\\hat{\\Delta}^{\\mathrm{safe}}(t)\\leq\\Delta_{0}$ then Return arm $J$ , uniformly sampled from $\\boldsymbol{A}(t)$ . end if   \nend while ", "page_idx": 6}, {"type": "text", "text": "Note that on the above two inequalities, only $\\mathbb{E}_{\\mu}[\\tau]$ is the value that depends on the algorithm $\\pi$ . Now our main idea is that we can relax these two inequalities to the following optimization problem by substituting $\\mathbb{E}_{\\mu}[\\tau]$ to an arbitrary $\\tilde{n}:\\mathbb{R}^{2}\\rightarrow[0,\\infty)$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV:=\\operatorname*{inf}_{\\substack{{\\tilde{n}}:\\mathbb{R}^{2}\\rightarrow[0,\\infty)}}\\int_{\\mu\\in\\mathbb{R}^{2}}\\exp\\left(-\\frac{(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma^{2}}\\tilde{n}(\\mu)\\right)\\mathrm{d}H(\\mu)\\,\\,\\,\\mathrm{s.t.}\\,\\int_{\\mu\\in\\mathbb{R}^{2}}\\tilde{n}(\\mu)\\,\\mathrm{d}H(\\mu)\\leq N_{V}\\,\\mathrm{d}\\sigma_{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and $V\\leq2.4V_{0}\\leq2.4\\delta$ . Let $\\begin{array}{r}{\\mathcal{N}:=\\{\\pmb{\\mu}\\in\\mathbb{R}^{2}:|\\mu_{1}-\\mu_{2}|<\\Delta:=\\frac{8\\delta}{L(\\pmb{H})}\\}}\\end{array}$ . From the discussions in Section 3.1, one might notice that $\\mathcal{N}$ is an important region for bounding $\\mathbb{E}_{\\mu\\sim H}[\\tau]$ . We can relax the above (Opt2) to the following version, which focuses more on $\\mathcal{N}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nV^{\\prime}:=\\operatorname*{inf}_{\\substack{\\tilde{n}\\colon\\mathbb{R}^{2}\\to[0,\\infty)}}\\int_{\\mu\\in N}\\exp\\left(-\\frac{(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma^{2}}\\tilde{n}(\\mu)\\right)\\mathrm{d}H(\\mu)\\;\\mathrm{s.t.}\\;\\int_{\\mu\\in N}\\tilde{n}(\\mu)\\,\\mathrm{d}H(\\mu)\\leq N_{V}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "One can prove $V^{\\prime}\\,\\leq\\,V\\,\\leq\\,2.4\\delta$ . Now, if we notice that function $\\begin{array}{r}{x\\,\\mapsto\\,\\exp\\left(-\\frac{(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma^{2}}x\\right)}\\end{array}$ is a convex function, we can use Jensen\u2019s inequality to verify that the optimal solution for (Opt3) is when $\\begin{array}{r}{\\tilde{n}=\\frac{N_{V}}{\\mathbb{E}_{\\mu\\sim H}\\left[\\mathbf{1}_{\\mathcal{N}}\\right]}\\mathbf{1}_{\\mathcal{N}}}\\end{array}$ , and when we use $N_{V}$ in Theorem 4, one can get: $\\begin{array}{r l r}&{}&{V^{\\prime}\\geq\\displaystyle\\int_{\\mu\\in\\mathcal{N}}\\exp\\left(-\\frac{\\Delta^{2}}{2\\sigma^{2}}\\cdot\\left(\\frac{N_{V}}{\\mathbb{E}_{\\mu\\sim H}[\\mathbf{1}_{N}]}\\right)\\right)\\mathrm{d}H(\\mu)\\quad\\quad\\quad\\mathrm{(by~optimality~of~}\\tilde{n}=\\frac{N_{V}}{\\mathbb{E}_{\\mu\\sim H}[\\mathbf{1}_{N}]})}\\\\ &{}&{\\geq\\displaystyle\\int_{\\mu\\in\\mathcal{N}}\\exp\\left(-\\frac{\\Delta^{2}\\cdot N_{V}}{\\sigma^{2}\\Delta L(H)}\\right)\\mathrm{d}H(\\mu)\\geq\\exp\\left(-\\frac{\\Delta\\cdot N_{V}}{\\sigma^{2}L(H)}\\right)\\cdot\\Delta L(H)\\quad\\mathrm{(both~by~Lemma~1)}}\\\\ &{}&{\\sim\\texttt{o l s}\\quad\\,\\quad\\quad}\\end{array}$ > 2.4\u03b4, (by definition of $N_{V}$ and $\\Delta$ ) ", "page_idx": 6}, {"type": "text", "text": "which is a contradiction. This means no algorithm satisfies (Opt1), and the proof is completed. ", "page_idx": 6}, {"type": "text", "text": "5 Main algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section introduces our main algorithm (Algorithm 1). In short, our algorithm is a modification of the elimination algorithm with the incorporation of the indifference zone technique. Define ", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r}{\\Delta_{0}:=\\frac{\\delta}{4L(H)}}\\end{array}$ which satisfies the following condition, thanks to Lemma 1: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\pmb{\\mu}\\sim R}\\big(\\mu_{i^{*}(\\pmb{\\mu})}-\\mu_{j^{*}(\\pmb{\\mu})}\\le\\Delta_{0}\\big)\\le\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In each iteration of the while loop of Algorithm 1, the learner selects and observes each arm in the active set. After drawing each arm once, the algorithm calculates the confidence bounds for each arm in the active set using the formula as follows: let $\\operatorname{Conf}(i,t)$ and $\\hat{\\mu}_{i}(t)$ be the confidence width and the empirical mean of arm $i$ at time $t$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Conf}(i,t):=\\sqrt{2\\sigma_{i}^{2}\\frac{\\log(6(N_{i}(t))^{2}/((\\frac{\\delta^{2}}{2K})\\pi^{2}))}{N_{i}(t)}},\\qquad\\hat{\\mu}_{i}(t):=\\sum_{s=1}^{t-1}X_{s}\\mathbf{1}[A_{s}=i].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then the upper and lower confidence bounds of arm $i$ at timestep $t$ , denoted as UCB and LCB respectively, can be defined in the following manner: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{UCB}(i,t):=\\hat{\\mu}_{i}(t)+\\mathrm{Conf}(i,t),\\qquad\\mathrm{LCB}(i,t):=\\hat{\\mu}_{i}(t)-\\mathrm{Conf}(i,t).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This confidence bounds ensure that, with high probability, for all $t\\ \\in\\ [T]$ and $i\\ \\in\\ [K]$ , $\\mu_{i}~\\in$ $(\\mathrm{UCB}(i,t),\\mathrm{LCB}(i,t))$ (See Lemma 15 in Appendix for details). After calculating UCB and LCB, the algorithm eliminates arms with UCB smaller than the largest LCB and maintains only arms that could be optimal in the active set $\\boldsymbol{\\mathcal{A}}$ . Up to this point, it follows the traditional elimination approach. ", "page_idx": 7}, {"type": "text", "text": "The main difference in our algorithm lies in the stopping criterion. At the end of each iteration, the algorithm checks the stopping criterion. Unlike typical elimination algorithms that continue until only one arm remains, we have introduced an additional indifference condition. This condition arises when the suboptimality gap is so small that identifying them would require an excessive number of samples. In such cases, our algorithm stops additional attempts to identify differences between arms in the active set and randomly recommends one from the active set instead. ", "page_idx": 7}, {"type": "text", "text": "Remark 2. In the context of PAC- $(\\epsilon,\\delta)$ identification, Even-Dar et al. [2006, Remark 9] introduced a similar approach. The largest difference is that they use the parameter $\\epsilon$ as a parameter that defines the indifference-zone level, whereas our parameter $\\Delta_{0}$ is spontaneously derived from the prior $\\pmb{H}$ and the confidence level $\\delta$ without specifying the indifference-zone. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6 describes the theoretical guarantee of the Algorithm 1. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6. For $\\begin{array}{r}{\\delta\\,<\\,4L({\\pmb H})\\cdot\\operatorname*{min}\\biggl(\\frac{L({\\pmb H})}{\\sum_{i\\in[k]}\\frac{k-1}{\\xi_{i}}},\\Bigl(\\operatorname*{min}_{i,j\\in[k]}\\xi_{i}L_{i j}({\\pmb H})\\Bigr)^{2}\\biggr)}\\end{array}$ mini,j\u2208[k] \u03beiLij(H) 2 , Algorithm 1 which consists of $\\left(\\left(A_{t}\\right),\\tau,J\\right)$ has the expected stopping time upper bound as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu\\sim H}[\\tau]\\leq C\\cdot\\sigma_{\\operatorname*{max}}^{2}\\frac{L(H)^{2}}{\\delta}\\log\\left(\\frac{L(H)}{\\delta}\\right)+O(\\log\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{C=320\\Big(\\frac{\\pi^{2}}{3}+1\\Big)}\\end{array}$ is a universal constant and $\\sigma_{\\operatorname*{max}}=\\operatorname*{max}_{i\\in[k]}\\sigma_{i}$ . Here, $O(\\log\\delta^{-1})$ is a function of $\\delta$ and $\\pmb{H}$ that is proportional to $\\log\\delta^{-1}$ when we view prior parameters $\\pmb{H}$ as constants. Plus, the strategy defined by Algorithm 1 is in $A^{b}(\\delta,H)$ . ", "page_idx": 7}, {"type": "text", "text": "See Appendix $\\boldsymbol{\\mathrm E}$ for the formal proof of Theorem 6. ", "page_idx": 7}, {"type": "text", "text": "Remark 3. When we compare the lower bound (Theorem 4) with the upper bound of Algorithm 1 (Theorem 6), we can see the algorithm is near-optimal. If we view $\\sigma_{\\mathrm{max}}/\\sigma_{\\mathrm{min}}$ as a constant, the bounds are tight up to a log L(\u03b4H) factor. ", "page_idx": 7}, {"type": "text", "text": "Remark 4. The condition $\\begin{array}{r}{\\delta<4L(\\pmb{H})\\cdot\\operatorname*{min}\\Bigl(L(\\pmb{H})/\\sum_{i\\in[k]}\\frac{k-1}{\\xi_{i}}}\\end{array}$ , $\\begin{array}{r}{\\operatorname*{min}_{i,j\\in[k]}(\\xi_{i}L_{i j}({\\cal H}))^{2}\\biggr)}\\end{array}$ is only for cleaner illustration of the regret bound in Theorem 6. The non-asymptotic result, when is a moderately large constant, can be found in Appendix E.1. ", "page_idx": 7}, {"type": "text", "text": "Proof sketch of Theorem 6 We summarize the general strategy for the proof as follows. By the law of total expectation, $\\mathbb{E}_{\\mu\\sim H}[\\tau]=\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}_{\\mu}[\\tau]\\Big]$ . Therefore, we first derive a frequentist upper bound of $\\mathbb{E}_{\\mu}[\\tau]$ , and then marginalize it to obtain the expected Bayesian stopping time. ", "page_idx": 7}, {"type": "text", "text": "First, with the confidence bound defined as Eq. (5) we have the following guarantee that the true means for all arms are in the confidence bound interval with high probability. ", "page_idx": 7}, {"type": "text", "text": "Lemma 7. For any fixed $\\pmb{\\mu}\\in\\{\\mathbf{v}\\in\\mathbb{R}^{k}:v_{i}\\neq v_{j}$ for all $i,j\\in[k]\\}$ , let $\\mathcal{X}(\\pmb{\\mu}):=\\{\\forall i\\in[k]$ and $t\\in$ $\\mathbb{N}$ , $\\mu_{i}\\in(\\mathrm{LCB}(i,t),\\mathrm{UCB}(i,t))\\}$ . Then, $\\mathbb{P}_{\\mu}\\big[\\mathcal{X}(\\mu)\\big]\\geq1-\\delta^{2}$ . ", "page_idx": 8}, {"type": "text", "text": "Now we can rewrite $\\mathbb{E}_{\\mu\\sim H}\\left[\\tau\\right]$ as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\mu\\sim H}\\left[\\tau\\right]=\\mathbb{E}_{\\mu\\sim H}\\left[\\mathbb{E}_{\\mu}[\\tau]\\right]}}&{}&{\\left(\\mathrm{Law~of~Total~}\\right)}\\\\ &{}&{\\quad=\\mathbb{E}_{\\mu\\sim H}\\left[\\mathbb{E}_{\\mu}[\\tau\\mathbf{1}[\\mathcal{X}(\\mu)]]\\right]+\\mathbb{E}_{\\mu\\sim H}\\left[\\mathbb{E}_{\\mu}[\\tau\\mathbf{1}[\\mathcal{X}(\\mu)^{c}]]\\right]}\\\\ &{}&{\\quad=\\displaystyle\\sum_{i}\\mathbb{E}_{\\mu\\sim H}\\left[\\mathbb{E}_{\\mu}[N_{i}(\\tau)\\mathbf{1}[\\mathcal{X}(\\mu)]]\\right]+\\mathbb{E}_{\\mu\\sim H}\\left[\\mathbb{E}_{\\mu}[\\tau\\mathbf{1}[\\mathcal{X}(\\mu)^{c}]]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "$\\Delta_{i}=\\Delta_{i}(\\pmb{\\mu}):=(\\operatorname*{max}_{s\\in[k]}\\mu_{s})-\\mu_{i}$ $\\begin{array}{r}{R_{0}(\\Delta)\\approx\\lceil C\\sigma_{\\operatorname*{max}}^{2}\\cdot\\frac{\\log\\Delta^{-1}}{\\Delta^{2}}\\rceil}\\end{array}$ .x  EF)o,r  atnhde  ifnirtset gtreartme ,i tu nodveerr   \n$\\mathcal{X}(\\pmb{\\mu})$ $N_{i}(\\tau)$ $R_{0}(\\operatorname*{max}(\\Delta_{0},\\Delta_{i}))$   \nthe prior distribution to obtain the leading factor. For the second term, thanks to the indifference   \nstopping condition $(\\hat{\\Delta}^{\\mathrm{safe}}(t)\\leq\\Delta_{0})$ , one can prove that $\\tau$ is always smaller than $R(\\Delta_{0})$ (Lemma 14   \nin Appendix E), which leads to non-leading term. ", "page_idx": 8}, {"type": "text", "text": "To check that the expected probability of error is below $\\delta$ , we have an additional lemma: ", "page_idx": 8}, {"type": "text", "text": "Lemma 8 (Probability of dropping $i^{*}(\\mu)$ ). For any $\\pmb{\\mu}_{0}$ , under $\\begin{array}{r}{\\mathcal{H}_{\\mu_{0}},\\mathcal{X}(\\pmb{\\mu}_{0})\\subset\\bigcap_{t}\\left\\{i^{*}(\\pmb{\\mu}_{0})\\in\\mathcal{A}(t)\\right\\}.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "This lemma means under the event $\\mathcal{X}(\\pmb{\\mu})$ , the best arm is never dropped. We can also prove that under the event $\\mathcal{X}(\\pmb{\\mu})$ , if $\\Delta_{i}(\\pmb{\\mu})>\\Delta_{0}$ , the sub-optimal arm will eventually be dropped before the algorithm terminates (Lemma 14 in Appendix E). These two facts mean there are only two cases in which the prediction of Algorithm 1 could be wrong. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Under $\\chi(\\pmb{\\mu})^{c}$ , both facts cannot guarantee the correct identification. From Lemma 7, $\\mathbb{P}_{\\mu}[\\mathcal{X}(\\pmb{\\mu})^{c}]\\leq\\delta^{2}$ for all $\\pmb{\\mu}$ , and thus $\\mathbb{P}_{\\mu\\sim H}[\\mathcal{X}(\\pmb{\\mu})^{c}]\\leq\\delta^{2}$ .   \n\u2022 When $\\Delta_{i}(\\pmb{\\mu})\\leq\\Delta_{0}$ . From Lemma 1 and the definition of $\\Delta_{0}$ , the probability of drawing such $\\pmb{\\mu}$ from the prior is at most $\\delta/2$ . ", "page_idx": 8}, {"type": "text", "text": "Therefore, by union bound, Algorithm 1 has the expected probability of misidentification guarantee smaller than $\\delta^{2}+\\delta/2<\\delta$ . ", "page_idx": 8}, {"type": "text", "text": "6 Simulation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct two experiments to demonstrate that the expected stopping times of frequentist $\\delta$ -correct algorithms diverge in a Bayesian setting and that the elimination process in Algorithm 1 is necessary for more efficient sampling. In Tables 1 and 2, each column \u2018Avg\u2019, \u2018Max\u2019, and \u2018Error\u2019 represents the average stopping time, maximum stopping time, and the ratio of the misidentification, respectively.5 More details of these experiments are in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "Frequentist algorithms diverge in Bayesian Setting We evaluate the empirical performance of our Elimination algorithm (Algorithm 1) by comparing it with other frequentist algorithms such as Top-two Thompson Sampling (TTTS) [Russo, 2016] and Top-two UCB (TTUCB) [Jourdan and Degenne, 2022b]. ", "page_idx": 8}, {"type": "text", "text": "We design an experiment setup that has $k=2$ arms with standard Gaussian prior distribution, which means $m_{i}\\,=\\,0,\\xi_{i}\\,=\\,1$ for all $i~\\in~[k]$ . We set $\\delta\\:=\\:0.1$ and ran $N\\,=\\,1000$ Bayesian FC-BAI simulations to estimate the expected stopping time and success rate. ", "page_idx": 8}, {"type": "text", "text": "In Table 1, one can see that the two top-two algorithms exhibit very large maximum stopping time. This supports our theoretical result in Section 3 that the expected stopping time of Frequentist $\\delta$ - correct algorithms will diverge in the Bayesian setting. We did not check the track and stop algorithm [Garivier and Kaufmann, 2016] because it needs to solve an optimization for each round, but the fact that the expected stopping time of the track and stop is at least half of the TTTS and TTUCB for a small $\\delta$ implies that the performance of track and stop is similar to that of top-two algorithms. Algorithm 1 shows a significantly smaller average stopping time as well as an average computation time than that of these algorithms. ", "page_idx": 8}, {"type": "table", "img_path": "hFTye9Ge40/tmp/4ec92e18402408a7615eb710c6504c86cf238eec4694b272d188f6e5e1f22d82.jpg", "table_caption": ["Table 1: Comparison of two top-two algorithms and Algorithm 1. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "hFTye9Ge40/tmp/c8cb3e8834863e03368e5546d634637985bf48a132efc58b5dfa324d05a86167.jpg", "table_caption": ["Table 2: Comparison of Algorithm 1 and the no-elimination version of it. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Effect of the elimination process We implemented the modification of Algorithm 1 (denoted as NoElim) that never eliminates an arm from $\\bar{\\mathcal{A}}(t)$ 6 In this setup, we have $k=10$ arms with standard Gaussian prior distribution, which means $m_{i}=0,\\xi_{i}=1$ for all $i\\in[k]$ . We set $\\delta=0.01$ and ran $N=1000$ Bayesian FC-BAI simulations. ", "page_idx": 9}, {"type": "text", "text": "As one can check from Table 2, elimination of arms helps the efficient use of samples and reduces stopping time and computation time. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion and future works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have considered the Gaussian Bayesian best arm identification with fixed confidence. We show that the traditional Frequentist FC-BAI algorithms do not stop in finite time in expectation, which implies the suboptimality of such algorithms in the Bayesian FC-BAI problem. We have established $\\Omega\\big(\\frac{L(H)^{2}}{\\delta}\\big)$ . Moreover, we have introduced the elimination and early stopping algorithm, which achieves a matching stopping time up to a polylogarithmic factor of $L(H)$ and $\\delta$ . We conduct simulations to support our results. ", "page_idx": 9}, {"type": "text", "text": "In the future, we will attempt to tighten the logarithmic and $\\left({\\frac{\\operatorname*{max}_{i}\\,\\sigma_{i}}{\\operatorname*{min}_{i}\\,\\sigma_{i}}}\\right)^{2}$ gap between the lower and upper bound, extend the indifference zone strategy for other traditional BAI algorithms in the Bayesian setting, extend our analysis from Gaussian bandit instances to general exponential families, and design a robust algorithm against misspecified priors. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "K. Jang acknowledge the financial support from the MUR PRIN grant 2022EKNE5K (Learning in Markets and Society), the FAIR (Future Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRR-PE-AI scheme, and the the EU Horizon CL4-2022- HUMAN-02 research and innovation action under grant agreement 101120237, project ELIAS (European Lighthouse of AI for Sustainability). ", "page_idx": 9}, {"type": "text", "text": "J. Komiyama was supported by NYU Stern School of Business Research Scholars Fund no. 10-83004- BF478. ", "page_idx": 9}, {"type": "text", "text": "K. Yamazaki was supported by JSPS KAKENHI grant no. JP20K03758, JP24K06844 and JP24H00328 and the start-up grant by the School of Mathematics and Physics of the University of Queensland. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Alexia Atsidakou, Sumeet Katariya, Sujay Sanghavi, and Branislav Kveton. Bayesian fixed-budget best-arm identification, 2023. ", "page_idx": 9}, {"type": "text", "text": "Jean-Yves Audibert, S\u00e9bastien Bubeck, and R\u00e9mi Munos. Best arm identification in multiarmed bandits. In Conference on Learning Theory, pages 41\u201353, 2010. URL http:// colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page $=\\mathrel{49}$ . ", "page_idx": 9}, {"type": "text", "text": "Robert E Bechhofer. A single-sample multiple decision procedure for ranking means of normal populations with known variances. The Annals of Mathematical Statistics, pages 16\u201339, 1954. ", "page_idx": 9}, {"type": "text", "text": "Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7:1079\u20131105, 2006.   \nPeter I. Frazier. A fully sequential elimination procedure for indifference-zone ranking and selection with tight bounds on probability of correct selection. Operations Research, 62(4):926\u2013942, 2014. doi: 10.1287/opre.2014.1282. URL https://doi.org/10.1287/opre.2014.1282.   \nPeter I. Frazier. A tutorial on bayesian optimization. CoRR, abs/1807.02811, 2018. URL http: //arxiv.org/abs/1807.02811.   \nVictor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00e9on Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 3221\u20133229, 2012. URL https://proceedings.neurips.cc/paper/2012/hash/ 8b0d268963dd0cfb808aac48a549829f-Abstract.html.   \nAur\u00e9lien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In Conference on Learning Theory, pages 998\u20131027. PMLR, 2016.   \nShanti S. Gupta. Selection and ranking procedures: a brief introduction. Communications in Statistics - Theory and Methods, 6(11):993\u20131001, 1977. doi: 10.1080/03610927708827548. URL https://doi.org/10.1080/03610927708827548.   \nL Jeff Hong, Weiwei Fan, and Jun Luo. Review on ranking and selection: A new perspective. Frontiers of Engineering Management, 8(3):321\u2013343, 2021.   \nKevin G. Jamieson and Robert D. Nowak. Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting. In 48th Annual Conference on Information Sciences and Systems, CISS 2014, Princeton, NJ, USA, March 19-21, 2014, pages 1\u20136. IEEE, 2014. doi: 10.1109/CISS.2014.6814096. URL https://doi.org/10.1109/CISS.2014.6814096.   \nKevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, volume 51 of JMLR Workshop and Conference Proceedings, pages 240\u2013248. JMLR.org, 2016. URL http://proceedings.mlr.press/v51/jamieson16.html.   \nMarc Jourdan and R\u00e9my Degenne. Non-asymptotic analysis of a ucb-based top two algorithm. CoRR, abs/2210.05431, 2022a. doi: 10.48550/ARXIV.2210.05431. URL https://doi.org/10.48550/ arXiv.2210.05431.   \nMarc Jourdan and R\u00e9my Degenne. Non-asymptotic analysis of a ucb-based top two algorithm. arXiv preprint arXiv:2210.05431, 2022b.   \nMarc Jourdan, R\u00e9my Degenne, Dorian Baudry, Rianne de Heide, and Emilie Kaufmann. Top two algorithms revisited. Advances in Neural Information Processing Systems, 35:26791\u201326803, 2022.   \nMarc Jourdan, R\u00e9my Degenne, and Emilie Kaufmann. An varepsilon-best-arm identification algorithm for fixed-confidence and beyond. arXiv preprint arXiv:2305.16041, 2023.   \nHideaki Kano, Junya Honda, Kentaro Sakamaki, Kentaro Matsuura, Atsuyoshi Nakamura, and Masashi Sugiyama. Good arm identification via bandit feedback. Mach. Learn., 108(5):721\u2013745, 2019. doi: 10.1007/S10994-019-05784-4. URL https://doi.org/10.1007/s10994-019- 05784-4.   \nEmilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selection. In Shai Shalev-Shwartz and Ingo Steinwart, editors, COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, volume 30 of JMLR Workshop and Conference Proceedings, pages 228\u2013251. JMLR.org, 2013. URL http://proceedings.mlr.press/v30/Kaufmann13.html.   \nEmilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of a/b testing. In Conference on Learning Theory, pages 461\u2013481. PMLR, 2014.   \nEmilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best-arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17(1):1\u201342, 2016a.   \nEmilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best arm identification in multi-armed bandit models, 2016b.   \nJunpei Komiyama, Kaito Ariu, Masahiro Kato, and Chao Qin. Rate-optimal bayesian simple regret in best arm identification. Mathematics of Operations Research, Ahead of Print, 2023. doi: 10.1287/moor.2022.0011. URL https://doi.org/10.1287/moor.2022.0011.   \nTze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statistics, 15(3):1091 \u2013 1114, 1987. doi: 10.1214/aos/1176350495. URL https://doi.org/ 10.1214/aos/1176350495.   \nAndrea Locatelli, Maurilio Gutzeit, and Alexandra Carpentier. An optimal algorithm for the thresholding bandit problem. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 1690\u20131698. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/locatelli16.html.   \nOded Maron and Andrew W. Moore. Hoeffding races: Accelerating model selection search for classification and function approximation. In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, Advances in Neural Information Processing Systems 6, [7th NIPS Conference, Denver, Colorado, USA, 1993], pages 59\u201366. Morgan Kaufmann, 1993. URL http://papers.nips.cc/paper/841-hoeffding-races-acceleratingmodel-selection-search-for-classification-and-function-approximation.   \nJ. Mockus. Bayesian Approach to Global Optimization: Theory and Applications. Mathematics and its Applications. Springer Netherlands, 2012. ISBN 9789400909090. URL https:// books.google.fr/books?id=VuKoCAAAQBAJ.   \nEdward Paulson. A sequential procedure for selecting the population with the largest mean from $k$ normal populations. The Annals of Mathematical Statistics, 35(1):174 \u2013 180, 1964. doi: 10.1214/aoms/1177703739. URL https://doi.org/10.1214/aoms/1177703739.   \nChao Qin, Diego Klabjan, and Daniel Russo. Improving the expected improvement algorithm. In Advances in Neural Information Processing Systems, volume 30, pages 5381\u20135391, 2017a.   \nChao Qin, Diego Klabjan, and Daniel Russo. Improving the expected improvement algorithm. Advances in Neural Information Processing Systems, 30, 2017b.   \nHerbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527 \u2013 535, 1952.   \nDaniel Russo. Simple bayesian algorithms for best arm identification. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 1417\u20131418. PMLR, 23\u201326 Jun 2016.   \nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proc. IEEE, 104(1):148\u2013175, 2016. doi: 10.1109/JPROC.2015.2494218. URL https://doi.org/10.1109/JPROC.2015.2494218.   \nXuedong Shang, Rianne Heide, Pierre Menard, Emilie Kaufmann, and Michal Valko. Fixedconfidence guarantees for bayesian best-arm identification. In International Conference on Artificial Intelligence and Statistics, pages 1823\u20131832. PMLR, 2020.   \nNiranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Johannes F\u00fcrnkranz and Thorsten Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 1015\u20131022. Omnipress, 2010. URL https://icml.cc/Conferences/2010/papers/422.pdf.   \nKoji Tabata, Atsuyoshi Nakamura, Junya Honda, and Tamiki Komatsuzaki. A bad arm existence checking problem: How to utilize asymmetric problem structure? Mach. Learn., 109(2):327\u2013 372, 2020. doi: 10.1007/S10994-019-05854-7. URL https://doi.org/10.1007/s10994-019- 05854-7.   \nKoji Tabata, Junpei Komiyama, Atsuyoshi Nakamura, and Tamiki Komatsuzaki. Posterior tracking algorithm for classification bandits. In Francisco J. R. Ruiz, Jennifer G. Dy, and Jan-Willem van de Meent, editors, International Conference on Artificial Intelligence and Statistics, 25-27 April 2023, Palau de Congressos, Valencia, Spain, volume 206 of Proceedings of Machine Learning Research, pages 10994\u201311022. PMLR, 2023. URL https://proceedings.mlr.press/v206/ tabata23a.html.   \nWilliam R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.   \nYichong Xu, Xi Chen, Aarti Singh, and Artur Dubrawski. Thresholding bandit problem with both duels and pulls. CoRR, abs/1910.06368, 2019. URL http://arxiv.org/abs/1910.06368.   \nYao Zhao, Connor Stephens, Csaba Szepesv\u00e1ri, and Kwang-Sung Jun. Revisiting simple regret: Fast rates for returning a good arm. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 42110\u201342158. PMLR, 2023. URL https://proceedings.mlr.press/v202/zhao23g.html. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notation table ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "hFTye9Ge40/tmp/2c606f3466e85991cbae8846a61d626b4daf92ae4ee16dd101b8689317000c4d.jpg", "table_caption": ["Table 4: Notations for the lower bound proof, Section D "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "$R$ $\\overline{{e^{-4}}}$   \n$\\tilde{\\Delta}$ L(H)\u03b4   \n$\\nu(\\mu)$ alternative model where the top-two means are swapped   \n$\\begin{array}{r l}{n_{i}(\\mu)}&{\\;\\;\\;\\mathbb{E}_{\\mu}|(N_{i}(\\tau)]}\\\\ {D_{0}(H)}&{\\;\\;\\Bigg\\{W\\big(-\\frac{1}{32\\operatorname*{max}_{i\\in[k]}\\xi_{i}^{3/2}}\\big)}\\quad\\mathrm{If~max}_{i\\in[k]}\\;\\xi_{i}>\\sqrt[3]{\\frac{e^{2}}{2^{10}}}\\mathrm{~(}W\\mathrm{~is~the~Lamb}}\\\\ &{\\;\\;\\Big\\}1}\\\\ {D_{1}(H)}&{\\;\\operatorname*{min}_{i\\neq j}\\left[\\left|\\frac{m_{i}}{\\sigma_{i}^{2}}-\\frac{m_{j}}{\\sigma_{j}^{2}}\\right|^{-1},\\left[\\frac{1}{2\\sigma_{i}^{2}}+\\frac{1}{2\\sigma_{j}^{2}}\\right]^{-2}\\right]}\\\\ {\\delta_{L}(H)}&{\\;\\;\\frac{L(H)}{32e^{4}}\\cdot\\operatorname*{min}\\biggl(D_{0}(H),D_{1}(H),\\operatorname*{min}_{i\\in[k]}\\frac{1}{4m_{i}^{2}},\\frac{L(H)}{4(k-1)\\sum_{i\\in[k]}\\frac{1}{\\xi_{i}}}\\biggr)}\\end{array}$ ert W function.) ", "page_idx": 13}, {"type": "text", "text": "B Proof of Lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We will use the following formal version of the volume lemma for the proof. The first result is used for the upper bound, and the second result is used for the lower bound. ", "page_idx": 13}, {"type": "text", "text": "Lemma 9 (Volume Lemma, formal). Let $\\Theta_{i j}:=\\{\\pmb{\\mu}\\in\\mathbb{R}^{k}:i^{*}(\\pmb{\\mu})=i,j^{*}(\\pmb{\\mu})=j\\}.$ ", "page_idx": 13}, {"type": "text", "text": "1. For any $\\Delta\\in(0,1)$ , define ", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{i j}(\\pmb{H},\\Delta):=\\frac{1}{\\delta}\\int_{\\Theta_{i j}}\\pmb{1}[|\\mu_{i}-\\mu_{j}|\\le\\Delta]\\,\\mathrm{d}\\pmb{H}(\\pmb{\\mu}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Table 5: Notations for the upper bound proof, Section E definition ", "page_idx": 14}, {"type": "text", "text": "$\\overline{{\\mathrm{Conf}(i,t)}}$ confidence bound (Section 5)   \nUCB(i, t), LCB(i, t) upper and lower confidence bounds (Section 5)   \n\u2206\u02c6safe(t) $\\Delta_{0}$ $\\begin{array}{r l}&{\\underset{\\frac{\\delta}{4L}(H)}{\\operatorname*{max}}_{i\\in A(t)}\\operatorname{UCB}(i,t)-\\operatorname*{max}_{i\\in A(t)}\\operatorname{LCB}(i,t)}\\\\ &{\\frac{\\delta}{4L(H)}}\\\\ &{\\operatorname*{min}\\biggl(\\log\\frac{4\\sqrt{k}}{\\delta\\pi},\\frac{1}{B}\\biggr)}\\\\ &{B\\frac{\\log\\operatorname*{min}(\\Delta,\\Delta_{t h r})^{-1}}{\\operatorname*{min}(\\Delta,\\Delta_{t h r})^{2}}}\\\\ &{k R_{0}(\\Delta_{0})}\\\\ &{\\mu_{i^{*}(\\mu)}-\\mu_{s}}\\\\ &{\\operatorname{Event}\\bigcap_{i\\in[k]}\\left[\\left(\\bigcap_{t=1}^{\\infty}\\left\\{\\operatorname{LCB}(i,t)\\leq\\mu_{i}\\right\\}\\right)\\bigcap\\left(\\bigcap_{t=1}^{\\infty}\\left\\{\\operatorname{UCB}(i,t)\\geq\\mu_{i}\\right\\}\\right)\\right].\\ (\\mathrm{Eq}.}\\end{array}$   \n\u2206thr   \n$R_{0}(\\Delta)$   \n$T_{0}$   \n$\\Delta_{s}(\\pmb{\\mu})$   \n$\\mathcal{X}(\\pmb{\\mu})$ (24)) ", "page_idx": 14}, {"type": "text", "text": "Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{i j}(\\pmb{H},\\Delta)\\in\\bigg[L_{i j}(\\pmb{H})-\\frac{1}{\\xi_{i}}\\Delta,L_{i j}(\\pmb{H})+\\frac{1}{\\xi_{i}}\\Delta\\bigg].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, when $\\begin{array}{r}{\\Delta<\\frac{L(H)}{\\sum_{i\\in[k]}\\frac{k-1}{\\xi_{i}}},L(H,\\Delta)\\leq2L(H)}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "2. (Volume lemma for the lower bound) For small enough positive real number $\\Delta\\_$ $\\begin{array}{r}{\\operatorname*{min}\\biggl[\\frac{1}{4\\operatorname*{max}_{i\\in[k]}m_{i}^{2}},D_{0}(\\pmb{H})\\biggr]^{\\intercal}}\\end{array}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{i j}^{\\prime}(H,\\Delta):=\\frac{1}{\\Delta}\\int_{\\Theta_{i j}}\\mathbf{1}[|\\mu_{i}-\\mu_{j}|\\leq\\Delta]\\mathbf{1}[|\\mu_{i}|,|\\mu_{j}|\\leq\\frac{1}{\\sqrt{\\Delta}}]\\,\\mathrm{d}H(\\mu).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{i j}^{\\prime}(H,\\Delta)\\in\\left[L_{i j}(H)-\\frac{2}{\\xi_{i}}\\Delta,L_{i j}(H)+\\frac{1}{\\xi_{i}}\\Delta\\right]\\!.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[{\\textstyle{\\frac{1}{2}}}L(\\pmb{H}),2L(\\pmb{H})\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. First, let us prove the upper bound of Eq. (8), i.e. $\\begin{array}{r}{L_{i j}(H,\\delta)\\leq L_{i j}(H)+\\frac{1}{\\xi_{i}}\\Delta.}\\end{array}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\Theta_{i j}}{\\bf1}\\left[|\\mu_{i}-\\mu_{j}|\\leq\\Delta\\right]\\mathrm{d}H(\\mu)=\\int_{-\\infty}^{\\infty}\\int_{\\mu_{j}}^{\\mu_{j}+\\Delta}h_{i}(\\mu_{i})\\,\\mathrm{d}\\mu_{i}\\,\\prod_{k\\ne i,j}\\int_{-\\infty}^{\\mu_{j}}h_{k}(\\mu_{k})\\,\\mathrm{d}\\mu_{k}h_{j}(\\mu_{j})\\,\\mathrm{d}\\mu_{j}}}\\\\ &{}&{\\leq\\int_{-\\infty}^{\\infty}\\left(\\Delta\\underset{0\\leq y\\leq\\Delta}{\\operatorname*{max}}\\,h_{i}(\\mu_{j}+y)\\right)\\underset{k\\ne i,j}{\\prod_{k\\ne i,j}}\\int_{-\\infty}^{\\mu_{j}}h_{k}(\\mu_{k})\\,\\mathrm{d}\\mu_{k}h_{j}(\\mu_{j})\\,\\mathrm{d}\\mu_{j}}\\\\ &{}&{\\leq\\Delta\\int_{-\\infty}^{\\infty}\\bigg[h_{i}(\\mu_{j})+\\frac{e^{-1/2}}{\\xi_{i}}\\Delta\\bigg]\\,\\underset{k\\ne i,j}{\\prod_{k\\ne i,j}}\\int_{-\\infty}^{\\mu_{j}}h_{k}(\\mu_{k})\\,\\mathrm{d}\\mu_{k}h_{j}(\\mu_{j})\\,\\mathrm{d}\\mu_{j}\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(by the Lipschitz property of the Gaussian density, $e^{-1/2}/\\xi_{i}$ is the steepest slope of $N(m_{i},\\xi_{i}^{2}))$ ) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\leq\\Delta\\left(\\underbrace{\\int_{-\\infty}^{\\infty}h_{i}(\\mu_{j})\\prod_{k\\neq i,j}\\int_{-\\infty}^{\\mu_{j}}h_{k}(\\mu_{k})\\,\\mathrm{d}\\mu_{k}h_{j}(\\mu_{j})\\,\\mathrm{d}\\mu_{j}}_{=L_{i j}(H)}+\\left[\\frac{e^{-1/2}}{\\xi_{i}}\\Delta\\right]\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "7See Appendix G for the definition of $D_{0}(H)$ . ", "page_idx": 14}, {"type": "text", "text": "Therefore, we verified the upper bound side of Eq. (8). ", "page_idx": 15}, {"type": "text", "text": "For the lower bound, by following the same steps, one can prove $\\begin{array}{r}{\\int_{\\Theta_{i j}}\\mathbf{1}\\left[\\left|\\mu_{i}-\\mu_{j}\\right|\\leq\\Delta\\right]\\mathrm{d}H(\\mu)\\geq}\\end{array}$ $\\begin{array}{r}{(L_{i j}(H)-\\frac{1}{\\xi_{i}}\\Delta)\\Delta.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "For the proof of Eq. (9), by Chernoff\u2019s method we can bound the tail probability as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{\\Theta_{i j}}{\\bf1}\\left[|\\mu_{i}-m_{i}|>\\frac{1}{\\sqrt{\\Delta}}\\right]\\mathrm{d}H(\\mu)<\\int_{\\mathbb{R}^{k}}{\\bf1}\\left[|\\mu_{i}-m_{i}|>\\frac{1}{\\sqrt{\\Delta}}\\right]\\mathrm{d}H(\\mu)<2\\exp{\\left(-\\frac{1}{2\\Delta\\xi_{i}^{2}}\\right)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $\\begin{array}{r}{|m_{i}|<\\frac{1}{2\\sqrt{\\Delta}}}\\end{array}$ , then we can change the above inequality as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{\\Theta_{i j}}\\mathbf{1}\\left[|\\mu_{i}|>\\frac{1}{\\sqrt{\\Delta}}\\right]\\mathrm{d}H(\\mu)\\le\\int_{\\mathbb{R}^{k}}\\mathbf{1}\\left[|\\mu_{i}|>\\frac{1}{\\sqrt{\\Delta}}\\right]\\mathrm{d}H(\\mu)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\le\\int_{\\mathbb{R}^{k}}\\mathbf{1}\\left[|\\mu_{i}-m_{i}|>\\frac{1}{2\\sqrt{\\Delta}}\\right]\\mathrm{d}H(\\mu)<2\\exp\\left(-\\frac{1}{8\\Delta\\xi_{i}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\Theta_{i j}}\\mathbf{1}[|\\mu_{i}-\\mu_{j}|\\leq\\Delta]\\mathbf{1}[|\\mu_{i}|,|\\mu_{j}|\\leq\\displaystyle\\frac{1}{\\sqrt{\\Delta}}]\\,\\mathrm{d}H(\\mu)\\geq\\int_{\\Theta_{i j}}\\mathbf{1}\\left[|\\mu_{i}-\\mu_{j}|\\leq\\Delta\\right]\\mathrm{d}H(\\mu)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\int_{\\Theta_{i j}}\\mathbf{1}\\left[|\\mu_{i}-m_{i}|>\\displaystyle\\frac{1}{2\\sqrt{\\Delta}}\\right]\\mathrm{d}H(\\mu)-\\int_{\\Theta_{i j}}\\mathbf{1}\\left[|\\mu_{j}-m_{j}|>\\displaystyle\\frac{1}{2\\sqrt{\\Delta}}\\right]\\mathrm{d}H(\\mu)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq(L_{i}(H)-\\displaystyle\\frac{1}{\\xi_{i}}\\Delta)\\Delta-2\\exp\\left(-\\displaystyle\\frac{1}{8\\Delta\\xi_{i}^{2}}\\right)-2\\exp\\left(-\\displaystyle\\frac{1}{8\\Delta\\xi_{j}^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq(L_{i}(H)-\\displaystyle\\frac{2}{\\xi_{i}}\\Delta)\\Delta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(by~\\Delta<D_{0}(H))}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\mathrm{~or~}\\Delta<\\operatorname*{min}\\left[\\operatorname*{min}_{i\\in[k]}\\frac{1}{4m_{i}^{2}},D_{0}(H)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For $\\Delta>0$ , let $\\Theta_{i}(\\Delta):=\\{\\pmb{\\mu}\\in\\mathbb{R}^{k}:i^{\\ast}(\\pmb{\\mu})=i,\\mu_{i}-\\mu_{j^{\\ast}(\\pmb{\\mu})}\\leq\\Delta\\}$ . From Lemma 9, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mathbb{P}_{\\mu\\sim H}\\Big[i^{*}(\\mu)=i,\\mu_{i}-\\mu_{j^{*}(\\mu)}\\leq\\Delta\\Big]=\\sum_{j\\neq i}\\mathbb{P}_{\\mu\\sim H}\\Big[i^{*}(\\mu)=i,j^{*}(\\mu)=j,\\mu_{i}-\\mu_{j}\\leq\\Delta\\Big]}}\\\\ {{\\displaystyle\\geq\\sum_{j\\neq i}\\frac{1}{2}L_{i j}(H)\\Delta.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, for each $\\pmb{\\mu}\\in\\mathbb{R}^{k}$ , let $\\nu:\\mathbb{R}^{k}\\to\\mathbb{R}^{k}$ be a function such that for $s\\in[k]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nu(\\pmb{\\mu})_{s}:=\\left\\{\\begin{array}{l l}{\\mu_{i^{*}(\\pmb{\\mu})}}&{\\mathrm{~when~}s=j^{*}(\\pmb{\\mu})}\\\\ {\\mu_{j^{*}(\\pmb{\\mu})}}&{\\mathrm{~when~}s=i^{*}(\\pmb{\\mu})}\\\\ {\\mu_{s}}&{\\mathrm{~Otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For any $\\pmb{\\mu}\\in\\mathbb{R}^{k}$ , let $\\mathcal{E}(\\pmb{\\mu})=\\{J\\neq i^{*}(\\pmb{\\mu})\\}$ and $\\pmb{\\nu}=\\nu(\\pmb{\\mu})$ . By Lemma 1 in Kaufmann et al. [2016a], for any frequentist $\\delta$ -correct algorithm $\\pi=((A_{t})_{t},\\tau,J)$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{s\\in[k]}\\mathbb{E}_{\\mu}[N_{s}(\\tau)]\\mathrm{KL}_{s}(\\mu_{s}||\\nu_{s})\\geq d(P_{\\mu}(\\mathcal{E}(\\mu)),P_{\\nu}(\\mathcal{E}(\\mu))),\\quad\\mu\\in\\mathbb{R}^{k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the left side, from the construction of $\\pmb{\\nu}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}_{s}(\\mu_{s}||\\nu_{s}):=\\left\\{\\frac{(\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)})^{2}}{2\\sigma_{s}^{2}}\\right.\\ \\ s=i^{*}(\\mu),j^{*}(\\mu)}\\\\ {0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{Otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\pi\\in{\\mathcal{A}}^{f}(\\delta)$ and from the definition of $\\mathcal{E}(\\mu)$ and $\\pmb{\\nu}$ , $\\mathbb{P}_{\\mu}(\\mathcal{E}(\\mu))\\leq\\delta$ and $\\mathbb{P}_{\\nu}\\big(\\mathcal{E}(\\mu)\\big)\\ge1-\\delta$ . Overall, we can rewrite Eq. (10) to the following simpler form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mu}[N_{i^{*}(\\mu)}(\\tau)]\\frac{\\left(\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)}\\right)^{2}}{2\\sigma_{i^{*}(\\mu)}^{2}}+\\mathbb{E}_{\\mu}[N_{j^{*}(\\mu)}(\\tau)]\\frac{\\left(\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)}\\right)^{2}}{2\\sigma_{j^{*}(\\mu)}^{2}}\\geq d(\\delta,1-\\delta)}\\\\ {\\implies\\mathbb{E}_{\\mu}[N_{i^{*}(\\mu)}(\\tau)+N_{j^{*}(\\mu)}(\\tau)]\\geq\\frac{2d(\\delta,1-\\delta)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}}{\\left(\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\begin{array}{r}{\\tau=\\sum_{s=1}^{k}N_{s}(\\tau)}\\end{array}$ , we can lower bound the expected stopping time when $\\pmb{\\mu}$ is given as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}[\\tau]\\geq\\mathbb{E}_{\\mu}[N_{i^{*}(\\mu)}(\\tau)+N_{j^{*}(\\mu)}(\\tau)]\\geq\\frac{2d(\\delta,1-\\delta)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}}{(\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)})^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, when we compute marginal $\\mathbb{E}_{\\mu}[\\tau]$ over $\\pmb{\\mu}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb E_{\\mu\\sim H}\\big[\\tau\\big]=\\mathbb E_{\\mu\\sim H}\\Big[\\mathbb E_{\\mu}\\big[\\tau\\big]\\Big]}}&{}&{\\mathrm{(Law~of~total~expectation)}}\\\\ &{}&{\\geq\\mathbb E_{\\mu\\sim H}\\Big[\\mathbb E_{\\mu}\\big[\\tau\\big]\\mathbf1_{\\Theta_{i}(\\Delta)}\\Big]}\\\\ &{}&{\\geq\\mathbb E_{\\mu\\sim H}\\Bigg[\\frac{2d\\big(\\delta,1-\\delta\\big)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}}{(\\mu_{i^{*}}(\\mu_{i})-\\mu_{j^{*}}(\\mu))^{2}}\\mathbf1_{\\Theta_{i}(\\Delta)}\\Bigg]}\\\\ &{}&{\\geq\\mathbb E_{\\mu\\sim H}\\bigg[\\frac{2d\\big(\\delta,1-\\delta\\big)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}}{\\Delta^{2}}\\mathbf1_{\\Theta_{i}(\\Delta)}\\bigg]}\\\\ &{}&{\\geq\\bigg(\\sum_{j\\ne i}L_{i j}(H)\\bigg)d\\big(\\delta,1-\\delta\\big)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}}\\\\ &{}&{\\qquad+\\left.\\sum_{j\\ne i}L_{i j}(H)\\right)\\!\\!d\\big(\\delta,1-\\delta\\big)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}=\\bigg(\\sum_{j\\ne i}L_{i j}(H)\\bigg)d\\big(\\delta,1-\\delta\\big)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}}\\\\ &{}&{\\qquad+\\left.\\sum_{k=i}^{\\infty}\\!\\bigg[\\frac{2d\\big(\\delta,1-\\delta\\big)\\operatorname*{min}_{s\\in[k]}\\sigma_{s}^{2}}{\\Delta^{2}}\\mathbf1_{\\Theta_{i}(\\Delta)}\\!\\right]\\!\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now since $\\Delta$ is an arbitrary small positive number, we can conclude that $\\mathbb{E}_{\\mu\\sim H}[\\tau]$ diverges. ", "page_idx": 16}, {"type": "text", "text": "D Proof of Theorem 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this subsection, we will prove the following theorem: ", "page_idx": 16}, {"type": "text", "text": "Theorem 10 (Restatement of Theorem 4). Let $\\delta>0$ be sufficiently small such that $\\delta<\\delta_{L}(H)$ . For any best arm identification algorithm, if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{k}}\\left(\\sum_{i\\in[k]}n_{i}(\\pmb{\\mu})\\right)\\mathrm{d}\\pmb{H}(\\pmb{\\mu})\\leq N_{V},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{k}}\\mathbb{P}_{\\pmb{\\mu}}[J\\neq i^{*}(\\pmb{\\mu})]\\,\\mathrm{d}\\pmb{H}(\\pmb{\\mu})\\geq\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. By Lemma 1 in Kaufmann et al. [2016a], for any stopping time $\\tau$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i\\in[k]}\\mathbb{E}_{\\mu}[N_{i}(\\tau)]\\mathrm{KL}_{i}(\\mu_{i}||\\nu_{i})\\geq d\\big(\\mathbb{P}_{\\mu}(\\mathcal{E}(\\mu)),\\mathbb{P}_{\\nu}(\\mathcal{E}(\\mu))\\big),\\quad\\mu\\in\\mathbb{R}^{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To modify the RHS of Eq. (12), we will use the following lemma: ", "page_idx": 16}, {"type": "text", "text": "Lemma 11. For any $p,q^{\\prime},q\\in(0,1)$ such that $q^{\\prime}\\leq q$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ln{\\frac{1}{4(p+q)}}\\leq d(p,1-q^{\\prime}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By using Lemma 11 and the fact that $\\mathbb{P}_{\\nu}(\\mathcal{E}(\\pmb{\\mu}))\\geq1-\\mathbb{P}_{\\nu}[J\\neq i^{\\ast}(\\pmb{\\nu})]$ by definition, we can transform (12) into ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i\\in[k]}\\mathbb{E}_{\\mu}[N_{i}(\\tau)]\\mathrm{KL}_{i}(\\mu_{i}||\\nu_{i})\\geq\\ln\\frac{1}{4(\\mathbb{P}_{\\mu}[J\\neq i^{*}(\\mu)]+\\mathbb{P}_{\\nu}[J\\neq i^{*}(\\nu)])},\\quad\\forall\\mu\\in\\mathbb{R}^{k}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $\\mathbb{P}_{\\mu}(\\mathcal{E}(\\pmb{\\mu}))$ is exactly the error probability, and we are interested in the marginal error probability $\\mathbb{E}_{\\mu\\sim H}[\\mathbb{P}_{\\mu}(\\mathcal{E})]$ . Let $n_{i}(\\pmb{\\mu})=\\mathbb{E}_{\\pmb{\\mu}}[N_{i}(\\tau)]$ , and $\\tilde{\\Delta}$ is an arbitrary small enough positive variable which we will define later on Eq. (19). Then, by rearrangement, we can induce the following inequalities ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{\\mathbb{R}^{k}}\\mathbb{P}_{\\mu}[J\\neq i^{*}(\\mu)]\\,\\mathrm{d}H(\\mu)}\\\\ {\\displaystyle=\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\mathbb{P}_{\\mu}[J\\neq i]\\,\\mathrm{d}H(\\mu)}\\\\ {\\displaystyle=\\frac{1}{2}\\left(\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\mathbb{P}_{\\mu}[J\\neq i]\\,\\mathrm{d}H(\\mu)+\\sum_{j\\in[k]}\\sum_{i\\neq j}\\int_{\\Theta_{j i}}\\mathbb{P}_{\\nu(\\mu)}[J\\neq j]h_{i}(\\mu_{j})h_{j}(\\mu_{i})\\left(\\prod_{s\\neq i,j}h_{s}(\\mu_{s})\\right)\\mathrm{d}s\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(Symmetry of the Lebesgue measure) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{1}{2}\\left(\\displaystyle\\sum_{i\\in[k]}\\displaystyle\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\left[\\mathbb{P}_{\\mu}[J\\neq i]h_{i}(\\mu_{i})h_{j}(\\mu_{j})+\\mathbb{P}_{\\nu(\\mu)}[J\\neq j]h_{i}(\\mu_{j})h_{j}(\\mu_{i})\\right]\\left(\\prod_{s\\neq i,j}h_{s}(\\mu_{s})\\right)\\mathrm{d}\\mu\\right)}\\\\ &{\\ge\\displaystyle\\sum_{i\\in[k]}\\displaystyle\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\frac{\\mathbb{P}_{\\mu}[J\\neq i^{*}(\\mu)]+\\mathbb{P}_{\\nu}[J\\neq i^{*}(\\nu)]}{2}\\operatorname*{min}\\left(h_{i}(\\mu_{i})h_{j}(\\mu_{j}),h_{i}(\\mu_{j})h_{j}(\\mu_{i})\\right)\\mathrm{d}\\mu}\\\\ &{\\ge\\displaystyle\\sum_{i\\in[k]}\\displaystyle(A C+B D\\ge A\\operatorname*{min}(C,D)+B\\operatorname*{min}(C,D)=(A+B)\\operatorname*{min}(C,D)\\operatorname*{for}A,B,C,D>0)}\\\\ &{\\ge\\displaystyle\\sum_{i\\in[k]}\\displaystyle\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\frac{\\exp\\left(-n_{i}(\\mu)\\mathrm{KL}_{i}(\\mu_{i},\\mu_{j})-n_{j}(\\mu)\\mathrm{KL}_{j}(\\mu_{j},\\mu_{i})\\right)\\operatorname*{min}\\left(1,\\frac{h_{i}(\\mu_{j})h_{j}(\\mu_{i})}{h_{i}(\\mu_{i})h_{j}(\\mu_{j})}\\right)h_{i}(\\mu_{i})h_{j}(\\mu_{i})}{8}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\geq e^{-4}\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\frac{\\exp\\left(-n_{i}(\\mu)\\mathrm{KL}_{i}(\\mu_{i},\\mu_{j})-n_{j}(\\mu)\\mathrm{KL}_{j}(\\mu_{j},\\mu_{i})\\right)}{8}\\mathbf{1}\\Bigg[|\\mu_{i}|,|\\mu_{j}|\\leq\\frac{1}{\\sqrt{\\Delta}}\\Bigg]\\,\\mathrm{d}H(\\mu).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the last inequality, we used the following lemma. The proof for this lemma is found in Subsection D.1.2. ", "page_idx": 17}, {"type": "text", "text": "Lemma 12 (Ratio Lemma). For all a, b \u2208R which satisfy |a \u2212b| \u2264D1(H) and |a|, |b| \u2264\u221aD11(H) for some fixed $D_{1}(H)^{8}$ , $\\begin{array}{r}{\\frac{h_{i}\\left(a\\right)h_{j}\\left(b\\right)}{h_{i}\\left(b\\right)h_{j}\\left(a\\right)}\\geq e^{-4}}\\end{array}$ for all $i,j\\in[k]$ . ", "page_idx": 17}, {"type": "text", "text": "In short, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathbb{R}^{k}}\\!\\!\\mathbb{P}_{\\mu}[J\\neq i^{*}(\\mu)]\\,\\mathrm{d}H(\\mu)}}\\\\ &{\\quad\\ge e^{-4}\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\frac{\\exp{(-n_{i}(\\mu)\\mathrm{KL}_{i}(\\mu_{i},\\mu_{j})-n_{j}(\\mu)\\mathrm{KL}_{j}(\\mu_{j},\\mu_{i}))}}{8}{\\bf1}\\Bigg[|\\mu_{i}|,|\\mu_{j}|\\le\\frac{1}{\\sqrt{\\tilde{\\Delta}}}\\Bigg]\\,\\mathrm{d}H(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the following statement is a stronger statement than Theorem 10. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\d}\\int\\left(\\sum_{i\\in[k]}n_{i}(\\mu)\\right)\\mathrm{d}H(\\mu)\\leq N_{V},\\mathrm{\\then}\\left(\\mathrm{RHS~of~Eq.~}(14)\\right)\\geq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "RHS of Eq. (14) is represented in terms of $n:=(n_{1},\\cdot\\cdot\\cdot\\,,n_{k}):\\mathbb{R}^{k}\\to[0,\\infty)^{k}$ (the expected number of arm pulls) and does hold for any algorithm given $n$ . To prove the above statement, since \u2018set of all expected number of arm pulls\u2019 is a subset of $\\{\\tilde{n}:\\mathbb{R}^{k}\\ \\dot{\\rightarrow}\\ [0,\\infty)\\}$ , it suffices to show that the optimal value $V$ of the following objective ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{:=\\operatorname*{inf}_{\\bar{n}\\cdot\\log^{k}\\rightarrow[0,\\infty)^{k}}V(\\tilde{n})}}&{\\quad{\\mathrm{(15)}}}\\\\ &{\\int_{\\mathbb{R}^{k}}\\left(\\displaystyle\\sum_{s=1}^{k}\\tilde{n}_{s}(\\mu)\\right)\\mathrm{d}H(\\mu)\\leq N_{V}}\\\\ &{V(\\tilde{n}):=e^{-4}\\displaystyle\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\Theta_{i j}}\\frac{\\exp{\\left(-\\tilde{n}_{i}(\\mu)\\mathrm{KL}_{i}(\\mu_{i},\\mu_{j})-\\tilde{n}_{j}(\\mu)\\mathrm{KL}_{j}(\\mu_{j},\\mu_{i})\\right)}}{8}\\mathbf{1}\\left[|\\mu_{i}|,|\\mu_{j}|\\leq\\displaystyle\\frac{1}{\\sqrt{\\tilde{\\Delta}}}\\right]\\mathrm{d}H(\\mu_{i})\\leq N_{V}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is greater than $\\delta$ . ", "page_idx": 18}, {"type": "text", "text": "Let $\\begin{array}{r}{\\tilde{\\Theta}_{i j}:=\\{\\mu\\in\\Theta_{i j}:|\\mu_{i}-\\mu_{j}|\\leq\\tilde{\\Delta},|\\mu_{i}|\\leq\\frac{1}{\\sqrt{\\tilde{\\Delta}}},|\\mu_{j}|\\leq\\frac{1}{\\sqrt{\\tilde{\\Delta}}}\\}}\\end{array}$ . Then, it holds that $V^{\\mathrm{min}}\\geq V_{1}^{\\mathrm{min}}$ where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}^{\\operatorname*{min}}:=\\underset{\\Tilde{n}:\\mathbb{R}^{k}\\rightarrow[0,\\infty)^{k}}{\\operatorname*{inf}}\\;\\;V_{1}(\\Tilde{n})}\\\\ &{\\;\\;\\mathrm{s.t.}\\;\\;\\displaystyle\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\Tilde{\\Theta}_{i j}}\\left(\\sum_{s=1}^{k}\\Tilde{n}_{s}(\\mu)\\right)\\mathrm{d}H(\\mu)\\leq N_{V}}\\\\ &{\\mathrm{here}\\;\\;\\displaystyle V_{1}(\\Tilde{n}):=e^{-4}\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\Tilde{\\Theta}_{i j}}\\frac{\\exp\\left(-\\Tilde{n}_{i}(\\mu)\\mathrm{KL}_{i}(\\mu_{i},\\mu_{j})-\\Tilde{n}_{j}(\\mu)\\mathrm{KL}_{j}(\\mu_{j},\\mu_{i})\\right)}{8}\\,\\mathrm{d}H(\\mu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To see this, suppose $\\hat{\\boldsymbol{n}}$ is an optimal solution to (15). Then, by the constraint of optimization problem (15), $\\begin{array}{r}{\\int_{\\mathbb{R}^{k}}\\left(\\sum_{s\\in[k]}\\hat{n}_{s}(\\pmb{\\mu})\\right)\\mathrm{d}{\\pmb{H}}(\\pmb{\\mu})\\leq N_{V}}\\end{array}$ and since $\\hat{\\boldsymbol{n}}$ is a collection of positive functions, $\\begin{array}{r}{\\sum_{i,j\\in[k]:i>j}\\int_{\\Tilde{\\Theta}_{i j}\\cup\\Tilde{\\Theta}_{j i}}\\left(\\sum_{s=1}^{k}\\Tilde{n}_{s}(\\pmb{\\mu})\\right)\\mathrm{d}\\pmb{H}(\\pmb{\\mu})\\leq N_{V}}\\end{array}$ , which means $\\hat{\\boldsymbol{n}}$ satisfies the constraint of (16). By the minimality, $V_{1}^{\\mathrm{min}}\\leq V_{1}(\\hat{n})$ , and since exp is a positive function, we have $V_{1}(\\hat{n})\\leq V(\\hat{n})=$ V min. ", "page_idx": 18}, {"type": "text", "text": "Moreover, $V_{1}^{\\mathrm{min}}\\geq V_{2}^{\\mathrm{min}}$ holds for ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2}^{\\operatorname*{min}}:=\\underset{\\Tilde{n}:\\mathbb{R}^{k}\\xrightarrow{\\lfloor0,\\infty\\rfloor^{k}}0}{\\operatorname*{inf}}V_{2}(\\Tilde{n})}\\\\ &{\\mathrm{s.t.}\\ \\underset{i\\in[k]}{\\sum}\\underset{j\\neq i}{\\sum}\\int_{\\Tilde{\\Theta}_{i j}}\\left(\\underset{s=1}{\\overset{k}{\\sum}}\\Tilde{n}_{s}(\\mu)\\right)\\mathrm{d}H(\\mu)\\leq N_{V}}\\\\ &{\\mathrm{shere}\\ \\ V_{2}(\\Tilde{n}):=e^{-4}\\underset{i\\in[k]}{\\sum}\\underset{j\\neq i}{\\sum}\\int_{\\Tilde{\\Theta}_{i j}}\\frac{\\exp\\Big(-\\Big(\\Tilde{n}_{i}(\\mu)+\\Tilde{n}_{j}(\\mu)\\Big)\\frac{\\Tilde{\\Delta}^{2}}{2\\operatorname*{min}(\\sigma_{s}^{2})_{s\\in[k]}}\\Big)}{8}\\,\\mathrm{d}H(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by using the fact that $\\begin{array}{r}{\\mathrm{KL}_{i}(\\mu_{i},\\mu_{j})=\\frac{\\tilde{\\Delta}^{2}}{2\\sigma_{i}^{2}}\\leq\\frac{\\tilde{\\Delta}^{2}}{2\\operatorname*{min}(\\sigma_{s}^{2})_{s\\in[k]}}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Claim 1. We abuse our notation slightly so that $\\begin{array}{r}{H(E)=\\int_{E}\\mathrm{d}H(\\mu)}\\end{array}$ for any Lebesgue measurable set $E$ , and let $\\tilde{\\Theta}=\\cup_{i,j\\in[k]:i\\neq j}\\tilde{\\Theta}_{i j}$ (note that all $\\tilde{\\Theta}_{i j}$ are mutually disjoint except for the measure zero sets). Then, the following $n^{o p t}$ is an optimal solutions to (17) ", "page_idx": 19}, {"type": "equation", "text": "$$\nn_{s}^{o p t}(\\pmb{\\mu}):=\\frac{N_{V}}{2\\pmb{H}(\\tilde{\\Theta})}\\mathbf{1}\\bigg[\\mu\\in\\left(\\cup_{j\\neq s}\\Tilde{\\Theta}_{s j}\\right)\\cup\\left(\\cup_{i\\neq s}\\Tilde{\\Theta}_{i s}\\right)\\bigg].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Choose an arbitrary ${\\tilde{n}}:\\mathbb{R}^{k}\\rightarrow[0,\\infty)^{k}$ which satisfies the constraint of optimization problem (17). Let N\u02dc(\u00b5) :=  s\u2208[k] n\u02dcs(\u00b5). Now, since the function \u03c1 : x  \u219281R exp(\u2212x \u00b72 min(\u2206\u02dc\u03c3s22)s\u2208[k] ) is a convex and decreasing function, by Jensen\u2019s inequality we can say that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle V_{2}(\\tilde{n})=\\sum_{i\\in[k]}\\sum_{j\\neq i}\\int_{\\tilde{\\Theta}_{i j}}\\rho\\Big(\\tilde{n}_{i}(\\pmb{\\mu})+\\tilde{n}_{j}(\\pmb{\\mu})\\Big)\\,\\mathrm{d}{\\pmb{H}}(\\pmb{\\mu})}\\\\ {\\displaystyle\\geq\\sum_{i\\in[k]}\\sum_{j\\neq i}H(\\tilde{\\Theta}_{i j})\\rho\\Bigg(\\frac{1}{H(\\tilde{\\Theta}_{i j})}\\int_{\\tilde{\\Theta}_{i j}}\\Big(\\tilde{n}_{i}(\\pmb{\\mu})+\\tilde{n}_{j}(\\pmb{\\mu})\\Big)\\,\\mathrm{d}{\\pmb{H}}(\\pmb{\\mu})\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(Jensen\u2019s inequality for each integral on $\\tilde{\\Theta}_{i j}$ ) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=H(\\Tilde{\\Theta})\\cdot\\displaystyle\\sum_{i\\in[k]}\\frac{H(\\Tilde{\\Theta}_{i j})}{\\mu_{i}!\\mu_{j}!}\\rho\\left(\\frac{1}{H(\\Tilde{\\Theta}_{i j})}\\int_{\\Tilde{\\Theta}_{i j}}\\left(\\hat{n}_{i}(\\mu)+\\hat{n}_{j}(\\mu)\\right)\\mathrm{d}H(\\mu)\\right)}\\\\ &{\\geq H(\\Tilde{\\Theta})\\cdot\\rho\\left(\\displaystyle\\sum_{i\\in[k]}\\frac{H(\\Tilde{\\Theta}_{i j})}{\\mu(\\hat{\\Theta})}\\frac{1}{H(\\Tilde{\\Theta}_{i j})}\\int_{\\Tilde{\\Theta}_{i j}}\\left(\\hat{n}_{i}(\\mu)+\\hat{n}_{j}(\\mu)\\right)\\mathrm{d}H(\\mu)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathrm{Iemsu\"})\\mathrm{ineqp}}\\\\ &{\\geq H(\\Tilde{\\Theta})\\cdot\\rho\\left(\\displaystyle\\sum_{i\\in[k]}\\sum_{j\\neq i}\\frac{1}{H(\\Tilde{\\Theta})}\\int_{\\Tilde{\\Theta}_{i j}}\\left(\\sum_{s\\in[k]}\\hat{n}_{s}\\right)\\mathrm{d}H(\\mu)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left(\\Tilde{n}_{i}+\\hat{n}_{j}\\leq\\sum_{s\\in[k]}\\hat{n}_{s}\\mathrm{~and~}\\rho\\mathrm{is~a~decreasing~fum},}\\\\ &{\\geq H(\\Tilde{\\Theta})\\cdot\\rho\\left(\\frac{N_{V}}{H(\\Tilde{\\Theta})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "uality) ", "page_idx": 19}, {"type": "text", "text": "Since $\\tilde{n}$ is chosen arbitrarily, we can say $\\begin{array}{r}{V_{2}^{\\mathrm{min}}\\geq H(\\tilde{\\Theta})\\rho\\Bigg(\\frac{N_{V}}{H(\\tilde{\\Theta})}\\Bigg)}\\end{array}$ . One can check the above $n^{o p t}$ satisfies the constraint in the optimization problem (17) and also satisfies V2(nopt) = H(\u02dc\u0398)\u03c1( HN( V\u0398\u02dc)). Therefore, $n^{o p t}$ is an optimal solution of optimization problem (17). \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Using Lemma 9, we can get ", "page_idx": 19}, {"type": "equation", "text": "$$\nH(\\tilde{\\Theta})=\\sum_{i\\neq j}\\int_{\\Theta_{i j}}\\mathbf{1}[|\\mu_{i}-\\mu_{j}|\\leq\\tilde{\\Delta}]\\mathbf{1}[|\\mu_{i}|,|\\mu_{j}|\\leq\\frac{1}{\\sqrt{\\tilde{\\Delta}}}]\\,\\mathrm{d}H(\\mu)=L_{i j}^{\\prime}(H,\\tilde{\\Delta}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now applying Claim 1 and Eq. (18) on Optimization (17) implies the following result: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{V_{2}^{\\operatorname*{min}}=V_{2}(n^{o p t})}\\\\ {\\quad=\\frac{\\displaystyle\\exp\\left(-\\frac{N_{V}}{2L^{\\prime}(H,\\bar{\\Delta})}\\frac{\\bar{\\Delta}^{2}}{2\\operatorname*{min}(\\sigma_{s}^{2})s\\in[k]}\\right)}{\\displaystyle8e^{4}}\\sum_{i\\neq j}\\int_{\\Theta_{i j}}\\mathbf{1}[|\\mu_{i}-\\mu_{j}|\\leq\\tilde{\\Delta}]\\mathbf{1}[|\\mu_{i}|,|\\mu_{j}|\\leq\\frac{1}{\\sqrt{\\tilde{\\Delta}}}]\\,\\mathrm{d}H(\\mu)}\\\\ {\\quad\\geq\\frac{\\displaystyle\\exp\\left(-\\frac{N_{V}\\tilde{\\Delta}}{2\\operatorname*{min}(\\sigma_{s}^{2})s\\in[k]}L^{\\prime}(H,\\bar{\\Delta})\\right)}{\\displaystyle\\mathrm{(Eq.~(18))}L^{\\prime}(H,\\bar{\\Delta})}\\times L^{\\prime}(H,\\tilde{\\Delta})\\tilde{\\Delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $V\\leq\\delta$ is true, then $V_{2}\\leq\\delta$ , which implies ", "page_idx": 19}, {"type": "text", "text": "$\\frac{\\mathrm{xp}\\left(-\\frac{N_{V}\\bar{\\Delta}}{2\\operatorname*{min}\\left(\\sigma_{s}^{2}\\right)_{s\\in[k]}L^{\\prime}(H,\\bar{\\Delta})}\\right)}{8e^{4}}\\times L^{\\prime}(H,\\tilde{\\Delta})\\leq\\delta\\Longleftrightarrow N_{V}\\geq\\frac{2\\operatorname*{min}(\\sigma_{s}^{2})_{s\\in[k]}L^{\\prime}(H,\\tilde{\\Delta})}{\\tilde{\\Delta}}\\ln\\frac{L^{\\prime}(H,\\tilde{\\Delta})}{8e^{4}\\delta}.$ To make this lower bound greater than 0, ln L\u2032(8eH4,\u03b4 \u2206\u02dc)> . From Lemma 9, we know that for small enough $\\tilde{\\Delta}^{9},{\\cal L}^{\\prime}({\\cal H},\\tilde{\\Delta})\\in[\\frac{1}{2}{\\cal L}({\\cal H}),2{\\cal L}({\\cal H})]$ . Setting ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}:=\\frac{32e^{4}}{L(H)}\\delta,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nN_{V}\\geq\\operatorname*{min}(\\sigma_{s}^{2})_{s\\in[k]}\\frac{L(H)^{2}}{16e^{4}}\\ln2\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is the inequality we desired. ", "page_idx": 20}, {"type": "text", "text": "D.1 Proof of Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1.1 Proof of lemma 11 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. It is equivalent to prove $p+q\\,\\geq\\,\\textstyle{\\frac{1}{4}}\\exp(-d(p,1-q^{\\prime}))$ for any $p,q,q^{\\prime}\\,\\in\\,(0,1)$ such that $q^{\\prime}\\leq q$ . First, if $p\\geq1/4$ or $q\\geq1/4$ it trivially holds, and thus we assume $p<1/4$ and $q<1/4$ . We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{d(p,1-q^{\\prime})d(p,1-q)}}&{{(p,q\\leq\\frac{1}{4})}}\\\\ {{\\leq d(p+q,1-(p+q))}}&{{(p+q<\\frac{1}{2})}}\\\\ {{\\leq\\log\\displaystyle\\frac{1}{2.4(p+q)}}}&{{(\\mathrm{Eq.}(3)\\mathrm{~of~Kaufmann~et~al.~}[2016a])}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and transforming this yields ", "page_idx": 20}, {"type": "equation", "text": "$$\np+q\\ge\\frac{1}{2.4}e^{-(d(p,1-q^{\\prime}))}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This completes the proof. ", "page_idx": 20}, {"type": "text", "text": "D.1.2 Proof of the Lemma 12 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{h_{i}(a)h_{j}(b)}{h_{i}(b)h_{j}(a)}=\\exp\\left(-\\frac{(a-m_{i})^{2}}{2\\sigma_{i}^{2}}+\\frac{(b-m_{i})^{2}}{2\\sigma_{i}^{2}}-\\frac{(b-m_{j})^{2}}{2\\sigma_{j}^{2}}+\\frac{(a-m_{j})^{2}}{2\\sigma_{j}^{2}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\exp\\left(-\\frac{(a-b)(a+b-2m_{i})}{2\\sigma_{i}^{2}}-\\frac{(b-a)(a+b-2m_{j})}{2\\sigma_{j}^{2}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\exp\\left(2(a-b)\\bigg[\\frac{m_{i}}{\\sigma_{i}^{2}}-\\frac{m_{j}}{\\sigma_{j}^{2}}\\bigg]-(a-b)(a+b)\\bigg[\\frac{1}{2\\sigma_{i}^{2}}+\\frac{1}{2\\sigma_{j}^{2}}\\bigg]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\exp\\left(-2|a-b|\\,\\bigg|\\frac{m_{i}}{\\sigma_{i}^{2}}-\\frac{m_{j}}{\\sigma_{j}^{2}}\\bigg|-|(a-b)(a+b)|\\bigg[\\frac{1}{2\\sigma_{i}^{2}}+\\frac{1}{2\\sigma_{j}^{2}}\\bigg]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\exp\\left(-2\\delta\\bigg|\\frac{m_{i}}{\\sigma_{i}^{2}}-\\frac{m_{j}}{\\sigma_{j}^{2}}\\bigg|-2\\sqrt{\\delta}\\bigg|\\frac{1}{2\\sigma_{i}^{2}}+\\frac{1}{2\\sigma_{j}^{2}}\\bigg|\\right)\\frac{1}{h_{i j}^{\\prime}(H,\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define $\\begin{array}{r}{R^{\\prime}(H,\\delta)=\\operatorname*{min}_{i\\neq j}{R_{i j}^{\\prime}(H,\\delta)}}\\end{array}$ which satisfies the condition of the Ratio lemma. For $\\delta$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta<D_{1}(\\pmb{H}):=\\operatorname*{min}_{i\\neq j}\\left[\\left|\\frac{m_{i}}{\\sigma_{i}^{2}}-\\frac{m_{j}}{\\sigma_{j}^{2}}\\right|^{-1},\\left[\\frac{1}{2\\sigma_{i}^{2}}+\\frac{1}{2\\sigma_{j}^{2}}\\right]^{-2}\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{^{9}\\tilde{\\Delta}<\\operatorname*{min}\\biggl(D_{0}(H),\\operatorname*{min}_{i\\in[k]}\\frac{1}{4m_{i}^{2}},\\frac{L(H)}{4(k-1)\\sum_{i\\in[k]}\\frac{1}{\\xi_{i}}}\\biggr)}\\end{array}$ . Check Section $\\mathrm{G}$ for the condition. ", "page_idx": 20}, {"type": "text", "text": "E Proof of Theorem 6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For notational convenience, define $\\Delta_{s}(\\pmb{\\mu}):=\\mu_{i^{*}(\\pmb{\\mu})}-\\mu_{s}$ for $\\begin{array}{r}{\\mathfrak{s}\\in[k],\\,\\Delta(\\pmb{\\mu}):=\\operatorname*{min}_{\\substack{s\\neq i^{*}(\\pmb{\\mu})}}\\Delta_{s}=}\\end{array}$ $\\Delta_{j^{*}(\\mu)}$ . Let $A(t)$ be the subset of arms that have not been eliminated at time $t$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}_{\\mu\\sim H}(\\Delta_{0}\\geq\\Delta(\\mu))=\\displaystyle\\sum_{i\\neq j}\\int_{\\Theta_{i j}}\\mathbf{1}[|\\mu_{i}-\\mu_{j}|\\leq\\Delta_{0}]\\,\\mathrm{d}H(\\mu)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=L(H,\\Delta_{0})\\Delta_{0}\\leq2L(H)\\cdot\\Delta_{0}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{\\delta}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we consider the upper bound estimator such that $\\hat{\\Delta}^{\\mathrm{safe}}(t)\\geq\\Delta$ holds with high probability. Namely, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{UCB}(i,t),\\mathrm{LCB}(i,t)=\\hat{\\mu}_{i}(t)\\pm\\mathrm{Conf}(i,t),}\\\\ &{\\quad\\quad\\quad\\mathrm{Conf}(i,t)=\\sqrt{2\\sigma_{i}^{2}\\frac{\\log\\left(6\\left(N_{i}(t)\\right)^{2}\\right/\\left((\\frac{\\delta^{2}}{2k})\\pi^{2}\\right)\\right)}{N_{i}(t)}},}\\\\ &{\\quad\\quad\\quad\\hat{\\Delta}^{\\mathrm{safe}}(t)=\\underset{i}{\\mathrm{max}}\\,\\mathrm{UCB}(i,t)-\\underset{j}{\\mathrm{max}}\\,\\mathrm{LCB}(j,t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From the definition above, we can calculate how many arm pulls the learner needs to narrow down the confidence width. ", "page_idx": 21}, {"type": "text", "text": "Lemma 13. Define $B\\ :=\\ 320\\mathrm{{max}}_{i\\in[k]}\\,\\sigma_{i}^{2}$ and $\\begin{array}{r}{\\Delta_{t h r}\\ :=\\ \\operatorname*{min}\\biggl(\\log\\frac{4\\sqrt{k}}{\\delta\\pi},\\frac{1}{B}\\biggr)}\\end{array}$ . Let $R_{0}(\\Delta)\\;:=\\;$ $B\\frac{\\log\\operatorname*{min}(\\Delta,\\Delta_{t h r})^{-1}}{\\operatorname*{min}(\\Delta,\\Delta_{t h r})^{2}}$ . Then, for any $\\Delta\\in(0,\\infty)$ and for a timestep $t$ which satisfies $N_{i}(t)\\geq R_{0}(\\Delta)$ , $\\mathrm{Conf}(i,t)\\le\\Delta/4$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma $^{l3}$ . Only for this part of the proof, let $\\Delta^{\\prime}:=\\operatorname*{min}(\\Delta,\\Delta_{t h r})$ for notational convenience. Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Conf}(i,i)=\\sqrt{2\\sigma_{i}^{2}}\\frac{\\mathrm{log}\\left(6{R_{i}^{(1)}}\\left(7\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)^{2}/\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)\\right)\\right.}{\\mathrm{Nif}(i)}}&{{}}\\\\ {\\leq\\sqrt{2\\sigma_{i}^{2}}\\frac{\\mathrm{log}\\left(6{R_{i}^{(1)}}\\left(3\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)^{2}/\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)\\right)}{R_{i}\\left(3\\right)}}&{{\\left.{\\mathrm{(asumption~of~}\\eta_{i}~}\\right)\\right.}}\\\\ {=\\Delta\\sqrt{2\\sigma_{i}^{2}}\\sqrt{\\frac{\\mathrm{log}\\left(6{R_{i}^{(1)}}\\sin^{2}\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)^{2}/\\left(\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)^{2}\\right)}{R_{i}\\sin\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)}}}\\\\ {\\leq\\Delta\\sqrt{2\\sigma_{i}^{2}}\\sqrt{\\frac{\\mathrm{log}\\left(6{R_{i}^{(2)}}\\frac{2}{\\sigma_{i}^{2}}\\right)^{2}/\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)}{R_{i}\\cos^{2}(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}})}}}&{{\\left.{\\mathrm{(log}\\ \\ \\Delta^{-1})\\leq\\Delta^{-1}}\\right\\}}\\\\ {=\\Delta\\sqrt{2\\sigma_{i}^{2}}\\sqrt{\\frac{\\mathrm{log}\\left(6{R_{i}^{(1)}}\\left(3\\right)^{2}/\\left(\\frac{i\\tilde{\\sigma}_{i}^{2}}{\\sigma_{i}^{2}}\\right)^{2}}{R_{i}\\left(3\\right)}}}&{{\\left.{\\mathrm{(bsumption~of~}\\Delta\\mu \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From this lemma, one could induce the following corollary which states that Algorithm 5 always terminates before a certain timestep: ", "page_idx": 22}, {"type": "text", "text": "Corollary 14. Let $\\tau$ (and $\\gamma$ ) be the stopping time (and the last iteration of the while loop in Algorithm 1, respectively) where Algorithm 1 meets the stopping condition. Then, $\\gamma$ is always bounded by $R_{0}(\\Delta_{0})$ and $\\tau$ is uniformly bounded by $T_{0}:=k\\cdot R_{0}(\\Delta_{0})$ . ", "page_idx": 22}, {"type": "text", "text": "Proof of Corollary $I4.$ . Let us assume that $\\tau\\ \\geq\\ T_{0}\\ +\\ 1$ . Then, by Lemma 13, each $i\\in$ $\\boldsymbol{A}_{T_{0}}$ satisfies $\\mathrm{Conf}(i,T_{0})~\\le~\\Delta_{0}/4$ . Let $i^{u c b}(t)\\ =\\ \\arg\\operatorname*{max}_{i\\in\\mathcal{A}(t)}\\mathrm{UCB}(i,t)$ and $\\begin{array}{r l}{i^{l c b}(t)}&{{}=}\\end{array}$ arg $\\mathrm{max}_{i\\in\\mathcal{A}(t)}\\,\\mathrm{LCB}(i,t)$ . From definition, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}^{\\mathrm{safe}}(T_{0})=\\underset{i\\in A(T_{0})}{\\operatorname*{max}}\\mathrm{UCB}(i,T_{0})-\\underset{i\\in A(T_{0})}{\\operatorname*{max}}\\mathrm{LCB}(i,T_{0})}\\\\ &{\\qquad\\qquad=\\mathrm{UCB}(i^{u c b}(T_{0}),T_{0})-\\mathrm{LCB}(i^{L c b}(T_{0}),T_{0})}\\\\ &{\\qquad\\quad=2\\mathrm{Conf}(i^{u c b}(T_{0}),T_{0})+2\\mathrm{Conf}(i^{L c b}(T_{0}),T_{0})+\\mathrm{LCB}(i^{u c b}(T_{0}),T_{0})-\\mathrm{UCB}(i^{l c b}(T_{0}),T_{0})}\\\\ &{\\qquad\\qquad\\leq\\Delta_{0}+\\mathrm{LCB}(i^{u c b}(T_{0}),T_{0})-\\mathrm{UCB}(i^{l c b}(T_{0}),T_{0})}\\\\ &{\\qquad\\qquad<\\Lambda_{\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies $\\hat{\\Delta}^{\\mathrm{safe}}\\le\\Delta_{0}$ , contradicting $\\tau\\geq T_{0}$ since the algorithm should be terminated by Line 18 at timestep $T_{0}$ . Therefore, $\\tau\\leq T_{0}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 14 implies Algorithm 1 always stops before $T_{0}$ samples. Morever, the following lemma states that with high probability, true mean $\\pmb{\\mu}$ is in between UCB and LCB for all time steps. ", "page_idx": 22}, {"type": "text", "text": "Lemma 15. (Uniform confidence bound) The following holds for all $i\\in[k]$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mu}\\displaystyle\\left[\\bigcap_{t=1}^{\\infty}\\left\\{\\mathrm{LCB}(i,t)\\leq\\mu_{i}\\right\\}\\right]\\geq1-\\frac{\\delta^{2}}{2k},}\\\\ &{\\mathbb{P}_{\\mu}\\displaystyle\\left[\\bigcap_{t=1}^{\\infty}\\left\\{\\mu_{i}\\leq\\mathrm{UCB}(i,t)\\right\\}\\right]\\geq1-\\frac{\\delta^{2}}{2k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 15. The following derives the upper bound part, Eq. (23). The lower bound is derived by following the same steps. ", "page_idx": 22}, {"type": "text", "text": "Since each arm is independent of each other, Eq. (23) boils down to prove ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mu}\\left[\\bigcap_{s=1}^{\\infty}\\{\\mu_{i}\\leq\\mathrm{UCB}(i,t_{i}(s))\\}\\right]\\geq1-\\frac{\\delta^{2}}{2k},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $t_{i}(s)=\\operatorname*{min}\\{t\\in\\mathbb{N}:N_{i}(t)\\geq s\\}$ and $t_{i}(s)=\\infty$ if $\\{t\\in\\mathbb{N}:N_{i}(t)\\geq s\\}=\\emptyset$ . For each event $\\{\\mu_{i}\\leq U C B_{i}(t_{i}(s))\\}$ , since each arm pull is independent of each other, by Hoeffding\u2019s inequality we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mu}\\left(\\frac{1}{s}\\sum_{j=1}^{s}(X_{j}^{i}-\\mu_{i})\\leq-\\epsilon\\right)\\leq\\exp\\left(-\\frac{s\\epsilon^{2}}{2\\sigma_{0}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $\\epsilon>0$ . If we set $\\epsilon=\\operatorname{Conf}\\left(i,t_{i}(s)\\right)$ , we can transform the above inequality to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mu}\\bigl(\\mathrm{UCB}(i,t_{i}(s))\\leq\\mu_{i}\\bigr)\\leq\\frac{\\delta^{2}}{2k s^{2}}\\cdot\\frac{6}{\\pi^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the union bound, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mu}\\left[\\bigcap_{s=1}^{\\infty}\\{\\mu_{i}\\leq\\mathrm{UCB}(i,t_{i}(s))\\}\\right]\\geq1-\\sum_{s=1}^{\\infty}\\mathbb{P}_{\\mu}\\big[\\mu_{i}\\geq\\mathrm{UCB}(i,t_{i}(s))\\big]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\geq1-\\sum_{s=1}^{\\infty}\\frac{\\delta^{2}}{2k s^{2}}\\cdot\\frac{6}{\\pi^{2}}}\\\\ {\\displaystyle\\geq1-\\sum_{s=1}^{\\infty}\\frac{\\delta^{2}}{2k s^{2}}\\cdot\\frac{6}{\\pi^{2}}=1-\\frac{\\delta^{2}}{2k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the proof is completed. ", "page_idx": 23}, {"type": "text", "text": "Let us define a good event based on Lemma 15 as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{X}(\\mu):=\\bigcap_{i\\in[k]}\\left[\\left(\\bigcap_{t=1}^{\\infty}\\left\\{\\mathrm{LCB}(i,t)\\leq\\mu_{i}\\right\\}\\right)\\bigcap\\left(\\bigcap_{t=1}^{\\infty}\\left\\{\\mathrm{UCB}(i,t)\\geq\\mu_{i}\\right\\}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now prove that, under $\\mathcal{H}_{\\mu}$ and under this good event $\\mathcal{X}(\\pmb{\\mu})$ ", "page_idx": 23}, {"type": "text", "text": "\u2022 The best arm $i^{*}(\\pmb{\\mu})$ is always in the active arm set $\\mathbf{\\mathcal{A}}(t)$ for all $t$ (Lemma 16), ", "page_idx": 23}, {"type": "text", "text": "\u2022 Each count of the suboptimal arm pull, $N_{i}(T_{0})$ , is bounded by roughly $O\\big(\\frac{\\log\\Delta_{i}}{\\Delta_{i}^{2}}\\big)$ (Lemma 17). ", "page_idx": 23}, {"type": "text", "text": "Lemma 16. Let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\chi^{\\prime}(\\pmb{\\mu})=\\bigcap_{t=1}^{\\infty}\\left\\{i^{*}(\\pmb{\\mu})\\in\\pmb{\\mathcal{A}}(t)\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, under $\\mathcal{H}_{\\mu}$ , $\\mathcal{X}(\\pmb{\\mu})\\subset\\mathcal{X}^{\\prime}(\\pmb{\\mu})$ and therefore ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\pmb{\\mu}}\\left[(\\mathscr{X}^{\\prime}(\\pmb{\\mu}))^{c}\\right]\\leq\\delta^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 16. Suppose that event $\\mathcal{X}(\\pmb{\\mu})$ occurs under $\\mathcal{H}_{\\mu}$ . Then for all $i\\in[k]$ and for all $t$ , $\\hat{\\mu}_{i}-\\overset{\\cdot}{\\operatorname{Conf}}(i,t)\\leq\\mu_{i}$ and $\\hat{\\mu}_{i}+\\mathrm{Conf}(i,t)\\geq\\bar{\\mu}_{i}$ . Now, for any $i\\neq i^{*}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{LCB}(i,t)-\\mathrm{UCB}(i^{*},t)=\\hat{\\mu}_{i}-\\mathrm{Conf}(i,t)-\\left(\\hat{\\mu}_{i^{*}}+\\mathrm{Conf}(i^{*},t)\\right)}&{}\\\\ {\\leq\\mu_{i}+\\mathrm{Conf}(i,t)-\\mathrm{Conf}(i,t)-(\\mu_{i^{*}}-\\mathrm{Conf}(i^{*},t)+\\mathrm{Conf}(i^{*},t))}&{}\\\\ {\\mathrm{(Event~}\\mathcal{X}\\mathrm{~occur}}&{}\\\\ {=\\mu_{i}-\\mu_{i^{*}}<0}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which means when event $\\mathcal{X}$ occurs, the optimal arm will never be dropped, and thus $\\mathcal{X}\\subset\\mathcal{X}^{\\prime}$ . By Lemma 15, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\pmb{\\mu}}(\\mathcal{X}^{\\prime}(\\pmb{\\mu}))\\geq\\mathbb{P}_{\\pmb{\\mu}}(\\mathcal{X}(\\pmb{\\mu}))\\geq1-\\delta^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 17. For any $i\\neq i^{*}$ , under $\\mathcal{H}_{\\mu}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{N_{i}(T_{0})>R_{0}(\\operatorname*{max}(\\Delta_{i},\\Delta_{0})\\right)\\right\\}\\subset\\mathcal{X}(\\pmb{\\mu})^{c},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and therefore, $\\mathbb{E}_{\\pmb\\mu}[N_{i}(T_{0})\\mathbf{1}[\\mathcal{X}(\\pmb\\mu)]]\\leq R_{0}\\big(\\operatorname*{max}(\\Delta_{i},\\Delta_{0})\\big).$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma $I7.$ . Only for this part of the proof, let $T_{i}:=R_{0}\\left(\\operatorname*{max}(\\Delta_{i},\\Delta_{0})\\right)$ for brevity. ", "page_idx": 23}, {"type": "text", "text": "When $\\Delta_{i}\\textit{\\textbf{<}}\\Delta_{0}$ , $\\operatorname*{max}(\\Delta_{i},\\Delta_{0})~=~\\Delta_{0}$ , which, combined with Corollary 14, implies that $\\{N_{i}(T_{0})\\ge T_{i}\\}$ always holds. ", "page_idx": 23}, {"type": "text", "text": "For the case of $\\Delta_{i}>\\Delta_{0}$ , suppose that the learner is under the events $\\mathcal{X}(\\pmb{\\mu})$ and $\\{i\\in\\mathcal{A}(T_{i})\\}$ . Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Conf}\\!\\left(a,T_{i}\\right)\\leq\\frac{\\Delta_{i}}{4},\\quad\\forall a\\in\\mathcal{A}(T_{i}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in{\\mathcal{A}}(T_{i})}\\mathrm{LCB}_{a}(T_{i})-\\mathrm{UCB}_{i}(T_{i})\\geq\\mathrm{LCB}_{i^{*}}(T_{i})-\\mathrm{UCB}_{i}(T_{i})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\hat{\\mu}_{i^{*}}-\\operatorname{Conf}_{i^{*}}(T_{i})-(\\hat{\\mu}_{i}+\\operatorname{Conf}_{i}(T_{i}))}\\\\ &{\\geq\\mu_{i^{*}}-2\\mathrm{Conf}_{i^{*}}(T_{i})-(\\mu_{i}+2\\mathrm{Conf}_{i}(T_{i}))}\\\\ &{\\geq\\mu_{i^{*}}-\\mu_{i}-\\Delta_{i}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, when $\\mathcal{X}$ occurs, $\\mu_{i}$ should be eliminated after timestep $T_{i}$ so $N_{i}(T_{0})\\leq T_{i}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemmas 16 and 17 guarantee the Bayesian $\\delta$ -correctness of our Algorithm 1. Theorem 18. ( $\\boldsymbol{\\delta}$ -correctness) The Bayesian PoE of Algorithm 1 is at most $\\delta$ , i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{k}}\\mathrm{PoE}(\\pmb{\\mu})\\,\\mathrm{d}\\pmb{H}(\\pmb{\\mu})\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 18. Throughout the proof, we use the following results. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The probability that $\\mu_{i^{*}(\\pmb{\\mu})}-\\mu_{j^{*}(\\pmb{\\mu})}\\leq\\Delta_{0}$ is at most $\\frac{\\delta}{2}$ by Eq. (21). \u2022 The event $\\mathcal{X}$ fails to hold with probability at most $\\delta^{2}$ by Lemma 15. \u2022 When $\\mu_{i^{*}(\\pmb{\\mu})}-\\mu_{j^{*}(\\pmb{\\mu})}>\\Delta_{0}$ and event $\\mathcal{X}$ occurs, by Lemmas 16 and 17, all suboptimal arms will be eliminated before $T_{j^{*}}$ and only the optimal arm will remain in the set. This means $\\mathcal{E}(\\pmb{\\mu})\\subset\\mathcal{X}(\\pmb{\\mu})\\cup\\{\\mu_{i^{*}(\\pmb{\\mu})}\\overset{\\circ}{-}\\mu_{j^{*}(\\pmb{\\mu})}\\leq\\Delta_{0}\\}$ . ", "page_idx": 24}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{PoE}(\\pi;H)=\\mathbb{E}_{\\mu\\sim H}\\bigg[\\mathbb{P}\\Big(J\\neq i^{*}(\\mu)|\\mathcal{H}_{\\mu}\\Big)\\bigg]=\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}\\big[\\mathbf{1}(\\mathcal{E}(\\mu))|\\mathcal{H}_{\\mu}\\big]\\Big]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}\\big[\\mathbf{1}\\big(\\{\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)}\\leq\\Delta_{0}\\}\\big)+\\mathbf{1}(\\mathcal{X}(\\mu))|\\mathcal{H}_{\\mu}\\big]\\Big]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbf{1}\\big(\\{\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)}\\leq\\Delta_{0}\\}\\big)+\\mathbb{E}\\big[\\mathbf{1}(\\mathcal{X}(\\mu))|\\mathcal{H}_{\\mu}\\big]\\Big]}\\\\ &{\\qquad\\qquad\\leq\\frac{\\delta}{2}+\\delta^{2}<\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, Lemma 19 shows the upper bound of the expected stopping time of our algorithm. ", "page_idx": 24}, {"type": "text", "text": "Lemma 19. We have $\\begin{array}{r}{\\mathbb{E}[\\tau]\\leq B_{0}\\frac{L(H)^{2}}{\\delta}\\log\\Big(\\frac{L(H)}{\\delta}\\Big)+O(\\log\\delta^{-1}).}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau]=\\displaystyle\\sum_{s=1}^{k}\\mathbb{E}[N_{s}(T_{0})]=\\displaystyle\\sum_{s=1}^{k}\\mathbb{E}[\\mathbb{E}_{\\mu}[N_{s}(T_{0})]]}\\\\ &{\\qquad=\\displaystyle\\sum_{s=1}^{k}\\mathbb{E}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[\\mathcal{X}(\\mu)]]\\Big]+\\displaystyle\\sum_{s=1}^{k}\\mathbb{E}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[\\mathcal{X}^{c}(\\mu)]]\\Big]}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{s=1}^{k}\\mathbb{E}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[\\mathcal{X}(\\mu)]]\\Big]+T_{0}\\cdot\\delta^{2}\\qquad\\qquad\\qquad\\mathrm{(Corollary~14~and~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and since $\\Delta_{0}<\\Delta_{t h r}$ by assumption, ", "page_idx": 24}, {"type": "equation", "text": "$$\nT_{0}\\cdot\\delta^{2}=k\\cdot R_{0}(\\Delta_{0})\\cdot\\delta^{2}\\leq k B\\cdot\\frac{\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\cdot\\delta^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, it remains to compute the scale of the first term, $\\begin{array}{r}{\\sum_{s=1}^{k}\\mathbb{E}\\Big[\\mathbb{E}_{\\pmb{\\mu}}[N_{s}(T_{0})\\mathbf{1}[\\mathcal{X}(\\pmb{\\mu})]\\Big]}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "The following evaluates $\\mathbb{E}\\Big[\\mathbb{E}_{\\pmb{\\mu}}[N_{i}(T_{0})\\mathbf{1}[\\mathcal{X}(\\pmb{\\mu})]]\\Big]$ for each $i$ . For notational convenience, let $\\begin{array}{r}{\\mathcal{T}_{s}(\\pmb{\\mu}):=}\\end{array}$ $\\mathbb{E}_{\\mu}\\Big[R_{0}(\\operatorname*{max}\\big(\\Delta_{s},\\Delta_{0}\\big)\\big)\\mathbf{1}\\bar{[\\mathcal{X}}(\\mu)\\big]\\Big]$ . Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[\\mathcal{X}(\\mu)]]\\Big]\\leq\\mathbb{E}[T_{s}(\\mu)]}\\\\ {\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {\\displaystyle-\\sum_{i=1}^{k}\\int_{\\mu\\in\\Theta_{i}}T_{s}(\\mu)\\,\\mathrm{d}H(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(Recall that $\\Theta_{i}\\ =\\ \\{\\pmb{\\mu}\\ \\in\\ \\mathbb{R}^{k}\\ :\\ \\mu_{i}\\ \\geq\\ \\operatorname*{max}_{j\\neq i}\\mu_{j}\\}\\}$ ). In the following, we calculate each $\\begin{array}{r}{\\int_{\\mu\\in\\Theta_{i}}T_{s}(\\mu)\\,\\mathrm{d}H(\\mu)}\\end{array}$ . By assumption, $\\Delta_{0}<\\Delta_{t h r}$ . ", "page_idx": 25}, {"type": "text", "text": "For this part, we define a new notation that for each vector $\\boldsymbol{v}\\in\\mathbb{R}^{k}$ and $i,j\\in[k]$ , $v_{\\setminus i}$ (and $v_{\\setminus i,j})$ is the projection of $v$ to $\\mathbb{R}^{k-1}$ $\\mathbb{R}^{k-2}$ , respectively) by omitting $i$ -th coordinate $(i,j$ -th coordinate, respectively), $v_{\\backslash i}:=(v_{1},v_{2},\\cdot\\cdot\\cdot\\cdot,v_{i-1},v_{i+1},\\cdot\\cdot\\cdot,v_{k})$ . Similarly, for a $k$ -dimensional distribution $\\pmb{H}$ $k\\!-\\!1$ co o rdinates and $i,j\\in[k],H_{\\backslash i}$ (and $\\textstyle{H_{\\setminus{i,j}}})$ is the distribution which omits $i$ -th $(i,j$ -th, respectively) coordinate. ", "page_idx": 25}, {"type": "text", "text": "Case 1: $s\\neq i$ In this case, by Lemma 17, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mu(\\epsilon)_{i}}T_{\\alpha}(\\mu)\\,\\mathrm{d}H(\\mu)=\\int_{\\gamma\\in\\mathbb{R}_{+}^{n}}\\mathcal{F}_{\\alpha}(\\mu)(\\lfloor\\mu_{i}-\\mu_{\\delta}\\geq\\Delta_{0}\\right)+\\mathbf{1}[\\mu_{i}-\\mu_{\\delta}<\\Delta_{0}])\\,\\mathrm{d}H\\,(\\mu)}\\\\ &{=\\!\\int_{\\mu_{\\delta}\\subset\\mathbb{R}_{+}^{n}}\\!\\int_{\\mu_{\\delta}\\subset-\\infty}^{\\mu_{\\delta}-\\Delta_{0}}\\int_{\\mu_{1},\\ldots,\\epsilon(-\\infty,\\mu_{\\delta})^{1}}\\!T_{\\alpha}(\\mu)h_{i}(\\mu_{i})h_{\\epsilon}(\\mu_{\\delta})\\,\\mathrm{d}H_{\\{\\epsilon,\\epsilon,\\epsilon\\}}\\,\\mathrm{d}\\mu_{\\delta}\\,\\mathrm{d}\\mu_{\\epsilon}}\\\\ &{\\quad+\\int_{\\mu_{\\delta}\\in\\mathbb{R}_{+}^{n}}\\int_{\\mu_{\\delta}\\cap\\mathcal{H}_{-\\mu_{\\delta}-2,\\epsilon}}\\int_{\\mu_{1},\\ldots,\\epsilon(-\\infty,\\mu_{\\delta})^{1}}\\!T_{\\alpha}(\\mu)h_{i}(\\mu_{i})h_{\\epsilon}(\\mu_{\\delta})\\,\\mathrm{d}H_{\\{\\epsilon,\\epsilon,\\epsilon\\}}\\,\\mathrm{d}\\mu_{\\delta}\\,\\mathrm{d}\\mu_{\\epsilon}}\\\\ &{\\quad+\\int_{\\mu_{\\delta}\\in\\mathbb{R}_{+}^{n}}\\int_{\\mu_{\\delta}\\cap\\mathcal{H}_{-\\mu_{\\delta}-2,\\epsilon}}\\int_{\\gamma_{1,\\epsilon,\\epsilon}(-\\infty,\\mu_{\\delta})^{1}}\\!T_{\\alpha}(\\mu)h_{i}(\\mu_{i})h_{\\epsilon}(\\mu_{\\delta})\\,\\mathrm{d}H_{\\{\\epsilon,\\epsilon,\\epsilon\\}}\\,\\mathrm{d}\\mu_{\\delta}\\,\\mathrm{d}\\mu_{\\delta}}\\\\ &{\\leq\\!B\\cdot\\frac{\\log\\Delta_{\\epsilon}^{1}}{\\Delta_{\\mu_{\\delta}}^{1}}}\\\\ &{\\quad+\\int_{\\mu_{\\delta}\\in\\mathbb{R}_{+}^{n}}\\int_{\\mu_{\\delta}\\cap\\mathcal{H}_{-\\mu_{\\delta}-2,\\epsilon}}\\int_{\\gamma_{1,\\epsilon,\\epsilon}(-\\infty,\\mu_{\\delta})^{1}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle=\\int_{\\mu_{i}\\in\\mathbb{R}}\\int_{\\mu_{s}=-\\mu_{i}-\\Delta_{t h r}}^{\\mu_{i}-\\Delta_{0}}B\\frac{\\log(\\mu_{i}-\\mu_{s})^{-1}}{(\\mu_{i}-\\mu_{s})^{2}}h_{i}(\\mu_{i})h_{s}(\\mu_{s})\\Bigg[\\prod_{k\\in[k]\\backslash\\{i,s\\}}H_{k}(\\mu_{i})\\Bigg]\\;\\mathrm{d}\\mu_{s}\\,\\mathrm{d}\\mu_{i}}\\\\ {\\displaystyle\\quad+\\,B\\frac{\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\mathbb{P}(i^{*}(\\mu)=i,\\mu_{i}-\\mu_{s}\\le\\Delta_{0})+B\\cdot\\frac{\\log\\Delta_{t h r}^{-1}}{\\Delta_{t h r}^{2}}}\\\\ {\\displaystyle\\le B\\log\\Delta_{0}^{-1}\\int_{\\mu_{i}\\in\\mathbb{R}}h_{i}(\\mu_{i})\\int_{\\mu_{s}=-\\infty}^{\\mu_{i}-\\Delta_{0}}\\frac{1}{(\\mu_{i}-\\mu_{s})^{2}}h_{s}(\\mu_{s})\\Bigg[\\prod_{k\\in[k]\\backslash\\{i,s\\}}H_{k}(\\mu_{i})\\Bigg]\\;\\mathrm{d}\\mu_{s}\\,\\mathrm{d}\\mu_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n+\\,B\\frac{\\log{\\Delta_{0}^{-1}}}{\\Delta_{0}^{2}}\\underbrace{\\mathbb{P}(i^{*}(\\mu)=i,\\mu_{i}-\\mu_{s}\\leq\\Delta_{0})}_{(P_{i s})}+B\\frac{\\log{\\Delta_{t h r}^{-1}}}{\\Delta_{t h r}^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We first deal with the term $(A)$ . The following splits $(A)$ into the sum of two integrals $(A1)$ and $(A2)$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle(A)=\\int_{\\mu_{i}\\in\\mathbb{R}}h_{i}(\\mu_{i})\\left[\\prod_{k\\in[k]\\setminus\\{i,s\\}}H_{k}(\\mu_{i})\\right]\\int_{\\mu_{s}=-\\infty}^{\\mu_{i}-\\Delta_{0}}\\frac{1}{(\\mu_{i}-\\mu_{s})^{2}}h_{s}(\\mu_{s})\\,\\mathrm{d}\\mu_{s}\\,\\mathrm{d}\\mu_{i}}}\\\\ {{\\displaystyle\\qquad\\int_{\\mathbb{T}\\times\\mathbb{R}}\\!\\!\\!\\frac{1}{\\sqrt{\\Delta_{0}}}1\\!-\\!1\\,\\int_{\\mu_{i}\\in\\mathbb{R}}h_{i}(\\mu_{i})\\left[\\prod_{k\\in[k]\\setminus\\{i,s\\}}H_{k}(\\mu_{i})\\right]\\int_{\\mu_{s}=\\mu_{i}-(l+1)\\Delta_{0}}^{\\mu_{i}-l\\Delta_{0}}\\frac{1}{(\\mu_{i}-\\mu_{s})^{2}}h_{s}(\\mu_{s})\\,\\mathrm{d}\\mu_{s}\\,\\mathrm{d}\\mu_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n+\\int_{\\mu_{i}\\in\\mathbb{R}}h_{i}(\\mu_{i})\\left[\\prod_{k\\in[k]\\setminus\\{i,s\\}}H_{k}(\\mu_{i})\\right]\\int_{\\mu_{s}=-\\infty}^{\\mu_{i}-\\lceil\\frac{1}{2\\sqrt{\\Delta_{0}}}\\rceil\\Delta_{0}}\\frac{1}{(\\mu_{i}-\\mu_{s})^{2}}h_{s}(\\mu_{s})\\,\\mathrm{d}\\mu_{s}\\,\\mathrm{d}\\mu_{i}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For $(A1)$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l r l}&{[\\frac{1}{2\\sqrt{\\eta_{0}\\beta_{1}}}]^{-1}}&&{}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{r}}&{\\int_{\\mu_{1}\\in\\mathbb{R}^{3}}\\ln_{1}(\\mu_{l})\\Biggl[\\prod_{k\\in[k]\\backslash\\{i,\\alpha\\}}H_{k}(\\mu_{i})\\Biggr]\\int_{\\mu_{1}=\\mu_{1}-(i+1)\\Delta_{0}}^{\\mu_{1}-\\,l\\Delta_{0}}\\frac{1}{l^{\\Delta_{0}}}h_{*}(\\mu_{*})\\,\\mathrm{d}\\mu_{*}\\,\\mathrm{d}\\mu_{*}}\\\\ &{\\quad\\frac{1}{\\sqrt{\\eta_{0}\\beta_{1}}}-1}&&{}\\\\ &{=\\displaystyle\\sum_{l=1}^{r}}&{\\frac{1}{l^{\\Delta_{0}}}\\frac{1}{\\beta_{0}^{2}}\\mathbb{P}(i^{*}(\\mu)=i,\\mu_{1}-\\mu_{*}\\in[l\\Delta_{0},(l+1)\\Delta_{0}])}\\\\ &{\\quad\\quad\\times\\displaystyle\\sum_{l=1}^{r}}&&{}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{r}}&{\\frac{1}{l^{2}\\Delta_{0}^{2}}\\cdot\\mathbb{D}(i^{*}(\\mu)=i,\\mu_{1}-\\mu_{*}\\leq\\Delta_{0})}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{\\infty}}&&{\\mathrm{(Lemma~)}}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{\\infty}\\frac{1}{l^{2}\\Delta_{0}^{2}}\\cdot\\mathbb{D}P_{l}}\\\\ &{=\\displaystyle\\sum_{l=1}^{r}}&&{}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now for $(A2)$ , we evaluate the inner integral of $(A2)$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{Imer}-A2):=\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}}\\int_{-\\infty}^{\\mu_{1}-\\lceil\\frac{1}{2\\sqrt{3}0}\\rceil\\Delta_{0}}\\frac{1}{(\\mu_{i}-\\mu_{s})^{2}}\\exp\\left(-\\frac{(\\mu_{s}-m_{s})^{2}}{2\\sigma_{s}^{2}}\\right)\\mathrm{d}\\mu_{s}}\\\\ &{\\qquad\\qquad=\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}}\\bigg[\\frac{1}{(\\mu_{i}-\\mu_{s})}\\exp\\left(-\\frac{(\\mu_{s}-m_{s})^{2}}{2\\sigma_{s}^{2}}\\right)\\bigg]_{-\\infty}^{\\mu_{i}-\\lceil\\frac{1}{2\\sqrt{3}0}\\rceil\\Delta_{0}}}\\\\ &{\\qquad\\qquad+\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}}\\int_{-\\infty}^{\\mu_{i}-\\lceil\\frac{1}{2\\sqrt{3}0}\\rceil\\Delta_{0}}\\frac{\\mu_{s}-m_{s}}{\\sigma_{s}^{2}(\\mu_{i}-\\mu_{s})}\\exp\\left(-\\frac{(\\mu_{s}-m_{s})^{2}}{2\\sigma_{s}^{2}}\\right)\\mathrm{d}\\mu_{s}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}}\\frac{1}{\\sqrt{2\\Delta_{0}}}\\frac{1}{|\\Delta_{0}}+\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}^{2}}\\int_{-\\infty}^{\\mu_{i}-\\lceil\\frac{1}{2}\\rceil\\Delta_{0}}\\left(\\frac{\\mu_{i}-m_{s}}{\\mu_{i}-\\mu_{s}}-1\\right)\\exp\\left(-\\frac{(\\mu_{s}-m)}{2\\sigma_{s}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}}\\frac{1}{\\lceil\\frac{1}{2\\sqrt{\\Delta_{0}}}\\rceil\\Delta_{0}}+\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}^{3}}\\int_{-\\infty}^{\\mu_{i}-\\lceil\\frac{1}{2\\sqrt{\\Delta_{0}}}\\rceil\\Delta_{0}}\\frac{\\mu_{i}-m_{s}}{\\lceil\\frac{1}{2\\sqrt{\\Delta_{0}}}\\rceil\\Delta_{0}}\\exp\\left(-\\frac{(\\mu_{s}-m_{s})^{2}}{2\\sigma_{s}^{2}}\\right)\\mathrm{d}\\mu_{s}}\\\\ &{\\le\\displaystyle\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}}\\frac{1}{\\lceil\\frac{1}{2\\sqrt{\\Delta_{0}}}\\rceil\\Delta_{0}}+\\operatorname*{max}\\left[\\frac{1}{\\sigma_{s}^{2}}\\frac{\\mu_{i}-m_{s}}{\\lceil\\frac{1}{2\\sqrt{\\Delta_{0}}}\\rceil\\Delta_{0}},0\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By integrating above over variable $i$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(A^{2})\\leq\\int_{\\mathbb{R}^{h}}h_{i}(\\mu_{i})\\left[\\underset{k\\in\\dot{\\mathbb{Z}}_{m}}{\\prod}\\mathcal{H}_{k}(\\mu_{i})\\right]\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_{k}}\\frac{1}{\\int_{\\mathbb{R}^{h}}^{m}\\Delta_{0}}+\\operatorname*{max}\\left[\\frac{1}{\\sigma_{k}^{2}}\\frac{\\mu_{i}-m_{s}}{\\sum_{j=0}^{n}\\Delta_{0}},0\\right]\\right)\\,\\mathrm{d}\\mu_{i}}\\\\ &{\\quad\\leq\\int_{\\mathbb{R}^{h}}h_{i}(\\mu_{i})\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_{k}}\\frac{1}{\\int_{\\mathbb{R}^{h}}^{m}\\Delta_{0}}+\\operatorname*{max}\\left[\\frac{1}{\\sigma_{k}^{2}}\\frac{\\mu_{i}-m_{s}}{\\sum_{j=0}^{n}\\Delta_{0}},0\\right]\\right)\\,\\mathrm{d}\\mu_{i}}\\\\ &{\\quad=\\frac{1}{\\sqrt{2\\pi}\\sigma_{k}}\\frac{1}{\\int_{\\mathbb{R}^{h}}^{2}\\Delta_{0}}\\Delta_{0}^{+}\\frac{1}{\\sigma_{k}^{2}\\int_{\\mathbb{R}^{h}}^{2}\\Delta_{0}}\\Delta_{0}\\left[\\int_{h}^{h}(\\mu_{i})\\operatorname*{max}\\left[\\mu_{i}-m_{s},0\\right]\\,\\mathrm{d}\\mu_{i}\\right]}\\\\ &{\\quad\\leq\\frac{1}{\\sqrt{2\\pi}\\sigma_{k}}\\frac{1}{\\int_{\\mathbb{R}^{h}}^{2}\\Delta_{0}}\\Delta_{0}^{+}\\frac{1}{\\sigma_{k}^{2}\\int_{\\mathbb{R}^{h}}^{2}\\Delta_{0}}\\Delta_{0}\\left[\\int_{h}^{h}(\\mu_{i})|\\mu_{i}-m_{s}|\\,\\mathrm{d}\\mu_{i}\\right]}\\\\ &{\\quad\\leq\\frac{1}{\\sqrt{2\\pi}\\sigma_{k}}\\frac{1}{\\int_{\\mathbb{R}^{h}}^{2}\\Delta_{0}}\\Delta_{0}+\\frac{1}{\\sigma_{k}^{2}\\int_{\\mathbb{R}^{h}}^{2}\\Delta_{0}}\\Delta_{0}\\left[\\int_{h}^{h}(\\mu_{i})|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "(Mean of Half normal distribution is ${\\frac{\\sigma_{i}{\\sqrt{2}}}{\\sqrt{\\pi}}},$ ) ", "page_idx": 27}, {"type": "equation", "text": "$$\n=\\frac{1}{\\lceil\\frac{1}{2\\sqrt{\\Delta_{0}}}\\rceil\\Delta_{0}}\\underbrace{\\Bigg[\\frac{1}{\\sqrt{2\\pi}\\sigma_{s}}+\\frac{1}{\\sigma_{s}^{2}}(|m_{i}-m_{s}|+\\frac{\\sigma_{i}\\sqrt{2}}{\\sqrt{\\pi}})\\Bigg]}_{S_{i s}(H)/2}\\leq\\frac{S_{i s}(H)}{\\sqrt{\\Delta_{0}}}=O(\\Delta_{0}^{-1/2}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, by Eq. (29) and Eq. (30), ", "page_idx": 27}, {"type": "equation", "text": "$$\n(A)=(A1)+(A2)={\\frac{1}{\\Delta_{0}^{2}}}\\cdot{\\frac{\\pi^{2}}{3}}P_{i s}+O(\\Delta_{0}^{-1/2}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and therefore ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int_{\\mu\\in\\Theta_{i}}\\mathcal{T}_{s}(\\mu)\\,\\mathrm{d}\\pmb{H}(\\mu)\\leq\\frac{B\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}P_{i s}\\biggr[\\frac{\\pi^{2}}{3}+1\\biggr]+O(\\Delta_{0}^{-1/2}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Case 2: $s=i$ In this case, let ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\mu\\in\\Theta_{s}}T_{s}(\\mu)\\,\\mathrm{d}H(\\mu)=\\sum_{j\\not=s}\\int_{\\mu\\in\\Theta_{s j}}T_{s}(\\mu)\\,\\mathrm{d}H(\\mu)}}\\\\ &{}&{\\qquad\\qquad\\qquad\\le\\displaystyle\\sum_{j\\not=s}\\int_{\\mu\\in\\Theta_{s j}}\\operatorname*{max}_{i\\not=s}(T_{i}(\\mu))\\,\\mathrm{d}H(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{j\\neq s}\\int_{\\mu\\in\\Theta_{s j}}\\operatorname*{max}_{i\\neq s}\\bigl[R_{0}(\\Delta_{i}(\\mu),\\Delta_{0})\\bigr]\\,\\mathrm{d}H(\\mu)}\\\\ {\\displaystyle\\leq\\sum_{j\\neq s}\\int_{\\mu\\in\\Theta_{s j}}\\Bigl[R_{0}(\\Delta_{j}(\\mu),\\Delta_{0})\\Bigr]\\,\\mathrm{d}H(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and by the same calculation as Case 1, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\int_{\\mu\\in\\Theta_{s j}}\\Big[R_{0}(\\Delta_{j}(\\mu),\\Delta_{0})\\Big]\\,\\mathrm{d}\\mu\\leq\\frac{B\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}P_{s j}\\Big[\\frac{\\pi^{2}}{3}+1\\Big]+O(\\Delta_{0}^{-1/2}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\int_{\\mu\\in\\Theta_{s}}\\mathcal{T}_{s}(\\mu)\\,\\mathrm{d}H(\\mu)\\leq\\frac{B\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\bigg[\\frac{\\pi^{2}}{3}+1\\bigg]\\sum_{j\\neq s}P_{s j}+O(\\Delta_{0}^{-1/2}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For notational convenience, let $\\begin{array}{r}{B_{0}=B\\cdot\\left[\\frac{\\pi^{2}}{3}+1\\right]}\\end{array}$ . From Eq. (28), Eq. (31), Eq. (32), we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathcal{N}_{s}(T_{0})]=\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[X(\\mu)]]\\Big]+\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[X(\\mu)^{*}]]\\Big]}\\\\ &{\\phantom{\\sum_{\\mu\\in\\mathbb{Z}}}\\sum_{i=1}^{k}\\int_{\\mu\\in\\Theta_{s}}T_{s}(\\mu)\\,\\mathrm{d}H(\\mu)+\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[X(\\mu)^{*}]]\\Big]\\qquad\\qquad\\qquad\\mathrm{(Eq.~28)}}\\\\ &{\\phantom{\\sum_{\\mu\\in\\mathbb{Z}}}\\sum_{j\\in\\Theta_{s}}\\mathcal{T}_{s}(\\mu)\\,\\mathrm{d}H(\\mu)+\\int_{\\mu\\in\\Theta_{s}}T_{s}(\\mu)\\,\\mathrm{d}H(\\mu)+\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[X(\\mu)^{*}]]\\Big]}\\\\ &{\\phantom{\\sum_{\\mu\\in\\mathbb{Z}}}\\sum_{i\\in\\Theta_{s}}\\mathcal{T}_{s}(\\mu)\\,\\mathrm{d}H(\\mu)+\\int_{\\mu\\in\\Theta_{s}}T_{s}(\\mu)\\,\\mathrm{d}H(\\mu)+\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[X(\\mu)^{*}]]\\Big]}\\\\ &{\\phantom{\\sum_{\\mu\\in\\mathbb{Z}}}\\leq\\frac{B_{0}\\log\\Delta_{\\eta}^{-1}}{\\Delta_{\\eta}^{2}}\\Bigg[\\sum_{\\mu>\\tilde{\\mu}}P_{\\mu\\sim\\tilde{\\mu}}\\bigg]+O(\\Delta_{\\eta}^{-1/2})+\\mathbb{E}_{\\mu\\sim H}\\Big[\\mathbb{E}_{\\mu}[N_{s}(T_{0})\\mathbf{1}[X(\\mu)^{*}]]\\Big]}\\\\ &{\\phantom{\\sum_{\\mu\\in\\mathbb{Z}}}(\\mathbb{E}_{\\mu\\sim\\tilde{\\mu}})\\,\\mathrm{d}\\mu}\\\\ &{=\\frac{B_{0}\\log\\Delta_{\\eta}^{-1}}{\\Delta_{\\eta}^{2}}\\Big[\\mathbb{P}\\{(\\nu^{*}( \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, the total stopping time is bounded as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{z}[\\tau]=\\mathbb{E}[\\displaystyle\\sum_{s=1}^{k}N_{s}(T_{0})]}\\\\ &{\\displaystyle=\\frac{B_{0}\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\left[\\underbrace{\\vphantom{\\Delta_{0}^{k}}k}_{\\mathrm{\\normalfont~spar}}\\mathbb{E}(i^{\\ast}(\\mu)\\neq s,\\mu_{i^{\\ast}(\\mu)}-\\mu_{s}\\leq\\Delta_{0})}_{\\mathrm{\\normalfont~Psum1}}+\\underbrace{\\sum_{s=1}^{k}\\mathbb{P}(i^{\\ast}(\\mu)=s,\\mu_{s}-\\mu_{j^{\\ast}(\\mu)}\\leq\\Delta_{0})}_{\\mathrm{\\normalfont~Psum2}}\\right]}\\\\ &{\\displaystyle+\\left.O(\\Delta_{0}^{-1/2})+\\mathbb{E}[\\tau].\\mathcal{X}^{c}]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The final task we have left is bounding (Psum1) and (Psum2). Let us define $k^{*}(\\pmb{\\mu})$ as the third best arm in $\\pmb{\\mu}$ . For (Psum1), ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\mathrm{(Psum1)}=\\sum_{s=1}^{k}\\mathbb{P}(i^{*}(\\mu)\\neq s,\\mu_{i^{*}(\\mu)}-\\mu_{s}\\leq\\Delta_{0})}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{s=1}^{k}\\mathbb{P}(j^{*}(\\mu)=s,\\mu_{i^{*}(\\mu)}-\\mu_{s}\\leq\\Delta_{0})+\\sum_{s=1}^{k}\\mathbb{P}(j^{*}(\\mu)\\neq s,\\mu_{i^{*}(\\mu)}-\\mu_{s}\\leq\\Delta_{0})}\\\\ {\\displaystyle}&{\\displaystyle\\leq\\mathbb{P}(\\mu_{i^{*}(\\mu)}-\\mu_{j^{*}(\\mu)}\\leq\\Delta_{0})+\\sum_{s=1}^{k}\\mathbb{P}(\\mu_{i^{*}(\\mu)}-\\mu_{k^{*}(\\mu)}\\leq\\Delta_{0})}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{s=1}^{\\mathbf{\\mu_{N}}}\\mathbb{P}(j^{*}(\\mu)=s,\\mu_{i^{*}(\\mu)}-\\mu_{s}\\leq\\Delta_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\frac{\\delta}{2}+k\\cdot\\mathbb{P}\\big(\\mu_{i^{*}(\\pmb{\\mu})}-\\mu_{k^{*}(\\pmb{\\mu})}\\leq\\Delta_{0}\\big)}\\\\ {\\displaystyle\\leq\\frac{\\delta}{2}+O\\big(\\Delta_{0}^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For (Psum2), ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left(\\mathrm{Psum2}\\right)=\\sum_{s=1}^{k}\\mathbb{P}(i^{*}(\\pmb{\\mu})=s,\\mu_{s}-\\mu_{j^{*}(\\pmb{\\mu})}\\leq\\Delta_{0})}&{}\\\\ {\\displaystyle=\\mathbb{P}\\big(\\mu_{i^{*}(\\pmb{\\mu})}-\\mu_{j^{*}(\\pmb{\\mu})}\\leq\\Delta_{0}\\big)}&{}\\\\ {\\displaystyle=\\mathbb{P}\\big(\\Delta(\\pmb{\\mu})\\leq\\Delta_{0}\\big)\\leq\\frac{\\delta}{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and finally we can conclude ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\tau]\\leq\\frac{B_{0}\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\delta+O(\\Delta_{0}^{-1/2})+\\mathbb{E}[\\tau\\mathbf{1}[\\mathcal{X}^{c}]]\\qquad\\qquad\\mathrm{(Eq.~(34),(Psum1)~and~(Psum2))}}\\\\ {\\displaystyle\\leq\\frac{B_{0}\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\delta+O(\\Delta_{0}^{-1/2})+\\frac{K B\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\cdot\\delta^{2}}\\\\ {\\displaystyle=\\frac{B_{0}\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\delta+O(\\Delta_{0}^{-1/2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and the proof is completed. ", "page_idx": 29}, {"type": "text", "text": "E.1 Bound that holds for any $\\delta$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this case, we need to change the definition of $\\Delta_{0}$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Delta_{0}:=\\operatorname*{min}\\left(\\operatorname*{max}\\left\\{\\Delta\\in(0,1):L(H,\\Delta)\\Delta\\leq\\frac{\\delta}{2}\\right\\},\\operatorname*{min}_{i\\neq j}(\\xi_{i}L_{i j}(H))^{2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Assume that $\\Delta_{0}<\\Delta_{t h r}$ . Then, all the proof flows of Section $\\boldsymbol{\\mathrm E}$ after Eq. (21) follows accordingly, and we obtain the following upper bound: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathsf{g}[\\tau]\\le\\underbrace{\\frac{B_{0}\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\delta+\\frac{2\\sum_{i\\neq j}S_{i j}(H)}{\\sqrt{\\Delta_{0}}}}_{(35)}+\\underbrace{\\frac{K B_{0}\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}}\\delta^{2}}_{(27)}+\\underbrace{B_{0}\\left(\\sum_{i\\neq j\\neq k}Q_{i j k}\\right)\\log\\Delta_{0}^{-1}}_{\\mathrm{From}\\;(\\mathrm{Psuml})}+\\boldsymbol{k}^{2}.\\frac{B_{0}\\log\\Delta_{0}^{-1}}{\\Delta_{0}^{2}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $S_{i j}$ and $Q_{i j k}$ are defined in Eq. (30) and Lemma 21, respectively. ", "page_idx": 29}, {"type": "text", "text": "Eq. (37) also holds for the case of $\\Delta_{0}\\geq\\Delta_{t h r}$ . In this case, from the definition of $R_{0}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nT_{s}(\\mu)=\\mathbb{E}_{\\mu}\\Big[R_{0}(\\operatorname*{max}\\big(\\Delta_{s},\\Delta_{0}\\big)\\big)\\mathbf{1}[\\mathcal{X}]\\Big]=\\mathbb{E}_{\\mu}\\big[R_{0}(\\Delta_{t h r})\\mathbf{1}[\\mathcal{X}]\\big]\\leq R_{0}(\\Delta_{t h r})\\mathbb{P}_{\\mu}[\\mathcal{X}]=R_{0}(\\Delta_{t h r}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\tau]\\leq k\\cdot R_{0}(\\Delta_{t h r})+T_{0}\\cdot\\delta^{2}\\qquad\\qquad\\qquad\\qquad(T_{0}=k\\cdot R_{0}(\\Delta_{0})=R_{0}(\\Delta_{t h r}))}\\\\ {\\displaystyle=k\\cdot R_{0}(\\Delta_{t h r})\\cdot(1+\\delta^{2})\\qquad\\qquad\\qquad\\qquad\\quad(T_{0}=k\\cdot R_{0}(\\Delta_{0})=R_{0}(\\Delta_{t h r}))}\\\\ {\\displaystyle=k\\cdot\\frac{B_{0}\\log\\Delta_{t h r}^{-1}}{\\Delta_{t h r}^{2}}\\cdot(1+\\delta^{2})}\\\\ {\\displaystyle\\leq2k\\frac{B_{0}\\log\\Delta_{t h r}^{-1}}{\\Delta_{t h r}^{2}}\\leq\\mathrm{Eq}.\\,(37).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In summary, we obtain Eq. (37). ", "page_idx": 29}, {"type": "text", "text": "E.2 Proof of Lemmas ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "E.2.1 Proof of Lemma 20 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma 20. For $\\begin{array}{r}{S+1\\leq\\frac{1}{\\sqrt{\\delta}}}\\end{array}$ and for $\\delta\\leq\\big(\\xi_{i}L_{i s}(H)\\big)^{2}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}(i^{*}(\\pmb{\\mu})=i,\\mu_{i}-\\mu_{s}\\in[S\\delta,(S+1)\\delta])\\leq2\\mathbb{P}(i^{*}(\\pmb{\\mu})=i,\\mu_{i}-\\mu_{s}\\leq\\delta).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\int_{\\Theta_{i}}\\mathbf{1}\\left[\\left|\\mu_{i}-\\mu_{s}\\right|\\in\\left[S\\delta,(S+1)\\delta\\right]\\right]\\mathrm{d}H(\\mu)=\\int_{\\Theta_{i}\\setminus\\int_{\\mu_{i}}}\\int_{\\mu_{i}=\\mu_{s}+\\delta\\delta}^{\\mu_{s}+(S+1)\\delta}h_{i}(\\mu_{i})\\,\\mathrm{d}\\mu_{i}\\,\\mathrm{d}H_{\\mathrm{i}}(\\mu_{\\mathrm{i}})}&{}\\\\ {\\leq\\int_{\\Theta_{i}\\setminus\\delta}\\tilde{h}\\Big[h_{i}(\\mu_{s})+\\frac{e^{-1/2}}{\\xi_{i}}(S+1)\\delta\\Big]\\,\\mathrm{d}H_{\\mathrm{i}}(\\mu_{\\mathrm{i}})}&{}\\\\ {\\mathrm{(by~Lipschitz~property~of~Gausisin,~}e^{-1/2}/\\xi_{i}\\mathrm{~is~the~secpest~slope~of~}N(m_{i},\\xi_{i}^{\\prime}}&{}\\\\ {=\\delta(\\underbrace{\\int_{\\Theta_{i}\\setminus\\delta}h_{i}(\\mu_{s})}_{L_{i\\delta}(R)}+\\underbrace{e^{-1/2}}_{\\mathcal{E}_{i}}(S+1)\\delta)}\\\\ {\\leq(L_{i s}(H)+\\frac{1}{\\xi_{i}}(S+1)\\delta)\\delta.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "When $\\begin{array}{r}{S+1<\\frac{1}{\\sqrt{\\delta}}}\\end{array}$ and $\\sqrt{\\delta}<L_{i s}(\\pmb{H})\\xi_{i}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{\\Theta_{i}}\\mathbf{1}\\left[\\left|\\mu_{i}-\\mu_{j}\\right|\\in\\left[S\\delta,(S+1)\\delta\\right]\\right]\\mathrm{d}H(\\pmb{\\mu})\\leq2L_{i s}(H)\\delta\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "as intended. ", "page_idx": 30}, {"type": "text", "text": "E.2.2 Proof of Lemma 21 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We will show that the probability that three or more arms are $\\delta$ -close is $O(\\delta^{2})$ . Namely: ", "page_idx": 30}, {"type": "text", "text": "Lemma 21. We have $\\mathbb{P}(\\mu_{i^{*}(\\pmb{\\mu})}-\\mu_{k^{*}(\\pmb{\\mu})}\\leq\\Delta_{0})=O(\\delta^{2}).$ ", "page_idx": 30}, {"type": "text", "text": "Proof. For any $i\\neq j\\neq k\\in[k]$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int\\mathbf{1}\\left[|\\mu_{i}-\\mu_{j}|,|\\mu_{i}-\\mu_{k}|\\leq\\delta\\right]\\mathrm{d}H(\\mu)}\\\\ &{=\\displaystyle\\int_{\\mu_{1},\\mu_{j}=\\mu_{i}-\\delta}\\int_{\\mu_{k}=\\mu_{i}-\\delta}^{\\mu_{i}+\\delta}h_{j}(\\mu_{j})h_{k}(\\mu_{k})\\,\\mathrm{d}\\mu_{k}\\,\\mathrm{d}\\mu_{j}\\,\\mathrm{d}H_{\\langle j k}(\\mu_{j k})}\\\\ &{\\leq\\displaystyle\\int_{\\mu_{1},\\mu_{k}}\\int_{\\mu_{j}=\\mu_{i}-\\delta}^{\\mu_{i}+\\delta}\\int_{\\mu_{k}=\\mu_{i}-\\delta}^{\\mu_{i}+\\delta}\\left(h_{j}(\\mu_{i})+\\frac{e^{-1/2}\\delta}{\\xi_{j}}\\right)\\left(h_{k}(\\mu_{i})+\\frac{e^{-1/2}\\delta}{\\xi_{k}}\\right)\\mathrm{d}\\mu_{k}\\,\\mathrm{d}\\mu_{j}\\,\\mathrm{d}H_{\\langle j k}(\\mu_{\\backslash j k})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{(Lipschitz~property~of~Gaussian}}\\\\ &{\\leq\\left(2\\delta\\right)^{2}\\displaystyle\\int_{\\mu_{i},\\mu_{k}}\\left[h_{j}(\\mu_{i})h_{k}(\\mu_{i})+O(\\delta)\\right]\\mathrm{d}H_{\\langle j k}(\\mu_{\\backslash j k})=O(\\delta^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "F Experimental details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The code used in the experiments for this paper can be found at the following link: https:// github.com/jajajang/FC_BAI_Bayes. ", "page_idx": 30}, {"type": "text", "text": "F.1 Stopping condition ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "TTTS We use our theoretical results stated in Section 5 for our stopping criterion. For TTTS, we use Chernoff\u2019s stopping rule, as Garivier and Kaufmann [2016], Jourdan et al. [2022] did. Here is the description of how it works: for each arm $i,j\\in[k]$ , let ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{i j}(t):=\\frac{N_{i}(t)}{N_{i}(t)+N_{j}(t)}\\hat{\\mu}_{i}(t)+\\frac{N_{j}(t)}{N_{i}(t)+N_{j}(t)}\\hat{\\mu}_{j}(t),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and define ", "page_idx": 31}, {"type": "equation", "text": "$$\nZ_{i j}(t):=N_{i}(t)\\cdot K L_{i}(\\hat{\\mu}_{i},\\hat{\\mu}_{i j})+N_{j}(t)\\cdot K L_{j}(\\hat{\\mu}_{j},\\hat{\\mu}_{i j}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now the stopping time is defined as: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tau_{T T T S}:=\\operatorname*{inf}\\left\\{t\\in\\mathbb{N}:\\operatorname*{max}_{a\\in[k]}\\operatorname*{min}_{b\\in[k]\\setminus a}Z_{a b}(t)\\geq\\beta(t,\\delta)\\right\\}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for some threshold function $\\beta(t,\\delta)$ , which is defined by the following proposition of Garivier and Kaufmann [2016]: ", "page_idx": 31}, {"type": "text", "text": "Theorem 22 (Garivier and Kaufmann 2016, Proposition 12). Let $\\pmb{\\mu}$ be an exponential family bandit model. Let $\\delta\\,\\in\\,(0,1)$ and $\\alpha\\,>\\,1$ . There exists a constant $C\\,=\\,C(\\alpha,k)$ such that whatever the sampling strategy, using Chernoff\u2019s stopping rule with the threshold ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\beta(t,\\delta)=\\log\\frac{C t^{\\alpha}}{\\delta}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "ensures that for all $\\mu,\\mathbb{P}_{\\mu}\\!\\left(\\tau<\\infty,J\\neq i^{*}(\\pmb{\\mu})\\right)\\leq\\delta.$ ", "page_idx": 31}, {"type": "text", "text": "In this theorem, we give an advantage to the stopping time of TTTS by setting $\\alpha\\,=\\,1,C\\,=\\,1$ . Theoretically, $C(\\alpha,\\bar{k})>1$ and $C\\to\\infty$ as $\\alpha\\rightarrow1_{+}$ , but we set the threshold smaller than the theoretical guarantee so that TTTS stops earlier. ", "page_idx": 31}, {"type": "text", "text": "TTUCB We followed the stopping rule of the original paper Jourdan and Degenne [2022b]. Let $\\begin{array}{r}{\\mathcal{C}_{G}(x):=\\operatorname*{min}_{\\lambda\\in(\\frac{1}{2},1]}\\frac{2\\lambda-2\\lambda\\log\\bar{(}\\bar{4}\\lambda)+\\bar{\\log\\zeta}(2\\lambda)-0.5\\log(\\bar{1}-\\lambda)+\\bar{x}}{\\lambda}}\\end{array}$ 2\u03bb\u22122\u03bb log(4\u03bb)+log \u03b6\u03bb(2\u03bb)\u22120.5 log(1\u2212\u03bb)+xwhere \u03b6 is a Riemann \u03b6 function, and ", "page_idx": 31}, {"type": "equation", "text": "$$\nc(n,\\delta):=2\\mathcal C_{G}(\\frac12\\log\\frac{k-1}{\\delta})+4\\log(4+\\log\\frac{n}{2}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\hat{i}_{t}:=\\arg\\operatorname*{max}_{i\\in[k]}\\hat{\\mu}_{i}(t)$ , the empirical best arm at step $t$ . The TTUCB algorithm stops when ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\neq\\hat{i}_{t}}\\frac{\\hat{\\mu}_{\\hat{i}}(t)-\\hat{\\mu}_{i}(t)}{\\sqrt{\\frac{1}{N_{\\hat{i}}_{t}(t)}+\\frac{1}{N_{i}(t)}}}\\geq\\sqrt{c(t,\\delta)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "When the algorithm stops sampling, the TTUCB algorithm recommends the empirical best arm as its final suggestion. ", "page_idx": 31}, {"type": "text", "text": "Since the computation of ${\\mathcal{C}}_{G}(x)$ involves optimization, it is computationally heavy when the number of samples is excessively large (as our Table 1). Instead, we approximated $\\mathcal{C}_{G}(x)\\approx x+\\log x$ as mentioned in Jourdan and Degenne [2022b]. ", "page_idx": 31}, {"type": "text", "text": "F.2 NoElim algorithm ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The NoElim algorithm is shown in Algorithm 2. ", "page_idx": 31}, {"type": "text", "text": "F.3 Tables including computation time ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For all tables in this section, Comp represents the average computation time (second). ", "page_idx": 31}, {"type": "text", "text": "Input: Confidence level $\\delta$ , prior $\\pmb{H}$   \n$\\begin{array}{r}{\\bar{\\Delta\\bar{\\mathbf{\\rho}}_{0}}:=\\frac{\\delta}{4L(H)}}\\end{array}$   \nInitialize the candidate of best arms $\\mathcal{A}(1)=[k]$   \n$t=1$   \nwhile True do Draw each arm in $[k]$ once. {Main Difference with Algorithm 1}. $t\\to t+|A(t)|\\ \\hat{\\Delta}^{\\mathrm{safe}}(t)$ . for $i\\in\\mathcal{A}(t)$ do Calculate $\\mathrm{UCB}(i,t)$ and $\\mathrm{LCB}(i,t)$ from (5). if $\\mathrm{UCB}(i,t)\\leq\\operatorname*{max}_{j}\\mathrm{LCB}(j,t)$ then $\\mathcal{A}(t)\\gets\\mathcal{A}(t)\\setminus\\{i\\}$ . end if end for if $|\\mathcal{A}(t)|=1$ then Return arm $J$ in $\\boldsymbol{A}(t)$ . end if Calculate safe empirical gap if $\\hat{\\Delta}^{\\mathrm{safe}}(t)\\leq\\Delta_{0}$ then Return arm $J$ which is uniformly sampled from $\\boldsymbol{A}(t)$ . end if   \nend while ", "page_idx": 32}, {"type": "table", "img_path": "hFTye9Ge40/tmp/d6f5ff8c6f97df144f6564238c3b00e256a0f794c9ebb48b9d46f65ba4a369a5.jpg", "table_caption": ["Table 6: Extended version of Table 1 with computation time. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "F.4 Additional experiment results - Multiple arms, different prior mean/variance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In Section 6, we only used $k=2$ arms for the simulation of Table 1. We made a brief comparison between Algorithm 1 and TTUCB in $k=10$ arm environment with a prior distribution where prior means and variances are all different. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Number of arms $K=10$   \n\u2022 Prior means: we sample 10 random numbers from $N(0,1)$ before the experiment starts, and set them as prior means. Here is the list of prior means: [-0.053 0.528 -0.332 -0.368 -0.273 0.909 0.418 -1.17 0.873 -0.405]   \n\u2022 Prior variance: we sample 10 random numbers from Unif([0.5, 1.5]) before the experiment starts, and set them as prior means. Here is the list of prior std: [0.604 1.477 1.163 0.988 0.560 0.513 0.997 1.332 0.828 0.833]   \n\u2022 Instance variance: we sample 10 random numbers from Unif([0.5, 1.5]) before the experiment starts, and set them as prior means. Here is the list of instance std: [1.498, 1.262, 1.485, 0.963, 1.375, 0.969, 1.357, 1.238, 1.088, 0.699]   \n\u2022 Number of experiments: 500 ", "page_idx": 32}, {"type": "text", "text": "\u2022 We stop additional sampling of TTUCB when its number of samples is over $10^{8}$ because of the time constraint. This means we gave some \u2018advantage\u2019 on TTUCB about expected stopping time (since it makes TTUCB stop earlier than it should.) ", "page_idx": 33}, {"type": "text", "text": "Result Our algorithm had the average stopping time of $1.1\\times10^{5}$ , while TTUCB had the average stopping time of $7.1\\times10^{5}$ , again proving the superiority of Algorithm 1 over TTUCB in Bayesian settings. ", "page_idx": 33}, {"type": "table", "img_path": "hFTye9Ge40/tmp/d5d768e83ab9722ecc6b4560266afe286ac30e59daa4126fff2fc22d1f568508.jpg", "table_caption": ["Table 8: Multiple arms. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "F.5 Miscellaneous ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Computation of $\\Delta_{0}$ From Definition 2, ", "page_idx": 33}, {"type": "equation", "text": "$$\nL_{i j}(\\pmb{H}):=\\int_{-\\infty}^{\\infty}h_{i}(x)h_{j}(x)\\prod_{s:s\\in[k]\\backslash\\{i,j\\}}H_{s}(x)\\,\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In Scipy package, there are functions for computing the cumulative function of Gaussian $H_{s}$ (scipy.norm.cdf) and $h_{i}$ (scipy.norm.pdf). Scipy package also supports the numerical integration (scipy.integrate.quad) which we use to numerically compute $L_{i j}$ in our experiments. ", "page_idx": 33}, {"type": "text", "text": "Codes The codes are in the following GitHub repository: https://github.com/jajajang/ FC_BAI_Bayes. ", "page_idx": 33}, {"type": "text", "text": "Hardware We used Python 3.7 as our programming language and Macbook Pro M2 16 inch as our hardware. ", "page_idx": 33}, {"type": "text", "text": "G Scale restriction on $\\delta$ for each theorem ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The second result of the Lemma 9 is used for the lower bound. For this result to hold, we need the following two conditions for $D_{1}$ : ", "page_idx": 33}, {"type": "text", "text": "\u2022 Proof of Lemma 1, second result: For the proof of Eq. (9), we used $\\begin{array}{r}{|m_{i}|\\leq\\frac{1}{2\\sqrt{\\Delta}}}\\end{array}$ . \u2022 Proof of Lemma 1, second result: For the proof of Eq. (9), we used $\\begin{array}{r l}{\\frac{1}{\\xi_{i}}\\Delta^{2}}&{{}>}\\end{array}$ $\\begin{array}{r}{2\\exp\\biggl(-\\frac{1}{8\\Delta\\xi_{i}^{2}}\\biggr)+2\\exp\\biggl(-\\frac{1}{8\\Delta\\xi_{j}^{2}}\\biggr)}\\end{array}$ . To satisfy this condition, $\\Delta<D_{0}(H)$ where $D_{0}(H):=\\left\\{\\!\\!W(-\\frac{1}{32\\operatorname*{max}_{i\\in[k]}\\xi_{i}^{3/2}})\\quad\\mathrm{If}\\,\\operatorname*{max}_{i\\in[k]}\\xi_{i}>\\sqrt[3]{\\frac{e^{2}}{2^{10}}}\\,.\\right.$ ", "page_idx": 33}, {"type": "text", "text": "Here $W$ is the Lambert W function with the principal branch. ", "page_idx": 33}, {"type": "text", "text": "For the lower bound proof, we consider sufficiently small $\\delta$ subject to the following constraints on $\\begin{array}{r}{\\tilde{\\Delta}=\\frac{32e^{4}}{L(H)}\\delta}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "\u2022 Two conditions above for Lemma 9.   \n\u2022 For t $\\begin{array}{r l}&{\\mathrm{xe~Lemma~12:~}\\tilde{\\Delta}<D_{1}(H):=\\operatorname*{min}_{i\\neq j}\\left[\\left|\\frac{m_{i}}{\\sigma_{i}^{2}}-\\frac{m_{j}}{\\sigma_{j}^{2}}\\right|^{-1},\\left[\\frac{1}{2\\sigma_{i}^{2}}+\\frac{1}{2\\sigma_{j}^{2}}\\right]-2\\right].}\\\\ &{\\mathrm{ake~}L^{\\prime}(H,\\tilde{\\Delta})\\tilde{\\Delta}\\in(\\frac{1}{2}L(H),2L(H)),\\tilde{\\Delta}\\leq\\frac{L(H)}{4\\sum_{i\\in[k]}\\frac{k-1}{\\xi_{i}}}.}\\end{array}$ \u2022 To m ", "page_idx": 33}, {"type": "text", "text": "In summary, Theorem 1 holds for any ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\delta\\leq\\delta_{L}:=\\frac{L(\\pmb{H})}{32e^{4}}\\cdot\\operatorname*{min}\\left(D_{0}(\\pmb{H}),D_{1}(\\pmb{H}),\\operatorname*{min}_{i\\in[k]}\\frac{1}{4m_{i}^{2}},\\frac{L(\\pmb{H})}{4(k-1)\\sum_{i\\in[k]}\\frac{1}{\\xi_{i}}}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the upper bound proof (Theorem 6), we consider $\\delta$ such that $\\begin{array}{r}{\\Delta_{0}=\\frac{\\delta}{4L(H)}}\\end{array}$ satisfies the following conditions: ", "page_idx": 34}, {"type": "text", "text": "$\\Delta_{0}<\\frac{L(H)}{\\sum_{i\\in[k]}\\frac{k-1}{\\xi_{i}}}$ to make $L(H,\\Delta_{0})\\cdot\\Delta_{0}\\leq2L(H)\\Delta_{0}$ by the first result of Lemma 9. \u2022 $\\begin{array}{r}{\\Delta_{0}\\leq\\operatorname*{min}_{i,j\\in[k],i\\neq j}(L_{i j}\\xi_{i})^{2}}\\end{array}$ for the proof and usage of Lemma 20, and \u2022 $\\Delta_{0}<\\Delta_{t h r}$ . ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We accurately present the paper\u2019s contributions and scope in the abstract and introduction. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We present our limitations and possible future works in discussion and future works. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide the full set of assumptions and problem settings in Section 2 and Appendix G. Additionally, we include all the proofs in the Appendix. Throughout multiple revisions, we have ensured the correctness of our proofs. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide all the experimental details in Section 6 and Appendix F. Additionally, we provide our code to facilitate the reproduction of our results. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide open access to the data and code with sufficient instructions. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We specify all the experimental details in Section 6 and Appendix F. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: In Section 6 we provide average stopping time, maximum stopping time, and the ratio of misidentification to report our result statistically. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide the full information on the computer resource in Appendix F. In addition, we provide the full table in Appendix F to present our time of execution. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: This research conforms with the NeurIPS Code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This work poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]