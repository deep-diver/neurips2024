[{"type": "text", "text": "High Rank Path Development: an approach to learning the filtration of stochastic processes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiajie Tao   \nDepartment of Mathematics   \nUniversity College London ucahjta@ucl.ac.uk Hao Ni   \nDepartment of Mathematics   \nUniversity College London h.ni@ucl.ac.uk Chong Liu\u2217   \nInstitute of Mathematical Sciences ShanghaiTech University   \nliuchong@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the flitration, of stochastic processes; however was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems. Code is available at https://github.com/DeepIntoStreams/High-Rank-PCF-GAN.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A popular criterion for measuring the differences between two stochastic processes is the weak convergence. In this framework, one views stochastic processes as path-valued random variables and then defines the convergence for their laws, which are distributions on path space. However, this viewpoint ignores the filtration of stochastic processes, which models the evolution of information, and therefore such loss may have negative implications in multi-period optimisation problems. For example, for the American option pricing task, even if the two underlying processes are stochastic processes with very similar laws, the corresponding price of American options can be completely different, see a toy example A.1 in Appendix A.1. To address this shortcoming of weak convergence, D. Aldous [1] introduced the notion of extended weak convergence. The central object in this methodology is the so-called prediction process, which consists of conditional distributions of the underlying process based on available information at different time beings, and therefore reflects how the associated information flow (i.e., filtration) affects the prediction of the future evolution of the underlying process as time varies. Instead of considering the laws of processes (i.e., distributions on path space) in weak convergence, one compares the laws of prediction processes, which are distributions on the measure-valued path space, in extended weak convergence. Since the knowledge of filtration is captured through taking conditional distributions, it was shown in [3] the topology induced by extended weak convergence, which belongs to the so-called adapted weak topologies2, fully characterise essential properties of stochastic processes and endow multiperiod optimisation problems with continuity, provided filtration is generated by the process itself. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While the theoretical contributions to adapted weak topologies flourish in recent years (e.g., [3], [2], [4]), the related work on numerics is still very sparse because of the very complex nature of these topologies. In this paper, we propose a novel metric called High Rank Path Characteristic Function (HRPCFD) which can metrise the extended weak convergence, and, more importantly, admits an efficient implementation algorithm. The core idea of this approach is built on top of the unitary feature of $\\mathbb{R}^{d}.$ -valued paths ([18], [7]), which exploits the noncommutativity and the group structure of the unitary developments to encode information on order of paths. Based on the same consideration, Lou et al. [19] introduced the Path Characteristic Function (PCF) for stochastic pro", "page_idx": 1}, {"type": "image", "img_path": "w28i9oe9Xr/tmp/79130886310ae9582f420543bbead4a10c8a2e78e7b60477a3de705e3217e28b.jpg", "img_caption": ["Figure 1: The high-level illustration of the high rank path development. Here the prediction process $\\hat{X}_{t}:=\\mathbb{P}(X|\\mathcal{F}_{t})$ for all $t\\,\\in\\,[0,T]$ , $\\Phi_{\\hat{X}_{t}}(M_{1})$ is the PCF of the prediction process and $\\mathcal{U}_{M_{1},M_{2}}(\\hat{X})$ is the high rank development of the path $t\\mapsto\\Phi_{\\hat{X}_{t}}(M_{1})$ under the linear map $M_{2}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "cesses, which induces a computable distance (namely, PCFD) to metrise the weak convergence. As extended weak convergence is defined in terms of laws of prediction processes which are measurevalued stochastic processes, the scheme of PCF remains valid in adapted weak topologies as long as one can construct a PCF of measure-valued paths. One of the main contributions of the present work is to give such a suitable notion via the so-called high rank path development (see Figure 1 for illustration); moreover, we can show that the induced distance (called HRPCFD) does not only characterise the more complicated extended weak convergence, but also inherits almost all favourable analytic properties of classical PCFD mentioned in [19]. Since the measure-valued paths take values in an infinite dimensional nonlinear space, such a generalisation of the results in [19] from $\\mathbb{R}^{d}$ -valued paths to measure-valued paths is much technically involved and therefore significantly nontrivial. ", "page_idx": 1}, {"type": "text", "text": "On the numerical side, we design an efficient algorithm to train HRPCFD from data and construct the HRPCF-GAN model with HRPCFD as the discriminator for time series generation. A key computational challenge in applying distances based on extended weak topology is the accurate and efficient estimation of the conditional probability measure. To address this issue, we have implemented a sequence-to-sequence regression module that effectively resolves this bottleneck. Our work is the first of its kind to apply the adapted weak topology for generative models on time series generation. Moreover, to validate the effectiveness of our approach, we conduct experiments in (1) hypothesis testing to classify different stochastic processes, and (2) conditional time series generation to predict the future time series given the past time series. Our HRPCF-GAN can be viewed as a natural generalisation of PCF-GAN [19] to the setting of extended weak convergence, so that the data generated by HRPCF-GAN possesses not only a similar law but also a similar flitration with the target model. The numerical experiments validate the out-performance of this new approach based on HRPCFD compared with several state-of-the-art GAN models for time series generation in terms of various test metrics. ", "page_idx": 1}, {"type": "text", "text": "Related work. So far most of existing statistical and numerical methods for handling stochastic processes (e.g., [9, 16, 19]) are based on weak convergence, and the results on numerical implementation of adapted weak topologies are rather limited. The most relevant work is [21], whose theoretical foundation roots in [5]. The present paper shares a similar philosophy with [21] in the sense that both methods for defining metrics for extended weak convergence rely on the construction of a feature of the measure-valued path by transforming it into a linear space-valued path. In contrast to [21], where a measure-valued path is lifted to an infinite-dimensional Hilbert space, we reduce measure-valued paths into matrix-valued paths through unitary development which allows us to apply the techniques from [19] to design the algorithm. Another remarkable point is that in [21] one has to solve a large family of PDEs to compute the distance, which can be avoided in the numerical estimation of the HRPCFD proposed here. On the other hand, as Wasserstein distances can metrise weak convergence, the so-called causal Wasserstein distances can be used to measure adapted weak topologies. One related work is [22] which can be seen as an improved variant of the Sinkhorn divergence tailored to sequential data. Note that the discriminator (i.e., causal Wasserstein metric) used in [22] is slightly weaker than the HRPCFD, as the latter is actually equivalent to the bi-causal Wasserstein distance. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Prediction Processes and Extended Weak Convergence ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $I\\,=\\,\\{0,\\ldots,T\\}$ and $X\\,=\\,(X_{t})_{t\\in{I}}$ be an $\\mathbb{R}^{d}$ \u2013valued stochastic process defined on a filtered stochastic basis $(\\dot{\\Omega}^{X},\\mathcal{F},\\mathbb{F}\\,=\\,(\\mathcal{F}_{t})_{t\\in I},\\mathbb{P})$ such that $X$ is adapted to the filtration $\\mathbb{F}$ , i.e., $X_{t}$ is measurable with respect to $\\mathcal{F}_{t}$ for all $t\\in I$ . We call the five\u2013tuple $(\\Omega^{X},{\\mathcal{F}},\\mathbb{F},X,\\mathbb{P})$ a flitered process, and denote it by $\\mathbb{X}$ . Throughout this paper, we will use FP to denote the space of all ( $\\mathbb{R}^{d}.$ \u2013valued) filtered processes on the discrete time interval $I$ , and assume that $\\mathbb{F}$ is the natural filtration in the sense that for every $t\\in I$ , $\\mathcal{F}_{t}=\\sigma(X_{0},\\ldots,X_{t})$ . ", "page_idx": 2}, {"type": "text", "text": "Since each discrete time path $\\pmb{x}\\in(\\mathbb{R}^{d})^{T+1}$ can be uniquely extended to a piecewise linear path on $[0,T]$ by linear interpolation, we will not distinguish the product space $(\\mathbb{R}^{d})^{T+1}$ and the subspace $\\mathbf{\\bar{\\lambda}}:=\\{\\pmb{x}:[0,T]\\rightarrow\\mathbb{R}^{d}:\\pmb{x}$ is piecewise linear $\\}$ of $C^{1-\\mathrm{var}}([0,T],\\mathbb{R}^{d})$ (the space of all continuous functions in $\\mathbb{R}^{d}$ with bounded variation)3. Clearly each stochastic process $X$ can be seen as $\\mathcal{X}$ -valued random variable, and therefore the law of $X$ , denoted by $P_{X}\\,=\\,\\mathbb{P}\\circ X^{-1}$ , belongs to $\\mathcal{P}(\\mathcal{X})$ , the space of probability measures on the path space $\\mathcal{X}$ . Recall that a sequence of filtered processes $\\mathbb{X}^{n}=\\left(\\Omega^{n},\\mathcal{F}^{n},\\mathbb{F}^{n},X^{n},\\mathbb{P}^{n}\\right)$ converges to a limit $\\mathbb{X}$ weakly or in the weak topology (in notation: $\\mathbb{X}^{n}\\xrightarrow{W}\\mathbb{X}\\,)$ ) if the laws $P_{X^{n}}=\\mathbb{P}^{n}\\circ(X^{n})^{-1}$ converges to $P_{X}$ in $\\mathcal{P}(\\boldsymbol{\\mathcal{X}})$ weakly, i.e., for all continuous and bounded functions $f\\in C_{b}(\\mathcal{X})$ , it holds that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathbb{E}_{\\mathbb{P}^{n}}[f(X^{n})]=\\mathbb{E}_{\\mathbb{P}}[f(X)]}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "For each $t\\,\\in\\,I$ , we denote $\\hat{X}_{t}\\,:=\\,\\mathbb{P}(X\\,\\in\\,\\cdot\\vert\\mathcal{F}_{t})$ as the (regular) conditional distribution of $X$ given $\\mathcal{F}_{t}$ , which is a random measure taking values in $\\mathcal P(\\boldsymbol\\chi)$ . We call this measure-valued process $\\hat{X}=(\\hat{X}_{t})_{t\\in{I}}$ the prediction process of the filtered process $\\mathbb{X}$ . By definition it is clear that the state space of $\\hat{X}$ is $\\mathcal{P}(\\mathcal{X})^{T+1}$ and, again, by a routine linear interpolation4, we can embed $\\mathcal{P}(\\mathcal{X})^{T+1}$ into ${\\hat{\\mathcal{X}}}=\\{p:[0,T]\\rightarrow\\mathcal{P}(\\mathcal{X}):p$ is piecewise linear}. Thus the law of $\\hat{X}$ , denoted by $P_{\\hat{X}}=\\mathbb{P}\\circ\\hat{X}^{-1}$ , belongs to $\\mathscr{P}(\\hat{\\mathscr{X}})$ (the space of probability measures on the measure-valued path space $\\hat{\\mathcal X}$ ), where $\\hat{\\chi}$ is endowed with the product topology and $\\mathscr{P}(\\hat{\\mathscr{X}})$ is equipped with the corresponding weak topology. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. \u2022 Two filtered processes $\\mathbb{X}=(\\Omega^{X},\\mathcal{F},\\mathbb{F},X,\\mathbb{P})$ and $\\mathbb{Y}=(\\Omega^{Y},\\mathcal{G},\\mathbb{G},Y,\\mathbb{Q})$ are called synonymous if their prediction processes $\\hat{X}$ and $\\hat{Y}$ have the same law in $\\mathcal{P}(\\hat{\\mathcal{X}})$ , i.e., $P_{\\hat{X}}=P_{\\hat{Y}}$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 A sequence of filtered processes $\\mathbb{X}^{n}=(\\Omega^{n},\\mathcal{F}^{n},\\mathbb{F}^{n},X^{n},\\mathbb{P}^{n}),$ , $n\\in\\mathbb N$ converges to another filtered process $\\mathbb{X}=\\left(\\Omega^{X},\\mathcal{F},\\mathbb{F},X,\\mathbb{P}\\right)$ in the extended weak convergence if the law of their prediction processes ${\\hat{X}}^{n}$ converges to the law of $\\hat{X}$ in $\\mathcal{P}(\\hat{\\mathcal{X}})$ weakly, i.e., for all continuous and bounded functions $\\hat{f}\\,\\in\\,C_{b}(\\hat{\\mathcal{X}})$ , $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathbb{E}_{\\mathbb{P}^{n}}[\\hat{f}(\\hat{X}^{n})]\\,=\\,\\mathbb{E}_{\\mathbb{P}}[\\hat{f}(\\hat{X})]}\\end{array}$ . In notation: Xn\u2212E\u2212\u2212W\u2192 . ", "page_idx": 2}, {"type": "text", "text": "If $\\mathcal{F}_{0}^{n}$ and ${\\mathcal{F}}_{0}$ are the trivial $\\sigma$ -algebra, then $\\hat{X}_{0}^{n}\\,=\\,P_{X^{n}}$ and $\\hat{X}_{0}\\,=\\,P_{X}$ are laws of $X^{n}$ and $X$ respectively, so that $\\mathbb{X}^{n}\\xrightarrow{E W}\\mathbb{X}$ certainly implies that $\\mathbb{X}^{n}\\xrightarrow{W}\\mathbb{X}$ . This implies that extended weak convergence is stronger than weak convergence. Moreover, the extended weak convergence induces the correct topology in multi-period decision making problems, as the next theorem (see [1, 3]) shows. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.2. The extended weak convergence provides continuity for the value functions in multiperiod optimisation problems (e.g., optimal stopping problem, utility maximisation problem), as long as the reward function is continuous and bounded. ", "page_idx": 3}, {"type": "text", "text": "Admittedly, the above notions related to extended weak convergence (e.g., the spaces $\\hat{\\chi}$ and $\\mathscr{P}(\\hat{\\mathscr{X}})$ , the weak convergence in $\\mathscr{P}(\\hat{\\mathscr{X}})$ etc.) are rather abstract. Therefore, we provide some simple examples in Appendix A.1 to explain these notions in a more transparent way. We refer readers to [1] and [3] for more details on extended weak convergence. ", "page_idx": 3}, {"type": "text", "text": "2.2 Path Development and Path Characteristic Function (PCF) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this subsection, we review some important notions and properties of $\\mathbb{R}^{d}$ -valued path development and characteristic function (PCF) for $\\mathbb{R}^{d}$ -valued stochastic processes, which will be used later to construct characteristic functions for measure-valued stochastic processes. More technical details on PCF can be found in Appendix A.2. We also refer readers to [19] and [18] for a more detailed discussion on this topic. ", "page_idx": 3}, {"type": "text", "text": "For $m\\in\\mathbb{N}$ , let $\\mathbb{C}^{m\\times m}$ be the space of $m\\times m$ complex matrices, $I_{m}$ denote the identity matrix in $\\mathbb{C}^{m\\times m}$ , and $^*$ be conjugate transpose. Write $U(m)$ and $\\mathfrak{u}(m)$ for the Lie group of $m\\times m$ unitary matrices and its Lie algebra, resp.: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU(m)=\\big\\{A\\in\\mathbb{C}^{m\\times m}:A^{*}A=I_{m}\\big\\},\\quad\\mathfrak{u}(m)=\\big\\{A\\in\\mathbb{C}^{m\\times m}:A+A^{*}=0\\big\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ denote the space of linear mappings from $\\mathbb{R}^{d}$ to $\\mathfrak{u}(m)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3. Let $\\pmb{x}\\in C^{1-\\nu a r}([0,T],\\mathbb{R}^{d})$ be a continuous path with bounded variation and $M\\in$ $\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ be a linear map. The unitary feature of $\\textbf{\\em x}$ under $M$ is the solution $y:[0,T]\\to U(m)$ to the following equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd{\\pmb y}_{t}={\\pmb y}_{t}M(d{\\pmb x}_{t}),\\quad{\\pmb y}_{0}=I_{m},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\pmb y}_{t}M(d{\\pmb x}_{t})$ denotes the usual matrix product. We write $\\mathcal{U}_{M}(\\pmb{x}):=\\pmb{y}_{T}$ , i.e., the endpoint of the solution path, and by an abuse of notation, also call it the unitary feature of $\\textbf{\\em x}$ (under $M$ ). ", "page_idx": 3}, {"type": "text", "text": "The unitary feature is a special case of the path development, for which one may consider paths taking values in any Lie group $G$ . It is easy to see that for piecewise linear path ${\\pmb x}=({\\pmb x}_{0},...,{\\pmb x}_{T})\\in\\mathcal{X}$ , it holds $\\begin{array}{r}{\\mathcal{U}_{M}(\\pmb{x})=\\prod_{i=1}^{T}\\exp(M(\\Delta\\pmb{x}_{i}))}\\end{array}$ for $\\Delta\\pmb{x}_{i}=\\pmb{x}_{i}-\\pmb{x}_{i-1}$ and exp denotes the matrix exponential. We now use the unitary feature to define the Path Characteristic Function (PCF) for $\\mathbb{R}^{d}$ -valued stochastic processes: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4. Let $\\mathbb{X}\\,=\\,(\\Omega^{X},\\mathcal{F},\\mathbb{F},X,\\mathbb{P})$ be a filtered process and $P_{X}$ be its law. The Path Characteristic Function $(P C F)$ of $\\mathbb{X}$ is the map $\\begin{array}{r}{\\Phi_{\\mathbb{X}}:\\bigcup_{m\\in\\mathbb{N}}\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))\\to\\bigcup_{m\\in\\mathbb{N}}\\mathbb{C}^{m\\times m}}\\end{array}$ given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi_{\\mathbb{X}}(M):=\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)]=\\int_{X}\\mathcal{U}_{M}(\\pmb{x})P_{X}(d\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 2.5. In the present work, we only consider the discrete-time processes defined on $I\\,=$ $0,\\ldots,T$ , and therefore the time index $t$ appeared in the stochastic process $X_{t}$ and its flitration $\\mathcal{F}_{t}$ only takes values in $0,\\ldots,T$ . It is just a convention in the rough path community that one views a discrete time path defined on $I=0,\\dots,T$ as a piecewise linear path defined on the continuous time interval $[0,T]$ by a routine linear interpolation, because such identification may make some formulations and computations easier (e.g., by doing so the unitary feature of a path can be formulated as the solution of an ODE on $[0,T],$ . ", "page_idx": 3}, {"type": "text", "text": "To distinguish $\\Phi_{\\mathbb{X}}$ from the so-called high rank PCF which will be defined in the next subsection, we also call $\\Phi_{\\mathbb{X}}$ the rank 1 PCF. The next theorem (see [19, Theorem 3.2]) justifies why $\\Phi_{\\mathbb{X}}$ defined in Definition 2.4 is called PCF for path-valued random variables. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.6 (Characteristicity of laws). For X and Y two flitered processes, they have the same law (i.e., $P_{X}=P_{Y}$ ) if and only if $\\Phi_{\\mathbb{X}}=\\Phi_{\\mathbb{Y}}$ . ", "page_idx": 4}, {"type": "text", "text": "The characteristicity of PCF allows us to define a novel distance on FP which metrises the weak convergence (locally). This metric is called the PCF-based distance (PCFD), see [19, Definition 3.3]. Moreover, such PCFD possesses many nice analytic properties including boundedness ([19, Lemma 3.5]), Maximum Mean Discrepancy (MMD, [19, Proposition B.10]) among others, see [19, Section 3.2], which ensures the feasibility of using PCFD in numerical aspect. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.7. Rigorously speaking, we need to add an additional time component to every $\\mathbb{R}^{d}$ -valued process $X$ (i.e., consider $\\overset{\\cdot}{X}_{t}\\,=\\,\\overset{\\circ}{(t,X_{t}^{1},\\ldots,X_{t}^{d})})$ to guarantee Theorem 2.6 holds true. We will always implicitly use such time-augmentation throughout the whole paper and still write $X$ instead of $\\bar{X}$ for simplicity of notations. ", "page_idx": 4}, {"type": "text", "text": "3 High Rank Path Development Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now want to construct a characteristic function for prediction processes and use it to metrise the extended weak convergence just like PCFD metrises the weak convergence. Since prediction processes are $\\hat{\\chi}$ -valued random variables, we first need to find a suitable notion of unitary feature/development for measure-valued paths. ", "page_idx": 4}, {"type": "text", "text": "3.1 High Rank Development of Prediction Processes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a filtered process $\\mathbb{X}=(\\Omega^{X},\\mathcal{F},\\mathbb{F},X,\\mathbb{P})$ , remember that its prediction process $\\hat{X}$ satisfies $\\hat{X}_{t}=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{t})$ for $t\\in I$ . Now, for a linear operators $M\\in{\\mathcal{L}}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ for $n\\in\\mathbb{N}$ , we take the conditional expectation of $\\mathcal{U}_{M}$ against $\\hat{X}_{t}=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{t})$ to obtain a $\\mathbb{C}^{n\\times n}$ \u2013valued stochastic process $\\Phi_{\\hat{X}_{t}}(M)\\ {\\stackrel{=}{=}}\\ \\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}]$ , $t\\in I$ . Then, for any $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ with some $m\\in\\mathbb{N}$ , the unitary feature $\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\Phi_{\\hat{X}_{t}}(M))$ of $\\mathbb{C}^{n\\times n}$ \u2013valued path $(t\\mapsto\\Phi_{\\hat{X}_{t}}(M))$ is well defined and takes values in the unitary group $U(m)$ . We call each pair $(M,\\mathcal{M})\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))\\times\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ for $(n,m)\\in\\mathbb{N}^{2}$ an admissible pair of unitary representations, and the set of all admissible pairs of unitary representations is denoted by $\\boldsymbol{A}_{\\mathrm{unitary}}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. For $(M,\\mathcal{M})\\in\\mathcal{A}_{u n i t a r y}$ with $M\\,\\in\\,\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n)).$ , $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ and $\\mathbb{X}=$ $(\\Omega^{X},{\\mathcal{F}},\\mathbb{F},X,\\mathbb{P})$ a filtered process with its prediction process $\\hat{X}$ , we call ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{U}_{M,\\mathcal{M}}(\\hat{X}):=\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\Phi_{\\hat{X}_{t}}(M))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the high rank development of the prediction process $\\hat{X}$ under $(M,{\\mathcal{M}})$ . ", "page_idx": 4}, {"type": "text", "text": "See Figure 1 for the schematic overview of the high rank development. From above we can see that the construction of $\\mathcal{U}_{M,\\mathcal{M}}(\\hat{X})$ involves with taking finite dimensional path development in Section 2.2 twice: first use the PCF under $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ to transform each conditional distribution $\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{t})$ into a matrix $\\Phi_{\\hat{X}_{t}}(M)$ , and then apply the unitary feature $\\mathcal{U}_{\\mathcal{M}}(\\cdot)$ to the resulting matrixvalued path $\\mathbf{\\chi}^{\\prime}t\\mapsto\\Phi_{\\hat{X}_{t}}(M))$ for $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 High Rank Path Characteristic Function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the above notion of unitary feature of measure-valued paths, following Definition 2.4, we define the high rank Path Characteristic Function (HRPCF) for filtered processes. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2. For a filtered process $\\mathbb{X}=(\\Omega^{X},\\mathcal{F},\\mathbb{F},X,\\mathbb{P})\\in F P,$ , the function ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi_{\\mathbb{X}}^{2}:A_{u n i t a r y}\\to\\bigcup_{m=1}^{\\infty}\\mathbb{C}^{m\\times m};(M,\\mathcal{M})\\mapsto\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M,M}(\\hat{X})]=\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(t\\mapsto\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}])].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is called the High Rank Path Characteristic Function of $\\mathbb{X}$ (Abbreviation: HRPCF)5. ", "page_idx": 4}, {"type": "text", "text": "$\\Phi_{\\mathbb{X}}^{2}$ is said to be a HRPCF for $\\mathbb{X}$ as it satisfies the following characteristicity of synonym for flitered processes (see Definition 2.1). For a detailed proof please check the Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 (Characteristicity of synonym). Two flitered processes $\\mathbb{X}$ and $\\mathbb{Y}$ are synonymous if and only if they have the same high rank PCF, that is, $\\mathbf{\\dot{\\Phi}}_{\\mathbb{X}}^{2}(M,\\mathbf{\\dot{\\mathcal{M}}})=\\Phi_{\\mathbb{Y}}^{2}(M,\\mathcal{M}),\\forall(\\dot{M},\\dot{\\mathcal{M}})\\in\\mathcal{A}_{u n i t a r y}^{\\circ}$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 A New Distance induced by High Rank PCF ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we will use the second rank PCF to define a distance on FP, which can (locally) characterize the extended weak convergence, as the classical PCFD introduced in subsection 2.2 can metrise the weak topology on FP. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.4. For two filtered processes $\\mathbb{X}$ and Y, let $(M,{\\mathcal{M}})$ be a random admissible pair in $\\mathcal{A}_{u n i t a r y}$ with $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ for some $n_{i}$ , and $\\pmb{\\mathscr{M}}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ for some m. The High Rank Path Characteristic Function-based distance, for short HRPCFD, between $\\mathbb{X}$ and $\\mathbb{Y}$ with respect to $P_{M}$ and $P_{\\mathbf{{\\cal{M}}}}$ is defined by ", "page_idx": 5}, {"type": "equation", "text": "$$\nH R P C F D_{M,M}^{2}(\\mathbb{X},\\mathbb{Y})=\\int\\int d_{H S}^{2}(\\Phi_{\\mathbb{X}}^{2}(M,\\mathcal{M}),\\Phi_{\\mathbb{Y}}^{2}(M,\\mathcal{M}))P_{M}(d M)P_{M}(d\\mathcal{M}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d_{H S}(\\cdot,\\cdot)$ denotes the Hilbert-Schmidt distance6 on $\\mathbb{C}^{m\\times m}$ . ", "page_idx": 5}, {"type": "text", "text": "As previously mentioned in the introduction, the so-defined HRPCFD shares the same analytic properties as the classical PCF, e.g., the separation of points, boundedness and the MMD property, whose proof can be found in Appendix A. Moreover, it metrises a much stronger topology (the extended weak convergence). as shown in the next theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5. Suppose $(\\mathbb{X}^{i})_{i\\in\\mathbb{N}}$ and X are flitered processes whose laws $P_{X^{i}}$ and $P_{X}$ are supported in a compact subset of $\\mathcal{X}$ . Then $\\mathbb{X}^{i}\\xrightarrow{E W}\\mathbb{X}$ iff $H\\widetilde{R P C F D}(\\mathbb{X}^{i},\\mathbb{X})\\rightarrow0,$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\nH\\widetilde{R P C F D}(\\mathbb{X}^{i},\\mathbb{X}):=\\sum_{j=1}^{\\infty}\\frac{\\operatorname*{min}\\{1,H R P C F D_{M_{j},\\mathcal{M}_{j}}(\\mathbb{X}^{i},\\mathbb{X})\\}}{2^{j}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the sequence $(M_{j},\\pmb{\\mathscr{M}}_{j})_{j\\in\\mathbb{N}}$ satisfies that for any $(n,m)\\in\\mathbb{N}^{2}$ there is a $j\\in\\mathbb N$ such that $M_{j}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ and $\\pmb{\\mathscr{M}}_{j}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ and $P_{M_{j}}$ , $P_{\\pmb{\\mathscr{M}}_{j}}$ have full supports for all $j\\in\\mathbb N$ . ", "page_idx": 5}, {"type": "text", "text": "We provide a concrete example in the last paragraph of Appendix A.1 to verify the fact that HRPCFD really reflects the differences of filtrations via an explicit computation. ", "page_idx": 5}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, let $\\mathbb{X}$ and $\\mathbb{Y}$ be two filtered processes with the law $P_{X},P_{Y}\\;\\in\\;{\\mathcal{P}}({\\mathcal{X}})$ , let $\\textbf{X}=$ $(\\pmb{x}_{i})_{i=1}^{N}\\sim P_{X}$ and $\\mathbf{Y}=(\\pmb{y}_{i})_{i=1}^{N}\\sim P_{Y}$ be sample paths. ", "page_idx": 5}, {"type": "text", "text": "4.1 Estimating conditional probability measure and HRPCF ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A fundamental question is to estimate the conditional probability measure $\\hat{X}_{t}=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{t})$ from the finitely many data $({\\pmb x}_{i})_{i=1}^{N}$ , in particular the random variable $\\Phi_{\\hat{X}_{t}}(M)=\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(\\dot{X})|\\mathcal{F}_{t}]$ for any $M\\,\\in\\,\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ . We solve this problem by conducting a regression. Fix $M$ we learn a sequence-to-sequence model $F_{\\theta}^{X}:\\mathbb{R}^{d\\times(\\bar{T}+1)}\\rightarrow\\mathbb{C}^{\\bar{n}\\times n\\times(T+1)}$ , where the input and output pairs are $(\\mathbf{X}_{[0,T]},\\mathcal{U}_{M}(\\mathbf{X}_{[t,T]})_{t=0}^{T})$ . More specifically, we optimize the model parameters of $F_{\\theta}^{X}$ by minimizing the loss function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{RLoss}(\\theta;{\\pmb x},M)=\\sum_{t=0}^{T}\\sum_{{\\pmb x}\\in{\\bf X}}d_{H S}^{2}({\\cal F}_{\\theta}^{X}({\\pmb x}_{[0,T]})_{t},\\mathcal{U}_{M}({\\pmb x}_{[t,T]})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is worth noting that the choice of $F_{\\theta}^{X}$ must be autoregressive models to prevent information leakage. A detailed pseudocode is shown in Algorithm 1. Then, we approximate $\\Phi_{\\mathbb{X}}^{2}$ using the trained regression model $F_{\\theta}^{X}$ following the Algorithm 2. We denote by $\\hat{\\Phi}_{\\mathbb{X}}^{2}$ the estimation of $\\Phi_{\\mathbb{X}}^{2}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n{}^{6}\\mathrm{For}\\,A,B\\in\\mathbb{C}^{m\\times m},d_{\\mathrm{HS}}^{2}(A,B)=\\operatorname{tr}((A-B)(A-B)^{*}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.2 Optimizing HRPCFD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In most empirical applications as we will show in Section 5, we employ HRPCFD as a discriminator under the GAN setting. That is, we optimize the loss function $\\operatorname*{sup}_{M,\\mathcal{M}}\\mathrm{HRPCFD}_{M,\\mathcal{M}}^{2}(\\mathbb{X},\\mathbb{Y})$ . We would approximate the pair of random variables $(M,\\mathcal{M})$ by discrete random variables $\\begin{array}{r l r}{M_{K_{1}}\\!}&{{}=}&{\\!\\frac{1}{K_{1}}\\sum_{i=1}^{K_{1}}M_{i}}\\end{array}$ and $\\begin{array}{r l r}{{\\pmb{\\mathscr{M}}}_{K_{2}}\\!}&{{}=}&{\\!\\frac{1}{K_{2}}\\sum_{i=1}^{K_{2}}{\\pmb{\\mathscr{M}}}_{i}}\\end{array}$ , parametrized by $M_{i}\\ \\in\\ \\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ and $\\mathcal{M}_{i}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)),K_{1},K_{2}\\in\\mathbb{N}$ and optimize so-called Empirical HRPCFD ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{EHRPCFD}_{M_{K_{1}},M_{K_{2}}}^{2}(\\mathbb{X},\\mathbb{Y})=\\frac{1}{K_{1}K_{2}}\\sum_{i=1}^{K_{1}}\\sum_{j=1}^{K_{2}}d_{\\mathrm{HS}}^{2}(\\hat{\\Phi}_{\\mathbb{X}}^{2}(M_{i},\\mathcal{M}_{j}),\\hat{\\Phi}_{\\mathbb{Y}}^{2}(M_{i},\\mathcal{M}_{j})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In practice, the joint training on both $M_{K_{1}}$ and $\\mathcal{M}_{K_{2}}$ is computationally expensive and prone to overfitting. We alleviate this problem by splitting the optimization procedure in the following three steps: 1) Optimize $(M_{i})_{i=1}^{K_{1}}$ to maximize $\\begin{array}{r}{\\bar{\\mathrm{EPCFD}_{M_{K_{1}}}^{2}}(\\mathbf{\\bar{X}},\\mathbf{Y})=\\frac{1}{K_{1}}\\sum_{i=1}^{K_{1}}d_{\\mathrm{HS}}^{2}(\\boldsymbol{\\Phi}_{\\mathbf{X}}(M_{i}),\\boldsymbol{\\Phi}_{\\mathbf{Y}}(M_{i}))}\\end{array}$ $\\begin{array}{r}{(\\Phi_{\\mathbf{X}}(M)=\\frac{1}{N}\\sum_{i=1}^{n}\\mathcal{U}_{M}(\\pmb{x}_{i}))[19,}\\end{array}$ , Section 3.3] , denote by $M_{K_{1}}^{*}=(M_{i}^{*})_{i=1}^{K_{1}}$ the optimized linear maps. 2) Train regression modules $F_{\\theta_{i}}^{X},F_{\\theta_{i}}^{Y}$ for each $M_{i}^{*}$ using data sampled from $P_{X}$ and $P_{Y}$ respectively. 3) Optimize $(\\mathcal{M}_{i})_{i=1}^{K_{2}}$ to maximize EH $\\mathrm{RPCFD}_{M_{K_{1}}^{*},M_{K_{2}}}^{2}(\\mathbb{X},\\mathbb{Y})$ . ", "page_idx": 6}, {"type": "text", "text": "The reason behind it is natural: the optimal set $(M_{i}^{*})_{i=1}^{K_{1}}$ captures the most relevant information that discriminates the distribution $P_{X}$ from $P_{Y}$ . This difference is reflected in the design of higher rank expected path developments through regression models specifically trained for this purpose. Finally, the HRPCFD based on $(M_{i}^{*})_{i=1}^{K_{1}}$ tends to be more significant among other choices of $(M_{i})_{i=1}^{K_{1}}$ , making it a stronger discriminator. ", "page_idx": 6}, {"type": "text", "text": "4.3 HRPCF-GAN for conditional time series generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following [16, 13], we consider the task of conditional time series generation to simulate the law of the future path $\\mathbf{X}_{\\mathrm{future}}\\ :=\\ \\mathbf{X}_{(p,T]}$ given the past path $\\mathbf{X}_{\\mathrm{past}}\\;:=\\;\\mathbf{X}_{[0,p]}$ from samples of $\\mathbf{X}$ . To this end, we propose the so-called HRPCF-GAN by leveraging the autoregressive generator and the trainable HRPCFD as the discriminator. See Figure 2 for the flowchart illustration. ", "page_idx": 6}, {"type": "text", "text": "Conditional autoregressive generator To simulate future time series of length $T\\mathrm{~-~}p$ , we construct a generator $G_{\\theta}$ based on the step-1 conditional generator $g_{\\theta}$ following [16]. This generator, $g_{\\theta}:\\mathcal{X}_{\\mathrm{past}}\\times\\mathcal{Z}\\to\\mathbb{R}^{d}$ , aims to produce a random variable approximating $\\mathbb{P}(X_{t+1}|\\mathcal{F}_{t})$ . By applying $g_{\\theta}$ inductively, we can simulate future paths of arbitrary length. To address the limitation of AR-RNN generator proposed in [16], where $\\mathbb{P}(X_{t+1}|\\mathcal{F}_{t})$ depends solely on $p_{\\|}$ - lagged values of $X_{t}$ , we incorporate an embedding module. This module efficiently extracts past path information into a low-dimensional latent space. The output of this embedding module, along with the noise vector, serves as the input for $g_{\\theta}$ to generate subsequent steps in the fake time series. Further details of our proposed generator are provided in Appendix B.2. ", "page_idx": 6}, {"type": "image", "img_path": "w28i9oe9Xr/tmp/7403cc5f75dc2eaf2b61df89e46dfea5649a5ab841ac5b0cf4f5f9e660ecc963.jpg", "img_caption": ["Figure 2: Flowchart of HRPCF-GAN for learning condition distribution $\\mathbb{P}(X_{\\mathrm{future}}|X_{\\mathrm{past}})$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "High Rank development discriminator To capture the conditional law, we use the HRPCFD as the discriminator of joint law of $(\\mathbf{X}_{\\mathrm{past}},\\mathbf{X}_{\\mathrm{future}})$ under true and fake measures. Here the empirical measures of $M_{K_{1}}$ and ${\\pmb{\\mathscr{M}}}_{K_{2}}$ are model parameters of the discriminator, which are optimized by the following maximization: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{M_{K_{1}},{M_{K_{2}}}}\\mathrm{EHRPCFD}_{M_{K_{1}},{M_{K_{2}}}}^{2}({\\mathbf{X}}_{[0,T]},({\\mathbf{X}}_{[0,p]},G_{\\theta}({\\mathbf{X}}_{[0,p]},z))),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In principle, one can generate the fake data by the generator via Monte Carlo and apply the training procedure outlined in Section 4.2 for training the generative model. However, it would be computationally infeasible due to the need for recalibration of the regression module per generator update. To enhance the training efficiency for the regression module under the fake measure, we use the gradient descent method with efficient initialization obtained by the trained regression model under real data. For each generator, the corresponding regression model parameters are then updated to minimize the RLoss (Section 4.1) on a batch of newly generated samples by $G_{\\theta}$ . The detailed algorithm is described in Algorithm 3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Numerical results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Hypothesis testing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To showcase the power of EHRPCFD in discriminating laws of stochastic processes, we use it as the test statistic in the permutation test. Similar experiments have been done in [21, 15]. By regarding the permutation test as a decision rule, we assess its performance via computing its power (probability of correctly rejecting the null hypothesis) and type-I error (probability of falsely rejecting the null hypothesis). Similar to [15], we compare the law of 3-dimensional Brownian motion $B$ with the set of laws of 3-dimensional fractional Brownian motion $B^{H}$ with Hurst parameter $H$ ranging from [0.4, 0.6]. Details of the methodology and implementation can be found in Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "Baselines We compare the performance of HRPCFD with other test metrics including 1) the linear and RBF signature MMDs [8, 20] and its high-rank derivative, namely High Rank signature MMDs [21]; 2) Classical vector MMDs; 3) PCFD [15, 19]. ", "page_idx": 7}, {"type": "text", "text": "As shown in Table 1 of the test power, HRPCFD consistently outperforms other models, especially when $H$ is close to 0.5. We do see an improvement from the vanilla PCFD by considering a stronger topology. Furthermore, comparing HRPCFD and High Rank signature MMD, we observe a distinct advantage for HRPCFD. This may be due to the challenge of capturing the conditional probability measure, as High Rank signature MMD relies on linear regression for estimation, whereas we obtained a better estimation using a non-linear approach. Additional test metrics such type-I error and computational cost can be found in Appendix C.1. ", "page_idx": 7}, {"type": "table", "img_path": "w28i9oe9Xr/tmp/9d20c8eb93f055f105629051916ce58787a8e648abfc4f267b7909c6747970ea.jpg", "table_caption": [], "table_footnote": ["Table 1: Test power of the distances when $\\overline{{h\\neq0.5}}$ in the form of mean $\\pm$ std over 5 runs. After careful grid search, we set optimal $\\sigma={\\sqrt{0.05}}$ for the RBF signature MMD and classical RBF MMD, whereas $\\sigma_{1}=\\sigma_{2}=1$ for High Rank signature MMD. "], "page_idx": 7}, {"type": "text", "text": "5.2 Generative modeling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To validate the effectiveness of our proposed HRPCF-GAN, we consider the task of learning the law of future time series conditional on its past time series. ", "page_idx": 7}, {"type": "text", "text": "Dataset We benchmark our model on both synthetic and empirical datasets. 1) multivariate fractional Brownian Motion (fBM) with Hurst parameter $H\\,=\\,1/\\bar{4}$ : this dataset exhibits non-Markovian properties and high oscillation. 2) Stock dataset: We collected the daily log return of 5 representative stocks in the U.S. market from 2010 to 2020, sourced from Yahoo Finance. ", "page_idx": 7}, {"type": "text", "text": "Baseline We compare the performance of HRPCF-GAN with well-known models for time-series generation such as RCGAN [10] and TimeGAN [23]. Furthermore, we use PCFGAN [19] as a benchmarking model to showcase the significant improvement by considering the higher rank development as the discriminator. For fairness, we use the same generator structure (LSTM-based) for all these models. ", "page_idx": 7}, {"type": "image", "img_path": "w28i9oe9Xr/tmp/ca5a7207c3a6dd689d9ce8f0f44b1d8a386908f1dd71da69f80283c6e43a572d.jpg", "img_caption": ["Figure 3: Sample plots of the conditional distribution $\\mathbb{P}(X|\\mathcal{F}_{t})$ on fBM conditioned on the same past path, using both true and GAN models (arranged from top to bottom). Each column represents different $t$ . The thick red /green line indicates the conditional mean of the future path estimated by model simulated samples/true models. The shaded red area presents the region of $\\pm$ std of model simulated samples, whereas the shaded area shown corresponds to the region of $\\pm$ theoretical std. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Test metrics To assess the fidelity, usefulness, and diversity of synthetic time series, we consider 7 test metrics, including Auto-Correlation, Cross-Correlation, Discriminative Score, Sig- $W_{1}$ Distance, and Conditional Expectation. For the stock dataset, we also consider a test metric based on American option pricing. A detailed definition of these test metrics can be found in Appendix C.2. ", "page_idx": 8}, {"type": "text", "text": "We summarize in Table 2 the performance comparison between HRPCF-GAN and benchmarking models. For both datasets, HRPCF-GAN consistently outperforms the other models. Focusing on the fBM dataset, HRPCF-GAN achieves the lowest Auto-Correlation (.082) and Cross-Correlation (0.013), which is approximately $21.9\\%/72.3\\%$ lower than the second-best model, indicating better performance in fitting the dynamics of the underlying process across time and feature dimensions. We also observe strong evidence in capturing the conditional probability measure as HRPCFGAN achieves the lowest Conditional Expectation score (1.693 on fBM and 0.56 on Stock). Furthermore, we observe on average an improvement of $34\\%/14\\%$ of HRPCF-GAN with respect to PCFGAN on fBM/Stock datasets respectively. The strong empirical results demonstrate the effectiveness of considering high rank path development to capture the filtration of stochastic processes. Finally, HRPCF-GAN attained the best estimation of an at-the-money American put option, which demonstrates its potential usage for optimal stopping problems in finance. Sample plots from all models conditioned on the same path are also shown in Figures 3, 6 and 7 for a qualitative analysis of generative quality. For additional test metrics, we refer readers to Table 5. ", "page_idx": 8}, {"type": "table", "img_path": "w28i9oe9Xr/tmp/5ccc3b7a0726df525ea9cc2446de14f9d5efd90b28dfb14dd1608fe8b966dc2f.jpg", "table_caption": [], "table_footnote": ["Table 2: Performance comparison of HRPCF-GAN and baselines. The best for each task is shown in bold. Each test metric is shown in the form of mean\u00b1std over 5 runs. "], "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion: In this paper, we apply the unitary feature from rough path theory to define the CF for measure-valued paths, which further induces a distance (HRPCFD) for metrising the extended weak convergence. Theoretically, we prove the key properties of HRPCFD, such as characteristicity, uniform boundedness, etc. Additionally, the numerical experiments validate the out-performance of the approach based on HRPCFD compared with several state-of-the-art GAN models for tasks such as hypothesis testing and synthetic time series generation. ", "page_idx": 9}, {"type": "text", "text": "Limitation and Future work: The suitable choice of network architecture for generating data is crucial in the proposed HRPCF-GAN, which merits further investigation; in particular, it will be interesting to understand how the network architecture impacts the flitration structure of the generated stochastic process. Furthermore, there is room for further improvement on the estimation method of conditional expectation in terms of accuracy and training stability. Possible routes include exploring the interplay between the regression module and the generator. ", "page_idx": 9}, {"type": "text", "text": "Broader impacts: Our approach based on the extended weak convergence has the potential in many important financial and economic applications, such as optimal stopping, utility maximisation and stochastic programming. Unlike classical methods built on top of parametric stochastic differential equations, our non-parametric and data-driven method alleviates the risk of the model mis-specification, providing better solution to complex, real-world multi-period decision making problems. However, like other synthetic data generation models, it also poses risks of misuse, e.g., misrepresenting the synthetic data as real data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "HN is supported by the EPSRC under the program grant EP/S026347/1 and the Alan Turing Institute under the EPSRC grant EP/N510129/1. HN extends her gratitude to Terry Lyons and Hang Lou for insightful discussions. Moreover, HN is grateful to Jing Liu for her help with Figure 1. CL is supported by the National Key Research and Development Program of China: Young Scientist Project 2023YFA1010900. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David Aldous. Weak convergence and the general theory of processes. Unpublished Monograph, 1981.   \n[2] Julio Backhoff-Veraguas, Daniel Bartl, Mathias Beiglboeck, and Manu Eder. Adapted Wasserstein distances and stability in mathematical finance. Finance and Stochastics, 24, 2020.   \n[3] Julio Backhoff-Veraguas, Daniel Bartl, Mathias Beiglboeck, and Manu Eder. All adapted topologies are equal. Probability Theory and Related Fields, 178(3), 2020.   \n[4] Daniel Bartl, Mathias Beiglboeck, and Gudmund Pammer. The Wasserstein spaces of stochastic processes. arXiv:2104.14245, 2021.   \n[5] Patric Bonnier, Chong Liu, and Harald Oberhauser. Adapted topologies and higher rank signatures. The Annals of Applied Probability, 33(3), 2023.   \n[6] K. T. Chen. Integration of paths-a faithful representation of paths by noncommutative formal power series. Trans. Amer. Math. Soc., 89(2), 1958.   \n[7] Ilya Chevyrev and Terry Lyons. Characteristic functions of measures on geometric rough paths. Annals of Probability, 44(6), 2016.   \n[8] Ilya Chevyrev and Harald Oberhauser. Signature moments to characterize laws of stochastic processes. Journal of Machine Learning Research, 2022.   \n[9] Rama Cont, Mihai Cucuringu, Renyuan Xu, and Chao Zhang. TailGAN: Nonparametric scenario generation for tail risk estimation. arXiv:2203.01664, 2022.   \n[10] Crist\u00f3bal Esteban, Stephanie L. Hyland, and Gunnar R\u00e4tsch. Real-valued (medical) time series generation with recurrent conditional GANs, 2017.   \n[11] Peter Friz and Nicolas Victoir. Multidimensional Stochastic Processes as Rough Paths. Cambridge University Press, 2010.   \n[12] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.   \n[13] Daniel Levin, Terry Lyons, and Hao Ni. Learning from the past, predicting the statistics for the future, learning an evolving system. arXiv preprint arXiv:1309.0260, 2013.   \n[14] Mark Leznik, Arne Lochner, Stefan Wesner, and J\u00f6rg Domaschka. [sok] the great gan bake off, an extensive systematic evaluation of generative adversarial network architectures for time series synthesis. Journal of Systems Research, 2(1), 2022.   \n[15] Siran Li, Zijiu Lyu, Hao Ni, and Jiajie Tao. On the determination of path signature from its unitary development, 2024.   \n[16] Shujian Liao, Hao Ni, Marc Sabate-Vidales, Lukasz Szpruch, Magnus Wiese, and Baoren Xiao. Sig-Wasserstein GANs for conditional time series generation. Mathematical Finance, 34(2), 2024.   \n[17] Francis A. Longstaff and Eduardo S. Schwartz. Valuing american options by simulation: A simple least-squares approach. The Review of Financial Studies, 14(1):113\u2013147, 2001.   \n[18] Hang Lou, Siran Li, and Hao Ni. Path development network with finite-dimensional Lie group representation. arXiv:2204.00740, 2022.   \n[19] Hang Lou, Siran Li, and Hao Ni. PCF-GAN: generating sequential data via the characteristic function of measures on the path space. Advances in Neural Information Processing Systems, 36, 2023.   \n[20] Cristopher Salvi, Thomas Cass, James Foster, Terry Lyons, and Weixin Yang. The signature kernel is the solution of a goursat pde. SIAM Journal on Mathematics of Data Science, 3(3):873\u2013899, January 2021.   \n[21] Cristopher Salvi, Maud Lemercier, Chong Liu, Blanka Hovarth, Theodoros Damoulas, and Terry Lyons. Higher order kernel mean embeddings to capture flitrations of stochastic processes. Advances in Neural Information Processing Systems, 34:16635\u201316647, 2021.   \n[22] T. Xu, L. K. Wenliang, M. Munn, and B. Acciaio. Cot-gan: Generating sequential data via causal optimal transport. Advances in Neural Information Processing Systems, 33:8798\u20138809, 2020.   \n[23] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. Time-series generative adversarial networks. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Examples and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Examples related to extended weak convergence ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Prediction processes First let us give an explicit example for prediction processes of some simple flitered processes. Consider the two processes $\\bar{\\mathbb{X}}^{n}=(\\Omega^{n},^{-}\\!\\mathcal{F}^{n},\\mathbb{F}^{\\bar{n}},X^{n},\\mathbb{P}_{n})$ and $\\mathbb{X}=(\\Omega,\\mathcal{F},\\mathbb{F},X,\\mathbf{\\bar{P}})$ , where ", "page_idx": 12}, {"type": "text", "text": "\u2022 $\\Omega^{n}\\,=\\,\\{\\pmb{x}_{1}^{n},\\pmb{x}_{2}^{n}\\}$ , $\\begin{array}{r}{\\pmb{x}_{1}^{n}=(\\pmb{x}_{1}^{n}(0)=1,\\pmb{x}_{1}^{n}(1)=1+\\frac{1}{n},\\pmb{x}_{1}^{n}(2)=2)}\\end{array}$ and ${\\pmb x}_{2}^{n}\\,=\\,({\\pmb x}_{2}^{n}(0)\\,=$ $\\begin{array}{r}{1,{\\pmb x}_{2}^{n}(1)=1-\\frac{1}{n},{\\pmb x}_{2}^{n}(2)=0)}\\end{array}$ ;   \n\u2022 $X_{t}^{n}(\\pmb{x}_{i}^{n})=\\pmb{x}_{i}^{n}(t)$ for $t=0,1,2$ and $i=1,2$ is the coordinate process on $\\Omega^{n}$ ;   \n\u2022 $\\begin{array}{r}{\\mathbb{P}^{n}(\\pmb{x}_{1}^{n})=\\mathbb{P}^{n}(\\pmb{x}_{2}^{n})=\\frac{1}{2};}\\end{array}$ ;   \n\u2022 $\\mathbb{F}^{n}=(\\mathcal{F}_{0}^{n},\\mathcal{F}_{1}^{n},\\mathcal{F}_{2}^{n})$ is the natural filtration generated by $X^{n}$ : $\\mathcal{F}_{0}^{n}=\\{\\varnothing,\\Omega^{n}\\}$ and ${\\mathcal{F}}_{1}^{n}=$ $\\mathcal{F}_{2}^{n}=\\sigma({X}_{1}^{n},{X}_{2}^{n})$ is the power set of $\\Omega^{n}$ , ", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "text", "text": "\u2022 $\\Omega=\\{\\pmb{x}_{1},\\pmb{x}_{2}\\}$ , ${\\pmb x}_{1}=({\\pmb x}_{1}(0)=1,{\\pmb x}_{1}(1)=1,{\\pmb x}_{1}(2)=2)$ and ${\\pmb x}_{2}=({\\pmb x}_{2}(0)=1,{\\pmb x}_{2}(1)=$ $1,x_{2}(2)=0)$ ;   \n\u2022 $X_{t}(\\pmb{x}_{i}^{n})=\\pmb{x}_{i}(t)$ for $t=0,1,2$ and $i=1,2$ is the coordinate process on $\\Omega$ ;   \n\u2022 $\\mathbb{P}(\\pmb{x}_{1})=\\mathbb{P}(\\pmb{x}_{2})=\\frac{1}{2}$ ;   \n\u2022 $\\mathbb{F}\\;=\\;(\\mathcal{F}_{0},\\mathcal{F}_{1},\\mathcal{F}_{2})$ is the natural filtration generated by $X\\colon\\,{\\mathcal{F}}_{0}\\ =\\ {\\mathcal{F}}_{1}\\ =\\ \\{\\varnothing,\\Omega\\}$ and $\\mathcal{F}_{2}=\\sigma(X_{1},X_{2})$ is the power set of $\\Omega$ . ", "page_idx": 12}, {"type": "text", "text": "We plot the sample paths of $\\mathbb{X}^{n}$ and $\\mathbb{X}$ in Fig. 4. ", "page_idx": 12}, {"type": "image", "img_path": "w28i9oe9Xr/tmp/8caca83585f6a0f7bd9d4f45db368b686451a59ca026a56b387ae6fe86400c71.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure $4!\\mathbb{X}^{n}$ (left) converges to $\\mathbb{X}$ (right) weakly, but the corresponding price of American options on $\\mathbb{X}^{n}$ cannot converge to the counterpart on $\\mathbb{X}$ , see Example A.1 below. Therefore the usage of slightly erroneous models in weak topology may cause significant loss in decision making problems. This example is taken from [3] and [5]. ", "page_idx": 12}, {"type": "text", "text": "From the above, it is straightforward to check that the prediction process ${\\hat{X}}^{n}$ of $\\mathbb{X}^{n}$ is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{X}_{0}^{n}(\\pmb{x}_{1}^{n})=\\hat{X}_{0}^{n}(\\pmb{x}_{2}^{n})=\\mathbb{P}^{n}(X^{n}\\in\\cdot|\\mathcal{F}_{0}^{n})=P_{X^{n}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $P_{X^{n}}$ is the law of $X^{n}$ under $\\mathbb{P}^{n}$ ; ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{X}_{1}^{n}(x_{1}^{n})=\\mathbb{P}^{n}(X^{n}\\in\\cdot|\\mathcal{F}_{1}^{n})(x_{1}^{n})=\\delta_{x_{1}^{n}},\\quad\\hat{X}_{1}^{n}(x_{2}^{n})=\\mathbb{P}^{n}(X^{n}\\in\\cdot|\\mathcal{F}_{1}^{n})(x_{2}^{n})=\\delta_{x_{2}^{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\delta_{\\pmb{x}_{i}^{n}}$ $(i=1,2)$ denotes the Dirac measure at $\\pmb{x}_{i}^{n}$ ; and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{X}_{2}^{n}(x_{1}^{n})=\\mathbb{P}^{n}(X^{n}\\in\\cdot|\\mathcal{F}_{2}^{n})(x_{1}^{n})=\\delta_{x_{1}^{n}},\\quad\\hat{X}_{2}^{n}(x_{2}^{n})=\\mathbb{P}^{n}(X^{n}\\in\\cdot|\\mathcal{F}_{2}^{n})(x_{2}^{n})=\\delta_{x_{2}^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Consequently, it holds that the law of ${\\hat{X}}^{n}$ satisfies ", "page_idx": 12}, {"type": "equation", "text": "$$\nP_{\\hat{X}^{n}}=\\mathbb{P}^{n}(\\hat{X}^{n}=(P_{X^{n}},\\delta_{{\\pmb x}_{i}^{n}},\\delta_{{\\pmb x}_{i}^{n}}))=\\frac{1}{2},\\quad i=1,2.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, the prediction process $\\hat{X}$ of $\\mathbb{X}$ is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{X}_{0}(\\pmb{x}_{1})=\\hat{X}_{0}(\\pmb{x}_{2})=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{0})=P_{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $P_{X}$ is the law of $X$ under $\\mathbb{P}$ ; ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\hat{X}}_{1}(\\pmb{x}_{1})=\\mathbb{P}(X\\in\\cdot|{\\mathcal{F}}_{1})(\\pmb{x}_{1})=P_{X},\\quad{\\hat{X}}_{1}(\\pmb{x}_{2})=\\mathbb{P}(X\\in\\cdot|{\\mathcal{F}}_{1})(\\pmb{x}_{2})=P_{X};\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{X}_{2}(\\pmb{x}_{1})=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{2})(\\pmb{x}_{1})=\\delta_{\\pmb{x}_{1}},\\quad\\hat{X}_{2}(\\pmb{x}_{2})=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{2})(\\pmb{x}_{2})=\\delta_{\\pmb{x}_{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "so that the law of $\\hat{X}$ reads ", "page_idx": 12}, {"type": "equation", "text": "$$\nP_{\\hat{X}}=\\mathbb{P}(\\hat{X}=(P_{X},P_{X},\\delta_{\\pmb{x}_{i}}))=\\frac{1}{2},\\quad i=1,2.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Test functions for extended weak convergence For $I=\\{0,1,\\dots,T\\}$ and flitered process $\\mathbb{X}\\in\\operatorname{FP}$ on $I$ , the typical test functions for defining the extended weak convergence have the following form: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{f}(\\hat{X})=F(\\mathbb{E}_{\\mathbb{P}}[f_{0}(X)\\vert\\mathcal{F}_{0}],\\dots,\\mathbb{E}[f_{T}(X)\\vert\\mathcal{F}_{T}]),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $f_{0},\\ldots,f_{T}\\;\\in\\;C_{b}(\\mathcal{X})$ are continuous bounded functions on the path space $\\mathcal{X}$ and $\\textit{F}\\in$ $C_{b}(\\mathbb{R}^{T+1})$ . For instance, for the filtered processes ${\\mathbb{X}}^{n}$ and $\\mathbb{X}$ in the above example, we have $T=2$ , and by choosing $f_{0}(x_{0},x_{1},x_{2})=1,\\,f_{1}(x_{0},x_{1},x_{2})=x_{1}+x_{2}-2,\\,f_{3}(x_{0},x_{1},x_{2})=\\sin(x_{2}-x_{1})$ and $F(y_{0},y_{1},y_{2})=\\exp(-|y_{1}|-y_{2}^{2})$ , in view of the facts that $\\mathcal{F}_{1}^{n}=\\mathcal{F}_{2}^{n}$ are the power set of $\\Omega^{n}$ (see the last paragraph), we obtain that for each $n$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(\\hat{X}^{n}(\\pmb{x}_{i}^{n}))=\\exp(-|\\mathbb{E}_{\\mathbb{P}^{n}}[X_{1}^{n}+X_{2}^{n}-2|\\mathcal{F}_{1}^{n}](\\pmb{x}_{i}^{n})|-(\\mathbb{E}_{\\mathbb{P}^{n}}[\\sin(X_{2}^{n}-X_{1}^{n})|\\mathcal{F}_{2}^{n}](\\pmb{x}_{2}^{n}))^{2})}\\\\ &{\\quad\\quad\\quad\\quad=\\exp(-|\\pmb{x}_{i}^{n}(1)+\\pmb{x}_{i}^{n}(2)-2|-\\sin^{2}(\\pmb{x}_{i}^{n}(2)-\\pmb{x}_{i}^{n}(1)))}\\\\ &{\\quad\\quad\\quad\\quad=\\exp(-(1+\\displaystyle\\frac{1}{n})-\\sin^{2}(1-\\displaystyle\\frac{1}{n})),\\quad i=1,2;}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and therefore $\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{n}}[\\hat{f}(\\hat{X}^{n})]=\\exp(-(1+\\frac{1}{n})-\\sin^{2}(1-\\frac{1}{n}))}\\end{array}$ . On the other side, since ${\\mathcal{F}}_{0}={\\mathcal{F}}_{1}=$ $\\{\\varnothing,\\Omega\\}$ are trivial $\\sigma$ -algebra, for the prediction process $\\hat{X}$ of $\\mathbb{X}$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(\\hat{X}(\\pmb{x}_{i}))=\\exp(-|\\mathbb{E}_{\\mathbb{P}}[X_{1}+X_{2}-2|\\mathcal{F}_{1}](\\pmb{x}_{i})|-(\\mathbb{E}_{\\mathbb{P}}[\\sin(X_{2}-X_{1})|\\mathcal{F}_{2}](\\pmb{x}_{2}))^{2})}\\\\ &{\\quad\\quad\\quad\\quad=\\exp(-|\\mathbb{E}_{\\mathbb{P}}[X_{1}+X_{2}-2]|-\\sin^{2}(\\pmb{x}_{i}(2)-\\pmb{x}_{i}(1)))}\\\\ &{\\quad\\quad\\quad\\quad=\\exp(-\\sin^{2}(1)),\\quad i=1,2}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as $\\mathbb{E}_{\\mathbb{P}}[X_{1}+X_{2}-2]=0$ ; and therefore ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[\\hat{f}(\\hat{X})]=\\exp(-\\sin^{2}(1)).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Clearly, as $n\\to\\infty$ , we have $\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{n}}[\\hat{f}(\\hat{X}^{n})]=\\exp(-(1+\\frac{1}{n})-\\sin^{2}(1-\\frac{1}{n}))\\rightarrow\\exp(-1-\\sin^{2}(1))\\neq0.}\\end{array}$ $\\mathrm{exp}(-\\sin^{2}(1))\\,=\\,\\mathbb{E}_{\\mathbb{P}}[\\hat{f}(\\hat{X})]$ , which shows that ${\\mathbb{X}}^{n}$ cannot converge to $\\mathbb{X}$ in the extended weak convergence according to Definition 2.1, although it is easy to see that the laws of ${\\mathbb{X}}^{n}$ converges to the law of $\\mathbb{X}$ in the weak topology. ", "page_idx": 13}, {"type": "text", "text": "In the example, while the unconditional law of the processes $\\mathbb{X}^{n}$ converges to $\\mathbb{X}$ , weak convergence fails to capture a key difference between the financial models $\\mathbb{X}^{n}$ and $\\mathbb{X}$ . Specifically, if an agent believes the market dynamics as in $\\mathbb{X}^{n}$ , he/she always knows the outcome of the last day in advance, granting a predictive advantage, whereas in the \u201cfair\u201d market $\\mathbb{X}$ , the agent lacks this foresight. This crucial difference in the observed information flow-\"No knowledge $\\Rightarrow{}$ Full knowledge $\\Rightarrow{}$ Full knowledge\" for $\\mathbb{X}^{n}$ versus \"No knowledge $\\Rightarrow\\mathrm{No}$ knowledge $\\Rightarrow\\mathrm{Full}$ knowledge\" for $\\mathbb{X}$ \u2014is not reflected in weak convergence alone. ", "page_idx": 13}, {"type": "text", "text": "Extended Weak Topology (EWT) is vital in this case, because it captures this difference through the conditional distributions. For markets ${\\mathbb{X}}^{n}$ , where the agent has full information on day 1, the conditional distribution becomes a single Dirac measure, annihilating randomness. In contrast, $\\mathbb{X}$ retains genuine randomness at day 1, as reflected by a linear combination of Dirac measures. Since EWT is based on conditional distributions, it effectively measures differences in information evolution styles, ensuring continuity in multi-period decision-making as agents update their actions based on continually evolving information. ", "page_idx": 13}, {"type": "text", "text": "Some important multi-periods optimisation problems The following multi-periods optimisation problems are very important in financial and economic applications, whose value functions are, in general, discontinuous with respect to the weak convergence, but continuous in the extended weak topology. ", "page_idx": 13}, {"type": "text", "text": "Example A.1 (Optimal Stopping Problem). Let $g\\,:\\,I\\times\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ be a continuous and bounded non-anticipative (i.e., for any $t\\in I$ and $\\pmb{x}\\in\\mathcal{X}$ , the value of $g(t,\\pmb{x})$ only depends on $\\pmb{x}_{0},\\allowbreak\\cdot\\cdot,\\pmb{x}_{t})$ function. For each flitered process $\\mathbb{X}^{n}$ we set $S T_{n}:=\\{\\tau:\\mathbb{F}^{n}$ -stopping time} be the collection of all stopping times with respect to the flitration $\\mathbb{F}^{n}$ and similarly define ST for X. Then the value function ${v_{g}}(\\cdot)$ in the Optimal Stopping Problem (OSP) with the reward $g$ (in the context of mathematical finance, it is also called the price of American option) is defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{g}(\\mathbb{X}^{n})=\\operatorname*{sup}_{\\tau\\in S T_{n}}\\mathbb{E}_{\\mathbb{P}^{n}}[g(\\tau,X^{n})],\\quad v_{g}(\\mathbb{X})=\\operatorname*{sup}_{\\tau\\in S T}\\mathbb{E}_{\\mathbb{P}}[g(\\tau,X)].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "$I f\\mathbb{X}^{n}\\xrightarrow{E W}\\mathbb{X},$ , then $v_{g}(\\mathbb{X}^{n})\\to v_{g}(\\mathbb{X})$ , whilst this continuity fails in the weak convergence: for the processes $\\mathbb{X}^{n}$ and $\\mathbb{X}$ considered as above (see Fig. 4) and the reward function $g(t,\\pmb{x}):=\\pmb{x}_{t}.$ , one has $\\mathbb{X}^{n}\\xrightarrow{W}\\mathbb{X}$ but ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}v_{g}(\\mathbb{X}^{n})\\neq v_{g}(\\mathbb{X}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Indeed, since X is a martingale with initial value 1, it is obvious that for any stopping time $\\tau\\in S T$ it always holds that $\\mathbb{E}_{\\mathbb{P}}[g(\\tau,\\dot{X})]=\\mathbb{E}_{\\mathbb{P}}[X_{\\tau}]=1$ which in turn implies that $v_{g}(\\mathbb{X})=1$ ; on the other hand, since the filtration of $\\mathbb{X}^{n}$ satisfies that $\\mathcal{F}_{1}^{n}=\\mathcal{F}_{2}^{n}$ (i.e., the agent already knows everything at day 1), it is easy to check that $\\boldsymbol{\\tau}_{\\star}^{n}=2\\mathbf{1}_{x_{1}^{n}}+\\mathbf{1}_{x_{2}^{n}}$ is the optimal $\\mathbb{F}^{n}$ -stopping time for $v_{g}(\\mathbb{X}^{n})$ and consequently $\\begin{array}{r}{v_{g}(\\mathbb{X}^{n})=\\mathbb{E}_{\\mathbb{P}^{n}}[X_{\\tau_{\\star}^{n}}^{n}]=\\frac{1}{2}\\times2+\\frac{1}{2}\\times(1-\\frac{1}{n})=\\frac{3}{2}-\\frac{1}{2n}}\\end{array}$ converges to ${\\textstyle\\frac{3}{2}}\\neq0=v_{g}(\\mathbb{X})$ ", "page_idx": 14}, {"type": "text", "text": "Example A.2 (Utility Maximisation Problem). Let $g\\ :\\ \\mathbb{R}\\ \\rightarrow\\ \\mathbb{R}$ be a continuous, bounded and concave utility function. For each filtered process $\\mathbb{X}^{n}$ we set $\\Lambda_{n}\\;:=\\;\\{\\varphi\\,=\\,(\\varphi_{t})_{t=1,\\dots,T}\\,:$ $\\varphi$ is predictable w.r.t. $\\mathbb{F}^{n}\\}$ be the collection of all predictable strategies (i.e., $\\varphi_{t}$ is $\\mathcal{F}_{t-1}^{n}$ -measurable for all $t=1,\\dots,T)$ with respect to the filtration $\\mathbb{F}^{n}$ and similarly define $\\Lambda$ for $\\mathbb{X}$ . Then the value function $u_{g}(\\cdot)$ in the utility maximisation Problem with the utility function $g$ is defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{g}(\\mathbb{X}^{n})=\\operatorname*{sup}_{\\varphi\\in\\Lambda_{n}}\\mathbb{E}_{\\mathbb{P}^{n}}\\bigg[g(\\int_{0}^{T}\\varphi_{t}d X_{t}^{n})\\bigg],\\quad u_{g}(\\mathbb{X})=\\operatorname*{sup}_{\\varphi\\in\\Lambda}\\mathbb{E}_{\\mathbb{P}}\\bigg[g(\\int_{0}^{T}\\varphi_{t}d X_{t})\\bigg],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\int_{0}^{T}\\varphi_{t}d X_{t}=\\sum_{i=1}^{T}\\varphi_{t}(X_{t}-X_{t-1})}\\end{array}$ is the stochastic integral. $I f\\mathbb{X}^{n}\\xrightarrow{E W}\\mathbb{X},$ , then $u_{g}(\\mathbb{X}^{n})\\to$ $u(\\mathbb{X})$ , whilst this continuity fails in the weak convergence. ", "page_idx": 14}, {"type": "text", "text": "An example of HRPCF We still consider the example mentioned before (see the paragraph Prediction processes and Fig. 4). In the previous discussions we have known that $\\mathbb{X}^{n}\\to\\mathbb{X}$ in the weak convergence (i.e., the laws $P_{X^{n}}$ converges to the law $P_{X}$ ), but $\\mathbb{X}^{n}$ cannot converge to $\\mathbb{X}$ in the extended weak convergence. Now we show that there exists an admissible pair $(M,\\mathcal{M})\\in\\mathcal{A}_{\\mathrm{unitary}}$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}d_{\\mathrm{HS}}(\\Phi_{\\mathbb{X}}(M,{\\mathcal{M}}),\\Phi_{\\mathbb{X}^{n}}(M,{\\mathcal{M}}))\\neq0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "by an explicit calculation, which confirms that the HRPCFD does metrise the extended weak convergence and therefore reflect the differences of filtrations of stochastic processes. ", "page_idx": 14}, {"type": "text", "text": "Now we pick a linear operator $M\\in\\mathcal{L}(\\mathbb{R}^{2},\\mathfrak{u}(1))^{7}$ which is given by $M(t,y):=y(\\textstyle{\\frac{\\pi}{2}}\\dot{1})\\in\\mathfrak{u}(1)\\subset\\mathbb{C}$ , where i denotes the imaginary unit in $\\mathbb{C}$ . Since the prediction process $\\hat{X}$ of $\\mathbb{X}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{X}_{0}(\\pmb{x}_{1})=\\hat{X}_{0}(\\pmb{x}_{2})=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{0})=P_{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $P_{X}$ is the law of $X$ under $\\mathbb{P}$ ; ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\hat{X}}_{1}(\\pmb{x}_{1})=\\mathbb{P}(X\\in\\cdot|{\\mathcal{F}}_{1})(\\pmb{x}_{1})=P_{X},\\quad{\\hat{X}}_{1}(\\pmb{x}_{2})=\\mathbb{P}(X\\in\\cdot|{\\mathcal{F}}_{1}^{n})(\\pmb{x}_{2})=P_{X};\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{X}_{2}({\\pmb x}_{1})=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{2})({\\pmb x}_{1})=\\delta_{{\\pmb x}_{1}},\\quad\\hat{X}_{2}({\\pmb x}_{2})=\\mathbb{P}(X\\in\\cdot|\\mathcal{F}_{2})({\\pmb x}_{2})=\\delta_{{\\pmb x}_{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we can check that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{0}]=\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)]=\\frac{1}{2}(e^{\\frac{\\pi}{2}\\mathrm{i}}+e^{-\\frac{\\pi}{2}\\mathrm{i}})=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{1}]=\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)]=\\frac{1}{2}(e^{\\frac{\\pi}{2}\\mathrm{i}}+e^{-\\frac{\\pi}{2}\\mathrm{i}})=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{2}]({\\pmb x}_{1})=\\mathcal{U}_{M}({\\pmb x}_{1})=e^{\\frac{\\pi}{2}{\\mathrm{i}}}=\\mathrm{i},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{2}]({\\pmb x}_{2})=\\mathcal{U}_{M}({\\pmb x}_{2})=e^{-\\frac{\\pi}{2}\\mathrm{i}}=-\\mathrm{i},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which shows that the $\\mathbb{C}$ -valued process $(\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}])_{t=0,1,2}$ satisfies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}]({\\pmb x}_{1}))_{t=0,1,2}=(0,0,\\mathrm{i}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}]({\\pmb x}_{2}))_{t=0,1,2}=(0,0,-\\mathrm{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand, for every $n\\in\\mathbb N$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathbb{P}^{n}[X^{n}\\in\\cdot|\\mathcal{F}_{0}^{n}]=P_{X^{n}},}}\\\\ {{\\mathbb{P}^{n}[X^{n}\\in\\cdot|\\mathcal{F}_{1}^{n}](\\pmb{x}_{1}^{n})=\\mathbb{P}^{n}[X^{n}\\in\\cdot|\\mathcal{F}_{2}^{n}](\\pmb{x}_{1}^{n})=\\delta_{\\pmb{x}_{1}^{n}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}^{n}[X^{n}\\in\\cdot|\\mathcal{F}_{1}^{n}](\\pmb{x}_{2}^{n})=\\mathbb{P}^{n}[X^{n}\\in\\cdot|\\mathcal{F}_{2}^{n}](\\pmb{x}_{1}^{n})=\\delta_{\\pmb{x}_{2}^{n}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which provides that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{0}^{n}]=\\frac12(e^{(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}+e^{-(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{1}^{n}](x_{1}^{n})=\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{2}^{n}](x_{1}^{n})=\\mathcal{U}_{M}(x_{1}^{n})=e^{(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{1}^{n}](x_{2}^{n})=\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{2}^{n}](x_{2}^{n})=\\mathcal{U}_{M}(\\pmb{x}_{2}^{n})=e^{-(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, the $\\mathbb{C}$ -valued process $(\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}])_{t=0,1,2}$ satisfies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}](x_{1}^{n}))_{t=0,1,2}=(\\frac{1}{2}(e^{(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}+e^{-(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}),e^{(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}},e^{(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}](x_{2}^{n}))_{t=0,1,2}=(\\frac{1}{2}(e^{(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}+e^{-(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}),e^{-(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}},e^{-(1+\\frac{1}{n})\\frac{\\pi\\mathrm{i}}{2}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By viewing $\\mathbb{C}$ as $\\mathbb{R}^{2}$ and only consider the imaginary part of the above two processes $(\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}])_{t=0,1,2}$ in (6), (7) and $(\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\bar{\\mathcal{F}}_{t}^{n}])_{t=0,1,2}$ in (8), (9), we may without loss of generality assume that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}](x_{1}))_{t=0,1,2}=(0,0,1),(\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}](x_{2}))_{t=0,1,2}=(0,0,-1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}](x_{1}^{n}))_{t=0,1,2}=(0,\\sin((1+\\frac{1}{n})\\frac{\\pi}{2}),\\sin((1+\\frac{1}{n})\\frac{\\pi}{2})),}\\\\ &{(\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}](x_{2}^{n}))_{t=0,1,2}=(0,-\\sin((1+\\frac{1}{n})\\frac{\\pi}{2}),-\\sin((1+\\frac{1}{n})\\frac{\\pi}{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, adding the additional time component to the above real valued paths, and choosing $\\mathcal{M}\\in$ $\\mathcal{L}(\\mathbb{R}^{2},\\mathfrak{u}(2))$ via ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right])=\\left[\\!\\!\\begin{array}{l l}{0}&{1}\\\\ {-1}&{0}\\end{array}\\!\\!\\right],\\quad\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{0}\\\\ {1}\\end{array}\\!\\!\\right])=\\left[\\!\\!\\begin{array}{l l}{0}&{\\mathrm{i}}\\\\ {\\mathrm{i}}&{0}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we can easily verify that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}_{\\sf{M}}((\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}](x_{1}))_{t=0,1,2})=\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right]))\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {1}\\end{array}\\!\\!\\right]))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\neq\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {1}\\end{array}\\!\\!\\right]))\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right]))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathcal{U}_{\\sf{M}}((\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}](x_{1}^{n}))_{t=0,1,2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}_{\\sf{M}}((\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}](x_{2}))_{t=0,1,2})=\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right]))\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\!\\!\\right]))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\neq\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\!\\!\\right]))\\exp(\\mathcal{M}(\\left[\\!\\!\\begin{array}{l}{1}\\\\ {0}\\end{array}\\!\\!\\right]))}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathcal{U}_{\\sf{M}}((\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}](x_{2}^{n}))_{t=0,1,2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where exp denotes the matrix exponential on $\\mathbb{C}^{2\\times2}$ . From the above calculation, we can further derive that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}_{P^{n}}[\\mathcal{U}_{M}((\\mathbb{E}_{\\mathbb{P}^{n}}[\\mathcal{U}_{M}(X^{n})|\\mathcal{F}_{t}^{n}])_{t=0,1,2})]=\\frac{1}{2}\\exp(\\mathcal{M}(\\Big[\\frac{1}{1}\\Big]))\\exp(\\mathcal{M}(\\Big[\\frac{1}{0}\\Big]))}&{}\\\\ {+\\frac{1}{2}\\exp(\\mathcal{M}(\\Big[\\frac{1}{-1}\\Big]))\\exp(\\mathcal{M}(\\Big[\\frac{1}{0}\\Big]))}&{}\\\\ {\\neq\\frac{1}{2}\\exp(\\mathcal{M}(\\Big[\\frac{1}{0}\\Big]))\\exp(\\mathcal{M}(\\Big[1\\Big]))}&{}\\\\ {+\\frac{1}{2}\\exp(\\mathcal{M}(\\Big[\\frac{1}{0}\\Big]))\\exp(\\mathcal{M}(\\Big[\\frac{1}{-1}\\Big]))}&{}\\\\ {=\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}((\\mathbb{E}_{\\mathbb{P}}[\\mathcal{U}_{M}(X)|\\mathcal{F}_{t}])_{t=0,1,2})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "because the matrix multiplication is non-commutative. Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}d_{\\mathrm{HS}}(\\Phi_{\\mathbb{X}}(M,{\\mathcal{M}}),\\Phi_{\\mathbb{X}^{n}}(M,{\\mathcal{M}}))\\neq0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which coincides with our observation that ${\\mathbb{X}}^{n}$ cannot converge to $\\mathbb{X}$ for the extended weak convergence. ", "page_idx": 16}, {"type": "text", "text": "A.2 A Brief Introduction to Path Characteristic Functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we will summarise some crucial properties of the Path Characteristic Functions (PCF) of $\\mathbb{R}^{d}$ -valued stochastic processes which were obtained in [19] and [18], and briefly mention its connection with the signature theory. ", "page_idx": 16}, {"type": "text", "text": "Recall that for $\\pmb{x}\\in C^{1-\\mathrm{var}}([0,T],\\mathbb{R}^{d})$ and $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ , the unitary feature $\\mathcal{U}_{M}({\\pmb x})$ (also called unitary path development) of $\\textbf{\\em x}$ under $M$ is defined to be $y_{T}\\,\\in\\,U(m)$ with $\\textit{\\textbf{y}}$ being the unique solution to the following linear ODE driven by $M(d{\\pmb x}_{t})$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nd{\\pmb y}_{t}={\\pmb y}_{t}M(d{\\pmb x}_{t}),\\quad{\\pmb y}_{0}=I_{m}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $\\mathbb{X}$ is an $\\mathbb{R}^{d}$ -valued flitered process with sample paths in $C^{1-\\mathrm{var}}([0,T],\\mathbb{R}^{d})$ , then its path characteristic function (PCF) is given by the expectation of the unitary feature of $X$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Phi_{X}(M)=\\mathbb{E}_{\\mathbb{P}}[{\\mathcal{U}}_{M}(X)]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ , $m\\in\\mathbb{N}$ . ", "page_idx": 16}, {"type": "text", "text": "It is easy to see that the PCF of stochastic processes is a natural generalisation of the classical characteristic functions for $\\mathbb{R}^{d}$ -valued random variables. Indeed, for an $\\mathbb{R}^{d}$ -valued random variable $X$ , we may view it as a linear path from 0 to 1, i.e., $X_{t}=t X$ for $t\\in[0,1]$ . Then, for $m=1$ , as the 1-dimensional unitary Lie algebra $\\mathfrak{u}(1)$ is simple the real vector space spanned by the imaginary unit i, we know that every linear mapping $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(1))$ can be represented by ", "page_idx": 16}, {"type": "equation", "text": "$$\nM(x)=\\langle x,\\lambda\\rangle\\mathrm{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for some $\\lambda\\in\\mathbb{R}^{d}$ and $\\langle\\cdot,\\cdot\\rangle$ denotes the Eulidean inner product. In this case it holds that the unique solution $\\pmb{y}(\\omega)$ to the ODE ", "page_idx": 16}, {"type": "equation", "text": "$$\nd y_{t}(\\omega)=y_{t}(\\omega)M(d X_{t}(\\omega))=y_{t}(\\omega)\\langle X(\\omega),\\lambda\\rangle\\mathrm{i}d t,\\quad y_{0}(\\omega)=1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is simply $\\pmb{y}_{t}(\\omega)=\\exp(t\\langle X(\\omega),\\lambda\\rangle\\mathrm{i})$ , which implies that $\\mathcal{U}_{M}(X(\\omega)=y_{1}(\\omega)=\\exp(\\langle X(\\omega),\\lambda\\rangle\\mathrm{i})$ and consequently ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Phi_{X}(M)=\\int\\mathcal{U}_{M}(X(\\omega))\\mathbb{P}(d\\omega)=\\mathbb{E}_{\\mathbb{P}}[\\exp(\\langle X,\\lambda\\rangle\\mathrm{i})],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is exactly the classical characteristic function of $X$ evaluated at $\\lambda\\in\\mathbb{R}^{d}$ . ", "page_idx": 16}, {"type": "text", "text": "Connections of PCF with the Signature Theory Given a continuous bounded variation path $\\pmb{x}\\in C^{1-\\mathrm{var}}([0,T],\\mathbb{R}^{d})$ , its signature $S(x)$ (see e.g. [6]) is given by a formal series in the (dual of the) tensor algebra $\\begin{array}{r}{\\dot{T_{((\\mathbb{R}^{d}))}}=\\prod_{n=0}^{\\infty}(\\mathbb{R}^{d})^{\\otimes n}}\\end{array}$ over $\\mathbb{R}^{d}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nS(\\pmb{x})=(1,S_{1}(\\pmb{x}),\\pmb{\\ldots},S_{n}(\\pmb{x}),\\pmb{\\ldots})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{S_{n}(\\pmb{x})=\\sum_{i_{1},\\dots,i_{n}=1}^{d}\\int_{0<t_{1}<\\dots<t_{n}<1}d x_{t_{1}}^{i_{1}}\\dots d x_{t_{n}}^{i_{n}}e_{i_{1}}\\otimes\\dots\\otimes e_{i_{n}}\\in(\\mathbb{R}^{d})^{\\otimes n}.}\\end{array}$ , where $e_{1},\\ldots,e_{d}$ are the canonical basis of $\\mathbb{R}^{d}$ and $\\otimes$ denotes the tensor product. Thanks to the universal property of the tensor algebra $T((\\mathbb{R}^{d}))$ , every linear mapping $\\dot{M^{\\ast}}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ can be lifted to an algebra morphism $\\tilde{M}\\,:\\,T((\\mathbb{R}^{d}))\\,\\rightarrow\\,\\mathbb{C}^{m\\times m}$ (where $T((\\mathbb{R}^{d}))$ is equipped with the tensor product, and $\\mathbb{C}^{m\\breve{\\times}m}$ is endowed with the matrix multiplication). It can be shown that (see [18], [19]) the unitary feature $\\mathcal{U}_{M}({\\pmb x})$ is equal to the composition of $\\tilde{M}$ and the signature of $\\textbf{\\em x}$ , i.e., $\\mathcal{U}_{M}(\\pmb{x})=\\tilde{M}(\\pmb{S}(\\pmb{x}))$ . Moreover, the classical signature theory (see e.g. [7]) also tells that the signature $S(x)$ belongs to the character group of $T((\\mathbb{R}^{d}))$ with respect to a specified Hopf algebra structure. This algebraic property of signature together with the relation that $\\mathcal{U}_{M}(\\pmb{x})=\\tilde{M}(\\pmb{S}(\\pmb{x}))$ does not only guarantees that $\\mathcal{U}_{M}(\\mathbf{\\boldsymbol{x}})\\in U(m)$ takes values in the unitary group, but also the universality of unitary features of paths (see e.g. [19, Theorem A.8]): ", "page_idx": 17}, {"type": "text", "text": "Theorem A.3. \u2022 The linear functions on unitary features are stable under multiplication and complex conjugation. More precisely, for any $M_{1}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m_{1}))$ , $M_{2}\\in\\dot{\\mathcal{L}}(\\mathbb{R}^{d},\\mathfrak{u}(m_{2}))$ , $L_{1}\\,\\in\\,{\\mathcal{L}}(\\mathbb{C}^{m_{1}\\times m_{1}},\\mathbb{C})$ and $L_{2}\\,\\in\\,{\\mathcal{L}}(\\mathbb{C}^{m_{2}\\times m_{2}},\\mathbb{C}),$ , there exist an $M_{3}\\,\\in\\,{\\mathcal{L}}(\\mathbb{R}^{d},{\\mathfrak{u}}(m_{3}))$ , $a$ $L_{3}\\in\\mathcal{L}(\\mathbb{C}^{m_{3}\\times m_{3}},\\mathbb{C}),$ , an $M_{4}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m_{4}))$ and a $L_{4}\\in\\mathcal{L}(\\mathbb{C}^{m_{4}\\times m_{4}},\\mathbb{C})$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{1}({\\mathcal U}_{M_{1}}({\\pmb x}))L_{2}({\\mathcal U}_{M_{2}}({\\pmb x}))=L_{3}({\\mathcal U}_{M_{3}}({\\pmb x})),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and $\\overline{{L_{1}(\\mathcal{U}_{M_{1}}(\\pmb{x}))}}=L_{4}(\\mathcal{U}_{M_{4}}(\\pmb{x}))$ . ", "page_idx": 17}, {"type": "text", "text": "\u2022 Le $t\\,K\\subset C^{1-\\nu a r}([0,T],\\mathbb{R}^{d})$ be a compact subset. For any continuous and bounded function $f:\\,K\\,\\rightarrow\\,\\mathbb{C}$ and any $\\varepsilon\\ >\\ 0$ , there is an $m_{*}~\\in~\\mathbb{N}$ and finitely many $M_{1},\\dots,M_{N}\\,\\in$ $\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m_{*}))$ as well as linear functionals $L_{1},\\dots,L_{N}\\in\\mathcal{L}(U(m_{*}),\\mathbb{C})$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{{\\pmb x}\\in{\\mathcal K}}\\left|f({\\pmb x})-\\sum_{i=1}^{N}L_{i}({\\mathcal U}_{M_{i}}({\\pmb x}))\\right|<\\varepsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Clearly, the second statement of the above theorem follows immediately from the first statement together with the Stone-Weierstrass theorem for $\\mathbb{C}_{}$ -valued functions. Since the expectations of continuous bounded functions determines the distributions on $C^{1-\\mathrm{var}}([0,T],\\mathbb{R}^{d})$ , as a corollary of the universality theorem above, we obtain the following characteristicness of PCF as mentioned in Theorem 2.6, see also [19, Theorem B.1]. ", "page_idx": 17}, {"type": "text", "text": "Other Properties of the unitary features and PCF Besides the universality and the characteristicness, in [19] and [15] one can find some other nice properties of the unitary featues and PCF, which we will list below without proof: ", "page_idx": 17}, {"type": "text", "text": "1. Since $\\mathcal{U}_{M}({\\pmb x})$ takes values in the unitary group $U(m)$ if $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ , so the HilbertSchmidt norm of the PCF $\\Phi_{M}(\\mathbb{X})$ of any stoc\u221ahastic process $\\mathbb{X}$ (with continuous bounded variation sample paths) is always bounded by $\\sqrt{m}$ . In particular, $\\Phi_{M}(\\mathbb{X})$ can be defined for any stochastic process with no integrability requirement.   \n2. The unitary feature $\\mathcal{U}_{M}:C^{1-\\mathrm{var}}([0,T],\\mathbb{R}^{d})\\to U(m)$ is Lipschitz continuous with respect to the bounded variation norm, see [19, Proposition B.6].   \n3. If the laws of stochastic processes satisfies enough integrability condition (namely their expected signatures have infinite radius of convergence, see [7] for the definition), then one can use a special subclass of linear mappings $\\bar{M}\\,\\in\\,\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ to determine the laws. More explicitly, for $P_{X}$ and $P_{Y}$ two laws of stochastic processes with enough integrability, $P_{X}=P_{Y}$ if and only if $\\Phi_{P_{X}}(M)=\\Phi_{P_{Y}}(M)$ for all $\\dot{M}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{o}(m))$ such that $M$ only has possibly nonzero entries in $M_{i j}$ with $\\vert i-j\\vert=1$ , where $M_{i j}$ denotes the entry of $M$ at the $i$ -th row and $j$ -th column, and ${\\mathfrak{o}}(m)$ is the orthogonal Lie algebra. Thanks to the significant sparsity, such $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{o}(m))$ is easy to be implemented in the numerical application. See [15] for more details.   \n4. The PCF induces a metric which can (locally) characterise the weak convergence of the laws of stochastic processes, which is called the PCFD (see [19, Theorem 3.8]). In fact the HRPCFD defined in the present paper can be seen as a counterpart of PCFD in the extended weak convergence. ", "page_idx": 17}, {"type": "text", "text": "A.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we prove Theorem 3.3 in a more general setting. ", "page_idx": 18}, {"type": "text", "text": "Definition A.4. For $(M,\\mathcal{M})\\in\\mathcal{A}_{u n i t a r y}$ with $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ , $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ and $\\pmb{p}\\in\\hat{\\mathcal X}$ a measure-valued path, we call ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{U}_{M,\\mathcal{M}}(\\pmb{p}):=\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M}),\\quad\\pmb{p}_{t}^{M}=\\pmb{\\Phi}_{\\pmb{p}_{t}}(\\boldsymbol{M})=\\int_{\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the high rank development of $\\pmb{p}$ under $(M,{\\mathcal{M}})$ . ", "page_idx": 18}, {"type": "text", "text": "Definition A.5. For $\\mu\\in\\mathcal{P}(\\hat{\\mathcal{X}})$ a probability measure on the measure-valued path space $\\hat{\\chi}$ , the function ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Phi_{\\mu}^{2}:A_{u n i t a r y}\\rightarrow\\bigcup_{m=1}^{\\infty}\\mathbb{C}^{m\\times m}}\\\\ {\\displaystyle(M,\\mathcal{M})\\mapsto\\int_{p\\in\\hat{\\mathcal{X}}}\\mathcal{U}_{M,\\mathcal{M}}(p)\\mu(d p)=\\int_{p\\in\\hat{\\mathcal{X}}}\\mathcal{U}_{M}(t\\mapsto p_{t}^{M})\\mu(d p)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is called the high rank path characteristic function of $\\mu$ (Abbreviation: HRPCF). ", "page_idx": 18}, {"type": "text", "text": "The next lemma is straightforward, but will be helpful for us to construct the characteristicity for laws of measure-valued stochastic processes. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.6. Let $\\tilde{M}\\,=\\,(M_{j})_{j=1}^{k}\\,\\in\\,\\bigoplus_{j=1}^{k}\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(j))$ for some $k\\,\\in\\,\\mathbb{N}$ , Then, there exists an $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ for $n=(1+2+\\ldots+k)$ , such that for any measure\u2013valued path $\\pmb{p}\\in\\hat{\\mathcal X}$ and for any $t\\in[0,T]$ , one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p_{t}^{M}=\\displaystyle\\int_{\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})p_{t}(d\\pmb{x})}&{}\\\\ {=\\left[\\begin{array}{l l l l}{\\displaystyle\\int_{\\mathcal{X}}\\mathcal{U}_{M_{1}}(\\pmb{x})p_{t}(d\\pmb{x})}&{}&&{}\\\\ {}&{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{2}}(\\pmb{x})p_{t}(d\\pmb{x})}&{}&\\\\ {}&{\\cdots}&{}\\\\ {}&{}&{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{k}}(\\pmb{x})p_{t}(d\\pmb{x})}\\end{array}\\right]}\\\\ {\\in\\mathbb{C}^{n\\times n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Given an $\\tilde{M}\\;=\\;(M_{j})_{j=1}^{k}\\;\\in\\;\\bigoplus_{j=1}^{k}\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(j))$ , we define $\\boldsymbol{M}:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathfrak{u}(n)$ for $n\\,=$ $1+2+\\ldots+k$ via ", "page_idx": 18}, {"type": "equation", "text": "$$\nM(x)=\\left[\\begin{array}{c c c c}{{M_{1}(x)\\in{\\mathfrak{u}}(1)}}&{{}}&{{}}&{{}}\\\\ {{}}&{{M_{2}(x)\\in{\\mathfrak{u}}(2)}}&{{}}&{{}}\\\\ {{}}&{{}}&{{\\ddots}}&{{}}\\\\ {{}}&{{}}&{{}}&{{M_{k}(x)\\in{\\mathfrak{u}}(k)}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is obviously a linear mapping due to the linearity of $M_{1},\\ldots,M_{k}$ ", "page_idx": 18}, {"type": "text", "text": "For any $\\mathbb{R}^{d}.$ \u2013valued path $\\pmb{x}\\in\\mathcal{X}$ , we know that its unitary feature $\\mathcal{U}_{M}({\\pmb x})$ is the unique solution $\\textit{\\textbf{y}}$ (evaluated at time $T$ ) to the linear differential equation ", "page_idx": 18}, {"type": "equation", "text": "$$\nd{\\pmb y}_{t}={\\pmb y}_{t}M(d{\\pmb x}_{t}),\\quad{\\pmb y}_{0}=I_{n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, let $\\scriptstyle z_{t}$ be a curve in $U(n)$ defined by ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{t}=\\left[\\begin{array}{l l l l l}{z_{1}(t)\\in U(1)}&&&&\\\\ &{z_{2}(t)\\in U(2)}&&&\\\\ &&{\\ddots}&&\\\\ &&&{z_{k}(t)\\in U(k)}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $z_{j}(t),j=1,\\ldots,k$ is the unique solution to the linear differential equation ", "page_idx": 18}, {"type": "equation", "text": "$$\nd{\\pmb y}_{t}={\\pmb y}_{t}M_{j}(d{\\pmb x}_{t}),\\quad{\\pmb y}_{0}=I_{j}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is clear that $_{z}$ satisfies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d z_{t}=\\left[\\begin{array}{l l l l}{d z_{1}(t)}&&&\\\\ &{d z_{2}(t)}&&\\\\ &&{\\ddots}&\\\\ &&&{d z_{k}(t)}\\end{array}\\right]}\\\\ &{\\quad=\\left[\\begin{array}{l l l l}{z_{1}(t)M_{1}(d x_{t})}&&&\\\\ &{z_{2}(t)M_{2}(d x_{t})}&&\\\\ &&{\\ddots}&\\\\ &&&{z_{k}(t)M_{k}(d x_{t})}\\end{array}\\right]}\\\\ &{\\quad=\\left[\\begin{array}{l l l l}{z_{1}(t)}&&&\\\\ &{z_{2}(t)}&&\\\\ &{\\ddots}&\\\\ &&&{z_{k}(t)}\\end{array}\\right]\\left[\\begin{array}{l l l l}{M_{1}(d x_{t})}&&&\\\\ &{M_{2}(d x_{t})}&&\\\\ &&{\\ddots}&\\\\ &&&{M_{k}(d x_{t})}\\end{array}\\right]}\\\\ &{\\quad=z_{t}M(d x_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, by the uniqueness of the solution to the differential equation $d\\pmb{y}_{t}=\\pmb{y}_{t}M(d\\pmb{x}_{t})$ , and invoking that $z_{j}(\\dot{T})=\\mathcal{U}_{M_{j}}\\overline{{(\\pmb{x})}}$ for all $j=1,\\dots,k$ , we must have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{U}_{M}({\\pmb x})=z_{T}=\\left[\\begin{array}{c c c c}{\\mathcal{U}_{M_{1}}({\\pmb x})}&&&\\\\ &{\\mathcal{U}_{M_{2}}({\\pmb x})}&&\\\\ &&{\\ddots}&\\\\ &&&{\\mathcal{U}_{M_{k}}({\\pmb x})}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now it follows immediately that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\pmb{p}_{t}^{M}=\\int_{\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}&{}&\\\\ {=\\left[\\begin{array}{l l l l}{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{1}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}&{}&&{}\\\\ &{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{2}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}&&{}\\\\ &&{\\ddots}&\\\\ &&&{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{k}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 3.3 follows immediately from the next lemma by inserting $\\mu\\,=\\,P_{\\hat{X}}$ and $\\nu\\,=\\,P_{\\hat{Y}}$ for prediction processes $\\hat{X}$ and $\\hat{Y}$ of filtered processes $\\mathbb{X}$ and $\\mathbb{Y}$ , respectively. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.7. Let $\\mu$ and $\\nu$ be two probability measures on measure\u2013valued path space $\\hat{\\mathcal X}$ (that is, $\\mu,\\nu\\in\\mathcal{P}(\\hat{\\mathcal{X}}))$ . Then $\\mu=\\nu$ if and only if for every admissible pair of unitary representations $(M,\\mathcal{M})\\in\\mathcal{A}_{u n i t a r y}$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Phi_{\\mu}^{2}(M,{\\mathcal M})=\\Phi_{\\nu}^{2}(M,{\\mathcal M}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Before we start a rigorous proof, let us first give an informal proof to provide some intuition: For each measure-valued path $\\pmb{p}\\ =\\ (\\pmb{p}_{t})_{t\\in I}\\ \\in\\ \\hat{\\mathcal{X}}$ , we first compute the PCF $p_{t}^{M}\\;=\\;$ $\\textstyle\\int_{\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})\\in\\cal{U}(m)$ for every $t\\in I$ , where $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(m))$ . By doing so, the measurevalued path $\\pmb{p}$ is transformed to a matrix-valued path $\\pmb{p}^{M}$ in $\\mathbb{C}^{m\\times m}$ . Thanks to the characteristic property of the PCF (see Theorem 2.6), each measure $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$ is represented by its $\\mathrm{PCF}\\,p_{t}^{M}$ , therefore we may study the matrix-valued path $\\pmb{p}^{M}$ instead of the measure-valued path $\\pmb{p}$ . Under such identification, the distributions $\\mu$ and $\\nu$ on the measure-valued path space $\\hat{\\chi}$ can be represented by the push-forward measure $p_{\\sharp}^{M}\\overset{\\cdot}{\\mu}$ and $p_{\\sharp}^{{M}}\\nu$ respectively, which are distributions on the matrix-valued path space. In other words, showing $\\mu=\\nu$ is equivalent to showing that $p_{\\sharp}^{M}\\mu=p_{\\sharp}^{M}\\nu$ . But now using the characteristic property of the PCF again, $p_{\\sharp}^{M}\\mu=p_{\\sharp}^{M}\\nu$ holds if and only if their PCF under linear operator $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{m\\times m},\\mathfrak{u}(n))$ coincide with each other, i.e., $\\Phi_{p_{\\sharp}^{\\scriptscriptstyle M}\\mu}({\\mathcal M})\\,=\\,\\Phi_{p_{\\sharp}^{\\scriptscriptstyle M}\\nu}({\\mathcal M})$ , and by definition one has $\\Phi_{p_{\\sharp}^{M}\\mu}({\\mathcal M})=\\Phi_{\\mu}^{2}(M,{\\mathcal M}),\\,\\Phi_{p_{\\sharp}^{M}\\nu}({\\mathcal M})=\\Phi_{\\nu}^{2}(M,{\\mathcal M}).$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Now we provide the rigorous proof of the theorem. Obviously we only need to show the \u201cif\u201d part. ", "page_idx": 20}, {"type": "text", "text": "Step 1: By hypothesis, for any admissible pair of unitary representations $(M,{\\mathcal{M}})\\in{\\mathcal{A}}_{\\mathrm{unitary}}$ with $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ and $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Phi_{\\mu}^{2}(M,\\mathcal{M})=\\int_{p\\in\\hat{\\mathcal{X}}}\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})\\mu(d p)=\\int_{p\\in\\hat{\\mathcal{X}}}\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})\\nu(d p)=\\Phi_{\\nu}^{2}(M,\\mathcal{M}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which means that $\\Phi_{(p\\mapsto p^{M})_{\\sharp}(\\mu)}(\\mathcal{M})=\\Phi_{(p\\mapsto p^{M})_{\\sharp}(\\nu)}(\\mathcal{M})$ , where the push-forward measures $\\mathbf{\\dot{\\rho}}p\\mapsto$ $p^{M})_{\\sharp}(\\mu)$ and $(\\pmb{p}\\mapsto\\pmb{p}^{M})_{\\sharp}(\\nu)$ are probability measures on the $\\mathbb{C}^{n\\times n}$ \u2013valued path space. ", "page_idx": 20}, {"type": "text", "text": "In fact, if we fix an arbitrary $n\\in\\mathbb N$ and an arbitrary $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ , and let $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ vary over all $m\\in\\mathbb{N}$ , we actually have the above equality $\\Phi_{(p\\mapsto p^{M})_{\\sharp}(\\mu)}({\\mathcal{M}})=\\Phi_{(p\\mapsto p^{M})_{\\sharp}(\\nu)}({\\mathcal{M}})$ for all $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{R}^{n},\\mathfrak{u}(m))$ , $m\\in\\mathbb{N}$ . Therefore, by applying the characteristicity of PCF of measures on finite dimensional vector space valued path spaces, see Theorem 2.6, we obtain that $(p\\mapsto$ $p^{M})_{\\sharp}(\\mu)=(p\\mapsto p^{M})_{\\sharp}(\\nu)$ for any $n\\in\\mathbb N$ and any $M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ . ", "page_idx": 20}, {"type": "text", "text": "Step 2: Fix an $k\\,\\in\\,\\mathbb{N}$ and a sequence of operators $\\tilde{M}\\,=\\,(M_{j})_{j=1}^{k}\\,\\in\\,\\bigoplus_{j=1}^{k}\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(j))$ . Let $n=1+2+\\ldots+k$ . By Lemma A.6 above, there exists an $M\\in{\\mathcal{L}}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ such that for any $\\pmb{p}\\in\\hat{\\mathcal X}$ , one has ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\pmb{p}_{t}^{M}=\\int_{\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}&{}&\\\\ {=\\left[\\begin{array}{l l l l}{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{1}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}&{}&&{}\\\\ &{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{2}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}&&{}\\\\ &&{\\ddots}&\\\\ &&&{\\int_{\\mathcal{X}}\\mathcal{U}_{M_{k}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we take an arbitrary partition $\\{t_{1}\\,<\\,t_{2}\\,<\\,.\\,.\\,<\\,t_{N}\\}$ of the time interval $[0,T]$ . Since we have shown in Step 1 that $(p\\,\\mapsto\\,p^{\\tilde{M}})_{\\sharp}(\\mu)\\,=\\,(p\\,\\mapsto\\,p^{M})_{\\sharp}(\\nu$ ), that is, the law of $\\mathbb{C}^{\\bar{n}\\times n}$ \u2013valued stochastic process $\\begin{array}{r}{\\pmb{p}_{t}^{M}=\\int_{\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x}),t\\in[0,T]}\\end{array}$ under $\\mu\\in\\mathcal{P}(\\hat{\\mathcal{X}})$ coincides with its law under $\\nu\\in\\mathcal{P}(\\hat{\\mathcal{X}})$ , we indeed have that the distributions of their marginals at $t_{1},\\ldots,t_{N}$ are same, that is, ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\pmb{p}\\mapsto(\\pmb{p}_{t_{1}}^{M},\\dots,\\pmb{p}_{t_{N}}^{M}))_{\\sharp}\\mu=(\\pmb{p}\\mapsto(\\pmb{p}_{t_{1}}^{M},\\dots,\\pmb{p}_{t_{N}}^{M}))_{\\sharp}\\nu\\in\\mathcal{P}((\\mathbb{C}^{n\\times n})^{N}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, for each $i=1,\\ldots,N$ and $j=1,\\dots,k$ , we pick arbitrary linear functions $L_{j}(i)\\in\\mathcal{L}(\\mathbb{C}^{j\\times j},\\mathbb{R})$ and continuous and bounded functions $g_{i}\\in C_{b}(\\mathbb{R})$ , and use them to define a function $\\tilde{g}_{i}:\\mathbb{C}^{n\\times n}\\rightarrow\\mathbb{R}$ for $i=1,\\ldots,N$ such that for any matrix $A\\in\\mathbb{C}^{n\\times n}$ (recall that $n=1+2+\\ldots+k)$ written in the form ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A=\\left[\\begin{array}{c c c c c}{A_{1}\\in\\mathbb{C}^{1\\times1}}&&{\\star}&{\\star}&{\\star}\\\\ {\\star}&{A_{2}\\in\\mathbb{C}^{2\\times2}}&{\\star}&{\\star}\\\\ &{\\star}&{\\star}&{\\ddots}&{\\star}\\\\ {\\star}&&{\\star}&{\\star}&{A_{k}\\in\\mathbb{C}^{k\\times k}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{g}_{i}(A)=g_{i}\\circ\\biggl(\\sum_{j=1}^{k}{\\pmb{L}}_{j}(i)\\circ A_{i}\\biggr).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Obviously each function $\\tilde{g}_{i}$ is continuous and bounded. ", "page_idx": 20}, {"type": "text", "text": "Let $\\tilde{g}\\ :\\ \\dot{(\\mathbb{C}^{n\\times n})^{N}}\\ \\rightarrow\\ \\dot{\\mathbb{R}}$ be the continuous and bounded function such that $\\tilde{g}(A^{1},\\cdot\\cdot\\cdot,A^{N})\\;=$ $\\prod_{i=1}^{N}\\tilde{g}_{i}(A^{i})$ for every sequence $\\bar{A}\\ =\\ (A^{1},\\ldots,A^{N})\\ \\in\\ (\\mathbb{C}^{n\\times n})^{N}$ . From the equality $(p\\mapsto$ $\\mathring{(}p_{t_{1}}^{\\check{M}},\\cdot\\cdot\\cdot,p_{t_{N}}^{M}))_{\\sharp}\\mu=(p\\mapsto(p_{t_{1}}^{M},\\cdot\\cdot\\cdot,p_{t_{N}}^{M}))_{\\sharp}\\nu\\in\\mathcal{P}((\\mathbb{C}^{n\\times n})^{N})$ it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int\\tilde{g}(\\bar{A})(p\\mapsto(p_{t_{1}}^{M},\\ldots,p_{t_{N}}^{M}))_{\\sharp}\\mu(d\\bar{A})=\\int\\tilde{g}(\\bar{A})(p\\mapsto(p_{t_{1}}^{M},\\ldots,p_{t_{N}}^{M}))_{\\sharp}\\nu(d\\bar{A}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which can be reformulated as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\hat{\\mathcal{X}}}\\prod_{i=1}^{N}g_{i}\\left(\\sum_{j=1}^{k}\\mathbb{E}_{p_{t_{i}}}[\\mathbf{L}_{j}(i)\\circ\\mathcal{U}_{M_{j}}]\\right)\\mu(d p)=}}\\\\ &{}&{\\displaystyle\\int_{\\hat{\\mathcal{X}}}\\prod_{i=1}^{N}g_{i}\\left(\\sum_{j=1}^{k}\\mathbb{E}_{p_{t_{i}}}[\\mathbf{L}_{j}(i)\\circ\\mathcal{U}_{M_{j}}]\\right)\\nu(d p)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbb{E}_{p_{t}}[L_{j}(i)\\circ\\mathcal{U}_{M_{j}}]=\\int_{\\pmb{x}\\in\\mathcal{X}}\\pmb{L}_{j}(i)\\circ\\mathcal{U}_{M_{j}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x}).}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Step $^3$ : It is a well known fact (see e.g. [7]) that the vector space generated by all real-valued linear functionals of unitary representations on the path space $\\mathcal{X}$ , namely ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{C}=\\mathrm{span}\\{L\\circ\\mathcal{U}_{M}:\\mathcal{X}\\to\\mathbb{R}:L\\in\\mathcal{L}(\\mathbb{C}^{j\\times j},\\mathbb{R}),M\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(j)),j\\in\\mathbb{N}\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is a sub-algebra in the space $C_{b}(\\mathcal{X})$ of continuous and bounded (real-valued) functions on $\\mathcal{X}$ which separates the points. Moreover, by picking $M_{0}:\\mathbb{R}^{d}\\rightarrow\\mathfrak{u}(1)$ to be the trivial representation (i.e., $M_{0}(x)=0\\in\\mathbb{C}$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ) we see that for any path $\\pmb{x}\\in\\mathcal{X}$ , $M_{0}(\\mathbf{{x}})=1\\in\\mathbb{R}$ . Therefore, by the Giles\u2019 Theorem ([8, Theorem 9]) it follows that the set $\\mathcal{C}$ is dense in $C_{b}(\\mathcal{X})$ related to the so called strict topology8. ", "page_idx": 21}, {"type": "text", "text": "Now, fix arbitrary continuous and bounded functions $f_{i}\\,\\in\\,C_{b}(\\mathcal{X})$ , $i=1,\\ldots,N$ . From the density of $\\mathcal{C}$ in $C_{b}(\\mathcal{X})$ one can find a sequence of unitary representations $\\tilde{M}^{(k)}\\;=\\;(M_{j}^{(k)})_{j=1}^{k}\\;\\in$ $\\begin{array}{r}{\\bigoplus_{j=1}^{k}\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(j))}\\end{array}$ , $k\\in\\mathbb{N}$ together with a sequence of linear operators $(L^{(k)}(i))_{k\\in\\mathbb{N}}$ , $i=1,\\ldots,N$ with each $\\begin{array}{r}{\\pmb{L}^{k}(i)=(\\pmb{L}_{j}^{(k)}(i))_{j=1}^{k}\\in\\bigoplus_{j=1}^{k}\\mathcal{L}(\\mathbb{C}^{j\\times j},\\mathbb{R})}\\end{array}$ such that for every $i=1,\\ldots,N$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{i}=\\operatorname*{lim}_{k\\to\\infty}\\sum_{j=1}^{k}{\\pmb{L}}_{j}^{(k)}(i)\\circ\\mathcal{U}_{M_{j}^{(k)}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the convergence happens in the strict topology. Furthermore, since every probability measure $\\pmb{p}_{t_{i}}\\,\\in\\,\\mathcal{P}(\\mathcal{X})\\;(i\\,\\stackrel{!}{=}\\,1,\\dotsc,\\bar{\\mathcal{N}})$ belongs to the topological dual of $C_{b}(\\mathcal{X})$ equipped with the strict topology by the Giles\u2019 theorem, invoking the relation (12) we actually obtain that for every $i=$ $1,\\ldots,N$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{t_{i}}}[f_{i}]=\\int_{\\mathcal{X}}f_{i}(\\pmb{x})p_{t_{i}}(d\\pmb{x})=\\operatorname*{lim}_{k\\rightarrow\\infty}\\sum_{j=1}^{k}\\mathbb{E}_{p_{t_{i}}}[\\pmb{L}_{j}^{(k)}(i)\\circ\\mathcal{U}_{M_{j}^{(k)}}].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, as a consequence of the result (11) obtained in Step 2, we can apply the bounded convergence theorem to get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\hat{x}}\\prod_{i=1}^{N}g_{i}(\\mathbb{E}_{\\pmb{p}_{t_{i}}}[f_{i}])\\mu(d\\boldsymbol{p})=\\operatorname*{lim}_{k\\rightarrow\\infty}\\int_{\\hat{x}}\\prod_{i=1}^{N}g_{i}\\left(\\sum_{j=1}^{k}\\mathbb{E}_{\\mathbb{p}_{t_{i}}}[\\pmb{L}_{j}^{(k)}(i)\\circ\\mathcal{U}_{M_{j}^{(k)}}]\\right)\\mu(d\\boldsymbol{p})}}\\\\ &{}&{=\\operatorname*{lim}_{k\\rightarrow\\infty}\\int_{\\hat{x}}\\prod_{i=1}^{N}g_{i}\\left(\\displaystyle\\sum_{j=1}^{k}\\mathbb{E}_{\\mathbb{p}_{t_{i}}}[\\pmb{L}_{j}^{(k)}(i)\\circ\\mathcal{U}_{M_{j}^{(k)}}]\\right)\\nu(d\\boldsymbol{p})}\\\\ &{}&{=\\displaystyle\\int_{\\hat{x}}\\prod_{i=1}^{N}g_{i}(\\mathbb{E}_{\\pmb{p}_{t_{i}}}[f_{i}])\\nu(d\\boldsymbol{p}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, by the Urysohn\u2019s lemma, for any $i=1,\\ldots,N$ , any positive number $R_{i}>0$ , the indicator function $1_{[-R_{i},R_{i}]}$ can be pointwise approximated by a sequence of $[0,1].$ \u2013valued continuous functions $(g_{i}^{\\ell})_{\\ell\\in\\mathbb{N}}$ . Hence, by replacing the functions $g_{i}$ by $g_{i}^{\\ell}$ in (13) and then letting $\\ell\\to\\infty$ , using the bounded convergence theorem we can derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{\\hat{X}}\\prod_{i=1}^{N}1_{[-R_{i},R_{i}]}({\\mathbb{E}}_{p_{t_{i}}}[f_{i}])\\mu(d p)=\\int_{\\hat{X}}\\prod_{i=1}^{N}1_{[-R_{i},R_{i}]}({\\mathbb{E}}_{p_{t_{i}}}[f_{i}])\\nu(d p)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "8For the definition of the strict topology, see e.g. [8, Definition 8]. ", "page_idx": 21}, {"type": "text", "text": "or, equivalently, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{\\hat{X}}\\prod_{i=1}^{N}1_{\\theta_{f_{i}}^{-1}([-R_{i},R_{i}])}(p_{t_{i}})\\mu(d p)=\\int_{\\hat{X}}\\prod_{i=1}^{N}1_{\\theta_{f_{i}}^{-1}([-R_{i},R_{i}])}(p_{t_{i}})\\nu(d p)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\theta_{f_{i}}(\\pmb{p}_{t_{i}}):=\\mathbb{E}_{\\pmb{p}_{t_{i}}}[f_{i}]$ denotes the evaluation map of $f_{i}\\in C_{b}(\\mathcal{X})$ against the measure $\\pmb{p}_{t_{i}}$ ", "page_idx": 22}, {"type": "text", "text": "Step $^{4}$ : By the very definition of weak topology on $\\mathcal{P}(\\mathcal{X})$ , its Borel $\\sigma.$ \u2013algebra is generated by the sets of the form that $\\theta_{f}^{-1}([-R,R])$ for $f\\,\\in\\,C_{b}(\\mathcal{X})$ and $R\\,>\\,0$ . Consequently, the Borel $\\sigma-$ algebra on the product space $\\mathcal{P}(\\mathcal{X})^{N}$ is generated by the measurable rectangles of the form that $\\begin{array}{r}{\\prod_{i=1}^{N}\\theta_{f_{i}}^{-1}([-R_{i},R_{i}])}\\end{array}$ for $f_{i}\\in C_{b}(\\mathcal{X})$ and $R_{i}>0$ . From Eq. (14) we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu((p_{t_{1}},\\ldots,p_{t_{N}})\\in\\prod_{i=1}^{N}\\theta_{f_{i}}^{-1}([-R_{i},R_{i}]))=\\nu((p_{t_{1}},\\ldots,p_{t_{N}})\\in\\prod_{i=1}^{N}\\theta_{f_{i}}^{-1}([-R_{i},R_{i}]))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all such measurable rectangles. Since the above equation holds for any partition $\\{t_{1}<...<t_{N}\\}$ of $[0,T]$ and the laws of (continuous) stochastic processes are uniquely determined by their marginals on finitely many time points, we can conclude that $\\mu=\\nu$ in $\\mathcal{P}(\\hat{\\mathcal{X}})$ by a routine application of the monotone class theorem. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Now, for filtered processes $\\mathbb{X}$ and $\\mathbb{Y}$ , we note that the associated prediction processes $\\hat{X}$ and $\\hat{Y}$ are stochastic processes taking values in $\\mathcal{P}(\\mathcal{X})$ which can be viewed as $\\hat{\\chi}$ -valued random variable, which in turn implies that their laws $P_{\\hat{X}}$ and $P_{\\hat{Y}}$ are elements in $\\mathscr{P}(\\hat{\\mathscr{X}})$ . Hence, inserting $\\mu=P_{\\hat{X}}$ and $\\nu=P_{\\hat{Y}}$ into the above Lemma A.7 we can easily deduce Theorem 3.3. ", "page_idx": 22}, {"type": "text", "text": "A.4 Properties of HRPCFD ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we will mainly prove the properties recorded in section 3.3. ", "page_idx": 22}, {"type": "text", "text": "First let us prove the property of HRPCFD on the separation of laws of prediction processes. To achieve this we need the following useful continuity lemma. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.8. For any fixed $\\mu\\in\\mathcal{P}(\\hat{\\mathcal{X}})$ , any fixed $n$ and $m$ , the mapping ", "page_idx": 22}, {"type": "equation", "text": "$$\n(M,\\mathcal{M})\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))\\times\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))\\mapsto\\Phi_{\\mu}^{2}(M,\\mathcal{M})\\in\\mathbb{C}^{m\\times m}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is continuous for the operator norm topology on $\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))\\times\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ and the Hilbert\u2013 Schmidt norm topology on $\\mathbb{C}^{m\\times m}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. For admissible pairs $(M,\\mathcal{M})$ and $(M^{\\prime},\\mathcal{M}^{\\prime})$ from $\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))\\times\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ , by the definition of HRPCF we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi_{\\mu}^{2}(M,M)-\\Phi_{\\mu}^{2}(M^{\\prime},M^{\\prime})\\|_{\\mathrm{HS}}=\\Big\\|\\displaystyle\\int_{p\\in\\tilde{\\mathcal{X}}}\\mathcal{U}_{M}(t\\mapsto p_{t}^{M})\\mu(d p)-\\displaystyle\\int_{p\\in\\tilde{\\mathcal{X}}}\\mathcal{U}_{M^{\\prime}}(t\\mapsto p_{t}^{M^{\\prime}})\\mu(d p)\\Big\\|_{\\mathrm{HS}}}&{}\\\\ {\\le\\displaystyle\\int_{p\\in\\hat{\\mathcal{X}}}\\Big\\|\\mathcal{U}_{M}(t\\mapsto p_{t}^{M})-\\mathcal{U}_{M^{\\prime}}(t\\mapsto p_{t}^{M^{\\prime}})\\Big\\|_{\\mathrm{HS}}\\mu(d p)}&{}\\\\ {\\le\\displaystyle\\int_{p\\in\\hat{\\mathcal{X}}}\\Big\\|\\mathcal{U}_{M}(t\\mapsto p_{t}^{M})-\\mathcal{U}_{M}(t\\mapsto p_{t}^{M^{\\prime}})\\Big\\|_{\\mathrm{HS}}\\mu(d p)}&{}\\\\ {\\qquad+\\displaystyle\\int_{p\\in\\hat{\\mathcal{X}}}\\Big\\|\\mathcal{U}_{M}(t\\mapsto p_{t}^{M^{\\prime}})-\\mathcal{U}_{M^{\\prime}}(t\\mapsto p_{t}^{M^{\\prime}})\\Big\\|_{\\mathrm{HS}}\\mu(d p).}&{(1\\vee)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let us first estimate the first integrand on the right hand side of (15). By [19, Proposition B.6] we know that for each measure\u2013valued path $\\pmb{p}\\in\\hat{\\mathcal X}$ , one has ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})-\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M^{\\prime}})\\|_{\\mathrm{HS}}\\leq\\|\\mathcal{M}\\|_{\\mathrm{op}}\\|p^{M}-p^{M^{\\prime}}\\|_{1\\mathrm{-var}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb{p}_{t}^{M}=\\Phi_{\\pmb{p}_{t}}(M)=\\int_{\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}\\end{array}$ and $\\begin{array}{r}{\\pmb{p}_{t}^{M^{\\prime}}=\\Phi_{\\pmb{p}_{t}}(M^{\\prime})=\\int_{\\mathcal{X}}\\mathcal{U}_{M^{\\prime}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}\\end{array}$ are $\\mathbb{C}^{n\\times n}-$ valued paths. Since $\\pmb{p}\\in\\hat{\\mathcal X}$ is piecewise linear, these $\\mathbb{C}^{n\\times n}$ \u2013valued paths $\\pmb{p}^{M}=(t\\mapsto\\Phi_{p_{t}}(M))$ and ", "page_idx": 22}, {"type": "text", "text": "$p^{M^{\\prime}}=(t\\mapsto\\Phi_{p_{t}}(M^{\\prime}))$ are also piecewise linear, say, they are linear on time subintervals $[t_{i},t_{i+1}]$ for $i=0,\\ldots,N-1$ . Then we indeed have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Vert p^{M}-p^{M^{\\prime}}\\Vert_{1\\cdot\\mathrm{var}}=\\sum_{i=0}^{N-1}\\Vert(p^{M}-p^{M^{\\prime}})_{t_{i},t_{i+1}}\\Vert_{\\mathrm{HS}}\\leq2\\sum_{i=0}^{N}\\Vert p_{t_{i}}^{M}-p_{t_{i}}^{M^{\\prime}}\\Vert_{\\mathrm{HS}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "whence the estimates ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathcal{M}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})-\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M^{\\prime}})\\|_{\\mathrm{HS}}\\lesssim\\|\\mathcal{M}\\|_{\\infty}\\sum_{i=0}^{N}\\|p_{t_{i}}^{M}-p_{t_{i}}^{M^{\\prime}}\\|_{\\mathrm{HS}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we note that for each $i=0,\\ldots,N$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{t_{i}}^{M}-p_{t_{i}}^{M^{\\prime}}=\\int_{x\\in\\mathcal{X}}\\mathcal{U}_{M}(\\pmb{x})p_{t_{i}}(d\\pmb{x})-\\int_{x\\in\\mathcal{X}}\\mathcal{U}_{M^{\\prime}}(\\pmb{x})p_{t_{i}}(d\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recalling that for each $\\mathbb{R}^{d}$ \u2013valued path $\\pmb{x}\\in\\mathcal{X}$ , one has $\\mathcal{U}_{M}(\\pmb{x})\\b{=}\\pmb{y}_{T}^{M,\\pmb{x}}$ yTM,xand UM \u2032(x) = yTM where $\\pmb{y}^{M,\\pmb{x}}$ and $\\pmb{y}^{M^{\\prime},\\pmb{x}}$ are the unique solutions to the linear ODEs ", "page_idx": 23}, {"type": "equation", "text": "$$\nd\\pmb{y}_{t}^{M,x}=\\pmb{y}_{t}^{M,x}M(d\\pmb{x}_{t}),\\quad\\pmb{y}_{0}^{M,x}=I_{n}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\nd y_{t}^{M^{\\prime},x}=y_{t}^{M^{\\prime},x}M^{\\prime}(d x_{t}),\\quad y_{0}^{M^{\\prime},x}=I_{n}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "respectively, by the continuity of the flow of ODE (see e.g. [11, Theorem 3.15]), we obtain that for any $x\\in\\mathcal{X}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{U}_{M}(\\pmb{x})-\\mathcal{U}_{M^{\\prime}}(\\pmb{x})\\|_{\\mathrm{HS}}\\leq C(n,\\|\\pmb{x}\\|_{1\\times\\mathrm{var}})\\|M-M^{\\prime}\\|_{\\mathrm{op}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, if $\\|M^{\\prime}-M\\|_{\\mathrm{op}}\\rightarrow0$ , then for all $x\\in\\mathcal{X}$ we have $\\|\\mathcal{U}_{M}(\\pmb{x})-\\mathcal{U}_{M^{\\prime}}(\\pmb{x})\\|_{\\mathrm{HS}}\\rightarrow0$ . Then because $\\mathcal{U}_{M}$ and $\\mathcal{U}_{M^{\\prime}}$ are unitary representations taking values in the compact group $U(n)$ , by the dominated convergence theorem we have $\\lVert\\pmb{p}_{t_{i}}^{M}-\\pmb{p}_{t_{i}}^{M^{\\prime}}\\rVert_{\\mathrm{HS}}\\to0$ as $\\|\\boldsymbol{M}^{\\prime}-\\boldsymbol{M}\\|_{\\mathrm{op}}\\to0$ for any $i=0,\\ldots,N$ . As a result, in view of (16) we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|M^{\\prime}-M\\|_{\\Phi}\\rightarrow0\\Rightarrow\\|\\mathcal{U}_{M}(t\\mapsto p_{t}^{M})-\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M^{\\prime}})\\|_{\\mathrm{HS}}\\rightarrow0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $\\pmb{p}\\in\\hat{\\mathcal X}$ . Then, as $\\mathcal{U}_{\\mathcal{M}}$ is a unitary representation taking values in the compact group $U(m)$ , by the dominated convergence theorem again we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{M}^{\\prime}-\\boldsymbol{M}\\|_{\\mathrm{op}}\\to0\\Rightarrow\\int_{p\\in\\hat{\\mathcal{X}}}\\left\\|\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{\\boldsymbol{M}})-\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{\\boldsymbol{M}^{\\prime}})\\right\\|_{\\mathrm{HS}}\\mu(d p)\\to0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next we turn to bound the second integrand in (15), namely $\\|\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M^{\\prime}})-\\mathcal{U}_{\\mathcal{M}^{\\prime}}(t\\mapsto\\pmb{p}_{t}^{M^{\\prime}})\\|_{\\mathrm{HS}}.$ . Again, invoking that $\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M^{\\prime}})\\,=\\,\\pmb{y}_{T}^{\\mathcal{M},\\pmb{p}^{M^{\\prime}}}$ and $\\mathcal{U}_{\\mathcal{M}^{\\prime}}(t\\mapsto\\pmb{p}_{t}^{M^{\\prime}})=\\pmb{y}_{T}^{\\mathcal{M}^{\\prime},\\pmb{p}^{M^{\\prime}}}$ are the unique solutions (evaluated at $T$ ) to the linear ODEs ", "page_idx": 23}, {"type": "equation", "text": "$$\nd y_{t}^{\\mathcal{M},p^{M^{\\prime}}}=y_{t}^{\\mathcal{M},p^{M^{\\prime}}}\\mathcal{M}(d p_{t}^{M^{\\prime}}),\\quad y_{0}^{\\mathcal{M},p^{M^{\\prime}}}=I_{m}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\nd y_{t}^{\\mathcal{M}^{\\prime},p^{M^{\\prime}}}=y_{t}^{\\mathcal{M}^{\\prime},p^{M^{\\prime}}}\\mathcal{M}^{\\prime}(d p_{t}^{M^{\\prime}}),\\quad y_{0}^{\\mathcal{M}^{\\prime},p^{M^{\\prime}}}=I_{m}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "respectively, by the continuity of the flow of ODE, we obtain that for each $\\pmb{p}\\in\\hat{\\mathcal X}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{U}_{\\mathcal{M}}(t\\mapsto{p}_{t}^{M^{\\prime}})-\\mathcal{U}_{\\mathcal{M}^{\\prime}}(t\\mapsto{p}_{t}^{M^{\\prime}})\\|_{\\mathrm{HS}}\\le C(m,\\|{p^{M^{\\prime}}}\\|_{1\\mathrm{-var}})\\|\\mathcal{M}-\\mathcal{M}^{\\prime}\\|_{\\mathrm{op}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\mathcal{U}_{M^{\\prime}}$ takes values in the compact group $U(n)$ , it is easy to see that for piecewise linear path $\\begin{array}{r}{\\pmb{p}_{t}^{M^{\\prime}}=\\int\\mathcal{U}_{M^{\\prime}}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x})}\\end{array}$ it holds that $\\mathrm{sup}_{M^{\\prime}\\in{\\mathcal{L}}(\\mathbb{R}^{d},{\\mathfrak{u}}(n))}\\,\\|{\\pmb{p}}^{M^{\\prime}}\\|_{1\\mathrm{-var}}<\\infty$ , which implies that for any $\\pmb{p}\\in\\hat{\\mathcal X}$ and for any $M^{\\prime}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{M}^{\\prime}-\\mathcal{M}\\|_{\\mathrm{op}}\\to0\\Rightarrow\\|\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M^{\\prime}})-\\mathcal{U}_{\\mathcal{M}^{\\prime}}(t\\mapsto\\pmb{p}_{t}^{M^{\\prime}})\\|_{\\mathrm{HS}}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Again, since $\\mathcal{U}_{\\mathcal{M}}$ and $\\mathcal{U}_{\\mathcal{M}^{\\prime}}$ are unitary features with values in compact group $U(m)$ , by the dominated convergence theorem we must have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{p\\in\\hat{\\mathcal{X}}}\\left\\|\\mathcal{M}_{\\mathcal{M}}(t\\mapsto p_{t}^{M^{\\prime}})-\\mathcal{U}_{\\mathcal{M}^{\\prime}}(t\\mapsto p_{t}^{M^{\\prime}})\\right\\|_{\\mathrm{HS}}\\mu(d p)\\rightarrow0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as long as $\\|\\mathcal{M}^{\\prime}-\\mathcal{M}\\|_{\\mathrm{op}}\\to0$ . Now, combining (18), (17) and (15) we can conclude that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\Phi_{\\mu}^{2}(M,\\mathcal{M})-\\Phi_{\\mu}^{2}(M^{\\prime},\\mathcal{M}^{\\prime})\\|_{\\mathrm{HS}}\\rightarrow0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as long as $\\|M^{\\prime}-M\\|_{\\mathrm{op}}\\rightarrow0,\\|M^{\\prime}-\\mathcal{M}\\|_{\\mathrm{op}}\\rightarrow0$ , which is the desired continuity claim. ", "page_idx": 24}, {"type": "text", "text": "Now we are able to prove the first property of HRPCFD. ", "page_idx": 24}, {"type": "text", "text": "Theorem A.9 (Separation of points). Let $\\mu,\\nu\\in\\mathcal{P}(\\hat{\\mathcal{X}})$ be two distributions on measure\u2013valued path space such that $\\mu\\neq\\nu$ . Then there exists a pair of integers $(n,m)\\,\\in\\,\\mathbb{N}^{2}$ such that for any $P_{M}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n)))$ with full support and any $P_{\\pmb{\\mathscr{M}}}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)))$ with full support, one has ", "page_idx": 24}, {"type": "equation", "text": "$$\nH\\!R P C F D_{M,\\mathcal{M}}(\\mu,\\nu)>0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular, for filtered processes $\\mathbb{X}$ and Y, if they are not synonymous, then with $\\mu=P_{\\hat{X}}$ and $\\nu=P_{\\hat{Y}}$ there exists a pair of integers $(n,m)\\in\\mathbb{N}^{2}$ such that for any $P_{M}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n)))$ with full support and any $P_{\\pmb{\\mathscr{M}}}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)))$ ) with full support, one has ", "page_idx": 24}, {"type": "equation", "text": "$$\nH R P C F D_{M,\\mathcal{M}}(\\mathbb{X},\\mathbb{Y})>0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Thanks to Lemma A.7, if $\\mu\\neq\\nu$ , then there must exist an admissible pair of unitary representations $(M_{0},\\mathcal{M}_{0})\\in\\mathcal{A}_{\\mathrm{unitary}}$ with $\\dot{M}_{0}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ and $\\mathcal{M}_{0}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Phi_{\\mu}^{2}(M_{0},{\\mathcal M}_{0})\\neq\\Phi_{\\nu}^{2}(M_{0},{\\mathcal M}_{0}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, by the continuity result proved in Lemma A.8, there exists a $\\delta>0$ such that for all $M\\in$ $\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ and all $\\bar{\\mathcal{M}}\\in\\mathcal{L}(\\mathbb{C}^{\\bar{n}\\times n},\\mathfrak{u}(m))$ with $\\|M_{0}-M\\|_{\\mathrm{op}}\\leq\\delta$ and $\\|\\mathcal{M}_{0}-\\mathcal{M}\\|_{\\mathrm{op}}\\leq\\delta$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\nd_{\\mathrm{HS}}(\\Phi_{\\mu}^{2}(M,{\\mathcal{M}}),\\Phi_{\\nu}^{2}(M,{\\mathcal{M}}))>0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $B(M_{0},\\delta)\\subset\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ and $B(\\mathcal{M}_{0},\\delta)\\subset\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ denote the ball centered at $M_{0}$ and $\\mathcal{M}_{0}$ with radius $\\delta$ (with respect to the operator norms) respectively. Then if $P_{M}$ and $P_{\\mathbf{\\mathcal{M}}}$ have full supports, we have $P_{M}(B(M_{0},\\delta))>0$ and $P_{\\mathbf{\\mathcal{M}}}(B(\\mathcal{M}_{0},\\delta))>0$ , which implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{HRPCFD}_{M,M}^{2}(\\mu,\\nu)=\\int\\int d_{\\mathrm{HS}}^{2}(\\Phi_{\\mu}^{2}(M,\\mathcal{M}),\\Phi_{\\nu}^{2}(M,\\mathcal{M}))P_{M}(d M)P_{M}(d\\mathcal{M})}}\\\\ &{\\ge\\displaystyle\\int_{B(M_{0},\\delta)}\\int_{B(M_{0},\\delta)}d_{\\mathrm{HS}}^{2}(\\Phi_{\\mu}^{2}(M,\\mathcal{M}),\\Phi_{\\nu}^{2}(M,\\mathcal{M}))P_{M}(d M)P_{M}(d\\mathcal{M})}\\\\ &{>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as claimed. ", "page_idx": 24}, {"type": "text", "text": "The boundedness of HRPCFD is easy to show by using the same arguments as in the proof of [19, Lemma 3.5] for PCFD. ", "page_idx": 24}, {"type": "text", "text": "Lemma A.10. Let $\\mu,\\nu\\in\\mathcal{P}(\\hat{\\mathcal{X}})$ be two distributions on measure\u2013valued path space. Then for any given integers $(n,m)\\in\\mathbb{N}^{2}$ , for any $P_{M}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n)))$ and any $P_{\\pmb{\\mathscr{M}}}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)))$ , one has ", "page_idx": 24}, {"type": "equation", "text": "$$\nH\\!R P C F D_{M,\\mathcal{M}}(\\mu,\\nu)\\leq2\\sqrt{m}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. By the triangle inequality, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathrm{HRPCFD}_{M,\\mathcal{M}}^{2}(\\mu,\\nu)=\\int\\int d_{\\mathrm{HS}}^{2}(\\Phi_{\\mu}^{2}(M,\\mathcal{M}),\\Phi_{\\nu}^{2}(M,\\mathcal{M}))P_{M}(d M)P_{\\mathbf{\\mathcal{M}}}(d\\mathcal{M})}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\leq\\int\\int(\\|\\Phi_{\\mu}^{2}(M,\\mathcal{M})\\|_{\\mathrm{HS}}+\\|\\Phi_{\\nu}^{2}(M,\\mathcal{M})\\|_{\\mathrm{HS}})^{2}P_{M}(d M)P_{\\mathbf{\\mathcal{M}}}(d\\mathcal{M}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\begin{array}{r}{\\Phi_{\\mu}^{2}(M,\\mathcal{M})=\\int_{p\\in\\hat{\\mathcal{X}}}\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})\\mu(d p)}\\end{array}$ and $\\mathcal{U}_{\\mathcal{M}}$ takes values in $U(m)$ such that $\\Vert\\mathcal{U}_{\\mathcal{M}}\\Vert_{\\mathrm{HS}}=$ $\\sqrt{\\mathrm{tr}(\\mathcal{U}_{\\mathcal{M}}\\mathcal{U}_{\\mathcal{M}}^{*})}=\\sqrt{\\mathrm{tr}(I_{m})}=\\sqrt{m}$ , we indeed have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\Phi_{\\mu}^{2}(M,\\mathcal{M})\\|_{\\mathrm{HS}}\\leq\\bigg(\\int_{p\\in\\hat{\\mathcal{X}}}\\|\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})\\|_{\\mathrm{HS}}^{2}\\mu(d p)\\bigg)^{\\frac{1}{2}}\\leq\\sqrt{m}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, it holds that $\\Vert\\Phi_{\\nu}^{2}(M,\\mathcal{M})\\Vert_{\\mathrm{HS}}\\leq\\sqrt{m}$ . Combining all above together we can deduce that $\\mathrm{HRPCFD}_{M,\\mathcal{M}}^{2}(\\mu,\\nu)\\leq4\\overset{.}{m}$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Just like the classical PCFD (cf. [19, Proposition B.10]) we can also show that the HRPCFD is a specific Maximum Mean Discrepancy (MMD). For the definition of MMD, we refer readers to [19, Definition B.9]. ", "page_idx": 25}, {"type": "text", "text": "Proposition A.11. For any $(n,m)~\\in~\\mathbb{N}^{2}$ , any ${P}_{M}\\;\\;\\in\\;\\;{\\mathcal{P}}({\\mathcal{L}}(\\mathbb{R}^{d},{\\mathfrak{u}}(n)))$ and any $P_{\\pmb{\\mathscr{M}}}~\\in$ $\\mathcal{P}(\\bar{\\mathcal{L}}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)))$ , the HRPCFD with respect to $P_{M}$ and $P_{\\pmb{M}}$ is an MMD with the kernel function $\\hat{\\kappa}:\\hat{\\mathcal{X}}\\times\\hat{\\mathcal{X}}\\to\\mathbb{R}$ given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\kappa}(\\pmb{p},\\tilde{\\pmb{p}})=\\mathbb{E}_{P_{M}\\otimes P_{\\pmb{M}}}[\\langle\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M}),\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\tilde{\\pmb{p}}_{t}^{M})\\rangle_{H S}]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{P_{M}\\otimes P_{\\pmb{M}}}[t r(\\mathcal{U}_{\\mathcal{M}}(\\pmb{p}^{M}\\star(\\tilde{\\pmb{p}}^{M})^{-1}))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\star$ denotes the concatenation operator on paths and $(\\tilde{p}^{M})^{-1}$ denotes the reverse of the path t  \u2192p\u02dctM . ", "page_idx": 25}, {"type": "text", "text": "Proof. For $\\mu,\\nu\\in\\mathcal{P}(\\hat{\\mathcal{X}})$ , it is easy to deduce that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathrm{HRPCFD}_{M,\\mathcal{M}}^{2}(\\mu,\\nu)=\\int\\int\\|\\Phi_{\\mu}^{2}(M,\\mathcal{M})-\\Phi_{\\nu}^{2}(M,\\mathcal{M})\\|_{\\mathrm{HS}}^{2}P_{M}(d M)P_{M}(d\\mathcal{M})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{P_{M}\\otimes P_{M}}[\\|\\Phi_{\\mu}^{2}(M,\\mathcal{M})\\|_{\\mathrm{HS}}^{2}]+\\mathbb{E}_{P_{M}\\otimes P_{M}}[\\|\\Phi_{\\nu}^{2}(M,\\mathcal{M})\\|_{\\mathrm{HS}}^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\ 2\\mathbb{E}_{P_{M}\\otimes P_{M}}[\\langle\\Phi_{\\mu}^{2}(M,\\mathcal{M}),\\Phi_{\\nu}^{2}(M,\\mathcal{M})\\rangle_{\\mathrm{HS}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phantom{\\varepsilon\\otimes P_{\\mathbb{A}}}[\\langle\\Phi_{\\mu}^{2}(M,M),\\Phi_{\\nu}^{2}(M,\\mathcal{M})\\rangle_{\\mathbb{H}^{S}}]=\\int\\langle\\int\\mathcal{U}_{M}(p^{M})\\mu(d p),\\int\\mathcal{U}_{M}(\\tilde{p}^{M})\\nu(d\\tilde{p})\\rangle_{\\mathbb{H}^{S}}d(\\mathbb{P}_{M}\\otimes\\mathbb{P}_{M})}&{}\\\\ {=\\int\\int\\langle\\mathcal{U}_{M}(p),\\mathcal{U}_{M}(\\tilde{p}^{M})\\rangle_{\\mathbb{H}^{S}}\\mu(d p)\\otimes\\nu(d\\tilde{p})d(P_{M}\\otimes P_{M})}&{}\\\\ {=\\int\\bigg(\\int\\langle\\mathcal{U}_{M}(p),\\mathcal{U}_{M}(\\tilde{p}^{M})\\rangle_{\\mathbb{H}^{S}}d(P_{M}\\otimes P_{M})\\bigg)\\mu(d p)\\otimes\\nu(d p)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used the Fubini\u2019s theorem in the last equality. Therefore we actually obtain that for the kernel ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\kappa}(\\pmb{p},\\tilde{p})=\\mathbb{E}_{\\mathbb{P}_{M}\\otimes\\mathbb{P}_{M}}[\\langle\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M}),\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\tilde{\\pmb{p}}_{t}^{M})\\rangle_{\\mathrm{HS}}]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{^{3CFD_{\\!M\\!,\\!M}^{2}}}(\\mu,\\nu)=\\int\\hat{\\kappa}(p,\\tilde{p})\\mu(d p)\\otimes\\mu(d\\tilde{p})+\\int\\hat{\\kappa}(p,\\tilde{p})\\nu(d p)\\otimes\\nu(d\\tilde{p})-2\\int\\hat{\\kappa}(p,\\tilde{p})\\mu(d p)\\otimes\\nu(d\\tilde{p}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies that $\\mathrm{HRPCFD}_{M,M}$ is a MMD with the kernel function $\\hat{\\kappa}$ . ", "page_idx": 25}, {"type": "text", "text": "The last claim is obvious: for any $\\pmb{p},\\tilde{\\pmb{p}}\\in\\hat{\\mathcal{X}}$ , one has, due to the fact that every $A\\in U(m)$ satisfies $A^{-1}=A^{*}$ , that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\kappa(\\pmb{p},\\tilde{\\pmb{p}})=\\mathbb{E}_{P_{M}\\otimes P_{\\pmb{M}}}[\\langle\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M}),\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\tilde{\\pmb{p}}_{t}^{M})\\rangle_{\\mathrm{HS}}]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{P_{M}\\otimes P_{\\pmb{M}}}[\\mathrm{tr}(\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M})\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\tilde{\\pmb{p}}_{t}^{M})^{*})]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{P_{M}\\otimes P_{\\pmb{M}}}[\\mathrm{tr}(\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M})\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\tilde{\\pmb{p}}_{t}^{M})^{-1})]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{P_{M}\\otimes P_{\\pmb{M}}}[\\mathrm{tr}(\\mathcal{U}_{\\mathcal{M}}(p^{M}\\star(\\tilde{\\pmb{p}}^{M})^{-1}))],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used the multiplicative property of the unitary features for $\\mathbb{C}^{n\\times n}$ \u2013valued paths $\\pmb{p}^{M}$ and $\\tilde{\\pmb{p}}^{M}$ , see also [19, Lemma A.5]. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Now we will construct a metric from HRPCFD which can characterise the extended weak convergence on precompact subset of FP. ", "page_idx": 25}, {"type": "text", "text": "Lemma A.12. Suppose that $\\{(P_{M_{n}},P_{\\pmb{M_{m}}})\\in\\mathcal P(\\mathcal L(\\mathbb R^{d},\\mathfrak u(n)))\\times\\mathcal P(\\mathcal L(\\mathbb C^{n\\times n},\\mathfrak u(m))):n\\in\\mathbb N,m\\in\\mathbb N^{d}\\}$ $\\mathbb{N}\\}$ is a double sequence of distributions with full supports. After a re-numeration we label them as $a$ sequence $((P_{M_{j}},\\bar{P}_{M_{j}}))_{j\\in\\mathbb{N}}$ such that each $(M_{j},\\mathcal{M}_{j})$ is a random admissibe pair in $A_{u n i t a r y}$ . Then the following defines a metric on $\\mathcal{P}(\\hat{\\mathcal{X}})$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\widetilde{H R P C F D}}(\\mu,\\nu)=\\sum_{j=1}^{\\infty}\\frac{\\operatorname*{min}\\{1,H R P C F D_{M_{j},\\}_{\\neq}(\\mu,\\nu)\\}}{2^{j}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The symmetry and the triangle inequality are easy to check. We only need to show that $\\mathrm{H}\\bar{\\mathrm{RPCFD}}(\\mu,\\nu)=0$ if and only $\\mu=\\nu$ . The \u201cif\u201d part is trivial. Now suppose that $\\widetilde{\\mathrm{HRPCFD}}(\\mu,\\nu)=0$ holds but $\\mu\\neq\\nu$ . Then by Theorem A.9 we know that there exists a pair of integers $(n,m)\\in\\mathbb{N}^{2}$ such that for any $P_{M}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n)))$ and any $P_{\\pmb{\\mathscr{M}}}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)))$ with full supports, it holds that H $\\mathrm{RPCFD}_{M,\\mathcal{M}}(\\mu,\\nu)\\;>\\;0$ . So, let us pick some $j~\\in~\\mathbb{N}$ such that $(M_{j},\\bar{\\pmb{M}}_{j})\\in$ $\\mathcal{P}(\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n)))\\times\\mathcal{P}(\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)))$ , we must have $\\mathrm{HRPCFD}_{M_{j},{\\bf\\cal M}_{j}}(\\mu,\\nu)>0$ , which implies that $\\begin{array}{r}{\\widetilde{\\mathrm{\\tiny~JRPCFD}}(\\mu,\\nu)\\,\\geq\\,\\frac{\\operatorname*{min}\\{1,\\mathrm{HRPCFD}_{M_{j}},\\mathcal{M}_{j}(\\mu,\\nu)\\}}{2^{j}}\\,>\\,0}\\end{array}$ , a contradiction. Hence we obtain that the so\u2013defined $\\mathrm{HRPCFD}(\\mu,\\nu)$ is really a metric. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Theorem A.13. Fix a sequence of random admissible pairs $(M_{j},\\pmb{\\mathscr{M}}_{j})_{j\\in\\mathbb{N}}\\subset\\mathcal{A}_{u n i t a r y}$ such that for every $(n,m)\\in\\mathbb{N}^{2}$ there exists a $j\\in\\mathbb N$ with $M_{j}\\in\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n))$ and $\\mathcal{M}_{j}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ and their distributions $P_{M_{j}}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{R}^{d},\\mathfrak{u}(n)))$ and $P_{\\mathbf{\\mathcal{M}}_{j}}\\in\\mathcal{P}(\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m)))$ are fully supported. Let HRP CFD be the metric defined as in Lemma A.12 via this sequence $(M_{j},\\pmb{\\mathscr{M}}_{j})_{j\\in\\mathbb{N}}$ . ", "page_idx": 26}, {"type": "text", "text": "1. Let $\\ K\\subset F P$ be a compact subset in the space $F P$ of filtered processes equipped with the topology induced by extended weak convergence. Then, for every sequence of filtered processes $(\\stackrel{\\cdot}{\\mathbb{X}}^{k}\\,=\\,(\\Omega^{k},\\stackrel{\\cdot}{\\mathcal{F}}^{k},\\mathbb{F}^{k},X^{k},\\mathbb{P}^{k}))_{k\\in\\mathbb{N}}\\\\\\stackrel{\\smile}{\\subset}\\mathcal{K}$ and $\\mathbb{X}\\,=\\,(\\Omega,\\mathcal{F},\\mathbb{F},X,\\mathbb{P})\\,\\in\\,F P$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{X}^{k}\\xrightarrow{E W}\\mathbb{X}\\iff H\\widetilde{R P C F D}(\\mathbb{X}^{k},\\mathbb{X})\\rightarrow0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as $k\\rightarrow\\infty$ . ", "page_idx": 26}, {"type": "text", "text": "2. Let $K\\_{\\Sigma\\_{\\Lambda}}$ be a compact subset. Let $F P(K)$ be the space of all filtered processes taking values in $K$ . Then, for every sequence of filtered processes $\\begin{array}{r l}{(\\mathbb{X}^{k}}&{{}=}\\end{array}$ $(\\Omega^{k},\\mathcal{F}^{k},\\mathbb{F}^{k},\\boldsymbol{X}^{k},\\mathbb{P}^{k}))_{k\\in\\mathbb{N}}\\subset F P(K)$ and $\\mathbb{X}=(\\Omega,\\mathcal{F},\\mathbb{F},X,\\mathbb{P}))\\in F P(K),$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{X}^{k}\\xrightarrow{E W}\\mathbb{X}\\iff H\\widetilde{R P C F D}(\\mathbb{X}^{k},\\mathbb{X})\\to0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as $k\\rightarrow\\infty.$ ", "page_idx": 26}, {"type": "text", "text": "Proof. 1. First suppose that $\\mathbb{X}^{k}\\xrightarrow{E W}\\mathbb{X}$ for a sequence $(\\mathbb{X}^{k})_{k\\in\\mathbb{N}}\\subset K$ and $\\mathbb{X}\\in\\mathcal{K}$ . Clearly, for any sequence of piecewise linear measure-valued paths $\\boldsymbol{p}^{k}$ , $k\\in\\mathbb{N}$ and $\\pmb{p}$ (which are linear on each subinterval $[i,i+1],\\,i\\,=\\,0,\\dots,T-1)$ we have $p^{k}\\rightarrow p$ with respect to the product topology on $\\hat{\\mathcal X}$ implies that for each fixed unitary representation $M\\ \\in$ $\\mathcal{L}(\\mathbb{R}^{d},\\dot{\\mathfrak{u}}(n))$ , the $\\mathbb{C}^{n\\times n}$ \u2013valued paths $({\\pmb p}^{k})^{M}\\;=\\;(\\int\\mathcal{U}_{M}({\\pmb x}){\\pmb p}_{t}^{k}(d{\\pmb\\dot{x}}))_{t\\in[0,T]}^{~~~}$ converges to $\\begin{array}{r}{\\pmb{p}^{M}=(\\int\\mathcal{U}_{M}(\\pmb{x})\\pmb{p}_{t}(d\\pmb{x}))_{t\\in[0,T]}}\\end{array}$ with respect to the total variation norm as $k\\rightarrow\\infty$ . Then, for every fixed unitary representation $\\mathcal{M}\\in\\mathcal{L}(\\mathbb{C}^{n\\times n},\\mathfrak{u}(m))$ , by the continuity of unitary feature map $\\mathcal{U}_{\\mathcal{M}}$ relative to the total variation norm (see e.g. [19, Proposition B.6]), we have $\\mathcal{U}_{\\mathcal{M}}(\\boldsymbol{\\dot{t}}\\mapsto(\\mathbf{\\dot{p}}^{k})_{t}^{M})\\rightarrow\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\mathbf{p}_{t}^{M})$ in $\\mathbb{C}^{m\\times m}$ (relative to the Hilbert\u2013Schmidt norm) as $k\\rightarrow\\infty$ . Hence, we actually have shown that the function $p\\in\\hat{\\mathcal{X}}\\mapsto\\mathcal{U}_{\\mathcal{M}}(t\\mapsto\\pmb{p}_{t}^{M})\\in$ $\\mathbb{C}^{m\\times m}$ is continuous and bounded for the product topology on $\\hat{\\mathcal X}$ . Now, as $\\mathbb{X}^{k}\\xrightarrow{E W}\\mathbb{X}$ means that $P_{\\hat{X}^{k}}\\to P_{\\hat{X}}$ weakly in $\\mathcal{P}(\\hat{\\mathcal{X}})$ as $k\\rightarrow\\infty$ , we indeed have for all $(M,{\\mathcal{M}})\\in$ $\\ensuremath{\\mathcal{A}_{\\mathrm{unitary}}}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\int\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})P_{\\hat{X}^{k}}(d p)=\\int\\mathcal{U}_{\\mathcal{M}}(t\\mapsto p_{t}^{M})P_{\\hat{X}}(d p),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "that is, $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}\\|\\Phi_{\\mathbb{X}^{k}}^{2}(M,\\mathcal{M})-\\Phi_{\\mathbb{X}}^{2}(M,\\mathcal{M})\\|_{\\mathrm{HS}}\\,=\\,0}\\end{array}$ . This observation together with the boundedness of the HRPCFD (see Lemma A.10), allows us to apply the dominated convergence theorem to derive that for every $j\\in\\mathbb N$ one has ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{HRPCFD}_{M_{j},\\M_{j}}^{2}(\\mathbb{X}^{k},\\mathbb{X})}}\\\\ &{=\\displaystyle\\int\\int d_{\\mathrm{HS}}^{2}(\\Phi_{\\mathbb{X}^{k}}^{2}(M,\\mathcal{M}),\\Phi_{\\mathbb{X}}^{2}(M,\\mathcal{M}))P_{M_{j}}(d M)P_{\\pmb{M}_{j}}(d\\mathcal{M})\\rightarrow0}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as $k\\rightarrow\\infty$ . Consequently, we can conclude that $\\widetilde{\\mathrm{HRPCFD}}(\\mathbb{X}^{k},\\mathbb{X})\\rightarrow0$ as $k\\rightarrow\\infty$ . Conversely, suppose that $(\\mathbb{X}^{k})_{k\\in\\mathbb{N}}$ is a sequence of flitered processes in $\\kappa$ and $\\mathbb{X}\\in\\operatorname{FP}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathrm{H}\\widetilde{\\mathrm{RPCFD}}(\\mathbb{X}^{k},\\mathbb{X})=0}\\end{array}$ . Since $\\kappa$ is compact, there is a subsequence of $(\\mathbb{X}^{k})_{k\\in\\mathbb{N}}$ (without loss of generality, assume this subsequence is the sequence itself) converging to a limit $\\mathbb{Y}\\,=\\,(\\Omega^{\\breve{Y}},\\mathcal{G},\\mathbb{G},\\breve{Y},\\mathbb{Q})\\,\\in\\,\\mathcal{K}$ in the extended weak topology. From the previous argument we know that $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\widetilde{\\mathrm{HRPCFD}}(\\mathbb{X}^{k},\\mathbb{Y})\\,=\\,0}\\end{array}$ . Therefore, we actually obtain that $\\mathrm{HRPCFD}(\\mathbb{X},\\mathbb{Y})\\,=\\,0.$ . In view of Theorem A.9, the equality $\\widehat{\\mathrm{HRPCFD}}(\\mathbb{X},\\mathbb{Y})\\,=\\,0$ means that $P_{\\hat{X}}=P_{\\hat{Y}}$ , i.e., $\\mathbb{X}$ and $\\mathbb{Y}$ are synonymous. The above reasoning reveals that any accumulation point $\\mathbb{Y}$ of the sequence $(\\mathbb{X}^{k})_{k\\in\\mathbb{N}}$ in the extended weak convergence coincides with $\\mathbb{X}$ . As a consequence, we have $\\mathbb{X}^{k}\\to\\mathbb{X}$ in the extended weak topology as $k\\rightarrow\\infty$ . ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "2. By [4, Theorem 1.7] the subspace $\\mathrm{FP}(K)$ is precompact in FP for the extended weak topology, if $K\\subset\\mathcal{X}$ is compact. Hence the claim follows immediately from the result contained in the statement 1 with $K={\\overline{{\\operatorname{FP}(K)}}}$ (the closure of $\\mathrm{FP}(K)$ with respect to the extended weak convergence). ", "page_idx": 27}, {"type": "text", "text": "B Methodology and algorithm ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "B.1 Estimating the conditional probability measure ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Algorithm 1 Training algorithm seq-to-seq regression model   \nInput: $\\mathbf{\\deltaX}$ - real data; $B$ - batch size; $\\eta_{r}$ - learning rate for the regression module; $d$ - path feature dimension; $l$ - lie degree; $M\\in\\mathbb{R}^{d\\times\\dim\\mathfrak{u}_{l}}$ ; $T$ - path length; $\\eta_{r}$ - learning rate.   \n1: $F_{\\theta}^{X}\\gets$ initialize   \n2: for $i\\in(1,\\ldots,\\mathrm{iter}_{r})$ do   \n3: Sample $\\textbf{\\em x}$ from $\\mathbf{\\deltaX}$ of size $B$   \n4: $\\mathcal{U}_{j,M}(t)\\gets\\mathcal{U}_{M}({\\pmb x}_{j,[t,T]})$ with $t\\in\\{0,\\ldots,T\\},j\\in\\{1,\\ldots,B\\}$   \n5: $\\begin{array}{r}{\\mathrm{RLoss}(\\theta;{\\pmb x},M)\\gets\\frac{1}{B(T+1)}\\sum_{t=0}^{T}\\sum_{j=1}^{B}||F_{\\theta}^{X}({\\pmb x}_{j,[0,T]})_{t}-\\mathcal{U}_{j,M}(t)||_{H S}^{2}}\\end{array}$   \n6: $\\theta\\leftarrow\\theta-\\eta_{r}\\cdot\\nabla_{\\theta}(\\mathrm{RLoss}(\\theta;\\mathbf{x},M))$   \n7: end for   \n8: return $F_{\\theta^{*}}^{X}$ $\\triangleright$ Return the optimal model ", "page_idx": 27}, {"type": "text", "text": "Algorithm 2 Sampling algorithm to approximate $\\Phi_{\\mathbb{X}}^{2}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Input: $F_{\\theta}^{X}$ - regression module; $\\pmb{X}=(\\pmb{x}_{j})_{j=1}^{N}$ - data sampled from distribution $P_{X}$ ; $\\eta_{r}$ - learning rate for the regression module; $d$ - path feature dimension; $n,\\ m$ - Lie degrees; $M\\in\\mathbb{R}^{d\\times\\dim\\mathfrak{u}_{l}}$ $\\mathcal{M}\\in\\mathbb{R}^{\\dim\\mathfrak{u}_{(n)}\\times\\dim\\mathfrak{u}_{(m)}}$ ; $T$ - path length. ", "page_idx": 27}, {"type": "text", "text": "1: $\\hat{p}^{\\hat{X},M}\\gets$ zero matrix of length $N\\times(T+1)$   \n2: for $t\\in(0,\\ldots,T)$ do   \n3: for $j\\in(1,\\dots,N)$ do   \n4: $\\mathcal{U}_{j,M,\\mathrm{past}}(t)\\gets\\mathcal{U}_{M}(\\pmb{x}_{j,[0,t]})$   \n5: $\\hat{\\mathcal{U}}_{j,M,\\mathrm{future}}(t)\\gets F_{\\theta}^{X}(\\pmb{x}_{j,[0,T]})_{t}$   \n6: $\\hat{\\pmb{p}}_{j,t}^{\\hat{X},M}\\gets\\mathcal{U}_{j,M,\\mathrm{past}}(t)*\\hat{\\mathcal{U}}_{j,M,\\mathrm{future}}(t)$   \n7: end for   \n8: end for   \n9: $\\begin{array}{r}{\\hat{\\pmb{\\Phi}}_{\\mathbb{X}}^{2}(M,\\mathcal{M})\\gets\\frac{1}{N}\\sum_{j=1}^{N}\\mathcal{U}_{\\mathcal{M}}(\\hat{\\pmb{p}}_{j}^{\\hat{X},M})}\\end{array}$   \n10: return $\\hat{\\Phi}_{\\mathbb{X}}^{2}(M,\\mathcal{M})$ ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "B.2 HRPCF-GAN ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide the mathematical formulation of HRPCF-GAN for conditional time series generation. Let $\\bar{X^{\\backslash}}:=(X_{t})_{t=1}^{T}$ denote a $\\mathbb{R}^{d}$ -valued time series of length $T$ with its distribution $P_{X}$ . Suppose that we have i.i.d. samples $\\mathbf{X}=(x_{i})_{i}$ from $P_{X}$ . We are interested in generating synthetic ", "page_idx": 27}, {"type": "text", "text": "future paths to approximate the conditional distribution of the future path $X_{\\mathrm{future}}:=X_{(p,T]}$ given the past path $X_{\\mathrm{future}}:=X_{[0,p]}$ . For ease of notations, let $\\mathcal{X}_{\\mathrm{past}}:=\\mathbb{R}^{d\\times p}$ and $\\chi_{\\mathrm{future}}=\\mathbb{R}^{d\\times(T-p)}$ denote the space of the past path and future path, respectively. ", "page_idx": 28}, {"type": "text", "text": "Conditional generator One step generator $g_{\\theta}:\\mathcal{X}_{\\mathrm{past}}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}^{d}$ , which maps $(x_{\\mathrm{past}},z_{t})$ to samples of the next time step via the following formula: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{h=[F_{\\theta_{e}}(x_{\\mathrm{past}})]_{p}}\\\\ {o=F_{\\theta_{a}}(h,z)}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$F_{\\theta_{e}}:\\mathcal{X}_{\\mathrm{past}}\\to\\mathcal{H}$ is the sequence-to-sequence embedding module to extract the key information of the path up to time $t$ and $F_{\\theta_{a}}$ exhibits the autoregressive generator architecture. We denote by where $\\theta=\\left(\\theta_{e},\\theta_{a}\\right)$ the generator\u2019s parameter. ", "page_idx": 28}, {"type": "text", "text": "We then apply one step generator $g_{\\theta}$ in a rolling window basis to generate future time series of length $T-p$ . More specifically, $G_{\\theta}:(\\mathring{x_{[0:p]}},(z_{t})_{t=p+1}^{T})\\mapsto(o_{t})_{t=p+1}^{T}$ , where we first set $o_{0:p}=x_{0:p}$ and for every $t\\geq p$ , $o_{t+1}=g_{\\theta}\\big(o_{t-p:t},z_{t}\\big)$ . ", "page_idx": 28}, {"type": "text", "text": "In the following, we summarise the training algorithm for HRPCF-GAN in Algorithm 3. ", "page_idx": 28}, {"type": "text", "text": "Algorithm 3 Training algorithm for HRPCF-GAN ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Input: $p$ - past path length; $T$ - total path length; $d$ - path feature dimension; $\\mathbf{\\deltaX}$ - real data; $n$ - lie   \ndegree for EPCFD; $K_{1}$ - number of linear maps; $\\dot{M}\\in\\mathring{\\mathbb{R}}^{K_{1}\\times d\\times\\dim\\mathfrak{u}_{(n)}}$ ; $m$ - lie degree for EHRPCFD;   \n$K_{2}$ - number of linear maps for EHRPCFD; $\\mathcal{M}\\in\\mathbb{R}^{K_{2}\\times\\dim\\mathfrak{u}_{(n)}\\times\\dim\\mathfrak{u}_{(m)}}$ ; $G_{\\theta}$ - generator; $B$ - batch   \nsize; $z$ - noise dimension; $\\mathrm{iter}_{r}$ frequency of regression module fine-tuning; $\\eta_{g},\\eta_{d}$ - generator and   \ndiscriminator learning rates.   \n1: # Vanilla PCFGAN training   \n2: while $\\theta,M$ not converge do   \n3: Sample $z\\sim\\mathcal{N}^{z\\times(T-q)}(0,1)$ of size $B$ , sample $\\textbf{\\em x}$ from $\\mathbf{\\deltaX}$ of size $B$   \n4: $\\tilde{\\mathbf{x}}_{(p,T]}\\leftarrow G_{\\theta}(\\mathbf{x}_{[0,p]},z)$   \n5: $\\mathrm{Loss}(\\theta,M;x,z)\\gets\\mathrm{EPCFD}_{M}^{2}(x_{[0,T]},(x_{[0,p]},\\tilde{\\mathbf{x}}_{(p,T]}))$   \n6: $M\\gets M-\\eta_{d}\\cdot\\nabla_{M}(-\\mathrm{Loss}(\\theta,\\mathcal{M};\\mathbf{x},z))$ \u25b7Maximize the loss   \n7: $\\theta\\gets\\theta-\\eta_{g}\\cdot\\nabla_{\\theta}(\\mathrm{Loss}(\\theta,\\mathcal{M};\\mathbf{x},z))$ $\\triangleright$ Minimize the loss   \n8: end while   \n9: # Regression training for real measure   \n10: for $i\\in(1,\\ldots,K_{1})$ do   \n11: $F_{\\iota_{i}}^{\\mathrm{real}}\\gets$ initialize   \n12: Triain $F_{\\iota_{i}}^{\\mathrm{real}}$ using $\\mathbf{\\deltaX}$ as described in Algorithm 1   \n13: $F_{\\eta_{i}}^{\\mathrm{fake}}\\gets F_{\\iota_{i}}^{\\mathrm{res}}$ al \u25b7Set as the initialization   \n14: end for   \n15: # High-Rank PCF-GAN training   \n16: while $\\theta,\\mathcal{M}$ not converge do   \n17: Sample $z\\sim\\mathcal{N}^{z\\times(\\bar{T}-q)}(0,1)$ of size $B$ , sample $\\textbf{\\em x}$ from $\\mathbf{\\deltaX}$ of size $B$   \n18: $\\tilde{\\mathbf{x}}_{(p,T]}\\leftarrow G_{\\theta}(\\mathbf{x}_{[0,p]},z)$   \n19: for $i\\in\\left(1,\\ldots,K_{1}\\right)\\mathbf{d}$ o   \n20: for $t\\in(0,\\ldots,T)$ do   \n21: $\\begin{array}{r l}&{\\hat{\\pmb{p}}_{i,t}^{\\mathrm{real},M_{i}}\\leftarrow\\mathcal{U}_{M_{i}}(\\pmb{x}_{[0,t]})*F_{\\iota_{i}}^{\\mathrm{real}}(\\pmb{x}_{[0,T]})}\\\\ &{\\hat{\\pmb{p}}_{i,t}^{\\mathrm{fake},M_{i}}\\leftarrow\\mathcal{U}_{M_{i}}(\\pmb{x}_{[0,t]})*F_{\\iota_{i}}^{\\mathrm{fake}}((\\pmb{x}_{[0,p]},\\tilde{\\pmb{x}}_{(p,T]}))}\\end{array}$   \n22:   \n23: end for   \n24: end for   \n25: $\\begin{array}{r}{\\operatorname{Loss}(\\theta,\\mathcal{M};\\pmb{x},z,M)\\gets\\operatorname{EHRPCFD}_{M,\\mathcal{M}}^{2}(\\pmb{x}_{[0,T]},(\\pmb{x}_{[0,p]},\\tilde{\\pmb{x}}_{(p,T]}))}\\end{array}$ \u25b7Use Algorithm 2 and   \nEquation (5) with prie,tal, ${p}_{i,t}^{\\mathrm{real},M_{i}}$ and pfia,tk $p_{i,t}^{\\mathrm{fake},M_{i}}$   \n26: $\\mathcal{M}\\gets\\mathcal{M}-\\eta_{d}\\cdot\\nabla_{\\mathcal{M}}(-\\mathrm{Loss}(\\theta,\\mathcal{M};x,z,M))$ \u25b7Maximize the loss   \n27: $\\theta\\leftarrow\\theta-\\eta_{g}\\cdot\\nabla_{\\theta}(\\mathrm{Loss}(\\theta,\\mathcal{M};\\mathbf{x},z,M))$   \n28: Do the following every iterr iterations:   \n29: $\\tilde{X}\\gets(x_{[0,p]},\\bar{G_{\\theta}}(X_{[0,p]},z))$   \n30: Train $F_{\\eta_{i}}^{\\mathrm{fake}}$ using $\\tilde{X}$ as described in Algorithm 1 for every $i\\in(1,\\ldots,K_{1})$   \n31: end while ", "page_idx": 28}, {"type": "text", "text": "We provide the following two algorithms for training ERHPCFD for the permutation test and computing the test power/Type 1 error of the permutation test, respectively. ", "page_idx": 29}, {"type": "text", "text": "Algorithm 4 Training algorithm for the permutation test   \nInput: X - samples from distribution $\\mu$ ; Y - samples from distribution $\\nu$ ; $m\\,>\\,0$ - sample size of $\\mathbf{X}$ ; $n\\,>\\,0$ - sample size of $\\mathbf{Y}$ ; $n$ - lie degree for EPCFD; $K_{1}$ - number of linear maps; $M\\in$ $\\mathbb{R}^{K_{1}\\times d\\times\\dim\\mathfrak{u}_{(n)}}$ ; $m$ - lie degree for EHRPCFD; $K_{2}$ - number of linear maps for EHRPCFD; $\\mathcal{M}\\in$ $\\mathbb{R}^{K_{2}\\times\\dim\\mathfrak{u}_{(n)}\\times\\dim\\mathfrak{u}_{(m)}};B$ - batch size; $\\eta$ - learning rate; iter $\\mathrm{\\dot{\\Delta}_{1}}$ , $\\mathrm{iter_{2}}$ - number of iterations. 1: # Vanilla PCFD optimization   \n2: for iter $\\in(1,\\ldots,\\operatorname{iter}_{1})$ do   \n3: sample $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}$ from $X,Y$ of size $B$   \n4: $\\operatorname{Loss}(M;\\pmb{x},\\pmb{y})\\gets\\operatorname{EPCFD}_{M}^{2}(\\pmb{x}_{[0,T]},\\pmb{y}_{[0,T]})$   \n5: $M\\gets M-\\eta\\cdot\\nabla_{M}(-\\mathrm{Loss}(M;x,y))$ \u25b7Maximize the loss 6: end for   \n7: # Regression training for real measure   \n8: for $i\\in(1,\\ldots,K_{1})$ do   \n190:: $F_{\\iota_{i}}^{\\mathrm{X}}$ ,i $\\smash{\\boldsymbol{F}_{i_{i}}^{\\mathrm{Y}}\\leftarrow}$ isniintiga eand as described in Algorithm 1   \n$F_{\\iota_{i}}^{\\mathrm{X}}$ $\\mathbf{\\deltaX}$ $M_{i}$   \n11: Train $F_{\\iota_{i}}^{\\mathrm{Y}}$ using $\\mathbf{Y}$ and $M_{i}$ as described in Algorithm 1   \n12: end for   \n13: # High Rank PCFD optimization   \n14: for iter $\\in(1,\\ldots,\\mathrm{iter_{2}})$ do   \n15: sample $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}$ from $X,Y$ of size $B$   \n16: for $i\\in(1,\\ldots,K_{1})$ do   \n17: for $t\\in(0,\\ldots,T)$ do   \n18: p\u02c6iX,,tMi\u2190UMi(x[0,t]) \u2217F \u03b9Xi(x[0,T ])   \n19: p\u02c6iY,,tMi\u2190UMi(y[0,t]) \u2217F \u03b9Yi(y[0,T ])   \n20: end for   \n21: end for   \n22: $\\mathrm{Loss}(\\mathcal{M};x,y,M)\\gets\\mathrm{EHRPCFD}_{M,\\mathcal{M}}^{2}(x_{[0,T]},y_{[0,T]})\\triangleright\\mathrm{U},$ se Algorithm 2 and Equation (5) with piX,,tM and $p_{i,t}^{\\mathrm{Y},M_{i}}$   \n23: $\\mathcal{M}\\gets\\mathcal{M}-\\eta_{d}\\cdot\\nabla_{\\mathcal{M}}(-\\mathrm{Loss}(\\theta,\\mathcal{M};\\mathbf{x},\\mathbf{y},M))$ \u25b7Maximize the loss 24: end for   \n25: return $M,M$ \u25b7Return learnt parameters ", "page_idx": 29}, {"type": "text", "text": "Algorithm 5 Estimating the test power/Type-I error of the permutation test ", "page_idx": 30}, {"type": "text", "text": "Input: $\\alpha\\in(0,1)$ - significance level; $N>0$ - # of experiments; $M>0$ - # of permutations; X -   \nsamples from distribution $\\mu$ ; $\\mathbf{Y}$ - samples from distribution $\\nu$ ; $m>0$ - sample size of $\\mathbf{X}$ ; $n>0$ -   \nsample size of $\\mathbf{Y}$ ; $T$ - test statistic function; $H_{0}\\in\\{1,0\\}$ - whether the null hypothesis is true or false   \n$H_{0}=1$ if $\\mu=\\nu$ ; otherwise $H_{0}=0$ )   \n1: $\\mathbf{Z}\\leftarrow\\mathbf{C}\\mathrm{c}$ ncatenate $(\\mathbf{X},\\mathbf{Y})$   \n2: num_rejections $\\gets0$   \n3: $i\\gets1$   \n4: while $i\\leq N$ do   \n5: $\\tau\\leftarrow$ EmptyList   \n6: $j\\leftarrow1$   \n7: 8: while $\\begin{array}{r l}&{\\operatorname{\\\\sfmue}_{\\mathcal{I}}\\le M\\operatorname{\\bf\\bf~a}\\!0}\\\\ &{\\quad\\sigma\\sim\\!\\operatorname{Permutation}(\\{1,2,\\cdots,m+n\\})}\\\\ &{\\quad T_{\\sigma}\\leftarrow-T(\\{\\mathbf Z_{\\sigma(1)},\\mathbf Z_{\\sigma(2)},\\cdots,\\mathbf Z_{\\sigma(m)}\\},\\{\\mathbf Z_{\\sigma(m+1)},\\cdots,\\mathbf Z_{\\sigma(m+n)}\\})}\\\\ &{\\quad T.\\mathrm{append}(T_{\\sigma})}\\\\ &{\\quad j\\leftarrow j+1}\\end{array}$   \n9:   \n10:   \n11:   \n12: end while   \n13: if $T(\\mathbf{X},\\mathbf{Y})>(1-\\alpha)\\%$ quantile of $\\tau$ then   \n14: num_rejections $\\leftarrow$ num_rejections $+\\,1$   \n15: end if   \n16: $i\\gets i+1$   \n17: end while   \n18: ratio $\\leftarrow$ num_rejections $/\\ N$   \n19: if $H_{0}$ then   \n20: Type_I_error $\\leftarrow$ ratio   \n21: return Type_I_error   \n22: else   \n23: test_power $\\leftarrow$ ratio   \n24: return test_power   \n25: end if ", "page_idx": 30}, {"type": "text", "text": "C Numerical results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Code The code is written in Python 3.10.8 and Pytorch 1.11.0. The supplementary code is available at https://github.com/DeepIntoStreams/High-Rank-PCF-GAN.git for ensuring full reproducibility. The experiments were performed on a computational system running Ubuntu 22.04.2 LTS, comprising five Quadro RTX 8000 GPUs with 48GB of memory each. The experiments are run on single GPU and the training time ranges from 30 minutes to 4 hours. ", "page_idx": 30}, {"type": "text", "text": "C.1 Hypothesis testing ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Description The permutation test is a statistical method used to decide whether two measures $\\mu,\\nu$ are the same. The null hypothesis states $H_{0}:\\mu=\\nu$ whereas the alternative hypothesis $H_{1}:\\mu\\neq\\nu$ . Given a test metric $T$ and sample data $X\\,=\\,\\{x_{1},\\ldots,x_{n}\\}$ , $\\pmb{Y}\\,=\\,\\{{\\pmb y}_{1},\\dots\\}_{\\pmb{y}_{m}}\\}$ from $\\mu$ and $\\nu$ respectively. We construct the following distribution ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}:=\\Big\\{T(\\mathbf{Z}_{\\sigma(1):\\sigma(n)},\\mathbf{Z}_{\\sigma(m+1):\\sigma(n+m)})\\mid\\sigma\\in\\Sigma_{n+m}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathbf{Z}=(X,Y)$ and $\\Sigma_{n+m}$ is the permutation group of $n+m$ elements. Given the significance level $\\alpha$ , we reject the null hypothesis if $T(X,Y)\\stackrel{*}{>}(1\\stackrel{*}{-}\\alpha)\\%$ quantile of $\\tau$ . ", "page_idx": 30}, {"type": "text", "text": "Methodology For each $H$ , we sample the training dataset $\\mathcal{D}_{\\mathrm{train}}\\,=\\,\\left(B_{\\mathrm{train}},B_{\\mathrm{train}}^{H}\\right)$ and optimize the set $(M_{K_{1}},\\mathcal{M}_{K_{2}})$ to maximize EHRPCFD2 between the pair of measures, a detailed procedure can be found in Algorithm 4. Then, we sample two independent sets $\\mathcal{D}_{\\mathrm{test}}^{H_{0}}=(B_{\\mathrm{test}}^{H},\\tilde{B}_{\\mathrm{test}}^{H})$ , $\\mathcal{D}_{\\mathrm{test}}^{H_{1}}=$ $(B_{\\mathrm{test}},B_{\\mathrm{test}}^{H})$ and calculate the power and type-I error accordingly. We refer to Algorithm 5 for the computation of test metrics. ", "page_idx": 30}, {"type": "image", "img_path": "w28i9oe9Xr/tmp/03dd6bb9bf942efdd2a2c863ee491fe99f8591db56b9d82596e47a8131ea6f9f.jpg", "img_caption": ["Figure 5: Distributions of EPCFD (left) and EHRPCFD (right) under $H_{0}$ and $H_{1}$ with Hurst parameter $H=0.475$ . The distribution consists of 100 runs under both hypotheses. For EPCFD, fix $K_{1}=8$ and $n=5$ . For High Rank PCFD fix $K_{1}=1$ , $n=3$ , $K_{2}=10$ , $m=13$ . "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Implementation details We provide the full details of the implementation of the numerical experiment in Section 5.1. Adopting the notation in Algorithm 4, we fix $n=3$ , $m=13$ , $K_{1}=1$ and $K_{2}=10$ , these values are chosen via hyper-parameter fine-tuning. The regression model consists of a 2-layer LSTM module. The model\u2019s parameter is optimized using Adam optimizer with learning rates 0.001 (for regression) and 0.02 (for EHRPCFD). ", "page_idx": 31}, {"type": "table", "img_path": "w28i9oe9Xr/tmp/85369496c67451b913cec321e07774cc6e9fb4bcd1f69ac5080bd504e92e8aeb.jpg", "table_caption": ["Additional numerical results We provide comprehensive tables for summarising the Type-I error and the computational time involved in Section 5.1. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "w28i9oe9Xr/tmp/368d608cf3e2c6287d1bb0641fe0cc2ae0ed047d7ad56d8fbee281acfd729f7a.jpg", "table_caption": ["Table 3: Type-I error of the distances when $H\\neq0.5$ in the form of mean $\\pm$ std over 5 runs. For PCFD, fix $K_{1}=8$ and $n=5$ . For High Rank PCFD fix $K_{1}=1$ , $n=3$ , $K_{2}=10$ , $m=13$ . For the RBF signature MMD and classical RBF MMD, fix $2\\sigma^{2}=0.1$ . For High Rank signature MMD, fix $\\sigma_{1}=\\sigma_{2}=1$ . "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Table 4: Inference time of the permutation test across different sample sizes ${\\left[{m=n}\\right]}$ ) and the training time of High Rank PCFD and PCFD before conducting the permutation test. The result is in the form of mean $\\pm$ std over 5 runs. For PCFD, fix $K_{1}=8$ and $n=5$ . For High Rank PCFD fix $K_{1}=1$ , $n=3$ , $K_{2}=10$ , $m=13$ . For the RBF signature MMD and classical RBF MMD, fix $2\\sigma^{2}=0.1$ . Here fix $h=0.45$ . ", "page_idx": 31}, {"type": "text", "text": "C.2 Generative modeling ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Datasets construction (1) 3-dimensional fractional Brownian motion: we simulate samples using the publicly available Python package fbm. The total length of each sample is 11 (counting a fixed initial point). The training and test data consists of two independent sampled sets of size 10000. (2) Stock: we select 5 representative stocks in the U.S. market, namely, Apple, Lockheed Martin, J.P. Morgan, Amazon, and P& G, and collect the daily return data from 2010 to 2020. The data collection is done using the Python package yfinance. We then construct the dataset using a rolling-window basis with length 10 (two weeks in real time) and stride 2. Finally, we split the dataset into training and test sets with a ratio of 0.8. ", "page_idx": 32}, {"type": "text", "text": "Baseline We compare the performance of HRPCF-GAN with well-known models for time-series generation such as RCGAN [10] and TimeGAN [23]. Furthermore, we use PCFGAN [19] as a benchmarking model to showcase the significant improvement by considering the higher rank development as the discriminator. For fairness, we use the same generator structure (LSTM-based) for all these models. ", "page_idx": 32}, {"type": "text", "text": "Conditional Generator The generator design is described in Appendix B.2. In particular, we choose $F_{\\theta_{e}}$ and $F_{\\theta_{\\alpha}}$ to be two independent 2-layer LSTM modules. The first module takes the past path and encodes the necessary information to the latent space. The final hidden and cell state will be used as the input for the second LSTM module and the latent noise vector to produce the output distribution of the next time step. Also, we use the auto-regressive to simulate the future path recursively. ", "page_idx": 32}, {"type": "text", "text": "Implementation details The training procedure is described in Algorithm 3, we adopt the same notation in this section. For both datasets, we set $T=10$ and $p=5$ . We use the development layers on the unitary matrix [18] to calculate the PCFD distance, in particular, we fix $K_{1}\\,=\\,5$ , $n\\,=\\,5$ , $K_{2}=10$ , $m=13$ for the discriminator design, these are obtained via hyper-parameter tuning. The regression model consists of a 2-layer LSTM module. Finally, we use the ADAM optimizer [12], to train both the generator and discriminator with learning rates 0.0001 and 0.002 respectively. We fine-tune the regression every 500 generator optimization iterations. To improve the training stability of GAN, we employed three techniques. Firstly, we applied a constant exponential decay rate of 0.97 to the learning rate for every 500 generator training iterations. Secondly, we clipped the norm of gradients in both generator and discriminator to 10. ", "page_idx": 32}, {"type": "text", "text": "All benchmarking models are trained with 15000 training iterations. For HRPCF-GAN, we trained the vanilla PCF-GAN with 10000 iterations then we switched to HRPCF discriminator and trained the model for a further 5000 iterations. ", "page_idx": 32}, {"type": "text", "text": "Evaluation metrics We list here the test metrics we used for generative model assessment. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Marginal score [16]: the average of Wasserstein distance of the marginal distribution between real and fake data across each dimension.   \n\u2022 Auto-correlation score [16]: the $l_{1}$ norm of the difference in the ACF between real and fake data ", "page_idx": 32}, {"type": "equation", "text": "$$\nA C F(X,Y):=\\sum_{\\tau=1}^{T}\\sum_{i=1}^{d}\\left\\|\\hat{C}(\\tau;X^{(i)})-\\hat{C}(\\tau;Y^{(i)})\\right\\|,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\hat{C}(\\tau;X)$ is the empirical auto-correlation estimator of $X_{t}$ and $X_{t+\\tau}$ ", "page_idx": 32}, {"type": "text", "text": "\u2022 Cross-correlation score [16]: the $l_{1}$ norm of the difference in the correlation between real and fake data across each feature dimension. ", "page_idx": 32}, {"type": "equation", "text": "$$\nC o r r(X,Y)=\\sum_{s,t=1}^{T}\\sum_{i,j=1}^{d}\\left\\|\\rho(X_{s}^{(i)},X_{t}^{(j)})-\\rho(Y_{s}^{(i)},Y_{t}^{(j)})\\right\\|,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\rho$ is the empirical correlation estimator. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Discriminative score [23]: we train a post-hoc classifier to distinguish real data from fake data. Lower the score (absolute difference between classification accuracy and 0.5), meaning inability to classify, indicate better performance of the generative model. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Predictive score [23]: we train a sequence-to-sequence model to predict the latter part of a time series given the first part, using generated data and real data, resp. The trained models are then tested on the real data resp. The lower loss (|TSTR-TRTR|) means the better resemblance of synthetic data to real data for the predictive task. \u2022 $\\mathrm{Sig}W_{1}$ score [16]: by embedding the time series to the signature space, we can approximate the $W_{1}$ distance by the $l_{2}$ norm of the signature of the real and fake data. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{Sig}_{W_{1}}(X,Y)=||\\mathbb{E}_{X}[\\operatorname{Sig}(X_{[0,T]})]-\\mathbb{E}_{Y}[\\operatorname{Sig}(Y_{[0,T]})]||_{l_{2}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where Sig denotes the signature transform of a path. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Conditional expectation score: we estimate the conditional expectation of the future path on the fake measure via Monte Carlo and compute the averaged pairwise $l_{2}$ norm between real data.   \n\u2022 Outgoing Nearest Neighbour Distance score [14]: the ONND calculates for each example of real data the distance between the nearest generated data. This score tests the model\u2019s capability to capture the diversity of the target distribution.   \n\u2022 American put option score: we use Least-Square Monte Carlo method [17] to price an at-the-money American put option using both real and generated data. We set the strike date $T=5$ days and risk-free rate $r=0.01$ . The score is computed as the average of $l_{1}$ differences of the estimated price across each stock. ", "page_idx": 33}, {"type": "table", "img_path": "w28i9oe9Xr/tmp/aed1c7655256375c543fd56517aa65f205c73f70cdd6a85a7b75fce4ce2fa942.jpg", "table_caption": ["For each test metric, a lower value indicates better model performance. We provide the results on the additional metrics in Table 5. "], "table_footnote": ["Table 5: Performance comparison of High Rank PCF-GAN and baselines. The best for each task is shown in bold. Each test metric is shown in the form of mean\u00b1std over 5 runs. "], "page_idx": 33}, {"type": "text", "text": "In all the numerical experiments of GAN training, we used a moderate matrix order $(l\\leq30)$ ) to achieve satisfactory results. Specifically, our experiments were conducted on a single GPU, with the training time for HRPCF-GAN ranging from 30 minutes to 4 hours. Although HRPCF-GAN takes longer to train compared to other baselines, the total training time is kept at a manageable level, while the HRPCF-GAN consistently delivers better performance. We summarize the computation time of each of the models over 100 training iterations in Table 6. ", "page_idx": 33}, {"type": "table", "img_path": "w28i9oe9Xr/tmp/5c6b17466469952127c7dfbae21c1ef2c2b356d6576a0a00cc483595d27ace07.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "w28i9oe9Xr/tmp/284f93cc3df292885ccecdd325aecbc514cd6346577559d8fdaea0d6c4075cb7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 6: Sample plots from all models on fractional Brownian Motion conditioned on the same past path. The thick red line indicates the conditional mean of future estimated by fake samples, whereas the shaded red area presents the region of $\\pm\\mathrm{std}$ . The thick green line corresponds to the theoretical value for the future expectation and the shaded area shown corresponds to the region of $\\pm$ theoretical std. ", "page_idx": 34}, {"type": "image", "img_path": "w28i9oe9Xr/tmp/cb17acdc52bb4d5d5a6bd9341130504368afee0466388b25e35b184edc46de59.jpg", "img_caption": ["Figure 7: Sample plots from all models on Stock dataset conditioned on the same past path. The thick red line indicates the conditional mean of future estimated by fake samples, whereas the shaded red area presents the region of $\\pm\\mathrm{std}$ . "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: In the abstract and introduction, we clearly state the main contributions of this paper along with important assumptions and limitations. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: See Limitation and Future work in Section 6. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We state the assumptions In the theorems and lemmas in Section 3 and Appendix A, and provide the corresponding proof in Appendix A. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: To ensure the reproducibility of our numerical results, we provide the peusdocodes of all the main algorithms and the the implementation details in appendix C. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide open access to the data and submit the complete source code in a compressed (zipped) file. We will make the codes publicly available when the paper is published ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide all the necessary details of experiments in Section 5 and Appendix C. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We report the error bars to all the numerical test metrics. See Section 5 and Appendix C. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide the computing resource information in Appendix C. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We conducted this research in compliance with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We discuss both positive and negative impacts in Section 6. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: We use the synthetic data generated by fractional Brownian motion and publicly available stock data from Yahoo Finance. There is no foreseeable risk of using these data. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We cite the original paper that produced the code package and dataset in the paper. See Section 5 and Appendix C. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We submit the codes with the necessary documentation in the form of the anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}]