[{"type": "text", "text": "Improved Analysis for Bandit Learning in Matching Markets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fang Kong Southern University of Science and Technology kongf@sustech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Zilong Wang Shanghai Jiao Tong University wangzilong@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Shuai Li\u2217 Shanghai Jiao Tong University shuaili8@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A rich line of works study the bandit learning problem in two-sided matching markets, where one side of market participants (players) are uncertain about their preferences and hope to find a stable matching during iterative matchings with the other side (arms). The state-of-the-art analysis shows that the player-optimal stable regret is of order $O(K\\log T/\\Delta^{2})$ where $K$ is the number of arms, $T$ is the horizon and $\\Delta$ is the players\u2019 minimum preference gap. However, this result may be far from the lower bound $\\Omega(\\operatorname*{max}\\{N\\log T/\\Delta^{2},\\bar{K}\\log T/\\Delta\\})$ since the number $K$ of arms (workers, publisher slots) may be much larger than that $N$ of players (employers in labor markets, advertisers in online advertising, respectively). In this paper, we propose a new algorithm and show that the regret can be upper bounded by $O(\\dot{N}^{2}\\mathrm{\\dot{log}}\\,T/\\Delta^{2}+\\bar{K}\\log T/\\Delta)$ . This result removes the dependence on $K$ in the main order term and improves the state-of-the-art guarantee in common cases where $N$ is much smaller than $K$ . Such an advantage is also verified in experiments. In addition, we provide a refined analysis for the existing centralized UCB algorithm and show that, under $\\alpha$ -condition, it achieves an improved $O(N\\log T/\\bar{\\Delta}^{2}+K\\log T/\\Delta)$ regret. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The two-sided matching market problem has been extensively studied in the literature due to its wide range of applications like labor market, school admission, house allocation, and online advertising [25, 9, 1]. There are two sides of participants in the market, such as the employers and workers in the labor market, advertisers and publishers in online advertising. Each participant on the one side has a preference ranking over the other side. The concept of stability, which characterizes the equilibrium state of the market where no participant wants to break up the current matching relationship and find another partner, has attracted great interest from researchers [25]. Achieving stability is critical for ensuring the long-term viability of the market. ", "page_idx": 0}, {"type": "text", "text": "A rich line of works [9, 15, 24] study how to find a stable matching in the market. Most of them assume the preference ranking of each market participant is known beforehand, which we refer to as the offline setting. However, in real applications, the knowledge of the preferences may be uncertain. For example, in the labor market, employers usually do not know the working abilities of workers before being matched, and advertisers also do not know the exact conversion rate of placing the advertisement in a publisher slot. This makes the traditional algorithms unavailable to find an exact stable matching. With the emergence of online market platforms such as the online labor market UpWork and TaskRabbit as well as online advertising platforms where employers or advertisers have many similar tasks, market participants are able to learn their unknown preferences during iterative matchings with the other side of agents. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Multi-armed bandit (MAB) is a classic framework that characterizes the learning process during iterative interactions [3, 18]. It considers the setting with single player on one side and multiple arms on the other side. At each round, the player selects an arm and receives a reward. The player has unknown preferences over arms and would learn this knowledge based on the collected rewards. To accumulate as many rewards as possible, the player faces the dilemma of exploration and exploitation. The former selects arms with less observations while the latter focuses on arms with better historical performances. How to balance the exploration and exploitation trade-off is the key of bandit algorithm design. The upper confidence bound (UCB) [3], Thompson sampling (TS) [14, 2], and explore-then-commit (ETC) [10] are common strategies in MAB to achieve this objective. ", "page_idx": 1}, {"type": "text", "text": "Liu et al. [19] introduce the bandit learning problem in matching markets and try to provide theoretical guarantees. Two sides of agents in the market can be modeled as players and arms. Without loss of generality, denote $N$ and $K$ as the number of players and arms, respectively. It is worth noting that this work and all of the following works assume $N\\le K$ to ensure each player has a chance to be matched. In this problem, the objective is to find a stable matching and minimize the stable regret for each player, which is defined as the difference between the reward of the stable arm and that the player receives during the horizon. Since there may be more than one stable matching, they mainly focus on the players\u2019 most preferred one corresponding to the player-optimal stable matching and the least preferred one corresponding to the player-pessimal stable matching. Note that players receive more rewards in the player-optimal stable matching and thus the former objective is the most desirable. Liu et al. [19] first study a centralized setting where a central platform would compute allocations for players to avoid conflicts. Both ETC and UCB-type algorithms are proposed for this setting. The former achieves a player-optimal stable regret guarantee with prior knowledge of players\u2019 minimum preference gap $\\Delta$ and the latter can only ensure to reach the playerpessimal stable matching. Motivated by real applications where the central platform may not always exist, a rich line of works then study the decentralized case where no platform coordinates players\u2019 behavior [20, 27, 4, 17, 21]. This line of works again only achieve guarantees for player-pessimal stable regret [20, 17, 27, 4, 21]. Table 1 compares settings and regrets among these works. Until recently, Zhang et al. [30] and Kong and Li [16] independently derive algorithms that have polynomial player-optimal stable regret and show the upper bound is $\\dot{O}(K\\log T/\\Delta^{2})$ . However, this result may be still far from the lower bound $\\Omega(\\operatorname*{max}\\{\\bar{N}\\log T/\\Delta^{2},K\\log T/\\Delta\\})$ ) [27] since $K$ is usually much larger than $N$ such as that the number of workers (publisher slots) is usually much larger than that of employers in labor markets (advertisers in online advertising, respectively). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we try to provide more efficient algorithms and improve the results over existing works. The detailed contribution can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an adaptive online Gale-Shapley (AOGS) algorithm for general markets. Stateof-the-art works [30, 16] explicitly separate the exploration and exploitation processes, which can lead to unnecessary regret, as exploring certain preference rankings may not contribute to the exploitation process. To avoid excessive exploration, our AOGS algorithm integrates the players\u2019 learning process into the GS steps. Players explore only the necessary number of candidate arms and adaptively switch between exploration and exploitation.   \n\u2022 We prove that the player-optimal stable regret of AOGS can be upper bounded by $O(\\dot{N}^{2}\\log T/\\Delta^{2}+\\dot{K}\\log T/\\dot{\\Delta})$ . This is the first result that removes the dependence on $K$ in the main regret order term and improves existing works in common cases where $N$ is much smaller than $K$ . We also conduct experiments to show the advantages of the algorithm.   \n\u2022 We refine the analysis of the centralized UCB algorithm in Liu et al. [19] for markets satisfying the $\\alpha$ -condition. By investigating the preference hierarchy structure of the $\\alpha$ - condition, we demonstrate that the stable matching converges sequentially from player 1 to player $N$ . Through inductive analysis over players, we establish an $O\\dot{(}N\\log\\dot{T}/\\dot{\\Delta}^{2}+$ $K\\log T/\\Delta)$ regret upper bound, which improves the original result for this algorithm in this specific market. ", "page_idx": 1}, {"type": "table", "img_path": "07N0qoaZ2L/tmp/d4a7105b45bf72d8cbff67bc3391cc90bf50896f1fe4a279bef86d7eeffc693c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Table 1: Comparisons of settings and regret bounds with most related works, $^*$ represents the playeroptimal stable regret and bounds without labeling $^*$ are for player-pessimal stable regret, $\\#$ represents the centralized setting. $N$ and $K$ are the number of players and arms with $N\\le K$ , $T$ is the total horizon, $\\Delta$ corresponds to some preference gap, $\\varepsilon$ depends on the hyper-parameter of algorithms, and $C$ is related to the unique stable matching condition which can grow exponentially in $N$ . The definition of $\\Delta$ in different works requires particular care. We use $\\mathrm{gap_{1}}$ , $\\mathrm{{gap}_{2}}$ , $\\mathrm{{gap}_{3}}$ , ${\\mathrm{gap}}_{4}$ represent the minimum preference gap between the (player-optimal) stable arm and the next arm after the stable arm in the preference ranking among all players, the minimum preference gap between any different arms among all players, the minimum preference gap between the first $N+1$ ranked arms among all players, and the minimum preference gap between arms that are more preferred than the next of the player-optimal stable arm among all players, respectively. Based on the property that the player-optimal stable arm of each player must be its first $N$ -ranked (shown in Appendix), there would be $\\mathrm{{gap}_{1}\\geq\\mathrm{{gap}_{4}\\geq\\mathrm{{gap}_{3}\\geq\\mathrm{{gap}_{2}}}}}$ . So our dependence on $\\Delta$ is better than the state-of-the-art works [30, 16] for general markets. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The problem of bandit learning in matching markets is first introduced by Das and Kamenica [8]. They study the special case where both sides of agents have the same preferences and propose some empirical methods to solve the problem. Liu et al. [19] first theoretically formulate this problem and provide an upper bound for the stable regret of players. They propose a centralized explore-then-commit (ETC) algorithm and upper confidence bound (UCB) algorithm, which obtain an $O\\left(K\\log T/\\Delta^{2}\\right)$ player-optimal stable regret and $O\\left(N K\\log T/\\Delta^{2}\\right)$ player-pessimal stable regret, respectively. It is worth noting that the former ETC algorithm requires knowledge about $\\Delta$ to ensure the algorithmic operation. Due to the generality, the following works focus on the decentralized setting. Liu et al. [20] and Kong et al. [17] propose the UCB and TS-type algorithm for general decentralized markets, respectively. Such a setting is much more challenging and both of them only achieve $O\\left(\\exp(N^{4})N^{5}\\bar{K^{2}}\\log^{2}(\\bar{T})/\\Delta^{2}\\right)$ upper bound for the player-pessimal stable regret. ", "page_idx": 2}, {"type": "text", "text": "To improve the stable regret guarantee, a line of research studies some special markets with unique stable matching in which case the player-optimal stable matching is equivalent to the playerpessimal one. Sankararaman et al. [27] propose the UCB-D3 algorithm based on the assumption of serial dictatorship, i.e., all arms share the same preferences, and obtain an $O\\left(N K\\log T\\right/\\Delta^{2}\\right)$ ", "page_idx": 2}, {"type": "text", "text": "regret upper bound. To investigate the problem hardness, they also derive a lower bound $\\Omega$ max $\\dot{\\left\\{N\\log T/\\Delta^{2},K\\log T/\\tilde{\\Delta Y}\\right\\}}$ under this assumption. Basu et al. [4] consider more general $\\alpha$ -condition setting for unique stable matching. They propose the UCB-D4 algorithm and also achieve the $O\\left(N K\\log{T}/\\Delta^{2}\\right)$ regret bound. Later, Maheshwari et al. [21] study the market satisfying $\\alpha$ -reducible condition and proposes a communication-free algorithm. Their regret bound has an exponential dependence on the number of market participants. Recently, researchers have developed algorithms that can achieve player-optimal stable regret guarantees without assuming unique stable matching. Both Zhang et al. [30] and Kong and Li [16] propose ETC-type algorithms that achieve $O\\left(K\\log T/\\Delta^{2}\\right)$ player-optimal stable regret. Wang and Li [28] studies the matching markets with serial dictatorship and obtains the $O\\left(N\\log T/\\Delta^{2}+K\\log T/\\Delta\\right)$ regret. Table 1, compares our proposed algorithm with these related works in terms of their corresponding settings and theoretical guarantees. ", "page_idx": 3}, {"type": "text", "text": "There are also other works considering unknown preferences in matching markets. Wang et al. [29] study a many-to-one market where an arm can accept multiple players. Jagadeesan et al. [12] consider online matching markets with monetary transfers. Min et al. [22] investigate Markov matching markets where state transitions occur during the matching process and players\u2019 rewards depend on the current state. Other studies have focused on non-stationary rewards, such as Muthirayan et al. [23], Ghosh et al. [11], who propose robust algorithms to mitigate the impact of reward disturbances. Additionally, several studies have explored the problem of offline matching market learning. Dai and Jordan [6, 7] propose approaches that leverage historical data to design optimal matching or recommend participants on both sides. ", "page_idx": 3}, {"type": "text", "text": "3 Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This paper considers the problem of bandit learning in two-sided matching markets. Denote ${\\mathcal{N}}=$ $\\{p_{1},p_{2},\\dotsc,p_{N}\\}$ as the player set and $K=\\{a_{1},a_{2},\\ldots,a_{K}\\}$ as the arm set. Let $N$ and $K$ be the number of players and arms, respectively. To ensure that each player can be matched with an arm, we follow previous works and assume $N\\le K$ [19, 20, 27, 4, 17, 30, 16]. ", "page_idx": 3}, {"type": "text", "text": "For each player $p_{i}~\\in\\mathcal{N}$ , its preference towards arm $a_{j}$ can be portrayed by an absolute utility $\\mu_{i,j}\\,\\in\\,(0,1]$ . For any pair of arms $a_{j}$ and $a_{j^{\\prime}}$ , $\\mu_{i,j}\\,>\\,\\mu_{i,j^{\\prime}}$ indicates that player $p_{i}$ prefers arm $a_{j}$ over $a_{j^{\\prime}}$ . Following previous works for matching markets [9, 19, 20, 27, 4, 17, 30, 16], players are assumed to have distinct preferences over different arms, i.e., $\\mu_{i,j}\\,\\neq\\,\\mu_{i,j^{\\prime}}$ for any $a_{j}\\neq a_{j^{\\prime}}$ . In practice, players\u2019 preferences which correspond to workers\u2019 abilities and the publisher\u2019s conversion rates are typically unknown and can be learned through the interactive matching process. On the other side, each arm $a_{j}$ also has a fixed and distinct preference utility $\\pi_{j,i}$ over each player $p_{i}\\in\\mathcal N$ , and $\\pi_{j,i}\\,>\\,\\pi_{j,i^{\\prime}}$ means that arm $a_{j}$ prefers player $p_{i}$ over $p_{i^{\\prime}}$ . As in labor markets where workers usually know their preferences over employers based on the payments and task types, the preferences of arms are assumed to be known beforehand [19, 20, 17, 27, 4, 30, 16]. ", "page_idx": 3}, {"type": "text", "text": "At each round $t\\,=\\,1,2,\\ldots$ , each player $p_{i}$ proposes to an arm $A_{i}(t)$ . For each arm $a_{j}$ , denote $A_{j}^{-1}(t)\\:=\\:\\{p_{i}:A_{i}(t)=a_{j}\\}$ as the set of players who selects arm $a_{j}$ at round $t$ . When more than one player selects $a_{j}$ , it accepts its most-preferred one in $A_{j}^{-1}(t)$ , i.e. $a_{j}$ will match with $p_{i}\\in\\arg\\operatorname*{max}_{p_{i}\\in A_{j}^{-1}(t)}\\pi_{j,i}$ . If a player $p_{i}$ is successfully matched with arm $A_{i}(t)$ , it will receive a random reward $X_{i}(t)$ characterizing its matching experience, which we assume is a 1-subgaussian random variable with expectation $\\mu_{i,A_{i}(t)}$ . Otherwise, $p_{i}$ is rejected by its proposed arm and only gets reward $X_{i}(t)\\;=\\;0$ . Denote $\\bar{A}_{i}(t)$ as the final matched arm of player $p_{i}$ at round $t$ . Then $\\mathbf{\\check{A}}_{i}(t)=A_{i}(t)$ if $p_{i}$ is accepted by the arm $A_{i}(t)$ and we simply set $\\bar{A}_{i}(\\dot{t})=\\varnothing$ if $p_{i}$ is rejected. ", "page_idx": 3}, {"type": "text", "text": "Stability is a key property of a matching in two-sided markets [9, 26, 24]. A matching $\\bar{A}(t)\\,=$ $\\{(i,\\bar{A}_{i}(t))\\,:\\,i\\,\\in\\,\\dot{[N]}\\}$ is stable if no market participant wants to break up its current matching relationship and find a new partner. Formally speaking, there is no player-arm pair $(p_{i},a_{j})$ such that $\\mu_{i,j}~>~\\mu_{i,\\bar{A}_{i}(t)}$ and $\\pi_{j,i}~>~\\pi_{j,\\bar{A}_{j}^{-1}(t)}$ . It is worth noting that there may be multiple stable matchings in the market. Denoted $M\\stackrel{*}{=}\\{m:m{\\mathrm{~is~stable}}\\}$ as the set of all stable matchings. It is shown that there exists a stable matching $m^{*}\\in M$ such that all players are matched with their most preferred stable arm [9], i.e., $\\mu_{i,m_{i}^{*}}\\geq\\mu_{i,m_{i}}$ for any $m\\,\\in\\,M,i\\,\\in\\,[N]$ . Given a specified horizon $T$ , the learning objective is to minimize the player-optimal stable regret for each player $p_{i}$ which is defined as the difference between the cumulative reward received by being matched with $m_{i}^{*}$ and the cumulative reward received by $p_{i}$ over $T$ rounds: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nR e g_{i}(T)=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\mu_{i,m_{i}^{*}}-X_{i}(t)\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the expectation is taken over by the randomness of the reward generation and the randomness inherent in the player\u2019s strategy. ", "page_idx": 4}, {"type": "text", "text": "For completeness, we introduce the procedure of the offline Gale-Shapley (GS) algorithm, which would be useful when describing the algorithmic details. The offline GS algorithm is a classic algorithm to find the player-optimal stable matching when both sides of the market participants know their exact preference rankings. Following offline GS, each player proposes to the arm one by one based on its preference ranking. Until no rejection happens, the final matching is exactly the player-optimal stable matching [9]. Specifically, at the first step, all players propose to their most preferred arm. Arms would accept their most preferred player among those who propose to it and reject others. Then players who are rejected at previous steps would then propose to their next preferred arm. And arms still reject the players who propose to it except for their most preferred one. Such a process continues until no rejection happens. ", "page_idx": 4}, {"type": "text", "text": "For convenience, we also define some useful notations that quantify the hardness of the learning problem in matching markets and are used in the later analysis. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. For each player $p_{i}$ , denote $\\sigma_{i}$ as $p_{i}$ \u2019s preference ranking and let $\\sigma_{i,k}$ as $p_{i}$ \u2019s the $k$ -th preferred arm in its ranking. With a little abuse of notation, let $\\sigma_{i}(a_{j})$ represent the rank of arm $a_{j}$ in $p_{i}$ \u2019s preference. For each player $p_{i}$ and arm $a_{j}\\neq a_{j^{\\prime}}$ , let $\\Delta_{i,j,j^{\\prime}}=|\\mu_{i,j}-\\mu_{i,j^{\\prime}}|$ be the preference gap of $p_{i}$ between $a_{j}$ and $a_{j^{\\prime}}$ . Define $\\begin{array}{r}{\\Delta=\\operatorname*{min}_{i,k\\in[\\sigma_{i}(m_{i}^{*})]}\\Delta_{i,\\sigma_{i,k},\\sigma_{i,k+1}}}\\end{array}$ as the minimum preference gap between the arm ranked the first $(\\sigma_{i}(m_{i}^{*})+1)$ -th among all players. Further, define $\\begin{array}{r}{\\Delta_{N}\\,=\\,\\mathrm{min}_{i,k\\in[N]}\\,\\Delta_{i,\\sigma_{i,k},\\sigma_{i,k+1}}}\\end{array}$ as the minimum preference gap between the arm ranked the first $(N+1)$ -th among all players. ", "page_idx": 4}, {"type": "text", "text": "4 Adaptive Online Gale-Shapley Algorithm for General Markets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we propose an algorithm called adaptive online Gale-Shapley (AOGS). For simplicity, we present the centralized version of the algorithm in Algorithm 1 from view of player $p_{i}$ . The discussion on how to extend it to a decentralized version is deferred to later subsections. ", "page_idx": 4}, {"type": "text", "text": "In general, AOGS is an online version of the GS algorithm. Since players do not know their preference rankings, they need to learn this knowledge by exploring arms (Line 4). To reduce the regret during exploration, players would adaptively eliminate sub-optimal arms (Line 6-8). Until they find their most preferred arm among available arms, they will stop exploration and focus on this arm (Line 9-11). And once the player finds this arm is occupied by a more preferred player, it would re-start exploration to find the next preferred arm (Line 12-19). ", "page_idx": 4}, {"type": "text", "text": "Specifically, each player $p_{i}$ still maintains $\\hat{\\mu}_{i,j}(t)$ and $T_{i,j}(t)$ to represent the empirical mean and the number of observations on each arm $a_{j}$ at the end of round $t$ . To determine whether an arm is more preferred than another, it maintains a confidence interval for each arm $a_{j}$ with upper bound $\\mathrm{UCB}_{i,j}(t):=\\hat{\\mu}_{i,j}(t)+\\sqrt{6\\log T/T_{i,j}(t)}$ and lower bound $\\mathrm{LCB}_{i,j}(t):=\\hat{\\mu}_{i,j}-\\sqrt{6\\log T/T_{i,j}(t)}$ . When $T_{i,j}\\,=\\,0$ , they will be initialized as $+\\infty$ and $-\\infty$ , respectively. And once the confidence intervals of the two arms are disjoint, it can regard the arm with a higher empirical mean to be more preferred (Line 1). To be consistent with the offline GS, each player maintains $\\mathcal{D}_{i}$ to represent the set of arms that have rejected $p_{i}$ during previous steps. In the beginning, it is initialized as an empty set. And we use $A_{i}$ to represent the available arms with the potential to be the stable arm of $p_{i}$ , which is initialized as $\\kappa\\setminus D_{i}$ . For convenience, denote $\\mathrm{E}_{i}$ as the exploration status of player $p_{i}$ . $\\mathrm{E}_{i}=$ True means that $p_{i}$ still needs to explore arms in $\\mathcal{A}_{i}$ to determine its most preferred arm. And $\\mathrm{E}_{i}=$ False means that $p_{i}$ already finds its most preferred arm and now focuses on this arm (Line 2). ", "page_idx": 4}, {"type": "text", "text": "To reduce the regret suffered during exploration, players would update $A_{i}$ and eliminate sub-optimal arms in real-time (Line 6-8). Here to avoid collision during round-robin exploration, we would maintain $A_{i}$ such that it contains no less than $N$ arms if $p_{i}$ still has not determined its most preferred one. Thus the union of the available arm set over all players with $\\mathrm{E}_{i}=$ True contains more than $N$ arms and the round-robin exploration over $\\mathcal{A}_{i}$ for each such player $p_{i}$ can be carried out without ", "page_idx": 4}, {"type": "text", "text": "Input: $\\overline{{N,K,T}}$ .   \n1: Initialize: $\\begin{array}{r}{\\hat{\\mu}_{i,j}(0)=0,T_{i,j}(0)=0,\\mathrm{UCB}_{i,j}(0)=\\infty,\\mathrm{LCB}_{i,j}(0)=-\\infty,\\forall j\\in[K];}\\end{array}$   \n2: Initialize: $\\mathcal{D}_{i}=\\emptyset,\\mathcal{A}_{i}=\\mathcal{K},\\mathrm{E}_{i}=\\mathrm{True}$ ;   \n3: for round $t=1,2,\\cdots,T$ do   \n4: Select $A_{i}(t)\\in\\mathcal A_{i}$ in a round-robin manner;   \n5: Update $\\hat{\\mu}_{i,j}(t),T_{i,j}(t)$ as Algorithm 2; compute $\\mathrm{UCB}_{i,j}(t),\\mathrm{LCB}_{i,j}(t)$ for any $j\\in[K]$ ;   \n6: if $|{\\mathcal{A}}_{i}|>{\\tilde{N}}$ and $\\bar{\\exists}j\\in{\\cal A}_{i}$ s.t. $\\mathrm{UCB}_{i,j}(t)<\\operatorname*{max}_{j^{\\prime}\\in\\mathcal{A}_{i}}\\mathrm{LCB}_{i,j^{\\prime}}(t)$ then   \n7: $\\mathcal{A}_{i}=\\mathcal{A}_{i}\\backslash\\{j\\}$ ;   \n8: end if   \n9: if $\\exists j\\in{\\mathcal{A}}_{i}$ s.t. $\\mathrm{LCB}_{i,j}(t)>\\mathrm{UCB}_{i,j^{\\prime}}(t)$ for any $j^{\\prime}\\in\\mathcal{A}_{i}$ and $j^{\\prime}\\ne j$ then   \n10: $A_{i}=\\{j\\},\\mathrm{E}_{i}=\\mathrm{False},A_{i}=j$ ;   \n11: end if   \n12: for other player $p_{i^{\\prime}}$ with $\\mathrm{E}_{i^{\\prime}}=\\mathrm{False},A_{i^{\\prime}}\\in\\mathcal{A}_{i}$ do   \n13: if $\\pi_{A_{i^{\\prime}},i^{\\prime}}>\\pi_{A_{i^{\\prime}},i}$ then   \n14: $\\mathcal{D}_{i}\\dot{=}\\mathcal{D}_{i}\\cup\\{\\dot{A}_{i^{\\prime}}\\},\\mathcal{A}_{i}=\\mathcal{K}\\backslash\\mathcal{D}_{i};$   \n15: if $\\mathrm{E}_{i}=$ False and $A_{i}\\in\\mathcal{D}_{i}$ then   \n16: $\\mathrm{E}_{i}=\\mathrm{True}$ ;   \n17: end if   \n18: end if   \n19: end for   \n20: end for ", "page_idx": 5}, {"type": "text", "text": "collisions. For completeness, we defer how to arrange players\u2019 explorations in later discussions. And once there exists an arm in $A_{i}$ that can be regarded to be optimal, $p_{i}$ will set the exploration status $\\mathrm{E}_{i}$ to be False and update the exploration arm set $\\boldsymbol{A}_{i}$ to only contain this optimal arm. For convenience, denote $A_{i}$ as this arm (Line 9-11). ", "page_idx": 5}, {"type": "text", "text": "The update of the available arm set should not only depend on $p_{i}$ \u2019s own observations but also on the other market participants. Specifically, if a player $p_{i^{\\prime}}$ determines $A_{i^{\\prime}}$ as its most preferred arm, then the final stable player of $A_{i^{\\prime}}$ would be the same as or more preferred than $p_{i^{\\prime}}$ . So if $A_{i^{\\prime}}$ prefers $p_{i^{\\prime}}$ than $p_{i}$ , then $A_{i^{\\prime}}$ would not be the stable arm of $p_{i}$ and there is no need for $p_{i}$ to explore $A_{i^{\\prime}}$ anymore. In this case, $p_{i}$ deletes arm $A_{i^{\\prime}}$ from its available set and update $\\mathcal{A}_{i}$ (Line 12-19). It is worth noting that this operation may incorporate the eliminated arms again in $A_{i}$ . This is reasonable as the previously eliminated arm may be more preferred than the current arms in $\\mathcal{A}_{i}$ after the deletion operation. And if the deleted arm is $p_{i}$ \u2019s current most preferred arm, it will mark $\\mathrm{E}_{i}$ as True and restart exploration to find the next most preferred one (Line 15-17). ", "page_idx": 5}, {"type": "text", "text": "4.1 Theoretical Results. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Following Algorithm $^{\\,l}$ , the player-optimal stable regret for each player $p_{i}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\nR e g_{i}(T)\\le O\\left(N^{2}\\log T/\\Delta^{2}+K\\log T/\\Delta\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Due to the space limit, the proof of Theorem 4.1 is deferred to Appendix A. The following are discussions on the detailed implementation as well as the novelty and significance of the result. ", "page_idx": 5}, {"type": "text", "text": "Arrangement of the round-robin exploration process. Recall that the number of available arms of each player is always larger than $N$ based on Line 6 and we assume players can explore their available arms in a round-robin manner without conflict. We now propose an arrangement by letting players explore the available arms in units of $N$ to guarantee this property. Specifically, in every $2N$ rounds, each player selects the $N$ available arms with the fewest observations (randomly breaks ties) and explores them in a round-robin way. It can be shown by contradiction that there exists an assignment such that each player can successfully match with their respective $N$ arms once during these $2N$ rounds (Lemma B.2 in Appendix), which only doubles the original regret without influencing the regret order. This guarantees that after every $2N$ rounds, the observation count difference among all available arms is at most 1. Players would perform arm elimination and optimal arm identification (Line 6 and 9) in the end of each $2N$ rounds. Therefore, compared to the timely eliminating/deleting of arms, this approach ensures that each player will select each arm at most one additional time during each exploration cycle before the player finds the optimal one. Since each player may restart exploration (Line 12-13) up to $N^{2}$ times (each of $N$ players can focus on $N$ arms), this scenario leads to an additional $O(N^{2}K^{\\ast})$ constant regret and does not influence the final regret order. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Extension to the decentralized setting. For simplicity, we present the AOGS algorithm in a centralized manner. It can also be extended to the decentralized version where no central platform coordinates players\u2019 selections. Specifically, we divide the total horizon into several phases and the length of each grows exponentially, i.e., the lengths of phases are $2,4,8,\\cdots$ . At the end round of each phase, players would decide whether to eliminate sub-optimal arms as Line 6-8, whether to determine one arm as the most preferred one and update the exploration status as Line 9-11. After the end of each phase, players would communicate their current exploration status, update their deletion set and available arm set, re-update the exploration status as Line 12-19, and then communicate their updated available arm set to determine the round-robin exploration process in the next phase. If there exists a player whose exploration status becomes True from False during communication, the phase length would re-start from 2 and grows exponentially since new arms may be required for exploration. If the final exploration status of all players is False and their optimal arms are different, the next phase would continues until the end of the interaction. The detailed implementation of communication is deferred to the next paragraph. Based on the communication, players with $\\mathrm{E}_{i}=$ True can have a pre-agreed protocol to explore arms in their available arm set in a round-robin manner without collision as discussed in the last paragraph. Within each phase, they just round-robin explore arms and collect observations but do not make any decision on arms\u2019 optimality. If $L$ observations on a sub-optimal arm are enough to decide its sub-optimality in the centralized version, then this arm would be eliminated at the end of the corresponding phase with the selected time to be at most $2L$ due to the exponentially increasing phase length. So the regret in this decentralized version is at most two times as that suffered in the centralized version. ", "page_idx": 6}, {"type": "text", "text": "This paragraph describes the implementation of the communication procedure. Recall that players need to communicate their exploration status and available arm set (calculated by subtracting the deletion and eliminating set from $\\kappa$ ) at the end of each phase. For the phase length, recall that it grows exponentially until a player\u2019s exploration status becomes True from False and a player updates $\\mathrm{E}_{i}$ from False as True only when its most preferred arm is occupied by a higher-priority player (Line 15). As shown by Lemma A.3, each player may occupy $N$ arms, so such event happens at most $N^{2}$ times. And when all players find their unique optimal arm which requires $O(\\dot{N}^{\\dot{2}}\\log T/\\Delta^{2})$ times, the phase would continue until the end of the interaction. Above all, the total number of phases is of order $O$ $\\left(N^{2}\\log\\left(N^{2}\\log T/\\Delta^{2}\\right)\\right)$ . For the detailed communication procedure, as phase 1 in Kong and Li [16], players can first estimate their unique indices and we assume the matching results are public as [16, 17, 20]. During the communication block of each phase, players sequentially transmit their data based on their indices, received by others through matching outcomes. Specifically, in the corresponding round, player $p_{i}$ selects the focused arm if $\\mathrm{E}_{i}=$ False and nothing otherwise, incurring an $O$ $\\left(\\bar{N^{3}}\\log\\left(\\bar{N^{2}}\\log\\bar{T^{}}/\\Delta^{2}\\right)\\right)$ cost for status communication in all phases. For deletion (eliminating) sets, $p_{i}$ first selects the arm with index $k$ to indicate it will transmit $k$ arms and then sequentially selects these $k$ arms. The communication cost on the arm set size is $O\\left(N^{3}\\log\\left(N^{2}\\log T/\\Delta^{2}\\right)\\right)$ . Recall that players delete arms only when a higher-priority player focuses on this arm, so $N$ players focus on at most $N$ arms before reaching stability and each player deletes up to $N$ arms. Also, each player can eliminate up to $K-N$ arms during each exploration and would re-start exploration for at most $N^{2}$ times. Thus the communication cost on the deletion (eliminating) arms is ${\\hat{O}}(N^{3}K)$ and the total communication cost is $O$ $\\left(N^{3}\\log\\left(N^{2}\\log T/\\Delta^{2}\\right)+N^{3}K\\right)$ , which is not the main order of the regret. ", "page_idx": 6}, {"type": "text", "text": "Novelty and significance. Balancing the exploration-exploitation trade-off is the key to achieving low regret. Previous efforts were devoted to addressing pessimal stable regret [19, 20, 17] and uniqueness assumptions [27, 4] using classic UCB and TS strategies. Until recently, Zhang et al. [30] and Kong and Li [16] show that ETC-type strategies better fit this problem. Specifically, players first uniformly explore arms to learn the complete preference ranking of the top $N$ arms, and then use the GS procedure for exploitation to find the player-optimal stable matching. However, such a method may over-explore and cause unnecessary regret. The reason is that to learn the first $N$ -ranked arms, each sub-optimal arm $a_{j}$ must be selected ${\\cal O}\\bar{(\\log T/\\Delta_{i,\\sigma_{i,N},j}^{2})}$ times to be distinguished from the $N$ -ranked arm. And each time selecting this arm, the player pays $\\Delta_{i,m_{i}^{*},j}$ regret. The mismatch between the paid regret and the difference to be figured out results in $O(K\\log T/\\Delta_{N}^{2})$ regret. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In contrast to the existing approach, we present a more adaptive perspective that integrates the learning process into each GS step. To avoid additional regret, players do not need to estimate their complete preference. Instead, they would start exploitation once the optimal available arm is identified. And if this arm is occupied by a higher-priority player, this player would restart exploration to find the next preferred one. To avoid the additional cost while exploring the optimal arm, we also design a more efficient way to let players promptly eliminate $K-N$ sub-optimal arms and only maintain the remaining $N$ arms to guarantee no collision. For the eliminated arm $a_{j}$ , the reward difference to be figured out is at least $\\Delta_{i,m_{i}^{*},j}$ , which matches the regret when selecting this arm. So the regret caused by these eliminated arms is $O(K\\log T/\\Delta)$ , avoiding the dependence on $K$ in the main order term. This is a more effective way to balance exploration and exploitation in matching market scenarios. Our result also shows a significant improvement over Zhang et al. [30] and Kong and Li [16] in common applications such as labor markets and online advertising where the number $N$ of players (employers, advertisers) is far smaller than that $K$ of arms (workers, publisher slots). ", "page_idx": 7}, {"type": "text", "text": "Discussion on the definition of gaps. Recall that our $\\Delta$ is defined as the minimum preference difference among arms that ranked in the first $(\\sigma_{i}(m_{i}^{*})+1)$ -th positions $(\\mathrm{gap}_{4})$ , while the lower bound in Sankararaman et al. [27] for markets with serial dictatorship depends on $\\Delta$ that is defined as the minimum preference difference between the arm ranked $\\sigma_{i}(m_{i}^{*})$ and the arm ranked $\\sigma_{i}(m_{i}^{*})+1$ $(\\mathrm{gap_{1}}$ in Table 1, respectively). It is an open problem whether the lower bound should depend on our $\\mathrm{{gap}_{4}}$ in general markets. Here we would like to discuss that the knowledge of $\\mathrm{{gap}_{4}}$ is necessary to learn the true stable matching. Consider a market with 4 players and 5 arms. The preference rankings of players are $p_{1}:a_{1}~>~a_{2}~>~a_{3}~>~a_{4}~>~a_{5};p_{2}~:a_{2}~>~a_{3}~>~a_{1}~>~a_{4}~>~a_{5};p_{3}~:$ $a_{3}>a_{1}>a_{2}>a_{4}>a_{5};p_{4}:a_{1}>a_{2}>a_{3}>a_{4}>a_{5}$ and the preference rankings of arms are $a_{1}:p_{2}\\,>\\,p_{3}\\,>\\,p_{4}\\,>\\,p_{1};a_{2}\\,:\\,p_{3}\\,>\\,p_{4}\\,>\\,p_{1}$ $\\cdot\\ p_{1}\\,>\\,p_{2};a_{3}\\,:\\,p_{4}\\,>\\,p_{1}\\,>\\,p_{2}\\,>\\,p_{3};a_{4}\\,:\\,p_{1}\\,>$ p2 $>\\;p_{3}\\;>\\;p_{4};a_{5}\\;:\\;p_{1}\\;>\\;p_{2}\\;>\\;p_{3}\\;>\\;p_{4}.$ In this market, the player-optimal stable matching is $\\{(p_{1},a_{4}),(p_{2},a_{1}),(p_{3},a_{2}),(p_{4},a_{3})\\}$ . However, if player $p_{1}$ has collected enough observations to identify $\\mathrm{{gap}_{1}}$ but not collected enough observations to identify $\\mathrm{{gap}_{4}}$ and wrongly estimate the first $\\sigma_{i}(m_{i}^{*})$ ranked arms, i.e., $p_{1}$ wrongly estimate the preference ranking as $p_{1}\\ :\\ a_{1}\\ >\\ a_{2}\\ >$ $a_{4}\\,>\\,a_{3}\\,>\\,a_{5}$ . Then the computed player-optimal stable matching under this preference ranking is $\\{(p_{1},a_{4}),(p_{2},a_{3}),(p_{3},a_{1}),(p_{4},a_{2})\\}$ , which is not stable in the original market as player $p_{1}$ and arm $a_{3}$ form a blocking pair. This example shows that player $p_{1}$ must identify the gap among the first $\\sigma_{i}(m_{i}^{*})$ ranked arms to find a stable matching in the market, which further illustrates the crucial role of $\\mathrm{{gap}_{4}}$ in finding the real stable matching. We leave the lower bound in general markets as an important future direction. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we compare our Algorithm 1 with baselines ETGS [16], ML-ETC [30] and Phased ETC [4] which also enjoy guarantees for player-optimal stable regret in general decentralized oneto-one markets. To better illustrate the advantages of our algorithm, especially when $N$ is much smaller than $K$ , we set $N\\,=\\,3$ and $K\\,=\\,10$ . The preference rankings for both players and arms are generated as random permutations. The preference gap between any adjacent ranked arms is set as 0.1. The feedback $X_{i,j}(t)$ for player $p_{i}$ on arm $a_{j}$ at time $t$ is drawn independently from the Gaussian distribution with mean $\\mu_{i,j}$ and variance 1. We report the maximum cumulative playeroptimal stable regret among all players and the cumulative player-optimal instability in Figure 1 (a) and (b), respectively. Here the cumulative player-optimal unstability is defined as the number of matchings that are not the player-optimal stable one. All algorithms run for $T=100k$ rounds and all results are averaged over 50 independent run\u221as. The error bars represent standard errors, which are computed as standard deviations divided by $\\sqrt{50}$ . ", "page_idx": 7}, {"type": "text", "text": "As shown in the figure, our AOGS algorithm, which only conducts necessary explorations over unknown preferences and promptly eliminates sub-optimal arms, achieves the least cumulative regret and cumulative player-optimal unstability among all baselines. The ML-ETC and ETGS algorithms need to sufficiently explore $K$ arms to estimate the full preference ranking, requiring more exploration time to find the player-optimal stable matching. The PhasedETC algorithm has not yet converged within the displayed rounds due to the cold start problem. ", "page_idx": 7}, {"type": "image", "img_path": "07N0qoaZ2L/tmp/5f9d9703addd5dad887ab732dda509a3e116d83470e95c5485ac850755c62e06.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: Experimental comparisons of our AOGS with ETGS, ML-ETC and Phased ETC in oneto-one decentralized markets with $N=3$ players and $K=10$ arms. ", "page_idx": 8}, {"type": "text", "text": "6 Centralized UCB Algorithm for $\\alpha$ -condition ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide a new analysis for the centralized UCB algorithm in markets satisfying $\\alpha$ -condition. The algorithm is first introduced by Liu et al. [19]. For completeness, we present the full algorithm in Algorithm 2. At each round, players submit their UCB rankings to the centralized platform (Line 3). The platform runs the GS algorithm (based on players\u2019 submitted rankings) and returns the partner to each player (Line 4). ", "page_idx": 8}, {"type": "text", "text": "Algorithm 2 centralized UCB   \nInput: $N,K$ .   \n1: Initialize: $\\hat{\\mu}_{i,j}(0)=0,T_{i,j}(0)=0,\\mathrm{UCB}_{i,j}(1)=\\infty,\\forall i\\in[N],j\\in[K].$   \n2: for round t = 1, 2, . . . , T do   \n3: Receive rankings $\\begin{array}{r l r}{\\hat{\\sigma}}&{{}}&{:=\\,\\,\\quad\\{\\hat{\\sigma}_{i}\\}_{i\\in[N]}}\\end{array}$ according to the decreasing order of $\\{\\mathrm{UCB}_{i,j}(t)\\}_{j\\in[K]},\\forall i\\in[N]$ ;   \n4: $A_{i}(t)\\gets$ Gale-Shapley $({\\hat{\\sigma}},\\pi)$ for each player $p_{i}$ ;   \n5: Observe $X_{i}(t)$ , and update \u00b5\u02c6i,j(t), Ti,j(t), $\\mathrm{UCB}_{i,j}(t+1)$ for each $p_{i},a_{j}$ ;   \n6: end for ", "page_idx": 8}, {"type": "text", "text": "In the following, we introduce the $\\alpha$ -condition. Conditions guaranteeing the unique stable matching have been widely studied in the offline setting [5, 13] and also the online setting to improve the learning efficiency [27, 4, 21]. Among these conditions, the $\\alpha$ -condition is shown to be the weakest sufficient one [13] and incorporate the conditions studied in existing works [27, 4, 21]. ", "page_idx": 8}, {"type": "text", "text": "$\\beta$ n-toohft  epp lepareyrmemur tuiatntai $[N]$ $[K]$ r.s meT tish $[N]_{\\beta}=\\{Q_{1}^{(\\beta)},\\dots,Q_{N}^{(\\beta)}\\}$ $[K]_{\\beta}\\,=$ $\\{q_{1}^{(\\beta)},\\dots,q_{K}^{(\\beta)}\\}$ $[N]$ $[K]$ $j$ $[N]_{\\beta}$ $Q_{j}^{(\\beta)}$ $[N]$ $k$ $[K]_{\\beta}$ $q_{k}^{(\\beta)}$ $[K]$ can define the $\\alpha$ -condition below. ", "page_idx": 8}, {"type": "text", "text": "Definition 6.1. The $\\alpha$ -condition is satisfied if there is a stable matching $(\\mathbf{j}^{*},\\mathbf{i}^{*})$ , a left-order of players and arms s.t. $\\forall i\\;\\in\\;[N]_{l},\\forall j\\;>\\;i,j\\;\\in\\;[K]_{l}\\;:\\;\\mu_{i,j_{i}^{*}}\\;>\\;\\mu_{i,j}$ where $j_{i}^{*}$ is the partner of player $p_{i}$ in stable matching $(\\mathbf{j}^{*},\\mathbf{i}^{*})$ , and a (possibly different) right-order of players and arms s.t. $\\forall j<i\\leq N,q_{j}\\in[K]_{r},Q_{i}\\in[N]_{r}:\\pi_{q_{j},Q_{i_{q_{j}}^{*}}}>\\pi_{q_{j},Q_{i}}.$ Here similarly, $i_{q_{j}}^{*}$ is the partner of arm $a_{q_{j}}$ in stable matching $(\\mathbf{j}^{*},\\mathbf{i}^{*})$ . ", "page_idx": 8}, {"type": "text", "text": "Without loss of generality, we consider the identity of players and arms is just the left order, i.e., $[N]~=~[N]_{l}$ and $[K]\\;=\\;[K]_{l}$ . Thus we only deal with player order ${\\boldsymbol{Q}}_{i}^{(r)}\\,=\\,{\\boldsymbol{Q}}_{i}$ and arm order $q_{j}^{(r)}=q_{j}$ , for $i\\in[N],j\\in[K]$ in the rest of the paper. Under $\\alpha$ -condition, it is easy to inductively verify that for any $i\\in[N]$ , the player $p_{i}$ is matched with arm $a_{i}$ , and the player $p_{Q_{i}}$ is matched with the arm $a_{q_{i}}$ in the unique stable matching [4]. ", "page_idx": 8}, {"type": "text", "text": "6.1 Theoretical Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We analyze the regret for the centralized UCB algorithm under $\\alpha$ -condition. ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.2. When preferences of participants satisfy $\\alpha$ -condition, following Algorithm 2, the stable regret for each player $p_{i}$ satisfies ", "page_idx": 9}, {"type": "equation", "text": "$$\nR e g_{i}(T)\\le O\\left(N\\log T/\\Delta_{N}^{2}+K\\log T/\\Delta\\right)\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The centralized UCB algorithm is proposed by Liu et al. [19] and shown to have $O(N K\\log T/\\Delta^{2})$ player-pessimal stable regret for general markets. We provide a new analysis for markets satisfying $\\alpha$ -condition which removes the dependence of $K$ in the regret. Due to the space limit, the detailed proof is deferred to Appendix C. ", "page_idx": 9}, {"type": "text", "text": "Key idea of the proof. We investigate the preference structure of $\\alpha$ -condition to obtain the improved analysis. For player $p_{i}$ , its regret is due to selecting sub-optimal arm $a_{k}$ with $\\mu_{i,k}\\,<\\,\\mu_{i,i}$ . Arm $a_{k}$ will be selected by $p_{i}$ when its UCB value is higher than $p_{i}$ \u2019s stable matched arm $a_{i}$ , which time is bounded by ${\\cal O}(\\log\\bar{T}/\\Delta_{i,i,k}^{2})$ , and one $\\Delta_{i,i,k}$ on the denominator can be eliminated when multiplying $\\Delta_{i,i,k}$ to compute regret. This contributes $O\\left(K\\log T/\\Delta\\right)$ regret since there are at most $K-1$ sub-optimal arms. It is worth noting that arm $a_{k}$ will also be selected by $p_{i}$ if $p_{i}$ is rejected by $a_{i}$ in the GS algorithm. Recall that under $\\alpha$ -condition, there is a right order $Q_{i^{\\prime}}=i\\in[N]_{r}$ for player $p_{i}$ , such that $\\forall i^{\\prime\\prime}>i^{\\prime},Q_{i^{\\prime\\prime}}\\in[N]_{r}:\\pi_{q_{i^{\\prime}},Q_{i^{\\prime}}}>\\pi_{q_{i^{\\prime}},Q_{i^{\\prime\\prime}}}$ , which means arm $a_{q_{i^{\\prime}}}=a_{i}$ can only prefer players $p_{Q_{1}},p_{Q_{2}},\\cdot\\cdot\\cdot\\,,p_{Q_{i^{\\prime}-1}}$ than player $p_{Q_{i^{\\prime}}}=p_{i}$ . Thus $p_{i}$ is rejected by $a_{i}$ only when these players select $a_{i}$ , and $a_{i}$ is sub-optimal for those players. To bound the regret of $p_{i}$ when being rejected, we just need to bound the exploration times of these players $p_{Q_{1}},p_{Q_{2}},\\cdot\\cdot\\cdot\\,,p_{Q_{i^{\\prime}-1}}$ on arm $a_{i}$ . However, the exploration time of a single player $p_{Q_{\\ell}}$ with $1\\,\\leq\\,\\ell\\,\\leq\\,i^{\\prime}\\,-\\,1$ on $a_{i}$ can not be trivially bounded by ${\\cal O}(\\log T/\\Delta_{N}^{2})$ since $p_{Q_{\\ell}}$ may have to select arm $a_{i}$ after rejected by its stable arm $a_{q_{\\ell}}$ in offline GS, where $a_{q_{\\ell}}$ might be selected by $p_{Q_{1}},\\cdot\\cdot\\cdot\\cdot,p_{Q_{\\ell-1}}$ . This leads to a recursion form. We control this term using the fact that when a player is rejected by its stable matched arm in the GS, it can date back to a higher right-order player wrongly over-estimate its preference for a sub-optimal arm. This key observation and the definition of $\\Delta$ make it possible to derive the final $O\\left(\\dot{N7}{{\\log T}/{\\Delta_{N}^{2}}}\\right)$ bound. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate the problem of whether a tighter bound can be derived for the bandit learning problem in two-sided matching markets. For the general one-to-one matching markets, we try to improve the learning efficiency of the existing algorithms. By integrating the offline GS procedure into the online learning process and carefully designing the elimination strategy, we show that the player-optimal stable regret can be upper bounded by $O(\\bar{N}^{2}\\log T/\\Delta^{2}+K\\log\\bar{T}/\\Delta)$ . This result removes the dependence on $K$ in the main order term of existing works and improves the stateof-the-art result [30, 16] in common cases where the number of players is much smaller than that of arms. An experiment is conducted to verify its advantage over other baselines in such markets. We also present a novel analysis for the centralized UCB algorithm in markets satisfying $\\alpha$ -condition and derive an improved $\\dot{O}(N\\log T/\\Delta_{N}^{2}+K\\log T/\\Delta)$ regret upper bound. ", "page_idx": 9}, {"type": "text", "text": "One significant future direction is to investigate the optimality of algorithms. Although the dependence on $N,K,T$ in Theorem 6.2 matches the lower bound, the definition of $\\Delta$ differs. It remains unclear how the upper bound changes with the same $\\Delta$ . Furthermore, since the lower bound provided by [27] applies only to special markets, and the learning problem in general markets is more challenging due to the complex preference structure, determining whether an algorithm can perform better in general markets is still an open problem. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The corresponding author Shuai Li is supported by National Key Research and Development Program of China (2022ZD0114804) and National Natural Science Foundation of China (62376154). ", "page_idx": 9}, {"type": "text", "text": "We thank Yuhao Zhang and Wenqian Wang for valuable discussions and suggestions on the proof of Lemma B.2. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Atila Abdulkadirog\u02d8lu and Tayfun So\u00a8nmez. House allocation with existing tenants. Journal of Economic Theory, 88(2):233\u2013260, 1999.   \n[2] Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics, pages 99\u2013107, 2013.   \n[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235\u2013256, 2002.   \n[4] Soumya Basu, Karthik Abinav Sankararaman, and Abishek Sankararaman. Beyond $\\log^{2}(t)$ regret for decentralized bandits in matching markets. In International Conference on Machine Learning, pages 705\u2013715, 2021.   \n[5] Simon Clark. The uniqueness of stable matchings. Contributions in Theoretical Economics, 6(1), 2006.   \n[6] Xiaowu Dai and Michael Jordan. Learning in multi-stage decentralized matching markets. In Advances in Neural Information Processing Systems, volume 34, pages 12798\u201312809, 2021.   \n[7] Xiaowu Dai and Michael I Jordan. Learning strategies in decentralized matching markets under uncertain preferences. Journal of Machine Learning Research, 22(1):11806\u201311855, 2021.   \n[8] Sanmay Das and Emir Kamenica. Two-sided bandits and the dating market. In International Joint Conference on Artificial Intelligence, pages 947\u2013952, 2005.   \n[9] David Gale and Lloyd S Shapley. College admissions and the stability of marriage. The American Mathematical Monthly, 69(1):9\u201315, 1962.   \n[10] Aure\u00b4lien Garivier, Tor Lattimore, and Emilie Kaufmann. On explore-then-commit strategies. In Advances in Neural Information Processing Systems, volume 29, pages 784\u2013792, 2016.   \n[11] Avishek Ghosh, Abishek Sankararaman, Kannan Ramchandran, Tara Javidi, and Arya Mazumdar. Decentralized competing bandits in non-stationary matching markets. arXiv preprint arXiv:2206.00120, 2022.   \n[12] Meena Jagadeesan, Alexander Wei, Yixin Wang, Michael Jordan, and Jacob Steinhardt. Learning equilibria in matching markets from bandit feedback. In Advances in Neural Information Processing Systems, volume 34, 2021.   \n[13] Alexander Karpov. A necessary and sufficient condition for uniqueness consistency in the stable marriage matching problem. Economics Letters, 178:63\u201365, 2019.   \n[14] Emilie Kaufmann, Nathaniel Korda, and Re\u00b4mi Munos. Thompson sampling: An asymptotically optimal finite-time analysis. In International Conference on Algorithmic Learning Theory, pages 199\u2013213. Springer, 2012.   \n[15] Alexander S Kelso Jr and Vincent P Crawford. Job matching, coalition formation, and gross substitutes. Econometrica: Journal of the Econometric Society, pages 1483\u20131504, 1982.   \n[16] Fang Kong and Shuai Li. Player-optimal stable regret for bandit learning in matching markets. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 2023.   \n[17] Fang Kong, Junming Yin, and Shuai Li. Thompson sampling for bandit learning in matching markets. In International Joint Conference on Artificial Intelligence, 2022.   \n[18] Tor Lattimore and Csaba Szepesva\u00b4ri. Bandit algorithms. Cambridge University Press, 2020.   \n[19] Lydia T Liu, Horia Mania, and Michael Jordan. Competing bandits in matching markets. In International Conference on Artificial Intelligence and Statistics, pages 1618\u20131628. PMLR, 2020.   \n[20] Lydia T Liu, Feng Ruan, Horia Mania, and Michael I Jordan. Bandit learning in decentralized matching markets. Journal of Machine Learning Research, 22(211):1\u201334, 2021.   \n[21] Chinmay Maheshwari, Eric Mazumdar, and Shankar Sastry. Decentralized, communicationand coordination-free learning in structured matching markets. In Advances in Neural Information Processing Systems, 2022.   \n[22] Yifei Min, Tianhao Wang, Ruitu Xu, Zhaoran Wang, Michael Jordan, and Zhuoran Yang. Learn to match with no regret: Reinforcement learning in markov matching markets. In Advances in Neural Information Processing Systems, volume 35, pages 19956\u201319970, 2022.   \n[23] Deepan Muthirayan, Chinmay Maheshwari, Pramod Khargonekar, and Shankar Sastry. Competing bandits in time varying matching markets. In Learning for Dynamics and Control Conference, pages 1020\u20131031. PMLR, 2023.   \n[24] Alvin E Roth and Marilda Sotomayor. Two-sided matching. Handbook of game theory with economic applications, 1:485\u2013541, 1992.   \n[25] Alvin E Roth. The evolution of the labor market for medical interns and residents: a case study in game theory. Journal of political Economy, 92(6):991\u20131016, 1984.   \n[26] Alvin E Roth. Stability and polarization of interests in job matching. Econometrica: Journal of the Econometric Society, pages 47\u201357, 1984.   \n[27] Abishek Sankararaman, Soumya Basu, and Karthik Abinav Sankararaman. Dominate or delete: Decentralized competing bandits in serial dictatorship. In International Conference on Artificial Intelligence and Statistics, pages 1252\u20131260. PMLR, 2021.   \n[28] Zilong Wang and Shuai Li. Optimal analysis for bandit learning in matching markets with serial dictatorship. Theoretical Computer Science, 1010:114703, 2024.   \n[29] Zilong Wang, Liya Guo, Junming Yin, and Shuai Li. Bandit learning in many-to-one matching markets. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 2088\u20132097, 2022.   \n[30] Yirui Zhang, Siwei Wang, and Zhixuan Fang. Matching in multi-arm bandit with collision. In Advances in Neural Information Processing Systems, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Define F = \u22031 \u2264t \u2264T, i \u2208[N], j \u2208[K] : |\u00b5\u02c6i,j(t) \u2212\u00b5i,j| > 6T il,ojg( tT) as the failure event that the estimated reward is far from the expected reward at some time and some player-arm pair. The regret can be upper bounded as follows. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{i}(t)=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\left(\\mu_{i,m_{i}^{*}}-X_{i}(t)\\right)\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\left(\\mu_{i,m_{i}^{*}}-X_{i}(t)\\right)\\mid\\Im\\right]+\\mathbb{P}\\left(\\mathcal{F}\\right)\\cdot T\\cdot\\mu_{i,m_{i}^{*}}}\\\\ &{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\sum_{a_{j}}\\mathbb{I}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j}\\mid\\Im\\right]+\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{I}\\left\\{\\bar{A}_{i}(t)=\\emptyset\\right\\}\\cdot\\mu_{i,m_{i}^{*}}\\mid^{\\top}\\mathcal{F}\\right]}\\\\ &{\\quad+\\mathbb{P}\\left(\\mathcal{F}\\right)\\cdot T\\cdot\\mu_{i,m_{i}^{*}}}\\\\ &{\\leq\\mathbb{Q}6N^{2}\\log T/\\Delta^{2}+96N\\log T/\\Delta+192N^{2}\\log T/\\Delta^{2}+2N K}\\\\ &{=O\\left(N^{2}\\log T/\\Delta^{2}+K\\log T/\\Delta\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where Eq. (1) holds based on Lemma A.1, A.2, and A.4. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\sum_{a_{j}}\\mathbb{1}\\big\\lbrace\\bar{A}_{i}(t)=a_{j}\\big\\rbrace\\cdot\\Delta_{i,m_{i}^{*},j}\\,\\,|\\,^{\\top}\\mathcal{F}\\right]\\leq96N^{2}\\log T/\\Delta^{2}+96K\\log T/\\Delta\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Recall that player $p_{i}$ would update the available set $\\boldsymbol{A}_{i}$ when other players $p_{i^{\\prime}}$ sets $\\mathrm{E}_{i^{\\prime}}\\,=$ False and $\\pi_{A_{i^{\\prime}},i^{\\prime}}>\\pi_{A_{i^{\\prime}},i}$ as Line 14. Denote $t_{s}$ as the round index when this operation happens for the $s$ -th time. Without loss of generality, let $t_{0}=1$ . ", "page_idx": 12}, {"type": "text", "text": "Recall that at a high level, each time another player $p_{i^{\\prime}}$ sets $\\mathrm{E}_{i^{\\prime}}$ as False, it means that $p_{i^{\\prime}}$ learns its most preferred arm in current available set. Combined with $\\mathcal{F}$ and Lemma A.6, the determined arm of players during each exploration would be truly their most preferred one. Thus the AOGS algorithm is an online version of GS and the $s^{\\prime}$ -th time player $p_{i^{\\prime}}$ sets $\\mathrm{E}_{i^{\\prime}}$ as False is equivalent to that $p_{i^{\\prime}}$ proposes its $s^{\\prime}$ -th most preferred arm in the offline GS. According to Lemma A.3, at most $N-1$ arms are proposed by all players before reaching stability. Thus for player $p_{i}$ , the operation in Line 14 would happen for at most $N-1$ times. ", "page_idx": 12}, {"type": "text", "text": "Recall that for each $s$ , during time $t_{s}$ to $t_{s+1}$ , player $p_{i}$ would explore all available arms in a roundrobin manner, eliminate sub-optimal arms until $N$ arms are in the set, and focuses on the best one among these $N$ when it is identified. For convenience, denote $R_{s}$ as the set of the remaining $N$ arms that $p_{i}$ explored in $\\mathcal{A}_{i}$ in a round-robin manner until condition Line 9 is satisfied, $D_{s}$ as the set of arms that $p_{i}$ eliminated due to condition Line 6, and $j_{s}$ as the arm that $p_{i}$ focuses from the time it sets $\\mathrm{E}_{i}$ as False to time $t_{s+1}-1$ . Then it holds that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\sum_{a_{j}}\\mathbb{I}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j}\\;|\\,^{\\gamma}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{N-1}\\sum_{t=t_{s}}^{t_{s+1}-1}\\sum_{a_{j}}\\mathbb{I}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j}\\;|\\,^{\\gamma}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{N-1}\\sum_{t=t_{s}}^{t_{s+1}-1}\\left(\\sum_{a_{j}\\in R_{s}}\\mathbb{I}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j}+\\sum_{a_{j}\\in D_{s}}\\mathbb{I}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j}\\right.}\\\\ &{\\left.\\quad+\\mathbb{I}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j_{s}}\\right)|\\,^{\\gamma}\\mathcal{F}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\leq\\mathbb{E}\\left[\\sum_{s=0}^{N-1}\\sum_{t=t_{s}}^{t_{s+1}-1}\\left(\\sum_{a_{j}\\in R_{s}}\\mathbb{1}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j}+\\sum_{a_{j}\\in D_{s}}\\mathbb{1}\\left\\{\\bar{A}_{i}(t)=a_{j}\\right\\}\\cdot\\Delta_{i,m_{i}^{*},j}\\right)\\mid^{\\top}\\mathcal{F}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where Eq. (2) is due to that, based on Lemma A.6 and the offline GS algorithm, the arm $j_{s}$ before GS stops would be better than $m_{i}^{*}$ , thus the regret caused by selecting these arms is less than 0. ", "page_idx": 13}, {"type": "text", "text": "For the first term in Eq. (2), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{N-1}\\sum_{t=t,s_{i}}^{t_{s+1}-1}\\sum_{a,j\\in\\mathbb{R}_{s}}\\mathbb{1}\\{\\bar{A}_{i}(t)=a_{j}\\}\\cdot\\Delta_{i,m_{i}^{*},j}\\mid\\mathcal{F}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{N-1}\\sum_{a_{j}\\in\\mathbb{R}_{s}}\\sum_{t=t,s_{i}}^{t_{s+1}-1}\\mathbb{1}\\{\\bar{A}_{i}(t)=a_{j}\\}\\cdot\\Delta_{i,m_{i}^{*},j}\\mid\\mathcal{F}\\right]}\\\\ &{\\le\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{N-1}\\sum_{a_{j}\\in\\mathbb{R}_{s}}\\frac{96\\log T}{\\Delta_{i,j,s_{i+1}}^{2}}\\cdot\\Delta_{i,m_{i}^{*},j}\\mid^{\\top}\\mathcal{F}\\right]}\\\\ &{\\le\\frac{96N^{2}\\log T}{\\Delta^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where Eq. (4) is due to Lemma A.5, Eq. (5) holds since $R_{s}$ contains no more than $N$ arms according to the elimination condition (Line 6). ", "page_idx": 13}, {"type": "text", "text": "We now analyze the second term in Eq. (2). For any arm $a_{j}$ and $s\\;\\in\\;\\{0,...,N-1\\}$ , denote $T_{i,j,s}$ as the value of $T_{i,j}$ at the end of the round $t_{s+1}\\,-\\,1\\$ . For $s~\\geq~1$ and arm $a_{j}~\\in~D_{s}$ , if $T_{i,j,s-1}~\\le~96\\log T/\\Delta_{i,j_{s-1},j}^{2}$ , it must hold that $\\begin{array}{r l r}{T_{i,j,s}\\!}&{{}:=}&{\\!\\sum_{s^{\\prime}\\leq s}(T_{i,j,s^{\\prime}}\\;-\\;T_{i,j,s^{\\prime}-1})\\;\\;\\leq}\\end{array}$ $96\\log T/\\Delta_{i,j_{s},j}^{2}$ to ensure arm $a_{j}$ is eliminated from $A_{i}$ at step $s$ based on Lemma A.5. On the other hand, if $\\bar{T}_{i,j,s-1}>96\\log T/\\Delta_{i,j_{s-1},j}^{2}$ , based on Lemma A.5, it holds that $T_{i,j,s}-T_{i,j,s-1}\\leq$ $96\\log T/\\Delta_{i,j_{s},j}^{2}\\,-\\,96\\log T/\\Delta_{i,j_{s-1},j}^{2}$ when $a_{j}$ is eliminated. For any arm $a_{j}$ , denote $s_{j,1}~:=$ $\\begin{array}{r}{\\operatorname*{max}_{0\\le s\\le N-1}\\left\\{s:T_{i,j,s}\\le96\\log T/\\Delta_{i,j_{s},j}^{2}\\right\\}}\\end{array}$ as the last step when the number of observation times on $a_{j}$ is less than that threshold. Then the second term in Eq. (2) satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{N-1+\\nu-1}\\sum_{s=0}^{\\infty}\\{\\tilde{A}_{s,m,s}(t)=a_{j}\\}\\cdot\\Delta_{i,m;\\varepsilon,j}\\uparrow\\nabla\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{N-1}\\sum_{s=1}^{\\infty}\\frac{t^{s+1-\\nu}}{\\tilde{A}_{s,m}^{2}(t)}\\left\\{\\tilde{A}_{s}^{(t)}=a_{j}\\right\\}\\cdot\\Delta_{i,m;\\varepsilon,j}\\uparrow\\nabla\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\sum_{s=1}^{\\infty}\\sum_{c=0}^{\\infty}\\left(T_{i,j}\\circ T_{i-1,j+1}\\right)\\cdot\\Delta_{i,m;\\varepsilon,j}\\uparrow\\nabla\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\left(\\sum_{s=1}^{\\infty}\\sum_{c=0}^{\\infty}\\left(T_{i,j}\\circ T_{i-1,j+1}\\right)+\\sum_{s=0}^{\\infty}\\sum_{c=0}^{\\infty}(T_{i,j}\\circ T_{i,j-1})\\right)\\cdot\\Delta_{i,m;\\varepsilon,j}\\mid\\nabla\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{s=0}^{\\infty}\\left(T_{i,j+1}\\right)\\cdot\\sum_{s=0}^{\\infty}\\left(T_{i,j}\\circ T_{i-1,j+1}\\right)\\cdot\\Delta_{i,m;\\varepsilon,j}\\mid\\nabla\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{s=0}^{\\infty}\\sum_{s=1}^{\\infty}\\sum_{c=0}^{\\infty}\\left(\\mathrm{Re}_{i,j}\\circ T_{i-1,j+1}\\right)\\cdot\\Delta_{i,m;\\varepsilon,j}\\right)\\mid\\nabla\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left(\\displaystyle\\frac{\\Theta\\log T_{i}}{\\Delta_{i,j+1}}+\\sum_{s=0}^{\\infty}\\left(\\mathrm{Re}_{i,j}\\circ T_{i-1,j}\\circ\\Theta\\log T_{i}/\\Delta_{i,j-1,j}\\right)\\right)\\cdot\\Delta_{i,m;\\varepsilon,j}\\right]}\\end{array},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\leq\\sum_{a_{j}\\in K}\\frac{96\\log T}{\\Delta_{i,m_{i}^{*},j}^{2}}\\cdot\\Delta_{i,m_{i}^{*},j}\\leq96K\\log T/\\Delta\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second last line is due to the definition of $s_{j,1}$ and the above analysis. ", "page_idx": 14}, {"type": "text", "text": "Above all, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\sum_{a_{j}}\\mathbb{1}\\big\\lbrace\\bar{A}_{i}(t)=a_{j}\\big\\rbrace\\cdot\\Delta_{i,m_{i}^{*},j}\\,\\big|^{\\gamma}\\mathcal{F}\\right]\\le\\mathtt{E q}.\\,(2)\\le96N^{2}\\log T/\\Delta^{2}+96K\\log T/\\Delta\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{\\bar{A}_{i}(t)=\\emptyset\\}\\cdot\\mu_{i,m_{i}^{*}}\\mid\\top\\mathcal{F}\\right]\\le192N^{2}\\log T/\\Delta^{2}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Based on the AOGS algorithm, when $\\mathrm{E}_{i}=\\mathrm{True}$ , the central platform would assign arms in $A_{i}$ to player $p_{i}$ in a round-robin manner. Since the number of arms $\\scriptstyle\\left|\\cup_{i:\\mathrm{E}_{i}=\\mathrm{True}}\\mathcal{A}_{i}\\right|$ to be explored is larger than the number $\\begin{array}{r}{\\sum_{i}\\mathbb{1}\\{\\mathrm{E}_{i}=\\mathrm{True}\\}}\\end{array}$ of players with $\\mathrm{E}_{i}=$ True based on the elimination condition in Line 6, we ca n assume that there is no collision in the exploration phase as discussed in Section 4. So the regret caused by collision only occurs during time with $\\mathrm{E}_{i}=\\mathrm{False}$ . ", "page_idx": 14}, {"type": "text", "text": "Denote $\\underline{{t}}_{s}$ and $\\overline{{t}}_{s}$ as the round index when $p_{i}$ sets $\\mathrm{E}_{i}$ as False for the $s$ -th time and as True for the $s+1$ -th time, respectively. Recall that when $\\mathrm{E}_{i}=\\mathrm{False},{p}_{i}$ will always select arm $A_{i}$ . Here we use $j_{s}$ to represent the arm that is selected by $p_{i}$ from time $\\underline{{t}}_{s}$ to $\\bar{t}_{s}$ . ", "page_idx": 14}, {"type": "text", "text": "Further, recall that in the AOGS algorithm, each time an arm is added into $\\mathcal{D}_{i}$ (Line 13), the eliminated arms may be contained into $\\mathcal{A}_{i}$ again. And only when other players focus on their currently most preferred arm, such operation of adding arms to $\\mathcal{D}$ happens. Based on Lemma A.3, such an operation happens for at most $N$ times. For any player $p_{i^{\\prime}}$ , denote $t_{i^{\\prime},r}$ as the round index when $p_{i^{\\prime}}$ adds arms to $\\mathcal{D}_{i^{\\prime}}$ (Line 13) for $r$ -th time. Then $\\left\\{t_{i^{\\prime},r}\\right\\}_{r\\in[N]}$ further divide $\\left\\{\\left[\\underline{{t}}_{s},\\bar{t}_{s}\\right]\\right\\}_{s\\in[N]}$ into at most $2N$ slices. We use $\\underline{{t}}_{s}^{\\prime},\\overline{{t^{\\prime}}}_{s}$ to represent the start round and end round index of the $s$ -th slice, where $s\\,\\in\\,[2N]$ . Based on Lemma A.5, $p_{i^{\\prime}}$ and $p_{i}$ would select the same arm for at most $96\\log T/\\Delta^{2}$ times within each slice. ", "page_idx": 14}, {"type": "text", "text": "Then the regret satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbf{1}\\left\\{\\tilde{A}_{1}(t)=\\tilde{\\Phi}\\right\\}\\cdot\\boldsymbol{\\mu}_{t,m_{t}^{*}}\\left\\lvert\\mathcal{Y}\\right\\rangle\\right]\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{T}\\sum_{t=1}^{1}\\left\\{\\tilde{A}_{t}(t)=\\Phi\\right\\}\\cdot\\boldsymbol{\\mu}_{t,m_{t}^{*}}\\left\\lvert\\mathcal{Y}\\right\\rangle\\right]}\\\\ {=\\mathbb{E}\\left[\\displaystyle\\sum_{s=1}^{N}\\sum_{t=1}^{T}\\sum_{t=1}^{t}\\left\\{\\tilde{A}_{s}(t)=\\Phi,A_{t}(t)=j_{s}\\right\\}\\cdot\\boldsymbol{\\mu}_{t,m_{t}^{*}}\\left\\lvert\\mathcal{Y}\\right\\rangle\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t^{\\prime}=1}^{N}\\sum_{t=1}^{T}\\sum_{t=1}^{T}\\left\\{\\tilde{A}_{s}(t)=A_{t^{\\prime}}(t)-j_{s}\\right\\}\\cdot\\boldsymbol{\\mu}_{t,m_{t^{\\prime}}}\\left\\{\\mathcal{Y}\\right\\}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t^{\\prime}=1}^{T}\\sum_{s=1}^{T}\\sum_{t=1}^{T}\\left\\{\\tilde{A}_{s}(t)=A_{t^{\\prime}}(t)\\cdot\\mu_{t,m_{t^{\\prime}}}\\left\\lvert\\mathcal{Y}\\right\\rangle\\right\\}\\right]}\\\\ {\\leq\\sum_{t=1}^{2N}\\sum_{s=1}^{2N}\\Theta\\log T\\Delta^{2}\\cdot\\boldsymbol{\\mu}_{t,m_{t^{\\prime}}}\\left\\{\\mathcal{Y}\\right\\}}\\\\ {\\leq\\eta\\partial\\mathcal{X}^{2}\\log T\\Delta^{2}\\cdot\\boldsymbol{\\mu}_{t,m_{t^{\\prime}}}}\\\\ {\\leq\\eta\\partial\\mathcal{Y}^{2}\\log T\\Delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.3. In the offline GS algorithm, at most $N-1$ arms have been proposed by players before the algorithm stops. ", "page_idx": 14}, {"type": "text", "text": "Proof. Based on the offline GS algorithm, once an arm is proposed, it has a temporary player. By contradiction, once $N$ arms have been proposed, it means that $N$ players are occupied. In this case, each player has a partner and the algorithm stops. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.4. ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{F}\\right)\\le2N K/T\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\mathcal{F}\\right)=\\mathbb{P}\\left(\\exists1\\leq t\\leq T,i\\in[N],j\\in[K]:|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|>\\sqrt{\\frac{6\\log T}{T_{i,j}(t)}}\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in[N]}\\sum_{j\\in[K]}\\mathbb{P}\\left(|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|>\\sqrt{\\frac{6\\log T}{T_{i,j}(t)}}\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in[N]}\\sum_{j\\in[K]}\\sum_{s=1}^{t}\\mathbb{P}\\left(T_{i,j}(t)=s,|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|>\\sqrt{\\frac{6\\log T}{s}}\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in[N]}\\sum_{j\\in[K]}t\\cdot2\\exp(-3\\ln T)}\\\\ &{\\qquad\\leq2N K/T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second last inequality is due to Lemma B.1. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.5. For any player $p_{i}$ , let $\\bar{T}_{i}=96\\log T/\\Delta^{2}$ . For any two arms $j,j^{\\prime}$ with $\\mu_{i,j}>\\mu_{i,j^{\\prime}}$ and $\\sigma_{i}(a_{j})\\,\\in\\,[1,\\sigma_{i}(m_{i}^{*})]$ , if $T_{i}(t):=\\operatorname*{min}\\left\\{T_{i,j}(t),T_{i,j^{\\prime}}(t)\\right\\}>\\bar{T}_{i},$ , we have $\\mathrm{UCB}_{i,j^{\\prime}}(t)<\\mathrm{LCB}_{i,j}(t)$ conditioned on $\\daleth\\mathcal{F}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. By contradiction, suppose $\\mathrm{UCB}_{i,j^{\\prime}}(t)\\ge\\mathrm{LCB}_{i,j}(t)$ . According to $\\daleth\\mathcal{F}$ and the definition of LCB and UCB, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{i,j}-2\\sqrt{\\frac{6\\log T}{T_{i}(t)}}\\leq\\mathrm{LCB}_{i,j}(t)\\leq\\mathrm{UCB}_{i,j^{\\prime}}(t)\\leq\\mu_{i,j^{\\prime}}+2\\sqrt{\\frac{6\\log T}{T_{i}(t)}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can then conclude $\\begin{array}{r}{\\Delta_{i,j,j^{\\prime}}=\\mu_{i,j}-\\mu_{i,j^{\\prime}}\\,\\leq\\,4\\sqrt{\\frac{6\\log T}{T_{i}(t)}}}\\end{array}$ 6 Tlio(gt )T , which implies that Ti(t) \u2264 9 $\\begin{array}{r}{T_{i}(t)\\,\\le\\,\\frac{96\\log T}{\\Delta_{i,j,j^{\\prime}}^{2}}\\,\\le\\,}\\end{array}$ 96 l\u2206o2g T. This contradicts the fact that Ti(t) > T\u00afi. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.6. Conditioned on $\\daleth\\mathcal{F}$ , at any time $t_{:}$ , $\\mathrm{UCB}_{i,j}(t)<\\mathrm{LCB}_{i,j^{\\prime}}(t)$ implies $\\mu_{i,j}<\\mu_{i,j^{\\prime}}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. According to the definition of LCB and UCB, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{LCB}_{i,j}(t)=\\hat{\\mu}_{i,j}(t)-\\sqrt{\\frac{6\\log T}{T_{i,j}(t)}}\\le\\mu_{i,j}\\le\\hat{\\mu}_{i,j}(t)+\\sqrt{\\frac{6\\log T}{T_{i,j}(t)}}=\\mathrm{UCB}_{i,j}(t)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where two inequalities comes from $\\daleth\\mathcal{F}$ . Thus if $\\mathrm{UCB}_{i,j}(t)<\\mathrm{LCB}_{i,j^{\\prime}}(t)$ , there would be ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{i,j}\\leq\\mathrm{UCB}_{i,j}(t)<\\mathrm{LCB}_{i,j^{\\prime}}(t)\\leq\\mu_{i,j^{\\prime}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The lemma can thus be proved. ", "page_idx": 15}, {"type": "text", "text": "B Technical Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma B.1. (Corollary 5.5 in Lattimore and Szepesv\u00b4ari [18]) Assume that $X_{1},X_{2},\\ldots,X_{n}$ are independent, $\\sigma$ -subgaussian random variables centered around $\\mu$ . Then for any $\\varepsilon>0$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\geq\\mu+\\varepsilon\\right)\\leq\\exp\\left(-\\frac{n\\varepsilon^{2}}{2\\sigma^{2}}\\right)\\,,\\,\\,\\,\\,\\mathbb{P}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\leq\\mu-\\varepsilon\\right)\\leq\\exp\\left(-\\frac{n\\varepsilon^{2}}{2\\sigma^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. (Arrangement of players\u2019 round-robin exploration) Suppose there are $N$ players who need to explore their respective $N$ arms. There exists an assignment such that during $2N$ rounds, each player can match with each of its arm for once. ", "page_idx": 16}, {"type": "text", "text": "Proof. Without loss of generality, let\u2019s assume that players assign their respective $N$ arms in $2N$ rounds one by one, based on the players\u2019 and arms\u2019 indices, aiming to ensure that no arm is assigned to more than one player at the same round. By contradiction, suppose when player $p_{i}$ assigns its $j$ -th arm, there is no available round to make this assignment due to conflicting constraints. Given that player $p_{i}$ is currently assigning the $j$ -th arm, it implies that there are $2N-j+1$ rounds where no arm is assigned to player $p_{i}$ . Since none of these rounds satisfy the conflict constraint, it means that the previous $i-1$ players assigned arm $j$ in these $2N-j+1$ rounds. This creates a contradiction since the first $i-1$ players can only occupy $i-1$ rounds when selecting arm $j$ , where $i-1<2N-j+1$ with $i\\le N,j\\le N$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C Proof of Theorem 6.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we analyze the regret of the centralized-UCB algorithm under $\\alpha$ -condition. Recall that $\\begin{array}{r}{\\mathcal{F}=\\left\\{\\exists1\\leq t\\leq T,i\\in[N],\\dot{j}\\in[K]:|\\hat{\\mu}_{i,j}(t)-\\mu_{i,j}|>\\sqrt{\\frac{6\\log T}{T_{i,j}(t)}}\\right\\}}\\end{array}$ is the failure event that the estimated reward is far from the expected reward at some time and some player-arm pair. ", "page_idx": 16}, {"type": "text", "text": "For any player $p_{i}$ with $i\\in[N]$ , we know that its stable arm is $a_{i}$ under $\\alpha$ -condition. Thus its regret can be decomposed as ", "page_idx": 16}, {"type": "equation", "text": "$$\nR e g_{i}(T)\\leq\\mathbb{E}\\left[\\sum_{k:\\mu_{i,k}<\\mu_{i,i}}\\Delta_{i,i,k}\\sum_{t=1}^{T}\\mathbb{1}\\left\\{\\bar{A}_{i}(t)=k,\\mathbb{1}\\mathcal{F}\\right\\}\\right]+T\\cdot\\mathbb{P}\\left(\\mathcal{F}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first term is the number of selections for sub-optimal arms. The second term is the regret caused by the bad events. ", "page_idx": 16}, {"type": "text", "text": "For the arm $a_{k}$ such that it is sub-optimal for player $p_{i}$ , i.e., $\\mu_{i,k}<\\mu_{i,i}$ , it will be selected because the preference for arm $a_{k}$ of player $p_{i}$ is estimated higher than its stable matched arm $a_{i}$ , or player $p_{i}$ is rejected by arm $a_{k}$ in the GS algorithm. Note that under $\\alpha$ -condition, there is a right order $\\bar{Q}_{i^{\\prime}}\\,=\\,\\dot{\\iota}\\,\\in\\,[N]_{r}$ for player $p_{i}$ , such that $\\forall i^{\\prime}\\,<\\,i^{\\prime\\prime}\\,\\leq\\,N,Q_{i^{\\prime\\prime}}\\,\\in\\,[N]_{r};\\pi_{q_{i^{\\prime}},Q_{i^{\\prime}}}\\,>\\,\\pi_{q_{i^{\\prime}},Q_{i^{\\prime\\prime}}}$ , which means arm $a_{q_{i^{\\prime}}}=a_{i}$ can only prefer players $p_{Q_{1}},p_{Q_{2}},\\cdot\\cdot\\cdot\\,,p_{Q_{i^{\\prime}-1}}$ than player $p_{Q_{i^{\\prime}}}=p_{i}$ . Denote The right-order mapping for $\\alpha$ -condition for player $p_{i}$ is $l r(i)$ so that $Q_{l r(i)}=i$ with $Q_{i}$ defined in Definition 6.1, and $l r(i)\\leq N$ . For player $p_{i}$ , denote $\\mathcal{G}_{t,i}:=\\{\\forall1\\leq i^{\\prime}\\leq l r(i)-1,\\bar{A}_{Q_{i^{\\prime}}}(t)\\neq i\\}$ as the event all players preferred by arm $a_{i}$ do not select $p_{i}$ at time $t$ . Then the number of selections for sub-optimal arm $a_{k}$ can be decomposed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbf{1}\\{\\bar{A}_{(t)}(t)=k,\\uparrow\\mathcal{F}\\}\\right]}\\\\ &{\\displaystyle=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbf{1}\\{\\bar{A}_{(t)}(t)=k,\\mathcal{G}_{t,i},\\uparrow\\mathcal{F}\\}\\right]+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbf{1}\\{\\bar{A}_{(t)}(t)=k,\\mathcal{G}_{t,i},\\uparrow\\mathcal{F}\\}\\right]}\\\\ &{\\displaystyle\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbf{1}\\{\\bar{A}_{(t)}(t)=k,\\,\\mathrm{UCB}_{i,k}(t)>\\mathrm{UCB}_{i,i}(t),\\uparrow\\mathcal{F}\\}\\right]+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbf{1}\\{\\bar{A}_{(t)}(t)=k,\\mathcal{G}_{t,i},\\uparrow\\mathcal{F}\\}\\right]}\\\\ &{\\displaystyle\\leq\\frac{24\\log T}{\\Delta_{i,i,k}^{2}}+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{\\bar{A}_{i}(t)=k,\\mathcal{G}_{t,i},\\uparrow\\mathcal{F}\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last inequality is from Lemma C.1. ", "page_idx": 16}, {"type": "text", "text": "For the second term in the RHS of the last inequality, $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{\\bar{A}_{i}(t)=k,\\top\\mathcal{G}_{t,i},\\top\\mathcal{F}\\}\\right]}\\end{array}$ , we can sum over all sub-optimal arms and it turns out to be ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{k}\\sum_{t=1}^{T}\\mathbb{1}\\big\\{\\bar{A}_{i}(t)=k,\\top\\mathcal{G}_{t,i},\\top\\mathcal{F}\\big\\}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S}\\mathbb{E}\\left[\\frac{\\Gamma}{\\hbar-1}\\mathbf{1}\\{\\mathcal{G}_{\\hbar,\\varepsilon}(\\cdot,\\cdot)\\mathcal{F}_{\\hbar}\\}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\frac{\\hbar(\\theta_{1}-1)-\\Gamma}{\\hbar-1}\\mathbf{1}\\{\\mathcal{G}_{\\hbar,\\varepsilon}(t)=i,\\cdot)\\}\\right]}\\\\ &{\\stackrel{{,}}{\\leq}\\frac{\\hbar^{1/(1)}-1}{\\hbar}\\frac{F(1)}{\\hbar}\\frac{2\\mathrm{i}\\mathbf{1}\\log(\\cdot)}{\\hbar}}\\\\ &{\\stackrel{{,}}{\\leq}\\frac{\\hbar^{1/(1)}-1}{\\hbar}\\frac{F(1)}{\\hbar}\\frac{2\\mathrm{i}\\mathbf{1}\\log(\\cdot)}{\\hbar^{2}-\\hbar}}\\\\ &{\\stackrel{{,}}{\\leq}\\frac{\\hbar^{1/(1)}-1\\hbar^{1/(1)}}{\\hbar}\\frac{2\\mathrm{i}\\mathbf{1}\\log(T)}{\\hbar(\\hbar\\Delta)^{2}}}\\\\ &{\\leq(\\hbar(t)-1)\\left(\\frac{\\hbar(t)-\\nu^{\\varepsilon}}{\\hbar}\\frac{1}{k^{2}}\\right)\\frac{2\\mathrm{i}\\log(T)}{\\hbar\\Delta_{\\hbar}^{2}}}\\\\ &{\\leq(\\hbar(t)-1)\\frac{\\sqrt{\\pi^{1/(1)}-1}}{\\hbar}\\frac{2\\mathrm{i}\\mathbf{1}\\pi^{2}}{\\hbar},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third inequity is from the Lemma C.2. The fourth inequality is from the definition of $\\Delta_{N}$ . Above all, the stable regret of player $i$ can be bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{i}(T)\\leq\\mathbb{E}\\left[\\underset{k:\\mu_{i,k}<\\mu_{i,i}}{\\sum_{k:\\mu_{i,k}<\\mu_{i,i}}}\\Delta_{i,i,k}\\underset{t=1}{\\sum_{t=1}^{T}}1\\{\\bar{A}_{i}(t)=k,\\top\\mathcal{F}\\}\\right]+T\\cdot\\mathbb{P}\\left(\\mathcal{F}\\right)}\\\\ &{\\quad\\quad\\leq\\underset{k:\\mu_{i,k}<\\mu_{i,i}}{\\sum_{k:\\mu_{i,k}<\\mu_{i,i}}}\\Delta_{i,i,k}\\frac{24\\log T}{\\Delta_{i,i,k}^{2}}+\\Delta_{i,i,k}\\left(l(i)-1\\right)\\frac{5\\pi^{2}\\log T}{\\Delta_{N}^{2}}+2N K}\\\\ &{\\quad\\quad\\leq\\frac{24K\\log T}{\\Delta}+\\left(l r(i)-1\\right)\\frac{5\\pi^{2}\\log T}{\\Delta_{N}^{2}}+2N K}\\\\ &{\\quad\\quad\\leq O\\left(\\frac{K\\log T}{\\Delta}+\\frac{N\\log T}{\\Delta_{N}^{2}}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality is based on Lemma A.4. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. Conditioned on $\\daleth\\mathcal{F}$ , under the traditional single-player UCB algorithm with single player $p_{i}$ , the expected number of times at which the UCB index of arm $a_{j^{\\prime}}$ exceeds that of the better arm $a_{j}$ , is at most $24\\log(T)/\\Delta_{i,j,j^{\\prime}}^{2}$ by round $T$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Conditioned on $\\daleth\\mathcal{F}$ , for any $i,j,t$ we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu_{i,j}-\\sqrt{\\frac{6\\log(T)}{T_{i,j}(t-1)}}<\\hat{\\mu}_{i,j}(t-1)<\\mu_{i,j}+\\sqrt{\\frac{6\\log(T)}{T_{i,j}(t-1)}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall that the UCB index is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{UCB}_{i,j}(t)=\\hat{\\mu}_{i,j}(t-1)+\\sqrt{\\frac{6\\log(T)}{T_{i,j}(t-1)}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The event that arm $a_{j^{\\prime}}$ is successfully selected for player $p_{i}$ rather than the better arm $a_{j}$ at time $t$ implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{UCB}_{i,j^{\\prime}}(t)>\\mathrm{UCB}_{i,j}(t)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{i,j^{\\prime}}+2\\sqrt{\\frac{6\\log(T)}{T_{i,j^{\\prime}}(t-1)}}\\stackrel{(6\\,a)}{>}\\hat{\\mu}_{i,j^{\\prime}}(t-1)+\\sqrt{\\frac{6\\log(T)}{T_{i,j^{\\prime}}(t-1)}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\stackrel{(6c)}{>}\\hat{\\mu}_{i,j}(t-1)+\\sqrt{\\frac{6\\log(T)}{T_{i,j}(t-1)}}}\\\\ &{\\qquad\\qquad\\qquad\\quad>\\mu_{i,j}-\\sqrt{\\frac{6\\log(T)}{T_{i,j}(t-1)}}+\\sqrt{\\frac{6\\log(T)}{T_{i,j}(t-1)}}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mu_{i,j}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{i,j^{\\prime}}(t-1)<\\frac{24\\log(T)}{\\Delta_{i,j,j^{\\prime}}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Delta_{i,j,j^{\\prime}}$ is the reward difference between the $\\mu_{i,j^{\\prime}}$ and $\\mu_{i,j}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. For any player $p_{i}$ with right order $Q_{l r(i)}$ , the following inequality holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{i^{\\prime}=1}{\\overset{\\lceil r(i)-1}{\\sum}}\\underset{t=1}{\\overset{T}{\\sum}}1\\!\\left\\{\\bar{A}_{Q_{i^{\\prime}}}(t)=i,\\top\\!\\mathcal{F}\\right\\}\\right]}\\\\ &{\\leq\\!\\mathbb{E}\\left[\\underset{w^{\\prime}=1}{\\overset{\\lceil r(i)-1}{\\sum}}\\underset{u^{\\prime\\prime}=u^{\\prime}+1}{\\overset{\\lfloor r(i)}{\\sum}}\\underset{t=1}{\\overset{T}{\\sum}}1\\!\\left\\{\\bar{A}_{Q_{u^{\\prime}}}(t)=q_{u^{\\prime\\prime}},\\mathcal{G}_{t,u^{\\prime}},{\\top\\!\\mathcal{F}}\\right\\}\\right]}\\\\ &{\\overset{l r(i)\\!\\!-\\!1}{\\leq}\\underset{u^{\\prime}=1}{\\overset{l r(i)-1}{\\sum}}\\underset{u^{\\prime\\prime}=u^{\\prime}+1}{\\overset{24}{\\sum}}\\frac{24\\log T}{\\Delta_{Q_{u^{\\prime}},q_{u^{\\prime}},q_{u^{\\prime\\prime}}}^{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For player $p_{i}$ with right order $Q_{l r(i)}$ , from $\\alpha$ -condition we have that its stable matched arm ai may prefer pQ1, pQ2, \u00b7 \u00b7 \u00b7 , pQlr(i)\u22121 than player $p_{Q_{l r(i)}}$ . For any $i^{\\prime}\\,<\\,l r(i)$ , we know that the number of times player $p_{Q_{i^{\\prime}}}$ selects arm $a_{i}$ is decomposed as by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{\\bar{A}_{Q_{i^{\\prime}}}(t)=i,\\mathbb{1}_{\\mathcal{F}}\\}\\right]}\\\\ &{\\displaystyle=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{\\bar{A}_{Q_{i^{\\prime}}}(t)=i,\\mathcal{G}_{t,i^{\\prime}},\\mathbb{1}_{\\mathcal{F}}\\}\\right]+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{\\bar{A}_{Q_{i^{\\prime}}}(t)=i,\\mathbb{1}_{\\mathcal{G}_{t,i^{\\prime}}},\\mathbb{1}_{\\mathcal{F}}\\}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The event $\\daleth_{t,i^{\\prime}}$ implies that there exists another player $p_{Q_{i^{\\prime\\prime}}}$ with $i^{\\prime\\prime}<i^{\\prime}$ that selects the stable arm of $p_{Q_{i^{\\prime}}}$ . This leads to a recursion form. But it is easy to verify that every event $\\daleth_{t,i^{\\prime}}$ happens only when there exists two players $p_{Q_{u^{\\prime}}},p_{Q_{u^{\\prime\\prime}}}$ with $u^{\\prime}<\\dot{u}^{\\prime\\prime}\\leq l r(\\dot{\\iota}^{\\prime})$ , such that player $p_{Q_{u^{\\prime}}}$ explores the stable matched arm $\\boldsymbol{a}_{\\boldsymbol{q}_{u^{\\prime\\prime}}}$ of $p_{Q_{u^{\\prime\\prime}}}$ , i.e., $p_{Q_{u^{\\prime}}}$ selects $\\boldsymbol{a}_{\\boldsymbol{q}_{u^{\\prime\\prime}}}$ conditioned on $\\mathcal{G}_{t,u^{\\prime}}$ . And thus it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{i^{\\prime}=1}{\\overset{l-(i)-1}{\\sum}}\\underset{t=1}{\\overset{T}{\\sum}}1\\big\\{\\bar{A}_{Q_{i^{\\prime}}}(t)=i,\\top\\mathcal{F}\\big\\}\\right]}\\\\ &{\\leq\\!\\mathbb{E}\\left[\\underset{u^{\\prime}=1}{\\overset{l-(i)-1}{\\sum}}\\underset{u^{\\prime\\prime}=u^{\\prime}+1}{\\overset{l-(i)}{\\sum}}\\underset{t=1}{\\overset{T}{\\sum}}1\\big\\{\\bar{A}_{Q_{u^{\\prime}}}(t)=q_{u^{\\prime\\prime}},\\mathcal{G}_{t,u^{\\prime}},\\top\\mathcal{F}\\big\\}\\right]}\\\\ &{\\overset{l-(i)-1}{\\leq}\\underset{u^{\\prime}=1}{\\overset{l-(i)}{\\sum}}\\underset{u^{\\prime\\prime}=u^{\\prime}+1}{\\overset{24}{\\sum}}\\frac{24\\log T}{\\Delta_{Q_{u^{\\prime}},q_{u^{\\prime}},q_{u^{\\prime\\prime}}}^{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our abstract and introduction clearly describe the scope and outline our main contributions. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Section 7 discusses the limitation of this paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide techniques clearly in the paper, and their detailed proofs are in Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Section 5 provides the settings and results of our experiments carefully. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 20}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The codes are uploaded in supplementary material. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Section 5 provides the settings and results of our experiments carefully. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the error bar in the results of experiments in Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not provide specific information on the computer resources used, as most experiments on multi-armed bandits are lightweight. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research conducted in the paper complies with NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]