[{"figure_path": "pH3XAQME6c/tables/tables_2_1.jpg", "caption": "Table 1: Model families, sizes, alignment training type, and references.", "description": "This table lists the different families of large language models (LLMs) used in the study, their sizes (in billions of parameters), the type of alignment training used (AFT: aligned by fine-tuning, APO: aligned by preference optimization), and the corresponding references to the papers where these models were introduced.", "section": "2 Methodology"}, {"figure_path": "pH3XAQME6c/tables/tables_5_1.jpg", "caption": "Table 2: HARMBENCH attack success rate (ASR) across various jailbreaking methods. Our method is labeled as ORTHO. The baseline \"direct response\" rate with no jailbreak applied is labeled as DR. We differentiate general jailbreaks, which are applied across all prompts generically, from prompt-specific jailbreaks, which are optimized for each prompt individually. All evaluations use the model's default system prompt. We also report ASR without system prompt in blue.", "description": "This table compares the success rate of different jailbreaking methods on several language models. The success rate is measured by the proportion of times a method successfully bypasses the model's safety mechanisms. The methods are categorized into general methods (applied to all prompts) and prompt-specific methods (optimized for each prompt). The table shows that the proposed \"ORTHO\" method performs comparably to other state-of-the-art techniques, especially for the QWEN models.  The results with and without the default system prompt are shown, highlighting the impact of system prompts on the effectiveness of different methods.", "section": "4.2 Comparison to other jailbreaks"}, {"figure_path": "pH3XAQME6c/tables/tables_6_1.jpg", "caption": "Table 3: Model evaluations. For each evaluation, we report the orthogonalized model's performance, followed by the baseline model's performance, followed by the absolute increase or decrease. We display the largest model from each model family. Full results are reported in \u00a7G.1.", "description": "This table presents the results of four common language model evaluations (MMLU, ARC, GSM8K, TRUTHFULQA) for five large language models (LLaMA-2 70B, LLaMA-3 70B, QWEN 72B, YI 34B, GEMMA 7B).  For each model, the table shows the performance of both the original model and its orthogonalized version (where the \"refusal direction\" has been removed). The difference between the original model and the orthogonalized model's performance is shown for each evaluation metric.  This allows for a comparison of the effect of orthogonalization on general model capabilities.", "section": "Measuring model coherence"}, {"figure_path": "pH3XAQME6c/tables/tables_17_1.jpg", "caption": "Table 5: Direction selection details for each model. Note that i* = -1 indicates that the direction is selected from the last token position, i* = -2 the second-to-last token position, and so on. Also note that the layer index l* starts from index 0, while L indicates the total number of layers.", "description": "This table shows the results of the direction selection algorithm for each of the 13 models studied in the paper. For each model, it indicates the layer and token position from which the best refusal direction was extracted, and its corresponding metrics (bypass_score, induce_score, and kl_score).  The metrics quantify how effective the direction is at bypassing or inducing refusal while minimizing other behavioral changes.", "section": "2.4 Model interventions"}, {"figure_path": "pH3XAQME6c/tables/tables_19_1.jpg", "caption": "Table 4: The refusal token set R that we use for each model family, along with the refusal phrases corresponding to each token.", "description": "This table shows the set of tokens (R) that are most likely to start a refusal response for each family of language models considered in the paper.  For each model family, there is a corresponding set of refusal phrases associated with those tokens.  This information is useful to understand the methodology for creating an efficient proxy for measuring refusal, which uses the probability of these refusal tokens occurring to estimate the likelihood of a model refusing a particular instruction.", "section": "Refusal metric: an efficient proxy for measuring refusal"}, {"figure_path": "pH3XAQME6c/tables/tables_21_1.jpg", "caption": "Table 5: Direction selection details for each model. Note that i* = -1 indicates that the direction is selected from the last token position, i* = -2 the second-to-last token position, and so on. Also note that the layer index l* starts from index 0, while L indicates the total number of layers.", "description": "This table shows the results of the direction selection algorithm for each of the 13 models evaluated in the study.  For each model, it indicates the layer (l*) and token position (i*) from which the best refusal direction was selected.  It also provides the bypass score, induce score, and KL score for that direction.  The bypass score is a measure of how effective the selected direction is at bypassing refusal when ablated from the model. The induce score is a measure of how effective the direction is at inducing refusal when added to the model's activations. The KL score measures the difference between the probability distribution of model outputs for the validation set with and without directional ablation for that specific direction.  The lower the bypass score and higher the induce score, the better the selected direction is at mediating refusal behavior.", "section": "2.4 Model interventions"}, {"figure_path": "pH3XAQME6c/tables/tables_21_2.jpg", "caption": "Table 6: Model families and their corresponding chat templates. The user instruction is denoted as {x}. Post-instruction tokens, as defined in \u00a72.1, are labeled in red.", "description": "This table shows the chat templates used for each model family in the study.  Each template includes placeholders for user input and model response. The user instruction is represented as {x}, while post-instruction tokens, which are all tokens after the user instruction within the template, are highlighted in red.  These templates provide structure for the interactions between the user and the language model.", "section": "C.3 Chat templates"}, {"figure_path": "pH3XAQME6c/tables/tables_27_1.jpg", "caption": "Table 2: HARMBENCH attack success rate (ASR) across various jailbreaking methods. Our method is labeled as ORTHO. The baseline \"direct response\" rate with no jailbreak applied is labeled as DR. We differentiate general jailbreaks, which are applied across all prompts generically, from prompt-specific jailbreaks, which are optimized for each prompt individually. All evaluations use the model's default system prompt. We also report ASR without system prompt in blue.", "description": "This table compares the success rate of different jailbreaking methods on the HARMBENCH benchmark.  The methods include the authors' method (ORTHO), various other general jailbreaking approaches (GCG-M, GCG-T, HUMAN, AP, PAIR), and a baseline (DR) with no jailbreak.  The table shows the attack success rate (ASR) for each method, both with and without the model's default system prompt.  The results highlight the effectiveness of different methods and their sensitivity to the presence of system prompts.", "section": "Comparison to other jailbreaks"}, {"figure_path": "pH3XAQME6c/tables/tables_27_2.jpg", "caption": "Table 5: Direction selection details for each model. Note that i* = -1 indicates that the direction is selected from the last token position, i* = -2 the second-to-last token position, and so on. Also note that the layer index l* starts from index 0, while L indicates the total number of layers.", "description": "This table shows the details of the refusal direction selection process for each of the 13 language models studied. It indicates, for each model, the layer (l*) and token position (i*) where the direction was identified, and provides the values of the three metrics used for direction selection: bypass_score, induce_score, and KL_score.  These metrics help to evaluate the effectiveness of the selected direction in bypassing refusal, inducing refusal, and minimizing changes in model behavior on harmless inputs.", "section": "2.4 Model interventions"}, {"figure_path": "pH3XAQME6c/tables/tables_28_1.jpg", "caption": "Table 8: Model evaluations. For each evaluation, we report the orthogonalized model's performance, followed by the baseline model's performance, followed by the absolute increase or decrease.", "description": "This table presents the results of four different language model evaluations (MMLU, TINYHELLASWAG, ARC, WINOGRANDE, GSM8K, TRUTHFULQA) for 13 different models, comparing their performance before and after applying the weight orthogonalization technique described in the paper. For each model and each evaluation metric, the table shows three values: the performance of the orthogonalized model, the performance of the original model, and the difference between the two. This allows for a direct comparison of how the orthogonalization affects the model's overall capabilities, assessing whether any significant performance degradation or improvement occurs as a result of this modification.", "section": "G Model coherence evaluation"}, {"figure_path": "pH3XAQME6c/tables/tables_29_1.jpg", "caption": "Table 9: Model performance as measured by CE loss across different datasets.", "description": "This table presents the cross-entropy (CE) loss values for various language models, categorized by model family and dataset.  The CE loss is a measure of how well the model's predictions match the actual data. It is calculated for three different datasets: THE PILE, ALPACA, and an on-distribution subset of ALPACA. For each model, three loss values are provided: one for the baseline model, one for the model after directional ablation of the \"refusal direction\", and one after activation addition of the same direction. These results shed light on how different interventions impact model performance, especially in terms of harmfulness and coherence.", "section": "G.3 CE loss evaluation"}, {"figure_path": "pH3XAQME6c/tables/tables_30_1.jpg", "caption": "Table 5: Direction selection details for each model. Note that i* = -1 indicates that the direction is selected from the last token position, i* = -2 the second-to-last token position, and so on. Also note that the layer index l* starts from index 0, while L indicates the total number of layers.", "description": "This table shows the results of the direction selection algorithm for each of the thirteen models studied. The columns are:\n- Model: Name of the model.\n- i*: Index of the token in the input sequence where the direction was selected. A value of -1 indicates that the direction was selected from the last token, -2 indicates the second to last token, and so on.\n- l*/L: Layer index of the direction, divided by the total number of layers in the model. This shows the relative position of the layer in the model.\n- bypass_score: Average refusal metric across the validation set of harmful instructions, after ablating the selected direction.\n- induce_score: Average refusal metric across the validation set of harmless instructions, after adding the selected direction.\n- kl_score: Average Kullback-Leibler divergence between probability distributions at the last token position, with and without directional ablation of the direction. This metric shows the change in the model's behavior after ablating the direction.", "section": "2.4 Model interventions"}, {"figure_path": "pH3XAQME6c/tables/tables_32_1.jpg", "caption": "Table 10: Refusal and CE loss evaluation metrics for LLAMA-3 8B INSTRUCT, comparing the interventions of directional ablation, activation addition, and fine-tuning.", "description": "This table presents a comparison of three different methods for mitigating the refusal behavior of a large language model (LLM): directional ablation, activation addition, and fine-tuning.  The metrics used for comparison are the refusal score, safety score, and cross-entropy (CE) loss calculated across three different datasets (THE PILE, ALPACA, and on-distribution).  The refusal score represents the proportion of times the model refused to generate a response.  The safety score represents the proportion of times the model's response was deemed safe. The CE loss measures the difference between the model's predicted probability distribution and the true distribution of the next token in a sequence. By comparing these metrics across the three intervention methods, the table helps to assess the effectiveness and potential side effects of each approach in controlling the model's behavior.", "section": "I.2 Comparison to fine-tuning"}, {"figure_path": "pH3XAQME6c/tables/tables_39_1.jpg", "caption": "Table 1: Model families, sizes, alignment training type, and references.", "description": "This table lists the thirteen large language models used in the study, categorized by their model family (e.g., Qwen Chat, Yi Chat), their sizes (in terms of parameters), their alignment training type (either aligned by preference optimization - APO - or aligned by fine-tuning - AFT), and the references for each model.  The table shows the diversity of models considered, with sizes ranging from 1.8B parameters to 72B parameters, to demonstrate the generality of the findings across different models.", "section": "2 Methodology"}, {"figure_path": "pH3XAQME6c/tables/tables_39_2.jpg", "caption": "Table 1: Model families, sizes, alignment training type, and references.", "description": "This table lists the 13 large language models used in the study.  It shows their model family name, the number of parameters (sizes), the method used for alignment (Alignment type - either Preference Optimization (APO) or Fine-tuning (AFT)), and the bibliographic reference for each model.", "section": "2 Methodology"}]