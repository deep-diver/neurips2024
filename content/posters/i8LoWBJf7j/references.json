{"references": [{"fullname_first_author": "Dzmitry Bahdanau", "paper_title": "Neural machine translation by jointly learning to align and translate", "publication_date": "2014-09-04", "reason": "This paper introduces the attention mechanism, a fundamental concept that significantly influences the design of modern transformers, including the ones discussed in this paper."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduces the transformer architecture, which is the basis for many state-of-the-art models in various sequence-to-sequence tasks and is directly relevant to the transformer-like networks explored in this paper."}, {"fullname_first_author": "Karol Gregor", "paper_title": "Learning fast approximations of sparse coding", "publication_date": "2010-00-00", "reason": "This paper introduces Learned ISTA, a fundamental algorithm unrolling approach that inspires the unrolling technique used in this paper to build interpretable neural networks."}, {"fullname_first_author": "Yaodong Yu", "paper_title": "White-box transformers via sparse rate reduction", "publication_date": "2023-00-00", "reason": "This paper demonstrates the creation of interpretable transformer-like networks through algorithm unrolling, a key methodology that directly informs the approach in this paper."}, {"fullname_first_author": "D. I. Shuman", "paper_title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "publication_date": "2013-05-00", "reason": "This foundational paper introduces graph signal processing (GSP), which underpins the graph-based methods for building lightweight transformers developed in this work."}]}