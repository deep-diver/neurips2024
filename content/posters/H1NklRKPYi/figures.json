[{"figure_path": "H1NklRKPYi/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of our LCM and Transformer in terms of performance and efficiency.", "description": "This figure compares the proposed LCM model with the traditional Transformer-based model in terms of accuracy and computational complexity.  Subfigure (a) shows a comparison of the overall accuracy on ScanObjectNN against the number of model parameters (in millions). The LCM model achieves comparable accuracy with significantly fewer parameters than the Transformer model. Subfigure (b) illustrates the complexity growth curve as the input sequence length increases.  The Transformer displays quadratic complexity (O(N\u00b2)), while the LCM demonstrates linear complexity (O(N)), highlighting its superior efficiency.", "section": "1 Introduction"}, {"figure_path": "H1NklRKPYi/figures/figures_2_1.jpg", "caption": "Figure 2: The effect of using top-K attention in feature space and geometric space by the Transformer on the classification performance in ScanObjectNN, all results are the averages of ten repeated experiments.", "description": "This figure shows the impact of using top-K attention on the classification performance in ScanObjectNN dataset. The experiment is conducted using different top-K values in both feature and geometric space, and the results are averaged over 10 repeated experiments. The results indicate that using top-K attention in a static geometric space yields nearly identical representational capacity and offers the advantage of a smaller K value, especially when compared with dynamic feature space. ", "section": "3.1 Observation of Top-K Attention"}, {"figure_path": "H1NklRKPYi/figures/figures_2_2.jpg", "caption": "Figure 3: Point heatmap.", "description": "This figure shows heatmaps highlighting the importance of different points within point clouds representing an airplane and a vase.  The color intensity indicates the importance of each point for object recognition, with darker green representing higher importance. For the airplane, the wings are highlighted as highly important, while for the vase, the base is the most important area. This visual representation helps illustrate the concept of varying information density across point clouds, highlighting that not all points carry equal significance for object recognition tasks, a crucial observation that underpins the design of the Locally Constrained Compact Model (LCM).", "section": "3.1 Observation of Top-K Attention"}, {"figure_path": "H1NklRKPYi/figures/figures_3_1.jpg", "caption": "Figure 4: The pipeline of our Locally Constrained Compact Model (LCM) with Point-MAE pre-training. Our LCM consists of a locally constrained compact encoder and a locally constrained Mamba-based decoder.", "description": "This figure illustrates the architecture of the Locally Constrained Compact Model (LCM) for masked point modeling.  The LCM comprises two main parts: a locally constrained compact encoder and a locally constrained Mamba-based decoder. The encoder takes unmasked point cloud patches as input and uses local aggregation layers instead of self-attention to reduce computational complexity. The decoder receives both unmasked and masked tokens, utilizing a Mamba-based architecture to reconstruct the masked tokens by leveraging the information density differences between masked and unmasked patches and incorporating local geometric constraints.  The overall process involves patching, masking, embedding, encoding, decoding, and finally reconstruction based on a reconstruction loss function.", "section": "3.2 The Pipeline of Masking Point Modeling with LCM"}, {"figure_path": "H1NklRKPYi/figures/figures_4_1.jpg", "caption": "Figure 5: The structure of i-th locally constrained compact encoder layer (a) and i-th locally constrained Mamba-based decoder layer (b).", "description": "This figure shows the detailed architecture of the i-th layer of both the encoder and decoder used in the LCM model.  The encoder layer (a) consists of a local aggregation layer (LAL) incorporating KNN for local geometric information gathering, followed by MLPs for feature mapping and a feed-forward network (FFN). The decoder layer (b) combines a Mamba-based SSM layer for capturing temporal dependencies and a local constraints feedforward network (LCFFN) to exchange information among geometrically adjacent patches, effectively improving the accuracy while minimizing computation cost.", "section": "3.3 Locally Constrained Compact Encoder\n3.4 Locally Constrained Mamba-based Decoder"}, {"figure_path": "H1NklRKPYi/figures/figures_14_1.jpg", "caption": "Figure 6: A simple illustration of information processing of MPM decoder.", "description": "This figure illustrates the information processing in the decoder of a masked point modeling (MPM) model.  The input consists of two parts: X1 (unmasked patches with higher information density) and X2 (randomly initialized masked patches with lower information density). The decoder processes these inputs, producing outputs Y1 (for unmasked patches) and Y2 (for masked patches).  The goal is to reconstruct the masked points (X2) using the information from Y2, which ideally should utilize geometric priors from both X1 and X2. The figure visually represents the flow of information through the decoder.", "section": "5.1 An Information Theoretic Perspective of Our Mamba-based Decoder for MPM"}, {"figure_path": "H1NklRKPYi/figures/figures_17_1.jpg", "caption": "Figure 7: Effects of locally constrained K value.", "description": "This figure shows the relationship between the number of nearest neighbors (K) used in local aggregation and the classification accuracy on the ScanObjectNN dataset.  As K increases, the model considers more neighbors, potentially capturing more local geometric information, leading to increased accuracy initially. However, after reaching a peak, further increasing K leads to reduced accuracy, likely due to the introduction of redundant or less relevant information from distant neighbors.  The area of the circles represents the FLOPS (floating point operations), indicating the computational cost associated with each K value.", "section": "4.3 Ablation Study"}, {"figure_path": "H1NklRKPYi/figures/figures_18_1.jpg", "caption": "Figure 8: Training and testing curves for different encoders trained from scratch. We present the training and testing curves for both the classification task on ScanObjectNN and the detection task on ScanNetV2. All encoders were not pretrained.", "description": "This figure compares the training and testing curves of different encoders (Mamba with and without LCFFN, and 4x Mamba) for classification (ScanObjectNN) and detection (ScanNetV2) tasks.  The results show that the order of the input sequences significantly affects the Mamba-based model's performance, while the addition of LCFFN mitigates this order dependence and improves efficiency.  The figure highlights the trade-off between model performance and computational cost associated with different sequence ordering strategies.", "section": "4.3 Ablation Study"}, {"figure_path": "H1NklRKPYi/figures/figures_19_1.jpg", "caption": "Figure 9: Training and testing curves of different pre-trained encoders. We present the training and testing curves for both the classification task on ScanObjectNN and the detection task on ScanNetV2. All encoders are pre-trained.", "description": "This figure compares the training and testing curves of a compact encoder and a transformer encoder, both pre-trained.  The results show that while the transformer encoder achieves higher accuracy during training, the compact encoder exhibits superior performance during testing, indicating better generalization and less overfitting.", "section": "5.5 Additional Visualization"}, {"figure_path": "H1NklRKPYi/figures/figures_20_1.jpg", "caption": "Figure 9: Training and testing curves of different pre-trained encoders. We present the training and testing curves for both the classification task on ScanObjectNN and the detection task on ScanNetV2. All encoders are pre-trained.", "description": "This figure shows a comparison of the training and testing curves for a compact encoder and a transformer encoder, both pre-trained on the same dataset.  The left plot displays the classification accuracy on the ScanObjectNN dataset, while the right plot shows the average precision at 25% Intersection over Union (AP25) on the ScanNetV2 dataset. The results indicate that while the transformer encoder performs better during training, suggesting a potential overfitting to the training data, the compact encoder achieves superior performance during testing, highlighting its better generalization ability.", "section": "Additional Visualization"}, {"figure_path": "H1NklRKPYi/figures/figures_20_2.jpg", "caption": "Figure 11: The feature distribution visualization of the pre-trained models on the test set of ModelNet40.", "description": "This figure visualizes the feature distributions extracted by the LCM and Transformer models pretrained using Point-MAE, directly transferred to the test set of ModelNet40 without downstream finetuning. It uses t-SNE to reduce the dimensionality of the features to 2D for visualization. Each dot represents an instance from ModelNet40, and the color indicates its category. The compactness of the clusters shows the model's ability to represent features of the same category.", "section": "5.5 Additional Visualization"}]