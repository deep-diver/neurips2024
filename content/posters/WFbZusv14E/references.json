{"references": [{"fullname_first_author": "O. J. Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023", "reason": "This paper is a technical report on GPT-4, a large language model that is highly relevant to the study of planning capabilities in LLMs."}, {"fullname_first_author": "Z. Allen-Zhu", "paper_title": "Physics of language models: Part 1, context-free grammar", "publication_date": "2023", "reason": "This paper is part of a series that provides a theoretical understanding of the underlying mechanisms of LLMs, which is crucial for understanding planning capabilities."}, {"fullname_first_author": "Z. Allen-Zhu", "paper_title": "Physics of language models: Part 3.1, knowledge storage and extraction", "publication_date": "2023", "reason": "This paper explores the knowledge storage and extraction capabilities of LLMs, which are essential for the planning process."}, {"fullname_first_author": "Z. Allen-Zhu", "paper_title": "Physics of language models: Part 3.2, knowledge manipulation", "publication_date": "2023", "reason": "This paper delves into the knowledge manipulation abilities of LLMs, critical for complex planning tasks."}, {"fullname_first_author": "Z. Allen-Zhu", "paper_title": "Physics of language models: Part 3.3, knowledge capacity scaling laws", "publication_date": "2024", "reason": "This paper investigates the scaling laws of LLMs' knowledge capacity, providing insight into the relationship between model size and planning ability."}]}