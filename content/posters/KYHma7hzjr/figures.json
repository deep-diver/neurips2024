[{"figure_path": "KYHma7hzjr/figures/figures_1_1.jpg", "caption": "Figure 1: A simplified, intuitive example: an image of a grizzly bear is wrongly identified as an otter. Our method allows performing a concept-based intervention and flip the predicted class. In order of appearance from left to right and top to bottom, the depicted concepts and classes are \u201cfierce\u201d, \u201ctimid\u201d, \u201cmuscle\u201d, \u201cwalks\u201d, \u201cotter\u201d, and \u201cgrizzly bear\u201d.", "description": "The figure shows an example of how the proposed method can be used to correct a misclassification by intervening on the predicted concept values.  The initial prediction is for \"otter\", while the actual image is a grizzly bear. By changing the values of concepts such as \"fierce\", \"timid\", \"muscle\", and \"walks\", the model's prediction is changed to \"grizzly bear\". This demonstrates the model's ability to perform instance-specific interventions and illustrates the basic idea behind the concept-based intervention method.", "section": "Contributions"}, {"figure_path": "KYHma7hzjr/figures/figures_2_1.jpg", "caption": "Figure 2: Three steps of the intervention procedure. (i) A probe qe is trained to predict the concepts c from the activation vector z. (ii) The representations are edited according to Equation 1. (iii) The final prediction is updated to \u0177' based on the edited representations z'.", "description": "This figure illustrates the three steps involved in the intervention procedure.  First, a probe is trained to predict concepts from the model's internal representation (activation vector).  Second, the representation is edited to align with the desired concept values using a distance function and a hyperparameter that balances the tradeoff between the intervention's validity and its proximity to the original representation. Third, this edited representation is used to update the model's final prediction. The figure uses diagrams to represent the input, internal representation, and output of the model.", "section": "3 Methods"}, {"figure_path": "KYHma7hzjr/figures/figures_6_1.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure displays the Area Under the ROC Curve (AUROC) for different methods on synthetic bottleneck data, comparing how well interventions work when changing different numbers of concept values.  It shows the impact of the validation set size (Nval) on the performance of several methods including black-box models, concept bottleneck models (CBM), and variations of fine-tuned models.  The plots show the median AUROC across multiple trials, with error bars representing the interquartile range.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_7_1.jpg", "caption": "Figure 4: Intervention results on the (a) synthetic incomplete, (b) AwA2, (c) CIFAR-10, and (d) MIMIC-CXR datasets w.r.t. target AUROC (top) and AUPR (bottom) across ten seeds.", "description": "This figure displays the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall curve (AUPR) for four different datasets (synthetic incomplete, AwA2, CIFAR-10, and MIMIC-CXR).  Each plot shows the performance of different intervention methods (Black box, CBM, Post hoc CBM, Fine-tuned, MT, Fine-tuned, A, and Fine-tuned, I) as the percentage of concepts intervened upon increases. The top row shows AUROC, while the bottom row shows AUPR.  Each data point represents an average across 10 runs. This figure demonstrates the effectiveness of the proposed fine-tuning for intervenability method compared to other baseline methods in various datasets.", "section": "Results"}, {"figure_path": "KYHma7hzjr/figures/figures_14_1.jpg", "caption": "Figure 2: Three steps of the intervention procedure. (i) A probe q\u03b8 is trained to predict the concepts c from the activation vector z. (ii) The representations are edited according to Equation 1. (iii) The final prediction is updated to \u0177\u2032 based on the edited representations z\u2032.", "description": "This figure illustrates the three steps involved in the intervention procedure used in the paper. First, a probe is trained to predict concepts from the activation vector of a neural network. Then, the representation is edited to align with the desired concept values, using a method that balances similarity to the original representation and consistency with the desired concept. Finally, the model's output is updated based on the edited representation.", "section": "3 Methods"}, {"figure_path": "KYHma7hzjr/figures/figures_14_2.jpg", "caption": "Figure A.2: A model \u2018correction\u2019 example using concept-based instance-specific interventions on the AwA2 dataset. The black-box neural network wrongly predicts that the image depicts an otter. Using our technique, we intervene on the network\u2019s representation and flip the final prediction to the correct class.", "description": "The figure shows an example of how the proposed intervention method can correct a misclassification by a black-box model.  The model initially misclassifies a grizzly bear as an otter. The user then intervenes by modifying the predicted concept values (e.g., changing 'flippers' from 0.98 to 0.00 and 'strong' from 0.24 to 1.00). This intervention alters the model's internal representation, resulting in a corrected prediction of 'grizzly bear'. The example highlights the ability to correct errors in black-box models using instance-specific concept-based interventions.", "section": "A Motivation & Intuitive Examples"}, {"figure_path": "KYHma7hzjr/figures/figures_15_1.jpg", "caption": "Figure A.3: A model \u2018steering\u2019 on the AwA2 dataset. By editing the predicted concepts and applying our method, we can manipulate the model into wrongly predicting that the image depicts a polar bear instead of a grizzly bear.", "description": "This figure shows an example of how the proposed intervention method can be used to influence the model's prediction by editing the predicted concept values. The original image is a grizzly bear, and the model initially predicts it as a grizzly bear. However, by changing some of the predicted concept values (e.g., changing \"black\" to \"white\", \"swims\" to 1.00, etc.), the model's prediction is changed to polar bear. This demonstrates the ability of the proposed method to steer the model's predictions by manipulating its high-level attributes.", "section": "A Motivation & Intuitive Examples"}, {"figure_path": "KYHma7hzjr/figures/figures_16_1.jpg", "caption": "Figure 2: Three steps of the intervention procedure. (i) A probe qe is trained to predict the concepts c from the activation vector z. (ii) The representations are edited according to Equation 1. (iii) The final prediction is updated to \u0177' based on the edited representations z'.", "description": "This figure shows the three steps of the intervention procedure applied to a black-box model. First, a probe is trained to predict concepts from the activation vector. Second, the representations are edited to align with the desired concept values. Finally, the prediction is updated based on these edited representations. This procedure allows for instance-specific interventions by modifying the model's internal representations to reflect the user's input, thereby changing the model's output.", "section": "3 Methods"}, {"figure_path": "KYHma7hzjr/figures/figures_19_1.jpg", "caption": "Figure 2: Three steps of the intervention procedure. (i) A probe qe is trained to predict the concepts c from the activation vector z. (ii) The representations are edited according to Equation 1. (iii) The final prediction is updated to \u0177' based on the edited representations z'.", "description": "This figure illustrates the three steps involved in the intervention procedure. Firstly, a probe is trained to predict concepts from the activation vector. Secondly, the representation is edited to align with the desired concept values. Lastly, the final prediction is updated based on the edited representation. This process allows for concept-based instance-specific interventions on black-box models.", "section": "3 Methods"}, {"figure_path": "KYHma7hzjr/figures/figures_25_1.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure shows the results of intervention experiments on synthetic data with different validation set sizes.  It compares the performance of several methods, including CBMs (trained on the full and validation datasets), a post-hoc CBM, and black-box models (with various fine-tuning strategies) in terms of Area Under the Receiver Operating Characteristic (AUROC).  The x-axis represents the percentage of concepts intervened on, and the y-axis represents the AUROC. The plot displays the median AUROC and interquartile range (error bars) for each method and validation set size, highlighting the impact of different approaches and data availability on intervention effectiveness.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_25_2.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure displays the Area Under the Receiver Operating Characteristic curve (AUROC) for interventions performed on the synthetic bottleneck dataset.  The performance is shown for various sizes of the validation set used to train the probing function (Nval), which maps intermediate layer activations to concepts.  The results for the Concept Bottleneck Model (CBM) are shown both when trained on the validation set only, and when trained on the full dataset.  Other lines represent multiple baseline methods.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_26_1.jpg", "caption": "Figure F.3: Principal components (PC) for a batch of data point representations before (z) and after (z') concept-based interventions under the varying values of the parameter \u03bb for (a) \u03bb = 0.2 and (b) \u03bb = 0.4.", "description": "This figure shows the effect of interventions on the model's internal representation.  It uses principal component analysis (PCA) to reduce the dimensionality of the feature space and visualizes the original representations (z) and the modified representations (z') after the intervention.  The two subfigures (a) and (b) show different levels of the hyperparameter \u03bb which controls the trade-off between the closeness of the modified representation to the original and its consistency with the intervened concepts.  (a) shows \u03bb=0.2 where the modified representations are close to the original while (b) with \u03bb=0.4 shows a larger shift in distribution, indicating stronger modifications.", "section": "F.2 Effect of Interventions on Representations"}, {"figure_path": "KYHma7hzjr/figures/figures_27_1.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure shows the results of the intervention experiments on synthetic data under the bottleneck mechanism.  It displays the area under the receiver operating characteristic (AUROC) curve for different sizes of validation sets and varying numbers of intervened-on concepts. It compares the performance of black-box models with and without fine-tuning, comparing them to Concept Bottleneck Models (CBMs). The results demonstrate that the proposed fine-tuning improves intervention effectiveness, bringing performance closer to the CBMs.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_27_2.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure shows the results of intervention experiments on synthetic data using different validation set sizes.  It compares the performance of different methods (black box, CBM, and fine-tuned models) as the number of intervened concepts increases. The results show the impact of validation set size on the effectiveness of concept-based interventions.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_28_1.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure displays the Area Under the Receiver Operating Characteristic (AUROC) curves for different models under varying validation set sizes in a bottleneck scenario. It shows how the performance of interventions varies with the percentage of concepts intervened upon. The models include a black-box model, a concept bottleneck model (CBM), a post hoc CBM, and three fine-tuned models. The figure also demonstrates that the proposed fine-tuning strategy improves the intervention effectiveness and achieves comparable results to CBM, outperforming baseline models.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_28_2.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure displays the results of intervention experiments on synthetic data, focusing on the Area Under the Receiver Operating Characteristic (AUROC) metric.  It shows how the performance changes as the percentage of concepts intervened upon varies, comparing different model types (black box, CBM, and fine-tuned variations). The impact of validation set size on performance is also explored.  Each line represents the median AUROC across ten simulations, with confidence intervals showing variability.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_28_3.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure shows the results of intervention experiments on synthetic data with different sizes of validation sets.  The results are presented as AUROC curves showing the model performance after interventions.  It compares the performance of black box models, CBMs (trained on full and validation sets), post hoc CBMs, and black box models fine-tuned with different methods (MT, A, I).  The plot shows that fine-tuning for intervenability (I) yields the best results, closely matching the performance of CBMs trained on the full dataset, especially with larger validation sets.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_29_1.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure shows the results of intervention experiments performed on a synthetic dataset using different validation set sizes. The x-axis represents the percentage of concepts intervened upon, and the y-axis shows the AUROC. The plot shows that the fine-tuned model (FINE-TUNED, I) significantly outperforms the other methods, especially when the validation set size is small.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_29_2.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure shows the results of the intervention experiments on the synthetic dataset, in which the validation set size is varied. The x-axis represents the percentage of concepts intervened, and the y-axis shows the AUROC. The results show that the proposed fine-tuning method improves the effectiveness of interventions, especially when the validation set size is small.", "section": "5 Results"}, {"figure_path": "KYHma7hzjr/figures/figures_30_1.jpg", "caption": "Figure 4: Intervention results on the (a) synthetic incomplete, (b) AwA2, (c) CIFAR-10, and (d) MIMIC-CXR datasets w.r.t. target AUROC (top) and AUPR (bottom) across ten seeds.", "description": "This figure presents the results of intervention experiments on four different datasets: synthetic incomplete, AwA2, CIFAR-10 and MIMIC-CXR.  For each dataset, it shows AUROC and AUPR curves for different methods, comparing the performance of black-box models, concept bottleneck models (CBMs), post-hoc CBMs, and fine-tuned models.  The x-axis represents the percentage of concepts intervened on, while the y-axis shows AUROC and AUPR. The results illustrate the effectiveness of the proposed fine-tuning method in improving the performance of interventions, especially in datasets with complex relationships between concepts and targets.", "section": "Results"}, {"figure_path": "KYHma7hzjr/figures/figures_31_1.jpg", "caption": "Figure 3: Intervention results w.r.t. target AUROC on the synthetic bottleneck data. We explore the performance under varying validation set sizes (Nval). Percentages correspond to the fractions of the original validation set. For CBMs, we report the results obtained by training on the validation (CBM val) and full training sets (CBM full). Interventions were performed on test data across ten simulations. Lines correspond to medians, and confidence bands are given by interquartile ranges.", "description": "This figure shows the results of intervention experiments performed on a synthetic dataset with varying validation set sizes. The x-axis represents the percentage of concepts intervened on, while the y-axis represents the AUROC.  The different lines represent different models (black box, CBM trained on validation set, CBM trained on full set, and three fine-tuned models). The shaded regions represent confidence intervals.  The results demonstrate the impact of validation set size and model type on the effectiveness of interventions.", "section": "5 Results"}]