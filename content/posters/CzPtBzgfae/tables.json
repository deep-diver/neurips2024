[{"figure_path": "CzPtBzgfae/tables/tables_2_1.jpg", "caption": "Table 1: Summary of known and new complexity results for solving distributed finite-sum optimization problem (2). Column \"Complexity\" indicates the number of communication rounds to find a solution with accuracy \u20ac > 0. Column \"RR?\" shows whether an algorithm uses Random Reshuffling, \"C?\" indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication, \"H?\" means independence from the constant of data heterogeneity in the complexity, \"CVX?\" indicates whether each loss on the i-th datapoint on the m-th client is convex, but not strongly convex. Notation: K = Lmax/\u00b5 and K = Lmax/ are conditional number of problem (2), where Lmax = Lipschitz constant, \u00b5 and u are the strong convexity constants of f and fir respectively; variances at the solution point x*: \u03c3\u03b5 = \u2211m=1 ||\u2207fm(x*)||2; heterogeneity constant  \u03c3\u03c4\u03b7 = \u2211m=1 \u2211i=1 ||\u2207fim(x*) - \u2207fm(x*)||2 and \u03c3\u03ba\u03b7 = \u2211M m=1 \u2211i=1 ||\u2207fim(x*)||2. The results of this paper are highlighted in blue.", "description": "This table summarizes the communication complexities of various distributed optimization algorithms for solving finite-sum problems.  It compares algorithms with and without random reshuffling (RR) and gradient compression (C), highlighting their dependence on data heterogeneity (H) and convexity assumptions (CVX).  The complexity is measured by the number of communication rounds needed to achieve a certain accuracy (\u03b5).", "section": "1.3 Can Communication Compression and Random Reshuffling be Friends?"}, {"figure_path": "CzPtBzgfae/tables/tables_18_1.jpg", "caption": "Table 1: Summary of known and new complexity results for solving distributed finite-sum optimization problem (2). Column \"Complexity\" indicates the number of communication rounds to find a solution with accuracy  \u03b5 > 0. Column \"RR?\" shows whether an algorithm uses Random Reshuffling, \"C?\" indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication, \"H?\" means independence from the constant of data heterogeneity in the complexity, \"CVX?\" indicates whether each loss on the i-th datapoint on the m-th client is convex, but not strongly convex. Notation: K = Lmax/\u00b5 and K = Lmax/u are conditional number of problem (2), where Lmax = Lipschitz constant, \u00b5 and u are the strong convexity constants of f and fir respectively; variances at the solution point x*: \u03c3\u00b2 = 1/(Mn)\u2211M=1\u2211ni=1 ||\u2207fim(x*)\u2212\u2207fm(x*)||\u00b2 and \u03c3\u00b2 = \u2211M=1 ||\u2207fm(x*)||\u00b2; heterogeneity constant \u03c3\u00b2 = \u2211i=1 ||fi(x*)||\u00b2.", "description": "This table summarizes the communication complexity of various algorithms for solving distributed finite-sum optimization problems. It compares the algorithms' performance with and without random reshuffling and gradient compression, highlighting the impact of these techniques on communication efficiency and convergence speed under different convexity assumptions.", "section": "1. Introduction"}, {"figure_path": "CzPtBzgfae/tables/tables_18_2.jpg", "caption": "Table 1: Summary of known and new complexity results for solving distributed finite-sum optimization problem (2). Column \"Complexity\" indicates the number of communication rounds to find a solution with accuracy  \u03b5 > 0. Column \"RR?\" shows whether an algorithm uses Random Reshuffling, \"C?\" indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication, \"H?\" means independence from the constant of data heterogeneity in the complexity, \"CVX?\" indicates whether each loss on the i-th datapoint on the m-th client is convex, but not strongly convex. Notation: K = Lmax/\u00b5 and K = Lmax/\u0169 are conditional number of problem (2), where Lmax = Lipschitz constant, \u00b5 and \u0169 are the strong convexity constants of f and fim respectively; variances at the solution point x*: \u03c3\u00b2M = 1/Mn\u2211Mm=1\u2211nmi=1||\u2207fim(x*)\u2212\u2207fm(x*)||\u00b2 and \u03c3\u00b2\u03b7=\u2211Mm=1||\u2207fm(x*)||\u00b2; heterogeneity constant \u03c3\u00b2\u03ba=\u2211Mm=1\u2211nmi=1||\u2207fim(x*)\u2212\u2207f(x*)||\u00b2. The results of this paper are highlighted in blue.", "description": "This table compares the communication complexity of various optimization algorithms for solving distributed finite-sum problems.  It shows the number of communication rounds needed to achieve a specific accuracy (\u03b5) for each algorithm, indicating whether they use random reshuffling (RR), gradient compression (C), have a complexity independent of data heterogeneity (H), and handle non-strongly convex functions (CVX). The table uses specific notation (K, \u03c3\u00b2M, \u03c3\u00b2\u03b7, \u03c3\u00b2\u03ba) to represent problem parameters and variance terms.", "section": "1.3 Can Communication Compression and Random Reshuffling be Friends?"}, {"figure_path": "CzPtBzgfae/tables/tables_18_3.jpg", "caption": "Table 1: Summary of known and new complexity results for solving distributed finite-sum optimization problem (2). Column \"Complexity\" indicates the number of communication rounds to find a solution with accuracy \u20ac > 0. Column \"RR?\" shows whether an algorithm uses Random Reshuffling, \"C?\" indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication, \"H?\" means independence from the constant of data heterogeneity in the complexity, \"CVX?\" indicates whether each loss on the i-th datapoint on the m-th client is convex, but not strongly convex. Notation: K = Lmax/\u00b5 and K = Lmax/ are conditional number of problem (2), where Lmax = Lipschitz constant, \u00b5 and u are the strong convexity constants of f and fir respectively; variances at the solution point x*: \u03c3\u03b5 = \u2211M m=1\u2211nm i=1 ||\u2207fim(x*)\u2212\u2207fm(x*)||2 and \u03c3\u03ba\u03b7 = \u2211Mm=1||\u2207f(x*)||2; heterogeneity constant \u03c32\u03b7 = \u2211Mm=1\u2211nmi=1||\u2207fim(x*)-\u2207fm(x*)||2.", "description": "This table summarizes the time and communication complexities of various optimization algorithms for solving distributed finite-sum problems. It compares algorithms with and without random reshuffling and gradient compression, highlighting their convergence properties under different convexity assumptions and data heterogeneity levels.", "section": "1.3 Can Communication Compression and Random Reshuffling be Friends?"}, {"figure_path": "CzPtBzgfae/tables/tables_18_4.jpg", "caption": "Table 1: Summary of known and new complexity results for solving distributed finite-sum optimization problem (2). Column \"Complexity\" indicates the number of communication rounds to find a solution with accuracy \u20ac > 0. Column \"RR?\" shows whether an algorithm uses Random Reshuffling, \"C?\" indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication, \"H?\" means independence from the constant of data heterogeneity in the complexity, \"CVX?\" indicates whether each loss on the i-th datapoint on the m-th client is convex, but not strongly convex. Notation: K = Lmax/\u00b5 and K = Lmax/\u0169 are conditional number of problem (2), where Lmax = Lipschitz constant, \u00b5 and \u0169 are the strong convexity constants of f and fim respectively; variances at the solution point x*: \u03c3\u00b2M = \u2211Mm=1\u2211nmi=1||\u2207fim(x*)\u2212\u2207fm(x*)||\u00b2 and \u03c3\u00b2\u03b7=\u2211Mm=1||\u2207fm(x*)||\u00b2; heterogeneity constant \u03c3\u00b2\u03ba\u03b7=\u2211Mm=1\u2211nmi=1||\u2207fim(x*)\u2212\u2207fm(x*)||\u00b2", "description": "This table summarizes the communication complexity of various optimization algorithms for solving distributed finite-sum problems. It compares the performance of algorithms with and without random reshuffling and gradient compression, considering different convexity assumptions and data heterogeneity. The table provides a detailed overview of existing and novel algorithms' complexities, showing their dependence on relevant parameters such as the Lipschitz constant, strong convexity constants, variance, and the number of data samples and clients.", "section": "1.3 Can Communication Compression and Random Reshuffling be Friends?"}, {"figure_path": "CzPtBzgfae/tables/tables_19_1.jpg", "caption": "Table 1: Summary of known and new complexity results for solving distributed finite-sum optimization problem (2). Column \"Complexity\" indicates the number of communication rounds to find a solution with accuracy \u20ac > 0. Column \"RR?\" shows whether an algorithm uses Random Reshuffling, \"C?\" indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication, \"H?\" means independence from the constant of data heterogeneity in the complexity, \"CVX?\" indicates whether each loss on the i-th datapoint on the m-th client is convex, but not strongly convex. Notation: K = Lmax/\u00b5 and K = Lmax/ are conditional number of problem (2), where Lmax = Lipschitz constant, \u00b5 and u are the strong convexity constants of f and fir respectively; variances at the solution point x*: \u03c3\u03b5 = \u2211M m=1 \u2211nm i=1 ||\u2207fim(x*)\u2212\u2207fm(x*)||2 and \u03c3\u03ba\u03b7 = \u2211M m=1 ||\u2207fm(x*)||2; heterogeneity constant \u03c3\u03b7 = \u2211n i=1 ||\u2207fi(x*)||2.", "description": "This table summarizes the existing and new complexity results for solving distributed finite-sum optimization problems.  It compares various optimization methods based on several criteria, including whether they use random reshuffling (RR), gradient compression (C), and their dependence on data heterogeneity (H).  The table also indicates if the methods assume convex but not necessarily strongly convex loss functions (CVX).  Mathematical notation clarifies the complexity measures and parameters used.", "section": "1.3 Can Communication Compression and Random Reshuffling be Friends?"}]