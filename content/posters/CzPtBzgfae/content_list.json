[{"type": "text", "text": "Don't Compress Gradients in Random Reshuffling: Compress Gradient Differences ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Abdurakhmon Sadiev1,2 \\*, Grigory Malinovsky1, Eduard Gorbunov1,2,3,4 Igor Sokolov\\*, Ahmed Khaled, Konstantin Burlachenko\\*, Peter Richtarik ", "page_idx": 0}, {"type": "text", "text": "'King Abdullah University of Science and Technology, Saudi Arabia   \n2Moscow Institute of Physics and Technology, Russian Federation 3Mohamed bin Zayed University of Artificial Intelligence, UAE 4Mila, Universite de Montr\u00e9al, Canada 5Princeton University, USA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gradient compression is a popular technique for improving communication complexity of stochastic first-order methods in distributed training of machine learning models. However, the existing works consider only with-replacement sampling of stochastic gradients. In contrast, it is well-known in practice and recently confirmed in theory that stochastic methods based on without-replacement sampling, e.g., Random Reshuffling (RR) method, perform better than ones that sample the gradients with-replacement. In this work, we close this gap in the literature and provide the first analysis of methods with gradient compression and without-replacement sampling. We first develop a distributed variant of random reshufling with gradient compression (Q-RR), and show how to reduce the variance coming from gradient quantization through the use of control iterates. Next, to have a better fit to Federated Learning applications, we incorporate local computation and propose a variant of Q-RR called Q-NASTYA. Q-NASTYA uses local gradient steps and different local and global stepsizes. Next, we show how to reduce compression variance in this setting as well. Finally, we prove the convergence results for the proposed methods and outline several settings in which they improve upon existing algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributed learning plays a crucial role in the training of modern Deep Learning (DL) models since distributed approaches are able to significantly reduce training time [Goyal et al., 2017, You et al., 2019]. Moreover, distributed methods are mandatory for such applications as Federated learning (FL) [Konecny et al., 2016, McMahan et al., 2017], where multiple nodes connected over a network collaborate on a learning task. Each node possesses its own dataset and cannot share this data with other nodes or a central server. As a result, algorithms for federated learning often rely on local computation and lack access to the entire dataset of training examples. Federated learning finds applications in diverse fields, including language modeling for mobile keyboards [Liu et al., 2021], healthcare [Antunes et al., 2022], and wireless communications [Yang et al., 2022]. Its applications extend to various other domains [Kairouz et al., 2019]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Distributed learning tasks are often solved through empirical-risk minimization (ERM), where the $m$ -th device contributes an empirical loss function $f_{m}(x)$ representing the average loss of model $x$ on its local dataset, and our goal is to then minimize the average loss over all the nodes: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left[f(x)\\stackrel{\\mathrm{def}}{=}\\frac{1}{M}\\sum_{m=1}^{M}f_{m}(x)\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the function $f$ represents the average loss. Every $f_{m}$ is an average of sample loss functions $f_{m}^{i}$ each representing the loss of model $x$ on the $i$ -th datapoint on the $m$ -th clients\u2019 dataset: that is for each $m\\in\\{1,2,\\ldots,M\\}$ we have ", "page_idx": 1}, {"type": "equation", "text": "$$\nf_{m}(x)\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\frac{1}{n_{m}}}\\sum_{i=1}^{n_{m}}f_{m}^{i}(x).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For simplicity we shall assume that the datasets on all clients are of equal size: $n_{1}=n_{2}=...=n_{M}$ though this assumption is only for convenience and our results easily extend to the case when clients have datasets of unequal sizes. Thus our optimization problem is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left[f(x)=\\frac{1}{n M}\\sum_{m=1}^{M}\\sum_{i=1}^{n}f_{m}^{i}(x)\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Because $d$ is often very large in practice, the dominant paradigm for solving (2) relies on first-order (gradient) information. Federated learning algorithms have access to two key primitives: (a) local computation, where for a given model $x\\in\\mathbb{R}^{d}$ we can compute stochastic gradients $\\nabla f_{m}^{i}(x)$ locally on client $m$ , and (b) communication, where the different clients can exchange their gradients or models with a central server. ", "page_idx": 1}, {"type": "text", "text": "1.1  Communication compression ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In practice, communication is more expensive than local computation [Kairouz et al., 2019], and as such one of the chief concerns of algorithms for distributed learning is communication efficiency. Algorithms for distributed/federated learning have thus focused on achieving communication efficiency, with one common ingredient being the use of gradient compression, where each client sends a compressed or quantized version of their update instead of the full update vector, potentially saving communication bandwidth by sending fewer bits over the network. There are many operators that can be used for compressing the update vectors: stochastic quantization [Alistarh et al., 2017], random sparsification [Wangni et al., 2018, Stich et al., 2018], and others [Tang et al., 2020]. In this work we consider compression operators satisfying the following assumption: ", "page_idx": 1}, {"type": "text", "text": "Assumption 1. A compression operator is an operator $\\mathcal{Q}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ such that for some $\\omega>0,$ the relations ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathcal{Q}(x)\\right]=x\\qquad a n d\\qquad\\mathbb{E}\\left[\\left\\Vert\\mathcal{Q}(x)-x\\right\\Vert^{2}\\right]\\leq\\omega\\Vert x\\Vert^{2}\\quad h o l d f o r\\;x\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Unbiased compressors can reduce the number of bits clients communicate per round, but also increases the variance of the stochastic gradients used slowing down overall convergence, see e.g. [Khirirat et al., 2018, Theorem 5.2] and [Stich, 2020, Theorem 1]. By using control iterates, Mishchenko et al. [2019b] developed DlANA\u2014an algorithm that can reduce the variance due to gradient compression with unbiased compression operators, and thus ensure fast convergence. DlANA has been extended and analyzed in many settings [Horvath et al., 2019, Stich, 2020, Safaryan et al., 2021] and forms an important tool in our arsenal for using gradient compression. ", "page_idx": 1}, {"type": "text", "text": "1.2  Random Reshuffing ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Despite the importance of addressing the communication bottleneck, local computations also significantly affect the training. For simplicity, consider the 1-node scenario. In this case, the update rule of the standard work-horse method in stochastic optimization - stochastic gradient descent (SGD) ", "page_idx": 1}, {"type": "text", "text": "Table 1: Summary of known and new complexityresults for solving distributed fnte-sum optimization problem (2). Column \u201cComplexity\" indicates the number of communication rounds to find a solution with accuracy $\\varepsilon\\:>\\:0$ . Column \u201cRR?\" shows whether an algorithm uses Random Reshuffing,\u201c\"C?\" indicates whether a method applies the compression of gradients or difference between the gradients and also whether methods for communication,\u201cH?\" means independence from the constant of data heterogeneity in the complexity. $\\mathrm{{^{\\circ}C V X?}}$ indicates whether each loss on the $^{i}$ -th datapoint on the $_m$ -th client is convex, but not strongly convex. Notation: $\\kappa=L_{\\mathrm{max}}/\\mu$ and $\\widetilde{\\kappa}=\\left.L_{\\mathrm{max}}/\\widetilde{\\mu}\\right.$ are conditional number of problem (2), where $L_{\\mathrm{max}}=$ Lipschitz constant, $\\mu$ and $\\widetilde{\\mu}$ are the strong convexity constants of $f$ and $f_{m}^{i}$ respectively; variances at theslutionpoint 2: 0\u00b2 = n m1 $\\begin{array}{r}{\\boldsymbol{x}_{\\star}\\colon\\sigma_{*}^{2}\\,=\\,\\frac{1}{M n}\\,\\sum_{m=1}^{M}\\sum_{i=1}^{n}\\|\\nabla f_{m}^{i}(\\boldsymbol{x}_{\\star})-\\nabla f_{m}(\\boldsymbol{x}_{\\star})\\|^{2}}\\end{array}$ and $\\begin{array}{r}{\\sigma_{*,n}^{2}\\,=\\,\\frac{1}{n}\\,\\sum_{i=1}^{n}\\|\\nabla f^{i}(x_{\\star})\\|^{2}\\,}\\end{array}$ hergenity onstant 2 =1IIV f (m. \\ll12 The resnlts of this naner are hiohliohted in hlue. $\\begin{array}{r}{\\zeta_{\\star}^{2}=\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{\\star})\\|^{2}}\\end{array}$ ", "page_idx": 2}, {"type": "table", "img_path": "CzPtBzgfae/tmp/020ad59858243729e914befb59704a970ca32b4f3b1e19197fb1e869869cc55a.jpg", "table_caption": [], "table_footnote": ["(1) In the case of SGD, RR we use X in \"H?\" to show that the complexity of these methods is provided in the non-distributed setup. (2) The fllowing inequlity is useful for the comparison of complexties: 2,n (wedenole $\\begin{array}{r}{\\rho_{\\star}^{2}=\\frac{\\omega}{M}(\\sigma_{\\star}^{2}+\\zeta_{\\star}^{2})+\\sigma_{\\star,n}^{2}}\\end{array}$ "], "page_idx": 2}, {"type": "text", "text": "[Robbins and Monro, 1951] - can be written as follows: $\\boldsymbol{x}_{t+1}=\\boldsymbol{x}_{t}-\\gamma\\nabla f^{j}\\big(\\boldsymbol{x}_{t}\\big)$ , where $j$ is sampled from $\\{1,\\ldots,n\\}$ uniformly at random. This procedure thus uses with-replacement sampling in order to select the stochastic gradient used at each step from the dataset. However, in the training of DL models, without-replacement sampling is used much more often: that is, at the beginning of each epoch we choose a permutation $\\pi_{1},\\pi_{2},\\ldots,\\pi_{n}$ of $\\{1,2,\\ldots,n\\}$ and do the $i$ -th update using the $\\pi_{i}$ -ith gradient: $x_{t}^{i+1}=x_{t}^{i}-\\gamma\\nabla f^{\\pi_{i}}(x_{t}^{i})$ Without-replacement sampling SGD, also known as Random Reshuffing (RR) [Bottou, 2009], typically achieves better asymptotic convergence rates compared to with-replacement SGD and can improve upon it in many settings as shown by recent theoretical progress [Mishchenko et al., 2020, Ahn et al., 2020, Rajput et al., 2020, Safran and Shamir, 2021]. While with-replacement SGD achieves an error proportional to $\\textstyle{\\mathcal{O}}\\left({\\frac{1}{T}}\\right)$ after $T$ steps [Stich, 2019], Random Reshuffing achieves an error of $\\textstyle{\\mathcal{O}}\\left({\\frac{n}{T^{2}}}\\right)$ after $T$ steps, faster than SGD when the number of steps $T$ is large. ", "page_idx": 2}, {"type": "text", "text": "1.3  Can Communication Compression and Random Reshuffling be Friends? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As we described earlier, Random Reshuffling and communication compression are two important tools for training modern DL models, and both techniques are relatively well understood. However, there are no papers that study Random Reshuffling and communication compression in combination. This leads us to the natural question: how these techniques should be combined to improve the convergencespeedofexistingdistributedmethods? ", "page_idx": 2}, {"type": "text", "text": "1.4  Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we aim to develop methods for Distributed and Federated Learning that combine gradient compression and random reshuffling. While each of these techniques can aid in reducing the communication complexity of distributed optimization, their combination is under-explored. Thus our goal is to design methods that improve upon existing algorithms in convergence rates and in practice. We summarize our contributions as follows. ", "page_idx": 2}, {"type": "text", "text": "$\\diamond$ The issue: naive combination has no improvements. As a natural step towards our goal, we propose and study a new algorithm, Q-RR (Algorithm 1), that combines random reshuffling with gradient compression at every communication round. However, for Q-RR our theoretical results do not show any improvement upon QSGD when the compression level is reasonable (see Table 1). Moreover, we observe similar performance of Q-RR and QSGD in various numerical experiments. Therefore, we conclude that this phenomenon is not an artifact of our analysis but rather an issue of Q-RR: communication compression adds an additional noise that dominates the one coming from the stochastic gradients sampling. ", "page_idx": 3}, {"type": "text", "text": "$\\diamond$ The remedy: reduction of compression variance. To remove the additional variance added by the compression and unleash the potential of Random Reshufing in distributed learning with compression, we propose DIANA-RR (Algorithm 2), a combination of Q-RR and the DIANA algorithm. We derive the convergence rates of the new method and show that it improves upon the convergence rates of Q-RR, QSGD, and DIANA (see Table 1). We point out that to achieve such resultsweuse $n$ shift-vectors per worker in DIANA-RR unlike DIANA that uses only 1 shift-vector. ", "page_idx": 3}, {"type": "text", "text": "$\\diamond$ Extensions to the local steps. Inspired by the NASTYA algorithm of Malinovsky et al. [2022], we propose a variant of NASTYA, Q-NASTYA (Algorithm 3), that naively mixes quantization, local steps with random reshuffling, and uses different local and server stepsizes. Although it improves in per-round communication cost over NASTYA but, similar to Q-RR, we show that Q-NASTYA suffers from added variance due to gradient quantization. To overcome this issue, we propose another algorithm, DIANA-NASTYA (Algorithm 4), that adds DIANA-style variance reduction to Q-NASTYA and removes the additional variance. ", "page_idx": 3}, {"type": "text", "text": "Finally, to illustrate our theoretical findings we conduct experiments on federated logistic regression tasks and on distributed training of neural networks. ", "page_idx": 3}, {"type": "text", "text": "2  Algorithms and convergence theory ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will primarily consider the setting of strongly-convex and smooth optimization. We assume that the average function $f$ is strongly convex: ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. Function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $\\mu$ -strongly convex, i.e., for all $x,y\\in\\mathbb{R}^{d}$ \uff0c ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(x)-f(y)-\\langle\\nabla f(y),x-y\\rangle\\geq{\\frac{\\mu}{2}}\\|x-y\\|^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "andfunctions $f_{1}^{i},f_{2}^{i},\\dotsc,f_{M}^{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ are convex for all $i=1,\\hdots,n$ ", "page_idx": 3}, {"type": "text", "text": "Examples of objectives satisfying Assumption 2 include $\\ell_{2}$ -regularized linear and logistic regression. Throughout the paper, we assume that $f$ has the unique minimizer $x_{\\star}\\,\\in\\,\\mathbb{R}^{d}$ .We also use the assumption that each individual loss $f_{m}^{i}$ is smooth, i.e. has Lipschitz-continuous first-order derivatives: ", "page_idx": 3}, {"type": "text", "text": "Assumption 3. Function $f_{m}^{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ $\\boldsymbol{L}_{i,m}$ -smooth for every $i\\in[n]$ and $m\\in[M].$ i.e., for all $x,y\\in\\mathbb{R}^{d}$ and for all $m\\in[M]$ and $i\\in[n]$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla f_{m}^{i}(x)-\\nabla f_{m}^{i}(y)\\|\\leq L_{i,m}\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We denote the maximal smoothness constant as $L_{\\operatorname*{max}}\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}_{i,m}L_{i,m}$ ", "page_idx": 3}, {"type": "text", "text": "For some methods, we shall additionally impose the asumption that each function is strongly convex: ", "page_idx": 3}, {"type": "text", "text": "Assumption 4. Each function $f_{m}^{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $\\widetilde{\\mu}$ -strongly convex. ", "page_idx": 3}, {"type": "text", "text": "The Bregman divergence associated with a convex function $h$ is defined for all $x,y\\in\\mathbb{R}^{d}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{h}(x,y)\\stackrel{\\mathrm{def}}{=}h(x)-h(y)-\\left\\langle\\nabla h(y),x-y\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the inequality (3) defining strong convexity can be written as $\\begin{array}{r}{D_{f}(x,y)\\geq\\frac{\\mu}{2}\\|x-y\\|^{2}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "2.1  Algorithm Q-RR ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first method we introduce is Q-RR (Algorithm 1). Q-RR is a straightforward combination of distributed random reshuffling and gradient quantization. This method can be seen as the stochastic without-replacement analogue of the distributed quantized gradient method of Khirirat et al. [2018]. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Q-RR: Distributed Random Reshuffling with Quantization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: $x_{0}$ -starting point, $\\gamma>0-$ stepsize $t=0,1,\\ldots,T-1$ $x_{t}$ $x_{t,m}^{0}=x_{t}$ $[n]$ $\\pi_{m}=(\\pi_{m}^{0},\\ldots,\\pi_{m}^{n-1})$   \n4: for $i=0,1,\\dotsc,n-1$ do   \n5: for $m=1,\\dotsc,M$ in parallel do   \n6: Receive $\\boldsymbol x_{t}^{i}$ from the server, compute and send $\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)$ back   \n7: end for   \n8: Compute and send $\\begin{array}{r}{\\boldsymbol{x}_{t}^{i+1}=\\boldsymbol{x}_{t}^{i}-\\gamma\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol{x}_{t}^{i})\\right)}\\end{array}$ to the workers   \n9: end for   \n10: $x_{t+1}=x_{t}^{n}$   \n11: end for   \nOutput: $x_{T}$ ", "page_idx": 4}, {"type": "text", "text": "We shall use the notion of shufling radius defined by Mishchenko et al. [2021] for the analysis of distributed methods with random reshuffling: ", "page_idx": 4}, {"type": "text", "text": "Denit.eq $\\begin{array}{r}{x_{\\star}^{i+1}=x_{\\star}^{i}-\\frac{\\gamma}{M}\\sum_{m=1}^{M}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})}\\end{array}$ Then the shuffing radius is the quantity ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sigma_{r a d}^{2}\\overset{\\mathrm{def}}{=}\\operatorname*{max}_{i}\\left\\{\\frac{1}{\\gamma^{2}M}\\sum_{m=1}^{M}\\mathbb{E}D_{f_{m}^{\\pi^{i}}}(x_{\\star}^{i},x_{\\star})\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We provide clarifications regarding this term in Appendix C.1. To compare our subsequent results with known ones, we introduce bounds on the shufing radius. The following lemma demonstrates that these bounds are independent of the stepsize $\\gamma$ ,eventhough $\\gamma$ is used in Definition 2.1. ", "page_idx": 4}, {"type": "text", "text": "Lemma2.1 (Mishnketal, 200]. Lt Astions  4 hold Then the shffin radus $\\sigma_{r a d}^{2}$ satisfies the following inequlity ", "page_idx": 4}, {"type": "text", "text": "where $\\sigma_{\\star,n}^{2}\\stackrel{\\mathrm{def}}{=}\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f^{i}(x_{\\star})\\|^{2}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\frac{\\widetilde{\\mu}n}{8}\\sigma_{\\star,n}^{2}\\leq\\sigma_{r a d}^{2}\\leq\\frac{L_{\\operatorname*{max}}n}{4}\\sigma_{\\star,n}^{2},}\\\\ {\\|\\nabla f^{i}(x_{\\star})\\|^{2},\\,a n d\\,f^{i}=\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}f_{m}^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We now state the main convergence theorem for Algorithm 1: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1. Let Assumptions 1, 3, 4 hold and let the stepsize satisfy $\\begin{array}{r}{0<\\gamma\\le\\frac{1}{\\left(1+2\\frac{\\omega}{M}\\right)L_{\\operatorname*{max}}}}\\end{array}$ for all $T\\geq0$ the iterates produced by $\\upalpha$ -RR (Algorithm 1) satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|x_{T}-x_{\\star}\\|^{2}\\leq\\left(1-\\gamma\\widetilde{\\mu}\\right)^{n T}\\|x_{0}-x_{\\star}\\|^{2}+\\frac{2\\gamma^{2}\\sigma_{r a d}^{2}}{\\widetilde{\\mu}}+\\frac{2\\gamma\\omega}{\\widetilde{\\mu}M}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\zeta_{\\star}^{2}\\stackrel{\\mathrm{def}}{=}\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{\\star})\\|^{2},a n d\\,\\sigma_{\\star}^{2}\\stackrel{\\mathrm{def}}{=}\\frac{1}{M n}\\displaystyle\\sum_{m=1}^{M}\\sum_{i=1}^{n}\\|\\nabla f_{m}^{i}(x_{\\star})-\\nabla f_{m}(x_{\\star})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "All proofs are relegated to the appendix. By choosing the stepsize $\\gamma$ properly, we can obtain the communication complexity (number of communication rounds) needed to find an $\\varepsilon$ -approximate solution as follows: ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. Under the same conditions as Theorem 2.1 and for Algorithm $^{\\,l}$ , there exists a stepsize $\\gamma>0$ suchthatthenumberofcommunicationrounds $n T$ to find a solution with accuracy $\\varepsilon>0$ (i.e. $\\mathbb{E}\\lVert x_{T}-x_{\\star}\\rVert^{2}\\,\\leq\\,\\epsilon)$ is equal to $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\left(1+\\frac{\\omega}{M}\\right)\\frac{L_{\\operatorname*{max}}}{\\widetilde{\\mu}}+\\frac{\\omega(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2})}{M\\widetilde{\\mu}^{2}\\varepsilon}+\\frac{\\sigma_{n d}}{\\sqrt{\\widetilde{\\mu}^{3}\\varepsilon}}\\right)}\\end{array}$ ,where $\\widetilde O(\\cdot)$ hides constantsandlogarithmicfactors. ", "page_idx": 4}, {"type": "text", "text": "The complexity ofQuantized SGD (QSGD\uff09is[Gorbunovetal., 2020]: $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\left(1+\\frac{\\omega}{M}\\right)\\frac{L_{\\mathrm{max}}}{\\mu}+\\frac{\\left(\\omega\\zeta_{\\star}^{2}+(1+\\omega)\\sigma_{\\star}^{2}\\right)}{M\\mu^{2}\\varepsilon}\\right)}\\end{array}$ For simplicity, let us neglect the differences between ", "page_idx": 4}, {"type": "text", "text": "Input: $x_{0}$ - starting point, $\\{h_{0,m}^{i}\\}_{m,i=1,1}^{M,n}$ --initial shift-vectors, $\\gamma>0$ - stepsize, $\\alpha>0$ - stepsize for learning the shifts $t=0,1,\\ldots,T-1$ $x_{t}$ $x_{t,m}^{0}=x_{t}$   \n3: Samplerandom permutation of $[n]$ $\\pi_{m}=(\\pi_{m}^{0},\\ldots,\\pi_{m}^{n-1})$   \n4: for $i=0,1,\\dotsc,n-1$ do   \n5: for $m=1,2,\\ldots,M$ in parallel do   \n6: Recive $\\boldsymbol x_{t}^{i}$ $\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-h_{t,m}^{\\pi_{m}^{i}}\\right)$ back   \n7: Set gt,m $\\hat{\\boldsymbol g}_{t,m}^{\\pi_{m}^{i}}=\\boldsymbol h_{t,m}^{\\pi_{m}^{i}}+\\mathcal Q\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol x_{t,m}^{i})-\\boldsymbol h_{t,m}^{\\pi_{m}^{i}}\\right)$   \n8: $h_{t+1,m}^{\\pi_{m}^{i}}=h_{t,m}^{\\pi_{m}^{i}}+\\alpha\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})-h_{t,m}^{\\pi_{m}^{i}}\\right)$   \n9: end for   \n10: Compute $\\begin{array}{r}{x_{t}^{i+1}=x_{t}^{i}-\\gamma\\frac{1}{M}\\sum_{m=1}^{M}\\hat{g}_{t,m}^{\\pi_{m}^{i}}}\\end{array}$ an send $x_{t}^{i+1}$ to the workers   \n11: end for   \n12: $x_{t+1}=x_{t}^{n}$   \n13: end for ", "page_idx": 5}, {"type": "text", "text": "", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Output: $x_{T}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$\\mu$ and $\\widetilde{\\mu}$ .First, when $\\omega\\;=\\;0$ we recover the complexity of FedRR [Mishchenko et al., 2021] which is known to be better than the one of SGD as long as $\\varepsilon$ is sufficiently small as we have $n\\mu\\sigma_{\\star,n}^{2}/8\\,\\leq\\,\\sigma_{\\mathrm{rad}}^{2}\\,\\leq\\,n L\\sigma_{\\star,n}^{2}/4$ from Lemma 2.1.Next, when $M=1$ and $\\omega\\,=\\,0$ (single node, no compression) our results recovers the rate of RR [Mishchenko et al., 2020]. ", "page_idx": 5}, {"type": "text", "text": "However, it is more interesting to compare $\\upalpha$ -RR and QSGD when $M>1$ and $\\omega>1$ , which is typically the case. In these settings, $\\upalpha$ -RR and QSGD have the same complexity since the $\\mathcal{O}(^{1}/\\varepsilon)$ term dominates the ${\\mathcal{O}}({^{1}\\!/{\\sqrt{\\varepsilon}}})$ one if $\\varepsilon$ is sufficiently small. That is, the derived result for $\\upalpha$ -RR has no advantages over the known one for QSGD unless $\\omega$ is very small, which means that there is almost no compression at all. We also observe this phenomenon in the experiments. ", "page_idx": 5}, {"type": "text", "text": "The main reason for that is the variance appearing due to compression. Indeed, even if the current point is the solution of the problem $\\left\\langle x_{t}^{i}=x_{*}\\right\\rangle$ O, the update direction $\\begin{array}{r}{-\\gamma\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)}\\end{array}$ has the compression variance ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\Vert\\frac{\\gamma}{M}\\sum_{m=1}^{M}\\left(\\mathcal{Q}(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star}))-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right)\\right\\Vert^{2}\\right]\\leq\\frac{\\gamma^{2}\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This upper bound is tight and non-zero in general. Moreover, it is proportional to $\\gamma^{2}$ that creates the term proportional to $\\gamma$ in (5) like in the convergence results for QSGD/SGD, while the RR-variance is proportionalto $\\gamma^{2}$ in the same bound. Therefore, during the later stages of the convergence $\\upalpha$ RR behaves similarly to QSGD when we decrease the stepsize. ", "page_idx": 5}, {"type": "text", "text": "2.2  Algorithm DIANA-RR ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To reduce the additional variance caused by compression, we apply DIANA-style shift sequences [Mishchenko et al., 2019b, Horvath et al., 2019]. Thus we obtain DIANA-RR (Algorithm 2), which applies compression to the differences between the gradients and learnable shifts. Since the shifts are updated using the past gradients information, one can see DIANA-RR as a method with compression of gradient differences. We notice that unlike DIANA, DIANA-RR has $n$ shift-vectors on eachnode. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.2. Let Assumptions 1, 3, 4 hold and suppose that the stepsizes satisfy $\\gamma\\ \\leq$ [2 (1+) mms  and a \u2264 .Defnethe fllowin Lyapuno functionfoevery \u2265 0 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Psi_{t+1}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\|x_{t+1}-x_{\\star}\\|^{2}+{\\frac{4\\omega\\gamma^{2}}{\\alpha M^{2}}}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}(1-\\gamma\\mu)^{j}\\|\\Delta_{t+1,m}^{j}\\|^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Input: $x_{0}$ - starting point, $\\gamma>0-$ local stepsize, $\\eta>0$ -global stepsize   \n1: for $t=0,1,\\ldots,T-1$ do   \n23 $m\\in[M]$ $x_{t}$ $x_{t,m}^{0}=x_{t}$   \n4: Samplerandompermution of $[n]\\colon\\pi_{m}=(\\pi_{m}^{0},\\ldots,\\pi_{m}^{n-1})$   \n5: for $i=0,1,\\dotsc,n-1$ do   \n6: nset $\\boldsymbol{x}_{t,m}^{i+1}=\\boldsymbol{x}_{t,m}^{i}-\\gamma\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol{x}_{t,m}^{i})$   \n7: end for   \n8: Compute $\\begin{array}{r}{g_{t,m}=\\frac{1}{\\gamma n}\\left(x_{t}-x_{t,m}^{n}\\right)}\\end{array}$ and send $\\mathcal{Q}_{t}\\big(g_{t,m}\\big)$ to the server   \n9: end for   \n10: Compute $\\begin{array}{r}{g_{t}=\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{Q}_{t}(g_{t,m})}\\end{array}$   \n11: Compute $x_{t+1}=x_{t}-\\eta g_{t}$ and send $x_{t+1}$ to the workers   \n12: end for   \nOutput: $x_{T}$ ", "page_idx": 6}, {"type": "text", "text": "where i+1.m a $\\Delta_{t+1,m}^{j}=h_{t+1,m}^{\\pi_{m}^{j}}-\\nabla f_{m}^{\\pi_{m}^{j}}(x_{\\star})$ Then, for all $T\\geq0$ the iterates produced by DIANA-RR (Algorithm 2) satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\gamma\\widetilde{\\mu}\\right)^{n T}\\Psi_{0}+\\frac{2\\gamma^{2}\\sigma_{r a d}^{2}}{\\widetilde{\\mu}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Corollary 2. Under the same conditions as Theorem 2.2 and for Algorithm 2, there exists stepsizes $\\gamma,\\alpha>0$ suchthat thenumberof communicationrounds $n T$ tofindasolutionwithaccuracy $\\varepsilon>0$ is $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(n(1+\\omega)+\\left(1+\\frac{\\omega}{M}\\right)\\frac{L_{\\mathrm{max}}}{\\widetilde{\\mu}}+\\frac{\\sigma_{r a d}}{\\sqrt{\\varepsilon\\widetilde{\\mu}^{3}}}\\right).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Unlike Q-RR/QSGD/DIANA, DIANA-RR does not have a $\\widetilde{\\mathcal{O}}(^{1}\\!/\\varepsilon)$ -term, which makes it superior to Q-RR/QSGD/DIANA for small enough $\\varepsilon$ . However, the complexity of DIANA-RR has an additive $\\widetilde{\\mathcal{O}}(n(1+\\omega))$ term arising due to learning the shifts $\\{h_{t,m}^{i}\\}_{m\\in[M],i\\in[n]}$ Neverthel this additnal term is not the dominating one when $\\varepsilon$ is small enough. Next, we elaborate a bit more on the comparison between DIANA and DIANA-RR. That is, DIANA has $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\left(1+\\frac{\\omega}{M}\\right)\\frac{L_{\\mathrm{max}}}{\\mu}+\\frac{(1+\\omega)\\sigma_{\\star}^{2}}{M\\mu^{2}\\varepsilon}\\right)}\\end{array}$ complexity [Gorbunov et al., 2020]. Neglecting the differences between $\\mu$ and $\\widetilde{\\mu}$ ,we observe a similar relation between DIANA-RR and DIANA as between RR and SGD: instead of the term ${\\mathcal O}\\bigl((1{+}\\omega)\\sigma_{\\star}^{2}\\bigr/(M\\mu^{2}\\varepsilon)\\bigr)$ appearing in the complexity of DIANA, DIANA-RR has $O\\big(\\sigma_{\\mathrm{rad}}/\\sqrt{\\varepsilon\\widetilde{\\mu}^{3}}\\big)$ term much better depending on $\\varepsilon$ To the best of our knowledge, our result is the only known one establishing the theoretical superiority of RR to regular SGD in the context of distributed learning with gradient compression. Moreover, when $\\omega=0$ (no compression) we recover the rate of FedRR and when additionally $M=1$ (single worker) we recover the rate of RR. ", "page_idx": 6}, {"type": "text", "text": "2.3  Algorithms with Local Steps ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we study a new variant of NASTYA, Q-NASTYA (Algorithm 3), that unifies quantization, local steps with random reshufling, and uses different local and server stepsizes. Although it improves in per-round communication cost over NASTYA but, similar to $\\upalpha$ RR,weshow that $\\upalpha$ -NASTYA suffers from added variance due to gradient quantization. To overcome this issue, we propose another algorithm, DIANA-NASTYA (Algorithm 4), that adds DIANA-style variance reduction to Q-NASTYA and removes the additional variance. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.3. Let Assumptions 1, 2, 3 hold. Let the stepsizes $\\gamma,\\,\\eta$ satisfy $\\begin{array}{r}{0<\\eta\\le\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)}}\\end{array}$ $\\begin{array}{r}{0<\\gamma\\le\\frac{1}{5n L_{\\mathrm{max}}}}\\end{array}$ .Then,for all $T\\geq0$ the iterates produced by Q-NASTYA (Algorithm 3) satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert x_{T}-x_{\\star}\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\eta\\mu}2\\right)^{T}\\Vert x_{0}-x_{\\star}\\Vert^{2}+8\\frac{\\eta\\omega}{\\mu M}\\zeta_{\\star}^{2}+\\frac{9}{2}\\frac{\\gamma^{2}n L_{\\operatorname*{max}}}{\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Corollary 3. Under the same conditions as Theorem $E.l$ and for Algorithm 3, there exist stepsizes $\\gamma=\\eta/n$ and $\\eta>0$ such that the number of communication rounds $T$ to find a solution with accuracy ", "page_idx": 6}, {"type": "text", "text": "Input: $x_{0}$ - starting point, $\\{h_{0,m}\\}_{m=1}^{M}$ - initial shft-vectors, $\\gamma>0$ - local stepsize, $\\eta>0$ - global   \nstepsize, $\\alpha>0-$ stepsize for learning the shifts   \n1: for $t=0,1,\\ldots,T-1$ do   \n2: $m=1,\\dots,M$ \u00a50   \n34 Samplerandompemtaton of $[n]\\colon\\pi_{m}=(\\pi_{m}^{0},\\ldots,\\pi_{m}^{n-1})$ $x_{t,m}^{0}=x_{t}$   \n5:   \n6: Sset $\\boldsymbol{x}_{t,m}^{i+1}=\\boldsymbol{x}_{t,m}^{i}-\\gamma\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol{x}_{t,m}^{i})$   \n7: end for   \n8: Compute $\\begin{array}{r}{g_{t,m}=\\frac{1}{\\gamma n}\\left(x_{t}-x_{t,m}^{n}\\right)}\\end{array}$ and send $\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)$ to the server   \n9: Set $h_{t+1,m}=h_{t,m}+\\alpha\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)$   \n10: Set $\\hat{g}_{t,m}=h_{t,m}+\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)$   \n123 14: $\\begin{array}{r l}&{\\mathbf{\\Lambda}_{h+1}^{\\mathbf{unu}}=\\frac{1}{M}\\sum_{m=1}^{M}h_{t+1,m}=h_{t}+\\frac{\\alpha}{M}\\sum_{m=1}^{M}\\mathcal{Q}_{t}\\left(\\boldsymbol{g}_{t,m}-\\boldsymbol{h}_{t,m}\\right)}\\\\ &{\\hat{\\boldsymbol{g}}_{t}=\\frac{1}{M}\\sum_{m=1}^{M}\\hat{\\boldsymbol{g}}_{t,m}=h_{t}+\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{Q}_{t}\\left(\\boldsymbol{g}_{t,m}-\\boldsymbol{h}_{t,m}\\right)}\\\\ &{x_{t+1}=x_{t}-\\eta\\hat{\\boldsymbol{g}}_{t}}\\end{array}$   \n15: end for ", "page_idx": 7}, {"type": "text", "text": "Output: $x_{T}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "\u03b5>0isO( ((1+%)+\u524d+\u221a\u221a+\uff09f\u21920. omecanechoose $\\eta>0$ such that the above complextr bound improve to $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\frac{\\omega}{M}\\frac{\\zeta_{\\star}^{2}}{\\varepsilon\\mu^{3}}\\right)}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "We emphasize several differences with the known theoretical results. First, the FedCOM method of Haddadpour et al. [2021] was analyzed in the homogeneous setting only, i.e., $f_{m}(x)\\,=\\,f(x)$ for all $m\\,\\in\\,[M]$ , which is an unrealistic assumption for $\\mathrm{FL}$ applications. In contrast, our result holds in the fully heterogeneous case. Next, the analysis of FedPAQ of Reisizadeh et al. [2020] uses a bounded variance assumption, which is also known to be restrictive. Nevertheless, let us compare to their result. Reisizadeh et al. [2020] derive the following complexity for their method: $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{\\bar{L}_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\frac{\\omega}{M}\\frac{\\sigma^{2}}{\\mu^{2}\\varepsilon}+\\frac{\\sigma^{2}}{M\\mu^{2}\\varepsilon}\\right).}\\end{array}$ This result is inferior to the one we show for $\\boldsymbol{\\Omega}$ -NASTYA: when $\\omega$ is small, the main term in the complexity bound of FedPAQ is $\\widetilde{\\mathcal{O}}\\left(1/\\varepsilon\\right)$ , while for $\\upalpha$ -NASTYA the dominating term is of the order $\\widetilde{\\mathcal{O}}\\left({1}/{\\sqrt{\\varepsilon}}\\right)$ (when $\\omega$ and $\\varepsilon$ are sufficiently small). We also highlight that FedCRR [Malinovsky and Richtarik, 2022] does not converge if $\\omega\\stackrel{.}{>}M^{2}\\gamma\\dot{\\mu}\\varepsilon\\big/(2\\big|\\big|x_{*,m}^{n}\\big|\\big|^{2})$ ,while $\\upalpha$ NASTYA does for any $\\omega\\ge0$ . Finally, when $\\omega=0$ (no compression) we recover NASTYA as a special case, and using $\\gamma=\\eta/n$ , we recover the rate of FedRR [Mishchenko et al., 2021]. ", "page_idx": 7}, {"type": "text", "text": "Theorem2.4. LetAsumtions 1, 2, 3 hold. Suppose the stepsizes , 7,\u03b1 satisfy 0<\u226416Lmxn $\\begin{array}{r}{0<\\eta\\leq\\operatorname*{min}\\left\\{\\frac{\\alpha}{2\\mu},\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{9\\omega}{M}\\right)}\\right\\}}\\end{array}$ [2 16Lma(1+) and \u03b1 \u2264  Defne the fllowing Lyapumov function: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Psi_{t+1}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\|x_{t+1}-x_{\\star}\\|^{2}+{\\frac{8\\omega\\eta^{2}}{\\alpha M^{2}}}\\sum_{m=1}^{M}\\|h_{t+1,m}-h_{m}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then,for all $T\\geq0$ the iterates produced by DIANA-NASTYA (Algorithm 4) satisfy ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)^{T}\\Psi_{0}+\\frac{9}{2}\\frac{\\gamma^{2}n L}{\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Corollary 4. Under the same conditions as Theorem $E.2$ and for Algorithm 4, there exist stepsizes $\\gamma=\\eta/{n},\\,\\eta>0,\\,\\alpha>0$ such that the number of communication rounds $T$ to find a solution with accuracy $\\varepsilon>0$ .s $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\omega+\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\sqrt{\\frac{L_{\\mathrm{max}}}{\\varepsilon\\mu^{3}}}\\sqrt{\\zeta_{\\star}^{2}+\\frac{\\sigma_{\\star}^{2}}{n}}\\right).I f\\gamma\\rightarrow0,}\\end{array}$ . one can choose $\\eta>0$ such hat the above complexit bound inproves to $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\omega+\\frac{L_{\\operatorname*{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)\\right)}\\end{array}$ ", "page_idx": 7}, {"type": "image", "img_path": "CzPtBzgfae/tmp/1f41748d9a4bd92fe6dd56f4a9ce52936b97f85a01c3ace8b1cfae4cb138d9a6.jpg", "img_caption": ["Figure 2: Local methods "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: The comparison of the proposed methods (Q-NASTYA, DIANA-NASTYA, Q-RR, DIANA-RR), DIANA-RR-1S (a modification of DIANA-RR), and existing baselines (QSGD, DIANA, FedCOM, FedPAQ). All methods use tuned stepsizes and the Rand- $k$ compressor. ", "page_idx": 8}, {"type": "text", "text": "In contrast to Q-NASTYA, DIANA-NASTYA does not suffer from the $\\widetilde{\\mathcal{O}}(1/\\varepsilon)$ term in the complexity bound. This shows the superiority of DIANA-NASTYA to $\\upalpha$ -NASTYA. Next, FedCRR-VR [Malinovsky and Richtarik, 2022] has the rate $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{(\\omega+1)\\left(1-\\frac{1}{\\kappa}\\right)^{n}}{\\left(1-\\left(1-\\frac{1}{\\kappa}\\right)^{n}\\right)^{2}}+\\frac{\\sqrt{\\kappa}(\\zeta_{\\star}+\\sigma_{\\star})}{\\mu\\sqrt{\\varepsilon}}\\right)}\\end{array}$ , which depends on $\\widetilde{\\mathcal{O}}\\left({1}/{\\sqrt{\\varepsilon}}\\right)$ However, the first term is close to $\\widetilde{\\cal O}\\left((\\omega+1)\\kappa^{2}\\right)$ for a large condition number. FedCRR-VR-2 utilizes variance reduction technique from Malinovsky et al. [2021] and it allows to get rid of permutation variance. This method has $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{(\\omega+1)\\left(1-\\frac{1}{\\kappa\\sqrt{\\kappa n}}\\right)^{\\frac{n}{2}}}{\\left(1-\\left(1-\\frac{1}{\\kappa\\sqrt{\\kappa n}}\\right)^{\\frac{n}{2}}\\right)^{2}}+\\frac{\\sqrt{\\kappa}\\zeta_{\\star}}{\\mu\\sqrt{\\varepsilon}}\\right)}\\end{array}$ complexity, but it requires additional assumption on number of functions $n$ and thus not directly comparable with our result. Note that if we have no compression $\\mathbf{\\Pi}_{\\left\\langle\\omega\\right.=0\\right\\rangle}$ ), DIANA-NASTYA recovers rate of NASTYA. ", "page_idx": 8}, {"type": "text", "text": "In Appendix J, we provide versions of $\\upalpha$ -NASTYA and DIANA-NASTYA with partial participation of clients, which is another important aspect of $\\mathrm{FL}$ , and derive the convergence results for them. ", "page_idx": 8}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluated our methods for solving logistic regression problems and training neural networks in three parts: (i) Comparison of the proposed non-local methods with existing baselines; (ii) Comparison of the proposed local methods with existing baselines; (ii) Comparison of the proposed non-local methods in training ResNet -18 on CIFAR10. ", "page_idx": 8}, {"type": "text", "text": "Logistic Regression.  To confirm our theoretical results we conducted several numerical experiments on binary classification problem with L2 regularized logistic regression of the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left[f(x)\\stackrel{\\mathrm{def}}{=}\\frac{1}{M}\\sum_{m=1}^{M}\\frac{1}{n_{m}}\\sum_{i=1}^{n_{m}}f_{m,i}\\right],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Wwhere $f_{m,i}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\log\\left(1+\\exp(-y_{m i}a_{m i}^{\\top}x)\\right)+\\lambda\\|x\\|_{2}^{2}\\left(a_{m i},y_{m i}\\right)\\in\\mathbb{R}^{d}\\times\\in\\{-1,1\\},i=1,\\ldots,n_{m}$ are the training data samples stored on machines $m\\,=\\,1,\\ldots,M$ ,and $\\lambda\\,>\\,0$ is a regularization parameter. In all experiments, for each method, we used the largest stepsize allowed by its theory multiplied by some individually tuned constant multiplier. For better parallelism, each worker $m$ uses mini-batches of size $\\approx0.1n_{m}$ . In all algorithms, as a compression operator $\\mathcal{Q}$ we use Rand- $k$ [Beznosikov et al., 2020] with fixed compression ratio $k/d\\approx0.\\bar{0}2$ ,where $d$ is the number of features in the dataset. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In our first experiment (see Figure 1), we compare $\\upalpha$ -RR, DIANA-RR, and DIANA-RR-1S with classical baselines (QSGD [Alistarh et al., 2017], DIANA [Mishchenko et al., 2019b]) that use a withreplacement mini-batch SGD estimator. DIANA-RR-1S is a memory-friendly version of DIANA-RR that storesandunleshf $h_{t,m}$ on the worker ide ratherthan $n$ individual shifts $h_{t,m}^{\\pi_{m}^{i}}$ tm. Figure 1 illustrates that $\\Omega\\mathrm{.}$ -RR exhibits similar behavior to QSGD, with both methods being slower than DIANA methods across all considered datasets. DIANA-RR-1S and DIANA show comparable convergence rates, indicating that random reshuffing alone, without introducing additional shifts, does not make a significant difference. Finally, DIANA-RR achieves the best rate among all considered non-local methods, efficiently reducing the variance and reaching the lowest functional sub-optimality tolerance. These experimental results align perfectly with our theoretical analysis. ", "page_idx": 9}, {"type": "text", "text": "The second experiment shows that DlANA-based method can significantly outperform in practice when one applies it to local methods as well. In particular, whereas Q-NASTYA shows comparative behavior as existing methods FedCOM [Haddadpour et al., 2021], FedPAQ [Reisizadeh et al., 2020] in all considered datasets, DIANA-NASTYA noticeably outperforms other methods. ", "page_idx": 9}, {"type": "image", "img_path": "CzPtBzgfae/tmp/e2ac22d3324be98c9a682ed86f0199b0540c1c8e3c436a4c25c2f11e062a7c85.jpg", "img_caption": ["Figure 4: The comparison of Q-RR, QSGD, DIANA, and DIANA-RR on the task of training ResNet -18 OnCIFAR-10with $n=10$ workers. Top-1 accuracy on test set is reported. Stepsizes were tuned and workersusedRand- $k$ compressorwith $\\bar{k}/d\\approx0.05$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Training Deep Neural Network model: ResNet-18 on CIFAR-10. Since random reshuffling is a very popular technique in training neural networks, it is natural to test the proposed methods on such problems. Therefore, in the second set of experiments, we consider training ResNet -18 [He et al., 2016] model on the CIFAR10 dataset Krizhevsky and Hinton [2009]. To conduct these experiments we use FL_PyTorch simulator [Burlachenko et al., 2021]. ", "page_idx": 9}, {"type": "text", "text": "The main goal of this experiment is to verify the phenomenon observed in Experiment 1 on the training of a deep neural network. That is, we tested Q-RR, QSGD, DIANA, and DIANA-RR in the distributed training of ResNet -18 on CIFAR10, see Figure 4. As in the logistic regression experiments, we observe that (i) Q-RR and QSGD behave similarly and (i) DIANA-RR outperforms DIANA. For further experimental results and details, we refer to Appendix B. ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we provide the first study of distributed random reshuffing with communication compression. Our theoretical and empirical findings illustrate the inefficiency of naive combination of random reshuffling and communication compression. We also show how this issue can be resolved via the usage of shifts for communication compression. Finally, we develop and analyze methods with random reshufing, communication compression, and local steps. It is worth mentioning that although our theoretical results are obtained for strongly convex problems, the considered methods perform well in the experiments on non-convex tasks like training neural networks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, i) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. ", "page_idx": 10}, {"type": "text", "text": "The work of A. Sadiev and E. Gorbunov (while affiliated with MIPT) was supported by a grant for research centers in the field of artificial intelligence, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730324P540002) and the agreement with the Moscow Institute of Physics and Technology dated November 1,2021 No.70-2021-00138. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffling: optimal rates without component convexity and large epoch requirements. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https : //proceedings.neurips.cc/paper/2020/ hash/cb8acb1dc9821bf74e6ca9068032d623-[]Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "Dan Alistarh, Demjan Grubic, Jerry Z. Li, Ryota Tomioka, and Milan Vojnovic.  QSGD: Communication-efficient SGD via gradient quantization and encoding. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPs'17, page 1707-1718, Red Hook, NY, USA, 2017. Curran Ass0ciates Inc. ISBN 9781510860964.   \nRodolfo Stoffel Antunes, Cristiano Andr\u00e9 da Costa, Arne Kiderle, Imrana Abdullahi Yari, and Bjorn Eskofier. Federated learning for healthcare: Systematic review and architecture proposal. ACM Trans. Intell. Syst. Technol., 13(4), 2022. ISSN 2157-6904. doi: 10.1145/3501813. URL https://doi.0rg/10.1145/3501813.   \nDebraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations. volume 32, 2019.   \nAleksandr Beznosikov, Samuel Horvath, Peter Richtarik, and Mher Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, abs/2002.12410, 2020. URL https : //arXiv.0rg/abs/2002.12410.   \nL\u00e9on Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proceedings of the symposium on learning and data science, Paris, volume 8, pages 2624-2633. Citeseer, 2009.   \nKonstantin Burlachenko, Samuel Horvath, and Peter Richtarik. Fl_pytorch: optimization research simulator for federated learning. In Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning, pages 1-7, 2021.   \nChih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1-27, 2011.   \nIlyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter Richtarik. Ef21 with bells & whistles: Practical algorithmic extensions of modern error feedback. arXiv preprint arXiv:2110.03294,2021.   \nMargalit R. Glasgow, Honglin Yuan, and Tengyu Ma. Sharp bounds for federated averaging (local SGD) and continuous perspective. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 9050-9090. PMLR, 2022. URL https://proceedings.mlr.press/v151/glasgow22a.html.   \nEduard Gorbunov, Filip Hanzely, and Peter Richtarik. A unified theory of SGD: variance reduction, sampling, quantization and coordinate descent. In International Conference on Artificial Intelligence and Statistics, pages 680-690. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "Eduard Gorbunov, Filip Hanzely, and Peter Richtarik. Local SGD: Unified theory and new efficient methods. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3556-3564. PMLR, 13-15 Apr 2021. URL https : //proceedings . mlr.press/v130/gorbunov21a.html. ", "page_idx": 11}, {"type": "text", "text": "Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtarik. Sgd: General analysis and improved rates. In International conference on machine learning, pages 5200-5209. PMLR, 2019. ", "page_idx": 11}, {"type": "text", "text": "Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. ", "page_idx": 11}, {"type": "text", "text": "Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated learning with compression: Unified analysis and sharp guarantees. In Arindam Banerjee and Kenji Fukumizu, editors, The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 2350-2358. PMLR, 2021. URL http: //proceedings.mlr.press/v130/ haddadpour21a.html. ", "page_idx": 11}, {"type": "text", "text": "K. He et al. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. ", "page_idx": 11}, {"type": "text", "text": "Samuel Horvath, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtarik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv: 1904.05115, abs/1904.05115, 2019. URL https : //arXiv . org/abs /1904 . 05115. ", "page_idx": 11}, {"type": "text", "text": "Samuel Horvath, Maziar Sanjabi, Lin Xiao, Peter Richtarik, and Michael Rabbat. FedShuffe: Recipes for better use of local work in federated learning. arXiv preprint arXiv:2204.13169, abs/2204.13169, 2022. URL https : //arXiv.0rg/abs/2204.13169. ", "page_idx": 11}, {"type": "text", "text": "Kun Huang, Xiao Li, Andre Milzarek, Shi Pu, and Junwen Qiu. Distributed random reshuffling over networks. arXiv preprint arXiv:2112.15287, abs/2112.15287, 2021. URL https : //arXiv . org/ abs/2112.15287. ", "page_idx": 11}, {"type": "text", "text": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. PMLR,2015. ", "page_idx": 11}, {"type": "text", "text": "Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. arXiv preprint arXiv: 1912.04977, abs/1912.04977, 2019. URL https: //arXiv.org/abs/1912.04977. ", "page_idx": 11}, {"type": "text", "text": "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: stochastic controlled averaging for federated learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5132-5143. PMLR, 2020. URL http: //proceedings.mlr.press/v119/karimireddy20a.html. ", "page_idx": 11}, {"type": "text", "text": "Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573, abs/1806.06573, 2018. URL https : / /arXiv.org/abs/1806.06573.   \nJakub Konecny, H. Brendan McMahan, Felix Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: strategies for improving communication efficiency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.   \nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report O, University of Toronto, Toronto, Ontario, 2009.   \nMing Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, and He Zhang. Federated learning meets natural language processing: a survey. arXiv preprint arXiv:2107.12603, abs/2107.12603, 2021. URL https: //arXiv.org/abs/2107.12603.   \nGrigory Malinovsky and Peter Richtarik. Federated random reshuffling with compression and variancereduction. arXivpreprint arXiv:2205.03914, abs/2205.03914,2022.URLhttps: / /arXiv.org/abs/2205.03914.   \nGrigory Malinovsky, Alibek Sailanbayev, and Peter Richtarik. Random reshuffing with variance reduction: New analysis and better rates. arXiv preprint arXiv:2104.09342, 2021.   \nGrigory Malinovsky, Konstantin Mishchenko, and Peter Richtarik. Server-side stepsizes and sampling without replacement provably help in federated optimization. arXiv preprint arXiv:2201.11066, abs/2201.11066, 2022. URL https : //arXiv. org/abs/2201 . 11066.   \nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-effcient learning of deep networks from decentralized data. In Artifcial intelligence and statistics, pages 1273-1282. PMLR, 2017.   \nKonstantin Mischenko, Eduard Gorbuov, Martin Takac, and Peter Richtarik Distributed learing with compressed gradient differences. arXiv preprint arXiv: 1901.09269, 2019a.   \nKonstantin Mishchenko, Eduard Gorbunov, Martin Takac, and Peter Richtarik. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, abs/1901.09269, 2019b. URL https : //arXiv.org/abs/1901.09269.   \nKonstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis with vast improvements. In Hugo Larochelle, Marc' Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ c8cc6e90ccbff44c9cee23611711cdc4- []Abstract.html.   \nKonstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Proximal and federated random reshuffing. arXiv preprint arXiv:2102.06704, abs/2102.06704, 2021. URL https : //arXiv. org/abs/2102.06704.   \nAritra Mitra, Rayana Jafar, George J. Pappas, and Hamed Hassani. Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients. In M. RanZato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 14606-14619. Curran Associates, Inc., 2021. URL https: //proceedings.neurips.cc/paper/2021/file/ 7a6bda9ad6ffdac035c752743b7e9d0e-[]Paper.pdf.   \nTomoya Murata and Taiji Suzuki. Bias-variance reduced local SGD for less heterogeneous federated learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 7872-7881. PMLR, 2021. URL http: //proceedings.mlr.press/v139/murata21a.html.   \nJose Javier Gonzalez Ortiz, Jonathan Frankle, Mike Rabbat, Ari Morcos, and Nicolas Ballas. Tradeoffs of local SGD at scale: an empirical study. arXiv preprint arXiv:2110.08133, abs/2110.08133, 2021. URL https: //arXiv.org/abs/21i0.08133.   \nShashank Rajput, Anant Gupta, and Dimitris S. Papailiopoulos. Closing the convergence gap of SGD without replacement.In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 7964-7973. PMLR, 2020. URL http: //proceedings.mlr press/ v119/rajput20a.html.   \nAmirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization. In SilviaChiappa and Roberto Calandra,editors,The 23rd International Conference n ArtifcianllandSttistTTuust0m Italy], volume 108 of Procedings of Machine Learning Research, pages 2021-2031. PMLR, 2020. URL http: //proceedings.mlr.press/v108/reisizadeh20a.html.   \nPeter Richtarik, Elnur Gasanov, and Konstantin Burlachenko. Error feedback reloaded: From quadratic to arithmetic mean of smoothness constants. arXiv preprint arXiv:2402.10774, 2024.   \nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400-407, 1951.   \nMher Safaryan, Filip Hanzely, and Peter Richtarik. Smoothness matrices beat smoothness constants: Better communication compression techniques for distributed optimization. In M. Ranzato, A. Beygelzimer, Y. Dauphin, PS. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 25688-25702. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ d79c6256b9bdac53a55801a066b70da3- []Paper.pdf.   \nItay Safran and Ohad Shamir. Random shuffing beats SGD only after many epochs on ill-conditioned problems. arXiv preprint arXiv:2106.06880, abs/2106.06880, 2021. URL https : / /arXiv . org/ abs/2106.06880.   \nSebastian U. Stich. Unified optimal analysis of the (stochastic) gradient method. arXiv preprint arXiv:1907.04232, abs/1907.04232, 20i9. URL https : //arXiv . org/abs/1907.04232.   \nSebastian U. Stich. On communication compresson for distributed optimization on heterogeneous data. arXiv preprint arXiv:2009.02388, abs/2009.02388, 2020. URL https: //arXiv. org/abs/ 2009.02388.   \nSebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pages 4452-4463,2018. URL https ://proceedings .neurips . c/ paper/2018/hash/b440509a0106086a67bc2ea9df0a1dab- []Abstract .html.   \nZhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, and Bo Li. Communication-efficient distributed deep learning: a comprehensive survey. arXiv preprint arXiv:2003.06307, abs/2003.06307, 2020. URL https: //arXiv.org/abs/2003.06307.   \nJianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communicationefficient distributed optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and RGaett,ditors,Adances in Neural Information Processing Systms, volu 1. Curran Associates,Inc.,2018.URLhttps://proceedings.neurips.cc/paper/2018/file/ 3328bdf9a4b9504b9398284244fe97c2- []Paper.pdf.   \nBlake Woodworth, Brian Bullins, Ohad Shamir, and Nathan Srebro. The min-max complexity of distributed stochastic convex optimization with intermittent communication. arXiv preprint arXiv:2102.01583, abs/2102.01583, 2021. URL https : //arXiv . org/abs/2102 . 01583.   \nBlake E. Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local SGD for heterogeneous distributed learning. In Hugo Larochelle, Marc' Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a. URL https: //proceedings.neurips.cc/paper/2020/hash/ 45713f6ff2041d3fdfae927b82488db8- [] Abstract .html.   \nBlake E. Woodworth, Kumar Kshitij Patel, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan McMahan, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10334- 10343. PMLR, 2020b. URL http: //proceedings .mlr.press/v119/woodworth20a.html.   \nZhaohui Yang, Mingzhe Chen, Kai-Kit Wong, H. Vincent Poor, and Shuguang Cui. Federated learning for 6g: Applications, challenges, and opportunities. Engineering, 8:33-41, 2022. ISSN 2095- 8099. doi: https://doi.org/10.1016/j.eng.2021.12.002. URL https : //www . sciencedirect . com/science/article/pii/S2095809921005245.   \nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.   \nChulhee Yun, Shashank Rajput, and Suvrit Sra. Minibatch vs local SGD with shuffling: Tight convergence bounds and beyond. arXiv preprint arXiv:2110.10342, abs/2110.10342, 2021. URL https://arXiv.org/abs/2110.10342. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1Introduction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1.1 Communication compression 2   \n1.2 Random Reshuffing 2   \n1.3 Can Communication Compression and Random Reshuffling be Friends? 3   \n1.4 Contributions 3   \nAlgorithms and convergence theory 4   \n2.1 Algorithm Q-RR .. 4   \n2.2 Algorithm DIANA-RR . 6   \n2.3Algorithms with Local Steps 7 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "3 Experiments 9 ", "page_idx": 15}, {"type": "text", "text": "4 Conclusion 10 ", "page_idx": 15}, {"type": "text", "text": "A Extra Related Works 18 ", "page_idx": 15}, {"type": "text", "text": "B Experimental details 18 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Logistic Regression 18   \nB.1.1  Experiment 1: Comparison of the Proposed Non-Local Methods with Exist  \ning Baselines. 20   \nB.1.2 Experiment 2: Comparison of the Proposed Local Methods with Existing   \nBaselines 21   \nB.1.3  Experiment 3: Comparison of DIANA-RR with EF21 and DIANA 22   \nB.2 Training Deep Neural Network model: ResNet-18 on CIFAR-10 22   \nB.2.1 Computing Environment 23   \nB.2.2 Loss Function 23   \nB.2.3 Dataset and Metric 24   \nB.2.4 Tuning Process 24   \nB.2.5 Optimization-Based Fine-Tuning for Pretrained ResNet -18. 26   \nB.2.6 Experiments 26   \nB.3Discussion 27   \nC  Missing Proofs for Q-RR 29   \nC.1 Shuffle Radius Clarification 29   \nC.2 Proof of Theorem 2.1 29   \nC.3 Non-Strongly Convex Summands 32   \nD Missing Proofs for DIANA-RR 40   \nD.1 Proof of Theorem 2.2 40   \nD.2 Non-Strongly Convex Summands 44   \nE Theoretical Results for Q-NASTYA and DIANA-NASTYA 52   \nF Missing Proofs for Q-NASTYA 53   \nG Missing Proofs for DIANA-NASTYA 56   \nH Alternative Analysis of Q-NASTYA 61   \nI  Alternative Analysis of DIANA-NASTYA 64   \nPartial Participation for Method with Local Steps 67   \nJ.1 Analysis of Q-NASTYA with Partial Participation 67   \nJ.2 Analysis of DIANA-NASTYA with Partial Participation 72 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Extra Related Works", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Federated optimization has been the subject of intense study, with many open questions even in the setting when all clients have identical data [Woodworth et al., 2020b,a, 2021]. The FedAvg algorithm (also known as Local SGD) has also been a subject of intense study, with tight bounds obtained only very recently by Glasgow et al. [2022]. It is now understood that using many local steps adds bias to distributed SGD, and hence several methods have been developed to mitigate it, e.g. [Karimireddy et al., 2020, Murata and Suzuki, 2021], see the work of Gorbunov et al. [2021] for a unifying lens on many variants of Local SGD. Note that despite the bias, even vanilla FedAvg/Local SGD still reduces the overall communication overhead in practice [Ortiz et al., 2021]. ", "page_idx": 17}, {"type": "text", "text": "The success of RR in the single-machine setting has inspired several recent methods that use it as a local update method as part of distributed training: Mishchenko et al. [2021] developed a distributed variant of random reshuffling, FedRR. FedRR uses RR as a local client update method in lieu of SGD. They show that FedRR can improve upon the convergence of Local SGD when the number of local steps is fixed as the local dataset size, i.e. when $H=n$ . Yun et al. [2021] study the same method under the name Local RR under a more restrictive assumption of bounded inter-machine gradient deviation and show that by varying $H$ to be smaller than $n$ better rates can be obtained in this setting than the rates of Mishchenko et al. [2021]. Other work has explored more such combinations between RR and distributed training algorithms [Huang et al., 2021, Malinovsky et al., 2022, Horvath et al., 2022]. ", "page_idx": 17}, {"type": "text", "text": "There are several methods that combine compression or quantization and local steps: both Basu et al. [2019] and Reisizadeh et al. [2020] combined Local SGD with quantization and sparsification, and Haddadpour et al. [2021] later improved their results using a gradient tracking method, achieving linear convergence under strong convexity. In parallel, Mitra et al. [2021] also developed a variancereduced method, FedLin, that achieves linear convergence under strong convexity despite using local steps and compression. The paper most related to our work is [Malinovsky and Richtarik, 2022] in which the authors combine iterate compression, random reshufling, and local steps. We study gradient compression instead, which is a more common form of compression in both theory and practice [Kairouz et al., 2019]. We compare our results against [Malinovsky and Richtarik, 2022] and show we obtain better rates compared to their work. ", "page_idx": 17}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide missing details on the experimental setting from Section 3. The codes are provided in the following anonymous repository: https : //anonymous . 4open.science/r/ diana_rr-[]B0A5. ", "page_idx": 17}, {"type": "text", "text": "B.1 Logistic Regression ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To confirm our theoretical results we conducted several numerical experiments on binary classification problem with L2 regularized logistic regression of the form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left[f(x)\\stackrel{\\mathrm{def}}{=}\\frac{1}{M}\\sum_{m=1}^{M}\\frac{1}{n_{m}}\\sum_{i=1}^{n_{m}}f_{m,i}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $f_{m,i}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\log\\left(1+\\exp(-y_{m i}a_{m i}^{\\top}x)\\right)+\\lambda\\|x\\|_{2}^{2}\\left(a_{m i},y_{m i}\\right)\\in\\mathbb{R}^{d}\\times\\in\\{-1,1\\},i=1,\\ldots,n_{m}$ are the training data samples stored on machines $m\\,=\\,1,\\ldots,M$ , and $\\lambda\\,>\\,0$ is a regularization parameter. In all experiments, for each method, we used the largest stepsize allowed by its theory multiplied by some individually tuned constant multiplier. For better parallelism, each worker $m$ uses mini-batches of size $\\approx0.1n_{m}$ . In all algorithms, as a compression operator $\\mathcal{Q}$ , we use Rand- $k$ [Beznosikov et al., 2020] with fixed compression ratio $k/d\\approx0.\\bar{0}2$ , where $d$ is the number of features in the dataset. ", "page_idx": 17}, {"type": "text", "text": "Hardware and Software. All algorithms were written in Python 3.8. We used three different CPU cluster node types: ", "page_idx": 17}, {"type": "text", "text": "1. AMD EPYC 7702 64-Core; ", "page_idx": 17}, {"type": "text", "text": "2. Intel(R) Xeon(R) Gold 6148 CPU $@$ 2.40GHz; ", "page_idx": 18}, {"type": "text", "text": "3. Intel(R) Xeon(R) Gold 6248 CPU $\\textcircled{a}\\ 2.50\\mathrm{GHz}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Datasets. The datasets were taken from open LibsVM library Chang and Lin [2011], sorted in ascending order of labels, and equally split among 20 machines \\clients\\workers. The remaining part of size N - 20 [2/20] was assigned to the last worker, where N =M m=1 nm is the total size of the dataset. A summary of the splitting and the data samples distribution between clients can be found in Tables 2, 3, 4, 5. ", "page_idx": 18}, {"type": "table", "img_path": "CzPtBzgfae/tmp/5b3345796979d46684432a7187409378cabb6f582ac858469cc35e8e53ad8033.jpg", "table_caption": ["Table 2: Summary of the datasets and splitting of the data samples among clients. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "CzPtBzgfae/tmp/c07a35ad2ab0a04f55115ac1d2c8b41773307c598283fe31ef0dfbaf18c2f434.jpg", "table_caption": ["Table 3: Partition of the mushrooms dataset among clients "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "CzPtBzgfae/tmp/527f5bbbf696d217aee9a4f9acf18d801b7da1c13d4816bda40879a7f05d3a5b.jpg", "table_caption": ["Table 4: Partition of the w8a dataset among clients. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "CzPtBzgfae/tmp/fb54256720dea8d7eb9be7144c9cddaf64587e3749ddb6a54c9148ed68e5adac.jpg", "table_caption": ["Table 5: Partition of the a9a dataset among clients. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Hyperparameters. Regularization parameter $\\lambda$ was chosen individually for each dataset to guarantee the condition number $L/\\mu$ to be approximately $10^{4}$ , where $L$ and $\\mu$ are the smoothness and strong-convexity constants of function $f$ . For the chosen logistic regression problem of the form (10), smoothness and strong convexity constants $L$ $L_{m}$ \uff0c $L_{i,m},\\mu,\\widetilde{\\mu}$ of functions $f,f_{m}$ and $f_{m}^{i}$ were computed explicitly as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{{\\cal L}}}&{{=}}&{{\\displaystyle\\lambda_{\\mathrm{max}}\\left(\\frac{1}{M}\\sum_{m=1}^{M}{\\frac{1}{4n_{m}}\\bf A}_{m}^{\\top}{\\bf A}_{m}+2\\lambda{\\bf I}\\right)}}\\\\ {{{\\cal L}}_{m}}&{{=}}&{{\\displaystyle\\lambda_{\\mathrm{max}}\\left(\\frac{1}{4n_{m}}{\\bf A}_{m}^{\\top}{\\bf A}_{m}+2\\lambda{\\bf I}\\right)}}\\\\ {{{\\cal L}_{i,m}}}&{{=}}&{{\\displaystyle\\lambda_{\\mathrm{max}}\\left(\\frac{1}{4}a_{m i}a_{m i}^{\\top}+2\\lambda{\\bf I}\\right)}}\\\\ {{{\\displaystyle\\mu}}}&{{=}}&{{2\\lambda}}\\\\ {{{\\displaystyle\\tilde{\\mu}}}}&{{=}}&{{2\\lambda,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{A}_{m}$ is the dataset associated with client $m$ , and $a_{m i}$ is the $i$ -th row of data matrix $\\mathbf{A}_{m}$ . In general, the fact that $f$ is $L$ -smooth with ", "page_idx": 19}, {"type": "equation", "text": "$$\nL\\leq\\frac{1}{M}\\sum_{m=1}^{M}\\frac{1}{n_{m}}\\sum_{i=1}^{n_{m}}L_{i,m}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "follows from the $\\boldsymbol{L}_{i,m}$ -smoothness of $f_{m}^{i}$ (see Assumption 3). ", "page_idx": 19}, {"type": "text", "text": "In all algorithms, as a compression operator $\\mathcal{Q}$ ,we useRand- $k$ as a canonical example of unbiased compressor with relatively bounded variance, and fix the compression parameter $k=\\lfloor0.02d\\rfloor$ ,where $d$ is the number of features in the dataset. ", "page_idx": 19}, {"type": "text", "text": "In addition, in all algorithms, for all clients $m\\,=\\,1,\\ldots,M$ , we set the batch size for the SGD estimator to be $b_{m}=[0.1n_{m}]$ ,where $n_{m}$ is the size of the local dataset. ", "page_idx": 19}, {"type": "text", "text": "The summary of the values $L,L_{m},L_{i,m}\\;L_{\\mathrm{max}},\\mu,b_{m}$ and $k$ for each dataset can be found in Table 6. ", "page_idx": 19}, {"type": "table", "img_path": "CzPtBzgfae/tmp/9cacb72b3778fb431e6a9f547a868460ca8450cfecc124cf08081e92adb4d293.jpg", "table_caption": ["Table 6: Summary of the hyperparameters. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In all experiments, we follow constant stepsize strategy within the whole iteration procedure. For each method, we set the largest possible stepsize predicted by its theory multiplied by some individually tuned constant multiplier. For a more detailed explanation of the tuning routine, see Sections B.1.1 and B.1.2. ", "page_idx": 19}, {"type": "text", "text": "SGD implementation.  We considered two approaches to minibatching: random reshufling and with-replacement sampling. In the first, all clients $m=1,\\dots,M$ independently permute their local datasets and pass through them within the next subsequent $\\lfloor\\frac{n_{m}}{b_{m}}\\rfloor$ steps. In our implementations of Q-RR, Q-NASTYA and DIANA-NASTYA, all clients permuted their datasets in the beginning of every new epoch, whereas for the DIANA-RR method they do so only once in the beginning of the iteration procedure. Second approach of minibatching is called with-replacement sampling, and it requires every client to draw $b_{m}$ data samples from the local dataset uniformly at random. We used this strategy in the baseline algorithms (QSGD, DIANA, FedCOM and FedPAQ) we compared our proposed methods to. ", "page_idx": 19}, {"type": "text", "text": "Experimental setup. To compare the performance of methods within the whole optimization process, we track the functional suboptimality metric $f(x_{t})-f(x_{\\star})$ that was recomputed after each epoch. For each dataset, the value $f(x_{\\star})$ was computed once at the preprocessing stage with $10^{-16}$ tolerance via conjugate gradient method. We terminate our algorithms after performing 5000 epochs ", "page_idx": 19}, {"type": "text", "text": "B.1.1 Experiment 1: Comparison of the Proposed Non-Local Methods with Existing Baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our first experiment (see Figure 1), we compare $\\upalpha$ -RR, DIANA-RR, and DIANA-RR-1S with classical baselines (QSGD [Alistarh et al., 2017], DIANA [Mishchenko et al., 2019b]) that use ", "page_idx": 19}, {"type": "image", "img_path": "CzPtBzgfae/tmp/b286712b542e611384698c24de9351c78df5cf65c84e61df6db154d2ce2e9ec0.jpg", "img_caption": ["Figure 6: Local methods "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 7: The comparison of the proposed methods (Q-NASTYA, DIANA-NASTYA, Q-RR, DIANA-RR), DIANA-RR-1S (a modification of DIANA-RR), and existing baselines (QSGD, DIANA, FedCOM, FedPAQ). All methods use tuned stepsizes and the Rand- $k$ compressor. ", "page_idx": 20}, {"type": "text", "text": "a with-replacement mini-batch SGD estimator. DIANA-RR-1S is a memory-friendly version of DIANA-RR that stores and uses a single shift $h_{t,m}$ on the worker side rather than $n$ individual shifts $h_{t,m}^{\\pi_{m}^{i}}$ . Figure ilutrates that Q-R exhibis similar behavior to QSGD, with both methods being slower than DIANA methods across all considered datasets. DIANA-RR-1S and DIANA show comparable convergence rates, indicating that random reshuffling alone, without introducing additional shifts, does not make a significant difference. Finally, DIANA-RR achieves the best rate among all considered non-local methods, efficiently reducing the variance and reaching the lowest functional sub-optimality tolerance. These experimental results align perfectly with our theoretical analysis. For each of the considered non-local methods, we take the stepsize as the largest one predicted by the theory premultiplied by the individually tuned constant factor from the set {0.000975, 0.00195, 0.0039, 0.0078, 0.0156, 0.0312, 0.0625,0.125, 0.25, 0.5, 1,2,4,8,16, 32, 64, $128,256,512,1024,2048,4096\\}$ ", "page_idx": 20}, {"type": "text", "text": "Therefore, for each local method on every dataset, we performed 20 launches to find the stepsize multiplier showing the best convergence behavior (the fastest reaching the lowest possible level of functional suboptimality $f(x_{t})-f(x_{\\star}))$ ", "page_idx": 20}, {"type": "text", "text": "Theoretical stepsizes for methods $\\Omega\\cdot$ -RR and DIANA-RR are provided by the Theorems 2.1 and 2.2, whereas stepsizes for QSGD and DlANA were taken from the paper Gorbunov et al. [2020]. ", "page_idx": 20}, {"type": "text", "text": "B.1.2 Experiment 2: Comparison of the Proposed Local Methods with Existing Baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The second experiment shows that DlANA-based method can significantly outperform in practice when one applies it to local methods as well. In particular, whereas Q-NASTYA shows comparative behavior as existing methods FedCOM [Haddadpour et al., 2021], FedPAQ [Reisizadeh et al., 2020] in all considered datasets, DIANA-NASTYA noticeably outperforms other methods. ", "page_idx": 20}, {"type": "text", "text": "In this set of experiments, we tuned stepsizes similarly to the non-local methods. However, for algorithms Q-NASTYA, DIANA-NASTYA, and FedCOM we needed to independently adjust the client and server stepsizes, leading to a more extensive tunning routine. ", "page_idx": 20}, {"type": "text", "text": "As before, for each local method on every dataset, tuned client and server stepsizes are defined by the theoretical one and adjusted constant multiplier. Theoretical stepsizes for methods Q-NASTYA and DIANA-NASTYA are given by the Theorems E.1 and E.2, whereas FedCOM and FedPAQ stepsizes were taken from the papers by Haddadpour et al. [2021] and Reisizadeh et al. [2020] respectively. ", "page_idx": 20}, {"type": "text", "text": "We now list all the considered multipliers of client and server stepsizes for every method (i.e. $\\gamma$ and $\\eta$ respectively): ", "page_idx": 21}, {"type": "text", "text": "\u00b7 Q-NASTYA: - Multipliers for $\\gamma:\\{0.000975$ , 0.00195, 0.0039, 0.0078, 0.0156, 0.0312, 0.0625, 0.125, 0.25, 0.5, 1, 2,4, 8, 16, 32, 64,128]; - Multipliers for n : {0.0039, 0.0078, 0.0156, 0.0312, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128}.   \n\u00b7DIANA-NASTYA: - Multipliers for $\\gamma$ and $\\eta:\\{0.000975$ , 0.00195, 0.0039, 0.0078, 0.0156, 0.0312, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32, 64,128};   \n\u00b7FedCOM: - Multipliers for $\\gamma:\\{0.0312$ ,0.0625, 0.125, 0.25, 0.5, 1, 2,4, 8,16, 32, 64, 128, 256, 512,1024, 2048, 4096, 8192, 16384, 32768 ; - Multipliers for  : {0.000975, 0.00195, 0.0039, 0.0078, 0.0156, 0.0312, 0.0625, 0.125, 0.25, 0.5, 1,2, 4,8, 16, 32, 64, 128}.   \n\u00b7 FedPAQ: - Multipliers for $\\gamma:\\{0.00195$ , 0.0039, 0.0078, 0.0156, 0.0312, 0.0625, 0.125, 0.25, 0.5, 1,2,4,8,16,32,64,128,256,512,1024,2048, 4096,8192,16384, 32768,65536, 131072, 262144, 524288, 1048576 }. ", "page_idx": 21}, {"type": "text", "text": "For example, to find the best pair $(\\gamma,\\eta)$ for FedCOM method on each dataset, we performed 378 launches. A similar subroutine was executed for all algorithms on all datasets independently. ", "page_idx": 21}, {"type": "text", "text": "B.1.3 Experiment 3: Comparison of DIANA-RR with EF21 and DIANA ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In our third experiment (see Figure 8), we compared DIANA-RR with DIANA and EF21-SGD [Fatkhullin et al., 2021]. The EF21-SGD is the state-of-the-art algorithm for contractive compressors in distributed non-convex settings. All compared algorithms used a with-replacement mini-batch SGD estimator, consistent with the setup in Section B.1.1. However, in this experiment contrast with Section B.1.1, we employed reliable theoretical step sizes that ensure guaranteed convergence. ", "page_idx": 21}, {"type": "text", "text": "The EF21 algorithm family is designed for usage with contraction compressors in non-convex optimization for $L$ -smooth objective functions in for of Equation 1. For scenarios involving unbiased compressors such as Rand- $k$ , the EF21-SGD can be adapted through scaling [Fatkhullin et al., 2021j. More specifically, an unbiased compressor $\\mathcal{C}(\\boldsymbol{x}):\\mathbb{R}^{\\dot{d}}\\rightarrow\\mathbb{R}^{d}$ which satisfied Assumption 1 via applying the transformation $C^{\\prime}(x)\\stackrel{\\mathrm{def}}{=}(\\omega+1)^{-1}\\cdot{\\mathcal{C}}(x)$ yields a contraction compressor $\\mathbb{E}\\left[\\|C^{\\prime}(x)-x\\|^{2}\\right]\\le(1-\\alpha)\\|x\\|^{2},\\forall x\\in\\mathbb{R}^{d}$ with $\\alpha=1/\\omega\\!+\\!1$ . In particular, this procedure makes Rand- $k$ compatibie with the EF21 algorithm family. In this experiment, we implemented EF21-SGD following the refined analysis from [Richtarik et al., 2024], which offers a stricter better convergence guarantee through improved bounds on the theoretical step size compared to [Fatkhullin et al., 2021]. ", "page_idx": 21}, {"type": "text", "text": "As shown in Figure 8 EF21-SGD does not perform fast enough in scenarios involving using unbiased compressors for strongly-convex optimization problems compared to DIANA-RR and DIANA. ", "page_idx": 21}, {"type": "text", "text": "B.2Training Deep Neural Network model: ResNet-18 on CIFAR-10 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Since random reshuffling is a very popular technique in training neural networks, it is natural to test the proposed methods on such problems. Therefore, in the second set of experiments, we consider training ResNet-18 [He et al., 2016] model on the CIFAR10 dataset Krizhevsky and Hinton [2009]. To conduct these experiments we use FL_PyTorch simulator [Burlachenko et al., 2021]. ", "page_idx": 21}, {"type": "text", "text": "The main goal of this experiment is to verify the phenomenon observed in Experiment 1 on the training of a deep neural network. That is, we tested Q-RR, QSGD, DIANA, and DIANA-RR in the distributed training of ResNet -18 on CIFAR10, see Figure 9. As in the logistic regression experiments, we observe that (i) Q-RR and QSGD behave similarly and (ii) DIANA-RR outperforms DIANA. ", "page_idx": 21}, {"type": "image", "img_path": "CzPtBzgfae/tmp/e66342ea478c1763e00677be21237b5b7908983af8285cb63b91edc8f9703e40.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 8: The comparison of the proposed variance-reduced DIANA-RR and baselines DIANA, EF21- SGD. All algorithms use theoretical step-sizes, Rand- ${\\cdot k}$ compressor, number of workers is 20. ", "page_idx": 22}, {"type": "image", "img_path": "CzPtBzgfae/tmp/5d45660705d494937dd3967153a62a2646a78592e24ecd4b0b6e5c97d288d6a6.jpg", "img_caption": ["Figure 9: The comparison of Q-RR, QSGD, DIANA, and DIANA-RR on the task of training ResNet -18 On CIFAR-10with $n=10$ workers. Top-1 accuracy on test set is reported. Stepsizes were tuned and workersusedRand- $k$ compressorwith $\\bar{k}/d\\approx0.05$ "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "To illustrate the behavior of the proposed methods in training Deep Neural Networks (DNN), we consider the ResNet-18 [He et al., 2016] model. This model is used for image classification, feature extraction for image segmentation, object detection, image embedding, and image captioning. We train all layers of ResNet-18 model meaning that the dimension of the optimization problem equals $d=11$ , 173, 962. During the training, the ResNet-18 model normalizes layer inputs via exploiting 20 Batch Normalization [Ioffe and Szegedy, 2015] layers that are applied directly before nonlinearity in the computation graph of this model. Batch normalization (BN) layers add 9600 trainable parameters to the model. Besides trainable parameters, a BN layer has its internal state that is used for computing the running mean and variance of inputs due to its own specific regime of working.Weuse $H e$ initialization [He et al., 2015]. ", "page_idx": 22}, {"type": "text", "text": "B.2.1  Computing Environment ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We performed numerical experiments on a server-grade machine running Ubuntu 18.04 and Linux Kernel v5.4.0, equipped with 16-cores (2 sockets by 16 cores per socket) $3.3\\:\\mathrm{GHz}$ Intel Xeon, and four NVIDIA A100 GPU with 40GB of GPU memory. The distributed environment is simulated in Python 3.9 via using the software suite FL_PyTorch [Burlachenko et al., 2021] that serves for carrying complex Federate Learning experiments. FL_PyTorch allowed us to simulate the distributed environment in the local machine. Besides storing trainable parameters per client, this simulator stores all not trainable parameters including BN statistics per client. ", "page_idx": 22}, {"type": "text", "text": "B.2.2 Loss Function ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Training of ResNet-18 can be formalized as problem (1) with the following choice of $f_{m}^{i}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{m}(\\boldsymbol{x})=\\frac{1}{|n_{m}|}\\sum_{j=1}^{|n_{m}|}C E(b^{(j)},g(a^{(j)},\\boldsymbol{x})),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r}{C E(p,q)\\stackrel{\\mathrm{def}}{=}-\\sum_{k=1}^{\\#\\mathrm{classes}}p_{i}\\cdot\\log(q_{i})}\\end{array}$ $0\\!\\cdot\\!\\log(0)=0$ $g:\\mathbb{R}^{28\\times28}\\times\\mathbb{R}^{d}\\rightarrow[0,1]^{\\#{\\mathrm{classe}}}$ $a^{(j)}$ of parameters $x$ as an input and returning a vector in probability simplex, and $n_{m}$ is the size of the dataset onworker $m$ \uff1a ", "page_idx": 23}, {"type": "text", "text": "B.2.3 Dataset and Metric ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our experiments, we used CIFAR10 dataset Krizhevsky and Hinton [2009]. The dataset consists of input variables $a_{i}\\in\\mathbb{R}^{28\\times28\\times3}$ , and response variables $\\dot{b_{i}}\\in\\{0,1\\}^{10}$ and is used fortraining 10-way classification. The sizes of training and validation set are $5\\times10^{4}$ and $10^{4}$ respectively.The training set is partitioned heterogeneously across 10 clients. To measure the performance, we evaluate the loss function value $f(x)$ , norm of the gradient $\\Vert\\nabla f(x)\\Vert_{2}$ and the Top-1 accuracy of the obtained model as a function of passed epochs and the normalized number of bits sent from clients to the server. ", "page_idx": 23}, {"type": "text", "text": "B.2.4  Tuning Process ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this set of experiments, we tested QSGD [Alistarh et al., 2017], Q-RR (Algorithm 1), DlANA [Mishchenko et al., 2019a] and DIANA-RR (Algorithm 2) algorithms. For all algorithms, wetuned thestrategy $\\in\\{A,B,C\\}$ of decaying stepsize model via selecting the best in terms of the norm of the full gradient on the train set in the final iterate produced after 20000 rounds. The stepsize policies are described below. ", "page_idx": 23}, {"type": "text", "text": "A. Stepsizes decaying as inverse square root of the number epochs ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\gamma_{e}=\\left\\{\\!\\!\\begin{array}{l l}{\\gamma_{i n i t}\\cdot\\displaystyle\\frac{1}{\\sqrt{e-s+1}},}&{\\mathrm{if~}e\\geq s,}\\\\ {\\gamma_{i n i t},}&{\\mathrm{if~}e<s,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\gamma_{e}$ denotes the stepsize used during epoch $e+1,s$ is a fixed shift. ", "page_idx": 23}, {"type": "text", "text": "B. Stepsizes decaying as inverse of number epochs ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\gamma=\\left\\{\\!\\!\\begin{array}{l l}{\\gamma_{i n i t}\\cdot{\\frac{1}{e-s+1}},}&{{\\mathrm{if}\\;e\\geq s,}}\\\\ {\\gamma_{i n i t},}&{{\\mathrm{if}\\;e<s.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C. Fixed stepsize ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\gamma=\\gamma_{i n i t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We say that the algorithm passed $e$ epochs if the total number of computed gradient oracles lies between $e\\sum_{m=1}^{M}{\\bar{n}}_{m}$ and $(e+1)\\sum_{m=1}^{M}n_{m}$ For each algorithm the used stepsize $\\gamma_{i n i t}$ and shift parameter $s$ were tuned via selecting from the following sets: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{i n i t}\\in\\gamma_{s e t}\\overset{\\mathrm{def}}{=}\\{4.0,3.75,3.00,2.5,2.00,1.25,1.0,0.75,0.5,0.25,}\\\\ {0.2,0.1,0.06,0.03,0.01,0.003,0.001,0.0006\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\ns\\in s_{s e t}\\stackrel{\\mathrm{def}}{=}\\{50,100,200,500,1000\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In all tested methods, clients independently apply Rand- $k$ compression with carnality $k=\\lfloor0.05d\\rfloor$ Computation for all gradient oracles is carried out in single precision float (FP64) arithmetic. ", "page_idx": 23}, {"type": "image", "img_path": "CzPtBzgfae/tmp/f098f24f32b4c05a85c4d38a081b6f81dafe91bcae18ba46ff2eb7129c8bf75f.jpg", "img_caption": ["Figure 10 "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 11: Comparison of QSGD and Q-RR in the training of ResNet-18 on CIFAR-10, with $n=10$ workers. Here (a) and (d) show Top-1 accuracy on test set, (b) and (e) - norm of full gradient on the train set, (c) and (f) - loss function value on the train set. Stepsizes and decay shift has been tuned from $s_{s e t}$ and $\\gamma_{s e t}$ based on minimum achievable value of loss function on the train set. ", "page_idx": 24}, {"type": "image", "img_path": "CzPtBzgfae/tmp/cdfd03c9edd69e170313b93d6b230c4bf97455a525228f537c1b5e5003c303a8.jpg", "img_caption": ["Figure 12: Comparison of DIANA and DIANA-RR in the training of ResNet -18 on CIFAR-10, with $n=10$ workers. Here (a) and (d) show Top-1 accuracy on test set, (b) and (e) - norm of full gradient on the train set, (c) and (f) - loss function value on the train set. Stepsizes and decay shift has been tuned from $s_{s e t}$ and $\\gamma_{s e t}$ based on minimum achievable value of loss function on the train set. For both algorithms stepsize is fixed. For both algorithms stepsize is decaying according to srategy $B$ "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "B.2.5  Optimization-Based Fine-Tuning for Pretrained ResNet - 18. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this setting, we trained ResNet -18 image classification in a distributed way across $n=10$ clients.   \nIn this experiment, we have trained only the last linear layer. ", "page_idx": 25}, {"type": "text", "text": "Next, we have turned off batch normalization. Turning off batch normalization implies that the computation graph of NN $g(a,x)$ with weights of NN denoted as $x$ is a deterministic function and does not include any internal state. ", "page_idx": 25}, {"type": "text", "text": "The loss function is a standard cross-entropy loss augmented with extra $\\ell_{2}$ -regularization $\\alpha\\|\\boldsymbol{x}\\|^{2}/2$ With $\\alpha=0.0001$ . Initially used weights of NN are pretrained parameters after training the model on ImageNet. ", "page_idx": 25}, {"type": "text", "text": "The dataset distribution across clients has been set in a heterogeneous manner via presorting dataset $D$ by label class and after this, it was split across 10 clients. ", "page_idx": 25}, {"type": "text", "text": "The comparison of stepsizes policies used in QSGD and Q-RR is presented in Figure 14. The behavior of the algorithms with best tuned step sizes is presented in Figure 13. These results demonstrate that in this setting there is no real benefit of using Q-RR in comparison to QSGD. ", "page_idx": 25}, {"type": "text", "text": "B.2.6 Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The comparison of QSGD and $\\upalpha$ -RR is presented in Figure 11. In particular, Figure 9 shows that in terms of the convergence to stationary points both algorithms exhibit similar behavior. However, QRR has better generalization and in fact, converges to the better loss function value. This experiment demonstrates that $\\boldsymbol{\\Omega}$ -RR with manually tuned stepsize can be better compared to QSGD in terms of the final quality of obtained Deep Learning model. For QSGD the tuned meta parameters are: ", "page_idx": 25}, {"type": "text", "text": "The results of comparison of DIANA and DIANA-RR are presented in Figure 12. For DIANA the tuned meta parameters are: $\\gamma_{i n i t}=1.0,s=0$ ,strategy $=C$ and for DIANA-RR tuned meta parameters are: $\\gamma_{i n i t}=1.0$ $s=0$ , strategy $=C$ . These results show that DIANA-RR outperforms DIANA in terms of all reported metrics. ", "page_idx": 26}, {"type": "text", "text": "B.3Discussion ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "More about used arithmetics. We used FP64 (IEEE 754) due to its superior numerical stability compared to FP32, FP16, and BF1oat16. While FP32 and FP16 are commonly used for inference tasks, the choice of precision for training depends on the specific requirements of the task. In certain cases, FP32 may be sufficient, but for others, FP64 is necessary to ensure stability. ", "page_idx": 26}, {"type": "text", "text": "The performance gain from switching from FP64 to FP32 can indeed vary based on the GPU model. For instance, the NVIDIA A100 40GB GPU used in our experiments offers approximately a two-fold increase in computational throughput with FP32 compared to FP64. The specific architecture of the GPU influences the choice of precision, and these characteristics can differ across various GPU models and updates. ", "page_idx": 26}, {"type": "text", "text": "The computational burden. The primary focus of the paper is to highlight the fundamental complexities and limits of algorithmic behavior. The experiments presented in our paper are intended for illustrative purposes. ", "page_idx": 26}, {"type": "text", "text": "The computational demands of our work are significant. Performing experiments beyond ResNet-18/CIFAR-10/FP64 with 10 clients is near the limit of what is feasible with our computational resources. In our simulation involving 10 clients sharing a common dataset, we ran 2000 rounds/epochs for fine-tuning. Based on an estimate of 2 minutes per epoch, the total computation time would be approximately 66 hours per run (2 minutes/epoch $\\times\\,2000$ epochs $=66$ hours).Taking into account the grid search with 18 preset learning rates, 5 sets of decay parameters, and 4 algorithms, the total estimated computation time would be around 23760 hours (66 hours $\\times\\ 18\\times5\\times4_{,}$ ).This represents a substantial amount of computation time. Therefore, conducting a comprehensive comparison involving four algorithms with an extensive grid of hyperparameters is already challenging for models larger than ResNet-18 on CIFAR10. To cover 23760 hours of training would indeed require approximately 40 GPUs running continuously for about 25 days. Nonetheless, we have conducted numerous experiments to ensure a thorough and fair comparison. ", "page_idx": 26}, {"type": "text", "text": "Training in overparameterized regime._ During training image classification Convolution Neural Networks, we got two results for QSGD-RR as an improvement of QSGD. During training only the last layer (see Fig. 13) there are no benefits QSGD-RR, but QSGD does not behave worse. ", "page_idx": 26}, {"type": "text", "text": "When training the whole network (Fig. 11), the results suggest that Q-RR is much better than Q-SGD. Although we do not have formal proof explaining this phenomenon, we conjecture that this can be related to the significant overparameterization occurring during the training of a large model on a relatively small dataset. That is, the model can almost perfectly fit the training data on all clients, leading to a decrease in the heterogeneity parameter. In this case, there is no need for shifts since the variance coming from compression naturally goes to zero, and the complexities of QSGD and DIANA match (see Table 1). In this situation, Q-RR performs better than QSGD since the compression does not spoil the convergence of RR. Therefore, DIANA-type shifts are not always necessary to get improvements. Nevertheless, we conjecture that they are necessary when the datasets are larger and morecomplex. ", "page_idx": 26}, {"type": "image", "img_path": "CzPtBzgfae/tmp/79c4600fbc5401d819373771479c8a09d8af65b4200122bb34f675d05c590b1d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 13: Comparison of QSGD and $\\Omega.$ -RR in the training of the last linear layer of ResNet-18 on CIFAR-10, with $n=10$ workers. Here (a) shows Top-1 accuracy on test set, (b) - norm of full gradient on the train set, (c) - loss function value on the train set. Stepsizes and decay shift has been tuned from $s_{s e t}$ and $\\gamma_{s e t}$ based on minimum achievable value of loss function on the train set. Both algorithms used fixed stepsize during training. ", "page_idx": 27}, {"type": "image", "img_path": "CzPtBzgfae/tmp/eac9201d054afa5b62e2c79a320580c92506910e3e768b8443f1e89e9edc507e.jpg", "img_caption": ["Figure 14: Comparison of QSGD and Q-RR in the training of the last linear layer of ResNet-18 on CIFAR-10,with $n=10$ workers. Here (a) and (b) show Top-1 accuracy on test set, (c) and (d) - loss function value on the train set, (e) and (f) - norm of full gradient on the train set. Stepsizes and decay shift has been tuned from $s_{s e t}$ and $\\gamma_{s e t}$ based on minimum achievable value of loss function on the train set. During training stepsize was fixed. Batch Normalization was turned off. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "C Missing Proofs for Q-RR ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the main part of the paper, we introduce Assumptions 3 and 4 for the analysis of $\\boldsymbol{\\Omega}$ RRand DIANA-RR. These assumptions can be refined as follows. ", "page_idx": 28}, {"type": "text", "text": "Assumption 5. Function $\\begin{array}{r}{f^{\\pi^{i}}=\\frac{1}{M}\\sum_{i=1}^{M}f_{m}^{\\pi_{m}^{i}}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}}\\end{array}$ .is $\\widetilde{L}$ smoothforallsesof peuaions $\\pi=(\\pi_{1},\\ldots,\\pi_{m})$ from $[n]$ and alt $i\\in\\lfloor n\\rfloor$ , i.e., ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n],\\pi}\\|\\nabla f^{\\pi^{i}}(x)-\\nabla f^{\\pi^{i}}(y)\\|\\leq\\widetilde{L}\\|x-y\\|\\quad\\forall x,y\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Assumption 6. Function $\\begin{array}{r}{f^{\\pi^{i}}\\;=\\;\\frac{1}{M}\\sum_{i=1}^{M}f_{m}^{\\pi_{m}^{i}}\\;:\\;\\mathbb{R}^{d}\\;\\to\\;\\mathbb{R}}\\end{array}$ is $\\widetilde{\\mu}$ strongly convex for all sets of permutations $\\pi=(\\pi_{1},\\ldots,\\pi_{m})$ from $[n]$ and all $i\\in[n]$ i.e., ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[n],\\pi}\\left\\{f^{\\pi^{i}}(x)-f^{\\pi^{i}}(y)-\\langle\\nabla f^{i,\\pi}(y),x-y\\rangle\\right\\}\\geq\\frac{\\widetilde{\\mu}}{2}\\|x-y\\|^{2}\\quad\\forall x,y\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, functions $f_{1}^{i},f_{2}^{i},\\dotsc,f_{M}^{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ are convex for all $i=1,\\hdots,n$ ", "page_idx": 28}, {"type": "text", "text": "We notice that Assumptions 3 and 4 imply Assumptions 5 and 6. In the proofs of the results for Q-RR and DIANA-RR, we use Assumptions 5 in addition to Assumptions 3 and we use Assumption 6 instead of Assumption 4. ", "page_idx": 28}, {"type": "text", "text": "C.1  Shuffle Radius Clarification ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Our results depend on the so-called shuffling radius proposed by Mishchenko et al. [2021]: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{rad}}^{2}\\overset{\\mathrm{def}}{=}\\operatorname*{max}_{i}\\left\\{\\frac{1}{\\gamma^{2}M}\\sum_{m=1}^{M}\\mathbb{E}D_{f_{m}^{\\pi^{i}}}(x_{\\star}^{i},x_{\\star})\\right\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where x\u00b2+1 $\\begin{array}{r}{\\boldsymbol{x}_{\\star}^{i+1}=\\boldsymbol{x}_{\\star}^{i}-\\frac{\\gamma}{M}\\sum_{m=1}^{M}\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol{x}_{\\star}).}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "One can think of the shuffling radius as a counterpart to the variance term in SGD. Both concepts measure how much the algorithm's performance can fuctuate near the optimal solution, but the cause of these fuctuations is different: in SGD, it is due to random sampling, and in RR, it is due to reshuffling. Additionally, Lemma 2.1 provides bounds for the shuffling radius \u2014 showing the maximum and minimum possible values \u2014\u2014 based on the variance at the optimum, reinforcing the shuffling radius as a useful way to understand how RR behaves. This relationship helps clarify how the reshuffling process influences the algorithm's path and its efficiency in reaching an optimal point. ", "page_idx": 28}, {"type": "text", "text": "C.2Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For convenience, we restate the theorem below. ", "page_idx": 28}, {"type": "text", "text": "Thore C1Thm.1) Asons 1 5 6h d $\\begin{array}{r}{0<\\gamma\\le\\frac{1}{\\widetilde{L}+2\\frac{\\omega}{M}L_{\\operatorname*{max}}}}\\end{array}$ .Then,for all $T\\geq0$ the iterates produced by Q-RR satisfy ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|x_{T}-x_{\\star}\\|^{2}\\leq(1-\\gamma\\widetilde{\\mu})^{n T}\\,\\|x_{0}-x_{\\star}\\|^{2}+\\displaystyle\\frac{2\\gamma^{2}\\sigma_{r a d}^{2}}{\\widetilde{\\mu}}+\\displaystyle\\frac{2\\gamma\\omega}{\\widetilde{\\mu}M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right),}\\\\ {\\varrho\\,\\zeta_{\\star}^{2}=\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{\\star})\\|^{2},a n d\\,\\sigma_{\\star}^{2}=\\frac{1}{M n}\\sum_{m=1}^{M}\\displaystyle\\sum_{i=1}^{n}\\|\\nabla f_{m}^{i}(x^{\\star})-\\nabla f_{m}(x^{\\star})\\|^{2}.}\\end{array}\n$$wher ", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof Using $\\begin{array}{r}{x_{\\star}^{i+1}=x_{\\star}^{i}-\\frac{\\gamma}{M}\\sum_{m=1}^{M}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})}\\end{array}$ and line 7of Algorithm 1, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}}&{=}&{\\displaystyle\\left\\|x_{t}^{i}-x_{\\star}^{i}-\\gamma\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right)\\right\\|^{2}}\\\\ &{=}&{\\displaystyle\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}-2\\gamma\\left\\langle\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right),x_{t}^{i}-x_{\\star}^{i}\\right\\rangle}\\\\ &&{+\\gamma^{2}\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\mathcal{Q}}\\left[\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}\\right]}&{=}&{\\displaystyle\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}-2\\gamma\\left\\langle\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right),x_{t}^{i}-x_{\\star}^{i}\\right\\rangle}\\\\ &&{+\\gamma^{2}\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In view of Assumption 1 and $\\mathbb{E}_{\\xi}\\|\\xi-c\\|^{2}=\\mathbb{E}_{\\xi}\\|\\xi-\\mathbb{E}_{\\xi}\\xi\\|^{2}+\\|\\mathbb{E}_{\\xi}\\xi-c\\|^{2}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\|x_{t}^{i+1}-x_{*}^{i+1}\\|^{2}\\right]}&{=}&{\\Big\\|x_{t}^{i}-x_{t}^{i}\\Big\\|^{2}-\\frac{2\\gamma}{M}\\displaystyle\\sum_{m=1}^{M}\\left\\langle\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t}^{i})-\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{*}),x_{t}^{i}-x_{*}^{i}\\right\\rangle}\\\\ &{\\quad+\\gamma^{2}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\frac{\\Delta}{\\Delta\\eta-1}\\left(Q\\left(\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t}^{i})\\right)-\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t}^{i})\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad+\\gamma^{2}\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\frac{M}{\\sum_{m=1}^{M}\\left(\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t}^{i})-\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t})\\right)}\\right\\|^{2}}\\\\ {\\leq}&{\\Big\\|x_{t}^{i}-x_{t}^{i}\\Big\\|^{2}-\\displaystyle\\frac{2\\gamma}{M}\\displaystyle\\sum_{m=1}^{M}\\left\\langle\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t}^{i})-\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{*}),x_{t}^{i}-x_{*}^{i}\\right\\rangle}\\\\ &{\\quad+\\gamma^{2}\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\left(\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t}^{i})-\\nabla f_{m^{\\varepsilon}}^{\\varepsilon^{\\varepsilon}}(x_{*})\\right)\\right\\|^{2}}\\\\ &{\\quad+\\frac{\\gamma^{2}\\alpha}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}^{\\varepsilon^{\\varepsilon}}(x_{t}^{i})\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where in the last step we apply independence of $\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)$ for $m\\,\\in\\,[M]$ . Next, we use three-point identity4 and obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{Q}\\left[\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}\\right]}&{\\le}&{\\displaystyle\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}}\\\\ &&{\\displaystyle-\\frac{2\\gamma}{M}\\sum_{m=1}^{M}\\left(D_{f_{m}^{\\pi_{m}^{i}}}(x_{\\star}^{i},x_{t}^{i})+D_{f_{m}^{\\pi_{m}^{i}}}(x_{t}^{i},x_{\\star})-D_{f_{m}^{\\pi_{m}^{i}}}(x_{\\star}^{i},x_{\\star})\\right)}\\\\ &&{\\displaystyle+\\gamma^{2}\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right)\\right\\|^{2}}\\\\ &&{\\displaystyle+\\frac{\\gamma^{2}\\omega}{M^{2}}\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Applying $\\widetilde{L}$ smothnesad convexit of $\\textstyle{\\frac{1}{M}}\\sum_{m=1}^{m}f_{m}^{\\pi_{m}^{i}}$ \uff0c $\\widetilde{\\mu}$ strong convxit of $\\textstyle{\\frac{1}{M}}\\sum_{m=1}^{m}f_{m}^{\\pi_{m}^{i}}$ and $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}^{i}$ , we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\|x_{t}^{i+1}-x_{*}^{i+1}\\|^{2}\\right]}&{\\leq\\ (1-\\gamma\\tilde{\\mu})\\left\\|x_{t}^{i}-x_{*}^{i}\\right\\|^{2}-2\\gamma\\left(1-\\tilde{L}\\gamma\\right)\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\star\\star}}(x_{t}^{i},x_{*})}\\\\ &{\\quad+2\\gamma\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\star\\star}}(x_{*}^{i},x_{*})+\\frac{\\gamma^{2}\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}^{\\star\\star}(x_{t}^{i})\\right\\|^{2}}\\\\ {\\leq\\ }&{(1-\\gamma\\tilde{\\mu})\\left\\|x_{t}^{i}-x_{*}^{i}\\right\\|^{2}-2\\gamma\\left(1-\\tilde{L}\\gamma\\right)\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\star\\star}}(x_{t}^{i},x_{*})}\\\\ &{\\quad+2\\gamma\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\star\\star}}(x_{*}^{i},x_{*})+\\frac{2\\gamma^{2}\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}^{\\star\\star}(x_{*})\\right\\|^{2}}\\\\ &{\\quad+\\frac{2\\gamma^{2}\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}^{\\star\\star}(x_{t}^{i})-\\nabla f_{m}^{\\star\\star}(x_{*})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "So, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\mathcal{Q}}\\left[\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}\\right]}&{\\leq}&{\\left(1-\\gamma\\widetilde{\\mu}\\right)\\left\\Vert x_{t}^{i}-x_{\\star}^{i}\\right\\Vert^{2}+\\displaystyle\\frac{2\\gamma^{2}\\omega}{M^{2}}\\sum_{m=1}^{M}\\left\\Vert\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right\\Vert^{2}}\\\\ &&{\\displaystyle+\\frac{2\\gamma}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\pi_{m}^{i}}}(x_{\\star}^{i},x_{\\star})}\\\\ &&{\\displaystyle-2\\gamma\\left(1-\\gamma\\left(\\widetilde{L}+\\frac{2\\omega L_{\\operatorname*{max}}}{M}\\right)\\right)\\frac{1}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\pi_{m}^{i}}}(x_{t}^{i},x_{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking the fl expetatio and usin a defiton o shufl radust $\\begin{array}{r}{0\\,<\\,\\gamma\\,\\leq\\,\\frac{1}{\\left(\\widetilde{L}+2\\frac{\\omega}{M}L_{\\operatorname*{max}}\\right)}}\\end{array}$ and $D_{f_{m}^{\\pi_{m}^{i}}}(x_{t}^{i},x_{\\star})\\ge0$ we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}\\right]}&{\\leq}&{\\left(1-\\gamma\\widetilde{\\mu}\\right)\\mathbb{E}\\left[\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}\\right]+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}+\\displaystyle\\frac{2\\gamma^{2}\\omega}{M^{2}}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\left\\|\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right\\|^{2}\\right]}\\\\ &{=}&{\\left(1-\\gamma\\widetilde{\\mu}\\right)\\mathbb{E}\\left[\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}\\right]+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}+\\displaystyle\\frac{2\\gamma^{2}\\omega}{M^{2}n}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=1}^{n}\\left\\|\\nabla f_{m}^{j}(x_{\\star})\\right\\|^{2}}\\\\ &{\\leq}&{\\left(1-\\gamma\\widetilde{\\mu}\\right)\\mathbb{E}\\left[\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}\\right]+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}+\\displaystyle\\frac{2\\gamma^{2}\\omega}{M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Unrolling the recurrence in $i$ , we derive ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{t+1}-x_{\\star}\\|^{2}\\right]}&{\\leq}&{\\displaystyle(1-\\gamma\\widetilde{\\mu})^{n}\\,\\mathbb{E}\\left[\\|x_{t}-x_{\\star}\\|^{2}\\right]+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}}\\\\ &&{+\\displaystyle\\frac{2\\gamma^{2}\\omega}{M}\\,\\big(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\big)\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Unrolling the recurrence in $t$ , we derive ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{T}-x_{\\star}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\gamma\\widetilde{\\mu}\\right)^{n T}\\left\\|x_{0}-x_{\\star}\\right\\|^{2}+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}\\sum_{t=0}^{T-1}(1-\\gamma\\widetilde{\\mu})^{n t}\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}}\\\\ &&{\\displaystyle+\\frac{2\\gamma^{2}\\omega}{M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)\\sum_{j=0}^{n T-1}\\left(1-\\gamma\\widetilde{\\mu}\\right)^{n t}\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{j=0}^{n T-1}(1-\\gamma\\widetilde{\\mu})^{j}\\le\\frac{1}{\\gamma\\widetilde{\\mu}}}\\end{array}$ we get the resut. ", "page_idx": 30}, {"type": "text", "text": "Corollary 5. Let the assumptions of Theorem C.1 hold and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{\\widetilde{L}+2\\frac{\\omega}{M}L_{\\operatorname*{max}}},\\sqrt{\\frac{\\varepsilon\\widetilde{\\mu}}{6\\sigma_{r a d}^{2}}},\\frac{\\varepsilon\\widetilde{\\mu}M}{6\\omega\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)}\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, Q-RR finds a solution with accuracy $\\varepsilon\\,>\\,0$ after the following number of communication rounds: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{\\widetilde{L}}{\\widetilde{\\mu}}+\\frac{\\omega}{M}\\frac{L_{\\mathrm{max}}}{\\widetilde{\\mu}}+\\frac{\\omega}{M}\\frac{\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}}{\\varepsilon\\widetilde{\\mu}^{2}}+\\frac{\\sigma_{r a d}}{\\sqrt{\\varepsilon\\widetilde{\\mu}^{3}}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Theorem C.1 implies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|x_{T}-x_{\\star}\\|^{2}\\leq\\left(1-\\gamma\\widetilde{\\mu}\\right)^{n T}\\|x_{0}-x_{\\star}\\|^{2}+\\frac{2\\gamma^{2}\\sigma_{\\mathrm{rad}}^{2}}{\\widetilde{\\mu}}+\\frac{2\\gamma\\omega}{\\widetilde{\\mu}M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To estimate the number of communication rounds required to find a solution with accuracy $\\varepsilon>0$ ,we need to upper-bound each term from the right-hand side by $\\varepsilon/3$ . Thus, we get additional conditions on $\\gamma$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{2\\gamma^{2}\\sigma_{\\mathrm{rad}}^{2}}{\\widetilde{\\mu}}<\\frac{\\varepsilon}{3},\\quad\\frac{2\\gamma\\omega}{\\widetilde{\\mu}M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)<\\frac{\\varepsilon}{3}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and also the upper bound on the number of communication rounds $n T$ ", "page_idx": 31}, {"type": "equation", "text": "$$\nn T={\\tilde{\\mathcal{O}}}\\left({\\frac{1}{\\gamma{\\tilde{\\mu}}}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Substituting (12), we get a final result ", "page_idx": 31}, {"type": "text", "text": "C.3 Non-Strongly Convex Summands ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we provide the analysis of Q-RR without using Assumptions 4, 6. Before we move one to the proofs, we would like to emphasize that ", "page_idx": 31}, {"type": "equation", "text": "$$\nx_{t}^{i+1}=x_{t}^{i}-\\gamma\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}-\\gamma\\sum_{i=0}^{n-1}\\frac{1}{M}\\sum_{m=1}^{M}\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)=x_{t}-\\tau\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\tau=\\gamma n$ . For convenience, we denote ", "page_idx": 31}, {"type": "equation", "text": "$$\ng_{t}=\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "allowing to write the update rule as $x_{t+1}=x_{t}-\\tau g_{t}$ ", "page_idx": 31}, {"type": "text", "text": "Lemma C.1 (Lemma 1 from [Malinovsky et al., 2022]). For any $k\\;\\in\\;[n]$ let $\\xi_{\\pi_{1}},\\ldots,\\xi_{\\pi_{k}}$ be sampled uniformly without replacement from a set of vectors $\\{\\xi_{1},\\dotsc,\\xi_{n}\\}$ and $\\bar{\\xi}_{\\pi}$ be their average. Then,itholds ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bar{\\xi}_{\\pi}=\\bar{\\xi},\\quad\\mathbb{E}\\left[\\|\\bar{\\xi}_{\\pi}-\\bar{\\xi}\\|^{2}\\right]=\\frac{n-k}{k(n-1)}\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$\\begin{array}{r}{\\bar{\\xi}=\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i},\\,\\bar{\\xi}_{\\pi}=\\frac{1}{k}\\sum_{i=1}^{k}\\xi_{\\pi_{i}},\\,\\sigma^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\xi_{i}-{\\bar{\\xi}}\\|^{2}}\\end{array}$ ", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma C.2. Under Assumptions 1, 2, 3, 5, the following inequality holds ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Q}\\left[-2\\tau\\langle g_{t},x_{t}-x_{\\star}\\rangle\\right]\\leq-\\frac{\\tau\\mu}{2}\\|x_{t}-x_{\\star}\\|^{2}-\\tau(f(x_{t})-f(x_{\\star}))+\\frac{\\tau\\widetilde L}{n}\\sum_{i=0}^{n-1}\\|x_{t}^{i}-x_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Using that $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{Q}}\\left[g_{t}\\right]=\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})}\\end{array}$ and definition of $h^{\\star}$ , we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle-2\\tau\\mathbb{E}_{Q}\\left[\\left\\langle g_{t},x_{t}-x_{\\star}\\right\\rangle\\right]}&{=}&{\\displaystyle-\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\left\\langle\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i}),x_{t}-x_{\\star}\\right\\rangle}\\\\ &{=}&{\\displaystyle-\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\langle\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star}),x_{t}-x_{\\star}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using three-point identity, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle-2\\tau\\mathbb{E}_{Q}\\left[\\langle g_{t},x_{t}-x_{\\star}\\rangle\\right]}&{=}&{\\displaystyle-\\frac{2\\tau}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left(D_{f_{m}^{\\tau_{i}^{i}}}(x_{t},x_{\\star})+D_{f_{m}^{\\tau_{i}^{i}}}(x_{\\star},x_{t}^{i})-D_{f_{m}^{\\tau_{i}^{i}}}(x_{t},x_{t}^{i})\\right)}\\\\ &{=}&{\\displaystyle-2\\tau D_{f}(x_{t},x_{\\star})-\\frac{2\\tau}{n}\\sum_{i=0}^{n-1}D_{f^{\\tau_{i}^{i}}}(x_{\\star},x_{t}^{i})+\\frac{2\\tau}{n}\\sum_{i=0}^{n-1}D_{f^{\\tau_{i}^{i}}}(x_{t},x_{t}^{i})}\\\\ &{\\le}&{\\displaystyle-2\\tau D_{f}(x_{t},x_{\\star})+\\frac{\\tau\\tilde{L}}{n}\\sum_{i=0}^{n-1}\\|x_{t}^{i}-x_{t}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where in the last inequality we apply $\\widetilde{L}$ -smoothness and convexity of each function $f^{\\pi^{i}}$ . Finally, using $\\mu$ -strong convexity of $f$ , we finish the proof of the lemma. ", "page_idx": 32}, {"type": "text", "text": "Lemma C.3. Under Assumptions 1, 2, 3, 5, the following inequality holds ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\mathcal{Q}}\\left[\\|g_{t}\\|^{2}\\right]}&{\\leq}&{2\\widetilde{L}\\left(\\widetilde{L}+\\displaystyle\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]+\\displaystyle\\frac{4\\omega}{M n}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)}\\\\ &&{\\displaystyle+8\\left(\\widetilde{L}+\\displaystyle\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)(f(x_{t})-f(x_{\\star})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Taking the expectation w.r.t. $\\mathcal{Q}$ and using variance decomposition $\\begin{array}{r l r}{\\mathbb{E}\\left[\\|\\xi\\|^{2}\\right]}&{{}=}&{}\\end{array}$ $\\mathbb{E}\\left[\\|\\boldsymbol{\\xi}-\\mathbb{E}\\left[\\boldsymbol{\\xi}\\right]\\|^{2}\\right]+\\|\\mathbb{E}\\boldsymbol{\\xi}\\|^{2}$ , we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{Q}\\left[\\|g_{t}\\|^{2}\\right]}&{=}&{\\mathbb{E}_{Q}\\left[\\left\\|\\displaystyle\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}Q\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)\\right\\|^{2}\\right]}\\\\ &{=}&{\\mathbb{E}_{Q}\\left[\\left\\|\\displaystyle\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\left(Q\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)\\right\\|^{2}\\right]}\\\\ &&{+\\left\\|\\displaystyle\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Next, Assumption 1 and conditional independence of $\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})\\right)$ for $m\\;=\\;1,\\ldots,M,i\\;=$ $0,\\ldots,n-1$ imply ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|g_{1}\\right\\|^{2}\\right]}&{=\\;\\frac{1}{M^{2}n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\mathbb{M}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|Q\\left(\\nabla f_{m^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})\\right)-\\nabla f_{m^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\left\\|\\frac{1}{M n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\sum_{s=0}^{M}\\mathbb{E}_{f_{n}^{\\varepsilon,i}(x_{i}^{\\varepsilon})}^{2\\varepsilon}\\right\\|^{2}}\\\\ {\\leq\\;\\frac{\\omega}{M n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\sum_{s=0}^{M}\\left\\|\\nabla f_{n^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})\\right\\|^{2}+\\left\\|\\frac{1}{M n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\sum_{s=0}^{M}\\nabla f_{n^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})\\right\\|^{2}}\\\\ {\\leq\\;\\frac{2\\omega}{M n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\sum_{s=0}^{M}\\left\\|\\nabla f_{n^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})-\\nabla f_{n^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})\\right\\|^{2}+\\frac{2\\omega}{M^{2}n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\sum_{s=0}^{M}\\left\\|\\nabla f_{n^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})\\right\\|^{2}}\\\\ &{\\qquad+2\\left\\|\\displaystyle\\frac{1}{M n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\sum_{s=0}^{M}\\left(\\nabla f_{n^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})-\\nabla f_{n^{*}}^{\\varepsilon,i}(x_{i}^{\\varepsilon})\\right)\\right\\|^{2}}\\\\ &{\\qquad+2\\left\\|\\displaystyle\\frac{1}{M n_{0}^{2}}\\displaystyle\\sum_{i=0}^{n-1}\\sum_{s=0}^{M}\\nabla f_{\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}^{i}$ and $\\widetilde{L}$ -smoothness and convexity of $\\begin{array}{r l}{f^{\\pi^{i}}}&{{}=}\\end{array}$ $\\textstyle{\\frac{1}{M}}\\sum_{m=1}^{M}f_{m}^{\\pi_{m}^{i}}$ ,we derive ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\|\\boldsymbol{g}\\|^{2}\\right]}&{\\leq\\ \\frac{4\\omega}{M^{2}n}\\frac{t}{2}\\underset{\\mathrm{sup}}{\\sum}\\underset{\\mathbf{\\tilde{b}}_{i}=0}^{\\--1}\\sum_{j=0}^{M}D_{f_{i}^{n+1}}(x_{i}^{\\prime},\\boldsymbol{x}_{i})+\\frac{2\\omega}{M^{2}n^{2}}\\frac{\\nu^{\\infty-1}}{\\sum_{i=0}^{M}m_{i}}\\left\\|\\nabla f_{n}^{\\star}(x_{i}^{\\prime})\\right\\|^{2}}\\\\ &{\\quad+\\4\\overline{{L}}_{n}^{\\frac{1}{1}}\\underset{\\mathrm{sup}}{\\sum}_{i=0}^{M}D_{f_{i}^{n+1}}(x_{i}^{\\prime},\\boldsymbol{x}_{i}^{\\prime})+2\\|\\nabla f(\\boldsymbol{x}_{i}^{\\prime})\\|^{2}}\\\\ &{\\leq\\ 4\\Big(\\widetilde{L}+\\frac{\\omega}{M n}L_{n}\\Big)\\frac{1}{n}\\underset{\\mathrm{sup}}{\\sum}\\underset{\\mathbf{\\tilde{b}}_{i}=0}^{\\lfloor1-1}D_{f_{i}^{n+1}}(x_{i}^{\\prime},\\boldsymbol{x}_{i})+\\frac{4\\omega}{M^{2}n^{2}}\\underset{\\mathrm{sup}}{\\sum}\\underset{\\mathbf{\\tilde{b}}_{i}=0}^{\\lfloor1-1}\\underbrace{M^{2}\\rfloor}_{\\textnormal{d o m}}\\Big\\|\\nabla f_{n}^{\\star}(x_{i}^{\\prime})\\Big\\|^{2}}\\\\ &{\\quad+\\frac{4\\omega}{M^{2}n^{2}}\\frac{\\nu^{\\infty-1}}{\\sum_{i=0}^{M}\\sum_{i=0}^{M}\\big\\Vert\\nabla f_{n}^{\\star}(x_{i}^{\\prime})-\\nabla f_{n}^{\\star}(x_{i}^{\\prime})\\big\\Vert^{2}}+2\\|\\nabla f(\\boldsymbol{z}_{i})-\\nabla f(\\boldsymbol{x}_{i})\\|^{2}}\\\\ &{\\leq\\ 2\\widetilde{L}\\Big(\\widetilde{L}+\\frac{\\omega}{M n}L_{n}\\Big)\\frac{1}{n}\\underset{\\mathrm{sup}}{\\sum}\\|x_{i}^{\\prime}-x_{i}\\|^{2}+\\frac{4\\omega}{M^{2}n^{2}}\\underset{\\mathrm{s u\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Taking the full expectation, we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{\\bar{x}}\\left[\\left\\Vert g_{t}\\right\\Vert^{2}\\right]}&{\\leq}&{\\displaystyle2\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\left\\Vert x_{t}^{i}-x_{t}\\right\\Vert^{2}\\right]+\\frac{4\\omega}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\left\\Vert\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right\\Vert^{2}\\right]}\\\\ &&{\\displaystyle+\\left(4\\tilde{L}+\\frac{8\\omega}{M n}L_{\\operatorname*{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &{=}&{\\displaystyle2\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\left\\Vert x_{t}^{i}-x_{t}\\right\\Vert^{2}\\right]+\\frac{4\\omega}{M n}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)}\\\\ &&{\\displaystyle+\\left(4\\tilde{L}+\\frac{8\\omega}{M n}L_{\\operatorname*{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma C.4. Let Assumptions 1, 2, 3, 5 hold and $\\begin{array}{r}{\\tau\\,\\leq\\,\\frac{1}{2\\sqrt{\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}{L}_{\\operatorname*{max}}\\right)}}}\\end{array}$ Then, the following inequalityholds ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{\\leq}&{24\\tau^{2}\\left(\\widetilde{L}+\\displaystyle\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &&{\\displaystyle+8\\tau^{2}\\frac{\\omega}{M n}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)+8\\tau^{2}\\frac{\\sigma_{\\star,n}^{2}}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{\\star,n}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f^{i}(x_{\\star})\\|^{2},\\,f^{i}(x)=\\frac{1}{M}\\sum_{m=1}^{M}f_{m}^{i}(x),\\,i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Since $\\begin{array}{r}{x_{t}^{i}=x_{t}-\\frac{\\tau}{M n}\\sum_{m=1}^{M}\\sum_{j=0}^{i-1}\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{j}}(x_{t}^{j})\\right)}\\end{array}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|x_{t}^{i}-x_{t}\\right\\|^{2}\\right]}&{=\\;\\tau^{2}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{M n}\\sum_{\\stackrel{k=1}{m\\leq n}}^{M}Q\\left(\\nabla f_{m^{\\prime}}^{\\tau,t}(x_{t}^{i})\\right)\\right\\|^{2}\\right]}\\\\ &{=\\;\\tau^{2}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{M n}\\sum_{\\stackrel{k=1}{m\\leq n}}^{M}\\left(Q\\left(\\nabla f_{m^{\\prime}}^{\\tau,t}(x_{t}^{i})\\right)-\\nabla f_{m^{\\prime}}^{\\tau,t}(x_{t}^{i})\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad+\\tau^{2}\\left\\|\\frac{1}{M n}\\sum_{\\stackrel{k=1}{m\\leq n}}^{M}\\nabla f_{m^{\\prime}}^{\\tau,t}(x_{t}^{i})\\right\\|^{2}}\\\\ &{\\leq\\;\\;\\frac{\\tau^{2}}{M n^{2}}\\sum_{\\stackrel{k=1}{m\\leq n}}^{M}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|Q\\left(\\nabla f_{m^{\\prime}}^{\\tau,t}(x_{t}^{i})\\right)-\\nabla f_{m^{\\prime}}^{\\tau,t}(x_{t}^{i})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\tau^{2}\\left\\|\\frac{1}{M n}\\sum_{\\stackrel{k=1}{m\\leq n}}^{M}\\nabla f_{m^{\\prime}}^{\\tau,t}(x_{t}^{i})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using Assumpton I, $\\widetilde{L}$ smothmsand ovxity of $\\begin{array}{r}{f^{\\pi^{i}}=\\frac{1}{M}\\sum_{m=1}^{M}f_{m}^{\\pi_{m}^{i}}}\\end{array}$ and $L_{\\mathrm{max}}$ smoothnes and convexity of , we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\theta}\\Big[\\Big\\lVert\\bar{\\theta}_{t}-\\theta_{t}\\Big\\rVert^{2}}&{\\le\\frac{\\theta^{\\prime}\\dot{\\theta}}{\\sqrt{\\theta_{0}\\theta^{\\prime}}}\\sum_{u=0}^{\\infty}\\Big\\}\\Big\\rVert\\nabla\\bar{\\theta}_{t}\\Big[\\theta_{t}^{\\prime}(u_{t})\\Big]^{2}+r^{2}\\left|\\frac{1}{\\sqrt{\\theta_{0}\\theta_{0}}}\\frac{M_{t}^{2}-1}{\\sum_{u=0}^{\\infty}\\theta_{0}^{\\prime}(u_{t})}\\right|^{2}\\right.}\\\\ &{\\le\\left.\\frac{2r^{2}M_{t}^{2}\\theta_{0}^{\\prime}}{\\sqrt{\\theta_{0}\\theta_{0}}}\\sum_{u=0}^{\\infty}\\Big\\}\\Big\\rVert\\nabla\\bar{\\theta}_{t}\\Big[\\frac{\\theta_{0}^{\\prime}(u_{t})}{\\sqrt{\\theta_{0}\\theta_{0}}}-\\nabla\\bar{\\theta}_{t}^{\\prime}(u_{t})\\Big]^{2}+2r^{2}\\left|\\frac{1}{\\sqrt{\\theta_{0}\\theta_{0}}}\\nabla F^{\\prime}(u_{u})\\right|^{2}}\\\\ &{\\quad+2r^{2}\\left|\\frac{1}{\\sqrt{\\theta_{0}\\theta_{0}}}\\left(\\nabla F^{\\prime}(u_{t})-\\nabla F^{\\prime}(u_{u})\\right)\\right|^{2}}\\\\ &{\\quad+\\frac{2r^{2}M_{t}^{2}\\theta_{0}^{\\prime}}{\\sqrt{\\theta_{0}\\theta_{0}}}\\sum_{u=0}^{\\infty}\\Big|\\nabla F^{\\prime}(u_{t})\\Big|^{2}}\\\\ &{\\le\\frac{4r^{2}M_{t}^{2}}{{M_{t}^{2}\\theta_{0}}}\\sum_{u=0}^{\\infty}\\Big|\\nabla F^{\\prime}(u_{t})\\Big|^{2}}\\\\ &{\\quad+\\frac{2r^{2}M_{t}^{2}}{{M_{t}^{2}\\theta_{0}}}\\sum_{u=0}^{\\infty}D_{u}\\Big(2r^{2}u_{t}\\Big)+2r^{2}\\left|\\frac{1}{\\sqrt{\\theta_{0}\\theta_{0}}}\\nabla F^{\\prime}(u_{u})\\right|^{2}}\\\\ &{\\quad+\\Delta L_{t}^{2}\\frac{\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, we need to estimate the second term from the previous inequality. Taking the full expectation and using Lemma C.1 and using new notation $\\begin{array}{r}{\\sigma_{t}^{2}=\\frac{\\bullet}{n}\\sum_{j=1}^{n}\\mathbb{E}[\\|\\dot{\\nabla}f^{j}(\\bar{x}_{t})-\\breve{\\nabla}f(x_{t})\\|^{2}]}\\end{array}$ weget ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left[\\left\\lVert\\frac{1}{n}\\sum_{j=0}^{i-1}\\nabla f^{\\pi^{j}}(x_{t})\\right\\rVert^{2}\\right]}&{=}&{\\frac{i^{2}}{n^{2}}\\mathbb{E}\\left[\\|\\nabla f(x_{t})\\|^{2}\\right]+\\frac{i^{2}}{n^{2}}\\mathbb{E}\\left[\\left\\lVert\\frac{1}{i}\\sum_{j=0}^{i-1}\\left(\\nabla f^{\\pi^{j}}(x_{t})-\\nabla f(x_{t})\\right)\\right\\rVert^{2}\\right]}\\\\ &{\\le}&{\\frac{i^{2}}{n^{2}}\\mathbb{E}\\left[\\|\\nabla f(x_{t})\\|^{2}\\right]+\\frac{i^{2}}{n^{3}}\\frac{n-i}{i(n-1)}\\displaystyle\\sum_{j=1}^{n}\\mathbb{E}\\left[\\|\\nabla f^{j}(x_{t})-\\nabla f(x_{t})\\|^{2}\\right]}\\\\ &{\\le}&{\\mathbb{E}\\left[\\|\\nabla f(x_{t})\\|^{2}\\right]+\\frac{1}{n}\\sigma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Taking the full expectation from (16) and using (17), we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Vert x_{t}^{i}-x_{t}\\Vert^{2}\\right]}&{\\leq}&{4\\tau^{2}\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\displaystyle\\sum_{j=0}^{n-1}\\mathbb{E}\\left[D_{f^{\\pi j}}(x_{t}^{j},x_{t})\\right]}\\\\ &&{+2\\tau^{2}\\mathbb{E}\\left[\\Vert\\nabla f(x_{t})\\Vert^{2}\\right]+\\frac{2\\tau^{2}}{n}\\sigma_{t}^{2}+\\frac{2\\tau^{2}\\omega}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\left\\Vert\\nabla f_{m}^{\\pi_{m}^{j}}(x_{t})\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using $\\widetilde{L}$ -smoothness of $f^{\\pi^{j}}$ , we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Vert x_{t}^{i}-x_{t}\\Vert^{2}\\right]}&{\\leq}&{2\\widetilde{L}\\tau^{2}\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\displaystyle\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\Vert x_{t}^{j}-x_{t}\\Vert^{2}\\right]}\\\\ &&{+2\\tau^{2}\\mathbb{E}\\left[\\Vert\\nabla f(x_{t})\\Vert^{2}\\right]+\\frac{2\\tau^{2}}{n}\\sigma_{t}^{2}+\\displaystyle\\frac{2\\tau^{2}\\omega}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\left\\Vert\\nabla f_{m}^{\\pi_{m}^{j}}(x_{t})\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $\\begin{array}{r}{\\tau\\leq\\frac{1}{2\\sqrt{\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)}}}\\end{array}$ ,wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left|x_{t}^{t}-x_{t}^{t}\\right|\\right]^{2}}&{\\leq2\\left(1-2\\hat{L}^{2}x^{2}\\left(\\hat{L}+\\frac{M}{\\sqrt{2}\\pi}I\\log\\alpha\\right)\\right)\\sum_{k=0}^{\\infty}\\mathbb{E}\\left[\\nu_{k}^{t}-x_{t}\\Big|^{2}\\right]}\\\\ &{\\leq\\;4^{2/2}\\mathbb{E}\\left[\\|Y(F_{t})\\|^{2}+\\frac{1}{\\alpha^{2}}\\frac{M^{2}}{\\eta^{2}}\\frac{x_{t}^{2}}{\\alpha^{2}}\\Bigg\\|\\sum_{k=0}^{\\infty}\\frac{M^{2}}{\\sqrt{2}\\pi}\\sum_{i=1}^{\\infty}\\mathbb{E}\\left[\\nu_{k}^{t}\\mathcal{F}_{i k}^{t}\\left(x_{t}^{t}\\right)\\right]^{2}\\right.}\\\\ &{\\;\\;\\left.\\leq\\;\\frac{M^{2}M^{2}}{18\\pi^{2}}\\frac{M^{2}}{\\sum_{i=1}^{\\infty}\\sqrt{\\pi}}\\mathbb{E}\\left[\\Big|\\nabla f_{i}^{T}\\xi(x_{t}^{t})-\\nabla f_{i}^{T}\\xi(x_{t}^{t})\\Big|^{2}\\right]}\\\\ &{\\leq\\;\\frac{M^{2}M^{2}}{32^{\\frac{3}{2}}}\\frac{M^{2}}{\\nu_{i k}^{2}}\\frac{M^{2}}{2\\Gamma\\Theta}\\Bigg[\\left|\\nabla f_{i}^{T}\\xi(x_{t}^{t})-\\nabla f_{i}^{T}\\xi(x_{t}^{t})\\right|^{2}\\Bigg]}\\\\ &{\\;\\;\\;+\\frac{M^{2}M^{2}}{32^{\\frac{3}{2}}}\\frac{M^{2}}{2}\\underset{=}{\\sqrt{\\pi}}\\Bigg[\\left|\\nabla f_{i}^{T}\\xi(x_{t})\\right|-\\frac{4}{\\Gamma}\\left[\\nabla f_{i}\\xi(x_{t})\\right]^{2}\\Bigg]}\\\\ &{\\leq\\;\\frac{M^{2}M^{2}}{32^{\\frac{3}{2}}}\\frac{M^{2}}{2}\\Bigg[\\mathbb{E}\\left[\\eta^{T}(\\rho_{1})\\right]-\\mathbb{E}\\left[\\nu_{1}^{T}(\\rho_{1})\\right]^{2}\\Bigg]}\\\\ &{\\leq\\;\\frac{M^{2}M^{\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Summing from $i=0$ to $n-1$ and using $\\widetilde{L}$ smoothness of $f^{i}$ and $L_{\\mathrm{max}}$ smoothness of $f_{m}^{i}$ , we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\frac{16\\tau^{2}\\omega}{M n}L_{\\operatorname*{max}}\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]+\\frac{16\\tau^{2}}{n}\\tilde{L}\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &{}&{\\displaystyle+\\frac{8\\tau^{2}\\omega}{M n}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)+\\frac{8\\tau^{2}}{n}\\sigma_{\\star,n}^{2}+8\\tau^{2}\\tilde{L}\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Theorem C.2. Let Assumptions 1, 2, 3, 5 hold and stepsize $\\gamma$ satisfy ", "page_idx": 36}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{16n\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, for all $T\\geq0$ the iterates produced by Q-RR satisfy ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{T}-x_{\\star}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{n\\gamma\\mu}{2}\\right)^{T}\\|x_{0}-x_{\\star}\\|^{2}+18\\frac{\\gamma^{2}n\\widetilde{L}}{\\mu}\\left(\\frac{\\omega}{M}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2})+\\sigma_{\\star,n}^{2}\\right)}\\\\ &&{+8\\frac{\\gamma\\omega}{\\mu M}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sigma_{\\star,n}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f^{i}(x_{\\star})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Taking expectation w.r.t. $\\mathcal{Q}$ and using Lemma C.3, we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\|x_{t+1}-x_{\\star}\\|^{2}\\right]}&{=}&{\\displaystyle\\|x_{t}-x_{\\star}\\|^{2}-2\\tau\\mathbb{E}_{\\mathcal{Q}}\\left[\\langle g_{t},x_{t}-x_{\\star}\\rangle\\right]+\\tau^{2}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|g^{t}\\|^{2}\\right]}\\\\ &{\\leq}&{\\displaystyle\\|x_{t}-x_{\\star}\\|^{2}-2\\tau\\mathbb{E}_{\\mathcal{Q}}\\left[\\langle g^{t},x_{t}-x_{\\star}\\rangle\\right]}\\\\ &{}&{\\displaystyle+2\\tau^{2}\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}\\\\ &{}&{\\displaystyle+8\\tau^{2}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)(f(x_{t})-f(x_{\\star}))+\\frac{4\\tau^{2}\\omega}{M n}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using Lemma C.2, we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|x_{t+1}-x_{*}\\right\\|^{2}\\right]}&{\\leq}\\\\ &{\\displaystyle-\\frac{\\eta_{t}}{2}\\|x_{t}-x_{*}\\|^{2}-\\tau(f(x_{t})-f(x_{t}))+\\frac{\\tau\\tilde{L}}{n}\\sum_{i=0}^{n-1}\\|x_{i}^{i}-x_{*}\\|^{2}}\\\\ &{\\displaystyle+2\\tau^{2}\\tilde{L}\\left(\\tilde{L}+\\frac{w}{\\lambda n}I_{n n-1}\\right)\\frac{1}{n-1}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{i}^{i}-x_{i}^{*}\\|^{2}\\right]}\\\\ &{+s^{2}\\alpha^{2}\\left(\\tilde{L}+\\frac{w}{\\lambda n}I_{n n-1}\\right)(f(x_{t})-f(x_{*}))+\\frac{4\\tau^{2}w}{M n}(\\zeta_{*}^{2}+\\sigma_{*}^{2})}\\\\ {\\leq}&{\\left(1-\\frac{\\tau\\mu}{2}\\right)\\|x_{t}-x_{*}\\|^{2}}\\\\ &{-\\tau\\left(1-s\\tau\\left(\\tilde{L}+\\frac{w}{M n}I_{n n-1}\\right)\\right)(f(x_{t})-f(x_{*}))}\\\\ &{+\\tau\\tilde{L}\\left(1+2\\tau\\left(\\tilde{L}+\\frac{w}{M n}I_{n n-1}\\right)\\right)\\frac{1-1}{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}\\\\ &{+\\frac{4\\tau^{2}w}{M n}(\\zeta_{*}^{2}+\\sigma_{*}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Next, we take the full expectation and apply Lemma C.4: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{t+1}-x_{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{t}-x_{\\star}\\right\\Vert^{2}\\right]}\\\\ &{-\\tau\\left(1-8\\tau\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &{+24\\tau^{3}\\widetilde{L}\\left(1+2\\tau\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\right)\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)(f(x_{t})-f(x_{\\star}))}\\\\ &{+8\\tau^{3}\\widetilde{L}\\left(1+2\\tau\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\right)\\left(\\frac{\\omega}{M n}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2})+\\frac{\\sigma_{\\star,n}^{2}}{n}\\right)+\\frac{4\\tau^{2}\\omega}{M n}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using (18), we derive ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\mathbb{E}\\left[\\|x_{t+1}-x_{\\star}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\|x_{t}-x_{\\star}\\|^{2}\\right]}\\\\ &&{\\displaystyle+9\\tau^{3}\\tilde{L}\\left(\\frac{\\omega}{M n}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2})+\\frac{\\sigma_{\\star,n}^{2}}{n}\\right)+\\frac{4\\tau^{2}\\omega}{M n}(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Rursivlyn tqualt, sub $\\tau=n\\gamma$ and using $\\sum_{t=0}^{+\\infty}\\left(1-\\frac{\\tau\\mu}{2}\\right)^{t}\\leq\\frac{2}{\\mu\\tau}$ , we get the result. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Corollary 6. Let the assumptions of Theorem C.2 hold and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{16n\\left(\\widetilde L+\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)},\\sqrt{\\frac{\\varepsilon\\mu}{8^{2}n\\widetilde L}}\\left(\\frac{\\omega}{M}\\Delta_{\\star}^{2}+\\sigma_{\\star,n}^{2}\\right)^{-\\frac12},\\frac{\\varepsilon\\mu M}{24\\omega\\Delta_{\\star}^{2}}\\right\\},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\Delta_{\\star}^{2}=\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}$ Then, $\\upalpha$ -RR finds a solution with accuracy $\\varepsilon>0$ after the following number of communication rounds: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{n\\widetilde{L}}{\\mu}+\\frac{\\omega}{M}\\frac{L_{\\operatorname*{max}}}{\\mu}+\\frac{\\omega}{M}\\frac{\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}}{\\varepsilon\\mu^{2}}+\\sqrt{\\frac{n\\widetilde{L}}{\\varepsilon\\mu^{3}}}\\sqrt{\\frac{\\omega}{M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)+\\sigma_{\\star,n}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Theorem C.2 implies ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{T}-x_{\\star}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{n\\gamma\\mu}{2}\\right)^{T}\\|x_{0}-x_{\\star}\\|^{2}+18\\frac{\\gamma^{2}n\\widetilde L}{\\mu}\\left(\\frac{\\omega}{M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)+\\sigma_{\\star,n}^{2}\\right)}\\\\ &&{+8\\frac{\\gamma\\omega}{\\mu M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To estimate the number of communication rounds required to find a solution with accuracy $\\varepsilon>0$ ,we need to upper bound each term from the right-hand side by $\\varepsilon/3$ . Thus, we get additional conditions on $\\gamma$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n18\\frac{\\gamma^{2}n\\widetilde{L}}{\\mu}\\left(\\frac{\\omega}{M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)+\\sigma_{\\star,n}^{2}\\right)<\\frac{\\varepsilon}{3},\\quad8\\frac{\\gamma\\omega}{\\mu M}\\left(\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)<\\frac{\\varepsilon}{3},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and also the upper bound on the number of communication rounds $n T$ ", "page_idx": 38}, {"type": "equation", "text": "$$\nn T=\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\gamma\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Substituting (20) in the previous equation, we get the result. ", "page_idx": 38}, {"type": "text", "text": "D Missing Proofs for DIANA-RR ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "D.1 Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Lemma D.1. Let Assumptions 1, 3, 5, $^{6}$ hold and $\\begin{array}{r}{\\alpha\\leq\\frac{1}{1+\\omega}}\\end{array}$ . Then, the iterates of DIANA-RR satisfy ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|h_{t+1,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}\\right]}&{\\le}&{\\displaystyle\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}}\\\\ &&{\\displaystyle+\\frac{2\\alpha L_{\\operatorname*{max}}}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\pi_{m}^{i}}}(x_{t}^{i},x_{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. Taking expectation w.r.t. $\\mathcal{Q}$ , we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathcal{Q}}\\left[\\|h_{t+1,m}^{\\overline{{\\tau}}_{n}^{*}}-\\nabla f_{m}^{\\pi_{n}^{*}}(x_{*})\\|^{2}\\right]}&{=\\phantom{-}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|h_{t,m}^{\\pi_{n}^{*}}+\\alpha\\mathcal{Q}(\\nabla f_{m}^{\\pi_{n}^{*}}(x_{t}^{i})-h_{t,m}^{\\pi_{n}^{*}})-\\nabla f_{m}^{\\pi_{n}^{*}}(x_{*})\\|^{2}\\right]}\\\\ &{=\\phantom{-}\\|h_{t,m}^{\\pi_{n}^{*}}-\\nabla f_{m}^{\\pi_{n}^{*}}(x_{*})\\|^{2}}\\\\ &{\\quad+2\\alpha\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\langle\\mathcal{Q}(\\nabla f_{m}^{\\pi_{n}^{*}}(x_{t}^{i})-h_{t,m}^{\\pi_{n}^{*}}),h_{t,m}^{\\pi_{n}^{*}}-\\nabla f_{m}^{\\pi_{n}^{*}}(x_{*})\\right\\rangle\\right]}\\\\ &{\\quad+\\alpha^{2}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|\\mathcal{Q}(\\nabla f_{m}^{\\pi_{n}^{*}}(x_{t}^{i})-h_{t,m}^{\\pi_{n}^{*}})\\|^{2}\\right]}\\\\ {=}&{\\|h_{t,m}^{\\pi_{n}^{*}}-\\nabla f_{m}^{\\pi_{n}^{*}}(x_{*})\\|^{2}}\\\\ &{\\quad+2\\alpha\\left\\langle\\nabla f_{m}^{\\pi_{n}^{*}}(x_{t}^{i})-h_{t,m}^{\\pi_{n}^{*}},h_{t,m}^{\\pi_{n}^{*}}-\\nabla f_{m}^{\\pi_{n}^{*}}(x_{*})\\right\\rangle}\\\\ &{\\quad+\\alpha^{2}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|\\mathcal{Q}(\\nabla f_{m}^{\\pi_{n}^{*}}(x_{t}^{i})-h_{t,m}^{\\pi_{n}^{*}})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Assumption 1, $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}^{i}$ and $\\alpha\\leq{^1\\!/}({1\\!+\\!\\omega})$ imply ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\xi_{\\mathcal{C}}\\left[\\|h_{t+1,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\|^{2}\\right]}&{\\le}&{\\big\\|h_{t,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{2}\\,}\\\\ &{\\quad+2\\alpha\\Big\\langle\\nabla f_{m}^{\\pi^{*}}(x_{*})-h_{t,m}^{\\tau_{*}},h_{t,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\Big\\rangle}\\\\ &{\\quad+\\alpha^{2}(1+\\omega)\\big\\|\\nabla f_{m}^{\\pi^{*}}(x_{*})-h_{t,m}^{\\tau_{*}}\\big\\|^{2}}\\\\ &{\\le}&{\\big\\|h_{t,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{2}}\\\\ &{\\quad+\\alpha\\Big\\langle\\nabla f_{m}^{\\pi^{*}}(x_{*})-h_{t,m}^{\\tau_{*}},h_{t,m}^{\\tau_{*}}+\\nabla f_{m}^{\\pi^{*}}(x_{*})-2\\nabla f_{m}^{\\pi^{*}}(x_{*})\\Big\\rangle}\\\\ &{\\le}&{\\big\\|h_{t,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{2}}\\\\ &{\\quad+\\alpha\\big\\|\\nabla f_{m}^{\\pi^{*}}(x_{*})-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{2}-\\alpha\\big\\|h_{t,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{2}}\\\\ &{\\le}&{(1-\\alpha)\\big\\|h_{t,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{2}}\\\\ &{\\le}&{(1)\\Big\\|h_{t,m}^{\\tau_{*}}-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{2}}\\\\ &{\\quad+\\alpha\\big\\|\\nabla f_{m}^{\\pi^{*}}(x_{*})-\\nabla f_{m}^{\\pi^{*}}(x_{*})\\big\\|^{\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Summing up the above inequality for $m=1,\\dotsc,M$ , we get the result. ", "page_idx": 39}, {"type": "text", "text": "Theorem D.1. Let Assumptions 1, 3, 5, 6 hold and $\\begin{array}{r}{0<\\gamma\\leq\\operatorname*{min}\\Big\\{\\frac{\\alpha}{2n\\widetilde{\\mu}},\\frac{1}{\\widetilde{L}+\\frac{6\\omega}{M}L_{\\operatorname*{max}}}\\Big\\},\\alpha\\,\\leq\\,\\frac{1}{1+\\omega}}\\end{array}$ Then,for all $T\\geq0$ the iterates produced by DIANA-RR satisfy ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\gamma\\widetilde{\\mu}\\right)^{n T}\\Psi_{0}+\\frac{2\\gamma^{2}\\sigma_{r a d}^{2}}{\\widetilde{\\mu}},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\Psi_{t}$ is defined in (6). ", "page_idx": 39}, {"type": "text", "text": "Proo. Using $\\begin{array}{r}{x_{\\star}^{i+1}=x_{\\star}^{i}-\\frac{\\gamma}{M}\\sum_{m=1}^{M}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})}\\end{array}$ and line9ofAlgorithm 2 we derive ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}}&{=}&{\\displaystyle\\left\\|x_{t}^{i}-x_{\\star}^{i}-\\gamma\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\hat{g}_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right)\\right\\|^{2}}\\\\ &{=}&{\\displaystyle\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}-\\frac{2\\gamma}{M}\\sum_{m=1}^{M}\\left\\langle\\left(\\hat{g}_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right),x_{t}^{i}-x_{\\star}^{i}\\right\\rangle}\\\\ &&{+\\displaystyle\\gamma^{2}\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\hat{g}_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Taking expectation w.r.t. $\\mathcal{Q}$ and using $\\mathbb{E}\\|\\xi-c\\|^{2}=\\mathbb{E}\\|\\xi-\\mathbb{E}\\xi\\|^{2}+\\|\\mathbb{E}\\xi-c\\|^{2}$ , we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q}\\left[\\|x_{t}^{i+1}-x_{*}^{i+1}\\|^{2}\\right]=\\|x_{t}^{i}-x_{*}^{i}\\|^{2}-\\frac{2\\gamma}{M}\\displaystyle\\sum_{m=1}^{M}\\left\\langle\\nabla f_{m}^{\\pi_{i}^{i}}(x_{t}^{i})-\\nabla f_{m}^{\\pi_{i}^{i}}(x_{*}),x_{t}^{i}-x_{*}^{i}\\right\\rangle}\\\\ &{+\\gamma^{2}\\mathbb{E}_{Q}\\left[\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\left(Q\\left(\\nabla f_{m}^{\\pi_{i}^{i}}(x_{t}^{i})-h_{t,m}^{\\pi_{i}^{i}}\\right)+h_{t,m}^{\\pi_{i}^{i}}-\\nabla f_{m}^{\\pi_{i}^{i}}(x_{*})\\right)\\right\\|^{2}\\right]}\\\\ {\\leq}&{\\|x_{t}^{i}-x_{*}^{i}\\|^{2}-\\frac{2\\gamma}{M}\\displaystyle\\sum_{m=1}^{M}\\left\\langle\\nabla f_{m}^{\\pi_{i}^{i}}(x_{t}^{i})-\\nabla f_{m}^{\\pi_{i}^{i}}(x_{*}),x_{t}^{i}-x_{*}^{i}\\right\\rangle}\\\\ &{+\\gamma^{2}\\mathbb{E}_{Q}\\left[\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\left(Q\\left(\\nabla f_{m}^{\\pi_{i}^{i}}(x_{t}^{i})-h_{t,m}^{\\pi_{i}^{i}}\\right)-\\nabla f_{m}^{\\pi_{i}^{i}}(x_{t}^{i})+h_{t,m}^{\\pi_{i}^{i}}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\gamma^{2}\\left\\|\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\left(\\nabla f_{m}^{\\pi_{i}^{i}}(x_{*})-\\nabla f_{m}^{\\pi_{i}^{i}}(x_{t}^{i})\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Independence of $\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi^{i}}(x_{t}^{i})-h_{t,m}^{\\pi_{m}^{i}}\\right),m\\in$ $m\\in[M]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\sigma}\\left[\\big\\lVert\\hat{\\mathbf{z}}_{t}^{t+1}-\\hat{\\mathbf{z}}_{t}^{t+1}\\big\\rVert^{2}\\right]}&{\\leq\\big\\lVert\\hat{\\mathbf{z}}_{t}^{t}-\\hat{\\mathbf{z}}_{t}^{t}\\big\\rVert^{2}}\\\\ &{-\\frac{2\\gamma}{\\hbar}\\sum_{i=1}^{M}\\Bigg(\\mathcal{D}_{\\sigma,\\gamma,i}(z_{t}^{t},x_{t}^{\\prime})+\\mathcal{D}_{\\gamma,i}(z_{t}^{t},x_{t}^{\\prime})-\\mathcal{D}_{\\gamma,i}(z_{t}^{t},x_{t}^{\\prime})\\Bigg)}\\\\ &{\\quad+\\frac{\\gamma}{\\hbar^{2}\\sigma}\\frac{\\sqrt{1}}{\\sqrt{\\pi}}\\left[\\big\\lVert\\nabla f_{i}^{t+\\sigma}(z_{t}^{\\prime})-\\delta_{\\gamma,i}^{t}\\big\\rVert^{2}\\right.}\\\\ &{\\qquad\\Bigg.+\\frac{\\gamma}{2}\\Bigg\\lVert\\frac{1}{\\sqrt{\\pi}}\\sum_{i=1}^{M}\\Bigg(\\mathcal{D}_{\\sigma,i}^{t}\\big(z_{t}^{\\prime})-\\nabla f_{i}^{t+\\sigma}(z_{t}^{\\prime})\\Big)\\Bigg\\rVert^{2}}\\\\ &{\\leq\\big\\lVert\\hat{\\mathbf{z}}_{t}^{t}-\\hat{\\mathbf{z}}_{t}^{t}\\big\\rVert^{2}}\\\\ &{\\qquad-\\frac{2\\gamma}{\\hbar}\\sum_{i=1}^{M}\\Big(\\mathcal{D}_{\\sigma,i}(z_{t}^{t},x_{t}^{\\prime})+\\mathcal{D}_{\\gamma,i}(z_{t}^{t},x_{t}^{\\prime})-\\mathcal{D}_{\\gamma,i}(z_{t}^{t},x_{t}^{\\prime})\\Big)}\\\\ &{\\quad\\left.+\\frac{2\\gamma^{2}\\sigma}{\\hbar}\\frac{\\sqrt{1}}{\\sqrt{\\pi}}\\frac{\\gamma}{\\sigma}\\right\\lVert\\nabla f_{i}^{t+\\sigma}(z_{t}^{\\prime})-\\nabla f_{i}^{t+\\sigma}(z_{t}^{\\prime})\\right\\rVert^{2}}\\\\ &{\\qquad+\\gamma^{3}\\Bigg\\lVert\\frac{1}{\\sqrt{\\pi}}\\sum_{i=1}^{M}\\Bigg(\\nabla f_{i}^{t+\\sigma}(z_{t}^{\\prime})-\\nabla f_{i}^{t+\\sigma}(z_{t}^{\\prime}) \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using $L_{\\mathrm{max}}$ -smoothness and $\\mu$ -strong convexity of functions $f_{m}^{i}$ and $\\widetilde{L}$ -smoothness and $\\widetilde{\\mu}$ strong convexity of $\\begin{array}{r}{f^{\\pi^{i}}=\\frac{1}{M}\\sum_{i=1}^{M}f_{m}^{\\pi_{m}^{i}}}\\end{array}$ , we obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}\\right]}&{\\leq}&{\\displaystyle(1-\\gamma\\widetilde{\\mu})\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}}\\\\ &&{\\displaystyle-2\\gamma\\left(1-\\gamma\\left(\\widetilde{L}+\\frac{2\\omega}{M}L_{\\operatorname*{max}}\\right)\\right)\\frac{1}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\tau_{m}^{i}}}(x_{t}^{i},x_{\\star})}\\\\ &&{\\displaystyle+\\frac{2\\gamma}{M}\\sum_{m=1}^{M}D_{f_{m}^{\\tau_{m}^{i}}}(x_{\\star}^{i},x_{\\star})+\\frac{2\\gamma^{2}\\omega}{M^{2}}\\sum_{m=1}^{M}\\left\\|h_{t,m}^{\\tau_{m}^{i}}-\\nabla f_{m}^{\\tau_{m}^{i}}(x_{\\star})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Taking the full expectation and using Defenition 2, we derive ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{t}^{i+1}-x_{\\star}^{i+1}\\|^{2}\\right]}&{\\leq}&{(1-\\gamma\\widetilde{\\mu})\\mathbb{E}\\left[\\left\\|x_{t}^{i}-x_{\\star}^{i}\\right\\|^{2}\\right]}\\\\ &&{-2\\gamma\\left(1-\\gamma\\left(\\widetilde{L}+\\frac{2\\omega}{M}L_{\\operatorname*{max}}\\right)\\right)\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}\\left[D_{f_{m}^{\\pi_{m}^{i}}}(x_{t}^{i},x_{\\star})\\right]}\\\\ &&{+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}+\\frac{2\\gamma^{2}\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left[\\left\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Recursively unrolling the inequality, we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\left\\Vert x_{t+1}-x_{\\star}\\right\\Vert^{2}\\right]}&{\\leq}&{(1-\\gamma\\tilde{\\mu})^{n}\\mathbb{E}\\left[\\left\\Vert x_{t}-x_{\\star}\\right\\Vert^{2}\\right]}\\\\ &&{+\\frac{2\\gamma^{2}\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}(1-\\gamma\\tilde{\\mu})^{j}\\mathbb{E}\\left[\\left\\Vert h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right\\Vert^{2}\\right]}\\\\ &&{-2\\gamma\\left(1-\\gamma\\left(\\tilde{L}+\\displaystyle\\frac{2\\omega}{M}L_{\\operatorname*{max}}\\right)\\right)\\displaystyle\\frac{M}{M}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}(1-\\gamma\\tilde{\\mu})^{j}\\mathbb{E}\\left[D_{f_{m}^{\\pi_{m}^{i}}}(x_{t}^{i},x_{\\star})\\right]}\\\\ &&{+2\\gamma^{3}\\sigma_{\\operatorname*{mad}_{j=0}}^{2}(1-\\gamma\\tilde{\\mu})^{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Next, we apply (6) and Lemma D.1: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Psi_{t+1}\\right]\\leq(1-\\gamma\\widetilde{\\mu})^{n}\\mathbb{E}\\left[\\left\\Vert x_{t}-x_{\\star}\\right\\Vert^{2}\\right]+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}\\displaystyle\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}}\\\\ &{+\\left(c(1-\\alpha)+\\displaystyle\\frac{2\\omega}{M}\\right)\\frac{\\gamma^{2}}{M}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}\\mathbb{E}\\left[\\left\\Vert h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right\\Vert^{2}\\right]}\\\\ &{-2\\gamma\\left(1-c\\gamma\\alpha L_{\\operatorname*{max}}-\\gamma\\left(\\widetilde{L}+\\displaystyle\\frac{2\\omega}{M}L_{\\operatorname*{max}}\\right)\\right)\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}\\mathbb{E}\\left[D_{f_{m}^{\\pi_{m}^{i}}}(x_{\\star}^{i},x_{\\star})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\begin{array}{r}{c=\\frac{4\\omega}{\\alpha M^{2}}}\\end{array}$ Using $\\begin{array}{r}{\\alpha\\leq\\frac{1}{1+\\omega}}\\end{array}$ and $\\begin{array}{r}{\\gamma\\leq\\operatorname*{min}\\left\\{\\frac{\\alpha}{2n\\mu},\\frac{1}{\\left(\\widetilde{L}+{^{6\\omega}}/{M L_{\\operatorname*{max}}}\\right)}\\right\\}}\\end{array}$ ,we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left\\{\\Psi_{t+1}\\right.}&{\\leq\\left.\\left(1-\\gamma\\tilde{\\mu}\\right)^{n}\\mathbb{E}\\left[\\left|x_{t}-x_{t}\\right|^{2}\\right]\\right.}\\\\ &{\\left.\\quad+\\left(1-\\frac{\\alpha}{2}\\right)\\frac{4\\omega\\gamma^{2}}{\\alpha M^{2}}\\sum_{n=1}^{M}\\sum_{\\rho=0}^{n-1}-\\gamma\\tilde{\\mu}^{j}\\mathbb{E}\\left[\\left|h_{t,m}^{\\pi_{\\rho}^{\\pi_{\\varepsilon}^{\\prime}}}-\\nabla f_{m^{\\pi}}^{\\pi_{\\varepsilon}^{\\prime}}(x_{\\cdot})\\right|\\right]^{2}\\right\\}}\\\\ &{\\quad+2\\gamma^{2}\\sigma_{n\\omega}^{3}\\frac{n-1}{2\\omega}(1-\\gamma\\tilde{\\mu})^{j}}\\\\ &{\\leq\\left.\\begin{array}{l}{\\operatorname*{max}\\left\\{(1-\\gamma\\tilde{\\mu})^{n},\\left(1-\\frac{\\alpha}{2}\\right)\\right\\}\\mathbb{E}\\left[\\Psi_{t}\\right]}\\\\ {+2\\gamma^{2}\\sigma_{n\\omega}^{3}\\frac{n-1}{2\\omega^{2}}(1-\\gamma\\tilde{\\mu})^{j}}\\end{array}\\right.}\\\\ &{\\quad+\\left.(1-\\gamma\\tilde{\\mu})^{n}\\mathbb{E}\\left[\\Psi_{t}\\right]+2\\gamma^{3}\\sigma_{n\\omega}^{2}\\sum_{n=0}^{n-1}-\\gamma\\tilde{\\mu}^{j}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Recursively rewriting the inequality, we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Psi_{T}\\right]}&{\\leq}&{\\displaystyle(1-\\gamma\\widetilde{\\mu})^{n T}\\Psi_{0}+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}\\sum_{t=0}^{T-1}(1-\\gamma\\widetilde{\\mu})^{t n}\\sum_{j=0}^{n-1}(1-\\gamma\\widetilde{\\mu})^{j}}\\\\ &&{\\le}&{\\displaystyle(1-\\gamma\\widetilde{\\mu})^{n T}\\Psi_{0}+2\\gamma^{3}\\sigma_{\\mathrm{rad}}^{2}\\sum_{k=0}^{n-1}\\big(1-\\gamma\\widetilde{\\mu}\\big)^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using that $\\begin{array}{r}{\\sum_{k=0}^{+\\infty}\\left(1-\\frac{\\gamma\\widetilde{\\mu}}{2}\\right)^{k}\\leq\\frac{2}{\\widetilde{\\mu}\\gamma}}\\end{array}$ we fnish proof. ", "page_idx": 42}, {"type": "text", "text": "Corollary 7. Let the assumptions of Theorem $D.I$ hold, $\\begin{array}{r}{\\alpha=\\frac{1}{1+\\omega}}\\end{array}$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{\\frac{\\alpha}{2n\\widetilde{\\mu}},\\frac{1}{\\widetilde{L}+\\frac{6\\omega}{M}L_{\\mathrm{max}}},\\frac{\\sqrt{\\varepsilon\\widetilde{\\mu}}}{2\\sigma_{r a d}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then DIANA-RR finds a solution with accuracy $\\varepsilon>0$ after the following number of communication rounds: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(n(1+\\omega)+\\frac{\\widetilde{L}}{\\widetilde{\\mu}}+\\frac{\\omega}{M}\\frac{L_{\\operatorname*{max}}}{\\widetilde{\\mu}}+\\frac{\\sigma_{r a d}}{\\sqrt{\\varepsilon\\widetilde{\\mu}^{3}}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Theorem D.1 implies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\gamma\\widetilde{\\mu}\\right)^{n T}\\Psi_{0}+\\frac{2\\gamma^{2}\\sigma_{\\mathrm{rad}}^{2}}{\\widetilde{\\mu}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "To estimate the number of communication rounds required to find a solution with accuracy $\\varepsilon>0$ ,we need to upper bound each term from the right-hand side by $\\frac{\\varepsilon}{2}$ . Thus, we get an additional condition On $\\gamma$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{2\\gamma^{2}\\sigma_{\\mathrm{rad}}^{2}}{\\tilde{\\mu}}<\\frac{\\varepsilon}{2},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and also the upper bound on the number of communication rounds $n T$ ", "page_idx": 42}, {"type": "equation", "text": "$$\nn T=\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\gamma\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Substituting (22) in the previous equation, we get the result ", "page_idx": 42}, {"type": "text", "text": "D.2 Non-Strongly Convex Summands ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we provide the analysis of DIANA-RR without using Assumptions 4, 6. We emphasize $\\begin{array}{r}{x_{t}^{i+1}=x_{t}^{i}-\\gamma\\frac{1}{M}\\sum_{m=1}^{M}\\hat{g}_{t,m}^{\\pi_{m}^{i}}}\\end{array}$ Then we have ", "page_idx": 43}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}-\\gamma\\sum_{i=0}^{n-1}\\frac{1}{M}\\sum_{m=1}^{M}\\hat{g}_{t,m}^{\\pi_{m}^{i}}=x_{t}-\\tau\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\hat{g}_{t,m}^{\\pi_{m}^{i}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We denote $\\begin{array}{r}{\\hat{g}_{t}=\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\hat{g}_{t,m}^{\\pi_{m}^{i}}}\\end{array}$ ", "page_idx": 43}, {"type": "text", "text": "Lemma D.2. Let Assumptions $^{\\,l}$ , 2, 3, 5 hold. Then, the following inequality holds ", "page_idx": 43}, {"type": "equation", "text": "$$\n-2\\tau\\mathbb{E}_{Q}\\left[\\langle\\hat{g}_{t}-h_{\\star},x_{t}-x_{\\star}\\rangle\\right]\\leq-\\frac{\\tau\\mu}{2}\\|x_{t}-x_{\\star}\\|^{2}-\\tau\\left(f(x_{t})-f(x_{\\star})\\right)+\\tau\\tilde{L}\\frac{1}{n}\\sum_{i=1}^{n-1}\\|x_{t}-x_{t}^{i}\\|^{2},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $h^{\\star}=\\nabla f(x_{\\star})=0.$ ", "page_idx": 43}, {"type": "text", "text": "Proof. Since $h^{\\star}=\\nabla f(x_{\\star})=0$ , the proof of Lemma D.2 is identical to the proof of Lemma C.2. Lemma D.3. Let Assumptions 1, 2, 3, 5 hold. Then, the following inequality holds ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{\\xi}_{Q}\\left[\\|\\hat{\\boldsymbol{g}}_{t}-\\boldsymbol{h}_{\\star}\\|^{2}\\right]}&{\\le}&{2\\tilde{L}\\left(\\tilde{L}+\\displaystyle\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n-1}{\\|x_{t}^{i}-x_{t}\\|^{2}}+8\\left(\\tilde{L}+\\displaystyle\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\left(f(\\boldsymbol{x}_{t})-f(\\boldsymbol{x}_{\\star})\\right)}\\\\ &&{\\displaystyle+\\frac{4\\omega}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol{x}_{\\star})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. Taking expectation w.r.t. $\\mathcal{Q}$ ,we get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\hat{\\boldsymbol{g}}_{t}-\\boldsymbol{h}_{*}\\right\\|^{2}\\right]}&{=}&{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{M n}\\sum_{i=0}^{n-1}{\\sum_{j=0}^{n}{\\hat{y}}_{\\ell,m}^{n_{m}^{i}}}-\\boldsymbol{h}_{*}\\right\\|^{2}\\right]}\\\\ &{=}&{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{M n}\\sum_{i=0}^{n-1}{\\sum_{n=1}^{M}\\left(h_{t,m}^{\\tau_{i}^{n}}+\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-h_{t,m}^{\\tau_{i}^{n}}\\right)\\right)}-h_{*}\\right\\|^{2}\\right]}\\\\ &{=}&{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{M n}\\sum_{i=0}^{n-1}{\\sum_{n=1}^{M}\\left(h_{t,m}^{\\tau_{i}^{n}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})+\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-h_{t,m}^{\\pi_{m}^{i}}\\right)\\right)}\\right\\|^{2}\\right]}\\\\ &&{+\\left\\|\\frac{1}{M n}\\sum_{i=0}^{n-1}{\\sum_{n=1}^{M}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})}-h_{*}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Independence of $\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi^{i}}(x_{t}^{i})-h_{t,m}^{\\pi_{m}^{i}}\\right),m\\in$ $m\\in[M]$ and Assumption 1 imply ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\xi_{\\mathcal{Q}}\\left[\\left\\|\\hat{\\boldsymbol{g}}_{t}-\\boldsymbol{h}_{*}\\right\\|^{2}\\right]}&{=}&{\\displaystyle\\frac{1}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\mathbb{X}_{\\mathcal{Q}}\\left[\\left\\|h_{t,m}^{\\tau_{i}^{*}}-\\nabla f_{m}^{\\tau_{i}^{*}}(x_{t}^{i})+\\mathcal{Q}\\left(\\nabla f_{m}^{\\tau_{i}^{*}}(x_{t}^{i})-h_{t,m}^{\\tau_{i}^{*}}\\right)\\right\\|^{2}\\right]}\\\\ &&{+\\left\\|\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\nabla f_{m}^{\\tau_{i}^{*}}(x_{t}^{i})-h_{*}\\right\\|^{2}}\\\\ &{\\le}&{\\displaystyle\\frac{\\omega}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\sum_{\\ell=0}^{M}\\left\\|\\nabla f_{m}^{\\tau_{i}^{*}}(x_{t}^{i})-h_{t,m}^{\\tau_{i}^{*}}\\right\\|^{2}+\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f^{\\tau_{i}^{*}}(x_{t}^{i})-h_{*}\\right\\|^{2}}\\\\ &{\\le}&{\\displaystyle\\frac{2\\omega}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}^{\\tau_{i}^{*}}(x_{t}^{i})-\\nabla f_{m}^{\\tau_{i}^{*}}(x_{t})\\right\\|^{2}+\\frac{2}{n}\\sum_{i=0}^{n-1}\\left\\|\\nabla f^{\\tau^{*}}(x_{t}^{i})-\\nabla f^{\\tau^{*}}(x_{t}^{i})\\right\\|}\\\\ &&{+\\displaystyle\\frac{2\\omega}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\left\\|h_{t,m}^{\\tau_{i}^{*}}-\\nabla f_{m}^{\\tau_{i}^{*}}(x_{t})\\right\\|^{2}+2\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f^{\\tau^{*}}(x_{t})-\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Using $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}^{i}$ and $\\widetilde{L}$ -smoothness and convexity of $f^{\\pi^{i}}$ , we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\hat{y}_{t}-h_{*}\\right\\|^{2}\\right]}&{\\leq\\frac{4\\delta I_{\\infty,\\infty}}{M^{2}n^{2}}\\frac{8-1}{60}\\prod_{j=0}^{n_{\\ell+}}(x_{\\ell}^{+},x_{\\ell})+\\frac{4\\tilde{L}^{2}}{m}\\frac{\\Gamma_{j-1}^{+1}}{60}D_{f^{*,\\ell}}(x_{\\ell}^{+},x_{\\ell})}\\\\ &{\\quad+\\frac{4\\delta}{M^{2}n^{2}}\\frac{8-1}{60}\\left\\|h_{\\ell,n}^{+}-\\nabla\\int_{m^{*}}^{n_{\\ell+}}(x_{\\ell}^{+})\\right\\|^{2}+4\\tilde{L}(f(x_{\\ell})-f(x_{\\ell}))}\\\\ &{\\quad+\\frac{4\\delta}{M^{2}n^{2}}\\frac{8-1}{60}\\left\\|\\nabla f_{m^{*}}^{*}(x_{\\ell})-\\nabla f_{m^{*}}^{*}(x_{\\ell})\\right\\|^{2}}\\\\ &{\\leq\\ 2\\tilde{L}\\left(\\tilde{L}+\\frac{6}{M n}\\Gamma_{m\\times m}\\right)\\frac{1}{n}\\frac{n-1}{60}\\|x_{\\ell}^{+}-x_{\\ell}\\|^{2}+4\\tilde{L}(f(x_{\\ell})-f(x_{\\ell}))}\\\\ &{\\quad+\\frac{8\\delta}{M n}\\L_{{L}^{\\infty}}\\frac{1}{3M n}\\sum_{i=0}^{n-1}\\underbrace{M}_{f_{m^{*}}^{*}(x_{\\ell},x_{\\ell})}}\\\\ &{\\quad+\\frac{4\\delta}{M^{2}n^{2}}\\frac{n-1}{60}\\underbrace{M}_{i}^{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Lemma D.4. Let $\\begin{array}{r}{\\alpha\\leq\\frac{1}{1+\\omega}}\\end{array}$ adAs satisfy ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|h_{t+1,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}\\right]}&{\\le}&{\\displaystyle\\frac{1-\\alpha}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}}\\\\ &&{\\displaystyle+\\frac{2\\alpha\\tilde{L}L_{\\operatorname*{max}}}{n}\\sum_{i=0}^{n-1}\\|x_{t}^{i}-x_{t}\\|^{2}}\\\\ &&{\\displaystyle+4\\alpha L_{\\operatorname*{max}}\\left(f(x_{t})-f(x^{\\star})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Prof Fst fll we inrodue new notaton: $\\begin{array}{r}{\\mathcal{H}_{t}=\\frac{1}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}\\right]\\!.}\\end{array}$ Using (21) and summing it up for $i=0,\\ldots,n-1$ , we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{H}_{t+1}}&{\\leq}&{\\displaystyle\\frac{1-\\alpha}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}+\\displaystyle\\frac{\\alpha}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}}\\\\ &{\\leq}&{\\displaystyle\\frac{1-\\alpha}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}+\\displaystyle\\frac{2\\alpha}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t})\\|^{2}}\\\\ &&{\\displaystyle+\\frac{2\\alpha}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Next, we apply $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}^{i}$ and $\\widetilde{L}$ -smoothness and convexity of $f^{\\pi^{i}}$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathcal{H}_{t+1}}&{\\le}&{\\displaystyle\\frac{1-\\alpha}{M n}\\sum_{i=0}^{n-1}\\|h_{t,m}^{\\pi_{i}^{i}}-\\nabla f_{m^{\\pi}}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}+\\displaystyle\\frac{4\\alpha}{M n}L_{\\mathrm{max}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}D_{f_{m^{\\pi}}^{\\pi_{i}^{i}}}(x_{t}^{i},x_{t})}\\\\ &&{\\displaystyle+\\frac{4\\alpha}{M n}L_{\\mathrm{max}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}D_{f_{m^{\\pi}}^{\\pi_{i}^{i}}}(x_{t},x_{\\star})}\\\\ &{\\le}&{\\displaystyle\\frac{1-\\alpha}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|h_{t,m}^{\\pi_{i}^{i}}-\\nabla f_{m^{\\pi}}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}+\\displaystyle\\frac{2\\alpha}{n}\\widetilde L_{\\mathrm{max}}\\sum_{i=0}^{n-1}\\|x_{t}^{i}-x_{t}\\|^{2}}\\\\ &&{\\displaystyle+\\frac{4\\alpha}{M n}L_{\\mathrm{max}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}D_{f_{m^{\\pi}}^{\\pi_{i}^{i}}}(x_{t},x_{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "LemaD5Ass $\\begin{array}{r}{\\tau\\leq\\frac{1}{2\\sqrt{\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)}}}\\end{array}$ Then,hefolowing neguaity holds ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{\\le}&{24\\tau^{2}\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]+8\\tau^{2}\\frac{\\sigma_{\\star,n}^{2}}{n}}\\\\ &&{\\displaystyle+8\\frac{\\tau^{2}\\omega}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{\\star,n}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f^{i}(x_{\\star})\\|^{2}}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "Proo. Since $\\begin{array}{r}{x_{t}^{i}=x_{t}-\\frac{\\tau}{M n}\\sum_{m=1}^{M}\\sum_{j=0}^{i-1}\\left(h_{t,m}^{\\pi_{m}^{j}}+\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{j}}(x_{t}^{j})-h_{t,m}^{\\pi_{m}^{i}}\\right)\\right)}\\end{array}$ we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{\\xi}_{\\mathbb{Q}}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{=}&{\\tau^{2}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{j=0}^{i-1}\\left(h_{t,m}^{\\pi_{m}^{j}}+Q\\left(\\nabla f_{m}^{\\pi_{m}^{j}}(x_{t}^{j})-h_{t,m}^{\\pi_{m}^{j}}\\right)\\right)\\right\\|^{2}\\right]}\\\\ &{=}&{\\tau^{2}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{j=0}^{i-1}\\left(h_{t,m}^{\\pi_{m}^{j}}-\\nabla f_{m}^{\\pi_{m}^{j}}(x_{t}^{j})+Q\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{j})-h_{t,m}^{\\pi_{m}^{j}}\\right)\\right)\\right\\|^{2}\\right]}\\\\ &&{+\\tau^{2}\\left\\|\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{j=0}^{i-1}\\nabla f_{m}^{\\pi_{m}^{j}}(x_{t}^{j})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Independence of $\\mathcal{Q}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t}^{j})-h_{t,m}^{\\pi_{m}^{j}}\\right)\\!,m\\in[M]$ and Assumption 1 imply ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|x_{t}^{i}-x_{t}\\right\\|^{2}\\right]}&{=\\;\\;\\frac{\\tau^{2}}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{n=0}^{i-1}\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|h_{t,m}^{\\sigma_{n}^{\\prime}}-\\nabla\\textstyle{J_{m}^{\\sigma_{n}^{\\prime}}(x_{t}^{i})}+\\mathcal{Q}\\left(\\nabla J_{m}^{\\sigma_{n}^{\\prime}}(x_{t}^{i})-h_{t,m}^{\\sigma_{n}^{\\prime}}\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad+\\tau^{2}\\left\\|\\displaystyle\\frac{1}{M n}\\displaystyle\\sum_{m=1,j=0}^{M}\\sum_{\\bar{m}=0}^{i-1}\\nabla J_{m}^{\\sigma_{n}^{\\prime}}(x_{t}^{i})\\right\\|^{2}}\\\\ {\\leq}&{\\;\\;\\frac{\\tau^{2}\\omega}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{n=0}^{i-1}\\left\\|\\nabla f^{\\sigma_{n}^{\\prime}}(x_{t}^{i})-h_{t,m}^{\\sigma_{n}^{\\prime}}\\right\\|^{2}+\\tau^{2}\\left\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{\\bar{j}=0}^{i-1}\\nabla f^{\\sigma_{n}^{\\prime}}(x_{t}^{i})\\right\\|^{2}}\\\\ {\\leq}&{\\;\\;\\frac{2\\tau^{2}\\omega}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{i-1}\\left\\|\\nabla f_{m}^{\\sigma_{n}^{\\prime}}(x_{t}^{i})-\\nabla f_{m}^{\\sigma_{n}^{\\prime}}(x_{t})\\right\\|^{2}+2\\tau^{2}\\left\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{j=0}^{i-1}\\nabla f^{\\sigma_{n}^{\\prime}}(x_{t})\\right\\|^{2}}\\\\ &{\\quad+\\frac{2\\tau^{2}\\omega}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\left\\|h_{t,m}^{\\sigma_{n}^{\\prime}}-\\nabla f_{m}^{\\sigma_{n}^{\\prime}}(x_{t})\\right\\|\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{Q}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{\\le}&{\\displaystyle\\frac{4\\tau^{2}\\omega}{M^{2}n}E_{\\mathrm{max}}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}D_{f_{m}^{\\varepsilon}}(x_{t}^{j},x_{t})+2\\tau^{2}\\left\\|\\frac{1}{n}\\sum_{j=0}^{i-1}\\nabla f^{\\pi^{j}}(x_{t})\\right\\|^{2}}\\\\ &&{\\displaystyle+\\frac{2\\tau^{2}\\omega}{M^{2}n}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\left\\|h_{t,m}^{\\pi_{j}^{\\varepsilon}}-\\nabla f_{m}^{\\pi^{j}}(x_{t})\\right\\|^{2}+\\frac{2\\tau^{2}\\tilde{L}^{2}}{n}\\sum_{j=0}^{n-1}\\left\\|x_{t}^{j}-x_{t}\\right\\|^{2}}\\\\ &{\\le}&{\\displaystyle2\\tau^{2}\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)\\frac{1}{n}\\sum_{j=0}^{n-1}\\|x_{t}^{j}-x_{t}\\|^{2}+2\\tau^{2}\\left\\|\\frac{1}{n}\\sum_{j=0}^{i-1}\\nabla f^{\\pi^{j}}(x_{t})\\right\\|^{2}}\\\\ &&{\\displaystyle+\\frac{2\\tau^{2}\\omega}{M^{2}n}\\sum_{m=1}^{M}\\left\\|h_{t,m}^{\\pi_{j}^{\\varepsilon}}-\\nabla f_{m}^{\\pi^{j}}(x_{t})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Taking the full expectation and using (17), we derive ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Vert x_{t}^{i}-x_{t}\\Vert^{2}\\right]}&{\\le}&{2\\tau^{2}\\widetilde{L}\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)\\frac{1}{n}\\displaystyle\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\Vert x_{t}^{j}-x_{t}\\Vert^{2}\\right]+2\\tau^{2}\\mathbb{E}\\left[\\Vert\\nabla f(x_{t})\\Vert^{2}\\right]}\\\\ &&{+\\frac{4\\tau^{2}\\omega}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\left\\Vert h_{t,m}^{\\pi_{m}^{j}}-\\nabla f_{m}^{\\pi_{m}^{j}}(x_{\\star})\\right\\Vert^{2}\\right]+\\frac{2\\tau^{2}}{n}\\mathbb{E}\\left[\\sigma_{t}^{2}\\right]}\\\\ &&{+\\frac{8\\tau^{2}\\omega}{M^{2}n^{2}}L_{\\mathrm{max}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[D_{f_{m}^{\\pi_{m}^{j}}}(x_{t},x_{\\star})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Using $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}^{i}$ and $\\widetilde{L}$ -smoothness and convexity of $f^{\\pi^{j}}$ , we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{\\leq}&{2\\tau^{2}\\widetilde{L}\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)\\frac{1}{n}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{j}-x_{t}\\|^{2}\\right]}\\\\ &&{+\\frac{4\\tau^{2}\\omega}{M^{2}n^{2}}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\left\\|h_{t,m}^{\\pi_{m}^{j}}-\\nabla f_{m}^{\\pi_{m}^{j}}(x_{\\star})\\right\\|^{2}\\right]+\\frac{2\\tau^{2}}{n}\\mathbb{E}\\left[\\sigma_{t}^{2}\\right]}\\\\ &&{+4\\tau^{2}\\left(\\widetilde{L}+\\frac{2\\omega}{M^{2}n^{2}}L_{\\mathrm{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Now we need to estimate $\\frac{2\\tau^{2}}{n}\\mathbb{E}\\left[\\sigma_{t}^{2}\\right]$ . Due to $\\begin{array}{r}{\\mathbb{E}\\left[\\sigma_{t}^{2}\\right]\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\|\\nabla f^{i}(x_{t})\\|^{2}\\right]}\\end{array}$ we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{2\\tau^{2}}{n}\\mathbb{E}\\left[\\sigma_{t}^{2}\\right]}&{\\leq}&{\\displaystyle\\frac{2\\tau^{2}}{n^{2}}\\sum_{j=1}^{n}\\mathbb{E}\\left[\\|\\nabla f^{j}(x_{t})\\|^{2}\\right]}\\\\ &{\\leq}&{\\displaystyle\\frac{4\\tau^{2}}{n^{2}}\\sum_{j=1}^{n}\\mathbb{E}\\left[\\|\\nabla f^{j}(x_{t})-\\nabla f^{j}(x_{\\star})\\|^{2}\\right]+\\displaystyle\\frac{4\\tau^{2}}{n^{2}}\\sum_{j=1}^{n}\\mathbb{E}\\left[\\|\\nabla f^{j}(x_{\\star})\\|^{2}\\right]}\\\\ &{\\leq}&{\\displaystyle\\frac{8\\tau^{2}}{n^{2}}\\tilde{L}\\sum_{j=1}^{n}\\mathbb{E}\\left[D_{f^{j}}(x_{t},x_{\\star})\\right]+\\displaystyle\\frac{4\\tau^{2}}{n^{2}}\\sum_{j=1}^{n}\\sigma_{n,\\star}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Combining two previous inequalities, we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{\\leq}&{2\\tau^{2}\\widetilde{L}\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)\\displaystyle\\frac{1}{n}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{j}-x_{t}\\|^{2}\\right]}\\\\ &&{+\\frac{4\\tau^{2}\\omega}{M^{2}n^{2}}\\displaystyle\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\left\\|h_{t,m}^{\\tau_{j}^{i}}-\\nabla f_{m}^{\\pi_{m}^{j}}(x_{*})\\right\\|^{2}\\right]}\\\\ &&{+4\\tau^{2}\\left(\\widetilde{L}+\\frac{2\\omega}{M^{2}n^{2}}L_{\\mathrm{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{*})\\right]}\\\\ &&{+\\frac{8\\tau^{2}}{n}\\widetilde{L}\\mathbb{E}\\left[f(x_{t})-f(x_{*})\\right]+\\frac{4\\tau^{2}}{n^{2}}\\displaystyle\\sum_{j=1}^{n}\\sigma_{n,*}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Summing from $i=0$ to $n-1$ and using $\\begin{array}{r}{\\tau\\leq\\frac{1}{2\\sqrt{\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)}}}\\end{array}$ , we obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}&{\\le}&{2\\left(1-2\\tau^{2}\\widetilde{L}\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)\\right)\\displaystyle\\frac{1}{n}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t}^{i}-x_{t}\\|^{2}\\right]}\\\\ &{\\le}&{\\displaystyle\\frac{8\\tau^{2}\\omega}{M^{2}n}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\left\\|h_{t,m}^{\\tau_{j}^{j}}-\\nabla f_{m}^{\\tau_{m}^{j}}(x_{k})\\right\\|^{2}\\right]}\\\\ &&{+8\\tau^{2}\\left(\\widetilde{L}+\\frac{2\\omega}{M^{2}n^{2}}L_{\\mathrm{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &&{\\displaystyle+\\frac{16\\tau^{2}}{n}\\widetilde{L}\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]+\\frac{8\\tau^{2}}{n^{2}}\\sum_{j=1}^{n}\\sigma_{n,\\star}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We consider the following Lyapunov function: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Psi_{t+1}=\\|x_{t+1}-x_{\\star}\\|^{2}+\\frac{c\\tau^{2}}{M n}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\left\\|h_{t+1,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}\\big(x_{\\star}\\big)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Theorem D.2. Let Assumptions 1, 2, 3, 5 hold and ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\gamma\\leq\\operatorname*{min}\\left\\{\\frac{\\alpha}{n\\mu},\\frac{1}{12n\\left(\\widetilde{L}+\\frac{11\\omega}{M n}L_{\\mathrm{max}}\\right)}\\right\\},\\quad\\alpha\\leq\\frac{1}{1+\\omega},\\quad c=\\frac{10\\omega}{\\alpha M n}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Then, for all $T\\geq0$ the iterates produced by DIANA-RR satisfy ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\frac{n\\gamma\\mu}{2}\\right)^{T}\\Psi_{0}+20\\frac{\\gamma^{2}n\\widetilde{L}}{\\mu}\\sigma_{\\star,n}^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. Taking expectation w.r.t. $\\mathcal{Q}$ and using Lemma D.2, we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{Q}\\left[\\|x_{t+1}-x_{\\star}\\|^{2}\\right]}&{=}&{\\|x_{t}-\\tau\\hat{g}_{t}-x_{\\star}+\\tau h^{\\star}\\|^{2}}\\\\ &{=}&{\\|x_{t}-x_{\\star}\\|^{2}-2\\tau\\mathbb{E}_{Q}\\left[\\langle\\hat{g}_{t}-h^{\\star},x_{t}-x_{\\star}\\rangle\\right]+\\tau^{2}\\mathbb{E}_{Q}\\left[\\|\\hat{g}_{t}-h^{\\star}\\|^{2}\\right]}\\\\ &{\\leq}&{\\|x_{t}-x_{\\star}\\|^{2}-\\displaystyle\\frac{\\tau\\mu}{2}\\|x_{t}-x_{\\star}\\|^{2}+\\tau^{2}\\mathbb{E}_{Q}\\left[\\|\\hat{g}_{t}-h^{\\star}\\|^{2}\\right]}\\\\ &&{-\\tau\\left(f(x_{t})-f(x_{\\star})\\right)+\\tau\\displaystyle\\tilde{L}\\displaystyle\\frac{n-1}{n}\\sum_{i=1}^{n-1}\\|x_{t}-x_{t}^{i}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Next, due to Lemma D.3 we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|x_{t+1}-x_{*}\\right\\|^{2}\\right]}&{\\leq\\;\\left(1-\\frac{\\eta\\bar{\\mu}}{2}\\right)\\left\\|x_{t}-x_{*}\\right\\|^{2}-\\tau\\left(f(x_{t})-f(x_{*})\\right)+\\tau\\tilde{L}_{\\underline{{n}}}^{1}\\frac{1-\\nu-1}{\\sum_{i=1}^{j}\\left|x_{i}-x_{i}^{*}\\right|}}\\\\ &{\\quad+2\\tau^{2}\\tilde{L}_{\\overline{{n}}}\\left(\\tilde{L}+\\frac{\\omega}{M n}I_{\\operatorname*{max}}\\right)\\frac{1}{n}\\frac{\\Gamma-1}{\\log\\left|x_{*}^{*}\\right|}\\geq\\|x_{*}^{'}-x_{*}\\|^{2}}\\\\ &{\\quad+8\\tau^{2}\\left(\\tilde{L}+\\frac{\\omega}{M n}I_{\\operatorname*{max}}\\right)(f(x_{*})-f(x_{*}))}\\\\ &{\\quad+\\frac{4\\omega\\tau^{2}\\,n^{2}-1}{M^{2}\\,n^{2}}\\frac{1}{\\log\\left|1\\right|}h_{\\underline{{n}}}^{*}-\\nabla f_{\\underline{{n}}}^{*}(x_{*})\\|^{2}}\\\\ &{\\leq\\;\\left(1-\\frac{\\eta\\bar{\\mu}}{2}\\right)\\left\\|x_{*}-x_{*}\\right\\|^{2}+\\frac{4\\omega\\tau^{2}}{M^{2}\\,n^{2}}\\frac{1}{\\omega}\\sum_{i=1}^{M}\\left\\|h_{\\underline{{n}}}^{*}-\\nabla f_{\\overline{{n}}}^{*}(x_{*})\\right\\|^{2}}\\\\ &{\\quad-\\tau\\left(1-\\delta\\tau\\left(\\tilde{L}+\\frac{\\omega}{M n}I_{\\operatorname*{max}}\\right)\\right)\\left(f(x_{*})-f(x_{*})\\right)}\\\\ &{\\quad+\\tau\\tilde{L}\\left(1+2\\tau\\left(\\tilde{L}+\\frac{\\omega}{M n}I_{\\operatorname*{max}}\\right)\\right)\\frac{1}{n}\\frac{1-\\nu-1}{\\big|\\alpha\\big|^{2}}\\left|x_{*}^{*}-x_{*}\\right|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Using (23), we obtain ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{Q}\\left[\\Psi_{t+1}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{\\tau\\mu}{2}\\right)\\|x_{t}-x_{\\star}\\|^{2}+\\frac{4\\omega\\tau^{2}}{M^{2}n^{2}}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\|h_{t,m}^{\\overline{{\\tau}}_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}}\\\\ &&{\\displaystyle-\\tau\\left(1-8\\tau\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\right)(f(x_{t})-f(x_{\\star}))}\\\\ &&{\\displaystyle+\\tau\\tilde{L}\\left(1+2\\tau\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\right)\\frac{1}{n}\\sum_{i=0}^{n-1}\\|x_{t}^{i}-x_{t}\\|^{2}}\\\\ &&{\\displaystyle+\\frac{c\\tau^{2}}{M n}\\sum_{m=1}^{M}\\sum_{j=0}^{n-1}\\mathbb{E}\\left[\\left\\|h_{t+1,m}^{\\overline{{\\tau}}_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "To estimate the last term in the above inequality, we apply Lemma D.4: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\xi}[\\theta_{t+1}]}&{\\le\\left(1-\\frac{\\eta^{6}\\eta}{2}\\right)\\|x_{t}-x_{t}\\|^{2}+\\frac{4\\eta^{2}}{12\\eta^{2}}\\frac{1}{\\log{\\frac{3}{2}}}\\frac{\\|x_{t}^{*}-x_{t}^{*}\\|^{2}}{\\|x_{t}^{*}-x_{t}^{*}\\|^{2}}}\\\\ &{-\\tau\\left(1-8\\tau\\left(\\frac{1}{L}+\\frac{1}{\\eta}\\frac{1}{L}\\mathrm{for~and~}\\right)\\right)(\\eta(x_{t})-f(x_{t}))}\\\\ &{+\\tau\\left(1+2\\tau\\left(\\frac{1}{L}+\\frac{1}{\\eta}\\frac{1}{L}\\mathrm{for~and~}\\right)\\right)\\frac{1}{\\eta}\\frac{1}{\\log{\\frac{3}{2}}}\\|x_{t}^{*}-x_{t}\\|^{2}}\\\\ &{+\\sigma^{2}\\frac{1-\\alpha}{18\\eta}\\frac{1}{\\log{\\frac{3}{2}}}\\left\\|h_{t}^{*}-x_{t}^{*}\\right\\|^{2}}\\\\ &{+\\sigma^{2}\\frac{2\\delta\\left(L\\eta\\right)}{18\\eta}\\frac{\\sqrt{1}}{\\log{\\frac{3}{2}}}\\left\\|x_{t}^{*}-x_{t}\\right\\|^{2}+4\\tau^{2}\\alpha^{2}\\mathrm{for~and~}\\!\\left(f(x_{t})-f(x_{t}^{*})\\right)}\\\\ &{\\le\\left(1-\\frac{\\eta^{6}\\eta}{2}\\right)\\|x_{t}-x_{t}\\|^{2}+\\Big(1-\\alpha+\\frac{4\\delta}{\\sqrt{d t}}\\Big)\\frac{\\sigma^{2}-\\frac{1-\\delta}{\\eta}}{\\operatorname{M}_{m}^{2}\\delta_{m}}\\frac{\\|x_{t}^{*}-\\nabla_{x}^{2}\\|^{2}}{\\|x_{t}^{*}-x_{t}^{*}\\|^{2}}}\\\\ &{-\\tau\\left(1-4\\tau\\alpha\\mathrm{for~and~}-\\delta\\tau\\left(\\tilde{L}+\\frac{\\delta\\alpha}{\\sqrt{d t}}\\right)\\right)(\\eta(x_{t})-f(x_{t}))}\\\\ &{+\\tau\\left(1+2\\tau\\alpha\\mathrm{for~and~}+2\\tau\\left(\\tilde{L}+\\frac{1}{\\\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathcal{H}_{t}\\;=\\;\\frac{c\\tau^{2}}{M n}\\sum_{i=0}^{n-1}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\|h_{t,m}^{\\pi_{m}^{i}}-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star})\\|^{2}\\right]}\\end{array}$ .Taking thefll expetation anduing Lemma D.5, we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{t\\left|\\Psi_{t+1}\\right|}&{\\leq\\;\\left(1-\\frac{\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\|\\tau_{t}-\\tau_{k}\\|^{2}\\right]+\\bigg(1-\\alpha+\\frac{4\\omega}{c\\Delta t}\\bigg)\\,\\mathcal{H}_{t}}\\\\ &{\\quad-\\tau\\left(1-4c\\tau L_{n a x}-8\\tau\\left(\\tilde{L}+\\frac{\\omega}{M}\\frac{L_{0}}{M}L_{n a x}\\right)\\right)\\mathbb{E}\\left[f(\\tau_{k})-f(x_{*})\\right]}\\\\ &{\\quad+\\tau\\widetilde{L}\\left(1+2c\\tau L_{n a x}+2\\tau\\left(\\tilde{L}+\\frac{\\omega}{M}\\frac{L_{0}}{M}L_{n a x}\\right)\\right)\\frac{1-\\frac{1}{\\tau}}{n}\\mathbb{E}\\left[\\|x_{t}^{*}-x_{t}\\|^{2}\\right]}\\\\ &{\\leq\\;\\left(1-\\frac{\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\|x_{t}-x_{t}\\|^{2}\\right]+\\bigg(1-\\alpha+\\frac{4\\omega}{c\\Delta t}\\bigg)\\,\\mathcal{H}_{t}}\\\\ &{\\quad-\\tau\\left(1-4c\\tau L_{n a x}-8\\tau\\left(\\tilde{L}+\\frac{\\omega}{M}\\frac{L_{0}}{M}L_{n a x}\\right)\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{*})\\right]}\\\\ &{\\quad+2t^{3}\\widetilde{L}\\left(1+2c\\tau L_{n a x}+2\\tau\\left(\\tilde{L}+\\frac{\\omega}{M}\\frac{L_{0}}{M}L_{n a x}\\right)\\right)\\left(\\tilde{L}+\\frac{\\omega}{M}\\frac{L_{0}}{M}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{*})\\right]}\\\\ &{\\quad+8\\tau^{3}\\widetilde{L}\\left(1+2c\\tau L_{n a x}+2\\tau\\left(\\tilde{L}+\\frac{\\omega}{M}\\frac{L_{0}}{M}L_{n a x}\\right)\\right)\\frac{\\sigma_{n s}^{2}}{n}}\\\\ &{\\quad+\\frac{8\\tau\\widetilde{L}\\,\\omega}{1-\\lambda}\\left(1+2c\\tau L_{n a x}+2\\tau\\left(\\tilde{L}+\\frac{\\omega}{M}\\frac{L_{0}}{L_{n a x}}\\right)\\right)\\frac{\\sigma_{n s}^{2}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Selecting $\\textstyle c={\\frac{A\\omega}{\\alpha M n}}$ ,where $A$ is a positive number to be specified later, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1+2c\\tau\\alpha L_{\\operatorname*{max}}+2\\tau\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)=1+2\\tau\\left(\\widetilde{L}+\\frac{(A+1)\\omega}{M n}L_{\\operatorname*{max}}\\right),}\\\\ {1-4c\\tau\\alpha L_{\\operatorname*{max}}-8\\tau\\left(\\widetilde{L}+\\frac{\\omega}{M n}L_{\\operatorname*{max}}\\right)\\geq1-8\\tau\\left(\\widetilde{L}+\\frac{(A+1)\\omega}{M n}L_{\\operatorname*{max}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Psi_{t+1}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\|x_{t}-x_{*}\\|^{2}\\right]+\\left(1-\\alpha+\\frac{4\\alpha}{A}\\right)\\mathcal{H}_{t}}\\\\ &&{\\displaystyle-\\tau\\left(1-8\\tau\\left(\\tilde{L}+\\frac{(A+1)\\omega}{M n}L_{\\mathrm{max}}\\right)\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{*})\\right]}\\\\ &&{\\displaystyle+24\\tau^{3}\\tilde{L}\\left(\\tilde{L}+\\frac{\\omega}{M n}L_{\\mathrm{max}}\\right)\\left(1+2\\tau\\left(\\tilde{L}+\\frac{(A+1)\\omega}{M n}L_{\\mathrm{max}}\\right)\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{*})\\right]}\\\\ &&{\\displaystyle+8\\tau^{3}\\tilde{L}\\left(1+2\\tau\\left(\\tilde{L}+\\frac{(A+1)\\omega}{M n}L_{\\mathrm{max}}\\right)\\right)\\frac{\\sigma_{*,n}^{2}}{n}}\\\\ &&{\\displaystyle+\\frac{8\\alpha}{A}\\tau\\tilde{L}\\left(1+2\\tau\\left(\\tilde{L}+\\frac{(A+1)\\omega}{M n}L_{\\mathrm{max}}\\right)\\right)\\mathcal{H}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Taking T = $\\begin{array}{r}{\\tau=\\frac{1}{B\\left(\\widetilde{L}+\\frac{(A+1)\\omega}{M n}L_{\\operatorname*{max}}\\right)}}\\end{array}$ where $B$ is some positive constant, we obtain ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Psi_{t+1}\\right]}&{\\leq}&{\\left(1-\\displaystyle\\frac{\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\|x_{t}-x_{\\star}\\|^{2}\\right]+\\left(1-\\alpha+\\displaystyle\\frac{4\\alpha}{A}+\\displaystyle\\frac{8\\alpha}{A}\\tau\\widetilde{L}\\left(1+\\displaystyle\\frac{2}{B}\\right)\\right)\\mathcal{H}_{t}}\\\\ &&{-\\tau\\left(1-\\displaystyle\\frac{8}{B}-\\displaystyle\\frac{24}{B^{2}}\\left(1+\\displaystyle\\frac{2}{B}\\right)\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &&{+8\\tau^{3}\\widetilde{L}\\left(1+\\displaystyle\\frac{2}{B}\\right)\\displaystyle\\frac{\\sigma_{\\star,n}^{2}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Choosing $\\begin{array}{r}{A=10,B=12,\\tau\\leq\\frac{\\alpha}{\\mu}}\\end{array}$ we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\mathbb{E}\\left[\\Psi_{t+1}\\right]}&{\\leq}&{\\left(1-\\operatorname*{min}\\left\\{\\displaystyle\\frac{\\tau\\mu}{2},\\displaystyle\\frac{\\alpha}{2}\\right\\}\\right)\\mathbb{E}\\left[\\Psi_{t}\\right]+10\\tau^{3}\\widetilde L\\displaystyle\\frac{\\sigma_{\\star,n}^{2}}{n}}\\\\ &{\\leq}&{\\left(1-\\displaystyle\\frac{\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\Psi_{t}\\right]+10\\tau^{3}\\widetilde L\\displaystyle\\frac{\\sigma_{\\star,n}^{2}}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Reursielyngqulty $\\tau=n\\gamma$ and using $\\sum_{t=0}^{+\\infty}\\left(1-\\frac{\\tau\\mu}{2}\\right)^{t}\\leq\\frac{2}{\\mu\\tau}$ we finishproof. \u53e3", "page_idx": 50}, {"type": "text", "text": "Corollary 8. Let the assumptions of Theorem $D.2$ hold, $\\begin{array}{r}{\\alpha=\\frac{1}{1+\\omega}}\\end{array}$ 1+w,and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{\\frac{\\alpha}{2n\\mu},\\frac{1}{12n\\left(\\widetilde{L}+\\frac{11\\omega}{M n}L_{\\mathrm{max}}\\right)},\\sqrt{\\frac{\\varepsilon\\mu}{40n\\widetilde{L}\\sigma_{\\star,n}^{2}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then, DIANA-RR finds a solution with accuracy $\\varepsilon>0$ after the following number of communication rounds: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(n(1+\\omega)+\\frac{n\\widetilde{L}}{\\mu}+\\frac{\\omega}{M}\\frac{L_{\\operatorname*{max}}}{\\mu}+\\sqrt{\\frac{n\\widetilde{L}}{\\varepsilon\\mu^{3}}}\\sigma_{\\star,n}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. Theorem D.2 implies ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\gamma\\mu\\right)^{n T}\\Psi_{0}+20\\frac{\\gamma^{2}n\\widetilde{L}}{\\mu}\\sigma_{\\star,n}^{2}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "To estimate the number of communication rounds required to find a solution with accuracy $\\varepsilon>0$ ,we need to upper bound each term from the right-hand side by $\\frac{\\varepsilon}{2}$ . Thus, we get an additional condition oOn $\\gamma$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n20\\frac{\\gamma^{2}n\\widetilde{L}}{\\mu}\\sigma_{\\star,n}^{2}<\\frac{\\varepsilon}{2},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and also the upper bound on the number of communication rounds $n T$ ", "page_idx": 50}, {"type": "equation", "text": "$$\nn T=\\tilde{\\mathcal{O}}\\left(\\frac{1}{\\gamma\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Substituting (24) in the previous equation, we obtain the result. ", "page_idx": 50}, {"type": "text", "text": "E Theoretical Results for Q-NASTYA and DIANA-NASTYA ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Theorem E.1. Let Asumptions 1 2, 3 hold. Let the stepsizes , # satisfy $\\begin{array}{r}{0<\\eta\\le\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)}}\\end{array}$ $\\begin{array}{r}{0<\\gamma\\le\\frac{1}{5n L_{\\mathrm{max}}}}\\end{array}$ .Then,for all $T\\geq0$ the iterates produced by Q-NASTYA (Algorithm 3) satisfy ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert x_{T}-x_{\\star}\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\eta\\mu}2\\right)^{T}\\Vert x_{0}-x_{\\star}\\Vert^{2}+8\\frac{\\eta\\omega}{\\mu M}\\zeta_{\\star}^{2}+\\frac{9}{2}\\frac{\\gamma^{2}n L_{\\operatorname*{max}}}{\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Corollary 9. Under the same conditions as Theorem $E.l$ and for Algorithm 3, there exist stepsizes $\\gamma=\\eta/n$ and $\\eta>0$ such that the number of communication rounds $T$ to find a solution with accuracy \u03b5 > 0is O $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\frac{\\omega}{M}\\frac{\\zeta_{\\star}^{2}}{\\varepsilon\\mu^{3}}+\\sqrt{\\frac{L_{\\mathrm{max}}}{\\varepsilon\\mu^{3}}}\\sqrt{\\zeta_{\\star}^{2}+\\frac{\\sigma_{\\star}^{2}}{n}}\\right).I f\\gamma\\to0,}\\end{array}$ .one can choose $\\eta>0$ such thathviv $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\frac{\\omega}{M}\\frac{\\zeta_{\\star}^{2}}{\\varepsilon\\mu^{3}}\\right)}\\end{array}$ ", "page_idx": 51}, {"type": "text", "text": "We emphasize several differences with the known theoretical results. First, the FedCOM method of Haddadpour et al. [2021] was analyzed in the homogeneous setting only, i.e., $f_{m}(x)\\,=\\,f(x)$ for all $m\\,\\in\\,[M]$ , which is an unrealistic assumption for $\\mathrm{FL}$ applications. In contrast, our result holds in the fully heterogeneous case. Next, the analysis of FedPAQ of Reisizadeh et al. [2020] uses a bounded variance assumption, which is also known to be restrictive. Nevertheless, let us compare to their result. Reisizadeh et al. [2020] derive the following complexity for their method: $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\frac{\\omega}{M}\\frac{\\sigma^{2}}{\\mu^{2}\\varepsilon}+\\frac{\\sigma^{2}}{M\\mu^{2}\\varepsilon}\\right)}\\end{array}$ . This result is inferior to the one we show for Q-NASTYA: when $\\omega$ is small, the main term in the complexity bound of FedPAQ is $\\widetilde{\\mathcal{O}}\\left({\\sqrt[1]{\\varepsilon}}\\right)$ , while for $\\upalpha$ -NASTYA the dominating term is of the order $\\widetilde{\\mathcal{O}}\\left({1}/{\\sqrt{\\varepsilon}}\\right)$ (when $\\omega$ and $\\varepsilon$ are sufficiently small). We also highlight that FedCRR [Malinovsky and Richtarik, 2022] does not converge if $\\omega\\stackrel{.}{>}M^{2}\\gamma\\dot{\\mu}\\varepsilon\\big/(2\\big|\\big|x_{*,m}^{n}\\big|\\big|^{2})$ while Q-NASTYA does for any $\\omega\\ge0$ . Finally, when $\\omega=0$ (no compression) we recover NASTYA as a special case, and using $\\gamma=\\eta/n$ , we recover the rate of FedRR [Mishchenko et al., 2021]. ", "page_idx": 51}, {"type": "text", "text": "TheoremE.2. LetAsutions 1 2 3hold Suppose th stepsizes , ,\u03b1 satisfy 0<\u22641Lmn $\\begin{array}{r}{0<\\eta\\leq\\operatorname*{min}\\left\\{\\frac{\\alpha}{2\\mu},\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{9\\omega}{M}\\right)}\\right\\}}\\end{array}$ [2 16Lma(1+) and \u03b1 \u2264 I Defne the fllowing Lyapumov function: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\Psi_{t+1}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\|x_{t+1}-x_{\\star}\\|^{2}+{\\frac{8\\omega\\eta^{2}}{\\alpha M^{2}}}\\sum_{m=1}^{M}\\|h_{t+1,m}-h_{m}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Then,for all $T\\geq0$ the iterates produced by DIANA-NASTYA (Algorithm 4) satisfy ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)^{T}\\Psi_{0}+\\frac{9}{2}\\frac{\\gamma^{2}n L}{\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Corollary 10. Under the same conditions as Theorem $E.2$ and for Algorithm 4, there exist stepsizes $\\gamma=\\eta/n,\\,\\eta>0$ $\\alpha\\,>\\,0$ such that the number of communication rounds $T$ to find a solution with accuracy $\\varepsilon>0$ $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\omega+\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\sqrt{\\frac{L_{\\mathrm{max}}}{\\varepsilon\\mu^{3}}}\\sqrt{\\zeta_{\\star}^{2}+\\frac{\\sigma_{\\star}^{2}}{n}}\\right).I f\\gamma\\rightarrow0,}\\end{array}$ .one can choose $\\eta>0$ such that the aobove complexiy bound improves to $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\omega+\\frac{L_{\\operatorname*{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)\\right)}\\end{array}$ ", "page_idx": 51}, {"type": "text", "text": "In contrast to $\\upalpha$ -NASTYA, DIANA-NASTYA does not suffer from the $\\widetilde{\\mathcal{O}}(^{1}\\!/\\varepsilon)$ term in the complexity bound. This shows the superiority of DIANA-NASTYA to $\\upalpha$ -NASTYA. Next, FedCRR-VR [Malinovsky and Richtarik, 2022] has the rate $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{(\\omega+1)\\left(1-\\frac{1}{\\kappa}\\right)^{n}}{\\left(1-\\left(1-\\frac{1}{\\kappa}\\right)^{n}\\right)^{2}}+\\frac{\\sqrt{\\kappa}(\\zeta_{\\star}+\\sigma_{\\star})}{\\mu\\sqrt{\\varepsilon}}\\right)}\\end{array}$ which depends on $\\widetilde{\\mathcal{O}}\\left({1}/{\\sqrt{\\varepsilon}}\\right)$ However, the first term is close to $\\widetilde{\\cal O}\\left((\\omega+1)\\kappa^{2}\\right)$ for a large condition number. FedCRR-VR-2 utilizes variance reduction technique from Malinovsky et al. [2021] and it allows to get rid of permutation variance. This method has $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{(\\omega+1)\\left(1-\\frac{1}{\\kappa\\sqrt{\\kappa n}}\\right)^{\\frac{n}{2}}}{\\left(1-\\left(1-\\frac{1}{\\kappa\\sqrt{\\kappa n}}\\right)^{\\frac{n}{2}}\\right)^{2}}+\\frac{\\sqrt{\\kappa}\\zeta_{\\star}}{\\mu\\sqrt{\\varepsilon}}\\right)}\\end{array}$ complexity, but it requires additional assumption on number of functions $n$ and thus not directly comparable with our result. Note that if we have no compression ( $\\omega=0$ ), DIANA-NASTYA recovers rate of NASTYA. ", "page_idx": 51}, {"type": "text", "text": "F Missing Proofs for Q-NASTYA ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "We start with deriving a technical lemma along with stating several useful results from [Malinovsky et al., 2022]. For convenience, we also introduce the following notation: ", "page_idx": 52}, {"type": "equation", "text": "$$\ng_{t,m}=\\frac1n\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i}).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma F.1. Let Assumptions 1, 2, 3 hold. Then, for all $t\\geq0$ the iterates produced by Q-NASTYA satisfy ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{Q}\\left[\\Vert g_{t}\\Vert^{2}\\right]\\leq\\frac{2L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\omega}{M}\\right)}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\Vert x_{t,m}^{i}-x_{t}\\right\\Vert^{2}+8L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)(f(x_{t})-f(x_{\\star}))+\\frac{4\\omega}{M}\\zeta_{\\star}^{2},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\mathbb{E}_{\\mathcal{Q}}$ is expectation w.rt. $\\mathcal{Q}$ , and $\\begin{array}{r}{\\zeta_{\\star}^{2}=\\frac{1}{M}\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}(x_{\\star})\\right\\|^{2}}\\end{array}$ ", "page_idx": 52}, {"type": "text", "text": "Proof. Using the variance decomposition $\\mathbb{E}\\left[\\|\\xi\\|^{2}\\right]=\\mathbb{E}\\left[\\|\\xi-\\mathbb{E}\\left[\\xi\\right]\\|^{2}\\right]+\\|\\mathbb{E}\\xi\\|^{2}$ we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\mathbb{E}_{\\mathcal{Q}}\\left[\\|g_{t}\\|^{2}\\right]}&{}&{=}&{\\displaystyle\\frac{1}{M^{2}}\\sum_{m=1}^{M}\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\|\\mathcal{Q}\\left(\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})\\right)-\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})\\right\\|^{2}\\right]}\\\\ &{}&&{\\displaystyle+\\left\\|\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})\\right\\|^{2}}\\\\ &{}&{\\displaystyle\\operatorname*{Asm.}_{M^{2}}\\sum_{m=1}^{M}\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})\\right\\|^{2}+\\left\\|\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Next, we use $\\begin{array}{r}{\\nabla f_{m}(x_{t})=\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t})}\\end{array}$ and $\\|a+b\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2}$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\|g_{t}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\frac{2\\omega}{M^{2}}\\sum_{m=1}^{M}\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t})\\right)\\right\\|^{2}+\\frac{2\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{t})\\|^{2}}\\\\ &&{\\displaystyle+2\\left\\|\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t})\\right)\\right\\|^{2}+2\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\nabla f_{m}(x_{t})\\right\\|^{2}}\\\\ &{\\leq}&{\\displaystyle\\frac{2\\left(1+\\frac{\\omega}{M}\\right)}{M}\\sum_{m=1}^{M}\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t})\\right)\\right\\|^{2}}\\\\ &{\\displaystyle+\\frac{2\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{t})\\|^{2}+2\\left\\|\\nabla f(x_{t})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Using $\\boldsymbol{L}_{i,m}$ -smoothness of $f_{m}^{i}$ and $f$ and also convexity of $f_{m}$ , we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathfrak{z}_{Q}\\left[\\|g_{t}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\frac{2\\left(1+\\frac{\\omega}{M}\\right)}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t})\\right\\|^{2}+\\displaystyle\\frac{4\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{t})-\\nabla f_{m}\\|^{2}}\\\\ &&{\\displaystyle+\\frac{4\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{\\star})\\|^{2}+2\\left\\|\\nabla f(x_{t})-\\nabla f(x_{\\star})\\right\\|^{2}}\\\\ &{\\leq}&{\\displaystyle\\frac{2L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\omega}{M}\\right)}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{i}-x_{t}\\right\\|^{2}+\\displaystyle\\frac{8L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)}{M}\\sum_{m=1}^{M}D_{f_{m}}(x_{t},x_{\\star})+\\frac{4\\omega}{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma F.2 (see [Malinovsky et al., 2022]). Under Assumptions 1, 2, 3, it holds ", "page_idx": 53}, {"type": "equation", "text": "$$\n-\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\langle f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i}),x_{t}-x_{\\star}\\right\\rangle\\leq-\\frac{\\mu}{4}\\|x_{t}-x_{\\star}\\|^{2}-\\frac{1}{2}\\left(f(x_{t})-f(x_{\\star})\\right)+\\frac{L_{\\operatorname*{max}}}{2M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{i}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Lemma E3 (see [Malinovsky et al., 2022]). UnderAssumptions 1 2, 3 and \u2264 2Lmaxm ,itholds ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\Vert x_{t,m}^{i}-x_{t}\\right\\Vert^{2}\\leq8\\gamma^{2}n^{2}L_{\\operatorname*{max}}\\left(f(x_{t})-f(x_{\\star})\\right)+2\\gamma^{2}n\\left(\\sigma_{\\star}^{2}+(n+1)\\zeta_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Theorem F.1. Let Assumptions 1, 2, 3 hold and stepsizes $\\gamma,\\,\\eta$ satisfy ", "page_idx": 53}, {"type": "equation", "text": "$$\n0<\\eta\\leq\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)},\\quad0<\\gamma\\leq\\frac{1}{5n L_{\\operatorname*{max}}}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Then, for all $T\\geq0$ the iterates produced by Q-NASTYA satisfy ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\Vert x_{T}-x_{\\star}\\Vert^{2}\\right]}&{\\leq}&{\\left(1-\\displaystyle\\frac{\\eta\\mu}{2}\\right)^{T}\\Vert x_{0}-x_{\\star}\\Vert^{2}+\\displaystyle\\frac{9}{2}\\frac{\\gamma^{2}n L_{\\operatorname*{max}}}{\\mu}\\left(\\sigma_{\\star}^{2}+(n+1)\\zeta_{\\star}^{2}\\right)+8\\displaystyle\\frac{\\eta\\omega}{\\mu M}\\zeta_{\\star}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. Taking expectation w.r.t. $\\mathcal{Q}$ and using Lemma F.1, we get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{\\Sigma}_{\\Omega}\\left[\\lVert u_{t+1}-x_{*}\\rVert^{2}\\right]}&{=}&{\\lVert x_{t}-x_{*}\\rVert^{2}-2\\eta\\mathrm{g}_{2}[\\langle\\theta_{t},x_{t}-x_{*}\\rangle]+\\eta^{2}\\mathrm{g}_{2}\\left[\\lVert\\theta^{\\parallel}\\rVert^{2}\\right]}\\\\ {\\leq}&{\\lVert x_{t}-x_{*}\\rVert^{2}-2\\eta\\mathrm{g}_{2}\\mathbb{E}_{\\sigma}\\Bigg[\\left\\langle\\frac{1}{M}\\frac{M}{\\sum_{i=0}^{M}}Q\\left(\\frac{1}{n}\\frac{n-1}{\\log}\\nabla f_{m^{*}}^{\\mu}(x_{t,m}^{*})\\right),x_{t}-x_{*}\\right\\rangle\\Bigg]}\\\\ &{+\\frac{2\\eta^{2}I_{2\\Omega,\\alpha}^{2}}{M n}\\left(1+\\frac{\\eta}{M}\\right)\\frac{M}{\\sum_{i=1}^{M}}\\sum_{i=0}^{n-1}\\left\\lVert u_{t,m}^{*}-x_{t}\\right\\rVert^{2}}\\\\ &{+8\\eta^{2}I_{2\\Omega,\\alpha}\\left(1+\\frac{\\omega}{M}\\right)\\left(f(x_{t})-f(x_{*})\\right)+4\\eta^{2}\\frac{\\omega^{2}}{M}\\zeta^{2}}\\\\ {\\leq}&{\\lVert x_{t}-x_{*}\\rVert^{2}-2\\eta\\frac{1}{M n}\\frac{M}{m-1}\\sum_{i=0}^{n-1}\\left\\langle\\nabla f_{m^{*}}^{\\mu}(x_{t,m}^{*}),x_{t}-x_{*}\\right\\rangle}\\\\ &{+\\frac{2\\eta^{2}I_{2\\Omega,\\alpha}^{2}\\left(1+\\frac{\\eta}{M}\\right)}{M n}\\frac{M}{\\sum_{i=1}^{M}\\sum_{i=0}^{M}\\left\\lVert u_{t,m}^{*}-x_{t}\\right\\rVert^{2}}}\\\\ &{+8\\eta^{2}I_{\\Omega,\\alpha}\\left(1+\\frac{\\omega}{M}\\right)\\left(f(x_{t})-f(x_{*})\\right)+4\\eta^{2}\\frac{\\omega}{M}\\zeta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Next, Lemma F.2 implies ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\mathbb{\\hat{z}_{Q}}\\left[\\left\\|x_{t+1}-x_{*}\\right\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\|x_{t}-x_{*}\\|^{2}-\\frac{\\eta\\mu}{2}\\|x_{t}-x_{*}\\|^{2}-\\eta\\left(f(x_{t})-f(x_{*})\\right)}\\\\ &&{+8\\eta^{2}L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)\\left(f(x_{t})-f(x_{*})\\right)+\\frac{\\eta L_{\\operatorname*{max}}}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{i}-x_{t}\\right\\|^{2}}\\\\ &{+\\frac{2\\eta^{2}L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\omega}{M}\\right)}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{i}-x_{t}\\right\\|^{2}+4\\eta^{2}\\frac{\\omega}{M}\\zeta_{*}^{2}}\\\\ {\\leq}&{\\left(1-\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x_{*}\\|^{2}-\\eta\\left(1-8\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)\\right)\\left(f(x_{t})-f(x_{*})\\right)}\\\\ &{+\\frac{\\eta L_{\\operatorname*{max}}}{M n}\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)\\right)\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{i}-x_{t}\\right\\|^{2}+4\\eta^{2}\\frac{\\omega}{M}\\zeta_{*}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Using Lemma F.3, we get ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{Q}\\left[\\|x_{t+1}-x_{\\star}\\|^{2}\\right]}&{\\leq}&{\\left(1-\\displaystyle\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x_{\\star}\\|^{2}-\\eta\\left(1-8\\eta L\\left(1+\\displaystyle\\frac{\\omega}{M}\\right)\\right)\\left(f(x_{t})-f(x_{\\star})\\right)}\\\\ &&{+\\eta L_{\\operatorname*{max}}\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\displaystyle\\frac{\\omega}{M}\\right)\\right)\\cdot8\\gamma^{2}n^{2}L_{\\operatorname*{max}}\\left(f(x_{t})-f(x_{\\star})\\right)}\\\\ &&{+\\eta L_{\\operatorname*{max}}\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\displaystyle\\frac{\\omega}{M}\\right)\\right)\\cdot2\\gamma^{2}n\\left(\\sigma_{\\star}^{2}+(n+1)\\zeta_{\\star}^{2}\\right)}\\\\ &&{+4\\eta^{2}\\displaystyle\\frac{\\omega}{M}\\zeta_{\\star}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "In view of (27), we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\left[\\|x_{t+1}-x_{\\star}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x_{\\star}\\|^{2}+4\\eta^{2}\\frac{\\omega}{M}\\zeta_{\\star}^{2}}\\\\ &&{-\\eta\\left(1-8\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)-8\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\left(1+2L_{\\operatorname*{max}}\\eta\\left(1+\\frac{\\omega}{M}\\right)\\right)\\right)\\left(f(x_{t})\\right)}\\\\ &&{+2\\gamma^{2}n\\eta L_{\\operatorname*{max}}\\left(1+2\\eta L\\left(1+\\frac{\\omega}{M}\\right)\\right)\\left(\\sigma_{\\star}^{2}+n\\zeta_{\\star}^{2}\\right)}\\\\ &{\\leq}&{\\displaystyle\\left(1-\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x_{\\star}\\|^{2}+4\\eta^{2}\\frac{\\omega}{M}\\zeta_{\\star}^{2}+\\frac{9}{4}\\eta L_{\\operatorname*{max}}\\gamma^{2}n\\left(\\sigma_{\\star}^{2}+(n+1)\\sigma_{\\star}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Recursively unrolling the inequality and using $\\sum_{t=0}^{+\\infty}\\left(1-\\frac{\\eta\\mu}{2}\\right)^{t}\\leq\\frac{2}{\\mu\\eta}$ , we get the result. ", "page_idx": 54}, {"type": "text", "text": "Corollary 11. Let the assumptions of Theorem $E.l$ hold, $\\gamma=\\eta/n$ and ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}\\left\\{\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)},\\sqrt{\\frac{\\varepsilon\\mu n}{9L_{\\operatorname*{max}}}}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)^{-1/2},\\frac{\\varepsilon\\mu M}{24\\omega\\zeta_{\\star}^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Then, Q-NASTYA finds a solution with accuracy $\\varepsilon>0$ after the following number of communication rounds: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\frac{\\omega}{M}\\frac{\\zeta_{\\star}^{2}}{\\varepsilon\\mu^{3}}+\\sqrt{\\frac{L_{\\mathrm{max}}}{\\varepsilon\\mu^{3}}}\\sqrt{\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}/n}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "$I f\\gamma\\rightarrow0$ one can choose $\\begin{array}{r}{\\eta=\\operatorname*{min}\\left\\{\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)},\\frac{\\varepsilon\\mu M}{24\\omega\\zeta_{\\star}^{2}}\\right\\}}\\end{array}$ such that the above complexity bound improvesto ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\frac{\\omega}{M}\\frac{\\zeta_{\\star}^{2}}{\\varepsilon\\mu^{3}}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. Theorem E.1 implies ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert x_{T}-x_{\\star}\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)^{T}\\Vert x_{0}-x_{\\star}\\Vert^{2}+\\frac{9}{2}\\frac{\\gamma^{2}n L_{\\operatorname*{max}}}{\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)+8\\frac{\\eta\\omega}{\\mu M}\\zeta_{\\star}^{2}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "To estimate the number of communication rounds required to find a solution with accuracy $\\varepsilon>0$ ,we need to upper bound each term from the right-hand side by $\\varepsilon/3$ . Thus, we get additional conditions on $\\eta$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{9}{2}\\frac{\\eta^{2}L_{\\mathrm{max}}}{n\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)<\\frac{\\varepsilon}{3},\\quad8\\frac{\\eta\\omega}{\\mu M}\\zeta_{\\star}^{2}<\\frac{\\varepsilon}{3}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and also the upper bound on the number of communication rounds $T$ ", "page_idx": 54}, {"type": "equation", "text": "$$\nT=\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\eta\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Substituting (31) in the previous equation, we get the first part of the result. When $\\gamma\\to0$ theproof follows similar steps. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "G Missing Proofs for DIANA-NASTYA ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Lemma G.1. Under Assumptions 1, 2, 3, the iterates produced by DIANA-NASTYA satisfy ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle-\\mathbb{E}_{\\mathbb{Q}}\\left[\\frac{1}{M}\\sum_{m=1}^{M}\\langle\\hat{g}_{t,m}-h^{\\star},x_{t}-x_{\\star}\\rangle\\right]}&{\\le}&{\\displaystyle-\\frac{\\mu}{4}\\|x_{t}-x_{\\star}\\|^{2}-\\frac{1}{2}\\left(f(x_{t})-f(x_{\\star})\\right)}\\\\ &&{\\displaystyle-\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}D_{f_{m}^{\\pi_{m}^{i}}}(x_{\\star},x_{t,m}^{i})}\\\\ &&{\\displaystyle+\\frac{L_{\\operatorname*{max}}}{2M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\|x_{t}-x_{t,m}^{i}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $h^{\\star}=\\nabla f(x_{\\star})$ ", "page_idx": 55}, {"type": "text", "text": "Proof. Using that $\\mathbb{E}_{\\mathcal{Q}}\\left[\\hat{g}_{t,m}\\right]=g_{t,m}$ and definition of $h^{\\star}$ , we get ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle-\\mathbb{E}_{\\boldsymbol{Q}}\\left[\\frac{1}{M}\\sum_{m=1}^{M}\\langle\\hat{g}_{t,m}-h^{\\star},x_{t}-x_{\\star}\\rangle\\right]}&{=}&{\\displaystyle-\\frac{1}{M}\\sum_{m=1}^{M}\\langle g_{t,m}-h^{\\star},x_{t}-x_{\\star}\\rangle}\\\\ &{=}&{\\displaystyle-\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\langle\\nabla f_{m}^{\\pi_{m}^{i}}(x_{t,m}^{i})-\\nabla f_{m}^{\\pi_{m}^{i}}(x_{\\star}),x_{t}-x_{\\star}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Next, three-point identity and $L_{\\mathrm{max}}$ -smoothness of each function $f_{m}^{i}$ imply ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle2\\left[\\frac{1}{M}\\sum_{m=1}^{M}\\langle\\hat{g}_{t,m}-h^{\\star},x_{t}-x_{\\star}\\rangle\\right]}&{=}&{\\displaystyle-\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left(D_{f_{m}^{\\tau_{m}^{i}}}(x_{t},x_{\\star})+D_{f_{m}^{\\tau_{i}^{i}}}(x_{\\star},x_{t,m}^{i})-D_{f_{m}^{\\tau_{i}^{i}}}(x_{t},x_{t})\\right)}\\\\ &{\\le}&{\\displaystyle-D_{f}(x_{t},x_{\\star})-\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}D_{f_{m}^{\\tau_{m}^{i}}}(x_{\\star},x_{t,m}^{i})}\\\\ &&{\\displaystyle+\\frac{L_{\\operatorname*{max}}}{2M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\|x_{t}-x_{t,m}^{i}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Finally, using $\\mu$ -strong convexity of $f$ , we finish the proof of lemma. ", "page_idx": 55}, {"type": "text", "text": "Lemma G.2. Under Assumptions 1, 2, 3, the iterates produced by DIANA-NASTYA satisfy ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\uptau_{Q}\\left[\\left\\Vert\\hat{\\boldsymbol{g}}_{t}-\\boldsymbol{h}^{\\star}\\right\\Vert^{2}\\right]}&{\\le}&{\\displaystyle\\frac{2L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\omega}{M}\\right)}{M n}\\sum_{m=1}^{M}{\\sum_{i=0}^{n-1}\\left\\Vert\\boldsymbol{x}_{t,m}^{i}-\\boldsymbol{x}_{t}\\right\\Vert^{2}}+8L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)\\left(f(\\boldsymbol{x}_{t})-f(\\boldsymbol{x}_{\\star})\\right)}\\\\ &&{\\displaystyle+\\frac{4\\omega}{M^{2}}\\sum_{m=1}^{M}\\left\\Vert h_{t,m}-h_{m}^{\\star}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. Since $\\begin{array}{r}{g_{t}=\\frac{1}{M}\\sum_{m=1}^{M}g_{t,m}}\\end{array}$ and $\\mathbb{E}\\|\\xi-c\\|^{2}=\\mathbb{E}\\|\\xi-\\mathbb{E}\\xi\\|^{2}+\\mathbb{E}\\|\\mathbb{E}\\xi-c\\|^{2}$ , we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\mathcal{Q}}\\left[\\Vert\\hat{g}_{t}-h^{\\star}\\Vert^{2}\\right]}&{=}&{\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\Vert\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(h_{t,m}+\\mathcal{Q}\\left(g_{t,m}-h_{t,m}\\right)-h_{m}^{\\star}\\right)\\right\\Vert^{2}\\right]}\\\\ &{=}&{\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\Vert\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(h_{t,m}+\\mathcal{Q}\\left(g_{t,m}-h_{t,m}\\right)\\right)-g_{t}\\right\\Vert^{2}\\right]+\\left\\Vert g_{t}-h^{\\star}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Next, independence of $\\mathcal{Q}\\left(g_{t,m}-h_{t,m}\\right)$ \uff0c $m\\in M$ , Assumption 1, and $L_{\\mathrm{max}}$ -smoothness and convexity of each function $f_{m}^{i}$ imply ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\dot{\\varepsilon}_{\\mathcal{Q}}\\left[\\left\\|\\dot{R}_{t}-h^{*}\\right\\|^{2}\\right]}&{\\le}&{\\displaystyle\\frac{\\omega}{M^{2}}\\frac{M}{2}\\left\\|g_{t,m}-h_{t,m}\\right\\|^{2}+\\left\\|g_{t}-h^{*}\\right\\|^{2}}\\\\ &{\\le}&{\\displaystyle\\frac{2\\omega}{M^{2}}\\sum_{i=1}^{M}\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{i,m}^{*}(z_{i,m}^{i})-\\nabla f_{m}(x_{i})\\right\\|^{2}+\\displaystyle\\frac{2\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{i})-h_{t,m}\\|}\\\\ &{}&{\\displaystyle+2\\left\\|g_{t}-\\nabla f(z_{t})\\right\\|^{2}+2\\left\\|\\nabla f(x_{t})-h^{*}\\right\\|^{2}}\\\\ &{\\le}&{\\displaystyle\\frac{2\\omega}{M^{2}}\\sum_{i=2}^{M}\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{*}(z_{i,m}^{i})-\\nabla f_{m}(x_{i})\\right\\|^{2}+\\displaystyle\\frac{2\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{i})-h_{t,m}\\|}\\\\ &{}&{\\displaystyle+2\\left\\|\\frac{1}{M}\\sum_{i=0}^{M}\\left(\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{*}(x_{i,m}^{i})-\\nabla f_{m}(x_{i})\\right)\\right\\|^{2}+2\\left\\|\\nabla f(x_{i})-h^{*}\\right\\|^{2}}\\\\ &{\\le}&{\\displaystyle\\frac{2L_{m}^{2}\\Delta\\left(1+\\frac{\\delta}{M}\\right)}{M n}\\sum_{i=0}^{M}\\sum_{i=1}^{n-1}\\left\\|x_{i,m}^{i}-x_{i}\\right\\|^{2}+\\displaystyle\\frac{2\\omega}{M^{2}}\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{i})-h_{t,m}\\|^{2}}\\\\ &{}&{\\displaystyle+2\\|\\nabla f(x_{i})-h^{*}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Using $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}$ , we get ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\hat{y}_{t}-h^{*}\\right\\|^{2}\\right]}&{\\leq\\frac{2L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\nu_{t}}{L^{2}}\\right)}{M n}\\displaystyle\\sum_{m=1}^{M}\\sum_{n=1}^{n-1}\\left\\|x_{t,n}^{t}-x_{t}\\right\\|^{2}+\\frac{2\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{t})-h_{t,m}\\|^{2}}\\\\ &{\\quad+4L_{\\operatorname*{max}}(f(x_{t})-f(x_{s}))}\\\\ &{\\leq\\ \\ \\frac{2L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\nu_{t}}{L^{2}}\\right)}{M n}\\displaystyle\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\|x_{t,n}^{t}-x_{t}\\|^{2}+\\frac{4\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\|\\nabla f_{m}(x_{t})-h_{m}^{*}\\|^{2}}\\\\ &{\\quad+\\frac{4\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{*}\\|^{2}+4L_{\\operatorname*{max}}(f(x_{t})-f(x_{s}))}\\\\ &{\\leq\\ \\frac{2L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\nu_{t}}{L^{2}}\\right)}{M n}\\displaystyle\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\|x_{t,n}^{t}-x_{t}\\|^{2}+\\frac{8L_{\\operatorname*{max}}\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}D_{f_{m}}(x_{t},x_{i})}\\\\ &{\\quad+\\frac{4\\omega}{M^{2}}\\displaystyle\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{*}\\|^{2}+4L_{\\operatorname*{max}}(f(x_{t})-f(x_{s})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Lemma G.3. UnderAsumptions 1, 2, $^3$ and $\\begin{array}{r}{\\alpha\\leq\\frac{1}{1+\\omega}}\\end{array}$ the iterates produced by DIANA-NASTYA satisfy ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|h_{t+1,m}-h_{m}^{\\star}\\|^{2}\\right]}&{\\leq}&{\\displaystyle\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{\\star}\\|^{2}+\\displaystyle\\frac{2\\alpha L_{\\operatorname*{max}}^{2}}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\|x_{t,m}^{i}-x_{t}\\|^{2}}\\\\ &&{\\displaystyle+4\\alpha L_{\\operatorname*{max}}\\left(f(x_{t})-f(x_{\\star})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. Taking expectation w.r.t. $\\mathcal{Q}$ and using Assumption 1, we obtain ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{n=1}^{M}\\mathbb{E}_{Q}\\left[||h_{t+1,m}-h_{m}^{*}||^{2}\\right]}&{=}&{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{Q}\\left[||h_{t,m}+\\alpha\\mathcal{Q}(g_{t,m}-h_{t,m})-h_{m}^{*}||^{2}\\right]}\\\\ &{\\le}&{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(||h_{t,m}-h_{m}^{*}||^{2}+2\\alpha\\mathbb{E}_{Q}\\left[\\langle\\mathcal{Q}(g_{t,m}-h_{t,m}),h_{t,m}-h_{m}^{*}\\rangle\\right]\\right)}\\\\ &&{\\displaystyle+\\alpha^{2}\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{Q}\\left[||\\mathcal{Q}(g_{t,m}-h_{t,m})||^{2}\\right]}\\\\ &{\\le}&{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(||h_{t,m}-h_{m}^{*}||^{2}+2\\alpha\\left<g_{t,m}-h_{t,m},h_{t,m}-h_{m}^{*}\\right>\\right)}\\\\ &&{\\displaystyle+\\alpha^{2}(1+\\omega)\\frac{1}{M}\\sum_{m=1}^{M}||g_{t,m}-h_{t,m}||^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Using $\\begin{array}{r}{\\alpha\\leq\\frac{1}{1+\\omega}}\\end{array}$ , we get ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{Q}\\left[||h_{t+1,m}-h_{m}^{\\star}||^{2}\\right]}&{\\displaystyle\\leq}&{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(||h_{t,m}-h_{m}^{\\star}||^{2}+\\alpha\\left<g_{t,m}-h_{t,m},h_{t,m}+g_{t,m}-2h_{m}^{\\star}\\right|^{2}\\right.}\\\\ &{\\displaystyle\\leq}&{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(||h_{t,m}-h_{m}^{\\star}||^{2}+\\alpha||g_{t,m}-h_{m}^{\\star}||^{2}-\\alpha||h_{t,m}-h_{m}^{\\star}||^{2}\\right)}\\\\ &{\\displaystyle\\leq}&{\\displaystyle\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}||h_{t,m}-h_{m}^{\\star}||^{2}+\\displaystyle\\frac{\\alpha}{M}\\sum_{m=1}^{M}||g_{t,m}-h_{m}^{\\star}||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Finally, $L_{\\mathrm{max}}$ -smoothness and convexity of $f_{m}$ imply ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{\\sigma}\\big[\\|h_{t+1,m}-h_{m}^{*}\\|^{2}\\big]\\,\\le\\,}&{\\displaystyle\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{*}\\|^{2}}\\\\ &{+\\displaystyle\\frac{2\\alpha}{M}\\sum_{m=1}^{M}\\big(\\|g_{t,m}-\\nabla f_{m}(x_{t})\\|^{2}+\\|\\nabla f_{m}(x_{t})-h_{m}^{*}\\|^{2}\\big)}\\\\ {\\le\\,}&{\\displaystyle\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{*}\\|^{2}+\\displaystyle\\frac{4L_{\\operatorname*{max}}\\alpha}{M}\\sum_{m=1}^{M}D_{f_{m}}(x_{t},x_{t})}\\\\ &{+\\displaystyle\\frac{2\\alpha}{M}\\sum_{m=1}^{M}\\bigg\\|\\frac{1}{h_{t}}\\sum_{=0}^{n-1}(\\nabla_{f_{m}}^{\\alpha_{*}^{*}}(x_{t}^{*})-\\nabla f_{m}^{*}(x_{t}))\\bigg\\|^{2}}\\\\ {\\le\\,}&{\\displaystyle\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{*}\\|^{2}+4L_{\\operatorname*{max}}\\alpha\\,(f(x_{t})-f(x_{*}))}\\\\ &{+\\displaystyle\\frac{2L_{\\operatorname*{max}}^{2}\\alpha}{M}\\sum_{m=1}^{M}\\sum_{k=1}^{n-1}\\big\\|x_{t,m}^{*}-x_{t}\\big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Theorem G.1. Let Assumptions 1, 2, 3 hold and stepsizes $\\gamma,\\,\\eta,\\alpha$ satisfy ", "page_idx": 57}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{16L_{\\operatorname*{max}}n},\\quad0<\\eta\\leq\\operatorname*{min}\\left\\lbrace\\frac{\\alpha}{2\\mu},\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{9\\omega}{M}\\right)}\\right\\rbrace,\\quad\\alpha\\leq\\frac{1}{1+\\omega}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then,for all $T\\geq0$ the iterates produced by DIANA-NASTYA satisfy ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)^{T}\\Psi_{0}+\\frac{9}{2}\\frac{\\gamma^{2}n L}{\\mu}\\left(\\sigma_{\\star}^{2}+(n+1)\\zeta_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof. We have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\|x_{t+1}-x_{\\star}\\|^{2}}&{=}&{\\|x_{t}-\\eta\\hat{g}_{t}-x_{\\star}+\\eta h^{\\star}\\|^{2}}\\\\ &{=}&{\\|x_{t}-x_{\\star}\\|^{2}-2\\eta\\langle\\hat{g}_{t}-h^{\\star},x_{t}-x_{\\star}\\rangle+\\eta^{2}\\|\\hat{g}_{t}-h^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Taking expectation w.r.t. $\\mathcal{Q}$ and using Lemma G.1, we obtain ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\mathbb{5}_{\\mathcal{Q}}\\left[\\|x_{t+1}-x_{*}\\|^{2}\\right]}&{=}&{\\|x_{t}-x_{*}\\|^{2}-2\\eta\\mathbb{E}_{\\mathcal{Q}}\\left[\\langle\\hat{g}_{t}-h^{\\star},x_{t}-x_{\\star}\\rangle\\right]+\\eta^{2}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|\\hat{g}_{t}-h^{\\star}\\|^{2}\\right]}\\\\ &{\\le}&{\\left(1-\\displaystyle\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x_{*}\\|^{2}-\\eta(f(x_{t})-f(x_{\\star}))-\\displaystyle\\frac{2\\eta}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}D_{f_{m}^{\\eta_{m}^{i}}}(x_{\\star},x_{t,\\eta}^{i})}\\\\ &{}&{\\displaystyle+\\frac{L_{\\operatorname*{max}\\eta}}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\|x_{t,m}^{i}-x_{t}\\|^{2}+\\eta^{2}\\mathbb{E}_{\\mathcal{Q}}\\left[\\|\\hat{g}_{t}-h^{\\star}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Next, Lemma G.2 implies ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{\\varepsilon}_{\\mathrm{\\tiny{Q}}}\\left[\\left|x_{t+1}-x_{*}\\right|^{2}\\right]}&{\\le}&{\\left(1-\\frac{\\eta\\mu}{2}\\right)\\left\\|x_{t}-x_{*}\\right\\|^{2}-\\eta\\left(f(x_{t})-f(x_{*})\\right)-\\frac{2\\eta}{M n}\\sum_{m=1}^{M}D_{f=0}^{\\frac{x_{t}}{2}-1}D_{f=0}^{\\frac{x_{t}}{2}}\\left(x_{*},x_{*}^{\\bot}\\right.}\\\\ &{\\left.\\quad+\\frac{I_{m a x}\\eta}{M n}\\sum_{m=1}^{M}\\|x_{t,m}^{\\bot}-x_{t}\\|^{2}+\\frac{2\\eta^{2}L_{m a x}^{2}}{M n}(1+\\frac{\\dot{x}_{t}}{M})\\sum_{m=1}^{M}\\|x_{t,m}^{\\bot}-x_{t}^{\\bot}\\|^{2}\\right.}\\\\ &{\\quad\\left.+\\eta^{2}\\left(8L_{m a x}\\left(1+\\frac{\\dot{x}_{t}}{M}\\right)\\left(f(x_{t})-f(x_{*})\\right)+\\frac{4\\eta}{M^{2}}\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{*}\\|^{2}\\right)}\\\\ &{\\le}&{\\left(1-\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x_{*}\\|^{2}-\\eta\\left(1-8\\eta L_{m a x}\\left(1+\\frac{\\dot{x}_{t}}{M}\\right)\\right)\\left(f(x_{*})-f(x_{*})\\right)}\\\\ &{\\quad+L_{m a x}\\eta\\left(1+2\\eta L_{m a x}\\left(1+\\frac{\\dot{\\omega}}{M}\\right)\\right)\\frac{1}{M n}\\sum_{m=1}^{M}\\|x_{t,m}^{\\bot}-x_{t}\\|^{2}}\\\\ &{\\quad\\left.\\quad-\\frac{2\\eta}{M n}\\sum_{m=1}^{M-1}D_{f=0}^{\\frac{x_{t}}{2}}(x_{*},x_{t,m}^{\\bot})+\\frac{4\\eta^{2}\\omega}{M^{2}}\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Using (6) and Lemma G.3, we get ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{S}}\\left[\\mathbf{U}_{\\mathrm{Fi}}+\\mathbf{I}\\right]}\\\\ {\\leq}&{\\left(1-\\frac{\\eta_{0}}{2}\\right)\\Big\\Vert\\tau_{i}-\\tau_{*}\\mathbf{I}\\Big\\Vert^{2}-\\eta\\left(1-8\\eta I_{\\mathrm{om}}\\left(1+\\frac{\\lambda_{\\mathrm{om}}}{M}\\right)\\right)\\left(f(\\tau_{x})-f(x_{c})\\right)}\\\\ &{+I_{\\mathrm{om}}\\eta\\left(1+2\\eta I_{\\mathrm{om}}\\left(1+\\frac{\\lambda_{\\mathrm{om}}}{M}\\right)\\right)\\frac{1}{M}\\frac{M^{-1}}{\\displaystyle{m}_{\\mathrm{Sin}}^{-1}\\left(1-\\omega\\right)}\\Vert\\tau_{i}^{2}}\\\\ &{-\\frac{2\\eta}{M}\\sum_{m=1+\\infty}^{M}D_{m,\\tau_{m}^{\\prime}}^{-1}(x_{c},x_{i,m}^{*})+\\frac{4\\eta^{2}\\omega^{2}}{M^{2}}\\frac{\\mathbf{V}}{\\displaystyle{m}_{\\mathrm{Sin}}^{-1}\\left[1+m,-k_{\\mathrm{om}}^{*}\\right]^{2}}}\\\\ &{+\\sigma_{2}^{2}\\left(\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}\\Vert h_{i,m}-h_{m}^{*}\\Vert^{2}+\\frac{2\\alpha{T}_{\\mathrm{om}}^{2}}{M}\\frac{M^{-1}}{\\displaystyle{m}_{\\mathrm{Sin}}^{-1}\\left[1+m,-k_{\\mathrm{om}}^{*}\\right]^{2}}+4a L_{\\mathrm{max}}\\left(f(x_{i})-f(\\alpha)\\right)\\right)}\\\\ {\\leq}&{\\left(1-\\frac{\\eta(m)}{M}\\right)\\Vert\\tau_{i}-x_{i}\\Vert^{2}+\\eta^{2}\\left(\\epsilon(1-\\alpha)+\\frac{4\\eta}{M}\\right)\\frac{M}{M}\\sum_{m=1}^{M}\\Vert h_{i,m}-h_{m}^{*}\\Vert^{2}}\\\\ &{-\\eta\\left(1-8\\eta I_{\\mathrm{om}}\\left(1+\\frac{\\lambda_{\\mathrm{om}}}{M}\\right)-4\\eta\\mathrm{erfom}\\right)\\Vert\\left(f(x_{i})-f(x_{i})\\right)}\\\\ &{+L\\eta\\left(1+2\\eta I_{\\mathrm{om}}\\left(1+\\frac{\\lambda_{\\mathrm{o m\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Taking the full expectation, we derive ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Psi_{t+1}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{\\eta\\mu}{2}\\right)\\mathbb{E}\\left[\\|x_{t}-x_{\\star}\\|^{2}\\right]+\\eta^{2}\\left(c(1-\\alpha)+\\frac{4\\omega}{M}\\right)\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\|h_{t,m}-h_{m}^{\\star}\\|^{2}\\right]}\\\\ &&{\\displaystyle-\\eta\\left(1-8\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)-4\\alpha\\eta c L\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &&{\\displaystyle+L_{\\operatorname*{max}}\\eta\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)+2\\alpha\\eta c L_{\\operatorname*{max}}\\right)\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\mathbb{E}\\left[\\|x_{t,m}^{i}-x_{t}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Using Lemma F.3, we get ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Psi_{t+1}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{\\eta\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{t}-x_{\\star}\\right\\Vert^{2}\\right]+\\eta^{2}\\left(c(1-\\alpha)+\\frac{4\\omega}{M}\\right)\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\left\\Vert h_{t,m}-h_{m}^{\\star}\\right\\Vert^{2}\\right]}\\\\ &&{\\displaystyle-\\eta\\left(1-8\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)-4\\alpha\\eta c L_{\\operatorname*{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &&{\\displaystyle+8\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\eta\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)+2\\alpha\\eta c L_{\\operatorname*{max}}\\right)\\mathbb{E}\\left[f(x_{t})-f(x_{\\star})\\right]}\\\\ &&{\\displaystyle+2\\gamma^{2}n L_{\\operatorname*{max}}\\eta\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{M}\\right)+2\\alpha\\eta c L_{\\operatorname*{max}}\\right)\\left(\\sigma_{\\star}^{2}+(n+1)\\zeta_{\\star}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "In view of (29), we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Psi_{t+1}\\right]}&{\\leq}&{\\displaystyle\\left(1-\\frac{\\eta\\mu}{2}\\right)\\mathbb{E}\\left[\\|x_{t}-x_{\\star}\\|^{2}\\right]+\\left(1-\\frac{\\alpha}{2}\\right)\\frac{c\\eta^{2}}{M}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\|h_{t,m}-h_{m}^{\\star}\\|^{2}\\right]}\\\\ &&{\\displaystyle+\\frac{9}{4}\\gamma^{2}n L_{\\operatorname*{max}}\\eta\\left(\\sigma_{\\star}^{2}+(n+1)\\zeta_{\\star}^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Using defnition of Lyapunovfuntion and using $\\sum_{t=0}^{+\\infty}\\left(1-\\frac{\\eta\\mu}{2}\\right)^{t}\\leq\\frac{2}{\\mu\\eta}$ we get the result. ", "page_idx": 59}, {"type": "text", "text": "Corollary 12. Let the assumptions of Theorem $E.2$ hold, $\\gamma=\\eta/n$ $\\begin{array}{r}{\\alpha=\\frac{1}{1+\\omega}}\\end{array}$ and ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}\\left\\{\\frac{\\alpha}{2\\mu},\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{9\\omega}{M}\\right)},\\sqrt{\\frac{\\varepsilon\\mu n}{9L_{\\operatorname*{max}}}}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)^{-1/2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Then, DIANA-NASTYA finds a solution with accuracy $\\varepsilon>0$ after the following number of communicationrounds: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\omega+\\frac{L_{\\operatorname*{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)+\\sqrt{\\frac{L_{\\operatorname*{max}}}{\\varepsilon\\mu^{3}}}\\sqrt{\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}/n}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "$I f\\gamma\\rightarrow0$ onecanchoose $\\begin{array}{r}{\\eta=\\operatorname*{min}\\left\\{\\frac{\\alpha}{2\\mu},\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{9\\omega}{M}\\right)}\\right\\}}\\end{array}$ such thatthenumberofcommunication rounds $T$ tofind solutionwithaccuracy $\\varepsilon>0$ is ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{O}}\\left(\\omega+\\frac{L_{\\mathrm{max}}}{\\mu}\\left(1+\\frac{\\omega}{M}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof. Theorem E.2 implies ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)^{T}\\Psi_{0}+\\frac{9}{2}\\frac{\\gamma^{2}n L_{\\operatorname*{max}}}{\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "To estimate the number of communication rounds required to find a solution with accuracy $\\varepsilon>0$ ,we need to upper bound each term from the right-hand side by $\\frac{\\varepsilon}{2}$ . Thus, we get an additional restriction On $\\eta$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\frac{9}{2}\\frac{\\eta^{2}L_{\\mathrm{max}}}{n\\mu}\\left((n+1)\\zeta_{\\star}^{2}+\\sigma_{\\star}^{2}\\right)<\\frac{\\varepsilon}{2},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "and also the upper bound on the number of communication rounds $T$ ", "page_idx": 59}, {"type": "equation", "text": "$$\nT=\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\eta\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Substituting (31) in the previous equation, we get the first part of the result. When $\\gamma\\to0$ theproof follows similar steps. \u53e3 ", "page_idx": 59}, {"type": "text", "text": "H  Alternative Analysis of $\\Omega\\mathrm{.}$ NASTYA ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "In this analysis, we will use additional sequence: ", "page_idx": 60}, {"type": "equation", "text": "$$\nx_{\\star,m}^{i}=x_{\\star}-\\gamma\\sum_{j=0}^{i-1}\\nabla f_{m}(x_{\\star}).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Theorem H.1. Let Assumptions 1, 3, 4 hold. Moreover, we assume that $\\begin{array}{r}{(1-\\gamma\\mu)^{n}\\leq\\frac{9\\left/10-1\\right/C}{1+1/C}=}\\end{array}$ $\\widehat{C}<1$ for some numerical constant $C>1$ Alsolet n\u2264 0+1 and / \u2264 L Then, forall $T\\geq0$ the iterates produced by Q-NASTYA satisfy ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\leq\\operatorname*{max}\\left(1-\\frac{\\beta}{10},1-\\frac{\\alpha}{2}\\right)\\Psi_{t}+\\frac{2}{\\mu}\\beta\\gamma^{2}\\hat{\\sigma}_{r a d}^{2}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert x_{T}-x_{\\star}\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\beta}{10}\\right)\\Vert x_{t}-x_{\\star}\\Vert^{2}+\\frac{4}{\\mu}\\beta\\gamma^{2}\\hat{\\sigma}_{\\mathrm{rad}}^{2}+3\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\hat{\\Delta}_{\\star},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{\\Delta}_{\\star}=\\frac{1}{M}\\sum_{m=1}^{M}\\|x_{\\star,m}^{n}-x_{\\star}\\|^{2}}\\end{array}$ and $\\hat{\\sigma}_{\\mathrm{rad}}^{2}\\leq L_{\\mathrm{max}}\\left(\\zeta_{\\star}^{2}+n\\sigma_{\\star}^{2}/4\\right)$ ", "page_idx": 60}, {"type": "text", "text": "Proof. The update rule for one epoch can be rewritten as ", "page_idx": 60}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}-\\eta\\frac{1}{M}\\sum_{M}^{m=1}Q\\left(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Using this, we derive ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|x_{t+1}-x_{*}||^{2}=\\left\\|x_{t}-\\eta\\frac{1}{M}\\sum_{m=1}^{M}Q\\left(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right)-x_{*}\\right\\|^{2}}\\\\ {\\displaystyle=\\|x_{t}-x_{*}\\|^{2}-2\\eta\\left\\langle x_{t}-x_{*},\\frac{1}{M}\\sum_{m=1}^{M}Q\\left(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right)\\right\\rangle}\\\\ {\\displaystyle+\\left.\\eta^{2}\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}Q\\left(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Taking conditional expectation w.r.t. the randomness comming from compression, we get ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q}\\|x_{t+1}-x_{*}\\|^{2}=\\|x_{t}-x_{*}\\|^{2}-2\\eta\\left\\langle x_{t}-x_{*},\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right)\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad+\\eta^{2}\\mathbb{E}_{Q}\\left\\|\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}Q\\left(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Next, we use the definition of quantization operator and independence of $\\begin{array}{r}{Q\\Big(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\Big),m\\in[M]}\\end{array}$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q}\\|x_{t+1}-x_{*}\\|^{2}\\leq\\|x_{t}-x_{*}\\|^{2}-2\\eta\\left<x_{t}-x_{*},\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left(\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right)\\right>}\\\\ &{\\qquad\\qquad\\qquad+\\eta^{2}\\left(\\frac{\\omega}{M}\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left\\|\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right\\|^{2}+\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}\\right\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Since $\\begin{array}{r}{\\beta=\\frac{\\eta}{\\gamma n}}\\end{array}$ , we obtain ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\sigma}\\Vert\\mathbf{r}_{t+1}-\\mathbf{z}_{t}\\Vert^{2}\\leq\\Vert\\mathbf{r}_{t}-\\mathbf{z}_{t}\\Vert^{2}-2\\delta\\left(\\mathcal{A}_{\\mathbf{z}}-\\kappa_{t},\\frac{1}{M}\\sum_{i=0}^{M}\\left(\\alpha_{t}-x_{t,m}^{\\star}\\right)\\right)}\\\\ &{\\qquad+\\beta\\frac{\\delta^{2}}{M}\\frac{1}{M}\\frac{1}{m_{t}-1}\\Vert\\mathbf{r}_{t}-\\mathbf{z}_{t,m}^{\\star}\\Vert^{2}+\\beta^{2}\\left\\Vert\\frac{1}{M}\\frac{1}{m_{t}-1}\\left(\\kappa_{t}-x_{t,m}^{\\star}\\right)\\right\\Vert^{2}}\\\\ &{=\\Vert\\mathbf{r}_{t}-\\mathbf{z}_{t}\\Vert^{2}+2\\delta\\left(\\kappa_{t}-x_{t,m}^{\\star}\\right)\\frac{1}{M}\\sum_{i=0}^{M}\\left(x_{t}^{\\star}-x_{t}\\right)}\\\\ &{\\qquad+\\beta^{2}\\frac{1}{M}\\frac{1}{M}\\frac{1}{M}\\Vert\\mathbf{r}_{t}-x_{t,m}^{\\star}\\Vert^{2}+\\beta^{2}\\left\\Vert\\frac{1}{M}\\frac{1}{m_{t}-1}\\left(x_{t}^{\\star}-x_{t,m}^{\\star}\\right)\\right\\Vert^{2}}\\\\ &{=\\left\\Vert\\mathbf{r}_{t}-\\mathbf{z}_{t}+\\beta\\left(\\frac{1}{M}\\frac{1}{\\sum_{i=0}^{M}\\left(x_{t}^{\\star}-x_{t,m}^{\\star}\\right)}\\right)\\right\\Vert^{2}+\\beta^{2}\\frac{1}{M}\\frac{1}{M}\\frac{1}{\\sum_{i=0}^{M}\\left(1-x_{t,m}^{\\star}\\right)}\\Vert^{2}}\\\\ &{=\\left\\Vert(1-\\beta)(\\mathbf{r}_{t}-x_{t,m})+\\beta\\left(\\frac{1}{M}\\frac{1}{\\sum_{i=0}^{M}\\left(x_{t,m}^{\\star}-x_{t}\\right)}\\right\\Vert^{2}}\\\\ &{\\qquad+\\beta^{2}\\frac{1}{M}\\frac{1}{M}\\sum_{i=1}^{M}\\left(\\mathbf{r}_{t}-x_{t,m}^{\\star}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Using the condition that $\\begin{array}{r}{x_{*}=\\frac{1}{M}\\sum_{m=1}^{M}x_{*,m}^{n}}\\end{array}$ we have: ", "page_idx": 61}, {"type": "equation", "text": "$$\n|x_{t+1}-x_{*}\\|^{2}\\leq\\bigg\\|(1-\\beta)(x_{t}-x_{*})+\\beta\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\left(x_{t,m}^{n}-x_{*,m}^{n}\\right)\\right)\\bigg\\|^{2}+\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\sum_{m=1}^{M}\\big\\|x_{t}-x_{t,m}^{n}\\big\\|\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Convexity of squared norm and Jensen's inequality imply ", "page_idx": 61}, {"type": "equation", "text": "$$\n|x_{t+1}-x_{*}\\|^{2}\\leq(1-\\beta)\\|x_{t}-x_{*}\\|^{2}+\\beta\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\left(x_{t,m}^{n}-x_{*,m}^{n}\\right)\\right\\|^{2}+\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\sum_{m=1}^{M}\\left\\|x_{t}-x_{t,m}^{n}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Next, from Young's inequality we get ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{Q}\\|x_{t+1}-x_{*}\\|^{2}\\leq(1-\\beta)\\|x_{t}-x_{*}\\|^{2}+\\beta\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\left(x_{t,m}^{n}-x_{*,m}^{n}\\right)\\right\\|^{2}+3\\beta^{2}\\frac{\\omega}{M}\\|x_{t}-x_{*}\\|^{2}}\\\\ {\\displaystyle+3\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\sum_{m=1}^{M}\\|x_{t,m}^{n}-x_{*,m}^{n}\\|^{2}+3\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\sum_{m=1}^{M}\\|x_{*,m}^{n}-x_{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Theorem 4 from [Mishchenko et al., 2021] gives ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\|x_{t,m}^{n}-x_{*,m}^{n}\\|^{2}\\right]\\leq(1-\\gamma\\mu)^{n}\\left[\\|x_{t}-x_{\\star}\\|^{2}\\right]+2\\gamma^{3}\\hat{\\sigma}_{\\mathrm{rad}}^{2}\\left(\\displaystyle\\sum_{j=0}^{n-1}(1-\\gamma\\mu)^{j}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=(1-\\gamma\\mu)^{n}\\left[\\|x_{t}-x_{\\star}\\|^{2}\\right]+2\\gamma^{2}\\hat{\\sigma}_{\\mathrm{rad}}^{2}\\displaystyle\\frac{1}{\\gamma\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "It leads to ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|x_{t+1}-x_{*}\\|^{2}\\leq(1-\\beta)\\|x_{t}-x_{*}\\|^{2}+\\beta\\left((1-\\gamma\\mu)^{n}\\left[\\|x_{t}-x_{*}\\|^{2}\\right]+2\\gamma^{3}\\bar{\\sigma}_{\\operatorname*{rad}}^{2}\\frac{1}{\\gamma\\mu}\\right)}\\\\ &{\\qquad\\qquad\\qquad+3\\beta^{2}\\frac{\\omega}{M}\\|x_{t}-x_{*}\\|^{2}+3\\beta^{2}\\frac{\\omega}{M}\\left((1-\\gamma\\mu)^{n}\\left[\\|x_{t}-x_{*}\\|^{2}\\right]+2\\gamma^{3}\\bar{\\sigma}_{\\operatorname*{rad}}^{2}\\frac{1}{\\gamma\\mu}\\right)}\\\\ &{\\qquad\\qquad\\qquad+3\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\displaystyle\\frac{M}{m=1}\\|x_{*,m}^{n}-x_{*}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\left(1-\\beta+\\beta(1-\\gamma\\mu)^{n}+3\\beta^{2}\\frac{\\omega}{M}+3\\beta^{2}\\frac{\\omega}{M}(1-\\gamma\\mu)^{n}\\right)\\|x_{t}-x_{*}\\|^{2}}\\\\ &{\\qquad\\qquad+2\\beta\\gamma^{3}\\hat{\\sigma}_{\\operatorname*{rad}}^{2}\\frac{1}{\\gamma\\mu}\\left(1+3\\beta\\frac{\\omega}{M}\\right)+3\\beta^{2}\\frac{\\omega}{M}\\displaystyle\\frac{1}{M}\\displaystyle\\frac{M}{m=1}\\|x_{*,m}^{n}-x_{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Using $\\begin{array}{r}{(1-\\gamma\\mu)^{n}\\leq\\frac{9/10-1/C}{1+1/C}}\\end{array}$ , we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(1-\\gamma\\mu)^{n}\\leq\\frac{9/10-1/C}{1+1/C}}\\\\ {\\displaystyle(1-\\gamma\\mu)^{n}\\left(1+\\frac{1}{C}\\right)\\leq\\frac{9}{10}-\\frac{1}{C}}\\\\ {\\displaystyle-\\,\\frac{9}{10}\\beta+\\beta(1-\\gamma\\mu)^{n}+\\frac{\\beta}{C}+\\frac{\\beta}{C}(1-\\gamma\\mu)^{n}\\leq0}\\\\ {\\displaystyle1-\\beta+\\beta(1-\\gamma\\mu)^{n}+\\frac{\\beta}{C}+\\frac{\\beta}{C}(1-\\gamma\\mu)^{n}\\leq1-\\frac{\\beta}{10}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Next, applying $\\begin{array}{r}{\\beta\\le\\frac{1}{1+3C\\frac{\\omega}{M}}}\\end{array}$ , we derive ", "page_idx": 62}, {"type": "equation", "text": "$$\n1-\\beta+\\beta(1-\\gamma\\mu)^{n}+3\\beta^{2}\\frac{\\omega}{M}+3\\beta^{2}\\frac{\\omega}{M}(1-\\gamma\\mu)^{n}\\leq1-\\frac{\\beta}{10}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Finally, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\|x_{t+1}-x_{\\star}\\|^{2}\\leq\\left(1-\\frac{\\beta}{10}\\right)\\|x_{t}-x_{*}\\|^{2}+2\\beta\\gamma^{2}\\hat{\\sigma}_{\\mathrm{rad}}^{2}\\frac{1}{\\mu}\\left(1+\\frac{1}{C}\\right)}\\\\ {\\displaystyle+3\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\sum_{m=1}^{M}\\|x_{*,m}^{n}-x_{*}\\|^{2}}\\\\ {\\displaystyle\\leq\\left(1-\\frac{\\beta}{10}\\right)\\|x_{t}-x_{*}\\|^{2}+\\frac{4}{\\mu}\\beta\\gamma^{2}\\hat{\\sigma}_{\\mathrm{rad}}^{2}}\\\\ {\\displaystyle+3\\beta^{2}\\frac{\\omega}{M}\\frac{1}{M}\\sum_{m=1}^{M}\\|x_{*,m}^{n}-x_{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "I Alternative Analysis of DIANA-NASTYA ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Theorem I.1. Let Assumptions 1, 3, 4 hold. Moreover we assume that $\\begin{array}{r}{(1\\!-\\!\\gamma\\mu)^{n}\\leq\\frac{9/10-1/B}{1+^{1}/B}=\\widehat{B}<1}\\end{array}$ for somenumerical constant $B>1$ Also let $\\begin{array}{r}{\\beta=\\frac{\\eta}{\\gamma n}\\leq\\frac{1}{12B\\frac{\\omega}{M}+1}}\\end{array}$ and $\\begin{array}{r}{\\gamma\\leq\\frac{1}{L_{\\operatorname*{max}}}}\\end{array}$ - and also \u03b1 \u2264 w+1\u00b7 Then, for all $T\\geq0$ the iterates produced by DIANA-NASTYA satisfy ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Psi_{T}\\leq\\operatorname*{max}\\left(1-\\frac{\\beta}{10},1-\\frac{\\alpha}{2}\\right)^{T}\\Psi_{0}+\\frac{2}{\\mu\\operatorname*{min}\\bigl(\\frac{\\beta}{10},\\frac{\\alpha}{2}\\bigr)}\\beta\\gamma^{2}\\hat{\\sigma}_{r a d}^{2}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Proof. We start with expanding the square: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|x_{t+1}-x_{*}\\|^{2}=\\|x_{t}-\\eta\\hat{g}_{t}-x_{*}\\|^{2}}\\\\ {\\displaystyle=\\left\\|x_{t}-\\eta\\frac{1}{M}\\sum_{m=1}^{M}\\left(h_{t,m}+Q(g_{t,m}-h_{t,m})\\right)-x_{*}\\right\\|^{2}}\\\\ {\\displaystyle=\\|x_{t}-x_{*}\\|^{2}-2\\eta\\left\\langle\\frac{1}{M}\\sum_{m=1}^{M}\\left(h_{t,m}+Q(g_{t,m}-h_{t,m})\\right),x_{t}-x_{*}\\right\\rangle}\\\\ {\\displaystyle+\\left.\\eta^{2}\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\left(h_{t,m}+Q(g_{t,m}-h_{t,m})\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Taking the expectation w.r.t. $\\mathcal{Q}$ ,we get ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{q}\\left\\lVert u_{+1}-x_{*}\\right\\rVert^{2}=\\left[\\hbar-\\alpha_{*}\\right]^{1/2}-2\\eta\\left(\\frac{1}{\\sqrt{2}}\\sum_{q=0}^{N}\\alpha_{*}x_{*}-x_{*}\\right)}\\\\ &{\\quad+\\eta^{2}\\mathbb{E}_{q}\\left[\\frac{1}{\\sqrt{2}}\\sum_{q=0}^{N}\\left(\\hbar\\alpha_{*}+q(\\beta_{**}-\\hbar\\alpha_{*})\\right)\\right]^{2}}\\\\ &{\\quad-[\\hbar\\alpha_{*}-\\kappa_{*}]^{2}-2\\eta\\left(\\frac{1}{\\sqrt{2}}\\sum_{q=0}^{N}\\alpha_{*}x_{*}-x_{*}\\right)}\\\\ &{\\quad+\\eta^{2}\\alpha_{*}\\left[\\frac{1}{\\sqrt{2}}\\sum_{q=0}^{N}\\left(\\hbar\\alpha_{*}+q(\\beta_{**}-\\hbar\\alpha_{*})-q\\right)\\right]^{2}+\\eta^{2}\\left\\lVert\\frac{1}{\\sqrt{2}}\\sum_{q=1}^{N}\\alpha_{*}\\right\\rVert^{2}}\\\\ &{\\quad\\leq[\\hbar-\\alpha_{*}]^{1/2}-2\\eta\\left(\\frac{1}{\\sqrt{2}}\\sum_{q=0}^{N}\\beta_{*}x_{*}-x_{*}\\right)}\\\\ &{\\quad+\\eta^{2}\\frac{M^{2}}{2}\\sum_{q=0}^{N}\\left[\\imath\\beta_{*}-\\hbar\\alpha_{*}\\right]^{2}+\\eta^{2}\\left[\\frac{M^{2}}{\\hbar\\alpha_{*}}\\alpha_{*}\\right]^{2}}\\\\ &{\\quad\\leq[\\hbar\\alpha_{*}-\\kappa_{*}]^{2}-2\\eta\\left(\\frac{1}{\\sqrt{2}}\\sum_{q=0}^{N}\\beta_{*}-x_{*}\\right)}\\\\ &{\\quad+\\eta^{2}\\frac{M^{2}}{2}\\sum_{q=0}^{N}\\left[\\imath\\left(\\hbar\\alpha_{*}-\\hbar\\alpha_{*}\\right)+\\eta^{2}\\frac{M^{2}}{2}\\sum_{q=0}^{N}\\beta_{*}\\right]}\\\\ &{\\quad+\\eta^{2}\\frac{M^{2}}{2}\\sum_{q=0}^{N}\\left[\\imath\\left(\\hbar\\alpha_{*}-\\hbar\\alpha_{*}\\right)+\\eta^{2}\\frac{M^{2}}{2}\\sum_{q=0}^ \n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Next, using definition of $g_{t,m}$ , we obtain ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{n(n-1)}\\sum_{i=1}^{n}\\mathbb{E}_{1}-x_{i}\\Big[\\mathrm{s}_{i}-x_{i}^{1}-2i\\left(\\frac{1}{\\sqrt{n}}\\frac{M_{i}}{\\sum_{j=1}^{n}}\\frac{x_{j}-x_{j-n}}{n(n-1)},x_{i}-x_{j}\\right)+\\eta\\left|\\frac{1}{\\sqrt{n}}\\frac{M_{j}}{\\sum_{j=1}^{n}}\\frac{x_{j}-x_{j-n}}{n(n-1)}\\right|^{2}\\right.}\\\\ &{\\left.+\\eta\\frac{2}{\\sqrt{n}}\\frac{M_{j}}{\\sum_{j=1}^{n}}\\log_{-n}\\Lambda_{n,j}\\right]+\\eta^{2}\\frac{\\Gamma_{2}\\tilde{\\lambda}_{j}}{\\sum_{j=1}^{n}}\\frac{x_{j}-x_{j}}{n(n-1)}\\sum_{i=1}^{n}-\\Lambda_{n,j}\\right]^{2}}\\\\ &{=[1-x_{i},x_{i}]^{2}+2i\\alpha\\left(\\frac{1}{\\sqrt{n}}\\frac{M_{j}}{\\sum_{j=1}^{n}}(x_{j}-x_{j-n}),x_{i}-x_{j}\\right)+\\alpha^{2}\\left|\\frac{1}{\\sqrt{n}}\\frac{M_{j}}{\\sum_{j=1}^{n}}(x_{j}-x_{j})\\right|^{2}}\\\\ &{+\\eta^{2}\\frac{\\sqrt{n}}{\\sqrt{n}}\\frac{M_{j}}{\\sum_{j=1}^{n}}\\log_{-n}\\Lambda_{n,j}\\right|^{2}+\\eta^{2}\\frac{\\Gamma_{2}\\tilde{\\lambda}_{j}}{\\sum_{j=1}^{n}}\\frac{x_{j}-x_{j-n}}{n(n-1)}\\sum_{i=1}^{n}\\log_{-n}\\Lambda_{n,j}\\right|^{2}}\\\\ &{=\\Big|\\pi\\cdot\\alpha+\\frac{1}{\\sqrt{n}}\\frac{M_{j}}{\\sum_{j=1}^{n}}(x_{j}-x_{j-n})\\Big|^{2}}\\\\ &{+\\eta^{2}\\frac{\\sqrt{n}}{\\sqrt{n}}\\frac{M_{j}}{\\sum_{j=1}^{n}}\\log_{-n}\\Lambda_{n,j}+\\eta^{2}\\frac{\\sqrt{n}}{\\sum_{j=1}^{n}}\\frac{M_{j}}\n$$M ", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Let us consider recursion for control variable: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t+1,m}-h_{*,m}\\|^{2}=\\|h_{t,m}+\\alpha Q(g_{t,m}-h_{t,m})-h_{*,m}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|h_{t,m}-h_{*,m}\\|^{2}+\\alpha\\left<Q(g_{t,m}-h_{t,m}),h_{t,m}-h_{*,m}\\right>+\\alpha^{2}\\|Q(g_{t,m}-h_{t,m})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Taking the expectation w.r.t. $\\mathcal{Q}$ , we have ", "page_idx": 64}, {"type": "text", "text": "\u6b63 $\\begin{array}{r}{\\mathfrak{L}_{Q}\\|h_{t+1,m}-h_{*,m}\\|^{2}\\leq\\|h_{t,m}-h_{*,m}\\|^{2}+2\\alpha\\,\\langle g_{t,m}-h_{t,m},h_{t,m}-h_{*,m}\\rangle+\\alpha^{2}\\,(\\omega+1)\\,\\|g_{t,m}-h_{t,m}\\|^{2}.}\\end{array}$ ml . Using $\\begin{array}{r}{\\alpha\\leq\\frac{1}{\\omega+1}}\\end{array}$ we have ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[\\vert k_{t+1,m}-k_{t+\\frac{m}{2}}\\vert]^{2}\\leq\\vert k_{t,m}-k_{t+\\frac{m}{2}}\\vert^{2}}\\\\ {+2\\alpha(\\mu_{\\mathrm{g},m}-k_{t,m})\\vert^{2}}\\\\ {=\\vert\\|\\mu_{\\mathrm{a},m}-k_{t,m}\\vert^{2}}\\\\ {+2\\alpha(\\mu_{\\mathrm{m}}-k_{t,m})\\vert^{2}}\\\\ {+[\\mu_{\\mathrm{a},m}-k_{t,m}]^{2}}\\\\ {+2\\alpha(\\mu_{\\mathrm{m}}-k_{t,m}-h_{,m})+\\alpha(\\mu_{\\mathrm{m}}-h_{t,m},g_{\\mathrm{c},m}-h_{,m})}\\\\ {=\\vert\\|\\mu_{\\mathrm{m}}-h_{t,m}\\vert^{2}}\\\\ {+(\\mu_{\\mathrm{m}}-h_{t,m})\\vert^{2}}\\\\ {+(\\eta_{\\mathrm{m}}-h_{t,m})\\vert^{2},}\\\\ {+\\left\\vert\\|\\mu_{\\mathrm{m}}-h_{t,m}\\vert^{2}\\right\\vert}\\\\ {+\\alpha(\\phi_{t,m}-h_{t,m}+h_{t,m}-2h_{,m})}\\\\ {=\\vert\\|\\mu_{\\mathrm{m}}-h_{t,m}\\vert^{2}}\\\\ {+\\alpha(\\phi_{t,m}-h_{t,m}-h_{,m}+h_{t,m}-h_{t,m}+h_{t,m}-2h_{,m})}\\\\ {=\\vert\\|\\mu_{\\mathrm{m}}-h_{t,m}\\vert^{2}}\\\\ {+(\\phi_{t,m}-h_{t,m}-1)^{2}(h_{t,m}-h_{,m}),(\\mu_{\\mathrm{m}}-h_{,m})+(h_{t,m}-h_{,m})}\\\\ {+(\\eta_{\\mathrm{m}}-h_{t,m}-h_{,m})-(\\eta_{\\mathrm{m}}-h_{,m})\\vert^{2}-=\\vert h_{t,m}-h_{t,m}\\vert^{2}}\\\\ {=\\vert\\|\\mu_{\\mathrm{m}}-h_{t,m}\\vert^{2}-h_{t,m}\\vert^{2}}\\\\ {-(1-\\alpha)(\\phi_{t,m}-h_{t,m})\\vert^{2}+\\vert\\alpha(\\mu_{\\mathrm{m}}-h_{t,m})\\vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Using this bound we get that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{Q}\\|h_{t+1,m}-h_{*,m}\\|^{2}\\le(1-\\alpha)\\frac{1}{M}\\sum_{m=1}^{M}\\|h_{t,m}-h_{*,m}\\|^{2}+\\alpha\\frac{1}{M}\\sum_{m=1}^{M}\\|g_{t,m}-h_{*,m}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Let us consider Lyapunov function: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\Psi_{t}=\\|x_{t}-x_{*}\\|^{2}+\\frac{4\\omega\\eta^{2}}{\\alpha M}\\frac{1}{M}\\sum_{m=1}^{M}\\|h_{t,m}-h_{*,m}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Using previous bounds and Theorem 4 from [Mishchenko et al., 2021] we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Psi_{t+1}\\leq(1-\\beta)\\|x_{t}-x_{*}\\|^{2}+\\beta\\left((1-\\gamma\\mu)^{n}\\mathbb{E}\\|x_{t}-x_{*}\\|^{2}+\\gamma^{3}\\frac{1}{\\gamma\\mu}\\hat{\\sigma}_{r}^{2}\\alpha d\\right)}\\\\ &{\\qquad+\\eta^{2}\\frac{2\\omega}{M}\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\|g_{t,m}-h_{*,m}\\|^{2}+\\eta^{2}\\frac{2\\omega}{M}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\|h_{t,m}-h_{*,m}\\|^{2}}\\\\ &{\\qquad+(1-\\alpha)\\frac{4\\omega\\eta^{2}}{\\alpha M}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\|h_{t,m}-h_{*,m}\\|^{2}+\\alpha\\frac{4\\omega\\eta^{2}}{\\alpha M}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\|g_{t,m}-h_{*,m}\\|^{2}}\\\\ &{\\qquad\\leq\\left(1-\\frac{\\alpha}{2}\\right)\\frac{4\\omega\\eta^{2}}{\\alpha M}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\|h_{t,m}-h_{*,m}\\|^{2}+\\eta^{2}\\frac{6\\omega}{M}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\|g_{t,m}-h_{*,m}\\|^{2}}\\\\ &{\\qquad+(1-\\beta)\\mathbb{E}\\|x_{t}-x_{*}\\|^{2}+\\beta\\left((1-\\gamma\\mu)^{n}\\mathbb{E}\\|x_{t}-x_{*}\\|^{2}+\\gamma^{3}\\frac{1}{\\gamma\\mu}\\hat{\\sigma}_{r}^{2}\\alpha d\\cdot\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Let us consider ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\eta^{2}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\|g_{t,m}-h_{*,m}\\|^{2}=\\eta^{2}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left\\|\\frac{x_{t}-x_{t,m}^{n}}{\\gamma n}-\\frac{x_{*}-x_{*,m}^{n}}{\\gamma n}\\right\\|^{2}}\\\\ &{\\le2\\eta^{2}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left\\|\\frac{x_{t}-x_{*}}{\\gamma n}\\right\\|^{2}+2\\eta^{2}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left\\|\\frac{x_{t,m}^{n}-x_{*,m}^{n}}{\\gamma n}\\right\\|^{2}}\\\\ &{\\le2\\beta^{2}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left\\|x_{t}-x_{*}\\right\\|^{2}+2\\beta^{2}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left\\|x_{t,m}^{n}-x_{*,m}^{n}\\right\\|^{2}}\\\\ &{\\le2\\beta^{2}\\mathbb{E}\\left\\|x_{t}-x_{*}\\right\\|^{2}+2\\beta^{2}\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left\\|x_{t,m}^{n}-x_{*,m}^{n}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "$\\begin{array}{r}{(1-\\gamma\\mu)^{n}\\le\\frac{9/10-1/B}{1+1/B}=\\widehat{B}<1,\\beta\\le\\frac{1}{12B\\frac{\\omega}{M}+1}}\\end{array}$ we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Psi_{t+1}\\leq\\left(1-\\beta+12\\frac{\\omega}{M}\\beta^{2}+12\\frac{\\omega}{M}\\beta^{2}(1-\\gamma\\mu)^{n}+\\beta(1-\\gamma\\mu)^{n}\\right)\\mathbb{E}\\|x_{t}-x_{*}\\|^{2}+\\beta\\gamma^{3}\\frac{1}{\\gamma\\mu}\\hat{\\sigma}_{r a d}^{2}}\\\\ &{\\qquad+\\,2\\beta^{2}\\frac{6\\omega}{M}\\gamma^{3}\\frac{1}{\\gamma\\mu}\\hat{\\sigma}_{r a d}^{2}+\\left(1-\\frac{\\alpha}{2}\\right)\\frac{4\\omega\\eta^{2}}{\\alpha M}\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}\\|h_{t,m}-h_{*,m}\\|^{2}}\\\\ &{\\qquad\\leq\\left(1-\\frac{\\beta}{10}\\right)\\mathbb{E}\\|x_{t}-x_{*}\\|^{2}+\\frac{2}{\\mu}\\beta\\gamma^{2}\\hat{\\sigma}_{r a d}^{2}+\\left(1-\\frac{\\alpha}{2}\\right)\\frac{4\\omega\\eta^{2}}{\\alpha M}\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}\\|h_{t,m}-h_{*,m}\\|^{2}}\\\\ &{\\qquad\\leq\\operatorname*{max}\\left(1-\\frac{\\beta}{10},1-\\frac{\\alpha}{2}\\right)\\Psi_{t}+\\frac{2}{\\mu}\\beta\\gamma^{2}\\hat{\\sigma}_{r a d}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Input: $x_{0}$ - starting point,  > 0 - local stepsize, $\\eta>0$ - global stepsize   \n1: for $t=0,1,\\ldots,T-1$ do   \n2: Sample a cohort $S_{t}$ with cardinality $C$ uniformly   \n34 $m\\in S_{t}$ $x_{t}$ $x_{t,m}^{0}=x_{t}$   \n5: Sample random permutation of $[n]$ $\\!\\!\\,:\\pi_{m}=(\\pi_{m}^{0},\\,.\\,.\\,.\\,,\\pi_{m}^{n-1})$   \n6: for $i=0,1,\\dotsc,n-1$ do   \n7: Sete $\\boldsymbol{x}_{t,m}^{i+1}=\\boldsymbol{x}_{t,m}^{i}-\\gamma\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol{x}_{t,m}^{i})$   \n8: end for   \n9: Compute $\\begin{array}{r}{g_{t,m}=\\frac{1}{\\gamma n}\\left(x_{t}-x_{t,m}^{n}\\right)}\\end{array}$ and send $\\mathcal{Q}_{t}\\big(g_{t,m}\\big)$ to the server   \n10: end for   \n11: Compute $\\begin{array}{r}{g_{t}=\\frac{1}{C}\\sum_{m\\in S_{t}}\\mathcal{Q}_{t}(g_{t,m})}\\end{array}$   \n12: Compute $x_{t+1}=x_{t}-\\eta g_{t}$ and send $x_{t+1}$ to the workers   \n13: end for   \nOutput: $x_{T}$ ", "page_idx": 66}, {"type": "text", "text": "J   Partial Participation for Method with Local Steps ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "J.1 Analysis of Q-NASTYA with Partial Participation ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Lemma J1. Let Assumptions 1, 2, 3 hold. Then, for all $t\\geq0$ the iterates produced by Q-NASTYA-PP (Algorithm 5) satisfy ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{Q},S_{t}}\\left[\\left\\|g_{t}\\right\\|^{2}\\right]\\leq\\displaystyle\\frac{2L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\omega}{C}\\right)}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{i}-x_{t}\\right\\|^{2}+8L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)\\left(f\\left(x_{t}\\right)-f\\left(x^{\\star}\\right)\\right)}\\\\ &{\\qquad\\qquad+4\\left(\\displaystyle\\frac{\\omega}{C}+\\frac{M-C}{C\\operatorname*{max}M-1,1}\\right)\\sigma_{\\star}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "where $\\mathbb{E}_{\\mathcal{Q},S_{t}}$ is expectation w.r.t. $\\mathcal{Q},S_{t}$ and $\\begin{array}{r}{\\sigma_{\\star}^{2}=\\frac{1}{M}\\sum_{m=1}^{M}\\|\\nabla f_{m}\\left(x^{\\star}\\right)\\|^{2}}\\end{array}$ ", "page_idx": 66}, {"type": "text", "text": "Proof. $\\mathbb{E}\\left[\\|\\boldsymbol{\\xi}\\|^{2}\\right]=\\mathbb{E}\\left[\\|\\boldsymbol{\\xi}-\\mathbb{E}[\\boldsymbol{\\xi}]\\|^{2}\\right]+\\|\\mathbb{E}\\boldsymbol{\\xi}\\|^{2}$ we obtain ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{\\mathbb{Q}}\\left[\\|{\\bf\\Psi}_{H}^{\\bot}\\|^{2}\\right]}\\\\ {\\displaystyle=\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|\\frac{1}{C}\\sum_{n\\in\\mathbb{S}_{\\mathbb{S}}}\\left(Q\\left(\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{n n}^{\\pi_{i}^{i}}\\left(x_{t,n}^{i}\\right)\\right)-\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{n n}^{\\pi_{i}^{i}}\\left(x_{t,n}^{i}\\right)\\right)+\\frac{1}{C n}\\sum_{m\\in\\mathbb{S}_{\\mathbb{S}}}\\sum_{i=0}^{n-1}\\nabla f_{n n}^{\\pi_{i}^{i}}\\left(x_{t,n}^{i}\\right)\\right.}\\\\ {\\displaystyle=\\frac{1}{C^{2}}\\mathbb{E}_{\\mathbb{Q}}\\|\\sum_{m\\in\\mathbb{S}_{\\mathbb{S}}}(\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{n n}^{\\pi_{i}^{i}}\\left(x_{t,n}^{i}\\right))-\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{n n}^{\\pi_{i}^{i}}\\left(x_{t,n}^{i}\\right)\\|^{2}]}\\\\ {\\displaystyle~~~\\left.+\\left\\|\\frac{1}{C n}\\sum_{m\\in\\mathbb{S}_{\\mathbb{S}}}\\sum_{i=0}^{n-1}\\nabla f_{n n}^{\\pi_{i}^{i}}\\left(x_{t,n}^{i}\\right)\\right\\|^{2}}\\\\ {\\displaystyle=\\frac{1}{C^{2}}\\mathbb{E}_{\\mathbb{Q}}\\left[\\sum_{n\\in\\mathbb{S}_{\\mathbb{S}}}\\|\\xi_{m}\\|^{2}+\\sum_{m\\in\\mathbb{S}_{\\mathbb{S}}}2^{2}\\left\\langle\\xi_{m},\\xi_{l}\\right\\rangle\\right]+\\left\\|\\frac{1}{C n}\\sum_{m\\in\\mathbb{S}_{\\mathbb{S}}}\\sum_{i=0}^{n-1}\\nabla f_{n n}^{\\pi_{i}^{i}}\\left(x_{t,n}^{i}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Using independence between $\\xi_{m}$ and $\\xi_{l}$ for different $m,l$ and Using (2), (3), we get ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\|g_{t}\\right\\|^{2}\\right]=\\!\\!\\frac{1}{C^{2}}\\displaystyle\\sum_{m\\in S_{t}}\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\|\\mathcal{Q}\\left(\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}\\left(x_{t,m}^{i}\\right)\\right)-\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}\\left(x_{t,m}^{i}\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left\\|\\displaystyle\\frac{1}{C n}\\sum_{m\\in S_{t}}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}\\left(x_{t,m}^{i}\\right)\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\!\\!\\frac{\\omega}{C^{2}}\\displaystyle\\sum_{m\\in S_{t}}\\left\\|\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}\\left(x_{t,m}^{i}\\right)\\right\\|^{2}+\\left\\|\\displaystyle\\frac{1}{C n}\\sum_{m\\in S_{t}}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}\\left(x_{t,m}^{i}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Rewriting previous inequality and using $\\nabla f_{m}(x)=\\frac{1}{n}\\sum_{i=0}^{n-1}\\nabla f_{m}^{\\pi_{m}^{i}}\\left(x_{t}\\right)$ , we have ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\Vert g_{t}\\right\\Vert^{2}\\right]\\leq\\displaystyle\\frac{2\\omega}{C^{2}}\\sum_{m\\in\\mathcal{S}_{t}}\\left\\Vert\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(\\nabla f_{m n}^{\\pi_{n}^{i}}\\left(x_{t,m}^{i}\\right)-\\nabla f_{m n}^{\\pi_{n}^{i}}\\left(x_{t}\\right)\\right)\\right\\Vert^{2}+\\displaystyle\\frac{2\\omega}{C^{2}}\\sum_{m\\in\\mathcal{S}_{t}}\\left\\Vert\\nabla f_{m}\\left(x_{t}\\right)\\right\\Vert^{2}}\\\\ &{\\quad\\quad\\quad+\\displaystyle2\\left\\Vert\\frac{1}{C n}\\sum_{m\\in\\mathcal{S}_{t}}\\sum_{i=0}^{n-1}\\left(\\nabla f_{m n}^{\\pi_{n}^{i}}\\left(x_{t,m}^{i}\\right)-\\nabla f_{m n}^{\\pi_{n}^{i}}\\left(x_{t}\\right)\\right)\\right\\Vert^{2}+2\\left\\Vert\\frac{1}{C}\\sum_{m\\in\\mathcal{S}_{t}}\\nabla f_{m}\\left(x_{t}\\right)\\right\\Vert^{2}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\frac{2\\left(1+\\frac{\\omega}{C}\\right)}{C}\\sum_{m\\in\\mathcal{S}_{t}}\\left\\Vert\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(\\nabla f_{m n}^{\\pi_{n}^{i}}\\left(x_{t,m}^{i}\\right)-\\nabla f_{m n}^{\\pi_{n}^{i}}\\left(x_{t}\\right)\\right)\\right\\Vert^{2}}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\frac{2\\omega}{C^{2}}\\sum_{m\\in\\mathcal{S}_{t}}\\left\\Vert\\nabla f_{m}\\left(x_{t}\\right)\\right\\Vert^{2}+2\\left\\Vert\\frac{1}{C}\\sum_{m\\in\\mathcal{S}_{t}}\\nabla f_{m}\\left(x_{t}\\right)\\right\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Using $L$ -smoothness of $f_{m}^{i}$ and $f$ and also convexity of $f_{m}$ , we obtain ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|{R}\\right\\|^{2}\\right]\\leq\\displaystyle\\sum_{C=0}^{2\\lfloor1+\\frac{\\delta^{\\prime}}{\\delta}\\rfloor}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty\\rfloor-1}\\left\\|{\\nabla}\\int_{m}^{\\varepsilon_{m}^{\\prime}}\\left(x_{\\varepsilon,m}^{\\varepsilon}\\right)-\\nabla\\int_{m}^{\\varepsilon_{m}^{\\prime}}\\left(x_{\\varepsilon}\\right)\\right\\|^{2}}\\\\ &{\\qquad+\\displaystyle\\frac{4\\omega}{C^{2}}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty}\\left\\|\\nabla f_{m}\\left(x_{\\varepsilon}\\right)-\\nabla f_{m}\\left(x^{\\varepsilon}\\right)\\right\\|^{2}}\\\\ &{\\qquad+\\displaystyle\\frac{4\\omega}{C^{2}}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty}\\left\\|\\nabla f_{m}\\left(x^{\\varepsilon}\\right)\\right\\|^{2}+4\\left\\|\\frac{1}{C}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty}\\left(\\nabla f_{m}\\left(x_{\\varepsilon}\\right)-\\nabla f_{m}\\left(x^{\\varepsilon}\\right)\\right)\\right\\|^{2}}\\\\ &{\\qquad+4\\left\\|\\frac{1}{C}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty}\\nabla f_{m}\\left(x^{\\varepsilon}\\right)\\right\\|^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\frac{2L_{m\\in\\mathbb{S}}^{2}\\left(1+\\frac{\\delta^{\\prime}}{C^{2}}\\right)}{C D}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty\\rfloor-1}\\left\\|x_{\\varepsilon,m}^{\\varepsilon}-x_{\\varepsilon}\\right\\|^{2}+\\displaystyle\\frac{8L_{m\\in\\mathbb{S}}\\left(1+\\frac{\\delta^{\\prime}}{C}\\right)}{C}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty}D_{m,\\varepsilon}\\left(x_{\\varepsilon,m}^{\\varepsilon}\\right)}\\\\ &{\\qquad+\\displaystyle\\frac{4\\omega}{C^{2}}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty}\\left\\|\\nabla f_{m}\\left(x^{\\varepsilon}\\right)\\right\\|^{2}+4\\left\\|\\frac{1}{C}\\displaystyle\\sum_{m\\in\\mathbb{S}_{+}}^{\\infty}\\nabla f_{m}\\left(x^{\\varepsilon}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Taking expectation w.r.t. $S_{t}$ and using uniform sampling, we receive ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbb{Q},s_{t}}\\left[\\left\\|g_{t}\\right\\|^{2}\\right]\\leq\\frac{2L_{m a x}^{2}\\left(1+\\frac{\\kappa^{2}}{C}\\right)}{n}\\mathbb{E}_{s_{t}}\\left[\\frac{1}{C}\\sum_{m\\in\\mathcal{S}_{t}}\\sum_{i=m}^{n-1}\\|x_{t,m}^{i}-x_{t}|^{2}\\right]}&{}\\\\ &{\\quad+8L_{m a x}\\left(1+\\frac{\\omega}{C}\\right)\\mathbb{E}_{s_{t}}\\left[\\frac{1}{C}\\sum_{m\\in\\mathcal{S}_{t}}D_{f_{m}}\\left(x_{t,x},x^{*}\\right)\\right]}\\\\ &{\\quad+\\frac{4\\omega}{C}\\mathbb{E}_{S_{t}}\\left[\\frac{1}{C}\\sum_{m\\in\\mathcal{S}_{t}}\\|\\nabla f_{m}(x^{*})\\|^{2}\\right]+4\\mathbb{E}_{s_{t}}\\left[\\left\\|\\frac{1}{C}\\sum_{m\\in\\mathcal{S}_{t}}\\nabla f_{m}\\left(x^{*}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{2L_{m a x}^{2}\\left(1+\\frac{\\kappa^{2}}{C}\\right)}{M n}\\sum_{m=1}^{n}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{i}-x_{t}\\right\\|^{2}+\\frac{8L_{m a x}\\left(1+\\frac{\\kappa}{C}\\right)}{M}\\sum_{m=1}^{M}D_{f_{m}}\\left(x_{t,x},x^{*}\\right)}\\\\ &{\\quad+\\frac{4\\omega}{C}\\frac{1}{M}\\frac{M}{\\displaystyle\\sum_{m=1}^{n}\\|\\nabla f_{m}(x^{*})\\|^{2}}+4\\frac{M-C}{M C\\operatorname*{max}M-1,1}\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}\\left(x^{*}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Theorem J.1. Let step sizes $\\eta,\\gamma$ satisfy the following equations ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{16L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)},\\;\\;\\;\\;\\gamma=\\frac{1}{5n L_{\\operatorname*{max}}}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Under the Assumptions 1, 2, 3 iterates of Q-NASTYA-PP (Algorithm 5) satisfy ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{T}-x^{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{\\eta\\mu}{2}\\right)^{T}\\left\\Vert x_{0}-x^{\\star}\\right\\Vert^{2}+\\displaystyle\\frac{9}{2}\\frac{\\gamma^{2}n L_{\\operatorname*{max}}}{\\mu}\\left(\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\sigma_{\\star,m}^{2}+n\\sigma_{\\star}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle8\\frac{\\eta}{\\mu}\\left(\\displaystyle\\frac{\\omega}{C}\\sigma_{\\star}^{2}+\\displaystyle\\frac{M-C}{C\\operatorname*{max}(M-1,1)}\\sigma_{\\star}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sigma_{\\star}^{2}=\\frac{1}{M}\\sum_{m=1}^{M}\\left\\Vert\\nabla f_{m}\\left(x^{\\star}\\right)\\right\\Vert^{2},\\quad\\sigma_{\\star,m}^{2}=\\frac{1}{n}\\left\\Vert\\nabla f_{m}^{i}\\left(x^{\\star}\\right)\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "As we can see, there is an additional error term proportional to 0Cmax(M-1.1) that arises due to client sampling in the partial participation setting. Note that when $C=M$ (all clients are participating), this error term vanishes, allowing us to recover the previous result for the full participation case. This shows the consistency of our theoretical framework across different participation scenarios. ", "page_idx": 68}, {"type": "text", "text": "Proof. ", "page_idx": 69}, {"type": "text", "text": "Taking expectation w.t. $\\mathcal{Q},S_{t}$ and using Lemma J.1 updated, we get ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{Q},\\mathbf{x}}\\left[\\left\\|\\mathbf{z}_{i+1}-\\mathbf{z}^{\\prime}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\mathbb{Q},\\mathbf{x}}\\left[\\left\\|\\mathbf{z}_{i}-\\mathbf{z}^{\\prime}\\right\\|^{2}-2\\mathbb{I}\\mathbb{Q}\\mathbb{E}_{\\mathbb{S}}\\left[\\left\\|\\mathbf{z}_{i},\\mathbf{z}_{i}-\\mathbf{z}^{\\prime}\\right\\|^{2}\\right]+\\mathbb{I}_{\\mathbb{Q}}^{\\mathbb{R}}\\mathbb{Q}_{\\mathbb{L}}\\left[\\left\\|\\mathbf{z}^{\\prime}\\right\\|^{2}\\right]\\right.}\\\\ &{\\left.\\le\\mathbb{I}\\left[\\mathbf{z}_{i}-\\mathbf{z}^{\\prime}\\right]^{2}-2\\mathbb{I}\\mathbb{Q}\\mathbb{E}_{\\mathbb{S},\\mathbf{x}}\\left[\\left\\langle\\frac{1}{C}\\sum_{m=\\bar{m}}^{\\infty}\\left(\\frac{\\mathbf{z}_{i}^{\\prime}}{n}\\!\\!\\!\\frac{1}{C_{m}^{\\prime}}\\!\\!\\!\\frac{1}{C_{m}^{\\prime}}\\!\\left(\\frac{z_{i+m}^{\\prime}}{n}\\right)\\right),\\!\\!\\!x_{i}-\\mathbf{z}^{\\prime}\\right\\rangle\\right]}\\\\ &{\\quad+\\frac{2^{2}\\eta^{2}\\hat{L}_{\\mathbb{Q},\\mathbf{x}}^{2}\\left(1\\!\\!-\\!\\!\\frac{\\hat{y}_{i}^{\\prime}}{C_{m}^{\\prime}}\\!\\left(\\frac{1}{2C}\\!\\!-\\!\\!\\frac{1}{C_{m}^{\\prime}}\\!\\left|\\mathbf{z}_{i}^{\\prime},\\mathbf{z}_{m}^{\\prime}\\right|^{2}+8y^{2}\\!\\!I_{\\operatorname*{max}}\\left(1+\\frac{\\overline{{\\mathbf{u}}}}{C}\\right)(f(\\mathbf{x}_{i})-f(\\mathbf{z}^{\\prime})\\right)\\right.}\\\\ &{\\left.\\quad+\\frac{4\\eta^{2}}{C}\\left(\\frac{\\mathbf{z}}{C}+\\frac{M-C}{C_{m}\\mathbf{z}_{m}\\mathbf{z}_{m}\\mathbf{z}_{m}}\\right)\\right)\\sigma_{i}^{2}}\\\\ &{\\le\\left\\|\\mathbf{z}_{i}-\\mathbf{z}^{\\prime}\\right\\|^{2}-2\\eta\\frac{M-M}{M}\\frac{M-1}{C_{m}\\mathbf{z}_{m}^{\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Using Lemma F.2, we obtain ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{Q},\\mathcal{S}_{t}}\\left[\\left\\|x_{t+1}-x^{*}\\right\\|^{2}\\right]}\\\\ &{\\leq\\left\\|x_{t}-x^{*}\\right\\|^{2}-\\frac{\\eta\\mu}{2}\\left\\|\\pi_{t}-x^{*}\\right\\|^{2}-\\eta\\left(f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right)}\\\\ &{\\quad+8\\eta^{2}L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)\\left(f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right)+\\frac{\\eta\\hat{L}_{\\operatorname*{max}}}{M n}\\displaystyle\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{*}-x_{t}\\right\\|^{2}}\\\\ &{\\quad+\\frac{2\\eta^{2}L_{\\operatorname*{max}}^{2}\\left(1+\\frac{\\omega}{C}\\right)}{M n}\\displaystyle\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t}^{*}-x_{t}\\right\\|^{2}+4\\eta^{2}\\left(\\frac{\\omega}{C}+\\frac{M-C}{C}\\right)\\sigma_{z}^{2}}\\\\ &{\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)\\left\\|x_{t}-x^{*}\\right\\|^{2}-\\eta\\left(1-8f_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)\\right)\\left(f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right)}\\\\ &{\\quad+\\frac{\\eta\\hat{L}_{\\operatorname*{max}}\\left(1+2\\eta\\hat{L}_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)\\right)}{M n}\\displaystyle\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\|x_{t,m}^{*}-x_{t}\\right\\|^{2}+}\\\\ &{\\quad+\\eta^{2}\\left(\\frac{\\omega}{C}+\\frac{M-C}{C\\operatorname*{max}}\\right)\\prod_{i=1}^{n}\\sigma_{z}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Using Lemma F.3, we have ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q,S_{t}}\\left[\\left\\Vert x_{t+1}-x^{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)\\left\\Vert x_{t}-x^{\\star}\\right\\Vert^{2}-\\eta\\left(1-8\\eta L\\left(1+\\frac{\\omega}{C}\\right)\\right)(f\\left(x_{t}\\right)-f\\left(x^{\\star}\\right))}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\eta L_{\\operatorname*{max}}\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)\\right)\\cdot8\\gamma^{2}n^{2}L_{\\operatorname*{max}}\\left(f\\left(x_{t}\\right)-f\\left(x^{\\star}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\eta L_{\\operatorname*{max}}\\left(1+2\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)\\right)\\cdot2\\gamma^{2}n\\left(\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\sigma_{\\star,m}^{2}+n\\sigma_{\\star}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\left.4\\eta^{2}\\left(\\frac{\\omega}{C}+\\frac{M-C}{C\\operatorname*{max}M-1,1}\\right)\\sigma_{\\star}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Finally, we receive ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{Q},S_{t}}\\left[\\left\\|x_{t+1}-x^{*}\\right\\|^{2}\\right]}\\\\ &{\\le\\left(1-\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x^{*}\\|^{2}+4\\eta^{2}\\left(\\frac{\\omega}{C}+\\frac{M-C}{C\\operatorname*{max}M-1,1}\\right)\\sigma_{*}^{2}}\\\\ &{\\quad-\\,\\eta\\left(1-8\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)-8\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\left(1+2L_{\\operatorname*{max}}\\eta\\left(1+\\frac{\\omega}{C}\\right)\\right)\\right)\\left(f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right)}\\\\ &{\\quad+\\,2\\gamma^{2}n\\eta L_{\\operatorname*{max}}\\left(1+2\\eta L\\left(1+\\frac{\\omega}{C}\\right)\\right)\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\sigma_{*,m}^{2}+n\\sigma_{*}^{2}\\right)}\\\\ &{\\le\\left(1-\\frac{\\eta\\mu}{2}\\right)\\|x_{t}-x^{*}\\|^{2}+4\\eta^{2}\\left(\\frac{\\omega}{C}+\\frac{M-C}{C\\operatorname*{max}M-1,1}\\right)\\sigma_{*}^{2}}\\\\ &{\\quad+\\,\\frac{9}{4}\\eta L_{\\operatorname*{max}}\\gamma^{2}n\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\sigma_{*,m}^{2}+n\\sigma_{*}^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Recursively rewriting the inequality and using $\\begin{array}{r}{\\sum_{t=0}^{+\\infty}\\left(1-\\frac{\\eta\\mu}{2}\\right)^{t}\\leq\\frac{2}{\\mu\\eta}}\\end{array}$ we finish proof. ", "page_idx": 70}, {"type": "text", "text": "Input: $x_{0}$ - starting poin, $\\{h_{0,m}\\}_{m=1}^{M}$ - initia shift-vetors, $\\gamma>0$ - local tepsize, $\\eta>0$ - global   \nstepsize, stepsize for learning the shifts   \n1: for $t=0,1,\\ldots,T-1$ do   \n2: Sample a cohort $S_{t}$ with cardinality $C$ uniformly   \n34 $m\\in S_{t}$ $x_{t}$ $x_{t,m}^{0}=x_{t}$   \n5: Sample random permutation of $[n]$ $:\\pi_{m}=(\\pi_{m}^{0},\\ldots,\\pi_{m}^{n-1})$   \n6: for $i=0,1,\\dotsc,n-1$ do   \n7: Set $\\boldsymbol{x}_{t,m}^{i+1}=\\boldsymbol{x}_{t,m}^{i}-\\gamma\\nabla f_{m}^{\\pi_{m}^{i}}(\\boldsymbol{x}_{t,m}^{i})$   \n8: end for   \n9: Compute $\\begin{array}{r}{g_{t,m}=\\frac{1}{\\gamma n}\\left(x_{t}-x_{t,m}^{n}\\right)}\\end{array}$ and send $\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)$ to the server   \n10: Set $h_{t+1,m}=h_{t,m}+\\alpha\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)$   \n11: Set $\\hat{g}_{t,m}=h_{t,m}+\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)$   \n12: end for   \n13: $\\begin{array}{r l}&{h_{t+1}=\\frac{1}{C}\\sum_{m\\in S_{t}}h_{t+1,m}=h_{t}+\\frac{\\alpha}{C}\\sum_{m\\in S_{t}}\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)}\\\\ &{\\hat{g}_{t}=\\frac{1}{C}\\sum_{m\\in S_{t}}\\hat{g}_{t,m}=h_{t}+\\frac{1}{C}\\sum_{m\\in S_{t}}\\mathcal{Q}_{t}\\left(g_{t,m}-h_{t,m}\\right)}\\\\ &{x_{t+1}=x_{t}-\\eta\\hat{g}_{t}}\\end{array}$   \n14:   \n15:   \n16: end for   \nOutput: $x_{T}$ ", "page_idx": 71}, {"type": "text", "text": "", "page_idx": 71}, {"type": "text", "text": "J.2 Analysis of DIANA-NASTYA with Partial Participation ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Theorem J.2. Let step sizes $\\eta,\\gamma$ satisfy the following equations ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}\\left(\\frac{1}{80L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)},\\frac{C}{\\mu(1+\\omega)M}\\right),\\quad\\gamma=\\frac{1}{5n L_{\\operatorname*{max}}}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Define the Lyapunov function: ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\Psi_{t}=\\left\\|x_{t}-x^{\\star}\\right\\|^{2}+\\frac{A}{M}\\sum_{m=1}^{M}\\left\\|h_{t,m}-h_{m}^{\\star}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where $A\\ =\\ \\lambda\\eta^{2}$ .Selecting parameters $\\begin{array}{r c l c r}{{\\alpha\\!\\!}}&{{=}}&{{\\!\\!\\frac{1}{1+\\omega};\\lambda\\!\\!}}&{{=}}&{{\\!\\!\\frac{8\\omega}{\\alpha M},\\gamma\\!\\!}}&{{=}}&{{\\!\\!\\frac{1}{5n L_{\\mathrm{max}}}}}\\end{array}$ alsousing $\\eta\\ \\leq$ r $\\begin{array}{r}{\\operatorname*{min}\\left[\\frac{C}{\\mu(1+\\omega)M},\\frac{1}{80L_{\\mathrm{max}}\\left(1+\\frac{\\omega}{C}\\right)}\\right]}\\end{array}$ Under the Assumptions 1, 2, 3 iterates of DIANA-NASTYA-PP (Algorithm $^{6}$ )satisfy ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi_{T}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)^{T}\\mathbb{E}\\left[\\Psi_{0}\\right]+\\frac{3\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}}{\\mu}\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\sigma_{\\star,m}^{2}+n\\sigma_{\\star}^{2}\\right)+\\frac{2\\eta(M-C)}{\\mu C\\operatorname*{max}(1,M-1)}\\sigma_{\\star}^{2}.\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Note that we eliminate the variance term proportional to $\\omega:8\\frac{\\eta}{\\mu}\\frac{\\omega}{C}\\sigma_{\\star}^{2}$ . In the Partial Participation regime, we have a variance term proportional to Cmax(1.,M-1), which equals zero if C = M. This term decreases as $\\textstyle{\\mathcal{O}}\\left({\\frac{1}{C}}\\right)$ , so we achieve the expected linear speedup. ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\mathbb{E}_{t}\\Bigg[\\Bigg\\langle\\frac{1}{C}\\sum_{m=0}^{j_{1}}\\mathcal{R}_{m,n}-x^{*}\\Bigg\\rangle\\Bigg]=-\\Bigg\\langle\\frac{1}{C}\\mathbb{E}_{t}\\Bigg[\\sum_{\\ell=0}^{j_{2}}\\hat{\\mathcal{R}}_{m,n}\\Bigg],x_{\\ell}-x^{*}\\Bigg\\rangle}&{}\\\\ {=-\\Bigg\\langle\\frac{1}{M}\\sum_{m=1}^{N}\\mathbb{E}_{t}\\Bigg\\langle\\hat{\\mathcal{R}}_{m}\\Bigg\\vert z_{\\ell},x_{\\ell},x_{\\ell},x_{\\ell}\\Bigg\\rangle}&{}\\\\ {=-\\frac{1}{M}\\sum_{m=1}^{N}\\mathcal{R}_{m,n},x_{\\ell}-x^{*}\\Bigg\\rangle}&{}\\\\ {=-\\frac{1}{M}\\sum_{m=1}^{N}\\mathcal{R}_{m,n}-x_{\\ell},x_{\\ell}-x^{*}\\Bigg\\rangle}&{}\\\\ {\\leq-\\frac{1}{4}\\left[\\pi_{k}-x^{*}\\right]^{2}-\\frac{1}{2}(J(\\alpha)-J(x^{*}))}&{}\\\\ {-\\frac{1}{M}\\sum_{m=1}^{N}\\sum_{\\ell=0}^{N}\\int\\hat{\\mathcal{R}}_{\\ell}\\Bigg\\langle z^{*},z_{\\ell}^{*}\\Bigg\\rangle}&{}\\\\ {\\quad+\\frac{J(\\ensuremath{T}_{m})}{2M}\\sum_{m=1}^{N}\\frac{1}{\\sum_{\\ell=0}^{N}\\bigg\\Vert\\mathcal{R}_{m}-z_{\\ell}^{*}\\bigg\\Vert^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "STEP 2: We need to bound $\\mathbb{E}\\left\\|\\hat{g}_{t}\\right\\|^{2}$ .By $\\hat{g}_{t}=\\frac{1}{C}\\sum_{m\\in S_{t}}\\hat{g}_{t,m}$ we have ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{Q}}\\left[\\left\\|\\mathbf{j}_{H}\\right\\|^{2}\\right]=\\mathbb{E}_{\\mathbf{Q}}\\left[\\left\\|\\frac{1}{C}\\sum_{w\\in S_{t}}\\left(h_{t,m}+\\mathcal{Q}(\\theta_{m},\\ldots h_{t,m})-g_{t,m}+g_{t,m}\\right)\\right\\|^{2}\\right]}&{}\\\\ {=\\mathbb{E}_{\\mathbf{Q}}\\left[\\left\\|\\frac{1}{C}\\sum_{w\\in S_{t}}\\left(h_{t,m}+\\mathcal{Q}(\\theta_{m},\\ldots h_{t,m})-g_{t,m}\\right)\\right\\|^{2}\\right]+\\left\\|\\frac{1}{C}\\sum_{w\\in S_{t}}\\beta_{t,m}\\right\\|^{2}}&{}\\\\ {=\\frac{1}{C^{2}}\\sum_{w\\in S_{t}}\\mathbb{E}_{\\mathbf{I}}\\left[\\left|h_{t,m}+\\mathcal{Q}(\\theta_{m},\\ldots h_{t,m})\\right|^{2}\\right]+\\left\\|\\frac{1}{C}\\sum_{w\\in S_{t}}\\beta_{t,m}\\right\\|^{2}}&{}\\\\ {\\leq\\frac{C}{C^{2}}\\sum_{w\\in S_{t}}\\left\\|g_{t,m}-h_{t,m}\\right\\|^{2}+\\left\\|\\frac{1}{C}\\sum_{w\\in S_{t}}\\beta_{t,m}\\right\\|^{2}}&{}\\\\ {\\leq\\frac{C^{2}}{C^{2}}\\sum_{w\\in S_{t}}\\left\\|g_{t,m}-\\nabla f_{m}(\\mathbf{z})\\right\\|^{2}+\\frac{2C}{C^{2}}\\sum_{w\\in S_{t}}\\|\\nabla f_{m}(\\mathbf{z})-h_{t,m}\\|^{2}}&{}\\\\ {\\leq\\frac{C^{2}}{C^{2}}\\sum_{w\\in S_{t}}\\beta_{t,m}-\\nabla f_{m}(\\mathbf{z})\\|^{2}+2\\frac{C^{2}}{C^{2}}\\sum_{w\\in S_{t}}\\|\\nabla f_{m}(\\mathbf{z})-h_{t,m}\\|^{2}}&{}\\\\ {+2\\left\\|\\frac{1}{C}\\sum_{w\\in S_{t}}\\beta_{t,m}-h_{w}^{*}\\right\\|^{2}+2\\left\\|\\frac{1}{C}\\sum_{w\\in S_{t}}h_{w}^{*}\\right\\|^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{i}^{\\prime}\\left\\{\\sum_{j=1}^{M}\\frac{1}{\\alpha_{j+1}}\\sum_{u=-1\\atop h\\longrightarrow1}^{M}\\mathrm{i}_{\\infty}-\\nabla_{\\Gamma_{\\alpha}}(\\rho_{\\Gamma_{\\alpha}})\\right\\}^{2}\\mathrm{e}^{-\\frac{i}{\\alpha_{j}}\\sum_{u=-1\\atop h\\longrightarrow1}^{M}\\frac{1}{\\alpha_{j}}\\left[\\nabla_{\\Gamma_{\\alpha}}(\\rho_{\\Gamma})-\\rho_{\\mathrm{hos}}\\right]^{2}}}}\\\\ &{\\quad+\\frac{1}{2}\\frac{1}{\\alpha_{j}}\\sum_{u=-1\\atop h\\longrightarrow1}^{M}\\left\\{3_{\\alpha_{j}}-\\zeta_{\\alpha}\\right\\}^{2}+\\frac{2(M-T)}{\\zeta_{\\alpha}(M-1)!}\\frac{1}{\\alpha_{j}}\\left[\\alpha_{\\Gamma_{\\alpha}}\\right]^{2}}\\\\ &{\\leq\\frac{\\zeta_{\\alpha}^{2}}{2}\\frac{1}{\\alpha_{j+1}}\\sum_{u=-1\\atop h\\longrightarrow1}^{M}\\mathrm{i}_{\\infty}-\\nabla_{\\Gamma_{\\alpha}}(\\rho_{\\Gamma_{\\alpha}})\\right\\}^{2}+\\frac{2}{\\alpha_{j}}\\frac{1}{\\alpha_{j}}\\sum_{u=-1\\atop h\\longrightarrow1}^{M}\\left\\{\\nabla_{\\Gamma_{\\alpha}}(\\rho_{\\Gamma_{\\alpha}})-\\rho_{\\mathrm{hos}}\\right\\}^{2}}\\\\ &{\\quad+\\frac{4}{\\alpha_{j}}\\frac{1}{\\alpha_{j+1}}\\left\\{3_{\\alpha_{j}}-\\nabla_{\\Gamma_{\\alpha}}(\\rho_{\\Gamma_{\\alpha}})\\right\\}^{2}+\\frac{4}{\\alpha_{j}}\\frac{1}{\\alpha_{j}}\\left\\{\\nabla_{\\Gamma_{\\alpha}}(\\rho_{\\Gamma_{\\alpha}})-\\lambda_{\\alpha_{j}}\\right\\}^{2}}\\\\ &{\\quad+\\frac{2(M-T)}{\\zeta_{\\alpha}(M-1)!}\\frac{1}{\\alpha_{j}!}\\left\\{\\alpha_{\\Gamma_{\\alpha}}\\right\\}^{2}}\\\\ &{\\quad+\\frac{4}{\\alpha_{j}!}\\frac{1}{\\alpha_{j}!}\\frac{1}{\\alpha_{j+1}!}\\left\\{\\alpha_{\\Gamma_{\\alpha}}\\right\\}^{2}}\\\\ &{\\leq(1+\\frac{\\zeta_{\\alpha}}{\\alpha_{j \n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Thus, we have ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{Q},S_{t}}\\left[\\left\\Vert x_{t+1}-x^{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{\\eta\\mu}{2}\\right)\\left\\Vert x_{t}-x^{\\star}\\right\\Vert^{2}-\\eta\\left(1-4L_{\\operatorname*{max}}\\eta\\right)\\left(f\\left(x_{t}\\right)-f\\left(x^{\\star}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\eta L_{\\operatorname*{max}}\\left(1+4\\left(1+\\displaystyle\\frac{\\omega}{C}\\right)L_{\\operatorname*{max}}\\eta\\right)\\displaystyle\\frac{1}{M n}\\sum_{m=1}^{M}\\sum_{i=0}^{n-1}\\left\\Vert x_{t,m}^{i}-x_{t}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{2\\eta^{2}\\omega}{C}\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\left\\Vert\\nabla f_{m}\\left(x_{t}\\right)-h_{t,m}\\right\\Vert^{2}+\\displaystyle\\frac{2\\eta^{2}(M-C)}{C(M-1)M}\\sum_{m=1}^{M}\\left\\Vert h_{m}^{\\star}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "STEP 3: Note that ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}\\left\\|h_{t+1,m}-h_{m}^{\\star}\\right\\|^{2}=\\frac{C}{M}\\frac{1}{C}\\sum_{m\\in S_{t}}\\left\\|h_{t+1,m}-h_{m}^{\\star}\\right\\|^{2}+\\frac{M-C}{M}\\frac{1}{M-C}\\sum_{m\\notin S_{t}}\\left\\|h_{t+1,m}-h_{m}^{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{\\mathcal{Q}}\\left[\\frac{1}{C}\\sum_{m\\in S_{t}}\\left\\|h_{t+1,m}-h_{m}^{\\star}\\right\\|^{2}\\right]=\\mathbb{E}_{\\mathcal{Q}}\\left[\\frac{1}{C}\\sum_{m\\in S_{t}}\\|h_{t,m}+\\alpha\\mathcal{Q}(g_{t,m}-h_{t,m})-h_{m}^{\\star}\\|^{2}\\right]}\\\\ {\\displaystyle=\\frac{1}{C}\\sum_{m\\in S_{t}}\\left(\\|h_{t,m}-h_{m}^{\\star}\\|^{2}+2\\alpha\\left\\langle g_{t,m}-h_{t,m},h_{t,m}-h_{m}^{\\star}\\right\\rangle+\\alpha^{2}(1+\\omega)\\left\\|g_{t,m}-h_{t,m}\\right\\|^{2}\\right)}\\\\ {\\displaystyle\\propto\\frac{1/1+\\omega}{\\le}\\frac{1}{C}\\sum_{m\\in S_{t}}\\left(\\|h_{t,m}-h_{m}^{\\star}\\|^{2}+2\\alpha\\left\\langle g_{t,m}-h_{t,m},h_{t,m}-h_{m}^{\\star}\\right\\rangle+\\alpha\\left\\|g_{t,m}-h_{t,m}\\right\\|^{2}\\right)}\\\\ {\\displaystyle=\\frac{1-\\alpha}{C}\\sum_{m\\in S_{t}}\\|h_{t,m}-h_{m}^{\\star}\\|^{2}+\\displaystyle\\frac{\\alpha}{C}\\sum_{m\\in S_{t}}\\left\\|g_{t,m}-h_{t,m}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "Taking expectation by subsampling, we have ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{\\mathcal{Q},S_{t}}\\left[\\displaystyle\\frac{1}{C}\\sum_{m\\in S_{t}}\\|h_{t+1,m}-h_{m}^{\\star}\\|^{2}\\right]\\leq\\mathbb{E}_{S_{t}}\\left[\\displaystyle\\frac{1-\\alpha}{C}\\sum_{m\\in S_{t}}\\|h_{t,m}-h_{m}^{\\star}\\|^{2}+\\displaystyle\\frac{\\alpha}{C}\\sum_{m\\in S_{t}}\\|g_{t,m}-h_{m}^{\\star}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1-\\alpha}{M}\\sum_{m=1}^{M}\\|h_{t,m}-h_{m}^{\\star}\\|^{2}+\\displaystyle\\frac{\\alpha}{M}\\sum_{m=1}^{M}\\|g_{t,m}-h_{m}^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "Thus, we have ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{S,\\sigma_{k}}\\bigg[\\displaystyle\\frac{1}{N}\\sum_{t=1}^{M}\\|h_{t+1,n}-h_{n}^{*}\\|^{2}\\bigg]^{2}\\Bigg|\\frac{(1-\\sigma)C}{N^{2}}\\sum_{t=1}^{M}\\|h_{t,n}-h_{n}^{*}\\|^{2}+\\frac{\\sigma C}{N^{2}}\\frac{W^{2}}{C}\\|\\|\\rho_{t,n}-h_{n}^{*}\\|^{2}}\\\\ &{\\qquad+\\frac{M-C}{M}\\mathbb{E}_{S,\\sigma_{k}}\\bigg[\\frac{1}{M}\\sum_{t=1}^{M}\\|h_{t,n}-h_{n}^{*}\\|^{2}\\bigg]}\\\\ &{=\\frac{(1-\\sigma)C}{N}\\sum_{t=1}^{M}\\|h_{t,n}-h_{n}^{*}\\|^{2}+\\frac{M C}{M^{2}}\\sum_{t=1}^{M}\\|g_{t,n}-h_{n}^{*}\\|^{2}}\\\\ &{\\qquad+\\frac{M-C}{M}\\sum_{t=1}^{M}\\|h_{t,n}-h_{n}^{*}\\|^{2}}\\\\ &{\\leq\\Big(1-\\frac{\\sigma C}{M}\\Big)\\sum_{t=1}^{M}\\|h_{t,n}-h_{n}^{*}\\|^{2}}\\\\ &{\\qquad+\\frac{2M C_{\\sigma}^{2}}{M^{2}n}C\\sum_{t=1}^{M}\\|g_{t,n}-x_{t}\\|^{2}}\\\\ &{\\qquad+\\frac{4M\\sigma C_{\\sigma}^{2}}{M^{2}n}\\sum_{t=1}^{M}\\|g_{t,n}-x_{t}\\|^{2}}\\\\ &{\\qquad+\\frac{4M\\sigma C_{\\sigma}^{2}}{M^{2}n}\\sum_{t=1}^{M}\\|g_{t,n}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "STEP 4: Defining Lyapunov function as follows ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\Psi_{t}=\\left\\|x_{t}-x^{\\star}\\right\\|^{2}+\\frac{A}{M}\\sum_{m=1}^{M}\\left\\|h_{t,m}-h_{m}^{\\star}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "we have ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q,S_{t}}\\left[\\Psi_{t+1}\\right]\\leq\\Big(1-\\frac{\\eta\\mu}{2}\\Big)\\left\\|x_{t}-x^{*}\\right\\|^{2}-\\eta\\left(1-4L_{\\operatorname*{max}}\\eta\\right)\\left(f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\eta L_{\\operatorname*{max}}\\left(1+4\\left(1+\\frac{\\omega}{C}\\right)L_{\\operatorname*{max}}\\eta\\right)\\frac{1}{M n}\\sum_{m=1}^{n-1}\\left\\|x_{t,m}^{*}-x_{t}\\right\\|^{2}}\\\\ &{\\qquad\\qquad+\\frac{2\\eta^{2}\\omega}{C}\\frac{1}{M}\\sum_{m=1}^{M}\\left\\|\\nabla f_{m}\\left(x_{t}\\right)-h_{t,m}\\right\\|^{2}+\\frac{2\\eta^{2}\\left(M-C\\right)}{C\\left(M-1\\right)M}\\sum_{m=1}^{M}\\left\\|h_{m}^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad+\\left(1-\\frac{\\alpha C}{M}\\right)\\frac{A}{M}\\sum_{m=1}^{M}\\left\\|h_{t,m}-h_{m}^{*}\\right\\|^{2}}\\\\ &{\\qquad\\qquad+\\frac{2\\alpha L_{\\operatorname*{max}}^{2}A C}{M^{2}n}\\sum_{m=1}^{M}\\left\\|x_{t,m}^{*}-x_{t}\\right\\|^{2}+\\frac{4L_{\\operatorname*{max}}G C}{M}\\left(f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Setting $A=\\lambda\\eta^{2}$ and using Lemma F.3, we have ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Psi_{t+1}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{t}-x^{*}\\right\\Vert^{2}\\right]+\\left(1-\\frac{\\alpha C}{M}+\\frac{4\\omega}{\\lambda C}\\right)\\frac{\\lambda\\eta^{2}}{M}\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\left[\\left\\Vert h_{t,m}-h_{m}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad-\\eta\\left(1-8\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)-4\\eta L_{\\operatorname*{max}}\\alpha\\lambda\\frac{C}{M}\\right)\\mathbb{E}\\left[f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{\\qquad\\qquad+8\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\eta\\left(1+4\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)+2\\eta L_{\\operatorname*{max}}\\alpha\\lambda\\frac{C}{M}\\right)\\mathbb{E}\\left[f\\left(x_{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{\\qquad+\\left.2\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\eta\\left(1+4\\eta L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)+2\\eta L_{\\operatorname*{max}}\\alpha\\lambda\\frac{C}{M}\\right)\\left(\\frac{1}{M}\\displaystyle\\sum_{m=1}^{M}\\sigma_{*,m}^{2}+n\\sigma_{*}^{2}\\right)}\\\\ &{\\qquad\\qquad+\\frac{2\\eta^{2}\\left(M-C\\right)}{C(M-1)}\\sigma_{*}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Selecting $\\begin{array}{r l r}{\\alpha}&{{}=}&{\\frac{1}{1+\\omega}}\\end{array}$ $\\begin{array}{r l r}{\\lambda}&{{}=}&{\\frac{8\\omega}{\\alpha M}}\\end{array}$ $\\begin{array}{r l r}{\\eta}&{{}\\le}&{\\frac{C}{\\mu(1+\\omega)M}}\\end{array}$ \u03bc(1+w)M , also using n = $\\begin{array}{r c l}{\\eta}&{=}&{\\frac{1}{80L_{\\operatorname*{max}}\\left(1+\\frac{\\omega}{C}\\right)}}\\end{array}$ \uff0c $\\frac{1}{5n L_{\\mathrm{max}}}$ and applying previous steps we obtain ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\Psi_{t+1}\\right]\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)\\mathbb{E}\\left[\\Psi_{k}\\right]+3\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\eta\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\sigma_{\\star,m}^{2}+n\\sigma_{\\star}^{2}\\right)+\\frac{2\\eta^{2}(M-C)}{C(M-1)}\\sigma_{\\star}^{2},}\\\\ {\\displaystyle\\quad\\quad-\\,\\eta\\left(\\frac{1}{2}-10\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\right)\\mathbb{E}\\left[f\\left(x_{t}\\right)-f\\left(x^{\\star}\\right)\\right]}\\\\ {\\displaystyle\\leq\\left(1-\\frac{\\eta\\mu}{2}\\right)\\mathbb{E}\\left[\\Psi_{k}\\right]+3\\gamma^{2}n^{2}L_{\\operatorname*{max}}^{2}\\eta\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\sigma_{\\star,m}^{2}+n\\sigma_{\\star}^{2}\\right)+\\frac{2\\eta^{2}(M-C)}{C(M-1)}\\sigma_{\\star}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We clearly outline our contributions in the abstract and introduction, and we also include a dedicated Contributions section. ", "page_idx": 76}, {"type": "text", "text": "Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 76}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Justification: We clearly highlight all assumptions and limitations in the text ", "page_idx": 76}, {"type": "text", "text": "Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 76}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: The main contribution of the paper is its theoretical analysis. We support the paper with necessary definitions, assumptions, and lemmas. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 77}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: The paper is supported by reproducible experiments, with all stochastic elements from pseudo-random generators fixed in advance. For details, see the supplementary materials. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 77}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: We aim to make the paper and all source code for experiments open-sourced to accelerate scientific findings in the field of Federated Learning and Machine Learning in general. For details, see the supplementary materials. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 78}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: We provide detailed guidelines for experiments setup in Appendix and in the folder with experiment source code. For details, see the supplementary materials. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 78}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: We provide detailed guidelines for experiments setup in Appendix and in the folder with experiment source code. For details, see the supplementary materials. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 79}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 79}, {"type": "text", "text": "Justification: The paper provides information on the computer resources in Appendix. Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 79}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 79}, {"type": "text", "text": "Justification: Our research focuses on mathematical objects and does not involve human subjects or participants. The data used for our experiments consists of publicly available datasets. Our work does not explicitly address or examine the social implications of applying this research in practice. ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 79}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 79}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 79}, {"type": "text", "text": "Justification: Our work operates on mathematical objects, and the essence of our work provides a new optimization algorithm. Because of theoretical nature of our work the impact discussion is not applied. ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 80}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 80}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 80}, {"type": "text", "text": "Justification: Our paper provides an optimization algorithm with the required theory. The data or models are not output assets of our work. ", "page_idx": 80}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 80}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 80}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 80}, {"type": "text", "text": "Justification: We provide references for used datasets and deep learning models used in experiments. ", "page_idx": 80}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets. \u00b7 The authors should cite the original paper that produced the code package or dataset. \u00b7 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 80}, {"type": "text", "text": "\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 81}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 81}, {"type": "text", "text": "Justification: The output assets of our paper is Algorithm and Source code for experiments. Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetis used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 81}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 81}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 81}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 81}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 81}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 81}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 81}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 82}]