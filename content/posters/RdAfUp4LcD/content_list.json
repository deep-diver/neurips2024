[{"type": "text", "text": "Linear Mode Connectivity in Differentiable Tree Ensembles ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Linear Mode Connectivity (LMC) refers to the phenomenon that performance   \n2 remains consistent for linearly interpolated models in the parameter space. For   \n3 independently optimized model pairs from different random initializations, achiev  \n4 ing LMC is considered crucial for validating the stable success of the non-convex   \n5 optimization in modern machine learning models and for facilitating practical   \n6 parameter-based operations such as model merging. While LMC has been achieved   \n7 for neural networks by considering the permutation invariance of neurons in each   \n8 hidden layer, its attainment for other models remains an open question. In this   \n9 paper, we first achieve LMC for soft tree ensembles, which are tree-based differen  \n10 tiable models extensively used in practice. We show the necessity of incorporating   \n11 two invariances: subtree flip invariance and splitting order invariance, which do   \n12 not exist in neural networks but are inherent to tree architectures, in addition to   \n13 permutation invariance of trees. Moreover, we demonstrate that it is even possible   \n14 to exclude such additional invariances while keeping LMC by designing decision   \n15 list-based tree architectures, where such invariances do not exist by definition. Our   \n16 findings indicate the significance of accounting for architecture-specific invariances   \n17 in achieving LMC. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 A non-trivial empirical characteristic of modern machine learning models trained using gradient   \n20 methods is that models trained from different random initializations could become functionally   \n21 almost equivalent, even though their parameter representations differ. If the outcomes of all training   \n22 sessions converge to the same local minima, this empirical phenomenon can be understood. However,   \n23 considering the complex non-convex nature of the loss surface, the optimization results are unlikely to   \n24 converge to the same local minima. In recent years, particularly within the context of neural networks,   \n25 the transformation of model parameters while preserving functional equivalence has been explored by   \n26 considering the permutation invariance of neurons in each hidden layer [1, 2]. Notably, only a slight   \n2 performance degradation has been observed when using weights derived through linear interpolation   \n28 between permuted parameters obtained from different training processes [3, 4]. This demonstrates   \n2 that the trained models reside in different, yet functionally equivalent, local minima. This situation is   \n3 referred to as Linear Mode Connectivity (LMC) [5]. From a theoretical perspective, LMC is crucial   \n3 for supporting the stable and successful application of non-convex optimization. In addition, LMC   \n32 also holds significant practical importance, enabling techniques such as model merging [6, 7] by   \n33 weight-space parameter averaging.   \n34 Although neural networks are most extensively studied among the models trained using gradient   \n35 methods, other models also thrive in real-world applications. A representative is tree ensemble models,   \n36 such as random forests [8]. While they are originally trained by not gradient but greedy algorithms,   \n37 differentiable soft tree ensembles, which learn parameters of the entire model through gradient-based   \n38 optimization, have recently been actively studied. Not only empirical studies regarding accuracy   \n39 and interpretability [9\u201311], but also theoretical analyses have been performed [12, 13]. Moreover,   \n40 the differentiability of soft trees allows for integration with various deep learning methodologies,   \n41 including fine-tuning [14], dropout [15], and various stochastic gradient descent methods [16, 17].   \n42 Furthermore, the soft tree represents the most elementary form of a hierarchical mixture of experts [18\u2013   \n43 20]. Investigating soft tree models not only advances our understanding of this particular structure   \n44 but also contributes to broader research into essential technological components critical for the   \n45 development of large-scale language models [21].   \n46 A research question that we tackle in this paper   \n47 is: \u201cCan LMC be achieved for soft tree ensem  \n48 bles?\u201d. Our empirical results, which are high  \n49 lighted with a green line in the top left panel   \n50 of Figure 1, clearly show that the answer is   \n51 \u201cYes\u201d. This plot shows the variation in test accu  \n52 racy when interpolating weights of soft oblivi  \n53 ous trees, perfect binary soft trees with shared   \n54 parameters at each depth, trained from differ  \n55 ent random initializations. The green line is   \n56 obtained by our method introduced in this pa  \n57 per, where there is almost zero performance   \n58 degradation. Furthermore, as shown in the bot  \n59 tom left panel of Figure 1, the performance can   \n60 even improve when interpolating between mod  \n61 els trained on split datasets.   \n62 The key insight is that, when performing interpolation between two model parameters, considering   \n63 B only tree permutation invariance, which corresponds to the permutation invariance of neural networks,   \n64 is not sufficient to achieve LMC, as shown in the orange lines in the plots. An intuitive understanding   \n65 of this situation is also illustrated in the right panel of Figure 1. To achieve LMC, that is, the green   \n66 lines, we show that two additional invariances beyond tree permutation, subtree flip invariance and   \n67 splitting order invariance, which inherently exist for tree architectures, should be accounted for.   \n68 Moreover, we demonstrate that it is possible to exclude such additional invariances while preserving   \n69 LMC by modifying tree architectures. We realize such an architecture based on a decision list, a   \n70 binary tree structure where branches extend in only one direction. By designating one of the terminal   \n71 leaves as an empty node, we introduce a customized decision list that omits both subtree filp invariance   \n72 and splitting order invariance, and empirically show that this can achieve LMC by considering only   \n73 tree permutation invariance. Since incorporating additional invariances is computationally expensive,   \n74 we can efficiently perform weight-space averaging in model merging on our customized decision   \n75 lists. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/2179103b89bc25522c3042bfe22ad55c57229d8e11b2050d27ff4e344dd3a3d6.jpg", "img_caption": ["Figure 1: A representative experimental result on the MiniBooNE [22] dataset (left) and conceptual diagram of the LMC for tree ensembles (right). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "76 Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "77 \u2022 First achievement of LMC for tree ensembles with accounting for additional invariances beyond   \n78 tree permutation.   \n79 \u2022 Development of a decision list-based tree architecture that does not involve the additional invari  \n80 ances.   \n81 \u2022 A thorough empirical investigation of LMC across various tree architectures, invariances, and   \n82 real-world datasets. ", "page_idx": 1}, {"type": "text", "text": "83 2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "84 We prepare the basic concepts of LMC and soft tree ensembles. ", "page_idx": 1}, {"type": "text", "text": "85 2.1 Linear Mode Connectivity ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "86 Let us consider two models, $A$ and $B$ , that have the same architecture. In the context of evaluating   \n87 LMC, the concept of a \u201cbarrier\u201d is frequently used [4, 23]. Let $\\Theta_{A},\\Theta_{B}\\,\\in\\,\\mathbb{R}^{P}$ be vectorized   \n88 parameters of models $A$ and $B$ , respectively, for $P$ parameters. Assume that $\\mathcal{C}:\\mathbb{R}^{P}\\rightarrow\\mathbb{R}$ measures   \n89 the performance of the model, such as accuracy, given its parameter vector. If higher values of $\\mathcal C(\\cdot)$ ", "page_idx": 1}, {"type": "text", "text": "90 mean better performance, the barrier between two parameter vectors $\\Theta_{A}$ and $\\Theta_{B}$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{B}(\\Theta_{A},\\Theta_{B})=\\operatorname*{sup}_{\\lambda\\in[0,1]}\\left[\\lambda\\mathcal{C}(\\Theta_{A})+(1-\\lambda)\\mathcal{C}(\\Theta_{B})-\\mathcal{C}(\\lambda\\Theta_{A}+(1-\\lambda)\\Theta_{B})\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "91 We can simply reverse the subtraction order if lower values of $\\mathcal C(\\cdot)$ mean better performance like loss. ", "page_idx": 2}, {"type": "text", "text": "92 Several techniques have been developed to reduce barriers by transforming parameters while pre  \n93 serving functional equivalence. Two main approaches are activation matching (AM) and weight   \n94 matching (WM). AM takes the behavior of model inference into account, while WM simply com  \n95 pares two models using their parameters. The validity of both AM and WM has been theoretically   \n96 supported [24]. Numerous algorithms are available for implementing AM and WM. For instance, [4]   \n97 uses a formulation based on the Linear Assignment Problem (LAP) to find suitable permutations,   \n98 while [23] employs a differentiable formulation that allows for the optimization of permutations using   \n99 gradient-based methods.   \n100 Existing research has focused exclusively on neural network architectures such as multi-layer per  \n101 ceptrons (MLP) and convolutional neural networks (CNN). No study has been conducted from the   \n102 perspective of linear mode connectivity for soft tree ensembles. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "103 2.2 Soft Tree Ensemble ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 Unlike typical hard decision trees, which explicitly determine the data flow to the right or left at each   \n105 splitting node, soft trees represent the proportion of data flowing to the right or left as continuous   \n106 values between 0 and 1. This approach enables a differentiable formulation.   \n107 We use a sigmoid function, $\\sigma:\\mathbb{R}\\,\\rightarrow\\,(0,1)$ to formulate a function $\\mu_{m,\\ell}(\\pmb{x}_{i},\\pmb{w}_{m},\\pmb{b}_{m})\\,:\\,\\mathbb{R}^{F}\\,\\times$   \n108 $\\mathbb{R}^{F\\times N}\\times\\mathbb{R}^{\\bar{1}\\times N}\\rightarrow(0,1)$ that represents the proportion of the ith data point $\\pmb{x}_{i}$ flowing to the \u2113th   \n109 leaf of the $m$ th tree as a result of soft splittings: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu_{m,\\ell}(\\boldsymbol{x}_{i},\\boldsymbol{w}_{m},\\boldsymbol{b}_{m})\\!=\\!\\prod_{n=1}^{N}\\underbrace{\\sigma(\\boldsymbol{w}_{m,n}^{\\top}\\boldsymbol{x}_{i}+b_{m,n})}_{\\mathrm{flow~to~theleft}}^{\\mathbb{I}_{\\ell\\sim}n}\\!\\!\\underbrace{(1-\\sigma(\\boldsymbol{w}_{m,n}^{\\top}\\boldsymbol{x}_{i}+b_{m,n}))}_{\\mathrm{flow~to~the~right}}^{\\mathbb{I}_{n\\sim}\\ell},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "110 where $\\mathcal{N}$ denotes the number of splitting nodes in each tree. The parameters $\\pmb{w}_{m,n}\\,\\in\\,\\mathbb{R}^{F}$ and   \n111 $b_{m,n}\\in\\mathbb{R}$ correspond to the feature selection mask and splitting threshold value for nth node in a   \n112 mth tree, respectively. The expression $\\mathbb{1}_{\\ell\\swarrow n}$ (resp. $\\mathbb{1}_{n\\setminus\\ell})$ is an indicator function that returns 1 if the   \n113 \u2113th leaf is positioned to the left (resp. right) of a node $n$ , and 0 otherwise.   \n114 If parameters are shared across all splitting nodes at the same depth, such perfect binary trees are   \n115 called oblivious trees. Mathematically, $\\pmb{w}_{m,n}=\\pmb{w}_{m,n^{\\prime}}$ and $b_{m,n}=b_{m,n^{\\prime}}$ for any nodes $n$ and $n^{\\prime}$ at   \n116 the same depth in an oblivious tree. Oblivious trees can significantly reduce the number of parameters   \n117 from an exponential to a linear order of the tree depth, and they are actively used in practice [9, 11].   \n118 To classify $C$ categories, the output of the mth tree is computed by the function $f_{m}\\ :\\ \\mathbb{R}^{F}\\ \\times$   \n119 $\\mathbb{R}^{F\\times N}\\times\\mathbb{R}^{1\\times N}\\times\\mathbb{R}^{C\\times\\dot{C}}\\rightarrow\\mathbb{R}^{C}$ as sum of the leaf parameters $\\pi_{m,\\ell}$ weighted by the outputs of   \n120 $\\mu_{m,\\ell}(\\pmb{x}_{i},\\pmb{w}_{m},\\pmb{b}_{m})$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{m}(\\pmb{x}_{i},\\pmb{w}_{m},\\pmb{b}_{m},\\pi_{m})=\\sum_{\\ell=1}^{\\mathcal{L}}\\pi_{m,\\ell}\\mu_{m,\\ell}(\\pmb{x}_{i},\\pmb{w}_{m},\\pmb{b}_{m}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 where $\\mathcal{L}$ is the number of leaves in a tree. By combining this function for $M$ trees, we realize the   \n122 function $f:\\mathbb{R}^{F}\\times\\mathbb{R}^{M\\times F\\times N}\\times\\mathbb{R}^{M\\times1\\times N}\\times\\check{\\mathbb{R}}^{M\\times C\\times\\mathcal{L}}\\rightarrow\\mathbb{R}^{C}$ as an ensemble model consisting of   \n123 $M$ trees: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{i},\\pmb{w},b,\\pi)=\\sum_{m=1}^{M}f_{m}(\\pmb{x}_{i},\\pmb{w}_{m},b_{m},\\pi_{m}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 with the parameters $\\pmb{w}\\,=\\,(\\pmb{w}_{1},\\pmb{\\ldots},\\pmb{w}_{M})$ , $\\pmb{b}=(b_{1},\\ldots,b_{M})$ , and $\\pmb{\\pi}\\,=\\,(\\pmb{\\pi}_{1},\\dots,\\pmb{\\pi}_{M})$ being ran  \n125 domly initialized.   \n126 Despite the apparent differences, there are correspondences between MLPs and soft tree ensemble   \n127 models. The formulation of a soft tree ensemble with $D=1$ is: ", "page_idx": 2}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/3d283a1297ae8134f9376e6b21d0e8d76f39d6e2f51ce95cee6ae6a1dd86eaa5.jpg", "img_caption": ["Figure 2: (a) Subtree flip invariance. (b) Splitting order invariance for an oblivious tree. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\boldsymbol{x}_{i},\\boldsymbol{w},\\boldsymbol{b},\\pi)=\\displaystyle\\sum_{m=1}^{M}\\Big(\\sigma(\\boldsymbol{w}_{m,1}^{\\top}\\boldsymbol{x}_{i}+b_{m,1})\\pi_{m,1}+(1-\\sigma(\\boldsymbol{w}_{m,1}^{\\top}\\boldsymbol{x}_{i}+b_{m,1}))\\pi_{m,2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{m=1}^{M}\\Big(\\big(\\pi_{m,1}-\\pi_{m,2}\\big)\\sigma(\\boldsymbol{w}_{m,1}^{\\top}\\boldsymbol{x}_{i}+b_{m,1})+\\pi_{m,2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "128 When we consider the correspondence between $\\pi_{m,1}-\\pi_{m,2}$ in tree ensembles and second layer   \n129 weights in the two-layer perceptron, the tree ensembles model matches to the two-layer perceptron. It   \n130 is clear from the formulation that the permutation of hidden neurons in a neural network corresponds   \n131 to the rearrangement of trees in a tree ensemble. ", "page_idx": 3}, {"type": "text", "text": "132 3 Invariances Inherent to Tree Ensembles ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "133 In this section, we discuss additional invariances inherent to trees (Section 3.1) and introduce a   \n134 matching strategy specifically for tree ensembles (Section 3.2). We also show that the presence of   \n135 additional invariances varies depending on the tree structure, and we present tree structures where no   \n136 additional invariances beyond tree permutation exist (Section 3.3). ", "page_idx": 3}, {"type": "text", "text": "137 3.1 Parameter modification processes that maintains functional equivalence in tree ensembles ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "138 First, we clarify what invariances should be considered for tree ensembles, which are expected to   \n139 reduce the barrier significantly if taken into account. When we consider perfect binary trees, there are   \n140 three types of invariance:   \n141 \u2022 Tree permutation invariance. In Equation (4), the behavior of the function does not change even   \n142 if the order of the $M$ trees is altered. This corresponds to the permutation of internal nodes in   \n143 neural networks, which has been a subject of active interest in previous studies on LMC.   \n144 \u2022 Subtree flip invariance. When the left and right subtrees are swapped simultaneously with the   \n145 inversion of the inequality sign at the split, the functional behavior remains unchanged, which we   \n146 refer to subtree filp invariance. Figure 2(a) presents a schematic diagram of this invariance, which   \n147 is not found in neural networks but is unique to binary tree-based models. Since $\\sigma(-c)=1-\\sigma(c)$   \n148 for $c\\in\\mathbb R$ due to the symmetry of sigmoid, the inversion of the inequality is achieved by inverting   \n149 the signs of ${\\pmb w}_{m,n}$ and $b_{m,n}$ . [25] also focused on the sign of weights, but in a different way from   \n150 ours. They pay attention to the amount of change from the parameters at the start of fine-tuning,   \n151 rather than discussing the sign of the parameters.   \n152 Splitting order invariance. Oblivious trees share parameters at the same depth, which means   \n153 that the decision boundaries are straight lines without any bends. With this characteristic, even if   \n154 the splitting rules at different depths are swapped, functional equivalence can be achieved if the   \n155 positions of leaves are also swapped appropriately as shown in Figure 2(b). This invariance does   \n156 not exist for non-oblivious perfect binary trees without parameter sharing, as the behavior of the   \n157 decision boundary varies depending on the splitting order.   \n158 Note that MLPs also have an additional invariance beyond just permutation. Particularly in MLPs   \n159 that employ ReLU as an activation function, the output of each layer changes linearly with a zero   \n160 crossover. Therefore, it is possible to modify parameters without changing functional behavior by   \n161 multiplying the weights in one layer by a constant and dividing the weights in the previous layer by   \n162 the same constant. However, since the soft tree is based on the sigmoid function, this invariance does   \n163 not apply. Previous studies [3, 4, 23] have consistently achieved significant reductions in barriers   \n164 without accounting for this scale invariance. One potential reason is that changes in parameter scale   \n165 are unlikely due to the nature of optimization via gradient descent. Conversely, when we consider   \n166 additional invariances inherent to trees, the scale is equivalent to the original parameters. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "167 3.2 Matching Strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "168 Here, we propose a matching strategy for bi  \n169 nary trees. When considering invariances, it   \n170 is necessary to compare multiple functionally   \n171 equivalent trees and select the most suitable one   \n172 for achieving LMC. Although comparing tree   \n173 parameters is a straightforward approach, since   \n174 the contribution of all the parameters in a tree is   \n175 not equal, we apply weighting for each node for   \n176 better matching. By interpreting a tree as a rule   \n177 set with shared parameters as shown in Figure 3,   \n178 we determine the weight of each splitting node   \n179 by counting the number of leaves to which the node affects. For example, in the case of the left   \n180 example in Figure 3, the root node affects eight leaves, nodes at depth 2 affect four leaves, and nodes   \n181 at depth 3 affect two leaves. This strategy can apply to even trees other than perfect binary trees. For   \n182 example, in the right example of Figure 3, the root node affects four leaves, a node at depth 2 affects   \n183 three leaves, and a node at depth 3 affects two leaves.   \n184 In this paper, we employ the LAP, which is used as a standard benchmark [4] for matching algorithms.   \n185 The procedures for AM and WM are as follows. Detailed algorithms (Algorithms 1 and 2) are   \n186 described in Section A in the supplementary material.   \n187 \u2022 Activation Matching (Algorithm 1). In trees, there is nothing that directly corresponds to the   \n188 activations in neural networks. However, by treating the output of each individual tree as an   \n189 activation value of a neural network, it is possible to optimize the permutation of trees while   \n190 examining their output similarities. Regarding subtree flip and splitting order invariances, it is   \n191 possible to find the optimal pattern from all the possible patterns of filps and changes in the splitting   \n192 order. Since the tree-wise output remains unchanged, the similarity between each tree, generated   \n193 by considering additional invariances, and the target tree is evaluated based on the inner product of   \n194 parameters while applying node-wise weighting.   \n195 \u2022 Weight Matching (Algorithm 2). Similar to AM, WM also involves applying weighting while   \n196 extracting the optimal pattern by exploring possible flipping and ordering patterns. Although it is   \n197 necessary to solve the LAP multiple times for each layer in MLPs [4], tree ensembles require only   \n198 a single run of the LAP since there are no layers.   \n199 The time complexity of solving the LAP is $\\mathcal{O}(M^{3})$ using a modified Jonker-Volgenant algorithm   \n200 without initialization [26], implemented in SciPy [27], where $M$ is the number of trees. If only   \n201 considering tree permutation, this process needs to be performed only once in both WM and AM.   \n202 However, when considering additional invariances, we need to solve the LAP for each pattern   \n203 generated by considering these additional invariances. In a non-oblivious perfect binary tree with   \n204 depth $D$ , there are $2^{D}-1$ splitting nodes, leading to $2^{2^{D}-1}$ possible combinations of sign flips.   \n205 Additionally, in the case of oblivious trees, there are $D!$ different patterns of splitting order invariance.   \n206 Therefore, for large values of $D$ , conducting a brute-force search becomes impractical.   \n207 In Section 3.3, we will discuss methods to eliminate additional invariance by adjusting the tree   \n208 structure. This enables efficient matching even for deep models. Additionally, in Section 4.2, we   \n209 will present numerical experiment results and discuss that the practical motivation to apply these   \n210 algorithms is limited when targeting deep perfect binary trees.   \n212 In previous subsections, tree architectures are   \n213 fixed to perfect binary trees as they are most   \n214 commonly and practically used in soft trees.   \n215 However, tree architectures can be flexible as   \n216 we have shown in the right example in Figure 3,   \n217 and here we show that we can specifically de  \n218 sign tree architecture that has neither the subtree   \n219 flip nor splitting order invariances. This allows   \n220 efficient matching as considering such two in  \n221 variances is computationally expensive.   \n222 Our idea is to modify a decision list shown on   \n223 the left side of Figure 4, which is a tree structure   \n224 where branches extend in only one direction.   \n225 Due to this asymmetric structure, the number of   \n226 parameters does not increase exponentially with   \n227 the depth, and the splitting order invariance does   \n228 not exist. Moreover, subtree filp invariance also   \n229 does not exist for any internal nodes except for   \n230 the terminal splitting node, as shown in the left   \n231 side of Figure 4. To completely remove this invariance, we virtually eliminate one of the terminal   \n232 leaves by leaving the node empty, that is, a fixed prediction value of zero, as shown on the right   \n233 side of Figure 4. Therefore only permutation invariance exists for our proposed architecture. We   \n234 summarize invariances inherent to each model architecture in Table 1. ", "page_idx": 4}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/47a70d0d1c1fd31ab6fd0beeff84862ce972777672a883108822c1d285f31a6d.jpg", "img_caption": ["Figure 3: Weighting strategy. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/8e9523d47418d21864403b7ffcc1e9cc867c2772bf0ed4f0d7e226a31e6c8a10.jpg", "img_caption": ["Figure 4: Tree architecture where neither subtree filp invariance nor splitting order invariance exists. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/8ed057fbb83f0baa767d4d5ed1d4aca4560b20e45e09f8fe920883a17a272739.jpg", "table_caption": ["Table 1: Invariances inherent to each model architecture. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "235 4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "236 We empirically evaluate barriers in soft tree ensembles to examine LMC. ", "page_idx": 5}, {"type": "text", "text": "237 4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "238 Datasets. In our experiments, we employed Tabular-Benchmark [28], a collection of tabular   \n239 datasets suitable for evaluating tree ensembles. Details of datasets are provided in Section B in the   \n240 supplementary material. As proposed in [28], we randomly sampled 10, 000 instances for train and   \n241 test data from each dataset. If the dataset contains fewer than $20,000$ instances, they are randomly   \n242 divided into halves for train and test data. We applied quantile transformation to each feature and   \n243 standardized it to follow a normal distribution.   \n244 Hyperparameters. We used three different learning rates $\\eta\\in\\{0.01,0.001,0.0001\\}$ and adopted the   \n245 one that yields the highest training accuracy for each dataset. The batch size is set at 512. It is known   \n246 that the optimal settings for the learning rate and batch size are interdependent [29]. Therefore, it is   \n247 reasonable to fix the batch size while adjusting the learning rate. During AM, we set the amount of   \n248 data used for random sampling to be the same as the batch size, thus using 512 samples to measure the   \n249 similarity of the tree outputs. As the number of trees $M$ and their depths $D$ vary for each experiment,   \n250 these details will be specified in the experimental results section. During training, we minimized   \n251 cross-entropy using Adam [16] with its default hyperparameters1. Training is conducted for 50   \n252 epochs. To measure the barrier using Equation (1), experiments were conducted by interpolating   \n253 between two models with $\\lambda\\in\\{0,1/24,\\ldots,23/24,1\\}$ , which has the same granularity as in [4].   \n254 Randomness. We conducted experiments with five different random seed pairs: $r_{A}\\in\\{1,3,5,7,9\\}$   \n255 and $r_{B}\\in\\{2,4,6,8,10\\}$ . As a result, the initial parameters and the contents of the data mini-batches   \n256 during training are different in each training. In contrast to spawning [5] that branches off from the   \n257 exact same model partway through, we used more challenging practical conditions. The parameters   \n258 $w,b$ , and $\\pi$ were randomly initialized using a uniform distribution, identical to the procedure for a   \n259 fully connected layer in the $\\mathrm{MLP}^{2}$ .   \n260 Resources. All experiments were conducted on a system equipped with an Intel Xeon E5-2698 CPU   \n261 at 2.20 GHz, 252 GB of memory, and Tesla V100-DGXS-32GB GPU, running Ubuntu Linux (version   \n262 4.15.0-117-generic). The reproducible PyTorch [30] implementation is provided in the supplementary   \n263 material. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/d41a23fe63cb1f091a1251d07345e47e2fb37e7798b449fd4f3147d2ee9a29f1.jpg", "img_caption": ["Figure 5: Barriers averaged across 16 datasets with respect to considered invariances for nonoblivious (top row) and oblivious (bottom row) trees. The error bars show the standard deviations of 5 executions. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/70e572491d87700fc8d41a212fadd84739d30898156cbad312fe2a712083d3eb.jpg", "img_caption": ["Figure 6: Interpolation curves of test accuracy for oblivious trees on 16 datasets from TabularBenchmark [28]. Two model pairs are trained with on the same dataset. The error bars show the standard deviations of 5 executions. We used $M=256$ trees with a depth $D=2$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "264 4.2 Results for Perfect Binary Trees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "265 Figure 5 shows how the barrier between two perfect binary tree model pairs changes in each operation.   \n266 The vertical axis of each plot in Figure 5 shows the averaged barrier over datasets for each considered   \n267 invariance. The results for both the oblivious and non-oblivious trees are plotted separately in a   \n268 vertical layout. The panels on the left display the results when the depth $D$ of the tree varies, keeping   \n269 $M=256$ constant. The panels on the right show the results when the number of trees $M$ varies, with   \n270 $D$ fixed at 2. For both oblivious and non-oblivious trees, we observed that the barrier significantly   \n271 decreases as the considered invariances increase. Focusing on the test data results, after accounting for   \n272 various invariances, the barrier is nearly zero, indicating that LMC has been achieved. In particular,   \n273 the difference between the case of only permutation and the case where additional invariances are   \n274 considered tends to be larger in the case of AM. This is because parameter values are not used during   \n275 the rearrangement of the tree in AM. Additionally, it has been observed that the barrier increases as   \n276 trees become deeper, and the barrier decreases as the number of trees increases. These behaviors   \n277 correspond to the changes observed in neural networks when the depth varies or when the width of   \n278 hidden layers increases [3, 4]. Figure 6 shows interpolation curves when using AM in oblivious trees   \n279 with $D=2$ and $M=256$ . Other detailed results, such as performance for each dataset, are provided   \n280 in Section C in the supplementary material.   \n281 Furthermore, we conducted experiments with split data following the protocol in [4, 31], where   \n282 the initial split consists of randomly sampled $80\\%$ negative and $20\\%$ positive instances, and the   \n283 second split inverts these ratios. There is no overlap between the two split datasets. We trained two   \n284 model pairs using these separately split datasets and observed an improvement in performance by   \n285 interpolating their parameters. Figure 7 illustrates the interpolation curves under AM in oblivious   \n286 trees with parameters $D=2$ and $M=256$ . We can observe that considering additional invariances   \n287 improves performance after interpolation. Note that the data split is configured to remain consistent   \n288 even when the training random seeds differ. Detailed results for each dataset using WM or AM are   \n289 provided in Section C of the supplementary material.   \n290 Table 2 compares the average test barriers of an   \n291 MLP with a ReLU activation function, whose   \n292 width is equal to the number of trees, $M=256$ .   \n293 The procedure for MLPs follows that described   \n294 in Section 4.1. The permutation for MLPs is   \n295 optimized using the method described in [4].   \n296 Since [4] indicated that WM outperforms AM   \n297 in neural networks, WM was used for the com  \n298 parison. Overall, tree models exhibit smaller   \n299 barriers compared to MLPs while keeping sim  \n300 ilar accuracy levels. It is important to note that   \n301 MLPs with $D>1$ tend to have more parameters   \n302 at the same depth compared to trees, leading to   \n303 more complex optimization landscapes. Nev  \n304 ertheless, the barrier for the non-oblivious tree   \n305 at $D\\,=\\,3$ is smaller than that for the MLP at   \n306 ${\\cal D}\\,=\\,2$ , even with more parameters. Further  \n307 more, at the same depth of $D=1$ , tree models   \n308 have a smaller barrier. Here, the model size is   \n309 evaluated using $F=44$ , the average input fea  \n310 ture size of 16 datasets used in the experiments.   \n311 In Section 3.2, we have shown that considering additional invariances for deep perfect binary trees   \n312 is computationally challenging, which may suggest developing heuristic algorithms for deep trees.   \n313 However, we consider it is rather a low priority, supported by our observations that the barrier tends   \n314 to increase as trees deepen even if we consider invariances. This trend indicates that deep models are   \n315 fundamentally less important for model merging considerations. Furthermore, deep perfect binary   \n316 trees are rarely used in practical scenarios. [12] have demonstrated that generalization performance   \n317 degrades with increasing depth in perfect binary trees due to the degeneracy of the Neural Tangent   \n318 Kernel (NTK) [32]. This evidence further supports the preference for shallow perfect binary trees,   \n319 and increasing the number of trees can enhance the expressive power while reducing barriers. ", "page_idx": 6}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/d72b79f8b5b850ed4ab4c265d20a9ece6ae8d7425456b3647674a10c8deeeb3a.jpg", "img_caption": ["Figure 7: Interpolation curves of test accuracy for oblivious trees on 16 datasets from TabularBenchmark [28]. Two model pairs are trained on split datasets with different class ratios. The error bars show the standard deviations of 5 executions. We used $M=256$ trees with a depth $D=2$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/f8e671dfc34d1226beab757de4a2a06ced7f3c0fad350964a7f3c35931d13513.jpg", "table_caption": ["Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/ebdf8a931dce1afdd4ebb8b24d11ab9baf373b56e613c331e18dcef7a04b6f1f.jpg", "table_caption": ["Table 3: Barriers averaged for 16 datasets under WM with $D=2$ and $M=256$ "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/ce285382895bcfdd62c3436eec66b4214b860f8549b75d740a1af2f13324b034.jpg", "table_caption": ["Table 4: Barriers averaged for 16 datasets under AM with $D=2$ and $M=256$ "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "320 4.3 Results for Decision Lists ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "321 We present empirical results of the original de  \n322 cision lists and our modified decision lists, as   \n323 shown in Figure 4. As we have shown in Table 1,   \n324 they have fewer invariances.   \n325 Figure 8 illustrates barriers as a function of   \n326 depth, considering only permutation invariance,   \n327 with $M$ fixed at 256. In this experiment, we   \n328 have excluded non-oblivious trees from compar  \n329 ison as the number of their parameters exponen  \n330 tially increases as trees deepen, making them   \n331 infeasible computation. Our proposed modified   \n332 decision lists reduce the barrier more effectively   \n333 than both oblivious trees and the original de  \n334 cision lists. However, barriers of the modified   \n35 decision lists are still larger than those obtained by considering additional invariances with perfect   \n36 binary trees. Tables 3 and 4 show the averaged barriers for 16 datasets, with $D=2$ and $M=256$ .   \n37 Although barriers of modified decision lists are small when considering only permutations (Perm),   \n38 perfect binary trees such as oblivious trees with additional invariances (Ours) exhibit smaller barriers,   \n9 which supports the validity of using oblivious trees as in [9, 11]. To summarize, when considering   \n0 the practical use of model merging, if the goal is to prioritize efficient computation, we recommend   \n1 using our proposed decision list. Conversely, if the goal is to prioritize barriers, it would be preferable   \n42 to use perfect binary trees, which have a greater number of invariant operations that maintain the   \n43 functional behavior. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/0dbc10493f9b484cc3488ad6364955cb5d8c2239e1743bf9967fd7adccb3ad70.jpg", "img_caption": ["Figure 8: Averaged barrier for 16 datasets as a function of tree depth. The error bars show the standard deviations of 5 executions. The solid line represents the barrier in train accuracy, while the dashed line represents the barrier in test accuracy. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "344 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "345 We have presented the first investigation of LMC for soft tree ensembles. We have identified additional   \n346 invariances inherent in tree architectures and empirically demonstrated the importance of considering   \n347 these factors. Achieving LMC is crucial not only for understanding the behavior of non-convex   \n348 optimization from a learning theory perspective but also for implementing practical techniques such as   \n349 model merging. By arithmetically combining parameters of differently trained models, a wide range   \n350 of applications such as task-arithmetic [33], including unlearning [34] and continual-learning [35],   \n351 have been explored. Our research extends these techniques to soft tree ensembles that began training   \n352 from entirely different initial conditions. We will leave these empirical investigations for future work.   \n353 This study provides a fundamental analysis of ensemble learning, and we believe that our discussion   \n354 will not have any negative societal impacts. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "355 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "356 [1] Robert Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In   \n357 Advanced Neural Computers. 1990.   \n358 [2] An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. On the Geometry of Feedforward   \n359 Neural Network Error Surfaces. Neural Computation, 1993.   \n360 [3] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The Role of Permutation   \n361 Invariance in Linear Mode Connectivity of Neural Networks. In International Conference on   \n362 Learning Representations, 2022.   \n363 [4] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git Re-Basin: Merging Models   \n364 modulo Permutation Symmetries. In The Eleventh International Conference on Learning   \n365 Representations, 2023.   \n366 [5] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear Mode   \n367 Connectivity and the Lottery Ticket Hypothesis. In Proceedings of the 37th International   \n368 Conference on Machine Learning, 2020.   \n369 [6] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,   \n370 Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig   \n371 Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy   \n372 without increasing inference time. In Proceedings of the 39th International Conference on   \n373 Machine Learning, 2022.   \n374 [7] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task Arithmetic in the   \n375 Tangent Space: Improved Editing of Pre-Trained Models. In Thirty-seventh Conference on   \n376 Neural Information Processing Systems, 2023.   \n377 [8] Leo Breiman. Random Forests. In Machine Learning, 2001.   \n378 [9] Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural Oblivious Decision Ensembles   \n379 for Deep Learning on Tabular Data. In International Conference on Learning Representations,   \n380 2020.   \n381 [10] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The   \n382 Tree Ensemble Layer: Differentiability meets Conditional Computation. In Proceedings of the   \n383 37th International Conference on Machine Learning, 2020.   \n384 [11] Chun-Hao Chang, Rich Caruana, and Anna Goldenberg. NODE-GAM: Neural generalized   \n385 additive model for interpretable deep learning. In International Conference on Learning   \n386 Representations, 2022.   \n387 [12] Ryuichi Kanoh and Mahito Sugiyama. A Neural Tangent Kernel Perspective of Infinite Tree   \n388 Ensembles. In International Conference on Learning Representations, 2022.   \n389 [13] Ryuichi Kanoh and Mahito Sugiyama. Analyzing Tree Architectures in Ensembles via Neural   \n390 Tangent Kernel. In International Conference on Learning Representations, 2023.   \n391 [14] Guolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu. DeepGBM: A Deep Learning   \n392 Framework Distilled by GBDT for Online Prediction Tasks. In Proceedings of the 25th ACM   \n393 SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019.   \n394 [15] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.   \n395 Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine   \n396 Learning Research, 2014.   \n397 [16] Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International   \n398 Conference on Learning Representations, 2015.   \n399 [17] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware Mini  \n400 mization for Efficiently Improving Generalization. In International Conference on Learning   \n401 Representations, 2021.   \n402 [18] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. In   \n403 Proceedings of International Conference on Neural Networks, 1993.   \n404 [19] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E.   \n405 Hinton, and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of  \n406 Experts Layer. In International Conference on Learning Representations, 2017.   \n407 [20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,   \n408 Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models with   \n409 Conditional Computation and Automatic Sharding. In International Conference on Learning   \n410 Representations, 2021.   \n411 [21] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh   \n412 Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile   \n413 Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut   \n414 Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7B, 2023.   \n415 [22] Byron Roe. MiniBooNE particle identification. UCI Machine Learning Repository, 2010.   \n416 [23] Fidel A. Guerrero Pe\u00f1a, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric   \n417 Granger, and Marco Pedersoli. Re-basin via implicit Sinkhorn differentiation. In IEEE/CVF   \n418 Conference on Computer Vision and Pattern Recognition, 2023.   \n419 [24] Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, and Wei Hu. Going Beyond Linear   \n420 Mode Connectivity: The Layerwise Linear Feature Connectivity. In Thirty-seventh Conference   \n421 on Neural Information Processing Systems, 2023.   \n422 [25] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging:   \n423 Resolving interference when merging models. In Thirty-seventh Conference on Neural Informa  \n424 tion Processing Systems, 2023.   \n425 [26] David F. Crouse. On implementing 2D rectangular assignment algorithms. IEEE Transactions   \n426 on Aerospace and Electronic Systems, 2016.   \n427 [27] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David   \n428 Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J.   \n429 van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew   \n430 R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W.   \n431 Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.   \n432 Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul   \n433 van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific   \n434 Computing in Python. Nature Methods, 2020.   \n435 [28] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still   \n436 outperform deep learning on typical tabular data? In Thirty-sixth Conference on Neural   \n437 Information Processing Systems Datasets and Benchmarks Track, 2022.   \n438 [29] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don\u2019t Decay the Learning Rate,   \n439 Increase the Batch Size. In International Conference on Learning Representations, 2018.   \n440 [30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,   \n441 Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas   \n442 Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,   \n443 Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,   \n444 High-Performance Deep Learning Library. In Advances in Neural Information Processing   \n445 Systems, 2019.   \n446 [31] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. REPAIR:   \n447 REnormalizing permuted activations for interpolation repair. In The Eleventh International   \n448 Conference on Learning Representations, 2023.   \n449 [32] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and   \n450 Generalization in Neural Networks. In Advances in Neural Information Processing Systems,   \n451 2018.   \n452 [33] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Ha  \n453 jishirzi, and Ali Farhadi. Editing models with task arithmetic. In The Eleventh International   \n454 Conference on Learning Representations, 2023.   \n455 [34] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am  \n456 manabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.   \n457 In Advances in Neural Information Processing Systems, 2022.   \n458 [35] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan   \n459 Ghasemzadeh. Linear Mode Connectivity in Multitask and Continual Learning. In Inter  \n460 national Conference on Learning Representations, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "462 We present pseudo-code of algorithms for activation matching (Algorithm 1) and weight matching 463 (Algorithm 2). In these algorithms, if there is only one possible pattern for $U\\in\\mathbb{N}$ , which represents 464 the number of possible operations, and the corresponding operation does nothing in particular, it becomes equivalent to simply considering tree permutations. ", "page_idx": 12}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/af220b77d13a40b40d795bc9e50aa2e18d9dc6efd58bbdb141b925096f6ec702.jpg", "table_caption": ["Algorithm 1: Activation matching for soft trees "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Algorithm 2: Weight matching for soft trees ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/e394fb946892800640504cad4c12af323d0643d592d800e49fd5c0d7937caf60.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "465 ", "page_idx": 12}, {"type": "text", "text": "466 Here, we describe the specifications of the notations and functions used in Algorithms 1 and 2. In   \n467 Section 2.1, $\\Theta_{A}$ and $\\mathbf{{\\Theta}}_{B}$ are initially defined as vectors. However, for ease of use, in Algorithms 1   \n468 and 2, $\\Theta_{A}$ and $\\Theta_{B}$ are represented as matrices of size $\\mathbb{R}^{M\\times P_{\\mathrm{Tree}}}$ , where $P_{\\mathrm{Tree}}$ denotes the number of   \n469 parameters in a single tree. Multidimensional array elements are accessed using square brackets [\u00b7].   \n470 For example, for $\\bar{G}\\in\\mathbb{R}^{I\\times J}$ , $G[i]$ refers to the $i$ th slice along the first dimension, and $G[:,j]$ refers   \n471 to the $j$ th slice along the second dimension, with sizes $\\mathbb{R}^{J}$ and $\\mathbb{R}^{I}$ , respectively. Furthermore, it can   \n472 also accept a vector $\\mathbf{\\bar{\\mu}}\\mathbf{\\bar{\\sigma}}\\mathbf{e}\\;\\mathbb{N}^{l}$ as an input. In this case, $G[\\pmb{v}]\\in\\mathbb{R}^{l\\times J}$ . The FLATTEN function converts   \n473 multidimensional input into a one-dimensional vector format. As the LINEARSUMASSIGNMENT   \n474 function, scipy. optimize. linear_sum_assignment3 is used to solve the LAP. In the ADJUSTTREE   \n475 function, the parameters of a tree are modified according to the uth pattern among the enumerated $U$   \n476 patterns. Additionally, in the WEIGHTING function, parameters are multiplied by the square root   \n477 of their weights defined in Section 3.2 to simulate the process of assessing a rule set. If the first   \n478 argument for the UPDATEBESTOPERATION function, the input inner product, is larger than any   \n479 previously input inner product values, then $u^{\\prime}$ is updated with $u$ , the second argument. If not, $\\bar{u^{\\prime}}$   \n480 remains unchanged. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "481 B Dataset ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/d1d401c1fbf9ddd9d465260eb9a58ca3bc62572a09fab49c2ce841a187c6bc7b.jpg", "table_caption": ["Table 5: Summary of the datasets used in the experiments. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "482 C Additional Empirical Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "483 Tables 6, 7, 8 and 9 present the barrier for each dataset with D = 2 and M = 256. By incorporating   \n484 additional invariances, it has been possible to consistently reduce the barriers.   \n485 Tables 10 and 11 detail the characteristics of the barriers in the decision lists for each dataset with   \n486 $D=2$ and $M=256$ . The barriers in the modified decision lists tend to be smaller.   \n487 Tables 12 and 13 show the barrier for each model when only considering permutations with $D=2$   \n488 and $M\\,=\\,256$ . It is evident that focusing solely on permutations leads to smaller barriers in the   \n489 modified decision lists compared to other architectures.   \nFigures 9, 10, 11, 12, 13, 14, 15 and 16 show the interpolation curves of oblivious trees with $D=2$   \n491 and $M=256$ across various datasets and configurations. Significant improvements are particularly   \n492 noticeable in AM, but improvements are also observed in WM. These characteristics are also apparent   \n493 in the non-oblivious trees, as shown in Figures 17, 18, 19, 20, 21, 22, 23 and 24. Regarding split data   \n494 training, the dataset for each of the two classes is initially complete $(100\\%)$ . It is then divided into   \n495 splits of $80\\%$ and $20\\%$ , and $20\\%$ and $80\\%$ , respectively. Each model is trained using these splits.   \n496 Figures 13, 15, 21, and 23 show the training accuracy evaluated using the full dataset ( $100\\%$ for each   \n497 class). ", "page_idx": 13}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/6d0c505e85e71b30da0eed5072ee78769ad86b47d96630bb0a3655841200e629.jpg", "table_caption": ["Table 6: Accuracy barrier for non-oblivious trees with WM. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/caa4f8be619b9123e336bdc3d3d276c7f56400e558ce56a59e933b739758813d.jpg", "table_caption": ["Table 7: Accuracy barrier for non-oblivious trees with AM. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/8b93e3c1bcad245295aaa1c3f3d693b82436af546db095ceac3a97f0b6686e12.jpg", "table_caption": ["Table 8: Accuracy barrier for oblivious trees with WM. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/dc07fd3d2c714a8e755ed8d2955dff83dd3793c700a1b04695fcdc4f61038253.jpg", "table_caption": ["Table 9: Accuracy barrier for oblivious trees with AM. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/7c44b9cd08215f8ee9032a5b8332be01a79d83632decbed98dd250530ebfc0e6.jpg", "table_caption": ["Table 10: Accuracy barrier for decision lists with WM. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/9c61ec9f466e6ba28f0f5df6f6d9321f6fd62fa70c89803081d2ab161b6f4fcd.jpg", "table_caption": ["Table 11: Accuracy barrier for decision lists with AM. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/56ae0e1b3f8f6fa6c2b1a3cd17d1b97906cb92c6fcbe15e2d53eec6b244bece1.jpg", "table_caption": ["Table 12: Training accuracy barrier for permuted models with WM. The numbers in parentheses represent the original accuracy. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "RdAfUp4LcD/tmp/3c73cb14aa3326427209b425dcc54b27a8bf213aeec2df4e61ad38716e75fc9f.jpg", "table_caption": ["Table 13: Training accuracy barrier for permuted models with AM. The numbers in parentheses represent the original accuracy. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/4a96c97df600c17c0e04e12f5d31cec4709fb8c54351580e09de15680ed1a2ff.jpg", "img_caption": ["Figure 9: Interpolation curves of train accuracy for oblivious trees with AM. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/64f07a55eef8632e1c8927baa08f3a0370bab0eb1b0b13a8e24ed833ab6668d8.jpg", "img_caption": ["Figure 10: Interpolation curves of test accuracy for oblivious trees with AM. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/db66ff1dc06cf766bdfa4e7bd98b4e234756d12debbf70035e4c167f1661bb3c.jpg", "img_caption": ["Figure 11: Interpolation curves of train accuracy for oblivious trees with WM. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/396ce4d27f9f015e48aa4752e0fb5a4a28416f3f4c6aa26c42fc29d35745b6b4.jpg", "img_caption": ["Figure 12: Interpolation curves of test accuracy for oblivious trees with WM. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/389e963634a6decc6c9986816e2d148dd69133a8e5bd23cb44a3bf3d62e10abd.jpg", "img_caption": ["Figure 13: Interpolation curves of train accuracy for oblivious trees with AM by use of split dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/3df70ceb216df3b9e5ef593aae2c67bc6fe53c8c0099e5923a5b1f47ebd5b984.jpg", "img_caption": ["Figure 14: Interpolation curves of test accuracy for oblivious trees with AM by use of split dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/b6213932ad4cc9856ceea2eaaa036c825cc6d0ab97bb663ab79d099b14d0c08d.jpg", "img_caption": ["Figure 15: Interpolation curves of train accuracy for oblivious trees with WM by use of split dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/15b9caeb95bb3c5d150bb7337eff58d0736ac4a138d7e7fe8a4643aa3cca482c.jpg", "img_caption": ["Figure 16: Interpolation curves of test accuracy for oblivious trees with WM by use of split dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/c30f1ff61b957320740c363ea4b364fb66f41948a9b23984f1380e925f7f7ca2.jpg", "img_caption": ["Figure 17: Interpolation curves of train accuracy for non-oblivious trees with AM. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/c634a215ccdfdf74f6113b7b92f8618e9404371a76e8d1bd79bab5c3e330d527.jpg", "img_caption": ["Figure 18: Interpolation curves of test accuracy for non-oblivious trees with AM. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/5e40dc7abe817e2682e140b83683720c254cda39033edd6f172041e1c6e8de65.jpg", "img_caption": ["Figure 19: Interpolation curves of train accuracy for non-oblivious trees with WM. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/ad512909fae8199ce4a67ec32dd708763c123a9a945c6e012d564e8542364d50.jpg", "img_caption": ["Figure 20: Interpolation curves of test accuracy for non-oblivious trees with WM. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/88b345705940a211902b48db3e921cff448f69ed61866dc7525d8821d325eafb.jpg", "img_caption": ["Figure 21: Interpolation curves of train accuracy for non-oblivious trees with AM by use of split dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/c734dc0c509a34b5f0017b62cff2f1b74f454dc4675a8e00738cebbb340f2e73.jpg", "img_caption": ["Figure 22: Interpolation curves of test accuracy for non-oblivious trees with AM by use of split dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/13fa3c37e44cd4777e3d704032b09cbf32055146394424548062329053d34ce6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 23: Interpolation curves of train accuracy for non-oblivious trees with WM by use of split dataset. ", "page_idx": 19}, {"type": "image", "img_path": "RdAfUp4LcD/tmp/45769489a667a3f8b59cac2d20a770e1180f86569778974fccd3a5b8bb707cf2.jpg", "img_caption": ["Figure 24: Interpolation curves of test accuracy for non-oblivious trees with WM by use of split dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "498 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "500 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n501 paper\u2019s contributions and scope?   \n502 Answer: [Yes]   \n503 Justification: The abstract and introduction consistently present our research on tree ensem  \n504 bles from LMC perspectives.   \n505 Guidelines:   \n506 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n507 made in the paper.   \n508 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n509 contributions made in the paper and important assumptions and limitations. A No or   \n510 NA answer to this question will not be perceived well by the reviewers.   \n511 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n512 much the results can be expected to generalize to other settings.   \n513 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n514 are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "9 Guidelines:   \n0 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n21 the paper has limitations, but those are not discussed in the paper.   \n2 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n3 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n4 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n5 model well-specification, asymptotic approximations only holding locally). The authors   \n6 should reflect on how these assumptions might be violated in practice and what the   \nimplications would be.   \n8 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n9 only tested on a few datasets or with a few runs. In general, empirical results often   \n0 depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach.   \n2 For example, a facial recognition algorithm may perform poorly when image resolution   \n3 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n34 used reliably to provide closed captions for online lectures because it fails to handle   \n5 technical jargon.   \n6 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n7 and how they scale with dataset size.   \n8 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n9 address problems of privacy and fairness.   \n0 \u2022 While the authors might fear that complete honesty about limitations might be used by   \nreviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n2 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n3 judgment and recognize that individual actions in favor of transparency play an impor  \n4 tant role in developing norms that preserve the integrity of the community. Reviewers   \n5 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 20}, {"type": "text", "text": "46 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "547 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n548 a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "50 Justification: We do not provide theoretical results in this paper.   \n51 Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "562 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "63 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n64 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n65 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The experimental setup is detailed in Section 4.1. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "604 Answer: [Yes]   \n605 Justification: Reproducible source code is provided in the supplementary material.   \n606 Guidelines:   \n607 \u2022 The answer NA means that paper does not include experiments requiring code.   \n608 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n609 public/guides/CodeSubmissionPolicy) for more details.   \n610 \u2022 While we encourage the release of code and data, we understand that this might not be   \n611 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n612 including code, unless this is central to the contribution (e.g., for a new open-source   \n613 benchmark).   \n614 \u2022 The instructions should contain the exact command and environment needed to run to   \n615 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n616 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n617 \u2022 The authors should provide instructions on data access and preparation, including how   \n618 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n619 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n620 proposed method and baselines. If only a subset of experiments are reproducible, they   \n621 should state which ones are omitted from the script and why.   \n622 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n623 versions (if applicable).   \n624 \u2022 Providing as much information as possible in supplemental material (appended to the   \n625 paper) is recommended, but including URLs to data and code is permitted.   \n626 6. Experimental Setting/Details   \n627 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n628 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n629 results?   \n630 Answer: [Yes]   \n631 Justification: The experimental setup is detailed in Section 4.1.   \n632 Guidelines:   \n633 \u2022 The answer NA means that the paper does not include experiments.   \n634 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n635 that is necessary to appreciate the results and make sense of them.   \n636 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n637 material.   \n638 7. Experiment Statistical Significance   \n639 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n640 information about the statistical significance of the experiments?   \n641 Answer: [Yes]   \n642 Justification: We conducted experiments multiple times with different random seeds and   \n643 have reported the results, including the variability.   \n644 Guidelines:   \n645 \u2022 The answer NA means that the paper does not include experiments.   \n646 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n647 dence intervals, or statistical significance tests, at least for the experiments that support   \n648 the main claims of the paper.   \n649 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n650 example, train/test split, initialization, random drawing of some parameter, or overall   \n651 run with given experimental conditions).   \n652 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n653 call to a library function, bootstrap, etc.)   \n654 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n655 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n656 of the mean.   \n657 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n658 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n659 of Normality of errors is not verified.   \n660 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n661 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n662 error rates).   \n663 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n664 they were calculated and reference the corresponding figures or tables in the text.   \n665 8. Experiments Compute Resources   \n666 Question: For each experiment, does the paper provide sufficient information on the com  \n667 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n668 the experiments?   \n669 Answer: [Yes]   \n670 Justification: The computational resources used in our experiment is described in Section 4.1.   \n671 Guidelines:   \n672 \u2022 The answer NA means that the paper does not include experiments.   \n673 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n674 or cloud provider, including relevant memory and storage.   \n675 \u2022 The paper should provide the amount of compute required for each of the individual   \n676 experimental runs as well as estimate the total compute.   \n677 \u2022 The paper should disclose whether the full research project required more compute   \n678 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n679 didn\u2019t make it into the paper).   \n680 9. Code Of Ethics   \n681 Question: Does the research conducted in the paper conform, in every respect, with the   \n682 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n683 Answer: [Yes]   \n684 Justification: We reviewed the NeurIPS Code of Ethics and conducted our research in   \n685 accordance with it.   \n686 Guidelines:   \n687 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n688 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n689 deviation from the Code of Ethics.   \n690 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n691 eration due to laws or regulations in their jurisdiction).   \n692 10. Broader Impacts   \n693 Question: Does the paper discuss both potential positive societal impacts and negative   \n694 societal impacts of the work performed?   \n695 Answer: [Yes]   \n696 Justification: We have addressed societal impact in Section 5.   \n697 Guidelines:   \n698 \u2022 The answer NA means that there is no societal impact of the work performed.   \n699 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n700 impact or why the paper does not address societal impact.   \n701 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n702 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n703 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n704 groups), privacy considerations, and security considerations.   \n705 \u2022 The conference expects that many papers will be foundational research and not tied   \n706 to particular applications, let alone deployments. However, if there is a direct path to   \n707 any negative applications, the authors should point it out. For example, it is legitimate   \n708 to point out that an improvement in the quality of generative models could be used to   \n709 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n710 that a generic algorithm for optimizing neural networks could enable people to train   \n711 models that generate Deepfakes faster.   \n712 \u2022 The authors should consider possible harms that could arise when the technology is   \n713 being used as intended and functioning correctly, harms that could arise when the   \n714 technology is being used as intended but gives incorrect results, and harms following   \n715 from (intentional or unintentional) misuse of the technology.   \n716 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n717 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n718 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n719 feedback over time, improving the efficiency and accessibility of ML).   \n720 11. Safeguards   \n721 Question: Does the paper describe safeguards that have been put in place for responsible   \n722 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n723 image generators, or scraped datasets)?   \n724 Answer: [NA]   \n725 Justification: We provide the source code as supplementary material; however, since the   \n726 experiments concern the fundamental nature of machine learning models, we believe there   \n727 are no risks involved.   \n728 Guidelines:   \n729 \u2022 The answer NA means that the paper poses no such risks.   \n730 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n731 necessary safeguards to allow for controlled use of the model, for example by requiring   \n732 that users adhere to usage guidelines or restrictions to access the model or implementing   \n733 safety filters.   \n734 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n735 should describe how they avoided releasing unsafe images.   \n736 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n737 not require this, but we encourage authors to take this into account and make a best   \n738 faith effort.   \n739 12. Licenses for existing assets   \n740 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n741 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n742 properly respected?   \n743 Answer: [Yes]   \n744 Justification: We have used open datasets, citing them in accordance with their license   \n745 information.   \n746 Guidelines:   \n747 \u2022 The answer NA means that the paper does not use existing assets.   \n748 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n749 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n750 URL.   \n751 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n752 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n753 service of that source should be provided.   \n754 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n755 package should be provided. For popular datasets, paperswithcode.com/datasets   \n756 has curated licenses for some datasets. Their licensing guide can help determine the   \n757 license of a dataset. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of ", "page_idx": 25}, {"type": "text", "text": "759 the derived asset (if it has changed) should be provided.   \n760 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n761 the asset\u2019s creators.   \n762 13. New Assets   \n763 Question: Are new assets introduced in the paper well documented and is the documentation   \n764 provided alongside the assets?   \n765 Answer: [NA]   \n766 Justification: We do not provide any new assets.   \n767 Guidelines:   \n768 \u2022 The answer NA means that the paper does not release new assets.   \n769 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n770 submissions via structured templates. This includes details about training, license,   \n771 limitations, etc.   \n772 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n773 asset is used.   \n774 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n775 create an anonymized URL or include an anonymized zip file.   \n776 14. Crowdsourcing and Research with Human Subjects   \n777 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n778 include the full text of instructions given to participants and screenshots, if applicable, as   \n779 well as details about compensation (if any)?   \n780 Answer: [NA]   \n781 Justification: This paper neither engages in crowdsourcing nor research involving human   \n782 subjects.   \n783 Guidelines:   \n784 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n785 human subjects.   \n786 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n787 tion of the paper involves human subjects, then as much detail as possible should be   \n788 included in the main paper.   \n789 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n790 or other labor should be paid at least the minimum wage in the country of the data   \n791 collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "93 Subjects   \n94 Question: Does the paper describe potential risks incurred by study participants, whether   \n95 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n96 approvals (or an equivalent approval/review based on the requirements of your country or   \n97 institution) were obtained?   \n98 Answer: [NA]   \n99 Justification: This paper neither engages in crowdsourcing nor research involving human   \n00 subjects.   \n01 Guidelines:   \n02 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n03 human subjects.   \n04 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n05 may be required for any human subjects research. If you obtained IRB approval, you   \n06 should clearly state this in the paper.   \n07 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n08 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n09 guidelines for their institution. ", "page_idx": 25}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]