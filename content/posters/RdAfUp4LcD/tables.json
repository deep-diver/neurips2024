[{"figure_path": "RdAfUp4LcD/tables/tables_5_1.jpg", "caption": "Table 1: Invariances inherent to each model architecture.", "description": "The table summarizes the presence or absence of three invariances (permutation, subtree flip, and splitting order) in four different tree architectures: non-oblivious trees, oblivious trees, decision lists, and modified decision lists.  A checkmark (\u2713) indicates the presence of the invariance, while a cross (x) indicates its absence.  Parentheses around a checkmark ((\u2713)) indicate that the invariance is only partially present.", "section": "3 Invariances Inherent to Tree Ensembles"}, {"figure_path": "RdAfUp4LcD/tables/tables_7_1.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function (whose width is equal to the number of trees, M=256), non-oblivious trees, and oblivious trees.  The results are shown for different tree depths (1, 2, and 3).  The table demonstrates that tree models, particularly the oblivious tree with additional invariances considered in the paper, show smaller barriers than the MLPs, suggesting improved linear mode connectivity (LMC). The model size is calculated using F=44 (average input feature size).", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_8_1.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function, whose width is equal to the number of trees (M=256), with non-oblivious and oblivious trees. The procedure for MLPs follows the one described in Section 4.1.  Since it has been shown that weight matching outperforms activation matching for neural networks, weight matching is used for the comparison. The table shows the barrier, accuracy, and model size for each model architecture at different depths (1, 2, and 3).", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_8_2.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function, whose width is equal to the number of trees, M = 256, with non-oblivious trees and oblivious trees.  The procedure for MLPs followed the one described in Section 4.1.  Since [4] indicated that WM outperforms AM in neural networks, WM was used for the comparison.  The table shows the barrier, accuracy, and model size for each model type at depths of 1, 2, and 3.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_12_1.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function, whose width is equal to the number of trees (M=256), with non-oblivious and oblivious trees.  The procedure for MLPs follows that described in Section 4.1 of the paper.  The permutation for MLPs is optimized using the method described in [4].  The table shows the barrier, accuracy, and model size for each model type at depths of 1, 2, and 3.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_12_2.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP (Multilayer Perceptron) with a ReLU (Rectified Linear Unit) activation function, whose width is equal to the number of trees (M=256), with non-oblivious and oblivious trees.  The comparison considers different depths (D) of the trees.  For each model type and depth, the table provides the average barrier (with standard deviation), average accuracy (with standard deviation), and the model size (number of parameters).", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_13_1.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function (whose width is equal to the number of trees, M=256) with non-oblivious and oblivious trees.  The procedure for MLPs follows that described in Section 4.1, and WM is used for the comparison since [4] indicates that WM outperforms AM in neural networks.  The table shows the barrier, accuracy, and model size for each model architecture with tree depths of 1, 2, and 3.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_14_1.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function (whose width is equal to the number of trees, M=256), non-oblivious trees, and oblivious trees.  The results demonstrate the barriers, accuracies, and model sizes across different depths (1, 2, and 3) for each model type.  The table highlights the relative performance of different models in terms of achieving Linear Mode Connectivity (LMC).", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_14_2.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function, whose width is equal to the number of trees, M = 256, with non-oblivious and oblivious trees.  It shows the naive barrier, the barrier after accounting for permutation invariance, and the barrier after considering all invariances for each model type at depths 1, 2 and 3.  Model sizes are also provided for comparison.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_14_3.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function (whose width is equal to the number of trees, M=256), non-oblivious trees, and oblivious trees.  It shows the naive barrier, the barrier after considering only tree permutation invariance, and the barrier after considering all invariances for each tree architecture.  It also includes the average test accuracy and the model size for each architecture.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_14_4.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function (whose width is equal to the number of trees, M=256) with non-oblivious trees and oblivious trees. The procedure for MLPs follows that described in Section 4.1. Since [4] indicated that WM outperforms AM in neural networks, WM was used for the comparison. The table shows the barrier, accuracy, and model size for each model architecture at depths 1, 2, and 3.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_15_1.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function, whose width is equal to the number of trees (M=256), with non-oblivious and oblivious trees.  The procedure for MLPs follows that described in Section 4.1. Since [4] indicated that WM outperforms AM in neural networks, WM was used for the comparison. The table shows the barrier, accuracy, and model size for each model architecture (MLP, non-oblivious trees, and oblivious trees) at depths 1, 2, and 3.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_15_2.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function (whose width is equal to the number of trees, M=256), non-oblivious trees, and oblivious trees.  The procedure for MLPs follows that described in Section 4.1. Since [4] indicated that WM outperforms AM in neural networks, WM was used for the comparison. The table shows that tree models exhibit smaller barriers compared to MLPs while keeping similar accuracy levels. ", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_15_3.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function (whose width is equal to the number of trees, M=256) and tree models.  The table shows that tree models exhibit smaller barriers compared to MLPs, while keeping similar accuracy levels. It also provides model sizes (number of parameters) for each model architecture at depths 1, 2, and 3.", "section": "4.2 Results for Perfect Binary Trees"}, {"figure_path": "RdAfUp4LcD/tables/tables_15_4.jpg", "caption": "Table 2: Barriers, accuracies, and model sizes for MLP, non-oblivious trees, and oblivious trees.", "description": "This table compares the average test barriers of an MLP with a ReLU activation function, whose width is equal to the number of trees, M = 256.  It also shows the accuracy and model size for MLPs, non-oblivious trees, and oblivious trees at different depths (1, 2, and 3). The results highlight that tree models exhibit smaller barriers compared to MLPs while keeping similar accuracy levels.", "section": "4.2 Results for Perfect Binary Trees"}]