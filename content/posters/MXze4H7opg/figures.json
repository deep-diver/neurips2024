[{"figure_path": "MXze4H7opg/figures/figures_1_1.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure compares several methods for pretraining the LLaMA 1B language model on the C4 dataset.  The methods are represented by circles, with the size and color of each circle corresponding to the parameter size of the model. The vertical axis represents the model's perplexity (a measure of how well the model predicts text), and the horizontal axis shows the memory usage.  Methods that result in smaller, lighter circles are preferred since they provide good performance with reduced parameter and memory requirements.  The figure highlights the trade-off between model size, memory, performance, and illustrates the advantages of SLTrain (a method proposed in the paper).", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of the last attention layer of pretrained full-rank LLaMA 60M model on 1.1B tokens. (a): singular value magnitudes of weight matrices where we observe a rapid decay of singular values. (b): Visualization of full-rank pretrained attention output matrix Wo in magnitude and the residual matrix after removing the best rank-r (r = 128) approximation of the Wo by SVD. We observe the magnitudes of the residual vary smoothly across different neuron-neuron interactions. (c): Cumulative density of the residual matrix in magnitude where we include a cut-off fraction at 0.97. We observe 97% entries in the residual matrix have magnitude less than 0.04.", "description": "This figure provides an illustration of the weight matrices in the last attention layer of a pretrained LLaMA 60M model. It shows the singular value distribution (a), visualizes the full-rank matrix and its residual after low-rank approximation (b), and presents the cumulative density of the residual matrix magnitudes (c). The figure supports the argument for using sparse plus low-rank parameterization by demonstrating that a significant portion of the residual matrix has small magnitudes, suitable for sparse representation.", "section": "3 SLTrain: proposed sparse plus low-rank pretraining"}, {"figure_path": "MXze4H7opg/figures/figures_7_1.jpg", "caption": "Figure 3: Actual memory consumption across different model size and algorithms on a single A100 80G GPU.", "description": "This figure shows the actual memory usage (in GB) of different training algorithms across various model sizes (350M, 1B, and 7B parameters) on a single NVIDIA A100 80GB GPU.  The algorithms compared include full-rank Adam training, 8-bit Adam with per-layer updates, 8-bit GaLore with per-layer updates, and 8-bit SLTrain with per-layer updates.  The bar chart visually represents the memory efficiency gains achieved by SLTrain compared to the other methods, especially as model size increases.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/figures/figures_8_1.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure compares the performance of various LLM pretraining methods on the LLaMA 1B model using the C4 dataset. The methods are compared based on three metrics: perplexity, memory usage, and parameter size.  Each method is represented by a circle where the size and color of the circle correspond to the parameter size.  Methods with smaller, lighter circles, located in the lower-left corner (low perplexity, low memory, and low parameter size), are considered more desirable for efficient pretraining.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/figures/figures_14_1.jpg", "caption": "Figure 2: Illustration of the last attention layer of pretrained full-rank LLaMA 60M model on 1.1B tokens. (a): singular value magnitudes of weight matrices where we observe a rapid decay of singular values. (b): Visualization of full-rank pretrained attention output matrix Wo in magnitude and the residual matrix after removing the best rank-r (r = 128) approximation of the Wo by SVD. We observe the magnitudes of the residual vary smoothly across different neuron-neuron interactions. (c): Cumulative density of the residual matrix in magnitude where we include a cut-off fraction at 0.97. We observe 97% entries in the residual matrix have magnitude less than 0.04.", "description": "This figure provides a visualization of the weight matrices in the last attention layer of a pretrained LLaMA 60M model. It shows the singular value distribution, which decays rapidly, indicating a low-rank structure.  The visualization also shows the residual matrix after removing the best rank-128 approximation, highlighting that the remaining values are small and smoothly distributed. The cumulative density plot further emphasizes that most (97%) of these residual values are less than 0.04.", "section": "3 SLTrain: proposed sparse plus low-rank pretraining"}, {"figure_path": "MXze4H7opg/figures/figures_14_2.jpg", "caption": "Figure 2: Illustration of the last attention layer of pretrained full-rank LLaMA 60M model on 1.1B tokens. (a): singular value magnitudes of weight matrices where we observe a rapid decay of singular values. (b): Visualization of full-rank pretrained attention output matrix Wo in magnitude and the residual matrix after removing the best rank-r (r = 128) approximation of the Wo by SVD. We observe the magnitudes of the residual vary smoothly across different neuron-neuron interactions. (c): Cumulative density of the residual matrix in magnitude where we include a cut-off fraction at 0.97. We observe 97% entries in the residual matrix have magnitude less than 0.04.", "description": "This figure shows the singular value distribution and visualization of the residual matrix in the last attention layer of a pretrained LLaMA 60M model.  Subfigure (a) illustrates the rapid decay of singular values, demonstrating the potential for low-rank approximation. Subfigure (b) visualizes the magnitude of the full-rank attention output matrix and the residual after low-rank approximation, showing the smooth variation of residual entries. Subfigure (c) displays the cumulative density of the residual matrix, indicating that 97% of the entries have a magnitude less than 0.04, suggesting that a sparse matrix could effectively approximate the residual.", "section": "3.1 Motivation for sparse plus low-rank parameterization"}, {"figure_path": "MXze4H7opg/figures/figures_15_1.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methdods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure compares several methods for pretraining the LLaMA 1B model on the C4 dataset.  It displays the perplexity (a measure of how well the model predicts the next word), memory usage, and the number of parameters for each method. The size of the circles represents the number of parameters, and the color corresponds to memory usage, while the vertical position shows perplexity. Methods with smaller and lighter circles are better because they offer lower perplexity, lower memory usage and lower parameter counts.", "section": "1 Introduction"}, {"figure_path": "MXze4H7opg/figures/figures_15_2.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure shows a comparison of different methods for pretraining the LLaMA 1B model on the C4 dataset.  Three metrics are visualized: perplexity (a measure of model performance), memory usage (in GB), and the number of parameters (in millions). Each method is represented by a circle, where the circle's size corresponds to the number of parameters, and its color and vertical position indicate memory and perplexity, respectively.  Methods located in the lower left corner represent the best balance of efficiency and performance.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/figures/figures_16_1.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure compares different methods for pretraining the LLaMA 1B model on the C4 dataset.  The methods are evaluated based on three metrics: perplexity (a measure of how well the model predicts the next word in a sequence), memory usage during training, and the number of parameters (which impacts model size and complexity). The visualization uses circles where the size of the circle represents the number of parameters, and the color represents the memory used. Smaller circles located in the lower-left corner (low perplexity, low memory, and few parameters) indicate more efficient and desirable methods for pretraining.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/figures/figures_16_2.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure compares different LLM pretraining methods (Full-Rank, Low-Rank, SLTrain, GaLore, ReLORA) on the perplexity, memory consumption and the number of parameters used for pretraining LLaMA 1B model on the C4 dataset.  Each method is represented by a circle, where the circle size corresponds to the number of parameters and the color corresponds to the memory usage. The vertical axis shows perplexity, and the horizontal axes show memory (GB) and parameter size (M). Methods with smaller, lighter circles in the bottom left corner are considered more efficient and preferable for pretraining.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/figures/figures_17_1.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure compares different methods for pretraining the LLaMA 1B language model on the C4 dataset.  The three metrics compared are perplexity (a measure of how well the model predicts text), memory usage during training, and the total number of parameters in the model.  Each method is represented by a circle, where the size of the circle corresponds to the number of parameters. The color and position (in terms of memory and perplexity) indicate the performance of the model.  Smaller and lighter colored circles in the bottom left corner represent preferable models for pretraining.", "section": "5.1 Pretraining LLMs"}, {"figure_path": "MXze4H7opg/figures/figures_18_1.jpg", "caption": "Figure 1: Shown are perplexity, memory, and parameter size for pretraining LLaMA 1B on the C4 dataset with different methods. The radius and color of each circle scale with parameter size. Overall, the methods which have smaller, lighter circles on the left bottom corner are desirable for pretraining. The details are in Section 5.1.", "description": "This figure compares different model pre-training methods for the LLaMA 1B model on the C4 dataset.  It presents a scatter plot where each point represents a method, with the x-axis showing perplexity (a measure of model performance), the y-axis showing memory usage (in GB), and the size of each point representing the number of parameters (in millions).  The visualization highlights the trade-off between model performance, memory efficiency, and parameter count. Methods in the bottom-left corner are preferred for their better efficiency.", "section": "5.1 Pretraining LLMs"}]