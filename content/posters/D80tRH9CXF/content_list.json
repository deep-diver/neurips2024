[{"type": "text", "text": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 In recent years, there has been a significant growth in research focusing on min  \n2 imum $\\ell_{2}$ norm (ridgeless) interpolation least squares estimators. However, the   \n3 majority of these analyses have been limited to an unrealistic regression error struc  \n4 ture, assuming independent and identically distributed errors with zero mean and   \n5 common variance. In this paper, we explore prediction risk as well as estimation   \n6 risk under more general regression error assumptions, highlighting the beneftis of   \n7 overparameterization in a more realistic setting that allows for clustered or serial   \n8 dependence. Notably, we establish that the estimation difficulties associated with   \n9 the variance components of both risks can be summarized through the trace of the   \n0 variance-covariance matrix of the regression errors. Our findings suggest that the   \n1 beneftis of overparameterization can extend to time series, panel and grouped data. ", "page_idx": 0}, {"type": "text", "text": "12 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "13 Recent years have witnessed a fast growing body of work that analyzes minimum $\\ell_{2}$ norm (ridgeless)   \n14 interpolation least squares estimators [see, e.g., 2, 17, 27, and references therein]. Researchers in this   \n15 field were inspired by the ability of deep neural networks to accurately predict noisy training data   \n16 with perfect ftis, a phenomenon known as \u201cdouble descent\u201d or \u201cbenign overftiting\u201d [e.g., 3\u20135, 29, 22,   \n17 among many others]. They discovered that to achieve this phenomenon, overparameterization is   \n18 critical.   \n19 In the setting of linear regression, we have the training data $\\{(x_{i},y_{i})\\in\\mathbb{R}^{p}\\times\\mathbb{R}:i=1,\\cdots\\,,n\\}$ , where   \n20 the outcome variable $y_{i}$ is generated from ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "equation", "text": "$$\ny_{i}=x_{i}^{\\top}\\beta+\\varepsilon_{i},\\;i=1,\\ldots,n,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "21 $x_{i}$ is a vector of features (or regressors), $\\beta$ is a vector of unknown parameters, and $\\varepsilon_{i}$ is a regression   \n22 error. Here, $n$ is the sample size of the training data and $p$ is the dimension of the parameter vector $_\\beta$ .   \n23 In the literature, the main object for the theoretical analyses has been mainly on the out-of-sample   \n24 prediction risk. That is, for the ridge or interpolation estimator $\\hat{\\beta}$ , the literature has focused on ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[(x_{0}^{\\top}{\\hat{\\boldsymbol{\\beta}}}-x_{0}^{\\top}{\\boldsymbol{\\beta}})^{2}\\mid x_{1},\\ldots,x_{n}\\right]\\!,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "25 where $x_{0}$ is a test observation that is identically distributed as $x_{i}$ but independent of the training data.   \n26 For example, Dobriban and Wager [13], Wu and $\\mathrm{Xu}$ [28], Richards et al. [23], Hastie et al. [17]   \n27 analyzed the predictive risk of ridge(less) regression and obtained exact asymptotic expressions under   \n28 the assumption that $p/n$ converges to some constant as both $p$ and $n$ go to infinity. Overall, they   \n29 found the double descent behavior of the ridgeless least squares estimator in terms of the prediction   \n30 risk. Bartlett et al. [2], Kobak et al. [19], Tsigler and Bartlett [27] characterized the phenomenon of   \n31 benign overfitting in a different setting.   \n32 To the best of our knowledge, a vast majority of the theoretical analyses have been confined to a   \n33 simple data generating process, namely, the observations are independent and identically distributed   \n34 (i.i.d.), and the regression errors have mean zero, have the common variance, and are independent of   \n35 the feature vectors. That is, ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n(y_{i},x_{i}^{\\top})^{\\top}\\sim\\mathrm{i.i.d.~with}\\;\\mathbb{E}[\\varepsilon_{i}]=0,\\mathbb{E}[\\varepsilon_{i}^{2}]=\\sigma^{2}<\\infty\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "36 This assumption, although convenient, is likely to be unrealistic in various real-world examples. For   \n37 instance, Liao et al. [21] adopted high-dimensional linear models to examine the double descent   \n38 phenomenon in economic forecasts. In their applications, the outcome variables include S&P firms\u2019   \n39 earnings, U.S. equity premium, U.S. unemployment rate, and countries\u2019 GDP growth rate. As in   \n40 their applications, economic forecasts are associated with time series or panel data. As a result, it   \n41 is improbable that (1) holds in these applications. As another example, Spiess et al. [26] examined   \n42 the performance of high-dimensional synthetic control estimators with many control units. The   \n43 outcome variable in their application is the state-level smoking rates in the Abadie et al. [1] dataset.   \n44 Considering the geographical aspects of the U.S. states, it is unlikely that the regression errors   \n45 underlying the synthetic control estimators adhere to (1). In short, it is desirable to go beyond the   \n46 simple but unrealistic regression error assumption given in (1).   \n47 To further motivate, we start with our own real-data example from American Community Survey   \n48 (ACS) 2018, extracted from IPUMS USA [24]. The ACS is an ongoing annual survey by the   \n49 US Census Bureau that provides key information about the US population. To have a relatively   \n50 homogeneous population, the sample extract is restricted to white males residing in California with at   \n51 least a bachelor\u2019s degree. We consider a demographic group defined by their age, the type of degree,   \n52 and the field of degree. Then, we compute the average of log hourly wages for each age-degree  \n53 field group, treat each group average as the outcome variable, and predict group wages by various   \n54 group-level regression models where the regressors are constructed using the indicator variables of   \n55 age, degree, and field as well as their interactions. We consider 7 specifications ranging from 209   \n56 to 2,182 regressors. To understand the role of non-i.i.d. regressor errors, we add the artificial noise   \n57 to the training sample. See Appendix A for details regarding how to generate the artificial noise.   \n58 In the experiment, the constant $c$ varies such that $c\\,=\\,0$ corresponds to no clustered dependence   \n59 across observations but as a positive $c$ gets larger, the noise has a larger share of clustered errors but   \n60 the variance of the overall regression errors remains the same regardless of the value of $c$ . Figure 1   \n61 shows the in-sample (train) vs. out-of-sample (test) mean squared error (MSE) for various values   \n62 of $c\\in\\{0,0.25,0.5,0.75\\}$ . It can be seen that the experimental results are almost identical across   \n63 different values of $c$ especially when $p>n$ , suggesting that the double descent phenomenon might   \n64 be universal for various degrees of clustered dependence, provided that the overall variance of the   \n65 regression errors remains the same. It is our main goal to provide a firm foundation for this empirical   \n66 phenomenon. To do so, we articulate the following research questions:   \n67 \u2022 How to analyze the out-of-sample prediction risk of the ridgeless least squares estimator   \n68 under general assumptions on the regression errors?   \n69 \u2022 Why does not the prediction risk seem to be affected by the degrees of dependence across   \n70 observations? ", "page_idx": 1}, {"type": "image", "img_path": "D80tRH9CXF/tmp/d2377eb5ee19f8d305e38540b855b9e3f3f08fa401b7c386ed4cb9ace721203a.jpg", "img_caption": ["Figure 1: Comparison of in-sample and out-of-sample mean squared error (MSE) across various degrees of clustered noise. The vertical line indicates $p=n$ $(=1,415)$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "71 To delve into the prediction risk, suppose that $\\Sigma:=\\mathbb{E}[x_{0}x_{0}^{\\top}]$ is finite and positive definite. Then, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[(x_{0}^{\\top}\\hat{\\boldsymbol{\\beta}}-x_{0}^{\\top}{\\boldsymbol{\\beta}})^{2}\\mid x_{1},\\ldots,x_{n}\\Big]=\\mathbb{E}\\Big[(\\hat{\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}})^{\\top}\\Sigma(\\hat{\\boldsymbol{\\beta}}-{\\boldsymbol{\\beta}})\\mid x_{1},\\ldots,x_{n}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "72 If $\\Sigma=I$ (i.e., the case of isotropic features), where $I$ is the identity matrix, the mean squared error   \n73 of the estimator defined by $\\mathbb{E}[\\|\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\|^{2}]$ , where $\\|\\cdot\\|$ is the usual Euclidean norm, is the same as   \n74 the expectation of the prediction risk defined above. However, if $\\Sigma\\neq I$ , the link between the two   \n75 quantities is less intimate. One may regard the prediction risk as the $\\Sigma$ -weighted mean squared error   \n76 of the estimator; whereas $\\mathbb{E}[\\lvert\\rvert\\hat{\\boldsymbol{\\beta}}-\\dot{\\boldsymbol{\\beta}}\\rvert\\rvert^{2}]$ can be viewed as an \u201cunweighted\u201d version, even if $\\Sigma\\neq I$ . In   \n77 other words, regardless of the variance-covariance structure of the feature vector, $\\mathbb{E}[\\lvert\\rvert\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\rvert\\rvert^{2}]$ treats   \n78 each component of $_\\beta$ \u201cequally.\u201d The mean squared error of the estimator is arguably one of the most   \n79 standard criteria to evaluate the quality of the estimator in statistics. For instance, in the celebrated   \n80 work by James and Stein [18], the mean squared error criterion is used to show that the sample mean   \n81 vector is not necessarily optimal even for standard normal vectors (so-called \u201cStein\u2019s paradox\u201d).   \n82 Many follow-up papers used the same criterion; e.g., Hansen [16] compared the mean-squared error   \n83 of ordinary least squares, James\u2013Stein, and Lasso estimators in an underparameterized regime. Both   \n84 $\\Sigma$ -weighted and unweighted versions of the mean squared error are interesting objects to study. For   \n85 example, Dobriban and Wager [13] called the former \u201cpredictive risk\u201d and the latter \u201cestimation risk\u201d   \n86 in high-dimensional linear models; Berthier et al. [6] called the former \u201cgeneralization error\u201d and the   \n87 latter \u201creconstruction error\u201d in the context of stochastic gradient descent for the least squares problem   \n88 using the noiseless linear model. In this paper, we analyze both weighted and unweighted mean   \n89 squared errors of the ridgeless estimator under general assumptions on the data-generating processes,   \n90 not to mention anisotropic features. Furthermore, our focus is on the finite-sample analysis, that is,   \n91 both $p$ and $n$ are fixed but $p>n$ .   \n92 Although most of the existing papers consider the simple setting as in (1), our work is not the first paper   \n93 to consider more general regression errors in the overparameterized regime. Chinot et al. [9], Chinot   \n94 and Lerasle [8] analyzed minimum norm interpolation estimators as well as regularized empirical   \n95 risk minimizers in linear models without any conditions on the regression errors. Specifically,   \n96 Chinot and Lerasle [8] showed that, with high probability, without assumption on the regression   \n97 errors, for the minimum norm interpolation estimator, $(\\hat{\\hat{\\beta}}-\\beta)^{\\top}\\Sigma(\\hat{\\beta}-\\beta)$ is bounded from above   \n98 by $\\begin{array}{r l r}{\\lefteqn{\\left(\\|\\beta\\|^{2}\\sum_{i\\geq c\\cdot n}\\lambda_{i}(\\Sigma)\\lor\\sum_{i=1}^{n}\\varepsilon_{i}^{2}\\right)/n}}\\end{array}$ , where $c$ is an absolute constant and $\\lambda_{i}(\\Sigma)$ is the eigenvalues of   \n99 $\\Sigma$ in descending order. Chinot and Lerasle [8] also obtained the bounds on the estimation error   \n100 $(\\hat{\\beta}-\\beta)^{\\top}(\\hat{\\beta}-\\beta)$ . Our work is distinct and complements these papers in the sense that we allow for   \n101 a general variance-covariance matrix of the regression errors. The main motivation of not making   \n102 any assumptions on $\\varepsilon_{i}$ in Chinot et al. [9] and Chinot and Lerasle [8] is to allow for potentially   \n103 adversarial errors. We aim to allow for a general variance-covariance matrix of the regression errors   \n104 to accommodate time series and clustered data, which are common in applications. See, e.g., Hansen   \n105 [15] for a textbook treatment (see Chapter 14 for time series and Section 4.21 for clustered data).   \n106 The main contribution of this paper is that we provide exact finite-sample characterization of the vari  \n107 ance component of the prediction and estimation risks under the assumption that $X=[x_{1},x_{2},\\cdot\\cdot\\cdot,x_{n}]^{\\intercal}$   \n108 is left-spherical (e.g., $x_{i}$ \u2019s can be i.i.d. normal with mean zero but more general); $\\varepsilon_{i}$ \u2019s can be corre  \n109 lated and have non-identical variances; and $\\varepsilon_{i}$ \u2019s are independent of $x_{i}$ \u2019s. Specifically, the variance   \n110 term can be factorized into a product between two terms: one term depends only on the trace of the   \n111 variance-covariance matrix, say $\\Omega$ , of $\\varepsilon_{i}$ \u2019s; the other term is solely determined by the distribution of   \n112 $x_{i}$ \u2019s. Interestingly, we find that although $\\Omega$ may contain non-zero off-diagonal elements, only the trace   \n113 of $\\Omega$ matters, as hinted by Figure 1, and further demonstrate our finding via numerical experiments. In   \n114 addition, we obtain exact finite-sample expression for the bias terms when the regression coefficients   \n115 follow the random-effects hypothesis [13]. Our finite-sample findings offer a distinct viewpoint on   \n116 the prediction and estimation risks, contrasting with the asymptotic inverse relationship (for optimally   \n117 chosen ridge estimators) between the predictive and estimation risks uncovered by Dobriban and   \n118 Wager [13]. Finally, we connect our findings to the existing results on the prediction risk [e.g., 17] by   \n119 considering the asymptotic behavior of estimation risk.   \n120 One of the limitations of our theoretical analysis is that the design matrix $X$ is assumed to be left  \n121 spherical, although it is more general than i.i.d. normal with mean zero. We not only view this as a   \n122 convenient assumption but also expect that our findings will hold at least approximately even if $X$   \n123 does not follow the left-spherical distribution. It is a topic for future research to formally investigate   \n124 this conjecture. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "125 2 The Framework under General Assumptions on Regression Errors ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "126 We first describe the minimum $\\ell_{2}$ norm (ridgeless) interpolation least squares estimator in the   \n127 overparameterized case $(p>n)$ . Define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y:=[y_{1},y_{2},\\cdot\\cdot\\cdot{\\bf\\nabla},y_{n}]^{\\top}\\in\\mathbb{R}^{n},}\\\\ &{\\varepsilon:=[\\varepsilon_{1},\\varepsilon_{2},\\cdot\\cdot{\\bf\\nabla},\\varepsilon_{n}]^{\\top}\\in\\mathbb{R}^{n},}\\\\ &{X^{\\top}:=\\left[x_{1},x_{2},\\cdot\\cdot{\\bf\\nabla},x_{n}\\right]\\in\\mathbb{R}^{p\\times n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "128 so that $\\mathrm{y}=X\\beta+\\varepsilon$ . The estimator we consider is ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\hat{\\beta}}:=\\operatorname*{arg\\,min}_{b\\in\\mathbb{R}^{p}}\\{\\|b\\|:X b=y\\}=(X^{\\top}X)^{\\dagger}X^{\\top}y=X^{\\dagger}y,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 where $A^{\\dagger}$ denotes the Moore\u2013Penrose inverse of a matrix $A$ . ", "page_idx": 3}, {"type": "text", "text": "130 The main object of interest in this paper is the prediction and estimation risks of $\\hat{\\beta}$ under the   \n131 data scenario such that the regression error $\\varepsilon_{i}$ may not be i.i.d. Formally, we make the following   \n132 assumptions.   \n133 Assumption 2.1. (i) $\\mathrm{y}=X\\beta+\\varepsilon$ , where $\\varepsilon$ is independent of $X$ , and $\\mathbb{E}[\\varepsilon]=0$ . (ii) $\\Omega:=\\mathbb{E}[\\varepsilon\\varepsilon^{\\top}]$ is finite   \n134 and positive definite (but not necessarily spherical).   \n135 We emphasize that Assumption 2.1 is more general than the standard assumption in the literature   \n136 on benign overfitting that typically assumes that $\\Omega\\equiv\\sigma^{2}I.$ . Assumption 2.1 allows for non-identical   \n137 variances across the elements of $\\varepsilon$ because the diagonal elements of $\\Omega$ can be different among each   \n138 other. Furthermore, it allows for non-zero off-diagonal elements in $\\Omega$ . It is difficult to assume that the   \n139 regression errors are independent among each other with time series or clustered data; thus, in these   \n140 settings, it is important to allow for general $\\Omega\\neq\\sigma^{2}I$ . Below we present a couple of such examples. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "141 Example 2.1 (AR(1) Errors). Suppose that the regressor error follows an autoregressive process: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\varepsilon_{i}=\\rho\\varepsilon_{i-1}+\\eta_{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 where $\\rho\\in(-1,1)$ is an autoregressive parameter, $\\eta_{i}$ is independent and identically distributed with   \n143 mean zero and variance $\\sigma^{2}(0\\stackrel{\\circ}{<}\\sigma^{2}<\\stackrel{\\circ}{\\infty})$ and is independent of $X$ . Then, the $(i,j)$ element of $\\Omega$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Omega_{i j}=\\frac{\\sigma^{2}}{1-\\rho^{2}}\\rho^{|i-j|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "144 Note that $\\Omega_{i j}\\ne0$ as long as $\\rho\\neq0$ . ", "page_idx": 3}, {"type": "text", "text": "145 Example 2.2 (Clustered Errors). Suppose that regression errors are mutually independent across   \n146 clusters but they can be arbitrarily correlated within the same cluster. For instance, students in   \n147 the same school may affect each other and also have the same teachers; thus it would be difficult   \n148 to assume independence across student test scores within the same school. However, it might be   \n149 reasonable that student test scores are independent across different schools. For example, assume that   \n150 (i) if the regression error $\\varepsilon_{i}$ belongs to cluster $g$ , where $g=1,\\ldots,G$ and $G$ is the number of clusters,   \n151 $\\mathbb{E}[\\varepsilon_{i}^{2}]=\\sigma_{g}^{2}$ for some constant $\\sigma_{g}^{2}\\bar{>}0$ that can vary over $g$ ; (ii) if the regression errors $\\varepsilon_{i}$ and $\\varepsilon_{j}\\left(i\\neq j\\right)$   \n152 belong to the same cluster $g$ $\\mathbf{\\nabla},\\breve{\\mathbb{E}}[\\varepsilon_{i}\\varepsilon_{j}]=\\rho_{g}$ for some constant $\\rho_{g}\\neq0$ that can be different across $g$ ;   \n153 and (iii) if the regression errors $\\varepsilon_{i}$ and $\\varepsilon_{j}$ $(i\\neq j)$ do not belong to the same cluster, $\\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}]=0$ . Then,   \n154 $\\Omega$ is block diagonal with possibly non-identical blocks. ", "page_idx": 3}, {"type": "text", "text": "155 For vector $a$ and square matrix $A$ , let $\\|a\\|_{A}^{2}:=a^{\\top}A a$ . Conditional on $X$ and given $A$ , we define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Bias}_{A}({\\hat{\\beta}}\\mid X):=\\|{\\mathbb{E}}[{\\hat{\\beta}}\\mid X]-\\beta\\|_{A}\\quad{\\mathrm{and}}\\quad\\operatorname{Var}_{A}({\\hat{\\beta}}\\mid X):=\\operatorname{Tr}(\\operatorname{Cov}({\\hat{\\beta}}\\mid X)A),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "156 and we write $\\mathrm{Var}=\\mathrm{Var}_{I}$ and $\\mathrm{Bias}=\\mathrm{Bias}_{I}$ for the sake of brevity in notation. ", "page_idx": 3}, {"type": "text", "text": "157 The mean squared prediction error for an unseen test observation $x_{0}$ with the positive definite   \n158 covariance matrix $\\Sigma:=\\mathbb{E}[x_{0}x_{0}^{\\top}]$ (assuming that $x_{0}$ is independent of the training data $X$ ) and the mean   \n159 squared estimation error of $\\hat{\\boldsymbol\\beta}$ conditional on $X$ can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal R}_{P}(\\hat{\\beta}\\mid X):=\\mathbb{E}\\bigl[(x_{0}^{\\top}\\hat{\\beta}-x_{0}^{\\top}\\beta)^{2}\\mid X\\bigr]=[\\mathrm{Bias}_{\\Sigma}(\\hat{\\beta}\\mid X)]^{2}+\\mathrm{Var}_{\\Sigma}(\\hat{\\beta}\\mid X),}\\\\ &{{\\cal R}_{E}(\\hat{\\beta}\\mid X):=\\mathbb{E}\\bigl[\\|\\hat{\\beta}-\\beta\\|^{2}\\mid X\\bigr]=[\\mathrm{Bias}(\\hat{\\beta}\\mid X)]^{2}+\\mathrm{Var}(\\hat{\\beta}\\mid X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "160 In what follows, we obtain exact finite-sample expressions for prediction and estimation risks: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{P}({\\hat{\\beta}}):=\\mathbb{E}_{X}[R_{P}({\\hat{\\beta}}\\mid X)]\\quad{\\mathrm{and}}\\quad R_{E}({\\hat{\\beta}}):=\\mathbb{E}_{X}[R_{E}({\\hat{\\beta}}\\mid X)].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 We first analyze the variance terms for both risks and then study the bias terms. ", "page_idx": 4}, {"type": "text", "text": "162 3 The Variance Components of Prediction and Estimation Risks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 3.1 The variance component of prediction risk ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "164 We rewrite the variance component of prediction risk as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{\\Sigma}({\\hat{\\beta}}\\mid X)=\\operatorname{Tr}(\\operatorname{Cov}({\\hat{\\beta}}\\mid X)\\Sigma)=\\operatorname{Tr}(X^{\\dagger}\\Omega X^{\\dagger\\top}\\Sigma)=\\|S\\,X^{\\dagger}T\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "165 where positive definite symmetric matrices $S:=\\Sigma^{1/2}$ and $T:=\\Omega^{1/2}$ are the square root matrices of   \n166 the positive definite matrices $\\Sigma$ and $\\Omega$ , respectively. To compute the above Frobenius norm of the   \n167 matrix $S X^{\\dagger}T$ , we need to compute the alignment of the right-singular vectors of $B:=S X^{\\dagger}\\in\\mathbb{R}^{p\\times n}$   \n168 with the left-eigenvectors of $T\\in\\mathbb{R}^{n\\times n}$ . Here, $B$ is a random matrix while $T$ is fixed. Therefore, we   \n169 need the distribution of the right-singular vectors of the random matrix $B$ .   \n170 Perhaps surprisingly, to compute the expected variance $\\mathbb{E}_{X}[{\\mathrm{Var}}_{\\Sigma}({\\hat{\\beta}}\\mid X)]$ , it turns out that we do not   \n171 need the distribution of the singular vectors if we make a minimal assumption (the left-spherical   \n172 symmetry of $X$ ) which is weaker than the assumption that $\\{x_{i}\\}_{i=1}^{n}$ is i.i.d. normal with $\\mathbb{E}[x_{1}]=0$ .   \n173 Definition 3.1 (Left-Spherical Symmetry [10\u201312, 14]). A random matrix $Z$ or its distribution is   \n174 called to be left-spherical if $o Z$ and $Z$ have the same distribution $(O Z\\,\\mathrm{\\large~\\underline{{d}}~}_{Z})$ for any fixed orthogonal   \n175 matrix $O\\in O(n):=\\{A\\in\\mathbb{R}^{n\\times n}:A A^{\\top}=A^{\\top}A=I\\}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "176 Assumption 3.2. The design matrix $X$ is left-spherical. ", "page_idx": 4}, {"type": "text", "text": "177 For the isotropic error case $\\boldsymbol{\\Omega}\\,=\\,\\boldsymbol{I})$ , we have $\\mathbb{E}_{X}[\\operatorname{Var}_{\\Sigma}({\\hat{\\beta}}\\mid X)]\\,=\\,\\mathbb{E}_{X}[\\operatorname{Tr}((X^{\\top}X)^{\\dagger}\\Sigma)]$ directly from   \n178 equation 3 since $X^{\\dagger}X^{\\dagger\\top}=(X^{\\top}X)^{\\dagger}$ . Moreover, for the arbitrary error, the left-spherical symmetry of $X$   \n179 plays a critical role to factor out the same $\\mathbb{E}_{X}[\\mathrm{Tr}((X^{\\top}X)^{\\dagger}\\Sigma)]$ and the trace of the variance-covariance   \n180 matrix of the regression errors, $\\mathrm{Tr}(\\Omega)$ , from the variance after the expectation over $X$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3. For a subset $S\\subset\\mathbb{R}^{m\\times m}$ satisfying $C^{-1}\\in S$ for all $C\\in S,$ , if matrix-valued random variables $Z$ and AZ have the same distribution measure $\\mu_{Z}$ for any $A\\in S$ , then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Z}[f(Z)]=\\mathbb{E}_{Z}[f(A Z)]=\\mathbb{E}_{Z}[\\mathbb{E}_{A^{\\prime}\\sim\\nu}[f(A^{\\prime}Z)]]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "181 for any function $f\\in L^{1}(\\mu_{Z})$ and any probability density function $\\nu$ on $s$ . ", "page_idx": 4}, {"type": "text", "text": "182 Theorem 3.4. Let Assumptions 2.1, and 3.2 hold. Then, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}[\\operatorname{Var}_{\\Sigma}({\\hat{\\beta}}\\mid X)]={\\frac{1}{n}}\\operatorname{Tr}(\\Omega)\\mathbb{E}_{X}[\\operatorname{Tr}((X^{\\top}X)^{\\dagger}\\Sigma)].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 Sketch of Proof. With $B=\\Sigma^{1/2}X^{\\dagger}$ and $T=\\Omega^{1/2}$ , we can rewrite the variance as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{\\Sigma}({\\hat{\\beta}}\\mid X)=\\|B T\\|_{F}^{2}=\\|U D V^{\\top}U_{T}D_{T}V_{T}^{\\top}\\|_{F}^{2}\\!=\\|D V^{\\top}U_{T}D_{T}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 from the singular value decompositions $B\\,=\\,U D V^{\\top}$ and $T\\,=\\,U_{T}D_{T}V_{T}^{\\top}$ with orthogonal matrices   \n185 $U,V,U_{T},V_{T}$ , and diagonal matrices $D,D_{T}$ . Then, we need to compute the alignment $V^{\\top}U_{T}$ of the   \n186 right-singular vectors of $B$ with the left-eigenvectors of $T$ because ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|D V^{\\top}U_{T}D_{T}\\|_{F}^{2}=\\lambda\\left((X^{\\top}X)^{\\dagger}\\Sigma\\right)^{\\top}\\Gamma(X)\\lambda(\\Omega)=a(X)^{\\top}\\Gamma(X)b,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "D80tRH9CXF/tmp/1edc0e10252c422c9ca98331115daacf16c33ffac5e4786b9a98c8f1ad677aa0.jpg", "img_caption": ["Figure 2: Our theory (dashed lines) matches the expected variances (solid lines) of the prediction (left) and estimation risks (right) in Example 2.1 (AR(1) Errors). Each point $(\\sigma^{2},\\rho^{2})$ represents a different noise covariance matrix $\\Omega$ , but with the same ${\\mathrm{Tr}}(\\Omega)$ along each line $\\{(\\sigma^{2},\\stackrel{.}{\\rho}^{2}):\\dot{\\sigma^{2}}/\\kappa^{2}+\\rho^{2}=1\\}$ for some $\\kappa^{2}>0$ , they have the same expected variance. We set $n=50,p=100$ , and evaluate on 100 samples of $X$ and 100 samples of $\\varepsilon$ (for each realization of $X$ ) to approximate the expectations. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "187 where $\\nu^{(i)}:=\\,V_{:i},\\,u^{(j)}:=(U_{T})_{:j},\\,\\gamma_{i j}\\,:=\\,\\langle\\nu^{(i)},u^{(j)}\\rangle^{2}\\,\\geq\\,0,\\,\\Gamma(X)\\,:=\\,(\\gamma_{i j})_{i,j}\\,\\in\\mathbb{R}^{n\\times n}$ and $\\mathcal{A}(A)\\,\\in\\,\\mathbb{R}^{n}$ is a   \n188 vector where its elements are the eigenvalues of $A$ .   \n189 Now, we want to compute the expected variance. To do so, from Lemma 3.3 with $S=O(n)$ and the   \n190 left-spherical symmetry of $X$ , we can obtain ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X}[a(X)^{\\top}\\Gamma(X)b]=\\mathbb{E}_{X}\\left[\\mathbb{E}_{O\\sim\\nu}[a(O X)^{\\top}\\Gamma(O X)b]\\right]=\\mathbb{E}_{X}\\left[a(X)^{\\top}\\mathbb{E}_{O\\sim\\nu}[\\Gamma(O X)]b\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "191 where $\\nu$ is the unique uniform distribution (the Haar measure) over the orthogonal matrices $O(n)$ . ", "page_idx": 5}, {"type": "text", "text": "192 Here, we can show that $\\mathbb{E}_{O\\sim\\nu}[\\Gamma(O X)]\\;=\\;{\\textstyle\\frac{1}{n}}J,$ , where $J$ is the all-ones matrix with $J_{i j}\\;=\\;1(i,j\\;=$   \n193 $1,2,\\cdots\\,,n)$ . Therefore, we have the expected variance as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}[\\mathrm{Var}_{\\Sigma}(\\hat{\\beta}\\mid X)]=\\mathbb{E}_{X}\\left[a(X)^{\\top}\\frac{1}{n}J b\\right]=\\frac{1}{n}\\sum_{i,j=1}^{n}\\mathbb{E}_{X}[a_{i}(X)]b_{j}=\\frac{1}{n}\\mathbb{E}_{X}[\\mathrm{Tr}((X^{\\top}X)^{\\dagger}\\Sigma)]\\,\\mathrm{Tr}(\\Omega).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "194 ", "page_idx": 5}, {"type": "text", "text": "195 The proofs of Lemma 3.3 and Theorem 3.4 are in the supplementary appendix. ", "page_idx": 5}, {"type": "text", "text": "196 3.2 The variance component of estimation risk ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "197 For the expected variance $\\mathbb{E}_{X}[{\\mathrm{Var}}({\\hat{\\beta}}\\mid X)]$ of the estimation risk, a similar argument still holds if   \n198 plugging-in $B=X^{\\dagger}$ instead of $B=\\Sigma^{1/2}X^{\\dagger}$ . ", "page_idx": 5}, {"type": "text", "text": "199 Theorem 3.5. Let Assumptions 2.1, and 3.2 hold. Then, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}[\\mathrm{Var}({\\hat{\\beta}}\\mid X)]={\\frac{1}{n p}}\\operatorname{Tr}(\\Omega)\\mathbb{E}_{X}[\\mathrm{Tr}(\\Lambda^{\\dagger})],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "200 where $X X^{\\top}/p=U\\Lambda U^{\\top}$ for some orthogonal matrix $U\\in O(n)$ . ", "page_idx": 5}, {"type": "text", "text": "201 3.3 Numerical experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "202 In this section, we validate our theory with some numerical experiments of Examples 2.1 and   \n203 2.2, especially how the expected variance is related to the general covariance $\\Omega$ of the regressor   \n204 error $\\varepsilon$ . In the both examples, we sample $\\{x_{i}\\}_{i=1}^{n}$ from ${\\cal N}(0,\\Sigma)$ with a general feature covariance   \n205 $\\Sigma=U_{\\Sigma}D_{\\Sigma}U_{\\Sigma}^{\\top}$ for an orthogonal matrix $U_{\\Sigma}\\in O(p)$ and a diagonal matrix $D_{\\Sigma}\\succ0$ . In this setting, we   \n206 have rank $\\operatorname{\\langle}X X^{\\top})=n$ and $\\Lambda^{\\dagger}=\\Lambda^{-1}$ almost everywhere.   \n207 AR(1) Errors As shown in Example 2.1, when the regressor error follows an autoregressive process   \n208 in equation 2, we have $\\Omega_{i j}=\\sigma^{2}\\rho^{|\\vec{i}-j|}/(1-\\rho^{2})$ and $\\mathrm{Tr}(\\Omega)/n=\\sigma^{2}/(1-\\rho^{2})$ . Therefore, for pairs of   \n209 $(\\sigma^{2},\\rho^{2})$ with the same ${\\mathrm{Tr}}(\\Omega)/n$ , they are expected to yield the same variances of the prediction and   \n210 estimation risk from Theorem 3.4 and 3.5 even though they have different off-diagonal elements in $\\Omega$ .   \n211 To be specific, the pairs $(\\sigma^{2},\\rho^{2})$ on a line $\\{(\\sigma^{2},\\rho^{2}):\\!\\sigma^{2}/\\kappa^{\\frac{\\check{2}}{2}}\\!+\\!\\rho^{2}=1\\}$ have the same ${\\mathrm{Tr}}(\\Omega)/n$ and the   \n212 same expected variance which gets larger for the line with respect to a larger $\\kappa^{2}$ .   \n213 Figure 2 (left) shows the contour plots of $\\mathbb{E}_{X}[{\\mathrm{Var}}_{\\Sigma}({\\hat{\\beta}}\\mid X)]$ and $\\textstyle\\frac{1}{n}\\operatorname{Tr}(\\Omega)\\mathbb{E}_{X}[\\operatorname{Tr}((X^{\\top}X)^{\\dag}\\Sigma)]$ for different   \n214 pairs of $(\\sigma^{2},\\rho^{2})$ in Example 2.1. They have different slopes $-\\kappa^{-2}$ according to the value of $\\kappa^{2}\\,=$   \n215 ${\\mathrm{Tr}}(\\Omega)/n$ . The right panel shows equivalent contour plots for estimation risk.   \n216 Clustered Errors Now consider the block diagonal covariance matrix $\\Omega=\\mathrm{diag}(\\Omega_{1},\\Omega_{2},\\cdot\\cdot\\cdot,\\Omega_{G})$   \n217 in Example 2.2, where $\\Omega_{g}$ is an $n_{g}\\times n_{g}$ matrix with $(\\Omega_{g})_{i i}=\\sigma_{g}^{2}$ and $(\\Omega_{g})_{i j}=\\rho_{g}$ $(i\\neq j)$ for each   \n218 $i,j=1,2,\\cdots,n_{g}$ and $g=1,2,\\cdots,G$ . Let $\\begin{array}{r}{n=\\sum_{g=1}^{G}n_{g}}\\end{array}$ . We then have $\\begin{array}{r}{\\mathrm{Tr}(\\Omega)/n=\\sum_{g=1}^{G}\\mathrm{Tr}(\\Omega_{g})/n=}\\end{array}$   \n219 $\\begin{array}{r}{\\sum_{g=1}^{G}(n_{g}/n)\\sigma_{g}^{2}}\\end{array}$ . Therefore, given a partition $\\{n_{g}\\}_{g=1}^{G}$ of the $n$ observations, the covariance matrices $\\Omega$   \n220 with different $\\{\\sigma_{g}^{2}\\}_{g=1}^{G}$ have the same ${\\mathrm{Tr}}(\\Omega)/n$ if $(\\sigma_{1}^{2},\\sigma_{2}^{2},\\cdots,\\sigma_{G}^{2})\\in\\mathbb{R}^{G}$ are on the same hyperplane   \n221 $\\begin{array}{r}{\\frac{n_{1}}{n}\\sigma_{1}^{2}+\\frac{n_{2}}{n}\\sigma_{2}^{2}+\\cdots+\\frac{n_{G}}{n}\\sigma_{G}^{2}=\\kappa^{2}}\\end{array}$ for some $\\kappa^{2}>0$ .   \n222 Figure 3 (left) shows the contour plots of $\\mathbb{E}_{X}[{\\mathrm{Var}}_{\\Sigma}({\\hat{\\beta}}\\mid X)]$ and $\\textstyle\\frac{1}{n}\\operatorname{Tr}(\\Omega)\\mathbb{E}_{X}[\\operatorname{Tr}((X^{\\top}X)^{\\dagger}\\Sigma)]$ for different   \n223 pairs of $(\\sigma_{1}^{2},\\sigma_{2}^{2})$ for a simple two-clusters example $G=2,$ ) of Example 2.2 with $(n_{1},n_{2})=(5,15)$ .   \n224 Here, we use a fixed value of $\\rho_{1}=\\rho_{2}=0.05$ , but the results are the same regardless of their values,   \n225 as shown in the appendix. Unlike Example 2.1, the hyperplanes are orthogonal to $\\boldsymbol{\\nu}\\,=\\,[n_{1},n_{2}]^{\\top}$   \n226 regardless of the value of $\\kappa^{2}=\\mathrm{Tr}(\\Omega)/n$ . Again, the right panel shows equivalent contour plots for   \n227 estimation risk. ", "page_idx": 5}, {"type": "image", "img_path": "D80tRH9CXF/tmp/2ca95747f53d0144fa5bdad6ef7f8144d62a28219c4bba516347e8053ed0591b.jpg", "img_caption": ["Figure 3: Our theory (dashed lines) matches the expected variances (solid lines) of the prediction (left) and estimation risks (right) in Example 2.2 (Clustered Errors). Each point $(\\sigma^{2},\\rho^{2})$ represents a different noise covariance matrix $\\Omega$ , but with the same ${\\mathrm{Tr}}(\\Omega)$ along each line $\\{(\\sigma_{1}^{2},\\sigma_{2}^{2}):{\\textstyle\\frac{\\dot{n}_{1}}{n}}\\sigma_{1}^{\\dot{2}}\\!+\\!{\\textstyle\\frac{n_{2}}{n}}\\sigma_{2}^{2}=$ $\\kappa^{2}\\}$ for some $\\kappa^{2}>0$ , they have the same expected variance. We set $G=2$ , $(n_{1}=5,n_{2}=15),n=$ $20,p\\,=\\,40,\\rho_{1}\\,=\\,\\rho_{2}\\,=\\,0.05$ , and evaluate on 100 samples of $X$ and 100 samples of $\\varepsilon$ (for each realization of $X$ ) to approximate the expectations. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "228 4 The Bias Components of Prediction and Estimation Risks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "229 Our main contribution is to allow for general assumptions on the regression errors, and thus the bias   \n230 parts remain the same as they do not change with respect to the regression errors. For completeness,   \n231 in this section, we briefly summarize the results on the bias components. First, we make the following   \n232 assumption for a constant rank deficiency of $X^{\\top}X$ which holds, for example, each $x_{i}$ has a positive   \n233 definite covariance matrix and is independent of each other. ", "page_idx": 6}, {"type": "text", "text": "234 Assumption 4.1. rank $(X)=n$ almost everywhere. ", "page_idx": 6}, {"type": "text", "text": "235 4.1 The bias component of prediction risk ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "236 The bias term of prediction risk can be expressed as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n[\\operatorname{Bias}_{\\Sigma}(\\hat{\\beta}\\mid X)]^{2}=(S\\beta)^{\\top}\\operatorname*{lim}_{\\lambda\\searrow0}\\lambda^{2}(S^{-1}\\hat{\\Sigma}S\\,+\\lambda I)^{-2}S\\beta,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "237 where $\\hat{\\Sigma}:=X^{\\top}X/n$ . Now, in order to obtain an exact closed form solution, we make the following   \n238 assumption:   \n240 A similar assumption (see Assumption 4.4) has been shown to be useful to obtain closed-form   \n241 expressions in the literature [e.g., 13, 23, 20, 7].   \n242 Under this assumption, since $[\\operatorname{Bias}_{\\Sigma}({\\hat{\\beta}}\\mid X)]^{2}\\,=\\operatorname{Tr}[S\\beta(S\\beta)^{\\top}\\operatorname*{lim}_{\\lambda\\setminus0}\\lambda^{2}(S^{-1}{\\hat{\\Sigma}}S\\,+\\lambda I)^{-2}]$ from equa  \n243 tion 4, we have the expected bias (conditional on $X$ ) as follows: ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\beta}[\\mathrm{Bias}_{\\Sigma}(\\hat{\\beta}\\mid X)^{2}\\mid X]=\\frac{r_{\\Sigma}^{2}}{p}\\operatorname*{lim}_{\\lambda\\setminus0}\\sum_{i=1}^{p}\\frac{\\lambda^{2}}{(\\tilde{s}_{i}+\\lambda)^{2}}=\\frac{r_{\\Sigma}^{2}}{p}\\left\\lvert\\{i\\in[p]:\\tilde{s}_{i}=0\\}\\right\\rvert=r_{\\Sigma}^{2}\\frac{p-n}{p},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "244 where $\\Tilde{s}_{i}$ are the eigenvalues of $S^{-1}\\hat{\\Sigma}S\\in\\mathbb{R}^{p\\times p}$ and rank $(S^{-1}\\hat{\\Sigma}S\\,)=\\mathrm{rank}(X)=n$ almost everywhere   \n245 under Assumption 4.1. This bias is independent of the distribution of $X$ or the spectral density of   \n246 $S^{-1}\\hat{\\Sigma}S$ , but only depending on the rank deficiency of the realization of $X$ . ", "page_idx": 7}, {"type": "text", "text": "247 Finally, the prediction risk $R_{P}({\\hat{\\beta}})$ can be summarized as follows: ", "page_idx": 7}, {"type": "text", "text": "248 Corollary 4.3. Let Assumptions 2.1, 3.2, 4.1, and 4.2 hold. Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{P}(\\hat{\\beta})=r_{\\Sigma}^{2}\\left(1-\\frac{n}{p}\\right)+\\frac{\\mathrm{Tr}(\\Omega)}{n}\\mathbb{E}_{X}\\left[\\mathrm{Tr}((X^{\\top}X)^{\\dagger}\\Sigma)\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "249 4.2 The bias component of estimation risk ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "250 For the bias component of estimation risk, we can obtain a similar result with 4.1 as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n[\\operatorname{Bias}({\\hat{\\beta}}\\mid X)]^{2}=\\beta^{\\top}(I-{\\hat{\\Sigma}}^{\\dagger}\\hat{\\Sigma})\\beta=\\operatorname*{lim}_{\\lambda\\setminus0}\\beta^{\\top}\\lambda(\\hat{\\Sigma}+\\lambda I)^{-1}\\beta.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "251 Assumption 4.4. $\\mathbb{E}_{\\beta}[\\beta\\beta^{\\top}]=r^{2}I/p$ , where $r^{2}:=\\mathbb{E}_{\\beta}[||\\beta||^{2}]<\\infty$ and $_\\beta$ is independent of $X$ . ", "page_idx": 7}, {"type": "text", "text": "252 Under Assumption 4.4, we have the expected bias (conditional on $X$ ) as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\beta}[\\operatorname{Bias}(\\hat{\\beta}\\mid X)^{2}\\mid X]=\\frac{r^{2}}{p}\\operatorname*{lim}_{\\lambda\\searrow0}\\sum_{i=1}^{p}\\frac{\\lambda}{s_{i}+\\lambda}=\\frac{r^{2}}{p}\\left\\|\\{i\\in[p]:s_{i}=0\\}\\right\\|=r^{2}\\frac{p-n}{p},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "253 where $s_{i}$ are the eigenvalues of $\\hat{\\Sigma}\\in\\mathbb{R}^{p\\times p}$ and ra $\\operatorname{1}\\!\\mathrm{k}({\\hat{\\Sigma}})=\\operatorname{rank}(X)=n$ under Assumption 4.1. ", "page_idx": 7}, {"type": "text", "text": "254 Thanks to Theorem 3.5 and equation 5, we obtain the following corollary for estimation risk. ", "page_idx": 7}, {"type": "text", "text": "255 Corollary 4.5. Let Assumptions 2.1, 3.2, 4.1, and 4.4 hold. Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{E}(\\hat{\\beta})=r^{2}\\left(1-\\frac{n}{p}\\right)+\\frac{\\mathrm{Tr}(\\Omega)}{n}\\mathbb{E}_{X}\\left[\\int\\frac{1}{s}d F^{X X^{\\top}/n}(s)\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "256 where $\\begin{array}{r}{F^{A}(s)~:=~\\frac{1}{n}\\sum_{i=1}^{n}1\\{\\lambda_{i}(A)~\\le~s\\}}\\end{array}$ is the empirical spectral distribution of a matrix A and   \n257 $\\lambda_{1}(A),\\lambda_{2}(A),\\cdot\\cdot\\cdot{},\\dot{\\lambda_{n}}(A)$ are the eigenvalues of $A$ . ", "page_idx": 7}, {"type": "text", "text": "258 The proof of Corollary 4.5 is in the appendix. ", "page_idx": 7}, {"type": "text", "text": "259 4.2.1 Asymptotic analysis of estimation risk ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "260 To study the asymptotic behavior of estimation risk, we follow the previous approaches [13, 17].   \n261 First, we define the Stieltjes transform as follows: ", "page_idx": 7}, {"type": "text", "text": "Definition 4.6. The Stieltjes transform $s_{F}(z)$ of a df $F$ is defined as: ", "page_idx": 7}, {"type": "equation", "text": "$$\ns_{F}(z):=\\int{\\frac{1}{x-z}}d F(x),{\\mathrm{~for~}}z\\in\\mathbb{C}\\ \\backslash\\ {\\mathrm{supp}}(F).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "D80tRH9CXF/tmp/060e7e78ec4523aa7b02c3311dc1068f22ca4b6ed3e03ee926811d0baeddcecc.jpg", "img_caption": ["Figure 4: The \u201cdescent curve\u201d in the overparameterization regime for prediction risk (left) and estimation risk (right). We test $\\Omega$ \u2019s with $\\mathrm{Tr}(\\Omega)/n=1,2,4$ in black, blue, red, respectively. For the anisotropic feature, the expected variance $(\\times)$ and its theoretical expression $\\mathbf{\\tau}(\\bullet)$ are $\\Theta\\left(\\frac{\\mathrm{Tr}(\\Omega)/n}{\\gamma-1}\\right)$ and larger than that in the high-dimensional asymptotics for the isotropic $\\Sigma=I$ . For the isotropic $\\Sigma=I$ , the variance terms (dotted) and the bias terms (dashed) in the high-dimensional asymptotics are ${\\begin{array}{r}{{\\frac{1}{\\gamma-1}}\\operatorname*{lim}_{n\\to\\infty}{\\frac{\\operatorname{Tr}(\\Omega)}{n}}}\\end{array}}$ Tr(n\u2126)and r2  1 \u2212\u03b31 , respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "262 We are now ready to investigate the asymptotic behavior of the mean squared estimation error with   \n263 the following theorem:   \n264 Theorem 4.7. $I25_{:}$ , Theorem 1.1] Suppose that the rows $\\{x_{i}\\}_{i=1}^{n}$ in $X$ are i.i.d. centered random vectors   \n265 with E[ $\\boldsymbol{x}_{1}\\boldsymbol{x}_{1}^{\\intercal}]=\\boldsymbol{\\Sigma}$ and that the empirical spectral distribution $\\begin{array}{r}{\\dot{F}^{\\Sigma}(s)=\\frac{1}{p}\\sum_{i=1}^{p}1\\{\\tau_{i}\\leq s\\}}\\end{array}$ of $\\Sigma$ converges   \n266 almost surely to a probability distribution function $H$ as $p\\rightarrow\\infty$ . When $p/n\\to\\gamma>0$ as $n,p\\to\\infty$ ,   \n267 then a.s., $F^{X X^{\\top}/n}$ converges vaguely to a df $F$ and the limit $\\begin{array}{r}{s^{\\ast}:=\\operatorname*{lim}_{z\\searrow0}s_{F}(z)}\\end{array}$ of its Stieltjes transform   \n268 $s_{F}$ is the unique solution to the equation: ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n1-\\frac{1}{\\gamma}=\\int\\frac{1}{1+\\tau s^{*}}d H(\\tau).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "269 This theorem is a direct consequence of Theorem 1.1 in Silverstein and Bai [25]. Then, from Corollary   \n270 4.5, we can write the limit of estimation risk as follows:   \n271 Corollary 4.8. Let Assumptions 2.1, 3.2, 4.1, and 4.4 hold. Then, under the same assumption as   \n272 Theorem 4.7, as $\\eta,p\\to\\infty$ and $p/n\\rightarrow\\gamma,$ , where $1<\\gamma<\\infty$ is a constant, we have ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{E}(\\hat{\\beta})=\\mathbb{E}[\\|\\hat{\\beta}-\\beta\\|^{2}]\\to r^{2}\\left(1-\\frac{1}{\\gamma}\\right)+s^{*}\\operatorname*{lim}_{n\\to\\infty}\\frac{\\mathrm{Tr}(\\Omega)}{n}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "273 Here, the limit $s^{*}$ of the Stieltjes transform $s_{F}$ is highly connected with the shape of the spectral   \n274 distribution of $\\Sigma$ . For example, in the case of isotropic features $(\\Sigma=I)$ , i.e., $d H(\\tau)=\\delta(\\tau-1)d\\tau$ , we   \n275 have $s_{\\mathrm{iso}}^{*}=(\\gamma-1)^{-1}$ from $\\begin{array}{r}{\\dot{1}-\\frac{1}{\\gamma}=\\frac{1}{1+s_{\\mathrm{iso}}^{*}}}\\end{array}$ 1+1s\u2217 . In addition, if \u2126= \u03c32I, then the limit of the mean squared   \n276 error is exactly the same as the expression for $\\gamma>1$ in equation (10) of Hastie et al. [17, Theorem 1].   \n277 This is because prediction risk is the same as estimation risk when $\\Sigma=I$ .   \n278 Remark 4.9. Generally, if the support of $H$ is bounded within $[c_{H},C_{H}]\\subset\\mathbb{R}$ for some positive constants   \n279 $0<c_{H}\\leq C_{H}<\\infty$ , then we can observe the double descent phenomenon in the overparameterization   \n280 regime with $\\mathrm{lim}_{\\gamma\\searrow1}\\,s^{*}=\\infty$ and $\\operatorname*{lim}_{\\gamma\\to\\infty}s^{*}=0$ with $\\begin{array}{r}{s^{*}=\\Theta\\left(\\frac{1}{\\gamma-1}\\right)}\\end{array}$ from the following inequalities: ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\nC_{H}^{-1}\\frac{1}{\\gamma-1}\\leq s^{*}\\leq c_{H}^{-1}\\frac{1}{\\gamma-1}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "281 In fact, a tighter lower bound is available: ", "page_idx": 8}, {"type": "equation", "text": "$$\ns^{*}\\geq\\mu_{H}^{-1}(\\gamma-1)^{-1},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "282 where $\\mu_{H}:=\\mathbb{E}_{\\tau\\sim H}[\\tau]$ , i.e., the mean of distribution $H$ . The proofs of equation 7 and equation 8 are   \n283 given in the supplementary appendix.   \n284 We conclude this paper by plotting the \u201cdescent curve\u201d in the overparameterization regime in Figure   \n285 4. On one hand, the expected variance $(\\times)$ perfectly matches its theoretical counterpart $(\\bullet)$ and goes   \n286 to zero as $\\gamma$ gets large. On the other hand, the bias term is bounded even if $\\gamma\\to\\infty$ . The appendix   \n287 contains the experimental details for all the figures. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "288 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "289 [1] Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for compar  \n290 ative case studies: Estimating the effect of california\u2019s tobacco control program. Journal of the   \n291 American Statistical Association, 105(490):493\u2013505, 2010.   \n292 [2] Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in   \n293 linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070,   \n294 2020.   \n295 [3] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to   \n296 understand kernel learning. In International Conference on Machine Learning, pages 541\u2013549.   \n297 PMLR, 2018.   \n298 [4] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine  \n299 learning practice and the classical bias\u2013variance trade-off. Proceedings of the National Academy   \n300 of Sciences, 116(32):15849\u201315854, 2019.   \n301 [5] Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM   \n302 Journal on Mathematics of Data Science, 2(4):1167\u20131180, 2020.   \n303 [6] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Tight nonparametric convergence rates for   \n304 stochastic gradient descent under the noiseless linear model. Advances in Neural Information   \n305 Processing Systems, 33:2576\u20132586, 2020.   \n306 [7] Xin Chen, Yicheng Zeng, Siyue Yang, and Qiang Sun. Sketched ridgeless linear regression: The   \n307 role of downsampling. In International Conference on Machine Learning, pages 5296\u20135326.   \n308 PMLR, 2023.   \n309 [8] Geoffrey Chinot and Matthieu Lerasle. On the robustness of the minimum $\\ell_{2}$ interpola  \n310 tor. Bernoulli, 2023. forthcoming, available at https://www.bernoullisociety.org/   \n311 publications/bernoulli-journal/bernoulli-journal-papers.   \n312 [9] Geoffrey Chinot, Matthias L\u00f6ffler, and Sara van de Geer. On the robustness of minimum norm   \n313 interpolators and regularized empirical risk minimizers. The Annals of Statistics, 50(4):2306 \u2013   \n314 2333, 2022. doi: 10.1214/22-AOS2190. URL https://doi.org/10.1214/22-AOS2190.   \n315 [10] AP Dawid. Spherical matrix distributions and a multivariate model. Journal of the Royal   \n316 Statistical Society: Series B (Methodological), 39(2):254\u2013261, 1977.   \n317 [11] AP Dawid. Extendibility of spherical matrix distributions. Journal of Multivariate Analysis, 8   \n318 (4):559\u2013566, 1978.   \n319 [12] AP Dawid. Some matrix-variate distribution theory: Notational considerations and a Bayesian   \n320 application. Biometrika, pages 265\u2013274, 1981.   \n321 [13] Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge   \n322 regression and classification. The Annals of Statistics, 46(1):247 \u2013 279, 2018. doi:   \n323 10.1214/17-AOS1549. URL https://doi.org/10.1214/17-AOS1549.   \n324 [14] Arjun K Gupta and Daya K Nagar. Matrix variate distributions, volume 104. CRC Press, 1999.   \n325 [15] Bruce Hansen. Econometrics. Princeton University Press, 2022.   \n326 [16] Bruce E Hansen. The risk of James\u2013Stein and Lasso shrinkage. Econometric Reviews, 35(8-10):   \n327 1456\u20131470, 2016.   \n328 [17] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high  \n329 dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949\u2013986,   \n330 2022.   \n331 [18] W. James and Charles Stein. Estimation with quadratic loss. In Proc. 4th Berkeley Sympos.   \n332 Math. Statist. and Prob., Vol. I, pages 361\u2013379. Univ. California Press, Berkeley, Calif., 1961.   \n333 [19] Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The optimal ridge penalty for real-world   \n334 high-dimensional data can be zero or negative due to the implicit ridge regularization. The   \n335 Journal of Machine Learning Research, 21(1):6863\u20136878, 2020.   \n336 [20] Zeng Li, Chuanlong Xie, and Qinwen Wang. Asymptotic normality and confidence intervals   \n337 for prediction risk of the min-norm least squares estimator. In International Conference on   \n338 Machine Learning, pages 6533\u20136542. PMLR, 2021.   \n339 [21] Yuan Liao, Xinjie Ma, Andreas Neuhierl, and Zhentao Shi. Economic forecasts using many   \n340 noises. arXiv preprint arXiv:2312.05593, 2023. URL https://arxiv.org/abs/2312.   \n341 05593.   \n342 [22] Song Mei and Andrea Montanari. The generalization error of random features regression:   \n343 Precise asymptotics and the double descent curve. Communications on Pure and Applied   \n344 Mathematics, 75(4):667\u2013766, 2022.   \n345 [23] Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge (less)   \n346 regression under general source condition. In International Conference on Artificial Intelligence   \n347 and Statistics, pages 3889\u20133897. PMLR, 2021.   \n348 [24] Steven Ruggles, Sarah Flood, Matthew Sobek, Daniel Backman, Annie Chen, Grace Cooper,   \n349 Stephanie Richards, Renae Rodgers, and Megan Schouweiler. IPUMS USA: Version 15.0   \n350 [dataset]. https://doi.org/10.18128/D010.V15.0, 2024. Minneapolis, MN: IPUMS.   \n351 [25] Jack W Silverstein and ZD Bai. On the empirical distribution of eigenvalues of a class of large   \n352 dimensional random matrices. Journal of Multivariate analysis, 54(2):175\u2013192, 1995.   \n353 [26] Jann Spiess, Guido Imbens, and Amar Venugopal. Double and single descent in causal inference   \n354 with an application to high-dimensional synthetic control. In Thirty-seventh Conference on   \n355 Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=   \n356 dL0GM9Wwtq.   \n357 [27] Alexander Tsigler and Peter L. Bartlett. Benign overfitting in ridge regression. Journal of   \n358 Machine Learning Research, 24(123):1\u201376, 2023. URL http://jmlr.org/papers/v24/   \n359 22-1398.html.   \n360 [28] Denny Wu and Ji Xu. On the optimal weighted $\\ell_{2}$ regularization in overparameterized linear   \n361 regression. Advances in Neural Information Processing Systems, 33:10112\u201310123, 2020.   \n362 [29] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Benign   \n363 overfitting of constant-stepsize SGD for linear regression. In Conference on Learning Theory,   \n364 pages 4633\u20134635. PMLR, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "365 Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "366 A Details for drawing Figure 1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "367 To draw Figure 1, we use a sample extract from American Community Survey (ACS) 2018. To   \n368 have a relatively homogeneous population, the sample extract is restricted to white males residing in   \n369 California with at least a bachelor\u2019s degree. We consider a demographic group defined by their age in   \n370 years (between 25 and 70), the type of degree (bachelor\u2019s, master\u2019s, professional, and doctoral), and   \n371 the field of degree (172 unique values). Then, we compute the average of log hourly wages for each   \n372 age-degree-field group (all together 7,073 unique groups in the sample). We treat each group average   \n373 as the outcome variable (say, $y_{a,d,f}$ ) and predict group wages by various group-level regression models   \n374 where the regressors are constructed using the indicator variables of age, degree, and field as well as   \n375 their interactions: that is, ", "page_idx": 11}, {"type": "equation", "text": "$$\ny_{a,d,f}=x_{a,d,f}^{\\top}\\beta+\\varepsilon_{a,d,f}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "376 For the regressors $x_{a,d,f}$ , we consider 7 specifications ranging from 209 to 2,183 regressors: ", "page_idx": 11}, {"type": "text", "text": "377 \u2022 Spec. 1 $(p=209)$ : dummy variables for age (say, $x_{a})+$ dummy variables for the type of   \n378 degree (say, $x_{d})+$ dummy variables for the field of degree (say, $x_{f}$ ),   \n379 \u2022 Spec. 2 $y=391$ ): Spec. $1+$ all interactions between $x_{d}$ and $x_{a}$ ,   \n380 \u2022 Spec. 3 $\\boldsymbol{p}=598)$ : Spec. $1+$ all interactions between $x_{d}$ and $x_{f}$ ,   \n381 \u2022 Spec. 4 $y=778)$ : Spec. $1+$ all interactions between $x_{d}$ and $x_{a}+$ all interactions between   \n382 $x_{d}$ and $x_{f}$ ,   \n383 \u2022 Spec. 5 $\\mathit{\\Theta}_{\\mathit{p}}=1640)$ : Spec. $^{1+}$ all interactions between $x_{d}$ and $x_{a}+$ all interactions between   \n384 $x_{a}$ and $x_{f}$ ,   \n385 \u2022 Spec. 6 $(p=1754)$ : Spec. $1+{\\mathrm{all}}$ interactions between $x_{d}$ and $x_{f}+$ all interactions between   \n386 $x_{a}$ and $x_{f}$ ,   \n387 \u2022 Spec. 7 $y=2182)$ : Spec. $^{1+}$ all three-way interactions among $x_{a},x_{d}$ and $x_{f}$ .   \n388 Here, the dummy variable are constructed using one-hot encoding. We randomly split the sample   \n389 into the train and test samples with a ratio of $1:4$ . The resulting sample sizes are 1,415 and 5,658,   \n390 respectively. To understand the role of non-i.i.d. regressor errors, we add the artificial noise to the   \n391 training sample: that is, we compute the ridgeless least squares estimator using the training sample of   \n392 $(\\tilde{y}_{a,d,f},x_{a,d,f}^{\\top})^{\\top}$ , where $\\tilde{y}_{a,d,f}=y_{a,d,f}+u_{a,d,f}$ . Here, the artificial noise $u_{a,d,f}$ has the form ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\nu_{a,d,f}\\equiv\\frac{(1-c)e_{a,d,f}+c\\cdot e_{f}}{\\sqrt{(1-c)^{2}+c^{2}}},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "393 where $e_{a,d,f}\\sim N(0,\\sigma^{2})$ , independently across age $(a)$ , degree $(d)$ and field $(f);e_{f}$ is the average of   \n394 another independent $N(0,\\sigma^{2})$ variable within $f$ (hence, $e_{f}$ is identical for each value of $f$ ) and thus   \n395 the source of clustered errors; and $c\\in\\{0,0.25,0.5,0.75\\}$ is a constant that will be varied across the   \n396 experiment. As $c$ gets larger, the noise has a larger share of clustered errors but the variance of the   \n397 overall regression errors $(u_{a,d,f})$ remains the same: in other words, $\\mathrm{var}(u_{a,d,f})=\\sigma^{2}$ for each value of   \n398 $c$ . Figure 1 was generated with $\\sigma=0.5$ by generating the artificial noise only once. ", "page_idx": 11}, {"type": "text", "text": "399 B Details for drawing Figures 2, 3, and 4 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "400 To draw Figure 2, 3, and 4, we sample $\\{x_{i}\\}_{i=1}^{n}$ from $N(0,\\Sigma)$ with $\\Sigma\\,=\\,U_{\\Sigma}D_{\\Sigma}U_{\\Sigma}^{\\top}$ where $U_{\\Sigma}$ is an   \n401 orthogonal matrix random variable, drawn from the uniform (Haar) distribution on $O(p)$ , and $D_{\\Sigma}$   \n402 is a diagonal matrix with its elements $\\begin{array}{r}{d_{i}=|z_{i}|/\\sum_{i=1}^{p}|z_{i}|}\\end{array}$ being sampled with $z_{i}\\sim\\mathcal{N}(0,1)$ for each   \n403 $i=1,2,\\cdots,p$ . With this general anisotropic $\\Sigma$ , the term $\\mathbb{E}_{X}[{\\mathrm{Tr}}(\\Lambda^{-1})]/p$ is somewhat larger than   \n404 $\\mu_{H}^{-1}s_{\\mathrm{iso}}^{*}=(\\gamma-1)^{-1}$ which is 1 in Figure 2 and 3 since $\\mu_{H}=1$ and $\\gamma=2$ . For example, in Figure 2,   \n405 when $\\sigma^{2}=1,\\rho^{2}=0$ , we have $\\mathrm{Tr}(\\Omega)/n=1$ but $\\mathrm{Tr}(\\Omega)\\mathbb{E}_{X}[\\mathrm{Tr}(\\Lambda^{-1})]/(n p)>1$ . ", "page_idx": 11}, {"type": "text", "text": "406 In Figure 4, we fix $n=50$ and use $p=n\\gamma$ for $\\gamma\\in[1,100]$ . ", "page_idx": 11}, {"type": "text", "text": "407 To compute the expectations of $\\mathbb{E}_{X}[\\mathrm{Var}(\\hat{\\beta}|X)]$ and $\\mathbb{E}_{X}[\\mathrm{Tr}(\\Lambda^{-1})]$ over $X$ , we sample $N_{X}$ sam  \n408 ples of $X\"s$ , $X_{1},X_{2},\\cdot\\cdot\\cdot\\,,X_{N_{X}}$ . Moreover, to compute the expectation over $\\varepsilon$ in $\\operatorname{Var}({\\hat{\\beta}}|X_{i})\\ \\equiv\\operatorname{\\hat{\\beta}}$   \n409 $\\operatorname{Tr}\\left(\\mathbb{E}_{\\varepsilon}[\\hat{\\beta}\\hat{\\beta}^{\\top}]-\\mathbb{E}_{\\varepsilon}[\\hat{\\beta}]\\mathbb{E}_{\\varepsilon}[\\hat{\\beta}]^{\\top}\\right)$ , we sample $N_{\\varepsilon}$ samples of $\\varepsilon$ \u2019s, $\\varepsilon_{1},\\varepsilon_{2},\\cdots,\\varepsilon_{N_{\\varepsilon}}$ for each realization $X_{i}$ .   \n410 To be specific, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{B}_{X}[\\mathrm{Var}(\\hat{\\beta}|X)]\\approx\\frac{1}{N_{X}}\\sum_{i=1}^{N_{X}}\\mathrm{Var}(\\hat{\\beta}|X_{i})\\approx\\frac{1}{N_{X}}\\sum_{i=1}^{N_{X}}\\mathrm{Tr}\\left(\\frac{1}{N_{\\varepsilon}}\\sum_{j=1}^{N_{\\varepsilon}}\\hat{\\beta}_{i,j}\\hat{\\beta}_{i,j}^{\\top}-\\frac{1}{N_{\\varepsilon}}\\sum_{j=1}^{N_{\\varepsilon}}\\hat{\\beta}_{i,j}\\frac{1}{N_{\\varepsilon}}\\sum_{j=1}^{N_{\\varepsilon}}\\hat{\\beta}_{i,j}^{\\top}\\right)}\\\\ &{\\displaystyle\\frac{1}{p}\\mathbb{B}_{X}[\\mathrm{Tr}(\\Lambda^{-1})]\\approx\\frac{1}{N_{X}}\\sum_{i=1}^{N_{X}}\\mathrm{Tr}((X_{i}X_{i}^{\\top})^{-1})=\\frac{1}{N_{X}}\\sum_{i=1}^{N_{X}}\\sum_{k=1}^{n}\\frac{1}{\\lambda_{k}(X_{i}X_{i}^{\\top})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "411 where $\\hat{\\beta}_{i,j}=\\arg\\operatorname*{min}_{\\beta}\\{\\|b\\|:X_{i}b-y_{i,j}=0\\}$ , $y_{i,j}=X_{i}\\beta+\\varepsilon_{j}$ , and $\\lambda_{k}(X_{i}X_{i}^{\\top})$ is the $k$ -th eigenvalue of   \n412 $X_{i}X_{i}^{\\top}$ . We can do similarly for the variance part of the prediction risk. ", "page_idx": 12}, {"type": "text", "text": "413 Figure 5 shows an additional experimental result. ", "page_idx": 12}, {"type": "image", "img_path": "D80tRH9CXF/tmp/a7791b4849a36a3d2bf39e992aa9703b1353358f28da0b2e652ddda97977f011.jpg", "img_caption": ["Figure 5: We use the same setting as Figure 3, except uniformly sample each $\\rho_{i}$ from [0, 0.05] for each experiment with the pairs $(\\sigma_{1}^{2},\\sigma_{1}^{2})$ . As expected, the off-diagonal elements $\\rho_{i}$ of $\\Omega$ do not affect the expected variances. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "414 C Proofs omitted in the main text ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "415 Proof of Lemma 3.3. For a given $A\\in S$ , since $A^{-1}\\in S$ , we have $Z\\,\\overset{d}{=}\\,A^{-1}Z:=\\tilde{Z}$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Z}[f(Z)]=\\mathbb{E}_{A^{-1}Z}[f(Z)]=\\mathbb{E}_{\\tilde{Z}}[f(A\\tilde{Z})]=\\mathbb{E}_{Z}[f(A Z)].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This naturally leads to ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Z}[\\mathbb{E}_{A^{\\prime}\\sim\\nu}[f(A^{\\prime}Z)]]=\\mathbb{E}_{A^{\\prime}\\sim\\nu}[\\mathbb{E}_{Z}[f(A^{\\prime}Z)]]=\\mathbb{E}_{A^{\\prime}\\sim\\nu}[\\mathbb{E}_{Z}[f(Z)]]=\\mathbb{E}_{Z}[f(Z)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "416 where the first equality comes from Fubini\u2019s theorem and the integrability of $f$ . ", "page_idx": 12}, {"type": "text", "text": "417 Proof of Theorem 3.4. Since ${\\hat{\\beta}}=X^{\\dagger}\\mathbf{y}$ , we have $\\operatorname{Cov}({\\hat{\\beta}}\\mid X)=X^{\\dagger}\\operatorname{Cov}(y\\mid X)X^{\\dagger\\top}=X^{\\dagger}\\Omega X^{\\dagger\\top}$ , which   \n418 leads to the following expression for the variance component of prediction risk: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{\\Sigma}({\\hat{\\beta}}\\mid X)=\\operatorname{Tr}(\\operatorname{Cov}({\\hat{\\beta}}\\mid X)\\Sigma)=\\operatorname{Tr}(X^{\\dagger}\\Omega X^{\\dagger^{\\top}}\\Sigma)=\\|S X^{\\dagger}T\\|_{F}^{2}=\\|B T\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "419 where $S=\\Sigma^{1/2},T=\\Omega^{1/2}$ , and $B=S X^{\\dagger}$ . Using the singular value decomposition (SVD) of $B$ and $T$ ,   \n420 respectively, we can rewrite this as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|B T\\|_{F}^{2}=\\|U D V^{\\top}U_{T}D_{T}V_{T}^{\\top}\\|_{F}^{2}\\!=\\|D V^{\\top}U_{T}D_{T}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "421 where $B=U D V^{\\top}$ and $T=U_{T}D_{T}V_{T}^{\\top}$ with orthogonal matrices $U,V,U_{T},V_{T}$ , and diagonal matrices   \n422 $D,D_{T}$ . Now we need to compute the alignment $V^{\\top}U_{T}$ of the right-singular vectors of $B$ with the ", "page_idx": 12}, {"type": "text", "text": "423 left-eigenvectors of $T$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\|D V^{\\top}U_{T}D_{T}\\|_{F}^{2}=\\displaystyle\\sum_{i,j=1}^{n}\\left(D_{i i}\\sum_{k=1}^{n}V_{i k}^{\\top}(U_{T})_{k j}(D_{T})_{j j}\\right)^{2}}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{i,j=1}^{n}\\lambda_{i}(B)^{2}\\lambda_{j}(T)^{2}\\gamma_{i j}}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{i,j=1}^{n}\\lambda_{i}\\left((X^{\\top}X)^{\\dagger}\\Sigma\\right)\\lambda_{j}(\\Omega)\\gamma_{i j}}\\\\ {\\displaystyle}&{\\displaystyle=\\underbrace{\\lambda\\left((X^{\\top}X)^{\\dagger}\\Sigma\\right)^{\\top}}_{\\mathrm{\\normalfont~1sn}}\\!\\!\\!\\!\\prod_{n>n}^{\\Gamma}\\!\\!\\!\\!\\!\\frac{\\lambda(\\Omega)}{n\\times n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "424 where $\\gamma_{i j}:=\\langle V_{:i},(U_{T})_{:j}\\rangle^{2}\\,\\geq0,\\,\\Gamma(X)\\,:=(\\gamma_{i j})_{i,j}\\,\\in\\,\\mathbb{R}^{n\\times n}$ and $\\lambda(A)\\,\\in\\,\\mathbb{R}^{n}$ is a vector with its element   \n425 $\\lambda_{i}(A)$ as the $i$ -th largest eigenvalue of $A$ . ", "page_idx": 13}, {"type": "text", "text": "426 Therefore, we can rewrite the variance as $\\operatorname{Var}_{\\Sigma}({\\hat{\\beta}}\\mid X)=a(X)^{\\top}\\Gamma(X)b$ with ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a(X):=\\lambda\\left((X^{\\top}X)^{\\dagger}\\Sigma\\right)\\in\\mathbb{R}^{n},}\\\\ &{\\qquad b:=\\lambda(\\Omega)\\in\\mathbb{R}^{n},}\\\\ &{\\Gamma(X)_{i j}=\\gamma_{i j}=\\langle\\nu^{(i)},u^{(j)}\\rangle^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "427 where $\\boldsymbol\\nu^{(i)}:=\\boldsymbol V_{:i}$ and $u^{(j)}:=(U_{T})_{:j}$ . Note that the alignment matrix $\\Gamma(X)$ is a doubly stochastic matrix   \n428 since $\\begin{array}{r}{\\sum_{j}\\gamma_{i j}=\\sum_{i}\\gamma_{i j}=1}\\end{array}$ and $0\\leq\\gamma_{i j}\\leq1$ .   \n429 Now, we want to compute the expected variance. To do so, from Lemma 3.3 with $S={\\cal O}(n)$ , we can   \n430 obtain ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X}[a(X)^{\\top}\\Gamma(X)b]=\\mathbb{E}_{X}\\left[\\mathbb{E}_{O\\sim\\nu}[a(O X)^{\\top}\\Gamma(O X)b]\\right]=\\mathbb{E}_{X}\\left[a(X)^{\\top}\\mathbb{E}_{O\\sim\\nu}[\\Gamma(O X)]b\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "431 where $\\nu$ is the unique uniform distribution (the Haar measure) over the orthogonal matrices $O(n)$ . For   \n432 an orthogonal matrix $O\\in O(n)$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Gamma(O X)_{i j}=\\langle O\\nu^{(i)},u^{(j)}\\rangle^{2}=(\\nu^{(i)\\top}O^{\\top}u^{(j)})^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "433 since $S(O X)^{\\dagger}=S X^{\\dagger}O^{\\top}=B O^{\\top}=U D(O V)^{\\top}$ . Here, $(O X)^{\\dagger}=X^{\\dagger}O^{\\top}$ follows from the orthogonality   \n434 of $O\\in O(n)$ . Since the Haar measure is invariant under the matrix multiplication in $O(n)$ , if we take   \n435 the expectation over the Haar measure, then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{\\Gamma}(X)_{i j}:=\\mathbb{E}_{O\\sim\\nu}[\\Gamma(O X)_{i j}]=\\mathbb{E}_{O\\sim\\nu}[(\\nu^{(i)\\top}O^{\\top}u^{(j)})^{2}]=\\mathbb{E}_{O\\sim\\nu}[(\\nu^{(i)\\top}O^{\\top}O^{(j)\\top}u^{(j)})^{2}].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "436 Here, for a given $j$ , we can choose a matrix $O^{(j)}\\ \\in\\ O(n)$ such that its first column is $u^{(j)}$ and   \n437 $O^{(j)\\top}u^{(j)}=\\bar{e_{1}}$ , then $\\bar{\\Gamma}(X)_{i j}$ is independent of $j$ (say $\\bar{\\Gamma}(X)_{i j}=\\alpha_{i})$ . Since $\\Gamma(X)$ is doubly stochastic,   \n438 so is $\\bar{\\Gamma}(X)$ and we have $\\begin{array}{r}{\\dot{\\sum}_{j=1}^{n}\\,\\bar{\\Gamma}(X)_{i j}\\,=\\,n\\alpha_{i}\\,=\\,1}\\end{array}$ which yields $\\bar{\\Gamma}(X)_{i j}\\,=\\,\\alpha_{i}\\,=\\,1/n.$ , regardless of the   \n439 distribution of $V$ ; thus, $\\textstyle{\\bar{\\Gamma}}(X)={\\frac{1}{n}}J$ , where $J_{i j}=1(i,j=1,2,\\cdots,n)$ . ", "page_idx": 13}, {"type": "text", "text": "440 Therefore, we have the expected variance as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}[\\mathrm{Var}_{\\Sigma}(\\hat{\\beta}\\mid X)]=\\mathbb{E}_{X}[a(X)^{\\top}\\frac{1}{n}J b]=\\frac{1}{n}\\sum_{i,j=1}^{n}\\mathbb{E}_{X}[a_{i}(X)]b_{j}=\\frac{1}{n}\\mathbb{E}_{X}[\\mathrm{Tr}((X^{\\top}X)^{\\dagger}\\Sigma)]\\,\\mathrm{Tr}(\\Omega).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "441 ", "page_idx": 13}, {"type": "text", "text": "442 Proof of Corollary 4.5. Note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X}[\\mathrm{Var}(\\hat{\\beta}|X)]=\\frac{\\mathrm{Tr}(\\Omega)}{p}\\mathbb{E}_{X}\\Bigg[\\frac{1}{n}\\sum_{i}\\frac{1}{\\lambda_{i}}\\Bigg]}\\\\ &{\\mathrm{~\\~\\}=\\frac{\\mathrm{Tr}(\\Omega)}{p}\\mathbb{E}_{X}\\Bigg[\\int\\frac{1}{s}d F^{X X^{\\top}/p}(s)\\Bigg]}\\\\ &{\\mathrm{~\\~\\}=\\frac{\\mathrm{Tr}(\\Omega)}{n}\\mathbb{E}_{X}\\Bigg[\\int\\frac{1}{s}d F^{X X^{\\top}/n}(s)\\Bigg]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "443 Then, the desired result follows directly from equation 5. ", "page_idx": 13}, {"type": "text", "text": "444 Proof of equation 4. The bias term of the prediction risk can be expressed as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathrm{Bias}_{\\Sigma}(\\hat{\\beta}\\mid X)]^{2}=\\|\\mathbb{E}[\\hat{\\beta}\\mid X]-\\beta\\|_{\\Sigma}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|(\\hat{\\Sigma}^{\\dagger}\\hat{\\Sigma}-I)\\beta\\|_{\\Sigma}^{2}}\\\\ &{\\qquad\\qquad\\quad=\\beta^{\\top}(I-\\hat{\\Sigma}^{\\dagger}\\hat{\\Sigma})\\Sigma(I-\\hat{\\Sigma}^{\\dagger}\\hat{\\Sigma})\\beta}\\\\ &{\\qquad\\qquad\\quad=\\beta^{\\top}\\operatorname*{lim}_{\\lambda\\setminus0}\\lambda(\\hat{\\Sigma}+\\lambda I)^{-1}\\Sigma\\operatorname*{lim}_{\\lambda\\setminus0}\\lambda(\\hat{\\Sigma}+\\lambda I)^{-1}\\beta}\\\\ &{\\qquad\\qquad\\quad=(S\\beta)^{\\top}\\operatorname*{lim}_{\\lambda\\setminus0}\\lambda^{2}(S^{-1}\\hat{\\Sigma}S+\\lambda I)^{-2}S\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "445 where $\\hat{\\Sigma}=X^{\\top}X/n$ . Here, the fourth equality comes from the equation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I-\\hat{\\Sigma}^{\\dagger}\\hat{\\Sigma}=\\displaystyle\\operatorname*{lim}_{\\lambda\\setminus0}I-(\\hat{\\Sigma}+\\lambda I)^{-1}\\hat{\\Sigma}}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{lim}_{\\lambda\\setminus0}I-(\\hat{\\Sigma}+\\lambda I)^{-1}(\\hat{\\Sigma}+\\lambda I-\\lambda I)}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{lim}_{\\lambda\\setminus0}\\lambda(\\hat{\\Sigma}+\\lambda I)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "446 ", "page_idx": 14}, {"type": "text", "text": "447 Proof of equation 7. The RHS of equation 6 is bounded above by $\\textstyle\\int{\\frac{1}{1+c_{H}s^{*}}}d H(\\tau)={\\frac{1}{1+c_{H}s^{*}}}$ , and thus   \n448 $\\begin{array}{r}{1-\\frac{1}{\\gamma}\\leq\\frac{1}{1+c_{H}s^{*}}}\\end{array}$ , which yields $\\begin{array}{r}{s^{*}\\leq c_{H}^{-1}\\frac{1}{\\gamma-1}}\\end{array}$ . We can similarly prove the other inequality in equation 7   \n449 with a lower bound $\\frac{1}{1\\!+\\!C_{H}s^{*}}$ on the RHS of equation 6. \u25a1   \n450 Proof of equation 8. To further explore the inequalities equation 7, we rewrite equation 6 from   \n451 Theorem 4.7 as follows: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n1-{\\frac{1}{\\gamma}}=\\mathbb{E}_{\\tau\\sim H}\\left[g(\\tau;s^{*})\\right],\\quad{\\mathrm{where~}}g(t;s):={\\frac{1}{1+t s}}{\\mathrm{~for~}}t,s>0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, since $g(t;s)$ is convex with respect to $t>0$ for a given $s>0$ , by Jensen\u2019s inequality, we then have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\tau\\sim H}[g(\\tau;\\mu_{H}^{-1}s_{\\mathrm{iso}}^{*})]\\ge g\\left(\\mu_{H};\\mu_{H}^{-1}s_{\\mathrm{iso}}^{*}\\right)=g(1;s_{\\mathrm{iso}}^{*})=1-\\gamma^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "452 where $\\mu_{H}=\\mathbb{E}_{\\tau\\sim H}[\\tau]$ . Therefore, the limit Stieltjes transform $s^{*}$ in the anisotropic case should be larger   \n453 than $\\mu_{H}^{-1}s_{\\mathrm{iso}}^{*}$ of the isotropic case to satisfy $\\mathbb{E}_{\\tau\\sim H}[g(\\tau;s^{*})]=1\\!-\\!\\gamma^{-1}$ since $g(t;s)$ is a decreasing function   \n454 with respect to $s\\geq0$ when $t>0$ . This leads to a tighter lower bound $s^{*}\\geq\\mu_{H}^{-1}s_{\\mathrm{iso}}^{*}=\\mu_{H}^{-1}(\\gamma-1)^{-1}$ than   \n455 equation 7 because $\\mu_{H}\\leq C_{H}$ . \u25a1 ", "page_idx": 14}, {"type": "text", "text": "456 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "457 1. Claims   \n458 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n459 paper\u2019s contributions and scope?   \n460 Answer: [Yes]   \n461 Justification: The claims accurately reflect the paper\u2019s contributions and scope.   \n462 Guidelines:   \n463 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n464 made in the paper.   \n465 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n466 contributions made in the paper and important assumptions and limitations. A No or   \n467 NA answer to this question will not be perceived well by the reviewers.   \n468 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n469 much the results can be expected to generalize to other settings.   \n470 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n471 are not attained by the paper.   \n472 2. Limitations   \n473 Question: Does the paper discuss the limitations of the work performed by the authors?   \n474 Answer: [Yes]   \n475 Justification: See the last paragraph of Introduction.   \n476 Guidelines:   \n477 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n478 the paper has limitations, but those are not discussed in the paper.   \n479 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n480 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n481 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n482 model well-specification, asymptotic approximations only holding locally). The authors   \n483 should reflect on how these assumptions might be violated in practice and what the   \n484 implications would be.   \n485 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n486 only tested on a few datasets or with a few runs. In general, empirical results often   \n487 depend on implicit assumptions, which should be articulated.   \n488 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n489 For example, a facial recognition algorithm may perform poorly when image resolution   \n490 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n491 used reliably to provide closed captions for online lectures because it fails to handle   \n492 technical jargon.   \n493 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n494 and how they scale with dataset size.   \n495 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n496 address problems of privacy and fairness.   \n497 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n498 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n499 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n500 judgment and recognize that individual actions in favor of transparency play an impor  \n501 tant role in developing norms that preserve the integrity of the community. Reviewers   \n502 will be specifically instructed to not penalize honesty concerning limitations.   \n503 3. Theory Assumptions and Proofs   \n504 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 15}, {"type": "text", "text": "505 a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We explicitly mention the assumptions and provide a complete proof (see Appendix). ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "520 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide the code (supplementary material) and all the information needed to reproduce the main results (Appendix). ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "559 5. Open access to data and code ", "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instruc  \n1 tions to faithfully reproduce the main experimental results, as described in supplemental   \n62 material?   \n3 Answer: [Yes]   \n64 Justification: We provide the code (supplementary material) and all the information needed   \n65 to reproduce the main results (Appendix).   \n66 Guidelines:   \n67 \u2022 The answer NA means that paper does not include experiments requiring code.   \n68 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n69 public/guides/CodeSubmissionPolicy) for more details.   \n0 \u2022 While we encourage the release of code and data, we understand that this might not be   \n71 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n2 including code, unless this is central to the contribution (e.g., for a new open-source   \n73 benchmark).   \n74 \u2022 The instructions should contain the exact command and environment needed to run to   \n75 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n6 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how   \n78 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n79 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n80 proposed method and baselines. If only a subset of experiments are reproducible, they   \nshould state which ones are omitted from the script and why.   \n82 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n83 versions (if applicable).   \n84 \u2022 Providing as much information as possible in supplemental material (appended to the   \n85 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We provide all the details necessary to understand the results (Appendix). Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "598 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: To support our claim, the experiments in the paper do not need error bars. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "624 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "625 Question: For each experiment, does the paper provide sufficient information on the com  \n626 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n627 the experiments? ", "page_idx": 18}, {"type": "text", "text": "Justification: Our models are linear regression models which do not require much resources. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "650 10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "651 Question: Does the paper discuss both potential positive societal impacts and negative   \n652 societal impacts of the work performed?   \n653 Answer: [NA]   \n654 Justification: The paper is a theoretical work.   \n655 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 18}, {"type": "text", "text": "663 \u2022 The conference expects that many papers will be foundational research and not tied   \n664 to particular applications, let alone deployments. However, if there is a direct path to   \n665 any negative applications, the authors should point it out. For example, it is legitimate   \n666 to point out that an improvement in the quality of generative models could be used to   \n667 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n668 that a generic algorithm for optimizing neural networks could enable people to train   \n669 models that generate Deepfakes faster.   \n670 \u2022 The authors should consider possible harms that could arise when the technology is   \n671 being used as intended and functioning correctly, harms that could arise when the   \n672 technology is being used as intended but gives incorrect results, and harms following   \n673 from (intentional or unintentional) misuse of the technology.   \n674 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n675 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n676 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n677 feedback over time, improving the efficiency and accessibility of ML).   \n678 11. Safeguards   \n679 Question: Does the paper describe safeguards that have been put in place for responsible   \n680 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n681 image generators, or scraped datasets)?   \n682 Answer: [NA]   \n683 Justification: The paper is a theoretical work.   \n684 Guidelines:   \n685 \u2022 The answer NA means that the paper poses no such risks.   \n686 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n687 necessary safeguards to allow for controlled use of the model, for example by requiring   \n688 that users adhere to usage guidelines or restrictions to access the model or implementing   \n689 safety filters.   \n690 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n691 should describe how they avoided releasing unsafe images.   \n692 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n693 not require this, but we encourage authors to take this into account and make a best   \n694 faith effort.   \n695 12. Licenses for existing assets   \n696 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n697 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n698 properly respected?   \n699 Answer: [Yes]   \n700 Justification: We used some datasets and properly credited the creators of assets (cf. ACS   \n701 2018 [24]).   \n702 Guidelines:   \n703 \u2022 The answer NA means that the paper does not use existing assets.   \n704 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n705 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n706 URL.   \n707 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n708 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n709 service of that source should be provided.   \n710 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n711 package should be provided. For popular datasets, paperswithcode.com/datasets   \n712 has curated licenses for some datasets. Their licensing guide can help determine the   \n713 license of a dataset.   \n714 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n715 the derived asset (if it has changed) should be provided.   \n716 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n717 the asset\u2019s creators.   \n718 13. New Assets   \n719 Question: Are new assets introduced in the paper well documented and is the documentation   \n720 provided alongside the assets?   \n721 Answer: [Yes]   \n722 Justification: We provide the code with the details as a supplementary material.   \n723 Guidelines:   \n724 \u2022 The answer NA means that the paper does not release new assets.   \n725 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n726 submissions via structured templates. This includes details about training, license,   \n727 limitations, etc.   \n728 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n729 asset is used.   \n730 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n731 create an anonymized URL or include an anonymized zip file.   \n732 14. Crowdsourcing and Research with Human Subjects   \n733 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n734 include the full text of instructions given to participants and screenshots, if applicable, as   \n735 well as details about compensation (if any)?   \n736 Answer: [NA]   \n737 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n738 Guidelines:   \n739 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n740 human subjects.   \n741 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n742 tion of the paper involves human subjects, then as much detail as possible should be   \n743 included in the main paper.   \n744 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n745 or other labor should be paid at least the minimum wage in the country of the data   \n746 collector.   \n747 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n748 Subjects   \nQuestion: Does the paper describe potential risks incurred by study participants, whether ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]