[{"figure_path": "yQL5tutdaH/tables/tables_6_1.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the LeHaCE scores for 12 large vision-language models (LVLMs) evaluated on two datasets: MSCOCO and NoCaps.  LeHaCE is a novel framework introduced in the paper to evaluate object hallucination in a more stable and fair manner. The table shows the scores for two variants of CHAIR (CHAIR\u2081 and CHAIRs) at four different image description lengths (20, 40, 60, and 80 words) along with the growth rate (LeHaCEGR) of hallucination. The best performing LVLMs for each metric on each dataset are highlighted in bold, while the second-best are underlined.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}, {"figure_path": "yQL5tutdaH/tables/tables_7_1.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the LeHaCE scores (CHAIR1 and CHAIRs) for twelve different large vision-language models (LVLMs) on two datasets: MSCOCO and NoCaps.  LeHaCE is a novel evaluation framework proposed in the paper, designed to provide a more stable and fair evaluation of object hallucination.  The table shows the scores for different description lengths (20, 40, 60, and 80 words) and includes the growth rate (LeHaCEGR) of hallucination.  The best performing LVLM for each metric on each dataset is highlighted in bold, with the second-best underlined. This allows for a comparison of the models' performance and stability across different description lengths.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}, {"figure_path": "yQL5tutdaH/tables/tables_8_1.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the LeHaCE scores (CHAIR\u2081 and CHAIRs) for twelve large vision-language models (LVLMs) evaluated on two benchmark datasets: MSCOCO and NoCaps.  LeHaCE scores are provided for four different description lengths (20, 40, 60, and 80 words), along with the growth rate (LeHaCEGR) indicating the change in hallucination rate with respect to description length. The best performing LVLM for each metric in each dataset is highlighted in bold, while the second-best is underlined.  This allows for a comparison of LVLMs' performance in terms of hallucination rates at various description lengths.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}, {"figure_path": "yQL5tutdaH/tables/tables_15_1.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the LeHaCE scores for twelve Large Vision-Language Models (LVLMs) on two benchmark datasets: MSCOCO and NoCaps.  LeHaCE is a novel evaluation framework proposed in the paper. The table shows the performance of each LVLM using two metrics derived from CHAIR (CHAIR\u2081 and CHAIRs) at four different lengths (20, 40, 60, and 80 words) of image descriptions. The best and second-best performing models for each metric and dataset are highlighted.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}, {"figure_path": "yQL5tutdaH/tables/tables_16_1.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the results of evaluating twelve Large Vision-Language Models (LVLMs) using the LeHaCE framework on two datasets: MSCOCO and NoCaps.  LeHaCE is a novel evaluation framework for object hallucination in LVLMs. The table shows the LeHaCE scores for two metrics: CHAIR\u2081 and CHAIRs, at four different lengths (20, 40, 60, and 80 words) of image descriptions.  The best and second-best performing models for each metric and dataset are highlighted.  This allows for a comparison of LVLMs across different description lengths and highlights models that are consistently strong performers or those whose performance is significantly affected by description length.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}, {"figure_path": "yQL5tutdaH/tables/tables_18_1.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the LeHaCE scores (a novel framework for evaluating object hallucination in Large Vision-Language Models) for twelve different LVLMs on two datasets: MSCOCO and NoCaps.  LeHaCE uses two metrics, Lc\u2081 (CHAIR\u2081) and Lcs (CHAIRs), to assess the hallucination rate.  The table highlights the best and second-best performance for each metric and dataset, indicating superior performance among the models.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}, {"figure_path": "yQL5tutdaH/tables/tables_19_1.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the LeHaCE (Length-Hallucination Curve-based Evaluation Framework) scores for twelve large vision-language models (LVLMs) on two datasets: MSCOCO and NoCaps.  LeHaCE evaluates object hallucination at specific description lengths and includes two metrics: Lc\u2081 (CHAIR\u2081) and Lcs (CHAIRs).  The table highlights the best and second-best performing models for each metric on each dataset, indicating the relative strengths of different LVLMs in handling object hallucination.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}, {"figure_path": "yQL5tutdaH/tables/tables_19_2.jpg", "caption": "Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. Lc\u2081 and Lcs represent CHAIR\u2081 and CHAIRs with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.", "description": "This table presents the results of evaluating twelve Large Vision-Language Models (LVLMs) using the LeHaCE framework on two datasets: MSCOCO and NoCaps.  LeHaCE is a novel evaluation framework proposed in the paper to assess object hallucinations, which are inconsistencies between generated image descriptions and the actual image content.  The table shows the LeHaCE scores for two different metrics: CHAIR\u2081 and CHAIRs, which measure different aspects of hallucinations.  The best and second-best scores for each metric on each dataset are highlighted for easy comparison of the LVLMs.", "section": "4.3 Evaluation on MSCOCO and NoCaps"}]