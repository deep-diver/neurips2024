[{"type": "text", "text": "Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hongliang Wei, Xingtao Wang\u2217, Xianqi Zhang, Xiaopeng Fan, Debin Zhao Harbin Institute of Technology Harbin, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given different instructions, large vision-language models (LVLMs) exhibit different degrees of object hallucinations, posing a significant challenge to the evaluation of object hallucinations. Overcoming this challenge, existing object hallucination evaluation methods average the results obtained from a set of instructions. However, these methods fail to provide consistent evaluation across instruction sets that generate image descriptions of significantly different lengths. In this paper, we present the first systematic investigation into the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by image description lengths. A valuable finding is that instructions indirectly affect hallucinations through the length of image descriptions. The longer the image description, the higher the object hallucination degree. Accordingly, we fti an informative length-hallucination curve, upon which a fine-grained evaluation framework named LeHaCE is introduced for evaluating object hallucinations at any given image description length. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of description lengths, promoting stability and fairness. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation. Experimental results demonstrate that LeHaCE provides a more stable, fair, and comprehensive evaluation of object hallucinations in LVLMs compared to existing methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Drawing inspiration from the remarkable language capabilities exhibited by large language models (LLMs) [1\u20133], large vision-language models (LVLMs) [2, 4\u20137] have been well-developed, achieving significant advancements in complex multimodal tasks. However, the practical application of LVLMs is heavily hindered by hallucination phenomena [8, 9], which refer to situations where objects in image descriptions generated by LVLMs are inconsistent with the provided visual content. Considerable efforts have been dedicated to both evaluation [9\u201311] and mitigation [12\u201314] of hallucination phenomena, leading to notable advancements. ", "page_idx": 0}, {"type": "text", "text": "A significant challenge in object hallucination evaluation arises from the effect of instructions on object hallucinations [9]. Overcoming this challenge, existing object hallucination evaluation methods typically adopt an average-based framework, which averages the results obtained from a set of instructions. However, as shown in Figure 1, this framework fails to provide consistent evaluation across instruction sets that generate image descriptions of significantly varying lengths. Specifically, while evaluation results of LVLMs remain consistent across certain instruction sets (e.g., set 1 and set 2), inconsistencies arise when comparing instruction sets with significantly different average image description lengths (e.g., set 2 and set 3). ", "page_idx": 0}, {"type": "image", "img_path": "yQL5tutdaH/tmp/80472085c7cef61e17ba8fec23eab1222fb67807b924874d1fc29acc97dd4096.jpg", "img_caption": ["Figure 1: The evaluation results of LVLMs on four instruction sets using the CHAIR with the average-based framework. Length refers to the average length of generated image descriptions. Each instruction set consists of six distinct instructions, and there is no overlap between instructions in different sets. All instructions prompt LVLMs to describe the image. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we present the first systematic investigation into the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by the length of image descriptions. Technically, we evaluate lengths and object hallucination degrees (measured by CHAIR scores) of the image descriptions generated by LVLMs under different instructions (see Section 3.1 for more details). The experimental results are shown in Figures 2 & 3, from which we can observe that the degree of object hallucination is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on description lengths. The longer the image description, the higher the object hallucination degree, and there is a clear linear relation between them. Hence, it is imperative to take into account the length of image descriptions in hallucination evaluation. Unfortunately, the average-based framework can only select instructions, without the ability to directly control the length of image descriptions. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the findings, we propose a fine-grained evaluation framework called LeHaCE, which fits an informative length-hallucination curve to evaluate object hallucinations at any given image description length within a large range. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of image description length, ensuring stable evaluations for the same LVLM across different instruction sets and fair comparisons among different LVLMs. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is influenced by the image description length, achieving a more comprehensive evaluation. Experiment results on 12 representative LVLMs show that LeHaCE can evaluate object hallucinations of LVLMs in a more stable, fair, and comprehensive way. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We conduct the first systematic investigation into the effect of instructions on object hallucinations in LVLMs and find that the degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths.   \n\u2022 We propose an object hallucination evaluation framework called LeHaCE, which fits an informative length-hallucination curve to evaluate object hallucination at a uniform image description length, realizing a more stable and fair evaluation.   \n\u2022 We employ the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Large Vision-Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inspired by the success of LLMs in NLP [1\u20133], researchers have extended LLMs to multimodal tasks [15\u201328], proposing numerous LVLMs and achieving new advancements [7, 14, 29\u201335]. These LVLMs align the multi-modal encoders with LLM through multitask fine-tuning and instruction fine-tuning on multi-modal datasets, enabling LLM to acquire multi-modal perception and instructionfollowing capabilities. Specifically, to integrate multimodal features, Flamingo [29] proposes a cross-attention structure to achieve arbitrary interleaved multi-modal feature fusion. BLIP-2 [35] introduces Q-Former to bridge the visual backbone model and LLM. mPLUG-Owl2 [36] introduces a modality adaptive module to facilitate the fusion between different modules. To enhance generalization and improve instruction-following capabilities, some methods [4\u20136, 12, 37] propose multi-task fine-tuning and instruction fine-tuning for LVLMs. Among them, LRV-instruction [12], MiniGPT-4 [5], LLaVA [6] and SViT [37] employ ChatGPT to augment instruction data. To mitigate the risk of catastrophic forgetting of language knowledge during the training process, mPLUG-Owl [38] and LLaVA-1.5 [6] perform joint training on pure language and visual-language instructional data. More recently, mPLUG-DocOwl [31], InternLM-XComposer [32], Kosmos-2 [33], Shikra [34], Cantor [39], BuboGPT [30], and Qwen-VL [7] further enhance the capabilities of LVLMs in optical character recognition, document understanding, multi-modal interleaved composition and visual grounding. ", "page_idx": 2}, {"type": "text", "text": "2.2 Hallucination in LVLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Works on the hallucination in LVLMs focus on two aspects: evaluation and mitigation. For the hallucination evaluation, POPE [9] designs a polling-based query method to avoid the influence of instructions on hallucination evaluation. By presenting LVLMs with brief \"yes\" or \"no\" questions regarding the target of detection, the evaluation of hallucination is transformed into a simple binary classification task. NOPE [10] designs a novel benchmark to evaluate the performance of LVLMs in recognizing the non-existence of objects in visual questions. AMBER [11] designs a multi-dimensional LVLMs hallucination evaluation benchmark without LLMs, targeting existence, attribute, and relation hallucination. For the hallucination mitigation, LRV-Instruction [40] creates a balanced set of positive and negative instructions to perform robust visual instruction adjustment for LVLMs. VIGC [14] employs an iterative approach to generate detailed and accurate answers gradually. Woodpecker [13] proposes a postprocessing method that utilizes expert models to locate and correct hallucinations from generated text. ", "page_idx": 2}, {"type": "text", "text": "While existing methods [9] observe that the hallucination degree of LVLMs is unstable across different instructions, this phenomenon has not been thoroughly investigated to date. This work presents the first comprehensive study investigating the influence of instructions on the hallucination rate of LVLMs. Building upon our findings, we propose LeHaCE framework, which can evaluate hallucination of LVLMs in a more stable and comprehensive manner. Contrary to polling-based query methods, LeHaCE can directly evaluate the hallucination rate of image descriptions generated by LVLMs, which is more in line with the practical application scenarios of LVLMs. ", "page_idx": 2}, {"type": "text", "text": "3 Hallucination of LVLMs Under Different Instructions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section provides the investigation into the effect of instructions on hallucinations, with a specific focus on the role played by the length of image descriptions. The experimental settings are presented initially, followed by a comprehensive analysis of the experimental results ", "page_idx": 2}, {"type": "text", "text": "3.1 Experimental Settings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this investigation, twelve popular LVLMs are included, namely Gemini-Pro-Vision pro [2], QwenVL [7], MiniGPT-4 [5], LLaVA [6], InstructBLIP [4], LLaMA-Adapter-v2 [41], mPLUG-Owl2 [36], mPLUG-Owl [38], InternLM-XComposer [32], VPGTrans [42], Otter [43] and Lynx [44]. All LVLMs are prompted by 25 different instructions to generate image descriptions for 256 images in MSCOCO [45]. All descriptions are generated using beam search with a beam size of 5. For the instructions, we utilize those from [4] and additionally propose several others, as detailed in the appendix. We use CHAIR [8] as the evaluation metric for hallucinations, which has two variants: ${\\mathrm{CHAIR}}_{\\mathrm{I}}$ and CHAIRS. Given the ground truth objects in the image, CHAIRI calculates the proportion of objects that appear in the descriptions but not in the image, while $\\mathrm{CHAIR_{S}}$ is the proportion of descriptions that include hallucination. Formally, CHAIRI and CHAIRS can be expressed as follows: ", "page_idx": 2}, {"type": "image", "img_path": "yQL5tutdaH/tmp/ebf49ec23fb0cd041126addbd45544cfbb9d3bb33bf82232ab9b4965ee8c3be8.jpg", "img_caption": ["Figure 2: Scatter plots of CHAIR scores and average lengths of the 25 sets of image descriptions generated by 25 instructions. $r$ denotes the Pearson correlation coefficient between the hallucination rates and the average image description lengths, $R^{2}$ and $P$ represent the coefficient of determination and $p$ -value respectively for the linear regression. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{CHAIR}_{I}=\\frac{\\left|\\left\\{\\mathrm{hallucinated\\;objects}\\right\\}\\right|}{\\left|\\left\\{\\mathrm{all\\;mentioned\\;objects}\\right\\}\\right|},\\quad}\\\\ &{}&{\\mathrm{CHAIR}_{S}=\\frac{\\left|\\left\\{\\mathrm{descriptions\\;with\\;hallucinated\\;objects}\\right\\}\\right|}{\\left|\\left\\{\\mathrm{all\\;descriptions}\\right\\}\\right|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For more experimental settings, we use the Pearson correlation coefficient to measure the correlation between the average length and the hallucination rate of image descriptions. Lengths are measured in word count. ", "page_idx": 3}, {"type": "text", "text": "3.2 Experimental Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The results are presented in Figure 2 & 3, from which we get two key observations: 1) Figure 2 shows the relationship between the hallucination rate and the average image description length, we can observe that the hallucination rate increases with the average image description length and there is a clear linear correlation between them. Specifically, the Pearson correlation coefficient between hallucination rates and the average image description lengths exceeds 0.6 for all LVLMs, with 10 LVLMs exceeding 0.8 and 5 LVLMs exceeding 0.9. 2) Figure 3 shows the impact of instructions on the length of image descriptions generated by LVLMs, from which we can observe that the length of image descriptions generated by the same LVLM with different instructions can vary significantly, e.g., Gemini-Pro-Vision with Instruction 11 (101 words in average) v.s. Gemini-Pro-Vision with Instruction 18 (20 words in average). Furthermore, the length of image descriptions generated by different LVLMs with the same instruction can also differ greatly, e.g., MiniGPT-4 (11 words in average) v.s. Gemini-ProVision (97 words in average) with Instruction 17. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Based on the aforementioned two observations, we can draw the following conclusions: 1) The degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths. Hence, it is imperative to take into account the length of image descriptions in hallucination evaluation. However, controlling the length of image descriptions generated by LVLMs is challenging 2, given that even subtle semantic differences between instructions can significantly impact the output length of LVLMs (shown in Figure 3). 2) In addition to the hallucination degree , the rate at which hallucination degree increase with description length is also a meaningful indicator for characterizing the nature of LVLMs hallucinations. Considering both the hallucination degree and the growth rate of hallucination degree can provide a more comprehensive evaluation for hallucinations in LVLMs. For example, as shown in the Figure 2, although InstructBLIP has the lowest hallucination degrees in short image descriptions, it exhibits high instability with a rapid increase in hallucination degrees, resulting in high hallucination in long image descriptions. ", "page_idx": 4}, {"type": "image", "img_path": "yQL5tutdaH/tmp/450dbfbfeeaf8c62ed83bd1ad39356855950e02eec2dbaa814fd2aa3c48636c2.jpg", "img_caption": ["Figure 3: The average lengths of image descriptions generated by LVLMs when prompted by different instructions. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4 Length-Hallucination Curve Based Hallucination Evaluation Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we first introduce the average-based hallucination evaluation framework and discuss its limitations. Then, we elaborate on the proposed LeHaCE framework and evaluate representative LVLMs with LeHaCE. Finally, the stability of LeHaCE is analysed. ", "page_idx": 4}, {"type": "text", "text": "4.1 Average-Based Hallucination Evaluation Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The average-based hallucination evaluation framework mitigates the challenge caused by instructions by averaging the hallucination rates over different instructions. Formally, the hallucination rates and average lengths of the image descriptions generated by the LVLM under N instructions are denoted as $\\{\\ell_{i},h r_{i}\\}_{i=1}^{N}$ . The average hallucination rate $\\bar{h}r$ and average length $\\bar{\\ell}$ of image descriptions over all instructions can be calculated as follows: $\\begin{array}{r}{\\bar{h^{r}}=\\frac{1}{N}\\!\\sum_{i=1}^{N}\\!h r_{i}}\\end{array}$ and $\\begin{array}{r}{\\bar{\\ell}=\\frac{1}{N}{\\sum_{i=1}^{N}}\\ell_{i}}\\end{array}$ . The average-based hallucination evaluation framework utilizes $\\bar{h}r$ to evaluate the hallucination of LVLMs. ", "page_idx": 4}, {"type": "text", "text": "However, due to substantial variations in the average lengths of image descriptions generated by different instruction sets, the average-based framework struggles to mitigate the effect of image description lengths on object hallucinations, resulting in unstable and unfair evaluations. Specifically, as shown in Figure 4 (left), when the average-based framework evaluates an LVLM under different instruction sets, the inconsistent average image description lengths lead to unstable evaluation. Moreover, Figure 4 (right) shows that when the average-based framework evaluates different LVLMs under the same instructions, the inconsistent average image description lengths lead to unfair evaluation. ", "page_idx": 4}, {"type": "text", "text": "4.2 Length-Hallucination Curve Based Hallucination Evaluation Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Section 3.2 reveals the significant effect of image description lengths on the hallucination degree. To mitigate this effect, it is crucial to control the description length during the hallucination evaluation. ", "page_idx": 4}, {"type": "image", "img_path": "yQL5tutdaH/tmp/01e0af5f09a80f6148dbb20828a3b39b635590df754c9afb1ed3d5a58d32159b.jpg", "img_caption": ["Figure 4: Illustrations of the average-based evaluation framework (ABF) and our LeHaCE framework. The left figure presents the object hallucination evaluation of LLaVa on two instruction sets. The right figure presents the object hallucination evaluation of LLaVa and mPLUG-Owl on the same set of instructions. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "However, controlling the length of generated image descriptions is challenging because LVLMs are highly sensitive to instructions. To address this challenge, we fit a length-hallucination curve to evaluate LVLMs at any desired length. Specifically, based on the clear linear correlation observed in Section 3.2, we assume a linear correlation between image description lengths and hallucination rates of LVLMs. Figure 4 intuitively illustrates the LeHaCE framework. ", "page_idx": 5}, {"type": "text", "text": "Formally, we use $\\{\\ell_{i},h r_{i}\\}_{i=1}^{N}$ to represent the average lengths and hallucination rates of image descriptions generated by the LVLM under $\\Nu$ instructions. The linear regression curve of $\\{\\ell_{i},h r_{i}\\}_{i=1}^{N}$ , which we refer to as the Length-Hallucination Curve (LHC), can be formalized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{LHC}\\left(\\ell\\right)=\\beta*\\ell+\\alpha,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ and $\\alpha$ are: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta=\\frac{\\sum_{i=1}^{N}\\left(\\ell_{i}-\\bar{\\ell}\\right)\\left(h r_{i}-\\bar{h r}\\right)}{\\sum_{i=1}^{N}\\left(\\ell_{i}-\\bar{\\ell}\\right)^{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Length-hallucination curve summarizes the trend between the hallucination rate and the description length. The regression coefficient $\\beta$ represents the rate at which the hallucination rate increases with the growth of description length. LeHaCE uses the LHC to evaluate the hallucination in LVLMs. LeHaCE consists of two metrics: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathrm{LeHaCE}\\left(\\ell\\right)=\\mathrm{LHC}\\left(\\ell\\right),}\\\\ {\\mathrm{LeHaCE}_{G R}=\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where LeHaCE (\u2113) measures the hallucination rate at the specified length $\\ell$ , and $\\mathrm{LeHaCE}_{G R}$ measures the rate at which the hallucination rate increases with the increase in description length. Note that LeHaCE can be built upon any hallucination degree evaluation metric, enhancing their stability, fairness, and comprehensiveness. In this paper, we use CHAIR as the metric for measuring the hallucination degree. ", "page_idx": 5}, {"type": "text", "text": "Compared to the average-based hallucination evaluation framework, LeHaCE has three advantages. As intuitively shown in Figure 4, LeHaCE can evaluate the hallucination degree of LVLMs at a uniform image description length, thereby mitigating the influence of description length on hallucination degree and resulting in a more stable and fair evaluation. Moreover, LeHaCE can evaluate the hallucination degree at multiple lengths and the growth rate of hallucination degree, leading to a more comprehensive evaluation. ", "page_idx": 5}, {"type": "text", "text": "4.3 Evaluation on MSCOCO and NoCaps ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate twelve LVLMs with LeHaCE at lengths of 20, 40, 60, and 80 words. This evaluation is conducted on subsets of the MSCOCO [45] test set and the NoCaps [46] validation set, each comprising randomly selected 256 images. The length-hallucination curve in LeHaCE is ftited on the CHAIR scores of image descriptions generated by 25 different instructions. To calculate CHAIR scores on NoCaps, we follow the setting proposed in [8, 47]. All descriptions are generated using beam search with a beam size of 5. The experiments are conducted with PyTorch on Nvidia GeForce RTX 3090 GPUs. ", "page_idx": 5}, {"type": "table", "img_path": "yQL5tutdaH/tmp/53cccdf443ef4c8f13c0657c6eac43508a5e798cda57b81b887b53f4168ceaaf.jpg", "table_caption": ["Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. $\\mathrm{L}_{\\mathrm{C}_{\\mathrm{I}}}$ and $\\mathrm{L}_{\\mathrm{C_{\\mathrm{S}}}}$ represent CHAIRI and ${\\mathrm{CHAIR}}_{\\mathrm{S}}$ with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The results are shown in Table 1, which demonstrate that LeHaCE can evaluate the object hallucination degree of LVLMs at given image description lengths, as well as the growth rate of the hallucination degree, providing a fair and comprehensive evaluation. Specifically, 1) For short descriptions, InstructBLIP achieves the best performance on both the MSCOCO and NoCaps datasets. However, its higher growth rate of hallucination degree leads to poor performance on longer descriptions. 2) For medium-length and long descriptions, Gemini-Pro-Vision and Qwen-VL exhibit the best performance on the MSCOCO and NoCaps datasets, respectively. This is attributed to their relatively small growth rate in hallucination degree. 3) Gemini-Pro-Vision and LLaVA exhibit the lowest growth rate in hallucination degree on the MSCOCO and NoCaps datasets, respectively. ", "page_idx": 6}, {"type": "text", "text": "In Table 1, LVLMs exhibit higher degrees of hallucination on the NoCaps dataset compared to the MSCOCO dataset. This is attributed to the fact that LVLMs typically use the MSCOCO for training, making the NoCaps dataset an out-of-distribution dataset. The results show that the distributional differences not only increase the hallucination degree of LVLMs at various description lengths but also amplify the growth rate of hallucination degree. ", "page_idx": 6}, {"type": "text", "text": "4.4 Stability of LeHaCE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As mentioned above, LeHaCE evaluates the hallucination degree of LVLMs in a more stable manner. This subsection verifies the stability of the proposed LeHaCE framework. Specifically, LVLMs are prompted by three sets of different instructions to generate three sets of image descriptions. Each instruction set consists of multiple instructions randomly drawn from a pool of 25 instructions, with no overlap between instructions in different sets. The image descriptions generated by different instructions in each set are evaluated using the LeHaCE framework and the average-based framework, respectively. The stability of the LeHaCE and the average-based frameworks on the three sets of image descriptions is evaluated using the Relative Standard Deviation (RSD), which is defined as the ratio of the standard deviation $\\sigma$ to the mean $\\mu$ , $R D S=\\sigma/\\mu$ . The lower the RSD, the more stable the results. For the number of instructions in each instruction set, we conduct extensive experiments under five different conditions: 3, 4, 5, 6, 7, and 8. The experiments are carried out 10 times using distinct instruction sets, and the final results are determined by averaging the outcomes of these 10 experiments. ", "page_idx": 6}, {"type": "table", "img_path": "yQL5tutdaH/tmp/2798d0b287d35f95a394bcf14cb93192763d32c95158b6946410e31580d4928e.jpg", "table_caption": [], "table_footnote": ["Table 2: The average RSD of CHAIR with the LeHaCE and the average-based frameworks, lower is better. ${\\mathrm{C}}_{\\mathrm{I}}$ and $C_{S}$ respectively represent $\\mathrm{CHAIR_{I}}$ and ${\\mathrm{CHAIR}}_{\\mathrm{S}}$ with the average-based hallucination evaluation framework. $\\mathrm{L}_{\\mathrm{C}_{\\mathrm{I}}}$ and $\\mathrm{L}_{\\mathrm{C_{\\mathrm{S}}}}$ respectively represent CHAIRI and CHAIRS with the LeHaCE framework. The best result under each setting is represented in bold. "], "page_idx": 7}, {"type": "table", "img_path": "yQL5tutdaH/tmp/6195cc1ce47e055aff3247d9157abf9752fa5aaef402df570238682e401783ef.jpg", "table_caption": [], "table_footnote": ["Table 3: The average RSD of CHAIR scores with the LeHaCE on MSCOCO with different fitting methods. $\\mathrm{L}_{1}$ represents LeHaCE with linear ftiting, while $\\mathrm{L}_{2}$ and $\\mathrm{L}_{3}$ represent LeHaCE with quadratic and cubic polynomial fitting, respectively. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The results are shown in Table 2, from which we can observe that LeHaCE demonstrates superior stability compared to the average-based framework in nearly all cases. Notably, 1) On the MSCOCO dataset, for the CHAIRI metric, LeHaCE consistently outperforms the average-based framework across all twelve LVLMs when the number of instructions reaches five or more. Similarly, for the ${\\mathrm{CHAIR}}_{\\mathrm{S}}$ metric, LeHaCE exhibits superior performance across all twelve LVLMs when the number of instructions reaches four or more. 2) On the NoCaps dataset, when the number of instructions reaches four or more, LeHaCE consistently outperforms the average-based framework across all twelve LVLMs on both CHAIRI and $\\mathrm{CHAIR_{S}}$ metrics. In Table 2, we observe that when the number of instructions is very low, such as three, the stability of LeHaCE is compromised due to the difficulty in accurately fitting the length-hallucination curve. However, with just four or five instructions, LeHaCE consistently exhibits superior stability. ", "page_idx": 8}, {"type": "text", "text": "To verify the validity of the linear assumption we make in the LeHaCE method, we evaluate the stability of LeHaCE with different fitting methods. Specifically, we assess the RSD of LeHaCE on the MSCOCO dataset when applying linear, quadratic, and cubic fitting. As shown in Table 3, the results indicate that linear fitting significantly outperforms polynomial fitting, particularly when the instruction count is low. ", "page_idx": 8}, {"type": "image", "img_path": "yQL5tutdaH/tmp/c2af08cfab0e8f1bbb48795835592248262835895bfdb2d6ea3234d334968280.jpg", "img_caption": ["Figure 5: Average RSD of CHAIR with the LeHaCE framework at different lengths, lower is better. ABF refers to the average-based evaluation framework. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "For the stability of LeHaCE at different lengths, the results are shown in Figure 5, from which we can see that LeHaCE significantly improves the stability of the CHAIR metrics across a wide range of description lengths. All of these experimental results validate the superior stability of LeHaCE. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion: In this paper, we find the degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths. The degree of object hallucination and the length of image descriptions exhibit a clear positive linear correlation. Based on our findings, a stable, fair and comprehensive object hallucination evaluation framework named LeHaCE is introduced. Extensive experimental results validate the superiority of LeHaCE over existing frameworks. ", "page_idx": 9}, {"type": "text", "text": "Limitations: Despite exhaustive investigations, this work still has potential limitations. 1) We focus on object hallucination, leaving other types of hallucinations for future work. 2) Due to computational constraints, we evaluate LVLMs on only a subset of each dataset. Nevertheless, we conduct thorough experiments across various datasets to validate our findings and method. 3) Due to high API fees, we only explore one proprietary business LVLM in our experiments. However, we conduct in-depth analyses on eleven open-source LVLMs, validating the broad applicability of our method. 4) In the typical practice of evaluating hallucination levels in LVLMs, multiple instructions are usually used to enhance the stability of the evaluation results. Although LeHaCE cannot be used with just one instruction, this limitation does not affect its ability to provide stable evaluations. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Key R&D Program of China (2021YFF0900500), and the National Natural Science Foundation of China (NSFC) under grants U22B2035 and 62441202. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971. ", "page_idx": 9}, {"type": "text", "text": "[2] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, ", "page_idx": 9}, {"type": "text", "text": "Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805. ", "page_idx": 10}, {"type": "text", "text": "[3] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303. 08774. URL https://doi.org/10.48550/arXiv.2303.08774. ", "page_idx": 10}, {"type": "text", "text": "[4] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html. ", "page_idx": 10}, {"type": "text", "text": "[5] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592, 2023. doi: 10.48550/ARXIV.2304.10592. URL https://doi.org/10. 48550/arXiv.2304.10592.   \n[6] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. CoRR, abs/2304.08485, 2023. doi: 10.48550/ARXIV.2304.08485. URL https://doi.org/10. 48550/arXiv.2304.08485.   \n[7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023. doi: 10.48550/ARXIV.2308.12966. URL https://doi.org/10.48550/arXiv.2308.12966. [8] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 4035\u20134045. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1437. URL https: //doi.org/10.18653/v1/d18-1437.   \n[9] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 292\u2013305. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.emnlp-main. 20.   \n[10] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. Negative object presence evaluation (NOPE) to measure object hallucination in vision-language models. CoRR, abs/2310.05338, 2023. doi: 10.48550/ARXIV.2310.05338. URL https://doi.org/ 10.48550/arXiv.2310.05338.   \n[11] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. CoRR, abs/2311.07397, 2023. doi: 10.48550/ARXIV.2311.07397. URL https: //doi.org/10.48550/arXiv.2311.07397.   \n[12] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023.   \n[13] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. CoRR, abs/2310.16045, 2023. doi: 10.48550/ARXIV.2310.16045. URL https://doi.org/10.48550/arXiv.2310.16045.   \n[14] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, and Conghui He. VIGC: visual instruction generation and correction. CoRR, abs/2308.12714, 2023. doi: 10.48550/ARXIV.2308.12714. URL https://doi.org/ 10.48550/arXiv.2308.12714.   \n[15] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156\u20133164, 2015.   \n[16] Wensheng Pan, Timin Gao, Yan Zhang, Xiawu Zheng, Yunhang Shen, Ke Li, Runze Hu, Yutao Liu, and Pingyang Dai. Semi-supervised blind image quality assessment through knowledge distillation and incremental learning. February 2024.   \n[17] Xudong Li, Timin Gao, Xiawu Zheng, Runze Hu, Jingyuan Zheng, Yunhang Shen, Ke Li, Yutao Liu, Pingyang Dai, Yan Zhang, and Rongrong Ji. Adaptive feature selection for no-reference image quality assessment using contrastive mitigating semantic noise sensitivity. July 2024.   \n[18] XuDong Li, Runze Hu, Jingyuan Zheng, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Ke Li, Yunhang Shen, Yutao Liu, Pingyang Dai, et al. Integrating global context contrast and local sensitivity for blind image quality assessment. July 2024.   \n[19] Guanyi Qin, Runze Hu, Yutao Liu, Xiawu Zheng, Haotian Liu, Xiu Li, and Yan Zhang. Dataefficient image quality assessment with attention-panel decoder. pages 2091\u20132100.   \n[20] Wenrui Li, Xi-Le Zhao, Zhengyu Ma, Xingtao Wang, Xiaopeng Fan, and Yonghong Tian. Motion-decoupled spiking transformer for audio-visual zero-shot learning. In Proceedings of the 31st ACM International Conference on Multimedia, page 3994\u20134002, 2023. ISBN 9798400701085. doi: 10.1145/3581783.3611759. URL https://doi.org/10.1145/ 3581783.3611759.   \n[21] Wenrui Li, Penghong Wang, Ruiqin Xiong, and Xiaopeng Fan. Spiking tucker fusion transformer for audio-visual zero-shot learning. IEEE Transactions on Image Processing, 33:4840\u20134852, 2024. doi: 10.1109/TIP.2024.3430080.   \n[22] Wenrui Li, Zhengyu Ma, Liang-Jian Deng, Xiaopeng Fan, and Yonghong Tian. Neuron-based spiking transmission and reasoning network for robust image-text retrieval. IEEE Transactions on Circuits and Systems for Video Technology, 33(7):3516\u20133528, 2023. doi: 10.1109/TCSVT. 2022.3233042.   \n[23] Zhuangzhuang Chen, Zhuonan Lai, Jie Chen, and Jianqiang Li. Mind marginal non-crack regions: Clustering-inspired representation learning for crack segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12698\u201312708, June 2024.   \n[24] Cunhui Dong, Haichuan Ma, Zhuoyuan Li, Li Li, and Dong Liu. Temporal wavelet transformbased low-complexity perceptual quality enhancement of compressed video. IEEE Transactions on Circuits and Systems for Video Technology, 2023.   \n[25] Zhuoyuan Li, Yao Li, Chuanbo Tang, Li Li, Dong Liu, and Feng Wu. Uniformly accelerated motion model for inter prediction. arXiv preprint arXiv:2407.11541, 2024.   \n[26] Chuanbo Tang, Xihua Sheng, Zhuoyuan Li, Haotian Zhang, Li Li, and Dong Liu. Offline and online optical flow enhancement for deep video compression. Proceedings of the AAAI Conference on Artificial Intelligence, 38(6):5118\u20135126, Mar. 2024. doi: 10.1609/aaai.v38i6. 28317. URL https://ojs.aaai.org/index.php/AAAI/article/view/28317.   \n[27] Zhuangzhuang Chen, Jin Zhang, Zhuonan Lai, Jie Chen, Zun Liu, and Jianqiang Li. Geometryaware guided loss for deep crack recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4703\u20134712, June 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[28] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015. ", "page_idx": 12}, {"type": "text", "text": "[29] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\u00e9n Simonyan. Flamingo: a visual language model for few-shot learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html.   \n[30] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms. CoRR, abs/2307.08581, 2023. doi: 10.48550/ ARXIV.2307.08581. URL https://doi.org/10.48550/arXiv.2307.08581.   \n[31] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal large language model for document understanding. CoRR, abs/2307.02499, 2023. doi: 10.48550/ARXIV.2307.02499. URL https://doi.org/10.48550/arXiv.2307. 02499.   \n[32] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. CoRR, abs/2309.15112, 2023. doi: 10.48550/ARXIV.2309. 15112. URL https://doi.org/10.48550/arXiv.2309.15112.   \n[33] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. CoRR, abs/2306.14824, 2023. doi: 10.48550/ARXIV.2306.14824. URL https://doi.org/10.48550/arXiv.2306. 14824.   \n[34] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. CoRR, abs/2306.15195, 2023. doi: 10.48550/ARXIV.2306.15195. URL https://doi.org/10.48550/arXiv.2306.15195.   \n[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 19730\u201319742. PMLR, 2023. URL https://proceedings.mlr.press/v202/li23q.html.   \n[36] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. CoRR, abs/2311.04257, 2023. doi: 10.48550/ARXIV.2311.04257. URL https://doi.org/10.48550/arXiv.2311.04257.   \n[37] Bo Zhao, Boya Wu, and Tiejun Huang. SVIT: scaling up visual instruction tuning. CoRR, abs/2307.04087, 2023. doi: 10.48550/ARXIV.2307.04087. URL https://doi.org/10. 48550/arXiv.2307.04087.   \n[38] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178, 2023. doi: 10.48550/ARXIV.2304.14178. URL https://doi.org/10.48550/arXiv.2304.14178.   \n[39] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, et al. Cantor: Inspiring multimodal chain-of-thought of mllm. October 2024.   \n[40] Anonymous. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot$ J44HfH4JCg.   \n[41] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter V2: parameter-efficient visual instruction model. CoRR, abs/2304.15010, 2023. doi: 10.48550/ARXIV.2304.15010. URL https://doi.org/10.48550/arXiv.2304.15010.   \n[42] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual prompt generator across llms. CoRR, abs/2305.01278, 2023. doi: 10.48550/ARXIV.2305.01278. URL https://doi.org/10.48550/arXiv.2305.01278.   \n[43] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. CoRR, abs/2305.03726, 2023. doi: 10.48550/ARXIV.2305.03726. URL https://doi.org/10.48550/arXiv.2305.03726.   \n[44] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong. What matters in training a gpt4-style language model with multimodal inputs? CoRR, abs/2307.02469, 2023. doi: 10.48550/ARXIV.2307.02469. URL https: //doi.org/10.48550/arXiv.2307.02469.   \n[45] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, Tom\u00e1s Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740\u2013755. Springer, 2014. doi: 10.1007/978-3-319-10602-1\\_48. URL https://doi.org/10.1007/ 978-3-319-10602-1_48.   \n[46] Harsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, and Stefan Lee. nocaps: novel object captioning at scale. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 8947\u20138956. IEEE, 2019. doi: 10.1109/ICCV. 2019.00904. URL https://doi.org/10.1109/ICCV.2019.00904.   \n[47] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible may not be faithful: Probing object hallucination in vision-language pre-training. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 2128\u20132140. Association for Computational Linguistics, 2023. doi: 10.18653/V1/ 2023.EACL-MAIN.156. URL https://doi.org/10.18653/v1/2023.eacl-main.156. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "7 Technical Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "7.1 Instructions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We primarily referred to the instructions from [4] and additionally designed some instructions, totaling 25 in number. ", "page_idx": 14}, {"type": "text", "text": "\u2022 $I_{1}$ :\u2019Describe the image in one sentence.\u2019,   \n\u2022 $I_{2}$ :\u2019Summarize the image in a single sentence.\u2019,   \n\u2022 $I_{3}$ :\u2019Give a one-sentence depiction of the image.\u2019,   \n\u2022 $I_{4}$ :\u2019Provide a concise sentence describing the image.\u2019,   \n\u2022 $I_{5}$ :\u2019Give a brief summary of the image in a single sentence.\u2019,   \n\u2022 $I_{6}$ :\u2019Describe this image in short.\u2019,   \n\u2022 $I_{7}$ :\u2019Describe this image in a few words.\u2019,   \n\u2022 $I_{8}$ :\u2019Provide a brief caption for this image.\u2019,   \n\u2022 $I_{9}$ :\u2019Provide a short caption for this image.\u2019,   \n\u2022 $I_{10}$ :\u2019Briefly describe the content of the image.\u2019,   \n\u2022 $I_{11}$ :\u2019Describe this image.\u2019,   \n\u2022 $I_{12}$ :\u2019What does the image show?\u2019,   \n\u2022 $I_{13}$ :\u2019What can you see in the image?\u2019,   \n\u2022 $I_{14}$ :\u2019What is described in the image?\u2019,   \n\u2022 $I_{15}$ :\u2019Provide a caption for this image.\u2019,   \n\u2022 $I_{16}$ :\u2019Describe the objects in this image.\u2019,   \n\u2022 $I_{17}$ :\u2019Can you provide a description of the image?\u2019,   \n\u2022 $I_{18}$ :\u2019What objects or subjects are present in the image?\u2019,   \n\u2022 $I_{19}$ :\u2019Describe this image in detail.\u2019,   \n\u2022 $I_{20}$ :\u2019Describe this image in extremely detail.\u2019,   \n\u2022 $I_{21}$ :\u2019Provide a detailed description of this image.\u2019,   \n\u2022 $I_{22}$ :\u2019Can you describe the scene in the image in great detail?\u2019,   \n\u2022 $I_{23}$ :\u2019Give a thorough account of what is depicted in this image.\u2019,   \n\u2022 $I_{24}$ :\u2019Provide an elaborate and comprehensive analysis of this image.\u2019,   \n\u2022 $I_{25}$ :\u2019Give a comprehensive and in-depth description of what is shown in this image. ", "page_idx": 14}, {"type": "text", "text": "7.2 Further Exploration ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Why does the hallucination rate of MLLMs increase with the increase in image description length? The underlying reasons behind this phenomenon are difficult to determine, as the output of MLLMs is influenced by multiple factors such as visual encoders, language models, and training data. In this section, we aim to shed light on this phenomenon by delving into an analysis of common hallucination patterns in long image descriptions. ", "page_idx": 14}, {"type": "text", "text": "As shown in Figure 7, We found that hallucinations are more likely to occur after some words or phrases that indicate enumeration or introduce additional information, such as \"in addition\", \"addition to\", \"additionally\", \"include\", \"including\", \"such as\", \"as well\" and \"also\". We refer to these words/phrases as \"hallucinogenic words\". As shown in Figure 6 left, we performed a statistical analysis on a subset of MSCOCO dataset comprising 256 images to investigate the hallucination rate of image descriptions containing hallucinogenic words. The experimental results demonstrate a notable increase in the hallucination rate of image descriptions that incorporate hallucinogenic words, compared to descriptions that lack such words. This phenomenon was consistently observed across all twelve MLLMs. ", "page_idx": 14}, {"type": "image", "img_path": "yQL5tutdaH/tmp/818338b07b806a9f73b7795ced46eedb9dd785def6e75567ad1cf855e6fc8ba3.jpg", "img_caption": ["Figure 6: Left: Comparison of the hallucination rates between image descriptions containing hallucinogenic words and image descriptions without hallucinogenic words. Right: Proportion of hallucinogenic words in image descriptions containing hallucinations under different instructions. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Therefore, we propose a hypothesis that MLLMs are more likely to employ hallucinogenic words in generating lengthy and detailed image descriptions, resulting in a higher hallucination rate. To validate our hypothesis, we explored the relationship between the occurrence frequency of hallucinogenic words and the average length of image descriptions. The image descriptions with varying average lengths were generated by different instructions. The results are depicted in Figure 6 right. As the length of the description increases, the number of incorrect descriptions containing hallucinogenic words increases and constitutes a significant portion of all incorrect descriptions. This validates our hypothesis.\" ", "page_idx": 15}, {"type": "table", "img_path": "yQL5tutdaH/tmp/5ba28110a3f3c0e82279edb38c43bcb28126a0d4674278ae83ab5fdf955d395b.jpg", "table_caption": [], "table_footnote": ["Table 4: Hallucination rate and quality of the generated image descriptions after disabling hallucinogenic words. The values in the table are averaged across 10 MLLMs. "], "page_idx": 15}, {"type": "text", "text": "We further explored whether the hallucination rate of MLLMs can be reduced by disabling hallucinogenic words. Specifically, we prohibited the use of previously mentioned hallucinogenic words (\"in addition,\" \"addition to,\" \"additionally,\" \"include,\" \"includes,\" \"including,\" \"such as,\" \"as well,\" \"also\") during the generation process in the MLLMs, ensuring that the generated image descriptions did not contain these hallucinogenic words. The results, presented in Table 4, clearly demonstrate that disabling hallucinogenic words can significantly decrease the hallucination rate without compromising the quality of image descriptions. Furthermore, it helps alleviate the tendency for the hallucination rate to increase as the description length grows. These experimental findings not only highlight the substantial impact of hallucinogenic words on the hallucination rate of MLLMs but also offer valuable insights for mitigating hallucination in MLLMs. ", "page_idx": 15}, {"type": "text", "text": "7.3 More Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The average length and CHAIR scores of image descriptions generated by 12 MLLMs (Gemini-ProVision pro [2], Qwen-VL [7], MiniGPT-4 [5], LLaVA [6], InstructBLIP [4], LLaMA-Adapter-v2 [41], mPLUG-Owl2 [36], mPLUG-Owl [38], InternLM-XComposer [32], VPGTrans [42], Otter [43] and Lynx [44]) under 25 instructions on subsets of MSCOCO [45] and NoCap [46] are shown in Table 5 and Table 6, respectively. ", "page_idx": 15}, {"type": "text", "text": "The hallucination degree of each MLLM with hallucinogenic words disabled are shown in Table 4 and Table 8. Disabling hallucinogenic terms can effectively alleviate hallucinations in MLLMs. ", "page_idx": 15}, {"type": "text", "text": "7.4 More Case Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 7 showcases examples of image descriptions generated by MLLMs before and after disabling hallucinogenic words. ", "page_idx": 15}, {"type": "table", "img_path": "yQL5tutdaH/tmp/6b6a7abdfb45d77a73dbb7c7586dc6939e763c86266c3f841acb17d9bce582b2.jpg", "table_caption": [], "table_footnote": ["Table 5: The average length and CHAIR scores of descriptions generated by 12 MLLMs prompted by the 25 instructions on a subset of MSCOCO containing 256 images. "], "page_idx": 16}, {"type": "text", "text": "7.5 Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The LeHaCE framework provides a stable, fair, and comprehensive way to evaluate object hallucinations in large vision-language models. It helps in assessing the usability of large vision-language models, thereby helping to prevent safety incidents. ", "page_idx": 16}, {"type": "text", "text": "We acknowledge the potential ethical concerns associated with the use of the MSCOCO dataset, particularly regarding data privacy, copyright, and consent, as the images in this dataset were collected from Flickr without explicit user consent. However, it is important to note that our study\u2019s methodology and findings are independent of the dataset used. Our research focuses on evaluating the relationship between instruction length and hallucinations in Large Vision-Language Models (LVLMs), and does not rely on or alter the underlying data in the MSCOCO dataset. Nonetheless, we recognize the ethical implications of using such datasets and recommend future research to continue exploring these issues in greater depth. ", "page_idx": 16}, {"type": "image", "img_path": "yQL5tutdaH/tmp/e01c3073d53cfed5f7de1d2afb79642a01809ae2651be947d3aa9d260abc4023.jpg", "img_caption": ["Figure 7: Example of detailed image descriptions generated by beam search and beam search with out hallucinogenic words. The hallucination content is highlighted in red, and the hallucinogenic words are highlighted in green. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "yQL5tutdaH/tmp/0e9b367d2373ac8fa80cf2cb8d33eac3420dbb7c5633f4158ca7aed2331886e4.jpg", "table_caption": [], "table_footnote": ["Table 6: The average length and CHAIR scores of descriptions generated by 12 MLLMs prompted by the 25 instructions on a subset of NoCaps containing 256 images. "], "page_idx": 18}, {"type": "table", "img_path": "yQL5tutdaH/tmp/501e233f6b881532c5b8edce468ff6b73e8ff2e1e2122e2210789314cd94f5a5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "yQL5tutdaH/tmp/4c61b33c962a39d69c005a66737eb904a1dee122670f3ccdb70eb6167d75b486.jpg", "table_caption": ["Table 7: Hallucination rate and quality of the generated image descriptions on MSCOCO after disabling hallucinogenic words. BS stands for Beam Search and HW stands for hallucinogenic Words. "], "table_footnote": ["Table 8: Hallucination rate and quality of the generated image descriptions on NoCaps after disabling hallucinogenic words. BS stands for Beam Search and HW stands for hallucinogenic Words. Disabling hallucinogenic words can alleviate hallucinations in MLLMs. "], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: Yes ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: Yes ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide a detailed discussion of the limitations of our work in Section 5 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: NA ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: Yes ", "page_idx": 21}, {"type": "text", "text": "Justification: All experimental settings are detailed in Sections 3 & 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: Yes ", "page_idx": 22}, {"type": "text", "text": "Justification: The code is included in the supplementary materials. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: Yes Justification: All experimental settings are detailed in Sections 3 & 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: Yes ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper reports the confidence intervals and statistical significance tests in Section 3. The assumptions used in our method are provided in Section 4 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: Yes ", "page_idx": 23}, {"type": "text", "text": "Justification: We detail the information on the computer resources in Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: Yes ", "page_idx": 23}, {"type": "text", "text": "Justification: This research conforms with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: Yes ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the broder impacts of our work in Appendix Section 7.5 Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: NA ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: Yes ", "page_idx": 24}, {"type": "text", "text": "Justification: CC-BY 4.0 ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: Yes ", "page_idx": 25}, {"type": "text", "text": "Justification: New assets introduced in the paper are well documented. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: NA ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: NA ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]