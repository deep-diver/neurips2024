[{"figure_path": "Tw9nfNyOMy/figures/figures_1_1.jpg", "caption": "Figure 1: Resolution comparison. Vista predicts at a higher resolution than previous literature.", "description": "This figure compares the resolution of Vista with several other real-world driving world models.  It shows that Vista operates at a significantly higher resolution (576x1024 pixels) compared to the other models, which range from 80x160 to 512x576 pixels. This higher resolution allows Vista to capture more detailed and accurate information about the driving environment, improving the fidelity of its predictions.", "section": "1 Introduction"}, {"figure_path": "Tw9nfNyOMy/figures/figures_2_1.jpg", "caption": "Figure 2: Capabilities of Vista. Starting from arbitrary environments, Vista can anticipate realistic and continuous futures at high spatiotemporal resolution (A-B). It can be controlled by multi-modal actions (C), and serve as a generalizable reward function to evaluate real-world driving actions (D).", "description": "This figure showcases the main capabilities of the Vista model.  Subfigure (A) demonstrates high-fidelity future prediction from arbitrary starting points with high temporal and spatial resolution. Subfigure (B) shows the model's ability to perform coherent long-horizon rollouts. Subfigure (C) highlights the model's versatile action controllability through the usage of multi-modal action formats. Finally, subfigure (D) illustrates the model's unique ability to serve as a generalizable reward function for evaluating real-world driving actions without the need for ground truth action data.", "section": "3 Learning a Generalizable Driving World Model"}, {"figure_path": "Tw9nfNyOMy/figures/figures_3_1.jpg", "caption": "Figure 3: [Left]: Vista pipeline. In addition to the initial frame, Vista can absorb more priors about future dynamics via latent replacement. Its prediction can be controlled by different actions and be extended to long horizons through autoregressive rollouts. [Right]: Training procedure. Vista takes two training phases, where the second phase freezing the pretrained weights to learn action controls.", "description": "This figure illustrates the Vista pipeline and training procedure. The left side shows how Vista uses dynamic priors (position, velocity, acceleration) from the past three frames via latent replacement, resulting in high-fidelity predictions, and allows for multi-modal action control and long-horizon predictions through autoregressive rollouts. The right side demonstrates the two-phase training process, where the first phase trains the base model and the second phase utilizes the pretrained weights to learn action controllability using LoRA adapters.", "section": "3 Learning a Generalizable Driving World Model"}, {"figure_path": "Tw9nfNyOMy/figures/figures_4_1.jpg", "caption": "Figure 4: Illustration on loss design. Different from the standard diffusion loss (b) that is distributed uniformly, our dynamics enhancement loss (d) enables an adaptive concentration on critical regions (c) (e.g., moving vehicles and roadsides) for dynamics modeling. Moreover, by explicitly supervising high-frequency features (e), the learning of structural details (e.g., edges and lanes) can be enhanced.", "description": "This figure illustrates the design of the loss functions used in Vista. It compares the standard diffusion loss with the proposed dynamics enhancement loss and structure preservation loss. The dynamics enhancement loss focuses more on critical regions with significant motion, while the structure preservation loss emphasizes high-frequency features to preserve structural details.  The visualization shows how the different losses affect the learning process and improve prediction quality.", "section": "3.1 Phase One: Learning High-Fidelity Future Prediction"}, {"figure_path": "Tw9nfNyOMy/figures/figures_6_1.jpg", "caption": "Figure 5: Driving futures predicted by different models using the same condition frame. We contrast Vista to publicly available video generation models using their default configurations. Whilst previous models produce misaligned and corrupted results, Vista does not suffer from these caveats.", "description": "This figure compares the driving scene prediction results of Vista against three other video generation models (SVD, DynamiCrafter, and I2VGen-XL).  All models receive the same starting frame as input. The results show that Vista generates more realistic and coherent future predictions compared to other models, which have problems such as misalignment and corrupted details.", "section": "4.1 Comparisons of Generalization and Fidelity"}, {"figure_path": "Tw9nfNyOMy/figures/figures_6_2.jpg", "caption": "Figure 6: [Top]: Long-horizon prediction. Vista can forecast 15 seconds high-resolution futures without much degradation, encompassing long driving distances. The length of the blue lines indicates the duration of the longest prediction showcased by previous works. [Bottom]: Long-term extension results of SVD. SVD fails to generate consistent high-fidelity videos autoregressively as Vista does.", "description": "This figure compares the long-horizon prediction capabilities of Vista with several other state-of-the-art models. The top part shows Vista's ability to predict 15 seconds into the future with high fidelity and consistency.  The bottom illustrates the failure of SVD (Stable Video Diffusion) to generate consistent high-fidelity predictions over long time horizons, highlighting Vista's advantage in long-term prediction.", "section": "4 Experiments"}, {"figure_path": "Tw9nfNyOMy/figures/figures_7_1.jpg", "caption": "Figure 7: Human evaluation results. The value denotes the percentage of the times that one model is preferred over the other. Vista outperforms existing works in both metrics.", "description": "This figure presents the results of a human evaluation comparing Vista's performance against several baseline models on two aspects: visual quality and motion rationality.  Participants were presented pairs of videos and asked to choose which one was better in terms of each metric.  The results show that Vista was preferred significantly more often than other models, indicating its superior performance in generating both visually appealing and realistically moving videos.", "section": "4.1 Comparisons of Generalization and Fidelity"}, {"figure_path": "Tw9nfNyOMy/figures/figures_7_2.jpg", "caption": "Figure 2: Capabilities of Vista. Starting from arbitrary environments, Vista can anticipate realistic and continuous futures at high spatiotemporal resolution (A-B). It can be controlled by multi-modal actions (C), and serve as a generalizable reward function to evaluate real-world driving actions (D).", "description": "This figure showcases the capabilities of the Vista model.  It demonstrates high-fidelity future prediction from arbitrary starting points (A and B), showing consistent long-horizon predictions.  It also highlights Vista's versatility in action control, handling multiple modes (C) such as commands, goal points, and low-level maneuvers. Finally, (D) illustrates Vista's novel use as a generalizable reward function for evaluating real-world driving actions without ground truth.", "section": "3 Learning a Generalizable Driving World Model"}, {"figure_path": "Tw9nfNyOMy/figures/figures_8_1.jpg", "caption": "Figure 10: [Left]: Average reward on Waymo with different L2 errors. [Right]: Case study. The relative contrast of our reward can properly assess the actions that the L2 error fails to judge.", "description": "This figure shows two plots. The left plot is a line graph showing the relationship between average reward and average L2 error. The average reward decreases as the average L2 error increases. The right plot is an image showing a road scene with three different action trajectories (ground truth, Action1, and Action2) overlaid. Each trajectory has an associated L2 error and reward value. The reward values indicate that the proposed reward function can better distinguish between actions than the L2 error alone.", "section": "4.3 Results of Reward Modeling"}, {"figure_path": "Tw9nfNyOMy/figures/figures_8_2.jpg", "caption": "Figure 11: Effect of dynamic priors. Injecting more dynamic priors yields more consistent future motions with the ground truth, such as the motions of the white vehicle and the billboard on the left.", "description": "This figure shows the effect of using different numbers of dynamic priors (consecutive frames) on the quality of future predictions.  The results show that using more dynamic priors leads to more realistic and consistent movement of objects in the scene, particularly those that are in motion.  This highlights the importance of incorporating historical context into the prediction process to improve accuracy and coherence.", "section": "3.1 Phase One: Learning High-Fidelity Future Prediction"}, {"figure_path": "Tw9nfNyOMy/figures/figures_8_3.jpg", "caption": "Figure 12: [Left]: Effect of dynamics enhancement loss. The model supervised by the dynamics enhancement loss generates more realistic dynamics. In the first example, instead of remaining static, the front car moves forward normally. In the second example, when the ego-vehicle steers right, the trees shift towards the left naturally adhering to the real-world geometric rules. [Right]: Effect of structure preservation loss. The proposed loss yields a clearer outline of the objects as they move.", "description": "This figure demonstrates the effects of two losses used in the Vista model: the dynamics enhancement loss and the structure preservation loss.  The left side shows that the dynamics enhancement loss improves the realism of motion in generated videos by encouraging more realistic movement of objects. The right side shows that the structure preservation loss enhances structural details in the generated videos, resulting in sharper edges and more defined shapes.", "section": "3.1 Phase One: Learning High-Fidelity Future Prediction"}, {"figure_path": "Tw9nfNyOMy/figures/figures_9_1.jpg", "caption": "Figure 3: [Left]: Vista pipeline. In addition to the initial frame, Vista can absorb more priors about future dynamics via latent replacement. Its prediction can be controlled by different actions and be extended to long horizons through autoregressive rollouts. [Right]: Training procedure. Vista takes two training phases, where the second phase freezing the pretrained weights to learn action controls.", "description": "This figure illustrates the Vista pipeline and its two-phase training process. The left side shows how Vista uses latent replacement to incorporate dynamic priors from historical frames to predict realistic and continuous futures and how these predictions can be controlled by various actions. The right side details Vista's training procedure that consists of two phases: (1) a prediction-focused phase that utilizes dynamic priors for higher fidelity and (2) an action control learning phase that freezes pretrained weights and learns action controls using LoRA adapters for efficient learning.", "section": "3 Learning a Generalizable Driving World Model"}, {"figure_path": "Tw9nfNyOMy/figures/figures_22_1.jpg", "caption": "Figure 14: Sensitivity of reward estimation to hyperparameters. Increasing the number of denoising steps can produce more discriminative rewards, whereas increasing the ensemble size can slightly stabilize the estimations.", "description": "This figure shows the results of an ablation study on the impact of hyperparameters on the proposed reward function. The x-axis represents the average L2 error between the estimated trajectory and the ground truth, and the y-axis represents the average reward. Three lines are plotted, each corresponding to a different combination of the number of denoising steps and ensemble size used in the reward estimation. The results show that increasing the number of denoising steps leads to a larger difference in rewards between trajectories with different L2 errors. This indicates that increasing the number of denoising steps improves the discriminative ability of the reward function. On the other hand, increasing the ensemble size has a smaller impact on the discriminative ability and mainly contributes to stabilizing the reward estimations. The dotted lines represent the average reward for the ground truth trajectories and the worst trajectories.", "section": "D Additional Experiments"}, {"figure_path": "Tw9nfNyOMy/figures/figures_23_1.jpg", "caption": "Figure 15: Effect of guidance scale. We predict 15s long-term videos with different CFG schemes. Our method achieves the optimal equilibrium between detail generation and saturation maintenance.", "description": "This figure compares the effect of different classifier-free guidance (CFG) schemes on the quality of long-term video prediction (15 seconds).  Four different CFG schemes are tested: none, constant, linear (as used in Stable Video Diffusion), and triangular (the authors' proposed method). The results show that the triangular scheme provides the best balance between generating detailed videos and avoiding oversaturation, a common issue in long-term video prediction.", "section": "4.4 Ablation Study"}, {"figure_path": "Tw9nfNyOMy/figures/figures_23_2.jpg", "caption": "Figure 16: Necessity of LoRA adaptation. Training newly added projections alone without LoRA results in visual corruptions. The compared variants are trained on nuScenes and inferred on Waymo.", "description": "This figure demonstrates the importance of using Low-Rank Adaptation (LoRA) when fine-tuning the Vista model for action controllability. Two versions of the model were trained, one with LoRA and one without.  Both models were trained on the nuScenes dataset, but their predictions were tested on the unseen Waymo dataset. The results show that the model trained without LoRA suffers from visual artifacts, indicating that LoRA is crucial for preserving visual quality during the fine-tuning process.", "section": "D Additional Experiments"}, {"figure_path": "Tw9nfNyOMy/figures/figures_24_1.jpg", "caption": "Figure 5: Driving futures predicted by different models using the same condition frame. We contrast Vista to publicly available video generation models using their default configurations. Whilst previous models produce misaligned and corrupted results, Vista does not suffer from these caveats.", "description": "This figure compares the prediction results of Vista with other video generation models given the same starting frame.  It demonstrates Vista's ability to produce high-fidelity, aligned predictions compared to other models that suffer from misalignment and corruption, especially in long horizon prediction.", "section": "4.1 Comparisons of Generalization and Fidelity"}, {"figure_path": "Tw9nfNyOMy/figures/figures_26_1.jpg", "caption": "Figure 18: Generalization ability of Vista. We apply Vista across diverse scenes (e.g., countrysides and tunnels) with unseen camera poses (e.g., the perspective of a double-decker bus). Our model can predict high-resolution futures with vivid behaviors of vehicles and pedestrians, exhibiting strong generalization abilities and profound comprehension of world knowledge. Best viewed zoomed in.", "description": "This figure shows the generalization ability of Vista across diverse driving scenes.  It demonstrates Vista's capacity to predict high-resolution future video frames, accurately depicting the behavior of vehicles and pedestrians, even in scenarios with camera poses unseen during training. The scenes shown are diverse, including countrysides, tunnels, and various perspectives. This highlights Vista's impressive understanding of real-world driving dynamics.", "section": "4.1 Comparisons of Generalization and Fidelity"}, {"figure_path": "Tw9nfNyOMy/figures/figures_27_1.jpg", "caption": "Figure 2: Capabilities of Vista. Starting from arbitrary environments, Vista can anticipate realistic and continuous futures at high spatiotemporal resolution (A-B). It can be controlled by multi-modal actions (C), and serve as a generalizable reward function to evaluate real-world driving actions (D).", "description": "This figure showcases four key capabilities of Vista. (A) and (B) demonstrate Vista's ability to generate high-fidelity predictions of future driving scenarios, with (B) highlighting its capability to produce consistent predictions over long horizons. (C) shows that Vista supports diverse control modalities, enabling flexible control. (D) shows that Vista's predictive capabilities can be used to evaluate real-world driving actions without explicit ground-truth access, setting it apart from existing methods.", "section": "3 Learning a Generalizable Driving World Model"}, {"figure_path": "Tw9nfNyOMy/figures/figures_28_1.jpg", "caption": "Figure 18: Generalization ability of Vista. We apply Vista across diverse scenes (e.g., countrysides and tunnels) with unseen camera poses (e.g., the perspective of a double-decker bus). Our model can predict high-resolution futures with vivid behaviors of vehicles and pedestrians, exhibiting strong generalization abilities and profound comprehension of world knowledge. Best viewed zoomed in.", "description": "This figure demonstrates Vista's ability to generalize to unseen environments and diverse camera perspectives.  It shows several examples of Vista's predictions in various scenes (countryside roads, tunnels, city streets), each from a different camera viewpoint.  The high-resolution predictions showcase realistic interactions between vehicles and pedestrians, highlighting Vista's understanding of real-world driving scenarios.", "section": "4.1 Comparisons of Generalization and Fidelity"}, {"figure_path": "Tw9nfNyOMy/figures/figures_29_1.jpg", "caption": "Figure 2: Capabilities of Vista. Starting from arbitrary environments, Vista can anticipate realistic and continuous futures at high spatiotemporal resolution (A-B). It can be controlled by multi-modal actions (C), and serve as a generalizable reward function to evaluate real-world driving actions (D).", "description": "This figure demonstrates the key capabilities of the Vista model.  Subfigure (A) and (B) showcase Vista's ability to predict realistic and temporally consistent future frames at a high resolution and over a long time horizon. Subfigure (C) shows that the model can be controlled using various modalities of action inputs, ranging from high-level instructions (commands and goal points) to low-level maneuvers (trajectories, steering angle and speed).  Finally, subfigure (D) illustrates the novel use of the Vista model itself as a generalizable reward function to evaluate the quality of driving actions without relying on ground-truth data.", "section": "3 Learning a Generalizable Driving World Model"}, {"figure_path": "Tw9nfNyOMy/figures/figures_30_1.jpg", "caption": "Figure 22: Counterfactual reasoning ability. By imposing actions that violate the traffic rules, we discover that Vista can also predict the consequences of abnormal interventions. In the first example, the ego-vehicle passes over the road boundary and rushes into the bush following our instructions. In the second example, the passing car stops and waits to avoid a collision when we force the ego-vehicle to proceed at the crossroads. This showcases Vista\u2019s potential for facilitating closed-loop simulation.", "description": "This figure demonstrates Vista's ability to predict the outcomes of actions that violate traffic rules.  The top row shows the ground truth video. The middle row shows the model's prediction when given an action that causes the car to drive off the road. The bottom row shows the model's prediction when given an action that causes the car to stop at an intersection, even though there is no obstacle. This highlights Vista's capacity for counterfactual reasoning and potential for closed-loop simulation.", "section": "E Additional Visualizations"}, {"figure_path": "Tw9nfNyOMy/figures/figures_30_2.jpg", "caption": "Figure 23: Diverse scenes collected for human evaluation. We carefully curate 60 scenes from OpenDV-YouTube-val [136], nuScenes [10], Waymo [112], and CODA [79]. The distinctive attributes of each dataset jointly represent the diversity of real-world environments, permitting a comprehensive human evaluation.", "description": "This figure shows a collection of 60 diverse driving scenes used for human evaluation in the paper. The scenes are sampled from four different datasets: OpenDV-YouTube-val, nuScenes, Waymo, and CODA.  The selection of scenes aims to represent the variety and complexity of real-world driving conditions for a comprehensive human evaluation of the model's performance. Each small image represents a single scene or a frame from a short video clip.", "section": "Additional Visualizations"}]