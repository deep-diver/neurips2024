[{"figure_path": "pgUQFIJ6BE/figures/figures_8_1.jpg", "caption": "Figure 1: Results for convex-concave minimax problem (12) on a9a.", "description": "This figure presents the results of experiments conducted on the a9a dataset for solving a convex-concave minimax optimization problem. Three performance metrics are presented, Communication Rounds, Communication Complexity, and Local Gradient Calls.  The performance of the proposed SVOGS method is compared against several other methods such as EG, SMMDS, EGS and TPAPP.  The plots display how the gradient mapping decreases as a function of each performance metric.  Each method's performance is represented by a different colored line on the plot.", "section": "7 Experiments"}, {"figure_path": "pgUQFIJ6BE/figures/figures_8_2.jpg", "caption": "Figure 1: Results for convex-concave minimax problem (12) on a9a.", "description": "This figure shows the results of the convex-concave minimax optimization problem (12) applied to the a9a dataset.  The results are presented across three plots showing the convergence of the algorithms in terms of communication rounds, communication complexity, and local gradient calls.  Each plot compares the performance of six different algorithms: EG, SMMDS, EGS, TPAPP, and SVOGS. The y-axis represents the duality gap, a measure of sub-optimality, while the x-axis represents the computational cost in terms of the respective metrics. The figure demonstrates the superior performance of SVOGS in achieving a lower duality gap within fewer rounds, lower complexity and gradient calls.", "section": "7 Experiments"}, {"figure_path": "pgUQFIJ6BE/figures/figures_9_1.jpg", "caption": "Figure 1: Results for convex-concave minimax problem (12) on a9a.", "description": "This figure presents the results of the convex-concave minimax optimization problem (12) on the a9a dataset.  It shows the performance of several algorithms, including EG, SMMDS, EGS, TPAPP, and SVOGS, across three different metrics: communication rounds, communication complexity, and local gradient calls.  Each algorithm's convergence is illustrated graphically, allowing for a comparison of their efficiency and scalability.", "section": "7 Experiments"}, {"figure_path": "pgUQFIJ6BE/figures/figures_9_2.jpg", "caption": "Figure 1: Results for convex-concave minimax problem (12) on a9a.", "description": "This figure presents the results of solving the convex-concave minimax optimization problem (12) on the a9a dataset.  It compares the performance of five different algorithms: Extra Gradient (EG), Star Min-Max Data Similarity (SMMDS), Extra-Gradient Sliding (EGS), Three Pillars Algorithm with Partial Participation (TPAPP), and the proposed Stochastic Variance-Reduced Optimistic Gradient Sliding (SVOGS) method. The results are shown for three different metrics: communication rounds, communication complexity, and local gradient calls. Each plot shows the convergence of the respective metric, with the x-axis representing the computational cost and the y-axis representing the duality gap or suboptimality measure.", "section": "7 Experiments"}, {"figure_path": "pgUQFIJ6BE/figures/figures_35_1.jpg", "caption": "Figure 1: Results for convex-concave minimax problem (12) on a9a.", "description": "This figure presents the results of the convex-concave minimax optimization problem (12) on the a9a dataset. It compares the performance of several optimization methods, including EG, SMMDS, EGS, TPAPP, and SVOGS, across three metrics: communication rounds, communication complexity, and local gradient calls. Each plot shows the convergence of the gradient mapping with respect to each metric.  The results illustrate the relative efficiency of these different algorithms in solving the given minimax problem.", "section": "7 Experiments"}, {"figure_path": "pgUQFIJ6BE/figures/figures_35_2.jpg", "caption": "Figure 1: Results for convex-concave minimax problem (12) on a9a.", "description": "This figure presents the results of the convex-concave minimax optimization problem (12) on the a9a dataset. The results are shown in terms of communication rounds, communication complexity, and local gradient calls. The figure compares the performance of several optimization algorithms: EG, SMMDS, EGS, TPAPP, and SVOGS.  Each algorithm's performance is plotted against the corresponding metric on a logarithmic scale.  The figure aims to demonstrate the empirical advantages of the proposed SVOGS method in terms of efficiency and convergence.", "section": "7 Experiments"}]