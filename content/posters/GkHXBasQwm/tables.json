[{"figure_path": "GkHXBasQwm/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative evaluation of HOI-Swap using (1) automatic metrics: contact agreement (cont. agr.), hand agreement (hand agr.), hand fidelity (hand fid.), subject consistency (subj. cons.), and motion smoothness (mot. smth.)\u2014the last two are for video only and (2) user preference rate (user pref.) from our user study. For video editing, users were given an option to report if they found all edits unsatisfactory, which has an average selection rate of 10.9%. All values are percentages.", "description": "This table presents a quantitative comparison of HOI-Swap against several baselines for both image and video editing tasks.  It uses both automatic metrics (measuring things like hand realism and motion smoothness) and human preference rates from a user study.  The results show that HOI-Swap significantly outperforms the baselines in terms of overall quality and user preference.", "section": "4.1 Experimental Setup"}, {"figure_path": "GkHXBasQwm/tables/tables_20_1.jpg", "caption": "Table 2: Video editing results with splitting by subject and action (Table 1 in the main paper reports results with splitting by object instances).", "description": "This table presents the quantitative results of video editing experiments, broken down by two different splitting methods: one by subject and the other by action.  It shows the performance of several methods (Per-frame, AnyV2V, VideoSwap, and HOI-Swap) across various metrics including subject consistency, motion smoothness, contact agreement, hand agreement, and hand fidelity.  The results highlight the relative strengths of each method in handling different aspects of video editing, particularly regarding temporal and spatial consistency in the context of hand-object interactions.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/tables/tables_20_2.jpg", "caption": "Table 3: Video editing results breakdown: in-domain videos (left) and out-of-domain videos (right).", "description": "This table presents a detailed quantitative evaluation of the HOI-Swap model's video editing performance. It breaks down the results based on whether the videos used for evaluation are from datasets that were part of the model's training (in-domain) or not (out-of-domain).  The metrics used to evaluate the performance include subject consistency, motion smoothness, contact agreement, hand agreement, hand fidelity, and user preference.  The table allows for a comparison of HOI-Swap against several baseline methods, illustrating the model's generalization capabilities across different video datasets.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/tables/tables_20_3.jpg", "caption": "Table 1: Quantitative evaluation of HOI-Swap using (1) automatic metrics: contact agreement (cont. agr.), hand agreement (hand agr.), hand fidelity (hand fid.), subject consistency (subj. cons.), and motion smoothness (mot. smth.)\u2014the last two are for video only and (2) user preference rate (user pref.) from our user study. For video editing, users were given an option to report if they found all edits unsatisfactory, which has an average selection rate of 10.9%. All values are percentages.", "description": "This table presents a quantitative comparison of HOI-Swap against various baselines for both image and video editing tasks.  It uses both automatic metrics (measuring aspects like hand realism, object consistency, and motion smoothness) and user preference rates from a user study.  The user study involved participants choosing their preferred edits across different methods for both image and video editing scenarios.", "section": "4.1 Experimental Setup"}, {"figure_path": "GkHXBasQwm/tables/tables_21_1.jpg", "caption": "Table 5: Video editing results: comparison of DINO and CLIP encoders for stage II. The two object encoders yield similar performance.", "description": "This table presents a comparison of the performance of the HOI-Swap model when using either DINO or CLIP encoders in stage II of the two-stage pipeline.  The results show that both encoders achieve very similar performance across various video editing metrics, including subject consistency, motion smoothness, contact agreement, hand agreement, and hand fidelity.  This suggests that the choice of encoder in stage II has minimal impact on the overall performance of the method.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/tables/tables_21_2.jpg", "caption": "Table 6: Video editing results: comparison of using ground truth object bounding boxes vs. SAM-2 estimated ones as model input.", "description": "This table presents a comparison of video editing results obtained using two different methods for providing object bounding box information to the model: ground truth bounding boxes and bounding boxes estimated using the Segment Anything Model (SAM-2).  The table shows the performance of HOI-Swap on several video editing metrics (subject consistency, motion smoothness, contact agreement, hand agreement, and hand fidelity) using each type of input bounding box. The comparison highlights the effectiveness of using SAM-2 generated bounding boxes as a more user-friendly alternative to manually generating ground truth boxes.", "section": "C.5 Discussion on sampling region"}]