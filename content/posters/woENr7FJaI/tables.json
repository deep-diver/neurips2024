[{"figure_path": "woENr7FJaI/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of conventional MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench, and LLaVA-Bench. \"MEG\" represents training data generated via Multi-size Expert Generation, while \"IG\" indicates training data produced using Incremental Generation.", "description": "This table compares the performance of various Multimodal Large Language Models (MLLMs) on three different benchmark datasets: MMHal-Bench, MRHal-Bench, and LLaVA-Bench.  It shows the scores for each model on various metrics, including hallucination rate, conversation quality, detail of descriptions, and complexity of questions. The table also differentiates between models trained with conventional methods and those fine-tuned using Reinforcement Learning from Human Feedback (RLHF), and shows the impact of two different data generation methods (Multi-size Expert Generation and Incremental Generation) on the performance of the models.  Higher scores generally indicate better performance.", "section": "4 Experiments and Analysis"}, {"figure_path": "woENr7FJaI/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of conventional MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench, and LLaVA-Bench. \"MEG\" represents training data generated via Multi-size Expert Generation, while \"IG\" indicates training data produced using Incremental Generation.", "description": "This table compares the performance of various Multimodal Large Language Models (MLLMs) across three different benchmark datasets: MMHal-Bench, MRHal-Bench, and LLaVA-Bench.  The models are categorized into conventional MLLMs and those fine-tuned using Reinforcement Learning from Human Feedback (RLHF).  The table shows scores for hallucination rate, conversation quality, detailed description accuracy, and complex question answering, providing a comprehensive performance comparison.  Two different data generation methods (MEG and IG) are also compared for RLHF-based models.", "section": "4 Experiments and Analysis"}, {"figure_path": "woENr7FJaI/tables/tables_7_2.jpg", "caption": "Table 3: Study on preference quantity.", "description": "This table presents the results of an ablation study investigating the impact of varying the number of preference levels (2, 3, 4, and 5) on the performance of the model. The metrics used for evaluation are MMHal-Bench (Score, Hallucination rate), MRHal-Bench (Score (cumulative/mean), Hallucination rate), and LLaVA-Bench (Conversation, Detail, Complex). The results show that the 4-level preference setting yields the best performance across all benchmarks.", "section": "4 Experiments and Analysis"}, {"figure_path": "woENr7FJaI/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of conventional MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench, and LLaVA-Bench. \"MEG\" represents training data generated via Multi-size Expert Generation, while \"IG\" indicates training data produced using Incremental Generation.", "description": "This table compares the performance of various Multimodal Large Language Models (MLLMs) on three different benchmarks: MMHal-Bench, MRHal-Bench, and LLaVA-Bench.  It shows the scores for each model on metrics such as hallucination rate, conversation quality, detail in descriptions, and complexity of questions answered.  The table also differentiates between models trained using conventional methods and those trained with Reinforcement Learning from Human Feedback (RLHF), and further distinguishes training data generated using two different methods (MEG and IG).", "section": "4 Experiments and Analysis"}, {"figure_path": "woENr7FJaI/tables/tables_8_2.jpg", "caption": "Table 5: Ablations on the human-free multi-level preference dataset using different annotations, including AI (i.e., GPT-4V), Auto-check, and initial annotations from MEG and IG.", "description": "This table presents ablation studies on the human-free multi-level preference dataset. It compares the performance of the model trained with different types of annotations: AI annotations (GPT-4V), annotations refined by the Auto-check mechanism, and initial annotations generated by MEG and IG methods.  The performance is evaluated across three benchmarks: MMHal-Bench, MRHal-Bench, and LLaVA-Bench, using metrics like Score, Hallucination rate, and others. This helps assess the impact of different annotation strategies on the final model's performance.", "section": "4 Experiments and Analysis"}, {"figure_path": "woENr7FJaI/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of conventional MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench, and LLaVA-Bench. \"MEG\" represents training data generated via Multi-size Expert Generation, while \"IG\" indicates training data produced using Incremental Generation.", "description": "This table compares the performance of various Multimodal Large Language Models (MLLMs) on three different benchmark datasets: MMHal-Bench, MRHal-Bench, and LLaVA-Bench.  It contrasts conventional MLLMs with those fine-tuned using Reinforcement Learning from Human Feedback (RLHF).  The table shows scores for hallucination rate, conversation quality, detailed description, and complex question answering.  It also breaks down the results based on whether the training data was generated using the Multi-size Expert Generation (MEG) or Incremental Generation (IG) method.", "section": "4 Experiments and Analysis"}]