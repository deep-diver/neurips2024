[{"figure_path": "woENr7FJaI/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Depicted are the input image, text prompt, and corresponding multi-level preference dataset. Contents highlighted in red signify hallucinations. Responses range from A to C, representing varying degrees of quality from superior to inferior. Right: Illustrating the strategy for leveraging inferior responses. (a) displays the conventional RLHF baseline, which adpots the binary-level preference. (b) To mitigate the gap between adjacent levels, we first split a single comparison into multiple comparisons by inserting extra medium responses. (c) Furthermore, we introduce the cross-level comparison to augment the dataset with more hallucination examples.", "description": "This figure illustrates the core concept of the paper: using multi-level preferences instead of binary preferences in reinforcement learning from human feedback (RLHF) for multimodal large language models (MLLMs). The left side shows an example of an image and three corresponding responses with varying levels of quality. The right side demonstrates how multi-level preferences improve RLHF. (a) shows the standard binary approach. (b) explains how to narrow the gap between quality levels by introducing intermediate responses. (c) illustrates the additional benefit of including comparisons between non-adjacent levels to incorporate a broader range of examples, especially those with hallucinations.", "section": "1 Introduction"}, {"figure_path": "woENr7FJaI/figures/figures_3_1.jpg", "caption": "Figure 2: Pipeline for Constructing Human-free Multi-level Preference Dataset. We initiate the process with Multi-size Expert Generation and Incremental Generation to establish the initial dataset. Then, to enhance the quality of the initial preference dataset, we introduce the Auto-check Mechanism, which calculates both global and local metrics based on sentences and noun chunks, respectively.", "description": "This figure illustrates the pipeline used to create a human-free multi-level preference dataset for training multi-modal language models (MLLMs).  It shows two main stages: (a) Dataset Generation, which uses Multi-size Expert Generation and Incremental Generation to create an initial dataset; and (b) Auto-check Mechanism, which refines the initial dataset using global and local metrics based on sentence and noun chunk analysis to improve quality and accuracy. The goal is to generate a high-quality dataset for MLLM training without relying on human annotation.", "section": "3.1 Human-free Multi-level Preference Dataset Generation"}, {"figure_path": "woENr7FJaI/figures/figures_7_1.jpg", "caption": "Figure 1: Left: Depicted are the input image, text prompt, and corresponding multi-level preference dataset. Contents highlighted in red signify hallucinations. Responses range from A to C, representing varying degrees of quality from superior to inferior. Right: Illustrating the strategy for leveraging inferior responses. (a) displays the conventional RLHF baseline, which adpots the binary-level preference. (b) To mitigate the gap between adjacent levels, we first split a single comparison into multiple comparisons by inserting extra medium responses. (c) Furthermore, we introduce the cross-level comparison to augment the dataset with more hallucination examples.", "description": "The figure on the left shows an example of the input image, prompt, and multi-level responses generated by an MLLM.  The responses are categorized into three levels (A, B, C) representing superior, medium, and inferior quality, respectively. Hallucinations in the responses are highlighted in red. The right side illustrates three strategies for improving MLLM performance using multi-level preferences: reducing the gap between adjacent levels, increasing the number of comparisons, and introducing cross-level comparisons to incorporate a wider range of response quality.", "section": "1 Introduction"}, {"figure_path": "woENr7FJaI/figures/figures_9_1.jpg", "caption": "Figure 1: Left: Depicted are the input image, text prompt, and corresponding multi-level preference dataset. Contents highlighted in red signify hallucinations. Responses range from A to C, representing varying degrees of quality from superior to inferior. Right: Illustrating the strategy for leveraging inferior responses. (a) displays the conventional RLHF baseline, which adpots the binary-level preference. (b) To mitigate the gap between adjacent levels, we first split a single comparison into multiple comparisons by inserting extra medium responses. (c) Furthermore, we introduce the cross-level comparison to augment the dataset with more hallucination examples.", "description": "This figure illustrates the core idea of the paper: using multi-level preferences instead of binary preferences in reinforcement learning for multimodal large language models (MLLMs). The left side shows an example of a multi-level preference dataset, where responses are ranked as superior (A), medium (B), and inferior (C). The right side shows how the proposed method improves MLLM training by (b) reducing the gap between preference levels and (c) incorporating cross-level comparisons to provide a broader range of examples for learning to avoid hallucinations.", "section": "1 Introduction"}, {"figure_path": "woENr7FJaI/figures/figures_14_1.jpg", "caption": "Figure 1: Left: Depicted are the input image, text prompt, and corresponding multi-level preference dataset. Contents highlighted in red signify hallucinations. Responses range from A to C, representing varying degrees of quality from superior to inferior. Right: Illustrating the strategy for leveraging inferior responses. (a) displays the conventional RLHF baseline, which adopts the binary-level preference. (b) To mitigate the gap between adjacent levels, we first split a single comparison into multiple comparisons by inserting extra medium responses. (c) Furthermore, we introduce the cross-level comparison to augment the dataset with more hallucination examples.", "description": "This figure illustrates the core idea of the AMP framework.  The left side shows an example of a multi-level preference dataset with varying response qualities (superior, medium, inferior) and highlights hallucinated parts. The right side contrasts the traditional binary preference RLHF approach with the proposed multi-level approach, emphasizing the benefits of reducing the gap between levels and incorporating cross-level comparisons for improved MLLM training.", "section": "1 Introduction"}, {"figure_path": "woENr7FJaI/figures/figures_15_1.jpg", "caption": "Figure 1: Left: Depicted are the input image, text prompt, and corresponding multi-level preference dataset. Contents highlighted in red signify hallucinations. Responses range from A to C, representing varying degrees of quality from superior to inferior. Right: Illustrating the strategy for leveraging inferior responses. (a) displays the conventional RLHF baseline, which adopts the binary-level preference. (b) To mitigate the gap between adjacent levels, we first split a single comparison into multiple comparisons by inserting extra medium responses. (c) Furthermore, we introduce the cross-level comparison to augment the dataset with more hallucination examples.", "description": "The figure on the left shows an example of the multi-level preference dataset used in the paper.  It highlights how different responses to the same image prompt are ranked into three levels (superior, medium, inferior) based on quality and the presence of hallucinations. The figure on the right illustrates the three strategies used to improve the multi-level preference framework: reducing the gap between adjacent levels by adding medium responses; enabling cross-level comparisons to expose the model to a broader range of hallucinations; and leveraging inferior responses for better learning.", "section": "1 Introduction"}, {"figure_path": "woENr7FJaI/figures/figures_16_1.jpg", "caption": "Figure 1: Left: Depicted are the input image, text prompt, and corresponding multi-level preference dataset. Contents highlighted in red signify hallucinations. Responses range from A to C, representing varying degrees of quality from superior to inferior. Right: Illustrating the strategy for leveraging inferior responses. (a) displays the conventional RLHF baseline, which adopts the binary-level preference. (b) To mitigate the gap between adjacent levels, we first split a single comparison into multiple comparisons by inserting extra medium responses. (c) Furthermore, we introduce the cross-level comparison to augment the dataset with more hallucination examples.", "description": "This figure illustrates the core idea of the AMP framework. The left side shows an example of a multi-level preference dataset, where responses are categorized into superior (A), medium (B), and inferior (C) based on the presence of hallucinations. The right side compares the conventional RLHF baseline (binary preference) with the proposed AMP framework. The AMP framework addresses two key challenges: reducing the gap between adjacent preference levels and incorporating cross-level comparisons to leverage a broader range of comparisons and better learn to suppress hallucinations.", "section": "1 Introduction"}]