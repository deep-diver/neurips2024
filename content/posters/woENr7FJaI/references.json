{"references": [{"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-00-00", "reason": "This paper introduces BLIP-2, a highly influential multimodal large language model that serves as a foundational model for many subsequent studies mentioned in the paper, including the current work."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-00-00", "reason": "This paper introduces a novel visual instruction tuning method that significantly improves the performance of multimodal large language models, which is directly relevant to the current paper's focus on improving MLLM performance."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-00-00", "reason": "This paper introduces Direct Preference Optimization (DPO), a crucial reinforcement learning algorithm that is directly adopted and extended upon in the current research for its efficiency and effectiveness."}, {"fullname_first_author": "Yifan Li", "paper_title": "Evaluating object hallucination in large vision-language models", "publication_date": "2023-00-00", "reason": "This paper introduces a benchmark for evaluating object hallucination in large vision-language models, which is directly used and further developed in the current paper to provide a more comprehensive evaluation of MLLMs."}, {"fullname_first_author": "Zhiqing Sun", "paper_title": "Aligning large multimodal models with factually augmented RLHF", "publication_date": "2023-00-00", "reason": "This paper presents a method for aligning large multimodal models using reinforcement learning from human feedback (RLHF), which is directly relevant to the current paper's approach of leveraging multi-level preferences for improved RLHF."}]}