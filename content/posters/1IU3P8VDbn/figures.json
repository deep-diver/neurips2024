[{"figure_path": "1IU3P8VDbn/figures/figures_1_1.jpg", "caption": "Figure 1: The motivation of this work. (a) LLMs work well on common causal reasoning tasks, whose topics usually are widely discussed. (b) LLMs struggle to tackle rare tasks, whose corpora are possibly brand new for them. (c) Here is an example of the CausalProbe 2024 benchmark that is introduced to examine the true level of causal reasoning in LLMs, including an easy one-choice version (CausalProbe-E), a hard one-choice version (CausalProbe-H), and an uncertain multiple-choice version (CausalProbe-M). They have analogous formats but different construction strategies. (d) Compared to previous causal Q&A benchmarks, the studied LLMs exhibit a significant performance drop on CausalProbe 2024. (*) represents our benchmarks.", "description": "This figure illustrates the motivation behind the research by showcasing the limitations of current LLMs in handling causal reasoning tasks.  Panel (a) demonstrates the strong performance of LLMs on common causal reasoning tasks, while (b) highlights their struggles with less common or novel tasks. Panel (c) introduces the new benchmark, CausalProbe-2024, with variations in difficulty (CausalProbe-E, CausalProbe-H, CausalProbe-M). Finally, (d) visually compares the performance of several LLMs on CausalProbe-2024 against existing benchmarks, revealing a significant drop in accuracy, underscoring the need for improvement in LLM causal reasoning capabilities.", "section": "1 Introduction"}, {"figure_path": "1IU3P8VDbn/figures/figures_3_1.jpg", "caption": "Figure 2: An diagram of illustrating how autoregression fails to capture the correct causal knowledge.", "description": "This figure illustrates the difference between the actual causal relationship and the sequential causal relationship captured by autoregression in LLMs.  Panel (a) shows the ground truth causal relationship: rain causes school closure, which in turn causes Jack to learn at home. Panel (b) shows how autoregression might represent this: The model sees a sequence of events (rain, school closure, learn at home) and creates a shallow, sequential relationship.  It fails to properly model the actual causal links and might mistakenly infer causation where none exists, or miss crucial causal links present in the real-world scenario.", "section": "3 Problem Formalization"}, {"figure_path": "1IU3P8VDbn/figures/figures_4_1.jpg", "caption": "Figure 3: The diagram of G\u00b2-Reasoner. This framework consists of two modules. One module is a retrieval-augmented generation (RAG) system to retrieve general knowledge that is related to the causal question. Another is a goal-oriented prompt to steer LLMs to race toward the ultimate goal of causal reasoning.", "description": "The figure illustrates the G\u00b2-Reasoner framework, which enhances LLMs' causal reasoning capabilities. It consists of two main components: a retrieval-augmented generation (RAG) system that retrieves relevant general knowledge from a knowledge base, and a goal-oriented prompt designed to guide the LLM's reasoning process towards the correct causal relationship.  The RAG system uses an embedding model and a vector database to retrieve relevant knowledge, while the goal-oriented prompt provides instructions and context to ensure focused and logical causal inference.", "section": "5 G2-Reasoner: A General-Knowledge-Assisted and Goal-Driven Reasoner"}, {"figure_path": "1IU3P8VDbn/figures/figures_19_1.jpg", "caption": "Figure 8: The pipeline of constructing CausalProbe 2024. CausalProbe 2024 consists of three datasets: CausalProbe-E, CausalProbe-H, and CausalProbe-M. They follow different strategies. In particular, CausalProbe-H introduces made-up fake cause-effect pairs, and can be used to examine the LLMs' genuine causal reasoning capability when encountering counterfactual disturbance term. CausalProbe-M features questions with varying numbers of correct options, preventing LLMs from providing right responses through random guessing.", "description": "This figure illustrates the three-step pipeline for constructing the CausalProbe 2024 benchmark dataset.  It details how GPT-3.5 Turbo was used to generate three different types of causal reasoning questions: CausalProbe-E (easy, one correct answer), CausalProbe-H (hard, one correct answer with distractors), and CausalProbe-M (multiple choice, multiple correct answers). The process emphasizes the use of example prompts to guide the LLM, ensuring the quality and variety of questions.", "section": "6.1 Construction of CausalProbe 2024 and its superiority"}, {"figure_path": "1IU3P8VDbn/figures/figures_22_1.jpg", "caption": "Figure 1: The motivation of this work. (a) LLMs work well on common causal reasoning tasks, whose topics usually are widely discussed. (b) LLMs struggle to tackle rare tasks, whose corpora are possibly brand new for them. (c) Here is an example of the CausalProbe 2024 benchmark that is introduced to examine the true level of causal reasoning in LLMs, including an easy one-choice version (CausalProbe-E), a hard one-choice version (CausalProbe-H), and an uncertain multiple-choice version (CausalProbe-M). They have analogous formats but different construction strategies. (d) Compared to previous causal Q&A benchmarks, the studied LLMs exhibit a significant performance drop on CausalProbe 2024. (*) represents our benchmarks.", "description": "This figure demonstrates that large language models (LLMs) perform well on common causal reasoning tasks but struggle with less common ones.  It introduces a new benchmark, CausalProbe-2024, designed to evaluate LLMs' true causal reasoning abilities.  This benchmark includes three versions: easy, hard, and uncertain, each with varying difficulty. The figure shows that the LLMs' performance drops significantly on CausalProbe-2024 compared to previous benchmarks.", "section": "1 Introduction"}, {"figure_path": "1IU3P8VDbn/figures/figures_22_2.jpg", "caption": "Figure 12: The statistic of the number of correct options in each Q&A data of CausalProbe-M.", "description": "This bar chart displays the distribution of the number of correct answers within the multiple-choice questions of the CausalProbe-M dataset.  The x-axis represents the number of correct answers (1 to 4), and the y-axis represents the count of questions with that number of correct answers.  The distribution shows that a significant portion of the questions have 2 or 3 correct answers. This design prevents LLMs from simply guessing to get correct answers.", "section": "I Additional Introduction and Analysis of CausalProbe 2024"}, {"figure_path": "1IU3P8VDbn/figures/figures_22_3.jpg", "caption": "Figure 13: The statistic of query types in each Q&A data of CausalProbe-M.", "description": "The pie chart shows the distribution of question types in the CausalProbe-M dataset.  The majority of questions (81%) ask about the cause of an event, while a smaller proportion (19%) ask about the effect.", "section": "I Additional Introduction and Analysis of CausalProbe 2024"}, {"figure_path": "1IU3P8VDbn/figures/figures_23_1.jpg", "caption": "Figure 1: The motivation of this work. (a) LLMs work well on common causal reasoning tasks, whose topics usually are widely discussed. (b) LLMs struggle to tackle rare tasks, whose corpora are possibly brand new for them. (c) Here is an example of the CausalProbe 2024 benchmark that is introduced to examine the true level of causal reasoning in LLMs, including an easy one-choice version (CausalProbe-E), a hard one-choice version (CausalProbe-H), and an uncertain multiple-choice version (CausalProbe-M). They have analogous formats but different construction strategies. (d) Compared to previous causal Q&A benchmarks, the studied LLMs exhibit a significant performance drop on CausalProbe 2024. (*) represents our benchmarks.", "description": "This figure demonstrates the motivation behind the research by highlighting the strengths and weaknesses of LLMs in causal reasoning. It shows that while LLMs perform well on common causal reasoning tasks (a), they struggle with rare or unseen tasks (b).  The figure introduces a new benchmark, CausalProbe-2024 (c), which is designed to evaluate the true level of causal reasoning ability in LLMs.  The three variants of the benchmark (CausalProbe-E, CausalProbe-H, and CausalProbe-M) differ in difficulty and format. Finally, (d) illustrates the significant performance drop that LLMs exhibit on CausalProbe-2024 compared to existing benchmarks, suggesting that they primarily perform shallow causal reasoning.", "section": "1 Introduction"}, {"figure_path": "1IU3P8VDbn/figures/figures_23_2.jpg", "caption": "Figure 3: The diagram of G\u00b2-Reasoner. This framework consists of two modules. One module is a retrieval-augmented generation (RAG) system to retrieve general knowledge that is related to the causal question. Another is a goal-oriented prompt to steer LLMs to race toward the ultimate goal of causal reasoning.", "description": "This figure shows the architecture of the G2-Reasoner framework, which is proposed to enhance LLMs' causal reasoning capabilities. It consists of two main modules: a RAG (Retrieval-Augmented Generation) module to incorporate external knowledge related to the causal question and a goal-oriented prompt module to guide the LLM towards achieving the desired outcome of causal reasoning. The RAG module retrieves relevant knowledge from a general knowledge base using an embedding model and a retriever, while the goal-oriented prompt module provides specific instructions to improve the LLM's performance in causal reasoning tasks.", "section": "5 G2-Reasoner: A General-Knowledge-Assisted and Goal-Driven Reasoner"}, {"figure_path": "1IU3P8VDbn/figures/figures_23_3.jpg", "caption": "Figure 3: The diagram of G\u00b2-Reasoner. This framework consists of two modules. One module is a retrieval-augmented generation (RAG) system to retrieve general knowledge that is related to the causal question. Another is a goal-oriented prompt to steer LLMs to race toward the ultimate goal of causal reasoning.", "description": "The G\u00b2-Reasoner framework enhances LLMs' causal reasoning by incorporating a retrieval-augmented generation (RAG) system and goal-oriented prompts. The RAG system retrieves relevant general knowledge, while the goal-oriented prompt guides the LLM towards accurate causal reasoning, improving performance particularly in novel and counterfactual scenarios.", "section": "5 G2-Reasoner: A General-Knowledge-Assisted and Goal-Driven Reasoner"}, {"figure_path": "1IU3P8VDbn/figures/figures_23_4.jpg", "caption": "Figure 1: The motivation of this work. (a) LLMs work well on common causal reasoning tasks, whose topics usually are widely discussed. (b) LLMs struggle to tackle rare tasks, whose corpora are possibly brand new for them. (c) Here is an example of the CausalProbe 2024 benchmark that is introduced to examine the true level of causal reasoning in LLMs, including an easy one-choice version (CausalProbe-E), a hard one-choice version (CausalProbe-H), and an uncertain multiple-choice version (CausalProbe-M). They have analogous formats but different construction strategies. (d) Compared to previous causal Q&A benchmarks, the studied LLMs exhibit a significant performance drop on CausalProbe 2024. (*) represents our benchmarks.", "description": "This figure demonstrates the motivation behind the research by highlighting LLMs' inconsistent performance on causal reasoning tasks.  Panel (a) shows strong LLM performance on common tasks, while (b) shows poor performance on less common tasks with novel data. Panel (c) introduces the CausalProbe-2024 benchmark, showcasing its variations in difficulty. Finally, (d) presents a comparison of LLM performance across different benchmarks, revealing a significant drop on the novel CausalProbe-2024 benchmark.", "section": "1 Introduction"}]