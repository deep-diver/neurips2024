[{"figure_path": "1IU3P8VDbn/tables/tables_7_1.jpg", "caption": "Table 2: Results of the studied LLMs on four causal Q&A benchmarks. The metric is exact match (EM). \u201cVanilla\u201d denotes doing inference directly. \u201cC-E\u201d, \u201cC-H\u201d represnt CausalProbe-E and CausalProbe-H. The standard deviations are presented in Appendix G.", "description": "This table presents the exact match scores achieved by four different large language models (LLMs) on four causal question answering benchmarks.  The LLMs are evaluated using three different methods: vanilla (direct inference), chain-of-thought prompting, and retrieval augmented generation. The benchmarks include established datasets (COPA, e-CARE, CausalNet) and a newly introduced dataset (CausalProbe 2024), which is further divided into easier and harder sub-tasks (C-E and C-H).  The results highlight the performance differences between the LLMs and across different reasoning methods, showing how performance varies across different levels of causal reasoning complexity and how a new reasoning strategy might help enhance the performance.", "section": "6.3 Result Analysis"}, {"figure_path": "1IU3P8VDbn/tables/tables_7_2.jpg", "caption": "Table 3: Results of training data detection using Min-K% Prob [53]. We conduct this evaluation on LLAMA 2 7B and LLaMA 3 8B. The metric is the average negative log-likelihood on a dataset. A smaller value indicates better freshness. \u201cC-E\u201d, \u201c\u0421-\u041d\" represnt CausalProbe-E and CausalProbe-H.", "description": "This table presents the results of a membership inference attack using the Min-K% Prob method to assess the freshness of the data used in the CausalProbe 2024 benchmark and three other benchmarks (COPA, e-CARE, and CausalNet).  The attack was conducted on two LLMs, LLaMA 2 7B and LLaMA 3 8B.  A lower average negative log-likelihood indicates that the benchmark's data is less likely to be present in the LLMs' training data, suggesting greater freshness.  The results are broken down by different percentages (Min-10%, Min-20%, Min-30%) of the tokens considered in the evaluation.", "section": "4.2 Empirical Study on Causal Reasoning Capabilities of LLMs"}, {"figure_path": "1IU3P8VDbn/tables/tables_20_1.jpg", "caption": "Table 4: Statistical results of volunteer selection.", "description": "This table presents the results of a volunteer selection process for quality control of the CausalProbe-2024 dataset.  It shows the difficulty level (Diff level) of 20 questions rated by 17 volunteers on a 1-10 scale (10 being the most difficult), their accuracy (Acc (%)) in answering those questions, and whether they were deemed qualified (Qualified) based on accuracy and difficulty criteria.", "section": "H Quality Control"}, {"figure_path": "1IU3P8VDbn/tables/tables_20_2.jpg", "caption": "Table 3: Results of training data detection using Min-K% Prob [53]. We conduct this evaluation on LLAMA 2 7B and LLaMA 3 8B. The metric is the average negative log-likelihood on a dataset. A smaller value indicates better freshness. \u201cC-E\u201d, \u201c\u0421-\u041d", "description": "This table presents the results of a membership inference attack (Min-K% Prob) used to assess the freshness of the data used in different causal reasoning benchmarks.  The lower the average negative log-likelihood, the less likely the data was present in the model's training data. The results show that CausalProbe 2024 data is significantly fresher compared to existing benchmarks (COPA, e-CARE, and CausalNet), supporting the claim that LLMs perform poorly on CausalProbe 2024 due to the lack of exposure during training.  The evaluation is performed on Llama 2 7B and Llama 3 8B.", "section": "4.2 Empirical Study on Causal Reasoning Capabilities of LLMs"}, {"figure_path": "1IU3P8VDbn/tables/tables_21_1.jpg", "caption": "Table 2: Results of the studied LLMs on four causal Q&A benchmarks. The metric is exact match (EM). \"Vanilla\" denotes doing inference directly. \"C-E\", \"C-H\" represnt CausalProbe-E and CausalProbe-H. The standard deviations are presented in Appendix G.", "description": "This table presents the exact match scores achieved by four different Large Language Models (LLMs) across four causal question and answering (Q&A) benchmark datasets.  The benchmarks include COPA, e-CARE, CausalNet, and the authors' newly created CausalProbe 2024 (with easy and hard variations). The models were evaluated using a \"vanilla\" approach (direct inference) and three enhanced approaches: Chain of Thought (COT), Retrieval Augmented Generation (RAG), and the authors' G2-Reasoner. The table demonstrates the performance of each model and approach on each benchmark, showing how the different LLMs and methods compare.  Standard deviations, offering a measure of the variability of the results, are detailed in Appendix G.", "section": "6.3 Result Analysis"}, {"figure_path": "1IU3P8VDbn/tables/tables_22_1.jpg", "caption": "Table 2: Results of the studied LLMs on four causal Q&A benchmarks. The metric is exact match (EM). \u201cVanilla\u201d denotes doing inference directly. \u201cC-E\u201d, \u201cC-H\u201d represnt CausalProbe-E and CausalProbe-H.", "description": "This table presents the exact match scores achieved by four different large language models (LLMs) on four distinct causal question and answering (Q&A) benchmark datasets.  The models' performance is evaluated using the 'exact match' metric, assessing the accuracy of their responses.  The 'vanilla' method indicates a direct, unprompted inference; while 'C-E' and 'C-H' refer to the CausalProbe-E and CausalProbe-H subsets within the CausalProbe 2024 benchmark.", "section": "4.2 Empirical Study on Causal Reasoning Capabilities of LLMs"}, {"figure_path": "1IU3P8VDbn/tables/tables_22_2.jpg", "caption": "Table 1: The data cut-off time comparison of the studied LLMs and CausalProbe 2024 benchmark.", "description": "This table compares the last updated time of the training datasets for four large language models (LLMs): LLaMA 2 7B chat, LLaMA 3 8B instruct, GPT 3.5 turbo, and Claude 3 opus.  It also shows the release date of the CausalProbe 2024 benchmark. The comparison highlights that the CausalProbe 2024 dataset was released significantly later than the training data cutoff times for all four LLMs, ensuring that the benchmark data is new and unseen by the models during their training.", "section": "4.2 Empirical Study on Causal Reasoning Capabilities of LLMs"}]