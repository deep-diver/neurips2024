[{"type": "text", "text": "SEEV: Synthesis with Efficient Exact Verification for ReLU Neural Barrier Functions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hongchao Zhang\u2217 ", "page_idx": 0}, {"type": "text", "text": "Zhizhen Qin\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Electrical & Systems Engineering Washington University in St. Louis hongchao@wustl.edu ", "page_idx": 0}, {"type": "text", "text": "Computer Science & Engineering University of California, San Diego zhizhenqin@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Sicun Gao ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrew Clark ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science & Engineering University of California, San Diego scungao@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Electrical & Systems Engineering Washington University in St. Louis andrewclark@wustl.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural Control Barrier Functions (NCBFs) have shown significant promise in enforcing safety constraints on nonlinear autonomous systems. State-of-the-art exact approaches to verifying safety of NCBF-based controllers exploit the piecewiselinear structure of ReLU neural networks, however, such approaches still rely on enumerating all of the activation regions of the network near the safety boundary, thus incurring high computation cost. In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV). Our framework consists of two components, namely (i) an NCBF synthesis algorithm that introduces a novel regularizer to reduce the number of activation regions at the safety boundary, and (ii) a verification algorithm that exploits tight over-approximations of the safety conditions to reduce the cost of verifying each piecewise-linear segment. Our simulations show that SEEV significantly improves verification efficiency while maintaining the CBF quality across various benchmark systems and neural network structures. Our code is available at https://github.com/HongchaoZhang-HZ/SEEV. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Safety is a crucial property for autonomous systems that interact with humans and critical infrastructures in applications including medicine, energy, and robotics [1, 2], which has motivated recent research into safe control [3, 4, 5, 6, 7]. Control Barrier Functions (CBFs), which apply a constraint on the control input at each time in order to ensure that safety constraints are not violated, have attracted significant research attention due to their ease of implementation and compatibility with a variety of safety and performance criteria [8]. Recently, CBFs that are defined by neural networks, denoted as Neural Control Barrier Functions (NCBFs), have been proposed to leverage the expressiveness of NNs for safe control of nonlinear systems [9, 10, 11]. NCBFs have shown substantial promise in applications including robotic manipulation [10], navigation [12, 13], and flight control [14]. ", "page_idx": 0}, {"type": "text", "text": "A key challenge in NCBF-based control is safety verification, which amounts to ensuring that the constraints on the control can be satisfied throughout the state space under actuation limits. The NCBF safety verification problem effectively combines two problems that are known to be difficult, namely, input-output verification of neural networks (VNN) [15, 16, 17, 18, 19, 20] and reachability verification of nonlinear systems. While sound and complete verifiers such as dReal can be applied to ", "page_idx": 0}, {"type": "text", "text": "NCBFs, they typically can only handle systems of dimension three or small neural networks [21, 22]. In [23], exact conditions for safety verification of NCBFs with ReLU activation functions were proposed that leverage the piecewise-linearity of ReLU-NNs to reduce verification time compared to dReal for general activation functions. The exact conditions, however, still require checking correctness of the NCBF by solving a nonlinear optimization problem along each piecewise-linear segment. Hence, the NCBF verification problem remains intractable for high-dimensional systems. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV) for piecewise-linear NCBFs. The main insight of SEEV is that the computational bottleneck of NCBF verification is the inherent requirement of verifying each linear segment of the neural network. We mitigate this bottleneck by (i) developing a training procedure that reduces the number of segments that must be verified and (ii) constructing verification algorithms that efficiently enumerate the linear segments at the safety boundary and exploit easily-checked sufficient conditions to reduce computation time. Towards (i), we introduce a regularizer to the loss function that penalizes the dissimilarity of activation patterns along the CBF boundary. Towards (ii), we propose a breadthfirst search algorithm for efficiently enumerating the boundary segments, as well as tight linear over-approximations of the nonlinear optimization problems for verifying each segment. Moreover, we integrate the synthesis and verification components by incorporating safety counterexamples returned by the safety verifier into the training dataset. Our simulation results demonstrate significant improvements in verification efficiency and reliability across a range of benchmark systems. ", "page_idx": 1}, {"type": "text", "text": "Related Work: Neural control barrier functions have been proposed to describe complex safety sets to remain inside and certify safety of a controlled system [24, 21, 25, 26] or synthesize control input based on NCBFs to ensure safety [8, 9, 10, 27]. However, the synthesized NCBF may not ensure safety. Safety verification of NCBFs is required. Sum-of-squares (SOS) optimization [28, 29, 30, 31, 32] has been widely used for polynomial barrier functions, however, they are not applicable due to the non-polynomial and potentially non-differentiable activation functions of NCBFs. VNN [33, 34, 35] and methods for ReLU neural networks [36, 37] are also not directly applicable to NCBF verification. Nonlinear programming approach [23] provides another route for exact verification but is computationally intensive and relies on VNN tools. To synthesize neural networks with verifiable guarantees, Counterexample Guided Inductive Synthesis (CEGIS) has been applied using SMTbased techniques [21, 38, 39, 40, 41]. Other verification-in-the-loop approaches utilize reachability analysis [42] and branch-and-bound neural network verification tools [43]. However, existing works suffer from the difficulty of performing verification and generating counterexamples in a computationally efficient manner. Sampling-based approaches [41, 11] aim to prove safety using Lipschitz conditions, but they rely on dense sampling over the state space, which is computationally prohibitive. In this work, we present SEEV to integrate the synthesis and efficient verification by incorporating safety counterexamples from the exact verification. ", "page_idx": 1}, {"type": "text", "text": "Organization The remainder of the paper is organized as follows. Section 2 gives the system model and background on neural networks and the conditions of valid NCBFs. Section 3 presents the SEEV framework. Section 4 presents our efficient and exact verification. Section 5 contains simulation results. Section 6 concludes the paper. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section presents the system model, notations on neural networks, and exact conditions of safety. ", "page_idx": 1}, {"type": "text", "text": "2.1 System Model ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a system with state $x(t)\\in\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ and input $u(t)\\in\\mathcal{U}\\subseteq\\mathbb{R}^{m}$ , with initial condition $x(t_{0})\\,=\\,x_{0}$ where $x_{0}$ lies in an initial set $\\mathcal{T}\\subseteq\\mathcal{X}$ . The continuous-time nonlinear control-affine system has the dynamics given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\dot{x}}(t)=f(x(t))+g(x(t))u(t)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ and $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n\\times m}$ are known continuous functions. ", "page_idx": 1}, {"type": "text", "text": "We consider the case that the system is required to remain inside a given set of states, i.e., $x(t)\\in\\mathcal{C}$ for all time $t\\geq t_{0}$ . The set $\\mathcal{C}$ , referred to as the safe set, is defined as ${\\mathcal{C}}=\\{x:h(x)\\geq0\\}$ by some given continuous function $h:\\mathbb{R}^{n}\\to\\mathbb{R}$ . The unsafe region is given by ${\\mathcal{X}}\\setminus{\\mathcal{C}}$ . ", "page_idx": 1}, {"type": "text", "text": "2.2 Neural Network Model and Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We let $\\mathbf{W}$ and $\\mathbf{r}$ denote the weight and bias of a neural network, and let $\\theta$ be a parameter vector obtained by concatenating $\\mathbf{W}$ and $\\mathbf{r}$ . We consider a $\\theta$ -parameterized feedforward neural network $b_{\\theta}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ constructed as follows. The network consists of $L$ layers, with each layer $i$ consisting of $M_{i}$ neurons. We let $(i,j)\\in\\{1,\\ldots,L\\}\\times\\{1,\\ldots,M_{i}\\}$ denote the $j$ -th neuron at the $i$ -th layer. We denote the pre-activation input as $z_{j}^{(i)}$ , piecewise linear activation function $\\sigma$ and the post-activation output as z\u02c6j(i) $\\hat{z}_{j}^{(i)}=\\sigma(z_{j}^{(i)})$ . Specifically, we assume that the NN has the Rectified Linear Unit (ReLU) activation function, defined by $\\sigma(z)=z$ for $z\\geq0$ and $\\sigma(z)=0$ for $z<0$ . We define the neuron $(i,j)$ as active if $z_{j}^{(i)}>0$ , inactive if $z_{j}^{(i)}<0$ and unstable if $z_{j}^{(i)}=0$ . Let ${\\bf S}=\\tau_{S}(x)=\\{(i,j):$ $z_{j}^{(i)}\\ge0\\}\\subseteq\\{(i,j):i=1,\\dots,L,j=1,\\dots,M_{i}\\}$ denote the set of activated and unstable neurons, produced by state $x$ and function $\\tau_{S}$ . Let ${\\bf T}(x)=\\tau_{T}(x)=\\{(i,j):z_{j}^{(i)}=0\\}$ zj(i) = 0} denote the set of unstable neurons produced by $x$ and $\\tau_{T}$ . $\\mathbf{T}(\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r})$ denote the set of unstable neurons produced by activation sets $\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r}$ . The set of inactive neurons is given by $\\mathbf{S}^{c}$ , i.e., the complement of S, and consists of neurons with negative pre-activation input. We define vectors $\\overline{{W}}_{i j}\\bar{(\\mathbf{S})}\\in\\mathbb{R}^{n}$ and scalars rij(S) \u2208R such that zj(i) $z_{j}^{(i)}=\\overline{{W}}_{i j}(\\mathbf{S})^{T}x+\\overline{{r}}_{i j}(\\mathbf{S})$ in the Appendix A.1. The symmetric difference between two sets $\\mathbf{A}$ and $\\mathbf{B}$ , denoted by $\\mathbf{A}\\Delta\\mathbf{B}$ , is defined as $\\mathbf{A}\\Delta\\mathbf{B}=(\\mathbf{A}\\setminus\\mathbf{B})\\cup(\\mathbf{B}\\setminus\\mathbf{A})$ . Finally, we define the terms hyperplane and hinge. For any $\\mathbf{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,M_{i}\\}$ , we define $\\overline{{\\mathcal{X}}}(\\mathbf{S}):=\\{x:\\mathbf{S}(x)=\\mathbf{S}\\}$ . The collection of $\\overline{{\\mathcal{X}}}(\\mathbf{S})$ for all $\\mathbf{S}$ is the set of hyperplanes associated with the ReLU neural network. A hyperplane that intersects the set $\\{x:b_{\\theta}(x)=0\\}$ is a boundary hyperplane. The intersection of hyperplanes $\\overline{{\\mathcal{X}}}(\\mathbf{S}_{1}),\\ldots,\\overline{{\\mathcal{X}}}(\\mathbf{S}_{r})$ is called a hinge. A hinge that intersects the set $\\{x:b_{\\theta}(x)=0\\}$ is a boundary hinge. ", "page_idx": 2}, {"type": "text", "text": "2.3 Guaranteeing Safety via Control Barrier Functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Barrier certificates [28] ensure the safety of a feedback-controlled system under policy $\\mu(x\\mid\\lambda)$ by identifying a CBF to represent the invariant safe set. The barrier certificate defines an inner safe region ${\\mathcal{D}}:=\\{x:b(x)\\geq0\\}$ for some continuous function $b$ . The verifiable invariance of $\\mathcal{D}$ is obtained from the following result. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (Nagumo\u2019s Theorem [44], Section 4.2). A closed set $\\mathcal{D}$ is controlled positive invariant $i f,$ whenever $x\\in\\partial{\\mathcal{D}}$ , where $\\partial\\mathcal{D}$ denotes the boundary of $\\mathcal{D}$ . we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n(f(x)+g(x)u)\\in A_{\\mathcal{D}}(x)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for some $u\\in\\mathcal{U}$ where $A_{\\mathit{D}}(x)$ is the tangent cone to $\\mathcal{D}$ at $x$ . ", "page_idx": 2}, {"type": "text", "text": "We denote a state $\\hat{x}_{c e}^{c}$ with $\\hat{x}_{c e}^{c}\\,\\in\\,\\partial D$ that violates (2) as a safety counterexample. In the case where $b$ is continuously differentiable, (2) can be satisfied by selecting $u$ to satisfy the condition $\\begin{array}{r}{\\frac{\\partial b}{\\partial x}(f(x(t))+g(x(t))u(t))\\geq-\\alpha(b(x(t)))}\\end{array}$ , where $\\alpha:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a strictly increasing function with $\\alpha(0)=0$ . When $b$ is not continuously differentiable, as in a ReLU NCBFs, a modified condition is needed. Prior work [23] introduces exact conditions for safety verification of ReLU NCBFs, based on the following proposition. A collection of activation sets $\\mathbf{S_{1}},\\dotsc...,\\mathbf{S_{r}}$ is complete if, for any $\\mathbf{S}^{\\prime}\\notin\\{\\mathbf{S_{1}},\\ldots,\\mathbf{S_{r}}\\}$ , we have $\\overline{{\\mathcal{X}}}(\\mathbf{S_{1}})\\cap\\cdots\\cap\\overline{{\\mathcal{X}}}(\\mathbf{S_{r}})\\cap\\overline{{\\mathcal{X}}}(\\mathbf{S}^{\\prime})=\\emptyset.$ . ", "page_idx": 2}, {"type": "text", "text": "Proposition 1. Suppose the function ReLU neural network-defined function $b$ satisfies the following conditions: ", "page_idx": 2}, {"type": "text", "text": "(i) For all activation sets $\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r}$ with $\\{\\mathbf{S}_{1},\\hdots,\\mathbf{S}_{r}\\}$ complete and any $x$ satisfying $b(x)=0$ and ", "page_idx": 2}, {"type": "equation", "text": "$$\nx\\in\\left(\\bigcap_{l=1}^{r}\\overline{{\\chi}}(\\mathbf{S}_{l})\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "there exist $l\\in\\{1,\\ldots,r\\}$ and $u\\in\\mathcal{U}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\overline{{\\mathbf{W}}}_{i-1}(\\mathbf{S}_{l})W_{i j})^{T}(f(x)+g(x)u)\\ge0\\quad\\forall(i,j)\\in\\mathbf{T}(\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r})\\cap\\mathbf{S}_{l}}\\\\ &{(\\overline{{\\mathbf{W}}}_{i-1}(\\mathbf{S}_{l})W_{i j})^{T}(f(x)+g(x)u)\\le0\\quad\\forall(i,j)\\in\\mathbf{T}(\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r})\\setminus\\mathbf{S}_{l}}\\\\ &{\\qquad\\qquad\\overline{{W}}(\\mathbf{S}_{l})^{T}(f(x)+g(x)u)\\ge0}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "(ii) For all activation sets S, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\overline{{\\mathcal{X}}}(\\mathbf{S})\\cap\\mathcal{D})\\setminus\\mathcal{C}=\\emptyset\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If $b(x(0))\\geq0,$ , then $x(t)\\in\\mathcal{C}$ for all $t\\geq0$ . ", "page_idx": 3}, {"type": "text", "text": "Any feedback control law $\\mu:\\mathcal{X}\\to\\mathcal{U}$ that satisfies (4)\u2013(6) is guaranteed to ensure safety and is referred to as an NCBF control policy. Given a nominal control policy $\\pi_{n o m}(x)$ , safe actions can be derived from a ReLU NCBF as a safety filter[29, 9] by solving the following optimization problem proposed in [23, Lemma 2]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{S}\\in\\mathbf{S}(x),u}||u-\\pi_{n o m}(x)||_{2}^{2}\\quad\\mathrm{s.t.}\\quad\\overline{{W}}(\\mathbf{S})^{T}(f(x)+g(x)u)\\geq-\\alpha(b(x)),u\\in\\mathcal{U},(4)-(6)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The solution to this optimization problem provides a control $u$ that minimally deviates from the nominal control $\\pi_{n o m}(x)$ while satisfying NCBF constraints derived in Proposition 1 ensuring that $\\mathcal{D}$ is positive invariant and is contained in $\\mathcal{C}$ . Based on Proposition 1, we can define different types of safety counterexamples. Correctness counterexamples, denoted by $\\hat{x}_{c e}^{c}$ , refers to a state $\\hat{x}_{c e}^{c}\\in\\mathcal{D}\\cap(\\mathcal{X}\\setminus\\mathcal{C})$ . Hyperplane verification counterexamples refer to states $\\hat{x}_{c e}^{h}$ that violate (6). Hinge verification counterexamples are states $x$ with $\\mathbf{T}(\\boldsymbol{x})\\neq\\emptyset$ that violate (4)\u2013(6). ", "page_idx": 3}, {"type": "text", "text": "3 Synthesis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the framework to synthesize NCBF $b_{\\theta}(x)$ to ensure the safety of the system (1). The synthesis framework aims to train an NCBF and construct an NCBF-based safe control policy. We first formulate the problem and present an overview of the framework in 3.1. Then we demonstrate the design of the loss function in 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Overall Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our primary objective is to synthesize a ReLU Neural Control Barrier Function (ReLU-NCBF) for (1) and develop a safe control policy to ensure system safety. ", "page_idx": 3}, {"type": "text", "text": "Problem 1. Given a system (1), initial set $\\mathcal{T}$ and a safety set $\\mathcal{C}$ , synthesize a ReLU NCBF $b_{\\theta}(x)$ parameterized by $\\theta$ such that the conditions in Proposition $^{\\,I}$ are satisfied. Then construct a control policy to synthesize $u_{t}$ such that the system remains positive invariant in $\\mathcal{D}:=\\{x:b_{\\theta}(x)\\geq0\\}\\subseteq\\mathcal{C}$ . ", "page_idx": 3}, {"type": "text", "text": "We propose SEEV to address this problem with the synthesis framework demonstrated in Fig. 1. The training dataset $\\tau$ is initialized by uniform sampling over $\\mathcal{X}$ . The training framework consists of two loops. The inner loop attempts to choose parameter $\\theta$ for $b_{\\theta}(x)$ to satisfy the safety condition by minimizing the loss function over training data $\\tau$ . The outer loop validates a given NCBF $b_{\\theta}(x)$ by searching for safety counterexamples $\\hat{x}_{c e}$ and updates the training dataset as $\\mathcal{T}\\cup\\{\\hat{x}_{c e}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "To train the parameters of the NCBF to satisfy the conditions of Proposition 1, we propose a loss function that penalizes the NCBF for violating constraints (i) and (ii) at a collection of sample points. The loss function is a weighted sum of three terms. The first term is the correctness loss penalizing state $\\hat{x}\\in\\mathcal{X}\\setminus\\mathcal{C}$ with $b_{\\theta}(\\bar{x})\\ge0$ . The second term is verification loss that penalizes states $\\hat{x}$ that $\\nexists u$ such that (4)-(6) hold. The third term is a regularizer minimizing the number of hyperplanes and hinges along the boundary. However, minimizing the loss function is insufficient to ensure safety [9] because there may exist safety counterexamples outside of the training dataset. In order to guarantee safety, SEEV introduces an efficient exact verifier to certify whether $b_{\\theta}(x)$ meets the safety conditions outlined in Proposition 1. The verifier either produces a proof of safety or generates a safety counterexample that can be added to the training dataset to improve the NCBF. ", "page_idx": 3}, {"type": "text", "text": "The integration of the verifier can improve safety by adding counterexamples to guide the training process, however, it may also introduce additional computation complexity. We propose a combined approach, leveraging two complementary methods to address this issue. First, the verification of SEEV introduces an efficient algorithm in Section 4 to mitigate the computational scalability challenges that arise as neural network depth and dimensionality increase. Second, SEEV introduces a regularizer to limit the number of boundary hyperplanes and hinges to be verified, addressing the complexity that arises as neural network size increases. ", "page_idx": 3}, {"type": "image", "img_path": "nWMqQHzI3W/tmp/5317f471c50fa30aa7d4ef6f9ff4e958a2d36fc9ac6812bec18d7dde594e98fd.jpg", "img_caption": ["Figure 1: SEEV: Synthesis with Efficient Exact Verifier for ReLU NCBF "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 Loss Function Design and NCBF Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The goal of the inner loop is to choose parameters $\\theta$ so that the conditions of Proposition 1 are satisfied for all ${\\hat{x}}\\in{\\mathcal{T}}$ and the computational cost of verifying safety is minimized. To achieve the latter objective, we observe (based on results presented in Table 2) that the computational complexity of verification grows with the cardinality of the collection of activation sets that intersect the safety boundary $\\partial\\mathcal{D}$ . The collection is defined as $\\mathcal{B}:=\\left\\{\\mathbf{S}_{i}:\\partial\\mathcal{D}\\cap\\overline{{\\mathcal{X}}}(\\mathbf{S}_{i})\\neq\\emptyset\\right\\}$ . Hence, we formulate the following unconstrained optimization problem to search for $\\theta$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\theta}{\\operatorname*{min}}}&{{}\\ \\lambda_{B}\\mathcal{L}_{B}(\\mathcal{T})+\\lambda_{f}\\mathcal{L}_{f}(\\mathcal{T})+\\lambda_{c}\\mathcal{L}_{c}(\\mathcal{T})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}_{B}(\\mathcal{T})$ regularizer to minimize $|\\beta|$ , $\\mathcal{L}_{f}(\\mathcal{T})$ is the loss penalizing the violations of constraint (4)-(6) ((i) of Proposition 1), $\\mathcal{L}_{c}(\\tau)$ penalizes the violations of constraint (7) ((ii) of Proposition 1), and $\\lambda_{B},\\,\\lambda_{f}$ and $\\lambda_{c}$ are non-negative coefficients defined as follows. ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{L}_{f}$ Regularizer: For each sample ${\\hat{x}}\\in{\\mathcal{T}}$ the safe control signal is calculated by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{u,r}\\quad||u-\\pi_{n o m}(x)||_{2}^{2}\\qquad s.t.\\quad\\mathcal{W}(\\mathbf{S}_{l})^{T}(f(\\hat{x})+g(\\hat{x})u)+r\\geq0\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{W}=\\overline{{W}}$ for differentiable points and $\\mathcal{W}=\\tilde{W}$ defined as the subgradient at non-differentiable points. The regularizer $\\mathcal{L}_{f}$ enforces the satisfaction of the constraint by inserting a positive relaxation term $r$ in the constraint and minimizing $r$ with a large penalization in the objective function. We have the loss $\\mathcal{L}_{f}$ defined as $\\mathcal{L}_{f}=||u-\\pi_{n o m}(x)||_{2}^{2}+r$ . We use [45] to make this procedure differentiable, allowing us to employ the relaxation loss into the NCBF loss function design. ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{L}_{c}$ Regularizer: $\\mathcal{L}_{c}$ regularizer enforces the correctness of the NCBF. In particular, it enforces the $b_{\\theta}(x)$ of safe samples $x\\in\\mathcal{T}_{\\mathbb{Z}}$ to be positive, and $b_{\\theta}(x)$ of unsafe samples $\\tau_{x\\setminus c}$ to be negative. Define $N_{\\mathrm{safe}}=|\\mathcal{T}_{\\mathbb{Z}}|$ and $N_{\\mathrm{unsafe}}=|\\mathcal{T}_{\\mathcal{X}\\backslash c}|$ , and with a small positive tuning parameter $\\epsilon>0$ , the loss term $\\mathcal{L}_{c}$ can be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}=a_{1}\\frac{1}{N_{\\mathrm{safe}}}\\sum_{x\\in\\mathcal{T}_{\\mathcal{T}}}\\left[\\epsilon-b_{\\theta}(x)\\right]_{+}+a_{2}\\frac{1}{N_{\\mathrm{unsafe}}}\\sum_{x\\in\\mathcal{T}_{\\mathcal{X}\\backslash\\mathcal{C}}}\\left[\\epsilon+b_{\\theta}(x)\\right]_{+}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $[\\cdot]_{+}=m a x(\\cdot,0)$ . $a_{1}$ and $a_{2}$ are positive parameters controlling penalization strength of the violations of safe and unsafe samples. ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{L}_{\\mathcal{B}}$ Regularizer: We propose a novel regularizer to limit the number of boundary hyperplanes and hinges by penalizing the dissimilarity, i.e., $\\mathbf{S}(\\hat{x}_{i})\\Delta\\mathbf{S}(\\hat{x}_{j})$ of boundary activation sets $\\mathbf{\\bar{S}}(\\hat{x})\\in\\mathcal{B}$ However, the dissimilarity measure of boundary activation sets is inherently nondifferentiable. To address this issue the regularizer introduces the generalized sigmoid function \u03c3k(z) =1+exp1(\u2212k\u00b7z) to compute the vector of smoothed activation defined as $\\phi_{\\sigma_{k}}(x):=[\\sigma_{k}(z_{i,j}),\\forall i,j\\in\\{1,\\ldots,L\\}\\;\\times$ $\\{1,\\dotsc,M_{i}\\}]$ . The $\\mathcal{L}_{B}$ regularizer conducts the following two steps to penalize dissimilarity. ", "page_idx": 4}, {"type": "text", "text": "In the first step, the regularizer identifies the training data in the boundary hyperplanes and hinges denoted as $\\hat{x}\\in\\mathcal{T}_{\\partial D}$ . The set is defined as $\\mathcal{T}_{\\partial\\mathcal{D}}:=\\{\\hat{x}:\\hat{x}\\in\\overline{{\\mathcal{X}}}(\\mathbf{S}),\\forall\\mathbf{S}\\in\\mathcal{B}\\}$ . To further improve the efficiency, the regularizer approximates $\\mathcal{T}_{\\partial\\mathcal{D}}$ with a range-based threshold $\\epsilon$ on the output of the NCBF, i.e., $|b_{\\theta}(\\hat{x})|\\le\\epsilon$ . ", "page_idx": 4}, {"type": "text", "text": "The second step is to penalize the dissimilarity of $\\mathbf{S}\\in\\mathcal{B}$ . To avoid the potential pitfalls of enforcing similarity across the entire boundary $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , the regularizer employs an unsupervised learning approach to group the training data into $N_{\\mathbf{B}}$ clusters. We define the collection of the activation set in each cluster as $\\mathbf{B}_{i}\\subseteq\\mathcal{B}$ and the collection in each cluster as $\\begin{array}{r}{\\mathscr{T}_{\\mathbf{B}_{i}}:=\\{\\underline{{\\hat{x}}}:\\hat{x}\\in\\bigcup_{\\mathbf{S}\\in\\mathbf{B}_{i}}\\overline{{\\mathscr{X}}}(\\mathbf{S})\\}}\\end{array}$ . The $\\mathcal{L}_{\\mathcal{B}}$ is then defined as follows, with an inner sum over all pairs of $\\hat{x}\\in\\mathcal{T}_{\\mathbf{B}_{i}}$ and an outer sum over all clusters. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{B}}=\\frac{1}{N_{\\mathbf{B}}}\\sum_{\\mathbf{B}_{i}\\in B}\\frac{1}{|\\mathcal{T}_{\\mathbf{B}_{i}}|^{2}}\\sum_{\\hat{x}_{i},\\hat{x}_{j}\\in\\mathcal{T}_{\\mathbf{B}_{i}}}\\Vert\\phi_{\\sigma_{k}}(x_{i})-\\phi_{\\sigma_{k}}(x_{j})\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Verification ", "text_level": 1, "page_idx": 5}, {"type": "image", "img_path": "nWMqQHzI3W/tmp/fcd48a57650e923ec9787ec9999d8940c155fb33b3f14254ac85ecac774705c3.jpg", "img_caption": ["Figure 2: Overview of the Efficient Exact Verifier for ReLU NCBFs "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In this section, we demonstrate the efficient exact verification of the given NCBF $b_{\\theta}(x)$ to ensure the positive invariance of the set $\\mathcal{D}$ under the NCBF-based control policy. In what follows, we propose an efficient enumeration and progressive verification moving from sufficient to exact conditions. The overview of the proposed approach is as shown in Fig. 2. SEEV decomposes the NCBF into hyperplanes and hinges and verifies each component hierarchically, with novel tractable sufficient conditions verified first and the more complex exact conditions checked only if the sufficient conditions fail. Given an NCBF $b_{\\theta}(x)$ , the verification of SEEV returns (i) a Boolean variable that is \u2018true\u2019 if the conditions of Proposition 1 are satisfied and \u2018false\u2019 otherwise, and (ii) a safety counterexample $\\hat{x}$ that violates the conditions of Proposition 1 if the result is \u2018false\u2019. Algorithm 1 presents an overview of the verification of SEEV. The algorithm consists of an enumeration stage to identify all boundary hyperplanes and hinges, and a verification stage to certify satisfaction of conditions in Proposition 1 for all hyperplanes and hinges. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Efficient Exact Verification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input: $n,\\tau_{x\\backslash c},\\tau_{z}$   \n2: Output: Verification Boolean result $r$ , Categorized counterexample $\\hat{x}_{c e}$ ", "page_idx": 5}, {"type": "text", "text": "3: procedure EFFICIENT EXACT VERIFICATION $({\\mathcal{T}}_{\\mathcal{X}\\backslash{\\mathcal{C}}},{\\mathcal{T}}_{\\mathcal{Z}})$ ", "page_idx": 5}, {"type": "text", "text": "4: $\\mathbf{S}_{0}\\gets\\mathrm{ENUMNIT}(\\mathcal{T}_{\\mathcal{X}\\backslash\\mathcal{C}},\\mathcal{T}_{\\mathcal{Z}})$   \n5: $S\\gets\\mathrm{NBFS}(\\mathbf{S}_{0})$   \n6: r, x\u02c6(cce) \u2190CorrectnessVerifier(S)   \n7: r, x\u02c6(che) \u2190HyperplaneVerifier(S)   \n8: V \u2190HingeEnum(S, n)   \n9: r, x\u02c6(cge) \u2190HingeVerifier(V)   \n10: Return r, x\u02c6ce   \n\u25b7Initial Activation Set Identification, Section 4.1   \n$\\triangleright$ Activation Sets Enumeration, Section 4.1   \n\u25b7Correctness Verification, Section 4.2   \n$\\triangleright$ Hyperplane Verification, Section 4.2   \n\u25b7Hinges Enumeration, Section 4.1   \n\u25b7Hinge Verification, Section 4.2 ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.1 Enumeration of Hyperplanes and Hinges ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The boundary enumeration identifies the boundary hyperplanes $\\mathbf{S}_{0}\\in B$ . It initially identifies the initial boundary activation set $\\mathbf{S}_{0}\\in B$ , enumerates all $\\mathbf{S}\\in\\mathcal{B}$ by NBFS starting from $\\mathbf{S}_{0}$ , and finally enumerates all hinges consisting of the intersections of hyperplanes. The NBFS approach avoids over-approximation of the set of boundary hyperplanes that may be introduced by, e.g., interval propagation methods, and hence is particularly suited to deep neural networks. ", "page_idx": 5}, {"type": "text", "text": "In what follows, we assume that the unsafe region ${\\mathcal{X}}\\setminus{\\mathcal{C}}$ and initial safe set $\\mathcal{T}$ are connected, and use $\\partial\\mathcal{D}$ to refer to the connected component of the boundary of $\\mathcal{D}$ that separates $\\mathcal{T}$ and ${\\mathcal{X}}\\setminus{\\mathcal{C}}$ . This assumption is without loss of generality since we can always repeat the following procedure for each connected component of $\\mathcal{T}$ and ${\\mathcal{X}}\\setminus{\\mathcal{C}}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Initial Activation Set Identification: First, we identify the initial boundary activation set $\\mathbf{S}_{0}$ . Given $\\hat{x}_{U}\\in\\mathcal{X}\\setminus\\mathcal{C}$ and $\\hat{x}z\\in{\\cal Z}$ , define a line segment ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\tilde{L}}=\\left\\{x\\in\\mathbb{R}^{n}\\mid x=(1-\\lambda){\\hat{x}}_{X\\backslash{\\mathcal{C}}}+\\lambda{\\hat{x}}_{\\mathbb{Z}},\\ \\lambda\\in[0,1]\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following lemma shows the initial boundary activation set $\\mathbf{S}_{0}$ can always be produced as $\\mathbf{S}_{0}=\\mathbf{S}(\\tilde{x})$ for some $\\tilde{x}\\in\\tilde{L}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. Given two sample points $\\hat{x}_{U}$ , such that $b(\\hat{x}_{U})<0,$ , and $\\hat{x}_{\\mathbb{Z}}$ , such that $b(\\hat{x}_{U})>0,$ , let $\\tilde{L}$ denote the line segment connecting these two points. Then, there exists a point $\\tilde{x}\\in\\tilde{L}$ with $b_{\\theta}(\\tilde{x})=0$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 follows from the intermediate value theorem and continuity of $b_{\\theta}(x)$ . In order to search for S0, we choose a sequence of NL\u02dc points x01, . . . , x0N L\u02dc . For each $\\boldsymbol{x}_{i}^{0}$ , we check to see if $\\overline{{\\mathcal{X}}}(\\mathbf{S}(x_{i}^{0}))\\cap\\partial\\mathcal{D}\\neq\\emptyset$ by solving boundary linear program BoundaryLP $\\big(\\mathbf{S}(x_{i}^{0})\\big)$ (14) in Appendix A.4 ", "page_idx": 6}, {"type": "text", "text": "Activation Sets Enumeration: We next describe Neural Breadth-First Search (NBFS) for enumerating all activation sets along the zero-level set, given an initial set $\\mathbf{S}_{0}$ . NBFS enumerates a collection of boundary hyperplanes denoted as $\\boldsymbol{S}$ by repeating the following two steps. ", "page_idx": 6}, {"type": "text", "text": "Step 1: Given a set S, NBFS identifies a collection of neighbor activation sets denoted as $\\tilde{\\boldsymbol{B}}$ as follows. For each $(i,j)$ with $i=1,\\dots,L$ and $j=1,\\dots,M_{i}$ , construct $\\mathbf{S}^{\\prime}$ as $\\mathbf{S}^{\\prime}=\\left\\{\\mathbf{S}\\setminus(i,j),\\quad(i,j)\\in\\mathbf{S}^{\\prime}\\right.$ We check whether $\\mathbf{S}^{\\prime}\\in\\mathcal{B}$ by solving the linear program USLP $(\\mathbf{S},(i,j))$ (15) in Appendix A.4. ", "page_idx": 6}, {"type": "text", "text": "If there exists such a state $x$ , then $\\mathbf{S}^{\\prime}$ is added to $\\tilde{\\boldsymbol{B}}$ . To further improve efficiency, we employ the simplex algorithm to calculate the hypercube that overapproximates the boundary hyperplane, denoted as ${\\mathcal{H}}(\\mathbf{S})\\supseteq{\\overline{{\\mathcal{X}}}}(\\mathbf{S})$ , and relax the last constraint of (14) to $x\\in{\\mathcal{H}}(\\mathbf{S})$ . ", "page_idx": 6}, {"type": "text", "text": "Step 2: For each $\\mathbf{S}^{\\prime}\\in\\tilde{\\mathcal{B}}$ , NBFS determines if the activation region $\\overline{{\\mathcal{X}}}(\\mathbf{S}^{\\prime})$ is on the boundary (i.e., satisfies $\\overline{{\\mathcal{X}}}(\\mathbf{S}^{\\prime})\\cap\\partial\\mathcal{D}\\neq\\emptyset)$ ) by checking the feasibility of $B o u n d a r y L P(\\mathbf{S}^{\\prime})$ . If there exists such a state $x$ , then $\\mathbf{S}^{\\prime}$ is added to $\\boldsymbol{S}$ . This process continues in a breadth-first search manner until all such activation sets on the boundary have been enumerated. When this phase terminates, $S=B$ . Detailed procedure is described as Algorithm 3 in Appendix A.3. ", "page_idx": 6}, {"type": "text", "text": "Hinge Enumeration: The verifier of SEEV enumerates a collection $\\mathcal{V}$ of boundary hinges, where each boundary hinge $\\mathbf{V}\\in\\mathcal{V}$ is a subset $\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r}$ of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ with $\\overline{{\\mathcal{X}}}(\\mathbf{S}_{1})\\cap\\overline{{\\mathcal{X}}}(\\mathbf{S}_{r})\\cap\\partial\\mathcal{D}\\neq\\emptyset$ . ", "page_idx": 6}, {"type": "text", "text": "Given ${\\cal S}_{i}\\subseteq{\\cal B}$ and S, hinge enumeration filter the set of neighbor activation sets of $\\mathbf{S}$ defined as $\\mathcal{N}_{\\mathbf{S}}(S_{i}):=\\{\\mathbf{S}^{\\prime}:\\mathbf{S}^{\\prime}\\Delta\\mathbf{S}=1,\\mathbf{S}^{\\prime}\\in S_{i}\\}$ . Then, hinge enumeration identifies hinges $\\mathbf{V}$ by solving linear program HingeLP $(\\mathcal{N}_{\\bf S}^{(d)}(\\mathcal{S}_{i}))$ (16) in Appendix A.4. If $\\exists x$ , hinge enumeration includes the hinge into the set $\\mathcal{V}\\cup\\mathbf{V}$ . The efficiency can be further improved by leveraging the sufficient condition verification proposed in Section 4.2. The following result describes the completeness guarantees of $\\boldsymbol{S}$ and $\\nu$ enumerated in Line 5 and 8 of Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Proposition 2. Let $\\boldsymbol{S}$ and $\\nu$ denote the output of Algorithm $^{\\,l}$ . Then the boundary $\\partial\\mathcal{D}$ satisfies $\\partial\\overline{{\\cal D}}\\ \\mathbf\\subseteq\\bigcup_{\\mathbf{S}\\in\\cal S}\\overline{{\\cal X}}(\\mathbf{S})$ . Furthermore, $i f S$ is complete and $(\\bigcap_{i=1}^{r}\\overline{{\\chi}}(\\mathbf{S}_{i}))\\cap\\{x:b(x)=0\\}\\neq\\emptyset$ , then $\\{\\mathbf{S}_{1},\\allowbreak\\cdot\\cdot\\cdot,\\allowbreak\\mathbf{S}_{r}\\}\\in\\mathcal{V}$ . ", "page_idx": 6}, {"type": "text", "text": "The proof is omitted due to the space limit. A detailed proof is provided in Appendix A.2 ", "page_idx": 6}, {"type": "text", "text": "4.2 Efficient Verification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The efficient verification component takes the sets of boundary hyperplanes $\\boldsymbol{S}$ and boundary hinges $\\nu$ and checks that the conditions of Proposition 1 hold for each of them. As pointed out after Proposition 1, the problem of searching for safety counterexamples can be decomposed into searching for correctness, hyperplane, and hinge counterexamples. In order to maximize the efficiency of our approach, we first consider the least computationally burdensome verification task, namely, searching for correctness counterexamples. We then search for hyperplane counterexamples, followed by hinge counterexamples. ", "page_idx": 6}, {"type": "text", "text": "Correctness Verification: The correctness condition ((7)) can be verified for boundary hyperplane $\\overline{{\\mathcal{X}}}(\\mathbf{S})$ by solving the nonlinear program (17) in Appendix A.5. When $h(x)$ is convex, (17) can be solved efficiently. Otherwise, dReal can be used to check satisfiability of (17) in a tractable runtime. ", "page_idx": 7}, {"type": "text", "text": "Hyperplane Verification: Hyperplane counterexamples can be identified by solving the optimization problem (18) in Appendix A.5. Solving (18) can be made more efficient when additional structures on the input set $\\boldsymbol{\\mathcal{U}}$ and dynamics $f$ and $g$ are present. Consider a case $\\mathcal{U}=\\{D\\omega:||\\omega||_{\\infty}\\leq1\\}$ . In this case, the problem reduces to the nonlinear program (19) in Appendix A.5. If $f(x)$ and $g(x)$ are linear in $x$ , then the problem boils down to a linear program. If bounds on the Lipschitz coefficients of $f$ and $g$ are available, then they can be used to derive approximations of (19). ", "page_idx": 7}, {"type": "text", "text": "If $\\mathcal{U}\\,=\\,\\mathbb{R}^{m}$ , then by [23, Corollary 1], the problem can be reduced to the nonlinear program (20) in Appendix A.5. If $g(x)$ is a constant matrix $G$ , then safety is automatically guaranteed if $\\overline{{W}}(\\mathbf{S})^{T}G\\neq0$ . If $f(x)$ is linear in $x$ as well, then (20) is a linear program. ", "page_idx": 7}, {"type": "text", "text": "Hinge Verification: The hinge $\\mathbf{V}\\,=\\,\\{\\mathbf{S}_{1},\\hdots,\\mathbf{S}_{r}\\}$ can be certified by solving the nonlinear optimization problem (21) in Appendix A.5. In practice, simple heuristics are often sufficient to verify safety of hinges without resorting to solving (21). If $\\overline{{W}}(\\mathbf{S})^{T}f(x)>0$ for all $x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S}_{1})\\cap\\cdot\\cdot\\cap\\overline{{\\mathcal{X}}}(\\mathbf{S}_{r})$ , then the control input $u=0$ suffices to ensure safety. Furthermore, if $\\mathcal{U}=\\mathbb{R}^{m}$ and there exists $i\\in\\{1,\\ldots,m\\}$ and $s\\in\\{0,1\\}$ such that $\\mathrm{sign}((\\overline{{W}}({\\bf S}_{l})^{T}g(x))_{i})=s$ for all $x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S}_{1})\\cap\\ldots\\overline{{\\mathcal{X}}}(\\mathbf{S}_{r})$ and $l=1,\\hdots,r$ , then $u$ can be chosen as $u_{i}=K s$ for some sufficiently large $K>0$ to ensure that the conditions of Proposition 1. ", "page_idx": 7}, {"type": "text", "text": "Safety Guarantee: The safety guarantees of our proposed approach are summarized in Theorem 2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Given a NCBF $b_{\\theta}(x),\\,b_{\\theta}(x)$ is a valid NCBF if it passes the verification of Algorithm 1 using dReal to solve the optimization problems (17), (18), and (21). ", "page_idx": 7}, {"type": "text", "text": "The proof is derived from completeness of the enumeration in Algorithm 1 and dReal. A detailed proof can be found in Appendix A.6 ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the proposed SEEV in regularizer efficacy and verification efficiency. We also demonstrated the improved performance with counter-example guidance in the synthesis framework, whose results are detailed in B.2. We experiment on four systems, namely Darboux, obstacle avoidance, hi-ord8, and spacecraft rendezvous. The experiments run on a workstation with an Intel i7-11700KF CPU, and an NVIDIA GeForce RTX 3080 GPU. We include experiment settings in Appendix B.1 and hyperparameters settings in Table 4 in Appendix B.3. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experiment Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Darboux: We consider the Darboux system [46], a nonlinear open-loop polynomial system with detailed settings presented in Appendix B.1 ", "page_idx": 7}, {"type": "text", "text": "Obstacle Avoidance (OA): We evaluate our proposed method on a controlled system [47]. We consider Unmanned Aerial Vehicles (UAVs) avoiding collision with a tree trunk. The system state consists of a 2-D position and aircraft yaw rate $\\boldsymbol{x}:=[\\bar{x}_{1},x_{2},\\psi]^{T}$ . The system is manipulated by the yaw rate input $u$ with detailed settings presented in Appendix B.1. ", "page_idx": 7}, {"type": "text", "text": "Spacecraft Rendezvous (SR): We evaluate our approach on a spacecraft rendezvous problem from [48]. The system state is $\\boldsymbol{x}=[p_{x},p_{y},p_{z},v_{x},\\bar{v_{y}},v_{z}]^{T}$ and control input is $\\boldsymbol{u}=[u_{x},\\dot{u}_{y},u_{z}]^{T}$ with with detailed settings presented in Appendix B.1. ", "page_idx": 7}, {"type": "text", "text": "hi-ord8: We evaluate our approach on an eight-dimensional system that first appeared in [21] to evaluate the scalability of the proposed method. Detailed settings can be found in Appendix B.1 ", "page_idx": 7}, {"type": "text", "text": "5.2 Regularizer Efficacy Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 2 and Figure 3 illustrates the impact of regularization on the CBF boundary\u2019s activation sets. Table 2 compares various configurations, where $n$ denotes the input dimensions, $L$ represents the number of layers, and $M$ indicates the number of hidden units per layer. $N_{o}$ and $N_{r}$ are the number of hyperplanes along the zero-level boundary of the CBF without and with regularization, respectively, with $r$ indicating the regularization strength. $C_{o}$ captures the CBF\u2019s coverage of the safe region, while $\\rho_{(\\cdot)}=C_{(\\cdot)}\\bar{\\cdot}/C_{o}$ represents the safety coverage ratio relative to the unregularized CBF. Notably, \"N/A\" entries indicate configurations where training a fully verifiable network was infeasible due to the excessive number of boundary hyperplanes, which leads the verification process to time out. ", "page_idx": 7}, {"type": "image", "img_path": "nWMqQHzI3W/tmp/a5188cc29c3837c8a96c85f7237a35c6699cc354c327fd81773c9c823571b47a.jpg", "img_caption": ["Figure 3: Effects of boundary regularization $(r)$ on activation sets along the boundary. The figures show the results from a neural network with 4 layers of 8 hidden units, applied to the Spacecraft case. The surface represents the first two dimensions with the last four dimensions fixed at 0. Increasing $r$ results in more organized boundary activation sets. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "nWMqQHzI3W/tmp/ad12380134b9134639e0d9194daca9c83eb9b95d8c77becfcb1d80b52d8baa98.jpg", "table_caption": ["Table 1: Comparison of $N$ the number of boundary hyperplanes and $C$ coverage of the safe region $\\mathcal{D}$ of NCBF trained with and without boundary hyperplane regularizer denoted with subscripts $_r$ and $^o$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The results demonstrate that regularization effectively reduces the number of activation sets along the CBF boundary without compromising the coverage of the safe region. The efficiency is especially improved in cases with a greater number of hidden layers, where the unregularized model results in a significantly higher number of hyperplanes. For instance, in the SR case with $n=6$ , $L=4$ , and $M=8$ , the regularization reduces $N_{r=50}$ to 627 from $N_{o}=6218$ , maintaining the same safety coverage $\\rho_{r=50}=1$ ). See Appendix B.4 for hyperparameter sensitivity analysis. ", "page_idx": 8}, {"type": "text", "text": "Figure 3 illustrates the level sets of two CBFs trained for the SR case with $n\\,=\\,6$ , $L\\,=\\,4$ , and $M=8$ . These level sets are extracted from the first two dimensions with the rest set to zero. Each colored patch represents an activation pattern. The regularizer organizes the activation sets around the boundary, reducing unnecessary rapid changes and thereby enhancing the verification efficiency. ", "page_idx": 8}, {"type": "text", "text": "5.3 Efficient Verification Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The results presented in Table 2 illustrate a significant improvement in verification efficiency for Neural Control Barrier Functions NCBFs using the proposed method. In the table $t_{h}$ represents the time spent searching for hyperplanes containing the CBF boundary and verifying CBF sufficient conditions on these boundaries, and $t_{g}$ represents the time spent in hinge enumeration and verification. The total time, $T$ , is the sum of $t_{h}$ and $t_{g}$ . We compare our approach with three baselines, exact verification [23], SMT-based verification [21] with dReal and $Z3$ . Baseline methods\u2019 run times are represented by Baseline [23], dReal, and Z3. ", "page_idx": 8}, {"type": "text", "text": "In the Darboux cases, our method achieves verification in 2.5 seconds and 3.3 seconds for $M=256$ and $M=512$ respectively, whereas baseline methods take substantially longer, with Baseline [23] taking 315 seconds and 631 seconds, and both dReal and Z3 taking more than 3 hours. Similarly, in the OA cases, our method\u2019s run times range from 0.39 seconds to 20.6 seconds, faster than the baseline methods. In the more higher dimensional systems high-ord $^{\\cdot8}$ and SR, our method significantly outperforms Baseline [23]. Specifically, in high- $\\mathrm{ord}_{8}$ our methods finishes within 22.4 seconds while Baseline [23], dReal and Z3 times out, due to the need to enumerate the 8-dimensional input space. For the SR case, SEEV\u2019s run time are 9.8 seconds and 60.1 seconds, beating Baseline [23] which takes 179 seconds and 298.7 seconds respectively. Neural barrier certificate based dReal and Z3 are able to directly applicable since they require an explicit expression of the controlled feedback system. However, the SR system is manipulated by an NCBF-based safe controller that is nontrivial to derive an explicit expression. ", "page_idx": 8}, {"type": "table", "img_path": "nWMqQHzI3W/tmp/b4ed7c26b794db80403425329fdff8a7082a33f31bd1c1d4bdaa1dd631302830.jpg", "table_caption": ["Table 2: Comparison of verification run-time of NCBF in seconds. We denote the run-time as \u2018UTD\u2019 when the method is unable to be directly used for verification. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Note that hinge enumeration and certification may be time-consuming procedure, since they involve enumerating all combinations of hyperplanes. However, the results from Table 2 show that the certification can be completed on most hyperplanes with sufficient condition verification in Section 4.2, greatly improving the overall run time. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper considered the problem of synthesizing and verifying NCBFs with ReLU activation function in an efficient manner. Our approach is guided by the fact that the main contribution to the computational cost of verifying NCBFs is enumerating and verifying safety of each piecewise-linear activation region at the safety boundary. We proposed Synthesis with Efficient Exact Verification (SEEV), which co-designs the synthesis and verification components to enhance scalability of the verification process. We augment the NCBF synthesis with a regularizer that reduces the number of piecewise-linear segments at the boundary, and hence reduces the total workload of the verification. We then propose a verification approach that efficiently enumerates the linear segments at the boundary and exploits tractable sufficient conditions for safety. ", "page_idx": 9}, {"type": "text", "text": "Limitations: The method proposed in this paper mitigated the scalability issue. However, the synthesis and verification of NCBFs for higher-dimensional systems is challenging. Exact verification of non-ReLU NCBFs, which lack ReLU\u2019s simple piecewise linearity, remains an open problem. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by NSF grant CNS-1941670, CMMI-2418806, AFOSR grant FA9550-22-1-0054, NSF Career CCF 2047034, NSF CCF DASS 2217723, NSF AI Institute CCF 2112665, and Amazon Research Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shao-Chen Hsu, Xiangru Xu, and Aaron D Ames. Control barrier function based quadratic programs with application to bipedal robotic walking. In 2015 American Control Conference (ACC), pages 4542\u20134548. IEEE, 2015.   \n[2] Devansh R Agrawal and Dimitra Panagou. Safe control synthesis via input constrained control barrier functions. In 2021 60th IEEE Conference on Decision and Control (CDC), pages 6113\u20136118. IEEE, 2021.   \n[3] Xiangru Xu, Jessy W Grizzle, Paulo Tabuada, and Aaron D Ames. Correctness guarantees for the composition of lane keeping and adaptive cruise control. IEEE Transactions on Automation Science and Engineering, 15(3):1216\u20131229, 2017. [4] Joseph Breeden and Dimitra Panagou. High relative degree control barrier functions under input constraints. In 2021 60th IEEE Conference on Decision and Control (CDC), pages 6119\u20136124. IEEE, 2021. [5] Hongkai Dai and Frank Permenter. Convex synthesis and verification of control-lyapunov and barrier functions with input constraints. In 2023 American Control Conference (ACC), pages 4116\u20134123. IEEE, 2023.   \n[6] Shucheng Kang, Yuxiao Chen, Heng Yang, and Marco Pavone. Verification and synthesis of robust control barrier functions: Multilevel polynomial optimization and semidefinite relaxation, 2023.   \n[7] Nicholas Rober, Michael Everett, Songan Zhang, and Jonathan P How. A hybrid partitioning strategy for backward reachability of neural feedback loops. In 2023 American Control Conference (ACC), pages 3523\u20133528. IEEE, 2023. [8] Charles Dawson, Sicun Gao, and Chuchu Fan. Safe control with learned certificates: A survey of neural Lyapunov, barrier, and contraction methods for robotics and control. IEEE Transactions on Robotics, 2023.   \n[9] Oswin So, Zachary Serlin, Makai Mann, Jake Gonzales, Kwesi Rutledge, Nicholas Roy, and Chuchu Fan. How to train your neural control barrier function: Learning safety filters for complex input-constrained systems. arXiv preprint arXiv:2310.15478, 2023.   \n[10] Charles Dawson, Zengyi Qin, Sicun Gao, and Chuchu Fan. Safe nonlinear control using robust neural Lyapunov-barrier functions. In Conference on Robot Learning, pages 1724\u20131735. PMLR, 2022.   \n[11] Manan Tayal, Hongchao Zhang, Pushpak Jagtap, Andrew Clark, and Shishir Kolathaya. Learning a formally verified control barrier function in stochastic environment. arXiv preprint arXiv:2403.19332, 2024.   \n[12] Kehan Long, Cheng Qian, Jorge Cort\u00e9s, and Nikolay Atanasov. Learning barrier functions with memory for robust safe navigation. IEEE Robotics and Automation Letters, 6(3):4931\u20134938, 2021.   \n[13] Wei Xiao, Tsun-Hsuan Wang, Ramin Hasani, Makram Chahine, Alexander Amini, Xiao Li, and Daniela Rus. Barriernet: Differentiable control barrier functions for learning of safe robot control. IEEE Transactions on Robotics, 2023.   \n[14] Hengjun Zhao, Xia Zeng, Taolue Chen, Zhiming Liu, and Jim Woodcock. Learning safe neural network controllers with barrier certificates. Formal Aspects of Computing, 33:437\u2013455, 2021.   \n[15] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. Advances in Neural Information Processing Systems, 31:4939\u20134948, 2018.   \n[16] Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020.   \n[17] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. Advances in Neural Information Processing Systems, 32:9835\u20139846, 2019.   \n[18] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and Complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In International Conference on Learning Representations, 2021.   \n[19] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Beta-CROWN: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. Advances in Neural Information Processing Systems, 34, 2021.   \n[20] Huan Zhang, Shiqi Wang, Kaidi Xu, Yihan Wang, Suman Jana, Cho-Jui Hsieh, and Zico Kolter. A branch and bound framework for stronger adversarial attacks of ReLU networks. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pages 26591\u201326604, 2022.   \n[21] Alessandro Abate, Daniele Ahmed, Alec Edwards, Mirco Giacobbe, and Andrea Peruffo. Fossil: A software tool for the formal synthesis of Lyapunov functions and barrier certificates using neural networks. In Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control, pages 1\u201311, 2021.   \n[22] Alec Edwards, Andrea Peruffo, and Alessandro Abate. Fossil 2.0: Formal certificate synthesis for the verification and control of dynamical models. arXiv preprint arXiv:2311.09793, 2023.   \n[23] Hongchao Zhang, Junlin Wu, Yevgeniy Vorobeychik, and Andrew Clark. Exact verification of relu neural control barrier functions. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. Advances in Neural Information Processing Systems, 30, 2017.   \n[25] Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen, and Chuchu Fan. Learning safe multiagent control with decentralized neural barrier certificates. arXiv preprint arXiv:2101.05436, 2021.   \n[26] Zhizhen Qin, Tsui-Wei Weng, and Sicun Gao. Quantifying safety of learning-based selfdriving control using almost-barrier functions. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 12903\u201312910. IEEE, 2022.   \n[27] Simin Liu, Changliu Liu, and John Dolan. Safe control under input limits with neural control barrier functions. In Conference on Robot Learning, pages 1970\u20131980. PMLR, 2023.   \n[28] Stephen Prajna, Ali Jadbabaie, and George J Pappas. A framework for worst-case and stochastic safety verification using barrier certificates. IEEE Transactions on Automatic Control, 52(8):1415\u20131428, 2007.   \n[29] Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath, and Paulo Tabuada. Control barrier functions: Theory and applications. In 2019 18th European control conference (ECC), pages 3420\u20133431. IEEE, 2019.   \n[30] Weiye Zhao, Tairan He, Tianhao Wei, Simin Liu, and Changliu Liu. Safety index synthesis via sum-of-squares programming. In 2023 American Control Conference (ACC), pages 732\u2013737. IEEE, 2023.   \n[31] Michael Schneeberger, Florian D\u00f6rfler, and Silvia Mastellone. SOS construction of compatible control Lyapunov and barrier functions. arXiv preprint arXiv:2305.01222, 2023.   \n[32] Andrew Clark. Verification and synthesis of control barrier functions. In 2021 60th IEEE Conference on Decision and Control (CDC), pages 6105\u20136112. IEEE, 2021.   \n[33] Claudio Ferrari, Mark Niklas Muller, Nikola Jovanovic, and Martin Vechev. Complete verification via multi-neuron relaxation guided branch-and-bound. arXiv preprint arXiv:2205.00263, 2022.   \n[34] Patrick Henriksen and Alessio Lomuscio. Deepsplit: An efficient splitting method for neural network verification via indirect effect analysis. In IJCAI, pages 2549\u20132555, 2021.   \n[35] Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. General cutting planes for bound-propagation-based neural network verification. Advances in Neural Information Processing Systems, 35:1656\u20131670, 2022.   \n[36] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pages 97\u2013117. Springer, 2017.   \n[37] Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic\u00b4, et al. The Marabou framework for verification and analysis of deep neural networks. In Computer Aided Verification: 31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I 31, pages 443\u2013452. Springer, 2019.   \n[38] Hengjun Zhao, Xia Zeng, Taolue Chen, and Zhiming Liu. Synthesizing barrier certificates using neural networks. In Proceedings of the 23rd international conference on hybrid systems: Computation and control, pages 1\u201311, 2020.   \n[39] Alessandro Abate, Daniele Ahmed, Mirco Giacobbe, and Andrea Peruffo. Formal synthesis of lyapunov neural networks. IEEE Control Systems Letters, 5(3):773\u2013778, 2020.   \n[40] Andrea Peruffo, Daniele Ahmed, and Alessandro Abate. Automated and formal synthesis of neural barrier certificates for dynamical models. In Tools and Algorithms for the Construction and Analysis of Systems, pages 370\u2013388. Springer International Publishing, 2021.   \n[41] Mahathi Anand and Majid Zamani. Formally verified neural network control barrier certificates for unknown systems. IFAC-PapersOnLine, 56(2):2431\u20132436, 2023.   \n[42] Yixuan Wang, Chao Huang, Zhaoran Wang, Zhilu Wang, and Qi Zhu. Design-while-verify: correct-by-construction control learning with verification in the loop. In Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 925\u2013930, 2022.   \n[43] Xinyu Wang, Luzia Knoedler, Frederik Baymler Mathiesen, and Javier Alonso-Mora. Simultaneous synthesis and verification of neural control barrier functions through branch-and-bound verification-in-the-loop training. In 2024 European Control Conference (ECC), pages 571\u2013578. IEEE, 2024.   \n[44] Franco Blanchini and Stefano Miani. Set-Theoretic Methods in Control, volume 78. Springer, 2008.   \n[45] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter. Differentiable convex optimization layers. In Advances in Neural Information Processing Systems, 2019.   \n[46] Xia Zeng, Wang Lin, Zhengfeng Yang, Xin Chen, and Lilei Wang. Darboux-type barrier certificates for safety verification of nonlinear hybrid systems. In Proceedings of the 13th International Conference on Embedded Software, pages 1\u201310, 2016.   \n[47] Andrew J Barry, Anirudha Majumdar, and Russ Tedrake. Safety verification of reactive controllers for uav flight in cluttered environments using barrier certificates. In 2012 IEEE International Conference on Robotics and Automation, pages 484\u2013490. IEEE, 2012.   \n[48] Christopher Jewison and R Scott Erwin. A spacecraft benchmark problem for hybrid control and estimation. In 2016 IEEE 55th Conference on Decision and Control (CDC), pages 3300\u20133305. IEEE, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Supplement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Definition of $\\overline{{W}}_{i j}(\\mathbf{S})$ and $\\overline{{r}}_{i j}(\\mathbf{S})$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In what follows, we define vectors $\\overline{{W}}_{i j}(\\mathbf{S})\\ \\in\\ \\mathbb{R}^{n}$ and scalars $\\overline{{r}}_{i j}(\\mathbf{S})\\ \\in\\ \\mathbb{R}$ such that $z_{j}^{(i)}~=~$ $\\overline{{W}}_{i j}(\\mathbf{S})^{T}x+\\overline{{r}}_{i j}(\\mathbf{S})$ . The weight and bias of the input layer is defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\overline{{W}}_{1j}(\\mathbf{S})=\\left\\{\\begin{array}{l l}{W_{1j},}&{(1,j)\\in\\mathbf{S}}\\\\ {0,}&{\\mathrm{else}}\\end{array}\\right.\\quad\\overline{{r}}_{1j}(\\mathbf{S})=\\left\\{\\begin{array}{l l}{r_{1j},}&{(1,j)\\in\\mathbf{S}}\\\\ {0,}&{\\mathrm{else}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proceeding inductively, define $\\overline{{W}}_{i}(\\mathbf{S})\\,\\in\\,\\mathbb{R}^{M_{i}\\times n}$ to be a matrix with columns $\\overline{{W}}_{i j}(\\mathbf{S})$ for $j\\,=$ $1,\\dots,M_{i}$ and $\\overline{r}_{i}(\\mathbf{S})\\in\\mathbb{R}^{M_{i}}$ to be a vector with elements $\\overline{{r}}_{i j}(\\mathbf{S})$ . We then define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{W}}_{i j}(\\mathbf{S})=\\left\\{\\begin{array}{l l}{\\overline{{W}}_{i-1}(\\mathbf{S})W_{i j},}&{(i,j)\\in\\mathbf{S}}\\\\ {0,}&{\\mathrm{else}}\\end{array}\\right.\\quad\\overline{{r}}_{i j}(\\mathbf{S})=\\left\\{\\begin{array}{l l}{W_{i j}^{T}\\overline{{r}}_{i-1}(\\mathbf{S})+r_{i j},}&{(i,j)\\in\\mathbf{S}}\\\\ {0,}&{\\mathrm{else}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "At the last layer, let $\\overline{{W}}(\\mathbf{S})=W_{L}(\\mathbf{S})\\Omega$ and $\\boldsymbol{\\overline{{r}}}(\\mathbf{S})=\\Omega^{T}\\boldsymbol{r}_{L}(\\mathbf{S})+\\boldsymbol{\\psi}$ , so that $y=\\overline{{W}}(\\mathbf{S})^{T}x+\\overline{{r}}(\\mathbf{S})$ if $\\mathbf{S}=\\{(i,j):z_{j}^{(i)}\\geq0\\}$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Suppose that $x^{\\prime}\\in\\partial\\mathcal{D}\\setminus\\left(\\bigcup_{\\mathbf{S}\\in\\mathcal{S}}\\overline{{\\boldsymbol{\\chi}}}(\\mathbf{S})\\right)$ , and let $x$ denote the state on $\\partial\\mathcal{D}$ found by Line 5 of Algorithm 1. Since $\\partial\\mathcal{D}$ is connected, there exists a path $\\gamma$ with $\\gamma(0)=x$ and $\\gamma(1)=x^{\\prime}$ contained in $\\partial\\mathcal{D}$ . Let ${\\bf S}_{0},{\\bf S}_{1},\\ldots,{\\bf S}_{K}$ denote a sequence of activation sets with $\\mathbf{S}_{0}\\in\\mathbf{S}(x)$ , $\\mathbf{S}_{K}\\in\\mathbf{S}(\\boldsymbol{x}^{\\prime})$ , and $\\mathbf{S}_{0}$ equal to the set computed at Line 5 of Algorithm 1. Then there exists $i\\geq2$ such that $\\mathbf{S}_{i-1}\\in\\mathcal{S}$ and $\\mathbf{S}_{i}\\notin\\mathcal{S}$ , and there exists $t^{\\prime}$ such that $\\gamma(t^{\\prime})\\in\\overline{{\\mathcal{X}}}(\\mathbf{S}_{i-1})\\cap\\overline{{\\mathcal{X}}}(\\mathbf{S}_{i})\\cap\\{x:b(x)=0\\}$ . We then have that $\\mathbf{T}(\\mathbf{S}_{i-1},\\mathbf{S}_{i})$ is a subset of the set $\\mathbf{T}$ , and hence $\\mathbf{S}_{i}$ will be identified and added to $\\boldsymbol{S}$ at Line 5 of Algorithm 1. ", "page_idx": 13}, {"type": "text", "text": "Now, suppose that $\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r}\\in S$ is complete and $\\begin{array}{r}{\\left(\\bigcap_{i=1}^{r}\\overline{{\\mathcal{X}}}_{0}(\\mathbf{S}_{i})\\right)\\cap\\{x:b(x)=0\\}\\neq\\emptyset}\\end{array}$ . Let $\\mathbf{T}=\\mathbf{T}(\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r})$ . Then $\\mathbf{T}$ is a subset of the sets $\\mathbf{S}_{1},\\ldots,\\mathbf{S}_{r}\\in S$ . Since the intersection of the $\\overline{{\\mathcal{X}}}(\\mathbf{S})$ sets with $\\{x:b(x)=0\\}$ is nonempty, $\\{\\mathbf{S}_{1},\\dotsc,\\mathbf{S}_{r}\\}$ is added to $\\nu$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.3 Algorithm for Verification ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "SEEV identifies the initial activation set by conducting a BoundaryLP-based binary search as shown in Algorithm. 2. The algorithm presents a procedure to identify a hyperplane characterized by an initial activation set $\\mathbf{S}_{0}$ that may contain the boundary $\\partial\\mathcal{D}$ . It iterates over pairs of sample states from the unsafe training set $\\tau_{x\\setminus c}$ and the safe training set $\\mathcal{T}_{\\mathcal{Z}}$ . For each pair $(\\hat{x}_{\\mathcal{X}\\setminus\\mathcal{C}},\\hat{x}_{\\mathcal{Z}})$ , the algorithm initializes the left and right points of a search interval. It then performs a binary search by repeatedly computing the midpoint $x_{\\mathrm{mid}}$ and checking the feasibility of the boundary linear program BoundaryLP $\\ (x_{\\mathrm{mid}})$ . The algorithm terminates when BoundaryLP $\\ (x_{\\mathrm{mid}})$ is feasible, returning the activation set $\\mathrm{\\bfS}(x_{\\mathrm{mid}})$ as $\\mathbf{S}_{0}$ . ", "page_idx": 13}, {"type": "text", "text": "SEEV utilizes NBFS for enumerating all activation sets along the zero-level set in a breadth-first search manner, starting from an initial set $\\mathbf{S}_{0}$ . The algorithm initializes a queue $\\mathcal{Q}$ and a set $\\boldsymbol{S}$ with $\\mathbf{S}_{0}$ . While $\\mathcal{Q}$ is not empty, it dequeues an activation set S and checks if the set $\\mathbf{S}\\in\\mathcal{B}$ by solving the boundary linear program BoundaryLP(S). If so, S is identified and added to $\\boldsymbol{S}$ . The algorithm then explores its neighboring activation sets by filpping each neuron activation $(i,j)$ in S. For each filp, it solves the unstable neuron linear program $\\bar{\\mathrm{USLP}}(\\mathbf{S},(i,j))$ . If USLP is feasible, the new activation set $\\mathbf{S}^{\\prime}$ obtained by flipping $(i,j)$ is added to $\\mathcal{Q}$ for further search on its neighbors. This process continues until all relevant activation sets are explored, resulting in a set $\\boldsymbol{S}$ that contains activation sets potentially on the boundary. ", "page_idx": 13}, {"type": "text", "text": "SEEV enumerates all hinges $\\mathbf{V}\\in\\mathcal{V}$ with Algorithm 4. Algorithm 4 outlines a method to enumerate all feasible hinge hyperplanes formed by combinations of activation sets up to size $n$ . The algorithm takes as input a set of activation sets $\\boldsymbol{S}$ and a maximum combination size $n$ . It initializes an empty list to store feasible hinges. For each combination size $k$ from 2 to $n$ , the algorithm iterates over all activation sets in $\\boldsymbol{S}$ . For each activation set S, it generates candidate combinations $\\nu_{c}$ based on adjacency\u2014either the set of adjacent activation sets when $k=2$ , or the feasible combinations ", "page_idx": 13}, {"type": "table", "img_path": "nWMqQHzI3W/tmp/7f7e526c2f459315478ef78422c870354ccbe27c0abb659e6a568208655f15c6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Enumerate Activated Sets ", "page_idx": 14}, {"type": "table", "img_path": "nWMqQHzI3W/tmp/511ef4ab1a0323b8f68f76f418095e3768395d98de2f11b54b9d27d43c6be7af.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "from the previous iteration when $k>2$ . It then checks each candidate combination $\\ensuremath{\\mathcal{S}}_{c}$ by verifying adjacency and solving the hinge linear program HingeLP $(S_{c}\\cup\\{\\mathbf{S}\\})$ . If the HingeLP is feasible, the combination is added to the list of feasible hinges $\\mathcal{V}$ . This process continues until all combinations up to size $n$ have been examined, resulting in a comprehensive list of feasible hinge hyperplanes that are essential for understanding the intersections of activation regions. ", "page_idx": 14}, {"type": "text", "text": "A.4 Linear Programs for Enumeration ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given a state $\\boldsymbol{x}_{i}^{0}$ , we define the activation set is $\\mathbf{S}=\\tau_{S}(x_{i}^{0})$ . To determine if the activation set $\\mathbf{S}\\in\\mathcal{B}$ , we solve a linear program referred to as the boundary linear program. The program checks the existence of a state $x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S}(x_{i}^{0}))$ that satisfies $\\overline{{W}}(\\mathbf{S}(x_{i}^{0}))^{T}x+\\overline{{r}}(\\mathbf{S}(x_{i}^{0}))=0$ . The boundary linear program (BoundaryLP $\\big(\\mathbf{S}(x_{i}^{0})\\big)$ ) is defined as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{BoundaryLP}(\\mathbf{S}(x_{i}^{0}))=\\left\\{\\begin{array}{l l}{\\mathrm{find}}&{x}\\\\ {\\mathrm{s.t.}}&{\\overline{{W}}(\\mathbf{S}(x_{i}^{0}))^{T}x+\\overline{{r}}(\\mathbf{S}(x_{i}^{0}))=0}\\\\ &{x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S}(x_{i}^{0}))}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "NBFS conducts its search in a breadth-first manner. To determine if a neighboring activation set, resulting from a flip in its $(i,j)$ neuron, may contain the boundary, NBFS solves a linear program, referred to as the unstable neuron linear program of $\\mathrm{USLP}(\\mathbf{S},(i,j))$ . This linear program checks the existence of a state $x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S})\\cap\\{x:W_{i j}x+r_{i j}=0\\}$ that satisfies $\\overline{{W}}(\\mathbf{S})x+\\overline{{r}}(\\mathbf{S})=0$ . The ", "page_idx": 14}, {"type": "text", "text": "1: Input: S, n   \n2: Output: $\\nu$   \n3: procedure HINGEENUM $(S,n)$   \n4: Initialize hinge_list as an empty list   \n5: for $k$ from 2 to $n$ do $\\triangleright$ Outer loop to iterate over combination sizes   \n6: for $\\mathbf{S}\\in S$ do $\\triangleright$ Iterate over all activation set   \n7: if $k=2$ then   \n8: $\\mathcal{V}_{c}\\gets\\mathcal{N}_{\\mathbf{S}}(S)$   \n9: else   \n10: Vc \u2190V(k\u22121)   \n11: for each combination $\\ensuremath{\\mathcal{S}}_{c}\\in\\ensuremath{\\mathcal{V}}_{c}$ and $\\mathbf{S}\\notin\\mathcal{S}_{c}$ do \u25b7Iterate over combinations   \n12: if S and $\\ensuremath{\\mathcal{S}}_{c}$ are not adjacent then   \n13: Continue \u25b7skip non-adjacent combinations   \n14: if HingeLP $(S_{c}\\cup\\{\\mathbf{S}\\})$ then   \n15: Add $\\mathbf{V}\\gets S_{c}\\cup\\{\\mathbf{S}\\}$ to $\\mathscr{V}^{(k)}$ $\\triangleright$ Add to set of corresponding $k$   \n16: Add $\\mathcal{V}^{(k)}$ to $\\nu$   \n17: return hinge_list ", "page_idx": 15}, {"type": "text", "text": "unstable neuron linear program is defined as follows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{USLP}(\\mathbf{S},(i,j))=\\left\\{\\begin{array}{l l}{\\mathrm{find}}&{x}\\\\ {\\mathrm{s.t.}}&{\\overline{{W}}(\\mathbf{S})^{T}x+\\overline{{r}}(\\mathbf{S})=0}\\\\ &{W_{i j}(\\mathbf{S})^{T}x+r_{i j}(\\mathbf{S})=0}\\\\ &{x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we enumerate all hinges by solving the hinge linger program HingeLP $(S_{i})$ , defined as follows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{HingeLP}(\\mathcal{N}_{\\mathbf{S}}^{(d)}(S_{i}))=\\left\\{\\!\\!\\begin{array}{l l}{\\mathrm{find}}&{x}\\\\ {\\mathrm{s.t.}}&{\\overline{{W}}(\\mathbf{S})^{T}x+\\overline{{r}}(\\mathbf{S})=0,\\quad\\forall\\mathbf{S}\\in\\mathcal{N}_{\\mathbf{S}}(S_{i})}\\\\ &{x\\in\\mathcal{X}(\\mathbf{S}),\\quad\\forall\\mathbf{S}\\in\\mathcal{N}_{\\mathbf{S}}(S_{i})}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.5 Nonlinear Programs for Verification ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The correctness condition ((7)) can be verified for boundary hyperplane $\\overline{{\\mathcal{X}}}(\\mathbf{S})$ by solving the nonlinear program ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}_{x}}&{{}h(x)}\\\\ {\\mathrm{s.t.}\\quad}&{{}\\overline{{W}}(\\mathbf{S})^{T}x+\\overline{{r}}(\\mathbf{S})=0,x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If we found some state $x$ such that $h(x)<0$ and $\\overline{{W}}(\\mathbf{S})^{T}x+\\overline{{r}}(\\mathbf{S})=0$ , it indicates that the state lies on the boundary of the set $\\mathcal{D}$ but not inside the set $\\mathcal{C}$ , i.e., $x\\in\\partial\\mathcal{D}\\notin\\mathcal{C}$ . This implies a violation of the condition $\\mathcal{D}\\subseteq\\mathcal{C}$ . ", "page_idx": 15}, {"type": "text", "text": "Hyperplane counterexamples can be identified by solving the optimization problem ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}_{x}}&{\\operatorname*{max}\\left\\{\\overline{{W}}(\\mathbf{S})^{T}(f(x)+g(x)u):u\\in\\mathcal{U}\\right\\}}\\\\ {\\mathrm{s.t.~}}&{\\overline{{W}}(\\mathbf{S})^{T}x+\\overline{{r}}(\\mathbf{S})=0,x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider a case $\\mathcal{U}\\,=\\,\\{D\\omega\\,:\\,||\\omega||_{\\infty}\\,\\le\\,1\\}$ . The bounded input set $\\boldsymbol{\\mathcal{U}}$ allows us to replace the maximization over $u$ with $L{-1}$ norm term $\\|\\overline{{W}}(\\mathbf{S})^{T}g(x)D\\|_{1}$ , which simplifies the computational complexity. In this case, the problem reduces to the nonlinear program ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x}{\\operatorname*{min}}}&{\\overline{{W}}(\\mathbf{S})^{T}f(x)+\\|\\overline{{W}}(\\mathbf{S})^{T}g(x)D\\|_{1}}\\\\ {\\mathrm{s.t.}}&{\\overline{{W}}(\\mathbf{S})^{T}x+\\overline{{r}}(\\mathbf{S})=0,x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\mathcal{U}=\\mathbb{R}^{m}$ , then by [23, Corollary 1], the problem can be reduced to the nonlinear program ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}_{x}}&{\\overline{{W}}(\\mathbf{S})^{T}f(x)}\\\\ {\\mathrm{s.t.}\\quad}&{\\overline{{W}}(\\mathbf{S})^{T}g(x)=0,\\overline{{W}}(\\mathbf{S})x+\\overline{{r}}(\\mathbf{S})=0,x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The hinge $\\mathbf{V}=\\{\\mathbf{S}_{1},\\hdots,\\mathbf{S}_{r}\\}$ can be certified by solving the nonlinear optimization problem ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}_{x}}&{\\operatorname*{max}\\left\\{\\overline{W}(\\mathbf{S}_{l})^{T}(f(x)+g(x)u):l=1,\\ldots,r,u\\mathrm{~sat~}\\right.}\\\\ {\\mathrm{s.t.~}}&{\\left.x\\in\\overline{{\\mathcal{X}}}(\\mathbf{S}_{1})\\cap\\ldots\\cap\\overline{\\mathcal{X}}(\\mathbf{S}_{r}),\\overline{W}(\\mathbf{S}_{1})^{T}x+\\overline{r}(\\mathbf{S}_{1})=0\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.6 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. By Lemma 1, the initial boundary activation set $\\mathbf{S}_{0}$ is ensured to be identified by the verification of SEEV in Line 4 Algorithm. 1. Given $\\mathbf{S}_{0}$ and the enumeration in Line 5 and 6 Algorithm. 1 being complete, the completeness of $\\boldsymbol{S}$ and $\\mathcal{V}$ is guaranteed by Proposition 2. By dReal solving the equivalent NLPs, the conditions of Proposition 1, are satisfied. Therefore, $b_{\\theta}(x)$ is a valid NCBF ", "page_idx": 16}, {"type": "text", "text": "B Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Experiment Settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Experiment Settings of Darboux: The dynamic model of Darboux is given as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\end{array}\\right]=\\left[\\begin{array}{c}{x_{2}+2x_{1}x_{2}}\\\\ {-x_{1}+2x_{1}^{2}-x_{2}^{2}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We define state space, initial region, and safe region as $\\mathcal{X}\\;:\\;\\left\\{\\mathbf{x}\\in\\mathbb{R}^{2}:x\\in[-2,2]\\times[-2,2]\\right\\}$ , $\\mathcal{T}:\\left\\{\\mathbf{x}\\in\\mathbb{R}^{2}:0\\leq x_{1}\\leq1,1\\leq x_{2}\\leq2\\right\\}$ and $\\mathcal{C}:\\left\\{\\mathbf{x}\\in\\mathbb{R}^{2}:x_{1}+\\dot{x}_{2}^{2}\\geq0\\right\\}$ respectively. ", "page_idx": 16}, {"type": "text", "text": "Experiment Settings of the Obstacle Avoidance: The dynamic model of obstacle avoidance is given as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c}{\\dot{x}_{1}}\\\\ {\\dot{x}_{2}}\\\\ {\\dot{\\psi}}\\end{array}\\right]=\\left[\\begin{array}{c}{v\\sin\\psi}\\\\ {v\\cos\\psi}\\\\ {0}\\end{array}\\right]+\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {u}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We define the state space, initial region and safe region as $\\mathcal{X},\\mathcal{T}$ and $\\mathcal{C}$ , respectively as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{X}:\\left\\{\\mathbf{x}\\in\\mathbb{R}^{3}:x_{1},x_{2},\\psi\\in[-2,2]\\times[-2,2]\\times[-2,2]\\right\\}}\\\\ &{\\mathcal{Z}:\\left\\{\\mathbf{x}\\in\\mathbb{R}^{3}:-0.1\\le x_{1}\\le0.1,-2\\le x_{2}\\le-1.8,\\ -\\pi/6<\\psi<\\pi/6\\right\\}}\\\\ &{\\mathcal{C}:\\left\\{\\mathbf{x}\\in\\mathbb{R}^{3}:x_{1}^{2}+x_{2}^{2}\\ge0.04\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Experiment Settings of the Spacecraft Rendezvous: The state of the chaser is expressed relative to the target using linearized Clohessy\u2013Wiltshire\u2013Hill equations, with state $\\boldsymbol{x}=[p_{x},p_{y}^{\\dot{\\ }},p_{z},v_{x},v_{y},v_{z}]^{T}$ , control input $\\boldsymbol{u}=[u_{x},u_{y},u_{z}]^{T}$ and dynamics defined as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c}{\\dot{p}_{x}}\\\\ {\\dot{p}_{y}}\\\\ {\\dot{p}_{z}}\\\\ {\\dot{v}_{x}}\\\\ {\\dot{v}_{y}}\\\\ {\\dot{v}_{z}}\\end{array}\\right]=\\left[\\begin{array}{c c c c c c}{1}&{0}&{0}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}&{0}&{0}\\\\ {3n^{2}}&{0}&{0}&{0}&{2n}&{0}\\\\ {0}&{0}&{0}&{-2n}&{0}&{0}\\\\ {0}&{0}&{-n^{2}}&{0}&{0}&{0}\\end{array}\\right]\\left[\\begin{array}{c}{p_{x}}\\\\ {p_{y}}\\\\ {p_{z}}\\\\ {v_{x}}\\\\ {v_{y}}\\\\ {v_{z}}\\end{array}\\right]+\\left[\\begin{array}{c c}{0}&{0}&{0}\\\\ {0}&{0}&{0}\\\\ {1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]\\left[\\begin{array}{c}{u_{x}}\\\\ {u_{y}}\\\\ {u_{z}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We define the state space and safe region as $\\mathcal{X}$ , initial safe region $\\chi_{\\mathrm{{Z}}}$ and $\\mathcal{C}$ , respectively as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{X}}:\\left\\{\\mathbf{x}\\in\\mathbb{R}^{3}:p,v,\\in[-5,5]\\times[-1,1]\\right\\}}\\\\ &{{\\mathcal{Z}}:\\left\\{r\\geq0.75,\\,\\mathrm{where}\\,r=\\sqrt{p_{x}^{2}+p_{y}^{2}+p_{z}^{2}}\\right\\}}\\\\ &{{\\mathcal{C}}:\\left\\{r\\geq0.25,\\,\\mathrm{where}\\,r=\\sqrt{p_{x}^{2}+p_{y}^{2}+p_{z}^{2}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "hi- $\\mathbf{ord}_{8}$ : The dynamic model of hi-ord $^{\\cdot8}$ is given as follows. ", "page_idx": 16}, {"type": "text", "text": "$x^{(8)}+20x^{(7)}+170x^{(6)}+800x^{(5)}+2273x^{(4)}+3980x^{(3)}+4180x^{(2)}+2400x^{(1)}+576=0$ (27) where we denote the $i$ -th derivative of variable $x$ by $\\boldsymbol{x}^{(i)}$ . We define the state space $\\mathcal{X}$ , initial region $\\chi_{\\mathrm{{Z}}}$ and safe region $\\mathcal{C}$ , respectively as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{X}:\\left\\{x_{1}^{2}+\\ldots+x_{8}^{2}\\leq4\\right\\}}\\\\ &{\\mathcal{Z}:\\left\\{(x_{1}-1)^{2}+\\ldots+(x_{8}-1)^{2}\\leq1\\right\\}}\\\\ &{\\mathcal{C}:\\left\\{(x_{1}+2)^{2}+\\ldots+(x_{8}+2)^{2}\\geq3\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Synthesis Framework Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The experimental results presented in Table 3 demonstrate the effectiveness of Counter Example (CE) guided training on Darboux and hi-ord8 system. In this method, after each training epoch, we calculate the Control Barrier Function (CBF) outputs on representative samples. If the CBF correctly categorizes the samples into safe and unsafe regions, the certification procedure is initiated. If the CBF fails certification, the counter example is added to the training dataset for retraining. Otherwise, training is stopped early. ", "page_idx": 17}, {"type": "text", "text": "We capped the maximum training epochs at 50 and conducted three rounds of training for each network structure and system using different random seeds. The results indicate that without CE, the training process could basrely generate a CBF that passes certification. In contrast, with CE enabled, there was a success rate of at least 1/3 for most network structure, with verifiable policies generated in as few as 10 epochs. This highlights the improvement in training efficiency and reliability with the incorporation of CEs. ", "page_idx": 17}, {"type": "table", "img_path": "nWMqQHzI3W/tmp/b87a16263d79e16830b5023f0ab10afb2348e29fb01a66b2541c67630a3a94d0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 4 shows values the following hyperparameters used during CBF synthesis: ", "page_idx": 17}, {"type": "text", "text": "\u2022 $N_{\\mathrm{data}}$ : number of samples to train CBF on.   \n\u2022 $a_{1}$ : weight penalizing incorrect classification of safe samples in Equation 11.   \n\u2022 $a_{2}$ : weight penalizing incorrect classification of unsafe samples Equation 11.   \n\u2022 $\\lambda_{f}$ : weight penalizing violation of Lie derivative condition of CBF in Equation 9.   \n\u2022 $\\lambda_{c}$ : weight penalizing correct loss for in Equation 9.   \n\u2022 $n_{\\mathrm{cluster}}$ : number of clusters in $\\mathcal{L}_{\\mathcal{B}}$ regularization.   \n\u2022 $k_{\\sigma}$ : value of $k$ used in generalized sigmoid function to perform differentiable activation pattern approximation.   \n\u2022 \u03f5boundary: the threshold for range-based approximation of CBF boundary. ", "page_idx": 17}, {"type": "table", "img_path": "nWMqQHzI3W/tmp/1749612db3ac51fd2481e35875ca5f14544e9ff7f5d57d674189b2c30a711148.jpg", "table_caption": [], "table_footnote": ["Table 4: Hyperparameters of CBF synthesis "], "page_idx": 17}, {"type": "text", "text": "B.4 Sensitivity Analysis of Hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Next, we performed a sensitivity analysis of the hyperparameters. We chose the case study of Spacecraft Rendezvous with the number of layers $L=4$ and the number of hidden units per layer ", "page_idx": 17}, {"type": "table", "img_path": "nWMqQHzI3W/tmp/9d67b8f28f33be25be7c006a2b2cfe9763207af77a4624ae4215928aefa46fa1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 5: Ablation study for training hyperparameters. In each table, the bold lines indicate the baseline setting. SR: the success rate among runs with three random seeds. ME: the average first training epoch when a valid NCBF is obtained. $\\mathbf{N}$ : the average number of boundary hyperplanes. ", "page_idx": 18}, {"type": "text", "text": "$N\\,=\\,8$ . We studied the sensitivity of the training performance to the hyperparameters $\\lambda_{B},\\,\\lambda_{f}$ , and $\\lambda_{c}$ in Equation 8, corresponding to the weightings for regularizing the number of boundary hyperplanes, NCBF value violation, and NCBF Lie derivative violation, respectively. We also studied the sensitivity to the hyperparameter $k$ employed in the modified sigmoid function $\\sigma_{k}(z)=$ 1+exp1(\u2212k\u00b7z) to approximate the regularization pattern. We compared against the settings used in the original paper: $\\lambda_{B}=10$ , $\\lambda_{f}=2$ , $\\lambda_{c}=100$ , and $k=4$ . For each hyperparameter, we chose four values to perform the ablation study: $\\lambda_{B}\\in\\{0,1,10,50\\}$ , $\\lambda_{f}\\in\\{1,2,\\bar{4},\\bar{8}\\}$ , $\\lambda_{c}\\in\\{1,10,100,200\\}$ , and $k\\in\\{1,2,4,8\\}$ . For each setting, we performed three runs with different random seeds. We measured the results by Success Rate (SR), Min Epoch (ME), and $\\mathbf{N}$ , as described in the caption of Table 5. ", "page_idx": 18}, {"type": "text", "text": "$\\lambda_{c}$ regularizes the shape of the NCBF by penalizing incorrectly categorized samples. Table 5a indicates that when $\\lambda_{c}$ is too small, the training procedure fails to train an NCBF that correctly separates the safe and unsafe regions, resulting in failure of certification. Meanwhile, a larger weight delivers similarly good performance. ", "page_idx": 18}, {"type": "text", "text": "$\\lambda_{f}$ penalizes violations of Lie derivative conditions. Table 5b shows that the result is not sensitive to this hyperparameter, as this term quickly goes down to 0 when the Lie derivative condition is satisfied. We note that over-penalizing this condition should be avoided since the NCBF would otherwise learn an unrecoverable incorrect shape, as demonstrated by the failure case when $\\lambda_{f}=8$ . ", "page_idx": 18}, {"type": "text", "text": "$\\lambda_{B}$ has been studied in the original paper, with detailed analysis in Section 5.2. The boundary regularization term reduces the number of boundary hyperplanes and benefits convergence. ", "page_idx": 18}, {"type": "text", "text": "Table 5c shows the importance of the term $k$ used in the modified sigmoid function. Since this term appears in the exponential part of the sigmoid function, when it is too large, it leads to gradient explosions during backpropagation, which crashes the training process. Conversely, a reasonably larger $k$ better approximates the activation pattern, leading to a reduced number of boundary hyperplanes. ", "page_idx": 18}, {"type": "text", "text": "In summary, balancing the hyperparameters is relatively straightforward, as the training performance remains robust across a wide range of hyperparameter values. When training failures do occur, we can systematically identify the cause from observation. This enables proper guidance in choosing and adjusting the appropriate hyperparameters. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 19}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 19}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 19}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 19}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 19}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction clearly states the paper\u2019s contributions and scope. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We point out the limitation on dynamical models in Section 2.1 and limitations on the type of activation functions throughout the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We present assumptions in Section 2, and theoretical contributions in Section 4 with proof in the appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We specified random seeds for reproducibility. The code is uploaded with all the commands containing random seeds. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The code is submitted with the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 21}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide the detailed experiment settings with hyperparameters in the appendix. We present how to generate training data in Section 3. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All verification experiments are conducted in a deterministic manner. We also present the success rate of training with random seeds in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We present the details of the machine we experiment on in Section 5. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper focuses on the safe control problem of dynamical systems. Therefore it would not harm or raise technology to harm the society. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: the paper poses no such risks. The experiments we conduct only generate simulation-based data of dynamic systems with no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cited all references in the both the paper and the code Readme file. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We upload our code with the submission. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "the paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]