[{"figure_path": "NN9U0lEcAn/figures/figures_1_1.jpg", "caption": "Figure 1: Task-specific models vs. ActFusion (ours). (a) Conventional task-specific models for TAS and LTA. (b) Our unified model ActFusion to address both tasks. (c) Performance comparison across tasks. Tasks-specific models such as DiffAct [43] for TAS and FUTR [25] for LTA exhibits poor performance on cross-task evaluations. ActFusion outperforms task-specific models on both TAS and LTA, including TempAgg [52], which trains separate models for each task. Note that the performance of ActFusion is evaluation result of a single model through a single training process.", "description": "This figure compares task-specific models (DiffAct for TAS and FUTR for LTA) with a unified model (ActFusion).  It shows that task-specific models perform poorly when applied to the other task. In contrast, ActFusion, which handles both action segmentation and anticipation jointly, significantly outperforms the task-specific models. The performance difference highlights the effectiveness of ActFusion's unified architecture and training process.", "section": "1 Introduction"}, {"figure_path": "NN9U0lEcAn/figures/figures_1_2.jpg", "caption": "Figure 1: Task-specific models vs. ActFusion (ours). (a) Conventional task-specific models for TAS and LTA. (b) Our unified model ActFusion to address both tasks. (c) Performance comparison across tasks. Tasks-specific models such as DiffAct [43] for TAS and FUTR [25] for LTA exhibits poor performance on cross-task evaluations. ActFusion outperforms task-specific models on both TAS and LTA, including TempAgg [52], which trains separate models for each task. Note that the performance of ActFusion is evaluation result of a single model through a single training process. The reported performance represents the average of each task in the original paper or evaluated with the official checkpoint (See Sec. F for details).", "description": "This figure compares the performance of task-specific models for Temporal Action Segmentation (TAS) and Long-term Action Anticipation (LTA) with the proposed unified model, ActFusion.  (a) shows traditional approaches using separate models for TAS and LTA. (b) illustrates ActFusion, which combines both tasks into a single model.  (c) presents a bar chart comparing the performance of ActFusion and other models on both TAS and LTA, demonstrating ActFusion's superior performance and efficiency.", "section": "1 Introduction"}, {"figure_path": "NN9U0lEcAn/figures/figures_3_1.jpg", "caption": "Figure 2: Overall pipeline of ActFusion. During training, we randomly select one of five masking strategies and apply it to input video frames F, replacing masked regions with learnable tokens to obtain masked features F\u02b9. These features are processed by the encoder g to produce visual embeddings E, which condition the decoder h to denoise action labels from As to A\u00ba at time-step s. For inference, we use different masking strategies depending on the task: no masking for TAS and anticipative masking for LTA. The decoder then iteratively denoises action labels following \u00c2S \u2192 \u00c2S-A \u2192 ... \u2192 \u00c2\u00ba using the DDIM update rule [54].", "description": "The figure illustrates the overall pipeline of the ActFusion model.  During training, various masking strategies are randomly applied to the input video frames, replacing masked portions with learnable tokens. These masked features are then processed by an encoder to generate visual embeddings, which in turn condition a decoder. The decoder iteratively denoises action labels. During inference, different masking strategies are used for different tasks (no masking for action segmentation, anticipative masking for action anticipation). The decoder iteratively denoises the action labels to generate predictions.", "section": "4 Proposed approach"}, {"figure_path": "NN9U0lEcAn/figures/figures_8_1.jpg", "caption": "Figure 1: Task-specific models vs. ActFusion (ours). (a) Conventional task-specific models for TAS and LTA. (b) Our unified model ActFusion to address both tasks. (c) Performance comparison across tasks. Tasks-specific models such as DiffAct [43] for TAS and FUTR [25] for LTA exhibits poor performance on cross-task evaluations. ActFusion outperforms task-specific models on both TAS and LTA, including TempAgg [52], which trains separate models for each task. Note that the performance of ActFusion is evaluation result of a single model through a single training process. The reported performance represents the average of each task in the original paper or evaluated with the official checkpoint (See Sec. F for details).", "description": "This figure compares the performance of task-specific models (DiffAct for TAS and FUTR for LTA) against the unified ActFusion model. It highlights ActFusion's superior performance on both tasks, demonstrating the benefits of a joint learning approach.", "section": "1 Introduction"}, {"figure_path": "NN9U0lEcAn/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative results", "description": "This figure shows the qualitative results of the ActFusion model on the tasks of temporal action segmentation (TAS) and long-term action anticipation (LTA).  It displays video frames from three different datasets (Breakfast, 50 Salads, and GTEA). For each video sequence, the ground truth action labels (GT), the model's predictions for action segmentation (TAS), and the model's predictions for action anticipation (LTA) are displayed alongside.  The dashed line indicates the boundary between the observed frames (used for segmentation) and the unobserved frames (used for anticipation). The figure visually demonstrates the model's ability to accurately classify both currently occurring and future actions. ", "section": "5 Qualitative results"}, {"figure_path": "NN9U0lEcAn/figures/figures_16_1.jpg", "caption": "Figure 2: Overall pipeline of ActFusion. During training, we randomly select one of five masking strategies and apply it to input video frames F, replacing masked regions with learnable tokens to obtain masked features F'. These features are processed by the encoder g to produce visual embeddings E, which condition the decoder h to denoise action labels from As to A\u02c60 at time-step s. For inference, we use different masking strategies depending on the task: no masking for TAS and anticipative masking for LTA. The decoder then iteratively denoises action labels following A\u02c6S \u2192 A\u02c6S\u2212\u2206 \u2192 ... \u2192 A\u02c60 using the DDIM update rule [54].", "description": "This figure illustrates the overall pipeline of the ActFusion model.  It shows how the model uses different masking strategies during training and inference.  During training, random masking helps the model to learn from incomplete data, while during inference, different masking is applied depending on whether the task is temporal action segmentation (TAS) or long-term action anticipation (LTA). The encoder processes the masked features, and the decoder iteratively refines the action labels through a denoising process. The diagram clearly shows the flow of information from input video frames to final action predictions.", "section": "4 Proposed approach"}, {"figure_path": "NN9U0lEcAn/figures/figures_16_2.jpg", "caption": "Figure 2: Overall pipeline of ActFusion. During training, we randomly select one of five masking strategies and apply it to input video frames F, replacing masked regions with learnable tokens to obtain masked features F\u02b9. These features are processed by the encoder g to produce visual embeddings E, which condition the decoder h to denoise action labels from As to A\u00ba at time-step s. For inference, we use different masking strategies depending on the task: no masking for TAS and anticipative masking for LTA. The decoder then iteratively denoises action labels following \u00c2S \u2192 \u00c2S-A \u2192 ... \u2192 \u00c2\u00ba using the DDIM update rule [54].", "description": "This figure illustrates the overall pipeline of the ActFusion model.  It shows how the model uses different masking strategies during training and inference for action segmentation and anticipation.  During training, five masking strategies are randomly selected and applied, replacing masked parts of the video with learnable tokens.  The encoder processes these masked features and generates embeddings that condition the decoder. The decoder iteratively denoises action labels to reconstruct the ground truth. During inference, the masking strategy varies depending on whether action segmentation or action anticipation is the task.", "section": "4 Proposed approach"}, {"figure_path": "NN9U0lEcAn/figures/figures_21_1.jpg", "caption": "Figure 5: Qualitative results", "description": "This figure presents qualitative results from ActFusion, evaluated on both TAS and LTA using a single model. The figure includes video frames, ground-truth action sequences, and predicted results for TAS and LTA. For LTA, only the visible parts (observed frames) are used as input. The results show that ActFusion effectively handles both visible and future segments, accurately classifying current actions and anticipating future ones.", "section": "5.3 Qualitative results"}, {"figure_path": "NN9U0lEcAn/figures/figures_21_2.jpg", "caption": "Figure 2: Overall pipeline of ActFusion. During training, we randomly select one of five masking strategies and apply it to input video frames F, replacing masked regions with learnable tokens to obtain masked features F\u02b9. These features are processed by the encoder g to produce visual embeddings E, which condition the decoder h to denoise action labels from As to A\u00ba at time-step s. For inference, we use different masking strategies depending on the task: no masking for TAS and anticipative masking for LTA. The decoder then iteratively denoises action labels following \u00c2S \u2192 \u00c2S-A \u2192 ... \u2192 \u00c2\u00ba using the DDIM update rule [54].", "description": "This figure illustrates the overall pipeline of the ActFusion model. It shows how the model uses different masking strategies during training and inference to handle both action segmentation and anticipation tasks.  The training process involves randomly selecting one of five masking strategies and replacing masked regions with learnable tokens. The inference process uses different masking strategies depending on the task, either with no masking for action segmentation or anticipative masking for action anticipation. The encoder and decoder processes are also shown, with the decoder iteratively denoising action labels to generate predictions.", "section": "4 Proposed approach"}, {"figure_path": "NN9U0lEcAn/figures/figures_22_1.jpg", "caption": "Figure 5: Qualitative results", "description": "This figure showcases qualitative results from ActFusion, evaluated on both TAS and LTA using a single model.  It displays video frames alongside ground truth action sequences and the model's predictions for both TAS (action segmentation) and LTA (action anticipation).  The LTA predictions are based solely on the visible (observed) portion of the video frames. The visualization highlights the model's ability to accurately classify actions in the observed frames (TAS) and anticipate future actions in unseen portions (LTA).", "section": "5 Qualitative results"}, {"figure_path": "NN9U0lEcAn/figures/figures_22_2.jpg", "caption": "Figure 5: Qualitative results", "description": "This figure presents qualitative results from ActFusion, evaluated on both TAS and LTA using a single model. The figure includes video frames, ground-truth action sequences, and predicted results for TAS and LTA. For LTA, only the visible parts (observed frames) are used as input. The results show that ActFusion effectively handles both visible and future segments, accurately classifying current actions and anticipating future ones. Additional results are provided in Figures S3 and S4.", "section": "Qualitative results"}]