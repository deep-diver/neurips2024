[{"figure_path": "NG16csOmcA/tables/tables_4_1.jpg", "caption": "Table 1: The main results for image generation on ImageNet [61] (Class-to-Image) and JourneyDB [53] (Text-to-Image) with 256 \u00d7 256 image resolution. We highlight the best value in blue, and the second-best value in green. The Scalability column indicates the scaling capability of the parameter scale and architecture.", "description": "This table presents the main results of image generation experiments on two datasets, ImageNet and JourneyDB, using various methods and architectures.  The FID, sFID, and IS scores are used to evaluate the quality of generated images.  The \"Scalability\" column indicates whether the method and architecture used in each experiment support scalable training.", "section": "3 Experiments"}, {"figure_path": "NG16csOmcA/tables/tables_6_1.jpg", "caption": "Table 2: The main results for video generation on the SkyTimelapse [62], Taichi-HD [63] and UCF-101 [64] with 256 \u00d7 256 resolution of each frame. We highlight the best value in blue, and the second-best value in green.", "description": "This table presents a comparison of different video generation models' performance on three benchmark datasets: SkyTimelapse, Taichi-HD, and UCF-101.  The metrics used for evaluation are FID (Fr\u00e9chet Inception Distance), IS (Inception Score), and FVD (Fr\u00e9chet Video Distance).  The table also indicates whether each model is scalable and shows the results for both None-to-Video (unconditional generation) and Class-to-Video (conditional generation) tasks. The best and second-best results for each metric and task are highlighted.", "section": "3.3 Experiments on Video Generation with Deep Scalable Temporal Learning"}]