[{"type": "text", "text": "Neural Residual Diffusion Models for Deep Scalable Vision Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhiyuan $\\mathbf{M}\\mathbf{a}^{1}$ , Liangliang Zhao1,2, Biqing $\\mathbf{Q}\\mathbf{i}^{3}$ , Bowen Zhou1,3\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Department of Electronic Engineering, Tsinghua University, Beijing, China 2Frontis.AI, Beijing, China 3Shanghai AI Laboratory, Shanghai, China {mzyth,zhoubowen}@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The most advanced diffusion models have recently adopted increasingly deep stacked networks (e.g., U-Net or Transformer) to promote the generative emergence capabilities of vision generation models similar to large language models (LLMs). However, progressively deeper stacked networks will intuitively cause numerical propagation errors and reduce noisy prediction capabilities on generative data, which hinders massively deep scalable training of vision generation models. In this paper, we first uncover the nature that neural networks being able to effectively perform generative denoising lies in the fact that the intrinsic residual unit has consistent dynamic property with the input signal\u2019s reverse diffusion process, thus supporting excellent generative abilities. Afterwards, we stand on the shoulders of two common types of deep stacked networks to propose a unified and massively scalable Neural Residual Diffusion Models framework (Neural-RDM for short), which is a simple yet meaningful change to the common architecture of deep generative networks by introducing a series of learnable gated residual parameters that conform to the generative dynamics. Experimental results on various generative tasks show that the proposed neural residual models obtain state-of-the-art scores on image\u2019s and video\u2019s generative benchmarks. Rigorous theoretical proofs and extensive experiments also demonstrate the advantages of this simple gated residual mechanism consistent with dynamic modeling in improving the fidelity and consistency of generated content and supporting large-scale scalable training. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models (DMs) [1, 2, 3, 4] have emerged as a class of powerful generative models and have recently exhibited high quality samples in a wide variety of vision generation tasks such as image synthesis [5, 6, 7, 8, 9, 10, 11], video generation [12, 13, 14, 15, 16, 17, 18, 19] and 3D rendering and generation [20, 21, 22, 23, 24]. Relying on the advantage of iterative denoising and high-fidelity generation, DMs have gained enormous attention from the community and have been significantly improved in terms of sampling procedure [25, 26, 27, 28], conditional guidance [29, 30, 31, 32], likelihood maximization [33, 34, 35, 36] and generalization ability [37, 38, 39, 10] in previous efforts. ", "page_idx": 0}, {"type": "text", "text": "However, current diffusion models still face a scalability dilemma, which will play an important role in determining whether could support scalable deep generative training on large-scale vision data and give rise to emergent abilities [40] similar to large language models (LLMs) [41, 42]. Representatively, the recent emergence of Sora [43] has pushed the intelligent emergence capabilities of generative models to a climax by treating video models as world simulators. While unfortunately, ", "page_idx": 0}, {"type": "text", "text": "Sora is still a closed-source system and the mechanism for the intelligence emergence is still not very clear, but the scalable architecture must be one of the most critical technologies, according to the latest investigation [44] on its reverse engineering. ", "page_idx": 1}, {"type": "text", "text": "To alleviate this dilemma and spark further research in the open source community beyond the realms of well established U-Net and Transformers, and enable DMs to be trained in new scalable deep generative architectures, we propose a unified and massively scalable Residual-style Diffusion Models framework (Neural-RDM for short) with a learnable gating residual mechanism, as shown in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "The proposed Neural-RDM framework aims to unify the current mainstream residual-style generative architecture (e.g., $U$ -Net or Transformer) and guide the emergence of brand new scalable network architectures with emergent capabilities. To achieve this goal, we first introduce a continuous-time neural ordinary differential equation (ODE) to prove that the generative denoising ability of the diffusion models is closely related to the residual-style network structure, which almost reveals the essential reason why any network rich in residual structure can denoise well: Residual-style neural units implicitly build an ordinary differential equation that can well fit the reverse denoising process through ", "page_idx": 1}, {"type": "image", "img_path": "NG16csOmcA/tmp/a3315812d356146146093627ddcf237e0d44f9e5b7f6803c8ee6352ea60e654f.jpg", "img_caption": ["(a) MRS-Unit (b)\u00a0 Neural Residual Denoising Models ${\\mathcal{F}}_{\\theta}$ ", "Figure 1: Neural Residual-style Diffusion Models framework with massively scalable gating-based minimum residual stacking unit (mrs-unit). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "ever-deepening neural units, thus supporting excellent generative abilities. Further, we also show that the gating-residual mechanism plays an important role in adaptively correcting the errors of network propagation and approximating the mean and variance of data, which avoids the adverse factors of network deepening. On this basis, we further present the theoretical advantages of the Neural-RDM in terms of stability and score prediction sensitivity when stacking this residual units to a very long depth by introducing another residual-sensitivity ODE. From a dynamic perspective, it reveals that deep stacked networks have the challenge of gradually losing sensitivity as the network progressively deepens, and our proposed gating weights have advantages in reverse suppression and error control. ", "page_idx": 1}, {"type": "text", "text": "Our proposed framework has several theoretical and practical contributions: ", "page_idx": 1}, {"type": "text", "text": "Unified residual denoising framework: We unify the residual-style diffusion networks (e.g., $U$ -Net and Transformer) by introducing a simple gating-residual mechanism and reveal the significance of the residual unit for effective denoising and generation from a brand new dynamics perspective. ", "page_idx": 1}, {"type": "text", "text": "Theoretically deep scalability: Thanks to the introduction of continuous-time ODE, we demonstrate that the dynamics equation expressed by deep residual networks possesses excellent dynamic consistency to the denoising probability flow ODE (PF-ODE) [45]. Based on this property, we achieve the simplest improvement to each mrs-unit by parameterizing a learnable mean-variance scheduler, which avoids to manually design and theoretically support massively deep scalable training. ", "page_idx": 1}, {"type": "text", "text": "Adaptive stability maintenance and error sensitivity control: When the mrs-units are infinitely stacked to express the dynamics of an overall network $\\mathcal{F}_{\\theta}$ , the main technical difficulty is how to reduce the numerical errors caused by network propagation and ensure the stability of denoising. By introducing a sensitivity-related ODE in Sec. 2.3, we further demonstrate the theoretical advantages of the proposed gated residual networks in enabling stable denoising and effective sensitivity control. Qualitative and quantitative experimental results also consistently show their effectiveness. ", "page_idx": 1}, {"type": "text", "text": "2 Neural Residual Diffusion Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We propose Neural-RDM, a simple yet meaningful change to the architecture of deep generative networks that facilitates effective denoising, dynamical isometry and enables the stable training of extremely deep networks. This framework is supported by three critical theories: 1) Gating-Residual ODE (Sec. 2.1), which defines the dynamics of the minimum residual stacking unit (mrs-unit for short) that serves as the foundational denoising module, as shown in Figure 1 (a). Based on this gating-residual mechanism, we then introduce 2) Denoising-Dynamics ODE (Sec. 2.2) to further stack the mrs-units to become a continuous-time deep score prediction network $\\mathcal{F}_{\\theta}$ . Different from previous human-crafted mean-variance schedulers (e.g., variance exploding scheduler SMLD [46] and variance preserving scheduler DDPM [2]), which may cause concerns about instability in denoising efficiency and quality, we introduce a parametric method to implicitly learn the mean and variance distribution, which lowers the threshold of manual design and enhances the generalization ability of models. Last but not least, to maintain the stability of the deep stacked networks and verify the sensitivity of each residual unit $\\mathcal{F}_{\\theta_{i}}(\\cdot)$ to the network $\\mathcal{F}_{\\theta}$ , we stand on the shoulders of the adjoint sensitivity method [47, 48] to propose 3) Residual-Sensitivity ODE (Sec. 2.3), which means the sensitivity-related dynamics of each latent state $\\boldsymbol{z}_{i}$ from $\\mathcal{F}_{\\theta_{i}}(\\cdot)$ to the deep network $\\mathcal{F}_{\\theta}$ . Through rigorous derivation, we prove that the parameterized gating weights have a positive inhibitory effect on sensitivity decaying as network deepening. We will elaborate on them below. ", "page_idx": 1}, {"type": "image", "img_path": "NG16csOmcA/tmp/b78154b5cf22a34191c533a9b89bf68d189f015a7b7279f9109ec73cdec26a91.jpg", "img_caption": ["Figure 2: Overview. (a) Flow-shaped residual stacking networks. (b) U-shaped residual stacking networks. (c) Our proposed unified and massively scalable residual stacking architecture (i.e., NeuralRDM) with learnable gating-residual mechanism. (d) Residual denoising process via Neural-RDM. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.1 Gating-Residual Mechanism ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{F}_{\\theta_{i}}$ represents the minimum residual unit $\\mathtt{b l o c k}_{i}$ (Figure 1 (a)), $f(\\cdot)$ denotes any feature mapper wrapped by $\\mathcal{F}_{\\theta_{i}}$ . Instead of propagating the signal $_{z}$ through each of vanilla neural transformation $\\hat{z}=\\bar{f}_{\\theta}(z)$ directly, we introduce a gating-based residual connection for the signal $_{\\textit{z}}$ , which relys on the two learnable gating weights $\\hat{\\alpha}$ and $\\hat{\\beta}$ to modulate the non-trivial transformation ${\\mathcal{F}}_{\\theta_{i}}(z_{i})$ as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{z}}_{i}=\\boldsymbol{z}_{i}+\\hat{\\alpha}_{i}\\cdot\\mathcal{F}_{\\theta_{i}}(\\boldsymbol{z}_{i})+\\hat{\\beta}_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For a deep neural network $\\mathcal{F}_{\\theta}(\\cdot)$ with depth $L$ , consider two common residual stacking fashions: Flow-shaped Stacking (FS) [49, 50] and U-shaped Stacking (US) [51, 52]. For the flow-based deep stacking networks as shown in Figure 2 (a), each residual unit $f(\\cdot)$ accepts the output $\\boldsymbol{z}_{i}$ of the previous mrs-unit as input, and obtains a new hidden state $z_{i+1}$ through gating-residual connection, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{z}_{i}=z_{i+1}=z_{i}+[\\alpha_{i}\\cdot f_{\\theta_{i}}(z_{i})+\\beta_{i}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that Eq. 2 is a refined form of Eq. 1 in the case of flow-shaped stacking. In contrast, for the U-shaped deep stacking networks as in Figure 2 (b), each minimum residual unit contains two symmetrical branches, where the left branch receives the output $z_{i}$ of the previous mrs-unit\u2019s left branch as input (called the read-in branch), and the right branch performs the critical nonlinear residual transformation for readout (called the read-out branch), which can be formally described as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{z}_{i}=\\alpha_{i}^{(l)}\\cdot f_{\\theta_{i}^{(l)}}(z_{i})+\\beta_{i}^{(l)}\\hookrightarrow z_{i}+\\alpha_{i}^{(r)}\\cdot f_{\\theta_{i}^{(r)}}(z_{2L-2-i})+\\beta_{i}^{(r)}=z_{i}+\\hat{\\alpha}_{i}\\cdot{\\mathcal F}_{\\theta_{i}}(z_{i})+\\hat{\\beta}_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here Eq. 3 is a refined form of Eq. 1 in the case of U-shaped stacking, $\\hat{\\alpha}_{i}$ and ${\\hat{\\beta}}_{i}$ collectively denotes the gating weights from the left and right branches, $\\mathcal{F}_{\\theta_{i}}$ is the $i$ -th minimum residual unit of the U-shaped networks, and $\\omega\\omega\\rightarrow\\cdots$ denotes the skip-connection for $z_{i+1}\\rightarrow z_{2L-2-i}{}^{,}$ , which is computed recursively via ${\\mathcal{F}}_{\\theta_{i+1:L-1}}$ . To enable the networks to be infinitely stacked, we introduce a continuous-time Gating-Residual ordinary differential equation (ODE) to express the neural dynamics of these two types of deep stacking networks $\\begin{array}{r}{(\\delta=\\frac{1}{L},L\\to\\infty}\\end{array}$ denotes the number of the mrs-units), ", "page_idx": 2}, {"type": "image", "img_path": "NG16csOmcA/tmp/8b6d7400fd871f7e2f37bec9fe1acb34cdf570b845cff802ec075758c556a1ba.jpg", "img_caption": ["Figure 3: Compared with the latest baseline (SDXL-1.0 [7]), the samples produced by NeuralRDM (trained on JourneyDB [53]) exhibit exceptional quality, particularly in terms of fidelity and consistency in the details of the subjects in adhering to the provided textual prompts. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{z_{i+\\delta}-z_{i}}{\\delta}=\\hat{z}_{i}-z_{i}=\\hat{\\alpha}_{i}\\cdot\\mathcal{F}_{\\theta_{i}}(z_{i})+\\hat{\\beta}_{i}\\Longrightarrow\\frac{d z_{t}}{d t}=\\hat{\\alpha}_{\\phi}\\cdot\\mathcal{F}_{\\theta_{t}}(z_{t})+\\hat{\\beta}_{\\phi},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi$ represents the gating weights, which can be independently trained or fine-tuned without considering the parameters $\\theta$ of the feature mapping network $\\mathcal{F}_{\\theta}(\\cdot)$ itself. ", "page_idx": 3}, {"type": "text", "text": "2.2 Denoising Dynamics Parameterization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The above-mentioned gating-residual mechanism is utilized to modulate mainstream deep stacking networks and unify them into a residual-style massively scalable generative framework, as shown in Figure 2 (c). Next, we further explore the essential relationship between residual neural networks and score-based generative denoising models from a dynamic perspective. ", "page_idx": 3}, {"type": "text", "text": "First, inspired by the theory of continuous-time diffusion models [45, 54], the forward add-noising process can be expressed as a dynamic process with stochastic differential equation (SDE) as, ", "page_idx": 3}, {"type": "equation", "text": "$$\nd z_{t}=\\mu(z_{t},t)d t+\\sigma(t)d\\mathbf{w}_{t}\\Longrightarrow\\frac{d z_{t}}{d t}=\\mu(z_{t},t)+\\sigma(t)\\cdot\\epsilon_{t},\\epsilon_{t}\\in\\mathcal{N}(\\mathbf{0},\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which describes a data perturbation process controlled by a mean-variance scheduler composed of $\\pmb{\\mu}(\\pmb{z}_{t},t)$ and ${\\pmb\\sigma}(t)$ respectively, ${\\bf w}_{t}$ denotes the standard Brownian motion. Compared with the forward process, the core of the diffusion model is to utilize a deep neural network (as deep and large as possible) for score-based reverse prediction [46, 55]. A remarkable property of this SDE is the existence of a reverse ODE (also dubbed as the Probability Flow (PF) ODE by [45]), which retain the same marginal probability densities as the SDE (See Appendix. A.2 for detailed proof) and could effectively guide the dynamics of the reverse denoising, it can be formally described as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d z_{t}}{d t}=\\pmb{\\mu}(z_{t},t)-\\frac{1}{2}\\pmb{\\sigma}(t)^{2}\\cdot\\Big[\\nabla_{z}\\log p_{t}(z_{t})\\Big]=\\hat{\\alpha}_{t,\\phi}\\cdot\\mathcal{F}_{\\theta}(z_{t},t)+\\hat{\\beta}_{t,\\phi},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nabla_{z}\\log{p_{t}(z_{t})}$ denotes the gradient of the log-likelihood of $p_{t}\\big(\\boldsymbol{z}_{t}\\big)$ , which can be estimated by a score matching network $\\mathcal{F}_{\\theta}(z_{t},t)$ . Here we re-parameterize the PF-ODE by utilizing gated weights to replace the manually designed mean-variance scheduler, in which $\\hat{\\pmb{\\alpha}}_{t,\\phi}$ and $\\hat{\\rho}_{t,\\phi}$ denotes the time-dependent dynamics parameters, which is respectively parameterized to represent $-\\frac{1}{2}\\pmb{\\sigma}(t)^{2}$ and $\\pmb{\\mu}(\\pmb{z}_{t},t)$ by our proposed gating-residual mechanism. Note that $\\mathcal{F}_{\\theta}(\\cdot)$ is a score estimation network composed of infinite mrs-units $\\mathtt{b l o c k}_{i}$ (i.e., $\\mathcal{F}_{\\theta_{i}}$ ), which enables massively scalable generative training on large-scale vision data, but also presents the challenge of numerical propagation errors. ", "page_idx": 3}, {"type": "text", "text": "2.3 Residual Sensitivity Control ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To control the numerical errors in back-propagation and achieve steadily and massively scalable training, we stand on the shoulders of the adjoint sensitivity method [47, 48] to introduce another ", "page_idx": 3}, {"type": "table", "img_path": "NG16csOmcA/tmp/3c5f1d094a6fb9415fc2d6b61e1420abcc908b769a89979f1f99fc797a6f2c0c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Table 1: The main results for image generation on ImageNet [61] (Class-to-Image) and JourneyDB [53] (Text-to-Image) with $256\\times256$ image resolution. We highlight the best value in blue , and the second-best value in green . The Scalability column indicates the scaling capability of the parameter scale and architecture. ", "page_idx": 4}, {"type": "text", "text": "Residual-Sensitivity ODE, which is utilized to evaluate the sensitivity of each residual-state $\\boldsymbol{z}_{t}$ of the mrs-unit $\\mathcal{F}_{\\theta_{i}}$ to the total loss $\\mathcal{L}$ derived by score estimation network $\\mathcal{F}_{\\theta}(\\cdot)$ (the sensitivity is denoted as $\\begin{array}{r}{s_{t}=\\frac{d\\mathcal{L}}{d z_{t}}}\\end{array}$ , $\\delta$ denotes an infinitesimal time interval) and can be formally described by the chain rule, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{s}_{t}=\\frac{d\\mathcal{L}}{d z_{t}}=\\frac{d\\mathcal{L}}{d z_{t+\\delta}}\\cdot\\frac{d z_{t+\\delta}}{d z_{t}}=\\boldsymbol{s}_{t+\\delta}\\cdot\\frac{d z_{t+\\delta}}{d z_{t}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "On the basis of Eq. 7, we next continue to discuss the dynamic equation of sensitivity changing with time $t$ . First, considering the trivial transformation $f_{\\theta}({\\bar{\\cdot}})$ without gating-residual mechanism, ", "page_idx": 4}, {"type": "equation", "text": "$$\nd z_{t+\\delta}=d z_{t}+\\int_{t}^{t+\\delta}f_{\\theta}(z_{t},t)d t.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can rewrite Eq. 7 based on Eq. 8 as: ", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{t}=s_{t+\\delta}+s_{t+\\delta}\\cdot\\frac{\\partial}{\\partial z_{t}}(\\int_{t}^{t+\\delta}f_{\\theta}(z_{t},t)d t).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The Residual-Sensitivity ODE under vanilla situation then can be derived, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{d s_{t}}{d t}=\\operatorname*{lim}_{\\delta\\rightarrow0^{+}}\\frac{s_{t+\\delta}-s_{t}}{\\delta}=\\operatorname*{lim}_{\\delta\\rightarrow0^{+}}\\frac{-s_{t+\\delta}\\cdot\\frac{\\partial}{\\partial z_{t}}(\\int_{t}^{t+\\delta}f_{\\theta}(z_{t})d t)}{\\delta}=-s_{t}\\cdot\\frac{\\partial f_{\\theta}(z_{t},t)}{\\partial z_{t}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "According to the derived residual-sensitivity equation in Eq. 10, we further use the Euler solver to obtain the sensitivity $\\pmb{{s}}_{t_{0}}$ of the starting state $z_{t_{0}}$ to network $\\mathcal{F}_{\\theta}(\\cdot)$ as, ", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{t_{0}}=s_{t_{L}}+\\int_{t_{L}}^{t_{0}}\\frac{d\\mathbf{s}_{t}}{d t}d t=s_{t_{L}}-\\int_{t_{L}}^{t_{0}}s_{t}\\cdot\\frac{\\partial f_{\\theta}(z_{t},t)}{\\partial z_{t}}d t.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Due to the non-negativity of the integral and the gradient \u2202f\u03b8\u2202(zzt,t) not equals to 0, we can obtain a gradually decaying sensitivity sequence: $\\pmb{\\mathscr{s}}_{t_{L}}\\;>\\;\\pmb{\\mathscr{s}}_{t_{L-1}}\\;>\\;\\cdot\\,\\cdot\\;>\\;\\pmb{\\mathscr{s}}_{t_{0}}$ . Similarly, when defining parameter-sensitivity $\\begin{array}{r}{s_{\\theta}=\\frac{d\\mathcal{L}}{d\\theta}}\\end{array}$ , the same decaying results for ${\\boldsymbol{s}}_{\\theta_{0}}$ can be obtained: ", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{\\theta_{0}}=s_{\\theta_{L}}+\\int_{t_{L}}^{t_{0}}\\frac{d\\mathbf{s}_{\\theta}}{d t}d t=s_{\\theta_{L}}-\\int_{t_{L}}^{t_{0}}\\mathbf{s}_{\\theta}\\cdot\\frac{\\partial f_{\\theta}(z_{t},t)}{\\partial\\theta}d t.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "NG16csOmcA/tmp/31ab357ea58c785a8af86b8636238aa5c4cba3a8c85d108850cb2166a5c96b48.jpg", "img_caption": ["Figure 4: Compared with the latest baseline (Latte-XL [60]), the sample videos from SkyTimelapse [62], Taichi-HD[63] and UCF101 [64] all exhibit better frame quality, temporal consistency and coherence. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "To alleviate this problem, and enable stable training in massively deep scalable architecture, we introduce the following non-trivial solution with gating-residual transformation, ", "page_idx": 5}, {"type": "equation", "text": "$$\nd\\hat{z}_{t+\\delta}=d\\hat{z}_{t}+\\int_{t}^{t+\\delta}\\Big[\\alpha_{t,\\phi}\\cdot f_{\\theta}(\\hat{z}_{t})+\\beta_{t,\\phi}\\Big]d t.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Substitute Eq. 13 into Eq. 7 to obtain the corrected sensitivity s\u02c6t = ddz\u02c6Lt as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol s}_{t}=\\hat{\\boldsymbol s}_{t+\\delta}+\\hat{\\boldsymbol s}_{t+\\delta}\\cdot\\frac{\\partial}{\\partial\\hat{\\boldsymbol z}_{t}}(\\int_{t}^{t+\\delta}\\Big[\\alpha_{t,\\phi}\\cdot f_{\\theta}(\\hat{\\boldsymbol z}_{t})+\\beta_{t,\\phi}\\Big]d t).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The non-trivial Residual-Sensitivity ODE can be derived as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{d\\hat{\\bf s}_{t}}{d t}=\\operatorname*{lim}_{\\delta\\rightarrow0^{+}}\\frac{\\hat{\\bf s}_{t+\\delta}-\\hat{\\bf s}_{t}}{\\delta}=-(\\alpha_{t,\\phi}\\cdot\\hat{\\bf s}_{t})\\cdot\\frac{\\partial f_{\\theta}(\\hat{z}_{t},t)}{\\partial\\hat{z}_{t}}-(\\beta_{t,\\phi}\\cdot\\hat{\\bf s}_{t}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Through the Euler solver, we can also obtain the sensitivity $\\hat{\\pmb{s}}_{t_{0}}$ of the starting state adjusted by the gating-residual weights, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{s}}_{t_{0}}=\\hat{\\boldsymbol{s}}_{t_{L}}+\\int_{t_{L}}^{t_{0}}\\frac{d\\hat{\\boldsymbol{s}}_{t}}{d t}d t=\\hat{\\boldsymbol{s}}_{t_{L}}-\\int_{t_{L}}^{t_{0}}\\left[(\\alpha_{t,\\phi}\\cdot\\hat{\\boldsymbol{s}}_{t})\\cdot\\frac{\\partial f_{\\theta}(\\hat{\\boldsymbol{z}}_{t},t)}{\\partial\\hat{\\boldsymbol{z}}_{t}}+(\\beta_{t,\\phi}\\cdot\\hat{\\boldsymbol{s}}_{t})\\right]d t.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where $\\alpha_{t,\\phi}$ and $\\beta_{t,\\phi}$ adaptively modulate and update the sensitivity of each mrs-unit to the final loss, which supports being trained through minimizing $\\mathcal{L}_{s}=||\\mathcal{F}_{\\theta}(z_{t},t)-\\nabla_{z}\\log p_{t}(z_{t})||_{2}^{2}+\\gamma$ \u00b7 $\\begin{array}{r}{\\sum_{L}||\\alpha_{t,\\phi}\\cdot\\frac{\\partial f_{\\theta}(\\hat{\\z}_{t},t)}{\\partial\\hat{z}_{t}}-\\beta_{t,\\phi}||_{2}^{2}}\\end{array}$ in full-parameter training or model fine-tuning fashions. ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present the main experimental settings in Sec. 3.1. To evaluate the generative performance of Neural-RDM, we compare it with state-of-the-art conditional/unconditional diffusion models for image synthesis and video generation in Sec. 3.2 and Sec. 3.3 respectively. We also visualize and analyze the effects of the proposed gated residuals and illustrate their advantages in enabling deep scalable training, which are presented in Sec. 3.4 and Sec. 3.5. ", "page_idx": 5}, {"type": "text", "text": "3.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. For image synthesis tasks, we train and evaluate the Class-to-Image generation models on the ImageNet [61] dataset and train and evaluate the Text-to-Image generation models on the ", "page_idx": 5}, {"type": "table", "img_path": "NG16csOmcA/tmp/06d5ea3a7cdf959fa4c0d60e89b2769142d191627710eacbcf065d950124bc60.jpg", "table_caption": ["Table 2: The main results for video generation on the SkyTimelapse [62], Taichi-HD [63] and UCF-101 [64] with $256\\times256$ resolution of each frame. We highlight the best value in blue , and the second-best value in green . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "MSCOCO [65] and JourneyDB [53] datasets. All images are resized to $256\\times256$ resolution for training. For video generation tasks, we follow the previous works [12, 60] to train None-to-Video (i.e., unconditional video generation) models on the SkyTimelapse [62] and Taichi [63] datasets, and train Class-to-Video models on the UCF-101 [64] dataset. Moreover, we follow previous works [12, 60] to sample 16-frame video clips from these video datasets and then resize all frames to $256\\times256$ resolution for training and evaluation. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We implement our Neural-RDMs into Neural-RDM-U (U-shaped) and Neural-RDM-F (Flow-shaped) two versions on top of the current state-of-the-art diffusion models LDM [30] and Latte [60] for image generation, and further employ the Neural-RDM-F version for video generation. Specifically, we first load the corresponding pre-trained models and initialize gating parameters $\\{\\alpha=1,\\beta=0\\}$ of each layer, then perform full-parameter fine-tuning to implicitly learn the distribution of the data for acting as a parameterized mean-variance scheduler. During the training process, we adopt an explicit supervision strategy to enhance the sensitivity correction capabilities of $\\alpha$ and $\\beta$ for deep scalable training, where the explicitly supervised hyper-parameter $\\gamma$ is set to 0.35. Eventually, we utilize the AdamW optimizer with a constant learning rate of $5\\times10^{4}$ for all models and exploit an exponential moving average (EMA) strategy to obtain and report all results. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. Following the previous baselines [30, 58, 59, 60], we adopt Fr\u00e9chet Inception Distance (FID) [66], sFID [67] and Inception Score (IS) [68] to evaluate the image generation quality and the video frame quality (except for sFID). Furthermore, we utilize a Fr\u00e9chet Video Distance (FVD) [69] metric similar with FID to evaluate the unconditional and conditional video generation quality. Among these metrics, FVD is closer to human subjective judgment and thus better reflects the visual quality of the generated video content. Adhering to the evaluation guidelines proposed by StyleGAN-V [70], we calculate the FVD scores by analyzing 2048 generated video clips with each clip consists of 16 frames. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare the proposed method with the recent state-of-the-art baselines, and categorize them into three groups: 1) GAN-based. BigGAN-deep [56] and StyleGAN-XL [57] for image task, MoCoGAN [71], MoCoGAN-HD [72], DIGAN [73], StyleGAN-V [70] and MoStGAN-V [74] for video task. 2) U-shaped. ADM [58] and LDM [30] for image task, PVDM [75] and LVDM [12] for video task. 3) F-shaped. DiT-XL/2 [59] and Latte-XL [60] for image task, VideoGPT [76] and Latte-XL [60] (with temporal attention learning) for video task. ", "page_idx": 6}, {"type": "text", "text": "3.2 Experiments on Image Synthesis with Deep Scalable Spatial Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For a more objective comparison, we maintain approximately the same model size to perform classconditional and text-conditional image generation experiments, which are shown in Table 1. From Table 1, it can be observed that our Neural-RDMs have obtained state-of-the-art results. Specifically, the flow-based version (i.e., Neural-RDM-F) consistently outperforms all class-to-image baselines in all three image\u2019s generative benchmarks and meanwhile obtains relatively suboptimal results on another text-to-image evaluations. It is worth noting that another Neural-RDM-U version have made up for this shortcoming and achieved optimal results, which may benefit from the more powerful semantic guidance abilities of the cross-attention layer built into U-Net. To more clearly present the actual effects of the gated residuals, we further perform qualitative comparative experiments, which are shown in Figure 3. Compared with the latest baseline (SDXL-1.0 [7]), we can observe that the samples produced by Neural-RDM exhibit exceptional quality, particularly in terms of fidelity and consistency in the details of the subjects in adhering to the provided textual prompts, which consistently demonstrates the effectiveness of our proposed approach in deep scalable spatial learning. ", "page_idx": 6}, {"type": "image", "img_path": "NG16csOmcA/tmp/0f6f98ffbf63cc535f074706d260d7d2729ac3b798e5677c96813da73cd2b257.jpg", "img_caption": ["Figure 6: (a), (b), and (c) respectively illustrate the performance of the five residual structures variant models across the SkyTimelapsee [62], Taichi-HD[63], and UCF-101 [64]. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.3 Experiments on Video Generation with Deep Scalable Temporal Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further explore the effectiveness and specific contributions of proposed gating-residual mechanism in temporal learning, we continue to perform the video generation evaluations, which are shown in Table 2. From Table 2, we find that our model (flow-shaped version) basically achieves the best results (except for the second-best results in class-to-video evaluation). Specifically, compare with Latte-XL [60], Neural-RDM respectively achieves an improvement of $33.3\\%$ and $42.8\\%$ in FVD scores on SkyTimelapse and Taichi-HD datasets, which hints the powerful potential of flow-based deep residual networks in promoting generative emergent capabilities of video models. Furthermore, we exhibit a number of visual comparison results of the 16-frames video produced by Neural-RDM and baseline (Latte-XL [60]), as shown in Figure 4. We can observe that some generated frames from the baseline partially exhibits poor quality and temporal inconsistency. Compare with the baseline, Neural-RDM maintains temporal coherence and consistency, resulting in smoother and more dynamic video frames, which further reflects the effectiveness of proposed method in both quantitative and qualitative evaluations. ", "page_idx": 7}, {"type": "image", "img_path": "NG16csOmcA/tmp/f6b03daa755ad0f46987864536e6be1e4d5ec62d29b829119d94286104ea8f01.jpg", "img_caption": ["Figure 5: The sensitivity of $\\alpha$ and $\\beta$ at different depths of the residual denoising network during the training process. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.4 The Analyses of Gating Residual Sensitivity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To better illustrate the advantage of the gated residuals and understand the positive suppression effect for sensitivity attenuation as network deepening, we visualize the normalized sensitivity at different depths of our Neural-RDM during the training process, as shown in Figure 5. From Figure 5, we can observe that $\\alpha$ and $\\beta$ can adaptively modulate the sensitivity of each mrs-unit to correct the denoising process as network deepening, which is consistent with Eq. 16. Moreover, we can also observe that at the beginning of training, the sensitivity scores are relatively low. As training advances, $\\alpha$ and $\\beta$ are supervised to correct the sensitivity until obtaining relatively higher sensitivity scores. ", "page_idx": 7}, {"type": "text", "text": "To explore the actual effects of different residual settings in deep training, we first perform the comparison experiments on 5 different residual variants: $^{\\,I}$ ) Variant-0 (Ours): $z_{i+1}=z_{i}\\!+\\!\\alpha f(z_{i})\\!+\\!\\beta$ ; 2) Variant-1 (AdaLN [77]): $z_{i+1}=z_{i}+f(\\alpha z_{i}+\\beta);\\,3,$ ) Variant-2: $z_{i+1}=\\alpha z_{i}+f(z_{i})+\\beta;\\mathbf{4})$ Variant-3 (ResNet $I78J)$ : $z_{i+1}\\,=\\,z_{i}\\,+\\,f(z_{i});\\,\\xi$ 5) Variant- $^{4}$ (ReZero [79]): $z_{i+1}\\,=\\,z_{i}\\,+\\,\\alpha f(z_{i})$ . ", "page_idx": 8}, {"type": "text", "text": "We utilize Latte-XL as backbone to train each variant from scratch and then evaluate their performance for video generation. As depicted in Figure 6, as the number of training steps increases, almost all variants can converge effectively, but only Variant-0 (Our approach) achieves the best FVD scores. We speculate that it may be because this post-processing gating-residual setting maintains excellent dynamic consistency with the reverse denoising process, thus achieving better performance. ", "page_idx": 8}, {"type": "text", "text": "Moreover, we further perform the deeep scalability experiments, which are shown in Figure 7. We can observe that as the depth of residual units increases, the performance of the model can be further improved, which illustrates the positive correlation between model performance and the depth of residual units and further highlights the deep scalability advantage of our Neural-RDM. ", "page_idx": 8}, {"type": "image", "img_path": "NG16csOmcA/tmp/bf5b640916ba7a57d7dd23ecbc562c80c435dd6afb6f7ff980b7cbfed6952ba1.jpg", "img_caption": ["Figure 7: The performance of NeuralRDM with different network depths on the UCF-101 dataset [64]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Deep Residual Networks. Most common deep residual networks can be divided into two types of architectures: flow-shaped stacking (FS) and u-shaped stacking (US) architectures. As a milestone of flow-based deep residual networks, ResNet [78] has led the research of visual understanding tasks [80]. In fact, the pratices [81, 82] and theories [83, 84, 85] that introducing the highway connections [86, 87] have been studied for a long time and have demonstrated excellent advantages in dealing with vanishing/exploding gradients and numerical propagation errors in deep stacked networks. Different from ResNet, U-Net [51] is a leader of u-shaped networks and almost dominated diffusion-based generative models [2, 30]. Though achieving remarkable success, both types of CNN-based models still face concerns about training efficiency. Recent years, Transformer [49] and ViT [50] have emerged as new state-of-the-art backbones in computer vision and multimodal [88, 89, 90, 91] and have also gained prominence in various diffusion models. Among them, DiT [59] and U-ViT [52] are two representative works by respectively adopting flow-shaped and u-shaped residual stacking fashions, which have enabled many studies on deep generative models [60, 92, 93, 94, 95]. In this work, we unify the above two types of residual stacking architectures from a dynamic perspective and propose a unified and deep scalable neural residual framework with a same gating-residual ODE. ", "page_idx": 8}, {"type": "text", "text": "Diffusion Models. Recent years has witnessed the remarkable success of diffusion models [2, 3, 4], due to their impressive generative capabilities. Previous efforts mainly focus on sampling procedure [25, 26, 27, 28], conditional guidance [31, 32, 96, 97, 98], likelihood maximization [33, 34, 35, 36] and generalization ability [37, 99, 39, 10] and have gained enormous attention. Recently, a major research topic on diffusion models is scalability. DiT [59] is one of the most representative models by exploiting a scalable Transformer to train latent diffusion models for image generation. Latte [60] stands on the shoulders of DiT to further perform temporal learning for video generation. However, both Latte and DiT adopt the residual structure of Transformer by default and utilize S-AdaLN to incorporate guidance information, they generally lack: 1) attention to the residual structure and 2) study the dynamic nature of the deep generative models, and 3) ignore the error propagation issues from deeper networks and therefore are still limited by the bottleneck of massively scalable training. ", "page_idx": 8}, {"type": "text", "text": "Overall, we practically unify u-shaped and flow-shaped stacking networks and to propose a unified and deep scalable neural residual diffusion model framework. Moreover, we theoretically parameterize the previous human-designed mean-variance scheduler and demonstrate excellent dynamics consistency. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have presented Neural-RDM, a simple yet meaningful change to the architecture of deep generative networks that facilitates effective denoising, dynamical isometry and enables the stable training of extremely deep networks. Further, we have explored the nature of two common types of neural networks that enable effective denoising estimation. On this basis, we introduce a parametric method to replace previous human-designed mean-variance schedulers into a series of learnable gating-residual weights. Experimental results on various generative tasks show that Neural-RDM obtains the best results, and extensive experiments also demonstrate the advantages in improving the fidelity, consistency of generated content and supporting large-scale scalable training. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Science and Technology Major Project (2023ZD0121403), National Natural Science Foundation of China (No. 62406161), China Postdoctoral Science Foundation (No. 2023M741950), and the Postdoctoral Fellowship Program of CPSF (No. GZB20230347). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Zhiyuan Ma, Yuzhu Zhang, Guoli Jia, Liangliang Zhao, Yichao Ma, Mingjie Ma, Gaofeng Liu, Kaiyan Zhang, Jianjun Li, and Bowen Zhou. Efficient diffusion models: A comprehensive survey from principles to practices. arXiv preprint arXiv:2410.11795, 2024. [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in NeurIPS, 33:6840\u20136851, 2020. [3] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, pages 8162\u20138171, 2021. [4] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [5] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022. [6] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[7] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [8] Zhiyuan Ma, Guoli Jia, Biqing Qi, and Bowen Zhou. Safe-sd: Safe and traceable stable diffusion with text prompt trigger for invisible generative watermarking. In ACM Multimedia 2024. [9] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[10] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.   \n[11] Zhiyuan Ma, Guoli Jia, and Bowen Zhou. Adapedit: Spatio-temporal guided adaptive editing algorithm for text-based continuity-sensitive image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4154\u20134161, 2024.   \n[12] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022.   \n[13] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.   \n[14] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023.   \n[15] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.   \n[16] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024.   \n[17] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistencyaware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23040\u201323050, 2023.   \n[18] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, et al. Magicvideo-v2: Multi-stage high-aesthetic video generation. arXiv preprint arXiv:2401.04468, 2024.   \n[19] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024.   \n[20] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2022.   \n[21] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2023.   \n[22] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309, 2023.   \n[23] Joseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d with advanced diffusion guidance. arXiv preprint arXiv:2305.18766, 2023.   \n[24] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion. arXiv preprint arXiv:2403.12008, 2024.   \n[25] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In International Conference on Machine Learning, pages 42390\u201342402. PMLR, 2023.   \n[26] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.   \n[27] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, pages 32211\u201332252, 2023.   \n[28] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.   \n[29] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[31] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In The Twelfth International Conference on Learning Representations, 2023.   \n[32] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7346\u20137356, 2023.   \n[33] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-Chul Moon. Maximum likelihood training of implicit nonlinear diffusion models. arXiv preprint arXiv:2205.13699, 2022.   \n[34] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning, pages 4672\u20134712. PMLR, 2023.   \n[35] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood training for score-based diffusion odes by high order denoising score matching. In International Conference on Machine Learning, pages 14429\u201314460. PMLR, 2022.   \n[36] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415\u2013 1428, 2021.   \n[37] Yuhan Li, Yishun Dou, Xuanhong Chen, Bingbing Ni, Yilin Sun, Yutian Liu, and Fuzhen Wang. Generalized deep 3d shape prior via part-discretized diffusion process. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16784\u201316794, 2023.   \n[38] Zhiyuan Ma, Jianjun Li, Bowen Zhou, et al. Lmd: Faster image reconstruction with latent masking diffusion. arXiv preprint arXiv:2312.07971, 2023.   \n[39] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4296\u20134304, 2024.   \n[40] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022.   \n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, and Yasmine et al Babaei. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.   \n[42] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[43] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024.   \n[44] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024.   \n[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.   \n[46] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[47] ML Chambers. The mathematical theory of optimal processes. Journal of the Operational Research Society, 16(4):493\u2013494, 1965.   \n[48] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[50] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020.   \n[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241, 2015.   \n[52] Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based diffusion models. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.   \n[53] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024.   \n[54] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[55] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence, pages 574\u2013584. PMLR, 2020.   \n[56] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2018.   \n[57] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 1\u201310, 2022.   \n[58] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in NeurIPS, 34:8780\u20138794, 2021.   \n[59] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[60] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024.   \n[61] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   \n[62] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2364\u20132373, 2018.   \n[63] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019.   \n[64] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.   \n[65] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[66] Mandi Luo, Jie Cao, Xin Ma, Xiaoyu Zhang, and Ran He. Fa-gan: Face augmentation gan for deformation-invariant face recognition. IEEE Transactions on Information Forensics and Security, 16:2341\u20132355, 2021.   \n[67] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. In International Conference on Machine Learning, pages 7958\u20137968. PMLR, 2021.   \n[68] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 2830\u20132839, 2017.   \n[69] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717, 2018.   \n[70] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3626\u20133636, 2022.   \n[71] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526\u20131535, 2018.   \n[72] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In International Conference on Learning Representations, 2020.   \n[73] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2021.   \n[74] Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal motion styles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5652\u20135661, 2023.   \n[75] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18456\u201318466, 2023.   \n[76] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.   \n[77] Yunhui Guo, Chaofeng Wang, Stella X Yu, Frank McKenna, and Kincho H Law. Adaln: a vision transformer for multidomain learning and predisaster building information extraction from images. Journal of Computing in Civil Engineering, 36(5):04022024, 2022.   \n[78] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[79] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pages 1352\u20131361. PMLR, 2021.   \n[80] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.   \n[81] Tapani Raiko, Harri Valpola, and Yann LeCun. Deep learning made easier by linear transformations in perceptrons. In Artificial intelligence and statistics, pages 924\u2013932. PMLR, 2012.   \n[82] Alex Graves and Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pages 37\u201345, 2012.   \n[83] Nicol Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical report/IDSIA, 98, 1998.   \n[84] Nicol N Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pages 207\u2013226. Springer, 2002.   \n[85] Tommi Vatanen, Tapani Raiko, Harri Valpola, and Yann LeCun. Pushing stochastic gradient towards second-order methods\u2013backpropagation learning with transformations in nonlinearities. In Neural Information Processing: 20th International Conference, ICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part I 20, pages 442\u2013449. Springer, 2013.   \n[86] Rupesh Kumar Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015.   \n[87] Rupesh K Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Training very deep networks. Advances in neural information processing systems, 28, 2015.   \n[88] Zhiyuan Ma, Jianjun Li, Guohui Li, and Kaiyan Huang. Cmal: A novel cross-modal associative learning framework for vision-language pre-training. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4515\u20134524, 2022.   \n[89] Zhiyuan Ma, Jianjun Li, Guohui Li, and Yongjing Cheng. Unitranser: A unified transformer semantic representation framework for multimodal task-oriented dialog system. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 103\u2013114, 2022.   \n[90] Zhiyuan Ma, Zhihuan Yu, Jianjun Li, and Guohui Li. Hybridprompt: bridging language models and human priors in prompt tuning for visual question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 13371\u201313379, 2023.   \n[91] Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. Generative multi-modal knowledge retrieval with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18733\u201318741, 2024.   \n[92] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024.   \n[93] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.   \n[94] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boff,i Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024.   \n[95] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2023.   \n[96] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, et al. Follow-your-click: Open-domain regional image animation via short prompts. arXiv preprint arXiv:2403.08268, 2024.   \n[97] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023.   \n[98] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. arXiv preprint arXiv:2406.01900, 2024.   \n[99] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages 10696\u201310706, 2022.   \n[100] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 12873\u201312883, 2021.   \n[101] Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker\u2013planck equations through gradient\u2013log\u2013density estimation. Entropy, 22(8):802, 2020.   \n[102] Bernt \u00d8ksendal and Bernt \u00d8ksendal. Stochastic differential equations. Springer, 2003. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Theoretical Interpretations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide mathematical intuitions for our Neural-RDMs. ", "page_idx": 16}, {"type": "text", "text": "Continuous-time Residual Networks. For a deep neural network $\\mathcal{F}_{\\theta}(\\cdot)$ with depth $L$ , let ${\\mathcal{F}}_{\\theta_{i}}$ represents the minimum residual unit $\\mathtt{b l o c k}_{i}$ (Figure 1 (a)). Instead of propagating the signal $_{z}$ through vanilla neural transformation $\\hat{z}=f_{\\theta}(z)$ , we introduce a gating-based skip-connection for the signal $_{z}$ , which relys on the gating weights $\\hat{\\alpha}$ and $\\hat{\\beta}$ to modulate the non-trivial transformation $\\mathcal{F}_{\\theta}(z)$ as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\pmb{z}}=\\pmb{z}+\\hat{\\alpha}\\cdot\\mathcal{F}_{\\theta}(\\pmb{z})+\\hat{\\beta}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the case of continuous time, this dynamic equation describing the change process of the signal $_{z}$ is called the gating-residual ODE: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d z_{t}}{d t}=\\hat{\\alpha}_{\\phi}\\cdot\\mathcal{F}_{\\theta_{t}}(z_{t})+\\hat{\\beta}_{\\phi},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Diffusion Probability Models. The diffusion \u221aprobability models are modeled as: 1) a deterministic forward noising process $q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\bar{\\sqrt{\\alpha_{t}}}\\mathbf{x}_{t-1},(1-\\alpha_{t})\\mathbf{I})$ from the original image $\\mathbf{X}_{\\mathrm{0}}$ to a pure-Gaussian distribution $\\mathbf{x}_{T}\\sim\\mathcal{N}(0,\\mathbf{I})$ , which can be formulated in an accumulated form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,\\quad\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2) a iteratively predictable reverse denoising process $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})=\\mathcal{N}(\\mathbf{x}_{t-1};\\mu_{\\theta}(\\mathbf{x}_{t},t),\\Sigma_{\\theta}(\\mathbf{x}_{t},t))$ , which can be trained in a simplied denoising objective $\\mathcal{L}_{\\mathrm{simple}}$ by merging $\\pmb{\\mu}_{\\theta}$ and $\\Sigma_{\\theta}$ into predicting noise $\\epsilon_{\\theta}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{simple}}=E_{\\mathbf{x}_{0},t,\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}[||\\epsilon-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)||_{2}^{2}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $t\\,\\sim\\mathcal{U}[1,T]$ is time parameters, $\\mathcal{U}(\\cdot)$ denotes uniform distribution. Moreover, in Stable Diffusion [30], the image $\\mathbf{X}_{t}$ is compressed into a latent variable $\\mathbf{Z}_{t}$ by encoder $\\mathcal{E}$ for more efficient training, i.e., ${\\bf z}_{t}=\\mathcal{E}({\\bf x}_{t})$ , thus this preliminary objective is usually defined as making $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ as close to $\\pmb{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I})$ as possible. ", "page_idx": 16}, {"type": "text", "text": "Reverse Denoising ODE. A remarkable property of the SDE (Eq. 19) is the existence of a reverse ODE (also dubbed as the Probability Flow (PF) ODE by [45]), which retains the same marginal probability densities as the SDE (See Appendix A.2 for detailed proof) and could effectively guide the dynamics of the reverse denoising, it can be formally described as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d z_{t}}{d t}=\\pmb{\\mu}(z_{t},t)-\\frac{1}{2}\\pmb{\\sigma}(t)^{2}\\cdot\\Big[\\nabla_{z}\\log p_{t}(z_{t})\\Big]=\\hat{\\alpha}_{t,\\phi}\\cdot\\mathcal{F}_{\\theta}(z_{t},t)+\\hat{\\beta}_{t,\\phi},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\nabla_{z}\\log{p_{t}(z_{t})}$ denotes the gradient of the log-likelihood of $p_{t}\\big(\\boldsymbol{z}_{t}\\big)$ , which can be estimated by a score matching network $\\bar{\\mathcal{F}}_{\\theta}(z_{t},\\bar{t})$ . ", "page_idx": 16}, {"type": "text", "text": "Dynamics Consistency. Refer to Eq. 18 and Eq. 21, we define this dynamic consistency as: For any time-dependent signal $\\boldsymbol{z}_{t}$ , the different dynamics systems describe it with the same motion path (or the same change rate of data distribution). Note that in Eq. 21, we achieve this by using a re-parameterized approach. ", "page_idx": 16}, {"type": "text", "text": "Latent Space Projection. The latent space projection is proposed by [30] to compress the input images $\\mathbf{X}_{\\mathrm{0}}$ into a perceptual high-dimensional space to obtain $\\mathbf{z}_{\\mathrm{0}}$ by leveraging a pretrained VQ-VAE model [100]. The VQ-VAE is also used by our Neural-RDM, it consists of an encoder $\\mathcal{E}$ and a decoder $\\mathcal{G}$ . The mathematical definition is: Given an input image $\\boldsymbol{x}\\in\\mathbb{R}^{H\\times W\\times3}$ , the VQ-VAE first compress the image $x$ into a latent variable $\\hat{z}$ by encoder $\\mathcal{E}$ , i.e., $\\hat{z}=\\mathcal{E}(x)$ and $\\hat{z}\\in\\mathbb{R}^{h\\times\\bar{w}\\times d}$ , where $h$ and $w$ respectively denote scaled height and width (scaled factor $f={\\dot{H}}/h=W/w=8\\rangle$ ), and $d$ is the dimensionality of the compressed latent variable. After going through the diffusion step described in Eq. 5 and Eq. 6, the latent variable $\\hat{z}$ is updated and finally reconstructed into $\\hat{x}$ by decoder $\\mathcal{G}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{x}=\\mathcal{G}_{\\pi}(\\mathrm{LDM}_{\\mathcal{F}_{\\theta}(\\cdot)}(\\mathcal{E}_{\\pi}(x))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where LDM(\u00b7) represents the latent diffusion models (including Unet-based or Transformer-based), $\\theta$ denotes the parameters of LDM, and $\\pi$ denotes the parameters of the VQVAE that are frozen to train our Neural-RDM models. ", "page_idx": 16}, {"type": "text", "text": "A.2 Additional Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Motivated by [101], we follow [45] to give a proof: A remarkable property of the SDE (Eq. 5) is the existence of a reverse ODE (PF-ODE [45]), which retain the same marginal probability densities as the SDE. We consider the SDE in Eq. 5, which possesses the following form: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\boldsymbol{z}_{t}=\\mu(\\boldsymbol{z}_{t},t)d t+{\\sigma}(\\boldsymbol{z}_{t},t)d\\mathbf{w}_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\pmb{\\mu}(\\cdot,t):R^{d}\\rightarrow R^{d}$ and $\\pmb{\\sigma}(\\cdot,t):R^{d}\\rightarrow R^{d\\times d}$ . The marginal probability density $p_{t}(\\pmb{z}_{t})$ evolves according to Kolmogorov\u2019s forward equation [102] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial p_{t}(z)}{\\partial t}=-\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}\\big[\\mu_{i}(z_{t},t)p_{t}(z_{t})\\big]+\\frac{1}{2}\\sum_{i=1}^{d}\\sum_{j=1}^{d}\\frac{\\partial^{2}}{\\partial z_{i}\\partial z_{j}}\\bigg[\\displaystyle\\sum_{k=1}^{d}\\sigma_{i k}(z_{t},t)\\sigma_{j k}(z_{t},t)p_{t}(z_{t})\\bigg]\\cdot}\\\\ {\\displaystyle=-\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}\\big[\\mu_{i}(z_{t},t)p_{t}(z_{t})\\big]+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}\\bigg[\\displaystyle\\sum_{j=1}^{d}\\frac{\\partial}{\\partial z_{j}}\\bigg[\\displaystyle\\sum_{k=1}^{d}\\sigma_{i k}(z_{t},t)\\sigma_{j k}(z_{t},t)p_{t}(z_{t})\\bigg]\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since the sub-part of Eq. 24 can be written in the following form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{d}\\frac{\\partial}{\\partial z_{j}}\\Bigl[\\sum_{k=1}^{d}\\sigma_{i k}(z_{t},t)\\sigma_{j k}(z_{t},t)p_{t}(z_{t})\\Bigr]}\\\\ &{=\\displaystyle\\sum_{j=1}^{d}\\frac{\\partial}{\\partial z_{j}}\\Bigl[\\sum_{k=1}^{d}\\sigma_{i k}(z_{t},t)\\sigma_{j k}(z_{t},t)\\Bigr]p_{t}(z_{t})+\\displaystyle\\sum_{j=1}^{d}\\sum_{k=1}^{d}\\sigma_{i k}(z_{t},t)\\sigma_{j k}(z_{t},t)p_{t}(z_{t})\\frac{\\partial}{\\partial z_{j}}\\log p_{t}(z_{t})}\\\\ &{=p_{t}(z_{t})\\nabla\\cdot\\left[\\sigma(z_{t},t)\\sigma^{\\top}(z_{t},t)\\right]+p_{t}(z_{t})\\sigma(z_{t},t)\\sigma^{\\top}(z_{t},t)\\nabla z_{t}\\log p_{t}(z_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus we can obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial p_{\\varepsilon}(z_{t})}{\\partial t}=-\\displaystyle\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}[\\mu_{i}(z_{t},t)p_{\\varepsilon}(z_{t})]+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}\\biggl[\\displaystyle\\sum_{j=1}^{d}\\frac{\\partial}{\\partial z_{j}}\\biggl[\\displaystyle\\sum_{k=1}^{d}\\sigma_{k j}[z_{\\varepsilon},t)\\rho_{\\varepsilon k}(z_{\\varepsilon},t)p_{\\varepsilon}(z_{\\varepsilon})\\biggr]\\biggr]}\\\\ {\\displaystyle}&{=-\\displaystyle\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}[\\mu_{i}(z_{t},t)p_{\\varepsilon}(z_{t})]}\\\\ &{\\displaystyle~~~+\\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}\\biggl[p_{\\varepsilon}(z_{t})\\nabla\\cdot[\\sigma(z_{t},t)]\\sigma^{\\top}(z_{t},t)\\biggr]+p_{\\varepsilon}(z_{t})\\sigma(z_{t},t)\\sigma^{\\top}(z_{t},t)\\nabla_{z_{\\varepsilon}}\\log p_{\\varepsilon}(z_{t})\\biggr]}\\\\ {\\displaystyle}&{=-\\displaystyle\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}\\biggl\\{\\mu_{i}(z_{t},t)p_{\\varepsilon}(z_{t})}\\\\ &{\\displaystyle~~~-\\frac{1}{2}\\biggl[\\nabla\\cdot[\\sigma(z_{t},t)]\\sigma^{\\top}(z_{t},t)]+\\sigma(z_{t},t)\\sigma^{\\top}(z_{t},t)\\nabla_{z_{i}}\\log p_{\\varepsilon}(z_{t})\\biggr\\}}\\\\ &{\\displaystyle=-\\displaystyle\\sum_{i=1}^{d}\\frac{\\partial}{\\partial z_{i}}[\\tilde{\\mu}_{i}(z_{t},t)p_{\\varepsilon}(z_{t})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we define $\\tilde{\\pmb{\\mu}}_{i}(\\cdot)$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\mu}(z_{t},t):=\\mu(z_{t},t)-\\frac{1}{2}\\nabla\\cdot[\\sigma(z_{t},t)\\sigma^{\\top}(z_{t},t)]-\\frac{1}{2}\\sigma(z_{t},t)\\sigma^{\\top}(z_{t},t)\\nabla_{z_{t}}\\log p_{t}(z_{t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining Eq. 26 and Eq. 27, we can conclude that Equation Eq. 26 still describes a Kolmogorov\u2019s forward process but with $\\tilde{{\\boldsymbol{\\sigma}}}(z_{t},t):={\\bf0}$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\boldsymbol{z}_{t}=\\tilde{\\mu}(\\boldsymbol{z}_{t},t)d t+\\tilde{\\sigma}(\\boldsymbol{z}_{t},t)d\\mathbf{w}_{t},\\quad\\tilde{\\sigma}(\\boldsymbol{z}_{t},t)=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Which proves that it is actually an ODE after reverse transformation $\\tilde{\\pmb{\\mu}}(\\cdot)$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\pmb{z}_{t}=\\tilde{\\pmb{\\mu}}(\\boldsymbol{z}_{t},t)d t,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is essentially the same with our Denoising-Diffusion-ODE given by Eq. 6. Therefore, we demonstrate the existence of the reverse ODE and the practicality of parameterizing the mean-variance scheduler from the reverse ODE. ", "page_idx": 17}, {"type": "image", "img_path": "NG16csOmcA/tmp/614891f140287e00e4ed74e6b43473ec6eff2acba8ec0dfe4fa3d431e1e458c9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 8: The samples produced by Neural-RDM (trained on JourneyDB [53]) . ", "page_idx": 18}, {"type": "text", "text": "A.4 Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Limitation Discussion. Although significant improvements have been achieved, Neural-RDM still has some limitations, the most important of which is that the gated residual mechanism only inhibits rather than completely avoids the sensitivity decrease and numerical propagation errors caused by the deepening of the network. If we want to completely avoid it, we may have to give up stacking-based deep network architectures, but that will lead to a significant reduction in performance. Therefore, our method chooses to continue to deepen the stacking of the network and suppress error propagation as much as possible in the trade-off between the two. ", "page_idx": 18}, {"type": "text", "text": "A.5 Social Impact ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Potential Social Implications. We believe that Neural-RDM will bring new thinking about deep network architectures to the generative community, and hopefully promote the generative emergence capabilities of vision generation models in the open source community. In addition, we hope that more researchers can follow the powerful capabilities of residual denoising to build brand new scalable network architectures beyond the realms of well established U-Net and Transformers. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We believe that the main claims made in the abstract and introduction accurately reflect the contributions and scope of the paper, refer to theoretical proof in Sec. 2.1- 2.3 and experimental proof in Sec. 3.2- 3.5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Refer to Sec. A.4 for a discussion of limitations. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided all detailed assumptions and theoretical proofs in the main content and appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We are convinced that we have achieved this and promise to disclose all code details after the paper is accepted. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We are convinced that we have achieved this and promise to disclose all code details after the paper is accepted. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We are convinced that we have achieved this, please refer to Sec. 3.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have presented the results of statistical significance in the main quantitative results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have reported the GPU power used in the experiments in Sec. 3.1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we comply with all NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we have discussed social impacts in Sec. A.5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we discuss it in Social Impact but in reality our approach is largely irrelevant to safety precautions. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we have followed these license specifications and accurately stated contributions from previous work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]