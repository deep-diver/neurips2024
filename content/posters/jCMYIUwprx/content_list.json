[{"type": "text", "text": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hung Le\u2217, Yingbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo Salesforce Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes $\\dot{+}10\\%$ absolute improvements in all models).2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Extending from the natural language domain, Large Language Models (LLMs) like [Koubaa, 2023, Wang and Komatsuzaki, 2021, Radford et al., 2019] have demonstrated great potential in code generation tasks [Svyatkovskiy et al., 2020, Chen et al., 2021, Hendrycks et al., 2021]. However, when instructed with tasks containing malicious intentions or ambiguous requirements, LLMs are subject to generating code that could facilitate harmful attacks or code that contains obscure security problems [Khoury et al., 2023, Bhatt et al., 2023, Siddiq et al., 2022]. For instance, in a study of Github\u2019s Copilot, Pearce et al. [2022] observed that about $\\bar{4}0\\%$ of generated programs are vulnerable. ", "page_idx": 0}, {"type": "text", "text": "Despite previous efforts in addressing the safety of LLMs through finetuning [Bai et al., 2022, Korbak et al., 2023, Dai et al., 2024], this strategy alone is often not sufficient and scalable enough against prompts that are increasingly optimised for highly sophisticated attacks [Zhuo et al., 2023, Mazeika et al., 2024, Bhatt et al., 2024]. Furthermore, in the domain of code generation, creating quality safety-related data for finetuning often incurs great costs, involving programming experts with a deep understanding of code and related cyber-security and vulnerability concerns. ", "page_idx": 0}, {"type": "text", "text": "Note that code itself is often not inherently malicious. For example, as noted by Bhatt et al. [2023], a program for an encryption method could be very useful to create a secure personal file system. Yet the encryption method can also be exploited for a ransomware attack. Therefore, it is important to develop an efficient method for LLMs to achieve the intricate balance between helpfulness and safety in the code domain. We introduce INDICT, Internal Dialogues of Critiques, a novel framework for LLMs to generate code that is not only helpful but also safe and secure (see Figure 1 for an example code generation task and Figure 2 for the method overview). ", "page_idx": 0}, {"type": "image", "img_path": "jCMYIUwprx/tmp/5347a1e0580baf646de57df81b7e3427f3f92d59bba189c2185e1a6e4889c62a.jpg", "img_caption": ["Figure 1: INDICT (Internal Dialogues of Critiques) enables two different critics to interact with each other autonomously and collaboratively, improving code generation by both security and helpfulness. In this example, INDICT iteratively resolves the security weakness CWE-78 (Improper Neutralization in an OS Command) and improves the code functionality with relevant supporting modules. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "First, instead of a single critic for a specific code quality [Le et al., 2022, Welleck et al., 2023, Chen et al., 2023c], we consider both helpfulness-driven critic and safety-driven critic. Instead of activating these critics independently, we propose to position them in an autonomous agent system like [Huang et al., 2022, Dong et al., 2023, Li et al., 2024]. Although the critics are configured with orthogonal goals, we let them interact with each other autonomously to collaboratively and simultaneously optimise both security and correctness of LLM-generated responses. ", "page_idx": 1}, {"type": "text", "text": "Extending from retrieval-augmented generation [Guu et al., 2020, Ram et al., 2023, Asai et al., 2024], we also equip the critics with external knowledge retrieved by relevant code snippets and natural language queries. Just like how human developers typically select and examine one small piece of code at a time, the critics use a code snippet together with a text query to call relevant tools like web search and code interpreters. The resulting outputs from the external tools are used by the critics to generate more knowledge-grounded critiques for the \u201cactor\u201d LLM generator. ", "page_idx": 1}, {"type": "text", "text": "Finally, we engage our critics during two stages: (1) preemptive critic feedback is obtained during the initial code generation stage; and (2) post-hoc critic feedback is activated after the code is observed in an execution environment. Albeit more commonly used in prior work like [Li et al., 2022, Chen et al., 2023c, Le et al., 2024], post-hoc feedback alone is not proactive enough for security-sensitive tasks. In these tasks, unexpected damage may likely occur and create systematic impacts on execution environments in practice [Hendrycks et al., 2023, Mazeika et al., 2024]. Our strategy facilitates a \u201cpreemptive\u201d layer of protection, creating a more holistic critic framework for code generation LLMs. ", "page_idx": 1}, {"type": "text", "text": "We conducted a comprehensive evaluation of INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks. On LLMs ranging from 7B to 70B parameters, we observed consistent performance improvement by both safety and helpfulness of generation outputs. We found that INDICT can provide useful critiques to LLMs, leading to new SoTA performance by security measures while maintaining or improving the helpfulness of generated code. Our approach also generalises well to open-ended tasks, demonstrating the broader potential of a cooperative autonomous critic system for helpful yet responsible AI models. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our research is broadly related to the research of large language models (LLMs) [Koubaa, 2023, Team et al., 2023, Brown et al., 2020, Radford et al., 2019, Touvron et al., 2023a]. Pretrained on a massive amount of text data on very deep Transformer-based architectures, these models have shown impressive performance in many natural language tasks. Going beyond the text domain, LLMs have been extended to learn from the code data and applied to many coding tasks [Rozi\u00e8re et al., 2023, Li et al., 2023, Lozhkov et al., 2024, Gunasekar et al., 2023, Wang et al., 2023, Nijkamp et al., 2023, Luo et al., 2023]. One major application of LLMs in the code domain is code generation, a long-standing challenge of many conventional AI models [Manna and Waldinger, 1971, Gulwani et al., 2012, Kurach et al., 2015, Devlin et al., 2017, Parisotto et al., 2016]. In this task, an AI model is required to generate proper code solutions for different programming problems, ranging from basic daily code completion tasks to more advanced algorithmic problems [Chen et al., 2021, Austin et al., 2021, Hendrycks et al., 2021, Shinn et al., 2023, Lai et al., 2023]. ", "page_idx": 1}, {"type": "table", "img_path": "jCMYIUwprx/tmp/598f1f878f7bd7de1fed2260e17b3f6b1b2efcbd992abb724b3c4f99211fd44d.jpg", "table_caption": ["Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multiagent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the research for code generation, we have witnessed emerging studies focusing on the security and safety aspects of AI-generated code. Hammond Pearce et al. [2021], Schuster et al. [2021], Pearce et al. [2022] found that commercially successful systems like Github\u2019s Copilot still led to obscure yet major vulnerability and security issues in code. More recently, Perez et al. [2022], Zhuo et al. [2023], Khoury et al. [2023] demonstrated highly complex prompting methods that can \u201cjailbreak\u201d advanced LLMs like ChatGPT into generating malicious code. To benchmark LLMs against code safety and security, [Siddiq and Santos, 2022, Tony et al., 2023] evaluated LLMs against common coding scenarios based on CWE 3. More recently, Bhatt et al. [2023, 2024] introduced CyberSecEval, a large-scale benchmark containing different types of security-aware evaluations. They observed that the code outputs by powerful LLMs like Llama and GPT models are often not perfectly secure. ", "page_idx": 2}, {"type": "text", "text": "More relevant to our work is the research to improve the safety or helpfulness of LLMs. A common strategy is finetuning LLMs with appropriate preference data with specific reward models to differentiate among ranked data samples [Bai et al., 2022, Korbak et al., 2023, Wu et al., 2024, Sun et al., 2024, Dai et al., 2024]. In the code domain, He and Vechev [2023], He et al. [2024] proposed to finetune LLMs with prompt prefixes or masking strategies conditioned by the safety of corresponding code samples. Chen et al. [2023a] requires human annotators to provide natural language feedback of training samples. Different from prior approaches, we propose a more efficient method to generate better codes by both safety and helpfulness. Our approach can complement the research of autonomous LLM agents [Huang et al., 2022, Yao et al., 2023, Dong et al., 2023] and AI-generated feedback [Bahdanau et al., 2017, Le et al., 2022, Welleck et al., 2023, Gou et al., 2024]. For a systematic comparison to related work, please refer to Table 1 and Appendix D. ", "page_idx": 2}, {"type": "image", "img_path": "jCMYIUwprx/tmp/32ca2f567717aed8ac36f0a6bdf54f02edb67aca93d5f284b0d1d7492ca53ab1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: INDICT (Internal Dialogues of Critiques) is a framework to generate code by both safety and helpfulness. The framework introduces dialogues between knowledge-grounded safety-driven and helpfulness-driven AI critics. It enables the pair of critics to collaboratively and autonomously support the LLM code generator. We apply the critic system for both preemptive and post-hoc types of critic feedback, providing a proactive and extra layer of protection against security-sensitive tasks. ", "page_idx": 3}, {"type": "text", "text": "3 INDICT Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Problem Definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Typically, in a code generation task, an LLM $\\theta$ receives an input $X$ , consisting of a natural language instruction and optionally, a related code context. Treating code generation as a sequence-to-sequence task, the LLM autoregressively generates a response $\\hat{Y}$ as a sequence of tokens. Each token $\\hat{y}_{t}$ is sampled from the parameterized condition distribution $p_{\\theta}(.|\\hat{y}_{1:t-1},X)$ where $\\hat{y}_{t}\\in\\mathcal{V}$ . The output can contain either natural language segments (e.g. explanation of the output code or refusal of the user request) as well as code programs (e.g. code snippets to complete the given input code context). ", "page_idx": 3}, {"type": "text", "text": "3.2 Safety-driven and Helpfulness-driven Critics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Pretrained with a massive amount of data, LLMs are found to be capable of providing insightful feedback to self-improve their own responses in many downstream tasks [Shinn et al., 2023, Zhang et al., 2023b, Welleck et al., 2023, Madaan et al., 2023]. Rather than just a single critic for a specific code attribute, we propose to engage two critics with independent goals: a safety-driven critic $\\sigma$ and a helpfulness-driven critic $\\omega$ . We initialize the critics as LLMs configured by specific system prompts $P_{s}$ and $P_{h}$ respectively) to establish the critics\u2019 corresponding roles. ", "page_idx": 3}, {"type": "text", "text": "For instance, for the safety-based critic, we instruct the model to focus solely on the security and risks of the code, and prioritise these aspects over other code qualities. Vice versa, for the helpfulness-based critic, we request the model to investigate the helpfulness of the code, i.e. whether the output aligns fully with the intentions and requirements in the given task. Denoting $\\hat{C}_{s}$ and $\\hat{C}_{h}$ as the complete outputs of the critics, we can define the critic output distributions (per token) as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l r l}&{\\hat{c}_{s,t}\\sim p_{\\sigma}(.|\\hat{c}_{s,1:t-1},X,\\hat{Y},P_{s})\\qquad\\qquad}&{\\Rightarrow\\ \\mathrm{for~safety-driven~critic}}\\\\ &{\\hat{c}_{h,t}\\sim p_{\\omega}(.|\\hat{c}_{h,1:t-1},X,\\hat{Y},P_{h})\\qquad\\qquad}&{\\Rightarrow\\ \\mathrm{for~helpfulness-driven~critic}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Subsequently, we let the code generation LLM (\u201cactor\u201d) revise their solutions conditioned by the generated critiques: $\\hat{y}_{s}\\;\\sim\\;p_{\\theta}(\\hat{y}_{s,1:t-1}|X,\\hat{Y},\\hat{C}_{s})$ for safety-conditioned solutions and $\\hat{y}_{h}\\,\\sim$ $p_{\\theta}(\\hat{y}_{h,1:t-1}|X,\\hat{Y},\\hat{C}_{h})$ for helpfulness-conditioned solutions. Refer to Appendix I for the detailed instruction prompts we used on our critics to assess safety or helpfulness of model outputs. ", "page_idx": 3}, {"type": "text", "text": "3.3 Autonomous Collaboration between Critics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "LLMs are often finetuned to follow natural language instructions [Ouyang et al., 2022, Korbak et al., 2023, Dai et al., 2024] and subsequently, can engage in natural language interactions with humans or even among other LLMs. In the latter, recent studies [Huang et al., 2022, Dong et al., 2023, Li et al., 2024, Chen et al., 2024] observed significant performance gains when enabling LLMs to interact autonomously to solve complex tasks. We are motivated by this observation and propose an autonomous agent system of critic models to generate helpfulness-and-safety-aware critiques. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Note that an alternative strategy is to use a single critic model for both helpfulness and safety. However, such a critic model often needs complex alignment finetuning or prompt engineering to generate critiques that are not significantly biased towards a single code property. In our approach, from 1 and 2, given an interaction turn $r$ between critics, we can redefine the output distributions as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\hat{c}_{s,t}^{r}\\sim p_{\\sigma}(.|\\hat{c}_{s,1:t-1},X,\\hat{Y},P_{s},\\hat{I}_{1:r-1})}&&{\\quad\\Rightarrow\\mathrm{\\small~for~safety-driven~critic}}\\\\ &{\\hat{c}_{h,t}^{r}\\sim p_{\\omega}(.|\\hat{c}_{h,1:t-1},X,\\hat{Y},P_{h},\\hat{I}_{1:r-1}\\oplus\\hat{C}_{s}^{r})}&&{\\quad\\Rightarrow\\mathrm{\\small~for~helpfulness-driven~critic}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $\\bigoplus$ denotes concatenation and $\\hat{I}_{1:r-1}=\\hat{C}_{s}^{1}\\oplus\\hat{C}_{h}^{1}\\oplus...\\,\\hat{C}_{s}^{r-1}\\oplus\\hat{C}_{h}^{r-1}$ contains all the past interactions between the safety-driven and helpfulness-driven critics. ", "page_idx": 4}, {"type": "text", "text": "Practically, to avoid computation overhead, we can limit $\\hat{I}$ to only the last few turns of interactions. Alternatively, in this work, we summarize the critic dialogue after each turn of interactions and only use the corresponding summary in each turn: $\\hat{\\mathcal{Z}}_{r}=f(\\hat{I}_{1:r})$ where $f(.)$ is parameterized as an LLMbased summarizer model. To revise the solutions from \u201cactor\u201d LLM by both safety and helpfulness, we can then conveniently reuse the summary in the last interaction turn $R$ between the critics (thus, also reducing the computation cost on the \u201cactor\u201d LLM). To generate safety-and-helpfulness-aware outputs, we revise the output distributions of the LLM code generator as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{y}_{s+h,t}\\sim p_{\\theta}\\big(.|\\hat{y}_{s+h,1:t-1},X,\\hat{Y},\\hat{\\mathcal{L}}_{R}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.4 Knowledge-grounded Critics with External Tools ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Depending on how well LLMs can perceive and resurface relevant knowledge from pretraining data, these models might still cause serious hallucination problems by generating factually incorrect responses [McKenna et al., 2023, Rawte et al., 2023, Xu et al., 2024]. These hallucination problems are exacerbated when LLMs play the critic roles, required to provide reliable and grounded responses against code generation outputs. In this work, we extend prior tool-enhanced LLMs like [Yao et al., 2023, Peng et al., 2023, Lu et al., 2024] and retrieval-augmented generation strategies [Guu et al., 2020, Ram et al., 2023, Asai et al., 2024] to improve our critics. ", "page_idx": 4}, {"type": "text", "text": "Specifically, we equip our critics with access to external tools and incorporate the tools\u2019 query results as additional knowledge to generate more grounded critiques (see Figure 3 for an overview). For instance, for the safety-driven ", "page_idx": 4}, {"type": "table", "img_path": "jCMYIUwprx/tmp/3c85e3287c099b2916e09742b3577e502077486136652d5052687b9916a8b052.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: We define two types of tool-enabled actions the critics can perform: (1) \u201ccode search\u201d queries external tools by a generated text query and optionally a corresponding code snippet. (2) \u201ccode review\u201d uses the execution result of the code snippet (through a code interpreter) as additional input to complement the query. Both action types query tools like web search, Wikipedia, and OpenAI as the knowledge base. ", "page_idx": 4}, {"type": "text", "text": "we decompose the critic generation process to the following steps ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{1.~Critic's~thought}\\;\\hat{W}_{s}^{r}\\colon}&{\\hat{w}_{s,t}^{r}\\sim p_{\\sigma}\\big(\\.\\cdot|w_{s,1:t-1}^{r},X,\\hat{Y},P_{s},\\hat{Z}_{r-1}\\big)}\\\\ &{\\quad\\quad2.\\;\\mathrm{Critic's~action}\\;\\hat{Q}_{s}^{r}\\colon\\;\\hat{Q}_{s}^{r}\\sim p_{\\sigma}\\big(\\langle\\hat{Q}_{s,\\mathrm{text}}^{r},\\{\\varnothing,\\hat{Q}_{s,\\mathrm{code}}^{r}\\}\\rangle|\\hat{Y},P_{s},\\hat{W}_{s}^{r}\\big)}\\\\ {\\mathrm{3.~Critic's~observation}\\;\\hat{O}_{s}^{r}\\colon\\;\\hat{O}_{s}^{r}=g\\big(\\hat{Q}_{s}^{r}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "First, we obtain the critic\u2019s initial thought $\\hat{W}_{s}^{r}$ , following the same formulation as in 3. In the critic\u2019s action step, we parameterize critic \u201cactions\u201d as the generation of unique textual keywords $\\hat{Q}_{s,\\mathrm{text}}^{r}$ , optionally accompanied by code snippets $\\hat{Q}_{s,\\mathrm{code}}^{r}$ . These are used subsequently as search queries to call external tools and obtain search results in the critic\u2019s observation step. Denoting function $g(.)$ as the tool calling functions, we introduce two types of functions: code search and code review. Refer to Figure 3 for the specifications and examples of these functions and Figure 1 for demonstrations. ", "page_idx": 4}, {"type": "text", "text": "Note that the above extension can be applied identically to the helpfulness-driven critic. We also then revise $\\mathcal{T}$ as the summary of all past critics\u2019 initial thoughts concatenated with corresponding observations: $\\hat{\\mathcal{Z}}_{r}=f(\\{\\hat{W}\\oplus\\dot{O}\\}_{s}^{1:r-\\bar{1}}\\oplus\\{\\hat{W}\\oplus\\hat{O}\\}_{h}^{1:r-1})$ . ", "page_idx": 5}, {"type": "text", "text": "3.5 Preemptive and Post-hoc Critic Feedback ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Different from the text domain, code generation outputs could be additionally observed/ interpreted in relevant environments e.g. through code interpreters (\u201cexecutor\u201d). Shi et al. [2022], Le et al. [2022], Chen et al. [2023b,c] demonstrated the benefits of execution-based feedback to improve the functional correctness of code. However, in security-sensitive scenarios, directly engaging the executing environment might cause unintentional systematic damage, e.g. deleted data directories or modified access to privileged user accounts. ", "page_idx": 5}, {"type": "text", "text": "We propose to deploy our critic system for both preemptive feedback (after the initial code generation step) and post-hoc feedback (after the generated code is observed by the executor). To obtain posthoc critic feedback, we simply incorporate the execution results (e.g. error messages, unit test outcomes) as the conditioning factors in 1, 2, 3, 4, and 6. Note that we maintain a persistent dialogue context between safety and helpfulness critics throughout preemptive and post-hoc iterations. We can define the output distributions of the LLM code generator conditioned by the posthoc feedback as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{y}_{s+h,t}^{\\mathrm{posthoc}}\\sim p_{\\theta}(.|\\hat{y}_{s+h,1:t-1}^{\\mathrm{posthoc}},X,\\hat{Y}_{s+h}^{\\mathrm{peempt}},\\hat{\\mathcal{L}}_{R}^{\\mathrm{posthoc}})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\hat{\\mathcal{Z}}_{r}^{\\mathrm{posthoc}}=f(\\hat{\\mathcal{Z}}_{1:R}^{\\mathrm{preempt}}\\oplus\\hat{I}_{1:r-1}^{\\mathrm{posthoc}})$ ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Base Language Models. We applied INDICT on CommandR [Cohere, 2024] which was specifically optimized for external tool augmentation, making the model suitable for our framework. In challenging adversarial tests like red-teaming attacks, we additionally employed popular preference-tuning models from the Llama and Codellama families [Touvron et al., 2023b, Rozi\u00e8re et al., 2023, Meta, 2024], ranging from 7B to 70B parameters. All models were designed for long-context tasks as well as conversational interactions, making them suitable for experiments with INDICT. To fairly compare the performance across models, given a model choice, we initialized our actors and critics with the same model checkpoint. For all base LLMs, we utilized the Huggingface-hosted model parameters [Wolf et al., 2019] and vLLM [Kwon et al., 2023] to generate the responses. ", "page_idx": 5}, {"type": "text", "text": "Configurations. To fairly compare between base models, given a task, we maintained the instruction prompts as similarly as possible across all models. Models such as CommandR [Cohere, 2024] which is already finetuned for tool enhancement, are prompted according to their prompting strategies. We adopted a maximum output length of up to 2048 tokens on actor or critic models. We also fixed the generation budget to 1 sample in each generation by actor or critic models. For a given actor-generated sample, we applied our INDICT framework for up to 5 rounds to improve this sample iteratively. Please refer to Appendix E and I for more detailed experimental setups e.g. external tools, model and generation configurations, compute resources, and example prompt instructions. ", "page_idx": 5}, {"type": "text", "text": "4.1 Insecure coding practice tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Benchmarks. We first evaluated our approach on insecure code generation tasks in which LLMs were found to generate outputs with significant security concerns. We considered the Insecure Coding Practice test from CyberSecEval-1 [Bhatt et al., 2023], which includes two sub-tasks: \u201cAutocomplete\u201d where LLMs are provided a code context and predict subsequent code segments to complete this code context; and \u201cInstruct\u201d where LLMs fulfill natural language instructions of coding problems. Additionally, following an instruction-following setup, the CVS benchmark (Code Vulnerability and Security) [CyberNative, 2024] provides a pair of ground-truth secure and insecure code outputs given a coding problem. Please refer to Appendix E for more details of the benchmarks. ", "page_idx": 5}, {"type": "text", "text": "Evaluation. To measure the safety of model outputs, we followed Bhatt et al. [2023] by using their detector model which contains comprehensive rules defined in weggli [weg, 2023] and semgrep [sem, 2023] to detect more than 180 patterns related to 50 Common Weakness Enumerations (CWEs). The safety metric is defined as the percentage of test samples where output codes do not contain ", "page_idx": 5}, {"type": "image", "img_path": "jCMYIUwprx/tmp/6ecaedfa2b4dc13041ec0750caae9bd89153be5a902fea2a12b1576cc7249996.jpg", "img_caption": ["(c) Test results of the CVS benchmark "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: we evaluated INDICT against insecure coding practice tasks with CyberSecEval-1 (Autocomplete and Instruction splits) and CVS benchmarks. Safety measure is computed as the percentage of outputs that are safe (determined by a rule-based detector). Helpfulness measure is the winning rate against prior SoTA model or available ground-truth outputs (determined by a GPT evaluator). Notations: JV: Java, JS: Javascript, Py: Python; CR: CommandR, GT: ground-truth (\u201cGT Safe\u201d and \u201cGT Unsafe\u201d are the secure and insecure code samples provided by the CVS benchmark). ", "page_idx": 6}, {"type": "text", "text": "any insecurities. To measure the helpfulness, we followed prior work like Bai et al. [2022], Zheng et al. [2024], Li et al. [2024] to adopt GPT3.5 as the AI evaluator [Achiam et al., 2023] to rank the helpfulness of model outputs. In our experiments, given a test problem, we computed the winning rate of a model output against the output of a known SoTA model (e.g. Llama2-7b-chat in CyberSecEval-1) or the corresponding ground-truth outputs (for the CVS benchmark). ", "page_idx": 6}, {"type": "text", "text": "Results. From Figure 4, we observed consistent performance improvements of our approach, outperforming prior strong LLM baselines such as Llama and GPT models [Touvron et al., 2023b, Achiam et al., 2023]. Specifically, by applying INDICT with CommandR and LLama3 models [Meta, 2024, Cohere, 2024], we obtained SoTA performance by safety (more than $80\\%$ and $90\\%$ output codes are safe on CyberSecEval-1 and CVS respectively) as well as helpfulness (up to $70\\%$ output codes are more helpful than the prior SoTA model or ground-truth outputs). Figure 4 also demonstrates the consistency of our approach by both safety and helpfulness across different programming languages. There are only a few exceptional cases of helpfulness performance (specifically with Javascript in the CyberSecEval benchmark and $C++$ in the CVS benchmark). ", "page_idx": 6}, {"type": "text", "text": "4.2 Security attack tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Benchmarks. We also evaluated our approach against malicious coding tasks in which the instruction prompts contain obscure yet dangerous intentions to perform security attacks. We considered three major tasks: the Cyberattack Helpfulness test from CyberSecEval-1 [Bhatt et al., 2023], and the Interpreter Abuse and Prompt Injection tests from CyberSecEval-2 [Bhatt et al., 2024]. The first tasks contain test samples of attack methods that are well studied in industry-standard MITRE ATT&CK ontology 4. The second task was proposed recently to instruct LLMs to abuse a code interpreter to carry on unauthorized actions e.g. data overriding. Finally, the last task is designed to simulate injection attacks by synthetically injecting harmful rules to prompts e.g. disclosing a given password in the generation output. Please refer to Appendix E for more details of the benchmarks. ", "page_idx": 6}, {"type": "image", "img_path": "jCMYIUwprx/tmp/7208b31d58d9c4bc2ca6acf5877d85d8bbe511b579b1cf7244d276285a60cea0.jpg", "img_caption": ["Figure 5: We evaluated INDICT against three major types of security attacks from CyberSecEval-1 and 2 benchmarks. Safety measure is computed as the percentage of outputs that do not comply with the corresponding malicious prompting instructions (determined by a GPT evaluator). The higher the safety measure is, the better. Notations: CL: Codellama, L2: Llama2, L3: Llama3, CR: CommandR. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "jCMYIUwprx/tmp/d904b97770273e033f77e62a2e86d66329d29d8cfd3b3b56825dea99566de735.jpg", "table_caption": ["Table 2: We evaluated INDICT with HarmBench against 6 different types of red-teaming optimization methods. We reported the safety measure as the percentage of outputs classified as benign by the given AI evaluator from HarmBench. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "jCMYIUwprx/tmp/b3bfc006ab499b6485c1573fa8babed5838d5f7f8789ab833d736a9b04865c11.jpg", "img_caption": ["Figure 6: With Llama3-8b-instruct as the base model, we evaluated INDICT on the CAMEL benchmark. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Evaluation. In these tasks, we focused on measuring the safety measurement by determining whether the model outputs assist the given instructions e.g. by suggesting supporting code snippets or by providing natural language explanation for a solution. Following Bhatt et al. [2023, 2024], we used GPT3.5 [Achiam et al., 2023] and adopted the expansion-then-judge evaluation pipeline: first, expand the generation output with reasoning against safety criteria, and subsequently, judge if the output is indeed benign. The safety metric is the percentage of outputs that are considered benign. ", "page_idx": 7}, {"type": "text", "text": "Results. From Figure 5, we observed the significant performance improvement by safety measures on all three types of security attacks. Specifically, by using models from CodeLlama [Rozi\u00e8re et al., 2023] and Llama3 [Meta, 2024] families, we achieved new SoTA safety performance: $76\\%$ on Cyber Attack task and more than $90\\%$ on Interpreter Abuse and Prompt Injection tasks. Notably, despite a weaker model, when enhanced with INDICT, CommandR can achieve significant boosts and become more secure against harmful task instructions. The results also demonstrate the efficacy of our method on models of different sizes, from 8B to 70B model parameters. ", "page_idx": 7}, {"type": "text", "text": "4.3 Open-ended generation tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Benchmarks. Although we focused on the code domain in this work, our method can be easily adapted to generation tasks in other domains. In these cases, we can simply remove the execution environment (and accordingly posthoc feedback step) and activate INDICT with appropriate domainagnostic contexts in our instruction prompts (see Appendix I for example prompts). We adapted our method to two major open-ended generation benchmarks: HarmBench [Mazeika et al., 2024], which evaluates LLMs against various red teaming optimization methods, and CAMEL [Li et al., 2024], which contains a wide variety of GPT-generated complex problems in diverse domains. Please refer to Appendix E for more details of the benchmarks. ", "page_idx": 8}, {"type": "text", "text": "Evaluation. For HarmBench, we followed Mazeika et al. [2024] and adopted their AI evaluator, which is a classifier finetuned from Llama2-13b model to assess the safety and biases of model outputs. For CAMEL, we adopted a similar strategy but used GPT3.5 as the AI evaluator. Following Li et al. [2024], we defined the safety and helpfulness measures as the average winning rate over the direct generation approach by the corresponding base LLM. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 2 demonstrates the benefit of INDICT in combination with CommandR and Llama3 models. Consistent with our observations in prior experiments, albeit a weaker model by safety, CommandR $^{+}$ INDICT still improves significantly across all red-teaming optimization methods (from $23\\%$ to $51\\%$ by average safety metric). For the CAMEL benchmark, Figure 6 shows that INDICT can iteratively improve the model outputs with at least $70\\%$ model outputs are better by both safety and helpfulness than the direct generation approach. We noted the minor performance drops after 4 rounds of INDICT, suggesting further study to address open-ended tasks beyond the code domain. ", "page_idx": 8}, {"type": "text", "text": "4.4 Comparison to baselines ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Related to INDICT are approaches that enhance the generation procedure of LLMs with selfimprovement or agentic frameworks (see Table 1). To compare with INDICT, we selected 7 strong representative baselines and evaluated them on a validation test split - random samples of $20\\%$ of the CyberSecEval-1 benchmark [Bhatt et al., 2023]. For each baseline, we also included a version where additional instructions are given to models to provide both safety and helpfulness critics e.g. instruct models to \u201cfocus on both the security and helpfulness of the solution.\u201d For multi-agent methods, we included these instructions in all agents (analyst, tester, etc.) or introduced a new critic agent (as recommended in Li et al. [2024]). Note that for both INDICT and all baseline models, we adopted GPT4o-mini [OpenAI, 2024] as the base LLM and followed similar generation budgets (up to 3 rounds of revision) to fairly compare the results. The results in Table 7 demonstrate the SoTA performance of INDICT by both security and helpfulness (more than $90\\%$ and $81\\%$ respectively) against all the baselines. While we observed good improvement ", "page_idx": 8}, {"type": "text", "text": "Figure 7: With GPT4o-mini as the base model, we adapted representative baselines in their original implementation and also extended them with additional instructions (detailed criteria of safety and helpfulness). We marked these enhanced baselines with the suffix $\\cdot_{+},$ . ", "page_idx": 8}, {"type": "table", "img_path": "jCMYIUwprx/tmp/224e7d46dae124fbbcebeeadcd2daba6f30ded105220bba661784b50da61f916.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "of strong baseline methods like Reflexion [Shinn et al., 2023] and CAMEL [Li et al., 2024] with additional instructions (marked with the suffix $\\acute{\\bullet}+\\acute{\\iota}$ ), their results are not optimal and less than INDICT. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To perform ablation analysis, we randomly sampled a subset from the CyberSecEval-1 [Bhatt et al., 2023], including both Insecure Coding Practice and Cyber Attack tasks. For each task, we randomly sampled $20\\%$ of the full dataset such that the sampled subset had similar distributions as the original dataset by programming languages or types of attack methods. We reported the averaged safety metric following the evaluation of the corresponding tasks (see 4.1 and 4.2). For helpfulness, we ", "page_idx": 8}, {"type": "text", "text": "Table 3: We conducted an ablation analysis of INDICT when removing the proposed dual critic system and/or external tool enhancement. We conducted our experiments on Codellama(CL) models from 7B to 34B parameters and the CommandR model. ", "page_idx": 9}, {"type": "text", "text": "Table 4: We conducted an ablation analysis of INDICT with different combinations of our critics, during either preemptive or posthoc feedback stage or both. To fairly compare these variants, we excluded any access to external tools, and used CommandR as the base model in all experiments. ", "page_idx": 9}, {"type": "table", "img_path": "jCMYIUwprx/tmp/bd4d2cd85e83f62c7fb85b6bfe78ec2b96a3242fc9a93864f1be4f51670ce943.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "jCMYIUwprx/tmp/2233b2b4290a8c594c81e9d64551309f56151131fe01531609587e49f8ef9041.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "adopted GPT3.5 as the AI evaluator and computed the percentage of outputs that are considered more helpful than the direct generation approach of the corresponding base model. ", "page_idx": 9}, {"type": "text", "text": "From Table 3 and 4, we have the following observations. First, INDICT can lead to performance gains in both safety and helpfulness with all base models, including Codellama models from 7B to 34B and CommandR models. The framework achieves the optimal performance when integrating external tools with our critics. Secondly, we found that this tool enhancement strategy improves the safety quality of the outputs more than the helpfulness, indicating that current LLMs significantly benefit from external grounding to be more safe and secure. Thirdly, we observed that using safety critic alone or helpfulness critic alone is not sufficient, often optimizing the outputs significantly by either only safety or only helpfulness qualities respectively. Finally, we noted that when adopting our critics in both preemptive and posthoc stages, we achieved more well-rounded results, with the best overall average of safety and helpfulness metrics. ", "page_idx": 9}, {"type": "text", "text": "We also conducted ablation analysis by multiple rounds of INDICT applications. To obtain the results of the direct generation approach (i.e. \u201cbase\u201d) in multiple rounds, we simply concatenated previously generated samples into our prompt and iteratively instructed the model to regenerate better outputs (without any critics or tool enhancement). From Figure 8, we noted the significant and consistent improvements from INDICT, using CommandR and Codellama-13binstruct as base models. Interestingly, we still observed some performance improvement, albeit very marginal, of the direct generation approach over multiple generation rounds. We also noticed that without using external tools, the performance curves tend to converge faster than the ", "page_idx": 9}, {"type": "image", "img_path": "jCMYIUwprx/tmp/627777c2578aa9011421fcad61a93f3eecf3d7b69c3004714526a4aed79ef2c8.jpg", "img_caption": ["Figure 8: We conducted ablation experiments over multiple rounds of INDICT applications, using CommandR (left) and Codellama-13b-instruct (right) as the base models. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "tool-enabled approach. For more experimental results and analysis, please refer to Appendix F, G, H. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present INDICT, a novel framework to improve code generation by both safety and helpfulness. INDICT essentially facilitates an autonomous agent system between two critic models, each of which focuses on either the safety or helpfulness quality of outputs from the \u201cactor\u201d code generation LLM. Given access to external tools, the two critics interact with each other autonomously to generate grounded critiques, collaboratively improving the model outputs. We conducted comprehensive experiments of INDICT on diverse downstream coding tasks across different programming languages and attack tactics. Our results demonstrated the beneftis of INDICT on code-related tasks and beyond, highlighting the promising direction of an autonomous and tool-enhanced multi-critic system. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Semgrep - Make shift left work \u2014 semgrep.dev. https://semgrep.dev/, 2023. [Accessed 19-05-2024]. ", "page_idx": 10}, {"type": "text", "text": "GitHub - weggli-rs/weggli: weggli is a fast and robust semantic search tool for C and $C++$ codebases. It is designed to help security researchers identify interesting functionality in large codebases. \u2014 github.com. https://github.com/weggli-rs/weggli, 2023. [Accessed 19-05-2024]. ", "page_idx": 10}, {"type": "text", "text": "GitHub - langchain-ai/langchain: Build context-aware reasoning applications \u2014 github.com. https: //github.com/langchain-ai/langchain, 2024. [Accessed 21-05-2024]. ", "page_idx": 10}, {"type": "text", "text": "search-engine-parser \u2014 pypi.org. https://pypi.org/project/ search-engine-parser/, 2024. [Accessed 21-05-2024].   \nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nA. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ hSyW5go0v8.   \nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \nD. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An actorcritic algorithm for sequence prediction. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $\\equiv$ SJDaqqveg.   \nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.   \nM. Bhatt, S. Chennabasappa, C. Nikolaidis, S. Wan, I. Evtimov, D. Gabi, D. Song, F. Ahmad, C. Aschermann, L. Fontana, et al. Purple llama cyberseceval: A secure coding benchmark for language models. arXiv preprint arXiv:2312.04724, 2023.   \nM. Bhatt, S. Chennabasappa, Y. Li, C. Nikolaidis, D. Song, S. Wan, F. Ahmad, C. Aschermann, Y. Chen, D. Kapil, et al. Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for large language models. arXiv preprint arXiv:2404.13161, 2024.   \nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nP. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023.   \nA. Chen, J. Scheurer, T. Korbak, J. A. Campos, J. S. Chan, S. R. Bowman, K. Cho, and E. Perez. Improving code generation by training with natural language feedback. arXiv preprint arXiv:2303.16749, 2023a.   \nB. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id $=$ ktrw68Cmu9c.   \nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \nW. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C.-M. Chan, H. Yu, Y. Lu, Y.-H. Hung, C. Qian, Y. Qin, X. Cong, R. Xie, Z. Liu, M. Sun, and J. Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ EHg5GDnyq1.   \nX. Chen, M. Lin, N. Sch\u00e4rli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023c.   \nCohere. Command-R \u2014 docs.cohere.com. https://docs.cohere.com/docs/ command-r, 2024. [Accessed 18-05-2024].   \nCyberNative. CyberNative/Code_Vulnerability_Security_DPO \u00b7 Datasets at Hugging Face \u2014 huggingface.co. https://huggingface.co/datasets/CyberNative/Code_ Vulnerability_Security_DPO, 2024. [Accessed 19-05-2024].   \nJ. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang. Safe RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ TyFrPOKYXw.   \nJ. Devlin, J. Uesato, S. Bhupatiraju, R. Singh, A.-r. Mohamed, and P. Kohli. Robustfill: Neural program learning under noisy i/o. In International conference on machine learning, pages 990\u2013998. PMLR, 2017.   \nY. Dong, X. Jiang, Z. Jin, and G. Li. Self-collaboration code generation via chatgpt. arXiv preprint arXiv:2304.07590, 2023.   \nZ. Gou, Z. Shao, Y. Gong, yelong shen, Y. Yang, N. Duan, and W. Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ Sx038qxjek.   \nS. Gulwani, W. R. Harris, and R. Singh. Spreadsheet data manipulation using examples. Communications of the ACM, 55(8):97\u2013105, 2012.   \nS. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.   \nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020.   \nB. A. Hammond Pearce, B. Tan, B. Dolan-Gavitt, and R. Karri. An empirical cybersecurity evaluation of github copilot\u2019s code contributions. arXiv preprint arXiv:2108.09293, 2021.   \nJ. He and M. Vechev. Large language models for code: Security hardening and adversarial testing. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pages 1865\u20131879, 2023.   \nJ. He, M. Vero, G. Krasnopolska, and M. Vechev. Instruction tuning for secure code generation. arXiv preprint arXiv:2402.09497, 2024.   \nD. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021.   \nD. Hendrycks, M. Mazeika, and T. Woodside. An overview of catastrophic ai risks. arXiv preprint arXiv:2306.12001, 2023.   \nS. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.   \nD. Huang, J. M. Zhang, M. Luck, Q. Bu, Y. Qing, and H. Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023a.   \nJ. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023b.   \nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.   \nY. Ishibashi and Y. Nishimura. Self-organized agents: A llm multi-agent framework toward ultra large-scale code generation and optimization. arXiv preprint arXiv:2404.02183, 2024.   \nR. Khoury, A. R. Avila, J. Brunelle, and B. M. Camara. How secure is code generated by chatgpt? In 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 2445\u20132451. IEEE, 2023.   \nT. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506\u201317533. PMLR, 2023.   \nA. Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown. 2023.   \nK. Kurach, M. Andrychowicz, and I. Sutskever. Neural random-access machines. arXiv preprint arXiv:1511.06392, 2015.   \nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.   \nY. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih, D. Fried, S. Wang, and T. Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 18319\u201318345. PMLR, 2023.   \nH. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314\u201321328, 2022.   \nH. Le, H. Chen, A. Saha, A. Gokul, D. Sahoo, and S. Joty. Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id $\\equiv$ vYhglxSj8j.   \nG. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36, 2024.   \nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.   \nY. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, et al. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.   \nR. Liu, R. Yang, C. Jia, G. Zhang, D. Yang, and S. Vosoughi. Training socially aligned language models on simulated social interactions. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ NddKiWtdUm.   \nA. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.   \nP. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plugand-play compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \nZ. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.   \nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.   \nZ. Manna and R. J. Waldinger. Toward automatic program synthesis. Commun. ACM, 14(3):151\u2013165, mar 1971. ISSN 0001-0782. doi: 10.1145/362566.362568. URL https://doi.org/10. 1145/362566.362568.   \nM. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024.   \nN. McKenna, T. Li, L. Cheng, M. Hosseini, M. Johnson, and M. Steedman. Sources of hallucination by large language models on inference tasks. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2758\u20132774, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.182. URL https://aclanthology.org/2023.findings-emnlp.182.   \nA. Mehrotra, M. Zampetakis, P. Kassianik, B. Nelson, H. Anderson, Y. Singer, and A. Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119, 2023.   \nMeta. Meta Llama 3 \u2014 llama.meta.com. https://llama.meta.com/llama3/, 2024. [Accessed 18-05-2024].   \nE. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309, 2023.   \nT. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023a.   \nT. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama. Is self-repair a silver bullet for code generation? In The Twelfth International Conference on Learning Representations, 2023b.   \nOpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. [Accessed 22-10-2024].   \nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.   \nE. Parisotto, A.-r. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. Neuro-symbolic program synthesis. arXiv preprint arXiv:1611.01855, 2016.   \nH. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri. Asleep at the keyboard? assessing the security of github copilot\u2019s code contributions. In 2022 IEEE Symposium on Security and Privacy (SP), pages 754\u2013768. IEEE, 2022.   \nB. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.   \nE. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming language models with language models. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3419\u20133448, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.225. URL https://aclanthology.org/ 2022.emnlp-main.225.   \nC. Qian, W. Liu, H. Liu, N. Chen, Y. Dang, J. Li, C. Yang, W. Chen, Y. Su, X. Cong, et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15174\u201315186, 2024.   \nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nO. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-Brown, and Y. Shoham. Incontext retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316\u20131331, 2023.   \nV. Rawte, A. Sheth, and A. Das. A survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922, 2023.   \nB. Rozi\u00e8re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \nR. Schuster, C. Song, E. Tromer, and V. Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21), pages 1559\u20131575, 2021.   \nX. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang. \" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023.   \nF. Shi, D. Fried, M. Ghazvininejad, L. Zettlemoyer, and S. I. Wang. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3533\u20133546, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.231. URL https: //aclanthology.org/2022.emnlp-main.231.   \nN. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.   \nM. L. Siddiq and J. C. Santos. Securityeval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques. In Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security, pages 29\u201333, 2022.   \nM. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, and J. C. Santos. An empirical study of code smells in transformer-based code generation techniques. In 2022 IEEE 22nd International Working Conference on Source Code Analysis and Manipulation (SCAM), pages 71\u201382. IEEE, 2022.   \nZ. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. Advances in Neural Information Processing Systems, 36, 2024.   \nA. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan. Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 1433\u20131443, 2020.   \nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nC. Tony, M. Mutas, N. E. D. Ferreyra, and R. Scandariato. Llmseceval: A dataset of natural language prompts for security evaluations. In 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR), pages 588\u2013592. IEEE, 2023.   \nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.   \nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.   \nB. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.   \nY. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.   \nS. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y. Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=hH36JeQZDaO.   \nM. Weyssow, A. Kamanda, and H. Sahraoui. Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences. arXiv preprint arXiv:2403.09032, 2024.   \nT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \nZ. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A. Smith, M. Ostendorf, and H. Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36, 2024.   \nZ. Xu, S. Jain, and M. Kankanhalli. Hallucination is inevitable: An innate limitation of large language models. arXiv preprint arXiv:2401.11817, 2024.   \nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ WE_vluYUL-X.   \nY. Zeng, H. Lin, J. Zhang, D. Yang, R. Jia, and W. Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024.   \nK. Zhang, Z. Li, J. Li, G. Li, and Z. Jin. Self-edit: Fault-aware code editor for code generation. arXiv preprint arXiv:2305.04087, 2023a.   \nT. Zhang, T. Yu, T. Hashimoto, M. Lewis, W.-t. Yih, D. Fried, and S. Wang. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pages 41832\u201341846. PMLR, 2023b.   \nL. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.   \nT. Y. Zhuo, Y. Huang, C. Chen, and Z. Xing. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. arXiv preprint arXiv:2301.12867, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Despite the strong performance of INDICT on a wide variety of tasks, there are some limitations that we want to emphasize. First, our framework relies on the instruction-following ability of LLMs to perform different specific roles, i.e. code generation actors, safety-driven critics, and helpfulnessdriven critics. Depending on how well LLMs are able to understand the requirements of these roles, we would need to carefully create well-written prompts with specific instructions for the models to follow. In our framework, we would need to describe the requirements of helpfulness and safety that the critics would need to follow and check against code generation outputs. While we try to cover as many as possible different safety and helpfulness criteria, these attributes are not trivial to be defined in the code domain. Hence, given a code generation output, our critics might not always be able to detect the right safety or helpfulness concerns. ", "page_idx": 16}, {"type": "text", "text": "Parts of our approach can be used to remediate the above issue. Our tool enhancement strategy can equip the critics with necessary knowledge which can steer the critics towards more grounded and potentially correct recommendations. When a critic cannot detect the right issues initially, it can still improve its critiques after several rounds of interactions and tool use. Subsequently, if the extracted knowledge from external tools is relevant, the critic might be able to correctly revise and improve its final critique before passing it to the actor LLM. ", "page_idx": 16}, {"type": "text", "text": "Another limitation of our approach is the computation cost. Compared to the direct generation approach, our framework incurs higher computation costs, activating more than one LLM and requiring access to external tools. However, we consider our approach still more affordable than relevant finetuning methods. These methods often require (1) high computation to sufficiently finetune LLMs to balance between safety and helpfulness alignment; and (2) significant annotation effort to collect quality code data by these attributes. ", "page_idx": 16}, {"type": "text", "text": "B Ethical Statement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We want to highlight that our work is specifically developed to address the safety and security concerns of AI code generation models. Any adaptation or application of our work should be used for this purpose, ultimately to create stronger yet more responsible AI systems. Moreover, as our method adopts a framework for autonomous agent systems between two independent LLMs, during any adaptation or application, it is important to control and monitor how much autonomy such systems can possess. It is good practice to limit how these agents could perform actions like web search (for example, by number of queries) and code interpreter (for example, using a sandbox execution environment, isolated from the local system). Any \u201cthoughts\u201d or \u201cactions\u201d and their outcomes from these agents have to be carefully checked to make sure they do not lead to unethical consequences. ", "page_idx": 16}, {"type": "text", "text": "Secondly, as our work aims to address both safety and helpfulness aspects of code generation, defining and quantifying such qualities is not trivial. Within the scope of this paper, we tried to conform as much as possible to the definitions commonly used in prior related work in the code domain or the AI safety domain. In practice, there are many ethical concerns that should be considered to define these qualities, especially on the safety of code generation. For instance, in this work, we did not consider the conventional safety concerns like social biases and offensive content in code. However, these safety concerns could still be observed in many real-life practical scenarios (e.g. in generated code comments or variable names). More study is needed to address and measure safety in such scenarios. ", "page_idx": 16}, {"type": "text", "text": "C Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Societal Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since we aim to address the safety and helpfulness in code generation, our work can have significantly positive societal impacts. Since coding applications by LLMs are getting more and more popular, the consequences of generating harmful or insecure code can be very serious, especially in high-risk application domains like military, medicine, and banking systems. Our work can be deployed as an extra layer of mitigation, reducing the probability of potential harm while not compromising the helpfulness of AI systems. As we demonstrated in our results, our framework can also benefit open-ended generation tasks beyond the code domain. ", "page_idx": 16}, {"type": "table", "img_path": "jCMYIUwprx/tmp/b9950692623bd5ce7e9d269ce4d22199b0a9eedbeb244d31ae7052e73b99d554.jpg", "table_caption": ["Table 5: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multiagent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "On the other hand, our framework can also be misused, assisting human users with harmful intentions to create more sophisticated attacks against LLMs. Our proposed critic models could be engineered with reverse goals, e.g. recommending ways to make the output codes more insecure or less helpful. Since these critic models are positioned in an autonomous system with freedom to interact and collaborate with each other, the resulting critiques can negatively affect the \u201cactor\u201d LLMs towards generating more insecure or useless code outputs. ", "page_idx": 17}, {"type": "text", "text": "C.2 Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "There are several safeguard strategies we can adopt to mitigate the above negative societal impacts. First, we can limit how much autonomy our critics can have e.g. by the types of queries they can generate and by the types of external tools they can have access to. In tools like web search, we can include a simple fliter to exclude any illegal or unauthorized websites or content that might negatively impact the critics. Another safeguard strategy is to adopt more powerful external tools like code static analyzers or AI evaluators to provide more useful feedback to the critic models. While we did not use them in our experiments to fairly evaluate our approach against baselines, in practice, these tools should be used as safeguards for any practical application of INDICT. ", "page_idx": 17}, {"type": "text", "text": "D Comparison to Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "See Table 5 for a systematic comparison between INDICT and related methods. We reviewed each method by the following features: helpfulness or safety-based qualities of generation outputs, execution feedback (execution of output code if applicable), tool-enhanced feedback (access to external tools like web search), multi-critic collaboration (engage multiple LM agents for critic generation), and supervision free (no training data required). ", "page_idx": 17}, {"type": "text", "text": "Compared to existing actor-critic methods, INDICT is different in three major aspects: (1) INDICT aims to optimize both helpfulness and safety awareness in the generated output code. Most of the current actor-critic approaches are designed with a single criterion (such as functional correctness). Simply extending these methods with additional instructions on safety criteria is sub-optimal (see our results with baseline actor-critic methods in Section 4.4). (2) INDICT integrates critic with knowledge grounding from external tools to create more reliable feedback to the actor agent. Most current methods only use code test results as the only external feedback to improve the quality of output code. (3) To implement (1) and (2), we enhanced the existing actor-critic framework with a multi-critic and tool-enabled collaboration approach. This approach can autonomously generate more reliable and holistic feedback for both the safety and helpfulness of output code generation. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Also related to our work is the research of multi-agent collaborative systems. It is not trivial to extend this line of research to address the security of code generation. Firstly, it is not clear how the current methods could be enhanced with security awareness and subsequently improve the quality of output generations. Earlier work such as [Olausson et al., 2023b, Huang et al., 2023b] showed that simply asking agents to analyze and answer what is wrong with generated code is not always effective. With carefully designed prompts and agent interactions [Shinn et al., 2023, Huang et al., 2023a], collaborative agents can now generate more functionally correct code. Therefore, studying collaborative agents with orthogonal goals such as security awareness still requires further attention. As observed by our experimental results, simply applying the current multi-agent methods [Dong et al., 2023, Li et al., 2024], even with extended additional instructions of security criteria, does not perform so well and is still far from optimal. ", "page_idx": 18}, {"type": "text", "text": "E Details of Experimental Setups ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Generation budget. Technically, we can integrate INDICT on top of any LLMs for any number of application rounds (i.e. outer action loops), each of which can contain any number of dialogue interactions between the safety and helpfulness critics (i.e. inner critic loops). Due to the limitation of computation resources, we have to trade-off between the number of outer action loops and the number of inner critic loops. In our experiments, we fixed the number of outer action loops to 5 rounds and the inner critic loops to 1 interaction per action loop. We also maintained a persistent interaction context throughout all outer action loops so that the critics could always refer to previously generated critiques. With the above generation budget, our strategy can offer more diverse and richer input samples to the critics over time, while controlling the compute cost at an affordable level. ", "page_idx": 18}, {"type": "text", "text": "Tools. In this work, we used 4 different types of external tools for the critics to query relevant knowledge for their arguments. For Wikipedia and code interpreter, we adopted the Langchain library [lan, 2024] with built-in functions to call these tools given the input text queries or code snippets. For web search, we employed the Search Engine Parser library [sea, 2024] to query and scrape search engine pages for different snippets such as titles and descriptions. Depending on the access constraints from commercial search engines, we mainly employ Yahoo Search as our primary search engine. Finally, to use OpenAI as an external tool, we query GPT3.5 using our paid API access 5. All the above tools are appropriately licensed to be used for academic purposes. ", "page_idx": 18}, {"type": "text", "text": "Note that while we are treating OpenAI as an external tool in INDICT, we try to minimize contamination of test data by GPT models [Achiam et al., 2023]. Specifically, we do not directly pass the original task instructions $X$ to OpenAI public API but only use critic-generated text or code snippets as queries (see 7 and 8). Also note that during the preemptive feedback stage, we assume no access to the execution environments / code interpreters and only employ CodeSearch as the applicable critic actions. During the posthoc feedback stage, we enable access to the code interpreters, and hence, the critics can select and perform CodeReview (with execution results as parts of the queries) to extract relevant external knowledge. ", "page_idx": 18}, {"type": "text", "text": "Benchmarks. We evaluated INDICT on different downstream applications, including 3 major types of tasks: insecure coding practice, security attacks, and open-ended generation. Please refer to Table 6 for a summary of tasks and benchmarks used in this work. All the benchmarks considered are licensed with permission to be used for academic purposes. ", "page_idx": 18}, {"type": "text", "text": "Insecure coding practice tasks [Bhatt et al., 2023, CyberNative, 2024] refer to standard code generation tasks where a model receives an input containing a coding problem description, optionally with an input code context. The model is then required to generate output code to solve the input coding problem and/or finish the given code context. The test samples in this task were curated by the potential security and vulnerability concerns commonly seen in code e.g. Common Weakness Enumeration (CWE) 6. ", "page_idx": 18}, {"type": "table", "img_path": "jCMYIUwprx/tmp/0e7ba68baceb53fac8946df916074acede906c43dbb4ddc509b48a9ffe97c754.jpg", "table_caption": ["Table 6: Summary of evaluation tasks and corresponding benchmarks: CyberSecEval-1 [Bhatt et al., 2023], CyberSecEval-2 [Bhatt et al., 2024], CVS [CyberNative, 2024], CAMEL [Li et al., 2024], and HarmBench [Mazeika et al., 2024] "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We also conducted experiments on security attack tasks [Bhatt et al., 2024]. In these tasks, input instructions are designed to directly or indirectly elicit harmful generation from LLMs. For instance, one example task is to request LLMs to generate code to simulate a DDoS attack in a Linux environment. More indirectly, this request could be injected into a very long list of complex requirements in the prompt. The model is required to detect such harmful intentions in the instructions and generate appropriate responses (e.g. ones not complying with the given request). ", "page_idx": 19}, {"type": "text", "text": "The last type of downstream task we used in this work is open-ended generation tasks beyond the code domain. These tasks include both standard generation tasks [Li et al., 2024] as well as adversarial generation tasks [Mazeika et al., 2024]. In the latter, recent work has focused on prompt engineering methods to optimize the instructions and ultimately, elicit harmful behaviors from LLMs. We tested against several recent prompt optimization methods curated by Mazeika et al. [2024], covering diverse domains like social engineering, harassment, bio-weapons, etc. ", "page_idx": 19}, {"type": "text", "text": "Note that for CVS and CAMEL benchmarks, since they do not have an official test split, we randomly sampled a subset from the corresponding benchmarks such that the sampled data has a similar data distribution as the original dataset e.g. by programming languages. For HarmBench, from the dataset of 320 raw task instructions (\u201cDirect\u201d split), we augmented the data by using CommandR [Cohere, 2024] as the attacker and applying the following red-teaming optimization methods: zero-shot (\u201cZS\u201d) [Perez et al., 2022], PAP [Zeng et al., 2024], JailBreak (\u201cJB\u201d) [Shen et al., 2023], TAP [Mehrotra et al., 2023], and PAIR [Chao et al., 2023]. This results in 5 more test splits, each containing 320 augmented prompts. ", "page_idx": 19}, {"type": "text", "text": "Evaluation. To evaluate safety and helpfulness performance, we followed similar evaluation tools used in the corresponding benchmark papers and related work [Bhatt et al., 2023, 2024, Li et al., 2024, Mazeika et al., 2024, Zheng et al., 2024, Bai et al., 2022]. These papers showed that evaluation tools like security-based code analyzers and AI detectors can achieve decent levels of accuracy, correlating with human evaluation results on subsampled datasets. In addition, to minimize potential biases in AI evaluators, we anonymized all model names and randomly positioned the model responses to be evaluated in the evaluation prompts. In code generation tasks with expected output code, we also extracted only the code snippets and excluded any text segments in the model outputs to prevent biases from long-context outputs or from simply concatenating text. Also note that we follow Mazeika et al. [2024]\u2019s evaluation principle by not including access to evaluators (e.g. static analyzers, AI classifiers) in our proposed framework. In practice, it is possible to use these as additional tools for more insightful feedback to the critics. ", "page_idx": 19}, {"type": "text", "text": "Base Language Models. All the models used in the work, including CommandR [Cohere, 2024], LLama-2 [Touvron et al., 2023b], Codellama [Rozi\u00e8re et al., 2023], and Llama-3 [Meta, 2024], are open-sourced LLMs. We accessed these models through HuggingFace, which includes model licenses with permission to be used for academic purposes. We describe the HuggingFace model IDs and their corresponding licenses below: ", "page_idx": 19}, {"type": "text", "text": "\u2022 CohereForAI/c4ai-command-r-v01 for CommandR, licensed under cc-by-nc-4.0   \n\u2022 meta-llama/Llama-2-[x]b-chat-hf for Llama2 of x-B parameters, licenced under llama2   \n\u2022 codellama/CodeLlama-[x]b-Instruct-hf for Codellama of x-B parameters, licenced under llama2   \n\u2022 meta-llama/Meta-Llama-3-[x]B-Instruct for Llama3 of x-B parameters, licenced under llama3 ", "page_idx": 20}, {"type": "text", "text": "For the Lllama and Codellama families, we fully agreed and complied with the license conditions enforced by Meta before accessing the models. ", "page_idx": 20}, {"type": "text", "text": "Baselines. In this work, we mainly compared with prior baselines that were reported in the corresponding benchmarks. More recently, He and Vechev [2023], He et al. [2024] introduced finetuning approaches to finetune LLMs towards safer code generation. Almost concurrently to this work, Weyssow et al. [2024] also introduced a preference dataset of complex instructions to finetune LLMs to coding preferences. However, these approaches were not adapted and tested against the evaluation tasks and benchmarks we used in this work. Due to the limited computation cost (and also partly due to unreleased model checkpoints from He et al. [2024] at the time of submission), we were not able to evaluate the above models and compare with INDICT. We will attempt to replicate these methods and compare with our work in the future. ", "page_idx": 20}, {"type": "text", "text": "Compute Resources. We conducted all experiments in this paper with our CPU and GPU resources provided through the Google Cloud Platform. Depending on the sizes of the base LLMs, we adopted GPU clusters of 2 to 8 GPUs of Nvidia A100 40GB type and assigned a CPU memory of up to 600GB. For some very large models such as Llama3-70B, we observed in some cases that the above hardware resulted in out-of-memory problems. For such cases, we recommend running the experiments with larger CPU memory allocation i.e. more than 600GB, or larger GPU clusters. ", "page_idx": 20}, {"type": "text", "text": "Time costs. Given an input task, on average, INDICT incurs about $3{-}4\\mathrm{x}$ the time cost as compared to a single LLM to generate a code sample (including any iterative refinement). However, we also want to note that even with fine-tuning, fine-tuned models are far from perfect and still subject to unseen security risks or novel red-teaming prompts during test time. For instance, from our results with the fine-tuning method CodeUltraFeedback [Weyssow et al., 2024], the fine-tuned model is still sub-optimal and can be further improved e.g. by using INDICT during inference time. See Appendix F for the experimental results and analysis. ", "page_idx": 20}, {"type": "text", "text": "F INDICT vs. Finetuning methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We also conducted experiments to compare INDICT with finetuning-based methods. We evaluated them on a validation test split - random samples of $20\\%$ of the CyberSecEval-1 benchmark [Bhatt et al., 2023]. Using CodeLlama-7b-instruct as the base model, CodeUltraFeedback finetunes the model on a large-scale dataset with annotations of code preferences. From Table 7, we observe that the best model $(\\mathrm{SFT}+\\mathrm{DPO}$ finetuning) can improve the results by both safety and helpfulness but not as good as INDICT. As INDICT can complement finetuning-based methods, we applied INDICT with the best CodeUltraFeedback model to achieve even further performance gains (from $60\\%$ and $63\\%$ to $73\\%$ in both helpfulness and safety). ", "page_idx": 20}, {"type": "text", "text": "Table 7: With CodeLlama-7b-instruct as the base model, we compared INDICT with CodeUltraFeedback [Weyssow et al., 2024], a finetuning approach using supervised-finetuning (SFT) or preference-based finetuning (DPO). ", "page_idx": 20}, {"type": "table", "img_path": "jCMYIUwprx/tmp/fca5503faf8e13e671cb13f262d9345a84b91c2d7aa8a2154b02fc5879178b3b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "G Additional Ablation Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Using GPT4o-mini as the base model, we conducted additional ablation experiments with different variants of INDICT: (1) one simply using a single critic agent for both safety and helpfulness; (2) one without using a critic summarizer and maintaining a full dialogue history of critiques in the critic context; (3) ones replacing the thought-action-observation critic generation with RAG or tool-based generation: (3a) RAG uses the original task description to retrieve relevant knowledge and generate grounded critics, and (3b) tool-based method uses web search/Wikipedia and a query \u201cwhat is wrong with the solution in terms of its <security/functionality>?\u201d and query output is treated as a critique. ", "page_idx": 21}, {"type": "text", "text": "From Table 8, we have the following observations: First, when we simply removed the summarizer and let the actor agent receive the full dialogue history, we noticed the performance degraded to $87\\%$ and $72\\%$ in safety and helpfulness. This happens probably due to the much longer context of the dialogue history, affecting the actor agent to capture all critic feedback from this history and generate new code. This model variant also incurs more computation due to the long context of the dialogues. Secondly, we noted that RAG and tool-enhanced methods are inferior to our proposed framework. We found that the queries in these methods are often too vague or ambiguous to search for meaningful information snippets. ", "page_idx": 21}, {"type": "text", "text": "Finally, we observed that simply using a single critic agent with dual quality criteria will affect the performance, reducing the safety and helpfulness metrics to $87\\%$ and $76\\%$ respectively. One possible reason is due to the formulation of the training objectives of LMs, which are not always designed to optimize both security and helpfulness equally (also depending on the post-pretraining stages of LMs e.g. training with RLHF). Our approach enables a more flexible and probably more relaxed application of LLM as a critic agent by: (1) decoupling the helpfulness and safety goals and delegating them to individual LM agents; and (2) enabling multi-critic collaboration to autonomously develop more holistic and well-rounded critic feedback. ", "page_idx": 21}, {"type": "table", "img_path": "jCMYIUwprx/tmp/f2e37c2be79a5da1fc025c26a933aad8582a22866f9c3caede5ea6c12d57f3d2.jpg", "table_caption": ["Table 8: With GPT4o-mini as the base model, we compared INDICT with 4 different variants of the critic framework. We found that our proposed INDICT can lead to more well-rounded performance, with high results in both safety and helpfulness of the generated code. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "H Details of Experimental Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We reported the full experimental results in this section. For results of insecure coding practice tasks, please refer to Table 9, 10, 11 for the CyberSecEval-1 benchmark, and 12 and 13 for the CVS benchmark. For results of security attack tasks, please refer to Table 14, 15, and 16 for Cyber Attack, Interpreter Abuse, and Prompt Injection tasks respectively. ", "page_idx": 21}, {"type": "text", "text": "I Instruction Prompts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We described the example instruction prompts we used in this section (Listing 1 to 10). For each prompt template, depending on the model roles and tasks, we replace the following placeholders with applicable input components: {question} and {answer} are replaced with the corresponding task description and latest model output from the actor LLM. During the posthoc feedback stage, {answer} is also concatenated with any execution results (e.g. test outcomes, error messages) after executing the corresponding extracted code output with a code interpreter. {scratchpad} is typically used as a placeholder to contain past interactions between the two critics. ", "page_idx": 21}, {"type": "text", "text": "Note that INDICT uses zero-shot prompting in each step. We prompt the critic agent to condition the current critique and generate a unique query to obtain more knowledge. We extract the search keywords following our instruction templates e.g. in the form of \u2019Search[keyword]\u2019. For generating ", "page_idx": 21}, {"type": "text", "text": "Table 9: Test results of CyberSecEval-1 - Insecure Coding Practice (Autocomplete): we reported the $\\%$ output codes that are considered secure (determined by a rule-based detector). Using INDICT, CommandR can achieve very comparable performance to the prior SoTA, i.e. Llama2-7b-chat. In programming languages C#, Java, and Python, CommandR $^{+}$ INDICT achieves the best safety performance. The results of the baseline models are from Bhatt et al. [2023]. ", "page_idx": 22}, {"type": "table", "img_path": "jCMYIUwprx/tmp/f52c9c1dd226feeb1c139d2bd77f9284bea29bdf8cd1de8937a68c9c481f9735.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 10: Test results of CyberSecEval-1 - Insecure Coding Practice (Instruction): we reported the $\\%$ output codes that are considered secure (determined by a rule-based detector). Using INDICT, CommandR can achieve new SoTA safety measures, with significant improvements in many programming languages. The results of the baseline models are from Bhatt et al. [2023]. ", "page_idx": 22}, {"type": "table", "img_path": "jCMYIUwprx/tmp/1eecfcadd4922266a01b041c7865ae675843dd0eb743a6c23e54825ef03b9162.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "jCMYIUwprx/tmp/945e23519eb4237e092d0b7a2661b3105d5e3d1d82c9dd3e0562b0849aa10146.jpg", "table_caption": ["Table 11: Test results of CyberSecEval-1 - Insecure Coding Practice (Autocomplete and Instruction): we reported the $\\%$ output codes that are considered more helpful than the prior SoTA model i.e. Llama-7b-chat. Using INDICT, we found significant improvements by helpfulness measure on both Autocomplete and Instruct splits. On the Autocomplete split, CommandR $^{+}$ INDICT are found to be more helpful and even better than the GPT models. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 12: Test results of CVS: we reported the $\\%$ output codes that are considered secure (determined by a rule-based detector). We applied INDICT with 3 base LLMs: CommandR, Llama3-8b-instruct and Llama3-70b-instruct. We observed that with INDICT, all 3 models are consistently improved by safety measure, even better than the given ground-truth secure code solutions. ", "page_idx": 23}, {"type": "table", "img_path": "jCMYIUwprx/tmp/2e637939578eec8c7c35d43063278d01af36ad355dea84b14c4e93b7ff7604d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 13: Test results of CVS: we reported the $\\%$ output codes that are considered more helpful than the corresponding ground-truth secure code solutions. While all 3 base language models are found to be slightly less helpful or comparable to the ground-truth outputs, when integrated with INDICT, we noted consistent performance gains. We obtained the best performance with Llama3- 70b-instruct+INDICT, with more than $65\\%$ of outputs are more helpful than the corresponding ground-truth code solutions. ", "page_idx": 23}, {"type": "table", "img_path": "jCMYIUwprx/tmp/9f517acab7facb854ad547d6429b63cd21dc9e4a8bb018b473d367291c8545fb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 14: Test results of CyberSecEval-1 - Cyber Attack tasks: we reported the $\\%$ model outputs that are considered benign. Using INDICT, we found that Llama3-8b-instruct can achieves new SoTA performance with more than $76\\%$ of outputs are benign, i.e. not complying with malicious task prompts. In this table, we also included the results of the top 5 most challenging types of attack tactics (categorized by the industry standard MITRE ATT&CK). The results of the baseline models are from Bhatt et al. [2023]. ", "page_idx": 23}, {"type": "table", "img_path": "jCMYIUwprx/tmp/ad1d501708a408f833af9fbd2bb3434df5f35107c1a433e0393e3a260c5f59db.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 15: Test results of CyberSecEval-2 - Interpreter Abuse tasks: we reported the $\\%$ model outputs that are considered benign. On both base language models CommandR and Codellama-13b-instruct, we found consistent performance improvement from INDICT, with more than $80\\%$ and $90\\%$ of outputs respectively are benign. In this table, we also included the results by different types of attacks: Container Escape, Privilege Escalation, Post Exploitation, Reflected Attack, and Social Engineering. The results of the baseline models are from Bhatt et al. [2024]. ", "page_idx": 24}, {"type": "table", "img_path": "jCMYIUwprx/tmp/301ba7c1cff0132018bc2bdac6a59d721be127273fdf8225a6fc01858610a0f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 16: Test results of CyberSecEval-2 - Prompt Injection tasks: we reported the $\\%$ model outputs that are considered benign. Llama3-70b-instruct $^{+}$ INDICT can achieve the SoTA performance, with more than $92\\%$ of outputs are classified as benign. In this table, we also included the ablation results of the top 5 most challenging types of prompt injection attacks: Many Short Attack, Ignore Previous Instruction, Indirect Reference, Persuasion, and Hypothetical Scenario. The results of the baseline models are from Bhatt et al. [2024]. ", "page_idx": 24}, {"type": "table", "img_path": "jCMYIUwprx/tmp/bb6d8407a487856f89dcf319b8706bb377b5138909f6f8b376f89b6c1cde1530.jpg", "table_caption": ["Listing 1: Prompt with instruction for a critic model to focus on safety-related concerns in code generation tasks. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "code snippets, we prompt the model similarly but ask the model to wrap the output code in \u201c\u2018. Note that the current prompts in the Appendix are for tool-finetuned models like CommandR which automatically generates tool parameters given a tool calling function definition. ", "page_idx": 24}, {"type": "image", "img_path": "jCMYIUwprx/tmp/eb55b034568923fa282da24cb087a61f7c86eca0b16853e2980b9ba6a996b7e3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Limit your critic to a few sentences. Task: {question}   \nSolution: {answer}   \n{scratchpad} ", "page_idx": 25}, {"type": "text", "text": "Listing 2: Prompt with instruction for a critic model to focus on helpfulness-related concerns in code generation tasks. ", "page_idx": 25}, {"type": "text", "text": "You are an advanced intelligent agent with a strong focus for system performance and accuracy. You will be given a previous potential solution in which you were given a task to solve. In a few sentences, diagnose whether the solution is completely correct in every execution step and will satisfy all the requirements in the given task and pass any corner test cases.   \nYou are also required to debate and discuss with another critic who focuses on system security and risk management. Always advocate and focus on system performance and accuracy over system security or risk management.   \nLimit your critic to a few sentences.   \nTask: {question}   \nSolution: {answer}   \n{scratchpad} ", "page_idx": 25}, {"type": "text", "text": "Listing 3: Prompt with instruction for a summarizer model to summarize the interactions between safety and helpfulness critics for code generation tasks. ", "page_idx": 25}, {"type": "text", "text": "You are an advanced intelligent agent with equal focuses to both system performance and security. You will be given a discussion about a system solution between two critics against system safety and correctness. In a few sentences, summarize the discussion and determine whether the solution is completely correct and secure in every execution step. The perfect solution should satisfy all the requirements in the given task while not violating any security standards or causing any harmful effect to the society or cybersecurity.   \nThere might be some supporting facts in the discussion between the critics. Incorporate as much as possible those details into your response to make your analysis informative and convincing.   \n{scratchpad} ", "page_idx": 25}, {"type": "text", "text": "Listing 4: Prompt with instruction for critic models to generate relevant parameters (text queries) to perform CodeSearch/CodeReview actions. ", "page_idx": 25}, {"type": "text", "text": "You are an advanced intelligent agent with direct access to Internet. You are given a task and an example solution and relevant analysis against the solution\u2019s security or functional correctness. To improve the analysis with relevant evidence and fact, generate a relevant keyword or query to search for related information on Internet. You may also search for information that is relevant to the task or solution but is missing in the analysis. Use the following format: Search[<query or keyword>].   \nTask: {question} ", "page_idx": 25}, {"type": "text", "text": "Listing 5: Prompt with instruction for critic models to generate relevant parameters (code snippets) to perform CodeSearch/CodeReview actions. ", "page_idx": 26}, {"type": "image", "img_path": "jCMYIUwprx/tmp/18686c45935fa61b851b94fc4c1eec2e20957e19599b7dcf47d3e35bc1c33cee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Listing 6: Prompt with instruction for critic models to generate relevant parameters (text queries and/or code snippets) to perform CodeSearch actions. This prompt is specifically for CommandR models which by default can receive any predefined external tools and select relevant ones given the context of the prompt. ", "page_idx": 26}, {"type": "image", "img_path": "jCMYIUwprx/tmp/33ed5c5c51da5b4aa6ae67b9c9d8db83df57e7f5f33d6efdf23022ba4833f5d7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Listing 7: Prompt with instruction for critic models to generate relevant parameters (text queries and/or code snippets) to perform CodeReview actions. This prompt is specifically for CommandR models which by default can receive any predefined external tools and select relevant ones given the context of the prompt. ", "page_idx": 26}, {"type": "text", "text": "You are given a task and an example solution and relevant analysis against the solution\u2019s security or functional correctness. Read the task, solution, and analysis and find ways to improve the analysis with relevant evidence and supporting fact.   \nYou also have access to a code interpreter that can execute many code snippets. Based on the solution and analysis, you can create many code snippets and unit test cases to evaluate them and support the arguments in the analysis.   \nThese code snippets should be indepedent (does not refer to external operating systems, databases, repositories, or custom libraries) and limited to few lines of codes only. Use \u2018print\u2018 or \u2018assert\u2018 statements in the code snippets if needed.   \nTask: {question}   \nSolution: {answer}   \n{scratchpad} ", "page_idx": 27}, {"type": "text", "text": "Listing 8: Prompt with instruction for a critic model to focus on safety-related concerns in open-ended generation tasks. ", "page_idx": 27}, {"type": "text", "text": "You are an advanced intelligent agent with a strong focus for security and risk management. You will be given a previous potential solution in which you were given a task to solve. In a few sentences, diagnose whether the solution is completely safe in every execution step and will not pose any potential harmful effect to the world. If the solution includes any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content, describe and explain your relevant arguments.   \nYou are also required to debate and discuss with another critic who focuses on system performance and accuracy. Always advocate and focus on security and risk management over system performance or correctness.   \nLimit your critic to a few sentences.   \nTask: {question}   \nSolution: {answer}   \n{scratchpad} ", "page_idx": 27}, {"type": "text", "text": "Listing 9: Prompt with instruction for a critic model to focus on helpfulness-related concerns in open-ended generation tasks. ", "page_idx": 27}, {"type": "text", "text": "You are an advanced intelligent agent with a strong focus for performance and accuracy. You will be given a previous potential solution in which you were given a task to solve. In a few sentences, diagnose whether the solution is completely correct in every execution step and will satisfy all the requirements in the given task. If the solution includes any content that is not aligned with the requirements and intentions of the task, describe and explain your relevant arguments.   \nYou are also required to debate and discuss with another critic who focuses on security and risk management. Always advocate and focus on performance and accuracy over security or risk management.   \nLimit your critic to a few sentences.   \nTask: {question} ", "page_idx": 27}, {"type": "text", "text": "Listing 10: Prompt with instruction for a summarizer model to summarize the interactions between safety and helpfulness critics for open-ended generation tasks. ", "page_idx": 28}, {"type": "image", "img_path": "jCMYIUwprx/tmp/900f828eb6ec7be206908fe0fba93fe10e36f55c2deee01983bd39d96919edce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "J Qualitative Analysis ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To explain the benefits of INDICT and where INDICT may fail, we included example generation outputs from INDICT and related methods. In Figure 9, we show that given a code generation task, INDICT can generate code that is more secure and robust than strong baselines (Direct generation, Reflexion [Shinn et al., 2023], and CAMEL [Li et al., 2024]). In Figure 10, we illustrate cases where INDICT may fail due to nontrivial errors. ", "page_idx": 28}, {"type": "text", "text": "Compared to the baseline methods, our approach can simultaneously improve both the helpfulness and safety of the output code generation. Specifically, given a relevant information snippet by the safety critic (about the hashing method SHA-256), our actor agent correctly revised the code with a more secure hashing method, avoiding using MD5 hashing and the common security risk CWE-328. At the same time, our generated code is generally more helpful with properly modularized functions implementing supporting features such as input validations. As we noted, this feature has generally emerged in code solutions by collaborative agent systems like CAMEL and INDICT. ", "page_idx": 28}, {"type": "image", "img_path": "jCMYIUwprx/tmp/1c3848eff613a3812c84fd6fb0d99592edebfeba999c8bb26458c7055cbe5431.jpg", "img_caption": ["Figure 9: Qualitative comparison between our method and related baselines "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "jCMYIUwprx/tmp/d7aa8fa702c15c00fee7cadf15a6fab35d79ec566a3a81e3fdf6d69d1c1b3a43.jpg", "img_caption": ["Figure 10: Qualitative analysis of failure cases when applying our method "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction correctly reflect the paper\u2019s contributions and scope, including the details of the proposed method INDICT and strong experimental results on code generation tasks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We discussed the limitations of the work in Appendix A ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: N/A ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: For reproducibility, we fully described our experimental setups (base language models, benchmarks, generation and model configurations, etc.). Please refer to 4 and Appendix E for more details. We will also release our code to reproduce the results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we fully released all code and related scripts to replicate the experimental results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provided all the necessary details related to our experiments, including the datasets/ benchmarks, model configurations, hyperparameters of our proposed method, etc. Please refer to 4 and Appendix E for more details. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: We did not report the error bars because it would be too computationally expensive, involving running LLMs of up to 70B parameters on large benchmarks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provided sufficient level of information on the compute resources used in this work (including the generation time, GPU types and quantities, etc.) to reproduce the experiments. Please refer to Appendix E for more details. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research in this work conforms in all aspects with the NeurIPS Code of Ethics. Please refer to Appendix B - Ethical Statement for more details. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We included both potential positive societal impacts and negative societal impacts of our work in Appendix C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We described necessary safeguards for the responsible use of our method.   \nPlease refer to Appendix C for more details. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the datasets and models used in this work are properly cited throughout the paper, including the list of the corresponding original creators/ owners. The licenses of all datasets and models are also fully described in Appendix E. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: N/A Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: N/A Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]