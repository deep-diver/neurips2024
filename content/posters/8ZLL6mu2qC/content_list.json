[{"type": "text", "text": "Optimal and Approximate Adaptive Stochastic Quantization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ran Ben Basat Yaniv Ben-Itzhak Michael Mitzenmacher Shay Vargaftik UCL VMware Research Harvard University VMware Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quantization is a fundamental optimization for many machine learning (ML) use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is adaptive, where the error is minimized with respect to a given input rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements. ", "page_idx": 0}, {"type": "text", "text": "We revisit the Adaptive Stochastic Quantization (ASQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexities. Our experiments indicate that our algorithms may open the door to using ASQ more extensively in a variety of ML applications. We also present an even faster approximation algorithm for quantizing large inputs on the fly. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quantization is central to optimizing a large range of machine learning (ML) applications. It is often used for compressing gradients to reduce network requirements in distributed and federated learning (e.g., [1, 2, 3, 4, 5, 6]); for quantization of datasets for faster training and inference (e.g., [7]); and for reducing the memory footprint while accelerating the computation for large models\u2019 inference via post-training quantization (e.g., [8, 9]) and quantization-aware training (e.g., [10, 11]) of model weights, activations and key-value (KV) caches [12]. ", "page_idx": 0}, {"type": "text", "text": "A fundamental quantization method is stochastic quantization, where one quantizes an input vector $X\\,\\in\\,\\mathbb{R}^{d}$ to $\\widehat{X}\\ \\in\\ Q^{d}$ using a set $Q\\ \\subset\\ \\mathbb{R}$ of $|Q|\\,=\\,s$ quantization values so that each entry is unbiased [13 ]. That is, each $x\\in X$ is (randomly) quantized to a value $\\widehat{x}\\in Q$ such that $\\mathbb{E}\\left[\\widehat{x}\\right]=x$ . ", "page_idx": 0}, {"type": "text", "text": "Previous unbiased quantization works considered different approaches. Some are distributionagnostic, i.e., design the quantization without optimizing it for the specific input. For example, [1, 14, 15] set quantization values with respect to global properties such as the vector\u2019s norm, or minimum and maximum values. ", "page_idx": 0}, {"type": "text", "text": "Other works, e.g., [1, 3, 4, 16, 17, 18, 19], optimize for the worst case $X$ by applying a reversible transformation (e.g., the randomized Hadamard transform) before quantization that conve\u221arts it into a vector $X^{\\prime}$ with a controlled distribution (e.g., with $\\operatorname*{max}(X^{\\prime})-\\operatorname*{min}(X^{\\prime})=\\tilde{O}(\\|X\\|_{2}/\\sqrt{d}))$ . The decoder then applies the inverse transformation on the quantized $X^{\\prime}$ to obtain an estimate of $X$ . ", "page_idx": 0}, {"type": "text", "text": "In contrast, some solutions use the fact that, in many cases, the inputs to be quantized have a significant structure that can be leveraged to reduce the quantization error. For example, DNN gradients (which are often compressed in distributed and federated learning applications to reduce bandwidth [20, 21]) were observed to follow LogNormal-like [22] or Normal-like [23, 24] distributions. As another example, the distribution of deep activation layers appears to follow a sub-Weibull distribution [25]. ", "page_idx": 0}, {"type": "text", "text": "To alleviate the need to assume an input distribution, the Adaptive Stochastic Quantization (ASQ) problem (e.g., [26, 27, 28]) considers selecting $Q$ adaptively, i.e., with respect to the specific input $X$ , that minimizes the mean squared error (MSE, also known as the sum of variances) given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert{\\widehat{X}}-X\\right\\rVert_{2}^{2}\\right]=\\sum_{x\\in X}\\operatorname{Var}[{\\widehat{x}}]~,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where ${\\widehat{X}}=\\{{\\widehat{x}}\\mid x\\in X\\}$ is the vector of quantized values. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, known ASQ solutions are not practical for the large-size vectors that commonly appear in ML applications. One aspect of the problem\u2019s difficulty is that it is known to be non-convex even for $s=4$ (two-bit quantization) [28], which excludes many natural solution methods such as gradient descent. ZipML [26] approaches the challenge using a dynamic programming approach that allows one to optimize $Q$ in polynomial time. However, this solution has a significant overhead and solving the problem optimally is often considered to be impractical; for example, [28] states ", "page_idx": 1}, {"type": "text", "text": "\u201cTo find the optimal sequence of quantization values, a dynamic program is solved whose computational and memory cost is quadratic ... For this reason, ZipML is impractical for quantizing on the fly\u201d. ", "page_idx": 1}, {"type": "text", "text": "As another evidence of the problem\u2019s hardness, previous work [27] solves the problem only for a given (Weibull) distribution, writing that ", "page_idx": 1}, {"type": "text", "text": "\u201cThe empirical distribution is usually non-differentiable, making the searching of $Q$ infeasible\u201d. ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, there is significant interest in advancing ASQ solutions towards wider adoption as even approximate adaptive solutions like ALQ [28] have been shown to have lower MSE than advanced distribution-agnostic methods such Non-Uniform QSGD (NUQSGD) [29]. ASQ methods can also improve more complex schemes (e.g., including the aforementioned that utilize worst-case to average-case transformations) by replacing distribution-agnostic quantization with an adaptive one. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we show that one can, in fact, solve the ASQ problem optimally and efficiently. To this end, we introduce QUIVER, an algorithm that features novel acceleration methods and leverages the structure of the underlying problem to reduce the runtime complexity from $O(s\\cdot d^{2})$ to $O(s\\cdot{\\bar{d}})$ and the space complexity from $\\dot{O}(d^{2})$ to $O(s\\cdot d)$ . ", "page_idx": 1}, {"type": "text", "text": "This improvement arises from the observation that the optimal solution, for given input parameters $s,d$ , can be efficiently derived from the solutions for $\\left\\{s\\stackrel{-}{-}1,d^{\\prime}\\mid d^{\\prime}\\in\\{2,3,\\stackrel{-}{\\dots},d\\}\\right\\}$ by a reduction to the problem of finding the row maximas in an implicitly defined totally monotone matrix. This problem is known to have fast algorithms assuming that, for any $1\\leq k\\leq j\\leq d$ , the sum of variances of points $\\{x_{k},\\ldots,x_{j}\\}$ can be computed in constant time when quantized to $\\{x_{k},x_{j}\\}$ , a property that is achieved by our new preprocessing method. ", "page_idx": 1}, {"type": "text", "text": "We then further accelerate QUIVER by deriving a closed-form solution for $s=3$ . In turn, this yields a faster solution for any $s$ , by a variant of QUIVER that places two quantization values at a time instead of one. Finally, by discretizing the search space for $Q$ , we show a fast approximation variant of QUIVER. This variant introduces an appealing tradeoff between accuracy and speed, making it suitable for quantizing large vectors on the fly. ", "page_idx": 1}, {"type": "text", "text": "We implement our algorithms in $C++$ and demonstrate their efficiency. For example, on a commodity PC, QUIVER can compute the optimal 4-bit quantization values $(s=16)$ ) for a vector with $d=1M$ entries in under a second and compute an accurate approximation in just six milliseconds. We evaluate our solutions compared to state-of-the-art ASQ methods on a variety of distributions considering different vector sizes and number of quantization values and demonstrate a speedup of up to four orders of magnitude. We open source the code of the paper [30]. ", "page_idx": 1}, {"type": "text", "text": "We note that there are many works that investigate different forms of compression, including non-adaptive quantization (e.g., QSGD [14]), biased quantization (e.g., top- $k$ [31]), sparsification (e.g., [32]), sparse coding (e.g., [33]), low-rank decomposition (e.g., PowerSGD [34]), variable-length coding (e.g., EDEN [4]) and more. Many of these are orthogonal to our work and can be used in conjunction with it. For example, one can use ASQ to quantize a sparsified or transformed vector or apply variable-length encoding to further reduce the size of the quantized vector. ", "page_idx": 1}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/1e18089563e115a0205ff3230106af0f40f5af143227d1524a41f6834878626c.jpg", "img_caption": ["QSGD (Unbiased, Non-adaptive ) N UQSGD (Unbiased, Non-adaptive ) R TN  (Biased, Non-adaptive) Powiered by TCaPDF (www.tcpdfs.org)edOptimal AdaptiveO Bpitiasmeadl nbiaseOdptimal Adaptive Unbiased "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: An experiment with dimension $d=10M$ and $s=10$ quantization values. Figure 1(a) shows the empirical MSE of quantizing a single vector with i.i.d. LogNormal $(0,\\sigma^{2})$ entries. It shows that adaptive methods are more accurate than non-adaptive and that the optimal biased method is more accurate than the optimal unbiased one. However, as shown in Figure 1(b), for distributed mean estimation, the bias may not cancel out when averaging quantized inputs (here, we used a standard setup where all vectors are identical, e.g., see [17], with i.i.d. LogNormal $(0,1/2)$ distributed entries) and the advantage of unbiased methods accordingly increases with the number of inputs. Each data point is averaged over ten runs with the standard deviation reported. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now briefly explain the benefits of ASQ compared to alternative methods. ", "page_idx": 2}, {"type": "text", "text": "The benefits of adaptivity Unbiased solutions such as QSGD [14] and NUQSGD [29] rely only on global properties (e.g., the input\u2019s norm) when selecting $Q$ . Figure 1(a) shows the benefit of adaptivity by illustrating the potential MSE reduction from selecting $Q$ optimally for the specific input. A similar behavior is observed for biased methods where the non-adaptive Round-To-Nearest (RTN) has a higher error than the optimal adaptive biased scalar quantizer, $k$ -means. As shown, this can translate to orders of magnitude lower error, depending on the data\u2019s skew. ", "page_idx": 2}, {"type": "text", "text": "The benefits of unbiasedness In many cases, it is beneficial for the quantization to be unbiased. For example, when there are $n$ senders (e.g., when doing distributed mean estimation [1, 2, 4, 17, 18]), having unbiased and independent estimates of the vectors allows the mean estimation\u2019s MSE to decay proportionally to $\\scriptstyle{\\frac{1}{n}}$ ; with biased quantization, the MSE may not decay with respect to $n$ since the errors may be correlated [17] (e.g., when all clients have the same vector). This benefti is demonstrated in Figure 1(b), which shows that while biased adaptive solutions have lower error for a small number of vectors (1-2), having unbiased quantization is critical to lowering the error for a large $n$ . ", "page_idx": 2}, {"type": "text", "text": "As another example, it was recently shown that compressing large language model parameters with biased techniques such as RTN may result in inferior performance than uniform stochastic quantization [35]. This outcome arises because the LLM layers\u2019 parameters are used to compute inner products with their inputs. Having these inner products themselves be unbiased leads to smaller errors in layers\u2019 outputs, which in turn leads to better performance. ", "page_idx": 2}, {"type": "text", "text": "2.2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given two quantization values $a,b$ and a number $x\\in[a,b]$ , Stochastic Quantization (SQ) is a procedure that rounds $x$ to $\\widehat{x}$ where ${\\widehat{x}}\\in\\{a,b\\}$ . Specifically, $\\widehat{x}$ obtains the value $a$ with probability $p_{a}=$ $\\frac{b\\!-\\!x}{b\\!-\\!a}$ and the value $b$ ot h erwise,  i. e., with probability $\\begin{array}{r}{p_{b}=1\\!-p_{a}=\\frac{x-a}{b-a}}\\end{array}$ . An important property of SQ is that the expected rounded value is unbiased, i.e., E $:[{\\widehat{x}}]=a\\!\\cdot\\!p_{a}\\!+\\!b\\!\\cdot\\!p_{b}=x$ . The variance of stochastically quantizing $x$ is then given by $\\mathbb{E}\\left[(x-{\\widehat{x}})^{2}\\right]=(x-a)^{2}\\cdot p_{a}+(x-b)^{2}\\cdot p_{b}=(b-x)(x-a)$ . Given a vector $X\\in\\mathbb{R}^{d}$ and an integer $s\\geq2$ , the Adaptive Stochastic Quantization (ASQ) problem [26, 27, 28] looks for a set of quantization values $Q$ where $|Q|\\leq s$ and $Q$ minimizes the mean squared error (MSE) that results from rounding $X$ to $\\widehat{X}\\in Q^{d}$ by stochastically quantizing each entry $x\\in X$ with values $a_{x}=\\operatorname*{max}\\left\\{q\\in Q\\mid q\\leq x\\right\\}$ and $b_{x}=\\operatorname*{min}\\left\\{q\\in Q\\mid q\\geq x\\right\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Formally, ASQ seeks to minimize the MSE, given by $\\begin{array}{r}{\\mathbb{E}[\\|X-\\widehat{X}\\|_{2}^{2}]=\\sum_{x\\in{X}}(b_{x}-x)(x-a_{x})}\\end{array}$ , where $\\mathbb{E}[\\widehat{X}]=X$ holds by construction. ", "page_idx": 2}, {"type": "text", "text": "2.3 Existing ASQ methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Leveraging the fact that there exists an optimal solution in which $Q\\subseteq X$ [26] (i.e., the quantization values are a subset of the input), one can naively solve the problem in $d^{\\Theta(s)}$ time by going over all choices for the quantization values. Instead, the following dynamic program (DP) allows us to solve it optimally and in polynomial time for any $s$ [26]. Given a sorted vector $X=\\langle x_{1},\\ldots,x_{d}\\rangle$ , we denote by $M S E[i,j]$ the optimal MSE of quantizing the prefix vector $X_{j}=\\langle x_{1},\\ldots,x_{j}\\rangle$ using $i$ quantization values that include $x_{j}$ , that is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nM S E[i,j]=\\operatorname*{min}_{Q:|Q|\\leq i,x_{j}\\in Q}\\sum_{x\\in X_{j}}(b_{x}-x)(x-a_{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our goal is to compute a set of quantization values $Q$ that results in an optimal MSE of $M S E[s,d]$ . Accordingly, we express the dynamic program as follows. We first define $C[k,j]$ as the sum of variances of all vector entries in the range $[x_{k},x_{j}]$ where $x_{k},x_{j}\\ \\in\\ Q$ are two consecutive quantization values, i.e., $\\begin{array}{r}{C[k,j]=\\sum_{x\\in[x_{k},x_{j}]}\\[\\dot{x_{j}}-x)\\mathring(x-x_{k})}\\end{array}$ . Here and when clear from context, to simplify notation, we write $\\textstyle\\sum_{x}$ to denote $\\sum_{x\\in X}$ . ", "page_idx": 3}, {"type": "text", "text": "For $i\\in\\left\\{2,\\ldots,s\\right\\},j\\in\\left\\{i,\\ldots,d\\right\\}$ , we set $M S E[2,j]=C[1,j]\\ \\forall j$ and use the recurrence ", "page_idx": 3}, {"type": "equation", "text": "$$\nM S E[i,j]=\\operatorname*{min}_{k\\in\\{i,\\ldots,j\\}}\\ {M S E}[i-1,k]+C[k,j]\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, the index $k$ denotes the entry in $X$ , $x_{k}$ , of the rightmost quantization value to the left of $x_{j}$ . A naive solution for the above DP is first to compute the matrix $C$ (which takes $O(d^{3})$ time and $\\bar{O(d^{2})}$ space) and then calculate $M S E[i,j]$ for all $i,j$ , and thus $Q$ , in $O(s\\cdot d^{2})$ time and $O(s\\cdot d)$ space. In Appendix A, we describe a simple algorithm that implements this dynamic program. ", "page_idx": 3}, {"type": "text", "text": "An improved solution, ZipML [26], uses $O(s\\cdot d^{2})$ time and $O(d^{2})$ space, but it remains infeasible even for moderate (e.g., $\\bar{d}=10^{5}$ ) dimensions. Accordingly, we next design novel techniques to asymptotically improve both the space and time complexities. ", "page_idx": 3}, {"type": "text", "text": "3 Optimization Using Pre-processing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first ingredient in our solution is the usage of preprocessed arrays that allow us to efficiently compute $C[k,j]$ in constant time, at the cost of only $O(d)$ additional space. We define the following arrays, $\\beta,\\gamma\\in\\mathbb{R}^{d}$ , that store the cumulative sums of the vector and its squared entries: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta_{j}=\\sum_{x\\in X_{j}}x\\quad,\\quad\\gamma_{j}=\\sum_{x\\in X_{j}}x^{2}\\qquad\\qquad\\forall j\\in\\{1,\\ldots,d\\}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Denoting $\\beta_{0}=\\gamma_{0}=0$ , both are computable in $O(d)$ time as $\\beta_{j}=\\beta_{j-1}+x_{j}$ and $\\gamma_{j}=\\gamma_{j-1}+x_{j}^{2}$ . We can then express $C[k,j]$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\begin{array}{c}{C[k,j]=\\displaystyle\\sum_{x\\in[x_{k},x_{j}]}(x_{j}-x)(x-x_{k})=\\displaystyle\\sum_{x\\in(x_{k},x_{j}]}(x_{j}-x)(x-x_{k})}\\\\ {=-x_{j}\\cdot x_{k}\\cdot\\displaystyle\\sum_{x\\in(x_{k},x_{j}]}1+(x_{j}+x_{k})\\cdot\\displaystyle\\sum_{x\\in(x_{k},x_{j}]}x-\\displaystyle\\sum_{x\\in(x_{k},x_{j}]}x^{2}}\\\\ {=-x_{j}\\cdot x_{k}\\cdot(j-k)+(x_{j}+x_{k})\\cdot(\\beta_{j}-\\beta_{k})-(\\gamma_{j}-\\gamma_{k}).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With this optimization, we can evaluate $C[k,j]$ in constant time, yielding a solution that uses $O(s\\cdot d)$ memory instead of $O(d^{2})$ . Next, we show how to improve the runtime. ", "page_idx": 3}, {"type": "text", "text": "4 The QUIVER Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To derive a faster algorithm, we observe that $C$ satisfies the quadrangle inequality, defined below: ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1. A function $\\boldsymbol{w}\\colon\\{1,\\dots,d\\}\\times\\{1,\\dots,d\\}\\rightarrow\\mathbb{R}$ satisfies the quadrangle inequality if for any $a\\leq b\\leq c\\leq d$ : $w[\\mathsf{a},\\mathsf{c}]+w[\\mathsf{b},\\mathsf{d}]\\leq w[\\mathsf{a},\\mathsf{d}]+w[\\mathsf{b},\\mathsf{c}]$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.2. $C$ satisfies the quadrangle inequality. ", "page_idx": 4}, {"type": "text", "text": "Proof. We first observe that for any $x\\in[x_{\\mathsf{a}},x_{\\mathsf{b}}]$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n(x_{\\mathrm{c}}-x)(x-x_{\\mathrm{a}})=(x_{\\mathrm{d}}-x)(x-x_{\\mathrm{a}})+(x_{\\mathrm{c}}-x_{\\mathrm{d}})(x-x_{\\mathrm{a}})\\leq(x_{\\mathrm{d}}-x)(x-x_{\\mathrm{a}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For any $x\\in[x_{\\mathsf{c}},x_{\\mathsf{d}}]$ , we similarly get: ", "page_idx": 4}, {"type": "equation", "text": "$$\n(x_{\\mathsf{d}}-x)(x-x_{\\mathsf{b}})=(x_{\\mathsf{d}}-x)(x-x_{\\mathsf{a}})+(x_{\\mathsf{d}}-x)(x_{\\mathsf{a}}-x_{\\mathsf{b}})\\leq(x_{\\mathsf{d}}-x)(x-x_{\\mathsf{a}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similarly, for $x\\in[x_{\\mathsf{b}},x_{\\mathsf{c}}]$ , we have that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(x_{\\mathrm{c}}-x)(x-x_{\\mathrm{a}})+(x_{\\mathrm{d}}-x)(x-x_{\\mathrm{b}})=(x_{\\mathrm{c}}-x)(x-x_{\\mathrm{b}})+(x_{\\mathrm{d}}-x)(x-x_{\\mathrm{a}})+(x_{\\mathrm{a}}-x_{\\mathrm{b}})(x_{\\mathrm{d}}-x_{\\mathrm{c}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(x_{\\mathrm{c}}-x)(x-x_{\\mathrm{b}})+(x_{\\mathrm{d}}-x)(x-x_{\\mathrm{a}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, we get: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle C[\\mathsf{a},\\mathsf{c}]+C[\\mathsf{b},\\mathsf{d}]\\qquad=\\sum_{x\\in[x_{0},x_{0}]}\\big(x_{c}-x\\big)(x-x_{\\mathsf{a}})+\\sum_{x\\in[x_{\\flat},x_{a}]}(x_{\\mathsf{d}}-x)(x-x_{\\mathsf{b}})}\\\\ &{=\\displaystyle\\sum_{x\\in[x_{\\flat},x_{\\mathsf{b}}]}\\big(x_{c}-x\\big)(x-x_{\\mathsf{a}})+\\sum_{x\\in[x_{c},x_{a}]}\\big(x_{d}-x\\big)(x-x_{\\mathsf{b}})+\\sum_{x\\in[x_{\\flat},x_{c}]}\\big(x_{c}-x\\big)(x-x_{\\mathsf{a}})+\\big(x_{\\mathsf{d}}-x\\big)(x-x_{\\mathsf{b}})}\\\\ &{\\leq\\displaystyle\\sum_{x\\in[x_{\\flat},x_{\\mathsf{b}}]}\\big(x_{a}-x\\big)(x-x_{\\mathsf{a}})+\\sum_{x\\in[x_{c},x_{d}]}\\big(x_{d}-x\\big)(x-x_{\\mathsf{a}})+\\displaystyle\\sum_{x\\in[x_{\\flat},x_{c}]}\\big(x_{c}-x\\big)(x-x_{\\mathsf{b}})+\\big(x_{d}-x\\big)(x-x_{\\mathsf{a}}).}\\\\ &{\\qquad=\\displaystyle\\sum_{x\\in[x_{\\flat},x_{d}]}\\big(x_{d}-x\\big)(x-x_{\\mathsf{a}})+\\displaystyle\\sum_{x\\in[x_{\\flat},x_{c}]}\\big(x_{c}-x\\big)(x-x_{\\mathsf{b}})\\qquad=\\qquad C[\\mathsf{a},\\mathsf{d}]+C[\\mathsf{b},\\mathsf{c}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the inequality follows from equations (1)-(3). ", "page_idx": 4}, {"type": "text", "text": "Next, let us implicitly define a matrix $A\\in\\mathbb{R}^{\\{1,...,d\\}\\times\\{1,...,d\\}}$ such that $A[k,j]=M S E[i-1,k]+$ $C[k,j]$ . Importantly, $A$ is not stored in memory but admits constant time lookups as $M S E[i-1,\\cdot]$ is stored and $C$ is efficiently computable (Section 3). Also, $C$ satisfies the quadrangle inequality and thus $A$ is a totally monotone matrix [36], i.e., for any $a<\\mathtt{b}$ and $c<\\mathrm{d}$ : $(A[\\mathsf{a},\\mathsf{c}]>A[\\mathsf{\\bar{b}},\\mathsf{c}])\\overset{\\cdot}{\\longrightarrow}$ $(A[\\mathsf{a},\\mathsf{d}]>A[\\mathsf{b},\\mathsf{d}])$ . By applying the SMAWK algorithm [37], which finds the row minimas of an implicitly defined totally monotone matrix, on $A^{\\overline{{T}}}$ , we obtain in $O(d)$ time and space the indices $k_{j}=\\mathrm{argmin}_{k\\in\\{1,\\ldots,d\\}}\\,A[k,j]$ for all $j\\in\\{1,\\ldots,d\\}$ . This immediately gives the next row of the dynamic program, as $\\hat{M S E}[i,j]=M S E[i-1,k_{j}]+C[k_{j},j]$ . ", "page_idx": 4}, {"type": "text", "text": "The resulting solution, which we call QUIVER, is given in Algorithm 1 and requires just $O(s\\cdot d)$ time and space to compute the optimal quantization values. ", "page_idx": 4}, {"type": "text", "text": "5 The Accelerated QUIVER Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To accelerate QUIVER, we rely on the observation that while the problem is non-convex for $s>3$ , it admits a closed-form solution when $s=3$ . ", "page_idx": 4}, {"type": "text", "text": "Denoting by $\\begin{array}{r}{C^{2}[k,j]=\\operatorname*{min}_{b\\in\\{k,\\ldots,j\\}}\\left(C[k,b]+C[b,j]\\right)}\\end{array}$ the optimal MSE of quantizing the range $[x_{k},x_{j}]$ using three quantization values (at $x_{k},x_{b},x_{j})$ , we show how to compute $C^{2}$ in constant time. ", "page_idx": 4}, {"type": "text", "text": "Namely, consider adding a quantization value $q\\in[x_{k},x_{j}]$ (not necessarily in $X$ ) between two existing quantization values $x_{k}$ and $x_{j}$ . Let us define the sum of variances of all input entries in $[x_{k},x_{j}]$ as a function of $q$ : $\\begin{array}{r}{Q(q)=\\sum_{x\\in[x_{k},q]}(q-x)(x-x_{k})+\\sum_{x\\in(q,x_{j}]}(x_{j}-x)(x-q)}\\end{array}$ . This function is differentiable in [xk, xj]  \\ X, and we get: dQd(qq)=  x \u2208[xk,q](x \u2212xk) \u2212 x\u2208(q,xj](xj \u2212x). ", "page_idx": 4}, {"type": "text", "text": "1: Input: $X\\in\\mathbb{R}^{d},s\\in\\mathbb{N}$ . $\\vartriangleright X$ is sorted.   \n2: Preprocess(X) \u25b7Enables computing $C[k,j]$ in constant time (Section 3).   \n3: for $j=2$ to $d$ do   \n4: $M S E[2,j]=C[1,j]$   \n5: for $i=3$ to $s$ do   \n6: $K[i,\\cdot]=\\operatorname{SMAWK}(A)$ $\\triangleright$ Where $A[k,j]\\triangleq M S E[i-1,k]+C[k,j]\\quad\\forall k,j.$   \n7: $M\\dot{S}E[i,j]=M{S}E[i-1,K[i,j]]+C[K[i,j],j]$ for all $j\\in\\{i,\\ldots,d\\}$ .   \n8: $Q=\\{x_{1},x_{d}\\}$   \n9: $j=d$   \n10: for $i=s$ down to 3 do   \n11: j = K[i, j]   \n12: Q = Q \u222a{xj}   \n13: return $Q$ ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Accelerated QUIVER ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input: $X\\in\\mathbb{R}^{d},s\\in\\mathbb{N}$ . $\\vartriangleright X$ is sorted.   \n2: Preprocess(X) $\\triangleright$ Enables computing $C[k,j]$ and $C^{2}[k,j]$ in constant time.   \n3: s\u2032 = (s mod 2)   \n4: if $s^{\\prime}=0$ then   \n5: for $j=2$ to $d$ do   \n6: $M S E[2,j]=C[1,j]$   \n7: else   \n8: for $j=3$ to $d$ do   \n9: $M S E[3,j]=C^{2}[1,j]$   \n10: for $i=2$ to $\\lfloor s/2\\rfloor$ do   \n11: $K[i,\\cdot]=\\mathbf{\\bar{S}}\\mathbb{M}\\bar{\\mathbb{A}}\\mathbb{W}\\mathbb{K}(B)$ \u25b7Where $B[k,j]\\triangleq M S E[2\\cdot(i-1)+s^{\\prime},k]+C^{2}[k,j]\\quad\\forall k,j$ .   \n12: $\\begin{array}{r}{M\\dot{S}E[2\\cdot i+s^{\\prime},j]=M S E[2\\cdot(i-1)+s^{\\prime},K[i,j]]+C^{2}[K[i,j],j]\\qquad\\forall j\\in\\{i,\\dots,d\\}.}\\end{array}$   \n13: $Q=\\{x_{1},x_{d}\\}$   \n14: $j=d$   \n15: for $i=\\lfloor s/2\\rfloor$ down to 2 do   \n16: $\\begin{array}{r l}&{b^{*}=\\mathop{\\mathrm{argmin}}_{b\\in\\{K[i,j],\\ldots,j\\}}\\left(C[K[i,j],b]+C[b,j]\\right)}\\\\ &{j=K[i,j]}\\\\ &{Q=Q\\cup\\{x_{j},x_{b^{*}}\\}}\\end{array}$ \u25b7Takes $O(1)$ time.   \n17:   \n18:   \n19: if $s^{\\prime}=1$ then   \n20: $\\begin{array}{r l}&{b^{*}=\\operatorname*{argmin}_{b\\in\\{0,...,j\\}}\\left(C[0,b]+C[b,j]\\right)}\\\\ &{Q=Q\\cup\\{x_{b^{*}}\\}}\\end{array}$ \u25b7Takes $O(1)$ time.   \n21:   \n22: return $Q$ ", "page_idx": 5}, {"type": "text", "text": "Notice that the derivative is monotonically non-decreasing and for any $\\ell\\in\\{k,k+1,\\ldots,j-1\\}$ the derivative is fixed (independent of $q$ ) over any interval $(x_{\\ell},x_{\\ell+1})$ . This means that $Q(q)$ is minimized at $\\begin{array}{r}{u=\\operatorname*{inf}_{q}(\\frac{d Q(q)}{d q}\\geq0)}\\end{array}$ , where $u\\in X$ . Denote by $b_{k,j}^{*}\\in\\{k,\\ldots,j\\}$ the value such that $x_{b_{k,j}^{*}}=u$ Notice that while dQd(qu) may not be defined, we have that $\\begin{array}{r}{\\operatorname*{lim}_{h\\to0^{+}}\\frac{d Q(u+h)}{d q}\\geq0}\\end{array}$ dQ(duq+h)\u22650 is well-defined. We thus require $\\begin{array}{r}{\\sum_{i=k+1}^{b_{k,j}^{*}}(x_{i}-x_{k})-\\sum_{i=b_{k,j}^{*}+1}^{j}(x_{j}-x_{i})\\geq0.}\\end{array}$ . With some simplifications, this is equivalent to:  ij=k+1 xi \u2212(b\u2217k,j \u2212k)xk \u2212(j \u2212b\u2217k,j)xj \u22650, yielding b\u2217k,j \u2265jxj\u2212kxxkj\u2212\u2212 xkij=k+1 xi. As $b_{k,j}^{*}$ is an integer, we get a formula for $C^{2}[k,j]$ that can be computed in constant time using: $\\begin{array}{r}{b_{k,j}^{*}\\,=\\Big\\lceil\\frac{j x_{j}-k x_{k}-\\sum_{i=k+1}^{j}x_{i}}{x_{j}-x_{k}}\\Big\\rceil\\,=\\,\\Big\\lceil\\frac{j x_{j}-k x_{k}-(\\beta_{j}-\\beta_{k})}{x_{j}-x_{k}}\\Big\\rceil}\\end{array}$ . That is, for any $1\\leq k\\leq j\\leq d$ we have that $C^{2}[k,j]=C[k,b_{k,j}^{*}]+C[b_{k,j}^{*},j]$ is the sum of the variances in quantizing the entries in $[x_{k},x_{j}]$ using the quantization values $\\left\\{x_{k},x_{b_{k,j}^{*}},x_{j}\\right\\}$ . ", "page_idx": 5}, {"type": "text", "text": "We can then use this method to halve the required number of invocations of SMAWK by always using it to pick the second-next quantization value and computing the optimal quantization value in between directly. Our accelerated dynamic program is then given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nM S E[i,j]=\\left\\{\\begin{array}{l l}{\\displaystyle\\operatorname*{min}_{k\\in\\{i,\\ldots,j\\}}}&{M S E[i-2,k]+C^{2}[k,j]}&{i>3}\\\\ {\\displaystyle C^{2}[1,j]}&{i=3}\\\\ {C[1,j]}&{i=2}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and the resulting pseudo-code for Accelerated QUIVER is given by Algorithm 2. Similarly to QUIVER, we start by initializing the first row of $M S E$ . Importantly, we now separate the even $s$ case (lines 5-6), in which we initialize the row using $C$ , and the odd case, where we use $C^{2}$ (lines 8-9). That is, the odd $s$ case \u2018skips\u2019 a quantization value that we later determine separately (lines 19-21). Next, denoting $s^{\\prime}=(s\\ \\ \\mathrm{mod}\\ 2)$ , we proceed with $\\lfloor s/2\\rfloor\\!-\\!1$ invocations of the SMAWK algorithm (lines 10- 12), applied on the implicitly defined matrix $B[k,j]\\triangleq M S E[2\\cdot(i-1)+s^{\\prime},K[i,j]]+C^{2}[K[i,j],j]$ . The output yields the minimizers of $M S E[2\\cdot i+s^{\\prime},j]$ used for reconstruction. In the reconstruction step (lines 15-21), we fill in the missing quantization values by finding the optimal value between every two outputs from the dynamic program minimizers $K$ . ", "page_idx": 6}, {"type": "text", "text": "Overall, the Accelerated QUIVER algorithm requires at most half of the number of SMAWK invocations compared to QUIVER and at most half of the memory to store $K$ and $M S E$ . ", "page_idx": 6}, {"type": "text", "text": "To establish correctness, we state the following lemma, whose proof appears in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.1. $C^{2}$ satisfies the quadrangle inequality. ", "page_idx": 6}, {"type": "text", "text": "In Appendix D, we discuss why this approach is not suitable for further acceleration by placing more than one quantization value in $[x_{a},x_{c}]$ . ", "page_idx": 6}, {"type": "text", "text": "6 The Approximate QUIVER Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now show how the usage of quantization value discretization gives a controllable tradeoff between accuracy and speed. Intuitively, by allowing the quantization values to be placed only on a uniform grid of controllable size $m+1\\geq s$ (for some $m\\in\\mathbb{N}^{+}$ ), we can accelerate the computation at the cost of a small additional error. Importantly, while the quantization values are from a discretized set of possibilities, we compute the optimal subset of discretized values for the original input vector. ", "page_idx": 6}, {"type": "text", "text": "To that end, consider the discrete set $S=\\left\\{x_{1}+{\\boldsymbol{\\ell}}\\cdot{\\frac{x_{d}-x_{1}}{m}}\\ |\\ {\\boldsymbol{\\ell}}\\in\\left\\{0,\\ldots,m\\right\\}\\right\\}$ . Our goal is then to find $Q\\in(\\mathcal{S})$ that minimizes the sum of variances for the original input. Denoting $\\begin{array}{r}{s_{\\ell}=x_{1}\\!+\\!\\ell\\!\\cdot\\!\\frac{x_{d}-x_{1}}{m}}\\end{array}$ , we modify our preprocessing scheme to consider the discretization: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha_{\\ell}=\\sum_{x\\in[s_{0},s_{\\ell}]}1\\quad,\\quad\\beta_{\\ell}=\\sum_{x\\in[s_{0},s_{\\ell}]}x\\quad,\\quad\\gamma_{\\ell}=\\sum_{x\\in[s_{0},s_{\\ell}]}x^{2}\\qquad\\qquad\\forall\\ell\\in\\{1,\\ldots,m\\}\\ .\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As we explain in Appendix E, we can compute these values in $O(d)$ time and space. ", "page_idx": 6}, {"type": "text", "text": "Using these arrays, we can express the sum of variances of all input entries between two quantization values $s_{k},s_{j}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{m}[k,j]=\\displaystyle\\sum_{x\\in[s_{k},s_{j}]}(s_{j}-x)(x-s_{k})=\\displaystyle\\sum_{x\\in(s_{k},s_{j}]}(s_{j}-x)(x-s_{k})}}\\\\ {{=-s_{j}\\cdot s_{k}\\cdot\\sum_{x\\in(s_{k},s_{j}]}1+(s_{j}+s_{k})\\cdot\\sum_{x\\in(s_{k},s_{j}]}x-\\displaystyle\\sum_{x\\in(s_{k},s_{j}]}x^{2}}}\\\\ {{=-s_{j}\\cdot s_{k}\\cdot(\\alpha_{j}-\\alpha_{k})+(s_{j}+s_{k})\\cdot(\\beta_{j}-\\beta_{k})-(\\gamma_{j}-\\gamma_{k}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that the quadrangle inequality trivially holds for this extension. The resulting algorithm, termed Approximate QUIVER (or in short, Apx. QUIVER), proceeds as QUIVER with $C_{m}$ instead of $C$ , except for the reconstruction stage where we pick $Q$ from $S$ instead of the input $X$ . Apx. QUIVER, whose pseudo-code is given in Appendix F, runs in space and time complexities of $O(d+m\\cdot s)$ . ", "page_idx": 7}, {"type": "text", "text": "We next analyze the approximation guarantee of Apx. QUIVER. Denote by $\\tt o p t_{X,s}$ the optimal MSE attainable for $X$ using $s$ quantization values, and by $\\mathtt{A Q}_{X,2s-2}$ the MSE of Apx. QUIVER with $2s-2$ values. We prove that the MSE of Apx. QUIVER with $2s-2$ quantization values is close to the optimal algorithm with $s$ values. In practice, we generally find Apx. QUIVER does better than the bound below, and for moderate $m$ , it is nearly optimal. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6.1. For any $X,s,m$ we have $\\begin{array}{r}{\\mathtt{A Q}_{X,2s-2}\\leq\\mathsf{o p t}_{X,s}+\\frac{d\\cdot(x_{d}-x_{1})^{2}}{4m^{2}}\\leq\\mathsf{o p t}_{X,s}+\\frac{d\\cdot\\|X\\|_{2}^{2}}{2m^{2}}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Proof. Let $Q^{*}\\subseteq X$ be the optimal solution with $|Q^{*}|\\ \\leq\\ s$ . For any $q\\ \\in\\mathrm{~}Q^{*}$ , denote by ${\\underline{{q}}}\\;=$ max $\\{s_{\\ell}\\in S\\mid s_{\\ell}\\leq q\\}$ and $\\overline{{q}}=\\operatorname*{min}\\left\\{s_{\\ell}\\in S\\mid s_{\\ell}\\geq q\\right\\}$ . Consider the solution $\\widetilde{Q}=\\left\\{\\underline{{{q}}},\\overline{{{q}}}\\mid q\\in Q^{*}\\right\\}$ . Note that $|\\widetilde{Q}|\\,\\leq\\,2s\\,-\\,2$ as $x_{1},x_{d}\\,\\in\\,Q^{*}$ and $\\overline{{x_{1}}}\\,=\\,\\underline{{x_{1}}}$ and $\\overline{{x_{d}}}\\,=\\,\\underline{{x_{d}}}$ . Also, $\\widetilde{Q}\\subseteq S$ and is thus a valid solution of Apx. QUIVER. Thus, $\\tt A Q_{X,2s-2}$ is upper bounded by the MSE when using $\\widetilde{Q}$ . ", "page_idx": 7}, {"type": "text", "text": "Next, consider $x\\in X$ and let $a_{x}=\\operatorname*{max}\\left\\{q\\in Q^{*}\\mid q\\leq x\\right\\}$ and $b_{x}=\\operatorname*{min}\\left\\{q\\in Q^{*}\\mid q\\geq x\\right\\}$ be the values between which $x$ is stochastically quantized in $Q^{*}$ . We consider two cases: ", "page_idx": 7}, {"type": "text", "text": "\u2022 $x\\in[\\underline{{a_{x}}},\\overline{{a_{x}}})\\cup(\\underline{{b_{x}}},\\overline{{b_{x}}}]$ . In this case, when using $\\widetilde{Q}$ , we have that $x$ is quantized in an interval of size $(x_{d}-x_{1})/m$ and thus its variance is bounded by $(x_{d}-x_{1})^{2}/4m^{2}$ . \u2022 $x\\in[\\overline{{a_{x}}},\\underline{{b_{x}}}]$ , in this case, using ${\\widetilde{Q}},x$ is quantized between $\\overline{{a_{x}}}$ and $\\underbar b_{x}$ , yielding a variance of $(\\underline{{b_{x}}}-x)(\\overline{{x}}-\\overline{{a_{x}}})\\leq(b_{x}-x)(x-a_{x})$ , i.e., lower than the variance under $Q^{*}$ . ", "page_idx": 7}, {"type": "text", "text": "As the two cases capture all options, summing the variances over all $x\\in X$ yields the result. ", "page_idx": 7}, {"type": "text", "text": "In terms of the vector normalized MSE (vNMSE),1 which is a normalized MSE measure given by $\\frac{\\mathbb{E}\\left[\\left\\|X-\\widehat{X}\\right\\|_{2}^{2}\\right]}{\\|X\\|_{2}^{2}}$ , Apx. QUIVER with $2s-2$ quantization values achieves an additive $\\frac{d}{2m^{2}}$ term to the optimal vNMSE when using $s$ quantization values. ", "page_idx": 7}, {"type": "text", "text": "However, the first inequality of Lemma 6.1 is generally much tighter than the second that uses the squared norm. For example, if the entries of $X$ were i.i.d. $U[\\bar{a},b]$ random variables, for some constants $a<b$ then $(x_{d}-x_{1})^{2}=O(1)$ while $\\left\\|X\\right\\|_{2}^{2}=\\Theta(d)$ . Similarly, for i.i.d $\\mathcal{N}(\\mu,\\sigma^{2})$ entries for constants $\\mu,\\sigma$ we have $(x_{d}-x_{1})^{2}=O(\\log d)$ while $\\left\\|X\\right\\|_{2}^{2}=\\Theta(d)$ (both with high probability). ", "page_idx": 7}, {"type": "text", "text": "7 Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate our algorithms\u2019 empirical vNMSE and runtime against SOTA ASQ solutions. ", "page_idx": 7}, {"type": "text", "text": "Setup. We implement all algorithms in $C++$ . Unless stated otherwise, we use a g4dn.4xlarge AWS EC2 server with custom Intel Cascade Lake CPUs with 64 GB RAM and Ubuntu 22.04 OS and average all results over 5 seeds. ", "page_idx": 7}, {"type": "text", "text": "Acceleration Speedup Appendix G shows the speedup attainable by Accelerated QUIVER. As we show, Accelerated QUIVER is consistently faster than QUIVER, providing up to $5.4\\times$ speedup. ", "page_idx": 7}, {"type": "text", "text": "Distributions. All experiments are done with vectors whose entries are independent and identically distributed. We present results for the LogNormal distribution and defer to Appendix H results for Normal, Exponential, TruncNorm, and Weibull distributions. As mentioned, these distributions are of interest as they are reported to capture gradients, model weights and activations (see Section 1). ", "page_idx": 7}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/d2fe2576e7cede16c2f478867e66d4ff43a8d50f306ebbe8dca7cba7435f376a.jpg", "img_caption": ["Figure 2: Comparing exact solutions with LogNormal $(0,1)$ distributed input. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/8c6c4064fe3df4ced5c62c99524671e309cc442c2c2c624ba1d9a5228608c5fc.jpg", "img_caption": ["Figure 3: Comparing approximate solutions with LogNormal $(0,1)$ distributed input. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Baselines. We evaluate Accelerated QUIVER and compare its runtime to ZipML [26]. For the approximate variants, we evaluate Apx. QUIVER and compare it with three approximation variants of $\\mathbf{ZipML}$ proposed in [26], namely ZipML-CP Quantiles, ZipML-CP Uniform, and ZipML 2- Approximation. ZipML-CP is an algorithm that runs the exact ZipML algorithm on a subset of the points called \u2018Candidate Points\u2019. Since ZipML runs in $O(d^{2}s)$ time, here we use M candidate points to get $O(d+M^{2}s)$ time. ZipML 2-Apx is an algorithm that computes an approximate solution in $O(d\\log d+s^{3})$ time. It guarantees that its sum of variances is at most twice that of an optimal solution with $\\lfloor s/2\\rfloor$ quantization values. We also compare with the recently proposed ALQ [28], which is an algorithm that finds good quantization values for a truncated normal distribution. It samples several gradients (by computing the gradient of several random batches) to fti the truncated normal parameters. To be fair to ALQ, since we evaluate a single-shot quantization scenario, we calculate the exact mean, variance, and support parameters for the input vector. This then runs for several (we used 10, as in their released code) iterations, so in total, they compute $\\approx10s$ integrals. While theoretically requiring $O(d)$ time, in a model where such integral calculation takes constant time, this is markedly slower than other approaches. We note that it is possible that with low-precision integral calculations, one may improve the runtime, but the error (which is already not competitive) will degrade further. We further discuss these approximation algorithms in Appendix I. ", "page_idx": 8}, {"type": "text", "text": "Exact algorithms experiments. The results are presented in Figure 2. Figure 2(a) shows the runtime for optimally solving the ASQ problem for different dimensions and $s$ . As shown, all our solutions are markedly faster than ZipML, which we are unable to run for dimensions $d\\geq2^{17}$ due to its prohibitively large memory requirements. The asymptotic difference $(O(s\\cdot d^{2})$ for ZipML and $O(\\bar{s}\\cdot d)$ for Accelerated QUIVER) is clearly visible in the different slopes on the log-log plot. As a result, Accelerated QUIVER can efficiently quantize vectors. For example, Acc. QUIVER can compute the optimal 4-bit $\\lvert s=16$ ) quantization values for a 1M-sized vector in under a second. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Next, Figure 2(b) and Figure 2(c) show the vNMSE and runtime with respect to the number of quantization values $s$ for $\\bar{d}\\,=\\,2^{12}$ and $d=2^{16}$ . As shown, the vNMSE decays linearly with $s$ while the runtime increases linearly. Even for these small dimensions, our algorithms are orders of magnitude faster than ZipML. ", "page_idx": 9}, {"type": "text", "text": "Approximate algorithms experiments. The comparison results are presented in Figure 3. It is evident in Figure 3(a) that approximate solutions are significantly faster than exact ones. Also, Apx. QUIVER offers both near-optimal vNMSE and the fastest runtime as the dimension increases. As shown in Figures 3(b) and 3(c), Apx. QUIVER offers these advantages for different $s,m$ values. ", "page_idx": 9}, {"type": "text", "text": "Notably, on a commodity PC, Apx. QUIVER can compute near-optimal 4-bit quantization values $s=16$ ) for a vector with $d=\\dot{2}^{20}$ entries in just six milliseconds, and about $70\\mathrm{ms}$ for $d=2^{24}$ , potentially enabling quantizing vectors on the fly for many applications. ", "page_idx": 9}, {"type": "text", "text": "8 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we presented algorithms for the Adaptive Stochastic Quantization (ASQ) problem with improved space and time complexities compared to the state of the art. For parameters of interest, our exact algorithms are up to four orders of magnitude faster compared to the alternatives while using markedly less memory. To potentially enable on-the-fly adaptive quantization of vectors, we also introduce an approximate algorithm with strong guarantees that runs faster while being significantly more accurate than other approximate solutions. ", "page_idx": 9}, {"type": "text", "text": "Limitations: QUIVER is not GPU friendly, and it remains an interesting future work to design GPUfriendly ASQ algorithms. Also, similarly to previous works (e.g., [26]), our exact solution assumes that the input vector is sorted. Otherwise, the runtime is increased to $O(d\\cdot\\log d+s\\cdot d)$ . We note that Apx. QUIVER does not require the vector to be sorted and the time complexity remains $O(d+s\\cdot m)$ even for non-sorted inputs, making it even more appealing compared to the exact solutions. ", "page_idx": 9}, {"type": "text", "text": "Offloading Computation to a GPU: For exact algorithms, one can sort the input vector on a GPU, bringing the CPU solution complexity to $O(s\\cdot d)$ which is faster for large vectors. In practice, GPU sorting is rarely the bottleneck; indeed, in Appendix J we measure the time it takes to sort the vector on a T4 GPU, and also to quantize the vector after an ASQ outputs the optimal quantization values. For example, the sorting and quantization time for a $1M$ -sized vector sums up to only $4\\mathrm{ms}$ where the runtime of Accelerated QUIVER is about one second. ", "page_idx": 9}, {"type": "text", "text": "Generalizing the algorithms for weighted inputs: An interesting generalization of the ASQ problem is the weighted variant, where each entry $x_{i}\\in X$ is associated with a weight $w_{i}\\in\\mathbb{R}$ and the goal is to minimize the weighted sum of variances $\\textstyle\\sum_{i=1}^{d}(x_{i}-{\\widehat{x_{i}}})^{2}\\cdot w_{i}$ . This variant is useful when, instead of getting an input vector, one wishes to  solve ASQ fo r an empirical distribution. In Appendix ${\\bf K}$ we explain how our algorithms and their analyses generalize to the weighted case, while maintaining the $O\\bar{(}d\\cdot s)$ and $O(d+M\\cdot s)$ runtime and space complexities for QUIVER and Apx. QUIVER accordingly. Our measurements indicate that the weighted variants are only $10\u201320\\%$ slower than their unweighted counterparts. ", "page_idx": 9}, {"type": "text", "text": "Reproducability: All our results are reproducible and our code is open sourced [30]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Wenchen Han for his insightful comments and suggestions. Michael Mitzenmacher was supported in part by NSF grants CCF-2101140, CNS-2107078, and DMS-2023528. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. T. Suresh, X. Y. Felix, S. Kumar, and H. B. McMahan, \u201cDistributed Mean Estimation With Limited Communication,\u201d in International Conference on Machine Learning. PMLR, 2017, pp. 3329\u20133337.   \n[2] J. Konec\u02c7ny\\` and P. Richt\u00e1rik, \u201cRandomized Distributed Mean Estimation: Accuracy vs. Communication,\u201d Frontiers in Applied Mathematics and Statistics, vol. 4, p. 62, 2018.   \n[3] S. Caldas, J. Konec\u02c7n\u00fd, H. B. McMahan, and A. Talwalkar, \u201cExpanding the Reach of Federated Learning by Reducing Client Resource Requirements,\u201d arXiv preprint arXiv:1812.07210, 2018. [4] S. Vargaftik, R. B. Basat, A. Portnoy, G. Mendelson, Y. B. Itzhak, and M. Mitzenmacher, \u201cEDEN: Communication-Efficient and Robust Distributed Mean Estimation for Federated Learning,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 21 984\u201322 014.   \n[5] R. Dorfman, S. Vargaftik, Y. Ben-Itzhak, and K. Y. Levy, \u201cDoCoFL: downlink compression for cross-device federated learning,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 8356\u20138388.   \n[6] W. Han, S. Vargaftik, M. Mitzenmacher, B. Karp, and R. Ben Basat, \u201cBeyond Throughput and Compression Ratios: Towards High End-to-end Utility of Gradient Compression,\u201d in HotNets, 2024.   \n[7] D. Zhou, K. Wang, J. Gu, X. Peng, D. Lian, Y. Zhang, Y. You, and J. Feng, \u201cDataset Quantization,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 17 205\u201317 216.   \n[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, \u201cOPTQ: Accurate quantization for generative pre-trained transformers,\u201d in The Eleventh International Conference on Learning Representations, 2023.   \n[9] Y. Jeon, C. Lee, K. Park, and H.-y. Kim, \u201cA Frustratingly Easy Post-Training Quantization Scheme for LLMs,\u201d in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 14 446\u201314 461.   \n[10] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al., \u201cMixed Precision Training,\u201d in International Conference on Learning Representations, 2018.   \n[11] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer, \u201cQBERT: Hessian Based Ultra Low Precision Quantization of BERT,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 8815\u20138821.   \n[12] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. R\u00e9, I. Stoica, and C. Zhang, \u201cFlexgen: High-Throughput Generative Inference of Large Language Models With a Single GPU,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 31 094\u2013 31 116.   \n[13] Ben Basat, Ran and Mitzenmacher, Michael and Vargaftik, Shay, \u201cHow to Send a Real Number Using a Single Bit (And Some Shared Randomness),\u201d in 48th International Colloquium on Automata, Languages, and Programming (ICALP 2021), vol. 198, 2021, p. 25.   \n[14] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, \u201cQSGD: Communication-Efficient SGD via Gradient Quantization and Encoding,\u201d Advances in Neural Information Processing Systems, vol. 30, pp. 1709\u20131720, 2017.   \n[15] S. Horv\u00f3th, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richt\u00e1rik, \u201cNatural Compression for Distributed Deep Learning,\u201d in Mathematical and Scientific Machine Learning. PMLR, 2022, pp. 129\u2013141.   \n[16] M. Safaryan, E. Shulgin, and P. Richt\u00e1rik, \u201cUncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor,\u201d Information and Inference: A Journal of the IMA, 2020.   \n[17] S. Vargaftik, R. Ben-Basat, A. Portnoy, G. Mendelson, Y. Ben-Itzhak, and M. Mitzenmacher, \u201cDrive: One-bit Distributed Mean Estimation,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 362\u2013377, 2021.   \n[18] R. B. Basat, S. Vargaftik, A. Portnoy, G. Einziger, Y. Ben-Itzhak, and M. Mitzenmacher, \u201cAccelerating Federated Learning with Quick Distributed Mean Estimation,\u201d in International Conference on Machine Learning, 2024.   \n[19] X. Chen, S. Vargaftik, and R. Ben-Basat, \u201cWhen ML Training Cuts Through Congestion: Just-in-Time Gradient Compression via Packet Trimming,\u201d in Hotnets, 2024.   \n[20] J. Kone\u02c7cn\u00fd, H. B. McMahan, F. X. Yu, P. Richt\u00e1rik, A. T. Suresh, and D. Bacon, \u201cFederated Learning: Strategies for Improving Communication Efficiency,\u201d arXiv preprint arXiv:1610.05492, 2017.   \n[21] M. Li, R. B. Basat, S. Vargaftik, C. Lao, K. Xu, X. Tang, M. Mitzenmacher, and M. Yu, \u201cTHC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression,\u201d in USENIX Symposium on Networked Systems Design and Implementation, 2024.   \n[22] B. Chmiel, L. Ben-Uri, M. Shkolnik, E. Hoffer, R. Banner, and D. Soudry, \u201cNeural Gradients are Near-Lognormal: Improved Quantized and Sparse Training,\u201d in International Conference on Learning Representations. OpenReview.net, 2021. [Online]. Available: https://openreview.net/forum?id=EoFNy62JGd   \n[23] R. Banner, Y. Nahshan, and D. Soudry, \u201cPost Training 4-Bit Quantization of Convolutional Networks for Rapid-Deployment,\u201d in NeurIPS, 2019.   \n[24] X. Ye, P. Dai, J. Luo, X. Guo, Y. Qi, J. Yang, and Y. Chen, \u201cAccelerating CNN Training by Pruning Activation Gradients,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 322\u2013338.   \n[25] M. Vladimirova, J. Arbel, and P. Mesejo, \u201cBayesian Neural Networks Become Heavier-Tailed With Depth,\u201d in NeurIPS 2018-Thirty-second Conference on Neural Information Processing Systems, 2018, pp. 1\u20137.   \n[26] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang, \u201cZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning,\u201d in International Conference on Machine Learning. PMLR, 2017, pp. 4035\u20134043.   \n[27] F. Fu, Y. Hu, Y. He, J. Jiang, Y. Shao, C. Zhang, and B. Cui, \u201cDon\u2019t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 3304\u20133314.   \n[28] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya, \u201cAdaptive Gradient Quantization for Data-parallel SGD,\u201d Advances in neural information processing systems, vol. 33, pp. 3174\u20133185, 2020.   \n[29] A. Ramezani-Kebrya, F. Faghri, I. Markov, V. Aksenov, D. Alistarh, and D. M. Roy, \u201cNUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization,\u201d The Journal of Machine Learning Research, vol. 22, no. 1, pp. 5074\u20135116, 2021.   \n[30] \u201cQUIVER code,\u201d https://github.com/ranbenbasat/QUIVER.   \n[31] S. U. Stich, J.-B. Cordonnier, and M. Jaggi, \u201cSparsified sgd with memory,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[32] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, and A. Sapio, \u201cEfficient Sparse Collective Communication and its Application to Accelerate Distributed Deep Learning,\u201d in Proceedings of the 2021 ACM SIGCOMM 2021 Conference, 2021, pp. 676\u2013691.   \n[33] V. Monga, Y. Li, and Y. C. Eldar, \u201cAlgorithm unrolling: Interpretable, efficient deep learning for signal and image processing,\u201d IEEE Signal Processing Magazine, vol. 38, no. 2, pp. 18\u201344, 2021.   \n[34] T. Vogels, S. P. Karimireddy, and M. Jaggi, \u201cPowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization,\u201d in Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[35] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, \u201cUp or Down? Adaptive Rounding for Post-training Quantization,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 7197\u20137206.   \n[36] Z. Galil and K. Park, \u201cA Linear-time Algorithm for Concave One-dimensional Dynamic Programming,\u201d Information Processing Letters, vol. 33, no. 6, pp. 309\u2013311, 1990.   \n[37] A. Aggarwal, M. Klawe, S. Moran, P. Shor, and R. Wilber, \u201cGeometric applications of a matrix searching algorithm,\u201d in Proceedings of the second annual symposium on Computational geometry, 1986, pp. 285\u2013292.   \n[38] \u201cExample SMAWK code,\u201d https://github.com/pombredanne/code-5/blob/master/recipes/Python/ 117244_SMAWK_totally_monotone_matrix_searching/recipe-117244.py, accessed 01-Mar-21. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Algorithm 3 Basic Dynamic Programming Algorithm ", "page_idx": 13}, {"type": "text", "text": "1: Input: $X\\in\\mathbb{R}^{d},s\\in\\mathbb{N}$ .   \n2: Compute $C:[d]\\times[d]\\rightarrow\\mathbb{R}^{+}$ using $X$ .   \n3: for $j=2$ to $d$ do   \n4: $M S E[2,j]=C[1,j]$   \n5: for $i=3$ to $s$ do   \n6: for $j=i$ to $d$ do   \n7: $M S E[i,j]=\\operatorname*{min}_{k\\in\\{i,...,j\\}}M S E[i-1,k]+C[k,j]$   \n8: $j=d$   \n9: $Q=\\{x_{1},x_{d}\\}$   \n10: for $i=s$ down to 3 do   \n11: j = argmink\u2208{i,...,j} $M S E[i-1,k]+C[k,j]$   \n12: Q = Q \u222a{xj}   \n13: return $Q$ ", "page_idx": 13}, {"type": "text", "text": "A Basic Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We now describe a simple algorithm that finds the optimal quantization values using the dynamic program, with pseudo-code given by Algorithm 3. After initialization (lines 2-4), the algorithm iteratively computes $M S E[i,\\cdot]$ given $\\bar{M}S\\bar{E[{i-1,\\cdot}]}$ (lines 5-7) and traces back the optimal quantization values given the solution (lines 8-12). ", "page_idx": 13}, {"type": "text", "text": "B The SMAWK Algorithm [37] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we provide some intuition into how SMAWK operates and achieves its efficiency. The SMAWK algorithm has four main steps: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Pruning Phase: Remove columns that cannot possibly contain a row maximum. This is done by comparing each column with its neighbors and discarding those that cannot be maxima based on the totally monotone property. At the end of this phase, the number of columns can be no larger than the number of rows.   \n\u2022 Recursive Reduction: The algorithm reduces the problem size by considering a subset of the rows and columns. It selects every other row and recursively solves the reduced problem.   \n\u2022 Candidate Set: After solving the smaller problem, the solution provides candidate columns for the original problem. The algorithm only needs to consider these columns to find the maxima for the skipped rows.   \n\u2022 Merge Phase: Combine the results from the reduced problem with the candidate set to find the maximum for each original row. ", "page_idx": 13}, {"type": "text", "text": "Regarding efficiency, the SMAWK algorithm achieves a time complexity of $O(d)$ for a $d\\times d$ matrix. This efficiency is due to the recursive reduction of the problem size and the properties of totally monotone matrices that limit the number of comparisons needed. Namely, the pruning step takes $O(\\#c o l s)$ , where $\\#c o l s$ is the number of columns still being considered. The crux is that the recursive step happens after the pruning, which means that the recursive invocation happens with a number of columns that is, at most, double the number of rows (as the number of rows is halved). This means that the overall complexity of each recursive step is proportional to the number of rows, yielding the recursion: $T(n)={\\bar{T}}(n/2)+O(n)=O(n)$ . A simple example Python implementation (by David Eppstein) appears here [38]. Our implementation is in optimized $C++$ [30]. ", "page_idx": 13}, {"type": "text", "text": "C Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 5.1. $C^{2}$ satisfies the quadrangle inequality. ", "page_idx": 13}, {"type": "text", "text": "Proof. The lemma claims that, for any $\\mathtt{a}\\leq\\mathtt{b}\\leq\\mathtt{c}\\leq\\mathtt{d}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nC^{2}[\\mathbf{a},\\mathbf{c}]+C^{2}[\\mathbf{b},\\mathbf{d}]\\leq C^{2}[\\mathbf{a},\\mathbf{d}]+C^{2}[\\mathbf{b},\\mathbf{c}].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that for any $a\\leq c\\in\\{1,\\ldots,d\\}$ , we denote ", "page_idx": 14}, {"type": "equation", "text": "$$\nb_{a,c}^{*}=\\operatorname*{argmin}_{b\\in\\{a,\\ldots,c\\}}C[a,b]+C[b,c].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We prove the lemma by a case analysis: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Case $b_{\\mathtt{b,c}}^{*}\\leq b_{\\mathtt{a,d}}^{*}$ . In this case, we have that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C^{2}(\\mathbf{a},\\mathbf{c})+C^{2}(\\mathbf{b},\\mathbf{d})=C(\\mathbf{a},b_{\\mathbf{a},\\mathbf{c}}^{*})+C(b_{\\mathbf{a},\\mathbf{c}}^{*},\\mathbf{c})+C(\\mathbf{b},b_{\\mathbf{b},\\mathbf{d}}^{*})+C(b_{\\mathbf{b},\\mathbf{d}}^{*},\\mathbf{d})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C(\\mathbf{a},b_{\\mathbf{b},\\mathbf{c}}^{*})+C(b_{\\mathbf{b},\\mathbf{c}}^{*},\\mathbf{c})+C(\\mathbf{b},b_{\\mathbf{a},\\mathbf{d}}^{*})+C(b_{\\mathbf{a},\\mathbf{d}}^{*},\\mathbf{d})}\\\\ &{\\qquad\\qquad\\qquad\\leq C(\\mathbf{b},b_{\\mathbf{b},\\mathbf{c}}^{*})+C(b_{\\mathbf{b},\\mathbf{c}}^{*},\\mathbf{c})+C(\\mathbf{a},b_{\\mathbf{a},\\mathbf{d}}^{*})+C(b_{\\mathbf{a},\\mathbf{d}}^{*},\\mathbf{d})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=C^{2}(\\mathbf{b},\\mathbf{c})+C^{2}(\\mathbf{a},\\mathbf{d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, the Inequality $(i)$ follows from the definition of $b_{a,c}^{*}$ that minimizes the MSE over the interval $[x_{\\mathsf{a}},x_{\\mathsf{c}}]$ and $b_{b,d}^{*}$ that minimizes it over $[x_{\\mathsf{b}},x_{\\mathsf{d}}]$ . Inequality $(i i)$ follows from the quadrangle inequality of $C$ (Lemma 4.2), as $\\mathtt{a}\\leq\\mathtt{b}\\leq b_{\\mathtt{b},\\mathtt{c}}^{*}\\leq b_{\\mathtt{a},\\mathtt{d}}^{*}$ , and thus ", "page_idx": 14}, {"type": "equation", "text": "$$\nC(\\mathbf{a},b_{\\mathbf{b},\\mathbf{c}}^{*})+C(\\mathbf{b},b_{\\mathbf{a},\\mathbf{d}}^{*})\\leq C(\\mathbf{b},b_{\\mathbf{b},\\mathbf{c}}^{*})+C(\\mathbf{a},b_{\\mathbf{a},\\mathbf{d}}^{*}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Case $b_{\\mathtt{b,c}}^{*}>b_{\\mathtt{a,d}}^{*}$ . In this case, we have that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C^{2}(\\mathbf{a},\\mathbf{c})+C^{2}(\\mathbf{b},\\mathbf{d})=C(\\mathbf{a},b_{\\mathbf{a},\\mathbf{c}}^{*})+C(b_{\\mathbf{a},\\mathbf{c}}^{*},\\mathbf{c})+C(\\mathbf{b},b_{\\mathbf{b},\\mathbf{d}}^{*})+C(b_{\\mathbf{b},\\mathbf{d}}^{*},\\mathbf{d})}\\\\ &{\\phantom{C^{2}(\\mathbf{a},\\mathbf{b},\\mathbf{c})+C(\\mathbf{a},b_{\\mathbf{a},\\mathbf{d}}^{*})+C(b_{\\mathbf{a},\\mathbf{d}}^{*},\\mathbf{c})+C(\\mathbf{b},b_{\\mathbf{b},\\mathbf{c}}^{*})+C(b_{\\mathbf{b},\\mathbf{c}}^{*},\\mathbf{d})}\\\\ &{\\phantom{C^{2}(\\mathbf{a},\\mathbf{c})+C(\\mathbf{b},b_{\\mathbf{b},\\mathbf{c}}^{*})+C(b_{\\mathbf{b},\\mathbf{c}}^{*},\\mathbf{c})+C(\\mathbf{a},b_{\\mathbf{a},\\mathbf{d}}^{*})+C(b_{\\mathbf{a},\\mathbf{d}}^{*},\\mathbf{d})}\\\\ &{\\phantom{C^{2}(\\mathbf{a},\\mathbf{b},\\mathbf{c})+C^{2}(\\mathbf{b},\\mathbf{c})+C^{2}(\\mathbf{a},\\mathbf{d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, the Inequality $(i)$ follows again from $b_{a,c}^{*}$ and $b_{b,d}^{*}$ being optimal for $[x_{\\mathsf{a}},x_{\\mathsf{c}}]$ and $[x_{\\mathsf{b}},x_{\\mathsf{d}}]$ . Inequality $(i i)$ follows from the quadrangle inequality of $C$ , as $b_{\\mathtt{a},\\mathtt{d}}^{*}\\leq b_{\\mathtt{b},\\mathtt{c}}^{*}\\leq\\mathtt{c}\\leq\\mathtt{d}$ and, therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\nC(b_{\\mathbf{a},\\mathbf{d}}^{*},\\mathsf{c})+C(b_{\\mathbf{b},\\mathsf{c}}^{*},\\mathsf{d})\\leq C(b_{\\mathbf{a},\\mathbf{d}}^{*},\\mathsf{d})+C(b_{\\mathbf{b},\\mathsf{c}}^{*},\\mathsf{c}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Together, this concludes the proof. ", "page_idx": 14}, {"type": "text", "text": "D No apparent closed-form solution for $s>3$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We explain why our acceleration method from Section 4 fails for $s>3$ . Consider computing the location of two additional quantization values $b\\leq u$ between $x_{a}$ and $x_{c}$ . ", "page_idx": 14}, {"type": "text", "text": "Similarly to the above analysis, we define by $Q(b,u)$ the resulting sum of variances for all entries in $[x_{a},x_{c}]$ . Then: ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ(b,u)=\\sum_{x\\in[x_{a},b]}(b-x)(x-x_{a})+\\sum_{x\\in(b,u]}(u-x)(x-b)+\\sum_{x\\in(u,x_{c}]}(x_{c}-x)(x-u).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Computing the partial derivatives, we then get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\frac{\\partial Q(b,u)}{\\partial b}}=\\displaystyle\\sum_{x\\in[x_{a},b]}(x-x_{a})-\\sum_{x\\in(b,u]}(u-x).}\\\\ &{{\\frac{\\partial Q(b,u)}{\\partial u}}=\\displaystyle\\sum_{x\\in(b,u]}(x-b)-\\sum_{x\\in(u,x_{c}]}(x_{c}-x).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The challenge now is that both derivatives are non-continuous, and there are multiple indices $i,j$ such that $Q(x_{i},x_{j})<0$ but $Q(x_{i+1},x_{j})\\geq0$ or $Q(x_{i},x_{j+1})\\ge0$ . Accordingly, it seems unlikely that a closed-form solution that is computable in constant time follows from this approach. ", "page_idx": 14}, {"type": "text", "text": "1: Input: $X\\in\\mathbb{R}^{d},s,m\\in\\mathbb{N}.$ .   \n2: $\\begin{array}{r}{S\\stackrel{{\\_}}{=}\\left\\{x_{1}+{\\ell}\\cdot\\frac{x_{d}-x_{1}}{m}\\mid{\\ell}\\in\\{0,\\ldots,m\\}\\right\\}}\\end{array}$   \n3: Preprocess $(X,m)$ \u25b7Enables computing $C_{m}[k,j]$ in constant time (Appendix E).   \n4: for $j=2$ to $m$ do   \n5: $M S E[2,j]=C_{m}[1,j]$   \n6: for $i=3$ to $s$ do   \n7: $K[i,\\cdot]=\\sf{S M A W K}(Z)$ \u25b7Where $Z[k,j]\\triangleq M S E[i-1,k]+C_{m}[k,j]\\quad\\forall k,j$ .   \n8: $M\\!S\\!E[i,j]=M\\!S\\!E[i-1,K[i,j]]+C_{m}[K[i,j],j]$ for all $j\\in\\{i,\\ldots,m\\}$ .   \n9: $Q=\\left\\{s_{0},s_{m}\\right\\}$   \n10: $j=m$   \n11: for $i=s$ down to 3 do   \n12: 13: $\\begin{array}{l}{j=K[i,j]}\\\\ {Q=Q\\cup\\{s_{j}\\}}\\end{array}$   \n14: return $Q$ ", "page_idx": 15}, {"type": "text", "text": "E Preprosessing for Apx. QUIVER ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that, for $S\\,=\\,\\left\\{x_{1}+{\\boldsymbol{\\ell}}\\cdot{\\frac{x_{d}-x_{1}}{m}}\\ |\\ {\\boldsymbol{\\ell}}\\in\\left\\{0,\\ldots,m\\right\\}\\right\\}$ and $\\begin{array}{r}{s_{\\ell}=x_{1}+\\ell\\cdot\\frac{x_{d}-x_{1}}{m}}\\end{array}$ , our goal is to compute the following arrays in $O(d)$ time: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{\\ell}=\\sum_{x\\in[s_{0},s_{\\ell}]}1\\quad,\\quad\\beta_{\\ell}=\\sum_{x\\in[s_{0},s_{\\ell}]}x\\quad,\\quad\\gamma_{\\ell}=\\sum_{x\\in[s_{0},s_{\\ell}]}x^{2}\\qquad\\qquad\\forall\\ell\\in\\{1,\\ldots,m\\}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denoting $\\delta\\,=\\,{\\frac{x_{d}-x_{1}}{m}}$ , the first step is to make a pass over the input and for each $x\\in X$ calculate $\\begin{array}{r}{\\ell_{x}=\\left\\lfloor\\frac{x-x_{1}}{\\delta}\\right\\rfloor}\\end{array}$ and set ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{\\ell}=\\sum_{x\\mid\\ell_{x}=\\ell}1\\quad,\\quad B_{\\ell}=\\sum_{x\\mid\\ell_{x}=\\ell}x\\quad,\\quad\\Gamma_{\\ell}=\\sum_{x\\mid\\ell_{x}=\\ell}x^{2}\\quad\\quad\\quad\\quad\\forall\\ell\\in\\{1,\\ldots,m\\}\\,\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we make an $O(m)$ time pass to compute the cumulative sums: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{\\ell}=\\sum_{i=1}^{\\ell}A_{i}\\quad,\\quad\\beta_{\\ell}=\\sum_{i=1}^{\\ell}B_{i}\\quad,\\quad\\gamma_{\\ell}=\\sum_{i=1}^{\\ell}\\Gamma_{i}\\quad\\qquad\\qquad\\forall\\ell\\in\\{1,\\dots,m\\}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We note that an optimization that proved useful for improving the runtime in practice is to remove empty intervals after the first step. That is, we retain only intervals for which $A_{\\ell}>0$ , thus reducing the number of intervals from $m$ to $m^{\\prime}\\leq m$ , which can be markedly smaller in practice. ", "page_idx": 15}, {"type": "text", "text": "F Apx. QUIVER Pseudo-code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We describe the pseudo-code of Apx. QUIVER, which is given by Algorithm 4. We start by preprocessing the input to obtain the $\\alpha,\\beta,\\gamma$ arrays ( Line 3). Next, we initialize the first row of the matrix, which only has $m$ columns, using $C_{m}$ (Line 4). Follows are $s-2$ invocations of the SMAWK algorithm, each yielding the next row in $M S E$ and its minimizers $K[i,\\cdot]$ (Line 6). Finally, we compute the resulting quantization value set $Q$ from $K$ and $S$ (Line 11). ", "page_idx": 15}, {"type": "text", "text": "G QUIVER Acceleration Evaluation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we evaluate by how much Accelerated QUIVER is faster than QUIVER. The results, depicted in Figure 4, show that Accelerated QUIVER is up to $5.4\\times$ faster for $s=3$ and is consistently faster throughout. Interestingly, the speedup is more significant in odd values of $s$ . This is because the number of SMAWK invocations is $\\lfloor s/2\\rfloor\\mathrm{~-~}1$ in Accelerated QUIVER (e.g., it does not invoke SMAWK at all for $s=3$ , only once for $s=5$ , etc.), compared to $s-2$ invocations in QUIVER. ", "page_idx": 15}, {"type": "text", "text": "H Additional evaluation results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Additional evaluation results of exact solutions. We provide results for additional input vectors distributions: Normal (Figure 5), Exponential (Figure 6), Truncated Normal (Figure 7), and Weibull (Figure 8). As shown, all follow the same trends in terms of vNMSE, while the runtime is largely independent of the input distribution. ", "page_idx": 16}, {"type": "text", "text": "I ASQ Approximation Baselines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the ZipML paper [26], the authors propose two heuristic methods for improving the runtime. The first heuristic includes calculating the optimal solution on a subset of $X$ called candidate points (CP); they further present an analysis that bounds the error with respect to the maximal difference between consecutive CPs and the maximal number of entries in $X$ between consecutive CPs; however, as they do not provide a way to select the CPs, we consider two natural choices: using Uniform CPs, i.e., $\\begin{array}{r}{\\left\\{x_{1}+\\overleftarrow{\\ell}\\cdot\\frac{x_{d}-x_{1}}{m}\\mid\\ell\\in\\{0,\\ldots,m\\}\\right\\}}\\end{array}$ .2 This variant is termed \u2018ZipML-CP Unif.\u2019 in our evaluation. The second choice of CP is Quantiles, which uses the set $\\left\\{x_{\\lfloor1+\\ell\\cdot(d-1)/m\\rfloor}\\mid\\ell\\in\\{0,\\ldots,m\\}\\right\\}$ . This variant is termed \u2018ZipML-CP Quant.\u2019 in our evaluation. ", "page_idx": 16}, {"type": "text", "text": "The second heuristic has a bicretira MSE guarantee: using $2s$ quantization values, it ensures that the MSE is at most twice that of the optimal solution with $s$ quantization values. This variant is termed \u2018ZipML 2-Apx\u2019 in our evaluation. ", "page_idx": 16}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/d763cec5c69bf826cd56c0d05ac7354adc500a8742e82a79e815ece640877aa8.jpg", "img_caption": ["Figure 4: The speedup attainable by Accelerated QUIVER, as a function of $s$ (for fixed $d=2^{23}$ ) and $d$ (for fixed $s=8,$ ), on the Normal and LogNormal distributions. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/3056f81871a2a1196dd8b05416f5414bca93451a349e0ab1bdd06e3c40569259.jpg", "img_caption": ["Figure 5: Comparing exact solutions with Norma $(0,1)$ distributed input. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/1fee2eb5842f7eff46d3bc77e0feaa6b106cbbdf5ed0e7490241270351e83819.jpg", "img_caption": ["Figure 6: Comparing exact solutions with Exponential(1) distributed input. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We also compare against ALQ [28], which fits the parameters of a truncated normal distribution to approximate the distribution of the input vector after normalizing it by its norm. It then uses an iterative solution to approximate the optimal quantization values of the fitted distribution up to the desired precision. As suggested by the authors, we use ten iterations, which were shown to converge to the optimal quantization values for the resulting (truncated normal) distribution. ", "page_idx": 17}, {"type": "text", "text": "Additional evaluation results of approximate solutions. Similarly, we show the approximation algorithms evaluation results for the various distributions and $s$ values: Normal (Figure 9), Exponential (Figure 10), Truncated Normal (Figure 11), and Weibull (Figure 12). Again, the runtime of all algorithms is weakly affected by the input distribution. Apx. QUIVER is always the most accurate for increasing $d$ values and has a near-optimal vNMSE when using a sufficient value for $m$ (e.g., $m\\geq400)$ while being markedly faster than all alternatives. ", "page_idx": 17}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/be89f9b5aac6aa093a2a430b1ac20c0a9999d9e80c4777d6831e8d095a34694c.jpg", "img_caption": ["Figure 7: Exact solutions with TruncNorm $\\langle\\mu=0,\\sigma^{2}=1,a=-1,b=1)$ distributed input. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/f3edbd03695e5c9adb1aaff5c2d1e19bc4923eaf98e89e499cab13b5d2d14328.jpg", "img_caption": ["Figure 8: Comparing exact solutions with Weibull $(1,1)$ distributed input. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/ae4b43bae0a4b9e39611c7c2f668e71ae44c6fd1b4556c49271e290131b2040f.jpg", "img_caption": ["Figure 9: Comparing approximate solutions with Normal $(0,1)$ distributed input. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "J Additional Overheads ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We measure the sort and quantize operations using the same EC2 server that is also equipped with an NVIDIA T4 GPU, PyTorch v2.1.2, and CUDA tool kit v12.3. As shown in Figure 13, both operations are fast even for large vectors, despite the usage of a somewhat weak GPU. This specific measurement was done over the LogNormal(0,1) distribution, but the sorting and quantization times are largely independent of the specific distribution and were similar to other tested distributions as well. ", "page_idx": 18}, {"type": "text", "text": "K Generalizing Our Algorithms to Weighted Inputs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We generalize our algorithms for processing sorted weighted inputs $X,W\\in\\mathbb{R}^{d}$ (where each entry has value $y_{\\ell}$ and weight $w_{\\ell}$ and $x_{1}\\leq x_{2}\\leq\\ldots,x_{d})$ .3 ", "page_idx": 18}, {"type": "text", "text": "Most of the algorithmic parts only require a revised method for computing $C$ in constant time, which is achieved through the modified pre-processing procedure below. ", "page_idx": 18}, {"type": "text", "text": "For simplicity, we only discuss the basic QUIVER variant and leave the acceleration as future work. ", "page_idx": 18}, {"type": "text", "text": "Pre-processing. To allow constant time computation of weighted $C$ , denoted $C_{w}$ , for weighted inputs we need another auxiliary array. Namely, we define the following: ", "page_idx": 18}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/2ce5f5a44579f56770375353b1263ddb038be1c6146a76714eeb799f521403fa.jpg", "img_caption": ["Figure 10: Comparing approximate solutions with Exponential(1) distributed input. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/64b1278c44d0145000291abd247af1d50ce2367a3ad03771a27cf630c168617d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 11: Approx. solutions with TruncNorm $(\\mu=0,\\sigma^{2}=1,a=-1,b=1)$ distributed input. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{j}=\\displaystyle\\sum_{(\\boldsymbol{x},\\boldsymbol{w})\\in{\\cal X}_{j}}\\boldsymbol{w}\\qquad,\\quad j\\in\\{1,\\ldots,d\\}~,}\\\\ &{\\beta_{j}=\\displaystyle\\sum_{(\\boldsymbol{x},\\boldsymbol{w})\\in{\\cal X}_{j}}\\boldsymbol{w}\\cdot\\boldsymbol{x}\\quad,\\quad j\\in\\{1,\\ldots,d\\}~,}\\\\ &{\\gamma_{j}=\\displaystyle\\sum_{(\\boldsymbol{x},\\boldsymbol{w})\\in{\\cal X}_{j}}\\boldsymbol{w}\\cdot\\boldsymbol{x}^{2}\\quad,\\quad j\\in\\{1,\\ldots,d\\}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can then write: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{C_{w}[k,j]=\\displaystyle\\sum_{x_{\\ell}\\in[x_{k},x_{j}]}w\\cdot(x_{j}-x_{\\ell})(x_{\\ell}-x_{k})}\\\\ &{\\qquad=\\displaystyle\\sum_{x_{\\ell}\\in(x_{k},x_{j}]}w\\cdot(x_{j}-x_{\\ell})(x_{\\ell}-x_{k})}\\\\ &{\\qquad=x_{j}\\cdot x_{k}\\cdot\\displaystyle\\sum_{x_{\\ell}\\in(x_{k},x_{j}]}w_{\\ell}+(x_{j}-x_{k})\\cdot\\displaystyle\\sum_{x_{\\ell}\\in(x_{k},x_{j}]}w_{\\ell}\\cdot x_{\\ell}-\\displaystyle\\sum_{x_{\\ell}\\in(x_{k},x_{j}]}w_{\\ell}\\cdot x_{\\ell}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad x_{\\ell}\\in(x_{k},x_{j}]}\\\\ &{=x_{j}\\cdot x_{k}\\cdot(\\alpha_{j}-\\alpha_{k})+(x_{j}-x_{k})\\cdot(\\beta_{j}-\\beta_{k})-(\\gamma_{j}-\\gamma_{k}).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Observe that $C_{w}$ clearly satisfies the quadrangle inequality, and thus, the correctness follows. The approximation variant also follows similarly. ", "page_idx": 19}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/5169d4f94ce5545dac4ca57b422da08dc6e2e9f87774982e0079e6b174a9945b.jpg", "img_caption": ["Figure 12: Comparing approximate solutions with Weibull $(1,1)$ distributed input. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "8ZLL6mu2qC/tmp/1e5899e3ad450c36c51e41bf12d47dd4b80ab6b53c82bc619f3eb3820d718dd0.jpg", "img_caption": ["Figure 13: Sort and quantization times $\\lvert s=16$ ) vs. $d$ on a T4 GPU. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide theoretical guarantees and supporting evaluation results. All our results are reproducible, and we plan to open-source our code upon publication. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We elaborate on the limitations in the Discussion section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Each claim is formulated to include all assumptions and is accompanied by a complete proof. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All our results are reproducible and we provide the entire set of parameters, datasets, and algorithm details. We further plan to open-source our code upon publication. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not use any proprietary data and the synthetic data can be reproduced by anyone. The code will be released as open source so anyone can easily reproduce our results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, all the details are documented in detail. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We repeat the experiments with different seeds and report error bars. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We measure execution time and document the used hardware. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Yes, it follows it fully. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We only used an implementation of the SMAWK algorithm by Daniel Steinberg, which is released under the MIT license. ", "page_idx": 25}, {"type": "text", "text": "We acknowledge this in the code where appropriate. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We plan to release the code upon publication with the appropriate license. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]