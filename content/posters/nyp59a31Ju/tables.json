[{"figure_path": "nyp59a31Ju/tables/tables_4_1.jpg", "caption": "Table 1: DDPG+BC is often the best policy extraction method. We aggregate the performances over the entire data-scaling matrix and then over 8 random seeds in each setting. Scores at or above 95% of the best score are highlighted in bold. The table shows that DDPG+BC is better than or as good as AWR in 15 out of 16 settings. We note that policy extraction hyperparameters are individually tuned for each setting (Figure 11).", "description": "This table presents an aggregated performance comparison of different offline RL algorithms across various environments.  It focuses on the impact of policy extraction method (AWR, DDPG+BC, SfBC) on performance, comparing these methods in conjunction with different value learning algorithms (IQL and SARSA or CRL). The results indicate that the behavior-constrained policy gradient method (DDPG+BC) significantly outperforms the other methods in most settings.", "section": "4.2 Results: Policy extraction mechanisms substantially affect data-scaling trends"}, {"figure_path": "nyp59a31Ju/tables/tables_18_1.jpg", "caption": "Table 1: DDPG+BC is often the best policy extraction method. We aggregate the performances over the entire data-scaling matrix and then over 8 random seeds in each setting. Scores at or above 95% of the best score are highlighted in bold. The table shows that DDPG+BC is better than or as good as AWR in 15 out of 16 settings. We note that policy extraction hyperparameters are individually tuned for each setting (Figure 11).", "description": "This table presents a comparison of the performance of three policy extraction methods (AWR, DDPG+BC, and SfBC) across various value learning algorithms and environments.  The performance is aggregated across the data-scaling matrices, representing the average performance across different data sizes. The results indicate that DDPG+BC consistently outperforms or matches the performance of AWR.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/tables/tables_18_2.jpg", "caption": "Table 1: DDPG+BC is often the best policy extraction method. We aggregate the performances over the entire data-scaling matrix and then over 8 random seeds in each setting. Scores at or above 95% of the best score are highlighted in bold. The table shows that DDPG+BC is better than or as good as AWR in 15 out of 16 settings. We note that policy extraction hyperparameters are individually tuned for each setting (Figure 11).", "description": "This table summarizes the performance of different policy extraction methods (AWR, DDPG+BC, SfBC) combined with different value learning algorithms (IQL, SARSA/CRL) across various tasks.  The results are aggregated across multiple runs and hyperparameter settings.  It highlights the superior performance of DDPG+BC in most cases compared to AWR and SfBC.", "section": "4.2 Results: Policy extraction mechanisms substantially affect data-scaling trends"}, {"figure_path": "nyp59a31Ju/tables/tables_19_1.jpg", "caption": "Table 1: DDPG+BC is often the best policy extraction method. We aggregate the performances over the entire data-scaling matrix and then over 8 random seeds in each setting. Scores at or above 95% of the best score are highlighted in bold. The table shows that DDPG+BC is better than or as good as AWR in 15 out of 16 settings. We note that policy extraction hyperparameters are individually tuned for each setting (Figure 11).", "description": "This table summarizes the performance of different policy extraction methods (AWR, DDPG+BC, and SfBC) across various value learning algorithms (IQL and SARSA/CRL) and environments.  It highlights the superior performance and scalability of DDPG+BC compared to AWR in most settings, suggesting that the choice of policy extraction significantly impacts offline RL performance.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}]