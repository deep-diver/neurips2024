{"references": [{"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "publication_date": "2020-05-01", "reason": "This paper provides a comprehensive overview of offline RL, establishing the foundational context for the current work's investigation into its bottlenecks."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-07", "reason": "This paper introduced the D4RL benchmark, a crucial dataset collection used extensively in the current work for empirical analysis."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "A minimalist approach to offline reinforcement learning", "publication_date": "2021-01-01", "reason": "This work proposed a key algorithm (DDPG+BC) used in the current paper's empirical study, highlighting its importance in policy extraction."}, {"fullname_first_author": "Benjamin Eysenbach", "paper_title": "Contrastive learning as goal-conditioned reinforcement learning", "publication_date": "2022-01-01", "reason": "This work presented CRL, a value learning method used in the current study, showcasing its effectiveness in goal-conditioned scenarios."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Offline reinforcement learning with implicit q-learning", "publication_date": "2022-01-01", "reason": "This paper proposed IQL, another value learning method used in the current study, demonstrating its efficiency in decoupled value learning and policy extraction."}]}