[{"figure_path": "nyp59a31Ju/figures/figures_4_1.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients ( , , ) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices showing the impact of varying value and policy data amounts on offline RL performance using different value learning (IQL, SARSA, CRL) and policy extraction (AWR, DDPG+BC, SfBC) methods across eight environments. Color gradients in each matrix illustrate how performance bottlenecks arise from either value function accuracy, policy extraction, or policy generalization, revealing insights into offline RL limitations and potential improvements.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_5_1.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure shows data-scaling matrices for different combinations of value learning methods (IQL, SARSA, CRL) and policy extraction methods (AWR, DDPG+BC, SfBC). Each matrix visualizes how performance changes with varying amounts of data used for value function training and policy extraction.  The color gradients indicate the main bottleneck in offline RL performance for each scenario (value function, policy, or generalization).", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_5_2.jpg", "caption": "Figure 3: AWR vs. DDPG actions.", "description": "This figure compares the test-time actions sampled from policies learned by AWR and DDPG+BC on exorl-walker.  It visually demonstrates that AWR actions are relatively centered around the origin, while DDPG+BC actions are more spread out, suggesting a higher degree of optimality for DDPG+BC.", "section": "4.4 Deep dive 2: Why is DDPG+BC better than AWR?"}, {"figure_path": "nyp59a31Ju/figures/figures_6_1.jpg", "caption": "Figure 4: AWR overfits.", "description": "This figure compares the training and validation policy losses of AWR and DDPG+BC on gc-antmaze-large with the smallest 0.1M dataset (8 seeds). The results show that AWR with a large temperature (\u03b1 = 3.0) causes severe overfitting, while DDPG+BC shows much better performance than AWR in low-data regimes.", "section": "4.3 Deep dive 1: How different are the scaling properties of AWR and DDPG+BC?"}, {"figure_path": "nyp59a31Ju/figures/figures_6_2.jpg", "caption": "Figure 5: Three distributions for the MSE metrics.", "description": "This figure illustrates the three different state distributions used to calculate the Mean Squared Error (MSE) metrics in the paper.  The training MSE measures the policy accuracy on states from the training dataset (Dtrain), the validation MSE measures accuracy on a held-out validation set (Dval), and the evaluation MSE measures policy accuracy on states encountered during evaluation (p\u03c0), representing the states the policy visits during deployment. This highlights the key difference between in-distribution (training and validation) and out-of-distribution (evaluation) performance, a central focus of the paper's analysis.", "section": "5 Empirical analysis 2: Policy generalization (B3)"}, {"figure_path": "nyp59a31Ju/figures/figures_7_1.jpg", "caption": "Figure 6: How do offline RL policies improve with additional interaction data? In many environments, offline-to-online RL only improves evaluation MSEs, while validation MSEs and training MSEs often remain completely flat (see Section 5 for the definitions of these metrics). This suggests that current offline RL algorithms may already be great at learning an effective policy on in-distribution states, and the performance of offline RL is often mainly determined by how well the policy generalizes on its own state distribution at test time.", "description": "This figure shows the results of an experiment evaluating how offline RL policies improve with additional online interaction data.  The results reveal that in many cases, offline-to-online RL primarily improves the evaluation MSE (mean squared error), a metric measuring performance on out-of-distribution test states. In contrast, the training and validation MSEs, representing performance on in-distribution states, often remain unchanged. This suggests the primary limitation of current offline RL algorithms lies not in learning optimal policies within the training data distribution, but rather in the ability of those policies to generalize to novel, out-of-distribution states encountered during deployment.", "section": "5 Empirical analysis 2: Policy generalization (B3)"}, {"figure_path": "nyp59a31Ju/figures/figures_8_1.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices that visualize how offline RL performance changes with varying amounts of data used for training value functions and policies.  It compares three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL, SARSA, and CRL) across eight different environments. The color gradients in the matrices indicate which factor\u2014value function learning or policy extraction\u2014is the primary bottleneck in each scenario.  Vertical gradients indicate policy is the main bottleneck, horizontal gradients indicate value function learning is the bottleneck, and diagonal gradients indicate both are significant bottlenecks.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_9_1.jpg", "caption": "Figure 8: Test-time policy improvement strategies (OPEX and TTT). Our two on-the-fly policy improvement techniques (OPEX and TTT) lead to substantial performance improvements on diverse tasks, by mitigating the test-time policy generalization bottleneck.", "description": "This figure shows the results of applying two test-time policy improvement techniques (OPEX and TTT) to six different offline RL tasks.  The results demonstrate that both OPEX and TTT significantly improve performance compared to standard offline RL methods (IQL and SfBC), especially in tasks where generalization to out-of-distribution test-time states is challenging. This supports the paper's hypothesis that policy generalization is a major bottleneck in offline RL.", "section": "5.2 Results: Test-time generalization is often the main bottleneck in offline RL"}, {"figure_path": "nyp59a31Ju/figures/figures_14_1.jpg", "caption": "Figure 9: A good state representation naturally enables test-time generalization, leading to substantially better performance.", "description": "This figure compares the performance of goal-conditioned behavioral cloning (BC) with two different state representations on the gc-antmaze-large environment.  The first uses the original state representation, while the second uses a transformed representation with a continuous invertible function.  Despite being topologically equivalent, the transformed representation leads to significantly better test-time generalization, as measured by Evaluation MSE. This highlights the importance of state representation in achieving good test-time performance in offline RL.", "section": "Policy generalization"}, {"figure_path": "nyp59a31Ju/figures/figures_16_1.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure shows data-scaling matrices which illustrate how the performance of offline RL is affected by varying amounts of data used for training the value function and the policy.  The three policy extraction methods (AWR, DDPG+BC, SfBC) and three value learning methods (IQL, SARSA, CRL) are tested in eight environments with varying data sizes and qualities. Color gradients in the matrices indicate whether the performance bottleneck is due to insufficient value function data, policy data, or a combination of both.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_1.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients ( , , ) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices that visualize the performance of offline RL algorithms with different amounts of data used for training value functions and policies. Each cell in the matrix represents the average performance of an algorithm under specific data amounts for value function training and policy extraction. The color gradient in each cell visually represents the effect of data amount on the performance, showing whether value function or policy learning is the main bottleneck for each algorithm and data setting.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_2.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices that analyze how varying the amounts of data used for training the value function and policy affects the performance of offline reinforcement learning algorithms.  Different value learning methods (IQL, SARSA, CRL) and policy extraction methods (AWR, DDPG+BC, SfBC) are compared. The color gradients in the matrices show how different aspects (value learning, policy extraction, generalization) contribute to offline RL performance bottlenecks.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_3.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices that analyze how varying the amounts of data used for training value functions and policies affects the performance of offline reinforcement learning algorithms.  It compares three policy extraction methods (AWR, DDPG+BC, SfBC) and three value learning methods (IQL, SARSA/CRL) across eight different environments. The color gradients in each matrix indicate which factor (value function quality, policy extraction method, or data amount) is the biggest bottleneck to performance in different data regimes.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_4.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices that analyze the bottlenecks in offline reinforcement learning.  It compares three policy extraction methods (AWR, DDPG+BC, SfBC) and three value learning methods (IQL, SARSA, CRL) across eight different environments. Varying the amount of data used for value function training and policy extraction reveals which component (value function or policy) presents the bigger performance bottleneck in different data regimes. The color gradients in the matrices indicate whether the bottleneck is primarily due to the policy, value function, or both.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_5.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices that visualize how offline RL performance changes with varying amounts of data used for value function training and policy extraction.  The matrices compare three policy extraction methods (AWR, DDPG+BC, SfBC) and three value learning methods (IQL, SARSA, CRL) across eight diverse environments. Color gradients in the matrices indicate whether the value function or policy extraction is the main bottleneck in offline RL performance for different data regimes.  Arrows indicate the direction of the performance change.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_6.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices which show the performance of offline RL algorithms with varying amounts of data used for training value functions and policies.  The color gradients in each matrix illustrate how different factors (value function learning, policy extraction, and generalization) influence the overall performance of offline RL in different scenarios. It helps to identify the main bottleneck of offline RL in various settings and algorithms.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_7.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure shows data-scaling matrices for different combinations of value learning and policy extraction methods in offline RL. Each matrix represents an environment, and each cell shows the performance with varying amounts of value and policy data. The color gradients indicate which factor (value function learning or policy extraction) is the bigger bottleneck for offline RL performance in each scenario.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_20_8.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure shows data-scaling matrices which analyze how the performance of offline RL is affected by the amount of data used for value function training and policy extraction.  It compares three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL, SARSA, and CRL) across eight different environments. The color gradients in the matrices indicate whether value learning or policy extraction is the bigger bottleneck to offline RL performance in each scenario. Vertical gradients indicate policy learning is the main bottleneck, horizontal gradients indicate value learning is the bottleneck, and diagonal gradients mean both are bottlenecks.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}, {"figure_path": "nyp59a31Ju/figures/figures_21_1.jpg", "caption": "Figure 1: Data-scaling matrices of three policy extraction methods (AWR, DDPG+BC, and SfBC) and three value learning methods (IQL and {SARSA or CRL}). To see whether the value or the policy imposes a bigger bottleneck, we measure performance with varying amounts of data for the value and the policy. The color gradients (\u2191,,\u27a1) of these matrices reveal how the performance of offline RL is bottlenecked in each setting.", "description": "This figure presents data-scaling matrices showing the performance of offline RL algorithms with different amounts of data used for training the value function and the policy.  The color gradients in each matrix cell indicate whether value function learning or policy extraction is the main bottleneck in offline RL performance for a given dataset size and algorithm combination.  The results suggest that the choice of policy extraction algorithm often has a larger impact on performance than the choice of value learning algorithm.", "section": "4 Empirical analysis 1: Is it the value or the policy? (B1 and B2)"}]