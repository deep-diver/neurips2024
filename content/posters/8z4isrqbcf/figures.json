[{"figure_path": "8z4isrqbcf/figures/figures_1_1.jpg", "caption": "Figure 1: Temporal compression difference between an image VAE and our video one.", "description": "This figure compares two different methods of temporal compression in video processing.  (a) shows the traditional method of uniform frame sampling, where frames are selected at equal intervals, resulting in jerky or uneven motion between frames. (b) demonstrates the proposed method using 3D convolutions for temporal compression, where the model processes the video data in its entirety, better capturing motion and temporal relationships between frames resulting in a smoother video.", "section": "1 Introduction"}, {"figure_path": "8z4isrqbcf/figures/figures_1_2.jpg", "caption": "Figure 9: Reconstruction results of consecutive frames using CV-VAE.", "description": "This figure shows the reconstruction results of consecutive frames from three different video clips using the proposed CV-VAE model. Each row presents a video clip, with the \"Real\" column displaying the original frames and the \"Reconstructed\" column showing the frames reconstructed by CV-VAE.  The results demonstrate the model's ability to reconstruct video frames with high fidelity, maintaining consistency in color, structure, and motion, even across multiple frames.", "section": "A.4 Qualitative Examples of Video Reconstruction"}, {"figure_path": "8z4isrqbcf/figures/figures_3_1.jpg", "caption": "Figure 3: (a-b): Two different regularization methods; (c) The framework of CV-VAE with the regularization of the pretrained 2D decoder.", "description": "This figure illustrates two different regularization methods used in the CV-VAE model for latent space alignment between the video VAE and the image VAE.  (a) shows encoder regularization where latent space from the pretrained 2D encoder is used to regularize the 3D encoder's output. (b) shows decoder regularization where the output of the 3D decoder is passed through the pretrained 2D decoder to create a regularization loss. Finally, (c) shows the overall framework of the CV-VAE model using decoder regularization, incorporating the pretrained 2D decoder, a 3D discriminator, and a mapping function to align latents.", "section": "3 Method"}, {"figure_path": "8z4isrqbcf/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative comparison of image and video reconstruction. Top: Reconstruction with different Image VAE models (i.e., VQGAN [12] and VAE-SD2.1 [28]) on images; Bottom: Reconstruction with different Video VAE models (i.e., TATS [14] and VAE-OSP [1]) on video frames.", "description": "This figure compares the image and video reconstruction quality of different Variational Autoencoders (VAEs). The top row shows the reconstruction of images using VQGAN and VAE-SD2.1, while the bottom row shows the reconstruction of video frames using TATS and VAE-OSP. The results demonstrate the superior performance of the proposed CV-VAE in both image and video reconstruction.", "section": "4.2 Image and Video Reconstruction"}, {"figure_path": "8z4isrqbcf/figures/figures_6_2.jpg", "caption": "Figure 5: Text-to-image generation comparison. In each pair, the left is generated by the SD2.1 [28] with the image VAE while the right is generated by the SD2.1 with our video VAE.", "description": "This figure compares the image generation results of Stable Diffusion 2.1 (SD2.1) using its original image VAE and using the proposed CV-VAE.  Each row shows a different text prompt used to generate the images. The left column presents images generated by SD2.1 with its original image VAE, and the right column presents images generated by SD2.1 but with the authors' proposed CV-VAE replacing the original VAE. This comparison showcases how the CV-VAE affects image generation results compared to the original SD2.1 method.", "section": "4.3 Compatibility with Existing Models"}, {"figure_path": "8z4isrqbcf/figures/figures_8_1.jpg", "caption": "Figure 6: Comparison between the image VAE and our video VAE on image-to-video generation of SVD [4]. \u2018SVD\u2019 means using the image VAE. \u2018SVD + CV-VAE\u2019 means using our video VAE and tuning the output layer of SVD. Click to play the video clips with Adobe or Foxit PDF Reader.", "description": "This figure compares the image-to-video generation results using SVD with the original image VAE (SVD) and with the proposed CV-VAE (SVD + CV-VAE).  The top row shows the results generated by SVD, while the bottom row shows the results generated after integrating CV-VAE into SVD and fine-tuning the output layer.  The videos are generated using the first frame as a condition and the same random seed.  CV-VAE significantly improves the smoothness and length of the generated videos. The reader is directed to click to play the video clips.", "section": "4.3 Compatibility with Existing Models"}, {"figure_path": "8z4isrqbcf/figures/figures_13_1.jpg", "caption": "Figure 7: Architecture of CV-VAE.", "description": "This figure shows the detailed architecture of the CV-VAE model, including the encoder, decoder, and discriminator.  The encoder and decoder are both composed of multiple ResBlock layers, downsampling and upsampling layers, and attention mechanisms. The 3D convolutional layers are highlighted in red, showcasing the key difference from a standard 2D VAE and the method used to inflate it into a 3D version. The discriminator is similarly constructed with convolutional and ResBlock layers. The architecture is designed to efficiently handle both image and video data by employing different inflation strategies in distinct blocks, allowing for truly spatio-temporal compression.", "section": "3.2 Architecture Design of Video VAE"}, {"figure_path": "8z4isrqbcf/figures/figures_15_1.jpg", "caption": "Figure 8: Our CV-VAE is capable of encoding and reconstructing images with high fidelity.", "description": "This figure shows several pairs of images. Each pair consists of an original image on the left and a reconstructed version of that image on the right. The reconstructed images were generated by the authors' CV-VAE model. The figure aims to demonstrate the high-fidelity reconstruction capability of their model, indicating that the model can effectively encode and decode images while preserving details and textures.", "section": "A.3 Qualitative Examples of Image Reconstruction"}, {"figure_path": "8z4isrqbcf/figures/figures_16_1.jpg", "caption": "Figure 9: Reconstruction results of consecutive frames using CV-VAE.", "description": "This figure shows the reconstruction results of four consecutive frames from different video clips using the proposed CV-VAE model.  Each row represents a different video clip, with the \"Real\" column showing the original frames and the \"Reconstructed\" column showing the frames generated by CV-VAE. The results demonstrate the ability of CV-VAE to reconstruct videos with high fidelity, preserving color, structure, and motion information.", "section": "A.4 Qualitative Examples of Video Reconstruction"}, {"figure_path": "8z4isrqbcf/figures/figures_17_1.jpg", "caption": "Figure 10: Comparison between the image VAE and our video VAE on text-to-video generation of VC2 [7]. We fine-tuned the last layer of U-Net in VC2 to adapt it to CV-VAE. VC2 generates videos with a resolution of 16 \u00d7 320 \u00d7 512, while the \u2018VC2 + CV-VAE\u2019 produces videos of 61 \u00d7 320 \u00d7 512 resolution under the same computation. The missing frames in the VC2 results are marked in gray.", "description": "This figure compares the video generation capabilities of the original VideoCrafter2 model (VC2) with a modified version that uses the proposed CV-VAE.  The prompt used is \"pianist playing somber music, abstract style, non-representational, colors and shapes, expression of feelings, highly detailed\". The comparison shows that integrating CV-VAE into VC2 allows for the generation of significantly longer videos (61 frames vs 16 frames) with smoother transitions, while maintaining comparable computational cost.  The missing frames in the original VC2 output are highlighted in gray.", "section": "4.3 Compatibility with Existing Models"}, {"figure_path": "8z4isrqbcf/figures/figures_17_2.jpg", "caption": "Figure 10: Comparison between the image VAE and our video VAE on text-to-video generation of VC2 [7]. We fine-tuned the last layer of U-Net in VC2 to adapt it to CV-VAE. VC2 generates videos with a resolution of 16 \u00d7 320 \u00d7 512, while the \u2018VC2 + CV-VAE\u2019 produces videos of 61 \u00d7 320 \u00d7 512 resolution under the same computation. The missing frames in the VC2 results are marked in gray.", "description": "This figure compares the video generation results of Videocrafter2 (VC2) using a standard 2D image VAE versus using the proposed CV-VAE.  The CV-VAE, when integrated into VC2, produces significantly more frames (61 vs 16) while maintaining similar computational costs, resulting in smoother and more fluid videos. The grayed-out areas in the VC2 results highlight the missing frames due to the lower frame rate.", "section": "4.3 Compatibility with Existing Models"}, {"figure_path": "8z4isrqbcf/figures/figures_17_3.jpg", "caption": "Figure 10: Comparison between the image VAE and our video VAE on text-to-video generation of VC2 [7]. We fine-tuned the last layer of U-Net in VC2 to adapt it to CV-VAE. VC2 generates videos with a resolution of 16 \u00d7 320 \u00d7 512, while the \u2018VC2 + CV-VAE\u2019 produces videos of 61 \u00d7 320 \u00d7 512 resolution under the same computation. The missing frames in the VC2 results are marked in gray.", "description": "This figure compares the video generation results of the original Videocrafter2 model (VC2) with those obtained after integrating the proposed CV-VAE.  By fine-tuning only a small portion of the VC2 model (last layer of U-Net), the CV-VAE significantly increases the number of generated frames (from 16 to 61) while maintaining comparable computational resources.  The grayed-out frames in the VC2 results highlight the increased frame count provided by CV-VAE, showcasing smoother and more comprehensive video generation.", "section": "4.3 Compatibility with Existing Models"}, {"figure_path": "8z4isrqbcf/figures/figures_17_4.jpg", "caption": "Figure 10: Comparison between the image VAE and our video VAE on text-to-video generation of VC2 [7]. We fine-tuned the last layer of U-Net in VC2 to adapt it to CV-VAE. VC2 generates videos with a resolution of 16 \u00d7 320 \u00d7 512, while the \u2018VC2 + CV-VAE\u2019 produces videos of 61 \u00d7 320 \u00d7 512 resolution under the same computation. The missing frames in the VC2 results are marked in gray.", "description": "This figure compares the video generation capabilities of the original VideoCrafter2 (VC2) model with a 2D VAE and a modified version of VC2 that incorporates the authors' proposed CV-VAE (a 3D video VAE).  The comparison highlights the increased frame count achieved by using the CV-VAE, resulting in smoother and more detailed videos with the same computational cost. The gray areas in the top row indicate missing frames generated by VC2, which are filled by CV-VAE in the bottom row.", "section": "4.3 Compatibility with Existing Models"}]