[{"figure_path": "YTHJ8O6SCB/figures/figures_1_1.jpg", "caption": "Figure 1: We present SpatialPIN, a framework to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with 3D priors in a zero-shot, training-free manner.", "description": "This figure illustrates the SpatialPIN framework.  It shows a modular pipeline that enhances Vision-Language Models (VLMs) spatial reasoning abilities by combining prompting techniques with 3D priors from multiple foundation models.  The process is zero-shot and training-free, making it efficient and adaptable.  The framework takes as input an image and a spatial task.  Through a series of steps involving 2D scene understanding, 3D scene understanding, and solving with spatial reasoning, the framework arrives at an answer to the posed spatial visual question or task.  The figure visually represents this process.", "section": "1 Introduction"}, {"figure_path": "YTHJ8O6SCB/figures/figures_2_1.jpg", "caption": "Figure 2: SpatialPIN. Our plug-and-play framework is fully modularized and designed for zero-shot deployment. Each module can be easily replaced with the latest updates. Exact prompts for VLMs are in Appendix.", "description": "This figure illustrates the SpatialPIN framework, a modular pipeline designed for zero-shot deployment.  It enhances the spatial understanding capabilities of Vision-Language Models (VLMs) through progressive interactions with scene decomposition, comprehension, and reconstruction. The framework consists of several modules, each easily replaceable with the latest improvements in its domain.  The figure shows the flow of processing from input RGB images and spatial tasks through 2D and 3D scene understanding to the final solving of spatial reasoning tasks (Spatial VQA and robotic task planning).  The Appendix contains details of the prompts used.", "section": "3 Method"}, {"figure_path": "YTHJ8O6SCB/figures/figures_3_1.jpg", "caption": "Figure 3: Our method of partial 3D scene reconstruction (a). The reconstructed scene (b) and the input image (c) show high alignment.", "description": "This figure shows the results of a partial 3D scene reconstruction method.  (a) illustrates the method's process, visualizing the scene's reconstruction from different perspectives.  (b) presents the reconstructed 3D scene, and (c) displays the original input image. The high alignment between the reconstructed scene and the input image demonstrates the effectiveness of the reconstruction method.", "section": "3 Method"}, {"figure_path": "YTHJ8O6SCB/figures/figures_4_1.jpg", "caption": "Figure 4: Qualitative examples of spatial VQA. SpatialPIN outputs answers with fine-grained 3D reasoning. Zoom in for better view.", "description": "This figure presents a qualitative comparison of SpatialPIN and SpatialVLM's performance on three spatial visual question answering (VQA) tasks: Intra-Image Object Relations VQA (IaOR-VQA), Intra-Image Angular Discrepancies VQA (IaAD-VQA), and Inter-Image Spatial Dynamics VQA (IrSD-VQA). For each task, example questions and answers from both models are shown. SpatialPIN demonstrates superior performance, providing more accurate and detailed answers that incorporate fine-grained 3D reasoning.", "section": "4.1 Spatial Visual Question Answering"}, {"figure_path": "YTHJ8O6SCB/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative examples of pick and stack (top) and task trajectory planning (bottom). SpatialPIN successfully outputs picking and stacking policies using spatial reasoning and plans 3D trajectories with geometric awareness to align with task descriptions.", "description": "This figure shows two examples of SpatialPIN's performance on pick-and-stack tasks. The top row shows images of a robot successfully picking up and stacking a banana on a cracker box, and the bottom row shows the successful task of slicing an orange with a knife on a cutting board. For each task, SpatialPIN not only provides the solution but also shows a 3D trajectory that the robot uses.", "section": "4.2 Robotics Pick and Stack"}, {"figure_path": "YTHJ8O6SCB/figures/figures_15_1.jpg", "caption": "Figure 6: Example input images of all forms of spatial VQA.", "description": "This figure shows example images used for the three types of spatial visual question answering (VQA) tasks explored in the paper: Intra-Image Object Relations VQA, Intra-Image Angular Discrepancies VQA, and Inter-Image Spatial Dynamics VQA.  Each row represents a different type of VQA task, showcasing the diverse image scenes and object arrangements used to evaluate the model's spatial reasoning capabilities.", "section": "Additional Experiments and Details"}, {"figure_path": "YTHJ8O6SCB/figures/figures_17_1.jpg", "caption": "Figure 7: Pipeline for discovering and planning for robotics tasks from a single image. It incorporates the task proposal and motion planning modules based on SpatialPIN.", "description": "This figure illustrates the overall pipeline of SpatialPIN for robotics tasks.  It starts with an input RGB image, which undergoes 2D scene understanding (inpainting, object identification).  This is followed by 3D scene understanding (depth estimation, 3D object reconstruction), which provides the necessary information for task proposal and planning. The VLM proposes a task, and the system uses motion planning to generate a smoothed trajectory, culminating in the generated task execution.", "section": "4.3 Discovering and Planning for Robotics Tasks from a Single Image"}, {"figure_path": "YTHJ8O6SCB/figures/figures_18_1.jpg", "caption": "Figure 8: Dataset statistics. Our dataset presents 51 scenes\u201413 from NOCS and 38 captured from varied perspectives\u2014featuring a wide range of object categories, quantities, and a diverse set of tasks and planned trajectories.", "description": "This figure presents a statistical overview of the dataset used in the paper.  It shows a pie chart breaking down the dataset's scene composition (13 scenes from the NOCS dataset and 38 captured from various locations). Additionally, images illustrate the diversity of scenes, object types, quantities, and task variations within the dataset.", "section": "4 Experiments"}, {"figure_path": "YTHJ8O6SCB/figures/figures_19_1.jpg", "caption": "Figure 9: More qualitative examples. With diverse input scenes and proposed tasks, our framework produces 3D trajectories with geometric awareness that aligns with the task descriptions. Zoom in for better view.", "description": "This figure shows five example scenarios where SpatialPIN successfully generates 3D trajectories for various tasks. Each row shows an input image, a task description, and the planned trajectory.  The tasks involve manipulating objects in different scenes and viewpoints, demonstrating the framework's ability to handle diverse and complex situations.", "section": "More Qualitative Examples"}]