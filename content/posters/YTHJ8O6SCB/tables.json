[{"figure_path": "YTHJ8O6SCB/tables/tables_5_1.jpg", "caption": "Table 1: Qualitative IaOR-VQA. We exclude comparisons to PaLI [14], PaLM-E [20], and PaLM 2-E [3] as they are not open source, and include experiments with GPT-40 [1] in addition to GPT-4V [47], LLaVA-1.5 [40], and InstructBLIP [18]. We use the HF version of SpatialVLM [51].", "description": "This table presents the results of qualitative Intra-image Object Relations Visual Question Answering (IaOR-VQA).  It compares the accuracy of several vision-language models (VLMs) with and without the SpatialPIN framework.  The models tested include GPT-4V, GPT-40, LLaVA-1.5, InstructBLIP, and SpatialVLM.  The \"w/o ours\" column represents the accuracy without SpatialPIN, while the \"w ours\" column shows the accuracy after integrating SpatialPIN.  The table highlights the significant improvement in accuracy achieved by using SpatialPIN across all models.", "section": "4.1 Spatial Visual Question Answering"}, {"figure_path": "YTHJ8O6SCB/tables/tables_5_2.jpg", "caption": "Table 2: Quantitative IaOR-VQA. SpatialVLM measures the accuracy by the percentage of answers that fall within 0.5x to 2.0x of the ground truth value. We also evaluate within narrower ranges of 0.75x to 1.33x and 0.9x to 1.11x. \"Output number\" means VLMs produce number in the response instead of vague descriptions.", "description": "This table presents the quantitative results for Intra-Image Object Relations Visual Question Answering (IaOR-VQA).  It shows the accuracy of different Vision-Language Models (VLMs) in answering quantitative spatial questions, comparing their performance with and without the SpatialPIN framework. Accuracy is measured by how closely the VLM's numerical answer matches the ground truth, considering different acceptable ranges of error (0.5x to 2.0x, 0.75x to 1.33x, and 0.9x to 1.11x).  It also notes the percentage of times the VLMs produced a numerical answer instead of a descriptive one.", "section": "4.1 Spatial Visual Question Answering"}, {"figure_path": "YTHJ8O6SCB/tables/tables_6_1.jpg", "caption": "Table 4: Quantitative IaAD-VQA & IrSD-VQA.", "description": "This table presents the quantitative results for two new Spatial Visual Question Answering (VQA) tasks: Intra-Image Angular Discrepancies VQA (IaAD-VQA) and Inter-Image Spatial Dynamics VQA (IrSD-VQA).  It shows the accuracy of different Vision-Language Models (VLMs) with and without the SpatialPIN framework.  The accuracy is measured by the percentage of answers that fall within specific ranges (0.5x to 2.0x, 0.75x to 1.33x, and 0.9x to 1.11x) of the ground truth value.  The \"Output numbers %\" row indicates the percentage of times the VLMs produced numerical answers instead of vague descriptions. The table compares the performance of GPT-40 and GPT-40 enhanced with SpatialPIN against SpatialVLM, highlighting the improvement achieved by SpatialPIN.", "section": "4.1 Spatial Visual Question Answering"}, {"figure_path": "YTHJ8O6SCB/tables/tables_7_1.jpg", "caption": "Table 5: Pick and stack. We classify the success rates into: 1) successfully picked, 2) successfully picked and contacted the target object but slipped/collided, and 3) successfully picked and stacked.", "description": "This table presents the success rates of a pick-and-stack robotics task, categorized into three levels: successfully picked, successfully picked and contacted but failed to stack, and successfully picked and stacked.  The results are compared across three different methods: GPT-40 + ours (the proposed method), using only direct 3D information, and SpatialVLM + RRT*. The table quantifies the effectiveness of different approaches for robotic manipulation tasks.", "section": "4.2 Robotics Pick and Stack"}, {"figure_path": "YTHJ8O6SCB/tables/tables_7_2.jpg", "caption": "Table 6: User study. Ratings (scale 1 \u2013 5) are averaged.", "description": "This table presents the results of a user study evaluating the quality of generated task executions in terms of task description alignment.  Specifically, 25 users rated 5 translation and 5 rotation task executions on a scale of 1 to 5, with higher scores indicating better alignment.  The table provides the average rating for each type of task (Rotation, Translation, Manipulation).", "section": "4.3 Discovering and Planning for Robotics Tasks from a Single Image"}, {"figure_path": "YTHJ8O6SCB/tables/tables_8_1.jpg", "caption": "Table 8: Ablation study. For quantitative IaOR-VQA, the accuracy is measured by the answers that fall within 0.75x to 1.33x of the ground truth value.", "description": "This table presents the results of an ablation study conducted to evaluate the effectiveness of each module within the SpatialPIN framework.  The study assesses the impact of removing different modules (2D understanding, coarse 3D understanding, fine-grained 3D understanding) on the performance of the system, both qualitatively and quantitatively on IaOR-VQA tasks.  The baseline results are compared against the performance of the complete SpatialPIN framework to highlight the contribution of each module. Quantitative results are specifically calculated based on the accuracy of the answers falling within a range (0.75x to 1.33x) of the ground truth.", "section": "4.4 Ablation Study"}, {"figure_path": "YTHJ8O6SCB/tables/tables_8_2.jpg", "caption": "Table 7: Results for machine understanding (classification and generation) on 278 task executions.", "description": "This table presents the results of evaluating the machine's understanding of the generated task executions.  It shows the raw accuracy, false positive rate, true accuracy (raw accuracy minus false positive rate), and OpenCLIP similarity score. The OpenCLIP score measures the alignment between the machine's perception and the human-provided ground truth descriptions of the tasks.", "section": "4.3 Discovering and Planning for Robotics Tasks from a Single Image"}, {"figure_path": "YTHJ8O6SCB/tables/tables_8_3.jpg", "caption": "Table 7: Results for machine understanding (classification and generation) on 278 task executions.", "description": "This table presents the results of evaluating the machine's understanding of generated task executions.  It compares the raw accuracy against a false positive rate to arrive at a true accuracy score. OpenCLIP scores are also provided, offering an additional metric for understanding the results.", "section": "4.3 Discovering and Planning for Robotics Tasks from a Single Image"}, {"figure_path": "YTHJ8O6SCB/tables/tables_19_1.jpg", "caption": "Table 10: Comparison of task diversity. We sample 106 proposed tasks for fair comparison with RoboGen and previous RL benchmarks.", "description": "This table compares the diversity of tasks generated by the proposed SpatialPIN framework with those from other existing methods such as RoboGen, Behavior-100, RLBench, MetaWorld, and Maniskill2.  Diversity is measured using Self-BLEU and embedding similarity, where lower scores indicate higher diversity.  The table shows that SpatialPIN generates a more diverse set of tasks compared to the other methods.", "section": "4 Experiments"}, {"figure_path": "YTHJ8O6SCB/tables/tables_19_2.jpg", "caption": "Table 7: Results for machine understanding (classification and generation) on 278 task executions.", "description": "This table presents the results of evaluating the machine's understanding of the generated task executions.  It compares the performance of a video understanding model (Video-LLaVA-7B) in classifying and generating descriptions of task executions. The \"Raw Acc\" column shows the model's accuracy, while \"Fal-Pos Rate\" is the false positive rate. The \"True Acc\" column is obtained after adjusting for the false positive rate to provide a more fair evaluation.", "section": "4.3 Discovering and Planning for Robotics Tasks from a Single Image"}]