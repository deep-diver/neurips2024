{"references": [{"fullname_first_author": "R. Anil", "paper_title": "Memory efficient adaptive optimization", "publication_date": "2019-MM-DD", "reason": "This paper proposes memory-efficient adaptive optimization methods, a topic highly relevant to the core goal of VeLoRA, which is to reduce memory consumption in training LLMs."}, {"fullname_first_author": "R. Anil", "paper_title": "Gemini: A family of highly capable multimodal models", "publication_date": "2023-MM-DD", "reason": "This paper introduces Gemini, a family of multimodal models, showcasing advancements in LLM capabilities that directly relate to the performance gains sought by the authors of VeLoRA."}, {"fullname_first_author": "T. Dettmers", "paper_title": "QLoRA: Efficient finetuning of quantized LLMs", "publication_date": "2023-MM-DD", "reason": "This paper introduces QLoRA, a method directly compared against VeLoRA, highlighting its significance in the field of memory-efficient LLM training and benchmarking."}, {"fullname_first_author": "E. J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-MM-DD", "reason": "LoRA is a prominent parameter-efficient fine-tuning technique for LLMs, and this paper is frequently cited and compared to the proposed VeLoRA method, demonstrating its importance."}, {"fullname_first_author": "J. Zhao", "paper_title": "Galore: Memory efficient training by gradient low-rank projection", "publication_date": "2024-MM-DD", "reason": "This paper, like QLoRA, is a direct competitor to the method introduced in VeLoRA and is used in the benchmarking process for memory-efficient LLM training."}]}