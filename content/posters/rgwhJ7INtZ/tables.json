[{"figure_path": "rgwhJ7INtZ/tables/tables_21_1.jpg", "caption": "Table 1: Summary of Parametrizations. The non residual block layers represent those trainable vectors/matrices that are not in a residual block (typically, the first and last ones). For these layers, we prescribe how to rescale both the weight variance and scaling multiplier.", "description": "This table summarizes the scaling of different hyperparameters for various neural network parametrizations.  It shows how the learning rate (\u03b7), output multiplier (\u03b3), and depth-scaling exponents (\u03b1) are adjusted for different methods (SGD and Adam) with and without residual connections.  The table highlights the \u00b5P parametrization as a special case of Depth-\u00b5P where depth dependence is ignored.  Note that for Adam, the epsilon parameter is set to zero.", "section": "K Summary of Feature Learning Parametrizations"}, {"figure_path": "rgwhJ7INtZ/tables/tables_38_1.jpg", "caption": "Table 1: Summary of Parametrizations. The non residual block layers represent those trainable vectors/matrices that are not in a residual block (typically, the first and last ones). For these layers, we prescribe how to rescale both the weight variance and scaling multiplier.", "description": "This table summarizes the different parameterizations used in the paper for scaling neural networks. It shows how the learning rate (\u03b7), output multiplier (\u03b3), depth-scaling exponent (\u03b1), and non-residual block layers are scaled for different optimizers (SGD and Adam).  The \u00b5P parametrization is a special case of Depth-\u00b5P where depth dependence is ignored.", "section": "K Summary of Feature Learning Parametrizations"}]