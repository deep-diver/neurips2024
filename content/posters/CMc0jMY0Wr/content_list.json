[{"type": "text", "text": "Optimal Private and Communication Constraint Distributed Goodness-of-Fit Testing for Discrete Distributions in the Large Sample Regime ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lasse Vuursteen Department of Statistics and Data Science The Wharton School of the University of Pennsylvania Philadelphia, PA 19104 lassev@wharton.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study distributed goodness-of-fit testing for discrete distribution under bandwidth and differential privacy constraints. Information constraint distributed goodness-of-fti testing is a problem that has received considerable attention recently. The important case of discrete distributions is theoretically well understood in the classical case where all data is available in one \u201ccentral\u201d location. In a federated setting, however, data is distributed across multiple \u201clocations\u201d (e.g. servers) and cannot readily be shared due to e.g. bandwidth or privacy constraints that each server needs to satisfy. We show how recently derived results for goodness-of-fit testing for the mean of a multivariate Gaussian model extend to the discrete distributions, by leveraging Le Cam\u2019s theory of statistical equivalence. In doing so, we derive matching minimax upper- and lower-bounds for the goodness-of-fti testing for discrete distributions under bandwidth or privacy constraints in the regime where number of samples held locally are large. ", "page_idx": 0}, {"type": "text", "text": "Keywords: distributed inference, goodness-of-fti testing, differential privacy, communication constraint, federated learning, statistical equivalence. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning is a fundamental problem in statistics and machine learning, where data is distributed across multiple locations (e.g. servers) and cannot readily be shared due to e.g. bandwidth or privacy constraints that each server needs to satisfy. The primary goal in these distributed data settings is to perform a single global inference task, such as hypothesis testing, regression, or classification, by aggregating the local information from each server. ", "page_idx": 0}, {"type": "text", "text": "Starting a few decades ago, investigations into distributed settings with bandwidth and other information constraints originated in the electrical engineering community, under the names \u201cdecentralized decision theory / the CEO problem\u201d e.g. [90, 12, 92, 18, 58, 87] or \u201cinference under multiterminal compression\u201d (see [88] for an overview). These were largely motivated by applications where data is by construction observed and processed locally, such as astronomy, meteorology, seismology, surveillance systems, wireless communication, military radar or air traffic control systems. ", "page_idx": 0}, {"type": "text", "text": "Modern federated learning often involves data distributed across siloed data centers (e.g., hospitals) or networks of cellphone users, applied in areas such as word prediction, facial and voice recognition, virtual assistants like Siri or Google Assistant, autonomous vehicles, and earthquake prediction [65, 56, 51, 73, 67, 31]. In these settings, bandwidth often becomes a limited or costly resource [57]. ", "page_idx": 0}, {"type": "text", "text": "Similarly, with advances in electronic record keeping, privacy has become a more and more pressing issue. These issues are prominent in tech industry products [32], including many federated learning applications mentioned earlier, as well as in scientific fields like medical sciences [60] and social sciences [69]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Methods that preserve privacy have been around in the statistics community for some time, starting in the 1980\u2019s [40, 41]. The current leading formal privacy framework is that of differential privacy (DP), as introduced in [42]. DP is a mathematical guarantee, describing whether results or data sets can be considered \u201cprivacy preserving\u201d and hence can be openly published. Whilst many other privacy frameworks exist, this notion of privacy holds a prominent position both theoretically and practically, finding application within industry giants like Google [45], Microsoft [35], Apple [89], as well as governmental entities such as the US Census Bureau [74]. ", "page_idx": 1}, {"type": "text", "text": "Quantifying the trade-off between privacy and statistical power means that researchers and data analysts can make an appropriate balance between data privacy and meaningful analysis. Similarly, by quantifying the impact of bandwidth constraints, systems can be designed to work as efficiently as possible within such bandwidth constraints. ", "page_idx": 1}, {"type": "text", "text": "The performance of distributed inference under bandwidth or differential privacy is well-studied for various estimation problems. For instance, distributed estimation under differential privacy has been studied for the many-normal-means model, discrete distributions and parametric models in [37, 38, 1, 98, 3], and density estimation [75, 59, 21], and nonparametric regression in [23]. Bandwidth constraints have been studied for the many-normal-means and parametric models in e.g. [101, 39, 77, 20, 97, 50, 26, 25], as well as nonparametric models, including Gaussian white noise [102], nonparametric regression [82], density estimation [17, 4], general, abstract settings [99] and online learning [95]. Distributed adaptive estimation methods under bandwidth constraints, where adaptation occurs to the unknown regularity of the functional parameter of interest, were derived in [82, 83, 25]. Testing simple hypotheses under bandwidth constraints has been studied by e.g. [92] and under differential privacy constraints by [29]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we consider goodness-of-fti testing for discrete distributions (i.e. the multinomial model) in scenarios where the number of samples received by each server is large. Specifically, we study testing a simple null versus a composite alternative, in the setting where $m$ servers receive $n$ observations each from a distribution on a sample space of cardinality $d$ , where $n$ is large comparatively to $m$ and $d$ . Recently, such multinomial distributed data have found many applications in areas that handle very large samples over (possibly also large) discrete domains. For example, in population genetics [71, 86] and computer science; where it is used for e.g. information retrieval [100, 76], speech and text and classification [55], text mining [24] and large language models [72]. This has sparked recent interest in studying the statistical decision theoretic properties of the multinomial model, see [15] for an overview. ", "page_idx": 1}, {"type": "text", "text": "Deriving minimax rates for goodness-of-fit testing of discrete distributions under bandwidth and differential privacy constraints is particularly challenging when each server holds multiple observations. To date, matching rates have been established only when each machine observes a single observation [9, 10, 5] (see also our discussion of related work below). The techniques used to derive lower-bounds in the aforementioned paper heavily rely on the fact that each server contains only one observation, see [9]. Moreover, whilst tight lower-bounds for the multiple observations case exist for the Gaussian model, the functional analytic techniques used to derive these results heavily rely on Gaussianity, see [85] and [22] for the respective bandwidth constraint and DP lower-bounds. Additionally, lower-bound techniques developed for estimation problems generally do not yield tight impossibility results for goodness-of-fti testing problems (see also the discussion in Section B of the appendix). ", "page_idx": 1}, {"type": "text", "text": "We derive matching upper- and lower-bounds for goodness-of-fit testing for discrete distributions under bandwidth and differential privacy constraints in scenarios where the \u221anumber of samples $n$ held by each of the servers is large in comparison to $d$ and $m;\\,m d\\log d/\\sqrt{n}\\,=\\,o(1)$ . This is achieved by leveraging the theory of statistical equivalence, as introduced by Le Cam (see e.g. [62, 81] for an introduction). Leveraging existing results concerning statistical equivalence of multinomial data with a multivariate Gaussian model proven in [30] allow us to show, roughly speaking, that the distributed goodness-of-fit testing problem for discrete distributions is statistically equivalent a distributed goodness-of-fit testing problem for the mean of a multivariate Gaussian model, and hence the minimax rate for the former problem is the same as the minimax rate for the latter problem, which was established in [85] and [22], for bandwidth and differential privacy constraints respectively. Furthermore, we exploit the bandwidth constraint distributed setting in which these two models have different minimax rates to show that, when $n$ is small compared to $d$ and $m$ , the multinomial model and multivariate Gaussian model are statistically non-equivalent. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. After a brief section on related work and notation, the article continues with a precise problem formulation in Section 2. Section 2.1 outlines the distributed framework, for both bandwidth and differential privacy constraints. In Section 2.2, we introduce the problem of distributed goodness-of-fit testing for discrete distributions under bandwidth and differential privacy constraints. In Section 3, the main results concerning the minimax rates are presented. Section 4 briefly outlines the main idea of the proof technique. Section 5 gives further insight into the comparison between discrete distributions and its comparable multivariate Gaussian model. The article ends with a brief discussion of the derived results. In the appendix, the tools of asymptotic equivalence within the distributed framework are presented and the technical proofs are provided. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Minimax goodness-of-fit testing knows a rich literature within the statistics and machine learning communities, see [46, 53, 64, 80, 52]. The $d$ -ary discrete distribution uniformity testing problem bares a close relationship with \u201cclassical\u201d nonparametric goodness-of-fit testing in the sense of [14, 79, 33, 96] and other nonparametric testing problems, see Section 1.4 in [54] and references therein. ", "page_idx": 2}, {"type": "text", "text": "For distributed goodness-of-fit testing specifically, much less is known. For multivariate Gaussian models under communication or differential privacy constraints, solutions have been established for the case where each server holds multiple observations. Communication constraints have been studied in [8, 84, 85] and differential privacy constraints in [22], with the authors deriving matching minimax upper- and lower-bounds for the goodness-of-fit testing for the mean of a multivariate Gaussian model. ", "page_idx": 2}, {"type": "text", "text": "For testing in discrete distributions, only the scenario where each server receives just one observation has been fully characterized in terms of the minimax rate in [9, 10, 5]. See also [28] for an overview. In these aforementioned works, the authors derive minimax rates goodness-of-fti testing for discrete distributions under bandwidth and differential privacy constraints. See [48, 78, 11, 2, 19] for investigations specifically under local DP (i.e. one observation per server with DP constraint). Nonparametric goodness-of-fit density testing for under local DP is considered in [36, 61], where in [61], the authors consider adaptation as well. For some investigations into the multiple observations per server case, see [34, 47]. ", "page_idx": 2}, {"type": "text", "text": "For estimation, the bandwidth constraint estimation discrete distributions in the large sample-perserver case has been studied by [3], who derive matching upper and lower-bounds up to logarithmic factors. However, their technique does not extend to the goodness-of-fit testing problem. ", "page_idx": 2}, {"type": "text", "text": "1.2 Notation and notions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Throughout this paper, we shall use the following notation. For two positive sequences $a_{k}$ and $b_{k}$ , we use $a_{k}\\lesssim b_{k}$ to mean that $a_{k}\\leq C b_{k}$ for some universal positive constant $C$ . We write $a_{k}\\asymp b_{k}$ if both $a_{k}\\lesssim b_{k}$ and $b_{k}\\lesssim a_{k}$ , and $a_{k}\\ll b_{k}$ if $a_{k}/b_{k}=o(1)$ . ", "page_idx": 2}, {"type": "text", "text": "We denote the maximum of $a$ and $b$ by $a\\vee b$ and the minimum by $a\\wedge b$ . For $k\\in\\mathbb{N}$ , $[k]$ represents the set $\\{1,\\ldots,k\\}$ . Universal constants $c$ and $C$ may vary between lines. The Euclidean norm of a vector $v\\in\\mathbb{R}^{d}$ is denoted by $\\|\\boldsymbol{v}\\|_{2}$ . For a matrix $M\\in\\mathbb{R}^{d\\times d}$ , $\\lVert M\\rVert$ represents the spectral norm, and ${\\mathrm{Tr}}(M)$ denotes its trace. $I_{d}$ is the $d\\times d$ identity matrix.   \nA non-negative sequence $M_{k}$ is said to be of poly-logarithmic order in non-negative sequences $a_{k},b_{k},c_{k}$ if there exists a constant $c>0$ such that $\\bar{M}_{k}\\ \\overset{\\bar{<}}{\\sim}(\\log(a_{k})\\log(b_{k})\\log(c_{k}))^{c}$ .   \nGiven measurable spaces $(\\mathcal{X},\\mathcal{X})$ and $(\\mathcal{Y},\\mathcal{Y})$ , a Markov kernel $K$ (between $(\\mathcal{X},\\mathcal{X})$ and target $(\\mathcal{Y},\\mathcal{Y}))$ is a map $K\\,\\equiv\\,K(\\cdot|\\cdot)\\,:\\,\\mathcal{Y}\\,\\times\\,\\mathcal{X}\\,\\rightarrow\\,[0,1]$ with the following two properties: The map $x\\mapsto K(A|x)$ is measurable for all $A\\in{\\mathcal{Y}}$ , and the map $A\\mapsto K(A|x)$ is a probability measure on $\\mathcal{Y}$ for every $x\\in\\mathscr{X}$ .   \nIf $S$ is a random variable on a probability space $(\\mathcal{X},\\mathcal{X},\\mathbb{P})$ , we let $\\mathbb{P}^{S}$ denote its push-forward ", "page_idx": 2}, {"type": "text", "text": "measure, i.e. the measure defined by $\\mathbb{P}^{S}(B):=\\mathbb{P}(S^{-1}(B))$ . We shall use $\\mathbb{E}$ and $\\mathbb{E}^{S}$ as the expectation operator corresponding to $\\mathbb{P}$ and $\\mathbb{P}^{S}$ . Random variables $X,Y,Z$ form a Markov chain $X\\rightarrow Y\\rightarrow Z$ whenever their joint distribution $\\mathbb{P}^{(X,Y,Z)}$ disintegrates as $d\\mathbb{P}^{(X,Y,Z)}=d\\mathbb{P}^{X}d\\mathbb{P}^{Y|X}d\\mathbb{P}^{Z|Y}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Problem formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin by formally introducing the general framework of distributed inference. ", "page_idx": 3}, {"type": "text", "text": "2.1 The distributed framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a measurable space $(\\mathcal{X},\\mathcal{X})$ with a statistical model $\\mathcal{P}=\\{P_{f}\\,:\\,f\\in\\mathcal{F}\\}$ defined on it. In the distributed framework, we consider $j=1,\\dots,m$ servers, each receiving data $X^{(j)}$ drawn from a given distribution $P_{f}\\in\\mathcal{P}$ . Each of the servers communicates a transcript $Y^{(j)}$ based on the data to a central server, which in turn computes its solution to the testing problem $T(Y)\\in\\{0,1\\}$ based on the aggregated transcripts $Y=(Y^{(1)},\\ldots,Y^{(m)})$ . We shall use the convention that $T(Y)=1$ means rejecting the null hypothesis. The transcript generating mechanisms are then given by Markov kernels $\\{K^{j}\\}_{j=1,\\dots,m}$ , with the Markov kernel (i.e. conditional distribution) of the transcript $Y^{(j)}$ given the data $X^{(j)}$ and the randomness $U$ shared by the servers denoted by $K^{j}(\\cdot|X^{(j)},U)$ . We formalize this in the following definition. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. $A$ distributed testing protocol for the model $\\mathcal{P}$ consists of a triplet $\\{T,\\{K^{j}\\}_{j=1}^{m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\},$ , where $\\left\\{K^{j}\\right\\}_{j=1}^{m}$ is a collection of Markov kernels $K^{j}:\\mathcal{Y}^{(j)}\\times\\mathcal{X}\\times\\mathcal{U}\\rightarrow$ $[0,1]$ defined on a measurable space $(\\boldsymbol{\\mathfrak{y}}^{(j)},\\boldsymbol{\\mathcal{Y}}^{(j)})$ , $T:\\otimes_{j=1}^{m}\\mathcal{V}^{(j)}\\to\\{0,1\\}$ is a measurable map and $(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})$ is probability space. ", "page_idx": 3}, {"type": "text", "text": "The probability space $(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})$ is used to (possibly) generate a source of randomness (independent of the data) that is shared by the servers. The distributed protocol is said to have no access to shared randomness or to be a local randomness protocol if $\\mathbb{P}^{U}$ is trivial1. In an abuse of notation, we shall often refer to the entire triplet $\\{T,\\{K^{j}\\}_{j=1,\\dots,m}^{\\,\\cdot},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ using just $T$ . ", "page_idx": 3}, {"type": "text", "text": "Given a distributed protocol and i.i.d. data from $P_{f}$ we shall use $\\mathbb{P}_{f}$ to denote the joint distribution of $Y\\,=\\,(Y^{(1)},\\ldots,Y^{(m)})$ , the data $X$ under $P_{f}^{m}$ and the shared randomness $U\\sim\\mathbb{P}^{U}$ . Writing $x=(x^{(1)},\\ldots,x^{(m)})\\in\\chi^{m}$ , let $x\\mapsto K(A|x,u)$ denote the Markov kernel $\\otimes_{j=1}^{m}K^{j}(\\cdot|x^{(j)},u)$ (i.e. the product measure). The independence structure of the data yields that $\\begin{array}{r}{P_{f}^{m}K=\\bigotimes_{j=1}^{m}P_{f}K^{j}}\\end{array}$ and the push-forward measure of $Y$ can be seen to disintegrate as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}_{f}^{Y}(A)=P_{f}^{m}\\mathbb{P}^{U}K(A)=\\mathbb{P}^{U}P_{f}^{m}K(A)=\\int\\int K(A|x,u)d P_{f}^{m}(x)d\\mathbb{P}^{U}(u),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the second equality follows from the independence of $U$ with the data drawn from $P_{f}$ . The above disintegration of the push-forward measure of $Y$ and the product structure of $K$ can be interpreted as $\\bar{(\\boldsymbol{X},Y,T(Y))}$ forming a Markov chain given $U$ , in the sense of the diagram ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c c c c c}{{X^{(1)}}}&{{\\longrightarrow}}&{{Y^{(1)}|U}}&{{\\sideset{}{'}{\\sum}}}&{{}}\\\\ {{\\vdots}}&{{\\longrightarrow}}&{{\\vdots}}&{{\\sideset{}{'}{\\sum}}}&{{T(Y).}}\\\\ {{X^{(m)}}}&{{\\longrightarrow}}&{{Y^{(m)}|U}}&{{\\sideset{}{'}{\\sum}}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The diagram indicates the flow of dependencies. The $m$ servers each obtain data $X^{(j)}$ from $P_{f}$ , and generate a transcript $Y^{(j)}$ based on the data and shared randomness $U$ . The central server then makes a decision $T(Y)$ based on the aggregated transcripts $Y$ . For a definition of Markov kernels and Markov chains, see Section 1.2. ", "page_idx": 3}, {"type": "text", "text": "Allowing transcript-generating mechanisms to access both shared and local randomness is important for our analysis, as shared randomness has been found to yield strictly better performance in distributed goodness-of-fti testing, see e.g. [9, 10, 5, 8, 84, 85, 22]. Shared randomness protocols can be seen as a subset of common interactive procedures, such as sequential and blackboard protocols (see e.g. [6]). The aforementioned paper shows that for discrete distribution goodness-of-fit testing in the single observation per server case, sequential and blackboard protocols offer no benefit over shared randomness protocols. Similarly, for mean shift problems in the multivariate Gaussian case, no advantage of sequential protocols over shared randomness protocols is known, except in the case of estimation with unknown variance [? ]. Since we study goodness-of-fit testing for discrete distributions in the large-number-of-observations case by comparing with a Gaussian model with known variance, we restrict the setting of the main article to local and shared randomness protocols only. Nevertheless, our theoretical framework is general enough to handle interactive protocols, which we discuss in Section A.2 of the appendix. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Next, we introduce the notion of a bandwidth constraint in the distributed setting. ", "page_idx": 4}, {"type": "text", "text": "Definition 2. A distributed protocol is said to satisfy a $b$ -bit bandwidth constraint $i f$ its kernels $\\{K^{j}\\}_{j=1,\\dots,m}$ are defined on measurable spaces $(\\boldsymbol{\\mathfrak{y}}^{(j)},\\boldsymbol{\\mathcal{Y}}^{(j)})$ satisfying $|\\mathcal{D}^{(j)}|\\ \\stackrel{.}{\\leq}\\ 2^{b}$ for $j=1,\\dots,\\stackrel{.}{m}$ . ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{T}_{\\mathrm{LR}}^{(b)}$ $\\mathcal{T}_{\\mathrm{SR}}^{(b)}$ to denote the classes of all local randomness and shared randomness distributed testing protocols with communication budget $b$ per machine, respectively. ", "page_idx": 4}, {"type": "text", "text": "Lastly, we introduce the notion of differential privacy in the distributed setting. We will be focusing on the notion of differential privacy as put forward by [43, 44]. Differential privacy provides a mathematical framework that guarantees preservation of privacy in a notion akin to cryptographical guarantees. Formally, a differential privacy constraint on a transcript in our setting is formulated as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 3. Let $\\epsilon\\;\\geq\\;0,\\delta\\;\\geq\\;0$ . The transcript $Y^{(j)}$ generated from $K^{j},u\\,\\in\\,\\mathcal{U}$ is said to be $(\\epsilon,\\delta)$ -differentially private $i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nK^{j}(A|x,u)\\leq e^{\\epsilon}K^{j}(A|x^{\\prime},u)+\\delta\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A distributed testing protocol $\\{T,\\{K^{j}\\}_{j=1}^{m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ , is said be $a$ distributed $(\\epsilon,\\delta)$ -differentially private testing protocol $i f\\{K^{j}\\}_{j=1,\\dots,m}$ satisfies (2) $\\mathbb{P}^{U}{-}a.s.$ . ", "page_idx": 4}, {"type": "text", "text": "Small values of $\\epsilon$ and $\\delta$ ensure that, even when the transcript $Y^{(j)}$ is publicly available, the sample $X^{(j)}$ underlying $Y^{(j)}$ is unidentifiable. We stress that this type of differential privacy guarantee concerns the local data $X^{(j)}$ in full, even $X^{(j)}$ consists of multiple observations. This is often referred local differential privacy, where the privacy guarantee regards each server as essentially pertaining data to \u201cone indiviual\u201d. For a thorough introduction on differential privacy guarantees, we refer the reader to [42]. We also note that the use of shared randomness does not affect the privacy guarantee provided by the protocol, as the guarantee holds even if the outcome of the shared randomness is known. ", "page_idx": 4}, {"type": "text", "text": "We use T LR and $\\mathcal{T}_{\\mathrm{SR}}^{(\\epsilon,\\delta)}$ to denote the classes of all local- and shared randomness $(\\epsilon,\\delta)$ -differentially private distributed testing protocols, respectively. We note that the machinery developed in Section A.2 allows consideration of both types of constraints simultaneously. In the main text of the article, we shall focus on the bandwidth constraint and differential privacy constraint separately as minimax rates for the joint constraints are not known for the Gaussian model we use for comparison to the multinomial model in the main article. ", "page_idx": 4}, {"type": "text", "text": "2.2 Distributed goodness-of-fit testing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start by giving a formal description of sampling from a discrete distribution in the distributed setting. Consider a set with cardinality $d$ ; for simplicity, we take $\\tilde{\\mathcal{X}}=\\{1,\\ldots,d\\}$ . Any probability distribution such a set can be characterized by an element of the $d-1$ -dimensional probability simplex $\\mathbb{S}^{d}$ , defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{q=(q_{1},\\ldots,q_{d})\\in[0,1]^{d}:\\sum_{i=1}^{d}q_{i}=1\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In our distributed framework, each server $j\\,=\\,1,\\ldots,m$ observes a data $\\tilde{X}^{(j)}$ taking values in $\\{1,\\ldots,d\\}^{n}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{X}^{(j)}=(\\tilde{X}_{1}^{(j)},\\ldots,\\tilde{X}_{n}^{(j)})\\sim Q\\equiv Q_{n,q},\\quad\\tilde{X}_{i}^{(j)}\\overset{i.i.d.}{\\sim}\\mathrm{Multinomial}(1,q)\\;\\;\\mathrm{for}\\;q\\in\\mathbb{S}^{d}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "That is, each server obtains $n$ i.i.d. draws from a multinomial distribution with parameter $q$ . ", "page_idx": 5}, {"type": "text", "text": "The statistical decision problem of interest shall be that of goodness-of-fit or uniformity testing, i.e. distinguishing the hypotheses ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{0}:q=q_{0}\\mathrm{~versus~}H_{1}:q\\in\\{q\\in\\mathcal{F}:\\|q-q_{0}\\|_{1}\\geq\\rho\\}=:H_{\\rho},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $q_{0}=(q_{01},\\dots,q_{0d})=(1/d,\\dots,1/d)\\in\\mathbb{S}^{d}$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\left\\{q\\in\\mathbb{S}^{d}\\,:\\,\\frac{\\operatorname*{max}_{i}q_{i}}{\\operatorname*{min}_{i}q_{i}}\\leq R\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some fixed constant $R\\,>\\,0$ . The statistical model under consideration shall be denoted by ${\\mathcal{Q}}=\\{Q_{q}^{n}:q\\in{\\mathcal{F}}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "We define the testing risk for a distributed testing protocol $T$ , for the hypotheses (4) (and statistical model $\\mathcal{Q}$ ) by sum of the type I and worst case type $\\mathrm{II}$ error over the alternative class; ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{Q}}(T,H_{\\rho}):=\\mathbb{Q}_{q_{0}}^{Y}T(Y)+\\operatorname*{sup}_{f\\in H_{\\rho}}\\mathbb{Q}_{f}^{Y}\\left(1-T(Y)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The minimax testing risk over a class of distributed protocols $\\mathcal{T}$ is then defined as $\\operatorname*{inf}_{T\\in\\mathcal{T}}\\mathcal{R}_{\\mathcal{Q}}(T,H_{\\rho})$ . ", "page_idx": 5}, {"type": "text", "text": "It is clear that, as $\\rho$ tends to 0, the minimax testing risk should increase. We are interested in finding the so called minimax separation rate, or detection boundary, which is a sequence $\\rho^{*}$ depending on the model characteristics $n,d,m$ and $\\mathcal{T}$ such that the minimax testing risk converges to 0 if $\\rho\\ll\\rho^{*}$ or 1 if $\\rho\\gg\\rho^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "The minimax separation rate captures how the testing problem becomes easier, or more difficult, for different model characteristics. The minimax rate for the hypothesis above case is $\\textstyle\\rho^{2}\\asymp{\\frac{\\sqrt{d}}{m n}}$ when $\\mathcal{T}$ consists of the class of all testing protocols, as was established in [70] and [94]. ", "page_idx": 5}, {"type": "text", "text": "When $\\mathcal{T}$ is taken to be one of the bandwidth or privacy constraint classes of tests, i.e. $\\mathcal{T}_{\\mathrm{LR}}^{(b)}$ and $\\mathcal{T}_{\\mathrm{SR}}^{(b)}\\ \\mathcal{T}_{\\mathrm{LR}}^{(\\epsilon,\\delta)}$ and $\\mathcal{T}_{\\mathrm{SR}}^{(\\epsilon,\\delta)}$ , it is sensible to expect $\\rho^{*}$ to depend on the bandwidth or differential privacy parameters, $b$ and $(\\epsilon,\\delta)$ , respectively. In the distributed discrete distribution setupj described above with $n=1$ , such minimax rates have been derived in [9, 10]. We discuss these results in the next section, co\u221antrasting them with the minimax separation rate derived in this paper for the case where $m d\\log d/\\sqrt{n}=o(1)$ . ", "page_idx": 5}, {"type": "text", "text": "3 Minimax rates in the large sample regime ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now turn to the main results of this paper, which concern the minimax rates for goodness-of-fit testing for discrete distributions under bandwidth and differential privacy constraints in the large sample regime. We shall show that the minimax rates for the distributed multinomial model under bandwidth and differential privacy constraints are the same as the minimax rates for a $d\\!.$ -dimensional distributed Gaussian model, as derived in [85] and [22], respectively. ", "page_idx": 5}, {"type": "text", "text": "The first theorem establishes the minimax rate for the distributed multinomial model under bandwidth constraints. A proof can be found in Section $\\mathrm{D}$ of the appendix. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Consider sequences $m\\equiv m_{\\nu},\\,b\\equiv b_{\\nu},\\,d\\equiv d_{\\nu}$ and $n\\equiv n_{\\nu}$ such that $m d\\rightarrow\\infty$ whilst ", "page_idx": 5}, {"type": "equation", "text": "$$\nm d\\log d/\\sqrt{n}\\stackrel{\\nu\\to\\infty}{\\rightarrow}0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Suppose that $\\rho\\equiv\\rho_{\\nu}$ is a nonnegative sequence satisfying ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho^{2}\\asymp\\left(\\frac{d}{\\sqrt{d\\wedge b}m n}\\right)\\wedge\\left(\\frac{\\sqrt{d}}{\\sqrt{m n}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{S_{R}}^{(b)}}\\mathcal{R}_{\\mathcal{Q}}(T,H_{M_{\\nu}\\rho})\\rightarrow\\left\\{0\\,\\,f o r\\,a n y\\,\\,M_{\\nu}\\rightarrow\\infty,\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When considering the class of only local randomness protocols (i.e. replacing $\\mathcal{T}_{S R}^{(b)}$ with $\\mathcal{T}_{L R}^{(b)}$ in the above display), the minimax separation rate is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho^{2}\\asymp\\left(\\frac{d^{3/2}}{(d\\wedge b)m n}\\right)\\Lambda\\left(\\frac{\\sqrt{d}}{\\sqrt{m n}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The theorem above shows that the minimax rate for the distributed multinomial model under bandwidth constraints is given by (6) in the case of access to shared randomness, and (7) in the case of no access to shared randomness. Both rates are the same as those established for a signal detection problem in a $d_{\\cdot}$ -dimensional distributed Gaussian model, as derived in [85], Theorems 3.1 and 3.2. In Section 4, we shall provide a proof of this result through the notion of statistical equivalence, where we explicitly use that the multinomial model is asymptotically similar to a specific Gaussian model and a corresponding signal detection problem. ", "page_idx": 6}, {"type": "text", "text": "The distributed $b$ -bit bandwidth constraint minimax rate for the hypotheses (4) in the multinomial model with $n=1$ is established in [9, 10]. Specifically, they find that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho^{2}\\asymp\\left\\{\\frac{d}{m\\sqrt{2^{b}\\wedge d}}\\begin{array}{l}{\\mathrm{in~case~of~access~to~shared~randomness,}}\\\\ {\\frac{d\\sqrt{d}}{m(2^{b}\\wedge d)}\\;\\;\\mathrm{without~access~to~shared~randomness.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Several aspects of this minimax rate are intriguing. First, unlike in the \u201clarge $n$ case\u201d for the same model and hypothesis ((6) and (7)), there is no elbow effect. Secondly, the benefit (i.e. \u201cefficiency gain\u201d) from an increase in bandwidth is exponential, whereas in the large sample scenario of Theorem 4 it is sub-linear. We shall comment on this \u201ccommunication super-efficiency\u201d phenomenon further below. ", "page_idx": 6}, {"type": "text", "text": "We now turn to the distributed multinomial model under differential privacy constraints. As in the case of the bandwidth constraint uniformity testing problem, we shall show that the minimax rate for the distributed multinomial model under differential privacy constraints is the same as the minimax rate for a $d$ -dimensional distributed Gaussian model, as derived in [22]. ", "page_idx": 6}, {"type": "text", "text": "The following theorem describes that the above rates are the minimax rates for uniformity testing in the distributed multinomial model under differential privacy constraints, for shared randomness and local randomness only protocols, respectively. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. For any sequences $m\\equiv m_{\\nu}$ , $d\\equiv d_{\\nu}$ and $n\\equiv n_{\\nu}$ such that $m d\\rightarrow\\infty$ , $\\frac{m d\\log d}{\\sqrt{n}}\\stackrel{\\iota\\rightarrow\\infty}{\\rightarrow}0,$ $n^{-1/4}\\ll\\epsilon\\equiv\\epsilon_{\\nu}\\,\\leq\\,1,$ , $\\delta\\,\\equiv\\,\\delta_{\\nu}\\,\\asymp\\,(m d)^{-p}$ for some $p>1$ . The minimax separation rate in the distributed multinomial model $\\mathcal{Q}$ for testing the hypotheses (4) using locally $(\\epsilon,\\delta)$ -differentially private protocols is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho^{2}\\asymp p o l y\\ \u2013l o g(d,m,n)\\left\\{\\frac{d}{m n\\epsilon^{2}}\\;\\;\\dot{i}f\\;\\;\\epsilon\\geq\\frac{\\sqrt{d}}{\\sqrt{m}},\\right.}\\\\ {\\left.\\frac{\\sqrt{d}}{\\sqrt{m}n\\epsilon}\\;\\;\\dot{i}f\\;\\;\\frac{1}{\\sqrt{m d}}\\leq\\epsilon<\\frac{\\sqrt{d}}{\\sqrt{m}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "in the case of having access to shared randomness. In the case of having only access to local randomness, it is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho^{2}\\asymp p o l y\\ \u2013l o g(d,m,n)\\left\\{\\frac{d\\sqrt{d}}{m n\\epsilon^{2}}\\;\\;i f\\;\\epsilon\\geq\\frac{d}{\\sqrt{m}},\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We provide a proof of the theorem in Section $\\mathrm{D}$ of the appendix. As with the bandwidth constraint case, the minimax separation rates for the distributed multinomial model under differential privacy constraints are derived by comparing the model and hypothesis test to a signal detection problem for the $d$ -dimensional distributed Gaussian model. The rates for the latter problem follow from the proofs of Theorems 4 and 5 in [22], who describe a more general setup which includes signal detection in the $d$ -dimensional distributed Gaussian model as a special case2. ", "page_idx": 6}, {"type": "text", "text": "Also in the case of privacy, there is a difference between the one observation per server case minimax rate $(n\\,=\\,1)$ ) and the multiple observations per server with local differential privacy case. The minimax rate in the multinomial model for $n=1$ is derived in [9, 5]; ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\rho^{2}\\asymp\\left\\{\\frac{d}{m\\epsilon^{2}}\\sp{2}\\right.\\mathrm{~in~case~of~access~to~shared~randomness,}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Comparing this rate to the rate obtained in Theorem 2, we observe phase transitions in the distributed testing problem for multinomial model under local differential privacy constraints which only occurs if the number of observations locally is large compared to the cardinality of the sample space. ", "page_idx": 7}, {"type": "text", "text": "4 Deriving the minimax rates through statistical equivalence ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The minimax rates for the distributed multinomial model under bandwidth and differential privacy constraints are derived through the notion of statistical equivalence (Le Cam theory), which is a powerful tool for establishing minimax rates in statistical decision theoretic problems. In this section, we shall provide a brief introduction to statistical equivalence, and show how it can be used to derive the minimax rates for the distributed multinomial model under bandwidth and differential privacy constraints. Further details on the statistical equivalence and a detailed proof are deferred Section A of the appendix. ", "page_idx": 7}, {"type": "text", "text": "Le Cam theory is a general framework for decision problems. At the core of this theory is the notion of a distance between statistical models, known as Le Cam\u2019s deficiency distance. The objective of this distance is to quantify the extent to which a complex statistical model can be approximated by a more simple one. If a model is close to another model in Le Cam\u2019s distance, then there is a mapping of solutions to decision theoretic problems from one model to the other. Whenever the risk of the decision problem is bounded, this means that similar performance can be achieved in the two models. Consequently, studying the complex model can be reduced to studying the corresponding simple model. For an extensive introduction to Le Cam theory, see e.g. [62, 81]. For a brief introduction; [63, 66]. ", "page_idx": 7}, {"type": "text", "text": "Consider a model $\\mathcal{P}=\\{P_{f}:f\\in\\mathcal{F}\\}$ (a collection of probability distributions) on a measurable space $(\\mathcal{X},\\mathcal{X})$ (the sample space). For this article, we consider only models with Polish sample spaces and corresponding Borel sigma-algebras and dominated models, meaning that there exists a sigma-finite measure $\\mu$ such that $P_{f}\\ll\\mu$ for all $f\\in\\mathcal F$ . This greatly simplifies the definition of deficiency, given next. ", "page_idx": 7}, {"type": "text", "text": "Given another model ${\\mathcal{Q}}=\\{Q_{f}:f\\in{\\mathcal{F}}\\}$ indexed by the same set $\\mathcal{F}$ and sample space $(\\Tilde{\\mathcal{X}},\\Tilde{\\mathcal{X}})$ , we define the deficiency of $\\mathcal{P}$ with respect to $\\mathcal{Q}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathfrak{d}(\\mathcal{P};\\mathcal{Q})=\\underset{C}{\\operatorname*{inf}}\\,\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,\\|P_{f}C-Q_{f}\\|_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the infimum is taken over all Markov kernels $C:\\tilde{\\mathcal{X}}\\times\\mathcal{X}\\rightarrow[0,1]$ and the probability measure $P_{f}C:\\tilde{\\mathcal{X}}\\to[0,1]$ is understood as $\\textstyle P_{f}C(A):=\\int_{x\\in\\mathcal{X}}C(A|x)d P_{f}(x)$ . This is equivalent to the more general notion of deficiency of [27] for dominated models on Polish spaces (see Proposition 9.2 in [68]). ", "page_idx": 7}, {"type": "text", "text": "Le Cam\u2019s deficiency distance between $\\mathcal{P}$ and $\\mathcal{Q}$ is then defined as $\\begin{array}{r l}{\\Delta(\\mathcal{P},\\mathcal{Q})}&{{}=}\\end{array}$ max $\\{\\mathfrak{d}(\\mathcal{P};\\mathcal{Q}),\\mathfrak{d}(\\mathcal{Q},\\dot{\\mathcal{P}})\\}$ . This semi-metric becomes a metric whenever $\\mathcal{P}$ and $\\mathcal{Q}$ are identified whenever $\\mathfrak{d}(\\mathscr{P};\\mathscr{Q})+\\mathfrak{d}(\\mathscr{Q},\\mathscr{P})=0$ . Two sequences of experiments $\\mathcal{P}_{\\nu}$ and $\\mathcal{Q}_{\\nu}$ are called asymptotically equivalent if their difference $\\Delta(\\mathcal{P}_{\\nu},\\mathcal{Q}_{\\nu})$ tends to zero as $\\nu$ approaches infinity. Conversely, such sequences shall be called asymptotically nonequivalent if $\\Delta(\\mathcal{P}_{\\nu},\\mathcal{Q}_{\\nu})>c$ as $\\nu\\to\\infty$ for a fixed constant $c>0$ . ", "page_idx": 7}, {"type": "text", "text": "In Section A.2, we prove that models that are close in the Le Cam metric (compared to $m$ ) have similar testing risks in the distributed setup. We leverage this result in combination with the fact that the distributed multinomial model is asymptotically equivalent to a $d$ -dimensional distributed Gaussian model, which we describe next. ", "page_idx": 7}, {"type": "text", "text": "Consider for $q\\in{\\mathcal{F}}$ and $i=1,\\ldots,d$ the random variables ", "page_idx": 7}, {"type": "equation", "text": "$$\nX_{i}^{(j)}=\\sqrt{q_{i}}+\\frac{1}{\\sqrt{2n}}Z_{i}^{(j)}\\quad\\mathrm{~with~}\\quad Z^{(j)}=(Z_{1}^{(j)},\\dots,Z_{d}^{(j)})\\sim N(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Let $P_{f}\\equiv P_{f}^{n}$ denote the distribution of $X^{(j)}=(X_{1}^{(j)},\\cdot\\cdot\\cdot,X_{d}^{(j)})$ . Let $\\mathcal{P}$ denote the corresponding experiment. It is shown in [30] that $\\mathcal{Q}$ is close to $\\mathcal{P}$ in the Le Cam metric when $d$ is relatively small compared to $n$ . More precisely, it follows from Theorem 1 and Section 7 in [30] that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Delta(\\mathcal{P},\\mathcal{Q})\\leq C_{R}\\frac{d\\log d}{\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $C_{R}\\,>\\,0$ is a constant depending only on $R$ . For the testing problem in Gaussian model, with hypotheses (4), the minimax rate can be derived using the results of [85] in case of bandwidth constraints and [22] in case of differential privacy constraints. The key tool from which the minimax rates can then be derived for the multinomial model is the following lemma, which allows comparison of the minimax testing risks for the multinomial and Gaussian models in regimes where the Le Cam distance is small. Its proof is given in Section A.2 of the appendix. ", "page_idx": 8}, {"type": "text", "text": "Lemma 1. Suppose $m\\Delta(\\mathcal{Q};\\mathcal{P})\\leq\\varrho$ for $\\varrho>0$ . Then, it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{inf}_{T\\in{\\mathcal{T}}({\\mathcal{P}})}\\mathcal{R}_{\\mathcal{P}}(T,H_{1})-\\operatorname*{inf}_{T\\in{\\mathcal{T}}({\\mathcal{Q}})}\\mathcal{R}_{\\mathcal{Q}}(T,H_{1})\\right|\\leq2\\varrho,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathcal{T}$ is either $\\mathcal{T}_{S R}^{b},\\mathcal{T}_{L R}^{b},\\mathcal{T}_{S R}^{(\\epsilon,\\delta)}\\;o r\\;\\mathcal{T}_{L R}^{(\\epsilon,\\delta)}$ ", "page_idx": 8}, {"type": "text", "text": "5 Statistical non-equivalence of discrete and multivariate Gaussian distributions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Theorem 1 describing uniformity testing in the large sample regime and the result derived for $n\\,=\\,1$ , as displayed in (8), shows a striking difference terms of the role of the communication budget. Specifically, in the $n=1$ regime, an exponential communication efficiency is observed, whereas in the large sample regime, the benefit is only linear. In this section, we shall provide some explanation for this phenomenon, and shall actually leverage this difference to show that the distributed multinomial model and the distributed Gaussian model are asymptotically non-equivalent: Two models are considered asymptotically nonequivalent if their Le Cam distance remains bounded away from zero, even as the amount of data increases in both models. ", "page_idx": 8}, {"type": "text", "text": "The multinomial model is equivalent to a model in which one observes $N^{(j)}=(N_{1}^{(j)},\\cdot\\cdot\\cdot,N_{d}^{(j)})$ taking values in $\\{1,\\ldots,n\\}^{d}$ , where $N_{k}^{(j)}\\equiv N_{k}^{(j)}\\left(\\tilde{X}^{(j)}\\right)=\\left|\\left\\{i:\\tilde{X}_{i}^{(j)}=k\\right\\}\\right|$ . Let $\\mathcal{Q}^{\\prime}$ denote the model generated by the observations $N^{(j)}$ . This model is equivalent to $\\mathcal{Q}$ , meaning $\\Delta(\\mathcal{Q},\\mathcal{Q}^{\\prime})=0$ . To see this, note that for $x=(x_{1},\\ldots,x_{n})\\in\\left\\{1,\\ldots,d\\right\\}^{n}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nQ\\left(\\tilde{X}^{(j)}=x\\right)=\\prod_{i=1}^{n}Q\\left(\\tilde{X}_{i}^{(j)}=x_{i}\\right)=\\prod_{k\\in\\{1,\\ldots,d\\}}q_{k}^{|\\{i:x_{i}=k\\}|}\\;\\mathrm{for}\\;\\mathrm{all}\\;Q\\in\\mathcal{Q},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "after which the aforementioned equivalence follows by the Neyman-Fisher factorization criterion, e.g. Lemma 2 in the appendix. When $n$ is large compared to $d$ , one could standardize the count statistics $N^{(j)}$ to obtain a statistic that tends towards a $d$ -dimensional Gaussian random vector. When $d$ and $m$ are not too large with respect to $n$ , one can obtain transcripts and corresponding test statistics from these approximately Gaussian vectors, that resemble those one would consider in the Gaussian model, and attain the corresponding minimax rates. ", "page_idx": 8}, {"type": "text", "text": "Since the observation $N^{(j)}$ takes values in $\\{1,\\ldots,n\\}^{d}$ , the full data can be transmitted whenever there are at least $d\\log_{2}n$ -bits are available per server. However, recalling that the observation $\\tilde{X}^{(j)}$ takes values in the space $\\{1,\\ldots,d\\}^{n}$ , which has cardinality bounded above by $2^{n\\log_{2}d}$ , we also obtain that the full data can be transmitted whenever $n\\log_{2}d$ -bits are available. Consequently, whenever ", "page_idx": 8}, {"type": "equation", "text": "$$\nb\\gtrsim d\\log_{2}(n+1)\\wedge n\\log_{2}d,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "the distributed problem has the same minimax separation rate for the hypothesis in (4) as the unconstrained problem with nm observations; $\\begin{array}{r}{\\rho_{Q}^{2}\\asymp\\frac{\\sqrt{d}}{m n}}\\end{array}$ . For the Gaussian problem, this is only the case whenever $b\\gtrsim d$ , as can be seen from Theorem 4. This indicates a kind of \u201ctipping point\u201d occuring whenever $n$ gets small compared to $d$ , where in a bandwidth constraint distributed setting, the testing problem in for the Gaussian model starts to exhibit very different behavior. ", "page_idx": 8}, {"type": "text", "text": "Interestingly, this does not imply that the multinomial model is \u201ceasier\u201d from a distributed testing under bandwidth constraints perspective, as there are sub-regimes in which the Gaussian model has a solution whereas the multinomial model does not and vice versa. It indicates that the \u201ccommunication complexity\u201d of the sample space matters in the respective decision problems. We can leverage this fact to obtain a lower-bound on the Le Cam distance between the multinomial model and the Gaussian model; which is the content of the next theorem. ", "page_idx": 9}, {"type": "text", "text": "Theorem 3. There exists constants $C>0$ and $c>0$ such that for any $n,d\\in\\mathbb{N}$ with ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\frac{d}{n\\log(d)}\\geq C\\quad a n d\\quad n\\geq\\sqrt{d}\\log(d),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "it holds that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathfrak{d}(\\mathcal{Q},\\mathcal{P})\\geq c,\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\mathcal{P}$ is the experiment generated by the observations in (13), $\\mathcal{Q}$ is generated according to (3), both indexed by $\\mathcal{F}$ as given in (5). ", "page_idx": 9}, {"type": "text", "text": "The proof of the theorem is given in Section D. It leverages that there exist distributed, $b$ -bit bandwidth constraint settings in which the (distributed) multinomial model allows for consistent goodness-of-fti testing, whereas the (distributed) Gaussian model does not. The result then readily follows from the distributed equivalence results derived in Section A.2. The fact that the separation in the respective (distributed) testing risks occurs for a constant number of servers, yields that the two models are asymptotically nonequivalent whenever $\\sqrt{d}/\\log^{2}(d)\\geq d/n\\gg1$ . This reasoning crucially exploits the differing minimax rates that occur under the bandwidth constraint, since without such a constraint, the same goodness-of-fti testing problem of (4) would have similar minimax performance for both of the models. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have derived minimax separation rates for uniformity testing in the distributed multinomial model und\u221aer bandwidth and differential privacy constraints, in the large sample regime where $m d\\log d/\\sqrt{n}=o(1)$ . When contrasted with existing results for large sample regimes, the minimax rates show that the large sample regime is subject to distinctly different phenomena. ", "page_idx": 9}, {"type": "text", "text": "The applicability of our results is somewhat constrained by the requirement that $m d\\log d/\\sqrt{n}=o(1)$ , which limits the range of model characteristics we can consider. Consequently, further work is needed to understand the behavior of the distributed multinomial model in other regimes. The non-equivalence result in Theorem 3 indicates that the distributed multinomial model and the distributed Gaussian model are fundamentally different regarding distributed statistical decision problems when the sample size is small. Therefore, direct analysis of the distributed multinomial model might be necessary, requiring new techniques to derive minimax rates. We note, however, that this pertains to the specific Gaussian model formulated in (13), and there might be a different Gaussian model that is equivalent to the distributed multinomial model even in the small $n$ regime. ", "page_idx": 9}, {"type": "text", "text": "The results in this paper are derived through the notion of statistical equivalence, which is a powerful tool for establishing minimax rates in statistical decision theoretic problems. The results and techniques can be applied more generally to other distributed inference problems, and proving more general results concerning statistical equivalence and distributed inference is an interesting avenue for future research. ", "page_idx": 9}, {"type": "text", "text": "A downside of leveraging statistical equivalence is that it generally does not provide a direct path to obtain methods that are minimax rate optimal. However, Theorem 1 and Section 7 in [30] provide a specific transformation that converts the local multinomial sample into a statistic approximately distributed as a Gaussian random vector. Such a transformation, combined with the rate optimal methods given in [85] and [22], provide guidance to construct methods that attain the minimax rates described in this article. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The author is grateful to the anonymous reviewers for their valuable feedback and suggestions and to Aad van der Vaart for his thorough reading of an earlier draft. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] ACHARYA, J., BONAWITZ, K., KAIROUZ, P., RAMAGE, D., AND SUN, Z. Context aware local differential privacy. In Proceedings of the 37th International Conference on Machine Learning (13\u201318 Jul 2020), H. D. III and A. Singh, Eds., vol. 119 of Proceedings of Machine Learning Research, PMLR, pp. 52\u201362. [2] ACHARYA, J., CANONNE, C., FREITAG, C., AND TYAGI, H. Test without trust: Optimal locally private distribution testing. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics (16\u201318 Apr 2019), K. Chaudhuri and M. Sugiyama, Eds., vol. 89 of Proceedings of Machine Learning Research, PMLR, pp. 2067\u20132076. [3] ACHARYA, J., CANONNE, C., LIU, Y., SUN, Z., AND TYAGI, H. Distributed estimation with multiple samples per user: Sharp rates and phase transition. In Advances in Neural Information Processing Systems (2021), M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., vol. 34, Curran Associates, Inc., pp. 18920\u201318931.   \n[4] ACHARYA, J., CANONNE, C., SINGH, A. V., AND TYAGI, H. Optimal rates for nonparametric density estimation under communication constraints. Advances in Neural Information Processing Systems 34 (2021), 26754\u201326766. [5] ACHARYA, J., CANONNE, C. L., FREITAG, C., SUN, Z., AND TYAGI, H. Inference under information constraints iii: Local privacy constraints. IEEE Journal on Selected Areas in Information Theory 2, 1 (2021), 253\u2013267.   \n[6] ACHARYA, J., CANONNE, C. L., LIU, Y., SUN, Z., AND TYAGI, H. Interactive inference under information constraints. IEEE Transactions on Information Theory 68, 1 (2022), 502\u2013 516. [7] ACHARYA, J., CANONNE, C. L., SUN, Z., AND TYAGI, H. Unified lower bounds for interactive high-dimensional estimation under information constraints. Advances in Neural Information Processing Systems 36 (2024). [8] ACHARYA, J., CANONNE, C. L., AND TYAGI, H. Distributed signal detection under communication constraints. In Conference on Learning Theory (2020), PMLR, pp. 41\u201363. [9] ACHARYA, J., CANONNE, C. L., AND TYAGI, H. Inference under information constraints i: Lower bounds from chi-square contraction. IEEE Transactions on Information Theory 66, 12 (2020), 7835\u20137855.   \n[10] ACHARYA, J., CANONNE, C. L., AND TYAGI, H. Inference under information constraints ii: Communication constraints and shared randomness. IEEE Transactions on Information Theory 66, 12 (2020), 7856\u20137877.   \n[11] ACHARYA, J., SUN, Z., AND ZHANG, H. Differentially private testing of identity and closeness of discrete distributions. Advances in Neural Information Processing Systems 31 (2018).   \n[12] AHLSWEDE, R., AND CSISZ\u00c1R, I. Hypothesis testing with communication constraints. IEEE transactions on information theory 32, 4 (1986), 533\u2013542.   \n[13] AHN, S., CHEN, W.-N., AND \u00d6ZG\u00dcR, A. Estimating sparse distributions under joint communication and privacy constraints. In 2022 IEEE International Symposium on Information Theory (ISIT) (2022), pp. 3144\u20133149.   \n[14] AN, K. Sulla determinazione empirica di una legge didistribuzione. Giorn Dell\u2019inst Ital Degli Att 4 (1933), 89\u201391.   \n[15] BALAKRISHNAN, S., AND WASSERMAN, L. Hypothesis testing for high-dimensional multinomials: A selective review. The Annals of Applied Statistics 12, 2 (2018), 727 \u2013 749.   \n[16] BARNES, L. P., HAN, Y., AND \u00d6ZG\u00dcR, A. Fisher information for distributed estimation under a blackboard communication protocol. In 2019 IEEE International Symposium on Information Theory (ISIT) (2019), IEEE, pp. 2704\u20132708.   \n[17] BARNES, L. P., HAN, Y., AND \u00d6ZG\u00dcR, A. Lower bounds for learning distributions under communication constraints via fisher information. The Journal of Machine Learning Research 21, 1 (2020), 9583\u20139612.   \n[18] BERGER, T., AND ZHANG, Z. On the ceo problem. In Proceedings of 1994 IEEE International Symposium on Information Theory (1994), pp. 201\u2013.   \n[19] BERRETT, T., AND BUTUCEA, C. Locally private non-asymptotic testing of discrete distributions is faster using interactive mechanisms. Advances in Neural Information Processing Systems 33 (2020), 3164\u20133173.   \n[20] BRAVERMAN, M., GARG, A., MA, T., NGUYEN, H. L., AND WOODRUFF, D. P. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing (2016), pp. 1011\u20131020.   \n[21] BUTUCEA, C., DUBOIS, A., KROLL, M., AND SAUMARD, A. Local differential privacy: Elbow effect in optimal density estimation and adaptation over Besov ellipsoids. Bernoulli 26, 3 (2020), 1727 \u2013 1764.   \n[22] CAI, T. T., CHAKRABORTY, A., AND VUURSTEEN, L. Federated nonparametric hypothesis testing with differential privacy constraints: Optimal rates and adaptive tests, 2024. arXiv:2406.06749 [math.ST].   \n[23] CAI, T. T., CHAKRABORTY, A., AND VUURSTEEN, L. Optimal federated learning for nonparametric regression with heterogeneous distributed differential privacy constraints, 2024. arXiv:2406.06755 [math.ST].   \n[24] CAI, T. T., KE, Z. T., AND TURNER, P. Testing high-dimensional multinomials with applications to text analysis. Journal of the Royal Statistical Society Series B: Statistical Methodology 86, 4 (02 2024), 922\u2013942.   \n[25] CAI, T. T., AND WEI, H. Distributed adaptive gaussian mean estimation with unknown variance: Interactive protocol helps adaptation. The Annals of Statistics 50, 4 (2022), 1992\u2013 2020.   \n[26] CAI, T. T., AND WEI, H. Distributed gaussian mean estimation under communication constraints: Optimal rates and communication-efficient algorithms. Journal of Machine Learning Research 25, 37 (2024), 1\u201363.   \n[27] CAM, L. L. Sufficiency and Approximate Sufficiency. The Annals of Mathematical Statistics 35, 4 (1964), 1419 \u2013 1455.   \n[28] CANONNE, C. L., ET AL. Topics and techniques in distribution testing: A biased but representative sample. Foundations and Trends\u00ae in Communications and Information Theory 19, 6 (2022), 1032\u20131198.   \n[29] CANONNE, C. L., KAMATH, G., MCMILLAN, A., SMITH, A., AND ULLMAN, J. The structure of optimal private tests for simple hypotheses. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (2019), pp. 310\u2013321.   \n[30] CARTER, A. V. Deficiency distance between multinomial and multivariate normal experiments. The Annals of Statistics 30, 3 (2002), 708 \u2013 730.   \n[31] COCHRAN, E. S., LAWRENCE, J. F., CHRISTENSEN, C., AND JAKKA, R. S. The quakecatcher network: Citizen science expanding seismic horizons. Seismological Research Letters 80, 1 (2009), 26\u201330.   \n[32] CRAIN, M. The limits of transparency: Data brokers and commodification. new media & society 20, 1 (2018), 88\u2013104.   \n[33] CRAM\u00c9R, H. On the composition of elementary errors: First paper: Mathematical deductions. Scandinavian Actuarial Journal 1928, 1 (1928), 13\u201374.   \n[34] DIAKONIKOLAS, I., GOULEAKIS, T., KANE, D. M., AND RAO, S. Communication and memory efficient testing of discrete distributions. In Proceedings of the Thirty-Second Conference on Learning Theory (Phoenix, USA, 25\u201328 Jun 2019), A. Beygelzimer and D. Hsu, Eds., vol. 99 of Proceedings of Machine Learning Research, PMLR, pp. 1070\u20131106.   \n[35] DING, B., KULKARNI, J., AND YEKHANIN, S. Collecting telemetry data privately. Advances in Neural Information Processing Systems 30 (2017).   \n[36] DUBOIS, A., BERRETT, T., AND BUTUCEA, C. Goodness-of-Fit Testing for H\u00f6lder Continuous Densities Under Local Differential Privacy. In Foundations of Modern Statistics, vol. PROMS-425 of Springer Proceedings in Mathematics & Statistics. Springer International Publishing, 2023, pp. 53\u2013119.   \n[37] DUCHI, J. C., JORDAN, M. I., AND WAINWRIGHT, M. J. Local privacy and statistical minimax rates. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science (2013), IEEE, pp. 429\u2013438.   \n[38] DUCHI, J. C., JORDAN, M. I., AND WAINWRIGHT, M. J. Minimax optimal procedures for locally private estimation. Journal of the American Statistical Association 113, 521 (2018), 182\u2013201.   \n[39] DUCHI, J. C., JORDAN, M. I., WAINWRIGHT, M. J., AND ZHANG, Y. Optimality guarantees for distributed statistical estimation. arXiv preprint arXiv:1405.0782 (2014).   \n[40] DUNCAN, G., AND LAMBERT, D. The risk of disclosure for microdata. Journal of Business & Economic Statistics 7, 2 (1989), 207\u2013217.   \n[41] DUNCAN, G. T., AND PEARSON, R. W. Enhancing access to microdata while protecting confidentiality: Prospects for the future. Statistical Science 6, 3 (1991), 219\u2013232.   \n[42] DWORK, C. Differential privacy. In International colloquium on automata, languages, and programming (2006), Springer, pp. 1\u201312.   \n[43] DWORK, C., MCSHERRY, F., NISSIM, K., AND SMITH, A. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3 (2006), Springer, pp. 265\u2013284.   \n[44] DWORK, C., AND SMITH, A. Differential privacy for statistics: What we know and what we want to learn. Journal of Privacy and Confidentiality 1, 2 (2010).   \n[45] ERLINGSSON, U., PIHUR, V., AND KOROLOVA, A. Rappor: Randomized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security (New York, NY, USA, 2014), CCS \u201914, Association for Computing Machinery, p. 1054\u20131067.   \n[46] ERMAKOV, M. Asymptotically minimax tests for nonparametric hypotheses concerning the distribution density. Journal of Soviet Mathematics 52 (1990), 2891\u20132898.   \n[47] FISCHER, O., MEIR, U., AND OSHMAN, R. Distributed uniformity testing. In Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing (New York, NY, USA, 2018), PODC \u201918, Association for Computing Machinery, p. 455\u2013464.   \n[48] GABOARDI, M., LIM, H., ROGERS, R., AND VADHAN, S. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. In Proceedings of The 33rd International Conference on Machine Learning (New York, New York, USA, 20\u201322 Jun 2016), M. F. Balcan and K. Q. Weinberger, Eds., vol. 48 of Proceedings of Machine Learning Research, PMLR, pp. 2111\u20132120.   \n[49] HAN, Y., MUKHERJEE, P., OZGUR, A., AND WEISSMAN, T. Distributed statistical estimation of high-dimensional and nonparametric distributions. In 2018 IEEE International Symposium on Information Theory (ISIT) (2018), IEEE, pp. 506\u2013510.   \n[50] HAN, Y., \u00d6ZG\u00dcR, A., AND WEISSMAN, T. Geometric lower bounds for distributed parameter estimation under communication constraints. In Conference On Learning Theory (2018), PMLR, pp. 3163\u20133188.   \n[51] HARD, A., RAO, K., MATHEWS, R., RAMASWAMY, S., BEAUFAYS, F., AUGENSTEIN, S., EICHNER, H., KIDDON, C., AND RAMAGE, D. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604 (2018).   \n[52] INGSTER, Y., AND SUSLINA, I. A. Nonparametric goodness-of-fit testing under Gaussian models, vol. 169. Springer Science & Business Media, 2003.   \n[53] INGSTER, Y. I. Asymptotically minimax hypothesis testing for nonparametric alternatives. i, ii, iii. Math. Methods Statist 2, 2 (1993), 85\u2013114.   \n[54] INGSTER, Y. I., AND SUSLINA, I. A. Nonparametric Goodness-of-Fit Testing Under Gaussian Models, vol. 169 of Lecture Notes in Statistics. Springer New York, New York, NY, 2003.   \n[55] JURAFSKY, D., AND MARTIN, J. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, vol. 2. 02 2008.   \n[56] KONE C\u02c7N Y\\`, J., MCMAHAN, H. B., RAMAGE, D., AND RICHT\u00c1RIK, P. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527 (2016).   \n[57] KONE C\u02c7N Y\\`, J., MCMAHAN, H. B., YU, F. X., RICHT\u00c1RIK, P., SURESH, A. T., AND BACON, D. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 (2016).   \n[58] KREIDL, O. P., TSITSIKLIS, J. N., AND ZOUMPOULIS, S. I. On Decentralized Detection With Partial Information Sharing Among Sensors. IEEE Transactions on Signal Processing 59, 4 (Apr. 2011), 1759\u20131765. Number: 4.   \n[59] KROLL, M. On density estimation at a fixed point under local differential privacy. Electronic Journal of Statistics 15, 1 (2021), 1783 \u2013 1813.   \n[60] KULYNYCH, J., AND GREELY, H. T. Clinical genomics, big data, and electronic medical records: reconciling patient rights with research when privacy and science collide. Journal of Law and the Biosciences 4, 1 (2017), 94\u2013132.   \n[61] LAM-WEIL, J., LAURENT, B., AND LOUBES, J.-M. Minimax optimal goodness-of-fti testing for densities and multinomials under a local differential privacy constraint. Bernoulli 28, 1 (2022), 579\u2013600.   \n[62] LE CAM, L. Asymptotic methods in statistical decision theory. Springer Science & Business Media, 2012.   \n[63] LE CAM, L., AND YANG, G. L. Asymptotics in statistics: some basic concepts. Springer Science & Business Media, 2000.   \n[64] LEPSKII, O. Asymptotically minimax adaptive estimation. i: Upper bounds. optimally adaptive estimates. Theory of Probability & Its Applications 36, 4 (1992), 682\u2013697.   \n[65] LI, T., SAHU, A. K., TALWALKAR, A., AND SMITH, V. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine 37, 3 (2020), 50\u201360.   \n[66] MARIUCCI, E. Le cam theory on the comparison of statistical models. arXiv preprint arXiv:1605.03301 (2016).   \n[67] NGUYEN, A., DO, T., TRAN, M., NGUYEN, B. X., DUONG, C., PHAN, T., TJIPUTRA, E., AND TRAN, Q. D. Deep federated learning for autonomous driving. In 2022 IEEE Intelligent Vehicles Symposium (IV) (2022), IEEE, pp. 1824\u20131830.   \n[68] NUSSBAUM, M. Asymptotic equivalence of density estimation and Gaussian white noise. The Annals of Statistics 24, 6 (1996), 2399 \u2013 2430.   \n[69] OBERSKI, D. L., AND KREUTER, F. Differential privacy and social science: An urgent puzzle. Harvard Data Science Review 2, 1 (2020), 1\u201321.   \n[70] PANINSKI, L. A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data. IEEE Transactions on Information Theory 54, 10 (Oct. 2008), 4750\u20134755.   \n[71] PRITCHARD, J. K., STEPHENS, M., AND DONNELLY, P. Inference of population structure using multilocus genotype data. Genetics 155, 2 (2000), 945\u2013959.   \n[72] RADFORD, A., WU, J., CHILD, R., LUAN, D., AMODEI, D., SUTSKEVER, I., ET AL. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.   \n[73] RAMASWAMY, S., MATHEWS, R., RAO, K., AND BEAUFAYS, F. Federated learning for emoji prediction in a mobile keyboard, 2019.   \n[74] RODRIGUEZ, I. M., SEXTON12, W. N., SINGER, P. E., AND VILHUBER, L. The modernization of statistical disclosure limitation at the us census bureau.   \n[75] SART, M. Density estimation under local differential privacy and Hellinger loss. Bernoulli 29, 3 (2023), 2318 \u2013 2341.   \n[76] SCH\u00dcTZE, H., MANNING, C. D., AND RAGHAVAN, P. Introduction to information retrieval, vol. 39. Cambridge University Press Cambridge, 2008.   \n[77] SHAMIR, O. Fundamental limits of online and distributed algorithms for statistical learning and estimation. Advances in Neural Information Processing Systems 27 (2014), 163\u2013171.   \n[78] SHEFFET, O. Locally private hypothesis testing. In International Conference on Machine Learning (2018), PMLR, pp. 4605\u20134614.   \n[79] SMIRNOV, N. Table for Estimating the Goodness of Fit of Empirical Distributions. The Annals of Mathematical Statistics 19, 2 (1948), 279 \u2013 281.   \n[80] SPOKOINY, V. G. Adaptive hypothesis testing using wavelets. The Annals of Statistics 24, 6 (Dec. 1996).   \n[81] STRASSER, H. Mathematical theory of statistics: statistical experiments and asymptotic decision theory. No. 7 in De Gruyter studies in mathematics. W. de Gruyter, Berlin ; New York, 1985.   \n[82] SZABO, B., AND VAN ZANTEN, H. Adaptive distributed methods under communication constraints. The Annals of Statistics 48, 4 (2020), 2347\u20132380.   \n[83] SZAB\u00d3, B., AND VAN ZANTEN, H. Distributed function estimation: Adaptation using minimal communication. Mathematical Statistics and Learning 5, 3 (2022), 159\u2013199.   \n[84] SZAB\u00d3, B., VUURSTEEN, L., AND VAN ZANTEN, H. Optimal distributed composite testing in high-dimensional gaussian models with 1-bit communication. IEEE Transactions on Information Theory 68, 6 (2022), 4070\u20134084.   \n[85] SZAB\u00d3, B., VUURSTEEN, L., AND VAN ZANTEN, H. Optimal high-dimensional and nonparametric distributed testing under communication constraints. The Annals of Statistics 51, 3 (2023), 909\u2013934.   \n[86] TAI, Y. C., AND SPEED, T. P. A multivariate empirical Bayes statistic for replicated microarray time course data. The Annals of Statistics 34, 5 (2006), 2387 \u2013 2412.   \n[87] TARIGHATI, A., GROSS, J., AND JALDEN, J. Decentralized Hypothesis Testing in Energy Harvesting Wireless Sensor Networks. IEEE Transactions on Signal Processing 65, 18 (Sept. 2017), 4862\u20134873.   \n[88] TE SUN HAN, AND AMARI, S. Statistical inference under multiterminal data compression. IEEE Transactions on Information Theory 44, 6 (Oct. 1998), 2300\u20132324. Number: 6.   \n[89] TEAM, A. D. P. Learning with privacy at scale.   \n[90] TENNEY, R. R., AND SANDELL, N. R. Detection with Distributed Sensors. IEEE Transactions on Aerospace and Electronic Systems AES-17, 4 (July 1981), 501\u2013510. Number: 4.   \n[91] THORISSON, H. Coupling, Stationarity, and Regeneration. Probability and Its Applications. Springer New York, 2000.   \n[92] TSITSIKLIS, J. N. Decentralized detection by a large number of sensors. Mathematics of Control, Signals, and Systems 1, 2 (June 1988), 167\u2013182. Number: 2.   \n[93] TSYBAKOV, A. B. Introduction to nonparametric estimation. Springer series in statistics. Springer, New York ; London, 2009. OCLC: ocn300399286.   \n[94] VALIANT, G., AND VALIANT, P. An automatic inequality prover and instance optimal identity testing. SIAM Journal on Computing 46, 1 (2017), 429\u2013455.   \n[95] VAN DER HOEVEN, D., HADIJI, H., AND VAN ERVEN, T. Distributed online learning for joint regret with communication constraints. In Proceedings of The 33rd International Conference on Algorithmic Learning Theory (29 Mar\u201301 Apr 2022), S. Dasgupta and N. Haghtalab, Eds., vol. 167 of Proceedings of Machine Learning Research, PMLR, pp. 1003\u20131042.   \n[96] VON MISES, R. Statistik und wahrheit. Julius Springer 20 (1928).   \n[97] XU, A., AND RAGINSKY, M. Information-Theoretic Lower Bounds on Bayes Risk in Decentralized Estimation. arXiv:1607.00550 [cs, math, stat] (July 2016). arXiv: 1607.00550.   \n[98] YE, M., AND BARG, A. Optimal schemes for discrete distribution estimation under locally differential privacy. IEEE Transactions on Information Theory 64, 8 (2018), 5662\u20135676.   \n[99] ZAMAN, A., AND SZAB\u00d3, B. Distributed nonparametric estimation under communication constraints. arXiv preprint arXiv:2204.10373 (2022).   \n[100] ZHAI, C., AND LAFFERTY, J. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS) 22, 2 (2004), 179\u2013214.   \n[101] ZHANG, Y., DUCHI, J., JORDAN, M. I., AND WAINWRIGHT, M. J. Information-theoretic lower bounds for distributed statistical estimation with communication constraints. Advances in Neural Information Processing Systems 26 (2013).   \n[102] ZHU, Y., AND LAFFERTY, J. Distributed nonparametric regression under communication constraints. In Proceedings of the 35th International Conference on Machine Learning (10\u201315 Jul 2018), J. Dy and A. Krause, Eds., vol. 80 of Proceedings of Machine Learning Research, PMLR, pp. 6009\u20136017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Le Cam theory in distributed setting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We introduce some formal notions of Le Cam theory first in Section A.1. Then, in Section A.2, we study the equivalence of models in the distributed setting. The theoretical developments presented in this section apply to general models denoted by $\\mathcal{P}$ and $\\mathcal{Q}$ ; although the main text specifically focuses on the Gaussian location model for $\\mathcal{P}$ and the multinomial model for $\\mathcal{Q}$ , the machinery developed here is applicable to general statistical models. ", "page_idx": 16}, {"type": "text", "text": "A.1 Preliminary notions of Le Cam theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A statistical experiment is a set of probability distributions $\\mathcal{P}\\,=\\,\\{P_{f}\\,:\\,f\\,\\in\\,\\mathcal{F}\\}$ (a model) on a measurable space $(\\mathcal{X},\\mathcal{X})$ (the sample space). For the purpose of simplification, we shall consider only statistical experiments with Polish sample spaces and corresponding Borel sigma-algebras. Furthermore, we shall only consider dominated models, meaning that there exists a sigma-finite measure $\\mu$ such that $P_{f}\\ll\\mu$ for all $f\\in\\mathcal F$ . In a slight abuse of terminology, we shall sometimes refer to $\\mathcal{P}$ as the experiment, suppressing the presence of the sample space and indexing set. ", "page_idx": 16}, {"type": "text", "text": "Given another statistical experiment with model ${\\mathcal{Q}}=\\{Q_{f}:f\\in{\\mathcal{F}}\\}$ indexed by the same set $\\mathcal{F}$ and sample space $(\\Tilde{\\mathcal{X}},\\Tilde{\\mathcal{X}})$ , we define the deficiency of $\\mathcal{P}$ with respect to $\\mathcal{Q}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{d}(\\mathcal{P};\\mathcal{Q})=\\underset{C}{\\operatorname*{inf}}\\,\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,\\|P_{f}C-Q_{f}\\|_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the infimum is taken over all Markov kernels $C:\\tilde{\\mathcal{X}}\\times\\mathcal{X}\\rightarrow[0,1]$ and the probability measure $P_{f}C:\\tilde{\\mathcal{X}}\\to[0,1]$ is understood as ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{f}C(A):=\\int_{x\\in\\chi}C(A|x)d P_{f}(x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is equivalent to the more general notion of deficiency of [27] for dominated models on Polish spaces (see Proposition 9.2 in [68]). ", "page_idx": 16}, {"type": "text", "text": "The deficiency $\\mathfrak{d}(\\mathcal{P};\\mathcal{Q})$ quantifies the degree to which $\\mathcal{Q}$ can be approximated by an experiment $\\mathcal{P}$ . If $\\mathfrak{d}(\\mathcal{P};\\mathcal{Q})\\leq\\varrho$ , it implies that for bounded loss functions, each decision procedure within $\\mathcal{Q}$ has an associated procedure in $\\mathcal{P}$ that achieves nearly the same risk, up to a multiple of $\\varrho$ . ", "page_idx": 16}, {"type": "text", "text": "To make this precise, let $\\mathcal{F}$ be a measurable space and consider a function $\\ell:\\mathcal{F}\\times\\mathcal{D}\\to[0,1]$ on a measurable space $(\\mathcal{D},\\mathcal{D})$ , such that $t\\mapsto\\ell(f,t)$ is measurable for all $f\\in\\mathcal F$ , which we shall refer to a loss functions. We shall consider a decision procedure for $(\\mathcal{Q},\\mathcal{D})$ to be a Markov kernel $D:\\mathcal{D}\\times\\tilde{\\mathcal{X}}\\rightarrow[0,1]$ . If $\\mathfrak{d}(\\mathcal{P};\\mathcal{Q})\\leq\\varrho$ , there exists $C:\\tilde{\\mathcal{X}}\\times\\mathcal{X}\\to[0,1]$ such that for all decision procedures $D$ for $(\\mathcal{Q},\\mathcal{D})$ we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int\\ell(f,\\varphi)d P_{f}C D(\\varphi)\\leq\\int\\ell(f,\\varphi)d Q_{f}D(\\varphi)+2\\varrho,\\quad\\mathrm{~for~all~}f\\in\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, the Markov kernel $Q_{f}D$ is to be understood in the sense of (18) and $C D:\\mathcal{D}\\times\\mathcal{X}\\to[0,1]$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\nC D(A|x)=\\int D(A|\\tilde{x})d C(\\tilde{x}|x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "There is also the following reverse implication; suppose that there exists a loss function $\\ell:\\mathcal{F}\\times\\mathcal{D}\\rightarrow$ $[0,1]$ on a measurable space $(\\mathcal{D},\\mathcal{D})$ , and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{C}\\operatorname*{inf}_{D}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left|\\int\\ell(f,\\varphi)d Q_{f}D(\\varphi)-\\,\\int\\ell(f,\\varphi)d P_{f}C D(\\varphi)\\right|>2\\varrho,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the two infimums are over all decision procedures $D$ and Markov kernels $C:\\tilde{\\mathcal{X}}\\times\\mathcal{X}\\rightarrow$ $[0,1]$ . Then, $\\mathfrak{d}(\\mathcal{Q},\\mathcal{P})>\\varrho$ . This follows immediately from e.g. Lemma 12 in the appendix, since $\\begin{array}{r}{\\dot{x}\\mapsto\\int\\ell(f,\\varphi)\\dot{d}D(\\varphi|x)}\\end{array}$ is measurable. In the more extensive framework considered in e.g. [27], such a reverse implication for risk functions fully characterizes the deficiency between two models, but this framework is not needed in what follows. ", "page_idx": 16}, {"type": "text", "text": "Le Cam\u2019s deficiency distance between $\\mathcal{P}$ and $\\mathcal{Q}$ is then defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta(\\mathcal{P},\\mathcal{Q})=\\operatorname*{max}\\left\\{\\mathfrak{d}(\\mathcal{P};\\mathcal{Q}),\\mathfrak{d}(\\mathcal{Q},\\mathcal{P})\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This semi-metric becomes a metric whenever $\\mathcal{P}$ and $\\mathcal{Q}$ are identified whenever $\\Im(\\mathcal{P};\\mathcal{Q})+\\mathfrak{d}(\\mathcal{Q},\\mathcal{P})=$ 0. Two sequences of experiments $\\mathcal{P}_{\\nu}$ and $\\mathcal{Q}_{\\nu}$ are called asymptotically equivalent if their difference $\\Delta(\\mathcal{P}_{\\nu},\\mathcal{Q}_{\\nu})$ tends to zero as $\\nu$ approaches infinity. Conversely, such sequences shall be called asymptotically nonequivalent if $\\Delta(\\mathcal{P}_{\\nu},\\mathcal{Q}_{\\nu})>c$ as $\\nu\\to\\infty$ for a fixed constant $c>0$ . ", "page_idx": 17}, {"type": "text", "text": "The final notion we shall recall is that of sufficiency. A statistic $S:\\mathcal{X}\\rightarrow\\tilde{\\mathcal{X}}$ is sufficient for the model $\\mathcal{P}$ if for any $A\\in{\\mathcal{X}}$ there exists a measurable map $\\psi_{A}:\\tilde{\\mathcal{X}}\\rightarrow\\mathbb{R}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{f}\\left(A\\cap S^{-1}(B)\\right)=\\int_{B}\\psi_{A}(\\tilde{x})d P_{f}^{S}(\\tilde{x})\\ \\mathrm{~for~all~}B\\in\\tilde{\\mathcal{X}}\\ \\mathrm{and}\\ f\\in\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, the measure $P_{f}^{S}$ is to be understood as the push-forward measure $P_{f}^{S}(B)=P_{f}(S^{-1}(B))$ . A sufficient statistic allows for transforming observations from one model to another, \u201csufficient\u201d model which is equivalent in the sense of Le Cam distance. That is, if $S$ is a sufficient statistic for $\\mathcal{P}$ , then the model ${\\dot{\\mathcal{P}}}^{\\prime}:=\\{{P}_{f}^{S}:f\\in{\\mathcal{F}}\\}$ satisfies $\\Delta(\\mathcal{P},\\mathcal{P}^{\\prime})=0$ . ", "page_idx": 17}, {"type": "text", "text": "The next lemma is the Neyman-Fisher factorization theorem gives a useful characterization of sufficiency of a statistic for models that admit densities with respect to the same dominating measure. Lemma 2. Suppose that $P_{f}\\ \\ll\\ \\mu$ for all $P_{f}\\ \\in\\ \\mathcal P$ with $\\mu$ a sigma-finite measure. A statistic $S:\\mathcal{X}\\rightarrow\\tilde{X}$ is sufficient for $\\mathcal{P}$ if and only if there exists measurable functions $g_{f}:\\mathbb{R}\\to\\mathbb{R}$ and $h:\\mathcal{X}\\to\\mathbb{R}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d P_{f}}{d\\mu}(x)=g_{f}(S(x))h(x)\\,\\,\\,f o r\\,a l m o s t\\,e\\nu e r y\\,\\,x\\in\\mathcal{X}\\,a n d\\,e\\nu e r y\\,\\,f\\in\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A proof for both the lemma and the last statement of the previous paragraph can be found in Chapter 5 of [62]. ", "page_idx": 17}, {"type": "text", "text": "A.2 Equivalence of distributed decision problems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now turn to the distributed setting considered in the paper, where $j=1,\\dots,m$ servers each receive data $X^{(j)}$ drawn from a distribution $P_{f}$ and sample space $(\\mathcal{X},\\mathcal{X})$ . Each of the servers communicates a transcript based on the data to a central server, which based on the aggregated transcripts computes its solution to the decision problem at hand. ", "page_idx": 17}, {"type": "text", "text": "The tools developed in this section apply to wider range of distributed architectures than the one considered in the main text of the paper, as introduced in Section 2. The framework introduced here accommodates various forms of interaction between servers, including sequential and blackboard protocols (see e.g. [6, 16]). In contrast, the main text focuses on servers that either do not communicate (local randomness protocols) or utilize a shared randomness source (a special case of sequential or blackboard communication). ", "page_idx": 17}, {"type": "text", "text": "A distributed protocol for the experiment $\\mathcal{P}$ with decision space $(\\mathcal{D},\\mathcal{D})$ consists of a triplet $\\{D,\\mathcal{K},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ , a Markov kernel $D:\\,\\mathcal{D}\\times\\otimes_{j=1}^{m}\\mathcal{V}^{(j)}\\ \\to\\ [0,1]$ and a probability space $(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})$ , and $\\kappa$ is a collection of Markov kernels. ", "page_idx": 17}, {"type": "text", "text": "To unpack all this notation: the Markov kernel $D$ takes the role of the decision procedure, where the decision is to made on the basis of the transcripts generated by the Markov kernels $\\kappa$ . The transcripts are in turn generated based on the data and a source of shared randomness independent of the data. The probability space $(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})$ plays the role of the source of randomness that is shared by the servers. The distributed protocol is said to have no access to shared randomness or to be a local randomness protocol if $\\mathcal{U}$ is the trivial sigma-algebra. ", "page_idx": 17}, {"type": "text", "text": "In this section, we shall consider three types of communication architectures: ", "page_idx": 17}, {"type": "text", "text": "\u2022 One shot protocols: $\\mathcal{K}=\\{K^{j}\\}_{j=1,\\dots,m}$ where $K^{j}:\\mathcal{Y}^{(j)}\\times(\\mathcal{X}\\times\\mathcal{U})\\,\\to\\,[0,1]$ . These protocols are what are considered in the main text of the paper.   \n\u2022 Sequential protocols: $\\mathcal{K}=\\{K^{j}\\}_{j=1,\\dots,m}$ where $K^{j}:\\mathcal{Y}^{(j)}\\times(\\mathcal{X}\\times\\mathcal{U}\\times\\mathcal{Y}^{(1)}\\times\\cdot\\cdot\\cdot\\times$ $\\boldsymbol{\\mathcal{V}}^{(j-1)})\\,\\to\\,[0,1]$ . That is, the transcript generated by server $j$ is based on the data, the shared randomness and the transcripts of the previous servers.   \n\u2022 Blackboard protocols: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Blackboard protocols: $\\mathcal{K}=\\{K_{t}^{j}\\}_{j=1,\\dots,m,t=1,\\dots,T}$ where $K_{1}^{j}:\\mathcal{Y}^{(j)}\\times(\\mathcal{X}\\times\\mathcal{U})\\rightarrow[0,1]$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\nK_{t}^{j}:\\mathcal{Y}^{(j)}\\times\\left(\\mathcal{X}\\times\\mathcal{U}\\times(\\mathcal{Y}^{(1)}\\times\\cdot\\cdot\\cdot\\times\\mathcal{Y}^{(m)})^{\\otimes(t-1)}\\right)\\to[0,1],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $t=2,\\ldots,T$ . That is, the transcript generated by server $j$ is based on the data, the shared randomness, and the transcripts of all the servers from the previous round. ", "page_idx": 18}, {"type": "text", "text": "For one shot protocols, we have in terms of random variables that $X^{(j)}~\\sim~P_{f}$ , $U\\;\\sim\\;\\mathbb{P}^{U}$ , $Y^{(j)}|(X^{(j)},U)\\sim K^{j}(\\cdot|X^{(j)},U)$ for $j=1,\\dots,m$ and $\\varphi\\sim D(\\cdot|Y)$ with $Y=(Y^{(1)},\\ldots,Y^{(m)})$ . This gives rise to a Markov chain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c c c c c}{{X^{(1)}}}&{{\\longrightarrow}}&{{Y^{(1)}|U}}&{{\\sideset{}{'}{\\sum}}}&{{}}\\\\ {{\\vdots}}&{{\\longrightarrow}}&{{\\vdots}}&{{\\sideset{}{'}{\\sum}}}&{{\\varphi.}}\\\\ {{X^{(m)}}}&{{\\longrightarrow}}&{{Y^{(m)}|U}}&{{\\sideset{}{'}{\\sum}}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $x=(x^{(1)},\\ldots,x^{(m)})\\in\\chi^{m}$ , $u\\in\\mathcal{U}$ and $\\{K^{j}\\}_{j=1,\\dots,m}$ , let $x\\mapsto K(A|x)$ be the Markov kernel product distribution $\\otimes_{j=1}^{m}K^{j}(\\cdot|x^{(j)},u)$ . Given a distributed protocol and i.i.d. data from $P_{f}$ we shall use $\\mathbb{P}_{f}$ to denote the joint distribution the data $X\\sim P_{f}^{m}$ , the shared randomness $U\\sim\\mathbb{P}^{U}$ and $Y=(Y^{(1)},\\ldots,Y^{(m)})$ with $Y|(X,U)\\sim K(Y|X,U)$ . We have that $\\begin{array}{r}{P_{f}^{m}K=\\bigotimes_{j=1}^{m}P_{f}K^{j}}\\end{array}$ and the push-forward measure of $Y$ then disintegrates as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{f}^{Y}(A)=P_{f}^{m}\\mathbb{P}^{U}K(A)=\\mathbb{P}^{U}P_{f}^{m}K(A)=\\int d\\!\\!\\bigotimes_{j=1}^{m}\\!\\!\\!P_{f}K^{j}(\\cdot|X^{(j)},u)(A)d\\mathbb{P}^{U}(u),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second equality follows from the independence of $U$ with the data $X:=(X^{(1)},\\ldots,X^{(m)})$ drawn from $P_{f}$ . ", "page_idx": 18}, {"type": "text", "text": "For sequential protocols, the push-forward measure of $Y$ instead disintegrates as ", "page_idx": 18}, {"type": "equation", "text": "$$\n^{\\mathrm{\\scriptsize~3}Y}_{f}(A)=\\int_{\\mathcal{U}}\\left[\\int\\cdots\\int\\mathbb{1}_{A}(y)d P_{f}K^{m}\\left(y^{m}\\mid X^{(m)},u,(y)_{j=1}^{m-1}\\right)\\cdot\\cdot\\cdot d P_{f}K^{1}(y^{1}\\mid X^{(1)},u)\\right]d\\mathbb{P}^{U}(u),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For blackboard protocols, a similar disintegration applies for each of the rounds. ", "page_idx": 18}, {"type": "text", "text": "A one shot or sequential distributed protocol is said to satisfy a $b$ -bit bandwidth constraint if its kernels {Kj}j=1,...,m are defined on spaces satisfying $|\\mathcal{V}^{(j)}|\\,\\le\\,2^{b}$ . For blackboard protocols, various bandwidth constraints can be imposed, such as a $b$ -bit bandwidth constraint for each round $t=1,\\dots,T$ . ", "page_idx": 18}, {"type": "text", "text": "Given Markov Kernels $C^{j}\\;:\\;\\mathcal{X}\\times\\tilde{\\mathcal{X}}\\,\\rightarrow\\,[0,1]$ , $j\\,=\\,1,\\ldots,m$ , a distributed one shot protocol $\\{D,\\{K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ for the model $\\mathcal{P}$ , yields a distributed protocol for the model $\\mathcal{Q}$ : $\\{D,\\{C K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ . If $\\{K^{j}\\}_{j=1,\\dots,m}$ is $b$ -bit bandwidth constraint, the collection of kernels $\\{C^{j}K^{j}\\}_{j=1,\\dots,m}$ do so too, as each $C^{j}K^{j}$ is defined on $\\mathcal{Y}^{(j)}\\times\\tilde{\\mathcal{X}}$ . ", "page_idx": 18}, {"type": "text", "text": "Similarly, for a sequential protocol, the Markov kernels $C^{j}$ and $K^{j}$ yield a distributed sequential protocol for the model $\\mathcal{Q}$ with kernels $\\{C^{j}K^{j}\\}_{j=1,\\dots,m}$ . If each $K^{j}$ is $b$ -bit bandwidth constraint, so is each $C^{j}K^{j}$ . For blackboard protocols, the same reasoning applies to each round $t=1,\\dots,T$ . That is, type of protocol defined by the kernels $\\kappa$ is \u201cclosed under composition\u201d with kernels $C^{j}$ between $\\tilde{\\chi}$ with target space $\\mathcal{X}$ , where bandwidth constraints are preserved. ", "page_idx": 18}, {"type": "text", "text": "We shall consider the notion of local $\\epsilon$ -differential privacy of Definition 3. A Markov kernel $K:\\mathscr{V}\\times\\mathcal{X}\\to[0,1]$ is called locally $(\\epsilon,\\delta)$ -differentially private if ", "page_idx": 18}, {"type": "equation", "text": "$$\nK(A|x)\\leq e^{\\epsilon}K(A|x^{\\prime})+\\delta\\;\\;\\mathrm{for}\\;\\mathrm{all}\\;A\\in{\\mathcal{Y}}\\;\\mathrm{and}\\;x,x^{\\prime}\\in{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the definition of differential privacy depends heavily on what one defines as the sample space, it is difficult to obtain a similar \u201ctransfer of distributed protocols\u201d that respects the $(\\epsilon,\\delta)$ -differential privacy constraint, hence the choice to consider local constraints only. ", "page_idx": 18}, {"type": "text", "text": "A one shot or sequential distributed protocol shall be called locally $\\epsilon_{}$ -differentially private if (23) holds for each $\\bar{K^{j}}$ ; $j=1,\\dots,m$ . For blackboard protocols, one can impose a $(\\epsilon,\\delta)$ -differential privacy constraint for each round $t=1,\\ldots,T$ , or for the entire output over $t=1,\\ldots,T$ rounds. The following lemma shows that local $\\epsilon$ -differential privacy, just like bandwidth constraints, carry over from one model to the other. ", "page_idx": 19}, {"type": "text", "text": "Lemma 3. Let $(\\mathcal{X},\\mathcal{X})$ and $(\\Tilde{\\mathcal{X}},\\Tilde{\\mathcal{X}})$ be measurable spaces and consider Markov kernels $C:$ $\\tilde{\\mathcal{X}}\\times\\mathcal{X}\\rightarrow[0,1]$ and $K:\\mathcal{Y}\\times\\mathcal{X}\\to[0,1]$ . If $K$ is $b$ -bit bandwidth constraint, so is the Markov kernel $C K:\\mathcal{Y}\\times\\tilde{\\mathcal{X}}\\rightarrow[0,1]$ . If $K$ is locally $\\epsilon$ -differentially private, so is $C K$ . Furthermore, for any collection of Markov kernels $\\kappa$ , the same reasoning applies to the collection $\\{C K\\}_{K\\in K}$ , preserving bandwidth constraints and local differential privacy, as well as the protocol\u2019s architecture. ", "page_idx": 19}, {"type": "text", "text": "Proof. The first statement has been remarked on earlier in the section. For the second statement, consider arbitrary $\\tilde{x},\\tilde{x}^{\\prime}\\in\\tilde{\\mathcal X}$ and $A\\in{\\mathcal{Y}}$ . Using that $C$ is a Markov kernel and applying (23) to $K$ yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{C K(A|\\tilde{x})=\\int K(A|x)d C(x|\\tilde{x})=\\int\\int K(A|x)d C(x|\\tilde{x})d C(x^{\\prime}|\\tilde{x}^{\\prime})}}\\\\ &{}&{\\leq e^{\\epsilon}\\int K(A|x^{\\prime})d C(x^{\\prime}|\\tilde{x}^{\\prime})+\\delta=e^{\\epsilon}C K(A|\\tilde{x}^{\\prime})+\\delta,~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which shows $C K$ is locally $(\\epsilon,\\delta)$ -differentially private. The above argument applies pointwise for all other conditional arguments in the Markov kernel, hence the same reasoning applies to shared randomness, sequential and blackboard protocols. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "In an abuse of notation, let $D$ denote the entire distributed protocol (triplet) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\{D,\\{K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for the experiment $\\mathcal{P}$ (indexed by $\\mathcal{F}$ ) with decision space $(\\mathcal{D},\\mathcal{D})$ . Given $D$ and a loss function $\\ell:\\mathcal{F}\\times\\mathcal{D}\\to[-1,1]$ , we define the distributed risk of $D$ in $\\mathcal{P}$ for $\\ell$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{P}}(D,\\ell):=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\int\\int\\ell(f,\\varphi)d D(\\varphi|y)\\,d\\!\\bigotimes_{j=1}^{m}\\!P_{f}K^{j}(\\cdot|X^{(j)},u)(y)d\\mathbb{P}^{U}(u).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We are now ready to formulate a straightforward consequence for the distributed risk, following from models being close in Le Cam distance. This finding, formulated in Lemma 4, shall serve as one of the main tools for deriving the main results. It states roughly that, whenever there is a $b$ -bit bandwidth constrained distributed protocol that achieves a certain risk is one model and there is small deficiency with the other model relative to the number of servers, there exists a $b$ -bit distributed protocol that achieves comparable risk for the other model. A similar statement holds under local differential privacy constraints. If there is a locally $(\\epsilon,\\delta)$ -differentially private distributed procedure in the one model and there is small deficiency with another model, it means that there is comparable risk for the privacy constraint distributed decision problem. ", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Let $m~\\in~\\mathbb{N}$ . Consider two experiments $\\mathcal{P}$ and $\\mathcal{Q}$ with indexing set $\\mathcal{F}$ , satisfying $m\\mathfrak{d}(\\mathcal{Q};\\mathcal{P})\\leq\\varrho$ for some $\\varrho>0$ . Let $\\mathcal{J}_{\\mathcal{P}}$ and $\\mathcal{J}_{\\mathcal{Q}}$ denote the class of $b$ -bit bandwidth constraint shared randomness protocols for the models $\\mathcal{P}$ and $\\mathcal{Q}$ respectively. ", "page_idx": 19}, {"type": "text", "text": "Then, for any loss function $\\ell:\\mathcal{F}\\times\\mathcal{D}\\to[0,1]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{D\\in\\mathcal{J}_{\\mathcal{Q}}}\\mathcal{R}_{\\mathcal{Q}}(D,\\ell)-\\operatorname*{inf}_{D\\in\\mathcal{J}_{\\mathcal{P}}}\\mathcal{R}_{\\mathcal{P}}(D,\\ell)\\leq\\varrho.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where in the infimum, in an abuse of notation, $D$ denotes the entire distributed protocol triplet $\\{D,\\{K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ . ", "page_idx": 19}, {"type": "text", "text": "The same statement is holds for $\\mathcal{J}_{\\mathcal{P}}$ and $\\mathcal{J}_{\\mathcal{Q}}$ denoting either classes of $b$ -bit bandwidth constraint local randomness, sequential protocols, or any of these distributed protocols satisfying local $(\\epsilon,\\delta)$ - differential privacy constraints, for the respective models $\\mathcal{P}$ and $\\mathcal{Q}.$ . If $T m\\pmb{0}(\\mathcal{Q};\\mathcal{P})\\leq\\varrho,$ , the same statement holds for blackboard protocols with $T$ rounds. ", "page_idx": 19}, {"type": "text", "text": "Remark 1. This exemplifies also that, even though models $\\mathcal{P}^{m}=\\{P_{f}^{m}:f\\in\\mathcal{F}\\}$ and $\\begin{array}{r}{\\mathcal{Q}^{m}=\\{Q_{f}^{m}:}\\end{array}$ $f\\in\\mathcal{F}\\}$ are close in Le Cam distance, distributed decision problems formulated in terms the models $\\mathcal{P}$ and $\\mathcal{Q}$ , can have greatly different performance in terms of associated risks. ", "page_idx": 20}, {"type": "text", "text": "Proof. By e.g. Theorem 2 in [62], $m\\mathfrak{d}(\\mathcal{Q};\\mathcal{P})\\leq\\varrho$ implies that there exists a kernel $C:\\mathcal{X}\\times\\tilde{\\mathcal{X}}\\rightarrow$ $[0,1]$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,\\Vert P_{f}-Q_{f}C\\Vert_{\\mathrm{TV}}\\leq\\varrho/m.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lemma 3, the kernels ${\\tilde{\\mathcal{K}}}=\\{C K:K\\in{\\mathcal{K}}\\}$ satisfy the a $b$ -bit bandwidth constraint or local $(\\epsilon,\\delta)$ - differential privacy constraint if the collection $\\{K^{j}\\}_{j=1,\\dots,m}$ does. To illustrate this further, consider a one shot distributed protocol for $\\mathcal{P}$ , $\\{D,\\{K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}\\in\\mathcal{J}_{\\mathcal{P}}$ , the distributed protocol $\\tilde{D}=\\{D,\\{C K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ is then an element of $\\mathcal{J}_{\\mathcal{Q}}$ . ", "page_idx": 20}, {"type": "text", "text": "Using the fact that $\\ell$ is bounded by one and Lemma 13 in the appendix, it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathcal{Q}}(\\tilde{D},\\ell)-\\mathcal{R}_{\\mathcal{P}}(D,\\ell)\\leq\\Vert\\mathbb{P}^{U}\\overset{m}{\\underset{j=1}{\\bigotimes}}P_{f}K^{j}-\\mathbb{P}^{U}\\overset{m}{\\underset{j=1}{\\bigotimes}}Q_{f}C K^{j}\\Vert_{\\mathrm{TV}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{j=1}^{m}\\Vert\\mathbb{P}^{U}P_{f}K^{j}-\\mathbb{P}^{U}Q_{f}C K^{j}\\Vert_{\\mathrm{TV}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lemma 14 in the appendix, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbb{P}^{U}P_{f}K^{j}-\\mathbb{P}^{U}Q_{f}C K^{j}\\|_{\\mathrm{TV}}\\leq\\|\\mathbb{P}^{U}P_{f}-\\mathbb{P}^{U}Q_{f}C\\|_{\\mathrm{TV}}=\\|P_{f}-Q_{f}C\\|_{\\mathrm{TV}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which combined with (24) finishes for one shot protocols. ", "page_idx": 20}, {"type": "text", "text": "Similar reasoning applies to sequential protocols. We start by noting that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|P_{f}K^{1}(\\cdot\\mid u)-Q_{f}C K^{1}(\\cdot\\mid u)\\|_{\\mathrm{TV}}\\leq\\|P_{f}-Q_{f}C\\|_{\\mathrm{TV}}\\leq\\varrho/m,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 11 implies that (for any $u$ ) there exists a coupling $\\mathbb{P}$ of $Y^{(1)}\\sim P_{f}K^{1}(\\cdot\\mid u)$ and $\\tilde{Y}^{(1)}\\sim$ $Q_{f}C K^{1}(\\cdot\\mid u)$ such that $2\\mathbb{P}(Y^{(1)}\\neq\\tilde{Y}^{(1)})\\leq\\varrho/m$ . Write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{K}^{m:2}(\\cdot\\mid x^{m},\\cdot\\;\\cdot\\;,x^{2},u,y^{1}):=\\int\\cdot\\cdot\\cdot\\int d K^{m}(y^{m}\\mid x^{m},u,y^{m-1},\\cdot\\;\\cdot\\;,y^{1})\\cdot\\cdot\\cdot d K^{2}(y^{2}\\mid x^{2},u,y^{1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the disintegration relationship of (22), we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{P}^{U}P_{f}^{m}K-\\mathbb{P}^{U}Q_{f}C K\\|_{\\mathrm{TV}}\\leq\\int\\|P_{f}^{m}K(\\cdot\\mid u)-Q_{f}C K(\\cdot\\mid u)\\|_{\\mathrm{TV}}d\\mathbb{P}^{U}(u)}\\\\ &{\\,\\leq\\varrho/m+\\displaystyle\\int\\mathbb{E}_{f}^{Y^{(1)}|U=u}\\|P_{f}^{m}K^{m:2}(\\cdot\\mid u,Y^{(1)})-Q_{f}C K^{m:2}(\\cdot\\mid u,Y^{(1)})\\|_{\\mathrm{TV}}d\\mathbb{P}^{U}(u).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Iterating the above argument $m-1$ times, we the statement for sequential protocols. The statement for blackboard protocols follows by combining the above arguments for each round $t=1,\\dots,T$ . ", "page_idx": 20}, {"type": "text", "text": "In the remainder of this text, we shall constrain ourselves to a particular bounded risk function and distributed decision problem; distributed hypothesis testing. The following corollary formalizes the statement at the start of the paragraph for the testing a simple null versus a compositive alternative hypothesis in the distributed setting. To that extent, consider a test of the hypotheses ", "page_idx": 20}, {"type": "equation", "text": "$$\nH_{0}:f=f_{0}{\\mathrm{~~versus~the~alternative~hypothesis~~}}f\\in H_{1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and an experiment $\\mathcal{P}$ with indexing set $\\mathcal{F}$ satisfying $\\{f_{0}\\}\\cup H_{1}\\subset\\mathcal{F}$ . Consider for $m\\in\\mathbb{N}$ a distributed testing protocol for the model $\\mathcal{P}$ to be a distributed protocol $T\\equiv\\{D,\\{K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ , where in a slight abuse of notation, we shall also use $T$ to denote the (possibly randomized) test $T|Y\\sim D(\\cdot|Y)$ . Recalling the notation $\\mathbb{P}_{f}^{Y}=P_{f}^{m}\\mathbb{P}^{U}K$ as given in (21), define the distributed testing risk for the hypotheses in (25) and the model $\\mathcal{P}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{P}}(T,H_{1}):=\\mathbb{P}_{f_{0}}^{Y}D(T|Y)+\\operatorname*{sup}_{f\\in H_{1}}\\mathbb{P}_{f}^{Y}\\left(1-D(T|Y)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, $D(T|Y):=D(\\{1\\}|Y)$ , but one can equivalently consider a deterministic measurable map $T:\\underset{j=1}{\\overset{m}{\\prod}}\\mathcal{V}^{(j)}\\rightarrow[0,1]$ without loss of generality. Let $\\mathcal{T}_{\\mathrm{SR}}^{b}(\\mathcal{P})$ (resp. $\\mathcal{T}_{\\mathrm{LR}}^{b}(\\mathcal{P}))$ denote the set of shared randomness (resp. local randomness) distributed testing protocols for $\\mathcal{P}$ satisfying a $b$ -bit bandwidth constraint. Similarly, let $\\mathcal{T}_{\\mathrm{SR}}^{(\\epsilon,\\delta)}(\\mathcal{P})$ (resp. $\\mathcal{T}_{\\mathrm{LR}}^{(\\epsilon,\\delta)}(\\mathcal{P}))$ denote the set of shared randomness (resp. local randomness) distributed testing protocols for $\\mathcal{P}$   \nconstraint. Define the same classes for the model $\\mathcal{Q}$ in the obvious way. Using Lemma 4, we obtain the following result, which is a more general version of Lemma 1. ", "page_idx": 21}, {"type": "text", "text": "Lemma 5. Consider experiments $\\mathcal{P}$ , $\\mathcal{Q}$ such that $m\\mathfrak{d}(\\mathcal{Q};\\mathcal{P})\\leq\\varrho$ for $\\varrho>0$ . It holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}(\\mathcal{P})}\\mathcal{R}_{\\mathcal{P}}(T,H_{1})\\leq\\operatorname*{inf}_{T\\in\\mathcal{T}(\\mathcal{Q})}\\mathcal{R}_{\\mathcal{Q}}(T,H_{1})+2\\varrho,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathcal{T}$ is either $\\mathcal{T}_{S R}^{b},\\mathcal{T}_{L R}^{b},\\mathcal{T}_{S R}^{(\\epsilon,\\delta)}\\;o r\\;\\mathcal{T}_{L R}^{(\\epsilon,\\delta)}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Given $\\{T,\\{K^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}\\in\\mathcal{T}(\\mathcal{P})$ , Lemma 4 applied to the loss function ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(f,t):=t\\mathbb{1}_{\\{f_{0}\\}}(f)+(1-t)\\mathbb{1}_{H_{1}}(f)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and using that $\\{f_{0}\\}\\cup H_{1}\\subset\\mathcal{F}$ gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}_{f_{0}}^{Y}D(T|Y)<\\mathbb{Q}_{f_{0}}^{Y}D(T|Y)+\\varrho\\;\\operatorname{and}\\;\\operatorname*{sup}_{f\\in H_{1}}\\mathbb{P}_{f}^{Y}\\left(1-D(T|Y)\\right)<\\operatorname*{sup}_{f\\in H_{1}}\\mathbb{Q}_{f}^{Y}\\left(1-D(T|Y)\\right)+\\varrho\\;\\operatorname{and}\\;\\operatorname*{sup}_{f\\in H_{1}}\\mathbb{P}_{f}^{Y}\\left(1-D(T|Y)\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some distributed testing protocol $\\{D,\\{\\tilde{K}^{j}\\}_{j=1,\\dots,m},(\\mathcal{U},\\mathcal{U},\\mathbb{P}^{U})\\}$ in $\\mathcal{T}(\\mathcal{Q})$ , which yields the first statement. The second statement follows by symmetry of the argument. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "The implications of Lemma 4 have implications beyond the testing framework. Whilst in distributed estimation settings, the loss function under consideration is typically not bounded, rates can still be derived in probability. That is, if the minimax rate for the distance $d$ on $\\mathcal{F}$ in the model $\\mathcal{P}_{\\nu}$ is $\\rho_{\\nu}$ , the bounded loss function ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell_{\\nu}(f,g)=\\mathbb{1}\\left\\{d(f,g)\\leq C\\rho_{\\nu}\\right\\}\\ \\mathrm{~for~}\\,C>0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "can be used describe minimax estimation rates (in probability) between models $\\mathcal{P}$ and $\\mathcal{Q}$ . Since the paper is about testing, we shall not pursue this direction any further beyond this remark. ", "page_idx": 21}, {"type": "text", "text": "In the next sections, we will explore the consequences of Lemma 5 for minimax distributed testing rates for both bandwidth- and privacy constraints. ", "page_idx": 21}, {"type": "text", "text": "B Difficulties in direct analysis of the multinomial model under information constraints ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lower-bounds for both estimation and testing problems are typically established by bounding divergence measures between probability distributions, such as the chi-square divergence, mutual information, or total variation; [3, 6, 7, 16? , 49, 9, 99, 13, 84, 85, 23, 22]. ", "page_idx": 21}, {"type": "text", "text": "The proof techniques used for discrete distribution estimation employed in [3, 6] and [7], tight lower-bounds can often be obtained by \u201ctensorizing\u201d the divergence\u2014breaking the problem into a sum of local divergences. The inferential cost incurred due to bandwidth or privacy constraints are then captured via data processing arguments. Similar tensorization techniques are employed in other estimation problems, see for example [16, 99]. ", "page_idx": 21}, {"type": "text", "text": "However, this tensorization approach does not yield tight bounds for testing problems. For example, [84] uses mutual information in a tensorization framework for testing but only recovers optimal rates when each server communicates a single bit. Similarly, [8] attempts an estimation-based approach for goodness-of-fit testing but obtains tight lower-bounds only under 1-bit constraints, as detailed in Section 4 of their paper. ", "page_idx": 21}, {"type": "text", "text": "To achieve tight lower-bounds in testing problems, especially under communication and privacy constraints, different techniques are required. Papers [9, 85, 22] employ methods that significantly diverge from those used in estimation. In [9], a combinatorial expansion of the likelihood is used, effective for small sample sizes in the multinomial model but not generalizable to large numbers of observations. [85] and [22] address this limitation in the Gaussian setting by utilizing the BrascampLieb inequality from functional analysis, which explicitly leverages the Gaussian properties of the log-likelihood. This approach is not directly applicable to discrete models, due to the lack of quadratic structure in the log-likelihood of the multinomial model. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "C Separation rates for the Gaussian model ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For completeness, we provide the relevant results for the Gaussian model studied in [85] and [22]. ", "page_idx": 22}, {"type": "text", "text": "The first two results come in the form of lower-bounds for the minimax detection thresholds under bandwidth- and differential privacy constraints for the distributed signal detection problem presented in the introduction. We recall that in this problem, each local machine $j\\in\\{1,\\dotsc,m\\}$ observes ", "page_idx": 22}, {"type": "equation", "text": "$$\nX_{i}^{(j)}=f+Z_{i}^{(j)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with $f\\in\\mathbb{R}^{d}$ and $Z_{i}^{(j)}\\sim N(0,I_{d})$ , i.i.d. for $i=1,\\hdots,n$ . The null hypothesis constitutes that $f=0$ versus the alternative hypothesis that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf\\in H_{\\rho}:=\\left\\{f\\in\\mathbb{R}^{d}:\\,\\|f\\|_{2}\\geq\\rho\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The following is Theorem 3.1 from [85]. ", "page_idx": 22}, {"type": "text", "text": "Theorem 4. For each $\\alpha\\in(0,1)$ there exists a constant $c_{\\alpha}>0$ (depending only on $\\alpha$ ) such that if ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho^{2}<c_{\\alpha}\\frac{\\sqrt{d}}{n}\\left(\\sqrt{\\frac{d}{b\\wedge d}}\\bigwedge\\sqrt{m}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then in the shared randomness protocol case ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{S_{R}}^{(b)}}~\\mathcal{R}(H_{\\rho},T)>\\alpha~f o r\\,a l l~\\,n,m,d,b\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, for ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho^{2}<c_{\\alpha}{\\frac{\\sqrt{d}}{n}}\\left({\\frac{d}{b\\wedge d}}\\bigwedge\\sqrt{m}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we have under the local randomness protocol that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{L R}^{(b)}}~\\mathcal{R}(H_{\\rho},T)>\\alpha~f o r\\,a l l~\\,n,m,d,b\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Following the proof Theorem 3.1 from [85], we obtain the following lemma. ", "page_idx": 22}, {"type": "text", "text": "Lemma 6. Let $\\mathcal{T}^{b}$ denote the class of $b$ -bit bandwidth constrained shared- or local randomness distributed testing protocols and let $\\rho$ satisfy either (28) or (29), respectively. For any $\\alpha\\in(0,1)$ , there exists $c_{\\alpha}>0$ such that for all $T\\in\\mathcal{T}^{\\left(b\\right)}$ it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}}\\mathcal{R}(H_{\\rho},T)>\\alpha-\\pi(H_{\\rho}^{c}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pi\\ =\\ N(0,c_{\\alpha}^{-1/2}d^{-1}\\rho^{2}\\bar{\\Gamma})$ for a symmetric, idempotent matrix $\\bar{\\Gamma}\\ \\in\\ \\mathbb{R}^{d\\times d}$ with $d/2\\ \\leq$ $r a n k(\\bar{\\Gamma})\\leq d$ . ", "page_idx": 22}, {"type": "text", "text": "Similarly, the following result can be derived from the proof of Theorem 5 in [22], by taking $s>0$ in the theorem such that $d_{L_{s}}=d$ . ", "page_idx": 22}, {"type": "text", "text": "Theorem 5. For each $\\alpha\\in(0,1)$ there exists a constant $c_{\\alpha}>0$ (depending only on $\\alpha$ ), such that for any $n,m,d\\in\\mathbb{N}$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n0<\\epsilon\\leq1\\;a n d\\,0\\leq\\delta\\leq\\left(c_{\\alpha}m^{-3/2}\\wedge n d^{-1}\\epsilon^{2}\\wedge n^{1/2}d^{-1/2}\\epsilon^{2}\\right)^{1+p}\\,f o r\\,s o m e\\,p>0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "the condition ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho^{2}<c_{\\alpha}\\left(\\frac{d}{m n\\sqrt{n\\epsilon^{2}\\wedge1}\\sqrt{n\\epsilon^{2}\\wedge d}}\\bigwedge\\left(\\frac{\\sqrt{d}}{\\sqrt{m}n\\sqrt{n\\epsilon^{2}\\wedge1}}\\bigvee\\frac{1}{m n^{2}\\epsilon^{2}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in{\\mathcal{T}}_{S R}^{(\\epsilon,\\delta)}}~{\\mathcal{R}}(H_{\\rho},T)>\\alpha.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, for any $n,m,d\\in\\mathbb{N}$ and $\\epsilon,\\delta$ satisfying (30), the condition ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho^{2}<c_{\\alpha}\\left(\\frac{d\\sqrt{d}}{m n(n\\epsilon^{2}\\wedge d)}\\bigwedge\\left(\\frac{\\sqrt{d}}{\\sqrt{m}n\\sqrt{n\\epsilon^{2}\\wedge1}}\\bigvee\\frac{1}{m n^{2}\\epsilon^{2}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in{\\mathcal{T}}_{L R}^{(\\epsilon,\\delta)}}~{\\mathcal{R}}(H_{\\rho},T)>\\alpha.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Following its proof, we obtain the following the following sub-result. ", "page_idx": 23}, {"type": "text", "text": "Lemma 7. Let $\\mathcal{T}$ denote the class of shared- or local randomness distributed testing protocols satisfying an $(\\epsilon,\\delta)$ -differential privacy constraint for $\\mathrm{~\\textit~{~\\textcent~}~}<\\mathrm{~\\textit~{~\\textcent~}~}\\leq\\mathrm{~\\textit~{~1~}~}$ , $0\\;\\;\\leq\\;\\;\\delta\\;\\;\\leq$ $(c_{\\alpha}m^{-1}\\wedge c_{\\alpha}\\epsilon m^{-1/2}\\wedge n\\epsilon^{2}\\wedge n^{2}d^{-1}\\epsilon^{2}\\wedge n^{3/2}d^{-1/2}\\epsilon^{2})$ and let $\\rho$ satisfy either (28) or (29), respectively. For any $\\alpha\\in(0,1)$ , there exists $c_{\\alpha}>0$ such that for all $T\\in\\mathcal{T}^{(\\epsilon,\\delta)}$ it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{R}(H_{\\rho},T)>\\alpha-\\pi(H_{\\rho}^{c}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\pi=N(0,c_{\\alpha}^{-1/2}d^{-1}\\rho^{2}\\bar{\\Gamma})$ for a symmetric, idempotent matrix $\\bar{\\Gamma}\\in\\mathbb{R}^{d\\times d}\\,w i t h\\;r a n k(\\bar{\\Gamma})\\asymp d.$ ", "page_idx": 23}, {"type": "text", "text": "D Proofs of Theorems 1, 2 and 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Theorems $^{\\,I}$ and 2. In what follows, let $\\mathcal{T}$ denote a class of distributed protocols satisfying either a $b\\equiv b_{\\nu}$ -bit bandwidth constraint or a local $(\\epsilon,\\delta)$ -differential privacy constraint for $\\epsilon\\equiv\\epsilon_{\\nu}$ , $\\delta\\equiv\\delta_{\\nu}$ , allowing either for shared randomness or only local randomness. ", "page_idx": 23}, {"type": "text", "text": "For any sequences $m\\equiv m_{\\nu}$ , $d\\equiv d_{\\nu}$ and $n\\equiv n_{\\nu}$ with $C_{R}\\,m d\\log d/\\sqrt{n}=o(1)$ , it follows from Lemma 5 and the bound (14) that the testing risks satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{Q}}\\mathcal{R}_{\\mathcal{Q}_{\\nu}}(H_{\\rho_{\\nu}},T)=\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{P}}}\\mathcal{R}_{\\mathcal{P}_{\\nu}}(H_{\\rho_{\\nu}},T)+o(1).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\rho^{*}\\equiv\\rho_{\\nu}^{*}$ be the minimax rate of the $\\mathcal{P}$ -distributed problem, over the class $\\mathcal{T}_{\\mathcal{P}}$ , in the sense that $\\rho^{*}$ equals (up to constants) the right-hand side of (6), (7), (9) or (10). We split the proof into showing that $\\rho^{*}$ is an upper and lower-bound for the $\\mathcal{Q}$ -distributed problem over the class $\\mathcal{T}_{\\mathcal{P}}$ . ", "page_idx": 23}, {"type": "text", "text": "The rate $\\rho^{*}$ is an upper-bound (up to a poly-logarithmic factor) for the minimax rate in $\\mathcal{Q}$ : Write, for $q\\in\\mathcal{F},\\sqrt{q}=(\\sqrt{q_{i}})_{i\\in[d]}$ . Since $X^{(\\hat{j})}\\,\\dot{-}\\,\\sqrt{q_{0}}$ is a sufficient statistic for $X^{(j)}$ , the model (13) is equivalent in the Le Cam sense the one generated by ", "page_idx": 23}, {"type": "equation", "text": "$$\nX^{(j)}=\\sqrt{q}-\\sqrt{q_{0}}+\\frac{1}{\\sqrt{2n}}Z^{(j)}\\;\\;\\mathrm{with}\\;Z^{(j)}\\sim N(0,I_{d}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $q\\in{\\mathcal{F}}$ , which we shall denote by $\\tilde{\\mathcal P}$ . Consequently, by another application of Lemma 5, it suffices to show ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\tilde{\\mathcal{P}}}}\\mathcal{R}_{\\tilde{\\mathcal{P}}}(H_{\\rho_{\\nu}},T)\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\|q-q_{0}\\|_{1}\\geq\\rho$ , Lemma 15 implies that $\\|\\sqrt{q}-\\sqrt{q_{0}}\\|_{2}\\ge\\rho/2$ . Consequently, if $\\rho\\equiv\\rho_{\\nu}\\gg M_{\\nu}\\rho^{*}$ where $\\rho^{*}$ is of equal order of the minimax rate for the respective class of distributed protocols $\\mathcal{T}_{\\mathcal{P}}$ and $M_{\\nu}$ is an appropriately large factor (of poly-logarithmic order in case of differential privacy constraints), a distributed p\u221arotoco\u221al $T\\in{\\mathcal{D}}_{\\mathcal{P}}$ exists for the G\u221aaussian\u221a model that achieves the separation rate for whenever $H_{0}:{\\sqrt{q}}-{\\sqrt{q_{0}}}\\,=\\,0$ versus $H_{\\rho}:\\|\\sqrt{q}-\\sqrt{q_{0}}\\|_{2}\\,\\geq\\,\\rho/2$ . By the established equivalence of the minimax risks (33), this implies that a protocol $T\\in\\mathcal{T}_{\\mathcal{Q}}$ exists for the multinomial model as well. Thus, $\\rho_{\\nu}$ is an upper-bound for the minimax separation rate for the class of distributed protocols $\\mathcal{T}_{\\mathcal{Q}}$ of the multinomial model. ", "page_idx": 23}, {"type": "text", "text": "The rate $\\rho^{*}$ is a lower-bound for the minimax rate in $\\mathcal{Q}$ : Suppose that $\\rho\\equiv\\rho_{\\nu}$ is of smaller order than the minimax rate $\\rho^{*}$ of the class $\\mathcal{T}_{\\mathcal{P}}$ , in the sense that $\\rho^{*}/\\rho\\rightarrow\\infty$ as $\\nu\\to\\infty$ . We aim to use the Bayes risk lower-bound of Lemmas 6 and 7, which apply to a Gaussian prior. To accommodate a Gaussian prior with sufficient mass on the alternative hypothesis, we first need to address the \u201cconstraint on the signal\u201d imposed by $\\textstyle\\sum_{i=1}^{d}q_{i}=1$ for $q\\in{\\mathcal{F}}$ . ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "To that extent, consider without loss of generality $d$ to be divisible by two. Let $I_{R}:=[-(R-1)/(R+$ $1),(R-1)/(R+1)]$ . For all $(f_{i})_{i\\in[d/2]}\\in I_{R}^{d/2}/\\sqrt{d},$ , there \u221aexists a $q^{f}:=(q_{i}^{f})_{i\\in[d]}\\in\\mathcal{F}$ such that $q_{i}^{f}=1/d+{f_{i}}/{\\sqrt{d}}$ for $i=1,\\ldots,d/2$ and $q_{i}^{f}=1/d-{f_{i}}/{\\sqrt{d}}$ for $i=d/2+1,\\ldots,d$ . To see that $q^{f}\\in\\mathcal{F}$ , note that $\\textstyle\\sum_{i=1}^{d}q_{i}^{f}=1$ , $q^{f}\\geq0$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq i,k\\leq d}\\!\\frac{q_{i}^{f}}{q_{k}^{f}}\\leq\\operatorname*{max}_{c\\in I_{R}}\\!\\frac{1+c}{1-c}=R.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Define ${\\mathcal{F}}^{\\prime}$ as the set ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{(q_{i})_{i\\in[d]}\\in\\mathcal{F}:(f_{i})_{i\\in[d/2]}\\in\\frac{I_{R}^{d/2}}{\\sqrt{d}}\\mathrm{~s.t.~}q_{i}^{f}=1/d+(1-2\\mathbb{1}_{i>d/2})\\frac{f_{i}}{\\sqrt{d}}\\mathrm{~for~}i=1,\\ldots,d\\right\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\nH_{\\rho}^{\\prime}:=\\left\\{q\\,:\\,q\\in{\\mathcal{F}}^{\\prime},\\|q-q_{0}\\|_{1}\\geq\\rho\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have ${\\mathcal{F}}^{\\prime}\\subset{\\mathcal{F}}$ , which in turn implies that $H_{\\rho}^{\\prime}\\subset H_{\\rho}$ . Combined with the fact that the testing risk decreases by considering smaller alternative hypotheses, this results in ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{P}}}\\mathcal{R}_{\\mathcal{P}}(T,H_{\\rho})\\geq\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{P}}}\\mathcal{R}_{\\mathcal{P}}(T,H_{\\rho}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Define $g_{f}=(1/2)(f,-f)\\in\\mathbb{R}^{d}$ . By Pinsker\u2019s inequality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|P_{\\sqrt{q^{f}}-\\sqrt{q_{0}}}^{n m}-P_{g_{f}}^{n m}\\right\\|_{\\mathrm{TV}}\\leq1\\wedge\\sqrt{\\frac{m n}{4}}D_{\\mathrm{KL}}(P_{\\sqrt{q}-\\sqrt{q_{0}}};P_{g_{f}})}}\\\\ &{}&{=1\\wedge\\frac{\\sqrt{m n}}{2}\\left\\|\\sqrt{q_{0}+2g_{f}/\\sqrt{d}}-\\sqrt{q_{0}}-g_{f}\\right\\|_{2}=:D_{f},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $P_{\\sqrt{q}-\\sqrt{q_{0}}}^{n}$ denotes the distribution of (34) and the square root is to be understood as applied coordinate wise. ", "page_idx": 24}, {"type": "text", "text": "Let $\\pi=N(0,d^{-1}(\\rho^{*})^{2}\\bar{\\Gamma})$ for a symmetric, idempotent matrix $\\bar{\\Gamma}\\in\\mathbb{R}^{d/2\\times d/2}$ with $d/4\\leq\\mathrm{rank}(\\bar{\\Gamma})\\leq$ $d/2$ . ", "page_idx": 24}, {"type": "text", "text": "We have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\in\\mathcal{T}_{\\mathcal{P}}}{\\operatorname*{inf}}\\mathcal{R}_{\\mathcal{P}}(T,H_{\\rho}^{\\prime})\\geq\\underset{T\\in\\mathcal{J}_{\\mathcal{P}}}{\\operatorname*{inf}}\\left[\\mathbb{P}_{0}T(Y)+\\displaystyle\\int\\mathbb{P}_{g_{f}}(1-T(Y))d\\pi(f)\\right]-2\\int D_{f}d\\pi(f)}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\pi\\left(f:f\\notin(I_{R}/\\sqrt{d})^{d/2}\\,\\,\\,\\mathrm{or}\\,\\,\\left\\|(q_{i}^{f})_{i\\in[d]}-q_{0}\\right\\|_{1}<\\rho\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma 8, the model $\\{P_{g_{f}}\\;:\\;f\\;\\in\\;I_{R}^{d/2}/\\sqrt{d}\\}$ is equivalent to the model generated by the observations ", "page_idx": 24}, {"type": "equation", "text": "$$\nS_{i}^{(j)}:=f_{i}+\\frac{1}{\\sqrt{n}}Z_{i}^{(j)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $i=1,\\ldots,d/2$ . Since ${\\mathcal{F}}^{\\prime}$ is bijective with $(I_{R}/\\sqrt{d})^{d/2}$ , Lemma 5 implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{J}_{\\mathcal{P}}}\\,\\bigg[\\mathbb{P}_{0}T(Y)+\\int\\mathbb{P}_{g_{f}}(1-T(Y))d\\pi(f)\\bigg]=\\operatorname*{inf}_{T\\in\\mathcal{J}_{\\mathcal{P}}}\\,\\bigg[\\mathbb{P}_{0}^{\\prime}T(Y)+\\int\\mathbb{P}_{f}^{\\prime}(1-T(Y))d\\pi(f)\\bigg]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\tilde{\\mathcal P}$ is the model generated by the observations in display (36) for $i=1,\\ldots,d/2$ and $\\mathbb{P}_{f}^{\\prime}$ denotes the distribution of the distributed protocol with data generated from $f\\in\\tilde{\\mathcal P}$ . ", "page_idx": 24}, {"type": "text", "text": "It follows from Lemma 6 in the case of bandwidth constraints or Lemma 7 in the case of privacy constraints (using that $\\rho\\ll\\rho^{*}$ in both cases) that the latter distributed testing risk is lower-bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-o(1)-\\pi\\left(f\\in\\mathbb{R}^{d/2}\\,:\\,f\\notin(I_{R}/\\sqrt{d})^{d/2}\\,\\mathrm{~or~}\\left\\|(q_{i}^{f})_{i\\in[d]}-q_{0}\\right\\|_{1}<\\rho\\right)-2\\int D_{f}d\\pi(f).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Addressing the third term in the display above; the theorem(s) ass\u221aume that md $\\log d/\\sqrt{n}\\stackrel{\\nu\\to\\infty}{\\rightarrow}0,$ $b\\geq1$ and $\\epsilon\\gg n^{-1/4}$ , we have that $\\rho^{*}\\ll1/\\sqrt{\\log(d)}$ , which gives $\\|\\sqrt{d}f_{i}\\|_{\\infty}\\rightarrow0$ with $\\pi$ -probability tending to one (see e.g. Lemma 16), which in turn implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\nf_{i}\\in I_{R}/{\\sqrt{d}}\\quad{\\mathrm{~for~all~}}i=1,\\ldots,d/2.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we show that $\\|(q_{i}^{f})_{i\\in[d]}-q_{0}\\|_{1}\\geq\\rho$ with $\\pi$ -probability tending to one. Since $\\textstyle\\sum_{i=1}^{d}|q_{i}^{f}-q_{0}|=$ $2\\sum_{i=1}^{d/2}|f_{i}/\\sqrt{d}|$ , we have that for some constants $c,c^{\\prime}>0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi\\left(\\left\\|q^{f}-q_{0}\\right\\|_{1}<\\rho\\right)\\leq\\pi\\left(\\left\\|f/\\sqrt{d}\\right\\|_{1}<\\rho\\right)\\leq1-\\operatorname*{Pr}\\left(\\left\\|\\bar{\\Gamma}Z\\right\\|_{1}\\geq c^{\\prime}d\\frac{\\rho}{\\rho^{*}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in the expression on the right-hand side, $Z\\sim N(0,I_{d/2})$ . Since $\\rho\\ll\\rho^{*}$ and $\\bar{\\Gamma}$ is idempotent with rank of the order $d$ , we can conclude that the expression vanishes. This takes care of the third term in (38). ", "page_idx": 25}, {"type": "text", "text": "For the last term in (38), the Taylor approximation $\\begin{array}{r}{\\sqrt{1+y}-1=y/2-y^{2}/8+\\frac{y^{3}}{16(1+\\eta_{y}^{5/2})}}\\end{array}$ for some $\\eta\\in[0,y]$ , combined with the fact that $\\|\\sqrt{d}f\\|_{\\infty}=o_{\\pi}(1)$ yields that ", "page_idx": 25}, {"type": "equation", "text": "$$\n2\\int D_{f}d\\pi(f)\\leq\\int1\\wedge\\sqrt{m n d}\\left\\|(f_{i}^{2})_{i\\in[d/2]}\\right\\|_{2}d\\pi(f)\\lesssim\\sqrt{m n}\\rho^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since the theorem(s) assume that $m d\\log d/\\sqrt{n}\\stackrel{\\nu\\to\\infty}{\\rightarrow}0,b\\geq1$ and $\\epsilon\\gg n^{-1/4}$ , the right-hand side of the above display vanishes. ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 3. Let $\\mathcal{T}_{\\Omega},\\mathcal{T}_{\\mathcal{P}}$ denote the class of distributed $b$ -bit bandwidth constrained testing protocols with $b\\in\\mathbb N$ , $m\\in\\mathbb{N}$ , $d\\in2\\mathbb{N}$ and $n\\in\\mathbb N$ and no access to shared randomness for the models $\\mathcal{Q}$ and $\\mathcal{P}$ , respectively. We note here that under the conditions of the theorem, we can assume $d$ and $n$ are both larger than some constant; and in particular we can assume $d\\in2\\mathbb{N}$ without loss of generality. Assume $d$ and $n$ satisfy (15), for a constant $C$ to be set later. The proof follows by the fact that the distributed testing problems have different minimax testing rates, for certain values of $b$ and $m$ . ", "page_idx": 25}, {"type": "text", "text": "Consider the hypothesis test given in (4), with $H_{0}:q_{0}=(1/d,\\dots,1/d)\\in\\mathbb{S}^{d}$ and $H_{\\rho}$ as in the display. ", "page_idx": 25}, {"type": "text", "text": "Set $b=\\lceil n\\log_{2}(d)\\rceil$ . When $b\\geq n\\log_{2}(d)$ , the observations $\\tilde{X}^{(j)}$ in the multinomial model as given in (3) are valid $b$ -bit transcripts, since $|\\{1,\\ldots,d\\}^{n}|\\,\\leq\\,n\\log_{2}(d)$ . These transcripts are therefore sufficient for the nondistributed $/$ unconstrained model $\\mathcal{Q}^{m}$ , i.e. corresponding to observations ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{X}\\sim Q_{q,n m}\\;\\;\\mathrm{for}\\;q\\in\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consequently, the distributed, $b$ -bit bandwidth constraint testing risk for $\\mathcal{Q}$ is equal to the testing risk $\\mathcal{Q}^{m}$ ; ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in{\\mathcal{T}}_{Q}}\\mathcal{R}_{\\mathcal{Q}}(H_{\\rho},T)=\\operatorname*{inf}_{T}\\mathcal{R}_{\\mathcal{Q}^{m}}(H_{\\rho},T).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This means that, for all $\\alpha\\in(0,1)$ , there exists $C_{\\alpha}>0$ and a distributed protocol $T$ satisfying a $b$ -bandwidth constraint for distributed experiment $\\mathcal{Q}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in{\\mathcal{T}}_{Q}}\\mathcal{R}_{Q}(H_{\\rho},T)<\\alpha\\;{\\mathrm{whenever}}\\,\\rho^{2}\\geq C_{\\alpha}{\\frac{\\sqrt{d}}{m n}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $H_{\\rho}$ as\u221a defined in (4), as the minimax rate for the unconstrained problem with mn observations is $\\rho_{\\mathcal{Q}^{m}}^{2}:=\\sqrt{d}/(m n)$ (see e.g. Theorem 3 in [70]). ", "page_idx": 25}, {"type": "text", "text": "On the other hand, whenever $m b=m\\lceil n\\log_{2}(d)\\rceil\\leq d$ , the minimax rate for the distributed testing risk of $\\mathcal{P}$ for the (comparable) hypotheses ", "page_idx": 25}, {"type": "equation", "text": "$$\nH_{0}:q=q_{0}\\ \\mathrm{{versus}}\\ \\tilde{H}_{\\rho}:\\|\\sqrt{q}-\\sqrt{q_{0}}\\|_{2}\\geq\\rho\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is bounded from below by $\\rho_{\\mathcal P}^{2}\\asymp\\sqrt d/(\\sqrt{m}n)$ , as a consequence of Theorem 4. Specifically, following the proof of Theorem 1 above, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{P}}}\\mathcal{R}_{\\mathcal{P}}(H_{\\rho},T)\\geq\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\hat{\\mathcal{P}}}}\\mathcal{R}_{\\tilde{\\mathcal{P}}}(\\tilde{H}_{\\rho},T),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{H}_{\\rho}:=\\left\\{f\\in(I_{R}/\\sqrt{d})^{d/2}:\\|f\\|_{1}\\geq\\rho\\right\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $I_{R}:=[\\sqrt{2}(1-\\sqrt{R})/\\sqrt{1+R},\\sqrt{2}(\\sqrt{R}-1)/\\sqrt{1+R}],$ , $\\tilde{\\mathcal P}$ is generated by the observations ", "page_idx": 26}, {"type": "equation", "text": "$$\nX^{(j)}=f+\\frac{1}{\\sqrt{n}}Z^{(j)}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $Z^{(j)}\\sim N(0,I_{d/2})$ , indexed by $f\\in(I_{R}/\\sqrt{d})^{d/2}$ and the class $\\mathcal{T}_{\\tilde{\\mathcal{P}}}$ is to be understood as the $b$ -bit bandwidth constraint distributed testing protocols for the model $\\tilde{\\mathcal{P}}$ and $j=1,\\dots,m$ machines. Lemma (6) implies that for all $\\alpha\\in(0,1)$ the latter is bounded by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\alpha-N(0,c_{\\alpha}^{-1/2}d^{-1}\\rho^{2}\\bar{\\Gamma})\\left(\\tilde{H}_{\\rho}^{c}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for a symm\u221aetric, idempotent matrix $\\bar{\\Gamma}\\in\\mathbb{R}^{d/2\\times d/2}$ with $d/4\\leq\\mathrm{rank}(\\bar{\\Gamma})\\leq d/2\\,\\alpha\\in(0,1)$ , whenever $\\rho^{2}\\leq c_{\\alpha}{\\frac{\\sqrt{d}}{\\sqrt{m}n}}$ for some small enough constant $c_{\\alpha}>0$ . By the same analysis as conducted in the proof of Theorem 1 above (using that $n\\leq\\log(d))$ , we find that the second term is at most $\\alpha/2$ for $c_{\\alpha}>0$ small enough. Summarizing, we find in particular that for some constant $c_{\\alpha}>0$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{P}}}\\,\\mathcal{R}_{\\mathcal{P}}(T,H_{\\rho})>1/3,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for all $\\rho^{2}\\leq c{\\sqrt{d}}/({\\sqrt{m}}n)$ and $m,n,b,d$ such that $m b\\le d$ , where the number $1/3$ is chosen without particular significance. ", "page_idx": 26}, {"type": "text", "text": "Whenever $m b=m\\lceil n\\log_{2}(d)\\rceil\\leq d,$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{Q}}\\mathcal{R}_{\\mathcal{Q}}(H_{\\rho},T)<1/6<1/3<\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{P}}}\\mathcal{R}_{\\mathcal{P}}(H_{\\rho},T).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some $C_{\\alpha}>0$ large enough and $c_{\\alpha}>0$ small enough. Take the constant $C=\\lceil C_{\\alpha}^{2}/c_{\\alpha}^{2}\\rceil$ such that if $m=C$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\nC_{\\alpha}\\frac{\\sqrt{d}}{m n}\\leq\\rho^{2}\\leq c_{\\alpha}\\frac{\\sqrt{d}}{\\sqrt{m}n},\\;\\mathrm{with}\\;\\rho^{2}:=C_{\\alpha}\\frac{\\sqrt{d}}{\\sqrt{M}\\sqrt{m}n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now suppose that $C{\\mathfrak{d}}(\\mathcal{Q},\\mathcal{P})\\leq1/6.$ Lemma 5 then implies in that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{P}}}\\mathcal{R}_{\\mathcal{P}}(H_{\\rho},T)\\leq\\operatorname*{inf}_{T\\in\\mathcal{T}_{\\mathcal{Q}}}\\mathcal{R}_{\\mathcal{Q}}(H_{\\rho},T)+1/6<1/3.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This contradicts (40). We conclude that ", "page_idx": 26}, {"type": "equation", "text": "$$\nC\\mathfrak{d}(\\mathcal{Q},\\mathcal{P})>1/6,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "whenever $d/\\lceil n\\log_{2}(d)\\rceil>C$ . The result now follows with $c=1/(6C)$ . ", "page_idx": 26}, {"type": "text", "text": "E Auxilliary lemmas ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The following lemma is used in the comparison of the multinomial model to the many-normal-means model. ", "page_idx": 26}, {"type": "text", "text": "Lemma 8. Let $d\\in2\\mathbb{N}$ , ${\\mathcal{F}}\\subset\\mathbb{R}^{d/2}$ , and consider for $i=1,\\ldots,d$ independent random variables $X_{i}=h_{i}+\\sigma Z_{i}$ with $\\sigma>0$ and $Z_{i}\\sim N(0,1)$ satisfying ", "page_idx": 26}, {"type": "equation", "text": "$$\nh_{i}=\\left\\{\\!\\!\\begin{array}{l l}{\\!a_{i}f_{i}\\quad}&{i f i\\leq d/2,}\\\\ {\\!-a_{i}f_{i-d/2}\\quad}&{i f i>d/2,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some $f\\in\\mathcal F$ and $a=(a_{i})_{i\\in[d]}\\in\\mathbb{R}^{d}$ . Let $\\mathcal{P}$ denote the model generated by the observations $X:=(X_{1},\\ldots,X_{d})\\sim P_{f}$ , $f\\in\\mathcal F$ and let $\\mathcal{Q}$ denote the model generated by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{X}_{i}=(a_{i}+a_{d/2+i})f_{i}+\\sqrt{2}\\sigma Z_{i},\\quad f o r\\,i=1,\\dots,d/2,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $Z_{i}\\overset{i.i.d.}{\\sim}N(0,1)$ and $f\\in\\mathcal F$ . ", "page_idx": 26}, {"type": "text", "text": "Then, $\\Delta(\\mathcal{P},\\mathcal{Q})=0$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. The statistic $S=\\left(a_{i}\\ X_{i}-a_{i}X_{d/2+1}\\right)_{i\\in[d]}$ is sufficient for the model $\\mathcal{P}$ by using NeymanFisher (Lemma 2). We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\frac{d P_{f}}{d P_{0}}(X)=\\underset{i=1}{\\overset{d}{\\prod}}\\exp\\bigg(\\sigma^{-1}X_{i}h_{i}-\\frac{1}{2\\sigma^{2}}h_{i}^{2}\\bigg)}\\\\ &{\\qquad\\qquad=\\underset{i=1}{\\overset{d/2}{\\prod}}\\exp\\bigg(\\sigma^{-1}(a_{i}X_{i}-a_{i}X_{d/2+1})f_{i}-\\frac{1}{\\sigma^{2}}f_{i}^{2}\\bigg)}&&{=e^{\\sigma^{-1}S^{\\top}f-\\frac{1}{\\sigma^{2}}\\|f\\|_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In distribution, $\\tilde{X}=(\\tilde{X}_{i})_{i\\in[d]}$ is equal to $S$ , which implies $\\Delta(\\mathcal{P},\\mathcal{Q})=0$ per Lemma 2. ", "page_idx": 27}, {"type": "text", "text": "The following lemmas are well known but included for completeness. ", "page_idx": 27}, {"type": "text", "text": "Lemma 9. Let $P_{f}$ denote the distribution of a $N(f,\\sigma I_{d})$ distributed random vector for $f\\in\\mathbb{R}^{d}$ and let $P_{f}^{n}$ denote the distribution of $n$ i.i.d. draws (i.e. $\\textstyle P_{f}^{n}=\\bigotimes_{i=1}^{n}P_{f})$ . ", "page_idx": 27}, {"type": "text", "text": "It holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|P_{f}^{n}-P_{g}^{n}\\right\\|_{\\mathrm{TV}}\\leq\\frac{n}{2\\sigma}\\left\\|f-g\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. By Pinsker\u2019s inequality, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|P_{f}^{n}-P_{g}^{n}\\|_{\\mathrm{TV}}\\leq\\sqrt{\\frac{n}{2}D_{\\mathrm{KL}}(P_{f};P_{g})}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "A straightforward calculation gives that the latter is bounded by ${\\frac{\\sqrt{n}}{2\\sigma}}\\left\\|f-g\\right\\|_{2}$ . ", "page_idx": 27}, {"type": "text", "text": "The following lemma relates the total variation distance between $P,Q$ to the $L_{1}$ -distance between corresponding densities. ", "page_idx": 27}, {"type": "text", "text": "Lemma 10. Let $P,Q$ be probability measures dominated by a sigma-finite measure $\\mu$ with corresponding probability densities $\\textstyle p={\\frac{d P}{d\\mu}}\\,$ and $\\begin{array}{r}{q=\\frac{d Q}{d\\mu}}\\end{array}$ ddQ\u00b5 . It holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|P-Q\\|_{\\mathrm{TV}}=\\frac{1}{2}\\int|p(x)-q(x)|d\\mu(x).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. See e.g. Section 2.4 in [93]. ", "page_idx": 27}, {"type": "text", "text": "Lemma 11. For any two probability measures $P$ and $Q$ on a measurable space $(\\mathcal{X},\\mathcal{X})$ with $\\mathcal{X}\\,a$ Polish space and $\\mathcal{X}$ its Borel sigma-algebra. There exists a coupling $\\mathbb{P}^{X,\\tilde{X}}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|P-Q\\|_{\\mathrm{TV}}=2\\mathbb{P}^{X,\\tilde{X}}\\left(X\\neq\\tilde{X}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. See e.g. Section 8.3 in [91]. ", "page_idx": 27}, {"type": "text", "text": "The next lemma gives a useful characterization of the total variation distance between two probability measures. ", "page_idx": 27}, {"type": "text", "text": "Lemma 12. Let $P$ be a signed, bounded measure defined on measurable space $(\\mathcal{X},\\mathcal{X})$ and suppose that $P\\ll\\nu$ for a sigma-finite measure $\\nu$ . It holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|P\\|_{\\mathrm{TV}}=\\frac{1}{2}\\operatorname*{sup}\\left\\{\\int f d P:|f|\\leq1\\,a n d\\,f:\\mathcal{X}\\to\\mathbb{R}\\,i s\\,m e a s u r a b l e\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Consider the Jordan measure decomposition $P=P^{+}-P^{-}$ , where $P^{+},P^{-}$ are both positive, bounded measures such that $P^{+}\\perp P^{-}$ . For any measurable $f,\\,\\{f\\geq0\\},\\{f\\leq0\\}\\in\\mathcal{X}$ , so $\\vert f\\vert\\le1$ means that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int f d P\\le\\int f\\mathbb{1}_{\\{f\\ge0\\}}d P^{+}-\\int f\\mathbb{1}_{\\{f\\le0\\}}d P^{-}}}\\\\ &{}&{\\le\\int\\mathbb{1}_{\\{f\\ge0\\}}d P^{+}+\\int\\mathbb{1}_{\\{f\\le0\\}}d P^{-}}\\\\ &{}&{\\le\\|P^{+}\\|_{\\mathrm{TV}}+\\|P^{-}\\|_{\\mathrm{TV}}\\le2\\|P\\|_{\\mathrm{TV}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the other direction, note that $f={\\mathrm{sign}}(p-q)$ is measurable and bounded by 1, which gives ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\int f d P=\\int|p-q|d\\nu=\\|P-Q\\|_{\\mathrm{TV}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last equality follows from Lemma 10. ", "page_idx": 28}, {"type": "text", "text": "Lemma 13. Let $\\textstyle P=\\otimes_{j=1}^{m}P_{j}$ and $\\boldsymbol{Q}=\\otimes_{j=1}^{m}Q_{j}$ for probability measures $P_{j},Q_{j}$ defined on $a$ common measurable space $(\\mathcal{X},\\mathcal{X})$ , with probability densities $p_{j},q_{j}$ for $j=1,\\cdot\\cdot\\cdot m$ . It holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|P-Q\\|_{\\mathrm{TV}}\\leq\\sum_{j=1}^{m}\\|P_{j}-Q_{j}\\|_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. The measures $P_{j}$ and $Q_{j}$ admit densities with respect to $P_{j}+Q_{j}$ , which we shall denote by $p_{j}$ and $q_{j}$ , respectively, with ", "page_idx": 28}, {"type": "equation", "text": "$$\np:=\\prod_{j=1}^{m}p_{j}=\\frac{d\\bigotimes_{j=1}^{m}P_{j}}{d\\bigotimes_{j=1}^{m}(P_{j}+Q_{j})}\\,\\,\\mathrm{~and~}\\,q:=\\prod_{j=1}^{m}q_{j}=\\frac{d\\bigotimes_{j=1}^{m}Q_{j}}{d\\bigotimes_{j=1}^{m}(P_{j}+Q_{j})}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Writing $\\mu=\\otimes_{j=1}^{m}(P_{j}+Q_{j})$ and applying Lemma 10 we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|P-Q\\|_{\\mathrm{TV}}=\\frac{1}{2}\\int|\\underset{j=1}{\\overset{m}{\\prod}}p_{j}(x_{j})-\\underset{j=1}{\\overset{m}{\\prod}}q_{j}(x_{j})|d\\mu(x_{1},\\dots,x_{m}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the telescoping product identity ", "page_idx": 28}, {"type": "equation", "text": "$$\na_{1}\\cdot a_{2}\\cdot\\cdot\\cdot a_{m}-b_{1}\\cdot b_{2}\\cdot\\cdot\\cdot b_{m}=\\sum_{j=1}^{m}(a_{j}-b_{j}){\\prod_{k=1}^{j-1}}a_{k}{\\prod_{k=j+1}^{m}}b_{k}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and Fubini\u2019s Theorem, the right-hand side of (43) is bounded by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{m}\\frac{1}{2}\\int|p_{j}(x_{j})-q_{j}(x_{j})|d(P_{j}+Q_{j})(x_{j})=\\sum_{j=1}^{m}\\lVert P_{j}-Q_{j}\\rVert_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The following lemma can be seen as a data processing inequality for the total variation distance. ", "page_idx": 28}, {"type": "text", "text": "Lemma 14. Let $(\\mathcal{X},\\mathcal{X})$ and $(\\mathcal{Y},\\mathcal{Y})$ be two measurable spaces and let $K:\\mathscr{V}\\times\\mathcal{X}\\to[0,1]$ be $a$ Markov kernel. For any probability measures $P,Q$ defined on $\\mathcal{X}$ it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|P K-Q K\\|_{\\mathrm{TV}}\\leq\\|P-Q\\|_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. This follows immediately from the representation in Lemma 12 combined with the fact that, for $|\\bar{f}|\\leq1,x\\mapsto\\textstyle\\int f(y)d K(y|\\bar{x})$ is a measurable function bounded by 1, since $K$ is Markov kernel. Hence, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{sup}_{A}\\!|P K(A)-Q K(A)|=\\frac{1}{2}\\operatorname*{sup}_{f}\\int\\int f(y)d K(y|x)d(P-Q)(x)}}\\\\ &{}&{\\leq\\frac{1}{2}\\mathrm{sup}\\int f(x)d(P-Q)(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The next lemma bounds the $L_{1}$ -distance $\\lVert p-q\\rVert_{1}$ between densities with a multiple of the Hellinger distance $2^{-1/2}\\lVert\\sqrt{p}-\\sqrt{q}\\rVert_{2}$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma 15. For two probability densities $p,q$ with respect to $\\mu_{\\cdot}$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\int|p(x)-q(x)|\\,d\\mu(x)\\leq{\\sqrt{\\int\\left({\\sqrt{p(x)}}-{\\sqrt{q(x)}}\\right)^{2}d\\mu(x)}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. The result follow from the Cauchy-Schwarz inequality and the fact that $\\textstyle{\\int{p d\\mu}=\\int q d\\mu}=1$ . See e.g. [93] for details. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma 16. Let $K\\in\\mathbb N$ and $M\\in\\mathbb{R}^{K\\times K}$ be symmetric and positive definite. Consider the random vector $G=(G_{1},\\ldots,G_{K})\\sim N(0,M).$ . It holds that $\\mathbb{E}_{1\\leq i\\leq K}|G_{i}|\\leq3\\|M\\|\\sqrt{\\log(K)\\vee\\log(2)}$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\nP r\\left(\\operatorname*{max}_{1\\leq i\\leq K}G_{i}^{2}\\geq\\|M\\|^{2}x\\right)\\leq\\frac{2K}{e^{x/4}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $x>0$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. It holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\nG\\stackrel{d}{=}\\sqrt{M}Z,\\quad\\mathrm{with}\\quad Z\\sim N(0,I_{K}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $M$ is symmetric, positive definite, it has SVD decomposition $M=V\\mathrm{Diag}(\\lambda_{1},\\dots,\\lambda_{K})V^{\\top}$ . Since $V$ is orthonormal, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sqrt{M}Z=V\\sqrt{\\mathbf{D}\\mathrm{i}\\mathbf{a}\\mathbf{g}(\\lambda_{1},\\ldots,\\lambda_{K})}(V^{\\top}Z)\\overset{d}{=}V\\sqrt{\\mathbf{D}\\mathrm{i}\\mathbf{a}\\mathbf{g}(\\lambda_{1},\\ldots,\\lambda_{K})}Z.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Writing $V=\\left[\\boldsymbol{v}_{1}~.~.~\\boldsymbol{v}_{K}\\right]$ where $v_{k}$ are orthogonal unit vectors, the latter display equals ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sqrt{\\lambda_{k}}v_{k}Z_{k}\\ \\sim\\ N\\left(0,\\mathrm{Diag}(\\lambda_{1},\\ldots,\\lambda_{K})\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consequently, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{k\\in[K]}|G_{k}|\\stackrel{d}{=}\\operatorname*{max}_{k\\in[K]}|\\lambda_{k}Z_{k}|\\leq\\|M\\|\\operatorname*{max}_{k\\in[K]}|Z_{k}|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, it suffices to show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\operatorname*{max}_{1\\leq i\\leq K}Z_{i}^{2}\\geq x\\right)\\leq{\\frac{2K}{e^{x/4}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The case where $K=1$ follows by standard Gaussian concentration properties. Assume $K\\geq2$ . For $0\\le t\\le1/4$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}e^{t\\operatorname*{max}_{i}{(Z_{i})^{2}}}=e^{t}\\mathbb{E}\\operatorname*{max}_{i}{e^{t(Z_{i}^{2}-1)}}\\leq K e^{2t^{2}+t}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking $t=1/4$ and applying Markov\u2019s inequality yields the second statement of the lemma. Furthermore, in view of Jensen\u2019s inequality ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{max}_{i}\\left(Z_{i}\\right)^{2}\\leq\\frac{\\log(K)}{t}+2t+1,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which in turn yields $\\mathbb{E}\\operatorname*{max}_{i}|Z_{i}|\\leq3{\\sqrt{\\log(K)}}$ . ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 30}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 30}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 30}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 30}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 30}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 30}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: Yes. ", "page_idx": 30}, {"type": "text", "text": "Justification: Proofs of all theory are provided. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: Yes. ", "page_idx": 30}, {"type": "text", "text": "Justification: See discussion ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: Yes. ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: NA   \nJustification: No code / experiments are provided.   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: NA ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: No code. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 32}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: NA   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: NA   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: Yes Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: NA ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper focuses on theoretical developments in statistical models without negative societal applications. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: NA   \nJustification: All the results are of theoretical nature; no data or models are released. Guidelines:   \n\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: NA ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: NA ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 36}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: NA Justification: Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]