[{"figure_path": "qXZVSy9LFR/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of multimodal emotion reasoning results on the EMER dataset. Clue Overlap and Label Overlap scores range from 0 to 10.", "description": "This table presents a comparison of the performance of various multimodal large language models (MLLMs) on the EMER dataset, which focuses on emotional reasoning capabilities.  The comparison is based on two key metrics: Clue Overlap, measuring the overlap between emotion-related clues identified by the model and ground truth; and Label Overlap, measuring the overlap between the summarized emotional states.  The table shows that Emotion-LLaMA outperforms other MLLMs, achieving the highest scores in both Clue Overlap and Label Overlap.", "section": "4.3 Comparison with State-of-the-Art Methods"}, {"figure_path": "qXZVSy9LFR/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of multimodal emotion recognition results on DFEW. The upper part shows zero-shot performance, while the lower part shows results after fine-tuning.", "description": "This table compares the performance of Emotion-LLaMA with other state-of-the-art models on the DFEW dataset for both zero-shot and fine-tuned settings.  It evaluates the models' ability to recognize seven basic emotions (Happy, Sad, Neutral, Angry, Surprise, Disgust, Fear) using metrics such as Unweighted Average Recall (UAR) and Weighted Average Recall (WAR), which are particularly suitable for imbalanced datasets. The zero-shot results demonstrate Emotion-LLaMA's generalization ability without any fine-tuning on DFEW, while fine-tuned results highlight the performance improvement achieved by fine-tuning on the target dataset.", "section": "4.3 Multimodal Emotion Recognition Results"}, {"figure_path": "qXZVSy9LFR/tables/tables_7_1.jpg", "caption": "Table 3: Comparison of multimodal emotion recognition results on MER2023. The table shows the performance of different models across various modalities, with the highest F1 scores achieved by our proposed method.", "description": "This table presents a comparison of the F1 scores achieved by different methods on the MER2023 dataset for multimodal emotion recognition.  The methods vary in the modalities they utilize (audio, video, text, or a combination thereof). The table highlights the superior performance of the Emotion-LLaMA model, achieving the highest F1 score of 0.9036 when using audio, video, and text modalities.", "section": "4 Experiments"}, {"figure_path": "qXZVSy9LFR/tables/tables_7_2.jpg", "caption": "Table 4: Performance (%) of Multimodal Large Language Models on MER2024 Challenge track 3: MER-OV. The \u201cavg\u201d column represents the average of \"Accuracys\" and \"Recalls\"", "description": "This table compares the performance of various multimodal large language models on the MER2024 Challenge track 3, specifically focusing on the MER-OV task.  MER-OV is an open-vocabulary task which means that the models are evaluated on their ability to recognize any number of emotion labels across diverse categories. The table shows the accuracy, recall, and average of accuracy and recall for each model, providing insights into their effectiveness in handling open-vocabulary multimodal emotion recognition.", "section": "4 Experiments"}, {"figure_path": "qXZVSy9LFR/tables/tables_7_3.jpg", "caption": "Table 1: Comparison of multimodal emotion reasoning results on the EMER dataset. Clue Overlap and Label Overlap scores range from 0 to 10.", "description": "This table compares the performance of Emotion-LLaMA with other state-of-the-art Multimodal Large Language Models (MLLMs) on the EMER dataset for multimodal emotion reasoning.  The EMER dataset uses human annotators to assess the quality of the models' reasoning, focusing on three metrics: Clue Overlap (measuring the overlap between emotion-related clues in the model's and the human's responses), Label Overlap (measuring the overlap between summarized emotional states), and completeness of the reasoning process.  Higher scores in Clue Overlap and Label Overlap indicate better performance.", "section": "4.3 Comparison with State-of-the-Art Methods"}, {"figure_path": "qXZVSy9LFR/tables/tables_8_1.jpg", "caption": "Table 6: Ablation Study Results for Different Encoders", "description": "This table presents the ablation study results focusing on the impact of different audio and visual encoders on the overall performance of the Emotion-LLaMA model. The F1 score is used as the evaluation metric to assess the model's performance with various combinations of encoders. The results highlight the importance of using multiple modalities, including audio and multi-view visual information, for accurate emotion recognition.", "section": "4.6 Ablation Evaluation"}, {"figure_path": "qXZVSy9LFR/tables/tables_9_1.jpg", "caption": "Table 7: Ablation Study Results for Different Stages of the MERR Dataset Instructions.", "description": "This table presents the ablation study results, demonstrating the impact of different stages of instruction data on the Emotion-LLaMA model's performance.  It shows the results for three stages: Raw (direct concatenation of visual and audio descriptions), Coarse-grained (Emotion-LLaMA trained on coarse-grained annotations from MERR dataset), and Fine-grained (further instruction-tuning using fine-grained annotations). The Clue Overlap and Label Overlap scores indicate the model's performance in terms of aligning emotional cues and identifying emotion labels, respectively. The table demonstrates that using the fine-grained annotations improves the model's performance on both metrics. ", "section": "4 Experiments"}, {"figure_path": "qXZVSy9LFR/tables/tables_9_2.jpg", "caption": "Table 3: Comparison of multimodal emotion recognition results on MER2023. The table shows the performance of different models across various modalities, with the highest F1 scores achieved by our proposed method.", "description": "This table compares the performance of different models on the MER2023 dataset for multimodal emotion recognition.  The models are evaluated across various modalities (audio, visual, text), and their performance is measured using the F1 score. The table highlights that the Emotion-LLaMA model achieves the highest F1 score, demonstrating its superior performance in this task.", "section": "4 Experiments"}, {"figure_path": "qXZVSy9LFR/tables/tables_20_1.jpg", "caption": "Table 9: The scores from manual evaluation of the MERR datasets for fine-grained annotation. The scores range from 0 to 5, with the Mean representing the average score across all categories.", "description": "This table presents the results of a human evaluation of the quality of the annotations in the MERR dataset.  Five volunteers assessed the accuracy and relevance of the video descriptions for 20 randomly selected samples from each of the nine emotion categories. Each volunteer scored a total of 180 descriptions (20 samples x 9 categories).  The table shows the individual scores for each volunteer, across the nine emotion categories, and the average score across all categories. The scores range from 0 to 5, representing the quality of the annotation in five levels.", "section": "A.4 Data Statistics and Comparisons"}, {"figure_path": "qXZVSy9LFR/tables/tables_25_1.jpg", "caption": "Table 15: Comparison of emotional datasets. The table presents a comparative analysis of several key emotional datasets, including DFEW, MER2023, EMER, and MERR. It highlights the unique features and contributions of each dataset, such as the range of emotion categories, availability of multimodal annotations, and dataset size. This comparison underscores the significance of the MERR dataset in advancing multimodal emotion recognition and reasoning research.", "description": "This table compares several emotional datasets (EmoSet, EmoVIT, DFEW, MER2023, EMER) against the proposed MERR dataset.  It highlights the differences in terms of the quantity of data, whether they include audio, visual objective and visual expression descriptions, if there are classification labels and finally if there are multimodal descriptions. The comparison emphasizes the richness of the MERR dataset, which provides more comprehensive annotations suitable for multimodal emotion recognition and reasoning.", "section": "A MERR Dataset Details"}, {"figure_path": "qXZVSy9LFR/tables/tables_36_1.jpg", "caption": "Table 15: Comparison of emotional datasets. The table presents a comparative analysis of several key emotional datasets, including DFEW, MER2023, EMER, and MERR. It highlights the unique features and contributions of each dataset, such as the range of emotion categories, availability of multimodal annotations, and dataset size. This comparison underscores the significance of the MERR dataset in advancing multimodal emotion recognition and reasoning research.", "description": "This table compares several key emotional datasets (DFEW, MER2023, EMER, MERR) based on several factors: quantity of samples, presence of audio/visual/textual descriptions, availability of visual expression descriptions and classification labels, and the availability of multimodal descriptions.  It highlights the relative strengths and weaknesses of each dataset for emotion recognition and reasoning research and emphasizes the unique contribution of the MERR dataset.", "section": "A.4 Data Statistics and Comparisons"}, {"figure_path": "qXZVSy9LFR/tables/tables_37_1.jpg", "caption": "Table 1: Comparison of multimodal emotion reasoning results on the EMER dataset. Clue Overlap and Label Overlap scores range from 0 to 10.", "description": "This table presents the results of a comparative analysis of different multimodal large language models' performance on the EMER dataset, focusing on two key metrics: Clue Overlap and Label Overlap.  These metrics assess the models' abilities to reason about emotions by evaluating the overlap between the models' generated reasoning and the ground truth reasoning provided in the dataset. Higher scores on both metrics indicate better emotion reasoning capabilities. The table allows for a comparison of Emotion-LLaMA's performance against other state-of-the-art models.", "section": "4.3 Comparison with State-of-the-Art Methods"}]