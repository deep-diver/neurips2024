[{"figure_path": "qXZVSy9LFR/figures/figures_2_1.jpg", "caption": "Figure 1: Example of the MERR dataset: It includes audio tone description, lexical subtitle, visual objective description, visual expression description, classification label, and multimodal description.", "description": "This figure shows an example from the MERR dataset, illustrating how multimodal data (audio, video, text) is annotated.  It highlights the various components of the annotation: the audio tone description captures vocal cues; the lexical subtitle provides the transcribed text; the visual objective description summarizes the broader scene; the visual expression description details specific facial expressions (Action Units); a classification label assigns a primary emotion; and the multimodal description integrates all modalities for a comprehensive understanding of the emotional context. This figure is crucial in illustrating the richness and complexity of the MERR dataset annotations.", "section": "3.1 MERR Dataset Construction"}, {"figure_path": "qXZVSy9LFR/figures/figures_4_1.jpg", "caption": "Figure 2: Architecture of Emotion-LLaMA, which integrates audio, visual, and text inputs for multimodal emotional recognition and reasoning.", "description": "The figure shows the architecture of the Emotion-LLaMA model, highlighting its multimodal nature.  Audio, visual (from a peak frame and video sequences), and textual inputs are processed by their respective encoders (HuBERT for audio, MAE, VideoMAE, and EVA for vision). These encoders' outputs are projected into a common embedding space using linear layers and then inputted into a modified LLaMA language model. The LLaMA model has learnable weights (LoRA) and frozen weights, integrating the information to produce the final multimodal output.  Instruction tuning is used to improve both emotional recognition and reasoning.", "section": "3.2 Multimodal Emotion-LLAMA Model"}, {"figure_path": "qXZVSy9LFR/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of the output probability distribution for multimodal emotion recognition by different models. Each sample is represented by two bar graphs: the left graph displays the results from other models, and the right graph shows the results from Emotion-LLaMA.", "description": "This figure visualizes the output probability distributions of different models for multimodal emotion recognition. Each sample is shown using two bar graphs. The left graph shows the results from other models in comparison to Emotion-LLaMA, highlighting the differences in their recognition capabilities. The right graph shows the results for Emotion-LLaMA, which demonstrates its superior performance.", "section": "4.6 Ablation Evaluation"}, {"figure_path": "qXZVSy9LFR/figures/figures_22_1.jpg", "caption": "Figure 4: Distribution and analysis of Action Units (AUs) in the MERR dataset. The top part displays the frequency of different AUs across all samples, highlighting the most prevalent facial movements. The bottom part presents a statistical breakdown of the top five most frequent AUs for each of the nine facial expression categories, providing insights into the facial patterns associated with each emotion.", "description": "This figure shows the distribution and analysis of Action Units (AUs) in the MERR dataset. The top part displays the frequency distribution of all AUs in the dataset. The bottom part shows the distribution of the top five most frequent AUs for each of the nine emotion categories (neutral, happy, angry, worried, sad, surprise, fear, doubt, contempt). This provides insights into the specific facial muscle movements associated with each emotion category, which helps in better understanding of the relationship between AUs and emotional expressions.", "section": "A MERR Dataset Details"}, {"figure_path": "qXZVSy9LFR/figures/figures_23_1.jpg", "caption": "Figure 5: Comparative analysis of label distributions across the DFEW, MER2023, EMER, and MERR datasets. The left side illustrates the individual label distributions for each dataset, while the right side presents a side-by-side comparison of the label distributions, highlighting the similarities and differences in emotional category representation among the datasets.", "description": "This figure shows a comparison of the distribution of emotion labels across four different datasets: DFEW, MER2023, EMER, and MERR. The left panels show the individual label distributions for each dataset, illustrating the relative frequency of each emotion category within each dataset. The right panel presents a side-by-side comparison of these distributions, allowing for a visual comparison of the similarities and differences in emotional category representation across the four datasets.  This visualization helps to highlight which datasets focus more on certain emotions than others and shows the diversity of emotion representations across the datasets.", "section": "A MERR Dataset Details"}, {"figure_path": "qXZVSy9LFR/figures/figures_23_2.jpg", "caption": "Figure 6: Overview of the video expression annotation process using Action Units (AUs). The top section illustrates the sampling of frames from a video, extraction of aligned faces, and measurement of AU intensities in each frame. The bottom section depicts the identification of the emotional peak frame based on the highest total AU intensity. The AUs of the peak frame are then translated into corresponding facial expression descriptions, providing a detailed characterization of the emotional expression at the most salient moment in the video.", "description": "This figure illustrates the process of annotating video data for emotional expression using Action Units (AUs).  The top row shows the steps involved in identifying the face, extracting its texture features, and classifying the action units (AUs) present in each frame. The bottom row shows the identification of the emotional peak frame (the frame with the highest cumulative AU intensity), the AU intensities across multiple frames, and finally, the generation of a visual description of the emotional expression based on the AUs.", "section": "3.1 MERR Dataset Construction"}, {"figure_path": "qXZVSy9LFR/figures/figures_24_1.jpg", "caption": "Figure 5: Comparative analysis of label distributions across the DFEW, MER2023, EMER, and MERR datasets. The left side illustrates the individual label distributions for each dataset, while the right side presents a side-by-side comparison of the label distributions, highlighting the similarities and differences in emotional category representation among the datasets.", "description": "This figure compares the distribution of emotion labels across four different datasets: DFEW, MER2023, EMER, and MERR.  The left panel shows individual bar charts for each dataset, illustrating the frequency of each emotion category. The right panel provides a direct comparison, making it easy to see similarities and differences in how each dataset represents different emotions. This visualization is helpful in understanding how the MERR dataset compares to others in terms of emotion category balance and diversity, a crucial factor for training robust emotion recognition models.", "section": "A MERR Dataset Details"}, {"figure_path": "qXZVSy9LFR/figures/figures_25_1.jpg", "caption": "Figure 4: Distribution and analysis of Action Units (AUs) in the MERR dataset. The top part displays the frequency of different AUs across all samples, highlighting the most prevalent facial movements. The bottom part presents a statistical breakdown of the top five most frequent AUs for each of the nine facial expression categories, providing insights into the facial patterns associated with each emotion.", "description": "This figure shows the distribution and analysis of Action Units (AUs) in the MERR dataset. The top panel displays the overall frequency of each AU across all samples.  The bottom panels show the distribution of the top five most frequent AUs for each of the nine emotion categories (neutral, happy, angry, worried, surprise, sad, fear, doubt, contempt). This breakdown helps visualize which AUs are most strongly associated with each emotion, providing insights into the characteristic facial patterns associated with different emotions.", "section": "A.1 Emotion Categories and Annotations"}, {"figure_path": "qXZVSy9LFR/figures/figures_25_2.jpg", "caption": "Figure 7: Distribution of video durations in the MERR dataset. The histogram illustrates the range and frequency of video lengths, providing insights into the temporal characteristics of the emotional expressions captured in the dataset. The majority of videos fall within the 2\u20134 second range, striking a balance between sufficient context and manageable data size for annotation and processing.", "description": "This figure shows a histogram visualizing the distribution of video durations in the MERR dataset.  The x-axis represents the video duration in seconds, and the y-axis shows the count of videos with that duration. The distribution is heavily skewed towards shorter videos (2-4 seconds), indicating that most emotional expressions are captured within that timeframe.  This duration balances sufficient context for emotion recognition with manageable data size for annotation. The longer videos are rare, indicating that the dataset focuses primarily on the key moments of emotional expression.", "section": "A.4 Data Statistics and Comparisons"}, {"figure_path": "qXZVSy9LFR/figures/figures_32_1.jpg", "caption": "Figure 8: Confusion matrices for multimodal emotion recognition datasets. The figure presents confusion matrices that visualize the performance of the Emotion-LLaMA model on benchmark datasets such as MER2023 and DFEW. The matrices provide insights into the model's classification accuracy, highlighting the challenges and successes in distinguishing between different emotional categories. [Zoom in to view]", "description": "This figure shows two confusion matrices, one each for the MER2023-SEMI and DFEW datasets.  Each matrix visualizes the model's performance in classifying different emotions. The color intensity in each cell represents the proportion of samples correctly classified for each emotion pair.  Lighter colors indicate higher accuracy, while darker colors show misclassifications.  The matrices highlight the model's strengths in recognizing certain emotions while pointing to challenges in distinguishing between similar emotions, especially those with fewer samples in the training data.", "section": "Additional Results and Analysis"}, {"figure_path": "qXZVSy9LFR/figures/figures_33_1.jpg", "caption": "Figure 9: Ablation study results. (a) illustrates the impact of different learning rates on the model's performance, while (b) presents the effects of varying data ratios. These experiments provide valuable insights into the optimal hyperparameter settings and data requirements for training the Emotion-LLaMA model effectively. [Zoom in to view]", "description": "This figure shows the results of ablation studies on the Emotion-LLaMA model, investigating the impact of hyperparameters and data settings on model performance.  The left graph (a) displays how different learning rates (1e-4, 1e-5, 1e-6) affect the F1 score over training epochs. The right graph (b) shows how varying the data ratio (from 20% to 100%) impacts the F1 score.  Error bars are included in graph (b) to illustrate variability in the results. These experiments help determine the optimal learning rate and the minimum data needed for effective training.", "section": "C.2.2 Ablation Study of Hyperparameters"}, {"figure_path": "qXZVSy9LFR/figures/figures_34_1.jpg", "caption": "Figure 10: Ranking and Scores of Teams in the MER-Noise Track of MER2024.", "description": "This figure shows the ranking and weighted average F-scores of different teams participating in the MER-Noise track of the MER2024 challenge. The Emotion-LLaMA model (Ours) achieved the highest score, demonstrating its robustness to noise in the data.", "section": "C.2.3 MER2024 Challenge"}, {"figure_path": "qXZVSy9LFR/figures/figures_39_1.jpg", "caption": "Figure 11: Online demo interface of the Emotion-LLaMA model. The figure presents a screenshot of the interactive web-based demo, showcasing the model's capabilities in multimodal emotion recognition and reasoning. Users can input various combinations of visual, audio, and textual data and receive real-time emotion predictions and explanations, facilitating intuitive exploration and evaluation of the model's performance.", "description": "This figure shows a screenshot of the online demo for the Emotion-LLaMA model.  The demo allows users to input multimodal data (video, image, text) and receive real-time emotion predictions and explanations.  The interface is user-friendly and interactive, showing the model's ability to process various data types and generate insightful analysis.", "section": "C.3 Demonstration of Emotion-LLaMA"}, {"figure_path": "qXZVSy9LFR/figures/figures_40_1.jpg", "caption": "Figure 12: Detailed examples of general tasks performed by the Emotion-LLaMA model. The figure illustrates the model's versatility and robustness in handling tasks beyond emotion recognition, such as face detection and question answering. These examples highlight the model's ability to process and understand visual and textual information, enabling its application in a wide range of scenarios.", "description": "This figure showcases the Emotion-LLaMA model's capabilities in performing tasks beyond emotion recognition.  It demonstrates the model's ability to process and understand different types of input, including images and text, and to perform tasks such as face detection and question answering.  This highlights the model's versatility and potential for a wide range of applications.", "section": "C.3 Demonstration of Emotion-LLaMA"}, {"figure_path": "qXZVSy9LFR/figures/figures_41_1.jpg", "caption": "Figure 3: Visualization of the output probability distribution for multimodal emotion recognition by different models. Each sample is represented by two bar graphs: the left graph displays the results from other models, and the right graph shows the results from Emotion-LLaMA.", "description": "This figure compares the emotion recognition results of the Emotion-LLaMA model against other models. Each sample is presented using two bar graphs; the left graph shows results from other models, while the right shows Emotion-LLaMA's results.  The samples demonstrate Emotion-LLaMA's ability to accurately identify subtle emotional expressions even when the visual and audio cues are less pronounced. The figure visually shows the superiority of Emotion-LLaMA in capturing and interpreting emotional nuances from different modalities.", "section": "4.6 Ablation Evaluation"}]