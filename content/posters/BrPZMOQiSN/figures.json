[{"figure_path": "BrPZMOQiSN/figures/figures_3_1.jpg", "caption": "Figure 1: Differentiable pruning of weight blocks", "description": "The figure illustrates the process of differentiable pruning applied to weight blocks.  A dense weight matrix is first partitioned into smaller blocks. Then, each block is scaled by a softmax mask,  A<sub>ij</sub> = exp(L<sub>ij</sub>) / \u03a3<sub>i'j'</sub> exp(L<sub>i'j'</sub>), where L<sub>ij</sub> represents the logits associated with the block.  The softmax mask emphasizes the more important blocks by assigning them larger weights.", "section": "1.3 Empirical results: SequentialAttention++"}, {"figure_path": "BrPZMOQiSN/figures/figures_8_1.jpg", "caption": "Figure 2: (a) Softmax attention vs magnitude pruning, and (b) the SPARSIFICATION phase.", "description": "Figure 2(a) compares the effectiveness of using softmax attention scores versus magnitude (Frobenius norm) for block importance scores in pruning.  It shows the difference in validation accuracy (in percentage points) between models pruned using softmax attention and those pruned using magnitude, for various block sizes and sparsities. A positive value indicates that softmax attention outperforms magnitude pruning. Figure 2(b) illustrates the sparsity schedules used in ACDC and SequentialAttention++. ACDC uses a sharp transition between dense and sparse phases, while SequentialAttention++ employs a smoother, exponential sparsity schedule during the SPARSIFICATION phase.", "section": "Empirical results: SequentialAttention++"}, {"figure_path": "BrPZMOQiSN/figures/figures_17_1.jpg", "caption": "Figure 1: Differentiable pruning of weight blocks", "description": "The figure illustrates the process of differentiable pruning applied to weight blocks in a neural network.  The weight matrix is partitioned into blocks, each block is scaled by a softmax mask (calculated from logits L), and the process is guided by differentiable pruning techniques. This figure is crucial for understanding the proposed SequentialAttention++ algorithm, which extends differentiable pruning to improve block sparsification.", "section": "1.3 Empirical results: SequentialAttention++"}, {"figure_path": "BrPZMOQiSN/figures/figures_18_1.jpg", "caption": "Figure 1: Differentiable pruning of weight blocks", "description": "This figure illustrates the process of differentiable pruning applied to blocks of weights in a neural network.  The weight matrix is partitioned into blocks. Each block is then scaled by a softmax mask, where the mask values are determined by the exponentiation of logits and normalization, resulting in a sparse representation. This process integrates differentiable pruning techniques with combinatorial optimization to achieve efficient and accurate pruning.", "section": "1.3 Empirical results: SequentialAttention++"}]