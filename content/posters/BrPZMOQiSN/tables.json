[{"figure_path": "BrPZMOQiSN/tables/tables_6_1.jpg", "caption": "Table 1: Block sparsification of ResNet50 on ImageNet. Our dense baseline validation accuracy is 76.90. The dashes are results where the algorithms diverged because of extreme sparsity. The sparsities where chosen as 70%, 80%, 90%, 95%. As seen in the table, for larger block sizes the real sparsity is lower because we are only sparsifying layers with at least 100 blocks.", "description": "This table presents the results of block sparsification experiments using ResNet50 on the ImageNet dataset.  It compares the validation accuracy of four different algorithms (ACDC, SequentialAttention++, Magnitude Pruning, and PowerPropagation) across various sparsity levels (70%, 80%, 90%, 95%) and block sizes (8x8, 16x16, 32x32, 64x64). Dashes indicate cases where the algorithms failed to converge due to extreme sparsity.  For larger block sizes, the achieved sparsity is lower because only layers with at least 100 blocks are considered for pruning.", "section": "4.2 Results"}, {"figure_path": "BrPZMOQiSN/tables/tables_7_1.jpg", "caption": "Table 1: Block sparsification of ResNet50 on ImageNet. Our dense baseline validation accuracy is 76.90. The dashes are results where the algorithms diverged because of extreme sparsity. The sparsities where chosen as 70%, 80%, 90%, 95%. As seen in the table, for larger block sizes the real sparsity is lower because we are only sparsifying layers with at least 100 blocks.", "description": "This table presents the results of block sparsification experiments using ResNet50 on the ImageNet dataset.  It compares the validation accuracy of four different pruning algorithms (Magnitude, PowerPropagation, ACDC, and SequentialAttention++) across various block sizes (8x8, 16x16, 32x32, 64x64) and sparsity levels (70%, 80%, 90%, 95%).  The baseline validation accuracy (no pruning) is 76.90%. Dashes indicate cases where algorithms failed to converge due to excessive sparsity. For larger block sizes, the actual sparsity achieved is lower because only layers with at least 100 blocks are pruned.", "section": "4.2 Results"}, {"figure_path": "BrPZMOQiSN/tables/tables_16_1.jpg", "caption": "Table 2: Block sparsification on Criteo. The validation losses are an average of three runs. Our dense baseline validation loss is 0.4489.", "description": "This table presents the results of block sparsification experiments on the Criteo dataset.  It compares four different algorithms (Magnitude, PowerPropagation, ACDC, and SequentialAttention++) across varying sparsity levels (90%, 95%, 97%, 98%, 99%) and block sizes (5, 10, 20).  The validation loss, averaged over three runs, is reported for each combination of algorithm, sparsity, and block size.  A baseline validation loss for a dense model (0.4489) is also provided for comparison.", "section": "4.2 Results"}, {"figure_path": "BrPZMOQiSN/tables/tables_19_1.jpg", "caption": "Table 1: Block sparsification of ResNet50 on ImageNet. Our dense baseline validation accuracy is 76.90. The dashes are results where the algorithms diverged because of extreme sparsity. The sparsities where chosen as 70%, 80%, 90%, 95%. As seen in the table, for larger block sizes the real sparsity is lower because we are only sparsifying layers with at least 100 blocks.", "description": "This table presents the results of block sparsification experiments on the ResNet50 model trained on the ImageNet dataset.  It compares the validation accuracy achieved by four different algorithms: ACDC, SequentialAttention++, Magnitude Pruning, and PowerPropagation, across various sparsity levels (70%, 80%, 90%, 95%) and block sizes (8x8, 16x16, 32x32, 64x64).  Dashes indicate cases where algorithms failed to converge due to excessive sparsity. For larger block sizes, the actual sparsity is lower because only layers with at least 100 blocks were pruned.", "section": "4.2 Results"}, {"figure_path": "BrPZMOQiSN/tables/tables_19_2.jpg", "caption": "Table 4: Modifying the exponent constant in the schedule of the SPARSIFICATION phase. Block-sparse training of ResNet50 on ImageNet for 90% sparsity.", "description": "This table presents the results of an ablation study on the impact of modifying the exponent constant (c) used in the sparsity schedule of the SPARSIFICATION phase within the SequentialAttention++ algorithm.  ResNet50 was trained on ImageNet with a target sparsity of 90%, and different block sizes (8x8, 16x16, 32x32, 64x64) were evaluated. The table shows the validation accuracy achieved for different values of c (2, 4, and 8).  This allows assessing the impact of the rate at which the sparsity increases during the SPARSIFICATION phase on the final model's performance.", "section": "D.2 Choice of the SPARSIFICATION exponent"}]