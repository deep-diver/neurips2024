[{"type": "text", "text": "Constrained Sampling with Primal-Dual Langevin Monte Carlo ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Luiz F. O. Chamon University of Stuttgart luiz.chamon@simtech.uni-stuttgart.de ", "page_idx": 0}, {"type": "text", "text": "Mohammad Reza Karimi ETH Z\u00fcrich mkarimi@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Anna Korba CREST, ENSAE, IP Paris anna.korba@ensae.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This work considers the problem of sampling from a probability distribution known up to a normalization constant while satisfying a set of statistical constraints specified by the expected values of general nonlinear functions. This problem finds applications in, e.g., Bayesian inference, where it can constrain moments to evaluate counterfactual scenarios or enforce desiderata such as prediction fairness. Methods developed to handle support constraints, such as those based on mirror maps, barriers, and penalties, are not suited for this task. This work therefore relies on gradient descent-ascent dynamics in Wasserstein space to put forward a discretetime primal-dual Langevin Monte Carlo algorithm (PD-LMC) that simultaneously constrains the target distribution and samples from it. We analyze the convergence of PD-LMC under standard assumptions on the target distribution and constraints, namely (strong) convexity and log-Sobolev inequalities. To do so, we bring classical optimization arguments for saddle-point algorithms to the geometry of Wasserstein space. We illustrate the relevance and effectiveness of PD-LMC in several applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sampling is a fundamental task in statistics, with applications to estimation and decision making, and of growing interest in machine learning (ML), motivated by the need for uncertainty quantification and its success in generative tasks [1\u20133]. In these settings, the distribution we wish to sample from (target distribution) is often known only up to its normalization constant. This is the case, for instance, of score functions learned from data or posterior distributions of complex Bayesian models. Markov Chain Monte Carlo (MCMC) algorithms can be used to tackle this problem [4, 5] and Langevin Monte Carlo (LMC), in particular, has attracted considerable attention due to its simplicity, theoretical grounding, and effectiveness in practice [3, 6\u20139]. These sampling algorithms, however, do not naturally incorporate requirements on the samples they generate. Specifically, standard MCMC methods do not enforce restrictions on the target distributions, such as support (e.g., truncated Gaussian), conditional probabilities (e.g., fairness), or moments (e.g., portfolio return) constraints. This limitation is often addressed by post-processing, transforming variables, or by introducing penalties in the target distribution. Though successful in specific settings, these approaches have considerable downsides. Post-processing techniques such as rejection sampling (see, e.g., [10, 11]) may substantially reduce the effective number of samples (number of samples generated per iteration of the algorithm). Variable transformations based on link functions, projections, or mirror/proximal maps (see, e.g., [12\u201318]) only accommodate (deterministic) support constraints and are not suited for statistical requirements such as robustness or fairness [19\u201322]. Though modifying the target distribution directly offers more flexibility (see, e.g., [23]), it does not guarantee that constraints are satisfied (Table 1). We refer the reader to Appendix A for a more detailed literature review. ", "page_idx": 0}, {"type": "table", "img_path": "o6Hk6vld20/tmp/e21fdb9a84fd2fb243094c91302e847d1c5788bc5183d179be7cc9e8fa0bc777.jpg", "table_caption": ["Table 1: Type and target of sampling constraints. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper overcomes these issues by directly tackling the constrained sampling problem. Explicitly, it seeks to sample not from a target distribution $\\pi$ on $\\mathbb{R}^{d}$ , but from the distribution $\\mu^{\\star}$ that solves ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P^{\\star}\\triangleq\\underset{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}{\\mathrm{min}}}&{\\mathrm{KL}(\\mu\\|\\pi)}\\\\ {\\mathrm{subject}\\tan}&{\\mathbb{E}_{x\\sim\\mu}\\big[g_{i}(x)\\big]\\le0,\\;i=1,\\dots,I,}\\\\ &{\\mathbb{E}_{x\\sim\\mu}\\big[h_{j}(x)\\big]=0,\\;j=1,\\dots,J,}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ denotes the set of probability measures on $\\mathbb{R}^{d}$ with bounded second moments and the functions $g_{i},h_{j}$ represent the requirements. Note that (PI) only considers measures $\\mu$ against which $g_{i},h_{j}$ are integrable. Otherwise, the expectations are taken to be $+\\infty$ , making the corresponding measure infeasible. Observe that (PI) is more general than the support-constrained sampling problem considered in, e.g., [12\u201318]. Indeed, it constrains the distribution $\\mu$ rather than its samples $x$ (Table 1). Algorithms based on projections, barrier, or mirror maps are not suited for this type of constraints (see Section 2.2 for more details). To tackle (PI), this paper instead derives and analyzes a primal-dual LMC algorithm (PD-LMC) that is the sampling counterpart of gradient descent-ascent (GDA) methods from (Euclidean) optimization. A dual ascent algorithm was previously proposed to tackle (PI), but it requires the exact computation of expectations with respect to intractable distributions [24]. This paper not only overcomes this limitation, but also provides convergence guarantees for a broader class of constraint functions. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this work include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 a discrete-time constrained sampling algorithm (PD-LMC, Algorithm 1) for solving (PI) that precludes any explicit integration (Section 3);   \n\u2022 an analysis of PD-LMC proving that it converges sublinearly (in expectation) with respect to the Kullback-Leibler divergence (convex case) or Wasserstein distance (strongly convex case). The analysis is performed directly on the discrete-time iterations and requires only local Lipschitz continuity and bounded variance assumptions (Section 3.1);   \n\u2022 an extension of these result for target distributions satisfying a log-Sobolev inequality (LSI) for a variant of PD-LMC (Algorithm 2, Section 3.2);   \n\u2022 numerical examples illustrating applications of (PI) and the effectiveness of PD-LMC (Section 4). ", "page_idx": 1}, {"type": "text", "text": "2 Problem formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Background on Langevin Monte Carlo ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider a target distribution $\\pi\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ absolutely continuous with respect to Lebesgue measure whose density (also denoted $\\pi$ ) can be expressed as $\\pi(x)\\,=\\,e^{-f(x)}/Z$ for some normalization constant $Z$ . Define the Kullback-Leibler (KL) divergence of $\\mu$ with respect to $\\pi$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mu||\\pi)=\\int\\log\\left(\\frac{d\\mu}{d\\pi}\\right)d\\mu=\\int f d\\mu+\\int\\log(\\mu)d\\mu-\\log(Z)\\triangleq\\mathcal{V}(\\mu)+\\mathcal{H}(\\mu)-\\log(Z),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\frac{d\\mu}{d\\pi}$ is the Radon\u2013Nikodym derivative, $\\nu$ is the potential energy, and $\\mathcal{H}$ is the negative entropy if $\\mu$ is absolutely continuous with respect to $\\pi$ ; and $+\\infty$ otherwise. For a wide class of functions $f$ (e.g., ", "page_idx": 1}, {"type": "text", "text": "smooth and strongly convex), samples from $\\pi$ can be obtained from the path of the Langevin diffusion process, whose instantaneous values $x(t)$ have distributions $\\mu(t)$ evolving according to the Fokker\u2013Planck equation [25]. Explicitly, ", "page_idx": 2}, {"type": "equation", "text": "$$\nd x(t)=-\\nabla f(x(t))d t+\\sqrt{2}d W(t)\\quad\\mathrm{and}\\quad\\frac{\\partial\\mu(t)}{\\partial t}=\\nabla\\cdot\\big[\\mu(t)\\nabla_{W_{2}}\\mathrm{KL}(\\mu(t)\\|\\pi)\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a $d$ -dimensional Brownian motion $W(t)$ , where $\\nabla\\,\\cdot\\,q$ denotes the divergence of $q$ and $\\nabla_{W_{2}}\\operatorname{KL}(\\mu||\\pi)$ denotes the Wasserstein-2 gradient of $\\operatorname{KL}(\\cdot|\\pi)$ at $\\mu$ [26, Theorem 10.4.17] (see Appendix B for more details). Indeed, the Langevin diffusion (2) brings the distribution $\\mu(t)$ of $x(t)$ progressively closer to the target $\\pi$ . In fact, the Fokker-Planck equation can be interpreted as a gradient flow of the KL divergence with respect to the Wasserstein-2 distance [7, 25]. ", "page_idx": 2}, {"type": "text", "text": "However, computing the path of the stochastic differential equation in (2) is not practical and discretizations are used instead. Chief among them is the (forward) Euler\u2013Maruyama scheme, which leads to the celebrated Langevin Monte Carlo (LMC) algorithm [6] ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\gamma_{k}\\nabla f(x_{k})+\\sqrt{2\\gamma_{k}}\\beta_{k},\\;\\;\\;\\beta_{k}\\stackrel{\\mathrm{iid}}{\\sim}\\mathcal{N}(0,\\mathrm{I}_{d}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a step size $\\gamma_{k}\\,>\\,0$ , where $\\mathrm{I}_{d}$ denotes the $d$ -dimensional identity matrix. Notice that it is not necessary to know $Z$ in order to evaluate (3). This has made LMC and its variants widely popular in practice and the subject of extensive research. Despite (3) being a biased time-discretizations of the Langevin diffusion in (2) [7], rates of convergence of LMC have been obtained for smooth and strongly convex [8, 27] or convex [28] potentials or when the target distribution $\\pi$ verifies an LSI [29]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Constrained sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our goal, however, is not to sample from $\\pi$ itself, but from a distribution close to $\\pi$ that satisfies a set of statistical requirements. Explicitly, we wish to sample from a distribution $\\mu^{\\star}$ that solves (PI). Since (PI) constrains the distribution $\\mu$ rather than its samples $x$ , it can accommodate more general requirements than the support constraints typically considered in constrained sampling (e.g., [12\u201318]). Next, we illustrate the wide range of practical problems that can be formulated as (PI). These examples are further explored in Section 4 and more details on their formulations are provided in Appendix E. ", "page_idx": 2}, {"type": "text", "text": "1. Sampling from convex sets: though we have stressed that (PI) accommodates other types of requirements, it can also be used to constrain the support of $\\pi$ , i.e., to sample from ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu^{\\star}\\in\\underset{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}{\\mathrm{argmin}}}&{\\mathrm{KL}(\\mu\\|\\pi)}\\\\ {\\mathrm{subject~to}}&{\\mathbb{P}_{x\\sim\\mu}[x\\in\\mathcal{C}]=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a closed convex set $\\mathcal{C}\\subset\\mathbb{R}^{d}$ . Indeed, let $\\mathcal{C}$ be the intersection of the 0-sublevel sets of convex functions $\\{s_{i}\\}_{i=1,\\ldots,I}$ . Such a description always exists (see Appendix E). Then, (PII) can be cast as (PI) using $g_{i}(x)\\,=\\,[s_{i}(x)]_{+}$ for $[z]_{+}=\\operatorname*{max}(0,z)$ . Notice that the $g_{i}$ are convex and that although they are not everywhere differentiable, $\\mathbb{I}(s_{i}(\\dot{x})>0)\\nabla s_{i}(x)$ is a subgradient of $g_{i}$ , where $\\mathbb{I}(\\mathcal{E})=1$ on the event $\\mathcal{E}$ and 0 otherwise. Observe that support constraints can also be imposed using projections, mirror/proximal maps, and barriers as in [12\u201318]. These methods, however, constrain the samples $x$ rather than their distribution $\\mu$ as in (PII). ", "page_idx": 2}, {"type": "text", "text": "2. Rate-constrained Bayesian models: rate constraints have garnered attention in ML due to their central role in fairness [20, 21]. Consider data pairs $(x,y)$ , where $x\\in\\mathscr{X}$ are features and $y\\in$ $\\{0,1\\}$ labels, and a protected (measurable) subgroup $\\mathcal G\\subset\\mathcal X$ . Let $\\pi$ be a Bayesian posterior of the parameters $\\theta$ of a model $q(\\cdot;\\theta)$ denoting the probability of a positive outcome (based, e.g., on a binomial model). We wish to enforce statistical parity, i.e., we wish the prevalence of positive outcomes within the protected group $\\mathcal{G}$ to be close to or higher than in the whole population. We cast this problem as (PI) by constraining the average probability of positive outcome as in ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P^{\\star}=\\underset{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}{\\mathrm{min}}}&{\\mathrm{KL}(\\mu\\|\\pi)}\\\\ {\\mathrm{subject~to}}&{\\mathbb{E}_{x,\\theta\\sim\\mu}\\big[q(x;\\theta)\\mid\\mathcal{G}\\big]\\geq\\mathbb{E}_{x,\\theta\\sim\\mu}\\big[q(x;\\theta)\\big]-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\delta>0$ denotes our tolerance [22]. Naturally, multiple protected groups can be accommodated by incorporating additional constraints. Hence, constrained sampling provides a natural way to encode fairness in Bayesian inference. ", "page_idx": 2}, {"type": "text", "text": "3. Counterfactual sampling: rather than imposing requirements on probabilistic models, constrained sampling can also be used to probe them by evaluating counterfactual statements. Indeed, let $\\pi$ denote a reference probabilistic model such that sampling from $\\pi$ yields realizations of the \u201creal world.\u201d Consider the counterfactual statement \u201chow would the world have been if $\\mathbb{E}[g(x)]\\leq0?^{\\ast}$ Constrained sampling not only gives realizations of this alternative world, but it also indicates its \u201ccompatibility\u201d with the reference model, namely the value $P^{\\star}$ of (PI). ", "page_idx": 3}, {"type": "text", "text": "More concretely, consider a Bayesian stock market model. Here, $\\pi$ is a posterior model for the (log-)returns of $I$ assets, e.g., distributed as Gaussians $\\mathcal{N}(\\rho,\\Sigma)$ . Here, the vector $\\rho$ describes the mean return of each stock and $\\Sigma$ their covariance. We can investigate what the market would look like if, e.g., the mean and variance of each stocks were to change by solving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P^{\\star}=\\underset{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}{\\mathrm{min}}}&{\\mathrm{KL}(\\mu\\|\\pi)}\\\\ {\\mathrm{subject~to}}&{\\mathbb{E}_{(\\rho,\\Sigma)\\sim\\mu}\\big[\\rho_{i}\\big]=\\bar{\\rho}_{i},\\;\\;\\;i=1,\\ldots,I}\\\\ &{\\mathbb{E}_{(\\rho,\\Sigma)\\sim\\mu}\\big[\\Sigma_{i i}\\big]\\leq\\bar{\\sigma}_{i}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Due to correlations in the market, certain choices of $\\bar{\\rho}_{i}$ or $\\bar{\\sigma}_{i}^{2}$ may be more \u201cunrealistic\u201d than others. Additionally, it could be that some of these conditions are vacuous conditioned on the others. As we show next, our approach to tackling (PI) effectively isolates the contribution of each requirement in the solution $\\mu^{\\star}$ , thus enabling us to identify which are (conditionally) vacuous and which are most at odds with the reference model $\\pi$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Lagrangian duality and dual ascent algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Although directly sampling from $\\mu^{\\star}$ does not appear straightforward, it admits a convenient characterization based on convex duality that is amenable to be sampled using the LMC algorithm (3). Indeed, let $g:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}^{I}}$ and $h:\\mathbb{R}^{d}\\stackrel{}{\\rightarrow}\\mathbb{R}^{J}$ be vector-valued functions collecting the constraint functions $g_{i}$ and $h_{j}$ respectively. The Lagrangian of (PI) is then defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\mu,\\lambda,\\nu)\\triangleq\\mathrm{KL}(\\mu\\|\\pi)+\\lambda^{\\top}\\,\\mathbb{E}_{\\mu}[g]+\\nu^{\\top}\\,\\mathbb{E}_{\\mu}[h]=\\mathrm{KL}(\\mu\\|\\mu_{\\lambda\\nu})+\\log\\left(\\frac{Z}{Z_{\\lambda\\nu}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $\\lambda\\in\\mathbb{R}_{+}^{I}$ and $\\nu\\in\\mathbb{R}^{J}$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{\\lambda\\nu}(x)={\\frac{e^{-U(x,\\lambda,\\nu)}}{Z_{\\lambda\\nu}}}\\quad{\\mathrm{for}}\\quad U(x,\\lambda,\\nu)=f(x)+\\lambda^{\\top}g(x)+\\nu^{\\top}h(x)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and a normalization constant $Z_{\\lambda\\nu}$ . Notice that $P^{\\star}=\\operatorname*{min}_{\\mu}\\operatorname*{max}_{\\lambda\\geq0,\\,\\nu}L(\\mu,\\lambda,\\nu)$ , which is why (PI) is referred to as the primal problem. ", "page_idx": 3}, {"type": "text", "text": "To obtain the dual problem of (PI), define the dual function ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(\\lambda,\\nu)\\triangleq\\operatorname*{min}_{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}\\,L(\\mu,\\lambda,\\nu).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notice from (4) that the minimum in (6) is achieved for $\\mu_{\\lambda\\nu}$ from (5), the Lagrangian minimizer, so that $d(\\lambda,\\nu)=\\log(Z/Z_{\\lambda\\nu})$ . The solution of (6) is therefore a tilted version of $\\pi$ , whose tilt is controlled by the dual variables $(\\lambda,\\nu)$ . Since (6) is a relaxation of (PI), it yields a lower bound on the primal value, i.e., $d(\\lambda,\\nu)\\leq P^{\\star}$ for all $(\\lambda,\\nu)\\in\\mathbb{R}_{+}^{I}\\times\\mathbb{R}^{J}$ . The dual problem seeks the tilts $(\\lambda^{\\star},\\nu^{\\star})$ that yield the best lower bound, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nD^{\\star}\\triangleq\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{+}^{I},\\,\\nu\\in\\mathbb{R}^{J}}\\;d(\\lambda,\\nu).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The set $\\Phi^{\\star}=\\operatorname*{argmax}_{\\lambda\\geq0,\\,\\nu}\\;d(\\lambda,\\nu)$ of solutions of (DI) is called the set of Lagrange multipliers.   \nNote from (6) that (DI) depends on the distributions $\\mu$ and $\\pi$ through its objective $d$ . ", "page_idx": 3}, {"type": "text", "text": "The dual problem (DI) has several advantageous properties. Indeed, while the primal problem (PI) is an infinite dimensional, smooth optimization problem in probability space, the dual problem (DI) is a finite dimensional, non-smooth optimization problem in Euclidean space. What is more, it is a concave problem regardless of the functions $f,g,h$ , since the dual function (6) is the minimum of a set of affine functions in $(\\lambda,\\nu)$ [30, Prop. 4.1.1]. These properties are all the more attractive given that, under mild conditions stated below, (DI) can be used to solve (PI). ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1. There exists $\\mu^{\\dagger}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ with $\\mathrm{KL}(\\mu^{\\dagger}\\|\\pi)\\leq C<\\infty$ such that $\\mathbb{E}_{\\mu^{\\dagger}}[g_{i}]\\leq-\\delta<0$ and $\\mathbb{E}_{\\mu^{\\dagger}}[h_{j}]=0$ for all $i,j$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.2. Under Assumption 2.1, the following holds: ", "page_idx": 4}, {"type": "text", "text": "$(i i)$ there exists a finite pair $(\\lambda^{\\star},\\nu^{\\star})\\in\\Phi^{\\star}$ ; ", "page_idx": 4}, {"type": "text", "text": "(iii) for any solution $\\mu^{\\star}$ of (PI) and $(\\lambda^{\\star},\\nu^{\\star})$ of (DI), it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\mu^{\\star},\\lambda,\\nu)\\leq L(\\mu^{\\star},\\lambda^{\\star},\\nu^{\\star})\\leq L(\\mu,\\lambda^{\\star},\\nu^{\\star}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$(i\\nu)$ the solution of (PI) is $\\mu^{\\star}=\\mu_{\\lambda^{\\star}\\nu^{\\star}}$ for $(\\lambda^{\\star},\\nu^{\\star})\\in\\Phi^{\\star}$ ; ", "page_idx": 4}, {"type": "text", "text": "(v) consider the perturbation of (PI) ", "page_idx": 4}, {"type": "equation", "text": "$$\nP^{\\star}(u,v)\\triangleq\\operatorname*{min}_{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}\\ \\mathrm{KL}(\\mu\\|\\pi)\\ \\ \\mathrm{subject}\\ \\mathrm{to}\\ \\ \\mathbb{E}_{x\\sim\\mu}\\big[g_{i}(x)\\big]\\leq u_{i},\\ \\mathbb{E}_{x\\sim\\mu}\\big[h_{j}(x)\\big]=v_{j}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. In finite dimensional settings, (i)\u2013(v) are well-known duality results (see, e.g., [30]). While they also hold for infinite dimensional optimization problems, their proofs are slightly more \u201cscattered.\u201d We collect their reference below. The objective of (PI) is a convex function and its constraints are linear functions of $\\mu$ . Hence, (PI) is a convex program. Under Slater\u2019s condition (Assumption 2.1), it is (i) strongly dual $(P^{\\star}=D^{\\star})$ ) and (ii) there exists at least one solution $(\\lambda^{\\star},\\nu^{\\star})$ of (DI) (see [31, Sec. 8.6, Thm. 1] or [32, Cor. 4.1]). This implies (iii) the existence of the saddle-point (7) [33, Prop. 2.156], (iv) that $\\mu^{\\star}\\in\\operatorname*{argmin}_{\\mu}L(\\mu,\\lambda^{\\star},\\bar{\\nu}^{\\star})=\\{\\mu_{\\lambda^{\\star}\\nu^{\\star}}\\}$ , since the KL divergence is strongly convex and its minimizer is unique [34, Thm. 7.3.7], and (v) that $(\\lambda^{\\star},\\nu^{\\star})$ are subgradients of the perturbation function $P^{\\star}(u,v)$ [33, Prop. 4.27]. \u25a0 ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.2 shows that given solutions $(\\lambda^{\\star},\\nu^{\\star})$ of (DI), the constrained sampling problem (PI) reduces to sampling from $\\mu_{\\lambda^{\\star}\\nu^{\\star}}\\,\\propto\\,e^{-U(\\cdot,\\lambda^{\\star},\\nu^{\\star})}$ (see Appendix F for an explicit example of this result). It is important to note that this results only relies on the KL divergence being (strongly) convex in the standard $L^{2}$ geometry, i.e., along mixtures of the form $t\\mu_{0}+\\bar{(1-t)}\\mu_{1}$ for $t\\in[0,1]$ . This does not imply that it is (geodesically) convex in the Wasserstein sense [35, Section 9.1.2]. This would require $U$ in (5) to be convex for all $\\lambda\\geq0$ and $\\nu\\in\\mathbb{R}^{J}$ , i.e., for $f,g$ to be convex and $h$ to be linear. ", "page_idx": 4}, {"type": "text", "text": "Hence, Proposition 2.2 reduces the constrained sampling problem (PI) to that of finding the Lagrange multipliers $(\\lambda^{\\star},\\nu^{\\star})$ . Despite their finite dimensionality, however, computing these parameters is intricate. Indeed, since (DI) is a concave program, we could obtain $(\\lambda^{\\star},\\nu^{\\star})\\in\\Phi^{\\star}$ using ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{k+1}=\\left[\\lambda_{k}+\\eta_{k}\\,\\mathbb{E}_{\\mu_{\\lambda_{k}\\nu_{k}}}[g]\\right]_{+}\\quad\\mathrm{and}\\quad\\nu_{k+1}=\\nu_{k}+\\eta_{k}\\,\\mathbb{E}_{\\mu_{\\lambda_{k}\\nu_{k}}}[h],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for $\\eta_{k}>0$ , where we used the fact that $\\mathbb{E}_{\\mu_{\\lambda\\nu}}[g]$ and $\\mathbb{E}_{\\mu_{\\lambda\\nu}}[h]$ are (sub)gradients of the dual function (6) at $(\\lambda,\\nu)$ [36, Thm. 2.87]. This procedure is known in optimization and game theory as dual ascent or best response [37, 38]. Notice, however, that (8) is not a practical algorithm as it requires explicit integration with respect to the intractable distribution $\\mu_{\\lambda\\nu}$ from (5). ", "page_idx": 4}, {"type": "text", "text": "This issue was partially addressed in [24] (in continuous time and without equality constraints, i.e., $J=0$ ) by replacing the Lagrangian minimizer $\\mu_{\\lambda_{k}\\nu_{k}}$ in (8) by the distribution of LMC samples, as in ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{k+1}=x_{k}-\\gamma_{k}\\nabla U(x_{k},\\lambda_{k})+\\sqrt{2\\gamma_{k}}\\beta_{k},\\ \\ \\beta_{k}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{N}(0,\\mathrm{I}_{d})}\\\\ &{\\lambda_{k+1}=\\Big[\\lambda_{k}+\\eta_{k}\\,\\mathbb{E}_{\\mu_{k}}[g]\\Big]_{+},\\ \\ \\mu_{k}=\\mathrm{Law}(x_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that since $J=0$ , we omit the argument $\\nu$ of $U$ for clarity. Nevertheless, the updates in (9) still require an explicit integration. While it is now possible to sample from $\\mu_{k}$ (namely, using the $x_{k}$ ), empirical approximations of $\\mathbb{E}_{\\mu_{k}[g]}$ may not only require an exponential (in the dimension $d$ ) number of samples (e.g., [39, Thm. 1.2]), but it introduces errors that are not taken into account in the analysis of [24]. In the sequel, we address these drawbacks by replacing these dual ascent algorithms by a saddle-point one. ", "page_idx": 4}, {"type": "image", "img_path": "o6Hk6vld20/tmp/962b5b28b65e3dfe81bec8b1a656ddc9a171068209d7b3de540368cea080f8dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3 Primal-dual Langevin Monte Carlo ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Consider the GDA dynamics for the saddle-point problem (DI) in Wasserstein space. Explicitly, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mu(t)}{\\partial t}=\\nabla\\cdot\\left[\\mu(t)\\nabla_{{W_{2}}}L\\big(\\mu(t),\\lambda(t),\\nu(t)\\big)\\right]}\\\\ &{\\frac{\\partial\\lambda_{i}(t)}{\\partial t}=\\left[\\nabla_{\\lambda_{i}}L\\big(\\mu(t),\\lambda(t),\\nu(t)\\big)\\right]_{\\lambda_{i}(t),+}}\\\\ &{\\frac{\\partial\\nu_{j}(t)}{\\partial t}=\\nabla_{\\nu_{j}}L\\big(\\mu(t),\\lambda(t),\\nu(t)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for the Lagrangian $L$ defined in (4), where $[z]_{\\lambda,+}\\,=\\,z$ for $\\lambda>0$ and $[z]_{\\lambda,+}\\,=\\,\\operatorname*{max}(a,0)$ otherwise [40, Sec. 2.2]. Observe that $\\nabla_{\\lambda_{i}}L(\\mu,\\lambda,\\nu)\\,=\\,\\mathbb{E}_{\\mu}[g_{i}]$ and $\\nabla_{\\nu_{j}}L(\\mu,\\lambda,\\nu)\\,=\\,\\mathbb{E}_{\\mu}[h_{j}]$ . Hence, the algorithm from [24] described in (9) involves a deterministic implementation of (10b) that fully integrates over $\\mu(t)$ . In contrast, we consider a stochastic, single-particle implementation of (10) that leads to the practical procedure in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Explicitly, we also use an Euler-Maruyama time-discretization of the Langevin diffusion associated to (10a) (step 3), but replace the expectations in (10b)\u2013(10c) by single-sample approximations (steps $4-$ 5). Algorithm 1 can therefore be seen as a particle implementation of the deterministic Wasserstein GDA algorithm (10). As such, it resembles a primal-dual counterpart of the LMC algorithm in (3), which is why we dub it primal-dual LMC (PD-LMC). Alternatively, Algorithm 1 can be interpreted as a stochastic approximation of the dual ascent method in (9). This suggests that the gradient approximations in steps 4\u20135 could be improved using mini-batches, which is in fact how [24] approximates the expectation in (9). Our theoretical analysis and experiments show that these mini-batches are neither necessary nor always worth the additional computational cost (see Section 3.1 and Section 4). Note that the \u201cstochastic approximations\u201d in Algorithm 1 refer to the dual updates (steps 4\u20135) rather than the LMC update (step 3) as in stochastic gradient Langevin [41]. Though these methods could be combined, it is beyond the scope of this work. ", "page_idx": 5}, {"type": "text", "text": "The remainder of this section is dedicated to analyzing the convergence properties of PD-LMC for both stochastic dual gradients (as in Algorithm 1) and exact dual gradient (as in (9)). For the latter, we obtain guarantees for the discrete implementation (9) under weaker assumptions than the continuous-time analysis of [24]. We consider strongly log-concave target distributions in Section 3.1 and those satisfying an LSI in Section 3.2. ", "page_idx": 5}, {"type": "text", "text": "3.1 PD-LMC with (strongly) convex potentials ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As opposed to the traditional LMC algorithm (3) or the deterministic updates in (9), Algorithm 1 involves three coupled random variables, namely, $(\\boldsymbol{x}_{k},\\lambda_{k},\\nu_{k})$ . Hence, the LMC update (step 3) is based on a stochastic potential $U$ and the distribution $\\mu_{k}$ of $x_{k}$ is now a random measure. Our analysis sidesteps this obstacle by using techniques from stochastic optimization. We also leverage techniques from primal-dual algorithms in the Wasserstein space, in the spirit of works such as [7, 8, 42] that studied the LMC (3) or alternative time-discretizations of gradient flows of the KL divergence as splitting schemes. ", "page_idx": 5}, {"type": "text", "text": "First, define the potential energy $\\mathcal{E}$ and the (negative) entropy $\\mathcal{H}$ for $(\\mu,\\lambda,\\nu)\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})\\times\\mathbb{R}_{+}^{I}\\times\\mathbb{R}^{J}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal{E}}(\\mu,\\lambda,\\nu)=\\int U(x,\\lambda,\\nu)d\\mu(x)\\quad{\\mathrm{and}}\\quad{\\mathcal{H}}(\\mu)=\\int\\log(\\mu)d\\mu,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $U$ as in (5). Notice from (4) that $L(\\mu,\\lambda,\\nu)=\\mathcal{E}(\\mu,\\lambda,\\nu)+\\mathcal{H}(\\mu)+\\log(Z)-\\log(Z_{\\lambda\\nu}).$ , where we used the KL divergence decomposition in (1). To proceed, consider the following assumptions: ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1. The potential energy $\\mathcal{E}(\\mu,\\lambda,\\nu)$ in (11) is $m$ -strongly convex with respect to $\\mu$ along Wasserstein-2 geodesics for $m\\geq0$ and all $(\\lambda,\\nu)\\in\\mathbb{R}_{+}^{I}\\times\\mathbb{R}^{J}$ . Explicitly, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\mu,\\lambda,\\nu)\\geq\\mathcal{E}(\\mu_{0},\\lambda,\\nu)+\\int\\langle\\nabla_{W_{2}}\\mathcal{E}(\\mu_{0},\\lambda,\\nu),x-y\\rangle d s(x,y)+\\frac{m}{2}W_{2}^{2}(\\mu,\\mu_{0}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $s$ is an optimal coupling achieving $W_{2}^{2}(\\mu,\\mu_{0})$ (see Appendix B). ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.2. The gradients and variances of $f,g,h$ are bounded along iterations $\\{\\mu_{k}\\}_{k\\ge0}$ , where $\\mu_{k}$ is the distribution of $x_{k}$ , i.e., there exists $G^{2}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\|\\nabla f\\|_{L^{2}(\\mu_{k})}^{2},\\|\\nabla g_{i}\\|_{L^{2}(\\mu_{k})}^{2},\\|\\nabla h_{j}\\|_{L^{2}(\\mu_{k})}^{2}\\right)\\le G^{2}\\,\\mathrm{~and~}\\,\\operatorname*{max}\\left(\\mathbb{E}_{\\mu_{k}}[\\|g\\|^{2}],\\mathbb{E}_{\\mu_{k}}[\\|h\\|^{2}]\\right)\\le G^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1 holds with $m=0$ if $f,g$ are convex and $h$ is linear. If $f$ is additionally strongly convex, then it holds with $m>0$ [26, Prop. 9.3.2]. Assumption 3.2 is typical in (stochastic) nonsmooth optimization analyses (see, e.g., [36, 43, 44]). Notice, however, that gradients are only required to be bounded along trajectories of Algorithm 1, a crucial distinction in the case of strongly convex functions whose gradients can only be bounded locally. Assumption 3.2 can be satisfied under mild conditions on $f,g,h$ , such as local Lipschitz continuity or linear growth. ", "page_idx": 6}, {"type": "text", "text": "The following theorem provides the first convergence analysis of the discrete-time PD-LMC. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3. Denote by $\\mu_{k}$ the distribution of $x_{k}$ in Algorithm 1. Under Assumptions 2.1, 3.1, and 3.2, there exists $R_{0}^{2}$ such that, for $\\eta_{k}\\leq\\eta,$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\left[\\mathrm{KL}(\\mu_{k}\\parallel\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\mu_{k},\\mu^{\\star})\\right]\\leq3\\eta G^{2}+\\frac{R_{0}^{2}}{\\eta K}+\\frac{\\eta G^{2}}{K}\\sum_{k=1}^{K}\\big(\\mathbb{E}[\\|\\lambda_{k}\\|^{2}]+\\mathbb{E}[\\|\\nu_{k}\\|^{2}]\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\bar{\\eta}_{k}\\Big[\\operatorname{KL}(\\mu_{k}\\parallel\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\mu_{k},\\mu^{\\star})\\Big]\\leq\\frac{R_{0}G(1+\\log(K))}{\\sqrt{K}}\\Big(3+\\operatorname*{max}_{k}\\,\\big\\{\\operatorname{\\mathbb{E}}[\\|\\lambda_{k}\\|^{2}]+\\operatorname{\\mathbb{E}}[\\|\\nu_{k}\\|^{2}]\\big\\}\\Big)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Additionally, there exists a sequence of step sizes $\\eta_{k}>0$ such that $W_{2}^{2}(\\mu_{k},\\mu^{\\star})\\leq R_{0}^{2}$ and $\\mathbb{E}[\\left\\Vert\\lambda_{k}\\right\\Vert^{2}]+$ $\\mathbb{E}[\\left\\lVert\\nu_{k}\\right\\rVert^{2}]<\\infty$ for all $k$ . The same results hold (without expectations) when using exact dual gradients, i.e., if the updates in steps 4\u20135 are replaced by $\\lambda_{k+1}=\\left[\\lambda_{k}\\!+\\!\\eta_{k}\\,\\mathbb{E}_{\\mu_{k}}[g]\\right]_{+}$ and $\\nu_{k+1}=\\nu_{k}\\!+\\!\\eta_{k}\\,\\mathbb{E}_{\\mu_{k}}[h]$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3, whose proof is deferred to Appendix C, implies rates similar to those for GDA schemes in finite-dimensional Euclidean optimization (see, e.g., [43]). To recover those rates, however, we must bound the magnitudes of $\\lambda_{k},\\nu_{k}$ . In [43], this is done by bounding the iterates in the algorithm itself, i.e., by projecting them onto the set $\\mathcal{D}_{r}=\\{(\\lambda,\\nu)\\in\\mathring{\\mathbb{R}}_{+}^{I}\\times\\mathbb{R}^{J}\\,\\,\\big|\\,\\operatorname*{max}(\\|\\lambda\\|^{2}\\,,\\|\\nu\\|^{2})\\leq r\\}$ and choosing $r$ such that $\\Phi^{\\star}\\subseteq\\mathcal{D}_{r}$ (Proposition 2.2(ii) ensures this is possible). We then incur a bias on the order of $\\eta$ in (12) that vanishes in the decreasing step size setting of (13). Though convenient, this is not necessary since there exists a sequence of step sizes such that both $\\mathbb{E}[\\left\\Vert\\lambda_{k}\\right\\Vert^{2}]$ and $\\mathbb{E}[\\left\\|\\nu_{k}\\right\\|^{2}]$ are bounded for all $k\\geq0$ . In the interest of generality, Theorem 3.3 holds without these hypotheses. It is worth noting that though faster rates and last iterates guarantees can be obtained for Euclidean saddle-point problems, they rely on more complex schemes than the GDA in Algorithm 1 involving acceleration or proximal methods [45\u201348]. ", "page_idx": 6}, {"type": "text", "text": "The results in Theorem 3.3 are stated for the stochastic scheme in Algorithm 1. However, Theorem 3.3 yields the same rates (without expectations) for exact dual gradients, i.e., for the dual ascent scheme (9). In this case, the second condition in Assumption 3.2 simplifies to $\\operatorname*{max}_{k}(\\|\\mathbb E_{\\mu_{k}}[g]\\|^{2},\\|\\mathbb E_{\\mu_{k}}[h]\\|^{2})\\leq G^{2}$ . Not only are these milder assumptions than [24, Eq. (16)], but the guarantees hold for discrete- rather than continuous-time dynamics. Finally, (12)\u2013(13) imply convergence with respect to the KL divergence for convex potentials $(m\\,=\\,0)$ ) with stronger guarantees in Wasserstein metric for strongly convex ones $(m>0)$ ). ", "page_idx": 6}, {"type": "text", "text": "The convergence rates for distributions $\\mu_{k}$ from Theorem 3.3 also imply convergence rates for empirical averages across iterates $x_{k}$ of Algorithm 1. This corollary is obtained by combining (12)\u2013 (13) with the following proposition. By taking $\\varphi$ to be the constraint functions $g$ or $h$ from (PI) yields feasibility guarantees for PD-LMC . ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 (Stochastic) dual LMC ", "page_idx": 7}, {"type": "text", "text": "1: Inputs: $N_{k}^{0}>0$ (burn-in), $\\gamma_{k},\\eta_{k}>0$ (step sizes), and $\\lambda_{0}=0$ .   \n2: for $k=0,\\ldots,K-1$   \n3: x0 \u223c\u00b50   \n4: $x_{n+1}\\'=x_{n}-\\gamma_{k}\\nabla_{x}U(x_{n},\\lambda_{k})+\\sqrt{2\\gamma_{k}}\\,\\beta_{n},\\;\\;\\beta_{n}\\sim{\\mathcal N}(0,\\mathrm{I}_{d}),\\quad\\mathrm{for}\\;n=0,\\dots,N_{k}^{0}-1$   \n5: $\\lambda_{k+1}=\\left[\\lambda_{k}+\\eta_{k}g(x_{N_{k}^{0}})\\right]_{+}$ ", "page_idx": 7}, {"type": "text", "text": "6: end ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Proposition 3.4. Consider samples $x_{k}$ distributed according to $\\mu_{k}$ and $c_{k}\\geq0$ with $\\textstyle\\sum_{k=1}^{K}c_{k}=1$ such that $\\begin{array}{r}{\\sum_{k=1}^{K}c_{k}\\Big[\\operatorname{KL}(\\mu_{k}\\parallel\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\mu_{k},\\mu^{\\star})\\Big]\\le\\Delta_{K}}\\end{array}$ for $m\\geq0$ . Then, it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[\\sum_{k=1}^{K}c_{k}\\varphi(x_{k})\\right]-\\mathbb{E}_{\\mu^{\\star}}[\\varphi]\\right|\\leq\\left\\{\\sqrt{\\sqrt{2\\Delta_{K}}},\\quad i f\\varphi\\,i s\\,b o u n d e d\\,b y\\,1,\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof. See Appendix C. ", "page_idx": 7}, {"type": "text", "text": "3.2 PD-LMC with LSI potentials ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we replace Assumption 3.1 on the convexity of the potential by an LSI common in the sampling literature. We consider only inequality constraints $(J=0)$ ) here and omit the function arguments $\\nu$ , since accounting for equality constraints requires significant additional assumptions. ", "page_idx": 7}, {"type": "text", "text": "Assumption 3.5. The distribution $\\mu_{\\lambda}$ satisfies the LSI for bounded $\\lambda$ , i.e., there exists $\\sigma>0$ such that $2\\sigma\\operatorname{KL}(\\zeta\\|\\mu_{\\lambda})\\leq\\|\\nabla\\log\\left(d\\zeta/d\\mu_{\\lambda}\\right)\\|_{L^{2}(\\zeta)}^{2}$ for all $\\zeta\\in\\mathcal{P}_{2}(\\ensuremath{\\mathbb{R}}^{d})$ . ", "page_idx": 7}, {"type": "text", "text": "The LSI in Assumption 3.5 is often used in the analysis of the standard LMC algorithm [29, 49]. It holds, e.g., when $f$ is strongly convex and $g$ is a (possibly non-convex) bounded function due to the Holley-Stroock perturbation theorem [50]. In fact, if $f$ is 1-strongly convex and $|g|$ is bounded by 1, then Assumption 3.5 holds for $\\sigma\\geq e^{-2\\lambda}$ (see, e.g., [51, Prop. 5.1.6] or [52, Thm 1.1]). The LSI is akin to the Polyak-\u0141ojasiewicz (PL) condition from Euclidean optimization [53], which supposes issues with GDA methods such as Algorithm 1. Indeed, it is not enough for the Lagrangian (4) to satisfy the PL condition in the primal variable to guarantee the convergence of GDA in Euclidean spaces. We must either modify Algorithm 1 using acceleration or proximal methods [47, 54\u201356] or impose the PL condition also on $\\lambda$ [57, 58]. Since the Lagrangian 4 is linear in $\\lambda$ , it is clear that Algorithm 1 will not suffice to provide theoretical guarantees in the LSI case. ", "page_idx": 7}, {"type": "text", "text": "We therefore consider the variant in Algorithm 2, where $N_{k}^{0}$ LMC iterations (step 3) are executed before updating the dual variables (step 4). This is akin to using different time-scales in continuoustime, a common technique for solving saddle-point problems [54, 58]. Since it resembles a dual ascent counterpart of the LMC algorithm (3), we refer to it as (stochastic) dual LMC (DLMC). As opposed to the dual ascent algorithm from [24] in (9), however, Algorithm 2 does not require any explicit evaluation of expected values. The following theorem provides an analysis of its convergence. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.6. Assume that the functions $f,g$ are $M$ -smooth, i.e., have $M$ -Lipschitz continuous gradients, satisfy Assumption 3.5, and that $\\bar{\\mathbb{E}_{\\mu}}[\\|g\\|^{2}]\\leq G^{2}$ for all $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Let $0<\\eta_{k}\\leq\\eta,$ , $0<\\epsilon\\le\\eta G^{2}<1,$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\gamma_{k}=\\gamma\\leq\\frac{\\sigma\\epsilon}{16d M^{2}},\\,\\,\\,\\,\\,\\,a n d\\,\\,\\,\\,\\,N_{k}^{0}\\geq\\frac{1}{\\gamma\\sigma}\\log\\left(\\frac{2\\,\\mathrm{KL}(\\mu_{0}\\|\\mu_{\\lambda_{k}})}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Under Assumption 2.1, there exists $B<\\infty$ such that the distributions $\\{\\mu_{k}\\}$ of the samples $\\left\\{x_{N_{k}^{0}}\\right\\}$ generated by Algorithm 2 satisfy ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathrm{KL}(\\mu_{k}\\|\\mu^{\\star})\\le\\epsilon+\\frac{\\eta G^{2}}{2}+\\frac{2I B^{2}}{\\eta K}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Recall from (PI) that $I$ is the number of inequality constraints. Additionally, $\\mathbb{E}[\\|\\boldsymbol{\\lambda}_{k}\\|_{1}]$ is bounded for all $k$ . ", "page_idx": 7}, {"type": "image", "img_path": "o6Hk6vld20/tmp/6fdcdce467882ef03b0eac4b064cb383750abd57680830e5e6fd482f1d31ad31.jpg", "img_caption": ["Figure 1: Sampling from a 1D truncated Gaussian (ground truth displayed as dashed lines). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "o6Hk6vld20/tmp/2b025649ef2536b2c829343624b69a040ba9e16d4da3a9161f8f1e99add369c5.jpg", "img_caption": ["Figure 2: Sampling from a 2D truncated Gaussian (true mean in red and sample mean in orange). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 3.6, whose proof is deferred to Appendix D, provides similar guarantees as (approximate) subgradient methods in finite-dimensional optimization (see, e.g., [22, 56]). This is not surprising seen as $\\gamma_{k},N_{k}^{0}$ in Theorem 3.6 are chosen to ensure that step 4 yields a sample $x_{k}\\sim\\bar{\\mu}_{k}$ such that $\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu_{\\lambda_{k}})\\stackrel{\\cdot\\cdot}{\\leq}\\epsilon$ using [29, Theorem 1]. At this point, $g(x_{N_{k}^{0}})$ in step 5 is an approximate, stochastic subgradient of the dual function (6). Though it may appear from (12) and (14) that Algorithms 1 and 2 have the same convergence rates, an informal computation shows that the latter evaluates on the order of $d\\kappa^{2}/\\eta$ as many gradient per iteration, where $\\kappa=M/\\sigma$ . Note that we can once again apply Theorem 3.6 to derive ergodic average and feasibility guarantees for Algorithm 2. ", "page_idx": 8}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now return to the applications described in Section 2.2 to showcase the behavior of PD-LMC. We defer implementation details and additional results to Appendix E. Code for these examples is publicly available at https://www.github.com/lfochamon/pdlmc. ", "page_idx": 8}, {"type": "text", "text": "1. Sampling from convex sets. We cast the problem of sampling from a Gaussian distribution $\\mathcal{N}(0,1)$ truncated to $\\mathcal{C}=[1,3]$ as (PI) by taking $f(x)=x^{2}/2$ and $\\bar{g(x)\\,=\\,}[(x-1)(x-3)]_{+}$ (see Section 2.2). Fig. 1 shows histograms for the samples obtained using PD-LMC, the projected LMC (Proj. LMC) from [13], and the mirror LMC from [59], all with the same step size. Both Proj. LMC and Mirror LMC generate an excess of samples close to the boundary (between 1.5 and 3 times more samples than expected). This leads to an underestimation of the mean (Proj. LMC: 1.488 / Mirror LMC: 1.470 vs. true mean: 1.510). In contrast, PD-LMC provides a more accurate estimate (1.508). Yet, since it constrains the distribution $\\mu$ rather than its samples, it is not an interior-point method and can produce samples outside of $\\mathcal{C}$ . Theorems 3.3\u20133.6 show that this becomes less frequent as the algorithm progresses (in Fig. 1, only $2\\%$ of the samples are not in $\\mathcal{C}$ ). This occurs even without using minibatches in steps 4\u20135 of Algorithm 1 as in [24]. In fact, our experiments show that mini-batches increase the computational complexity with no performance benefit (Appendix E). These issues are exacerbated in more challenging problems, such as sampling from a two-dimensional standard Gaussian centered at [2, 2] restricted to an unit $\\ell_{2}$ -norm ball (Fig. 2). In this case, Proj. LMC places almost $25\\%$ of its samples on the boundary (where only $0.14\\%$ of samples should be), while PD-LMC only places $1.8\\%$ of its samples outside of the support. Mirror LMC provides a better mean estimation in this setting, although a bit more asymmetric than PD-LMC [Mirror LMC: (0.312, 0.418) vs. PD-LMC: (0.446, 0.444) vs. true mean: (0.368, 0.368)]. ", "page_idx": 8}, {"type": "text", "text": "2. Rate-constrained Bayesian models. Here, we consider $\\pi$ to be the posterior of a Bayesian logistic regression model for the Adult dataset from [60], where the goal is to predict whether an individual makes more than $\\mathbb{S}50\\mathbf{k}$ based on socioeconomic information (details on data pre-processing can be found in [61]). We consider a standard Gaussian prior on the parameters $\\boldsymbol{\\theta}\\in\\mathbb{R}^{\\dot{d}+1}$ of the model, where $d$ is the number of features. Using the LMC algorithm to sample from the posterior (i.e., no constraints), we find that while the average probability of positive predictions is $19.1\\%$ over the whole test set, it is $26.2\\%$ among males and $5\\bar{\\%}$ among females (\u201cUnconstrained\u201d in Fig. 3). To overcome this disparity, we take gender to be the protected class in (PIII), constraining both $\\mathcal{G}_{\\mathrm{male}}$ and $\\mathcal{G}_{\\mathrm{female}}$ with $\\delta=0.01$ . Using PD-LMC, we obtain a Bayesian model that leads to an average probability of positive outcomes of $18.1\\%$ and $15.1\\%$ for males and females respectively. In fact, we now observe a substantial overlap of the distributions of positive predictions across genders for the constrained posterior $\\mu^{\\star}$ (\u201cConstrained $\\overset{\\prime}{\\delta}=0.01)$ \u201d in Fig. 3). This substantial reduction of prediction disparities comes at only a minor decline in accuracy (unconstrained: $84\\%$ vs constrained: $82\\%$ ). ", "page_idx": 8}, {"type": "image", "img_path": "o6Hk6vld20/tmp/7999aceea83295c7d0251aed3a123e7e8a2cdb43bda96e4e6395da523158ad9d.jpg", "img_caption": ["Figure 3: Distribution of the probability of pre- Figure 4: Counterfactual sampling of the stock dicting $>50\\mathrm{k}$ under the Bayesian logistic model market: dual variables. (black lines indicate the mean across genders). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "3. Counterfactual sampling. Though the distribution of positive predictions changes considerably for both male and female individuals, the final dual variables $\\lambda_{\\mathrm{male}}=0$ and $\\lambda_{\\mathrm{female}}\\approx160)$ show that these changes are due uniquely to the female group [as per Prop. 2.2(iv)]. This implies that the reference model $\\pi$ is itself compatible with the requirement for the male group, but that reducing the disparity for females requires considerable deviations from it. By examining $\\lambda_{\\mathrm{female}}$ , we conclude without recalculating $\\mu^{\\star}$ that even small changes in the tolerance $\\delta$ for the female constraint would substantially change the distribution of outcomes [Prop. 2.2(v)]. This is confirmed by \u201cConstrained $\\langle\\delta=0.03\\rangle^{\\circ}$ \u201d in Fig. 3. Notice that this is only possible due to the primal-dual nature of PD-LMC. This type of counterfactual analysis is even more beneficial in the presence of multiple requirements. Indeed, let $\\pi$ be the posterior of a Bayesian model for the daily (log-)return of a set of assets (see Appendix E for more details). Using (PIV), we consider how the market would look like if the average (log-)return of each asset were to have been (exactly) $20\\%$ higher. Inspecting the dual variables (Fig. 4), we notice that this increased market return is essentially driven by two stocks: NVDA and LLY $(\\nu<0)$ ). In fact, the reference model $\\pi$ would be consistent with an even higher increase for JNJ and GOOG $(\\nu>0)$ ). We confirm these observations by constraining only NVDA and LLY, which yields essentially the same (log-)return distribution for all assets. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We tackled the problem of sampling from a target distribution while satisfying a set of statistical constraints. Based on a GDA method in Wasserstein space, we put forward a fully stochastic, discretetime primal-dual LMC algorithm (PD-LMC) that precludes any explicit integration in its updates. We analyze the behavior of PD-LMC for (strongly) convex and log-Sobolev potentials, proving that the distribution of its samples converges to the optimal constrained distribution. We illustrated the use of PD-LMC for different constrained sampling applications. Future work include strengthening the convergence results to almost sure guarantees and improving the rates obtained using proximal and extra gradient methods, particularly in the LSI setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of L.F.O. Chamon is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy (EXC 2075-390740016). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. F. Faulkner and S. Livingstone, \u201cSampling algorithms in statistical physics: a guide for statistics and machine learning,\u201d arXiv preprint arXiv:2208.04751, 2022.   \n[2] R. van de Schoot, S. Depaoli, R. King, B. Kramer, K. M\u00e4rtens, M. G. Tadesse, M. Vannucci, A. Gelman, D. Veen, J. Willemsen et al., \u201cBayesian statistics and modelling,\u201d Nature Reviews Methods Primers, vol. 1, no. 1, p. 1, 2021.   \n[3] Y. Song and S. Ermon, \u201cGenerative modeling by estimating gradients of the data distribution,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[4] G. O. Roberts and J. S. Rosenthal, \u201cGeneral state space Markov chains and MCMC algorithms,\u201d Probability Surveys, vol. 1, 2004.   \n[5] C. Robert and G. Casella, Monte Carlo statistical methods. Springer Verlag, 2004.   \n[6] G. O. Roberts and R. L. Tweedie, \u201cExponential convergence of Langevin distributions and their discrete approximations,\u201d Bernoulli, pp. 341\u2013363, 1996.   \n[7] A. Wibisono, \u201cSampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem,\u201d in Conference on Learning Theory. PMLR, 2018, pp. 2093\u20133027.   \n[8] A. Durmus, S. Majewski, and B. Miasojedow, \u201cAnalysis of Langevin Monte Carlo via convex optimization,\u201d The Journal of Machine Learning Research, vol. 20, no. 1, pp. 2666\u20132711, 2019.   \n[9] Y. Wang and W. Li, \u201cAccelerated information gradient flow,\u201d J. Sci. Comput., vol. 90, no. 1, 2022.   \n[10] L. Lang, W.-s. Chen, B. R. Bakshi, P. K. Goel, and S. Ungarala, \u201cBayesian estimation via sequential Monte Carlo sampling\u2014Constrained dynamic systems,\u201d Automatica, vol. 43, no. 9, pp. 1615\u20131622, 2007.   \n[11] Y. Li and S. K. Ghosh, \u201cEfficient sampling methods for truncated multivariate normal and student-t distributions subject to linear inequality constraints,\u201d Journal of Statistical Theory and Practice, vol. 9, pp. 712\u2013732, 2015.   \n[12] Y.-P. Hsieh, A. Kavis, P. Rolland, and V. Cevher, \u201cMirrored Langevin dynamics,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.   \n[13] S. Bubeck, R. Eldan, and J. Lehec, \u201cSampling from a log-concave distribution with projected Langevin Monte Carlo,\u201d Discrete & Computational Geometry, vol. 59, no. 4, pp. 757\u2013783, 2018.   \n[14] A. Salim and P. Richtarik, \u201cPrimal dual interpretation of the proximal stochastic gradient Langevin algorithm,\u201d in Advances in Neural Information Processing Systems, 2020, pp. 3786\u2013 3796.   \n[15] K. Ahn and S. Chewi, \u201cEfficient constrained sampling via the mirror-Langevin algorithm,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 28 405\u201328 418, 2021.   \n[16] Y. Kook, Y. Lee, R. Shen, and S. Vempala, \u201cSampling with Riemannian Hamiltonian Monte Carlo in a constrained space,\u201d in Advances in Neural Information Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.   \n[17] L. Sharrock, L. Mackey, and C. Nemeth, \u201cLearning rate free Bayesian inference in constrained domains,\u201d in Conference on Neural Information Processing Systems, 2023.   \n[18] M. Noble, V. De Bortoli, and A. Durmus, \u201cUnbiased constrained sampling with self-concordant barrier Hamiltonian Monte Carlo,\u201d in Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[19] A. M a\u02dbdry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d in International Conference on Learning Representations, 2018.   \n[20] M. Kearns, S. Neel, A. Roth, and Z. S. Wu, \u201cPreventing fairness Gerrymandering: Auditing and learning for subgroup fairness,\u201d in International Conference on Machine Learning, 2018, pp. 2564\u20132572.   \n[21] A. Cotter, H. Jiang, M. Gupta, S. Wang, T. Narayan, S. You, and K. Sridharan, \u201cOptimization with non-differentiable constraints with applications to fairness, recall, churn, and other goals,\u201d Journal of Machine Learning Research, vol. 20, no. 172, pp. 1\u201359, 2019.   \n[22] L. F. O. Chamon, S. Paternain, M. Calvo-Fullana, and A. Ribeiro, \u201cConstrained learning with non-convex losses,\u201d IEEE Trans. on Inf. Theory, vol. 69[3], pp. 1739\u20131760, 2023.   \n[23] M. G\u00fcrb\u00fczbalaban, Y. Hu, and L. Zhu, \u201cPenalized Langevin and Hamiltonian Monte Carlo Algorithms for Constrained Sampling,\u201d 2022.   \n[24] X. Liu, X. Tong, and Q. Liu, \u201cSampling with trusthworthy constraints: A variational gradient framework,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 23 557\u201323 568, 2021.   \n[25] R. Jordan, D. Kinderlehrer, and F. Otto, \u201cThe variational formulation of the Fokker\u2013Planck equation,\u201d SIAM Journal on Mathematical Analysis, vol. 29, no. 1, pp. 1\u201317, 1998.   \n[26] L. Ambrosio, N. Gigli, and G. Savar\u00e9, Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.   \n[27] A. S. Dalalyan and A. Karagulyan, \u201cUser-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient,\u201d Stochastic Processes and their Applications, vol. 129, no. 12, pp. 5278\u20135311, 2019.   \n[28] A. S. Dalalyan, A. Karagulyan, and L. Riou-Durand, \u201cBounding the error of discretized Langevin algorithms for non-strongly log-concave targets,\u201d Journal of Machine Learning Research, vol. 23, no. 235, pp. 1\u201338, 2022.   \n[29] S. Vempala and A. Wibisono, \u201cRapid convergence of the unadjusted Langevin algorithm: Isoperimetry suffices,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 8092\u20138104.   \n[30] D. P. Bertsekas, Convex Optimization Theory. Athena Scientific, 2009.   \n[31] D. G. Luenberger, Optimization by Vector Space Methods. Wiley, 1968.   \n[32] V. Jeyakumar and H. Wolkowicz, \u201cGeneralizations of slater\u2019s constraint qualification for infinite convex programs,\u201d Math. Program., vol. 57, no. 1\u20133, pp. 85\u2013101, 1992.   \n[33] J. F. Bonnans and A. Shapiro, Perturbation Analysis of Optimization Problems. Springer, 2000.   \n[34] A. J. Kurdila and M. Zabarankin, Convex Functional Analysis. Birkh\u00e4user Basel, 2005.   \n[35] C. Villani, Topics in optimal transportation. American Mathematical Soc., 2021, vol. 58.   \n[36] A. P. Ruszczyn\u00b4ski, Nonlinear Optimization. Princeton University Press, 2006.   \n[37] N. Nisan, T. Roughgarden, \u00c9va Tardos, and V. V. Vazirani, Eds., Algorithmic Game Theory. Cambridge University, 2007.   \n[38] D. P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series), 1st ed. Athena Scientific, 1996.   \n[39] B. Kloeckner, \u201cApproximation by finitely supported measures,\u201d ESAIM: Control, Optimisation and Calculus of Variations, vol. 18, no. 2, pp. 343\u2013359, 2012.   \n[40] A. Nagurney and D. Zhang, Projected Dynamical Systems and Variational Inequalities with Applications. Springer, 1996.   \n[41] M. Welling and Y. W. Teh, \u201cBayesian learning via stochastic gradient Langevin dynamics,\u201d in Proceedings of the 28th International Conference on International Conference on Machine Learning, ser. ICML\u201911. Madison, WI, USA: Omnipress, 2011, p. 681\u2013688.   \n[42] A. Salim, A. Korba, and G. Luise, \u201cThe Wasserstein proximal gradient algorithm,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 12 356\u201312 366, 2020.   \n[43] A. Nedic\u00b4 and A. Ozdaglar, \u201cApproximate primal solutions and rate analysis for dual subgradient methods,\u201d SIAM Journal on Optimization, vol. 19, no. 4, pp. 1757\u20131780, 2009.   \n[44] A. Cherukuri, E. Mallada, and J. Cort\u00e9s, \u201cAsymptotic convergence of constrained primal\u2013dual dynamics,\u201d Systems & Control Letters, vol. 87, pp. 10\u201315, 2016.   \n[45] A. Nemirovski, \u201cProx-method with rate of convergence o(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems,\u201d SIAM Journal on Optimization, vol. 15, no. 1, pp. 229\u2013251, 2004.   \n[46] Y. Nesterov, \u201cDual extrapolation and its applications to solving variational inequalities and related problems,\u201d Math. Program., vol. 109, no. 2\u20133, p. 319\u2013344, 2007.   \n[47] T. Lin, C. Jin, and M. I. Jordan, \u201cNear-optimal algorithms for minimax optimization,\u201d in Proceedings of Thirty Third Conference on Learning Theory, 2020, pp. 2738\u20132779.   \n[48] A. Mokhtari, A. Ozdaglar, and S. Pattathil, \u201cA unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach,\u201d in Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020, pp. 1497\u20131507.   \n[49] S. Chewi, M. A. Erdogdu, M. Li, R. Shen, and S. Zhang, \u201cAnalysis of Langevin Monte Carlo: from Poincar\u00e9 to Log-Sobolev,\u201d in Proceedings of Thirty Fifth Conference on Learning Theory, ser. Proceedings of Machine Learning Research, P.-L. Loh and M. Raginsky, Eds., vol. 178. PMLR, 2022, pp. 1\u20132.   \n[50] R. Holley and D. W. Stroock, \u201cLogarithmic Sobolev inequalities and stochastic Ising models,\u201d Journal of Statistical Physics, no. 5\u20136, 1986.   \n[51] D. Bakry, I. Gentil, M. Ledoux et al., Analysis and geometry of Markov diffusion operators. Springer, 2014, vol. 103.   \n[52] P. Cattiaux and A. Guillin, \u201cFunctional inequalities for perturbed measures with applications to log-concave measures and to some Bayesian problems,\u201d Bernoulli, vol. 28, no. 4, 2022.   \n[53] H. Karimi, J. Nutini, and M. Schmidt, \u201cLinear convergence of gradient and proximal-gradient methods under the Polyak-\u0141ojasiewicz condition,\u201d in Machine Learning and Knowledge Discovery in Databases, P. Frasconi, N. Landwehr, G. Manco, and J. Vreeken, Eds. Cham: Springer International Publishing, 2016, pp. 795\u2013811.   \n[54] J. Yang, X. Li, and N. He, \u201cNest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization,\u201d in Advances in Neural Information Processing Systems, 2022.   \n[55] M. Boroun, Z. Alizadeh, and A. Jalilzadeh, \u201cAccelerated primal-dual scheme for a class of stochastic nonconvex-concave saddle point problems,\u201d in American Control Conference, 2023, pp. 204\u2013209.   \n[56] M. Sanjabi, J. Ba, M. Razaviyayn, and J. D. Lee, \u201cOn the convergence and robustness of training gans with regularized optimal transport,\u201d in Advances in Neural Information Processing Systems, 2018.   \n[57] J. Yang, N. Kiyavash, and N. He, \u201cGlobal convergence and variance reduction for a class of nonconvex-nonconcave minimax problems,\u201d in Advances in Neural Information Processing Systems, 2020, pp. 1153\u20131165.   \n[58] T. Fiez, L. Ratliff, E. Mazumdar, E. Faulkner, and A. Narang, \u201cGlobal convergence to local minmax equilibrium in classes of nonconvex zero-sum games,\u201d in Advances in Neural Information Processing Systems, 2021, pp. 29 049\u201329 063.   \n[59] K. Ahn and S. Chewi, \u201cEfficient constrained sampling via the mirror-Langevin algorithm,\u201d in Advances in Neural Information Processing Systems, vol. 34, 2021, p. 26.   \n[60] D. Dua and C. Graff, \u201cUCI machine learning repository,\u201d 2017. [Online]. Available: http://archive.ics.uci.edu/ml   \n[61] L. F. O. Chamon and A. Ribeiro, \u201cProbably approximately correct constrained learning,\u201d in Advances in Neural Information Processing, 2020.   \n[62] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent Dirichlet allocation,\u201d J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, 2003.   \n[63] G. Celeux, M. E. Anbari, J.-M. Marin, and C. P. Robert, \u201cRegularization in Regression: Comparing Bayesian and Frequentist Methods in a Poorly Informative Situation,\u201d Bayesian Analysis, vol. 7, no. 2, pp. 477\u2013502, 2012.   \n[64] A. Lamperski, \u201cProjected stochastic gradient Langevin algorithms for constrained sampling and non-convex learning,\u201d in Conference on Learning Theory. PMLR, 2021, pp. 2891\u20132937.   \n[65] L. Li, Q. Liu, A. Korba, M. Yurochkin, and J. Solomon, \u201cSampling with mollified interaction energy descent,\u201d arXiv preprint arXiv:2210.13400, 2022.   \n[66] K. S. Zhang, G. Peyr\u00e9, J. Fadili, and M. Pereyra, \u201cWasserstein control of mirror Langevin Monte Carlo,\u201d in Conference on Learning Theory. PMLR, 2020, pp. 3814\u20133841.   \n[67] Q. Jiang, \u201cMirror Langevin Monte Carlo: the case under isoperimetry,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 715\u2013725, 2021.   \n[68] V. Srinivasan, A. Wibisono, and A. Wilson, \u201cFast sampling from constrained spaces using the metropolis-adjusted mirror Langevin algorithm,\u201d arXiv preprint arXiv:2312.08823, 2023.   \n[69] J. Shi, C. Liu, and L. Mackey, \u201cSampling with mirrored Stein operators,\u201d International Conference of Learning Representations, 2022.   \n[70] M. Girolami and B. Calderhead, \u201cRiemann manifold Langevin and Hamiltonian Monte Carlo methods,\u201d Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 73, no. 2, pp. 123\u2013214, 2011.   \n[71] M. Brubaker, M. Salzmann, and R. Urtasun, \u201cA family of MCMC methods on implicitly defined manifolds,\u201d in Artificial intelligence and statistics. PMLR, 2012, pp. 161\u2013172.   \n[72] P. Tseng, \u201cOn linear convergence of iterative methods for the variational inequality problem,\u201d Journal of Computational and Applied Mathematics, vol. 60, no. 1, pp. 237\u2013252, 1995.   \n[73] N. Golowich, S. Pattathil, C. Daskalakis, and A. Ozdaglar, \u201cLast iterate is slower than averaged iterate in smooth convex-concave saddle point problems,\u201d in Proceedings of Thirty Third Conference on Learning Theory, 2020, pp. 1758\u20131784.   \n[74] V. M. Panaretos and Y. Zemel, An invitation to statistics in Wasserstein space. Springer Nature, 2020.   \n[75] F. Otto, \u201cThe geometry of dissipative evolution equations: the porous medium equation,\u201d Communications in Partial Differential Equations, vol. 26, no. 1-2, pp. 101\u2013174, 2001.   \n[76] A. T. Schwarm and M. Nikolaou, \u201cChance-constrained model predictive control,\u201d AIChE Journal, vol. 45[8], pp. 1743\u20131752, 1999.   \n[77] P. Li, M. Wendt, and G. Wozny, \u201cRobust model predictive control under chance constraints,\u201d Computers & Chemical Engineering, vol. 24[2-7], pp. 829\u2013834, 2000.   \n[78] F. Borrelli, A. Bemporad, and M. Morari, Predictive Control for Linear and Hybrid Systems. Cambridge University Press, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In constrained sampling, it is important to distinguish between two types of constraints: support constraints and statistical constraints. The former deals with sampling from a target distribution $\\pi$ that is supported on a proper subset $\\mathcal{X}\\subset\\mathbb{R}^{d}$ , which arises in applications such as latent Dirichlet allocation [62] and regularized regression [63]. The latter is the problem tackled in the current work. ", "page_idx": 15}, {"type": "text", "text": "A first family of constrained sampling methods relies on rejection sampling: it obtains samples via any (unconstrained) method, rejecting those that violate the constraint (see, e.g., [10, 11]). Though this approach can handle constraints of any nature, it is often inefficient in terms of number of samples generated per iteration of the method (effective number of samples), especially when confronted with intricate constraints and high dimensional problems. ", "page_idx": 15}, {"type": "text", "text": "These drawbacks can be addressed for support constraints using techniques inspired by finitedimensional constrained optimization. Projected LMC, for instance, deals with the problem of sampling from a target distribution restricted to a convex set [13, 64]. Barrier methods have also been used to tackle the same problem [18, 65]. Similarly, mirror and proximal descent versions of popular sampling algorithms such as LMC [12, 14, 15, 66\u201368] and Stein Variational Gradient Descent (SVGD) [69] have been proposed. Mirror descent algorithms enforce constraints by mapping (mirroring) the primal variables to a space with a different geometry (induced by a Bregman divergence) over which the optimization is carried out. Alternatively, methods adapted to target distributions support on manifolds have also been put forward [18, 70, 71]. In practice, these methods require explicit expressions for the projections, barriers, and mirror maps describing the constraints and are therefore not adapted to handle statistical requirements such as those considered in (PI). Langevin dynamics with constraint violation penalties were considered in [23], although they cannot enforce exact constraint satisfaction. ", "page_idx": 15}, {"type": "text", "text": "Statistical (moment) constraints such as those considered in (PI) were investigated in [24]. As we discussed at the end of Section 2.3, this paper considers the combination of LMC and approximate dual ascent shown in (9). It also introduces a similar version of SVGD as well as algorithms based on barriers. Aside from requiring exact integration against intractable measures (namely, $\\mu_{k}$ ), convergence guarantees for these methods hold under restrictive assumptions on the constraints $g_{i}$ Additionally, guarantees are derived only for continuous-time (gradient flows) dynamics. ", "page_idx": 15}, {"type": "text", "text": "This work is also closely related to saddle-point methods in finite-dimensional optimization. For the general problem of $\\operatorname*{max}_{x}~\\operatorname*{min}_{y}~f(x,y)$ , the behavior of descent-ascent methods have been investigated under a myriad of scenarios, including for functions $f$ that are (strongly) convex/(strongly) concave [43, 45\u201348, 72, 73] as well as non-convex/non-concave under, e.g., PL conditions [47, 54, 55, 57, 58]. In general, convergence holds for the ergodic average of iterates [43, 57, 73]. Last-iterate results often require different algorithms, involving proximal point or extra gradient methods and time scale separation [47, 48, 73]. In particular, guarantees for the GDA method used in Algorithm 1 often requires stringent conditions that are hard to enforce for dual problems such as (DI). ", "page_idx": 15}, {"type": "text", "text": "B Wasserstein space and discrete-time flows ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this subsection we give some background on the Wasserstein spaces and sampling as optimization. For $\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , we define the 2-Wasserstein distance as ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{2}^{2}(\\mu,\\nu)=\\operatorname*{inf}_{s\\in S(\\mu,\\nu)}\\int\\|x-y\\|^{2}d s(x,y),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $S(\\mu,\\nu)$ is the set of couplings between $\\mu$ and $\\nu$ . The metric space $(\\mathcal{P}_{2}(\\mathbb{R}^{d}),W_{2})$ is referred to as the Wasserstein space [74]. It can be equipped with a Riemannian structure [75]. In this geometric interpretation, the tangent space to $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ at $\\mu$ is included in $L^{2}(\\mu)$ and is equipped with a scalar product defined for $f,{\\bar{g}}\\in{\\bar{L^{2}}}(\\mu)$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle f,g\\rangle_{L^{2}(\\mu)}=\\int f(x)g(x)d\\mu(x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We use its differential structure as introduced in [75] and [26, Chapter 10]. For the functionals at stake in this paper (e.g., potential energies and negative entropy), the set of subgradients of a functional $\\mathcal{F}:\\dot{\\mathcal{P}}_{2}\\dot{(\\mathbb{R}^{d})}\\overset{\\cdot}{\\rightarrow}\\mathbb{R}$ at $\\mu\\in\\mathcal{P}(\\mathbb{R}^{d})$ is non-empty if $\\mu$ satisfies some Sobolev regularity [26, Theorem 10.4.13]. ", "page_idx": 16}, {"type": "text", "text": "A Wasserstein gradient flow of $\\mathcal{F}$ is a solution $(\\mu(t))_{t\\in(0,T)},T>0$ , of the continuity equation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mu(t)}{\\partial t}+\\nabla\\cdot(\\mu(t)v(t))=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "that holds in the distributional sense, where $v(t)$ is a subgradient of $\\mathcal{F}$ at $\\mu(t)$ . Among the possible processes $(\\boldsymbol{v}(t))_{t}$ , one has a minimal $L^{2}(\\mu(t))$ norm and is called the velocity field of $(\\mu(t))_{t}$ . In a Riemannian interpretation of the Wasserstein space [75], this minimality condition can be characterized by $v(t)$ belonging to the tangent space to $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ at $\\mu(t)$ denoted $\\dot{T}_{\\mu(t)}\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , which is a subset of $L^{2}(\\mu(t))$ . The Wasserstein gradient is defined as this unique element, and is denoted $\\nabla_{W_{2}}\\mathcal{F}(\\mu(t))$ . In particular, if $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ is absolutely continuous with respect to the Lebesgue measure, with density in $C^{1}(\\mathbb{R}^{d})$ and such that $\\mathcal{F}(\\mu)<\\infty,\\,\\nabla_{W_{2}}\\mathcal{F}(\\mu)(x)=\\nabla\\mathcal{F}^{\\prime}(\\mu)(x)$ for $\\mu$ -a.e. $x\\in\\mathbb{R}^{d}$ , where $\\mathcal F^{\\prime}(\\mu)$ denotes the first variation of $\\mathcal{F}$ evaluated at $\\mu$ , i.e. (if it exists) the unique function $\\mathcal{F}^{\\prime}(\\mu):\\mathbb{R}^{d}\\to\\mathbb{R}$ s.t. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0}\\frac{1}{\\epsilon}(\\mathcal{F}(\\mu+\\epsilon\\xi)-\\mathcal{F}(\\mu))=\\int\\mathcal{F}^{\\prime}(\\mu)(x)d\\xi(x)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $\\xi=\\nu-\\mu$ , where $\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . ", "page_idx": 16}, {"type": "text", "text": "Now, we denote by $T_{\\#}\\mu$ the pushforward measure of $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ by the measurable map $T$ . We recall that the KL divergence of $\\mu$ relative to $\\pi$ can be decomposed as (1). The distribution $\\mu_{k}$ of $x_{k}$ in (3) is known to follow a \u201cforward-flow\u201d splitting scheme [7] of the Fokker-Planck equation in (2), namely ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu_{k+1/2}=\\left[\\mathrm{I}-\\gamma_{k}\\nabla_{W_{2}}\\mathcal{V}(\\mu_{k})\\right]_{\\#}\\mu_{k}}&{\\mathrm{(forward~discretization~for~}\\mathcal{V})}\\\\ {\\mu_{k+1}=\\mu_{k+1/2}\\star\\!\\mathcal{N}(0,2\\gamma_{k}\\mathrm{I}_{d})}&{\\mathrm{(exact~flow~for~}\\mathcal{H}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where I denotes the identity map in $L^{2}(\\mu_{k})$ and $\\nabla_{W_{2}}\\mathcal{V}(\\mu_{k})=\\nabla_{x}f(x)$ . ", "page_idx": 16}, {"type": "text", "text": "C Proofs from Section 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 3.3. Consider the potential ", "page_idx": 17}, {"type": "equation", "text": "$$\nV(\\mu,\\lambda,\\nu)\\triangleq W_{2}^{2}(\\mu,\\mu^{\\star})+\\operatorname*{min}_{(\\lambda^{\\star},\\nu^{\\star})\\in\\Phi^{\\star}}\\|\\lambda-\\lambda^{\\star}\\|^{2}+\\|\\nu-\\nu^{\\star}\\|^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mu^{\\star}$ is the solution of the constrained sampling problem (PI) and $\\Phi^{\\star}$ is the set of solutions of the dual problem (DI). Our goal is to show that $V$ decreases (in some sense) along trajectories of Algorithm 1. We say \u201cin some sense\u201d because contrary to the standard LMC algorithm, the distribution $\\mu_{k}$ of $x_{k}$ is now a random measure that depends on the random variables $\\{\\lambda_{k},\\nu_{k}\\}$ Explicitly, we consider the filtration $\\mathcal{F}_{k}=\\sigma(\\mu_{0},\\{\\lambda_{\\ell},\\nu_{\\ell}\\}_{\\ell\\leq k})$ and show that $V$ decreases on average when conditioned on $\\mathcal{F}_{k}$ . This turns out to be enough to prove Theorem 3.3. ", "page_idx": 17}, {"type": "text", "text": "Indeed, notice that $\\mu_{k}\\in\\mathcal{F}_{k}$ . Hence, the potential energy $\\mathcal{E}(\\mu_{k},\\lambda_{k},\\nu_{k})\\in\\mathcal{F}_{k}$ and the conditional law $\\tilde{\\mu}_{k}=\\mathcal{L}(x_{k}|\\mathcal{F}_{k-1})$ evolves as in the regular LMC algorithm (3). That is to say, conditioned on the event $\\mathcal{F}_{k}$ , step 5 of Algorithm 1 follows a splitting scheme as (16), i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mu}_{k+1/2}=\\bigl[\\ensuremath{\\mathrm{I}}-\\eta_{k}\\nabla_{W_{2}}\\mathcal{E}(\\mu_{k},\\lambda_{k},\\nu_{k})\\bigr]_{\\#}\\mu_{k}}\\\\ &{\\quad\\tilde{\\mu}_{k+1}=\\tilde{\\mu}_{k+1/2}\\star\\!\\mathcal{N}(0,2\\gamma\\ensuremath{\\mathrm{I}_{d}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that all distributions in (18) are now deterministic. The core of the proof is collected in the following lemma that shows that $V$ is a non-negative supermartingale. Note that Lemma C.1 describes the gap between \u201chalf-iterations.\u201d This will be inconsequential for our purposes. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. Under the conditions of Theorem 3.3, we have ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\big[V(\\tilde{\\mu}_{k+1/2},\\lambda_{k+1},\\nu_{k+1})|\\mathcal{F}_{k}\\big]\\le V(\\tilde{\\mu}_{k-1/2},\\lambda_{k},\\nu_{k})-2\\eta_{k}\\bigg[\\mathrm{KL}(\\tilde{\\mu}_{k}\\,\\|\\,\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})\\bigg]}&{}\\\\ {+\\eta_{k}^{2}\\Big[\\|\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})\\|_{L^{2}(\\tilde{\\mu}_{k})}^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|g(x)\\|^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|h(x)\\|^{2}\\Big].}&{(10)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We defer the proof of Lemma C.1 to show how it implies the bounds in Theorem 3.3. To do so, take the expectation of (19) with respect to $\\{\\lambda_{\\ell},\\nu_{\\ell}\\}_{\\ell\\le k}$ to obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\Delta_{k}]\\triangleq\\mathbb{E}\\big[V(\\tilde{\\mu}_{k+1/2},\\lambda_{k+1},\\nu_{k+1})-V(\\tilde{\\mu}_{k-1/2},\\lambda_{k},\\nu_{k})\\big]\\leq}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,2\\eta_{k}\\,\\mathbb{E}\\bigg[\\mathrm{KL}(\\tilde{\\mu}_{k}\\,\\|\\,\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\eta_{k}^{2}\\Big[\\|\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})\\|_{L^{2}(\\mu_{k})}^{2}+\\mathbb{E}_{x\\sim\\mu_{k}}\\,\\|g(x)\\|^{2}+\\mathbb{E}_{x\\sim\\mu_{k}}\\,\\|h(x)\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the bounds in Assumption 3.2 then yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}[\\Delta_{k}]\\triangleq\\mathbb{E}\\big[V(\\tilde{\\mu}_{k+1/2},\\lambda_{k+1},\\nu_{k+1})-V(\\tilde{\\mu}_{k-1/2},\\lambda_{k},\\nu_{k})\\big]\\leq}\\\\ {\\quad\\qquad-\\,2\\eta_{k}\\,\\mathbb{E}\\bigg[\\mathrm{KL}(\\tilde{\\mu}_{k}\\,\\|\\,\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})\\bigg]+\\eta_{k}^{2}\\big(3+\\mathbb{E}[\\|\\lambda_{k}\\|^{2}]+\\mathbb{E}[\\|\\nu_{k}\\|^{2}]\\big)G^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, summing the LHS of (21) over $k$ and using the fact that $V$ is non-negative yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\mathbb{E}[\\Delta_{k}]=\\mathbb{E}\\big[V(\\tilde{\\mu}_{K+1/2},\\lambda_{k+1},\\nu_{k+1})\\big]-V(\\tilde{\\mu}_{1/2},\\lambda_{1},\\nu_{1})\\geq-\\,\\mathbb{E}[V(\\tilde{\\mu}_{1/2},\\lambda_{1},\\nu_{1})].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that the expectation here is taken only over $\\mu_{0}$ given that $(\\lambda_{0},\\nu_{0})=(0,0)$ are deterministic.   \nTo proceed, we use the following proposition, whose proof we defer. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.2. Under the hypothesis of Theorem 3.3 it holds that $\\mathbb{E}[V(\\tilde{\\mu}_{1/2},\\lambda_{1},\\nu_{1})]\\leq R_{0}^{2}\\,f o r$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n{R}_{0}^{2}=W_{2}^{2}(\\mu_{0},\\mu^{\\star})+\\eta_{0}^{2}\\Big[\\|\\nabla f\\|_{L^{2}(\\mu_{0})}^{2}+\\mathbb{E}_{\\mu_{0}}[\\|g\\|^{2}]+\\mathbb{E}_{\\mu_{0}}[\\|h\\|^{2}]\\Big]+\\|\\lambda^{\\star}\\|^{2}+\\|\\nu^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Back in (21), we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{=1}^{K}\\eta_{k}\\left[\\mathbb{E}[\\mathrm{KL}(\\tilde{\\mu}_{k}\\parallel\\mu^{\\star})]+\\frac{m}{2}\\,\\mathbb{E}[W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})]\\right]\\leq R_{0}^{2}+\\sum_{k=1}^{K}\\eta_{k}^{2}3G^{2}+G^{2}\\sum_{k=1}^{K}\\eta_{k}^{2}\\big(\\mathbb{E}[\\|\\lambda_{k}\\|^{2}]+\\mathbb{E}[\\|\\nu_{k}\\|^{2}]\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We conclude by using the convexity of the Wasserstein distance to write ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[W_{2}^{2}(\\tilde{\\mu}_{k+1/2},\\mu^{\\star})\\right]\\geq W_{2}^{2}(\\mathbb{E}[\\tilde{\\mu}_{k+1/2}],\\mu^{\\star})\\right]=W_{2}^{2}\\big(\\mu_{k+1/2},\\mu^{\\star}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "icnheoqicueasli toife $\\eta_{k}$ n  btyh $\\mathrm{KL}$ ocoinf gc tohnatit $\\textstyle\\sum_{k=1}^{K}1/{\\sqrt{k}}\\geq{\\sqrt{K}}$ mainndis $\\begin{array}{r}{\\sum_{k=1}^{K}1/k\\le1+\\log(K)}\\end{array}$ .i nN Tothiecoer tehmat  3a.l3l $\\{\\lambda_{k},\\nu_{k}\\}$ therefore also hold (without expectations) when using exact gradients to update the dual variables. ", "page_idx": 18}, {"type": "text", "text": "Finally, we show there exists a sequence of step sizes $\\eta_{k}$ such that $\\mathbb{E}[V(\\mu_{k-1/2},\\lambda_{k},\\nu_{k})]\\,\\le\\,R_{0}^{2}$ for all $k~\\ge~1$ , where the expectation is taken over the $\\{\\lambda_{k},\\nu_{k}\\}$ . This immediately implies that $W_{2}^{2}(\\mu_{k},\\mu^{\\star})\\ \\leq\\ R_{0}^{2}$ and both $\\mathbb{E}[\\left\\Vert\\lambda_{k}\\right\\Vert^{2}]$ and $\\mathbb{E}[\\left\\Vert\\nu_{k}\\right\\Vert^{2}]$ are bounded for all $k$ . We proceed by induction. The base case is covered by Lemma C.2. Suppose now that there exists a sequence $\\{\\eta_{0},\\dots,\\eta_{k-1}\\}$ such that $\\mathbb{E}[V(\\tilde{\\mu}_{k-1/2},\\lambda_{k},\\nu_{k})]\\,\\le\\,R_{0}^{2}$ . From the definition of $V$ in (17) and the fact that the $(\\lambda^{\\star},\\nu^{\\star})$ are bounded (Prop. 2.2), we then obtain that $\\mathbb{E}[\\|\\lambda_{k}\\|],\\mathbb{E}[\\|\\nu_{k}\\|]$ are bounded. Consequently, there exists $\\eta_{k}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\eta_{k}^{2}(3+\\mathbb{E}[\\left\\Vert\\lambda_{k}\\right\\Vert^{2}]+\\mathbb{E}[\\left\\Vert\\nu_{k}\\right\\Vert^{2}])G^{2}\\leq2\\eta_{k}\\mathbb{E}\\Big[\\mathrm{KL}(\\tilde{\\mu}_{k}\\left\\Vert\\,\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})\\Big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From (21), we obtain that $\\mathbb{E}[\\Delta_{k}]\\quad\\le\\quad0$ , which together with the induction assumption yields $\\mathbb{E}\\big[V(\\tilde{\\mu}_{k+1/2},\\lambda_{k+1},\\nu_{k+1})\\big]\\leq R_{0}^{2}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma C.1. The proof proceeds by combining two inequalities bounding the primal and dual terms in (17). ", "page_idx": 18}, {"type": "text", "text": "(i) $W_{2}^{2}(\\tilde{\\mu}_{k+1/2},\\mu^{\\star})$ . We proceed following a coupling argument. Let $s^{k}$ be an optimal coupling between the random variables $Y\\sim\\tilde{\\mu}_{k}$ and $Z\\sim\\mu^{\\star}$ , i.e., a coupling that achieves $W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})$ . Consider now the random variable $T=Y-\\eta_{k}\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})$ and observe from (18a) that it is distributed as $\\tilde{\\mu}_{k+1/2}$ . Naturally, the coupling $s^{k}$ is no longer optimal for $(T,Z)$ , so that by the definition of the Wasserstein distance it follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{2}^{2}(\\tilde{\\mu}_{k+1/2},\\mu^{\\star})\\leq\\int\\lVert x-\\eta_{k}\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})-y\\rVert^{2}d s^{k}(x,y).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Expanding the RHS of (23) and using the $m$ -strong convexity of $\\mathcal{E}$ (Assumption 3.1) yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(\\tilde{\\mu}_{k+1/2},\\mu^{\\star})\\leq W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})-\\eta_{k}m W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})}\\\\ &{\\qquad\\qquad\\qquad+\\left.2\\eta_{k}\\big[\\mathcal{E}(\\mu^{\\star},\\lambda_{k},\\nu_{k})-\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})\\big]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\eta_{k}^{2}\\|\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})\\|_{L^{2}(\\tilde{\\mu}_{k})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can then bound the effect of the diffusion step using [8, Lemma 5] as in ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})-W_{2}^{2}\\big(\\tilde{\\mu}_{k-1/2},\\mu^{\\star}\\big)\\leq2\\eta_{k}\\big[\\mathcal{H}(\\mu^{\\star})-\\mathcal{H}(\\tilde{\\mu}_{k})\\big],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(\\tilde{\\mu}_{k+1/2},\\mu^{\\star})\\leq W_{2}^{2}(\\tilde{\\mu}_{k-1/2},\\mu^{\\star})-\\eta_{k}m W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})}\\\\ &{\\qquad\\qquad\\qquad+\\left.2\\eta_{k}\\left[\\mathcal{E}(\\mu^{\\star},\\lambda_{k},\\nu_{k})+\\mathcal{H}(\\mu^{\\star})-\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})-\\mathcal{H}(\\tilde{\\mu}_{k})\\right]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\eta_{k}^{2}\\|\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})\\|_{L^{2}(\\tilde{\\mu}_{k})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(ii) $\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}+\\|\\nu_{k+1}-\\nu^{\\star}\\|^{2}$ . Notice that since $\\lambda^{\\star}\\in\\mathbb{R}_{+}^{I}$ , the projection $x\\mapsto[x]_{+}$ is a contraction, i.e., $\\|[\\lambda]_{+}-\\lambda^{\\star}\\|\\leq\\|\\lambda-\\lambda^{\\star}\\|$ for all $\\lambda\\in\\mathbb{R}^{I}$ . Using the definition of $\\lambda_{k+1},\\nu_{k+1}$ from Algorithm 1, we then obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}+\\|\\nu_{k+1}-\\nu^{\\star}\\|^{2}\\leq\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+2\\eta_{k}(\\lambda_{k}-\\lambda^{\\star})^{\\top}g(x_{k})+\\eta_{k}^{2}\\,\\|g(x_{k})\\|^{2}}\\\\ {+\\,\\|\\nu_{k}-\\nu^{\\star}\\|^{2}+2\\eta_{k}(\\nu_{k}-\\nu^{\\star})^{\\top}h(x_{k})+\\eta_{k}^{2}\\,\\|h(x_{k})\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $(\\lambda^{\\star},\\nu^{\\star})\\in\\Phi^{\\star}$ . To proceed, consider the conditional expectation of (26) with respect to $\\mathcal{F}_{k}$ , namely, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]+\\mathbb{E}\\big[\\|\\nu_{k+1}-\\nu^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]\\leq\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+\\|\\nu_{k}-\\nu^{\\star}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\,2\\eta_{k}\\Big[(\\lambda_{k}-\\lambda^{\\star})^{\\top}\\,\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\big[g(x)\\big]+(\\nu_{k}-\\nu^{\\star})^{\\top}\\,\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\big[h(x)\\big]\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\eta_{k}^{2}\\Big[\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|g(x)\\|^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|h(x)\\|^{2}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the fact that $\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k}\\,\\in\\mathcal{F}_{k}$ . We conclude by using the linearity of the potential energy $\\mathcal{E}$ from (11) in both $\\lambda$ and $\\nu$ to get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\lambda_{k}-\\lambda^{\\star})^{\\top}\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\left[g(x)\\right]+(\\nu_{k}-\\nu^{\\star})^{\\top}\\,\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\left[h(x)\\right]={\\mathcal E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})-{\\mathcal E}(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Back in (27), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\big[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]+\\mathbb{E}\\big[\\|\\nu_{k+1}-\\nu^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]\\leq\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+\\|\\nu_{k}-\\nu^{\\star}\\|^{2}}&{}\\\\ {+\\;2\\eta_{k}\\big[\\mathcal{E}\\big(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k}\\big)-\\mathcal{E}\\big(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star}\\big)\\big]}&{}\\\\ {+\\;\\eta_{k}^{2}\\Big[\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|g(x)\\|^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|h(x)\\|^{2}\\Big],}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $(\\lambda^{\\star},\\nu^{\\star})\\in\\Phi^{\\star}$ . ", "page_idx": 19}, {"type": "text", "text": "To proceed with the proof, combine (25) and (28) to get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(\\tilde{\\mu}_{k+1/2},\\mu^{\\star})+\\mathbb{E}\\big[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]+\\mathbb{E}\\big[\\|\\nu_{k+1}-\\nu^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]\\leq}\\\\ &{\\qquad\\qquad\\qquad W_{2}^{2}(\\tilde{\\mu}_{k-1/2},\\mu^{\\star})+\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+\\|\\nu_{k}-\\nu^{\\star}\\|^{2}}\\\\ &{\\qquad+\\,2\\eta_{k}\\bigg[\\mathcal{E}(\\mu^{\\star},\\lambda_{k},\\nu_{k})+\\mathcal{H}(\\mu^{\\star})-\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star})-\\mathcal{H}(\\tilde{\\mu}_{k})\\bigg]-\\eta_{k}m W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})}\\\\ &{\\qquad\\qquad\\qquad+\\,\\eta_{k}^{2}\\Big[\\|\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})\\|_{L^{2}(\\tilde{\\mu}_{k})}^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|g(x)\\|^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|h(x)\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, recall from the definition of the dual function in (6) and the dual problem in (DI) that $d(\\lambda,\\nu)=$ $\\log(Z)\\!-\\!\\log(Z_{\\lambda\\nu})\\leq\\log(Z)\\!-\\!\\log(Z_{\\lambda^{\\star}\\nu^{\\star}})=d(\\lambda^{\\star},\\nu^{\\star})$ . By decomposing the Lagrangian using (11), we therefore obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(\\mu^{\\star},\\lambda_{k},\\nu_{k})+\\mathcal{H}(\\mu^{\\star})-\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star})-\\mathcal{H}(\\tilde{\\mu}_{k})}\\\\ &{\\qquad\\qquad\\qquad\\leq L(\\mu^{\\star},\\lambda_{k},\\nu_{k})-\\log(Z_{\\lambda\\nu})-L(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star})+\\log(Z_{\\lambda^{\\star}\\nu^{\\star}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq L(\\mu^{\\star},\\lambda_{k},\\nu_{k})-L(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the saddle-point property (7), we then get ", "page_idx": 19}, {"type": "equation", "text": "$$\nL(\\mu^{\\star},\\lambda_{k},\\nu_{k})-L(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star})\\leq L(\\mu^{\\star},\\lambda^{\\star},\\nu^{\\star})-L(\\tilde{\\mu}_{k},\\lambda^{\\star},\\nu^{\\star})\\leq-\\operatorname{KL}(\\tilde{\\mu}_{k}\\,\\|\\,\\mu^{\\star}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the $\\log(Z)-\\log(Z_{\\lambda^{\\star}\\nu^{\\star}})$ terms canceled out and we used the fact that $\\mu^{\\star}\\,=\\,\\mu_{\\lambda^{\\star}\\nu^{\\star}}$ . We conclude that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(\\tilde{\\mu}_{k+1/2},\\mu^{\\star})+\\mathbb{E}\\big[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]+\\mathbb{E}\\big[\\|\\nu_{k+1}-\\nu^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]\\leq}\\\\ &{\\quad W_{2}^{2}(\\tilde{\\mu}_{k-1/2},\\mu^{\\star})+\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+\\|\\nu_{k}-\\nu^{\\star}\\|^{2}-2\\eta_{k}\\bigg[\\mathrm{KL}(\\tilde{\\mu}_{k}\\,\\|\\,\\mu^{\\star})+\\frac{m}{2}W_{2}^{2}(\\tilde{\\mu}_{k},\\mu^{\\star})\\bigg]}\\\\ &{\\quad\\qquad\\qquad+\\eta_{k}^{2}\\Big[\\|\\nabla_{W_{2}}\\mathcal{E}(\\tilde{\\mu}_{k},\\lambda_{k},\\nu_{k})\\|_{L^{2}(\\tilde{\\mu}_{k})}^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|g(x)\\|^{2}+\\mathbb{E}_{x\\sim\\tilde{\\mu}_{k}}\\,\\|h(x)\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since (30) holds for all $(\\lambda^{\\star},\\nu^{\\star})\\,\\in\\,\\Phi^{\\star}$ , it holds in particular for the minimizer of the RHS, for which we can then write $V(\\tilde{\\mu}_{k-1/2},\\lambda_{k},\\nu_{k})$ . By subsequently taking the minimum of the LHS, we obtain (19). ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma C.2. From the updates in Algorithm 1, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[V(\\tilde{\\mu}_{1/2},\\lambda_{1},\\nu_{1})]\\leq W_{2}^{2}(\\tilde{\\mu}_{1/2},\\mu^{\\star})+\\mathbb{E}[\\|\\left[\\eta_{0}g(x_{0})\\right]_{+}-\\lambda^{\\star}\\|^{2}]+\\mathbb{E}[\\|\\eta_{0}h(x_{0})-\\nu^{\\star}\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $(\\lambda^{\\star},\\nu^{\\star})\\in\\Phi^{\\star}$ . Notice that since $\\lambda^{\\star}\\,\\in\\,\\mathbb{R}_{+}^{I}$ , the projection $x\\mapsto[x]_{+}$ is a contraction, i.e., $\\|[\\lambda]_{+}-\\lambda^{\\star}\\|\\leq\\|\\lambda-\\lambda^{\\star}\\|$ for all $\\lambda\\in\\mathbb{R}^{I}$ . Using the triangle inequality then yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[V(\\mu_{1/2},\\lambda_{1},\\nu_{1})]\\leq W_{2}^{2}(\\tilde{\\mu}_{1/2},\\mu_{0})+W_{2}^{2}(\\mu_{0},\\mu^{\\star})+\\mathbb{E}[\\|\\eta_{0}g(x_{0})-\\lambda^{\\star}\\|^{2}]+\\mathbb{E}[\\|\\eta_{0}h(x_{0})-\\nu^{\\star}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\leq W_{2}^{2}(\\mu_{0},\\mu^{\\star})+\\|\\lambda^{\\star}\\|^{2}+\\|\\nu^{\\star}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\ W_{2}^{2}(\\tilde{\\mu}_{1/2},\\mu_{0})+\\eta_{0}^{2}\\,\\mathbb{E}[\\|g(x_{0})\\|^{2}]+\\eta_{0}^{2}\\,\\mathbb{E}[\\|h(x_{0})\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To proceed, observe from (18a) that $\\tilde{\\mu}_{1/2}=\\left[\\mathrm{I}_{d}-\\eta_{0}\\nabla f\\right]_{\\#}\\mu_{0}$ , which implies that $W_{2}^{2}(\\tilde{\\mu}_{1/2},\\mu_{0})\\leq$ $\\eta_{0}^{2}\\|\\nabla f\\|_{L^{2}(\\mu_{0})}^{2}$ . Using the bounds in Assumption 3.2, we obtain that $\\mathbb{E}[V(\\tilde{\\mu}_{1/2},\\lambda_{1},\\nu_{1})]\\leq R_{0}^{2}$ . \u25a0 ", "page_idx": 20}, {"type": "text", "text": "Proof of Proposition 3.4. Since $c_{k}\\geq0$ and $\\textstyle\\sum c_{k}=1$ , we can use Jensen\u2019s inequality to write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[\\sum_{k=1}^{K}c_{k}\\varphi(x_{k})\\right]-\\mathbb{E}_{\\mu^{\\star}}[\\varphi]\\right|\\leq\\sum_{k=1}^{K}c_{k}\\left|\\int\\varphi d\\mu_{k}-\\int\\varphi d\\mu^{\\star}\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the relation between the $\\ell_{1}-$ and $\\ell_{2}$ -norm, we further obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}c_{k}\\bigg|\\int\\varphi d\\mu_{k}-\\int\\varphi d\\mu^{\\star}\\bigg|\\leq\\sqrt{\\sum_{k=1}^{K}c_{k}\\bigg|\\int\\varphi d\\mu_{k}-\\int\\varphi d\\mu^{\\star}\\bigg|^{2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $\\varphi$ is bounded by 1, then the summands on the right-hand side of (31) can be bounded by $\\operatorname{TV}(\\mu_{k},\\mu^{\\star})$ . Indeed, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\int\\varphi d\\mu_{k}-\\int\\varphi d\\mu^{\\star}\\right|\\leq\\int\\varphi|d\\mu_{k}-d\\mu^{\\star}|\\leq\\int|d\\mu_{k}-d\\mu^{\\star}|=2\\mathrm{TV}(\\mu_{k},\\mu^{\\star}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The total variation distance can in turn be bounded by the $\\mathrm{KL}$ divergence using Pinsker\u2019s inequality. We therefore obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[\\sum_{k=1}^{K}c_{k}\\varphi(x_{k})\\right]-\\mathbb{E}_{\\mu^{\\star}}[\\varphi]\\right|\\leq\\sqrt{2\\sum_{k=1}^{K}c_{k}\\operatorname{KL}(\\mu_{k}\\|\\mu^{\\star})}\\leq\\sqrt{2\\Delta_{K}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the other hand, if $\\varphi$ is 1-Lipschitz, the summands on the right-hand side of (31) are bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\int\\varphi d\\mu_{k}-\\int\\varphi d\\mu^{\\star}\\right|^{2}\\leq W_{1}^{2}(\\mu_{k},\\mu^{\\star})\\leq W_{2}^{2}(\\mu_{k},\\mu^{\\star}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[\\sum_{k=1}^{K}c_{k}\\varphi(x_{k})\\right]-\\mathbb{E}_{\\mu^{\\star}}[\\varphi]\\right|\\leq\\sqrt{\\frac{2\\Delta_{K}}{m}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as long as $m>0$ . ", "page_idx": 20}, {"type": "text", "text": "D Proofs from Section 3.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3.6. The proof is based on the analysis of the stochastic dual ascent algorithm ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{k+1}=\\left[\\lambda_{k}+\\eta_{k}g(\\xi_{k})\\right]_{+},\\;\\;\\;\\mathrm{for}\\;\\xi_{k}\\sim\\bar{\\mu}_{k}\\;\\mathrm{such}\\;\\mathrm{that}\\;\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu_{\\lambda_{k}})\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mu_{\\lambda_{k}}$ is the Lagrangian minimizer from (4) and $\\epsilon>0$ . Observe that, once again, we analyze the conditional distribution $\\bar{\\mu}_{k}=\\mathcal{L}(\\xi_{k}|\\lambda_{k})$ . We collect this result in the following proposition: ", "page_idx": 21}, {"type": "text", "text": "Proposition D.1. Consider the iterations (32) and assume that $\\mathbb{E}_{\\mu}[g_{i}^{2}]\\leq G^{2}$ for all $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Then, for $0<\\eta_{k}\\leq\\eta$ and $\\epsilon\\leq\\eta G^{2}$ , there exists $B<\\infty$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}[\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu^{\\star})]\\le\\epsilon+\\frac{\\eta G^{2}}{2}+\\frac{2B^{2}I}{\\eta K}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The expectations are taken over the samples $\\xi_{k}\\sim\\bar{\\mu}_{k}$ . ", "page_idx": 21}, {"type": "text", "text": "We conclude by combining Prop. D.1 with [29, Theorem 1], which characterizes the convergence of the LMC algorithm (3) under Assumption 3.5. Indeed, using the $\\gamma_{k},N_{k}^{0}$ from Theorem 3.6 in Algorithm 1 guarantees that the law $\\bar{\\mu}_{k}$ of $x_{k}|\\lambda_{k}$ is such that $\\mathrm{KL}(\\bar{\\mu}_{k}||\\mu_{\\lambda_{k}})\\,\\leq\\,\\epsilon,$ , i.e., satisfies the conditions in Prop. D.1. We can then apply Jensen\u2019s inequality to get that $\\mathbb{E}[\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu^{\\star})]\\geq$ $\\mathrm{KL}(\\mathbb{E}[\\bar{\\mu}_{k}]\\|\\mu^{\\star})=\\mathrm{KL}(\\mu_{k}\\|\\mu^{\\star})$ , where $\\mu_{k}$ is the law of $x_{N_{k}^{0}}$ in Algorithm 2. \u25a0 ", "page_idx": 21}, {"type": "text", "text": "Proof of Prop. D.1. The proof relies on the following lemmata: ", "page_idx": 21}, {"type": "text", "text": "Lemma D.2. For all $\\mu\\;\\in\\;\\mathcal{P}_{2}(\\mathbb{R}^{d})$ such that $\\mathrm{KL}(\\mu\\|\\mu_{\\lambda})\\ \\leq\\ \\epsilon,$ , the expected value $E_{\\mu}[g]$ is an approximate subgradients of the dual function $d$ in (6) at $\\lambda\\in\\mathbb{R}_{I}^{+}$ , i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\nd(\\lambda)\\geq d(\\lambda^{\\prime})+(\\lambda-\\lambda^{\\prime})^{\\top}E_{\\mu}[g]-\\epsilon,\\ \\ \\,f o r\\,a l l\\,\\lambda^{\\prime}\\in\\mathbb{R}_{I}^{+}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma D.3. Under the conditions of Prop. D.1, it holds for all $k$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\lambda^{\\star}\\|_{1}\\leq B_{0}\\triangleq\\frac{C-D^{\\star}+\\eta G^{2}+\\epsilon}{\\delta}\\quad a n d\\quad\\mathbb{E}[\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}]\\leq B^{2}\\triangleq2B_{0}^{2}+3\\eta^{2}G^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma D.4. The sequence $(\\lambda_{k},g(\\xi_{k}))$ obtained from (32) is such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K=1}\\mathbb{E}\\left[\\lambda_{k}^{\\top}g(\\xi_{k})\\right]\\geq-\\frac{\\eta G^{2}}{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the expectation is taken over realizations of $\\xi_{k}$ . ", "page_idx": 21}, {"type": "text", "text": "Before proving these results, let us show how they imply Prop. D.1. Start by noticing from (32) that $\\lambda_{i,k+1}\\geq\\lambda_{i,k}+\\eta g_{i}(\\xi_{k})$ . Solving the recursion and recalling that $\\lambda_{0}=0$ then yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda_{i,K}\\geq\\eta\\sum_{k=0}^{K-1}g_{i}(\\xi_{k}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking the expected value over $\\xi_{k}$ and dividing by $\\eta K$ , we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}_{\\bar{\\mu}_{k}}[g_{i}]\\le\\frac{\\mathbb{E}[\\lambda_{i,K}]}{\\eta K}\\le\\frac{\\mathbb{E}[|\\lambda_{i,K}-\\lambda_{i}^{\\star}|]+\\lambda_{i}^{\\star}}{\\eta K},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last bound stems from the triangle inequality. Since the upper bound is non-negative for all $i$ , we use the fact that the maximum of a set of values is less than the sum of those values to write ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i=1,\\ldots,I}\\left[\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}_{\\bar{\\mu}_{k}}[g_{i}]\\right]\\leq\\frac{\\mathbb{E}[\\|\\lambda_{K}-\\lambda^{\\star}\\|_{1}]+\\|\\lambda^{\\star}\\|_{1}}{\\eta K}\\leq\\frac{\\left(1+\\sqrt{I}\\right)B}{\\eta K},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used Lemma D.3 together with $(\\mathbb{E}[\\|z\\|_{1}])^{2}\\leq I\\cdot\\mathbb{E}[\\|z\\|^{2}]$ and $B_{0}<B$ . Observe that (37) bounds the infeasibility of the ergodic average $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\bar{\\mu}_{k}}\\end{array}$ for $\\bar{\\mu}_{k}$ as in (32). ", "page_idx": 21}, {"type": "text", "text": "To proceed, use the relation between the normalization of $\\mu_{\\lambda}$ and the dual function value [see (6)] to decompose the KL divergence between $\\bar{\\mu}_{k}$ and $\\mu^{\\star}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu^{\\star})=\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu_{\\lambda_{k}})+(\\lambda^{\\star}-\\lambda_{k})^{\\top}\\,\\mathbb{E}_{\\bar{\\mu}_{k}}[g]+d(\\lambda_{k})-d(\\lambda^{\\star}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu_{\\lambda_{k}})\\leq\\epsilon$ and $d(\\lambda)\\leq d(\\lambda^{\\star})$ for all $\\lambda\\in\\mathbb{R}_{+}^{I}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu^{\\star})\\leq\\epsilon+(\\lambda^{\\star}-\\lambda_{k})^{\\top}\\,\\mathbb{E}_{\\bar{\\mu}_{k}}[g]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Averaging over $k$ and using H\u00f6lder\u2019s inequality then yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu^{\\star})\\leq\\epsilon+\\|\\lambda^{\\star}\\|_{1}\\cdot\\operatorname*{max}_{i=1,\\ldots,I}\\;\\left[\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}_{\\bar{\\mu}_{k}}[g_{i}]\\right]-\\frac{1}{K}\\sum_{k=0}^{K-1}\\lambda_{k}^{\\top}\\mathbb{E}_{\\bar{\\mu}_{k}}[g],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which from Lemma D.3 and (37) becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathrm{KL}(\\bar{\\mu}_{k}\\|\\mu^{\\star})\\leq\\epsilon+\\frac{\\big(1+\\sqrt{I}\\big)B^{2}}{\\eta K}-\\frac{1}{K}\\sum_{k=0}^{K-1}\\lambda_{k}^{\\top}\\mathbb{E}_{\\bar{\\mu}_{k}}[g],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used $B_{0}<B$ . Taking the expected value, applying Lemma D.4, and taking $1+\\sqrt{I}\\leq2I$ for $I\\geq1$ yields (33). ", "page_idx": 22}, {"type": "text", "text": "D.1 Proof of Lemmata D.2\u2013D.4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Lemma $D.2$ . From the definition of the Lagrangian (4) and the dual function (6) (with $J=$ 0), we obtain $L(\\mu,\\lambda)=\\mathrm{KL}(\\mu\\|\\mu_{\\lambda})+d(\\lambda)$ . Using the fact that $\\mathrm{KL}(\\mu||\\mu_{\\lambda})\\leq\\epsilon$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\leq d(\\lambda)-L(\\mu,\\lambda)+\\epsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Again using the definition of the dual function (6), we also obtain that $d(\\lambda^{\\prime})\\leq L(\\mu,\\lambda^{\\prime})$ . Adding to (38) then gives ", "page_idx": 22}, {"type": "equation", "text": "$$\nd(\\lambda^{\\prime})\\leq+d(\\lambda)+L(\\mu,\\lambda^{\\prime})-L(\\mu,\\lambda)+\\epsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Notice from (4) that the first term of the Lagrangians in (39) cancel out, leading to (34). ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma D.3. Start by combining the update in (32) and the fact that, since $\\lambda^{\\star}\\in\\mathbb{R}_{+}^{I}$ , the projection $x\\mapsto[x]_{+}$ is a contraction, to get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}\\leq\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+2\\eta(\\lambda_{k}-\\lambda^{\\star})^{\\top}g(\\xi_{k})+\\eta_{k}^{2}\\left\\|g(\\xi_{k})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking the conditional expectation given $\\lambda_{k}$ then yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}]\\le\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+2\\eta(\\lambda_{k}-\\lambda^{\\star})^{\\top}\\,\\mathbb{E}[g(\\xi_{k})\\mid\\lambda_{k}]+\\eta^{2}G^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the fact that $\\mathbb{E}_{\\mu}[\\|g\\|^{2}]\\leq G^{2}$ for any $\\mu$ . Noticing from (32) that $\\mathbb{E}[g(\\xi_{k})\\ |\\ \\lambda_{k}]=$ $\\mathbb{E}_{\\bar{\\mu}_{k}}[g]$ , we can use Lemma D.2 to get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}]\\le\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}+2\\eta\\Big[d(\\lambda_{k})-D^{\\star}+\\eta G^{2}+\\epsilon\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the fact that $\\eta/2<\\eta$ to simplify the relation. ", "page_idx": 22}, {"type": "text", "text": "To proceed, consider the set of approximate maximizers of the dual function ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathcal{D}}\\triangleq\\left\\{\\lambda\\in\\mathbb{R}_{+}^{I}\\ |\\ d(\\lambda)\\geq D^{\\star}-\\eta G^{2}-\\epsilon\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Notice that $\\Phi^{\\star}\\subseteq\\mathcal{D}$ . Since there exists at least one $\\lambda^{\\star}$ that achieves $D^{\\star}$ (Prop. 2.2), $\\mathcal{D}$ is not empty. Notice that for $\\lambda_{k}\\notin{\\mathcal{D}}$ , we have that $d(\\lambda_{k})-D^{\\star}+\\eta G^{2}+\\epsilon<0$ . By the towering property, we therefore obtain from (40) that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}\\notin{\\mathcal D}]<\\mathbb{E}[\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}\\notin{\\mathcal D}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since $\\eta>0$ . ", "page_idx": 22}, {"type": "text", "text": "To bound the case when $\\lambda_{k}\\,\\in\\,\\mathcal{D}$ we use the strictly feasible candidate $\\mu^{\\dagger}$ from Assumption 2.1. Indeed, recall that $\\mathrm{KL}(\\mu^{\\dagger}\\|\\pi)\\leq C$ and $E_{\\mu^{\\dagger}}[g_{i}]\\leq-\\dot{\\delta}<0$ for all $i$ . From the definition of the dual function (6), we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\nd(\\lambda)\\leq L(\\mu^{\\dagger},\\lambda)\\leq C-\\|\\lambda\\|_{1}\\,\\delta,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the fact that $\\lambda\\in\\mathbb{R}_{+}^{I}$ to write $\\textstyle\\sum_{i}\\lambda_{i}=\\|\\lambda\\|_{1}$ . Hence, it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\lambda\\right\\|_{1}\\leq B_{0}\\triangleq{\\frac{C-D^{\\star}+\\eta G^{2}+\\epsilon}{\\delta}},\\quad{\\mathrm{for~all~}}\\lambda\\in{\\mathcal{D}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the fact that $\\left\\|z\\right\\|^{2}\\leq\\left\\|z\\right\\|_{1}^{2}$ for all $z$ and that $\\Phi^{\\star}\\subset\\mathcal{D}$ , we immediately obtain that $\\left\\|\\lambda-\\lambda^{\\star}\\right\\|^{2}\\leq$ $2B_{0}^{2}$ for all $\\lambda\\in{\\mathcal{D}}$ . Using the towering property and the fact that $D^{\\star}\\geq d(\\lambda_{k})$ and $\\epsilon\\leq\\eta G^{2}$ yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}\\in{\\mathcal{D}}]\\le2B_{0}^{2}+2\\eta\\epsilon+\\eta^{2}G^{2}\\le B^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To conclude, we write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}]=\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}\\in{\\mathcal{D}}]\\,\\mathbb{P}[\\lambda_{k}\\in{\\mathcal{D}}]}\\\\ &{}&{\\quad+\\,\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}\\notin{\\mathcal{D}}]\\,\\mathbb{P}[\\lambda_{k}\\notin{\\mathcal{D}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using (42) and (45) then yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}]\\le B_{1}^{2}\\,\\mathbb{P}[\\lambda_{k}\\in{\\mathcal{D}}]+\\mathbb{E}[\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}\\notin{\\mathcal{D}}]\\,\\mathbb{P}[\\lambda_{k}\\notin{\\mathcal{D}}]}\\\\ &{\\qquad\\qquad\\qquad\\le\\operatorname*{max}(B^{2},\\mathbb{E}[\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}\\mid\\lambda_{k}\\notin{\\mathcal{D}}]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since both (42) and (45) holds independently of $\\lambda_{k+1}$ , we can also write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\lambda_{k}-\\lambda^{\\star}\\|^{2}\\ |\\ \\lambda_{k}\\notin{\\mathcal{D}}]\\le\\operatorname*{max}(B^{2},\\mathbb{E}[\\|\\lambda_{k-1}-\\lambda^{\\star}\\|^{2}\\ |\\ \\lambda_{k-1}\\notin{\\mathcal{D}}])\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Applying these relations recursively, we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\lambda_{k+1}-\\lambda^{\\star}\\|^{2}]\\le\\operatorname*{max}(B^{2},\\|\\lambda_{0}-\\lambda^{\\star}\\|^{2})=\\operatorname*{max}(B_{1}^{2},\\|\\lambda^{\\star}\\|^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Noticing from (44) that since $\\lambda^{\\star}\\in\\mathcal{D}$ we have $\\left\\|\\lambda^{\\star}\\right\\|^{2}\\leq B_{0}^{2}<B^{2}$ then concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma $D.4.$ . To bound (35), we once again use the non-expansiveness of the projection to obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\lambda_{k+1}\\|^{2}\\leq\\|\\lambda_{k}\\|^{2}+\\eta^{2}\\|g(\\xi_{k})\\|^{2}+2\\eta\\lambda_{k}^{\\top}g(\\xi_{k}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking the expectation and using the fact that $\\mathbb{E}_{\\mu}[\\|g\\|^{2}]\\leq G^{2}$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\lambda_{k+1}\\|^{2}]\\leq\\mathbb{E}[\\|\\lambda_{k}\\|^{2}]+\\eta^{2}G^{2}+2\\eta\\,\\mathbb{E}[\\lambda_{k}^{\\top}g(\\xi_{k})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Applying this relation recursively from $K$ and using the fact that $\\lambda_{0}=0$ (deterministic) yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\lambda_{K}\\|^{2}]\\leq K\\eta^{2}G^{2}+2\\eta\\sum_{k=0}^{K-1}\\mathbb{E}[\\lambda_{k}^{\\top}g(\\xi_{k})].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\mathbb{E}[\\|\\lambda_{K}\\|^{2}]\\geq0$ , we can divide by $2\\eta K$ to obtain the desired result. ", "page_idx": 23}, {"type": "text", "text": "E Applications ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide further details on the example applications described in Section 2.2 as well as additional results from the experiments in Section 4. In these experiments, we start all chains at zero (unless stated otherwise) and use different step-sizes for each of the updates in steps 3\u20135 from Algorithm 1. We refer to them as $\\eta_{x},\\,\\eta_{\\lambda}$ , and $\\eta_{\\nu}$ . In contrast, we do not use diminishing step-sizes. ", "page_idx": 24}, {"type": "text", "text": "E.1 Sampling from convex sets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We are interested in sampling from target distribution ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi^{o}(x)\\propto e^{-f(x)}\\,\\mathbb{I}(x\\in{\\mathcal{C}}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some closed, convex set $\\mathcal{C}\\subset\\mathbb{R}^{d}$ . Several methods have been developed to tackle this problem, based on projections [13, 64], mirror maps [12, 15, 16], and barriers [17, 18]. Here, we consider a constrained sampling approach based on (PI) instead. ", "page_idx": 24}, {"type": "text", "text": "To do so, note that sampling from (46) is equivalent to sampling from ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mu^{\\star}\\in\\operatorname*{argmin}_{\\mu\\in\\mathcal P_{2}(\\mathcal C)}\\quad\\mathrm{KL}(\\mu\\|\\pi)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for the unconstrained $\\pi\\propto e^{-f}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Note that this is exactly (PII). Now let $\\mathcal{C}$ be described by the intersection of the 0-sublevel sets of convex functions $\\{s_{i}\\}_{i=1,\\ldots,I}$ , i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{C}}=\\bigcap_{i=1}^{I}\\{x:s_{i}(x)\\leq0\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Such a description is always possible by, e.g., considering the distance function $d(x,\\mathcal{C})=\\operatorname*{inf}_{y\\in\\mathcal{C}}\\|x-$ $y\\Vert_{2}$ , which is convex (since $\\mathcal{C}$ is convex) and for which $\\{x:d(x,{\\mathcal{C}})\\leq0\\}={\\mathcal{C}}$ (since $\\mathcal{C}$ is closed). Immediately, we see that solving (PII) is equivalent to solving ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}{\\mathrm{min}}}&{\\mathrm{KL}(\\mu\\|\\pi)}\\\\ {\\mathrm{subject\\,}\\mathrm{to}}&{\\mathbb{E}_{x\\sim\\mu}\\left[[s_{i}(x)]_{+}\\right]\\leq0,\\;i=1,\\dots,I,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $[z]_{+}=\\operatorname*{max}(0,z)$ . Note that (PVI) has the same form as (PI). ", "page_idx": 24}, {"type": "text", "text": "To see why this is the case, consider without loss of generality that $i=1$ . Since $[s_{i}(x)]_{+}\\geq0$ for all $x$ by definition, it immediately holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{C}}=\\{x:s(x)\\leq0\\}=\\{x:[s(x)]_{+}\\leq0\\}=\\{x:[s(x)]_{+}=0\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the monotonicity of Lebesgue integration, we obtain that the feasibility set of (PVI) is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}=\\Big\\{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}):\\mathbb{E}_{x\\sim\\mu}\\big[[s(x)]_{+}\\big]\\leq0\\Big\\}}\\\\ &{\\quad=\\Big\\{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}):\\mathbb{E}_{x\\sim\\mu}\\big[[s(x)]_{+}\\big]=0\\Big\\}}\\\\ &{\\quad=\\Big\\{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}):[s(x)]_{+}=0,\\ \\mu\\mathrm{-}\\mathrm{a.e.}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In other words, the feasibility set of (PVI) is in fact $\\mathcal{F}=\\{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}):\\mu(\\mathcal{C})=1\\}=\\mathcal{P}_{2}(\\mathcal{C}).$ ", "page_idx": 24}, {"type": "text", "text": "To illustrate the use of (PVI), consider the one-dimensional truncated Gaussian sampling problem from Section 4. Namely, we wish to sample from a standard Gaussian distribution ${\\mathcal{N}}(0,1)$ truncated to $\\mathcal{C}\\,=\\,[1,3]$ . In the language of (PVI), we take $f(x)\\,=\\,x^{2}/2$ (i.e., $\\pi\\,\\propto\\,e^{-x^{2}/2},$ ) and $s_{i}(x)\\,=$ $(x-1)(x-3)$ . In order to satisfy the assumptions of our convergence guarantees (particularly 2.1), we leave some slack in the constraints by considering $\\mathbb{E}_{\\mu}[[s_{i}\\bar{(}x)]_{+}]\\bar{\\mathbf{\\Omega}}\\!\\leq\\,0.005$ . This also helps with the numerical stability of the algorithm. Fig. 1 shows histograms of the samples obtained using PD-LMC (Algorithm 1 with $\\bar{\\eta_{x}}^{-}=\\eta_{\\lambda}=10^{\\bar{-}3},$ ), the projected LMC (Proj. LMC, $\\eta=10^{-3},$ ) from [13], and the mirror LMC $\\langle\\eta=10^{-3}$ ) from [59]. In all cases, we take $5\\times10^{6}$ samples and keep only the second half. ", "page_idx": 24}, {"type": "table", "img_path": "o6Hk6vld20/tmp/44c067f0f0aeec4cc78ecae6fa3bc3159bd42f52cbf5d4237385d20c7f141a9c.jpg", "table_caption": ["Table 2: Mean and variance estimates "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "o6Hk6vld20/tmp/d244adf41c0bc048ea974d748273e171359cabdb055918fa986babba50e0edad.jpg", "img_caption": ["Figure 5: One-dimensional truncated Gaussian sampling: (a) Ergodic average of the constraint function (slack) and (b) Evolution of the dual variable $\\lambda$ . "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "o6Hk6vld20/tmp/d303cf0b294ca3d0432ce196cad0896d90d62358bde61b0454cb763f2fe5b1e1.jpg", "img_caption": ["Figure 6: The effect of the mini-batch size $N_{b}$ on PD-LMC for sampling from a 1D truncated Gaussian: Estimated mean vs. (a) iteration and (b) LMC evaluations. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Observe that, due to the projection step, Proj. LMC generates an excess of samples close to the boundaries. In fact, it generates over three times more than required. This leads to an underestimation of the distribution mean and variance (Table 2). A similar effect is observed for mirror LMC. In contrast, PD-LMC provides a more accurate estimate. Nevertheless, PD-LMC imposes constraints on the distribution $\\mu$ rather than its samples. Indeed, note from (47) that its feasibility set is such that samples belong to $\\mathcal{C}$ almost surely, which still allows for a (potentially infinite) number of realizations outside of $\\mathcal{C}$ . Yet, though PD-LMC is not an interior-point method, Theorems $3.3{-}3.6\\,$ show that excursions of iterates outside of $\\mathcal{C}$ become less frequent as the algorithm progresses. We can confirm this is indeed the case in Fig. 5a, which shows the ergodic average of $[\\bar{s(x)]_{+}}$ along the samples of PD-LMC. Note that it almost vanishes by iteration $1\\bar{0}^{4}$ even though the dual variable $\\lambda$ only begins to stabilize later (Fig. 5b). This is not surprising given that it is guaranteed by Prop. 3.4. In fact, only roughly $2\\%$ of the samples displayed in Fig. 1 are not in $\\mathcal{C}$ . ", "page_idx": 25}, {"type": "text", "text": "Before proceeding, we examine whether the convergence of PD-LMC could be improved by averaging more than one LMC samples when updating the dual variables, i.e., using mini-batches in steps 4\u20135 of Algorithm 1. Mini-batches will reduce the variance of the dual updates, although at the cost of additional LMC steps per iteration. To compensate for this fact, Fig. 6b displays the evolution of the ergodic average of PD-LMC samples as a function of the number of LMC evaluations rather than the number of iterations (as in Fig. 6a). Notice that, in this application, increasing the number of LMC samples $N_{b}$ does not lead to faster convergence. This illustrates that, though mini-batches could be useful in some applications (particularly when the constraints are not convex, as in Section 3.2), it is not immediate that their benefits always outweigh the increased computational cost. Oftentimes, using a single LMC sample is more than enough. It is worth noting that using PD-LMC with a large mini-batch $N_{b}$ was suggested in [24] to approximate the expectation needed by their continuous-time algorithm. As we see here, this is neither necessary nor always beneficial. ", "page_idx": 25}, {"type": "image", "img_path": "o6Hk6vld20/tmp/fd7c96bc5dec181f63dbb40236010283fdc7a74d62a6c48fb2f798831e732889.jpg", "img_caption": ["Proj. LMC ", "Figure 7: Density estimate of two-dimensional truncated Gaussian using samples from (a) Proj. LMC and (b) PD-LMC. ", "PD-LMC "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "We now turn to a more challenging, two-dimensional applications. We seek to sample from a Gaussian located at [2, 2] with covariance $\\bar{\\mathrm{diag}}([1,1])$ restricted to an $\\ell_{2}$ -norm unit ball (Fig. 1). Specifically, we use $f(x)=\\left\\|x\\right\\|^{2}/2$ (i.e., $\\pi\\propto e^{-\\|x\\|^{2}/2})$ and $s_{i}(x)\\,=\\,\\left\\|x\\right\\|^{2}-1$ . Once again, we leave some slack to the algorithm by taking the constraint in (PVI) to be $\\vec{\\mathbb{E}}_{\\mu}[[s_{i}(x)]_{+}]\\leq0.001$ . For reference, we also display samples from the real distribution obtained using rejection sampling. ", "page_idx": 26}, {"type": "text", "text": "This is indeed a challenging problem. The boundary of $\\mathcal{C}$ is 2 standard deviations away from the mean of the target distribution, which means that samples from the target $\\pi$ are extremely scarce this region. Indeed, using the untruncated Gaussian as a proposal for rejection sampling yields an acceptance rate of approximately $1\\%$ . The strong push of the potential $f$ towards the exterior of $\\mathcal{C}$ leads Proj. LMC $\\bar{y}=10^{-3}$ ; the last $10^{6}$ samples are used after running $5\\times10^{6}$ iterations) to be now even more concentrated around its boundary. In fact, almost $25\\%$ of its samples are in an annular region of radius $[0.999,1)$ , where only $0.14\\%$ of the samples should be according to rejection sampling. Indeed, note from Fig. 7a, that even as iterations advance, the samples of Proj. LMC continue concentrate close to the boundary. ", "page_idx": 26}, {"type": "text", "text": "In contrast, PD-LMC $\\eta_{x}\\,=\\,10^{-3}$ and $\\eta_{\\lambda}\\,=\\,2\\times10^{-1};$ ) only place $1.8\\%$ of its samples outside of $\\mathcal{C}$ , mostly during the initial phase of the algorithm (Fig. 7a). Indeed, the average of the constraint function along samples from PD-LMC essentially vanishes around iteration $5^{-}\\times10^{4}$ . Achieving this requires larger values of $\\lambda$ (on the order of 250, Fig. 8b) compared to the one-dimensional case (Fig. 5b). This reflects the difficulty of constraining samples to $\\mathcal{C}$ in this instance, a statement formalized in the perturbation results of Prop. 2.2(iv). Due to the more amenable numerical properties of the barrier function, mirror LMC $(\\eta=1\\bar{0}^{-3})$ ) performs well without concentrating samples on the boundary $(0.15\\%$ of the samples on the annular region of radius $[0.999,1)]$ ). ", "page_idx": 26}, {"type": "image", "img_path": "o6Hk6vld20/tmp/500bbd08c594b7fdfc7de7704e07a952cfe0bd83433e9b389afe5050a0ca6989.jpg", "img_caption": ["Figure 8: Two-dimensional truncated Gaussian sampling: (a) Ergodic average of the constraint function (slack) and (b) Evolution of the dual variable $\\lambda$ . "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "o6Hk6vld20/tmp/24da9a5202258b110fd13622a4e799e4f88d8b5c7486468424e623ac4a43db7d.jpg", "img_caption": ["Figure 9: Fair Bayesian logistic regression on the Adult dataset: (a) prevalence of positive outputs and (b) dual variables. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "E.2 Rate-constrained Bayesian models ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "While rate constraints have become popular in ML due to their central role in fairness (see, e.g., [20]), they find applications in robust control [76\u201378] and to express other requirements on the confusion matrix, such as precision, recall, and false negatives [21]. For illustration, we consider here the problem of fairness in Bayesian classification. ", "page_idx": 27}, {"type": "text", "text": "Let $q(x;\\theta)=\\mathbb{P}[y=1|x,\\theta]$ denote the probability of a positive outcome $(y=1)$ ) given the observed features $x\\,\\in\\,{\\mathcal{X}}$ and the parameters $\\theta$ distributed according to the posterior $\\pi$ . This posterior is determined, e.g., by some arbitrary Bayesian model based on observations $\\{(x_{n},y_{n})\\}_{n=1,\\ldots,N}$ . Hence, $\\mathbb{E}_{\\theta\\sim\\pi}[\\bar{q(x;\\theta)}]$ denotes the likelihood of a positive outcome for $x$ . Consider now a protected group, represented by a measurable subset $\\mathcal G\\subset\\mathcal X$ , for which we wish to enforce statistical parity. In other words, we would like the prevalence of positive outcomes to be roughly the same as that of the whole population. Thus, we now want to sample not from the posterior $\\pi$ , but from a close-by distribution of parameters $\\theta$ that ensures this parity. Explicitly, for some tolerance $\\delta>0$ , we want to sample from ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu^{\\star}\\in\\underset{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}{\\mathrm{min}}}&{\\mathrm{KL}(\\mu\\|\\pi)}\\\\ {\\mathrm{subject~to}}&{\\mathbb{E}_{x,\\theta\\sim\\mu}\\big[q(x;\\theta)\\mid\\mathcal{G}\\big]\\geq\\mathbb{E}_{x,\\theta\\sim\\mu}\\big[q(x;\\theta)\\big]-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Naturally, we can account for more than one protected by incorporating additional constraints. ", "page_idx": 27}, {"type": "text", "text": "In our experiments, we take $\\pi$ to be the posterior of a Bayesian logistic regression model for the Adult dataset from [60] (details on data pre-processing can be found in [61]). The $N=32561$ data points in the training set are composed of $d=62$ socio-economical features $\\mathbf{\\chi}_{x}\\in\\mathbb{R}^{d}$ , including the intercept) and the goal is to predict whether the individual makes more than $\\mathrm{US}\\mathbb{S}$ 50000 per year $(y\\in\\{0,1\\})$ ). The posterior is obtained by combining a binomial log-likelihood with independent zero-mean Gaussian (log)priors ( $\\sigma^{2}\\,=\\,3\\$ ) on each parameter of the model, i.e., we consider the ", "page_idx": 27}, {"type": "image", "img_path": "o6Hk6vld20/tmp/34fc58e80024769ed654eb498b6686f2643d8041f326828a75e01e78f9162e4c.jpg", "img_caption": ["Figure 10: The effect of the mini-batch size $N_{b}$ on PD-LMC in fair Bayesian classification. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "potential ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\beta)=\\sum_{n=1}^{N}\\log(1+e^{-(2y_{n}-1)x_{n}^{\\top}\\theta})+\\sum_{i=0}^{d}\\frac{\\theta_{i}^{2}}{2\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We begin by using the LMC algorithm from (3) (i.e., we impose no constraints) to collect samples of the coefficients $\\theta$ from this posterior $\\eta\\,=\\,10^{-4}$ ; the last $10^{4}$ samples are used after running $2\\times10^{4}$ iterations). We find that, while the probability of positive outputs is $19.1\\%$ across the whole test set, it is $26.2\\%$ among males and $0.05\\%$ among females. Looking at the distribution of this probability over the unconstrained posterior $\\pi$ (Fig. 9a), we see that this behavior goes beyond the mean. The model effectively amplifies the inequality already present in the test set, where the prevalence of positive outputs is $30.6\\%$ among males and $10.9\\%$ among females. ", "page_idx": 28}, {"type": "text", "text": "To overcome this disparity, we consider gender to be the protected class in (PVII), constraining both $\\mathcal{G}_{\\mathrm{female}}$ and $\\mathcal{G}_{\\mathrm{male}}$ . We formulate the constraint of (PVII) using an empirical distribution induced from the data. Explicitly, we consider constraints ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{|{\\mathcal G}_{\\mathrm{female}}|}\\sum_{n\\in{\\mathcal G}_{\\mathrm{female}}}\\mathbb{E}_{\\theta\\sim\\mu}\\big[q(x_{n};\\theta)\\big]\\geq\\displaystyle\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{E}_{\\theta\\sim\\mu}\\big[q(x_{n};\\theta)\\big]-\\delta}\\\\ &{\\displaystyle\\frac{1}{|{\\mathcal G}_{\\mathrm{male}}|}\\sum_{n\\in{\\mathcal G}_{\\mathrm{male}}}\\mathbb{E}_{\\theta\\sim\\mu}\\big[q(x_{n};\\theta)\\big]\\geq\\displaystyle\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{E}_{\\theta\\sim\\mu}\\big[q(x_{n};\\theta)\\big]-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathcal{G}_{\\mathrm{female}}$ , $\\mathcal{G}_{\\mathrm{male}}\\subseteq\\{1,\\ldots,N\\}$ partition the data set. For these experiments, we take $\\delta=0.01$ . Using PD-LMC $\\eta_{x}=10^{-4}$ , $\\eta_{\\lambda}=5\\times10^{-3},$ ), we then obtain a new set of samples from the logistic regression parameters $\\theta$ that lead to a prevalence of positive outcomes (in the test set) of $1\\bar{7}.1\\%$ over the whole population, $18.1\\%$ for males, and $15.1\\bar{\\%}$ for females. In fact, we notice a substantial overlap between the distributions of this probability across the constrained posterior $\\mu^{\\star}$ for male and female (Fig. 9a). Additionally, this substantial improvement over the previously observed disparity comes at only a minor reduction in accuracy. Though both distributions change considerably, notice from the value of the dual variables that these changes are completely guided by the female group. Indeed, $\\lambda_{\\mathrm{male}}=0$ throughout the execution of PD-LMC (Fig. 9b). ", "page_idx": 28}, {"type": "text", "text": "Before proceeding, we once again examine the effect of using multiple LMC samples to update the dual variables, i.e., using mini-batches in steps 4\u20135 of Algorithm 1. Fig. 10 shows the distribution of the prevalence of positive predictions $(>\\mathfrak{F}50\\mathbf{k})$ for different mini-batch sizes $N_{b}$ . In all cases, we collect $2\\times10^{4}$ samples, which means that we evaluate $2N_{b}\\times10^{4}$ LMC updates (step 3 in Algorithm 1). Same as in the 1D truncated Gaussian case, we notice no difference between the resulting distributions. This is to be expected given our results (Theorem 3.3). The computation time, on the other hand, increases considerably with the mini-batch size. Once again, we note that PD-LMC with a large mini-batch $N_{b}$ was used in the experiments of [24] to overcome the challenge of computing an expectation in their dual variable updates. In turns out that this computationally intensive modification is not necessary. ", "page_idx": 28}, {"type": "image", "img_path": "o6Hk6vld20/tmp/32786d650699b02dcb3e0d6341983caf75d4816ea75948f5bf73670eb590e283.jpg", "img_caption": ["Figure 11: Counterfactual sampling in fair Bayesian logistic regression: (a) Selected (mean) coefficients and (b) different tolerances. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "E.3 Counterfactual sampling ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Previous applications were primarily interested in sampling from $\\mu^{\\star}$ , the constrained version of the target distribution $\\pi$ . The goal of counterfactual sampling, on the other hand, is to probe the probabilistic model $\\pi$ by evaluating its compatibility with a set of moment conditions. It is therefore interested not only in $\\mu^{\\star}$ , but in how each condition affects the value $P^{\\star}=\\mathrm{KL}(\\mu^{\\star}\\|\\pi)$ . We next describe how constrained sampling can be used to tackle this problem. ", "page_idx": 29}, {"type": "text", "text": "Let $\\pi$ denote a reference probabilistic model, such as the posterior of the Bayesian logistic model in (48). Standard Bayesian hypothesis tests can be used to evaluate the validity of actual statements such as \u201cis it true that $\\begin{array}{r}{\\mathbb{E}_{x\\sim\\pi}[\\bar{g}(x)]\\leq0?}\\end{array}$ or $\\mathbb{E}_{x\\sim\\pi}[h(x)]=0^{\\ast}$ ?\u201d Hence, we could check \u201cis $\\pi$ more likely to yield a positive output for a male than a female individual?\u201d (from the distributions under Unconstrained in Fig. 9a, this is probably the case). In contrast, counterfactual sampling is concerned with counterfactual statements such as \u201chow would the world have been if $\\mathbb{E}[g(x)]\\bar{\\leq}\\,\\bar{0}?$ \u201d In the case of fairness, \u201chow would the model have been if it predicted positive outcomes more equitably?\u201d ", "page_idx": 29}, {"type": "text", "text": "Constrained sampling evaluates these counterfactual statements in two ways. First, by providing realizations of this alternative, counterfactual world $(\\mu^{\\star})$ . For instance, we can inspect the difference between realizations of $\\pi$ , obtained using the traditional LMC in (3), and $\\mu^{\\star}$ , obtained using PD-LMC (Algorithm 1). In Fig. 11a, we show the mean of some coefficients of the Bayesian logistic models from Section E.2. Notice that it is not enough to normalize the Intercept and reduce the advantage given to males (Female is encoded as Male $=0$ ). This alternative model also compensates for other correlated features, such as education (Bachelor), profession (Adm/clerical), and age. ", "page_idx": 29}, {"type": "text", "text": "Second, constrained sampling evaluates the \u201ccompatibility\u201d of each counterfactual condition (constraint) world with the reference model. While Algorithm 1 does not evaluate $P^{\\star}$ explicitly, it provides measures of its sensitivity to perturbations of the constraints: the Lagrange multipliers $(\\lambda^{\\star},\\nu^{\\star})$ . Indeed, recall from Prop. 2.2 that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mu^{\\star}\\propto\\pi\\times\\left(\\prod_{i=1}^{I}e^{-\\lambda_{i}^{\\star}g_{i}}\\right)\\times\\left(\\prod_{j=1}^{J}e^{-\\nu_{j}^{\\star}h_{j}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, $(\\lambda^{\\star},\\nu^{\\star})$ describe the magnitude of tilts needed for the reference model $\\pi$ to satisfy the conditions $\\mathbb{E}[g(x)]\\leq0$ or $\\mathbb{E}[h(x)]=0$ . This relation is made explicit in Prop. 2.2(iv). ", "page_idx": 29}, {"type": "text", "text": "Concretely, observe that the dual variable relative to the constraint on the male subgroup is always zero (Fig. 9b). This implies that $\\pi$ is fully compatible with the condition ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{|\\mathcal{G}_{\\mathrm{male}}|}\\sum_{n\\in\\mathcal{G}_{\\mathrm{male}}}\\mathbb{E}_{\\theta\\sim\\mu}\\big[q(x_{n};\\theta)\\big]\\ge\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{E}_{\\theta\\sim\\mu}\\big[q(x_{n};\\theta)\\big]-\\delta,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "i.e., the statement \u201cthe model predicts positive outcomes for males on average at least as much as for the whole population.\u201d In contrast, accommodating statistical parity for females requires considerable deviations from the reference model $\\pi$ $(\\lambda_{\\mathrm{female}}^{\\star}\\approx160)$ ). Without recalculating $\\mu^{\\star}$ , we therefore know that even small changes in the tolerance $\\delta$ for the female constraint would substantially change the distribution of outcomes. This statement is confirmed in Fig. 11b. Notice that this is only possible due to the primal-dual nature of PD-LMC. ", "page_idx": 29}, {"type": "image", "img_path": "o6Hk6vld20/tmp/9ce47ec51c5fcedc566b1729533ac2501da4de5cfd5d0bfe9fa03597544e27da.jpg", "img_caption": ["Figure 12: Counterfactual sampling of the stock market under a $20\\%$ average return increase on each stock: mean $(\\rho)$ and variance $[\\mathrm{diag}(\\Sigma)]$ distributions. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "o6Hk6vld20/tmp/f5b361f89c8f2af8604d623bfc1085462960cad115cf4a86422a28c38e691279.jpg", "img_caption": ["Figure 13: Counterfactual sampling of the stock market under a $20\\%$ average return increase on each stock: (a) ergodic average of constraint functions (slacks) and (b) dual variables. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "E.3.1 Stock market model ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Counterfactual analyses based on the dual variables become more powerful as the number of constraints grow. To see this is the case, consider the Bayesian stock market model introduced in Section 2.2. Here, $\\pi$ denotes the posterior model for the (log-)returns of 7 assets (TSLA, NVDA, JNJ, AAPL, GOOG, BRK-B, and LLY). The dataset is composed of 5 years of adjusted closing prices for a total of 1260 points per asset. The posterior is obtained by combining a Gaussian likelihood $\\mathcal{N}(\\rho,\\Sigma)$ with Gaussian prior on the mean $\\bar{\\rho}\\,[\\mathcal{N}(0,3I)]$ and an inverse Wishart prior on the covariance $\\Sigma$ (with parameters $\\Psi=I$ and $\\nu=12$ ). Using the LMC algorithm $\\mathit{\\Delta}\\eta=10^{-3}$ ; the last $3\\times10^{5}$ samples are used after running $6\\times10^{5}$ iterations), we collect samples from this posterior and estimate the mean and variance of the (log-)return for each stock (Table 3). In this case, $\\Sigma$ is initialized to $10\\times I$ . ", "page_idx": 30}, {"type": "text", "text": "We might now be interested in understanding what the market would look like if all stocks were to incur a $20\\%$ increase in their average (log-)returns. To do so, we use PD-LMC $(\\eta_{x}\\,=\\,10^{-3}$ and $\\eta_{\\nu}=6\\times10^{-3})$ to solve the following constrained sampling problem ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}{\\mathrm{minimize}}\\quad\\mathrm{KL}(\\mu\\|\\pi)}\\\\ &{\\mathrm{subject}\\,\\,\\mathrm{to}\\quad\\mathbb{E}_{(\\rho,\\Sigma)\\sim\\mu}\\left[\\rho_{i}\\right]=1.2\\bar{\\rho}_{i},\\quad i=1,\\dots,7,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\bar{\\rho}_{i}$ is the mean (log-)return of the $i$ -th stock shown in Table 3. The distribution of $\\rho$ and the diagonal of $\\Sigma$ are compared to those from the unconstrained model in Fig. 12. Notice that, though we only impose constraints on the average returns $\\rho$ , we also see small changes in the stock volatilities. ", "page_idx": 30}, {"type": "text", "text": "Inspecting the dual variables (Fig. 13b), we notice that three dual variables are essentially zero (TSLA, AAPL, and BRK-B). This means that their increased returns are completely dictated by those of other stocks. Said differently, their returns increasing $20\\%$ is consistent with the reference model $\\pi$ conditioned on the other returns increasing. Proceeding, two stocks have negative dual variables (LLY and NVDA). This implies that bringing their constraints down to $\\bar{\\rho}_{i}$ would yield a decrease in $P^{\\star}$ (distance to the reference model $\\pi$ ). This is in contrast to JNJ and GOOG, whose positive $\\lambda$ \u2019s imply that we should should increase their returns to reduce $P^{\\star}$ . Indeed, by inspecting the ergodic slacks (Fig. 13a) we see that all stocks approach zero (i.e., feasibility), but that JNJ and GOOG do so from above. This behavior is expected according to Prop. 3.4. ", "page_idx": 30}, {"type": "image", "img_path": "o6Hk6vld20/tmp/fce2bf4d8c5ce7fab5c32e2085687e06e757efbe9138609d896fabb6fbbbf8ac.jpg", "img_caption": ["Figure 14: Distribution of mean $(\\rho)$ and variance $[\\mathrm{diag}(\\Sigma)]$ of the stock market constrained to a $20\\%$ average return increase on all stocks vs. only LLY and NVDA. "], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "o6Hk6vld20/tmp/ffbda2a89ae68f7d5e7d4530c47c485484e8019569d3146c6c4ce8b847bed656.jpg", "table_caption": ["Table 3: Mean \u00b1 standard deviation of mean (log-)returns $(\\rho)$ . "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "These observations show two things. First, that an increase in the average returns of LLY and NVDA is enough to drive up the returns of all other stocks. In fact, it leads to essentially the same distribution as if we had required the increase to affect all stocks (Fig. 14). Second, that the increase we would see in JNJ (and to a lesser extent GOOG) would actually be larger than $20\\%$ . Once again, we reach these conclusion without any additional computation. Their accuracy can be corroborated by the results in Table 3. ", "page_idx": 31}, {"type": "text", "text": "F Example application of Prop. 2.2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section we illustrate the result in Prop. 2.2, i.e., we show that given solutions $(\\lambda^{\\star},\\nu^{\\star})$ of (DI), the constrained sampling problem (PI) reduces to sampling from $\\mu_{\\lambda^{\\star}\\nu^{\\star}}\\propto e^{-U(\\cdot,\\lambda^{\\star},\\nu^{\\star})}$ . ", "page_idx": 32}, {"type": "text", "text": "Indeed, consider a standard Gaussian target, i.e., $\\pi\\ \\propto\\ e^{-\\|x\\|^{2}/2}$ , and the linear moment constraint $\\mathbb{E}[x]=b$ , for $b\\in\\mathbb{R}^{d}$ . This can be cast as (PI) with $f(x)=\\|x\\|^{2}/2$ and $h(x)=b-x$ (no inequality constraints, i.e., $I=0$ ). Clearly, the solution of (PI) in this case is $\\mu^{\\star}=\\mathcal{N}(b,I)$ , i.e., a Gaussian distribution with mean $b$ . What Prop 2.2 claims is that rather than directly solving (PI), we can solve (DI) to obtain a Lagrange multiplier $\\nu^{\\star}$ such that $\\mu^{\\star}=\\mu_{\\nu^{\\star}}$ for $\\mu_{\\nu}$ defined as in (4). ", "page_idx": 32}, {"type": "text", "text": "In this setting, we can see this is the case by doing the computations explicitly. Indeed, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mu_{\\nu}(x)\\propto\\pi(x)e^{-\\nu^{\\top}h(x)}=\\exp\\bigg[-\\frac{\\|x\\|^{2}}{2}-\\nu^{\\top}(b-x)\\bigg].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Completing the squares, we then obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mu_{\\nu}(x)\\propto\\exp\\bigg[-\\frac{\\|x-\\nu\\|^{2}}{2}+\\frac{\\|\\nu\\|^{2}}{2}-\\nu^{\\top}b\\bigg].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To compute the Lagrange multiplier $\\nu^{\\star}$ , notice from the definition of the dual function in (6) that the dual problem (DI) is in fact a ratio of normalizing factors. Explicitly, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nu^{\\star}=\\underset{\\nu\\in\\mathbb{R}^{d}}{\\mathrm{argmax}}\\,\\log\\left(\\frac{\\int\\pi(x)d x}{\\int\\mu_{\\nu}(x)d x}\\right)=\\underset{\\nu\\in\\mathbb{R}^{d}}{\\mathrm{argmax}}\\,\\log\\left[\\frac{\\int\\exp\\Big(-\\frac{\\|x\\|^{2}}{2}\\Big)d x}{\\exp(\\|\\nu\\|^{2}/2-\\nu^{\\top}b)\\int\\exp\\Big(-\\frac{\\|x-\\nu\\|^{2}}{2}\\Big)d x}\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Immediately, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nu^{\\star}=\\underset{\\nu\\in\\mathbb{R}^{d}}{\\mathrm{argmax}}\\,\\,-\\|\\nu\\|^{2}/2+\\nu^{\\top}b=b.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that the dual problem is a concave program, as is always the case [30]. To conclude, we can combine (49) and (50) to get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mu_{\\nu^{\\star}}\\left(x\\right)\\Bigm|_{\\nu^{\\star}=b}\\infty\\exp\\bigg(-\\frac{\\|x-b\\|^{2}}{2}-\\frac{\\|b\\|^{2}}{2}\\bigg)\\Rightarrow\\mu_{\\nu^{\\star}}=\\mathcal{N}(b,I)=\\mu^{\\star}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The main advantage of using Algorithms 1 and 2 is that we do not need to determine the Lagrange multipliers $(\\lambda^{\\star},\\nu^{\\star})$ to then sample from $\\mu_{\\lambda^{\\star}\\nu^{\\star}}=\\mu^{\\star}$ . Indeed, Theorems 3.3 and 3.6 show that these stochastic primal-dual methods do both things simultaneously, without explicitly evaluating any expectations. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: the abstract and introduction list our contributions, i.e. the theoretical and experimental study of Primal-Dual Langevin Monte Carlo. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: we discuss the limitations of our theoretical results in the conclusion. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: For each theoretical result, we worked with justified assumptions, making all the dependencies of the problem clear. We provide clear and detailed proofs in the appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: For each experimental result, we provide the detailed setting. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We will provide a link to a public github repository with a Python code. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: the algorithms and settings of the experiments are rather simple, and detailed in the submission. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We give all precisions relative to the the evaluation of our models, either on simulated data or real data, including precising train/set proportions for instance. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our experiments run on a standard laptop in a few minutes, and are illustrative. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: In our opinion, this paper does not address societal impact directly, and consider the generic problem of optimization over measures and sampling. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: In our opinion the paper does not have direct positive or negative social impact. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not present such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 37}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: our paper does not use existing assets ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. ", "page_idx": 37}, {"type": "text", "text": "\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. ", "page_idx": 37}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] Justification: we only have experiments in Python that will be made public. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: our experiments do not involve crowdsourcing. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: our study do not involve risk for participants. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]