[{"figure_path": "KjNEzWRIqn/figures/figures_1_1.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates the Synatra approach.  Indirect knowledge, such as online tutorials and random web page observations without associated tasks, is transformed into direct demonstrations. These demonstrations show the next action an agent should take based on past actions and current observations.  The goal is to create high-quality training data for digital agents without relying on expensive human annotation or complex reinforcement learning setups.", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_2_1.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates the Synatra approach which transforms indirect knowledge (like online tutorials) into direct supervision for training digital agents.  The left side shows the indirect knowledge sources (tutorials and random observations without associated actions), while the right side depicts the synthesized direct demonstrations, which are sequences of actions and observations that lead to successful task completion.  The arrow visually represents the transformation process performed by the Synatra model.", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_5_1.jpg", "caption": "Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings", "description": "This figure visualizes the task intent and accessibility tree embeddings using t-SNE.  The left plot shows the distribution of task intents, demonstrating the diversity of tasks covered in the synthetic data, comparing those from tutorials and random observations. The right plot visualizes the accessibility tree embeddings showing the distribution of observations.  It helps illustrate the domain coverage of the synthetic data and how it compares to real-world web pages (from Mind2Web and random samples). The plots show the overlap between data from different sources, indicating the effectiveness of the data synthesis approach.", "section": "4 Data Statistics"}, {"figure_path": "KjNEzWRIqn/figures/figures_5_2.jpg", "caption": "Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings", "description": "This figure visualizes the distribution of task intents and accessibility tree embeddings using t-SNE.  The left plot shows the distribution of task intents, demonstrating that intents synthesized from random observations exhibit broader coverage than those from tutorials.  The right plot shows the distribution of accessibility tree embeddings, indicating that observations from tutorials overlap significantly with real web pages, both from random observations and those in the Mind2Web dataset. This visualization helps to understand the diversity and coverage of the synthetic data generated.", "section": "4 Data Statistics"}, {"figure_path": "KjNEzWRIqn/figures/figures_8_1.jpg", "caption": "Figure 3: the comparison between the models trained with trajectories generated by our approach and the data collected from human.", "description": "This figure shows the comparison of model performance on MiniWoB++ and WebArena using three different training datasets:  Synthetic data generated by Synatra, human-annotated data from Mind2Web, and a combination of both human and synthetic data.  It highlights the effectiveness of Synatra's synthetic data in improving model performance, especially on WebArena where human-only data resulted in 0% success rate, while Synatra achieved a 4.56% success rate.", "section": "6 Results"}, {"figure_path": "KjNEzWRIqn/figures/figures_8_2.jpg", "caption": "Figure 3: the comparison between the models trained with trajectories generated by our approach and the data collected from human.", "description": "This figure shows the comparison of model performance between models trained with synthetic data generated using Synatra and models trained with human-annotated data from Mind2Web.  The results are presented for three web-based benchmarks: MiniWoB++, WebArena, and Mind2Web.  It highlights the significant performance improvement achieved by Synatra-trained models, particularly in WebArena where the human-only model fails completely.  The results suggest that synthetic data can be highly effective in training web-based agents, even surpassing models trained with more expensive human-generated data.", "section": "6.2 Analysis"}, {"figure_path": "KjNEzWRIqn/figures/figures_9_1.jpg", "caption": "Figure 4: Ablation on trajectory formats, sources of knowledge, and ways to use knowledge.", "description": "This figure presents an ablation study to evaluate design choices in Synatra.  Subfigure (a) compares the performance of models trained using trajectories represented as programs versus natural language. (b) compares models trained with indirect knowledge from different sources (tutorials, random web observations, and a combination of both).  (c) contrasts the performance of models trained with the generated trajectories from Synatra against the performance of retrieval augmented generation models and baseline LLMs. These results illustrate the importance of program representation for trajectories, the complementary nature of different knowledge sources in generating synthetic data, and the effectiveness of using Synatra to transform indirect knowledge into effective training data.", "section": "6.3 Ablations"}, {"figure_path": "KjNEzWRIqn/figures/figures_15_1.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates the core idea of the Synatra approach.  The left side shows the indirect knowledge sources used, such as online tutorials and randomly sampled web page observations (without associated tasks and actions).  The right side shows the synthesized direct demonstrations generated from this indirect knowledge.  These demonstrations provide direct supervision for training digital agents by specifying the next action an agent should take, given a current state (previous actions and observations).", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_18_1.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates the Synatra approach, which transforms indirect knowledge (like online tutorials) into direct demonstrations for training digital agents.  The left side shows the indirect knowledge sources, while the right side depicts the synthesized direct demonstrations (next actions given states and observations) used to train the model.  The core idea is to use readily available indirect knowledge to create training data, overcoming the limitations and high cost of obtaining large-scale, manually created demonstrations for digital tasks.", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_19_1.jpg", "caption": "Figure 5: The rendered generated HTML between step 2 and step 3, where step 2 is goto (\"Google Chat\") and step 3 is click(\"jane.doe@gmail.com\"). The concrete element to interact with is tagged with id=\"next-action-target-element\".", "description": "This figure shows the HTML code generated by the model to represent the webpage after two consecutive actions (goto and click) in a task of enabling Google Chat in a specific Gmail account. The HTML highlights the webpage's structure and includes the \"next-action-target-element\" tag to indicate the element relevant to the next action in the task. This showcases the model's ability to generate realistic and actionable web page representations based on previous actions and the intended next action.", "section": "3.2 Synthesizing from Text Procedural Knowledge with Generative Environment"}, {"figure_path": "KjNEzWRIqn/figures/figures_19_2.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates Synatra's approach to generating direct demonstrations from indirect knowledge.  The left side shows the indirect knowledge sources: tutorials and random observations.  These sources lack the direct connection between state, action, and next observation needed for training digital agents. Synatra processes this indirect knowledge (left side of arrow) to generate direct demonstrations (right side of arrow) that explicitly show the sequence of actions and observations for completing a given task. These synthetic demonstrations are then used to train a more effective digital agent.", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_21_1.jpg", "caption": "Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings", "description": "This figure visualizes the distribution of task intents and accessibility tree embeddings using t-SNE. The left panel shows that intents synthesized from random observations have broader coverage than those from tutorials, likely because humans tend to write tutorials for critical domains.  The right panel demonstrates that generated observations from tutorials overlap significantly with real web pages from both random observations and Mind2Web, indicating good domain coverage of the synthetic data.", "section": "4 Data Statistics"}, {"figure_path": "KjNEzWRIqn/figures/figures_21_2.jpg", "caption": "Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings more complex tasks. The latter two measure the domain diversity of our synthetic data. We also present these statistics for the Mind2Web dataset as an example of human-collected trajectories.", "description": "This figure visualizes the distribution of task intents and accessibility tree embeddings using t-SNE, showing the diversity of the synthetic data generated by Synatra compared to the human-collected Mind2Web dataset.  The left plot shows that Synatra generates intents with broader coverage than tutorials alone, while the right plot shows overlap between synthetic observations from both tutorials and random samples, and real web pages from Mind2Web.  The diversity of intents and observations highlights the richness of the synthetic dataset.", "section": "4 Data Statistics"}, {"figure_path": "KjNEzWRIqn/figures/figures_22_1.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates Synatra's approach to generating direct demonstrations for digital agents.  It shows how Synatra transforms indirect knowledge sources (such as online tutorials meant for human users and randomly collected observations without associated task and action information) into direct demonstrations, which are sequences of actions and corresponding observations that a digital agent can directly learn from.  The arrow visually represents this transformation from indirect knowledge to synthesized direct demonstrations which can be used to fine-tune LLMs.", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_23_1.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates Synatra's approach to generating direct demonstrations from indirect knowledge sources.  The left side shows indirect knowledge sources, such as tutorials and randomly sampled observations without associated actions. The right side shows the direct demonstrations generated, which specify the next action based on the previous actions and current observations.  The arrow signifies the transformation process carried out by Synatra.", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_24_1.jpg", "caption": "Figure 5: The rendered generated HTML between step 2 and step 3, where step 2 is goto (\"Google Chat\") and step 3 is click(\"jane.doe@gmail.com\"). The concrete element to interact with is tagged with id=\"next-action-target-element\".", "description": "This figure shows the generated HTML code snippet, created by the Synatra model, that represents the webpage's state between two consecutive actions: going to the Google Chat settings page and then selecting a specific Gmail account.  The key element relevant to the next action (selecting the account) has the id \"next-action-target-element\" to illustrate how Synatra generates grounded actions and observations.", "section": "3.2 Synthesizing from Text Procedural Knowledge with Generative Environment"}, {"figure_path": "KjNEzWRIqn/figures/figures_25_1.jpg", "caption": "Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.", "description": "This figure illustrates the Synatra approach which transforms indirect knowledge into direct demonstrations for training digital agents.  The left side shows the indirect knowledge sources: tutorials for human consumption and randomly sampled observations without associated tasks and actions. The right side shows the generated direct demonstrations which specify the immediate next actions based on the previous actions and current observations.", "section": "3 Scalable Demonstration Synthesis for Digital Agents"}, {"figure_path": "KjNEzWRIqn/figures/figures_26_1.jpg", "caption": "Figure 6: An example task where Synatra-CodeLlama is successful by paying more attention to the displayed web page while GPT-4-turbo is not.", "description": "This figure shows a comparison between Synatra-CodeLlama and GPT-4-turbo's performance on a web-based task.  Both models attempt to find all issues labeled as \"bug\" on a GitLab issues dashboard. GPT-4-turbo incorrectly tries to use the search box with the keyword \"type:bug\", which is ineffective.  In contrast, Synatra-CodeLlama correctly identifies and clicks a link directly displaying the relevant issues, highlighting its superior ability to process information from the web page.", "section": "I Case Study"}, {"figure_path": "KjNEzWRIqn/figures/figures_26_2.jpg", "caption": "Figure 7: An example task where Synatra-CodeLlama is successful paying more attention to the displayed web page while GPT-4-turbo is not.", "description": "This figure shows a comparison of how Synatra-CodeLlama and GPT-4-turbo approached the task of finding the number of commits made by a specific person on a certain date on a GitLab webpage.  Synatra-CodeLlama correctly identifies and clicks the relevant link showing the required information, whereas GPT-4-turbo attempts a less effective search strategy, highlighting Synatra-CodeLlama's superior ability to accurately interpret webpage content and select appropriate actions.", "section": "I Case Study"}, {"figure_path": "KjNEzWRIqn/figures/figures_27_1.jpg", "caption": "Figure 8: An example task where Synatra-CodeLlama is successful while GPT-4-turbo is not.", "description": "This figure shows a comparison between GPT-4-turbo and Synatra-CodeLlama in completing a specific task.  Both models have correctly entered the start and end dates in a form, but GPT-4-turbo incorrectly predicts that the next step is to re-enter the starting date.  Synatra-CodeLlama, however, accurately interprets the status of the web page and executes the correct action. This highlights Synatra-CodeLlama's superior ability to interpret context and execute actions effectively.", "section": "6.2 Analysis"}, {"figure_path": "KjNEzWRIqn/figures/figures_27_2.jpg", "caption": "Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings", "description": "This figure visualizes the embeddings of task intents and accessibility trees using t-SNE.  The left plot shows the distribution of task intents, demonstrating that intents generated from random observations have broader coverage than those from tutorials.  The right plot visualizes the accessibility tree embeddings, showing significant overlap between observations from tutorials and real web pages from both random observations and the Mind2Web dataset. This suggests that the synthetic data generated by the method covers a similar range of tasks and webpage types as real-world data.", "section": "4 Data Statistics"}]