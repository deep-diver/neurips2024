[{"figure_path": "3JwMwL8i5f/tables/tables_7_1.jpg", "caption": "Table 1: Class-conditional image generation on ImageNet 256 \u00d7 256 and ImageNet 512 \u00d7 512. We report training epochs, number of parameters (#Params), GFLOPs, and FID-50K with and without Classifier-Free Guidance (CFG). Best results are marked in bold.", "description": "This table presents a comparison of different diffusion models on the ImageNet dataset for image generation at resolutions of 256x256 and 512x512.  The models are evaluated based on training epochs, the number of parameters, computational cost (GFLOPs), and the Fr\u00e9chet Inception Distance (FID) score, a metric of image quality.  FID scores are reported both with and without classifier-free guidance (CFG), which is a technique to improve the quality of generated images. The best results for each metric are highlighted in bold.", "section": "5.2 State-of-the-Art Diffusion Models"}, {"figure_path": "3JwMwL8i5f/tables/tables_8_1.jpg", "caption": "Table 2: Ablation study. Beginning with the baseline, we verify the effectiveness of each component.", "description": "This table presents the ablation study results, showing the impact of different components (AdaLN-Zero, TD-LN, Multi-branch, GLU, and Multi-scale Loss) on the FID score and the number of parameters of the DiMR model.  The baseline is U-ViT-M/4. Each row shows the FID and parameter count for a model that includes the checked components.", "section": "5.4 Ablation Studies"}, {"figure_path": "3JwMwL8i5f/tables/tables_13_1.jpg", "caption": "Table 4: DiMR family. The specific configuration of a DiMR variant is determined by the hyperparameters R (number of branches), N (number of layers per branch), and D (hidden size per branch).", "description": "This table shows the different DiMR model variants (DiMR-M, DiMR-L, DiMR-XL, DiMR-G) and their configurations.  Each variant is defined by three hyperparameters: R (the number of branches in the multi-resolution network), N (a tuple of R numbers specifying the number of layers in each branch), and D (a tuple of R numbers specifying the hidden size in each branch).  The input size and latent size (for latent diffusion models) are also listed for each variant. The total number of parameters for each model is included.", "section": "B DiMR Model Variants"}, {"figure_path": "3JwMwL8i5f/tables/tables_14_1.jpg", "caption": "Table 5: Experimental setup of DiMR. Experimental settings for all DiMR variants, including model architectures, training hyperparameters, training costs, and sampler information.", "description": "This table details the experimental setup used for different variants of the DiMR model.  It covers aspects like model architecture (number of branches, layers, dimensions, etc.), training hyperparameters (batch size, optimizer, learning rate, weight decay, etc.), hardware resources (number of A100 GPUs), training time, and sampler details (sampler used and number of sampling steps).  The table provides a comprehensive overview of the different configurations and parameters employed during the training process for each DiMR model variant.", "section": "5 Experimental Results"}, {"figure_path": "3JwMwL8i5f/tables/tables_15_1.jpg", "caption": "Table 6: Class-conditional image generation on ImageNet 64 \u00d7 64 (w/o classifier-free guidance). Metrics include Fr\u00e9chet Inception Distance (FID), Inception Score (IS), Precision, and Recall, where \u201c\u2193\u201d or \u201c\u2191\u201d indicate whether lower or higher values are better, respectively. \"Type\": the type of the generative model. \"Epoch\": the number of epochs trained on ImageNet [6]. \"#Params\": the number of parameters in the model. \u201c#Gflops\": the computational cost. \u201cDiff.\u201d: Diffusion models.", "description": "This table presents a comparison of different models' performance on ImageNet 64x64 image generation without classifier-free guidance.  It shows the FID, IS, precision, and recall scores, along with the model type, number of epochs trained, number of parameters, and computational cost (GFLOPs).  Lower FID scores indicate better image quality, while higher IS, precision, and recall scores are preferred.", "section": "5.2 State-of-the-Art Diffusion Models"}, {"figure_path": "3JwMwL8i5f/tables/tables_15_2.jpg", "caption": "Table 1: Class-conditional image generation on ImageNet 256 \u00d7 256 and ImageNet 512 \u00d7 512. We report training epochs, number of parameters (#Params), GFLOPs, and FID-50K with and without Classifier-Free Guidance (CFG). Best results are marked in bold.", "description": "This table presents a comparison of different diffusion models on ImageNet 256x256 and 512x512 datasets in terms of FID scores (with and without classifier-free guidance), the number of parameters, and GFLOPs. It shows that the proposed DiMR models achieve state-of-the-art performance in terms of FID scores, highlighting their efficiency and effectiveness in high-fidelity image generation.", "section": "5.2 State-of-the-Art Diffusion Models"}, {"figure_path": "3JwMwL8i5f/tables/tables_16_1.jpg", "caption": "Table 8: Class-conditional image generation on ImageNet 512 \u00d7 512 (with classifier-free guidance). Metrics include Fr\u00e9chet Inception Distance (FID), Inception Score (IS), Precision, and Recall, where \u201c\u2193\u201d or \u201c\u2191\u201d indicate whether lower or higher values are better, respectively. We report results of GAN-based models (GAN), BERT-style masked-prediction models (Mask.), autoregressive models (AR), visual autoregressive models (VAR), and diffusion based models (Diff.). \"Type\": the type of the generative model. \"Epoch\": the number of epochs trained on ImageNet [6]. \"#Params\": the number of parameters in the model. \u201c#Gflops\": the computational cost.", "description": "This table compares the performance of various generative models on the ImageNet 512x512 dataset when using classifier-free guidance.  It presents key metrics such as FID, IS, Precision, and Recall, along with model type, training epochs, the number of parameters, and computational cost (GFLOPs).  It allows for a comparison of different model architectures and training strategies on a challenging high-resolution image generation task.", "section": "5.2 State-of-the-Art Diffusion Models"}]