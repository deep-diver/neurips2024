[{"figure_path": "nJKfNiEBvq/tables/tables_7_1.jpg", "caption": "Table 1: Replace the generative model of CausalNL and InstanceGM with ours. Experiments are on CIFAR-10.", "description": "This table compares the classification accuracy of three models on the CIFAR-10 dataset with instance-dependent label noise at different noise rates (10%, 20%, 30%, 40%, 50%).  The models compared are CausalNL (original), CausalNL' (with the proposed generative model), InstanceGM (original), and InstanceGM' (with the proposed generative model).  The results demonstrate the improvement in accuracy achieved by replacing the original generative models of CausalNL and InstanceGM with the proposed generative model in this paper.", "section": "4.2 Demonstrate the effectiveness of the proposed data-generating process"}, {"figure_path": "nJKfNiEBvq/tables/tables_7_2.jpg", "caption": "Table 2: Replace the generative model of CausalNL and InstanceGM with ours. Experiments are on CIFAR-100.", "description": "This table presents the classification accuracy results on the CIFAR-100 dataset.  The original CausalNL and InstanceGM methods are compared to modified versions (CausalNL' and InstanceGM') where the generative model has been replaced with the proposed model from the paper.  The results are shown for different noise rates (IDN-10%, IDN-20%, IDN-30%, IDN-40%, IDN-50%), demonstrating the improved performance of the modified methods.  The numbers represent the average accuracy and standard deviation across multiple trials.", "section": "4.2 Demonstrate the effectiveness of the proposed data-generating process"}, {"figure_path": "nJKfNiEBvq/tables/tables_8_1.jpg", "caption": "Table 3: Means and standard deviations (percentage) of classification accuracy on Fashion-MNIST.", "description": "This table presents the classification accuracy results on the Fashion-MNIST dataset with different levels of instance-dependent label noise (IDN).  The results are shown for five different noise rates (10%, 20%, 30%, 40%, and 50%).  Multiple methods are compared, including the proposed CSGN method and several baselines (CE, MentorNet, Co-Teaching, Reweight, Forward, PTD, CausalNL, CCR, MEIDTM, BLTM, NPC, RENT, DivideMix, and SOP).  The mean accuracy and standard deviation are reported for each method and noise level.  This allows for a comparison of the performance of different algorithms in handling label noise at varying levels of severity.", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}, {"figure_path": "nJKfNiEBvq/tables/tables_8_2.jpg", "caption": "Table 4: Means and standard deviations (percentage) of classification accuracy on CIFAR-10.", "description": "This table presents the mean and standard deviation of classification accuracy for different label noise methods on the CIFAR-10 dataset.  The results are broken down by different levels of label noise (IDN-10% to IDN-50%), allowing for a comparison of method performance under varying noise conditions.  The table includes results for several baselines as well as the proposed CSGN method.", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}, {"figure_path": "nJKfNiEBvq/tables/tables_9_1.jpg", "caption": "Table 5: Means and standard deviations (percentage) of classification accuracy on and CIFAR-100.", "description": "This table presents the classification accuracy results on the CIFAR-100 dataset under different instance-dependent label noise levels (IDN-10%, IDN-20%, IDN-30%, IDN-40%, IDN-50%).  The performance of several methods, including the proposed CSGN method, is compared against various baseline approaches.  The mean and standard deviation of the accuracy are reported for each method and noise level.  This allows for a quantitative comparison of how well different approaches handle varying levels of noisy labels in the CIFAR-100 image classification task.", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}, {"figure_path": "nJKfNiEBvq/tables/tables_9_2.jpg", "caption": "Table 6: Means and standard deviations (percentage) of classification accuracy on CIFAR-N.", "description": "This table presents the mean and standard deviation of classification accuracy for different methods on the CIFAR-N dataset.  CIFAR-N is a real-world dataset with various types and rates of label noise. The results are broken down by noise type (Worst, Aggregate, Random 1, Random 2, Random 3, Noisy) and show how each method performs under different noise conditions.", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}, {"figure_path": "nJKfNiEBvq/tables/tables_9_3.jpg", "caption": "Table 7: Test accuracy of CSGN on the WebVision validation set and the ImageNet ILSVRC12 validation set.", "description": "This table presents the top-1 and top-5 accuracy results of the CSGN model and several baseline methods on the WebVision validation set and the ImageNet ILSVRC12 validation set.  It showcases the performance comparison on a real-world, large-scale image dataset, demonstrating CSGN's effectiveness in handling noisy labels and achieving state-of-the-art results.", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}, {"figure_path": "nJKfNiEBvq/tables/tables_19_1.jpg", "caption": "Table 8: Ablation study for warmup strategies. CSGN-WOSM indicates the version of CSGN without warmup using the semi-supervised learning technique. Experiments on CIFAR-10.", "description": "This table presents the results of an ablation study comparing the performance of the CSGN model with and without a semi-supervised learning warmup phase. The study investigates the impact of removing the semi-supervised learning warmup from the CSGN method, replacing it with a regular early-stopping approach instead. The results show that the CSGN model retains effectiveness even without the semi-supervised learning warmup.", "section": "I Ablation Study"}, {"figure_path": "nJKfNiEBvq/tables/tables_19_2.jpg", "caption": "Table 9: Ablation study for ELBO loss and LM. Experiments are on CIFAR-10.", "description": "This table presents the results of an ablation study conducted on the CIFAR-10 dataset to evaluate the impact of removing the ELBO loss and the L_M regularization term from the CSGN model.  The table compares the performance of four different model variations: the full CSGN model, a model without the ELBO loss, a model without the L_M loss, and a model without both losses.  The performance is measured using accuracy on the CIFAR-10 dataset under different levels of instance-dependent label noise (IDN). The results show the effect of each loss term on model performance.", "section": "4.1 Experiment Setup"}, {"figure_path": "nJKfNiEBvq/tables/tables_20_1.jpg", "caption": "Table 10: CSGN works with PES. Experiments are on CIFAR-10.", "description": "This table presents a comparison of classification accuracy on the CIFAR-10 dataset using three different methods: PES, CSGN-PES, and CSGN.  The methods are evaluated under different levels of instance-dependent label noise (IDN). CSGN-PES represents a variant of CSGN that utilizes the PES algorithm for sample selection and training. The results show how each method performs with varying amounts of label noise, demonstrating the effectiveness of the CSGN approach in handling noisy labels.", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}, {"figure_path": "nJKfNiEBvq/tables/tables_20_2.jpg", "caption": "Table 11: The number and the accuracy of the selected clean samples on CIFAR-10.", "description": "This table shows the number and accuracy of the selected clean samples for CIFAR-10 dataset under different noise levels (IDN-10%, IDN-20%, IDN-30%, IDN-40%, IDN-50%).  The results demonstrate that the method is effective in selecting a large number of clean samples with high accuracy, even with increasing noise.", "section": "J The Number and the Accuracy of the Selected Clean Samples"}, {"figure_path": "nJKfNiEBvq/tables/tables_20_3.jpg", "caption": "Table 12: The number and the accuracy of the selected clean samples on CIFAR-100.", "description": "This table presents the number and accuracy of clean samples selected from the CIFAR-100 dataset using the proposed method. The results are categorized by different noise levels (IDN-10%, IDN-20%, IDN-30%, IDN-40%, and IDN-50%).  The number of samples varies with the noise level, with a decrease as noise increases, and high accuracy (above 98%) is maintained except at the highest noise rate.", "section": "4.1 Experiment Setup"}, {"figure_path": "nJKfNiEBvq/tables/tables_20_4.jpg", "caption": "Table 4: Means and standard deviations (percentage) of classification accuracy on CIFAR-10.", "description": "This table presents the classification accuracy results for different methods on the CIFAR-10 dataset with varying instance-dependent label noise ratios (IDN).  The results are averaged over multiple trials, with standard deviations reported to indicate the reliability of the results.  'CE-clean' represents the performance of a classifier trained on the clean CIFAR-10 dataset, serving as a baseline. The other methods represent various label noise learning techniques, demonstrating their comparative performance against the baseline and each other.", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}, {"figure_path": "nJKfNiEBvq/tables/tables_20_5.jpg", "caption": "Table 5: Means and standard deviations (percentage) of classification accuracy on and CIFAR-100.", "description": "This table presents the mean and standard deviation of classification accuracy for CIFAR-100 using different methods under different label noise rates (IDN-10%, IDN-20%, IDN-30%, IDN-40%, IDN-50%). The results are compared against the performance on clean data (CE-clean) and the proposed method (CSGN).", "section": "4.2 Effectiveness the Proposed Data-Generating Process"}]