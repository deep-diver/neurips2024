[{"figure_path": "nJKfNiEBvq/figures/figures_1_1.jpg", "caption": "Figure 1: The pictures contain the same noisy labels.", "description": "This figure shows two examples from the CIFAR-10N dataset. Both images are mislabeled as \"Dog\" but their true labels are \"Cat\".  The purpose of this figure is to illustrate that similar images (in this case, both containing cats with fur) might be mislabeled in the same way, implying a relationship between the noise transition matrices of these similar images. This relationship is not explicitly defined but is rather implicitly captured by learning the latent causal structure.", "section": "1 Introduction"}, {"figure_path": "nJKfNiEBvq/figures/figures_1_2.jpg", "caption": "Figure 2: Noisy data-generating processes assumed by different methods. S is the variable unrelated to the clean label Y.", "description": "This figure compares two different data-generating processes for noisy labels.  (a) shows a model where the instance X directly causes the noisy label \u1ef8.  This model is considered insufficient because in many datasets (images, videos etc.), there are often latent variables (like shape or color) that are the direct cause of the label, rather than the perceptual data itself. (b) Shows the model proposed in the paper, where latent variables Z and S generate the instance X, while Z also generates the noisy label \u1ef8. This structure captures the latent factors that cause mislabeling more effectively.", "section": "1 Introduction"}, {"figure_path": "nJKfNiEBvq/figures/figures_1_3.jpg", "caption": "Figure 3: An illustration of the noisy data-generating process with 4 latent variables.", "description": "This figure illustrates the data generating process used in the CSGN model.  It shows a directed acyclic graph (DAG) representing the causal relationships between latent variables and observed variables.  There are four latent causal variables (Z1, Z2, Z3, Z4) with directed edges indicating causal influence.  These variables are influenced by the clean label (Y) and also influence each other. The latent noise variables (N1, N2, N3, N4) introduce randomness into the generation of these causal variables.  There is an additional latent variable S that is independent of the clean label and affects the generation of the observed instance X. The noisy label \u1ef8 is generated by Z1, Z2, Z3, Z4, and influenced by Y. The solid arrows represent causal influences, and the shaded nodes represent observed variables.", "section": "3 Learning the Latent Causal Structure for Generation of Noisy Data"}, {"figure_path": "nJKfNiEBvq/figures/figures_2_1.jpg", "caption": "Figure 3: An illustration of the noisy data-generating process with 4 latent variables.", "description": "This figure shows a directed acyclic graph (DAG) representing the data generating process. The observed instance X and noisy label \u1ef8 are generated by subsets of latent causal variables Z, which have causal dependencies among them.  The variables S are other variables not influenced by the clean label Y.  The arrows indicate the causal direction, showing how the variables influence each other, while the blue arrow indicates the edge weights vary with the clean label Y. This highlights the instance-dependent nature of noise transition matrices.", "section": "3 Learning the Latent Causal Structure for Generation of Noisy Data"}, {"figure_path": "nJKfNiEBvq/figures/figures_4_1.jpg", "caption": "Figure 4: A workflow of our method. In the inference stage, a classification network is used to learn the clean labels of instances; An encoder is used to learn the causal variables. In the generation stage, different subsets of causal variables used to generate instances and noisy labels are selected by masking. Two decoders are used to generate the instances and noisy labels.", "description": "This figure illustrates the workflow of the CSGN (Causal Structure for the Generation of Noisy Data) method.  It's broken down into three stages:\n\n1. **Estimate clean labels:** A classification network, q\u03c8(Y|X), takes an instance (X) as input and outputs the estimated clean label (Y).\n2. **Inference:** An encoder, q\u03b8(Z, S|X, Y), takes the instance (X) and the estimated clean label (Y) to infer the latent causal variables (Z) and other variables unrelated to the clean label (S).\n3. **Generation:** Two decoders, pfx(X|Mx\u2299Z, S) and pfy(\u1ef8|My\u2299Z), utilize the latent causal variables (Z) and S,  masked by Mx and My respectively, to generate the instance (X) and noisy label (\u1ef8). The masks, Mx and My, select the appropriate subset of causal variables Z for generating each part.  The process is designed to model how the latent causal structure influences both the instance and the resulting noisy label.", "section": "3.1 Methodology"}, {"figure_path": "nJKfNiEBvq/figures/figures_7_1.jpg", "caption": "Figure 5: The estimation error of noise transition matrices. The datasets are MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 with the instance-dependent label noise. The error bar for standard deviation in each figure has been shaded.", "description": "This figure shows the estimation error of noise transition matrices for four different datasets (MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100) using three different methods (CSGN, BLTM, and MEIDTM). The x-axis represents the noise rate, and the y-axis represents the estimation error. The error bars indicate the standard deviation. The results show that CSGN generally has a lower estimation error compared to BLTM and MEIDTM across all datasets and noise rates.", "section": "4.3 Noise Transition Matrices Estimation Error"}, {"figure_path": "nJKfNiEBvq/figures/figures_18_1.jpg", "caption": "Figure 5: The estimation error of noise transition matrices. The datasets are MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 with the instance-dependent label noise. The error bar for standard deviation in each figure has been shaded.", "description": "This figure displays the estimation error of noise transition matrices for four different datasets (MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100) using instance-dependent label noise.  The x-axis represents the noise rate, ranging from 0.1 to 0.5. The y-axis represents the estimation error.  Each dataset has its own subplot, showing how the estimation error changes with increasing noise rate. Error bars indicating standard deviations are included for each data point. The figure visually compares the performance of the proposed method to other approaches for estimating noise transition matrices under different noise levels.", "section": "4.1 Experiment Setup"}, {"figure_path": "nJKfNiEBvq/figures/figures_19_1.jpg", "caption": "Figure 7: The test accuracy on CIFAR-10 for different numbers of causal variables under instance-dependent noise with a noise rate of 0.5. The error bar for standard deviation has been shaded.", "description": "This figure shows the results of a sensitivity analysis testing the effect of varying the number of causal variables (Z) on the test accuracy of a model trained on the CIFAR-10 dataset with instance-dependent label noise at a rate of 0.5.  The x-axis represents the number of causal variables, while the y-axis represents the test accuracy.  Error bars indicate the standard deviation. The graph shows that the model's performance is relatively insensitive to the number of causal variables within the tested range.", "section": "4.1 Experiment Setup"}, {"figure_path": "nJKfNiEBvq/figures/figures_21_1.jpg", "caption": "Figure 8: The visualization of the training data. The noise rate is dependent on feature 2. The average noise rate is 0.3.", "description": "This figure shows a scatter plot of a synthetic dataset used in the paper's experiments.  The x-axis represents Feature 1, and the y-axis represents Feature 2. Each point is colored according to its associated noise rate, which is calculated as a function of Feature 2.  The color gradient ranges from dark purple (low noise rate) to bright yellow (high noise rate). The data forms a two-armed spiral shape, indicating that the noise rate varies systematically across the dataset, rather than being randomly distributed. This specific dataset is used to evaluate the model's ability to learn the latent causal structure underlying the noise generation process, focusing on how the noise rate is linked to specific features within the data.", "section": "L Experiments on the Synthetic Dataset"}, {"figure_path": "nJKfNiEBvq/figures/figures_22_1.jpg", "caption": "Figure 9: The t-SNE visualization of the similarity of the learned noise transition. The pairs of data points with the same predicted clean label are marked with the same number. The distances between two data points represent the difference between the two noise transition matrices of these data points. The distance between the pair with the same number is different in the two images.", "description": "This figure uses t-SNE to visualize the similarity of learned noise transition matrices by CSGN and MEIDTM.  It compares the distances between pairs of data points with the same predicted clean labels.  The key observation is that the distances between these pairs differ significantly between the two methods, indicating that CSGN learns a different similarity structure compared to MEIDTM, which is based on instance-dependent transition matrices.", "section": "M Comparison of the Noise Transition Matrix"}]