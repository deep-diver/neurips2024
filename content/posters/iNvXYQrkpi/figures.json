[{"figure_path": "iNvXYQrkpi/figures/figures_0_1.jpg", "caption": "Figure 1: Our Streamlined Inference is a training-free inference framework for video diffusion models that can reduce the computation and peak memory cost without sacrificing the quality.", "description": "This figure is a visual comparison of video generation results between the baseline method (Animatediff) and the proposed method (Streamlined Inference).  It shows a sequence of frames depicting a large man riding a small motorcycle. The top row displays the output from Animatediff, while the bottom row shows the results from the Streamlined Inference method.  The caption highlights the key advantage of the proposed method: significantly reduced peak memory usage (3.3x less) and inference latency (1.6x less) without compromising the visual quality of the generated video.", "section": "Abstract"}, {"figure_path": "iNvXYQrkpi/figures/figures_1_1.jpg", "caption": "Figure 2: Comparison on Animatediff and SVD inference using our Streamlined Inference. Memory requirement is crucial as \"Out of Memory\" errors prevent the GPU from performing inference.", "description": "This figure compares the peak memory usage and inference latency of the original Animatediff and SVD models with the proposed Streamlined Inference method.  The left panel (a) shows a bar chart illustrating the significant reduction in peak memory achieved by the Streamlined Inference method for both models at 576x1024 resolution.  The right panel (b) presents bar charts depicting the reduction in inference time (latency) achieved with the new method for both models. The key takeaway is that Streamlined Inference drastically reduces memory requirements, making video generation feasible on consumer-grade GPUs without sacrificing significant speed.", "section": "1 Introduction"}, {"figure_path": "iNvXYQrkpi/figures/figures_3_1.jpg", "caption": "Figure 3: The quality results of our method and na\u00efve slicing. Note that na\u00efve slicing will incur unpleasant artifacts due to lack of temporal correction by fewer frames.", "description": "This figure shows a comparison of video generation results between the proposed Streamlined Inference method and a naive slicing approach.  The left side shows a rocket launch, while the right depicts a woman blowing snow.  The \"Ours\" row displays high-quality, temporally consistent videos generated using the Streamlined Inference method.  In contrast, the \"Naive\" row demonstrates the artifacts that result from a naive slicing approach, which fails to maintain temporal coherence, resulting in less realistic and visually jarring video sequences. This highlights the importance of the proposed method's ability to preserve temporal consistency.", "section": "4 Streamlined Inference Framework"}, {"figure_path": "iNvXYQrkpi/figures/figures_4_1.jpg", "caption": "Figure 4: Overview of Operator Grouping with Pipeline in our framework.", "description": "This figure illustrates the three core components of the Streamlined Inference framework: Feature Slicer, Operator Grouping, and Pipeline. (a) shows the original, non-optimized operator sequence. (b) demonstrates the effect of Feature Slicing and Operator Grouping, where the input feature is partitioned, and operations are grouped to reduce memory usage. (c) further enhances performance with pipelining, enabling parallel processing of multiple sub-features within the same operator group.", "section": "4 Streamlined Inference Framework"}, {"figure_path": "iNvXYQrkpi/figures/figures_6_1.jpg", "caption": "Figure 5: The high similarity of output features after temporal layers in U3 between each timestep.", "description": "This figure shows heatmaps visualizing the similarity between output features of consecutive timesteps in the temporal layers of a U-Net architecture used in video diffusion models. The heatmaps indicate high similarity between adjacent steps, particularly in later steps, suggesting the potential for optimization by reusing or skipping unnecessary computations.  The (a) SVD and (b) AnimateDiff subfigures showcase the similarity patterns for different models, highlighting the varying degrees of similarity in different parts of the models.", "section": "4.3.1 Similarity of Temporal Features between Steps"}, {"figure_path": "iNvXYQrkpi/figures/figures_6_2.jpg", "caption": "Figure 6: Illustration of Step Rehash. Computation in grey areas are skipped.", "description": "This figure illustrates the Step Rehash method used to optimize the iterative denoising process in video diffusion models.  The figure shows two consecutive steps, s and s+1, in the video generation process.  The U-Net architecture is depicted, showing multiple down-sampling (D) and up-sampling (U) blocks.  In Step s+1, several computation blocks are shaded in gray, indicating that their computations are skipped in Step Rehash, as they reuse features from the previous step, reducing computational cost and time. The output feature of the temporal layer is highlighted in light blue.", "section": "4.3 Step Rehash"}, {"figure_path": "iNvXYQrkpi/figures/figures_8_1.jpg", "caption": "Figure 7: Quality evaluation of using our method on baseline models. The results show that our method can be generally applied to various video diffusion models and achieve competitive results.", "description": "This figure compares the visual quality of videos generated by the proposed Streamlined Inference method with the original baseline models (AnimateDiff, SVD, and SVD-XT).  It demonstrates that the proposed method produces high-quality videos comparable to the original methods across various video scenarios, including an owl and danger, children walking, and a cityscape at night.", "section": "5.4 Quality results"}, {"figure_path": "iNvXYQrkpi/figures/figures_12_1.jpg", "caption": "Figure A1: GPU memory snapshot of active cached segment timeline for Stable Video Diffusion with 14 frames 512 \u00d7 512", "description": "This figure shows GPU memory usage over time during the inference process for Stable Video Diffusion model with 14 frames and 512x512 resolution.  It compares memory usage with and without the proposed Streamlined Inference method. The graph illustrates how the Streamlined Inference significantly reduces the peak memory consumption during video generation.", "section": "Appendix A Memory Snapshot during inference"}, {"figure_path": "iNvXYQrkpi/figures/figures_13_1.jpg", "caption": "Figure A2: Similarity maps of different temporal layers in up_blocks.0.resnets.", "description": "This figure shows similarity maps of different temporal layers within the up_blocks.0.resnets of a U-Net architecture used in video diffusion models.  Each subplot represents a different layer, visualizing the similarity between features at various timesteps. The color intensity represents the similarity strength, with warmer colors indicating higher similarity. The purpose is to demonstrate the varying degrees of similarity between features across different layers at consecutive timesteps, which is crucial for the effectiveness of Step Rehash in Streamlined Inference. Higher similarity allows skipping computations in Step Rehash, enhancing efficiency.", "section": "Appendix C"}, {"figure_path": "iNvXYQrkpi/figures/figures_13_2.jpg", "caption": "Figure A3: Visual comparison of our method with DeepCache.", "description": "This figure shows a visual comparison of the results obtained using the proposed method and DeepCache. The comparison is done for three different video samples. For each sample, it includes the generated video frames using DeepCache and the proposed method. The figure visually demonstrates that the proposed method generates videos with better quality and more vivid details compared to DeepCache.", "section": "Appendix D"}, {"figure_path": "iNvXYQrkpi/figures/figures_14_1.jpg", "caption": "Figure 3: The quality results of our method and na\u00efve slicing. Note that na\u00efve slicing will incur unpleasant artifacts due to lack of temporal correction by fewer frames.", "description": "This figure compares the visual quality of video generated using the proposed Streamlined Inference method and a naive slicing method. The naive method involves processing video clips individually without considering temporal dependencies, resulting in noticeable artifacts and motion inconsistencies.  The Streamlined Inference method, on the other hand, produces smoother and more coherent videos.", "section": "4 Streamlined Inference Framework"}, {"figure_path": "iNvXYQrkpi/figures/figures_14_2.jpg", "caption": "Figure 3: The quality results of our method and na\u00efve slicing. Note that na\u00efve slicing will incur unpleasant artifacts due to lack of temporal correction by fewer frames.", "description": "This figure compares the visual quality of videos generated using the proposed Streamlined Inference method and a naive slicing approach.  The naive slicing method, which processes the video clip by clip, results in noticeable artifacts due to its failure to correct for temporal inconsistencies between frames. In contrast, the proposed method maintains high video quality, illustrating the effectiveness of its temporal feature preservation.", "section": "4 Streamlined Inference Framework"}, {"figure_path": "iNvXYQrkpi/figures/figures_14_3.jpg", "caption": "Figure 3: The quality results of our method and na\u00efve slicing. Note that na\u00efve slicing will incur unpleasant artifacts due to lack of temporal correction by fewer frames.", "description": "This figure compares the visual quality of video generated using the proposed Streamlined Inference method versus a naive slicing approach.  The naive slicing method, which processes the video clip by clip without considering temporal dependencies, results in artifacts and motion inconsistencies.  In contrast, the Streamlined Inference method maintains high visual quality and temporal coherence, as demonstrated by the smooth and realistic motion in the generated video.", "section": "4 Streamlined Inference Framework"}, {"figure_path": "iNvXYQrkpi/figures/figures_14_4.jpg", "caption": "Figure 3: The quality results of our method and na\u00efve slicing. Note that na\u00efve slicing will incur unpleasant artifacts due to lack of temporal correction by fewer frames.", "description": "This figure compares the visual quality of video generated by the proposed Streamlined Inference method against a naive slicing approach.  The naive approach involves processing video clips frame-by-frame, neglecting temporal coherence, which results in noticeable artifacts. In contrast, the Streamlined Inference method preserves temporal relationships, leading to smoother and more coherent video generation, which is clearly shown in the figure.", "section": "4 Streamlined Inference Framework"}, {"figure_path": "iNvXYQrkpi/figures/figures_15_1.jpg", "caption": "Figure A3: Visual comparison of our method with DeepCache.", "description": "This figure shows a visual comparison of the results obtained using the proposed method and DeepCache on several video samples.  It aims to demonstrate the visual quality differences between the two methods. Each row shows a video sample, with the upper row showcasing the output from DeepCache and the lower row the result using the authors' Streamlined Inference. The visual differences may include aspects of detail, artifacts, temporal consistency, or other visual characteristics.", "section": "Appendix D Visual comparison with DeepCache"}, {"figure_path": "iNvXYQrkpi/figures/figures_15_2.jpg", "caption": "Figure A5: Quality evaluation of using our method on baseline models.", "description": "This figure presents a qualitative comparison of video generation results between the original AnimateDiff model and the proposed Streamlined Inference method.  It shows several examples of videos generated with both methods using the same text prompts. The purpose is to visually demonstrate the high quality of videos produced by the Streamlined Inference method, which maintains quality while significantly reducing peak memory and computation costs.", "section": "Appendix"}, {"figure_path": "iNvXYQrkpi/figures/figures_15_3.jpg", "caption": "Figure 7: Quality evaluation of using our method on baseline models. The results show that our method can be generally applied to various video diffusion models and achieve competitive results.", "description": "This figure presents a qualitative comparison of video generation results between the proposed Streamlined Inference method and baseline video diffusion models (AnimateDiff).  The image shows example video frames generated from several text prompts. The results demonstrate the comparable quality of videos generated by the proposed method, with reduced memory and computational overhead.", "section": "5.4 Quality results"}, {"figure_path": "iNvXYQrkpi/figures/figures_15_4.jpg", "caption": "Figure 7: Quality evaluation of using our method on baseline models. The results show that our method can be generally applied to various video diffusion models and achieve competitive results.", "description": "This figure shows a comparison of video generation results between the proposed method (Ours) and the original method (AnimateDiff).  The results illustrate that the new method can produce videos of comparable quality to the original method, suggesting that it is a viable alternative that improves on efficiency while maintaining comparable quality.", "section": "5.4 Quality results"}, {"figure_path": "iNvXYQrkpi/figures/figures_16_1.jpg", "caption": "Figure 7: Quality evaluation of using our method on baseline models. The results show that our method can be generally applied to various video diffusion models and achieve competitive results.", "description": "This figure showcases a qualitative comparison of video generation results between the proposed Streamlined Inference method and baseline models (AnimateDiff).  Multiple video examples are presented, each with a text prompt describing the desired scene.  The goal is to demonstrate that the Streamlined Inference approach produces videos of comparable quality to those generated by existing, more computationally expensive methods.  The figure aims to visually support the paper's claim that the new method does not sacrifice video quality while achieving significant improvements in terms of memory usage and computational efficiency.", "section": "5.4 Quality results"}, {"figure_path": "iNvXYQrkpi/figures/figures_16_2.jpg", "caption": "Figure 7: Quality evaluation of using our method on baseline models. The results show that our method can be generally applied to various video diffusion models and achieve competitive results.", "description": "This figure displays a comparison of video generation results between the original AnimateDiff model and the proposed Streamlined Inference method.  The images demonstrate that the Streamlined Inference method produces videos of comparable quality to the original AnimateDiff, despite a significant reduction in computational cost and memory usage, as highlighted earlier in the paper.  The examples show diverse scenes, highlighting the method's versatility.", "section": "5.4 Quality results"}, {"figure_path": "iNvXYQrkpi/figures/figures_16_3.jpg", "caption": "Figure 3: The quality results of our method and na\u00efve slicing. Note that na\u00efve slicing will incur unpleasant artifacts due to lack of temporal correction by fewer frames.", "description": "This figure compares the visual quality of videos generated using the proposed Streamlined Inference method and a naive slicing approach.  The naive slicing method, which processes video clips independently, introduces artifacts and motion inconsistencies due to the lack of temporal context across frames. The Streamlined Inference method, on the other hand, maintains consistent visual quality by properly handling temporal dependencies.", "section": "4 Streamlined Inference Framework"}, {"figure_path": "iNvXYQrkpi/figures/figures_16_4.jpg", "caption": "Figure A5: Quality evaluation of using our method on baseline models.", "description": "This figure presents a visual comparison of video generation results between the baseline AnimateDiff model and the proposed method.  For several different video prompts, both methods generated short video clips.  The images show that the proposed method produces results with slightly improved visual quality compared to AnimateDiff, though the differences are subtle. The improved visual quality is particularly noticeable in the detail and clarity of the objects and background.", "section": "Appendix"}, {"figure_path": "iNvXYQrkpi/figures/figures_17_1.jpg", "caption": "Figure A7: Quality evaluation of using our method on baseline models.", "description": "This figure presents a qualitative comparison of video generation results between the proposed Streamlined Inference method and the baseline AnimateDiff model.  For several different video prompts, it shows the output videos generated by each approach. The purpose is to visually demonstrate the quality of videos produced by Streamlined Inference, showcasing its ability to generate high-quality results comparable to or exceeding those of AnimateDiff, while requiring less computation and memory.", "section": "Appendix"}, {"figure_path": "iNvXYQrkpi/figures/figures_18_1.jpg", "caption": "Figure A8: Quality evaluation of using our method on baseline models.", "description": "This figure shows a qualitative comparison of video generation results between the proposed Streamlined Inference method and baseline methods (SVD and SVD-XT).  Each row presents results for a different video generation task (different prompts), with the left column showing results from the baseline method and the right column showing the results from Streamlined Inference. The aim is to visually demonstrate the comparable quality of videos generated by the proposed method while achieving significant memory and computational savings. The visual quality appears very similar between the two approaches for each task.", "section": "Appendix"}]