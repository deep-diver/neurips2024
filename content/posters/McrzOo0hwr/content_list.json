[{"type": "text", "text": "Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiqin Lv Qi Wang\u2217 Dong Liang\u2217 Zheng Xie\u2217 College of Science, National University of Defense Technology Changsha, China Email to: {lvyiqin98,wangqi15,dongliangnudt,xiezheng81}@nudt.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Meta learning is a promising paradigm in the era of large models and task distributional robustness has become an indispensable consideration in real-world scenarios. Recent advances have examined the effectiveness of tail task risk minimization in fast adaptation robustness improvement [1]. This work contributes to more theoretical investigations and practical enhancements in the field. Specifically, we reduce the distributionally robust strategy to a max-min optimization problem, constitute the Stackelberg equilibrium as the solution concept, and estimate the convergence rate. In the presence of tail risk, we further derive the generalization bound, establish connections with estimated quantiles, and practically improve the studied strategy. Accordingly, extensive evaluations demonstrate the significance of our proposal and its scalability to multimodal large models in boosting robustness. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The past few years have witnessed a surge of research interest in meta learning due to its great potential in the academia and industry [2\u20135]. By leveraging previous experience, such a learning paradigm can extract knowledge as priors and empower learning models with adaptability to unseen tasks from a few examples [6]. ", "page_idx": 0}, {"type": "text", "text": "Nevertheless, the investigation of the robustness needs to be more comprehensive from the task distribution perspective. In particular, the recently developed large models heavily rely on the fewshot learning capability and demand robustness of prediction in risk-sensitive scenarios [7]. For example, when the GPT-like dialogue generation system [8\u201310] comes into medical consultancy domains, imprecise answers can cause catastrophic consequences to patients, families, and even societies in real-world scenarios. In light of these considerations, it is desirable to watch adaptation differences across tasks when deploying meta learning models and promote task robustness study for meeting substantial practical demands. ", "page_idx": 0}, {"type": "text", "text": "Recently, Wang et al. [1] proposes to increase task distributional robustness via employing the tail risk minimization principle [11] for meta learning. In circumventing the optimization intractability in the presence of nonconvex risk functions, a two-stage optimization strategy is adopted as the heuristic to solve the problem. In brief, the strategy consists of two phases in iteration, respectively: (i) estimating the risk quantile $\\operatorname{VaR}_{\\alpha}$ [11] with the crude Monte Carlo method [12] in the task space; (ii) updating the meta learning model parameters from the screened subset of tasks. Such a strategy is simple in implementation, with an improvement guarantee under certain conditions, and empirically shows improved robustness when faced with task distributional shifts. Despite these advances, there remain several unresolved theoretical or practical issues in the field. ", "page_idx": 0}, {"type": "text", "text": "Existing limitations. This paper also works on the robustness of fast adaptation in the task space and tries to flil gaps in [1]. Theoretically, we notice that in [1] (i) there constitutes no notion of solutions, (ii) it lacks an algorithmic understanding of the two-stage optimization strategy, (iii) the analysis on generalization capability is ignored in the tail risk of tasks. Empirically, the use of the crude Monte Carlo might be less efficient in quantile estimates and suffers from a higher approximation error of the $\\mathrm{VaR}_{\\alpha}$ , degrading the adaptation robustness. These bottlenecks may weaken the versatility of the two-stage optimization strategy\u2019s use in practice and require more understanding before deployment. ", "page_idx": 1}, {"type": "text", "text": "Primary contributions. In response to the above-mentioned concerns, we propose translating the two-stage optimization strategy for distributionally robust meta learning [1] into a max-min optimization problem [13]. Intrinsically, this work models the optimization steps as a Stackelberg game, and task selection and the sub-gradient optimizer work as the leader and follower players in decision-making, respectively. The theoretical understanding is from two aspects: ", "page_idx": 1}, {"type": "text", "text": "1. We constitute the local Stackelberg equilibrium as a solution concept, estimate the convergence rate, and characterize the asymptotic behavior in learning dynamics.   \n2. We derive the generalization bound in the presence of the tail task risk, which connects quantile estimates with fast adaptation capability in unseen tasks. ", "page_idx": 1}, {"type": "text", "text": "Meanwhile, the empirical influence of $\\operatorname{VaR}_{\\alpha}$ estimators is examined, and we advance meta learners\u2019 robustness by comprising more accurate quantile estimators. ", "page_idx": 1}, {"type": "text", "text": "2 Literature Review ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Meta Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Meta learning, or learning to learn, is an increasingly popular paradigm to distill knowledge from prior experience to unseen scenarios with a few examples [6]. Various meta learning methods have emerged in the past decade, and this section overviews some dominant families. ", "page_idx": 1}, {"type": "text", "text": "The context-based methods mainly use the encoder-decoder structure and represent tasks by latent variables. Typical ones are in the form of the conditional exchangeable stochastic processes and learn function distributions, such as neural processes [14], conditional neural processes [15] and their extensions [16\u201324]. The optimization-based approaches seek the optimal meta initialization of model parameters and update models from a few examples. Widely known are model agnostic meta learning [25] and related variants [26\u201329], such as MetaCurvature [30], which learns curvature information and transforms gradients in the inner-loop optimization. The metrics-based methods represent tasks in geometry and perform well in few-shot image classification [31\u201333]. For example, MetaOptNet [34] proposes to learn embeddings under a linear classifier and achieve SOTA few-shot classification performance. There also exist other methods, e.g., hyper-networks [35, 36], memory-augmented networks [37] and recurrent models [38]. ", "page_idx": 1}, {"type": "text", "text": "2.2 Robustness & Generalization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The robustness concept in meta learning attracts recent attention, particularly when deploying large models in real-world scenarios. Admittedly, previous literature works have investigated the scenarios where the meta dataset\u2019s input is corrupted [39, 40] or the model parameter is perturbed [29]. Studies regarding the fast adaptation robustness in task distribution remain limited. Wang et al. [41] explicitly generates task distribution for robust adaptation. Collins et al. [42] employs the worst-case optimization for promoting MAML\u2019s robustness to extreme worst cases. With the help of tail risk minimization, Wang et al. [1] proposes two-stage optimization strategies to robustify the fast adaptation. This work centers around [1] but stresses more theoretical understandings and performance improvement points. ", "page_idx": 1}, {"type": "text", "text": "As for generalization capability, there are a couple of works in meta learning. Chen et al. [43] exploits the information theory to derive the bound for MAML\u2019s like methods. From the data splitting perspective, Bai et al. [44] formulates the theoretical foundation and connects it to optimality. In [45], an average risk bound is constructed with the bias for improving performance. Importantly, prior work [1] ignores the generalization analysis, and meta learner\u2019s generalization in tail risk cases has not been studied in the literature. ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "General notations. Let $p(\\tau)$ be the task distribution in meta learning. We respectively express the task space and the model parameter space as $\\Omega_{\\tau}$ and $\\Theta$ . We denote the complete task set by $\\tau$ and refer to $\\mathfrak{D}_{\\tau}$ as the meta dataset. ", "page_idx": 2}, {"type": "text", "text": "For instance, $\\mathfrak{D}_{\\tau}$ comprises a collection of data points $\\{(x_{i},y_{i})\\}_{i=1}^{n+m}$ in regression. $\\mathfrak{D}_{\\tau}$ is ususally prepared into the support set $\\mathfrak{D}_{\\tau}^{S}$ for skill transfer and the query set $\\mathfrak{D}_{\\tau}^{Q}$ to assess adaptation performance. Take the conditional neural process [15] as an example, $\\mathfrak{D}_{\\tau}^{S}=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ works for task representation with $\\mathfrak{D}_{\\tau}^{Q}=\\{(x_{i},y_{i})\\}_{i=1}^{n+m}$ the all data points to fit in regression. ", "page_idx": 2}, {"type": "text", "text": "The meta risk function corresponds to a map $\\ell\\,:\\,\\mathfrak{D}_{\\tau}\\,\\times\\,\\Theta\\,\\mapsto\\,\\mathbb{R}^{+}$ , evaluating fast adaptation performance. Given $p(\\tau)$ and meta learning model parameters $\\theta$ , we can induce the cumulative distribution of the meta risk function value in the real space as $F_{\\ell}(l;\\theta):=\\mathbb{P}(\\{\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\leq l;\\tau\\in$ $\\mathcal{T},l\\in\\mathbb{R}^{+}\\}$ ), but there is no explicit parameterized form for $F_{\\ell}$ in practice as $F_{\\ell}$ is $\\theta$ -dependent. ", "page_idx": 2}, {"type": "text", "text": "When it comes to the tail risk minimization, we commonly use the conditional value-at-risk $\\left(\\mathbf{CVaR}_{\\alpha}\\right)$ ) with the probability threshold $\\alpha\\,\\in\\,[0,1)$ . The quantile of our interest is called the value-at-risk $\\left(\\operatorname{VaR}_{\\alpha}\\right)$ [11] with the definition: $\\operatorname{VaR}_{\\alpha}\\left[\\ell(\\mathcal{T},\\theta)\\right]\\stackrel{*}{=}\\operatorname*{inf}_{l\\in\\mathbb{R}^{+}}\\left\\{l|F_{\\ell}(l;\\theta)\\geq\\alpha,\\tau\\in\\mathcal{T}\\right\\}$ . The resulting normalized cumulative distribution $F_{\\ell}^{\\alpha}(l;\\theta)$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{\\ell}^{\\alpha}(l;\\theta)=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{l<\\mathrm{VaR}_{\\alpha}[\\ell(\\mathcal{T},\\theta)]}\\\\ {\\frac{F_{\\ell}(l;\\theta)-\\alpha}{1-\\alpha},}&{l\\ge\\mathrm{VaR}_{\\alpha}[\\ell(\\mathcal{T},\\theta)].}\\end{array}\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\forall\\theta\\in\\Theta$ , the meta learning operator $\\mathcal{M}_{\\theta}$ defines: $\\mathcal{M}_{\\theta}:\\tau\\mapsto\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)$ . Accordingly, the tail risk task subspace $\\begin{array}{r}{\\Omega_{\\alpha,\\tau}:=\\bigcup_{\\ell\\geq\\mathrm{VaR}_{\\alpha}[\\ell(\\mathcal{T},\\theta)]}\\left[\\mathcal{M}_{\\theta}^{-1}(\\ell)\\right]}\\end{array}$ , with the task distribution constrained in $\\Omega_{\\alpha,\\tau}$ by $p_{\\alpha}(\\tau;\\theta)$ . Please refer to Fig. 7 for illustrations of risk concepts. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. To proceed, we retain most assumptions from [1] for theoretical analysis, including: ", "page_idx": 2}, {"type": "text", "text": "1. The meta risk function $\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)$ is $\\beta_{\\tau}$ -Lipschitz continuous w.r.t. $\\theta$ ;   \n2. The cumulative distribution $F_{\\ell}(l;\\theta)$ is $\\beta_{\\ell}$ -Lipschitz continuous w.r.t. $l$ , and the normalized density function $p_{\\alpha}(\\tau;\\theta)$ is $\\beta_{\\theta}$ -Lipschitz continuous w.r.t. $\\theta$ ;   \n3. For arbitrary valid $\\theta\\ \\in\\ \\Theta$ and corresponding $p_{\\alpha}(\\tau;\\theta)$ , $\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)$ is bounded: $\\operatorname*{sup}_{\\tau\\in\\Omega_{\\alpha,\\tau}}\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\leq\\mathcal{L}_{\\operatorname*{max}}$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Risk Minimization Principles ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This subsection revisits commonly used risk minimization principles in the meta learning field. ", "page_idx": 2}, {"type": "text", "text": "Expected risk minimization. The standard principle is the expected/empirical risk minimization originated from statistical learning theory [46]. It minimizes meta risk based on the sampling chance of tasks from the original task distribution: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{E}(\\theta):=\\mathbb{E}_{p(\\tau)}\\left[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Worst-case risk minimization. Noticing that the worst fast adaptation can be disastrous in some risk sensitive scenarios, Collins et al. [42] proposes to conduct the worst-case optimization in meta learning: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\tau\\in\\mathcal{T}}\\mathcal{E}_{\\mathrm{w}}(\\theta):=\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, as observed from experiments in [42], such a principle inevitably sacrifices too much average performance for gains of worst-case robustness. Meanwhile, it requires a couple of implementation tricks and specialized algorithms in stabilizing optimization. ", "page_idx": 2}, {"type": "text", "text": "Expected tail risk minimization $\\left(\\mathbf{CVaR}_{\\alpha}\\right)$ . To balance the average performance and the worst-case performance, Wang et al. [1] minimizes the expected tail risk, or equivalently $\\mathrm{CVaR}_{\\alpha}$ risk measure: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{E}_{\\alpha}(\\theta):=\\mathbb{E}_{p_{\\alpha}(\\tau;\\theta)}\\bigg[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\bigg].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Due to no closed form of $p_{\\alpha}(\\tau;\\theta)$ , Wang et al. [1] introduces a slack variable $\\xi\\in\\mathbb{R}$ and reformulates the objective as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta,\\xi\\in\\mathbb{R}}\\mathcal{E}_{\\alpha}(\\theta,\\xi):=\\frac{1}{1-\\alpha}\\int_{\\alpha}^{1}v_{\\beta}d\\beta=\\xi+\\frac{1}{1-\\alpha}\\mathbb{E}_{p(\\tau)}\\left[\\big[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)-\\xi\\big]^{+}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r l r}{v_{\\beta}}&{{}:=}&{F_{\\ell}^{-1}(\\beta)}\\end{array}$ denotes the quantile statistics and $\\begin{array}{r l}{\\left[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\boldsymbol{\\theta})-\\xi\\right]^{+}}&{{}:=}\\end{array}$ $\\operatorname*{max}\\{\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\boldsymbol{\\theta})-\\xi,0\\}$ is the hinge risk. ", "page_idx": 3}, {"type": "text", "text": "The optimization objective involves the integral of quantiles in a continuous interval $(\\alpha,1]$ , which is intractable to precisely parameterize with neural networks. The form in Eq. (4) utilizes the duality trick [11], enabling tractable sampling from the complete task space. ", "page_idx": 3}, {"type": "text", "text": "3.2 Examples & Two-stage Heuristic Strategies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before delving deeper into the theoretical issues, we first present DR-MAML [1] as an instantiation to explain the expected tail risk minimization.   \nExample 1 (DR-MAML [1]). Given $p(\\tau)$ and vanilla MAML [25], the distributionally robust MAML within $C V a R_{\\alpha}$ can be written as a bi-level optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\xi+\\frac{1}{1-\\alpha}\\mathbb{E}_{p(\\tau)}\\left[\\big[\\ell(\\mathfrak{D}_{\\tau}^{Q};\\theta-\\lambda\\nabla_{\\theta}\\ell(\\mathfrak{D}_{\\tau}^{S};\\theta))-\\xi\\big]^{+}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the gradient update w.r.t. the support set $\\nabla_{\\theta}\\ell(\\mathfrak{D}_{\\tau}^{S};\\theta)$ indicates the inner loop with a learning rate $\\lambda.$ . The outer loop executes the gradient updates w.r.t. Eq. (5) and seeks the robust meta initialization in the parameter space. ", "page_idx": 3}, {"type": "text", "text": "Two-stage optimization strategies. Without loss of generality, we further detail the computational pipelines of Example 1 with two-stage optimization strategies. Note that MAML [25] is an optimization-based meta learning method, and the implementation is to execute the sub-gradient descent over a batch of tasks when updating the meta initialization $\\theta^{\\mathrm{meta}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\theta_{t}^{\\tau_{i}}=\\theta_{t}^{\\mathrm{meta}}-\\lambda_{1}\\nabla_{\\theta}\\ell(\\mathfrak{D}_{\\tau_{i}}^{S};\\theta),\\;i=1,\\dots,B}\\\\ {\\hat{\\xi}=\\hat{F}_{\\mathrm{MC}-B}^{-1}(\\alpha),}\\\\ {\\delta(\\tau_{i})=1[\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q};\\theta_{t}^{\\tau_{i}})\\geq\\hat{\\xi}],\\;i=1,\\dots,B}\\\\ {\\mathsf{m e t a}\\gets\\theta_{t}^{\\mathrm{meta}}-\\lambda_{2}\\Big[\\displaystyle\\sum_{i=1}^{B}\\nabla_{\\theta}[\\delta(\\tau_{i})\\cdot\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q};\\theta_{t}^{\\tau_{i}})]\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\lambda_{1}$ and $\\lambda_{2}$ are the inner loop and the outer loop learning rates, and the subscript $t$ records the iteration number, with $\\delta(\\tau_{i})$ the indicator variable. $\\hat{F}_{\\mathbf{MC}-B}$ is the empirical distribution with $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ Monte Carlo task samples. $\\delta(\\tau_{i})=1$ indicates the meta risk $\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q};\\theta_{t}^{\\tau_{i}})$ after fast adaptation falls into the defined tail risk region, otherwise $\\delta(\\tau_{i})=0$ . ", "page_idx": 3}, {"type": "text", "text": "Throughout optimizing DR-MAML, Stage-I includes the fast adaptation w.r.t. individual task in Eq. (6a), and the quantile estimate in Eq. (6b). Stage-II applies the sub-gradient updates to the model parameters in Eq. (6c)/(6d). These two stages repeat until convergence is achieved. ", "page_idx": 3}, {"type": "text", "text": "4 Theoretical Investigations ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "McrzOo0hwr/tmp/d96c2cecaab312669c48c9dabee99ca6ed4985f70179e5b9b5ad248adfc469bf.jpg", "img_caption": ["Figure 1: Illustration of optimization stages in distributionally robust meta learning from a Stackelberg game. Given the DRMAML example, the pipeline can be interpreted as bi-level optimization: the leader\u2019s move for characterizing tail task risk and the follower\u2019s move for robust fast adaptation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "This section presents theoretical insights into two-stage optimization strategies. We perform analysis from the algorithmic convergence, the asymptotic tail risk robustness, and the cross-task generalization capability in meta learning. ", "page_idx": 3}, {"type": "image", "img_path": "McrzOo0hwr/tmp/125a7a186c85ea8428c8ea4fcc223434157f60cc141d5c8615e948c73a6b5366.jpg", "img_caption": ["Figure 2: The sketch of theoretical and empirical contributions in two-stage robust strategies. On the left side is the two-stage distributionally robust strategy [1]. The contributed theoretical understanding is right-down, with the right-up the empirical improvement. Arrows show connections between components. ", "Stackelberg Game for Meta Learning Robustification "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 Distributionally Robust Meta Learning as a Stackelberg Game ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Implementing the two-stage optimization strategy in meta learning requires first specifying the stages\u2019 order. The default is the minimization of the risk measure w.r.t. the parameter space after the maximization of the risk measure $w.r t$ . the task subspace. Hence, we propose to connect it to max-min optimization [13] and the Stackelberg game [47]. ", "page_idx": 4}, {"type": "text", "text": "Max-min optimization. With the pre-assigned decision-making orders, the studied problem can be characterized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{q(\\tau)\\in\\mathcal{Q}_{\\alpha}}\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{F}(q,\\theta):=\\mathbb{E}_{q(\\tau)}\\Big[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\Big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{Q}_{\\alpha}:=\\{q(\\tau)|T_{q}\\subseteq\\mathcal{T},\\int_{\\tau\\in\\mathcal{T}_{q}}p(\\tau)d\\tau=1-\\alpha\\}}\\end{array}$ constitutes a collection of uncertainty sets [48] over task subspace $\\tau_{q}$ , and $q(\\tau)$ is the normalized probability density over the task subspace. Note that in the expected tail risk minimization principle, there is no closed form of optimization objective Eq. (4) as the tail risk is $\\theta$ -dependent. It is approximately interpreted as the max-min optimization when applied to the distribution over the uncertainty set $\\mathcal{Q}_{\\alpha}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. The uncertainty set $\\mathcal{Q}_{\\alpha}$ is convex and compact in terms of probability measures. ", "page_idx": 4}, {"type": "text", "text": "Practical optimization is achieved via mini-batch gradient estimates and sub-gradient updates with the task size $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ in [1]; the feasible subsets correspond to all combinations of size $\\lceil B*\\left(1-\\alpha\\right)\\rceil$ . Also, Eq. (7) is non-differentiable w.r.t. $q(\\tau)$ , leaving previous approaches [49\u201352] unavailable in practice. ", "page_idx": 4}, {"type": "text", "text": "Stackelberg game $\\pmb{\\&}$ best responses. The example computational pipelines in Eq. (6) can be understood as approximately solving a stochastic two-player zero-sum Stackelberg game. Mathematically, such a game referred to as $\\mathcal{S}\\mathcal{G}$ can be depicted as $\\begin{array}{r}{\\bar{S}\\bar{\\mathcal{G}}:=\\langle\\mathcal{P}_{L},\\mathcal{P}_{F};\\{q\\in\\mathcal{Q}_{\\alpha}\\}\\rangle,\\{\\theta\\in\\Theta\\};\\mathcal{F}(q,\\theta)\\rangle.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Moreover, we translate the two-stage optimization as decisions made by two competitors, which are illustrated in Fig. 1. The maximization operator executes in the task space, corresponding to the leader $\\mathcal{P}_{L}$ in $\\mathcal{S}\\mathcal{G}$ with the utility function ${\\mathcal{F}}(q,\\theta)$ . The follower $\\mathcal{P}_{F}$ attempts to execute sub-gradient updates over the meta learners\u2019 parameters via maximizing $-\\mathcal{F}(q,\\theta)$ . ", "page_idx": 4}, {"type": "text", "text": "The two players compete to maximize separate utility functions in $\\mathcal{S}\\mathcal{G}$ , which can be characterized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{S}\\mathcal{G}:\\;q_{t}=\\arg\\operatorname*{max}_{q\\in\\mathcal{Q}_{\\alpha}}\\mathbb{E}_{q}\\bigg[\\ell\\big(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta_{t}\\big)\\bigg],\\quad\\theta_{t+1}=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\mathbb{E}_{q_{t}}\\bigg[\\ell\\big(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta\\big)\\bigg],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the leader player $\\mathcal{P}_{L}$ specifies the worst case combinations from the uncertainty set $\\mathcal{Q}_{\\alpha}$ , and the follower $\\mathcal{P}_{F}$ reacts to the resulting normalized tail risk for increasing fast adaptation robustness. ", "page_idx": 4}, {"type": "text", "text": "It is worth noting that the update rules in Eq. (8) are also called best responses of players in game theory. The above procedures can be deemed the bi-level optimization [53] since the update of the meta learner implicitly depends on the leader\u2019s last time decision. ", "page_idx": 4}, {"type": "text", "text": "4.2 Solution Concept & Properties ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The improvement guarantee has been demonstrated when employing two-stage optimization strategies for minimizing the tail risk in [1]. Furthermore, we claim that under certain conditions, there converges to a solution for the proposed Stackelberg game $\\mathcal{S}\\mathcal{G}$ . The sufficient evidence is: ", "page_idx": 5}, {"type": "text", "text": "1. The two-stage optimization [1] results in a monotonic sequence: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{M o d e l\\ U p d a t e s:\\dots\\mapsto\\{q_{t-1},\\theta_{t}\\}\\mapsto\\{q_{t},\\theta_{t+1}\\}\\mapsto\\cdots\\qquad}\\\\ &{}&{M o n o t o n i c\\ I m p r o\\nu e m e n t:\\dots\\nmid\\dots\\geq\\mathcal{F}(q_{t-1},\\theta_{t})\\geq\\mathcal{F}(q_{t},\\theta_{t+1})\\geq\\dots\\cdot;}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Built on the boundness of risk functions and the theorem of improvement guarantee, such an optimization process can finally converge [54]. Then, a crucial question arises concerning the obtained solution: What is the notion of the convergence point in the game? ", "page_idx": 5}, {"type": "text", "text": "To answer this question, we need to formulate the corresponding solution concept in $\\mathcal{S}\\mathcal{G}$ . Here, the global Stackelberg equilibrium is introduced as follows. ", "page_idx": 5}, {"type": "text", "text": "Definition 1 (Global Stackelberg Equilibrium). Let $(q_{*},\\theta_{*})\\in\\mathcal{Q}_{\\alpha}\\times\\Theta$ be the solution. With the leader $q_{*}\\in\\mathcal{Q}_{\\alpha}$ and the follower $\\theta_{*}\\in\\Theta$ , $(q_{*},\\theta_{*})$ is called a global Stackelberg equilibrium if the following inequalities are satisfied, $\\forall q\\in\\mathcal{Q}_{\\alpha}$ and $\\forall\\theta\\in\\Theta$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta^{\\prime}\\in\\Theta}\\mathcal{F}(q,\\theta^{\\prime})\\leq\\mathcal{F}(q_{*},\\theta_{*})\\leq\\mathcal{F}(q_{*},\\theta).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 2 (Existence of Equilibrium). Given the Assumption $^{\\,l}$ , there always exists the global Stackelberg equilibrium as the Definition 1 for the studied $\\mathcal{S}\\mathcal{G}$ . ", "page_idx": 5}, {"type": "text", "text": "Nevertheless, the existence of the global Stackelberg equilibrium can be guaranteed; it is NP-hard to obtain the equilibrium with existing optimization techniques. The same as that in [55], we turn to the local Stackelberg equilibrium as the Definition 2, where the notion of the local Stackelberg game is restricted in a neighborhood $\\mathcal{Q}_{\\alpha}^{\\prime}\\times\\Theta^{\\prime}$ in strategies. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Local Stackelberg Equilibrium). Let $(q_{*},\\theta_{*})\\,\\in\\,\\mathcal{Q}_{\\alpha}\\,\\times\\,\\Theta$ be the solution. With the leader $q_{*}\\in\\mathcal{Q}_{\\alpha}$ and the follower $\\theta_{\\ast}\\in\\Theta$ , $(q_{*},\\theta_{*})$ is called a local Stackelberg equilibrium for the leader if the following inequalities hold, $\\forall q\\in\\mathcal{Q}_{\\alpha}^{\\prime}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta\\in S_{\\Theta^{\\prime}}(q_{*})}\\mathcal{F}(q_{*},\\theta)\\geq\\operatorname*{inf}_{\\theta\\in S_{\\Theta^{\\prime}}(q)}\\mathcal{F}(q,\\theta),\\mathrm{where~}S_{\\Theta^{\\prime}}(q):=\\{\\bar{\\theta}\\in\\Theta^{\\prime}|\\mathcal{F}(q,\\bar{\\theta})\\leq\\mathcal{F}(q,\\theta),\\forall\\theta\\in\\Theta^{\\prime}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The nature of nonconvex programming comprises the above local optimum, and we introduce concepts below for further analysis. It can be validated that ${\\mathcal{F}}(q,\\theta)$ is a quasi-concave function $w.r t$ . $q$ , meaning that for any positive number $l\\in\\mathbb{R}_{+}$ , the set $\\{q|q\\in\\dot{\\mathcal{Q}}_{\\alpha},\\mathcal{F}(\\dot{q},\\theta)>l\\}$ is convex in $\\mathcal{Q}_{\\alpha}$ . As a result, we deduce that there exists an implicit function $h(\\cdot):\\Theta\\to\\mathcal{Q}_{\\alpha}$ such that the condition holds $h(\\theta)=q$ with $q=\\arg\\operatorname*{max}_{\\bar{q}\\in\\mathcal{Q}_{\\alpha}}\\mathcal{F}(\\bar{q},\\theta)$ . For the implicit function $h$ , along with $\\nabla_{\\theta}\\mathcal{F}(q,\\theta)$ , we make the Assumption below. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. The implicit function $h(\\cdot)$ is $\\beta_{h}$ -Lipschitz continuous w.r.t. $\\theta\\in\\Theta$ , and $\\nabla_{\\theta}\\mathcal{F}(q,\\theta)$ is $\\beta_{q}$ -Lipschitz continuous w.r.t. $q\\in\\mathcal{Q}_{\\alpha}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Convergence Rate & Generalization Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Learning to learn scales with the number of tasks, but the optimization process is computationally expensive [56\u201360], particularly when large language models are meta learners [3, 61, 62]. In training distributionally robust meta learners, estimating the convergence rate allows monitoring of the convergence and designing early stopping criteria to reach a desirable performance, reducing computational burdens [63]. Consequently, we turn to another question regarding the solution concept: What is the convergence rate of the two-stage optimization algorithm? ", "page_idx": 5}, {"type": "text", "text": "The runtime complexity for the leader\u2019s move can be easily estimated from subset selection, while the analysis for the follower is non-trivial. Under certain conditions, we can derive the following convergence rate theorem, where $\\lambda$ is the learning rate in gradient descent w.r.t. $\\theta$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Convergence Rate for the Second Player). Let the iteration sequence in optimization be: $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ , with the converged equilibirum $(q_{*},\\theta_{*})$ . Under the Assumption 2 and suppose that $\\lvert\\lvert I\\stackrel{,}{-}\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{*},\\theta_{*})\\rvert\\rvert_{2}^{\\circ}<$ $1\\,-\\,\\lambda\\beta_{q}\\beta_{h}$ , we can have $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\frac{||\\theta_{t+1}-\\theta_{*}||_{2}}{||\\theta_{t}-\\theta_{*}||_{2}}\\ \\leq\\ 1}\\end{array}$ , and the iteration converges with the rate $\\left(||I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{*},\\theta_{*})||_{2}+\\lambda\\beta_{q}\\beta_{h}\\right)$ when $t$ approaches infinity. ", "page_idx": 6}, {"type": "text", "text": "Moreover, after executing the two-stage algorithm $T$ time steps and given learned $\\theta_{T}^{\\mathrm{meta}}$ , we can establish a bound on the asymptotic performance gap w.r.t. $\\mathrm{CVaR}_{\\alpha}$ in Theorem 4.2. For expositional clarity, we \u2217simplify $\\ell(\\mathfrak{D}_{\\tau}^{Q},\\dot{\\mathfrak{D}}_{\\tau}^{S};\\theta_{*})$ , $\\bar{\\ell}(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta_{T}^{\\mathrm{meta}})$ , $\\mathrm{VaR}_{\\alpha}\\left[\\ell(\\mathcal{T},\\theta_{*})\\right]$ , and $\\mathrm{VaR}_{\\alpha}\\left[\\ell({\\mathcal T},\\theta_{T}^{\\mathrm{meta}})\\right]$ as $\\ell^{*}$ , , $\\mathrm{VaR}_{\\alpha}^{*}$ , and $\\mathrm{VaR}_{\\alpha}^{\\mathrm{meta}}$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 (Asymptotic Performance Gap in Tail Task Risk). Under the Assumption $I$ and given a batch of tasks $\\{\\tau_{i}\\}_{i=1}^{B}$ , we can have ", "page_idx": 6}, {"type": "equation", "text": "$$\nC V a R_{\\alpha}(\\theta_{T}^{m e t a})-C V a R_{\\alpha}(\\theta_{*})\\leq\\beta_{\\tau}\\|\\theta_{T}^{m e t a}-\\theta_{*}\\|+\\frac{V a R_{\\alpha}^{*}}{1-\\alpha}\\Big(\\mathbb{P}(\\mathcal{T}_{1})-\\mathbb{P}(\\mathcal{T}_{2})\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{1}=\\{\\tau:\\ell^{*}<V a R_{\\alpha}^{*},\\ell^{m e t a}\\geq V a R_{\\alpha}^{m e t a}\\},\\mathcal{T}_{2}=\\{\\tau:\\ell^{*}\\geq V a R_{\\alpha}^{*},\\ell^{m e t a}<V a R_{\\alpha}^{m e t a}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For sufficiently large $T$ , the first term can be bounded by a small number due to the convergence, and the second term vanishes since $\\begin{array}{r}{\\operatorname*{lim}_{T\\to\\infty}\\ell^{\\mathrm{meta}}=\\ell^{*}}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{T\\to\\infty}\\mathrm{{VaR}}_{\\alpha}^{\\mathrm{{meta}}}=\\mathrm{{VaR}}_{\\alpha}^{*}}\\end{array}$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Another crucial issue regarding meta learning lies in the fast adaptation capability in unseen cases. This drives us to answer the following question: How does the resulting meta learner generalize in the presence of tail task risk? ", "page_idx": 6}, {"type": "text", "text": "To this end, we first define $\\begin{array}{r}{R(\\theta_{*})\\ =\\ \\mathbb{E}_{p_{\\alpha}(\\tau)}\\left[\\ell^{*}\\right]\\!,\\ \\widehat{R}(\\theta_{*})\\ =\\ \\frac{1}{B}\\sum_{i=1}^{B}\\delta\\big(\\tau_{i}\\big)\\ell\\big(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*}\\big)}\\end{array}$ , and $\\begin{array}{r}{\\widehat{R}_{w}(\\theta_{*})=\\frac{1}{\\mathcal{B}}\\sum_{i=1}^{\\mathcal{B}}\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*})}\\end{array}$ , where $\\tau_{i}\\sim p(\\tau)$ . Also note that the support of $p_{\\alpha}(\\tau;\\theta_{\\ast})$ is within that of $p(\\tau)$ , namely ${\\mathrm{supp}}(p_{\\alpha}(\\tau;\\theta_{*}))\\subseteq{\\mathrm{supp}}(p(\\tau))$ . Then we can induce Theorem $4.3\\;w.r t.$ the tail risk generalization. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 (Generalization Bound in the Tail Risk Cases). Given a collection of task samples $\\{\\tau_{i}\\}_{i=1}^{B}$ and corresponding meta datasets, we can derive the following generalization bound in the presence of tail risk: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R(\\theta_{*})\\leq\\widehat{R}(\\theta_{*})+\\sqrt{\\frac{2\\left(\\frac{\\alpha}{1-\\alpha}\\mathcal{L}_{\\operatorname*{max}}^{2}+\\mathbb{V}_{\\tau_{i}\\sim p_{\\alpha}(\\tau)}\\left[\\ell\\left(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*}\\right)\\right]\\right)\\ln\\left(\\frac{1}{\\epsilon}\\right)}{\\mathcal{B}}}}\\\\ {+\\frac{1}{3(1-\\alpha)}\\frac{\\mathcal{L}_{\\operatorname*{max}}}{\\mathcal{B}}\\left(2\\ln\\left(\\frac{1}{\\epsilon}\\right)+3\\alpha\\mathcal{B}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the inequality holds with probability at least $1-\\epsilon$ and $\\epsilon\\in(0,1),\\,\\mathbb{V}[\\cdot]$ denotes the variance operation, and ${\\mathcal{L}}_{\\mathrm{max}}$ is from the Assumption $^{\\,l}$ . ", "page_idx": 6}, {"type": "text", "text": "In conjunction with the confidence $\\epsilon$ and a task batch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ of significant size, Theorem 4.3 reveals the generalization bound given the meta-trained parameter $\\theta_{*}$ . It is also associated with the variance $\\breve{\\mathbb{V}}_{\\tau_{i}\\sim p_{\\alpha}(\\tau)}[\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\mathsf{\\bar{\\theta}}_{*})]$ . Besides, we also derive a specific bound in the case of MAML, and details are attached in Appendix Theorem C.1. ", "page_idx": 6}, {"type": "text", "text": "4.4 Practical Enhancements & Implementations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 reveals that an accurate estimate of $\\operatorname{VaR}_{\\alpha}$ yields a precise variance (i.e., $\\mathbb{V}_{\\tau_{i}\\sim p_{\\alpha}(\\tau)}[\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\boldsymbol{\\theta}_{*})])$ , leading to more reliable bounds. Accordingly, this section offers improvements over [1] via utilizing kernel density estimators (KDE) [64] for $\\operatorname{VaR}_{\\alpha}$ \u2019s estimates. Compared to crude Monte Carlo (MC) methods, KDE can handle arbitrary complex distributions, capture local statistics well, and smoothen the cumulative function in a non-parametric way. ", "page_idx": 6}, {"type": "text", "text": "Specifically, we can construct KDE with a batch of task risk values $\\{\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta)\\}_{i=1}^{\\mathcal{B}}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\ell\\mathrm{-KDE}}(l;\\theta)=\\int_{-\\infty}^{l}\\frac{1}{\\mathcal{B}h_{\\ell}}\\sum_{i=1}^{B}K\\Big(\\frac{t-\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta)}{h_{\\ell}}\\Big)d t,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $K:\\mathbb R^{d}\\to\\mathbb R$ is a kernel function, e.g., the Gaussian kernel, $\\begin{array}{r}{K(x)=\\frac{\\exp(-||x||^{2}/2)}{\\int\\exp(-||x||^{2}/2)d x}}\\end{array}$ , and $h_{\\ell}$ is the smoothing bandwidth. Once the KDE is built, it enables access to the quantile from the cumulative distribution functions or numeric integrals. The following Theorem 4.4 shows that KDE serves as a reliable approximation for $\\mathrm{VaR}_{\\alpha}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4. Let $F_{\\ell-K D E}^{-1}(\\alpha;\\theta)=V a R_{\\alpha}^{K D E}[\\ell(\\mathcal{T},\\theta)]$ and $F_{\\ell}^{-1}(\\alpha;\\theta)=V a R_{\\alpha}[\\ell(\\mathcal{T},\\theta)]$ . Suppose that $K(x)$ is lower bounded by a constant, $\\forall x$ . For any $\\epsilon>0$ , with probability at least $1-\\epsilon$ , we can have the following bound: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta}\\,\\left(F_{\\ell\\cdot K D E}^{-1}(\\alpha;\\theta)-F_{\\ell}^{-1}(\\alpha;\\theta)\\right)\\leq\\mathcal{O}\\left(\\frac{h_{\\ell}}{\\sqrt{B*\\log B}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As implied, one can close the distribution approximation gap by adopting a smaller, more flexible bandwidth. Additionally, KDE models offer a smooth estimate of the cumulative distribution function and require no prior assumptions. ", "page_idx": 7}, {"type": "text", "text": "Remark 1. In addition to smoothness, flexibility, and distribution agnostic traits, KDE in adoption can enhance the studied method\u2019s generalization capability. The crude Monte Carlo used in [1] typically incurs an error of approximately $\\textstyle{\\mathcal{O}}({\\frac{1}{\\sqrt{B}}})$ in estimating quantiles [65]. In contrast, that of KDE is no more than $\\begin{array}{r}{{\\mathcal{O}}(\\frac{h_{\\ell}}{\\sqrt{B*\\log B}})}\\end{array}$ from Theorem 4.4. ", "page_idx": 7}, {"type": "text", "text": "5 Empirical Findings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Prior sections mainly focus on the theoretical understanding of two-stage distributionally robust strategies. This section conducts extensive experiments on a broader range of benchmarks and examines the improvement tricks, e.g., the use of KDE for quantile estimates, from empirical results. ", "page_idx": 7}, {"type": "text", "text": "Benchmarks & baselines. We perform experiments on the few-shot regression, system identification, image classification, and meta reinforcement learning, where most of them keep setups the same as prior work [1, 42]. We evaluate the methods from risk minimization principles and corresponding indicators, including expected/empirical risk minimization (Average), worst-case risk minimization (Worst), and tail risk minimization $\\left(\\mathrm{CVaR}_{\\alpha}\\right)$ ). ", "page_idx": 7}, {"type": "text", "text": "MAML mainly works as the base meta learner, and we term the KDE-augmented DR-MAML as DR-MAML+. Then we compare DR-MAML $^{,+}$ with several baselines, including vanilla MAML [25], TR-MAML [42], DRO-MAML [66] and DR-MAML [1]. ", "page_idx": 7}, {"type": "image", "img_path": "McrzOo0hwr/tmp/9ea569173b4469bb4aa903e8830f93e66f34db66e6c8747a36cbc0105982607a.jpg", "img_caption": ["Figure 3: Meta testing performance in sinusoid regression problems (5 runs). The charts report testing mean square errors (MSEs) over 490 unseen tasks [42] with $\\alpha\\:=\\:0.7$ , where black vertical lines indicate standard error bars. ", "Figure 4: Meta testing performance in Pendulum 10-shot and 20-shot problems (5 runs). Reported are testing MSEs over 529 unseen tasks with $\\alpha=0.5$ , where black vertical lines indicate standard error bars. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 Sinusoid Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The goal of the sinusoid regression [25] is to quickly fti an underlying function $f(x)=A\\sin(x-B)$ from $K$ randomly sampled data points, and tasks are specified by $(A,B)$ . The meta-training and testing setups are the same as that in [1, 42], where many easy functions with a tiny fraction of difficult ones are included in the training. ", "page_idx": 7}, {"type": "text", "text": "Result $\\pmb{\\&}$ analysis. As illustrated in Fig. 3, we can observe that DR-MAML $^+$ consistently outperforms all baselines across average and $\\mathrm{CVaR}_{\\alpha}$ indicators in the 5-shot case. Though the average performance slightly lags behind DR-MAML in the 10-shot case, DR-MAML $^+$ surpasses other baselines in both the Worst and $\\mathrm{CVaR}_{\\alpha}$ indicators. This implies that DR-MAML $^{+}$ exhibits more robustness in challenging task distributions, e.g., 5-shot case. Furthermore, the standard error associated with our method is significantly smaller than others, underscoring the stability of DR-MAML $^+$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 System Identification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The system identification corresponds to learning a dynamics model from a few collected transitions in physics systems. Here, we consider the Pendulum system and create diverse dynamical systems by varying its mass $m$ and length $l$ , with $(m,l)\\sim\\mathcal{U}([0.4,1.6]$ , [0.4, 1.6]). A random policy collects transitions for meta training, and 10 random transitions work as a support dataset. ", "page_idx": 8}, {"type": "text", "text": "Result & analysis. Fig. 4 shows no significant difference between 10-shot and 20-shot cases. DR-MAML $^{+}$ dominates the performance across all indicators in both cases. Due to the min-max optimization, TR-MAML behaves well in the worst-case but sacrifices too much average performance. Within the studied strategies, DR-MAML $^+$ exhibits an advantage over DR-MAML regarding $\\mathrm{CVaR}_{\\alpha}$ . ", "page_idx": 8}, {"type": "text", "text": "5.3 Few-shot Image Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We perform few-shot image classification on the mini-ImageNet dataset [67], with the same setup in [42]. The task is a 5-way 1-shot classification problem. And 64 classes are selected for constructing meta-training tasks, with the remaining 32 classes for meta-testing. ", "page_idx": 8}, {"type": "table", "img_path": "McrzOo0hwr/tmp/022f94356ebd611a5e28a2c0918ad3f0cffce98065f96edfaacc2e173d011a45.jpg", "table_caption": ["Table 1: Average 5-way 1-shot classification accuracies in mini-ImageNet with reported standard deviations (3 runs). With $\\alpha=0.5$ , the best results are in bold. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Result & analysis. In Table 1, methods within a two-stage distributionally robust strategy, namely DR-MAML and DR-MAML+, show superiority to others across all indicators in both training and testing scenarios, which is similar to empirical findings in [1]. Interstingly, DR-MAML $^{+}$ and DRMAML are comparable in most scenarios, and we attribute this to the small batch size in training, which weakens KDE\u2019s quantile approximation advantage. ", "page_idx": 8}, {"type": "text", "text": "5.4 Meta Reinforcement Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here, we take 2-D point robot navigation as the meta reinforcement learning benchmark in evaluation. The goal is to reach the target destination with the help of a few exploration transitions for fast adaptation, and we retain the setup in MAML [25]. In meta ", "page_idx": 8}, {"type": "table", "img_path": "McrzOo0hwr/tmp/8f36b635da4215e26021f3df4aff8723ab68f92504184ab8a325efbe59428022.jpg", "table_caption": ["Table 2: Meta testing returns in point robot navigation (4 runs). The chart reports average return and $\\mathrm{CVaR}_{\\alpha}$ return with $\\alpha=0.5$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "testing, we randomly sample 80 navigation goals and examine methods\u2019 navigation performance. ", "page_idx": 8}, {"type": "text", "text": "Result & analysis. As reinforcement learning methods fluctuate fiercely in worst-case indicators, we only report Average and $\\mathrm{CVaR}_{\\alpha}$ returns in Table 2. We observe that using studied strategies in DR-MAML enhances the returns. DR-MAML $^{+}$ beneftis from a more reliable quantile estimate and achieves superior performance. The application of distributional robustness to reinforcement learning yields improvements in returns. ", "page_idx": 8}, {"type": "image", "img_path": "McrzOo0hwr/tmp/9e1ae7ef7ab77a55a7672fbe9cda4264f0be9f23415533591eeefa003329446e.jpg", "img_caption": ["Figure $5\\colon{\\bf V a R}_{\\alpha}$ approximation errors with the crude MC and KDE. We compute the difference between the estimated $\\hat{\\mathrm{VaR}}_{\\alpha}$ and the Oracle $\\mathrm{VaR}_{\\alpha}$ in the absolute value $|\\hat{\\mathbf{VaR}_{\\alpha}}-\\mathbf{VaR}_{\\alpha}|$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.5 Assessment of Quantile Estimators ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "With the meta trained model, e.g., DR-MAML $^{+}$ in sinusoid regression, we collect the testing task risk values with different task batch sizes to estimate the $\\operatorname{VaR}_{\\alpha}$ from respectively the crude MC and KDE. As observed from Fig. 5, the $\\operatorname{VaR}_{\\alpha}$ approximation error decreases with more tasks, and the KDE produces more accurate estimates with a sharper decreasing trend. The above well verifies the conclusion in Theorem 4.3. ", "page_idx": 9}, {"type": "text", "text": "5.6 Empricial Result Summarization ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Here, we summarize two points from the above empirical results and associated theorems. (i) From Theorem 4.2/4.3 and Fig. 3/4/5: the $\\operatorname{VaR}_{\\alpha}$ estimate relates to the reliable generalization bound, and cumulated tiny approximation errors along iterations potentially result in worse equilibrium. (ii) From Theorem $4.3/4.4$ , Remark 1, Fig. 3, and Table $1/2$ : with the studied strategy, the KDE is a better choice of task risk distribution modelling than the crude MC in tougher benchmarks, e.g., 5-shot sinusoid regression, meta-testing mini-ImageNet classification, and point robot navigation. ", "page_idx": 9}, {"type": "image", "img_path": "McrzOo0hwr/tmp/9af4cc935fc3c7b4fe52f6b64623547c969d11d07386d3b2d41f4f73d8afc50c.jpg", "img_caption": ["5.7 Compatibility with Large Models ", "Figure 6: Meta testing results on 5-way 1-shot classification accuracies with reported standard deviations (3 runs). The charts respectively report classification accuracies over 150 unseen tasks. We further conduct few-shot image classification experiments in the presence of large model. Note that CLIP [68] exhibits strong zero-shot adaptation capability; hence, we employ \"ViT-B/16\"-based CLIP as the backbone to enable few-shot learning in the same way as MaPLe with training setup ${\\mathbf{N}}_{\\mathbf{-}}{\\mathbf{C}}{\\mathbf{T}}{\\mathbf{X}}\\,=\\,2$ and $\\mathrm{MAX\\_EPOCH}=30\\$ [69], scaling to large neural networks in evaluation (See Appendix Section D for details). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Improved Robustness in Evaluation: As illustrated in Fig. 6, DR-MaPLe and DR-MaPLe+ consistently outperform baselines across both average and indicators in cases, demonstrating the advantage of the two-stage strategy in enhancing the robustness of few-shot learning. DR-MaPLe+ achieves better results as KDE quantiles are more accurate with large batch sizes. These results confirm the scalability and compatibility of our method on large models. ", "page_idx": 9}, {"type": "text", "text": "Learning Efficiency as Limitations: In terms of implementation time and memory cost, we retain the setup the same as that in [1]: use the same maximum number of meta gradient updates for all baselines in training processes, which means given $\\alpha=0.5$ , the tail risk minimization principle requires double task batches to evaluate and screen sub-batches. It can be seen that both DR-MaPLe and DR-MaPLe $^+$ consume more memories, and the extra training time over MaPLe arises from the evaluation and sub-batch screening in the forward pass. Such additional computations and memory costs raise computational and memory efficiency issues for exchanging extra significant robustness improvement in fast adaptation. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To conclude, this paper proposes to understand the two-stage distributionally robust strategy from optimization processes, define the convergence solution, and derive the generalization bound in the presence of tail task risk. Extensive experiments validate the studied improvement tricks and reveal more empirical properties of the studied strategy. We leave computational overhead reduction as a promising topic for future exploration in robust fast adaptation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is funded by National Natural Science Foundation of China (NSFC) with the Number # 62306326. We express particular gratitude to friends who guide large model-relevant experiments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Qi Wang, Yiqin Lv, Yanghe Feng, Zheng Xie, and Jincai Huang. A simple yet effective strategy to robustify the meta learning paradigm. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[2] Yuanfu Lu, Yuan Fang, and Chuan Shi. Meta-learning on heterogeneous information networks for cold-start recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1563\u20131573, 2020.   \n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[4] Yi Yuan, Gan Zheng, Kai-Kit Wong, and Khaled B Letaief. Meta-reinforcement learning based resource allocation for dynamic v2x communications. IEEE Transactions on Vehicular Technology, 70(9):8964\u20138977, 2021.   \n[5] Brenden M Lake and Marco Baroni. Human-like systematic generalization through a metalearning neural network. Nature, pages 1\u20137, 2023.   \n[6] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44 (9):5149\u20135169, 2021.   \n[7] Qi Wang, Yanghe Feng, Jincai Huang, Yiqin Lv, Zheng Xie, and Xiaoshan Gao. Large-scale generative simulation artificial intelligence: The next hotspot. The Innovation, page 100516, 2023.   \n[8] Young-Jun Lee, Chae-Gyun Lim, and Ho-Jin Choi. Does gpt-3 generate empathetic dialogues? a novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation. In Proceedings of the 29th International Conference on Computational Linguistics, pages 669\u2013683, 2022.   \n[9] Chen Tang, Hongbo Zhang, Tyler Loakman, Chenghua Lin, and Frank Guerin. Terminologyaware medical dialogue generation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[10] Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. Medically aware gpt-3 as a data generator for medical dialogue summarization. In Machine Learning for Healthcare Conference, pages 354\u2013372. PMLR, 2021.   \n[11] R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk, 2:21\u201342, 2000.   \n[12] Dirk P Kroese and Reuven Y Rubinstein. Monte carlo methods. Wiley Interdisciplinary Reviews: Computational Statistics, 4(1):48\u201358, 2012.   \n[13] John M Danskin. The theory of max-min, with applications. SIAM Journal on Applied Mathematics, 14(4):641\u2013664, 1966.   \n[14] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018.   \n[15] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International Conference on Machine Learning, pages 1704\u20131713. PMLR, 2018.   \n[16] Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard Turner. Meta-learning probabilistic inference for prediction. In International Conference on Learning Representations, 2018.   \n[17] Qi Wang, Marco Federici, and Herke van Hoof. Bridge the inference gaps of neural processes via expectation maximization. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=A7v2DqLjZdq.   \n[18] Andrew Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and Richard Turner. Meta-learning stationary stochastic process prediction with convolutional neural processes. Advances in Neural Information Processing Systems, 33:8284\u20138295, 2020.   \n[19] Qi Wang and Herke Van Hoof. Doubly stochastic variational inference for neural processes with hierarchical latent variables. In International Conference on Machine Learning, pages 10018\u201310028. PMLR, 2020.   \n[20] Muhammad Waleed Gondal, Shruti Joshi, Nasim Rahaman, Stefan Bauer, Manuel Wuthrich, and Bernhard Scholkopf. Function contrastive learning of transferable meta-representations. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 3755\u20133765. PMLR, 2021.   \n[21] Qi Wang and Herke van Hoof. Learning expressive meta-representations with mixture of expert neural processes. In Advances in neural information processing systems, 2022.   \n[22] Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh. Bootstrapping neural processes. Advances in neural information processing systems, 33:6606\u2013 6615, 2020.   \n[23] Qi Wang and Herke Van Hoof. Model-based meta reinforcement learning using graph structured surrogate models and amortized policy search. In International Conference on Machine Learning, pages 23055\u201323077. PMLR, 2022.   \n[24] Jiayi Shen, Xiantong Zhen, Marcel Worring, et al. Episodic multi-task learning with heterogeneous neural processes. arXiv preprint arXiv:2310.18713, 2023.   \n[25] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[26] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. Advances in neural information processing systems, 32, 2019.   \n[27] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.   \n[28] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. Advances in neural information processing systems, 31, 2018.   \n[29] Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen. Sharp-maml: Sharpnessaware model-agnostic meta learning. In International Conference on Machine Learning, pages 10\u201332. PMLR, 2022.   \n[30] Eunbyung Park and Junier B Oliva. Meta-curvature. Advances in neural information processing systems, 32, 2019.   \n[31] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017.   \n[32] Kelsey Allen, Evan Shelhamer, Hanul Shin, and Joshua Tenenbaum. Infinite mixture prototypes for few-shot learning. In International Conference on Machine Learning, pages 232\u2013241. PMLR, 2019.   \n[33] Sergey Bartunov and Dmitry Vetrov. Few-shot generative modelling with generative matching networks. In International Conference on Artificial Intelligence and Statistics, pages 670\u2013678. PMLR, 2018.   \n[34] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.   \n[35] Jacob Beck, Matthew Thomas Jackson, Risto Vuorio, and Shimon Whiteson. Hypernetworks in meta-reinforcement learning. In Conference on Robot Learning, pages 1478\u20131487. PMLR, 2023.   \n[36] Dominic Zhao, Johannes von Oswald, Seijin Kobayashi, Jo\u00e3o Sacramento, and Benjamin F Grewe. Meta-learning via hypernetworks. 4th Workshop on Meta-Learning at NeurIPS 2020, 2020.   \n[37] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International conference on machine learning, pages 1842\u20131850. PMLR, 2016.   \n[38] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.   \n[39] Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, and Meng Wang. On fast adversarial robustness adaptation in model-agnostic meta-learning. In International Conference on Learning Representations, 2020.   \n[40] Micah Goldblum, Liam Fowl, and Tom Goldstein. Adversarially robust few-shot learning: A meta-learning approach. Advances in Neural Information Processing Systems, 33:17886\u201317895, 2020.   \n[41] Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, and Xiangyang Ji. Robust fast adaptation from adversarially explicit task distribution generation. arXiv preprint arXiv:2407.19523, 2024.   \n[42] Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Task-robust model-agnostic metalearning. Advances in Neural Information Processing Systems, 33:18860\u201318871, 2020.   \n[43] Qi Chen, Changjian Shui, and Mario Marchand. Generalization bounds for meta-learning: An information-theoretic analysis. Advances in Neural Information Processing Systems, 34: 25878\u201325890, 2021.   \n[44] Yu Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason Lee, Sham Kakade, Huan Wang, and Caiming Xiong. How important is the train-validation split in meta-learning? In International Conference on Machine Learning, pages 543\u2013553. PMLR, 2021.   \n[45] Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochastic gradient descent with biased regularization. In International Conference on Machine Learning, pages 1566\u20131575. PMLR, 2019.   \n[46] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 1999.   \n[47] Tao Li and Suresh P Sethi. A review of dynamic stackelberg game models. Discrete & Continuous Dynamical Systems-B, 22(1):125, 2017.   \n[48] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341\u2013357, 2013.   \n[49] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48 (3):334\u2013334, 1997.   \n[50] Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, pages 6083\u20136093. PMLR, 2020.   \n[51] Pierre Loridan and Jacqueline Morgan. Weak via strong stackelberg problem: new results. Journal of global Optimization, 8:263\u2013287, 1996.   \n[52] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In International conference on artificial intelligence and statistics, pages 1540\u20131552. PMLR, 2020.   \n[53] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):10045\u201310067, 2021.   \n[54] Tom M Apostol. Mathematical analysis. 1974.   \n[55] Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study. In International Conference on Machine Learning, pages 3133\u20133144. PMLR, 2020.   \n[56] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13693\u201313696, 2020.   \n[57] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.   \n[58] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021.   \n[59] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology, page 100017, 2023.   \n[60] Enkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke H\u00fcllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023.   \n[61] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021, pages 3816\u20133830. Association for Computational Linguistics (ACL), 2021.   \n[62] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint of large language models. arXiv preprint arXiv:2309.14393, 2023.   \n[63] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[64] Mats Rudemo. Empirical choice of histograms and kernel density estimators. Scandinavian Journal of Statistics, pages 65\u201378, 1982.   \n[65] Hui Dong and Marvin K Nakayama. A tutorial on quantile estimation via monte carlo. Monte Carlo and Quasi-Monte Carlo Methods: MCQMC 2018, Rennes, France, July 1\u20136, pages 3\u201330, 2020.   \n[66] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2020.   \n[67] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016.   \n[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[69] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19113\u201319122, 2023.   \n[70] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported trust region optimization for offline reinforcement learning. In International Conference on Machine Learning, pages 23829\u201323851. PMLR, 2023.   \n[71] Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, and Xiangyang Ji. Insample actor critic for offilne reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[72] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported value regularization for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[73] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic programming: modeling and theory. Society for industrial Mathematics, 2009.   \n[74] Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual conservative q learning for offline multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[75] Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Liu Linc, Yang Feng, Lin Lai, Hongyang Qin, et al. Hokoff: real game dataset from honor of kings and its offline reinforcement learning benchmarks. Advances in Neural Information Processing Systems, 36, 2024.   \n[76] Lorna I Paredes and Chew Tuan Seng. Controlled convergence theorem for banach-valued hl integrals. Scientiae Mathematicae Japonicae, 56(2):347\u2013358, 2002.   \n[77] Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvexnonconcave minimax optimization? In International conference on machine learning, pages 4880\u20134889. PMLR, 2020.   \n[78] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[79] C Frappier and QI Rahman. On an inequality of s. bernstein. Canadian Journal of Mathematics, 34(4):932\u2013944, 1982.   \n[80] Rong Liu and Lijian Yang. Kernel estimation of multivariate cumulative distribution function. Journal of Nonparametric Statistics, 20(8):661\u2013677, 2008.   \n[81] Srh Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017.   \n[82] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprint arXiv:1803.00676, 2018.   \n[83] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15262\u201315271, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[84] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. ", "page_idx": 15}, {"type": "text", "text": "[85] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024\u20138035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html.   \n[86] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. {TensorFlow}: a system for {Large-Scale} machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pages 265\u2013283, 2016. ", "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1 Introduction ", "page_idx": 16}, {"type": "text", "text": "2 Literature Review 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "2.1 Meta Learning 2   \n2.2 Robustness & Generalization 2 ", "page_idx": 16}, {"type": "text", "text": "3 Preliminaries 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "3.1 Risk Minimization Principles 3   \n3.2 Examples & Two-stage Heuristic Strategies 4 ", "page_idx": 16}, {"type": "text", "text": "4 Theoretical Investigations 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "4.1 Distributionally Robust Meta Learning as a Stackelberg Game 5   \n4.2 Solution Concept & Properties 6   \n4.3 Convergence Rate & Generalization Bound 6   \n4.4 Practical Enhancements & Implementations 7 ", "page_idx": 16}, {"type": "text", "text": "5 Empirical Findings 8 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "5.1 Sinusoid Regression 8   \n5.2 System Identification 9   \n5.3 Few-shot Image Classification 9   \n5.4 Meta Reinforcement Learning 9   \n5.5 Assessment of Quantile Estimators 10   \n5.6 Empricial Result Summarization 10   \n5.7 Compatibility with Large Models 10 ", "page_idx": 16}, {"type": "text", "text": "6 Conclusion 10 ", "page_idx": 16}, {"type": "text", "text": "A Quick Guide to This Work 19 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Technical Comparison in Robust Fast Adaptation 19   \nA.2 Significance of Theoretical Understandings 19   \nA.3 Meanings of Indicators and Terms 20   \nA.4 Computational Complexity 20   \nA.5 Broader Impact & Future Extensions 20 ", "page_idx": 16}, {"type": "text", "text": "B Pseudo Algorithms 20 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C Expressions, Theorems & Proofs 21 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Characterization of Optimization Processes 21   \nC.2 Assumptions . . 22   \nC.3 Proof of Proposition 1 . 23   \nC.4 Proof of Proposition 2 . 23   \nC.5 Proof of Quasi-concavity for ${\\mathcal{F}}(q,\\theta)$ w.r.t. $q$ 24   \nC.6 Proof of Theorem 4.1 24   \nC.7 Proof of Theorem 4.2 25   \nC.8 Proof of Theorem 4.3 26   \nC.9 Proof of Theorem 4.4 28   \nC.10 Additional Theorem 28 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D Implementation Details 30 ", "page_idx": 17}, {"type": "text", "text": "D.1 Benchmark Details & Neural Architectures & Opensource Codes 30   \nD.2 Modules in Python 31   \nAdditional Experimental Results 32   \nE.1 Evaluation with Other Robust Meta Learners 32   \nE.2 Numeric Results in Tables and Histograms 32   \nE.3 Sensitivity Analysis to Confidence Level 33   \nE.4 Further Exploration on Adaptation 35 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "F F Computational Platforms & Softwares 35 ", "page_idx": 17}, {"type": "text", "text": "A Quick Guide to This Work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section mainly includes explanations and clarifications on this work. ", "page_idx": 18}, {"type": "text", "text": "A.1 Technical Comparison in Robust Fast Adaptation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 3: A summary of robust fast adaptation methods. We take MAML as an example, list related methods, and report their characteristics in literature. We mainly report the statistics according to whether existing literature works include the generalization analysis and convergence analysis. The form of meta learner and the robustness type are generally connected. ", "page_idx": 18}, {"type": "table", "img_path": "McrzOo0hwr/tmp/1b637043b82a6b7b225e13871ac9ef25320a94967d18d70445e8a17a273fc739.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Primary differences: As far as we know, literature work is quite limited regarding fast adaptation robustness in the task space. TR-MAML and DR-MAML are the most recent and typical ones that can handle task distributional shift scenarios well. As reported in Table 3, TR-MAML only focuses on the worst-case, which considers a bit extreme and rarely occurred cases. DRO-MAML is a new baseline, where the uncertainty set $\\mathcal{Q}$ is included for robust fast adaptation, hence there exists no theoretical analysis. As for the tail task risk, DR-MAML lacks generalization capability and convergence rate analysis $w.r t$ the meta learner. The meta learner in DR-MAML $^{+}$ is a more specific instantiation of that in DR-MAML. We claim that these theoretical understanding is necessary in the presence of the robust fast adaptation due to its potential applications in large models. ", "page_idx": 18}, {"type": "text", "text": "Theoretical and empirical insights: In comparison, this work not only contributes to the Stackelberg game for estimates, but also derives the generalization and the asymptotic performance gap in iterations based on a normalized but non-differentiable probability density space. Note that we lean more focus on theoretical understanding and pursuing SOTA performance is not the ultimate purpose of this work. The connections between different quantile estimators and generalization bound highlighted in Theorem $4.3/4.4$ and Remark 1 reveal the theoretical advantage of KDEs over crude Monte Carlo methods. This motivates us to replace crude Monte Carlo with KDEs. Such a replacement as a simple implementation trick is supported by rigorous theoretical analysis. The empirical results align with theoretical understanding. ", "page_idx": 18}, {"type": "text", "text": "In terms of improving the studied strategy, investigations in extensive experiments seem meaningful for practical implementations, and some non-trivial discoveries together with improvement tricks are also reported, such as the relationship between quantile estimate errors and adaptation robustness, the batch size\u2019s influence on several benchmarks, etc. ", "page_idx": 18}, {"type": "text", "text": "In this work, the theoretical and empirical parts are connected in an implicit manner. The generalization capability is empirically examined from experimental results, and the performance gap between DR-MAML $^{+}$ and DR-MAML can be attributed to the difference in generalization bounds. As for the convergence trait and asymptotic performance, the insight might guide the optimization process in training large models, such as early stopping criteria design. ", "page_idx": 18}, {"type": "text", "text": "A.2 Significance of Theoretical Understandings ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As pointed out in [3, 61, 62], large language models are few-shot learners. When a large model, such as a large decision-making model in the future, comes into practice, fast adaptation robustness can be a crucial issue as real-world scenarios are indeed risk-sensitive. ", "page_idx": 18}, {"type": "text", "text": "This work takes the latest work [1] as an example, and the interest is in the theoretical aspect. Most of the assumptions in this work are from [1]. The baselines are typical and latest, while the benchmarks cover diverse downstream tasks. In multimodal few-shot image classification experiments, our contributed points help guide the development of large models in terms of training and robustness enhancement. Our investigations also provide insight into robust policy optimization, particularly when safety is one necessary consideration [70\u201372]. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Shapiro et al.\u2019s book [73] is a comprehensive resource that addresses stochastic modeling and optimization methods, but it does not explore solution concepts in game theory or define generalization bounds relevant to meta learning and deep learning. Instead, our work further enriches the stochastic programming theory in meta learning, connects it to the Stackelberg game, and contributes to tail risk generalization bounds, convergence rates, asymptotic properties, and so on. Therefore, the solution concept and the theoretical properties are specific to our meta learning setup, distinctly from the scope covered by [73]. ", "page_idx": 19}, {"type": "text", "text": "A.3 Meanings of Indicators and Terms ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Illustration of $\\mathbf{VaR}_{\\alpha}$ , CDF and others: Fig. 7 illustrates a typical probability distribution, cumulative distribution, and the resulting mean, $\\operatorname{VaR}_{\\alpha}$ , and $\\mathrm{CVaR}_{\\alpha}$ . Given $\\alpha\\in[0,1)$ , $\\operatorname{VaR}_{\\alpha}$ is the $\\alpha$ quantile of the risk distribution. Specially, $\\mathrm{VaR}_{0.5}$ coincides with the mean. Upon the definition of $\\operatorname{VaR}_{\\alpha}$ , $\\mathrm{CVaR}_{\\alpha}$ can be define as $\\mathbf{CVaR}_{\\alpha}=\\mathbb{E}_{p(\\tau)}\\Big[\\ell|\\ell\\geq\\ \\mathbf{VaR}_{\\alpha}\\Big]$ . That is, $\\mathrm{CVaR}_{\\alpha}$ is the expectation of the risks of the $1-\\alpha$ tail of the distribution. Relative to the original probability distribution, $\\mathrm{CVaR}_{\\alpha}$ can be interpreted as a certain distribution shift, which reweighs arbitrary risk exceeding $\\mathrm{VaR}_{\\alpha}$ up to a coefficient1\u22121\u03b1. ", "page_idx": 19}, {"type": "text", "text": "Meaning of the asymptotic performance gap: We plot Fig. 8 to display the gap between the $\\mathrm{CVaR}_{\\alpha}$ value in iterations and that in the convergence. The area difference depicts this gap. ", "page_idx": 19}, {"type": "text", "text": "A.4 Computational Complexity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Analyzing computational complexity across all meta-learning methods is inherently challenging due to the diversity in methodological approaches within the field. Meta-learning encompasses a wide range of techniques, including gradient-based methods, which rely on iterative updates to model parameters, and non-parametric methods, which may instead focus on instance-based learning or kernel-based approaches. Therefore, the space complexity is specific to the meta-learning method, while this work is agnostic to it. Here, we report the computational complexity for the DR$\\mathrm{MAML+}$ as $\\mathcal{O}\\Big(B(B-\\alpha|\\bar{\\mathcal{M}}|)\\Big)$ while using KDE with the Gaussian kernel, and that of DR-MAML is $\\mathcal{O}\\Big(\\mathcal{B}(\\log(\\mathcal{B})-\\alpha|\\mathcal{M}|)\\Big)$ . ", "page_idx": 19}, {"type": "text", "text": "A.5 Broader Impact & Future Extensions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of robust meta learning. There are many potential societal consequences of our work, which we detail as follows. ", "page_idx": 19}, {"type": "text", "text": "The fast adaptation robustness is an urgent concern, particularly in large models and risk-sensitive control. This work provides versatile insights for theoretical analysis and performance improvement in the presence of tail task risk, and future explorations can be decision-making scenarios, such as multi-agent policy optimization [74, 75], and computational/memory cost reduction. ", "page_idx": 19}, {"type": "text", "text": "B Pseudo Algorithms ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For a better understanding of the game theoretical optimization, we take DR-MAML $^{+}$ and DR- $\\mathrm{CNP+}$ as examples and include the Pseudo Algorithms 1/2 in this section. Particularly, the algorithms specify the decision-making orders and highlight the use of KDE modules to build task risk value distributions and estimate the quantile. ", "page_idx": 19}, {"type": "image", "img_path": "McrzOo0hwr/tmp/7052f970c0cc8dd91f1a816b03f9c271726c792648cc23dce65124b0beddcbc0.jpg", "img_caption": ["Figure 7: Diagram of risk concepts in this work. Here, the $x$ -axis is the task risk value in fast adaptation given a specific $\\theta$ . The shadow-lined region illustrates the tail risk with a probability $1-\\alpha$ in the probability density. The area of the shadow-lined region after $1-\\alpha$ normalization corresponds to the expected tail risk $\\mathrm{CVaR}_{\\alpha}$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "McrzOo0hwr/tmp/094410c21899c4c73e8201ccb3294771aefb999812b1516d19274ae2b4ff345e.jpg", "img_caption": ["Figure 8: Illustration of the asymptotic behavior in approximating the equilibrium. Here, the $x$ -axis is the feasible task risk value in fast adaptation. The dark blue region indicates the histogram of the task risk values in the local Stackelberg equilibrium $(q_{*},\\theta_{*})$ . The shallow blue region describes the histogram of the task risk values at some iterated point $(q_{T-1},\\theta_{T}^{\\mathrm{meta}})$ . The sets $\\mathcal{T}_{1}$ and $\\mathcal{T}_{2}$ respectively collect the tasks resulting the opposite order. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Expressions, Theorems & Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Characterization of Optimization Processes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Without loss of generality, we can also express the process of solving the studied Stackelberg game as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{q\\in\\mathbb{Q}_{\\alpha}}{\\operatorname*{max}}\\mathcal{F}(q,\\theta_{*}(q))}&{\\ s.\\mathrm{.t.}\\ \\theta_{*}(q)=\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\mathcal{F}(q,\\theta)}&{\\qquad}&{(\\mathbf{13a};\\ L e\\mathrm{ader}^{\\prime}\\ s\\operatorname{Decision-Making})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\mathcal{F}(q,\\theta),}&{\\qquad}&{(\\mathbf{13b};\\mathrm{Follower}^{\\prime}\\ s\\operatorname{Decision-Making})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the optimization w.r.t. $(q,\\theta)$ is the computation of the best responses for two adversarial players. As a bi-level optimization in Eq. (14a)/(14b), the exact solution is intractable to obtain in a theoretical sense, and the two-stage distributionallly robust optimization is a heuristic approach. ", "page_idx": 20}, {"type": "text", "text": "Meaning of the obtained equilibrium. Here, we can interpret the obtained solution $(q_{*},\\theta_{*})$ from solving Eq. (7) as follows. Given the follower\u2019s decision $\\theta_{*}$ and the induced task risk distribution $F_{\\ell}(l;\\bar{\\theta_{*}})$ , the leader cannot further raise a proposal of a task subset with a probability $1\\mathrm{~-~}\\alpha$ to degradde the tailed expected performance. And this explains the meaning of robust fast adaptation solution $w.r t$ . the tail task risk. ", "page_idx": 20}, {"type": "text", "text": "Input :Task distribution $p(\\tau)$ ; Confidence level $\\alpha$ ; Task batch size $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ ; Learning rates: $\\lambda_{1}$ and $\\lambda_{2}$ .   \nOutput :Meta-trained model parameter $\\theta$ .   \nRandomly initialize the model parameter $\\theta$ ;   \nwhile not converged do Sample a batch of tasks $\\{\\tau_{i}\\}_{i=1}^{B}\\sim p(\\tau)$ ; # The Leader Player\u2019s Decision-Making for $i=1$ to $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ do // inner loop via gradient descent as the fast adaptation Evaluate the gradient: $\\nabla_{\\theta}\\ell(\\mathfrak{D}_{\\tau_{i}}^{S};\\theta)$ in Eq. (5); Perform task-specific gradient updates: $\\theta_{i}\\leftarrow\\theta-\\lambda_{1}\\nabla_{\\theta}^{\\star}\\ell(\\mathfrak{D}_{\\tau_{i}}^{S};\\theta)$ ; end // model the task risk distribution and estimate the quantile Evaluate performance $\\mathcal{L}_{B}=\\{\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q};\\theta_{i})\\}_{i=1}^{\\mathcal{B}}$ ; Estimate $\\mathrm{VaR}_{\\alpha}[\\ell({\\mathcal T},\\theta)]$ and set $\\xi=\\hat{\\xi}_{\\alpha}$ in Eq. (5) with kernel density estimators; Screen the subset $\\mathcal{L}_{\\hat{B}}=\\{\\ell(\\mathfrak{D}_{\\hat{\\tau}_{i}}^{Q};\\theta_{i})\\}_{i=1}^{K}$ with $\\hat{\\xi}_{\\alpha}$ for meta initialization updates; # The Follower Player $^,\\mathtt{s}$ Decision-Making Execute outer loop via gradient descent to increase adaptation robustness: $\\begin{array}{r}{\\theta\\leftarrow\\theta-\\lambda_{2}\\nabla_{\\theta}\\sum_{i=1}^{\\bar{K}}\\ell(\\mathfrak{D}_{\\hat{\\tau}_{i}}^{Q};\\theta_{i})}\\end{array}$ in Eq. (5);   \nend ", "page_idx": 21}, {"type": "text", "text": "Algorithm 2: Meta Training DR- $\\mathrm{CNP+}$ as A Stackelberg Game ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Input :Task distribution $p(\\tau)$ ; Confidence level $\\alpha$ ; Task batch size $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ ; Learning rate $\\lambda$   \nOutput :Meta-trained model parameter $\\theta$ .   \nRandomly initialize the model parameter $\\theta$ ;   \nwhile not converged do Sample a batch of tasks $\\{\\tau_{i}\\}_{i=1}^{B}\\sim p(\\tau)$ ; # The Leader Player $^,\\mathtt{s}$ Decision-Making // model the task risk distribution and estimate the quantile Evaluate performance $\\mathcal{L}_{\\mathcal{B}}=\\{\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q};z,\\theta_{i})\\}_{i=1}^{\\mathcal{B}}$ ; Estimate $\\mathrm{VaR}_{\\alpha}[\\ell(T,\\theta)]\\approx\\hat{\\xi}_{\\alpha}$ with kernel density estimators; Screen the subset $\\mathcal{L}_{\\hat{\\mathcal{B}}}=\\{\\ell(\\mathfrak{D}_{\\hat{\\tau}_{i}}^{Q};\\mathfrak{z},\\theta)\\}_{i=1}^{K}$ with $\\hat{\\xi}_{\\alpha}$ for meta initialization updates; # The Follower Player $^,\\mathtt{s}$ Decision-Making Execute gradient descent to increase adaptation robustness: $\\begin{array}{r}{\\theta\\leftarrow\\theta-\\lambda\\nabla_{\\theta}\\sum_{i=1}^{K}\\ell(\\mathfrak{D}_{\\hat{\\tau}_{i}}^{Q};z,\\theta)}\\end{array}$ ;   \nend ", "page_idx": 21}, {"type": "text", "text": "C.2 Assumptions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We list all of the assumptions mentioned in this work. These assumptions further serve the demonstration of propositions and theorems in the main paper. ", "page_idx": 21}, {"type": "text", "text": "Assumption 1. To proceed, we retain most assumptions from [1] for theoretical analysis, including: ", "page_idx": 21}, {"type": "text", "text": "1. The meta risk function $\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)$ is $\\beta_{\\tau}$ -Lipschitz continuous w.r.t. $\\theta$ ;   \n2. The cumulative distribution $F_{\\ell}(l;\\theta)$ is $\\beta_{\\ell}$ -Lipschitz continuous w.r.t. $l$ , and the normalized density function $p_{\\alpha}(\\tau;\\theta)$ is $\\beta_{\\theta}$ -Lipschitz continuous w.r.t. $\\theta$ ;   \n3. For arbitrary valid $\\theta\\ \\in\\ \\Theta$ and corresponding $p_{\\alpha}(\\tau;\\theta)$ , $\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)$ is bounded: $\\operatorname*{sup}_{\\tau\\in\\Omega_{\\alpha,\\tau}}\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\boldsymbol{\\theta})\\leq\\mathcal{L}_{\\operatorname*{max}}$ . ", "page_idx": 21}, {"type": "text", "text": "Assumption 2. The implicit function $h(\\cdot)$ is $\\beta_{h}$ -Lipschitz continuous w.r.t. $\\theta\\in\\Theta$ , and $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{F}(\\boldsymbol{q},\\boldsymbol{\\theta})$ is $\\beta_{q}$ -Lipschitz continuous w.r.t. $q\\in\\mathcal{Q}_{\\alpha}$ . ", "page_idx": 21}, {"type": "text", "text": "C.3 Proof of Proposition 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proposition 1. The uncertainty set $\\mathcal{Q}_{\\alpha}$ is convex and compact in terms of probability measures. ", "page_idx": 22}, {"type": "text", "text": "Proof: We firstly focus on the convexity of $\\mathcal{Q}_{\\alpha}$ . For any $\\{q_{1}\\,:=\\,q_{1}(\\tau),q_{2}\\,:=\\,q_{2}(\\tau)\\}\\,\\in\\,{\\mathcal{Q}}_{\\alpha}.$ , we partition these two task spaces with non-zero sampling probability mass respectively as $\\tau_{1}\\cup\\tau_{C}$ and $\\mathcal{T}_{2}\\cup\\mathcal{T}_{C}$ . As displayed in Fig. 9, $\\mathcal{T}_{C}$ denotes the shared subset task between $q_{1}$ and $q_{2}$ . Below we show that $\\lambda_{1}q_{1}+\\lambda_{2}q_{2}\\in\\mathcal{Q}_{\\alpha}$ with $\\lambda_{1}+\\lambda_{2}=1$ . This is true because ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(15\\times10)\\times}\\\\ &{=}&{\\int_{\\tau\\in\\mathcal{T}_{1_{1}}\\cup\\tau_{2}}p(\\tau)d\\tau+\\int_{\\tau\\in\\mathcal{T}_{c}}\\Big(\\lambda_{1}p(\\tau)+\\lambda_{2}p(\\tau)\\Big)d\\tau+\\int_{\\tau\\in\\mathcal{T}_{1}}\\lambda_{1}p(\\tau)d\\tau+\\int_{\\tau\\in\\mathcal{T}_{2}}\\lambda_{2}p(\\tau)d\\tau}\\\\ &{=}&{(15}\\\\ &{=0+\\lambda_{1}\\left(\\int_{\\tau\\in\\mathcal{T}_{c}}p(\\tau)d\\tau+\\int_{\\tau\\in\\mathcal{T}_{1}}p(\\tau)d\\tau\\right)+\\lambda_{2}\\left(\\int_{\\tau\\in\\mathcal{T}_{c}}p(\\tau)d\\tau+\\int_{\\tau\\in\\mathcal{T}_{2}}p(\\tau)d\\tau\\right)\\ \\ \\ \\ \\ \\ (15}\\\\ &{=\\lambda_{1}\\int_{\\tau\\in\\mathcal{T}_{1}}p(\\tau)d\\tau+\\lambda_{2}\\int_{\\tau\\in\\mathcal{T}_{2}}p(\\tau)d\\tau}\\\\ &{=}&{(15,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We next demonstrate the compactness of $\\mathcal{Q}_{\\alpha}$ . The distance between two distributions $\\forall\\{q_{1},q_{2}\\}\\in\\mathcal{Q}_{\\alpha}$ can be defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\mathcal{Q}_{\\alpha}}(q_{1},q_{2}):=\\int_{\\tau\\in\\mathcal{T}}\\Big|q_{1}(\\tau)-q_{2}(\\tau)\\Big|d\\tau.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $L^{1}$ space is a Banach space, the compactness is equivalent to the closedness and Boundedness of $\\mathcal{Q}_{\\alpha}$ . Considering a sequence $\\{q_{n}(\\tau)\\in\\mathbf{\\bar{\\mathcal{Q}}}_{\\alpha}\\}$ with the resulting limitation is $q_{*}(\\tau)$ , following the Controlled Convergence Theorem [76], we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\int_{\\tau\\in T}p_{n}(\\tau)-p_{*}(\\tau)d\\tau\\leq\\operatorname*{lim}_{n\\to\\infty}\\int_{\\tau\\in T}\\big|p_{n}(\\tau)-p_{*}(\\tau)\\big|d\\tau=\\operatorname*{lim}_{n\\to\\infty}d_{Q_{\\alpha}}(q_{n},q_{*})=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Due to the symmetry of the distance, we can have $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\int_{\\tau\\in\\mathcal{T}}p_{*}(\\tau)-p_{n}(\\tau)d\\tau\\leq0.}\\end{array}$ . Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{\\tau\\in\\mathcal{T}}p_{*}(\\tau)d\\tau=\\operatorname*{lim}_{n\\to\\infty}\\int_{\\tau\\in\\mathcal{T}}p_{n}(\\tau)d\\tau=1-\\alpha.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "That is, $p_{\\ast}(\\tau)\\in\\mathcal{Q}_{\\alpha}$ , indicating that $\\mathcal{Q}_{\\alpha}$ is a closed set. As the boundedness is clear in the studied problem, this completes the proof of Proposition 1. \u25a0 ", "page_idx": 22}, {"type": "text", "text": "C.4 Proof of Proposition 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proposition 2 (Existence of Equilibrium) Given the Assumption $^{\\,l}$ , there always exists the global Stackelberg equilibrium as the Definition 1 for the studied $\\mathcal{S}\\mathcal{G}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof: Note that $\\Theta$ is compact as a subspace of the Euclidean space. And it is trivial to see that $\\mathcal{F}(q,{\\boldsymbol{\\theta}}):=\\mathbb{E}_{q}\\left[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};{\\boldsymbol{\\theta}})\\right]$ is continuous w.r.t. $\\theta\\in\\Theta$ as $\\ell$ satisfies the $\\beta_{\\tau}$ -Lipschitz continuity in the Assumption 1. ", "page_idx": 22}, {"type": "text", "text": "Here we need to show the continuity of ${\\mathcal{F}}(q,\\theta)$ w.r.t. the collection of probability measures or probability functions $\\mathcal{Q}_{\\alpha}$ . To this end, with $\\forall\\theta\\in\\Theta$ fixed, We consider two metric spaces $\\left(\\mathcal{Q}_{\\alpha},d_{\\mathcal{Q}_{\\alpha}}\\right)$ and $(\\mathcal{L},\\mathbb{R}_{\\mathcal{L}})$ . The map of our interest is $g(q)=\\mathcal{F}(q,\\cdot):\\mathcal{Q}_{\\alpha}\\mapsto\\mathcal{L}\\subseteq\\mathbb{R}^{+}$ . ", "page_idx": 22}, {"type": "image", "img_path": "McrzOo0hwr/tmp/ac5cc48e56b24756af370be002bb8b6ecc271f52278f58557a6cc1f0ab9a5f00.jpg", "img_caption": ["Figure 9: Partition of the task subspace. Here we take two probability measure $\\{q_{1},q_{2}\\}\\in\\mathcal{Q}_{\\alpha}$ for illustration. $\\tau_{1}\\cup\\tau_{C}$ and $\\mathcal{T}_{2}\\cup\\mathcal{T}_{C}$ defines the corresponding task subspaces for $q_{1}$ and $q_{2}$ with non-zero probability mass in the whole space $\\tau$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Naturally, we can have the following inequality: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\vert g(q_{1})-g(q_{2})\\right\\vert=\\left\\vert\\mathbb{E}_{0}\\left[\\ell(3_{r}^{Q},3_{r}^{S};\\theta)\\right]-\\mathbb{E}_{0}\\left[\\ell(3_{r}^{Q},3_{r}^{S};\\theta)\\right]\\right\\vert}&{}&\\\\ {\\leq}&{\\Big\\vert\\int_{\\tau\\in T_{c}^{\\tau}}[q_{1}(\\tau)-q_{2}(\\tau)]\\ell(3_{r}^{Q},3_{r}^{S};\\theta)d\\tau\\Big\\vert}\\\\ &{}&{+\\Big\\vert\\int_{\\tau\\in T_{1}}q_{1}(\\tau)\\ell(3_{r}^{Q},3_{r}^{S};\\theta)d\\tau-\\int_{\\tau\\in T_{2}}q_{2}(\\tau)\\ell(3_{r}^{Q},3_{r}^{S};\\theta)d\\tau\\Big\\vert}\\\\ &{}&{\\leq\\int_{\\tau\\in T_{c}}\\left\\vert q_{1}(\\tau)-q_{2}(\\tau)\\right\\vert\\ell(3_{r}^{Q},3_{r}^{S};\\theta)d\\tau}\\\\ &{}&{+\\int_{\\tau\\in T_{1}}\\left\\vert q_{1}(\\tau)-q_{2}(\\tau)\\right\\vert\\ell(3_{r}^{Q},3_{r}^{S};\\theta)d\\tau+\\int_{\\tau\\in T_{2}}\\left\\vert q_{1}(\\tau)-q_{2}(\\tau)\\right\\vert\\ell(3_{r}^{Q},3_{r}^{S};\\theta)d\\tau}\\\\ &{}&{\\leq3Z_{m a x}\\int_{\\tau\\in T}\\Big\\vert q_{1}(\\tau)-q_{2}(\\tau)\\Big\\vert d\\tau=3Z_{m a x}d Q_{\\infty}(q_{1},q_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies $3\\mathcal{L}_{\\mathrm{max}}$ -Lipschitz continuity of $g(q)$ w.r.t. $\\forall q\\in\\mathcal{Q}_{\\alpha}$ . ", "page_idx": 23}, {"type": "text", "text": "According to the Remark in [77], there always exists the global Stackelberg equilibrium as the Definition 1 when $\\mathcal{Q}_{\\alpha}\\,\\times\\,\\Theta$ is compact and $\\dot{\\mathcal{F}}(q,\\theta)$ is continuous. This completes the proof of Proposition 2. ", "page_idx": 23}, {"type": "text", "text": "C.5 Proof of Quasi-concavity for ${\\mathcal{F}}(q,\\theta)$ w.r.t. $q$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "It can be validated that ${\\mathcal{F}}(q,\\theta)$ is a quasi-concave function $w.r t.\\textit{q}$ , meaning that for any positive number $l\\in\\mathbb{R}_{+}$ , the set $\\{q|q\\in\\mathcal{Q}_{\\alpha},\\mathcal{F}(q,\\theta)>l\\}$ is convex in $\\mathcal{Q}_{\\alpha}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof: According to the conventional definition (i.e., the superlevel set is convex [78]), for all $\\lambda_{1}+\\lambda_{2}=1,q_{1},q_{2}\\in\\{q|\\mathcal{F}(q,\\theta)>l\\}$ , we can have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}\\big(\\lambda_{1}q_{1}+\\lambda_{2}q_{2},\\theta\\big)=\\mathbb{E}_{\\lambda_{1}q_{1}+\\lambda_{2}q_{2}}\\Big[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\Big]}\\\\ &{\\qquad\\qquad\\qquad=\\lambda_{1}\\mathbb{E}_{q_{1}}\\Big[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\Big]+\\lambda_{2}\\mathbb{E}_{q_{2}}\\Big[\\ell(\\mathfrak{D}_{\\tau}^{Q},\\mathfrak{D}_{\\tau}^{S};\\theta)\\Big]}\\\\ &{\\qquad\\qquad\\qquad=\\lambda_{1}\\mathcal{F}\\big(q_{1},\\theta\\big)+\\lambda_{2}\\mathcal{F}\\big(q_{2},\\theta\\big)}\\\\ &{\\qquad\\qquad\\qquad>l.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, $\\lambda_{1}q_{1}+\\lambda_{2}q_{2}\\in\\{q|\\mathcal{F}(q,\\theta)>l\\}$ and the superlevel set is convex, implying that ${\\mathcal{F}}(q,\\theta)$ is quasi-concave w.r.t. $q$ . ", "page_idx": 23}, {"type": "text", "text": "C.6 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 4.1 (Convergence Rate for the Second Player) Let the iteration sequence in optimization be: $\\cdot\\cdot\\cdot\\;\\mapsto\\;\\{q_{t-1},\\theta_{t}\\}\\;\\;\\mapsto\\;\\;\\{q_{t},\\theta_{t+1}\\}\\;\\;\\mapsto\\;\\;\\cdot\\cdot\\;\\mapsto\\;\\;\\{q_{*},\\theta_{*}\\}$ , with the converged equilibirum $(q_{*},\\theta_{*})$ . Under the Assumption 2 and suppose that $\\lVert I\\stackrel{\\cdot}{-}\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{*},\\theta_{*})\\rVert_{2}^{\\cdot}<$ ", "page_idx": 23}, {"type": "text", "text": "$1\\,-\\,\\lambda\\beta_{q}\\beta_{h}$ , we can have $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\frac{||\\theta_{t+1}-\\theta_{*}||_{2}}{||\\theta_{t}-\\theta_{*}||_{2}}\\ \\leq\\ 1}\\end{array}$ , and the iteration converges with the rate $\\left(||I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{*},\\theta_{*})||_{2}+\\lambda\\beta_{q}\\beta_{h}\\right)$ . ", "page_idx": 24}, {"type": "text", "text": "Proof: Let the resulting stationary point be $[q_{*},\\theta_{*}]$ , we denote the difference terms by $\\hat{q}=q-q_{*}$ and $\\widehat{\\theta}=\\theta-\\theta_{*}$ . Then, according to the optimization step, we can have the following equations: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{t+1}=\\theta_{t}-\\lambda\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{t})\\implies\\hat{\\theta}_{t+1}=\\hat{\\theta}_{t}-\\lambda\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we perform the first-order Taylor expansion of the $\\theta$ related function $\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta)$ around $\\theta_{*}$ and can derive: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta)=\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{*})+\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{t};\\theta_{*})(\\theta-\\theta_{*})+\\mathcal{O}(||\\theta-\\theta_{*}||)}\\\\ &{\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{t})\\simeq\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{*})+\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{t};\\theta_{*})(\\theta_{t}-\\theta_{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have the following result with the help of Assumption 2: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{*})\\|_{2}=\\|\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{*})-\\nabla_{\\theta}\\mathcal{F}(q_{*};\\theta_{*})\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\|\\nabla_{\\theta}\\mathcal{F}(h(\\theta_{t});\\theta_{*})-\\nabla_{\\theta}\\mathcal{F}(h(\\theta_{*});\\theta_{*})\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\beta_{q}d_{\\mathcal{Q}_{\\alpha}}(h(\\theta_{t}),h(\\theta_{*}))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\beta_{q}\\beta_{h}\\|\\theta_{t}-\\theta_{*}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With Eq. (19), Eq. (20) and Eq. (21), we can derive the equation that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\theta}_{t+1}=\\hat{\\theta}_{t}-\\lambda\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{t})}\\\\ &{\\qquad\\qquad=\\hat{\\theta}_{t}-\\lambda\\Big[\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{*})+\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{t};\\theta_{*})\\hat{\\theta}_{t}\\Big]}\\\\ &{\\qquad\\quad=\\Big[I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{t};\\theta_{*})\\Big]\\hat{\\theta}_{t}-\\lambda\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{*})}\\\\ &{\\Longrightarrow\\ \\|\\hat{\\theta}_{t+1}\\|_{2}\\leq\\|I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{t};\\theta_{*})\\|_{2}||\\hat{\\theta}_{t}||_{2}+\\lambda\\|\\nabla_{\\theta}\\mathcal{F}(q_{t};\\theta_{*})\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\big(\\|I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{t};\\theta_{*})\\|_{2}+\\lambda\\beta_{q}\\beta_{h}\\big)||\\|\\hat{\\theta}_{t}\\||_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, when $\\Vert I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{*};\\theta_{*})\\Vert_{2}<1-\\lambda\\beta_{q}\\beta_{h}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\frac{||\\hat{\\theta}_{t+1}||_{2}}{||\\hat{\\theta}_{t}||_{2}}\\leq\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\|I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{t};\\theta_{*})\\|_{2}+\\lambda\\beta_{q}\\beta_{h}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\|I-\\lambda\\nabla_{\\theta\\theta}^{2}\\mathcal{F}(q_{*};\\theta_{*})\\|_{2}+\\lambda\\beta_{q}\\beta_{h}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof of Theorem 4.1. ", "page_idx": 24}, {"type": "text", "text": "C.7 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem 4.2 (Asymptotics in the Tail Risk Cases) Under the Assumption 1 and given a batch of tasks $\\{\\tau_{i}\\}_{i=1}^{B}$ , we can have ", "page_idx": 24}, {"type": "equation", "text": "$$\nC V a R_{\\alpha}(\\theta_{T}^{m e t a})-C V a R_{\\alpha}(\\theta_{*})\\leq\\beta_{\\tau}\\|\\theta_{T}^{m e t a}-\\theta_{*}\\|+\\frac{V a R_{\\alpha}^{*}}{1-\\alpha}\\Big(\\mathbb{P}(\\mathcal{T}_{1})-\\mathbb{P}(\\mathcal{T}_{2})\\Big),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{T}_{1}=\\{\\tau:\\ell^{*}<V a R_{\\alpha}^{*},\\ell^{m e t a}\\geq V a R_{\\alpha}^{m e t a}\\},\\mathcal{T}_{2}=\\{\\tau:\\ell^{*}\\geq V a R_{\\alpha}^{*},\\ell^{m e t a}<V a R_{\\alpha}^{m e t a}\\}.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Given a batch of tasks $\\{\\tau_{1},\\cdot\\cdot\\cdot,\\tau_{B}\\}$ and according to the definition of $\\mathrm{CVaR}_{\\alpha}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Cat}_{{\\mathcal{N}},1}(\\sigma_{0}^{\\mathrm{varDelta}})-\\mathrm{Cat}_{{\\mathcal{N}},2}(\\sigma_{0}^{\\mathrm{varDelta}})}\\\\ &{=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1,\\dots,N\\neq n_{i}}^{N}\\mathrm{con}_{\\mathcal{N}}(\\sigma_{i}^{\\mathrm{DF}}(\\hat{\\hat{N}})-\\frac{1}{N}\\sum_{i=1}^{N}\\int_{\\{\\mathrm{CF}\\}})\\mathrm{d}\\hat{\\hat{\\sigma}}^{i}}\\\\ &{=\\int_{{\\mathcal{N}}}\\sum_{i=1}^{N}\\mathrm{con}_{\\mathcal{N}}(\\sigma_{i}^{\\mathrm{DF}}\\hat{\\hat{N}})-\\int_{\\{\\hat{N}\\}}\\gamma_{i}(\\hat{\\hat{N}})d\\hat{\\hat{\\sigma}}^{i}\\int_{{\\mathrm{DF}}}\\mathrm{d}\\hat{\\hat{\\sigma}}^{i}}\\\\ &{=\\frac{1}{\\hat{\\mathcal{N}}}\\left(\\sum_{i=1}^{N}\\gamma_{i}(\\hat{\\hat{N}})^{2}-\\hat{\\mathcal{N}}_{\\mathrm{in}}(\\hat{\\hat{\\sigma}}^{i})\\right)d\\hat{\\hat{\\sigma}}^{i}\\int_{\\mathbb{R}^{n}}\\left(\\int_{\\{\\mathrm{FS}\\}}\\gamma_{i}(\\hat{\\hat{N}})^{2}-\\hat{\\mathcal{N}}_{\\mathrm{in}}(\\hat{\\hat{\\tau}}^{i})d\\hat{\\hat{\\sigma}}^{i}\\right)\\hat{\\hat{\\sigma}}^{i}}\\\\ &{=\\int_{{\\mathcal{N}}}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\hat{\\mathcal{N}}_{i}(\\hat{\\hat{\\sigma}}^{i})\\frac{1}{N}d\\hat{\\hat{\\sigma}}^{i}\\int_{\\mathbb{R}^{n}}\\left(\\sum_{i=1}^{N}\\hat{\\hat{\\sigma}}^{i}\\frac{1}{N}\\sum_{j=1}^{N}\\hat{\\hat{\\sigma}}^{i}(\\hat{\\hat{N}})\\right)d\\hat{\\hat{\\sigma}}^{i}}\\\\ &{\\leq\\beta\\left\\{\\hat{\\sigma}_{i}^{\\mathrm{DF}}-\\hat{\\sigma}_{i}^{\\mathrm{DF}}\\right\\}+\\int_{\\mathbb{R}^{n}}\\sum_{i\\in\\{1,\\dots,N\\neq n_{i}} \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In inequality (25f), ${\\mathcal{T}}_{1}\\,=\\,\\{\\tau\\,:\\,\\ell^{*}\\,<\\,\\mathrm{VaR}_{\\alpha}^{*},\\ell^{\\mathrm{meta}}\\,\\ge\\,\\mathrm{VaR}_{\\alpha}^{\\mathrm{meta}}\\},{\\mathcal{T}}_{2}\\,=\\,\\{\\tau\\,:\\,\\ell^{*}\\,\\ge\\,\\mathrm{VaR}_{\\alpha}^{*},\\ell^{\\mathrm{meta}}\\}\\,=\\,\\mathsf{P a R}_{\\alpha}^{*},$ $\\begin{array}{r}{\\mathrm{VaR}_{\\alpha}^{\\mathrm{meta}}\\big\\},\\mathcal{T}_{3}^{\\mathrm{~\\,~}}=\\;\\{\\tau\\,:\\,\\ell^{*}\\;<\\mathrm{VaR}_{\\alpha}^{*},\\ell^{\\mathrm{meta}}<\\mathrm{~\\widetilde{V}a R}_{\\alpha}^{\\mathrm{meta}}\\},\\mathcal{T}_{4}\\;=\\;\\{\\bar{\\tau}\\,:\\,\\ell^{*}\\;\\geq\\mathrm{~VaR}_{\\alpha}^{*},\\ell^{\\mathrm{meta}}\\geq\\mathrm{~\\widetilde{\\tau}~}\\}.}\\end{array}$ $\\ell^{\\mathrm{meta}}\\;\\geq\\;\\mathrm{VaR}_{\\alpha}^{\\mathrm{meta}}\\}$ . Moreover, this inequality holds due to the $\\beta_{\\tau}$ \u2212Lipschitz continuous of $\\ell(\\mathfrak{D}_{\\tau};\\theta)$ . In Eq. $(25\\mathrm{g})$ , $p_{\\alpha}(\\tau;\\theta_{T}^{\\mathrm{meta}})\\,=\\,p_{\\alpha}(\\tau;\\theta_{*})\\,=\\,0$ when $\\tau\\in{\\mathcal{T}}_{3}$ , and $\\begin{array}{r}{p_{\\alpha}(\\tau;\\theta_{T}^{\\mathrm{meta}})\\,=\\,p_{\\alpha}(\\tau;\\theta_{*})\\,=\\,\\frac{p(\\tau)}{1-\\alpha}}\\end{array}$ when $\\tau\\in{\\mathcal{T}}_{4}$ . Thus, we complete the proof of Theorem 4.2. \u25a0 ", "page_idx": 25}, {"type": "text", "text": "C.8 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 4.3 (Generalization Bound in the Tail Risk Cases) Given a collection of task samples $\\{\\tau_{i}\\}_{i=1}^{B}$ and corresponding meta datasets, we can derive the following generalization bound in the presence of tail risk: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R(\\theta_{*})\\leq\\widehat{R}(\\theta_{*})+\\sqrt{\\frac{2\\left(\\frac{\\alpha}{1-\\alpha}\\mathcal{L}_{\\operatorname*{max}}^{2}+\\mathbb{V}_{\\tau_{i}\\sim p_{\\alpha}(\\tau)}\\left[\\ell\\left(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*}\\right)\\right]\\right)\\ln\\left(\\frac{1}{\\epsilon}\\right)}{\\mathcal{B}}}}\\\\ {+\\frac{1}{3(1-\\alpha)}\\frac{\\mathcal{L}_{\\operatorname*{max}}}{\\mathcal{B}}\\left(2\\ln\\left(\\frac{1}{\\epsilon}\\right)+3\\alpha\\mathcal{B}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the inequality holds with probability at least $1-\\epsilon$ and $\\epsilon\\in(0,1),\\,\\mathbb{V}[\\cdot]$ denotes the variance operation, and ${\\mathcal{L}}_{\\mathrm{max}}$ is from the Assumption $^{\\,l}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. $R(\\theta_{*})-\\widehat{R}(\\theta_{*})$ can be decomposed to two parts, i.e., ", "page_idx": 25}, {"type": "equation", "text": "$$\nR(\\theta_{*})-\\widehat{R}(\\theta_{*})=\\Big(R(\\theta_{*})-\\widehat{R}_{w}(\\theta_{*})\\Big)+\\Big(\\widehat{R}_{w}(\\theta_{*})-\\widehat{R}(\\theta_{*})\\Big).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the first part (i.e., $R(\\theta_{*})\\mathrm{~-~}\\widehat{R}_{w}(\\theta_{*}))$ , we will adopt the Bernstein\u2019s inequality to provide an upper bound. Regarding $\\begin{array}{r}{\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*})-R(\\theta_{*})}\\end{array}$ as a random variable with respect to $\\tau_{i}$ and according to Assumption 1, we know that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*})-R(\\theta_{*})\\leq\\frac{1}{1-\\alpha}\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*})-R(\\theta_{*})\\leq\\frac{1}{1-\\alpha}\\mathcal{L}_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, following Bernstein\u2019s inequality [79], we know that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbb{P}}\\left(\\left|\\displaystyle\\frac{1}{{\\cal B}}\\sum_{i=1}^{\\cal B}\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*})-R(\\theta_{*})\\right|\\ge\\xi\\right)}\\\\ &{=\\,{\\mathbb{P}}\\left(\\left|\\widehat{R}_{w}(\\theta_{*})-R(\\theta_{*})\\right|\\ge\\xi\\right)}\\\\ &{\\le\\,\\exp\\left(-\\frac{B\\xi^{2}}{2\\mathbb{V}_{\\tau_{i}}\\left[\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*})-R(\\theta_{*})\\right]+\\frac{2}{3}\\frac{1}{1-\\alpha}\\mathcal{L}_{\\operatorname*{max}}\\xi}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{\\tau}\\left[\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\langle(\\hat{\\mathbf{x}}_{\\tau}^{0},\\hat{\\mathbf{x}}_{\\tau}^{*};\\hat{\\boldsymbol{\\theta}}_{\\mathbf{s}})-R(\\boldsymbol{\\theta},\\cdot)\\right]}\\\\ &{=\\nabla_{\\tau}\\left[\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\langle(\\hat{\\mathbf{x}}_{\\tau}^{0},\\hat{\\mathbf{x}}_{\\tau}^{*};\\hat{\\boldsymbol{\\theta}}_{\\mathbf{s}})\\rangle\\right]}\\\\ &{=\\mathbb{E}_{\\tau}\\left(\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\langle(\\hat{\\mathbf{x}}_{\\tau}^{0},\\hat{\\mathbf{x}}_{\\tau}^{*};\\hat{\\boldsymbol{\\theta}}_{\\mathbf{s}})\\rangle\\right)^{2}\\quad-\\left(\\mathbb{E}_{\\tau}\\left(\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\langle(\\hat{\\mathbf{x}}_{\\tau}^{0},\\hat{\\mathbf{x}}_{\\tau}^{*};\\hat{\\boldsymbol{\\theta}}_{\\mathbf{s}})\\rangle\\right)^{2}\\right)}\\\\ &{=\\int_{\\mathbb{R}}\\Big(\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\langle(\\hat{\\mathbf{x}}_{\\tau}^{0},\\hat{\\mathbf{x}}_{\\tau}^{*};\\hat{\\boldsymbol{\\theta}}_{\\mathbf{s}})\\rangle\\Big)^{2}\\quad\\scriptstyle\\mathcal{p}(\\tau)d\\tau-\\Big(\\int_{\\mathbb{R}}\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\langle(\\hat{\\mathbf{x}}_{\\tau}^{0},\\hat{\\mathbf{x}}_{\\tau}^{*};\\hat{\\boldsymbol{\\theta}}_{\\mathbf{s}})\\rangle\\Big)^{2}}\\\\ &{=\\int_{\\mathbb{R}}\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\langle(\\hat{\\mathbf{x}}_{\\tau}^{0},\\hat{\\mathbf{x}}_{\\tau}^{*};\\hat{\\boldsymbol{\\theta}}_{\\mathbf{s}})^{2}\\rangle_{\\mathbb{R}(\\tau)}d\\tau+\\Big(\\int_{\\mathbb{R}}\\frac{\\int_{\\mathbb{R}}(\\tau)}{p(\\tau)}\\frac{\\partial_{\\tau}^{2}}{\\partial\\tau_{\\ t}^{2}}\\langle\\hat\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Setting $\\epsilon$ to match the upper bound in inequality (29c) shows that with probability at least $1-\\epsilon$ , the following bound holds: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\widehat{R}_{w}(\\theta_{*})-R(\\theta_{*})\\right|\\leq\\sqrt{\\frac{2(\\frac{\\alpha}{1-\\alpha}\\mathcal{L}_{\\operatorname*{max}}^{2}+\\mathbb{V}_{\\tau\\sim p_{\\alpha}(\\tau)})\\ln\\left(\\frac{1}{\\epsilon}\\right)}{\\mathcal{B}}}+\\frac{2\\mathcal{L}_{\\operatorname*{max}}\\ln\\left(\\frac{1}{\\epsilon}\\right)}{3(1-\\alpha)\\mathcal{B}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the second part (i.e., $\\widehat{R}_{w}(\\theta_{*})-\\widehat{R}(\\theta_{*}))$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widehat{R}_{w}(\\theta_{*})-\\widehat{R}(\\theta_{*})\\ =}&{\\displaystyle\\frac{1}{B}\\sum_{i=1}^{B}\\Big(\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}-\\delta(\\tau_{i})\\Big)\\ell(\\mathfrak{D}_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta_{*})}\\\\ &{\\leq\\displaystyle\\frac{C_{\\operatorname*{max}}}{B}\\sum_{i=1}^{B}\\Big(\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}-\\delta(\\tau_{i})\\Big)}\\\\ &{=\\displaystyle\\frac{C_{\\operatorname*{max}}}{B}\\sum_{i=1}^{B}\\Big(\\frac{p_{\\alpha}(\\tau_{i})}{p(\\tau_{i})}-1\\Big)\\delta(\\tau_{i})}\\\\ &{=\\displaystyle\\frac{\\alpha}{1-\\alpha}\\frac{Z_{\\operatorname*{max}}}{B}\\sum_{i=1}^{B}\\delta(\\tau_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In summary, we can obtain an upper bound of $R(\\theta_{*})-\\widehat{R}(\\theta_{*})$ . That is, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R(\\theta_{*})-\\widehat{R}(\\theta_{*})\\,\\leq\\big|\\widehat{R}_{w}(\\theta_{*})-R(\\theta_{*})\\big|+\\widehat{R}_{w}(\\theta_{*})-\\widehat{R}(\\theta_{*})}}\\\\ &{\\leq\\sqrt{\\frac{2\\big(\\frac{\\alpha}{1-\\alpha}\\mathcal{L}_{\\operatorname*{max}}^{2}+\\mathbb{V}_{\\tau\\sim p_{\\alpha}(\\tau)}\\big)\\ln\\big(\\frac{1}{\\epsilon}\\big)}{B}}+\\frac{2\\mathcal{L}_{\\operatorname*{max}}\\ln\\big(\\frac{1}{\\epsilon}\\big)}{3(1-\\alpha)B}+\\frac{\\alpha}{1-\\alpha}\\frac{\\mathcal{L}_{\\operatorname*{max}}}{B}\\sum_{i=1}^{B}\\delta(\\tau_{i})}\\\\ &{\\leq\\sqrt{\\frac{2\\big(\\frac{\\alpha}{1-\\alpha}\\mathcal{L}_{\\operatorname*{max}}^{2}+\\mathbb{V}_{\\tau\\sim p_{\\alpha}(\\tau)}\\big)\\ln\\big(\\frac{1}{\\epsilon}\\big)}{B}}+\\frac{1}{3(1-\\alpha)}\\frac{\\mathcal{L}_{\\operatorname*{max}}}{B}\\left(2\\ln\\left(\\frac{1}{\\epsilon}\\right)+3\\alpha B\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This completes the proof of Theorem 4.3. ", "page_idx": 27}, {"type": "text", "text": "C.9 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 4.4 Let $F_{\\ell-K D E}^{-1}(\\alpha;\\theta)=V a R_{\\alpha}^{K D E}[\\ell(\\mathcal{T},\\theta)]$ and $F_{\\ell}^{-1}(\\alpha;\\theta)=V a R_{\\alpha}[\\ell(\\mathcal{T},\\theta)]$ . Suppose that $K(x)$ is lower bounded by a constant, $\\forall x$ . For any $\\epsilon>0$ , with probability at least $1-\\epsilon$ , we can have the following bound: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta}\\,\\left(F_{\\ell\\cdot K D E}^{-1}(\\alpha;\\theta)-F_{\\ell}^{-1}(\\alpha;\\theta)\\right)\\leq\\mathcal{O}\\left(\\frac{h_{\\ell}}{\\sqrt{B*\\log B}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. For any constant $M$ , we firstly notice that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\left(\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}\\left(F_{\\ell\\cdot\\mathrm{KDE}}^{-1}(\\alpha;\\theta)-F_{\\ell}^{-1}(\\alpha;\\theta)\\right)\\leq M\\right)\\geq1-\\epsilon}\\\\ &{\\Leftrightarrow\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\left(\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}\\left(F_{\\ell\\cdot\\mathrm{KDE}}^{-1}(\\alpha;\\theta)-F_{\\ell}^{-1}(\\alpha;\\theta)\\right)\\geq M\\right)\\leq\\epsilon}\\\\ &{\\Leftrightarrow\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\left(\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}\\left(F_{\\ell}(t-M;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\right)\\geq0\\right)\\leq\\epsilon,\\quad t=F_{\\ell\\cdot\\mathrm{KDE}}^{-1}(\\alpha;\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For any $\\theta$ and $t$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\big(F_{\\ell}(t-M;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\geq0\\big)\\leq\\epsilon}\\\\ &{\\Leftrightarrow\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\big(F_{\\ell}(t-M;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t-M;\\theta)+F_{\\ell\\cdot\\mathrm{KDE}}(t-M;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\geq0\\big)}\\\\ &{\\Leftrightarrow\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\big(F_{\\ell}(t-M;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t-M;\\theta)\\geq F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t-M;\\theta)\\big)\\leq\\epsilon}\\\\ &{\\Leftrightarrow\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\big(F_{\\ell}(t;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\geq F_{\\ell\\cdot\\mathrm{KDE}}(t+M;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\big)\\leq\\epsilon}\\\\ &{\\Leftrightarrow\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\big(F_{\\ell}(t;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\geq\\frac{M}{B h_{\\ell}}\\Big(\\sum_{i=1}^{B}K\\big(\\frac{t-\\ell(\\exists_{\\tau_{i}}^{Q},\\mathfrak{D}_{\\tau_{i}}^{S};\\theta)}{h_{\\ell}}\\big)\\Big)+o(M)\\big)\\leq\\epsilon}\\\\ &{\\Leftrightarrow\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\big(F_{\\ell}(t;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\geq\\frac{M}{h_{\\ell}}\\big)\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\kappa_{\\mathrm{min}}$ is the lower bound of the kernel function $K(x),i.e.,K(x)\\geq K_{\\operatorname*{min}},\\forall x.$ ", "page_idx": 27}, {"type": "text", "text": "According to Theorem 3 of [80], we know that for any $\\epsilon>0$ , with probability at least $1-\\epsilon$ , the following inequality holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\tau_{1},\\cdots,\\tau_{B}}\\left(\\operatorname*{sup}_{\\theta\\in\\Theta,t\\geq0}\\left(F_{\\ell}(t;\\theta)-F_{\\ell\\cdot\\mathrm{KDE}}(t;\\theta)\\right)\\geq\\frac{C}{\\sqrt{B*\\log B}}\\right)\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $C$ is a constant. Let $\\begin{array}{r}{M=\\frac{h_{\\ell}C}{\\mathcal{K}_{\\operatorname*{min}}\\sqrt{\\mathcal{B}*\\log B}}}\\end{array}$ . Thus, the Eq. (35f) holds and we complete the proof of Theorem 4.4. \u25a0 ", "page_idx": 27}, {"type": "text", "text": "C.10 Additional Theorem ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To gain more theoretical insights into a popular meta-learning method\u2014MAML [25], we provide the following Theorem C.1. Before proceeding, we introduce some notations. During meta-training, a finite number of task instances are observed by first sampling a task from the distribution $p(\\tau)$ . Each task $D_{\\tau_{i}}$ comprises a collection of $m_{i}$ data points $\\{(\\mathfrak{D}_{i,j}^{S},\\mathfrak{D}_{i,j}^{Q})\\}_{j=1}^{m_{i}}$ , which are distributed over $\\mathcal{Z}$ with each data point drawn from a distribution $D_{i}$ . For some risk $\\ell$ , define the family of functions $\\mathcal{F}_{\\mathcal{Z}}:=\\{\\ell(\\theta-\\bar{\\lambda}\\nabla_{\\theta}\\ell(\\mathfrak{D}_{\\tau_{i}}^{S};\\theta),\\mathfrak{D}_{\\tau_{i}}^{Q}):\\theta\\in\\Theta\\}$ . For each task $D_{\\tau_{i}}$ , the Rademacher complexity of $\\mathcal{F}$ on $m_{i}$ samples is ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{R}_{m_{i}}^{i}(\\mathcal{F}_{\\mathcal{Z}})=\\mathbb{E}_{(\\mathfrak{D}_{i,j}^{S},\\mathfrak{D}_{i,j}^{Q})\\sim(D_{i})^{m_{i}}}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\theta\\in\\Theta}\\frac{1}{m_{i}}\\sum_{j=1}^{m_{i}}\\epsilon_{j}\\ell(\\theta-\\lambda\\nabla_{\\theta}\\ell(\\mathfrak{D}_{i,j}^{S};\\theta),\\mathfrak{D}_{i,j}^{Q})\\right],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the $\\epsilon_{j}$ \u2019s are Rademacher random variables. Let $F_{i}(\\theta)\\,=\\,\\mathbb{E}_{D_{i}}\\ell(\\theta\\,-\\,\\lambda\\nabla_{\\theta}\\ell(\\mathfrak{D}_{i,j}^{S};\\theta),\\mathfrak{D}_{i,j}^{Q})$ , $\\begin{array}{r}{\\hat{F}_{i}(\\theta)=\\frac{1}{m_{i}}\\sum_{j=1}^{m_{i}}\\ell(\\theta-\\lambda\\nabla_{\\theta}\\ell(\\mathfrak{D}_{i,j}^{S};\\theta),\\mathfrak{D}_{i,j}^{Q})}\\end{array}$ . Denote by $\\theta^{*}$ the optimal model parameter under the two-stage algorithm. Theorem C.1 provides generalization of the algorithm to new tasks. ", "page_idx": 28}, {"type": "text", "text": "Theorem C.1 (Generalization Bound for MAML in the Tail Risk Cases). For a new task $\\tau_{\\boldsymbol{B}+1}$ with distribution $D_{\\mathcal{B}+1}$ , $\\textstyle i f D_{B+1}=\\sum_{i=1}^{B}a_{i}D_{i}$ , then with probability at least $1-\\delta$ for any $\\delta>0$ , we can have ", "page_idx": 28}, {"type": "equation", "text": "$$\nF_{B+1}(\\theta^{*})\\leq\\operatorname*{max}_{i}\\hat{F}_{i}(\\theta^{*})+\\sum_{i=1}^{B}\\left[2a_{i}\\mathcal{R}_{m_{i}}^{i}(\\mathcal{F}_{\\mathcal{Z}})+a_{i}\\sqrt{\\frac{\\log{(B/\\delta)}}{2m_{i}}}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. The proof consists of two parts. We first explore the generalization to new instances of previously-seen tasks. Then we solve the generalization to new tasks. ", "page_idx": 28}, {"type": "text", "text": "Step 1. For any sample set $\\boldsymbol{\\mathcal{A}}=\\{(\\mathfrak{D}_{i,j}^{S},\\mathfrak{D}_{i,j}^{Q})\\}_{j=1}^{m_{i}}$ , define $\\Phi(A)=\\operatorname*{sup}_{\\ell\\in{\\mathcal{F}}_{\\mathcal{Z}}}F_{i}(\\theta)-{\\hat{F}}_{i}(\\theta)$ . Let $\\boldsymbol{\\mathcal{A}}$ and $\\mathcal{A}^{\\prime}:=\\{((\\mathfrak{D}_{i,j}^{S})^{\\prime},(\\mathfrak{D}_{i,j}^{Q})^{\\prime})\\}_{j=1}^{m_{i}}$ be two samples that differ by exactly one point. According to the fact $\\operatorname*{sup}_{x}f(x)\\,-\\,\\operatorname*{sup}_{x}g(x)\\,\\leq\\,\\operatorname*{sup}_{x}(f(x)\\,-\\,g(x))$ , we know that $\\begin{array}{r}{\\Phi({\\mathcal A}^{\\prime})\\,-\\,\\Phi({\\mathcal A})\\,\\le\\,\\frac{1}{m_{i}}}\\end{array}$ due to the difference in exactly one point. Similarly, we can obtain $\\begin{array}{r}{\\Phi({\\mathcal{A}})\\,-\\,\\Phi({\\mathcal{A}}^{\\prime})\\,\\leq\\,\\frac{1}{m}}\\end{array}$ , thus $\\begin{array}{r}{\\left|\\Phi(\\mathcal{A})-\\Phi(\\mathcal{A}^{\\prime})\\right|\\leq\\frac{1}{m}}\\end{array}$ . Following McDiarmid\u2019s inequality, for any $\\delta>0$ , with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Phi(A)\\leq\\mathbb{E}_{A}[\\Phi({\\cal A})]+\\sqrt{\\frac{\\log(2/\\delta)}{2m_{i}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We next bound the expectation of the right-hand side of inequality (39) as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{4}|\\{\\Phi(\\lambda)\\}|}&{\\mathbb{E}_{4}\\Big[\\operatorname*{sup}_{i\\in D_{i}^{0}}F_{i}(\\theta)-F_{i}(\\theta)\\Big]}\\\\ &{=\\mathbb{E}_{4}\\left[\\operatorname*{sup}_{i\\in D_{i}^{0}}\\mathbb{E}_{4}\\Big[\\mathbb{E}_{6}(F_{i}(\\theta))-\\hat{F}_{i}(\\theta)\\Big]\\right]}\\\\ &{\\le\\mathbb{E}_{4}\\lambda\\left[\\operatorname*{sup}_{i\\in D_{i}^{0}}\\mathbb{E}_{6}(F_{i}(\\theta))-\\hat{F}_{i}(\\theta)\\right]}\\\\ &{=\\mathbb{E}_{4}\\lambda\\operatorname*{tar}\\left[\\operatorname*{sup}_{i\\in D_{i}^{0}}\\frac{1}{m_{i}}\\frac{\\sqrt{m}}{\\mu_{i}}\\left((\\mathbb{D}_{u_{i}^{0}}^{0},\\mathbb{D}_{u_{i}^{0}}^{0})-\\ell((\\mathbb{D}_{u_{i}^{0}}^{0})^{\\prime},(\\mathbb{D}_{u_{i}^{0}}^{0})^{\\prime})\\right)\\right]}\\\\ &{=\\mathbb{E}_{4}\\lambda_{\\ell}\\left[\\operatorname*{sup}_{i\\in D_{i}^{0}}\\frac{1}{m_{i}}\\frac{\\sqrt{m}}{\\mu_{i}}\\varphi\\left(([\\Omega_{u_{i}^{0}}^{0},\\mathbb{D}_{u_{i}^{0}}^{0})-\\ell((\\mathbb{D}_{u_{i}^{0}}^{0})^{\\prime},(\\mathbb{D}_{u_{i}^{0}}^{0})^{\\prime})\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{4}\\lambda_{\\ell}\\left[\\operatorname*{sup}_{i\\in D_{i}^{0}}\\frac{1}{m_{i}}\\frac{\\sqrt{m}}{\\mu_{i}}\\varphi_{i}^{i}([\\bar{\\Omega}_{u_{i}^{0}}^{0},\\mathbb{D}_{u_{i}^{0}}^{0}])+\\mathbb{E}_{4}\\lambda_{\\ell}\\left[\\operatorname*{sup}_{i\\in D_{i}}\\frac{1}{m_{i}}\\frac{\\sqrt{m}}{\\mu_{i}}\\varphi_{i}^{i}([\\bar{\\Omega}_{u_{i}^{0}}^{0},\\gamma(\\Omega \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Eq. (40) uses the law of total expectation. Inequality (41) holds by Jensen\u2019s inequality and the convexity of the supremum function. In Eq. (42), $\\ell(\\mathfrak{D}_{i,j}^{S},\\mathfrak{D}_{i,j}^{Q})\\,:=\\,\\ell(\\theta-\\lambda\\nabla_{\\theta}\\ell(\\mathfrak{D}_{i,j}^{S};\\theta),\\mathfrak{D}_{i,j}^{Q})$ , ", "page_idx": 28}, {"type": "text", "text": "where $(\\mathfrak{D}_{i,j}^{S},\\mathfrak{D}_{i,j}^{Q})\\in\\mathcal{A}$ . Following inequality (39), we can know that ", "page_idx": 29}, {"type": "equation", "text": "$$\nF_{i}(\\theta)\\leq\\hat{F}_{i}(\\theta)+2\\mathcal{R}_{m_{i}}(\\mathcal{F}_{\\mathcal{Z}})+\\sqrt{\\frac{\\log(2/\\delta)}{2m_{i}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Step 2. Since the new distribution $D_{\\mathcal{B}+1}$ is the convex combination of $D_{i},\\forall i=1,\\cdots\\,,B$ , we have $\\begin{array}{r}{F_{\\mathcal{B}+1}\\mathopen{}\\mathclose\\bgroup\\left(\\theta\\aftergroup\\egroup\\right)=\\sum_{i=1}^{\\mathcal{B}}a_{i}F_{i}\\mathopen{}\\mathclose\\bgroup\\left(\\theta\\aftergroup\\egroup\\right)}\\end{array}$ . Accordingly, with probability at least $1-\\delta$ over the choice of samples used to compute $\\hat{F}(\\theta)$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\nF_{B+1}(\\theta^{*})=\\sum_{i=1}^{B}a_{i}F_{i}(\\theta^{*})\\leq\\sum_{i=1}^{B}\\left[a_{i}\\hat{F}_{i}(\\theta^{*})+2a_{i}\\mathcal{R}_{m_{i}}(\\mathcal{F}_{Z})+a_{i}\\sqrt{\\frac{\\log(2/\\delta)}{2m_{i}}}\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which yields that ", "page_idx": 29}, {"type": "equation", "text": "$$\nF_{B+1}(\\theta^{*})\\leq\\operatorname*{max}_{i}\\hat{F}_{i}(\\theta^{*})+\\sum_{i=1}^{B}\\left[2a_{i}\\mathcal{R}_{m_{i}}^{i}(\\mathcal{F}_{\\mathcal{Z}})+a_{i}\\sqrt{\\frac{\\log{(2/\\delta)}}{2m_{i}}}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In summary, the two steps complete the proof of Theorem C.1. ", "page_idx": 29}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "D.1 Benchmark Details & Neural Architectures & Opensource Codes ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here, we illustrate all meta learning benchmark purposes in Fig. 10, which includes sinusoid regression, pendulum system identification, few-shot image classification, and meta reinforcement learning. We no longer run experiments on the Omniglot dataset, as most baselines can achieve SOTA performance and cannot tell the difference well from the openreview of [1]. ", "page_idx": 29}, {"type": "image", "img_path": "McrzOo0hwr/tmp/b7bcc08540c00bf01c2e7f052d642fa2d3738bbc06267bbeb2889f5aff67a3cb.jpg", "img_caption": ["Figure 10: Typical meta learning benchmarks in evaluation. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Sinusoid regression: In [1, 42], a lot of easy tasks and limited challenging tasks are sampled for meta-training, with the tasks from the whole space employed in evaluation. The default range of the phase parameter is $B\\in[0,\\pi]$ , while those of the amplitude are $A\\in[0.1,1.05]$ for easy tasks and $A\\in[4.95,5.0]$ for challenging tasks. Generally, sinusoid functions with larger amplitudes are hard to adapt from a few support data points. The mean square error works as the risk function to measure the gap between the predicted value $f(x)$ and the actual value. We set the task batch 50 for 5-shot and 25 for 10-shot, and the maximum iteration number is 70000. We refer the reader to TR-MAML and DR-MAML for all of the setups. ", "page_idx": 29}, {"type": "text", "text": "We retain the neural architectures [42, 1] in for all MAML like methods. In detail, all methods take a multilayer perceptron with two hidden layers and 40 ReLU activation units in each layer. The inner loop is achieved via one stochastic gradient descent step. As for CNP like methods, please refer to the vanilla set-up in [15] (The Github link is attached here: https://github.com/ google-deepmind/neural-processes). ", "page_idx": 29}, {"type": "text", "text": "As the task space is hugh, there is no way to exactly estimate the risk quantile. Hence, the Oracle quantile in Fig. 5 is roughly computed from the sampled 100 tasks given the pretrained DR-MAML $^+$ . The rationale behind this operation is that increasing the population number in statistics reduces the quantile estimate bias. ", "page_idx": 29}, {"type": "text", "text": "System identification: The pendulum system is a classical environment in the OpenAI gym (environment details are: https://github.com/openai/gym/blob/master/gym/envs/classic_ control/pendulum.py), and it is an actuated joint with one fixed end. The goal of system identification for the pendulum system is to predict the state transition given arbitrary actions with several randomly collected transitions as the support dataset. The observation is a tuple in the form $(\\cos\\theta,\\sin\\theta,\\theta^{\\prime})$ , where $\\theta\\in[\\pi,\\pi]$ . The action is in the range $a\\in[-2.0,2.0]$ and the torque is applied to the pendulum body. The mass $m$ and the length $l$ of the pendulum follows a uniform distribution $(m,l)\\,\\,{\\bar{\\sim}}\\,\\mathcal{U}([0.4,1.6]\\,\\times[0.4,1.6])$ , sampled variables configure a Markov decision process as the task. In each batch, there are 16 tasks, and each task comprises 200 data points. Specifically, 10 few-shot data points are randomly sampled to enable system identification per task, denoted as 10-shot. For 20-shot cases, the number of data points in support dataset is 20. And the maximum iteration number is 5000. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "For all MAML-like methods, the neural architecture used here is a multilayer perceptron with three hidden layers of 128 hidden units each and the activation function is ReLU. The learning rate for both the inner and outer loops is set at 1e-4. ", "page_idx": 30}, {"type": "text", "text": "Few-shot image classification: The few-shot image classification is mostly described as an N-way K-shot classification, where $\\mathtt{N}$ classes with K-labeled instances for each are considered. The dataset is organized in the same manner as that in [81, 42, 1]: These include 64 classes for meta-training, with the rest 36 classes for meta-testing. We generate each task in the way: 8 meta-training tasks from the class $\\{6,7,7,8,8,9,9,10\\}$ are randomly generated from 64 meta-training classes; the remaining classes are organized similarly. As a result, each task is constructed from sampling one image from five classes, corresponding to a 5-way 1-shot problem. The task batch is set 4 with a maximum number of iterations of 60000 in meta-training. ", "page_idx": 30}, {"type": "text", "text": "For all MAML-like methods, the neural architecture used here is a four-layer convolutional neural network for the mini-ImageNet datasets. The inner loop is achieved via one stochastic gradient descent step. We refer the reader to TR-MAML and DR-MAML for all of the setups (The Github link is attached here https://github.com/lgcollins/tr-maml). ", "page_idx": 30}, {"type": "text", "text": "Meta reinforcement learning: 2D Navigation is a classical meta reinforcement learning benchmark where efficient explorations matter. The task in 2D Navigation is to guide the point robot to take move actions for a purpose of reaching a specific goal location from the step-wise reward. The reward the agent receives from the environment is based on the distance to the goal, and 20 episodes work as the support dataset for navigation fast adaptation. In terms of the task distribution, we sample tasks from a uniform distribution $\\mathcal{U}([-0.5,0.5]\\times[-0.5,0.5])$ over goal locations. ", "page_idx": 30}, {"type": "text", "text": "As for the neural architecture for policy network set-ups, we refer the reader to vanilla MAML (Github link is attached here https://github.com/tristandeleu/pytorch-maml-rl) and CAVIA (The Github link is attached here https://github.com/lmzintgraf/cavia/tree/master/ $\\mathbf{\\check{r}}\\mathbf{1}$ ). And trust region policy optimization works for policy optimization. ", "page_idx": 30}, {"type": "table", "img_path": "McrzOo0hwr/tmp/62d4023eb2c1645b07a8a4e783a40333e6557b04734af59e5ad6d34022b1d466.jpg", "table_caption": ["Table 4: Computational and memory cost in MaPLe relevant experiments. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Few-Shot Image Classification with MaPLe [69]: The stochastic gradient descent is the default optimizer with the learning rate 0.0035, and A6000 GPUs work for computations. We examine tail task risk minimization effectiveness on three large datasets. The class number split setup in datasets (class number to train/validate/test) is TieredImageNet (351/97/160) [82], ImagenetA (128/32/40) [83], and ImagenetSketch (640/160/200) [84]. Table 4 reports the overall training time and memory, where the vanilla MaPLe serves as the anchor point, and $^+$ means additional costs from the two-stage operation. For details of experimental implementations and setups, feel free to access our code at https://github.com/lvyiqin/DRMAML. ", "page_idx": 30}, {"type": "text", "text": "D.2 Modules in Python ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This subsection includes the impelementation of KDE for the studied strategy. Here, the example of the hinge loss is illustrated as follows. ", "page_idx": 30}, {"type": "text", "text": "import numpy as np import torch from scipy.stats import gaussian_kde from scipy.optimize import brentq   \n6 def loss(batch_loss , confidence_level ):   \n7 # estimate the VaR_alpha according to kernel density estimator kde $=$ gaussian_kde(batch_loss)   \n9 try:   \n10 target_func $=$ lambda x: kde. integrate_box_1d (-np.inf , x) confidence_level   \n11 VaR_alpha $=$ brentq(target_func , np.min(batch_loss), np.max( batch_loss))   \n12 except ValueError:   \n13 x = np.linspace(np.min(batch_loss), np.max(batch_loss), 1000)   \n14 pdf $=$ kde.evaluate(x)   \n15 cdf $=$ np.cumsum(pdf) / np.sum(pdf)   \n16 index $=$ np.argmax(cdf $>=$ confidence_level )   \n17 VaR_alpha $=$ x[index]   \n18   \n19 # calculate the meta loss   \n20 tail_loss $=$ [i - VaR_alpha if (i - VaR_alpha) > 0 else torch. tensor (0.).cuda () for i in batch_loss]   \n21 new_batch_loss $=$ torch.stack(tail_loss).mean ()   \n22 factor $=$ 1 / (1 - confidence_level )   \n23 loss_meta $=$ VaR_alpha $^+$ factor $^\\ast$ new_batch_loss   \n24 return loss_meta Listing 1: The calculation process of $\\mathrm{CVaR}_{\\alpha}$ objective. ", "page_idx": 31}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Due to the page limit in the main paper, we include additional experiments and corresponding results in this section. ", "page_idx": 31}, {"type": "text", "text": "E.1 Evaluation with Other Robust Meta Learners ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In addition to MAML, we apply a similar modification to CNP, which results in TR-CNP, DRO-CNP, DR-CNP, and DR- $\\mathrm{\\Delta{CNP+}}$ (DR-CNP with KDE for $\\operatorname{VaR}_{\\alpha}$ estimates). We report the meta testing results on sinusoid regression and pendulum system identification benchmarks. ", "page_idx": 31}, {"type": "text", "text": "As illustrated in Table $5/6$ , all methods achieve comparable average performance in sinusoid and pendulum system identification. Regarding $\\mathrm{CVaR}_{\\alpha}$ , DR-CNP\u2019s improvement is relatively marginal over others except DR- $\\mathrm{CNP+}$ . Compared to MAML, CNP seems more sensitive to quantile estimate accuracies when meeting with the studied strategies. ", "page_idx": 31}, {"type": "text", "text": "Table 5: MSEs for Sinusoid 5-shot with reported standard deviations (5 runs). With $\\alpha=0.7$ , the best results are in bold. ", "page_idx": 31}, {"type": "table", "img_path": "McrzOo0hwr/tmp/f107acf448e879e645e2222f0c8edcf8c248376965a83348efacc51138d509d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "E.2 Numeric Results in Tables and Histograms ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "As the improving tricks in this work are regarding the quantile estimators, here we particularly include the quantitative results to show the difference between DR-MAML and DR-MAML $^{,+}$ in Table 7/8. ", "page_idx": 31}, {"type": "table", "img_path": "McrzOo0hwr/tmp/e6751b822e119a48b950a5482c9bd3133f2e95e8dfc1826882c850fcc553d54a.jpg", "table_caption": ["Table 6: MSEs for Pendulum 10-shot with reported standard deviations (5 runs). With $\\alpha=0.5$ , the best results are in bold. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Note that the studied distributionally robust strategy is on the tail risk minimization, and $\\mathrm{CVaR}_{\\alpha}$ is the direct optimization indicator. As can be seen, DR-MAML $^{,+}$ \u2019s performance superiority over DR-MAML is significant w.r.t. $\\mathrm{CVaR}_{\\alpha}$ values in 5-shot sinusoid regression and four mini-ImageNet meta-testing tasks. These scenarios are more challenging than others as (i) the context information for adaptation is limited in 5-shot data points and (ii) the distributional shift is severe in mini-ImageNet meta-testing phase. ", "page_idx": 32}, {"type": "table", "img_path": "McrzOo0hwr/tmp/ab460cf3d0e0dbaf3582b840b75faec5c7c6b81ed98138dcd828664314153b7f.jpg", "table_caption": ["Table 7: Test average mean square errors (MSEs) with reported standard deviations for sinusoid regression (5 runs). We respectively consider 5-shot and 10-shot cases with $\\alpha=0.7$ . The results are evaluated across the 490 meta-test tasks, as in [42]. The best results are in bold. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "McrzOo0hwr/tmp/1502b3d7f46a93f35ad7b13778317ba75e52e82ca86b86277c5ada0e228986f8.jpg", "table_caption": ["Table 8: Average 5-way 1-shot classification accuracies in mini-ImageNet with reported standard deviations (3 runs). With $\\alpha=0.5$ , the best results are in bold. The higher, the better for all values. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "We can attribute the performance differences of the two methods to the cumulative quantile estimation errors using the crude MC. Even though the quantile estimation error in Fig. 5 difference is tiny in each step, the cumulative error indeed affects the converged equilibrium a lot. This reflects the advantage of the KDE\u2019s used in DR-MAML $^+$ when the task batch size cannot be set larger in practice. ", "page_idx": 32}, {"type": "text", "text": "We also investigate the task risk value distributions in pendulum system identification. To this end, we visualize one run testing results for all methods in Fig. 11. It seems DR-MAML $^{,+}$ \u2019s result is more skewed to the left than others. ", "page_idx": 32}, {"type": "text", "text": "Fig. 12 displays all methods\u2019 performance w.r.t. the average and $\\mathrm{CVaR}_{\\alpha}$ returns along the metatraining process. We exclude TR-MAML in visualization due to its worse performance and unstable training properties. We can find that the DR-MAML exhibits a fast performance rise at the early stage but its capability to continuously improve diminishes over time. DR-MAML $^{+}$ consistently outperforms other baselines in most cases. The above suggests that the KDE module achieves performance gains over the crude MC when implemented with the two-stage distributionally robust strategy for meta RL scenarios. ", "page_idx": 32}, {"type": "text", "text": "E.3 Sensitivity Analysis to Confidence Level ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To reveal the impact of confidence levels on model performance, we perform a sensitivity analysis with respect to confidence levels. Since only DR-MAML and DR-MAML $^+$ are influenced by the confidence levels during the distributionally robust optimization across all baselines, we only compare the performance of the two methods to highlight the differences between them. As shown in Fig. ", "page_idx": 32}, {"type": "image", "img_path": "McrzOo0hwr/tmp/cdb68249c3a24357675fb5ace2d9ac08a1249c80da24df506f8de39dc41496a1.jpg", "img_caption": ["Figure 11: Histograms of meta-testing performance in system identification. With $\\alpha=0.5$ , we visualize the comprision results of baselines and our DR-MAML $^{+}$ in 10-shot prediction. The lower, the better for Average and $\\mathrm{CVaR}_{\\alpha}$ values. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "McrzOo0hwr/tmp/745f1e08f77f672b49e1663541a803e30bfb5fcf756eb756a3fbafaabfc83b1a.jpg", "img_caption": ["Figure 12: Learning curves for the point robot navigation task. Here, 20 trajectories work as the support set for adaptation. The curves report the normalized returns and are averaged over four random seeds, with $\\alpha=0.5$ . "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "13/14, we can observe that in both sinusoid 5-shot and 10-shot tasks, as the confidence level varies, DR-MAML $^{,+}$ exhibits more stable performance than DR-MAML, indicating that DR-MAML $^{,+}$ has a lower sensitivity to confidence levels. It can be illustrated that the crude Monte Carlo used in DR-MAML is more unstable in terms of quantile estimation than the kernel density estimator used in DR-MAML+. This can be due to the fact that the crude Monte Carlo method is more likely to get stuck in the local optimal solution. In addition, it can be seen from Fig. 13/14 that the performance of our developed DR-MAML $^{,+}$ is better than DR-MAML in most cases. DR-MAML $^{+}$ exhibits lower mean squared errors than DR-MAML in the average, worst, and $\\mathrm{CVaR}_{\\alpha}$ indicators, demonstrating the advantages of more accurate quantile estimation in improving robustness. ", "page_idx": 33}, {"type": "image", "img_path": "McrzOo0hwr/tmp/df3e462ba07bc78b0270530e68dcdc93aa67efd40a778efc49fd01461b78adee.jpg", "img_caption": ["Figure 13: Meta testing performance of DR-MAML and DR-MAML+ with different confidence level on Sinusoid 5-shot tasks. In the plots, the vertical axis is the MSEs, the horizontal axis is the confidence level, and the shaded area represents the standard deviation. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "McrzOo0hwr/tmp/23dce399c050cc9ac2ba43419aa685081c8e7bb00827c58b3322063bc3536b23.jpg", "img_caption": ["Figure 14: Meta testing performance of DR-MAML and DR-MAML+ with different confidence level on Sinusoid 10-shot tasks. In the plots, the vertical axis is the MSEs, the horizontal axis is the confidence level, and the shaded area represents the standard deviation. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "McrzOo0hwr/tmp/375da8f104633d8b4b24f36be4de2fb164e858357729a687af1dbeed88bce8f8.jpg", "img_caption": ["Figure 15: The fast adaptation risk landscape of meta-trained MAML, TR-MAML, DROMAML, DR-MAML and DR-MAML $^+$ . The figure illustrates a 5-shot sinusoid regression example, mapping to the function space $f(x)=A\\sin(x-B)$ . The $X$ -axis and $Y$ -axis represent the amplitude parameter $a$ and phase parameter $b$ respectively. The plots exhibit testing MSEs on the $Z$ -axis across random trials of task generation. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "E.4 Further Exploration on Adaptation ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We demonstrate the adaptation risk landscape of meta-trained MAML [25], TR-MAML [42], DROMAML [66], DR-MAML [1] and our DR-MAML $^+$ in Fig. 15. The adaptation risk landscape shows the superiority of our method in optimizing within the expected tail risk minimization. Compared to other methods, DR-MAML $^+$ exhibits smoother and smaller risk profiles, illustrating its robustness even in challenging tasks. ", "page_idx": 34}, {"type": "text", "text": "F Computational Platforms & Softwares ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "This work employs Pytorch [85] as the default deep learning toolkit when implementing the developed methods. As for baselines, TR-MAMAL follows the standard implementation as work [42] and runs with Tensorflow [86]. Others are implemented with Pytorch. All experimental results are computed by NVIDIA RTX6000 GPUs and A800 GPUs. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We discuss the limitations of the work in Sec 5.7. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Refer to Appendix C. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Refer to Appendix D. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Refer to Appendix D. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Refer to Appendix D. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Refer to Fig. 3/4/6/12/13/14 and Table 1/2/5/6/7/8. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Refer to Appendix F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We adhere to the NeurIPS Code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Refer to Appendix A.5. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 38}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The question is not applicable to the paper. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The question is not applicable to the paper. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The question is not applicable to the paper. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The question is not applicable to the paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The question is not applicable to the paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]