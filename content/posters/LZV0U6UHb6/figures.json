[{"figure_path": "LZV0U6UHb6/figures/figures_0_1.jpg", "caption": "Figure 1: Diverse editing results produced by MimicBrush, where users only need to specify the to-edit regions in the source image (i.e., white masks) and provide an in-the-wild reference image illustrating how the regions are expected after editing. Our model automatically captures the semantic correspondence between them, and accomplishes the editing with a feedforward network execution.", "description": "This figure showcases various examples of image editing results achieved using the MimicBrush model.  Users simply indicate the areas to be modified (using white masks) and provide a sample reference image. The model automatically identifies the semantic correspondence between the source and reference images and performs the edit in a single forward pass. This demonstrates the model's capability to handle diverse editing tasks without explicit instructions.", "section": "Abstract"}, {"figure_path": "LZV0U6UHb6/figures/figures_1_1.jpg", "caption": "Figure 2: Conceptual comparisons for different pipelines. To edit a local region, besides taking the source image and source mask (indicates the to-edit region), inpainting models use text prompts to guide the generation. Image composition methods take a reference image along with a mask/box to crop out the specific reference region. Differently, our pipeline simply takes a reference image, the reference regions are automatically discovered by the model itself.", "description": "This figure compares three different image editing pipelines: inpainting, composition, and the proposed MimicBrush method. Inpainting uses text prompts and source image masks to guide the generation of edits. Composition methods use a reference image and mask to specify the region for replacement.  MimicBrush takes only a reference image and source mask, automatically identifying the corresponding regions and performing the editing.", "section": "1 Introduction"}, {"figure_path": "LZV0U6UHb6/figures/figures_2_1.jpg", "caption": "Figure 3: The training process of MimicBrush. First, we randomly sample two frames from a video sequence as the reference and source image. The source image are then masked and exerted with data augmentation. Afterward, we feed the noisy image latent, mask, background latent, and depth latent of the source image into the imitative U-Net. The reference image is also augmented and sent to the reference U-Net. The dual U-Nets are trained to recover the masked area of source image. The attention keys and values of reference U-Net are concatenated with the imitative U-Net to assist the synthesis of the masked regions.", "description": "This figure illustrates the training process of the MimicBrush model.  It shows how two frames from a video are used: one as a source image (masked and augmented) and the other as a reference image (also augmented). Both images are fed into their respective U-Nets (Imitative and Reference).  The keys and values from the Reference U-Net are then concatenated with the Imitative U-Net to help reconstruct the masked region of the source image. Depth estimation is used to improve the accuracy of reconstruction, but is optional.", "section": "3 Method"}, {"figure_path": "LZV0U6UHb6/figures/figures_4_1.jpg", "caption": "Figure 4: Sample illustration for our benchmark. It covers the task of part composition (first row) and texture transfer (second row). Each task includes a Inter-ID and inner-ID track. The annotated data and evaluation metrics for each track are illustrated beside the exemplar images.", "description": "This figure shows example results for the benchmark used to evaluate the MimicBrush model.  It illustrates the two main tasks: part composition and texture transfer.  Each task is further divided into two tracks: Inter-ID (comparing different objects) and Inner-ID (comparing variations of the same object). The figure displays example source images, reference images, masks, and the metrics used for evaluation (SSIM, PSNR, LPIPS, DINO image similarity, CLIP image similarity, and CLIP text similarity).", "section": "3.4 Evaluation Benchmark"}, {"figure_path": "LZV0U6UHb6/figures/figures_5_1.jpg", "caption": "Figure 5: Qualitative comparisons. Noticing that other methods require additional inputs. Firefly [32] takes the detailed prompts descriptions. Besides, we mark the specific reference regions with boxes and masks for Paint-by-Example [47] and AnyDoor [7]. Even though, MimicBrush still demonstrates prominent advantages for both fidelity and harmony.", "description": "This figure presents a qualitative comparison of MimicBrush against other image editing methods (Firefly, Paint-by-Example, and AnyDoor) on several example images.  It highlights that MimicBrush achieves superior results in terms of image fidelity and harmony, even though the competing methods require additional user inputs (detailed text prompts or manually defined reference regions).", "section": "4 Experiments"}, {"figure_path": "LZV0U6UHb6/figures/figures_7_1.jpg", "caption": "Figure 6: Ablation study for different reference feature extractors. CLIP and DINOv2 encoders could also achieve imitative editing but lag behind the U-Net in preserving the fine details.", "description": "This ablation study compares the performance of three different reference feature extractors: CLIP, DINOv2, and a U-Net, in the task of imitative image editing.  The results show that while CLIP and DINOv2 can achieve imitative editing, the U-Net generally produces superior results, particularly in preserving fine details in the edited image regions. The figure visually demonstrates this difference by showing example edits performed using each feature extractor.", "section": "4.3 Ablation Studies"}, {"figure_path": "LZV0U6UHb6/figures/figures_8_1.jpg", "caption": "Figure 7: Diverse applications supported by MimicBrush. Our methods could be applied conveniently for product design, accessories wearing, editing the scene images, and refining the flawed generation results of other methods. MimicBrush is able to edit multiple regions in one pass.", "description": "This figure showcases the versatility of MimicBrush across various applications.  It demonstrates the ability to edit product designs (e.g., replacing textures on shoes), modify accessories in fashion images (e.g., swapping earrings or necklaces), enhance scene images (e.g., changing lighting or adding elements), and correct flaws in other image editing results. Importantly, the model can efficiently edit multiple regions simultaneously.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "LZV0U6UHb6/figures/figures_12_1.jpg", "caption": "Figure 4: Sample illustration for our benchmark. It covers the task of part composition (first row) and texture transfer (second row). Each task includes a Inter-ID and inner-ID track. The annotated data and evaluation metrics for each track are illustrated beside the exemplar images.", "description": "This figure shows examples of the benchmark datasets used to evaluate the MimicBrush model.  It highlights the two main tasks: part composition and texture transfer. Each task is further divided into \"Inter-ID\" and \"Inner-ID\" subtasks, representing different levels of difficulty and data characteristics.  The figure illustrates the input images (source, reference), masks, and associated text prompts, along with the evaluation metrics (SSIM, PSNR, LPIPS) and whether ground truth was available.  This provides a clear overview of how the benchmark is structured and the type of data used to train and test the model.", "section": "3.4 Evaluation Benchmark"}, {"figure_path": "LZV0U6UHb6/figures/figures_13_1.jpg", "caption": "Figure A2: Qualitative ablations for different training strategies. We present the results for various training strategies, including training with only image data, applying weak augmentations, and utilizing naive masking strategies. For comparison, we also provide the generation results of our full-version model.", "description": "This figure shows qualitative ablation studies for different training strategies used in the MimicBrush model.  The results demonstrate the model's performance when trained under various conditions: using only image data, applying weak augmentations, and using naive masking. A comparison with the results of the full model is also included, highlighting the positive impact of a robust training strategy that leverages visual variations and well-crafted masking.", "section": "Appendix"}, {"figure_path": "LZV0U6UHb6/figures/figures_15_1.jpg", "caption": "Figure 1: Diverse editing results produced by MimicBrush, where users only need to specify the to-edit regions in the source image (i.e., white masks) and provide an in-the-wild reference image illustrating how the regions are expected after editing. Our model automatically captures the semantic correspondence between them, and accomplishes the editing with a feedforward network execution.", "description": "This figure shows several examples of image editing results using the MimicBrush model.  Users only need to specify the areas to be edited (using white masks) in the source image and provide a reference image showing the desired result. The model automatically identifies the correspondence between the source and reference images and performs the edit in a single feedforward pass.  The examples highlight the model's ability to handle diverse editing tasks and styles.", "section": "Abstract"}, {"figure_path": "LZV0U6UHb6/figures/figures_15_2.jpg", "caption": "Figure 7: Diverse applications supported by MimicBrush. Our methods could be applied conveniently for product design, accessories wearing, editing the scene images, and refining the flawed generation results of other methods. MimicBrush is able to edit multiple regions in one pass.", "description": "This figure shows several examples of how MimicBrush can be used for different image editing tasks.  The examples demonstrate the versatility of the model across various applications including product design (e.g., changing the color of shoes, adding patterns to mugs), fashion (e.g., changing clothing styles or accessories), scene editing (e.g., altering backgrounds), and refining the outputs of other image editing models. The key takeaway is that MimicBrush is capable of handling multiple editing regions simultaneously within a single processing step.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "LZV0U6UHb6/figures/figures_16_1.jpg", "caption": "Figure A5: Demonstrations for texture transfer. When applying the shape control, MimicBrush could strictly follow the original shape and transfers the novel texture from the reference images.", "description": "This figure shows several examples of texture transfer using MimicBrush.  The method is able to transfer textures from a reference image onto a target image while preserving the original shape of the target object.  The examples demonstrate the ability to transfer various types of textures, including patterns (cow print), natural textures (cracked earth, ice), and even textures from images of animals.  The depth map is used to maintain the original shape.", "section": "More Visualization Results"}]