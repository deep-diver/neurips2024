[{"type": "text", "text": "Federated Learning over Connected Modes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dennis Grinwald1,2, Philipp Wiesner2, Shinichi Nakajima1,2,3 1BIFOLD, 2TU Berlin, 3RIKEN Center for AIP {dennis.grinwald, wiesner, nakajima}@tu-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Statistical heterogeneity in federated learning poses two major challenges: slow global training due to confilcting gradient signals, and the need of personalization for local distributions. In this work, we tackle both challenges by leveraging recent advances in linear mode connectivity \u2014 identifying a linearly connected low-loss region in the parameter space of neural networks, which we call solution simplex. We propose federated learning over connected modes (FLOCO), where clients are assigned local subregions in this simplex based on their gradient signals, and together learn the shared global solution simplex. This allows personalization of the client models to fit their local distributions within the degrees of freedom in the solution simplex and homogenizes the update signals for the global simplex training. Our experiments show that FLOCO accelerates the global training process, and significantly improves the local accuracy with minimal computational overhead in cross-silo federated learning settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) [1] is a decentralized machine learning paradigm that facilitates collaborative model training across distributed devices while preserving data privacy. However, in typical real applications, statistical heterogeneity\u2014non-identically and independently distributed (non-IID) data distributions at clients\u2014makes it difficult to train well-performing models. To tackle this difficulty, various methods have been proposed, e.g., personalized FL [2], clustered FL [3], advanced client selection strategies [4], robust aggregation [5], and federated meta- and multi-task learning approaches [6]. These methods aim either at training a global model that performs well on the global distribution [7], or, as it is common in personalized FL, at training multiple client-dependent models each of which performs well on its local distribution [8]. These two aims often pose a trade-off\u2014a model that shows better local performance tends to suffer from worse global performance, and vice versa. In this work, we aim to develop a FL method that improves local performance compared to state-of-the art methods without sacrificing global performance. ", "page_idx": 0}, {"type": "text", "text": "Our approach leverages recent findings on mode connectivity [9\u201311]\u2014the existence of low-loss paths in the parameter space between independently trained neural networks\u2014and its applications [12]. These works show that minima for the same task are typically connected by simple low-loss curves, and that this connectivity benefits training for multi-task and continual learning. In particular, the authors show that embracing mode connectivity between models improves accuracy on each task and remedies the risk of catastrophic forgetting. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we leverage such effects, and propose federated learning over connected modes (FLOCO), where the clients share and together train a solution simplex\u2014a linearly connected low-loss region in the parameter space. Specifically, FLOCO represents clients as points within the standard simplex based on the similarity between their gradients, and assigns each client a specific subregion of the simplex. Clients then participate in FL by sampling different models within their assigned subregions and sending back the gradient information to update the vertices of the global solution simplex (see ", "page_idx": 0}, {"type": "image", "img_path": "JL2eMCfDW8/tmp/66fe097fa8aacbd05b52cc70f7381beb0388a7c1a43aab46feee6450dfdfc1b6.jpg", "img_caption": ["Figure 1: FLOCO expresses each client as a point ( $\\star$ in the top-center plot) by projecting the gradient signals onto the simplex, so that similar clients are close to each other. In each communication round, each client uniformly samples points in the neighborhood of their projected point (top-right plot), and jointly train the solution simplex. The lower row shows the resulting test loss on the solution simplex, where the loss for the global distribution (left) is uniformly small, while the losses for individual local distributions (center for client 1 and right for client 2) are small around their projected points. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Fig.1). This method facilitates collaborative training through the common solution simplex, while allowing for client-specific personalization according to their local data distributions. ", "page_idx": 1}, {"type": "text", "text": "Our experiments show that FLOCO outperforms common FL baselines (FedAvg [1], FedProx [13]) and state-of-the-art personalized FL approaches (FedRoD [14], APFL [15], Ditto [16], FedPer [17]) on both local and global test metrics\u2014without introducing significant computational overhead\u2014in cross-silo FL settings. We also demonstrate additional beneftis of FLOCO, including better uncertainty estimation, improved worst client performance, and smaller divergence of gradient signals. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose FLOCO, a novel FL method that trains a solution simplex for mitigating the statistical heterogeneity of clients, and demonstrate its state-of-the-art performance for local personalized FL.   \n\u2022 We propose a simple projection method to express clients as points in the standard simplex based on the gradient signals, and establish a procedure of subregion assignments.   \n\u2022 We conduct experimental evaluations on semi-artificial and real-world FL benchmarks with detailed analyses of the behavior of FLOCO, which give insights into how the mechanism improves performance compared to the baselines. ", "page_idx": 1}, {"type": "text", "text": "We provide implementations of FLOCO in the FL frameworks FL-bench [18] and Flower [19]. Our code is publicly available: https://github.com/dennis-grinwald/floco. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we briefly explain the concepts behind federated learning and mode connectivity, which form the backbone of our approach. The symbols that we use throughout the paper are listed in Table 5 in Appendix. ", "page_idx": 1}, {"type": "text", "text": "2.1 Federated Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume a federated system where the server has a global model $g_{0}$ and the $K$ clients have their local models $\\{g_{k}\\}_{k=1}^{K}$ . FL aims to obtain the best performing models $\\{g_{k}^{*}\\}_{k=0}^{K}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{0}^{*}=\\operatorname*{argmin}_{g_{0}}F^{*}(g_{0})\\equiv\\sum_{k=1}^{K}p(k)F_{k}^{*}(g_{0}),}\\\\ &{g_{k}^{*}=\\operatorname*{argmin}_{g_{k}}F_{k}^{*}(g_{k})\\;\\mathrm{~for~}\\,k=1,\\ldots,K,}\\\\ &{\\quad\\mathrm{~where~}F_{k}^{*}(g)=\\mathbb{E}_{(\\pmb{x},y)\\sim p_{k}(\\pmb{x},y)}\\left[f(g,(\\pmb{x},y))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $p(k)$ is the normalized population of data samples for the $k$ -th client, $p_{k}(\\pmb{x},y)$ is the data distribution for the client $k$ , and $f(g,(x,y))$ is the loss, e.g., cross-entropy, of the model $g$ on a sample $(\\mathbf{x},y)\\in\\mathbb{R}^{I}\\times\\left\\{1,\\ldots,L\\right\\}$ , where $I$ is the dimension of an input data sample. Global [20] and personalized [8] FL aim to approximate $g_{0}^{*}$ and $\\{g_{k}^{*}\\}_{k=1}^{K}$ , respectively, by using the training data $\\mathcal{D}=\\{\\mathcal{D}_{k}\\}_{k=1}^{K}$ observed by the clients. Throughout the paper, we assume that all models are neural networks (NNs) $\\widehat{\\boldsymbol{y}}=g_{k}(\\boldsymbol{x};\\pmb{w}_{k})$ with the same architecture, and represent the model $g_{k}$ with its NN parameters $\\pmb{w}_{k}\\in\\mathbb{R}^{D}$ , i.e., we hereafter represent $g_{k}(\\pmb{x};\\pmb{w}_{k})$ by $\\pmb{w}_{k}$ and thus denote, e.g., $F_{k}^{*}(g_{k})$ by $F_{k}^{*}(w_{k})$ . Let $\\begin{array}{r}{N=\\sum_{k=1}^{K}N_{k}}\\end{array}$ be the total number of samples, where $N_{k}=|\\mathcal{D}_{k}|$ . ", "page_idx": 2}, {"type": "text", "text": "For the independent and identically distributed (IID) data setting, i.e., $p_{k}(x,y)\\,=\\,p(x,y),\\forall k\\,=$ $1,\\dots,K$ , the global and personalized FL aim for the same goal, and the minimum loss solution for the given training data is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\pmb{w}}_{0}=\\widehat{\\pmb{w}}_{k}=\\operatorname*{argmin}_{\\pmb{w}}F(\\pmb{w})\\equiv\\sum_{k=1}^{K}\\frac{N_{k}}{N}F_{k}(\\pmb{w}),}\\\\ {\\mathrm{where~}F_{k}(\\pmb{w})=\\frac{1}{N_{k}}\\sum_{(\\pmb{x},y)\\in\\mathcal{D}_{k}}f(\\pmb{w},(\\pmb{x},y)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In this setting, Federated Averaging (FedAvg) [1], ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}_{0}^{t+1}=\\pmb{w}_{0}^{t}+\\sum_{k\\in\\mathcal{S}^{t}}\\frac{N_{k}}{N}\\cdot\\Delta\\pmb{w}_{k}^{t+1}\\mathrm{~for~}t=1,\\dots,T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "is known to converge to $\\widehat{\\pmb{w}}_{0}$ , and thus solve Eq. (3). Here, $S^{t}$ is the set of clients that participate the t-th communication r o und, and \u2206wtk+1 $\\Delta\\pmb{w}_{k}^{t+1}=\\pmb{\\bar{w}}_{k}^{t+1}-\\pmb{w}_{0}^{t}$ is the update after $T^{\\prime}$ steps of the local gradient descent, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\breve{w}}^{t^{\\prime}+1}=\\pmb{\\breve{w}}^{t^{\\prime}}-\\gamma\\pmb{\\nabla}F_{k}(\\pmb{\\breve{w}}^{t^{\\prime}}),\\ \\mathrm{for}\\ t^{\\prime}=1,\\dots,T^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\check{\\pmb w}^{0}={\\pmb w}_{0}^{t},\\check{\\pmb w}^{T^{\\prime}}={\\pmb w}_{k}^{t+1}$ , and $\\gamma$ is the step size. FedAvg has been further enhanced with, e.g., proximity regularization [21], auxiliary data [22], and ensembling [23]. ", "page_idx": 2}, {"type": "text", "text": "On the other hand, in the more realistic non-IID setting, where $\\pmb{w}_{0}^{*}\\neq\\pmb{w}_{k}^{*}$ , FedAvg and its variants suffer from slow convergence and poor local performance [24]. To address such challenges, Ditto [16] was proposed for personalized $\\mathrm{FL}$ , i.e., to approximate the best local models $\\{w_{k}^{*}\\}_{k=1}^{\\breve{K}}$ . Ditto has two training phases: it first trains the global model $\\widehat{\\pmb{w}}_{0}$ by FedAvg, then trains the local models with proximity regularization to $\\widehat{\\pmb{w}}_{0}$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{w}}_{k}\\!=\\!\\operatorname*{argmin}_{\\pmb{w}_{k}}\\widetilde{F}_{k}(\\pmb{w}_{k},\\widehat{\\pmb{w}}_{0})\\equiv F_{k}(\\pmb{w}_{k})+\\frac{\\lambda}{2}\\|\\pmb{w}_{k}-\\widehat{\\pmb{w}}_{0}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda$ controls the divergence from the global model. Ditto has been shown to outperform many other non-IID FL methods, including the client clustering method HYPCLUSTER, adaptive federated learning (APFL), which interpolates between a global and local models [25], Loopless Local SGD (L2SGD), which applies global and local model average regularization [26], and MOCHA [6], which fits task-specific models through a multi-task objective. ", "page_idx": 2}, {"type": "text", "text": "2.2 Mode Connectivity and Solution Simplex ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Freeman and Bruna (2017) [27], as well as Garipov et al. (2018) [10], discovered the mode connectivity in the NN parameter space\u2014the existence of simple regions with low training loss between two well-trained models from different initializations. Nagarajan and Kolter (2019) [11] showed that the path is linear when the models are trained from the same initialization, but with different ordering of training data. Frankle et al. (2020) [28] showed that the same pre-trained models stay linearly-connected after fine-tuning with gradient noise or different data ordering. ", "page_idx": 2}, {"type": "text", "text": "Benton et al. (2021) [29] found that the low loss connection is not necessarily in 1D, and [30] showed that a simplex, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}(\\{\\pmb{\\theta}_{m}\\})=\\left\\{\\pmb{w}_{\\alpha}(\\{\\pmb{\\theta}_{m}\\})=\\sum_{m=1}^{M+1}\\alpha_{m}\\pmb{\\theta}_{m};\\pmb{\\alpha}\\in\\Delta^{M}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "within which any point has a small loss, can be trained from randomly initialized endpoints. ", "page_idx": 3}, {"type": "text", "text": "Here, $\\{\\pmb{\\theta}_{m}\\ \\in\\ \\mathbb{R}^{D}\\}_{m=1}^{M+1}$ are the endpoints or vertices of the simplex, and $\\Delta^{M}\\mathbf{\\Psi}=\\mathbf{\\Psi}\\{\\pmb{\\alpha}\\ \\in$ $[0,1]^{M+1};||\\pmb{\\alpha}||_{1}\\,=\\,1\\}$ denotes the -dimensional standard simplex. This simplex learning is performed by finding the endpoints that (approximately) minimize ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim p(\\pmb{x},\\pmb{y})}\\left[\\mathbb{E}_{\\pmb{w}\\sim\\mathcal{U}_{\\mathcal{W}(\\{\\pmb{\\theta}_{m}\\})}}[f(\\pmb{w},(\\pmb{x},\\pmb{y}))]\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{U}_{\\mathcal{W}}$ denotes the uniform distribution on a set $\\mathcal{W}$ . During training, one model realization $w_{\\alpha}$ from the simplex gets sampled and its gradient update wrt. the loss, e.g. cross-entropy, gets backpropagated to the simplex endpoints {\u03b8m}mM=+11 . ", "page_idx": 3}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce our approach, where the mode connectivity is leveraged for collaborative training between personalized client models. ", "page_idx": 3}, {"type": "text", "text": "3.1 Federated Learning over Connected Modes (FLOCO) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The main idea behind FLOCO is to assign subregions of the solution simplex (6) to clients in such a way that similar clients train neighboring (and overlapped) regions, while enforcing (linear) connectivity to all other client\u2019s subregions. The connectivity constraint systematically regularizes client training and allows for efficient collaboration between them. ", "page_idx": 3}, {"type": "text", "text": "The subregion assignments need to reflect the similarity between the clients. To this end, FLOCO expresses each client as a point in the standard simplex, based on the gradient update signals. Specifically, it applies the Euclidean projection onto the positive simplex [31] with the Riesz s-Energy regularization [32], which gives well spreaded projections that preserve the similarity between the client\u2019s gradient signals as much as possible. Once the clients are projected onto the standard simplex as $\\{\\pmb{\\alpha}_{k}\\in\\bar{\\Delta}^{M}\\}_{k=1}^{K}$ , we assign the L1-ball with radius $\\rho$ around $\\alpha_{k}$ , i.e., $\\mathcal{R}_{k}\\,=\\,\\{{\\pmb{\\alpha}}\\,\\in$ $\\Delta^{M};\\|\\pmb{\\alpha}-\\pmb{\\alpha}_{k}\\|_{1}\\leq\\rho\\rbrace$ , to the $k$ -th client. Note that the gradient update signals are informative for the subregion assignment only after the (global) model is trained to some extent. Therefore, the subregion assignment is performed after $\\tau$ FL rounds are performed. Before the assignment, i.e., $t\\leq\\tau$ , all clients train the whole standard simplex $\\mathcal{R}_{k}=\\dot{\\Delta^{M}},\\forall k$ , which corresponds to a simplex learning version of FedAvg. ", "page_idx": 3}, {"type": "text", "text": "sStteaprtsi fnogr  feraocmh  rpaanrtdicoimpaltyi ning itcilaileinzte lienx  eeancdh pcooinmtsm $\\{\\theta_{m}\\}_{m=1}^{M+1}$ ,o uFnLdO :O performs the following $k\\in S^{t}$ $t$ ", "page_idx": 3}, {"type": "text", "text": "1. The server sends the current endpoints $\\{\\pmb{\\theta}_{m}^{t}\\}_{m=1}^{M+1}$ to the client $k$ .   \n2. The client $k$ performs simplex learning only on the assigned subregion $\\mathcal{R}_{k}$ as a local update.   \n3. The client sends the local update of the endpoints to the server. ", "page_idx": 3}, {"type": "text", "text": "This way, FLOCO is expected to learn the global solution simplex $\\{\\pmb{w}_{\\alpha};\\alpha\\in\\Delta^{M}\\}$ , while allowing personalization to local client distributions within the solution simplex. Algorithm 1 shows the main steps. ", "page_idx": 3}, {"type": "text", "text": "Although the simplex learning can be applied to all parameters, our preliminary experiment showed that applying simplex learning only of the parameters in the last fully-connected layer (while pointestimating the other parameters) is sufficient. Therefore, our FLOCO only applies the simplex learning to the last layer, which gives other beneftis including applicability to fine-tuning of pre-trained models, and significant reduction of computational and communication costs, as shown in Section 4.3. ", "page_idx": 3}, {"type": "text", "text": "Below, we describe detailed procedures of client projection, local and global updates in the communication rounds, and inference in the test time. ", "page_idx": 3}, {"type": "text", "text": "Input :number of communication rounds $T$ , number of clients $K$ , simplex dimension $M$ , subregion assignment round $\\tau$ , subregion radius $\\rho$ ", "page_idx": 4}, {"type": "text", "text": "1 $\\{\\pmb{\\theta}_{m}^{0}\\}_{m=1}^{M+1}\\leftarrow$ initialize_simplex $(M)$   \n2 $\\mathcal{R}_{k}\\leftarrow\\Delta^{M},\\forall k=1,\\dots,K\\,\\,/$ / set all client subregions to the whole standard simplex   \n3 for $t=1$ to $T$ do   \n4 6 if $t=\\tau$ $\\begin{array}{r l}&{\\{\\{\\Delta\\pmb{\\theta}_{m,k}^{\\tau}\\}_{m=1}^{M+1}\\}_{k=1}^{K}\\leftarrow\\mathbf{collect\\_and\\_stack\\_gradients}}\\\\ &{\\{\\alpha_{k}\\}_{k=1}^{K}\\leftarrow\\mathbf{client\\_representation}(\\{\\{\\Delta\\pmb{\\theta}_{m,k}^{\\tau}\\}_{m=1}^{M+1}\\}_{k}^{R}}\\\\ &{\\{\\mathcal{R}_{k}\\}_{k=1}^{K}\\leftarrow\\mathbf{assign\\_subregions}(\\{\\alpha_{k}\\}_{k=1}^{K},\\rho)}\\end{array}$ collect_and_stack_gradients()   \n7   \n8 $S^{t}\\gets$ choose_participating_clients()   \n9 f $\\{\\pmb{\\theta}_{m,k}^{t+1}\\}_{m=1}^{M+1}\\gets\\mathbf{local\\_update}(\\{\\pmb{\\theta}_{m,k}^{t}\\}_{m=1}^{M+1},\\mathcal{R}_{k})$ $k\\in S^{t}$   \n11 $\\{\\pmb{\\theta}_{m}^{t+1}\\}_{m=1}^{M+1}\\gets\\mathbf{global\\_update}(\\{\\{\\pmb{\\theta}_{m,k}^{t+1}\\}_{m=1}^{M+1}\\}_{k\\in S^{t}})$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Client Gradient Projection onto Standard Simplex ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We explain how to obtain the representations $\\{\\alpha_{k}\\in\\Delta^{M}\\}$ of the clients in the standard simplex such that similar clients are located close to each other, while all clients are well-spread across the simplex. ", "page_idx": 4}, {"type": "text", "text": "At communication round $t=\\tau$ , FLOCO uses the gradient updates of the endpoints $\\{\\Delta\\pmb{\\theta}_{m,k}^{\\tau}\\}_{m=1}^{M+1}$ as a representation of the client . We concatenate the gradients for the endpoints into a $((M+1)\\cdot D)$ -dimensional vector, and apply the PCA projection onto the $M$ dimensional space, yielding $\\kappa_{k}\\in\\mathbb{R}^{M}$ as a low dimensional representation. To project $\\{\\kappa_{k}\\}$ onto the standard simplex $\\dot{\\Delta}^{M}$ , we solve the following minimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{z>0}{\\mathrm{min}}}&{{}\\sum_{i,j}\\frac{1}{\\|\\widehat{\\beta}_{i}(z)-\\widehat{\\beta}_{j}(z)\\|_{2}^{2}},}\\\\ {\\mathrm{subject}\\,\\,\\mathrm{to:}\\,}&{{}\\widehat{\\beta}_{k}(z)=\\mathrm{argmin}_{\\frac{\\beta_{k}}{z}\\in\\Delta^{M-1}}\\,\\|\\beta_{k}-\\kappa_{k}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The objective function in Eq. (8) is the Riesz s-Energy [32], a generalization of potential energy of multiple particles in a physical space, and therefore its minimizer correponds to the state where particles are well spread across the space. The minimization in the constraint (9) corresponds to the Euclidean projection onto the positive simplex [31], which forces $\\{\\beta_{k}\\}$ to keep the locations of the PCA projections $\\{\\kappa_{k}\\}$ of the clients. Fortunately, this minimization problem (for a fixed $z$ ) is convex, and can be efficiently solved (see Appendix A). We solve the main problem (8) by computing $\\widehat{\\beta}_{k}(z)$ $\\begin{array}{r}{\\alpha_{k}=\\frac{\\widehat{\\beta}_{k}(\\widehat{z})}{\\widehat{z}}}\\end{array}$ , where $\\widehat{z}$ $z\\in[0,1]$ imizer of Eq. (8). ", "page_idx": 4}, {"type": "text", "text": "3.3 Communication Round: Local and Global Updates ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the $t$ -th communication round, the server sends the current endpoints $\\{\\pmb{\\theta}_{m}^{t}\\}_{m=1}^{M+1}$ to the participating clients $S^{t}$ . Then, each client $k\\in S^{t}$ draws one sample per mini-batch from the uniform distribution $\\pmb{\\mathcal{A}}=\\{\\pmb{\\alpha}_{b}\\}_{b=1}^{B}\\sim\\mathcal{U}_{\\mathcal{R}_{k}}$ on the assigned subregion and applies $T^{\\prime}$ local updates, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\breve{\\pmb{\\theta}}_{m}^{t^{\\prime}+1}=\\breve{\\pmb{\\theta}}_{m}^{t^{\\prime}}-\\alpha_{m}\\cdot\\gamma\\cdot\\nabla F_{k}(\\pmb{w}_{\\alpha}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "to the endpoints with \u03b1 sequentially chosen from A.1 Here \u03b8\u02d80m = \u03b8tm, \u03b8\u02d8Tm . The local updates {\u2206\u03b8tm+,1k $\\{\\Delta\\pmb{\\theta}_{m,k}^{t+1}=\\pmb{\\theta}_{m,k}^{t+1}-\\pmb{\\theta}_{m}^{t}\\}_{m=1}^{M+1}$ are sent back to the server, which updates the endpoints as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\theta}_{m}^{t+1}=\\pmb{\\theta}_{m}^{t}+\\sum_{k\\in S^{t}}\\frac{N_{k}}{N}\\cdot\\Delta\\pmb{\\theta}_{m,k}^{t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As explained in Section 3.1, the client subregions are initially set to the whole simplex $\\Delta^{M}$ before the subregion assignment is performed at $t=\\tau$ , which corresponds to a straightforward application of the simplex learning to FedAvg. After the subregion assignment, FLOCO uses the degrees of freedom within the solution simplex to personalize clients models. ", "page_idx": 5}, {"type": "text", "text": "3.4 FLOCO+ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We can further enhance the personalized FL performance of FLOCO by additionally fine-tuning a local model as in Ditto [16]. In this extension, called $\\scriptstyle\\mathrm{FLOCO^{+}}$ , each client personalizes the global endpoints $\\{\\widehat{\\pmb{\\theta}}_{m}^{0}=\\pmb{\\theta}_{m}\\}_{m=1}^{M}$ by local gradient descent to minimize the Ditto objective, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{\\widehat{\\pmb{\\theta}}_{m}^{k}\\}=\\mathrm{argmin}_{\\{\\theta_{m}\\}}\\widetilde{F}_{k}(\\{\\pmb{\\theta}_{m}\\},\\{\\widehat{\\pmb{\\theta}}_{m}^{0}\\})}\\\\ &{\\equiv\\mathbb{E}_{\\pmb{\\alpha}\\sim\\mathcal{U}_{\\mathcal{R}_{z_{k}}}}\\left[F_{k}(\\pmb{w}_{\\alpha}(\\{\\pmb{\\theta}_{m}\\}))\\right]+\\frac{\\lambda}{2}\\sum_{m=1}^{M+1}||\\pmb{\\theta}_{m}-\\widehat{\\pmb{\\theta}}_{m}^{0}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.5 Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With the trained endpoints $\\{\\widehat{\\pmb{\\theta}}_{m}=\\pmb{\\theta}_{m}^{T}\\}_{m=1}^{M+1}$ , we simply use $w_{\\widehat{\\alpha}_{0}}(\\{\\widehat{\\pmb{\\theta}}_{m}\\}_{m=1}^{M+1})$ as the global model, where $\\begin{array}{r}{\\widehat{\\pmb{\\alpha}}_{0}=\\frac{1}{M+1}\\mathbf{1}_{M+1}}\\end{array}$ with ${\\mathbf{1}}_{D}$ denoting the -dimensional all one vector. For local models, we use $\\{\\pmb{w}_{\\widehat{\\pmb{\\alpha}}_{k}}(\\{\\widehat{\\pmb{\\theta}}_{m}\\}_{m=1}^{M+1})\\}_{k=1}^{K}$ where $\\widehat{\\alpha}_{k}=\\alpha_{k}$ . For $\\mathrm{FLOCO^{+}}$ , we fine-tune the corresponding subspace regions $\\mathcal{R}_{z_{k}}$ for $E$ local epochs. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we experimentally show the advantages of FLOCO and $\\mathrm{FLOCO^{+}}$ over the baselines. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and models. To evaluate our method, we perform image classification on the CIFAR10 [33] and FEMNIST [34] datasets. For CIFAR-10, we train a CNN (CifarCNN) from scratch, following [35], and fine-tune a ResNet-18 [36] pre-trained on ImageNet [37], as in [38]. For FEMNIST, we train a CNN (FemnistCNN) from scratch, as in [1], and fine-tune a SqueezeNet [39] pre-trained on ImageNet, following [38]. We provide a table with the training hyperparameters that we use for each dataset/model setting in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Data heterogeneity for non-FL benchmarks. The FEMNIST dataset is an FL benchmark based on real data, where client heterogeneity is inherently embedded in the dataset. For CIFAR-10, we simulate statistical heterogeneity by two partitioning procedures. The first procedure by [40] partitions clients in equally sized groups and assigns each group a set of primary classes. Every client gets $q\\,\\%$ of its data from its group\u2019s primary classes and $(100-q)\\,\\%$ from the remaining classes. We apply this method with $q=80$ for five groups and refer to this split as 5-Fold. For example, in CIFAR-10 5-Fold, $20\\,\\%$ of the clients get assigned $80\\,\\%$ samples from classes 1-2 and $20\\,\\%$ from classes 3-10. The second procedure, inspired by [41] and [42], draws the multinomial parameters of the client distributions $\\bar{p}_{k}(y)=\\mathrm{Multi}(y;\\phi_{k})$ from Dirichlet, i.e., $\\phi_{k}\\sim\\operatorname{Dir}_{L}(\\beta)$ , where $\\beta$ is the concentration parameter controlling the sparsity and heterogeneity\u2014 $\\beta\\to\\infty$ concentrates the mass to the uniform distribution (and thus homogeneous), while small $0<\\beta<1$ generates sparse and heterogeneous non-IID client distributions. ", "page_idx": 5}, {"type": "text", "text": "Baseline methods. Besides FedAvg [1] and FedProx [21] for global FL, we chose FedRoD [14], APFL [15], Ditto [16], and FedPer [17] as state-of-the-art personalized FL baselines. ", "page_idx": 5}, {"type": "text", "text": "FLOCO Hyperparameters. For CifarCNN on the simulated non-IID splits Dir(0.3)/Five-Fold, we set $\\tau\\,=\\,250,M\\,=\\,20/10,\\rho\\,=\\,0.1$ . For FemnistCNN on FEMNIST we set $\\tau\\,=\\,250,M\\,=$ $10,\\rho\\,=\\,0.5$ . For pre-trained ResNet-18 on the simulated non-IID splits Dir(0.3)/Five-Fold we set $\\bar{\\tau}=50,M=\\bar{2}0/10,\\rho=0.1$ and for the pre-trained SqueezeNet on FEMNIST we set $\\tau=$ $250,M\\,=\\,3,\\rho\\,=\\,0.5$ . We found those settings work well in our preliminary experiments, and conducted ablation study with other parameter settings in Appendix D. For the baselines, we follow the recommended parameter settings by the authors, which are detailed in Appendix B. ", "page_idx": 5}, {"type": "table", "img_path": "JL2eMCfDW8/tmp/8d797d702cd5c3b877ea7b4cb1232aecba24d6b1d963262c8719f0fef2742c65.jpg", "table_caption": ["Table 1: Average global and local test accuracy. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "JL2eMCfDW8/tmp/f8eff1d37aa337f85f42f9338268212f05aea151c3190e3bf91da61a32f109e1.jpg", "table_caption": ["Table 2: Average global and local expected test calibration error. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Evaluation criteria. For the performance evaluation, we adopt two metrics, the test accuracy measured after the last communication round (ACC) and the time-to-best-accuracy (TTA), each for evaluating the global and local $\\mathrm{FL}$ performance. ACC is the last test accuracy over $T$ communication rounds, i.e, $\\begin{array}{r}{\\mathrm{A}\\bar{\\mathrm{C}}\\mathrm{C}(T)=\\frac{1}{N_{\\mathrm{test}}}\\sum_{i=1}^{N_{\\mathrm{test}}}\\mathbb{1}(y_{i}=\\arg\\operatorname*{max}g(\\pmb{x}_{i};\\hat{\\pmb{w}}^{T}))}\\end{array}$ , where $\\mathbb{1}(\\cdot)$ is the indicator function that equals to 1 if the event is true and 0 otherwise. TTA evaluates the number of communication rounds needed to achieve the best baseline (FedAvg and Ditto in this paper) test accuracy, i.e., $\\operatorname{ACC}_{\\operatorname{FedAvg}}(T)$ . We report TTA improvement, i.e. the TTA of the baseline, e.g. FedAvg, divided by the TTA of the benchmarked method, e.g. FLOCO. Moreover, we report the expected-calibration-error (ECE) [43], a common measure that evaluates the quality of uncertainty estimation of a trained model, for the last communication round. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 and 2 summarize the main experimental results, where FLOCO and $\\scriptstyle\\mathrm{FLOCO^{+}}$ consistently outperform the baselines across the different experiments in terms of global (red) and local (blue) test accuracy, as well as test ECE. The global and local test metrics are measured after the last communication round and averaged over 5 different seed runs. The best performances are highlighted in bold, while the underlined entries indicate the settings that did not converge properly. Note that the global test performances of FEDAVG and DITTO, as well as FLOCO and $\\mathrm{FLOCO^{+}}$ , are the same since they use the same global model. Below we report on detailed observations. ", "page_idx": 6}, {"type": "text", "text": "Global and local FL test accuracy. We first evaluate the global and local test performance on CIFAR-10 with the non-IID data splits generated by the 5-Fold and $\\operatorname{Dir}(\\beta)$ procedures, as well as the natural non-IID data splits in the FEMNIST dataset. Table 1 shows the test accuracies on CIFAR-10 with CifarCNN trained from random initialization (left) and ResNet-18 fine-tuned from the ImageNet pre-trained model (center), respectively. It also shows the test accuracies on FEMNIST with FemnistCNN trained from random initialization (left) and SqueezeNet fine-tuned from the ImageNet pre-trained model (right). We clearly see that FLOCO and $\\mathrm{FLOCO^{+}}$ outperform all baselines in terms of average local (blue) test accuracy by up to $6\\%$ , as well as global (red) by up to $5\\%$ . ", "page_idx": 6}, {"type": "text", "text": "Calibration. We evaluate and benchmark the quality of uncertainty estimation of all methods. For this purpose we evaluate the global as well as average local ECE on each model-dataset combination for each baseline on the test dataset and show the results in Table 2. As shown, FLOCO and $\\mathrm{FLOCO^{+}}$ achieve better Expected Calibration Error (ECE) across all settings, with two exceptions: training a pre-trained ResNet-18 on the CIFAR-10 Dir(0.3) split and a pre-trained SqueezeNetV1 on FEMNIST. In the first case, the average local ECE for FLOCO and $\\mathrm{FLOCO^{+}}$ is slightly worse than that of FedPer, suggesting mild overconfident for some clients. In the second case, the next best method (APFL) yields a significantly lower global test accuracy than our method, making a fair comparison of their ECE difficult. ", "page_idx": 7}, {"type": "text", "text": "Worst client performance. We evaluate the average local and global test accuracies of the worst $5\\%$ of clients, a standard approach for assessing potential biases of the FL method toward specific clients or client groups [44]. The worst $5\\%$ client performance on all CIFAR-10/model combinations is evaluated over 5 trial runs, with results shown in the table on the right. We observe that FLOCO achieves the highest performance among worstperforming clients across all settings, with a $17\\%$ improvement over FedAvg, and up to $1.5\\%$ over the next best baseline. ", "page_idx": 7}, {"type": "table", "img_path": "JL2eMCfDW8/tmp/4666b3b9a0d2f16c8ad2449005c90175562339143ca2b6d8e02402ffac0e1c40.jpg", "table_caption": ["Table 3: Average local test accuracy for the $5\\%$ worst performing clients on CIFAR-10. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Time-to-accuracy. Similar to Table 1, we plot the TTA improvement for FLOCO. In particular, we show the TTA improvement of FLOCO over FedAvg and FedProx, and the TTA improvement of $\\mathrm{FLOCO^{+}}$ over Ditto, FedPer and FedRod, as all these methods include local fine-tuning. We report all TTAs in Table 4. The underlined entries indicate the cases where the test accuracies of our methods exceed the baseline method\u2019s maximum accuracy already at the initial evaluation round, while the entries labeled $\\cdot_{x l.O}\\!\\cdot$ represent the instances where our methods take the same evaluation rounds to achieve the baseline method\u2019s maximum accuracy, i.e., comparable in terms of TTA. In addition to test accuracy, we also observe an improvement in Time-to-Accuracy (TTA) for our method across all settings. ", "page_idx": 7}, {"type": "table", "img_path": "JL2eMCfDW8/tmp/84c3617aa7c15bee047f65aad0f2452910304bbc4ffd625e5128e6beec012029.jpg", "table_caption": ["Table 4: Improvements for global and local time-to-accuracy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Analysis and Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide further analyses and discussion on FLOCO. ", "page_idx": 7}, {"type": "text", "text": "Solution structure in simplex. First, we confirm that FLOCO uses the degrees of freedom within the solution simplex for personalization. To this end, we draw approximately 500 uniformly distributed points in the solution simplex, and evaluate the global and the local test accuracy of the corresponding models. Figure 1 (bottom row) shows the global test accuracy (left most) and the local test accuracy (center and right) for two clients. As expected, for the global test dataset the solution simplex performs uniformly well across all its area, while the losses for the two individual local client distributions are small around their projected points $(\\star)$ . This result indicates that the heterogeneous sharing of the solution simplex across the clients properly works as designed. ", "page_idx": 7}, {"type": "image", "img_path": "JL2eMCfDW8/tmp/d03f5e2d9fb25ea26ee905585751c82e3c6b1e7fe080502c44ca97c41d4ffcd6.jpg", "img_caption": ["Figure 2: Global (left) and average local (center) test accuracy for CifarCNN on CIFAR-10, 5-Fold. For FLOCO, we can clearly observe a jump in average local test accuracy at $\\tau\\,=\\,250$ , which is a result of our subregion assignment. Right shows the total variance of the gradients for the last fully-connected layer. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Gradient variance reduction and stability of training. Figure 2 shows the test accuracy curves during training for global (left) and average local (center) test accuracies of different methods with the standard deviation over 5 trials as shadows. We observe that FLOCO and $\\operatorname{FLOCO}^{+}$ not only converge faster than the global and $\\mathsf{p F L}$ baselines respectively, but also show small standard deviation across trials. The latter implies that our systematic regularization through the solution simplex stabilizes the training dynamics significantly. Figure 2 (right) shows the total gradient variance\u2014the sum of the variances of the updates $\\Delta\\pmb{w}_{k}^{t}=\\pmb{w}_{k}^{t}-\\pmb{w}_{0}^{t-1}$ for FedAvg and FedProx (which almost overlap with each other), and $\\Delta\\pmb{\\theta}_{m,k}^{t}=\\pmb{\\theta}_{m,k}^{t}-\\pmb{\\theta}_{m,0}^{t-1}$ for FLOCO, respectively. More specifically, we compute the variance over the last fully-connected layer, given by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{\\mathrm{total}}^{2}(t)=\\sum_{k\\in S^{t}}\\|\\Delta\\pmb{w}_{k}^{t}-\\frac{1}{|S^{t}|}\\sum_{k\\in S^{t}}\\Delta\\pmb{w}_{k}^{t}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for FedAvg and FedProx, and by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{\\mathrm{total}}^{2}(t)=\\frac{1}{M+1}\\sum_{m=1}^{M+1}\\sum_{k\\in{\\mathcal{S}}^{t}}\\|\\Delta\\pmb{\\theta}_{m,k}^{t}-\\frac{1}{|\\mathcal{S}^{t}|}\\sum_{k\\in{\\mathcal{S}}^{t}}\\Delta\\pmb{\\theta}_{m,k}^{t}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We have not plotted the gradient variances of $\\mathrm{FLOCO^{+}}$ and the other $\\mathsf{p F L}$ methods, since those are the same as for FLOCO and FEDAVG, respectively. As discussed in [45, 46], a small total variance indicates effective collaborations with consistent gradient signals between the clients, leading to better performance. From the figure, we see that the total gradient variance of FLOCO is much lower and more stable, in terms of standard deviation, than the baseline methods, which, together with its good performance observed in Table 1, is consistent with their discussion. The variance reduction with FLOCO implies that the degrees of freedom of the solution simplex can absorb the heterogeneity of clients to some extent, making the gradient signals more homogeneous. Moreover, [47] argued that the last classification layer has the biggest impact on performance, implying that reducing the total variance of the classification layer, as FLOCO does with simplex learning, is most effective. As we show in the Appendix C, applying simplex learning to only the last layer, instead of learning a simplex in the whole parameter space, achieves faster personalized and global convergence. ", "page_idx": 8}, {"type": "text", "text": "Computational complexity. If the batch size is one, simplex training adds $O(\\pi\\cdot M)$ computational complexity for each layer, where $\\pi$ is the parameter complexity of the layer, e.g., $\\pi\\,=\\,d\\cdot\\,L$ for a fully connected layer with $d$ input and $L$ output neurons, and $M$ is the simplex dimension [30]. For FLOCO, this additional complexity only applies to the classification layer. For inference, no additional complexity arises, compared to FedAvg, because inference is performed by the single model corresponding to the cluster center. Since the most modern architectures, e.g., ResNet-18 and Vision Transformer (ViT) [48], have parameter complexity of $O(\\mathbb{G}_{\\mathrm{FE}})\\gg O(\\mathbb{G}_{\\mathrm{C}})$ , where $\\mathbb{G}_{\\mathrm{FE}}$ and $\\mathbb{G}_{\\mathrm{C}}$ are the complexities of the feature extractor and the classification layer, respectively, the additional training complexity, applied only to the classification layer, of FLOCO is ignorable, i.e., $O(\\mathbb{G}_{\\mathrm{FE}})\\gg O(\\bar{\\mathbb{G}_{\\mathrm{C}}}\\cdot M)$ . The same applies to the communication costs: since the simplex learning is applied only to the classification layer, the increase of communication costs are ignorable compared to the communication costs for the feature extractor. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "There are few existing works that apply simplex learning to federated learning. [35] proposed SuPerFed, which enforces a low loss simplex between independently initialized global and client models, yielding good personalized FL performance. This approach builds on [25], which finds optimal interpolation coefficients between a global and local model to improve personalized FL. However, their simplex is restricted to be 1D, i.e., a line segment, and the global model performance is comparable to the plain FedAvg. Moreover, they train a solution simplex over all layers between global and local models, which is computationally expensive and limits its applicability to training from scratch. This should be avoided if pre-trained models are available [38, 49]. Our method generalizes to training low-loss simplices of higher dimensions in a FL setting, tackles both the global and personalized FL objectives, is applicable to pre-trained models, and shows significant performance gains by employing our proposed subregion assignment procedure. In Table 7 of Appendix E we benchmark FLOCO against the SuPerFed baseline on the CIFAR-10, 5-Fold, as well as Dir(0.5) splits using both a CifarCNN trained from scratch as well as a pre-trained ResNet18 on both global as well as local test performance, where we observe that FLOCO outperforms SuPerFed both in terms of global as well as local accuracy in all settings. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we only evaluate our method on cross-silo FL settings with up to 100 clients. Unlike cross-device FL, which typically involves a much larger set of stateless clients (i.e., clients with limited data that hinders reliable modeling), our approach assumes stateful clients, each with sufficient data to enable effective grouping of similar clients. While our current analysis focuses on cross-silo FL, extending our method to the cross-device setting is an important direction for future research. Additionally, a thorough theoretical analysis of our approach remains a future research objective. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "FL on highly non-IID client data distributions remains a challenging problem and a very actively researched topic. Recent works tackle non-IID FL settings either through global or personalized FL. While the former aims to find a single optimal set of parameters that fit a global objective, the latter tries to optimize multiple local models each of which ftis the local distribution well. These two different objectives may pose a trade-off, that is, personalized FL might adapt models to strongly to local distributions which might harm the global performance, while global FL solutions might fti none of the local distributions if the local distributions are diverse. In this paper, we addressed this issue by leveraging the mode-connectivity of neural networks. Specifically, we propose FLOCO, where each client trains an assigned subregion within the solution simplex, which allows for personalization, and at the same, contributes to learning a well-performing global model. FLOCO achieves state-of-the-art performance in both global and personalized FL, with minimal computational and communication overhead during training and no overhead during inference. ", "page_idx": 9}, {"type": "text", "text": "Promising future research directions include better understanding the decision-making process of solution simplex training through global and local explainable AI methods [50\u201352]. Furthermore, we want to apply our approach to continual learning problems and FL scenarios with highly varying client availability [53, 54]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was funded by the German Ministry for Education and Research as BIFOLD - Berlin Institute for the Foundations of Learning and Data (ref. BIFOLD24B). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273\u20131282, 2017.   \n[2] Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant. Survey of personalization techniques for federated learning. In 2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4), pages 794\u2013797. IEEE, 2020.   \n[3] Felix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered federated learning: Modelagnostic distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and Learning Systems, 32(8):3710\u20133722, 2020.   \n[4] Fan Lai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowdhury. Oort: Efficient federated learning via guided participant selection. In Symposium on Operating Systems Design and Implementation, pages 19\u201335, 2021.   \n[5] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. IEEE Transactions on Signal Processing, 70:1142\u20131154, 2022.   \n[6] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. Advances in Neural Information Processing Systems, 30, 2017.   \n[7] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.   \n[8] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[9] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In International Conference on Machine Learning, pages 1309\u20131318, 2018.   \n[10] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in Neural Information Processing Systems, 31, 2018.   \n[11] Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[12] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In International Conference on Learning Representations, 2021.   \n[13] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do different neural networks learn the same representations? arXiv preprint arXiv:1511.07543, 2015.   \n[14] Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning for image classification. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[15] Xueting Ma, Guorui Ma, Yang Liu, and Shuhan Qi. APCSMA: adaptive personalized clientselection and model-aggregation algorithm for federated learning in edge computing scenarios. Entropy, 26(8):712, 2024.   \n[16] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pages 6357\u20136368, 2021.   \n[17] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. CoRR, abs/1912.00818, 2019.   \n[18] Jiahao Tan and Xinpeng Wang. FL-bench: A federated learning benchmark for solving image classification tasks.   \n[19] Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao, Lorenzo Sani, Hei Li Kwing, Titouan Parcollet, Pedro PB de Gusm\u00e3o, and Nicholas D Lane. Flower: A friendly federated learning research framework. arXiv preprint arXiv:2007.14390, 2020.   \n[20] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. A survey on federated learning. Knowledge-Based Systems, 216:106775, 2021.   \n[21] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Machine Learning and Systems, 2:429\u2013450, 2020.   \n[22] Felix Sattler, Tim Korjakow, Roman Rischke, and Wojciech Samek. Fedaux: Leveraging unlabeled auxiliary data in federated learning. IEEE Transactions on Neural Networks and Learning Systems, 34(9):5531\u20135543, 2023.   \n[23] Naichen Shi, Fan Lai, Raed Al Kontar, and Mosharaf Chowdhury. Fed-ensemble: Ensemble models in federated learning for improved generalization and uncertainty quantification. IEEE Transactions on Automation Science and Engineering, 2023.   \n[24] Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-iid data: A survey. Neurocomputing, 465:371\u2013390, 2021.   \n[25] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning. arXiv preprint arXiv:2003.13461, 2020.   \n[26] Filip Hanzely and Peter Richt\u00e1rik. Federated learning of a mixture of global and local models. arXiv preprint arXiv:1812.01097, 2020.   \n[27] C. Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. In International Conference on Learning Representations, 2017.   \n[28] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259\u20133269, 2020.   \n[29] Gregory Benton, Wesley Maddox, Sanae Lotf,i and Andrew Gordon Gordon Wilson. Loss surface simplexes for mode connecting volumes and fast ensembling. In International Conference on Machine Learning, pages 769\u2013779, 2021.   \n[30] Mitchell Wortsman, Maxwell C Horton, Carlos Guestrin, Ali Farhadi, and Mohammad Rastegari. Learning neural network subspaces. In International Conference on Machine Learning, pages 11217\u201311227, 2021.   \n[31] Mathieu Blondel, Akinori Fujino, and Naonori Ueda. Large-scale multiclass support vector machine training via euclidean projection onto the simplex. In 2014 22nd International Conference on Pattern Recognition, pages 1289\u20131294. IEEE, 2014.   \n[32] Douglas P Hardin and Edward B Saff. Minimal riesz energy point configurations for rectifiable d-dimensional manifolds. Advances in Mathematics, 193(1):174\u2013204, 2005.   \n[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.   \n[34] Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Kone\u02c7cn\\`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.   \n[35] Seok-Ju Hahn, Minwoo Jeong, and Junghye Lee. Connecting low-loss subspace for personalized federated learning. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 505\u2013515, 2022.   \n[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[37] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. Ieee, 2009.   \n[38] John Nguyen, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi, and Michael Rabbat. Where to begin? on the impact of pre-training and initialization in federated learning. In International Conference on Learning Representations, 2023.   \n[39] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and $<0.5~\\mathrm{mb}$ model size. arXiv preprint arXiv:1602.07360, 2016.   \n[40] Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong Zhang. Personalized cross-silo federated learning on non-iid data. In AAAI Conference on Artificial Intelligence, volume 35, pages 7865\u20137873, 2021.   \n[41] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pages 7252\u20137261, 2019.   \n[42] Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu. Feddc: Federated learning with non-iid data via local drift decoupling and correction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[43] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321\u20131330, 2017.   \n[44] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. In International Conference on Learning Representations, 2020.   \n[45] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konec\u02c7ny\\`, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2021.   \n[46] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132\u20135143, 2020.   \n[47] Bo Li, Mikkel N Schmidt, Tommy S Alstr\u00f8m, and Sebastian U Stich. On the effectiveness of partial variance reduction in federated learning with heterogeneous data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3964\u20133973, 2023.   \n[48] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[49] Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han Wei Shen, and Wei-Lun Chao. On the importance and applicability of pre-training for federated learning. In International Conference on Learning Representations, 2022.   \n[50] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLOS ONE, 10(7):1\u201346, 07 2015.   \n[51] Wojciech Samek, Gr\u00e9goire Montavon, Sebastian Lapuschkin, Christopher J. Anders, and Klaus-Robert M\u00fcller. Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE, 109(3):247\u2013278, 2021.   \n[52] Kirill Bykov, Mayukh Deb, Dennis Grinwald, Klaus-Robert M\u00fcller, and Marina M-C H\u00f6hne. Dora: Exploring outlier representations in deep neural networks. Transactions on Machine Learning Research, 2023.   \n[53] A. Rodio, F. Faticanti, O. Marfoq, G. Neglia, and E. Leonardi. Federated learning under heterogeneous and correlated client availability. In IEEE International Conference on Computer Communications, 2023.   \n[54] Philipp Wiesner, Ramin Khalili, Dennis Grinwald, Pratik Agrawal, Lauritz Thamsen, and Odej Kao. Fedzero: Leveraging renewable excess energy in federated learning. In International Conference on Future and Sustainable Energy Systems. ACM, 2024.   \n[55] Richard L Burden and J Douglas Faires. 2.1 the bisection algorithm. Numerical analysis, 3, 1985. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This appendix provides a nomenclature, details to our optimization problem and experimental setup, as well as additional results and insights. ", "page_idx": 14}, {"type": "table", "img_path": "JL2eMCfDW8/tmp/ce736293d65c30a8956036defda4be73bc81e8302689390adc8708545e7693db.jpg", "table_caption": ["Table 5: Nomenclature. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Optimization Problem ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Lagrangian of the lower-level optimization problem in (9) has the following formulation $\\begin{array}{r}{\\mathcal{L}(\\alpha_{k},\\bar{\\lambda})\\,=\\,\\frac{1}{2}\\|\\alpha_{k}-\\kappa_{k}\\|_{2}^{2}+\\lambda({\\bf1}^{T}\\dot{\\alpha_{k}}-z)}\\end{array}$ with $\\lambda\\,\\in\\,\\mathbb R$ being the Langrange multiplier. The Lagrangian can be further rewritten to $\\begin{array}{r}{\\mathcal{L}(\\alpha_{k},\\lambda)=\\frac{1}{2}\\|\\alpha_{k}-(\\kappa_{k}-\\lambda\\mathbf{1})\\|_{2}^{2}+\\lambda(\\mathbf{1}^{T}\\kappa_{k}-z)-\\lambda^{2}n}\\end{array}$ such that the optimization problem reduces to solving ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{z\\in\\mathbb{R}}{\\operatorname*{min}}}&{\\frac{1}{2}\\|\\alpha_{k}-(\\kappa_{k}-\\lambda\\mathbf{1})\\|_{2}^{2}}\\\\ {\\mathrm{ject}\\,\\mathrm{to:}}&{\\alpha_{k}\\succeq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The optimal solution of (14) is given by ${\\pmb{\\alpha}}_{k}^{*}=[{\\pmb{\\kappa}}_{k}-\\lambda^{*}{\\bf1}]_{+}$ . Plugging it back into the Lagrangian we get the following dual function ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{L}(\\alpha_{k},\\lambda)=\\displaystyle\\frac{1}{2}\\|[\\kappa_{k}-\\lambda^{*}\\mathbf{1}]_{+}-(\\kappa_{k}-\\lambda\\mathbf{1})\\|_{2}^{2}+\\lambda(\\mathbf{1}^{T}\\kappa_{k}-z)-\\lambda^{2}n}}\\\\ {{=\\displaystyle\\frac{1}{2}\\|[\\kappa_{k}-\\lambda^{*}\\mathbf{1}]_{-}\\|_{2}^{2}+\\lambda(\\mathbf{1}^{T}\\kappa_{k}-z)-\\lambda^{2}n.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finding $\\alpha_{k}^{*}$ can be achieved by maximizing (17) using for example the bisection algorithm [55].   \nAfter that the projected points are obtained as $\\pmb{\\alpha}_{k}^{*}=[\\pmb{\\kappa}_{k}-\\lambda^{*}\\pmb{1}]_{+}$ . ", "page_idx": 14}, {"type": "text", "text": "B Training Hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 6 summarizes all hyperparameters that were used for each dataset/model combination. We train CifarCNN on CIFAR-10 for a total of 500 communication rounds, ResNet-18 on CIFAR-10 for 100 communication rounds, FemnistCNN on FEMNIST for 350 rounds, and SqueezeNetV1 on FEMNIST for 1000 rounds. Moreover, we train each setting using a total of 100 clients, and for FEMNIST we select a randomly chosen subset of 100 total clients for each trial, of which we select 10 randomly to participate in training in each communication round, except for CifarCNN on CIFAR-10 where we select 30 out of 100 clients to participate in each round. We evaluate all clients after every ten communication rounds. For CIFAR-10 we train a CifarCNN with batch size 50 using SGD with a learning rate of 0.02, momentum of 0.5, and weight decay of $10^{-5}$ , and a pre-trained ", "page_idx": 14}, {"type": "text", "text": "ResNet-18 with learning rate of batch size 32, using SGD with a learning rate of 0.01, momentum of 0.9, and weight decay of $10^{-4}$ . For FEMNIST we train a pre-trained SqueezeNet with batch size 32 using SGD with a learning rate of 0.005, momentum of 0, weight decay of $10^{-4}$ , and a FemnistCNN with batch size 32, learning rate 0.1, momentum of 0, weight decay of 0. For FedProx we set the proximity hyperparameter to $\\mu=0.01$ for all settings. For DITTO, FEDROD and FEDPER we set the local epochs to the same value as epochs for the global model, i.e. $E_{\\mathrm{DITT0}}=E$ . All training hyperparameters for CIFAR-10 and FEMNIST on a FemnistCNN were taken from [35], CIFAR-10 on a pre-trained ResNet-18 from [49] and FEMNIST on pre-trained SqueezeNet from [38]. ", "page_idx": 15}, {"type": "table", "img_path": "JL2eMCfDW8/tmp/f9192222afa87ca3942e3344d5e7cebe44b28f36fe955e63de4f88edc91632da.jpg", "table_caption": ["Table 6: Summary of used hyperparameters for training. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Simplex Learning on all NN parameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Figure 5, we compare the global (left) and average client (right) test accuracy of FLOCO and FLOCO-All, where the latter applies simplex learning to all NN parameters. As expected, FLOCO-All converges to the same global and average local test accuracy, but needs more communication rounds to do so, since it needs to train more parameters. ", "page_idx": 15}, {"type": "image", "img_path": "JL2eMCfDW8/tmp/c4bc7c8687943654836e933cb42e43c457177a9c752d2050cc03153f1bf2c638.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: Comparing simplex learning on all network layers vs. only on the last fully-connected layer. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Sensitivity to Parameter Setting ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We investigate how stable the performance of FLOCO is for different hyperparameter settings. Specifically, we tested FLOCO with the combination of $\\tau=50$ , 100, 200 (subregion assignment time step) and $\\rho=0.1,0.2,0.4$ (radius of subregions), and show the average local client and global test accuracy for CifarCNN on CIFAR-10 5-Fold in Figure 6. We observe that the average local client test accuracy (left) increases for earlier subregion assignment starting points $\\tau$ and lower client subregion radiuses $\\rho$ , with the best reached test accuracy being approximately $4\\%$ better than the worst, i.e., $82.79\\%$ against $78.18\\%$ . The intuition for this is that earlier client specialization in less overlapping regions allows for better personalization. On the other hand, as can be observed in the right heatmap of Figure 6 the global test performance is less sensitive to the choice of these hyperparameters, i.e., $70.66\\%$ against $69.30\\%$ . This is because, even after subregion assignment, the entire solution simplex remains to be trained, making the midpoint (global model) of the simplex less sensitive to the specialization process for client distributions. ", "page_idx": 15}, {"type": "table", "img_path": "JL2eMCfDW8/tmp/8ddddac465b8893e99ce5af658b63e031639bd87d42f747237769c5ca949165a.jpg", "table_caption": ["Table 7: Average global and local test accuracy on CIFAR-10. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "JL2eMCfDW8/tmp/8f6de4f471ba2dcefe46678229109904e8bb036cd35005cbe743e0db8583f3c5.jpg", "img_caption": ["Figure 6: Local average client (left) and global (right) test accuracies for different subregion assignment time step $\\tau$ and subregion radius $\\rho$ settings. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Comparing FLOCO to SuPerFed ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We benchmark FLOCO against the SuPerFed baseline on the CIFAR-10, 5-Fold, as well as Dir(0.5) splits using both a CifarCNN trained from scratch as well as a pre-trained ResNet18, on both global as well as local test performance. As shown in Table 7, FLOCO outperforms SuPerFed in all settings. Note, that for this benchmark we have implemented FLOCO as well as SUPERFED in the FL framework Flower [19]. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 17}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 17}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 17}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 17}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 17}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Our claims in the abstract and introduction are empirically proven and explained in our contributions. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We show that, in some cases, our method does not exceed the performance of a baseline method, however, we show that it saves up computational cost which is very relevant in the field. We discuss this point in more detail and give an alternative solution. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper does not include any theoretic assumption or proof. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper gives detailed information on how to reproduce our results, including all hyperparameters, models and dataset splits needed as well as a detailed description for our algorithm. Moreover, we upload our code together with the submission which includes a README that documents how experiments can be reproduced. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We provide our code in the submission which includes a README with detailed description on how to run our method in order to reproduce the shown results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide all the training and test details, including models, data splits, hyperparameters, model architectures etc. that are necessary to reproduce our results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Each of our experiments is run across 5 trial runs with different random seeds in order to show confidence intervals for ours training and test runs. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We did not explicitly compute the resources needed. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and made sure to respect it in every respect. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not release any data or models that pose such risks. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide citations for all used datasets, models, and baselines. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There are no new assets introduced in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]