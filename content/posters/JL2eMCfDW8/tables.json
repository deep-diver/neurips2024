[{"figure_path": "JL2eMCfDW8/tables/tables_6_1.jpg", "caption": "Table 1: Average global and local test accuracy.", "description": "This table presents the average global and local test accuracy for different federated learning methods across various datasets and model configurations.  The datasets include CIFAR-10 with two types of non-IID data splits (5-Fold and Dir(0.3)) and FEMNIST. Models used are CifarCNN, pre-trained ResNet-18, FemnistCNN, and pre-trained SqueezeNet.  The table compares the performance of FLOCO and FLOCO+ against several baselines such as FedAvg, FedProx, FedPer, APFL, Ditto, and FedRod.  Higher accuracy values indicate better performance.", "section": "4.2 Results"}, {"figure_path": "JL2eMCfDW8/tables/tables_6_2.jpg", "caption": "Table 1: Average global and local test accuracy.", "description": "This table presents the average global and local test accuracy results for different federated learning methods on CIFAR-10 and FEMNIST datasets.  Different data heterogeneity scenarios (5-Fold and Dir(0.3)) are used for CIFAR-10.  The results are broken down by model (CifarCNN, pre-trained ResNet-18, FemnistCNN, pre-trained SqueezeNet) and method (FedAvg, FedProx, FedPer, APFL, Ditto, FedRoD, FLOCO, FLOCO+).  It allows for a comparison of the performance of various federated learning approaches under different settings and levels of personalization.", "section": "4.2 Results"}, {"figure_path": "JL2eMCfDW8/tables/tables_7_1.jpg", "caption": "Table 3: Average local test accuracy for the 5% worst performing clients on CIFAR-10.", "description": "This table presents the average local test accuracy achieved by the 5% worst-performing clients across different federated learning methods.  The results are reported for two distinct non-IID data splits of CIFAR-10: 5-Fold and Dir(0.3).  It allows for a comparison of various methods' robustness to outliers and their ability to provide good performance even for the most challenging clients.", "section": "4.2 Results"}, {"figure_path": "JL2eMCfDW8/tables/tables_7_2.jpg", "caption": "Table 4: Improvements for global and local time-to-accuracy.", "description": "This table presents the time-to-best-accuracy (TTA) improvements achieved by FLOCO and FLOCO+ compared to various baseline methods (FedAvg, FedProx, Ditto, FedPer, and FedRod). It shows how many fewer communication rounds were needed by FLOCO and FLOCO+ to achieve the same test accuracy as the baseline methods for both global and local performance across different datasets and model settings.  A value of \"xN\" indicates that FLOCO or FLOCO+ achieved the same accuracy in 1/N of the communication rounds compared to the baseline methods. For example, \"x5.5\" means that FLOCO took only 1/5.5 the number of rounds as the baseline method to achieve the same level of accuracy.", "section": "4.2 Results"}, {"figure_path": "JL2eMCfDW8/tables/tables_14_1.jpg", "caption": "Table 5: Nomenclature.", "description": "This table lists the symbols used in the paper and their corresponding descriptions.  It provides a quick reference for understanding the notation used throughout the paper, covering variables related to clients, communication rounds, model parameters, simplex structure, subregions, and gradient updates.", "section": "Appendix"}, {"figure_path": "JL2eMCfDW8/tables/tables_15_1.jpg", "caption": "Table 6: Summary of used hyperparameters for training.", "description": "This table presents the hyperparameters used for training different models on various datasets.  It shows the number of communication rounds (T), number of clients (K), the size of the client subset selected for each round (|St|), the number of local epochs (e), the number of local epochs for the personalized methods (E/EDITTO), the learning rate (\u03b3), momentum (mom.), weight decay (wd), and proximity regularization parameter (\u03bc).  Each row corresponds to a different model-dataset combination.", "section": "B Training Hyperparameters"}, {"figure_path": "JL2eMCfDW8/tables/tables_16_1.jpg", "caption": "Table 7: Average global and local test accuracy on CIFAR-10.", "description": "This table compares the performance of FLOCO and SuPerFed on the CIFAR-10 dataset using two different non-IID data splits (5-Fold and Dir(0.3)).  It shows the global and local test accuracy for both CifarCNN (trained from scratch) and pre-trained ResNet-18.  The results demonstrate FLOCO's improved performance over SuPerFed.", "section": "E Comparing FLOCO to SuPerFed"}]