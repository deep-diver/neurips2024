[{"figure_path": "DNGfCVBOnU/figures/figures_3_1.jpg", "caption": "Figure 1: Weight alignment to randomly fixed synaptic feedback induced through random noise training. (a) Forward and backward pathways of backpropagation and feedback alignment. (b) Possible scenario of the feedback alignment algorithm in a biological synaptic circuit. (c) Schematic of random training, where the input x and label y are randomly sampled and paired in each iteration. (d) Cross-entropy loss during random training. (e) Alignment angle between forward weights and synaptic feedbacks in the last layer. (f) Alignment angle with various random input conditions.", "description": "This figure shows the results of experiments on weight alignment during random noise training using a neural network with feedback alignment.  Panel (a) compares the pathways of backpropagation and feedback alignment. Panel (b) illustrates a possible biological interpretation of the feedback alignment mechanism. Panel (c) details the random input and label generation process. Panel (d) plots the cross-entropy loss during random noise training. Panels (e) and (f) show the alignment angle between forward weights and synaptic feedback weights at different stages of training and under various input conditions.  The results demonstrate that random noise training aligns the network weights to the fixed random feedback weights.", "section": "4 Results"}, {"figure_path": "DNGfCVBOnU/figures/figures_4_1.jpg", "caption": "Figure 1: Weight alignment to randomly fixed synaptic feedback induced through random noise training. (a) Forward and backward pathways of backpropagation and feedback alignment. (b) Possible scenario of the feedback alignment algorithm in a biological synaptic circuit. (c) Schematic of random training, where the input x and label y are randomly sampled and paired in each iteration. (d) Cross-entropy loss during random training. (e) Alignment angle between forward weights and synaptic feedbacks in the last layer. (f) Alignment angle with various random input conditions.", "description": "This figure demonstrates the effect of random noise pretraining on weight alignment to synaptic feedback. Panel (a) shows the difference between backpropagation and feedback alignment, highlighting the weight transport problem solved by feedback alignment. Panel (b) illustrates the biological plausibility of this mechanism. Panel (c) depicts the process of random noise pretraining. Panel (d) shows the cross-entropy loss during training. Panel (e) shows the alignment angle between forward weights and synaptic feedback. Panel (f) shows the effect of various input conditions on alignment.", "section": "4 Results"}, {"figure_path": "DNGfCVBOnU/figures/figures_5_1.jpg", "caption": "Figure 2: Effect of random noise pretraining on subsequent data training. (a) Design of the MNIST classification task to investigate the effect of random training. (b) Test accuracy during the training process, where the inset demonstrates the convergence speed of each training method, calculated by the AUC of the test accuracy. (c) Alignment angle between weights and synaptic feedback across random training and data training. (d) Trajectory of weights (W1) toward synaptic feedback matrix (B1) in latent space obtained by PCA for random and data training. (e) Distance between the weights (W1) and the synaptic feedback matrix (B1). (f) Order dependence of the trajectory of the weights (W1). (g) Distance between the weights (W1) and the synaptic feedback matrix (B1) for different orders of random and data trainings.", "description": "This figure shows the effects of random noise pretraining on a neural network's subsequent learning with the MNIST dataset.  It demonstrates faster convergence and higher test accuracy in the pretrained network compared to a network trained without pretraining. The figure uses various visualizations, including test accuracy curves, alignment angles between forward weights and synaptic feedback, and PCA-based trajectories of weight vectors in latent space, to illustrate the positive impact of the pretraining phase.", "section": "4.2 Pretraining random noise enables fast learning during subsequent data training"}, {"figure_path": "DNGfCVBOnU/figures/figures_6_1.jpg", "caption": "Figure 1: Weight alignment to randomly fixed synaptic feedback induced through random noise training. (a) Forward and backward pathways of backpropagation and feedback alignment. (b) Possible scenario of the feedback alignment algorithm in a biological synaptic circuit. (c) Schematic of random training, where the input x and label y are randomly sampled and paired in each iteration. (d) Cross-entropy loss during random training. (e) Alignment angle between forward weights and synaptic feedbacks in the last layer. (f) Alignment angle with various random input conditions.", "description": "This figure demonstrates the effect of random noise pretraining on weight alignment to synaptic feedback. It shows the pathways of backpropagation and feedback alignment, a possible biological mechanism, and the random training process.  The graphs illustrate the decrease in cross-entropy loss during random training and the increase in alignment angle between forward weights and synaptic feedback, demonstrating the effectiveness of the random noise pretraining in aligning the weights.", "section": "4 Results\n4.1 Weight alignment to synaptic feedback during random noise training"}, {"figure_path": "DNGfCVBOnU/figures/figures_7_1.jpg", "caption": "Figure 5: Robust generalization of \u201cout-of-distribution\u201c tasks in randomly pretrained networks. (a) Training in-distribution data (MNIST) in untrained and randomly pretrained networks. (b) Out-of-distribution generalization tests on transformed MNIST. (c) Out-of-distribution generalization tests on USPS dataset.", "description": "This figure demonstrates the robust generalization capabilities of the randomly pretrained networks on out-of-distribution tasks.  Panel (a) shows the training setup using MNIST data and random noise pretraining. Panel (b) shows the testing results on transformed MNIST data (translated, scaled, and rotated), indicating superior performance for the randomly pretrained network.  Finally, panel (c) displays testing results on the USPS dataset, further highlighting the generalization ability of the randomly pretrained network.", "section": "4.3 Pre-regularization by random noise training enables robust generalization"}, {"figure_path": "DNGfCVBOnU/figures/figures_8_1.jpg", "caption": "Figure 6: Task-agnostic fast learning for various tasks in randomly trained networks. (a) Three tasks used to test the task-agnostic property of random training, showing the meta-loss during the random training process. The meta-loss is calculated from the sum of the losses measured during adaptation to each task. (b) Trajectory of weights in the latent space for adaptation to each task of an untrained network and a randomly trained network. (c) Adaptation to each task of an untrained network and a randomly trained network.", "description": "This figure demonstrates the task-agnostic fast learning capabilities of a randomly pretrained network.  Panel (a) shows the meta-loss (a measure of adaptation difficulty) decreasing during random noise pretraining across three different tasks (MNIST, Fashion-MNIST, and Kuzushiji-MNIST). Panel (b) visualizes the trajectory of weights in a latent space during adaptation to these tasks, highlighting the more efficient adaptation of the pretrained network.  Finally, panel (c) shows that the randomly pretrained network adapts to these tasks significantly faster than an untrained network.", "section": "4.4 Task-agnostic fast learning for various tasks by a network pretrained with random noise"}]