[{"figure_path": "SO1aRpwVLk/figures/figures_0_1.jpg", "caption": "Figure 1: 4Real is a 4D generation framework that can generate near-photorealistic dynamic scenes from text prompts. We use deformable 3D Gaussian Splats (D-3DGS) to model the scene. After generation, We can view the generated dynamic scenes at any timestep from different camera poses.", "description": "This figure demonstrates the 4Real framework's ability to generate photorealistic 4D dynamic scenes from text descriptions.  It shows how the system uses deformable 3D Gaussian Splats (D-3DGS) to represent the scene and allows viewing the generated scene from different viewpoints and at various timesteps.  The examples illustrate the photorealism and dynamic capabilities of the model.", "section": "Abstract"}, {"figure_path": "SO1aRpwVLk/figures/figures_2_1.jpg", "caption": "Figure 2: Generate reference and freeze-time videos. Given the input text prompt, we first generate a reference video using text-to-video diffusion model. The reference video will be our target to perform 4D reconstruction. In order to obtain canonical Gaussian Splats, we generate a freeze-time video by performing frame-conditioned video generation with prompt engineering and context embedding. We further perform auto-regressive generation to expand view angle coverage.", "description": "This figure illustrates the process of generating a reference video and a freeze-time video.  First, a reference video is generated from a text prompt using a text-to-video diffusion model. This video will serve as the target for 4D reconstruction. Then, a freeze-time video is created using frame-conditioned video generation, along with prompt engineering and context embedding, which ensures that the resulting video contains only minimal object movement while the camera moves around the scene. Finally, autoregressive generation is used to expand the viewpoint coverage of the freeze-time video.", "section": "3.2 Generating Reference and Freeze-time Videos"}, {"figure_path": "SO1aRpwVLk/figures/figures_4_1.jpg", "caption": "Figure 3: Reconstructing Deformable 3DGS. First, we reconstruct the canonical 3DGS using the freeze-time video. As the generated freeze-time video may still contain object motions and geometric inconsistencies, we propose to model these imperfections as per-frame deformations that are learned jointly with the canonical 3DGS. Then, we reconstruct the temporal deformation from the reference video with the learned canonical 3DGS. In addition to the reconstruction loss and motion regularization, we propose a video SDS strategy. The video SDS strategy includes a multi-view SDS, using videos of freeze time and sampled camera trajectory, and a temporal SDS, using videos of fixed camera and sampled time steps.  denotes learnable parameters.", "description": "This figure illustrates the process of reconstructing deformable 3D Gaussian Splats (D-3DGS) from a freeze-time video and a reference video.  It shows how the canonical 3D representation is learned from the freeze-time video, handling inconsistencies through per-frame deformations. Temporal deformations are then learned from the reference video to capture dynamic interactions. The process utilizes a video Score Distillation Sampling (SDS) strategy involving multi-view and temporal SDS to improve reconstruction quality.", "section": "3.3 Robust Reconstruction of 3DGS from Noisy Freeze-time Videos"}, {"figure_path": "SO1aRpwVLk/figures/figures_7_1.jpg", "caption": "Figure 4: Generating challenging scenes. 4Real is able to generate dynamic scenes with complex illumination and (semi)-transparent materials such as water. It is flexible to produce diverse content and generate multi-object scenes. Please refer to our supplementary webpage for the complete results.", "description": "This figure shows examples of 4D scenes generated by the 4Real model, demonstrating its ability to handle complex lighting conditions, semi-transparent materials (like water), and scenes with multiple interacting objects.  The image showcases the model's versatility in generating photorealistic and diverse dynamic scenes. The caption directs the reader to the supplementary materials for more examples.", "section": "Experiment"}, {"figure_path": "SO1aRpwVLk/figures/figures_7_2.jpg", "caption": "Figure 5: Qualitative comparison to state-of-the-arts object-centric 4D generation methods.", "description": "This figure compares the results of 4Real with two other state-of-the-art object-centric 4D generation methods (4Dfy and Dream-in-4D).  For each method, the figure shows example video frames generated from the same text prompts at different viewpoints and time steps. The comparison highlights the improved photorealism and scene diversity achieved by 4Real, particularly in handling complex scenes with multiple objects, semi-transparent materials, and dynamic lighting.", "section": "5 Qualitative comparison"}, {"figure_path": "SO1aRpwVLk/figures/figures_8_1.jpg", "caption": "Figure 6: Comparative user study with state-of-the-art object-centric 4D generation methods.", "description": "This figure displays the results of a user study comparing the proposed 4Real method to two state-of-the-art object-centric 4D generation methods: 4Dfy and Dream-in-4D.  Seven qualitative aspects were evaluated: Motion Realism, Foreground Photo-Realism, Background Photo-Realism, Shape Realism, Realism in General, Which is More dynamic, and Video-Text Alignment.  For each criterion, a bar chart shows the percentage of times each method was preferred.  The results indicate that 4Real significantly outperforms the competing methods across all seven criteria, demonstrating its superiority in generating photorealistic and dynamic 4D scenes.", "section": "4 Experiment"}, {"figure_path": "SO1aRpwVLk/figures/figures_8_2.jpg", "caption": "Figure 5: Qualitative comparison to state-of-the-arts object-centric 4D generation methods.", "description": "This figure presents a qualitative comparison of 4Real's 4D scene generation results against two state-of-the-art object-centric methods: 4Dfy and Dream-in-4D.  The comparison highlights 4Real's superior ability to generate photorealistic dynamic scenes with complex lighting and semi-transparent materials.  The figure showcases examples demonstrating different scenarios and object combinations, revealing 4Real's strength in creating diverse, high-quality 4D content.", "section": "Implementation Details"}, {"figure_path": "SO1aRpwVLk/figures/figures_9_1.jpg", "caption": "Figure 8: Ablation study of the impact of removing each component from the proposed pipeline.", "description": "This figure shows the ablation study results of removing each component from the proposed pipeline.  It demonstrates the individual contributions of per-frame deformation, multi-view SDS, freeze-time videos, and joint temporal & multi-view SDS to the overall quality and realism of the generated videos. By comparing the results with and without each component, the figure highlights the importance of each component for achieving high-quality photorealistic dynamic scenes.", "section": "3.3 Robust Reconstruction of 3DGS from Noisy Freeze-time Videos"}, {"figure_path": "SO1aRpwVLk/figures/figures_9_2.jpg", "caption": "Figure 8: Ablation study of the impact of removing each component from the proposed pipeline.", "description": "This figure shows an ablation study evaluating the impact of removing different components from the proposed 4Real pipeline.  By systematically removing key parts, such as per-frame deformation, multi-view SDS, freeze-time videos, and joint temporal & multi-view SDS, the authors analyze how each component affects the final generated video's quality and visual fidelity. The results highlight the importance of each component in achieving near-photorealistic dynamic scene generation.", "section": "3.3 Robust Reconstruction of 3DGS from Noisy Freeze-time Videos"}, {"figure_path": "SO1aRpwVLk/figures/figures_18_1.jpg", "caption": "Figure 3: Reconstructing Deformable 3DGS. First, we reconstruct the canonical 3DGS using the freeze-time video. As the generated freeze-time video may still contain object motions and geometric inconsistencies, we propose to model these imperfections as per-frame deformations that are learned jointly with the canonical 3DGS. Then, we reconstruct the temporal deformation from the reference video with the learned canonical 3DGS. In addition to the reconstruction loss and motion regularization, we propose a video SDS strategy. The video SDS strategy includes a multi-view SDS, using videos of freeze time and sampled camera trajectory, and a temporal SDS, using videos of fixed camera and sampled time steps. denotes learnable parameters.", "description": "This figure illustrates the process of reconstructing deformable 3D Gaussian Splats (D-3DGS) for dynamic scene representation.  It starts with a freeze-time video (containing minimal motion) and uses it to reconstruct canonical 3DGS. To address inconsistencies in the freeze-time video, per-frame deformations are learned jointly with the canonical 3DGS.  Finally, temporal deformations are learned from the reference video to capture dynamic scene behavior. The process uses a video score distillation sampling (SDS) strategy for improved quality and regularization. ", "section": "3.3 Robust Reconstruction of 3DGS from Noisy Freeze-time Videos"}]