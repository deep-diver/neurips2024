[{"figure_path": "wjbTHLUSzU/tables/tables_6_1.jpg", "caption": "Table 1: Information of the target datasets for instruction tuning.", "description": "This table presents information about the datasets used for instruction tuning in the TSDS paper. It includes the dataset name, the type of task (Multilingual QA, Multiple choice, Reasoning), the number of test instances, the number of query examples, the number of shots (number of QA examples provided in the prompt), and the evaluation metric used (F1 score or Accuracy). This table helps to understand the experimental setup and the evaluation metrics used in the paper's experiments on task-specific instruction tuning.", "section": "5.1 Evaluation on Task-Specific Instruction Tuning"}, {"figure_path": "wjbTHLUSzU/tables/tables_7_1.jpg", "caption": "Table 2: Performance of instruction tuning with dataset selected by our method compared with the baselines. The subscripts represent the standard deviations.", "description": "This table presents the results of instruction tuning experiments comparing three data selection methods: Rand (random selection), LESS (state-of-the-art gradient similarity-based selection), and the proposed TSDS method.  The performance (F1 score and accuracy) of two language models (LLAMA-2-7B and MISTRAL-7B) is evaluated on three tasks (TydiQA, MMLU, and BBH) with different data selection ratios (0.5%, 1.0%, 5%).  The \"Base\" row shows the performance without finetuning, and the \"Full\" row shows the performance using the full dataset.  The subscripts indicate standard deviations across three runs for each setting.", "section": "5.1 Evaluation on Task-Specific Instruction Tuning"}, {"figure_path": "wjbTHLUSzU/tables/tables_8_1.jpg", "caption": "Table 3: Training, validation, test sizes and the number of classes in the datasets.", "description": "This table presents the sizes of the training, validation, and test sets, along with the number of classes, for four different datasets used in the domain-specific continued pretraining experiments.  The datasets represent diverse domains, including biomedical, movie reviews, computer science, and news articles. The metrics used for evaluating performance are also listed.", "section": "5.2 Evaluation on Domain-Specific Continued Pretraining"}, {"figure_path": "wjbTHLUSzU/tables/tables_8_2.jpg", "caption": "Table 2: Performance of instruction tuning with dataset selected by our method compared with the baselines. The subscripts represent the standard deviations.", "description": "This table presents the results of instruction tuning experiments comparing three data selection methods: Random sampling, LESS (a state-of-the-art method), and the proposed TSDS method.  The performance is measured by F1 score (TydiQA) and accuracy (MMLU, BBH) across three different datasets (TydiQA, MMLU, BBH) and three selection ratios (0.5%, 1%, 5%).  The table shows the average performance and standard deviation across three runs with different random seeds for each method and selection ratio,  alongside the baseline results for the model without finetuning and with finetuning using the full dataset.", "section": "5.1 Evaluation on Task-Specific Instruction Tuning"}, {"figure_path": "wjbTHLUSzU/tables/tables_20_1.jpg", "caption": "Table 2: Performance of instruction tuning with dataset selected by our method compared with the baselines. The subscripts represent the standard deviations.", "description": "This table presents the results of instruction tuning experiments, comparing the performance of three different data selection methods (Rand, LESS, and Ours) against a baseline (Base) and the full dataset (Full).  The comparison is done across three different datasets (TydiQA, MMLU, BBH) and two language models (LLaMA-2-7B and MISTRAL-7B), with varying dataset selection ratios (0.5%, 1%, 5%).  The F1 score and accuracy are reported as the evaluation metrics, along with standard deviations to indicate variability.", "section": "5.1 Evaluation on Task-Specific Instruction Tuning"}, {"figure_path": "wjbTHLUSzU/tables/tables_20_2.jpg", "caption": "Table 6: Hyperparameters for continued pretraining.", "description": "This table lists the hyperparameters used for the continued pretraining stage of the experiments described in Section 5.2 of the paper.  The hyperparameters control various aspects of the training process, including the maximum length of input sequences, batch size, optimizer, weight decay parameters for Adam, warmup ratio, learning rate scheduler, and learning rate.", "section": "5.2 Evaluation on Domain-Specific Continued Pretraining"}, {"figure_path": "wjbTHLUSzU/tables/tables_22_1.jpg", "caption": "Table 8: Performance of instruction tuning with dataset selected by our method compared with the LESS. The dataset size is 4% of the candidate data repository and we train each model for one epoch on the selected set. The subscripts represent the standard deviations.", "description": "This table compares the performance of the proposed TSDS method with the LESS baseline method for task-specific instruction tuning.  The experiment uses a dataset size of 4% of the candidate data repository and trains each model for only one epoch.  The table shows F1 scores and accuracy for three different tasks (TydiQA, MMLU, BBH) and two different language models (LLAMA-2-7B and MISTRAL-7B).  Subscripts indicate standard deviations.", "section": "E.3 Task-Specific Instruction Tuning for One epoch"}, {"figure_path": "wjbTHLUSzU/tables/tables_22_2.jpg", "caption": "Table 2: Performance of instruction tuning with dataset selected by our method compared with the baselines. The subscripts represent the standard deviations.", "description": "This table presents the results of instruction tuning experiments, comparing the performance of three different data selection methods against a baseline.  The methods are: randomly selecting data, using the LESS method (a state-of-the-art technique), and using the authors' proposed TSDS method.  The table shows F1 scores and accuracy for three different tasks (TydiQA, MMLU, and BBH) across different model sizes and data selection ratios (0.5%, 1%, and 5%).  The subscripts indicate standard deviations across three different runs with different random seeds, demonstrating the performance variability of each method.", "section": "5.1 Evaluation on Task-Specific Instruction Tuning"}, {"figure_path": "wjbTHLUSzU/tables/tables_23_1.jpg", "caption": "Table 10: Performance of KNN-KDE when the kernel size varies. F1 scores of the downstream tasks are reported with standard deviations shown in the subscripts.", "description": "This table presents the results of experiments evaluating the impact of different kernel sizes on the performance of the KNN-KDE method.  The F1 scores for three downstream tasks (ChemProt, AGNews, IMDB) are shown, with standard deviations included to indicate the variability of the results.  The kernel sizes tested were 0.1, 0.3, and 0.5.", "section": "E.5.2 Effects of Kernel Size in KNN-KDE"}, {"figure_path": "wjbTHLUSzU/tables/tables_23_2.jpg", "caption": "Table 4: F1 scores of the downstream tasks. Standard deviations are shown in the subscripts.", "description": "This table presents the F1 scores achieved on four different downstream classification tasks (ChemProt, IMDB, SCIERC, AGNews) using three different data selection methods (Rand, DSIR, Ours) and three different sizes of annotated data (1K, 3K, 10K).  The \"Base\" row shows the performance without data selection or continued pre-training.  The table highlights the improvements in F1 score obtained by using the proposed TSDS method compared to baseline methods.", "section": "5.2 Evaluation on Domain-Specific Continued Pretraining"}, {"figure_path": "wjbTHLUSzU/tables/tables_24_1.jpg", "caption": "Table 12: The neighborhood size of KNN-Uniform / KNN-KDE for different values of \u03b1. The numbers before the slashes are for KNN-Uniform and those after are for KNN-KDE.", "description": "This table shows the average neighborhood size used by the KNN-Uniform and KNN-KDE algorithms for different values of the hyperparameter \u03b1.  The neighborhood size represents the number of nearest neighbors considered when selecting data for training. The numbers before the slash are the neighborhood sizes for KNN-Uniform, and the numbers after the slash are the neighborhood sizes for KNN-KDE.  The results are presented for three different datasets: ChemProt (1K), AGNews (3K), and IMDB (10K).", "section": "E.5 Micro-Benchmarks"}]