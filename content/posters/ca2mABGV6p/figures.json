[{"figure_path": "ca2mABGV6p/figures/figures_0_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure displays the results of applying the Faster Diffusion method to various image generation tasks, including Stable Diffusion, DeepFloyd-IF, and ControlNet. For each task, it compares the original inference time with the time achieved after applying the Faster Diffusion method, showing a significant reduction in inference time across all tasks.", "section": "1 Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_1_1.jpg", "caption": "Figure 3: Analyzing the UNet in Diffusion Model. (a) Feature evolving across adjacent time-steps is measured by MSE. (b) We extract the hierarchical features output of different layers of the UNet at each time-step, average them along the channel dimension to obtain two-dimensional hierarchical features, and then calculate their Frobenius norm. (c) The hierarchical features of the UNet encoder show a lower standard deviation, while those of the decoder exhibit a higher standard deviation.", "description": "This figure analyzes the UNet architecture used in diffusion models. It shows that the encoder features change minimally across time steps, while the decoder features exhibit substantial variations. This difference is quantified using Mean Squared Error (MSE) and Frobenius norm. The figure provides evidence that encoder computations can be omitted or reused at certain time steps for acceleration purposes, while preserving model accuracy.", "section": "3.2 Analyzing the UNet in Diffusion Model"}, {"figure_path": "ca2mABGV6p/figures/figures_2_1.jpg", "caption": "Figure 3: Analyzing the UNet in Diffusion Model. (a) Feature evolving across adjacent time-steps is measured by MSE. (b) We extract the hierarchical features output of different layers of the UNet at each time-step, average them along the channel dimension to obtain two-dimensional hierarchical features, and then calculate their Frobenius norm. (c) The hierarchical features of the UNet encoder show a lower standard deviation, while those of the decoder exhibit a higher standard deviation.", "description": "This figure analyzes the UNet architecture used in diffusion models. It shows that the encoder features change minimally across adjacent time steps, while the decoder features change significantly.  This analysis is based on the mean squared error (MSE) between adjacent time steps' features, the Frobenius norm of hierarchical features, and the standard deviation of those norms.  The minimal change in encoder features suggests that computational savings can be made by reusing these features across multiple time steps, while the significant variations in decoder features emphasizes the importance of computing these for each step.", "section": "3.2 Analyzing the UNet in Diffusion Model"}, {"figure_path": "ca2mABGV6p/figures/figures_3_1.jpg", "caption": "Figure 4: (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. We omit the encoder at certain adjacent time-steps and reuse in parallel the encoder features in the previous time-steps for the decoder. Applying encoder propagation for uniform strategy every two iterations. Note, at time-step t-1, predicting noise does not require Zt-1 (i.e., Eq. 1: 2t-2 = 1/\u221aat-1 (1 - at-1/at-2)zt-1 + \u221aat-1 \u03b5(zt-1,t-1,c)). (d) Decoder propagation. The generated images often fail to cover some specific objects in the text prompt. For example, given one prompt case \"A man with a beard wearing glasses and a beanie\", this method fails to generate the glasses subject. See Appendix F for quantitative evaluation. (e) Applying encoder propagation for non-uniform strategy. By benefiting from our propagation scheme, we are able to perform the decoder in parallel at certain adjacent time-steps.", "description": "This figure illustrates different sampling strategies in Stable Diffusion. (a) shows standard sampling, (b) the UNet architecture, (c) encoder propagation (reusing encoder features from previous steps), (d) decoder-only propagation (which fails to generate complete images), and (e) non-uniform encoder propagation (parallel processing for efficiency).", "section": "3 Method"}, {"figure_path": "ca2mABGV6p/figures/figures_5_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows the improvement in image generation speed achieved by the proposed method on various image generation tasks.  The table compares the original inference time of different diffusion models (Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, Custom Diffusion, VideoFusion, DiT, ControlNet) with their inference time after applying the proposed method. The percentage reduction in inference time is also indicated. The figure highlights the method's effectiveness across a variety of tasks and its significant speed improvement.", "section": "1 Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_6_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure showcases the speed improvements achieved by the proposed method across various image generation tasks.  It displays a table comparing the time taken for image generation using different diffusion models (Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, Custom Diffusion, VideoFusion, DiT, ControlNet) both with and without the proposed method. The significant reduction in generation time demonstrates the effectiveness of the approach in accelerating the inference process of diffusion models.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_8_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows a comparison of image generation times for various diffusion models, both with and without the proposed Faster Diffusion method.  It demonstrates significant speed improvements across different tasks such as Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, and custom diffusion models, as well as applications like VideoFusion and ControlNet. The speedup percentages are also indicated for each model.", "section": "1 Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_8_2.jpg", "caption": "Figure 9: Generating image with uniform and non-uniform encoder propagation. The result of uniform strategy II yields smooth and loses textual compared with SD. Both uniform strategy III and non-uniform strategy I, II and III generate images with unnatural saturation levels.", "description": "This figure shows the results of image generation using different encoder propagation strategies. The uniform strategies (I, II, and III) produce images with varying degrees of smoothness and detail loss. The non-uniform strategy (I, II, III, and IV (Ours)) shows an improved result compared to the uniform strategies, with the best result being from strategy IV (Ours).", "section": "4.4 Ablation study"}, {"figure_path": "ca2mABGV6p/figures/figures_16_1.jpg", "caption": "Figure 3: Analyzing the UNet in Diffusion Model. (a) Feature evolving across adjacent time-steps is measured by MSE. (b) We extract the hierarchical features output of different layers of the UNet at each time-step, average them along the channel dimension to obtain two-dimensional hierarchical features, and then calculate their Frobenius norm. (c) The hierarchical features of the UNet encoder show a lower standard deviation, while those of the decoder exhibit a higher standard deviation.", "description": "This figure analyzes the UNet architecture used in diffusion models. It shows that the encoder features change minimally across adjacent time steps, while decoder features show substantial variation. This observation supports the paper's method of omitting encoder computations at certain steps and reusing previous encoder features.", "section": "3.2 Analyzing the UNet in Diffusion Model"}, {"figure_path": "ca2mABGV6p/figures/figures_16_2.jpg", "caption": "Figure 11: DiT feature statistics (F-norm)", "description": "The figure visualizes the Frobenius norm (F-norm) of hierarchical features in the Diffusion Transformer (DiT) model across different time steps. It shows that the encoder features change minimally, whereas the decoder features exhibit substantial variations across different time steps. This observation supports the paper's claim that encoder features can be reused across multiple time steps to accelerate inference.", "section": "A Implementation Details"}, {"figure_path": "ca2mABGV6p/figures/figures_17_1.jpg", "caption": "Figure 7: Generated images at different time-steps.", "description": "This figure shows the generated images at different time steps during the diffusion process. It visually demonstrates how the image gradually becomes clearer and more refined as the number of steps increases, highlighting the iterative nature of diffusion models.  The figure likely accompanies results showing that the proposed method effectively preserves image quality even when reducing sampling steps, illustrating the method's ability to maintain high-fidelity image generation.", "section": "4 Experiments"}, {"figure_path": "ca2mABGV6p/figures/figures_20_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure showcases the speed improvements achieved by the proposed method across various image generation tasks.  It presents a comparison of the time taken for image generation (in seconds per image) between the original diffusion models (e.g., Stable Diffusion, DeepFloyd-IF, DDIM, DDPM) and the same models enhanced with the proposed Faster Diffusion technique. The results demonstrate significant speed improvements (e.g., 41% for Stable Diffusion, 24% for DeepFloyd-IF) across different methods and tasks, highlighting the effectiveness of Faster Diffusion in accelerating image generation.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_20_2.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows the results of applying the proposed Faster Diffusion method to various image generation tasks, including Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, and a custom diffusion model.  It demonstrates the significant speed improvements achieved by the method. For each task, the figure presents the original inference time and the inference time after applying the proposed method. The percentage reduction in inference time is also displayed. The tasks cover a range of complexity and model architectures, showing the broad applicability of Faster Diffusion.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_22_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows a comparison of image generation speeds for various diffusion models, both with and without the proposed method.  The models tested include Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, and a custom diffusion model. For each model, the image generation time is shown in seconds per image.  The improvement achieved by applying the proposed method (indicated by \"w/ Ours\") is also shown as a percentage reduction. The results demonstrate significant speed improvements across a variety of tasks.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_23_1.jpg", "caption": "Figure 4: (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. We omit the encoder at certain adjacent time-steps and reuse in parallel the encoder features in the previous time-steps for the decoder. Applying encoder propagation for uniform strategy every two iterations. Note, at time-step t-1, predicting noise does not require Zt-1 (i.e., Eq. 1: \\(\\frac{\\sqrt{a_{t-2}}}{\\sqrt{a_{t-1}}} = \\frac{\\sqrt{a_{t-2}}}{\\sqrt{a_{t-1}}} z_{t-1} + \\sqrt{a_{t-1} - a_{t-2}} \\epsilon_{t-1} \\)). (d) Decoder propagation. The generated images often fail to cover some specific objects in the text prompt. For example, given one prompt case \\\"A man with a beard wearing glasses and a beanie\\\". this method fails to generate the glasses subject. See Appendix F for quantitative evaluation. (e) Applying encoder propagation for non-uniform strategy.", "description": "This figure illustrates different sampling strategies in diffusion models. (a) shows standard sampling, (b) shows UNet architecture, (c) shows encoder propagation where encoder computations are omitted at certain steps and reused from previous steps, (d) shows decoder propagation where decoder is computed independently, and (e) shows a non-uniform encoder propagation strategy.", "section": "3 Method"}, {"figure_path": "ca2mABGV6p/figures/figures_24_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure showcases the speed improvements achieved by the proposed method across various image generation tasks.  It presents a comparison of inference times (in seconds per image) for several different diffusion models (Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, a custom diffusion model, and VideoFusion) both before and after applying the proposed acceleration technique.  The percentage reduction in inference time is also shown for each model. The tasks include text-to-image, text-to-video, and reference-guided generation, demonstrating the method's versatility and effectiveness across different applications.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_25_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows a comparison of image generation times for various diffusion models, both with and without the proposed Faster Diffusion method.  The models tested include Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, and a custom diffusion model. For each model, the image generation time in seconds per image is shown, along with the percentage reduction achieved by Faster Diffusion. The results demonstrate substantial speed improvements across various generation tasks.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_26_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows the results of applying the proposed faster diffusion method to various image generation tasks.  It demonstrates significant speed improvements across different diffusion models and generation types (Stable Diffusion, DeepFloyd-IF, DiT, VideoFusion, ControlNet) compared to their standard implementations.  The speedup is expressed as a percentage reduction in seconds per image.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_27_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure displays the results of applying the Faster Diffusion method to various image generation tasks, showcasing significant speed improvements across different models and tasks. The speed improvements are quantified in seconds per image, highlighting the method's effectiveness in accelerating image generation.  The tasks shown include Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, and custom diffusion, demonstrating the broad applicability of Faster Diffusion.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_27_2.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows the speed improvement achieved by the proposed method across various image generation tasks.  The results are presented as a table comparing the original inference time (in seconds per image) with the inference time after applying the proposed method. Significant speed improvements (in percentages) are shown for several popular diffusion models like Stable Diffusion, DeepFloyd-IF, and DiT, as well as for tasks such as text-to-video generation using VideoFusion, and reference-guided image generation with ControlNet.  The diverse range of models and tasks highlights the broad applicability of the proposed acceleration technique.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_28_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows the speedup achieved by the proposed method on various image generation tasks compared to existing methods.  It demonstrates the significant improvement in inference time across diverse models and applications, including Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, Custom Diffusion, and VideoFusion. The percentage improvements are also indicated.", "section": "1 Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_29_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows a comparison of the image generation speed achieved by various diffusion models (Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, Custom Diffusion, VideoFusion, DiT, ControlNet) with and without the proposed Faster Diffusion method.  For each model, the inference time in seconds per image is displayed, along with the percentage decrease in time achieved using Faster Diffusion.  The results demonstrate that the method significantly speeds up the image generation process across a variety of generation tasks.", "section": "Introduction"}, {"figure_path": "ca2mABGV6p/figures/figures_30_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows the results of applying the proposed Faster Diffusion method to several different image generation tasks.  The left side shows various baselines (e.g., Stable Diffusion, DeepFloyd-IF, DDIM, etc.) and their inference times. The right side displays the inference time after applying the Faster Diffusion technique to these same models, with a percentage reduction in inference time. The tasks used in the experiments are diverse and include text-to-image, video generation, and others. The results demonstrate the significant speedup achieved by the Faster Diffusion method across a wide range of diffusion models and generation tasks.", "section": "Abstract"}, {"figure_path": "ca2mABGV6p/figures/figures_31_1.jpg", "caption": "Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).", "description": "This figure shows a comparison of image generation speed using various diffusion models, both with and without the proposed Faster Diffusion method.  The models include Stable Diffusion, DeepFloyd-IF, DDIM, DPM-Solver++, and a custom diffusion model, showcasing the significant speed improvements achieved by Faster Diffusion across different generation tasks, from simple image generation to more complex tasks like VideoFusion and ControlNet.  Each model's speed is measured in seconds per image, and percentage improvements over the original models are indicated.", "section": "Introduction"}]