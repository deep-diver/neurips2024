[{"type": "text", "text": "Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rayna Andreeva\u22171, Benjamin Dupuis\u22172,3, Rik Sarkar1, Tolga Birdal\u20204, Umut S\u00b8im\u00b8sekli\u20202,3 ", "page_idx": 0}, {"type": "text", "text": "1 School of Informatics, University of Edinburgh, UK 2 INRIA, France 3 CNRS, Ecole Normale Sup\u00e9rieure PSL Research University, France 4 Department of Computing, Imperial College London, UK ", "page_idx": 0}, {"type": "text", "text": "\u2217\u2020 indicate equal contributions. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a novel set of rigorous and computationally efficient topology-based complexity notions that exhibit a strong correlation with the generalization gap in modern deep neural networks (DNNs). DNNs show remarkable generalization properties, yet the source of these capabilities remains elusive, defying the established statistical learning theory. Recent studies have revealed that properties of training trajectories can be indicative of generalization. Building on this insight, state-of-the-art methods have leveraged the topology of these trajectories, particularly their fractal dimension, to quantify generalization. Most existing works compute this quantity by assuming continuous- or infinite-time training dynamics, complicating the development of practical estimators capable of accurately predicting generalization without access to test data. In this paper, we respect the discrete-time nature of training trajectories and investigate the underlying topological quantities that can be amenable to topological data analysis tools. This leads to a new family of reliable topological complexity measures that provably bound the generalization error, eliminating the need for restrictive geometric assumptions. These measures are computationally friendly, enabling us to propose simple yet effective algorithms for computing generalization indices. Moreover, our flexible framework can be extended to different domains, tasks, and architectures. Our experimental results demonstrate that our new complexity measures correlate highly with generalization error in industry-standards architectures such as transformers and deep graph networks. Our approach consistently outperforms existing topological bounds across a wide range of datasets, models, and optimizers, highlighting the practical relevance and effectiveness of our complexity measures. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generalization, a hallmark of model efficacy, is one of the most fundamental attributes for certifying any machine learning model. Modern deep neural networks (DNN) display remarkable generalization abilities that defy the current wisdom of machine learning (ML) theory [85, 86]. The notion can be formalized through the risk minimization problem, which consists of minimizing the function: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{R}(w):=\\mathbb{E}_{z\\sim\\mu_{z}}\\left[\\ell(w,z)\\right],\n$$", "text_format": "latex", "page_idx": 0}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/d9b663c3d2a8058dc1c07f36013ae5e6e9207b0a1b754f760b38d0f2f3aa64ea.jpg", "img_caption": ["Figure 1: We devise a novel class of complexity measures that capture the topological properties of discrete training trajectories. These generalization bounds correlate highly with the test performance for a variety of deep networks, data domains and datasets. Figure shows different trajectories (a) embedded using multi-dimensional scaling based on the distance-matrices (b) computed using either the Euclidean distance $(\\parallel\\cdot\\parallel_{2})$ between weights as in [10] or via the loss-induced pseudo-metric $(\\rho_{S})$ as in [21]. (c) plots the average granulated Kendall coefficients for two of our generalization measures ( $E_{\\alpha}$ and $\\mathbf{PMag}({\\sqrt{n}}))$ in comparison to the state-of-the-art persistent homology dimensions [10, 21] for a range of models, datasets, and domains, revealing significant gains and practical relevance. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "where $z\\in{\\mathcal{Z}}:=\\mathcal{X}\\times\\mathcal{Y}$ denotes the data, distributed according to a probability distribution $\\mu_{z}$ on the data space $\\mathcal{Z}$ . In practice, as $\\mu_{z}$ is unknown, ML algorithms focus on minimizing the empirical risk, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{R}}_{S}(w)=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(w,z_{i}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $S:=(z_{1},\\ldots,z_{n})\\sim\\mu_{z}^{\\otimes n}:=\\mu_{z}\\otimes\\cdots\\otimes\\mu_{z}.$ , which means that $\\left(z_{1},\\ldots,z_{n}\\right)$ are independent samples from $\\mu_{z}$ . In many applications, the minimization of (2) is achieved by discrete stochastic optimization algorithms, such as stochastic gradient descent (SGD) or the ADAM [40] method. Such algorithms generate a sequence of iterates in $\\mathbb{R}^{d}$ , denoted $\\mathcal{W}_{S}:=\\left\\{w_{k}\\right\\}_{k\\geq0}$ , which depends on the data $S$ , the initialization $w_{0}\\in\\mathbb{R}^{d}$ , and some additional randomness $U$ , e.g., the random batch indices in SGD. The generalization error characterizing the model\u2019s performance is then defined as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nG_{S}(w_{k}):=\\mathcal{R}(w_{k})-\\widehat{\\mathcal{R}}_{S}(w_{k}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The empirical risk (2) typically has numerous local minima, which raises the question of how to characterize their generalization properties. Recently, training trajectories ( $c f.$ , Figure 1a) have been shown to be paramount to answer this question [84, 28]. Indeed, these trajectories can quantify the quality of a local minimum in a compact way, because they depend simultaneously on the algorithm, the hyperparameters, and the data, which is crucial for obtaining satisfactory bounds [37]. A wide family of trajectory-dependent bounds has been developed [84, 28, 50, 4, 36]. For instance, several results on stochastic gradient Langevin dynamics [57, 64, 49], continuous Langevin dynamics [57] and SGD [59] take into account the impact of the whole trajectory on the generalization error. ", "page_idx": 1}, {"type": "text", "text": "Parallel to these developments, several studies have brought to light the empirical links between topological properties of DNNs and their generalization performance [58, 52, 66, 70, 83], hereby making new connections with topological data analysis (TDA) tools [2]. These studies focus on the structural changes across the different layers of the network [51] or on the final trained network [66, 70, 83], and are almost exclusively empirical. This partially inspired a new class of trajectorydependent bounds focusing on topological properties of the trajectories. In particular, recent studies [78, 21, 35, 22, 10, 3] have proposed to relate the generalization error to various kinds of intrinsic fractal dimensions [26, 53] that characterize the learning trajectory. Informally, these bounds provide the guarantee that with probability at least $1-\\zeta$ , we have:1 ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathcal{W}_{S}}G_{S}(w)\\lesssim\\sqrt{\\frac{\\dim(\\mathcal{W}_{S})+\\operatorname{IT}+\\log(1/\\zeta)}{n}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathrm{dim}(\\mathcal{W}_{S})$ denotes various equivalent fractal dimensions, in particular the persistent homology dimension (PH-dim) [10, 21] and the magnitude dimension [3]. The term IT is an informationtheoretic quantity that takes different forms among different studies. Despite providing rigorous links between the topology of the trajectory and generalization, these bounds have major drawbacks. First and foremost, as noted in [75, 76, 13], fractal-trajectory bounds, such as Equation (4), do not apply to discrete-time algorithms. This creates a discrepancy between these theoretical results and the TDAinspired methods to numerically evaluate them on commonly used discrete algorithms [10, 21, 3]. Additionally, existing bounds rely on intricate geometric assumptions, such as Ahlfors-regularity [78, 35] or geometric stability [21], that are not realistic in a practical, discrete setting. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Previous attempts were made to address this discretization issue. Specifically, under the assumption that the training dynamics possess a stationary measure $\\mu_{w\\mid S}^{\\infty}$ for $T\\,\\rightarrow\\,\\infty$ ( $T$ is the number of iterations), it was shown in [13] that with probability $1-\\zeta$ over $S\\sim\\mu_{z}^{\\otimes n}$ and $w\\sim\\mu_{w\\mid S}^{\\infty}$ , we have: ", "page_idx": 2}, {"type": "equation", "text": "$$\nG_{S}(w)\\lesssim\\sqrt{\\frac{\\dim(\\mu_{w|S})+\\operatorname{IT}+\\log(1/\\zeta)}{n}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathrm{dim}(\\mu_{w|S})$ corresponds to the fractal dimension of the measure $\\mu_{w}$ (see [67] for formal definitions). While this was an important step, this bound only becomes practically relevant when the number of iterations grows to infinity, which is never attained in real-life experiments. Other attempts make use of so-called finite fractal dimensions [71] or fine properties of the Markov transition kernels associated with the dynamics [35]. However, these studies also rely on impractical assumptions and involve intricate quantities which make them not amenable to numerical evaluation. ", "page_idx": 2}, {"type": "text", "text": "Despite the theoretical limitations of existing topology-dependent generalization bounds, TDAinspired tools have been developed to numerically estimate the proposed intrinsic dimensions in practical settings. Two particular methods have emerged and successfully demonstrate correlation with the generalization error, based on persistent homology [10, 21] (PH-dim) and metric space magnitude [3] (magnitude dimension); these two dimensions are equivalent for compact metric spaces [3]. Because of the limitations discussed above, existing theories do not account for these experiments, conducted with finite-time discrete algorithms. Moreover, existing empirical studies [10, 21, 3, 78] only consider very simple models and small (image) datasets. Because of their lack of theoretical foundations, it is not clear whether they could be extended to more practical setups. ", "page_idx": 2}, {"type": "text", "text": "Contributions. In this paper, we investigate the building blocks of PH and magnitude dimensions, in order to propose new topology-inspired generalization bounds that rigorously apply to widely used discrete-time stochastic optimization algorithms, and experimentally test our new topological complexities2 on practically relevant DNN architectures. Our detailed contributions are as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We start by establishing the first theoretical links between generalization and a new kind of computationally thrifty topological complexity measure, the $\\alpha$ -weighted lifetime sums [73, 74]. \u2022 We propose and elaborate on another novel topological complexity, positive magnitude (PMag), a slightly modified version of magnitude [46, 55]. We rigorously link PMag with the generalization error, by relying on a new proof technique. Overall, our generalization bounds, rooted in TDA, admit the following generic form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathcal{W}_{S}}G_{S}(w)\\lesssim\\sqrt{\\frac{(\\mathrm{Topological\\;complexity})+\\mathrm{IT}+\\log(1/\\zeta)}{n}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "\u2022 We then provide a flexible computational implementation based upon dissimilarity measures between neural nets (Figure 1b), which enables quantifying generalization across different architectures and models, without the need for domain or problem-specific analysis as done in [39, 8]. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Unlike existing trajectory-based studies [10, 21] operating on small models, our experimental evaluation is extensive. We consider several vision transformers [20] and graph neural networks (GNN) [30] trained on multiple datasets spanning regular and irregular data domains (cf. Figure 1c). Our results demonstrate that the novel measures we introduce correlate strongly with the test performance across different architectures, hyperparameters and data modalities. ", "page_idx": 2}, {"type": "text", "text": "All the proofs of the main results are presented in the appendix, along with additional experiments. We will make our entire implementation publicly available under: https://github.com/rorondre/TDAGeneralization. ", "page_idx": 2}, {"type": "text", "text": "2 Technical Background ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our generalization indicators will be based upon $\\alpha$ -weighted lifetime sums and magnitude, capturing different topological features, as we shortly dicsuss below. Let $(X,\\rho)$ be a finite pseudometric space. ", "page_idx": 3}, {"type": "text", "text": "$\\alpha$ -weighted lifetime sums. Persistent homology (PH) is an important concept in the analysis of geometric complexes [11]. We focus on the persistent homology of degree 0 $\\bar{(\\mathrm{PH}^{0})}$ . Informally, it consists in tracking the \u201cconnected components\u201d of a finite set at different scales. We provide in Sections A.3 and A.4 exact definitions of this notion. For simplicity, we present here an equivalent formulation of the $\\alpha$ -weighted lifetime sums based on minimum spanning trees (MST) [42, 73]. ", "page_idx": 3}, {"type": "text", "text": "A tree over $X$ is a connected acyclic undirected graph (a set of edges) whose vertices are the points in $X$ . Given an edge $e$ linking the points $a$ and $b$ , we define its cost as $|e|:=\\rho(a,b)$ . An MST $\\tau$ on $X$ is a tree minimizing the total cost $\\textstyle\\sum_{e\\in{\\mathcal{T}}}|e|$ . The $\\alpha$ -weighted lifetime sums $E_{\\alpha}^{\\rho}$ are then written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\alpha\\geq0,\\;{\\pmb E}_{\\alpha}^{\\rho}(X):=\\sum_{e\\in\\mathcal{T}}|e|^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The celebrated persistent homology dimension (PH-dim) [1], of a compact pseudometric space $(A,\\rho)$ is then defined as $\\dim_{\\operatorname{PH}}^{\\rho}(A)=\\operatorname*{inf}_{\\alpha\\geq0}\\left\\{\\exists C>0,\\forall Y\\subset A\\right.$ finite, $E_{\\alpha}(Y)\\leq C\\}$ . The PH-dim has been proven to be related to generalization error for different pseudometrics $\\rho$ [10, 21]. ", "page_idx": 3}, {"type": "text", "text": "Magnitude. Magnitude is a recently introduced topological invariant [46] which encodes many important invariants from geometric measure theory and integral geometry [46, 55, 56]. Magnitude can be interpreted as the effective number of distinct points in a space [46]. For $s>0$ , we define a weighting of the modified space $(X,s\\rho)$ as a map $\\beta:X\\rightarrow\\mathbb{R}$ , such that $\\begin{array}{r}{\\mathrm{\\boldmath{~\\sigma~}}_{\\mathrm{\\boldmath{~\\sf~\\rho~}}},\\;\\in X,\\;\\sum_{b\\in X}e^{-s\\rho(a,b)}\\beta(\\bar{b})=\\bar{1}}\\end{array}$ Given such a weighting $\\beta$ , the magnitude function of $(X,s\\rho)$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Mag}^{\\rho}(s X):=\\sum_{a\\in X}\\beta(a).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The parameter $s>0$ should be interpreted as a \u201cscale\u201d through which we look at the set $(X,\\rho)$ . We present in Appendix A.5 additional properties of this function. Note that magnitude is usually defined in metric spaces; we show in Appendix B.2 that we can seamlessly extend it to the pseudometric setting. Magnitude can be extended to (infinite) compact spaces [46, 55] and, as for PH, an intrinsic dimension, the magnitude dimension, can be defined from magnitude by the formula $\\mathrm{dim}_{\\mathrm{Mag}}^{\\rho}(A)=$ $\\begin{array}{r}{\\operatorname*{lim}_{s\\to\\infty}{\\frac{\\log\\mathbf{M}\\mathbf{ag}(s A)}{\\log(s)}}}\\end{array}$ log lMoga(gs()sA). It is known that dim\u03c1PH and dim\u03c1Mag coincide for compact metric spaces [56, 73, 3]. As a result, $\\dim_{\\mathrm{Mag}}^{\\rho}$ has also been proposed as a topological generalization indicator [3]. ", "page_idx": 3}, {"type": "text", "text": "Total mutual information. Prior intrinsic dimension-based studies relied on \u201cmixing\u201d assumptions ([78, Assumption H5], [10, Assumption H1], [76, 13]) or various mutual information terms [35, 21] to take into account the statistical dependence between the data and the training trajectory. Recently, a new framework was proposed in [22] to unify these approaches by proving data-dependent uniform generalization bounds using simpler and smaller information-theoretic (IT) terms. By leveraging these methods, we derive new generalization bounds involving the same IT terms for all our introduced topological complexities. More precisely, they take the form of a total mutual information between the data $S$ and the training trajectory $w_{S}$ . This term is denoted $\\mathbf{I}_{\\infty}(S,\\mathcal{W}_{S})$ and measures the dependence between $S$ and $\\mathcal{W}$ . We refer to Appendix A.1 and [35, 81] for exact definitions. ", "page_idx": 3}, {"type": "text", "text": "3 Main Theoretical Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now introduce our learning-theoretic setup (section 3.1) before delving into our main theoretical results in Sections 3.2 and 3.3. ", "page_idx": 3}, {"type": "text", "text": "3.1 Mathematical setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Random trajectories. The primary goal of our theory is to prove uniform3 generalization bounds over the training trajectory $\\{\\bar{w}_{k},\\;k\\stackrel{!}{\\geq}\\bar{0}\\}$ . We are mostly interested in the behavior near local minima of $\\widehat{\\mathcal{R}}_{S}$ . To this end, we observe the trajectory between iterations $t_{0}$ and $T$ , where $t_{0}\\in\\mathbb{N}$ is the number of  iterations before reaching (near) a local minimum and $T\\geq t_{0}$ is the total number of iterations. ", "page_idx": 3}, {"type": "text", "text": "Therefore, we consider the set $\\mathcal{W}_{t_{0}\\rightarrow T}:=\\{w_{i},\\;t_{0}\\leq i\\leq T\\}$ , which we call the random trajectory. Note that $\\mathcal{W}_{t_{0}\\rightarrow T}$ is a set, i.e., it does not contain any information about the time-dependence. Moreover, our setup allows the random times $t_{0}$ and $T$ to depend on the data $S$ through the choice of a stopping criterion as opposed to being fixed predetermined times. ", "page_idx": 4}, {"type": "text", "text": "General Lipschitz conditions. The topological quantities described in section 2, as well as the intrinsic dimensions introduced in prior works [78, 10, 3, 21, 22], require a notion of distance between parameters (in $\\mathbb{R}^{d}$ ) to be computed. In the case of fractal-based generalization bounds, two cases have already been considered: the Euclidean distance [78] and the data-dependent pseudometric defined in [21]. In our work, we emphasize that both examples are particular cases of a more general family of pseudometrics on the parameter space $\\mathbb{R}^{d}$ . In order to fully characterize this family of pseudometrics, we define the data-dependent map $L_{S}:\\mathbb{R}^{d}\\longrightarrow\\mathbb{R}^{n}$ by $L_{S}(w)=(\\ell(w,z_{1}),\\ldots,\\ell(\\bar{w},z_{n}))$ . To fit into our framework, a pseudometric must satisfy the following general Lipschitz condition. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 $((q,L,\\rho)$ -Lipschitz continuity). For any pseudo-metric $\\rho$ on $\\mathbb{R}^{d}$ and $q\\geq1$ , we will say that $\\ell$ is $(q,L,\\rho)$ -Lipschitz in $w$ when $\\forall w,w^{\\prime}\\in\\mathbb{R}^{d}$ , $\\|L_{S}(w)-L_{S}(w^{\\prime})\\|_{q}\\leq L n^{1/q}\\rho(w,w^{\\prime})$ . A wide variety of distances have been proposed to compare the weights of two DNNs [19]. The above condition restricts our analysis to a family of pseudometrics containing the following examples. Example 3.2 (Data-dependent pseudometrics). For any $p~\\geq~1$ , we define the pseudometrics $\\rho_{S}^{(p)}(w,w^{\\prime})\\;:=\\;n^{-1/p}\\left\\|L_{S}(w)-L_{S}(w^{\\prime})\\right\\|_{p}$ . The case $\\rho_{S}^{(1)}$ corresponds to the \u201cdata-dependent pseudometric\u201d used in [21]; we will denote it $\\rho_{S}:=\\rho_{S}^{(1)}$ . Example 3.3 (Euclidean distance). If $\\ell(w,z)$ is $L$ -Lipschitz continuous in $w$ , i.e., $|\\ell(w,z)-$ $\\ell(w^{\\prime},z)|\\leq L\\|w-w^{\\prime}\\|$ for all $z$ , then $\\ell$ is $\\left(p,L,\\|\\cdot\\|_{2}\\right)$ -Lipschitz continuous for every $p\\geq1$ . ", "page_idx": 4}, {"type": "text", "text": "Assumptions. Given an $(q,L,\\rho)$ -Lipschitz continuous (pseudo-)metric, our approach relies only on a single assumption of a bounded loss function. For the case of the pseudometric $\\rho_{S}$ (Example 3.2), this assumption is already made in [21, 22]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. We assume that the loss $\\ell$ is bounded in $[0,B]$ , with $B>0$ a constant. ", "page_idx": 4}, {"type": "text", "text": "The boundedness of $\\ell$ is classically assumed in the fractal / TDA literature [21, 35, 22]. In particular, this assumption is valid for the usual $0-1$ loss. In [21], it is shown that the proposed theory seems to be experimentally valid even for unbounded losses. Our experimental findings suggest that this observation also applies to our work. ", "page_idx": 4}, {"type": "text", "text": "3.2 Persistent homology related generalization bounds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In contrast to all existing fractal dimension-based bounds [78, 10, 13, 21], we propose new generalization bounds that apply to practical discrete stochastic optimizers with a finite number of iterations. To this end, our key idea involves replacing the intrinsic dimension with intermediary quantities that are used to compute them numerically. Following [10, 3], this points us towards the two quantities, $E_{\\alpha}$ and Mag, defined in section 2. We are now ready to state the first generalization bound in terms of the $\\alpha$ -weighted lifetime sums, where we denote $E_{\\alpha}^{\\rho}$ for $E_{\\alpha}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. Let $\\rho$ be a pseudometric on $\\mathbb{R}^{d}$ . Supposes that Assumption $^{\\,l}$ holds and that $\\ell$ is $(q,L,\\rho)$ -Lipschitz, for $q\\geq1$ . Then, for all $\\alpha\\in[0,1]$ , with probability at least $1-\\zeta$ , we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\le i\\le T}G_{S}(w_{i})\\le2B\\sqrt{\\frac{2\\log{(1+K_{n,\\alpha}{\\bf E}_{\\alpha}^{\\rho})}}{n}}+\\frac{2B}{\\sqrt{n}}+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\to T})+\\log(1/\\zeta)}{2n}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $K_{n,\\alpha}:=2\\left(2L\\sqrt{n}/B\\right)^{\\alpha}$ . ", "page_idx": 4}, {"type": "text", "text": "The term $\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\to T})$ is the total mutual information (MI) term that is defined in Sections 2 and A.1. It measures the statistical dependence between the random set $\\mathcal{W}_{t_{0}\\rightarrow T}$ and the data $S\\sim\\mu_{z}^{\\otimes n}$ . Such MI terms appear in previous works related to fractal-based generalization bounds [78, 13, 21, 35]. Our proof technique, presented in Appendix B.5, makes use of a recently introduced PAC-Bayesian framework for random sets [22] to introduce this MI term. It is also shown in [22] that the MI term $\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\to T})$ is tighter than those appearing in the aforementioned works. ", "page_idx": 4}, {"type": "text", "text": "We highlight the fact that Theorem 3.4 is fundamentally different from the persistent homology dimension (PH-dim) based bounds studied in [10, 21]. Indeed, while the growth of $E_{\\alpha}$ for increasing ", "page_idx": 4}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/f499c857fea93d52af2d44e8e68af70f35d3bb03d0bc262bdb2e137a955f76e2.jpg", "img_caption": ["(a) Comparison of Mag and PMag. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/b26bd5c5925268a7f4dba29039f989a82ae4cfbd1b8fb650244298ff3df61e5e.jpg", "img_caption": ["(b) Relative variation of $E_{1}$ and Mag. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Left: Comparison of Mag and PMag (for $s=\\sqrt{n})$ , for different (pseudo)metrics (ViT on CIFAR10). Right: relative variation of the quantities ${\\cal E}_{\\alpha}(\\mathcal{W}_{t_{0}\\to T})$ and $\\mathbf{M}\\bar{\\mathbf{ag}}(\\sqrt{n}\\mathcal{W}_{t_{0}\\to T})$ , with respect to the proportion of the data used to estimated $\\rho_{S}^{(1)}$ (ViT on CIFAR10). ", "page_idx": 5}, {"type": "text", "text": "finite subsets of the trajectory are used in [10] to estimate the PH-dim, it does not provide any formal link between the generalization error and the value of $E_{\\alpha}$ . Therefore, the above theorem could not be cast as a corollary of these previous studies. Another important characteristic of the above theorem (as well as the results of section 3.3) is to be non-asymptotic, i.e., it is true for every $n\\in\\mathbb{N}^{*}$ . This is an improvement over the fractal dimensions-based bounds presented in [78, 10, 21, 22]. ", "page_idx": 5}, {"type": "text", "text": "3.3 Positive magnitude (PMag) and related generalization bounds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recent preliminary experimental results displayed a correlation between the generalization error of DNNs and magnitude [3]. To provide a theoretical justification for this behavior, it would be tempting to mimic the proof of Theorem 3.4 and build on existing covering arguments. However, while lower bounds of magnitude in terms of covering numbers have been derived in [56], they appear to be impractical in our case. Another possibility would be to use the magnitude dimension bounds of [3]. Yet, this could not apply to our finite and discrete setting where the dimension is 0. Hence, we identify a new quantity, closely related to magnitude, while being more relevant to learning theory. With the notations of section 2, we fix a finite metric space $(X,\\rho)$ and a weighting $\\beta_{s}:X\\longrightarrow\\mathbb{R}$ of $(X,s\\rho)$ , where $s>0$ is a \u201cscale\u201d parameter. We define the positive magnitude as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall s>0,\\;\\mathbf{PMag}^{\\rho}(s X):=\\sum_{a\\in X}\\beta_{s}(a)_{+},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{+}:=\\operatorname*{max}(x,0)$ denotes the positive part of $x$ . To avoid harming the readability of the paper, we refer to Appendix B.3 for the extension of PMag to the pseudometric case. Based on a new theoretical approach, we prove that the positive magnitude can be used to upper bound the generalization error (see the proof in Appendix B.7). This leads to the following theorem: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5. Let $\\rho$ be a pseudometric such that $(\\boldsymbol{\\mathscr W},\\lambda\\rho)$ admits a positive magnitude (according to Definition B.5) for every $\\lambda>0$ . We assume that $\\ell$ is $(q,L,\\rho)$ -Lipschitz continuous with $q\\geq1$ . Then, for any $s>0$ , we have with probability at least $1-\\zeta$ that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\leq i\\leq T}G_{S}(w_{i})\\leq\\frac{2}{s}\\log\\mathbf{PMag}^{\\rho}\\left(L s\\mathcal{W}_{t_{0}\\rightarrow T}\\right)+s\\frac{B^{2}}{n}+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}\\big(S,\\mathcal{W}_{t_{0}\\rightarrow T}\\big)+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We now present a quick sketch of the proof of Theorem 3.5, in order to highlight its key elements. ", "page_idx": 5}, {"type": "text", "text": "Proof. (Sketch) Let $\\mathcal{W}$ be a data-dependent random compact set (e.g., $\\mathcal{W}_{t_{0}\\rightarrow T}$ ). The proof is based on two technical elements. The first is a framework recently proposed in [22] for uniform generalization bounds for random sets. These results give that with high probability we have a bound of the form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathcal{W}}G_{S}(w)\\lesssim\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\rightarrow T})+\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\mathrm{log}(1/\\zeta)}{n}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\to T})$ is the celebrated Rademacher complexity [5], whose definition is given in Appendix A.2. The second technical element is a new link between the Rademacher complexity of a compact set and its positive magnitude. This result is discussed in Appendix B.7. \u53e3 ", "page_idx": 5}, {"type": "table", "img_path": "6U5fCHIWOC/tmp/8d4abe013b9af2b0362db6f4379a21839aa5bdec254ac8b35f2f24a5c259a420.jpg", "table_caption": ["Table 1: Correlation coefficients associated with the different topological complexities. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The IT term $(I_{\\infty})$ in the above result is the same as in Theorem 3.4. Given a fixed (finite) set $\\mathcal{W}$ and a big enough $s$ , we establish $\\mathbf{Mag}(s\\mathcal{W})=\\mathbf{PMag}(s\\mathcal{W})$ . Moreover, we present in Figure 2a an empirical comparison of $\\mathbf{Mag}$ and PMag, showing a small and almost monotonic relation between both quantities. Therefore, Theorem 3.5 may be seen as the first theoretical justification of the empirical relationship between magnitude and the generalization error observed in [3]. ", "page_idx": 6}, {"type": "text", "text": "A natural choice for the scale $s$ would be $s\\approx\\sqrt{n}$ , ensuring a convergence rate in $n^{-1/2}$ . However, our empirical evaluations (see section 5, in particular, Table 1) revealed that small values of $s$ (we typically use $s=10^{-2}$ ) can also provide good correlation with the generalization error. This could be explained by the fact that $\\mathbf{PMag}(s\\mathcal{W})\\to1$ as $s\\rightarrow0$ , i.e., the bound may not diverge when $s\\to0$ . For our topological complexities to be computationally efficient, we focus our experiments on fixed values of $s$ (in $\\{\\sqrt{n},10^{\\underline{{\\iota}}-2}\\})$ ). We further analyze the sensitivity of part of our experiments to the value of $s$ in Appendix D.2.2. We will omit the trajectory and denote $\\mathbf{Mag}(s)$ and $\\mathbf{PMag}(s)$ . Remark 3.6. As it is explained in Appendix B.7, a key element in the proof of Theorem 3.5 is a newly discovered link between the celebrated Rademacher complexity [5] and positive magnitude. This is an additional contribution of our work, which might be of independent interest. Moreover, this relation extends beyond the case of finite sets and applies in particular to compact trajectories (or hypothesis sets) $\\mathcal{W}$ . We refer the reader to Remark B.15 and lemma B.16 for more details. ", "page_idx": 6}, {"type": "text", "text": "4 Computational Considerations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now detail the numerical estimation of the topological complexities mentioned above. ", "page_idx": 6}, {"type": "text", "text": "Computation of $E_{\\alpha}$ . We compute $E_{\\alpha}$ by using the giotto-ph library introduced in [65, 7]. This setup is inspired by PH frameworks used in [10, 21]. This technique uses the equivalent formulation of $E_{\\alpha}$ in terms of PH (see Appendix A.3 for details). Theorem 3.4, and its proof (presented in Appendix B.6) suggest that the relevant value of $\\alpha$ is 1; similar to [10], this is what we used in our experiments. ", "page_idx": 6}, {"type": "text", "text": "Computation of Mag and PMag. Different methods exist to evaluate magnitude [47]. We use the Krylov approximation method [72], which is based on pre-conditioned conjugate gradient iteration, implemented in the Python library krypy.linsys. $\\mathtt{C g}$ to solve for the magnitude weights. We then sum over the weights to compute Mag, and sum over the positive weights to obtain PMag. ", "page_idx": 6}, {"type": "text", "text": "Distance matrix estimation.. Given a finite set (i.e., a trajectory) $\\mathcal{W}\\subset\\mathbb{R}^{d}$ , the calculation of our topological complexities requires computing the distance matrix $\\bar{D}_{\\rho}:=(\\rho(w,w^{\\prime}))_{w,w^{\\prime}\\in\\mathcal{W}}$ . For large DNNs, this may become challenging. Depending on $\\rho$ , we propose the following solutions. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Case 1: If $\\rho$ is the Euclidean distance, for large DNNs (in our case for the transformer experiments) storing the whole trajectory is challenging. In that case, we use sparse random projections inspired by the Johnson-Lindenstrauss lemma [82] to project the trajectories onto a lower-dimensional subspace. We use the implementation in scikit-learn [63] so that, with high probability, the relative variation of the distance matrices is at most $5\\%$ , see Appendix A.7 for details. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u2022 Case 2: If $\\rho$ is of the form $\\rho_{S}^{(q)}$ as in Example 3.2, then the computation of $D_{\\rho}$ requires the evaluation of the model on the entire dataset at each iteration, which becomes intractable for large DNNs. In [21, Figure 3], the authors show that the PH-dim based on the pseudometric $\\rho_{S}=\\rho_{S}^{(1)}$ is very robust to a random subsampling of a training dataset, i.e. when $\\rho_{S}$ is replaced by $\\rho_{B}$ with $B\\subseteq S$ and $|B|/|S|\\ll1$ . Figure 2b shows that $E_{\\alpha}$ and positive magnitude are also robust to this subsampling. We mainly used $|B|/|S|=10\\%$ . We refer the reader to Appendix C.2 for details. ", "page_idx": 7}, {"type": "text", "text": "Generalization error. Our theory, like many trajectory-based studies [78, 10, 21, 3] predicts upper bounds on the worst-case generalization error over the trajectory $\\mathcal{W}_{t_{0}\\rightarrow T}$ . Yet, experiments in previous works mainly reported the error at the last iteration. To estimate the worst-case error in a computationally feasible way, we periodically evaluated the test risk between times $t_{0}$ and $T$ (every 100 iterations) and reported (worst test risk - final train risk) as the error in our experiments. This is consistent as we start the trajectory $\\mathcal{W}_{t_{0}\\rightarrow T}$ from a weight $w_{t_{0}}$ already in a local minimum. Our main conclusions are still valid if the final generalization gap is used. This observation, which is to the best of our knowledge new, is briefly discussed in Appendix D.2.1. ", "page_idx": 7}, {"type": "text", "text": "5 Empirical Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In what follows, we study our bounds on a variety of datasets and model architectures. We first explain the setup and the evaluation metrics before delving into the results and analysis. ", "page_idx": 7}, {"type": "text", "text": "Setup. Given a DNN and a dataset, we start from a pre-trained weight vector $w_{t_{0}}$ , yielding high training accuracy on classification tasks. By varying the learning rate $(\\eta)$ and the batch size $(b)$ , we define a grid of $6\\times6$ hyperparameters. For each pair $(\\eta,b)$ , we compute the training trajectory $\\mathcal{W}_{t_{0}\\rightarrow T}$ for $5\\times10^{3}$ iterations. Unless specified, we use the ADAM optimizer [40]. Based on $\\mathcal{W}_{t_{0}\\rightarrow T}$ , we estimate distance matrices as described in section 4. For the sake of clarity, we focus on 3 relevant pseudometrics: (i) the Euclidean distance $\\lVert\\cdot\\rVert_{2}$ as in [10], (ii) the data-dependent pseudometric $\\rho_{S}$ , used in [21, 3], and (iii) the 01-loss distance. For (ii), $\\rho_{S}$ is computed based on the surrogate loss used in training (e.g., the cross-entropy loss), while the reported generalization error is always based on accuracy gap (01-loss), which is of interest in most applications (see section 4). For the last one (iii) $\\rho$ is defined as in Example 3.2, but with $\\ell$ being the 01-loss; we call it 01-pseudometric and denote it by 01 in the tables. This last setup matches exactly our theoretical requirements. ", "page_idx": 7}, {"type": "text", "text": "In terms of DNN architectures, we focus on practically relevant models, while previous studies mainly considered small networks [10, 35, 21, 76]. We examine two different families of architectures. The first family consists of vision transformers (ViT [79], CaiT [80], Swin [48], see Table 2), each evaluated on both the CIFAR10 [44] and CIFAR100 [43] datasets. Moreover, we also tested our theory on graph neural networks (GNN) architectures, namely GatedGCN [12] and GraphSage [32] trained on the Super-pixel MNIST dataset [23]. To the best of our knowledge, this is the first time these kinds of topological complexities have been evaluated on transformers and GNNs. We ran the experiments on 18 NVIDIA 2080Ti (11 GB) GPUs. ", "page_idx": 7}, {"type": "text", "text": "Granulated Kendall\u2019s coefficients.. We assess the correlation between our complexities and the generalization error by using the granulated Kendall\u2019s coefficients (GKC) [37]. While the classical Kendall\u2019s coefficients (KC) [38] (denoted $\\tau$ ) measures the correlation between two quantities, it may fail to capture their causal relationship. Instead, one \u201cgranulated\u201d coefficient is defined in [37] for each hyperparameter (i.e., $\\psi_{\\mathrm{LR}}$ for $\\eta$ and $\\psi_{\\mathrm{{BS}}}$ for $b$ ); it measures the correlation when only this hyperparameter is varying. In Table 1, we report $\\tau$ , $\\psi_{\\mathrm{LR}}$ and $\\psi_{\\mathrm{{BS}}}$ , and the averaged GKC, $\\Psi:=(\\psi_{\\mathrm{LR}}+\\psi_{\\mathrm{BS}})/2$ , for several models, datasets and topological complexities. In Figures 4a and 4b, we represent our topological complexities in the plane $(\\psi_{\\mathrm{BS}},\\psi_{\\mathrm{LR}})$ ; the red square indicates the region of best correlation (the coefficients are in $[-1,1]$ , their sign is the sign of the correlation). It should be noted that a scaling of this constant $B$ , coming from Assumption 1, would not impact the correlation between generalization and topological complexities that is observed in our experiments. ", "page_idx": 7}, {"type": "text", "text": "5.1 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As explained above, we focus our main experiments on the quantities $E_{1}$ , $\\mathbf{Mag}({\\sqrt{n}})$ , $\\mathbf{PMag}({\\sqrt{n}})$ , $\\mathbf{Mag}(10^{-2})$ and $\\mathbf{PMag}(10^{-2})$ , each computed for the 3 pseudometrics discussed above $(\\|\\cdot\\|_{2},\\rho_{S}$ , 01). In the interest of comparison, we also compute the PH-dim (proposed in [10] for the $\\lVert\\cdot\\rVert_{2}$ and in [21] for $\\rho_{S}$ ), which is thus tested for the first time on transformers and GNNs. ", "page_idx": 7}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/a19667b834cfd8dfaca51fd576c8232720b58059a552bdb7e2c9143c288cb16a.jpg", "img_caption": ["Figure 3: $\\rho_{S}$ -based complexity measures vs. generalization gap for a ViT trained on CIFAR10: $\\dim_{\\operatorname{PH}}$ (left), $\\mathbf{PMag}({\\sqrt{\\pi}})$ (middle), and $E_{1}$ (right). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Performance on vision transformers. We see in Table 1 and Figure 3 (additional graphical representation is given in Appendix D.1) that our proposed topological complexities consistently outperform the PH dimensions across several vision transformer models and datasets. This suggests that PH-dim, previously tested only on small architectures, is less scalable to industry-standards models with more parameters. Figure 4a, including all (model, dataset) pairs for the pseudometric $\\rho_{S}$ , reveals important observations. First, we notice that the GKC of our topological complexities are both positive and close to 1, indicating that they are indeed good measures of generalization. We note that for most models and datasets, $\\dim_{\\operatorname{PH}}$ has a small or negative $\\psi_{\\mathrm{{BS}}}$ , indicating that it has less ability to explain generalization for varying batch-sizes. As it was observed in [21] for PH-dim, our complexities computed from the pseudometric $\\rho_{S}$ correlate very well with the generalization gap while this gap is based on the 01 loss. ", "page_idx": 8}, {"type": "text", "text": "Performance on GNNs. An important aspect of our framework is the ability to seamlessly encapsulate different data domains. In particular, the possibility of using different pseudometrics can help define topological complexities that naturally take into account the internal symmetries of GNNs, without any model-specific analysis [39, 8]. The results of Table 1 and Figure 4a confirm that our proposed topological complexities outperform PH-dim and correlate strongly with the generalization error for GNNs. Additionally, it may be observed that $\\mathbf{Mag}({\\sqrt{n}})$ performs significantly well for GNNs, and in particular better than $\\dot{\\mathbf{PMag}}(\\sqrt{n})$ . This points us towards the idea that further theory would be desirable to formally relate magnitude to the generalization error in that case4. ", "page_idx": 8}, {"type": "text", "text": "Comparison of the topological complexities. In Table 1 and Figures 3 and 4a, it can be seen that $E_{1}$ and $\\mathbf{PMag}({\\sqrt{n}})$ perform equally well for the image and graph experiments across multiple datasets, models, and data domains. We see in Table 1 that most topological complexities perform better with data-dependent metrics (i.e., $\\rho_{S}$ and 01) than with the Euclidean distance, for transformer-based experiments. This extends results obtained for PH-dim in [21], for smaller architectures. However, the poor performance of Euclidean-based complexities may also be partially caused by the projections applied to the Euclidean distance matrices to make them memory-wise computable (see section 4). This is a remaining limitation of our algorithms. On the other hand, the 01 and $\\rho_{S}$ data-dependent pseudometrics seem to yield similar performance in all experiments. ", "page_idx": 8}, {"type": "text", "text": "Ablations. In Figure 4b, we reveal that changing the optimizer has little effect on the observed correlation (for the same model and dataset). Interestingly, we note that the PH-dim, computed with pseudometric $\\rho_{S}$ and obtained from the SGD trajectories, exhibits high GKCs. This observation agrees with the results in [21]. Figure 3 further displays the typical behavior of several topological complexities for ViT and CIFAR10. In addition to the correlation of our proposed complexities being stronger than for the PH-dim, we observe that $E_{\\alpha}$ and $\\mathbf{PMag}({\\sqrt{n}})$ seem to better correlate with the generalization gap for small learning rates. Finally, it is consistently observed in Table 1 and Figures 4a and 4b that using a relatively high value of the (positive) magnitude scale $\\textstyle s={\\sqrt{n}}$ ) yields better correlations than small values $(s=10^{-2})$ ). However, both cases still provide satisfying correlation, comforting the robustness of magnitude as a generalization indicator. ", "page_idx": 8}, {"type": "text", "text": "Due to limited space, we present all the correlation coefficient of one transformer model ViT for CIFAR10 and Swin for CIFAR100 in Table 1 as illustrative examples for each dataset. The remaining results appear in the Appendix, Tables 4, 6, 3 and 5, and they all follow a similar trend. Further empirical results and illustrations of this behavior are provided in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/9482243a4f278c49bd89e12577aee58d974842eca7386ddfba46ce2b2f76eecd.jpg", "img_caption": ["Figure 4: Granulated Kendall coefficients for several models, datasets and topological quantities. Note that our framework is directly applicable to graph networks. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proved novel generalization bounds based on several topological complexities coming from TDA, namely $\\alpha$ -weighted lifetime sums and a new variant of metric space magnitude, which we called positive magnitude. Compared to previous studies, we require fewer assumptions and operate in a discrete setting in which our proposed quantities are fully computable. Our algorithms are flexible enough to be seamlessly integrated with diverse data domains and tasks. These advantages of our framework allowed us to create a computationally cheap experimental setup, as close as possible to the theoretical setup. We thus provided a comprehensive suite of experiments with several industry-relevant architectures across vision transformers and graph neural networks, which have not been explored yet in this literature. We show that our proposed topological complexities correlate well with the generalization error, outperforming the previously studied intrinsic dimensions. ", "page_idx": 9}, {"type": "text", "text": "Limitations & future work. The main limitation of our theory is the lack of understanding of the IT terms, while they are still smaller than most prior works. The presence of this term renders our bounds not fully computable in practice. Indeed, we are not aware of existing techniques to evaluate the MI between random sets and the dimensionality of $\\mathcal{W}_{t_{0}\\rightarrow T}$ (billions of parameters) could make a direct computation intractable. Nevertheless, our work focuses on improving the topological part of the existing bounds. Our main goal is to demonstrate a correlation with the generalization error rather than directly quantifying the generalization. Our experiments show that the introduced complexities are important and meaningful in addition to being amplified in the first part of the bound, as the dependence is explicit. Moreover, a better understanding of the behavior of positive magnitude for small values of the scale factor $s$ would be a necessary improvement. Regarding our experiments, a refinement of the estimation techniques of the topological complexities would be beneficial. Despite experimenting with practically relevant architectures, our future works also include scaling up our empirical analysis to include larger models and datasets, in particular large language models, which are still beyond the scope of this study. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the reviewers of Neurips 2024, who helped to significantly improve this paper. R.A. is supported by the United Kingdom Research and Innovation (grant EP/S02431X/1), UKRI Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics. U.\u00b8S. is partially supported by the French government under management of Agence Nationale de la Recherche as part of the \u201cInvestissements d\u2019avenir\u201d program, reference ANR-19- P3IA-0001 (PRAIRIE 3IA Institute). B.D. and U.S\u00b8. are partially supported by the European Research Council Starting Grant DYNASTY \u2013 101039676. T.B. is partially supported by the Royal Society Research Grant RG\\R1\\241402. TB was supported by a UKRI Future Leaders Fellowship [grant number MR/Y018818/1]. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. Certifying generalization is key for safe and trusted AI systems, hence we believe that our study may have a positive societal impact. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Henry Adams, Manuchehr Aminian, Elin Farnell, Michael Kirby, Joshua Mirth, Rachel Neville, Chris Peterson, and Clayton Shonkwiler. A fractal dimension for measures via persistent homology. In Topological Data Analysis: The Abel Symposium 2018, pages 1\u201331. Springer, 2020.   \n[2] Henry Adams and Michael Moy. Topology applied to machine learning: From global to local. Frontiers in Artificial Intelligence, 4:668302, 2021. [3] Rayna Andreeva, Katharina Limbeck, Bastian Rieck, and Rik Sarkar. Metric space magnitude and generalisation in neural networks. In Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML), volume 221 of Proceedings of Machine Learning Research, pages 242\u2013253. PMLR, 2023.   \n[4] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks, May 2019. [5] Perter L. Barlett and Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and Structural Result. Journal of Machine Learning Research, 2002.   \n[6] Peter Bartlett and Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results. Journal of Machine Learning Research, 2002.   \n[7] Ulrich Bauer. Ripser: Efficient computation of Vietoris-Rips persistence barcodes. Journal of Applied and Computational Topology, 5(3):391\u2013423, September 2021.   \n[8] Arash Behboodi, Gabriele Cesa, and Taco S Cohen. A pac-bayesian generalization bound for equivariant networks. Advances in Neural Information Processing Systems, 35:5654\u20135668, 2022.   \n[9] Subhrajit Bhattacharya, Robert Ghrist, and Vijay Kumar. Persistent homology for path planning in uncertain environments. IEEE Transactions on Robotics, 31(3):578\u2013590, 2015.   \n[10] Tolga Birdal, Aaron Lou, Leonidas J Guibas, and Umut Simsekli. Intrinsic dimension, persistent homology and generalization in neural networks. Advances in Neural Information Processing Systems, 34:6776\u20136789, 2021.   \n[11] Jean-Daniel Boissonat, Fr\u00e9d\u00e9ric Chazal, and Mariette Yvinec. Geometrical and Topological Inference. Cambridge Texts in Applied Mathematics. Cambridge University Press, 2018.   \n[12] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017.   \n[13] Alexander Camuto, George Deligiannidis, Murat A Erdogdu, Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. Fractal structure and generalization properties of stochastic optimization algorithms. Advances in neural information processing systems, 34:18774\u201318788, 2021.   \n[14] Gunnar Carlsson. Topological pattern recognition for point cloud data\\*. Acta Numerica, 23:289\u2013368, May 2014.   \n[15] Fr\u00e9d\u00e9ric Chazal and Bertrand Michel. An introduction to topological data analysis: fundamental and practical aspects for data scientists. Frontiers in artificial intelligence, 4:108, 2021.   \n[16] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms. The MIT Press, 2nd edition, 2001.   \n[17] Ciprian A Corneanu, Meysam Madadi, Sergio Escalera, and Aleix M Martinez. What does it mean to learn in deep networks? and, how does one detect adversarial attacks? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4757\u20134766, 2019.   \n[18] Vin De Silva and Robert Ghrist. Coverage in sensor networks via persistent homology. Algebraic & Geometric Topology, 7(1):339\u2013358, 2007.   \n[19] Claire Donnat and Susan Holmes. Tracking network dynamics: a survey of distances and similarity metrics, 2018.   \n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, June 2021.   \n[21] Benjamin Dupuis, George Deligiannidis, and Umut Simsekli. Generalization bounds using data-dependent fractal dimensions. In International Conference on Machine Learning, pages 8922\u20138968. PMLR, 2023.   \n[22] Benjamin Dupuis, Paul Viallard, George Deligiannidis, and Umut Simsekli. Uniform generalization bounds on data-dependent hypothesis sets via pac-bayesian theory on random sets, 2024.   \n[23] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):1\u201348, 2023.   \n[24] Herbert Edelsbrunner and John Harer. Computational Topology - an Introduction | Semantic Scholar. American Mathematical Society, 2010.   \n[25] Kevin Emmett, Benjamin Schweinhart, and Raul Rabadan. Multiscale topology of chromatin folding. In 9th EAI International Conference on Bio-inspired Information and Communications Technologies (formerly BIONETICS), pages 177\u2013180, 2016.   \n[26] Kenneth Falconer. Fractal Geometry - Mathematical Foundations and Applications - Third Edition. Wiley, 2014.   \n[27] Carlos Fernandez-Granda. Lecture 5; random projections, 2016.   \n[28] Jingwen Fu, Zhizheng Zhang, Dacheng Yin, Yan Lu, and Nanning Zheng. Learning Trajectories are Generalization Indicators, October 2023.   \n[29] Hanan Gani, Muzammal Naseer, and Mohammad Yaqub. How to train vision transformer on small-scale datasets? arXiv preprint arXiv:2210.07240, 2022.   \n[30] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pages 729\u2013734 vol. 2, 2005.   \n[31] Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Miolane, Aldo Guzm\u00e1n-S\u00e1enz, Karthikeyan Natesan Ramamurthy, Tolga Birdal, Tamal K Dey, Soham Mukherjee, Shreyas N Samaga, et al. Topological deep learning: Going beyond graph data. arXiv preprint arXiv:2206.00606, 2022.   \n[32] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[33] Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002.   \n[34] Yasuaki Hiraoka, Takenobu Nakamura, Akihiko Hirata, Emerson G Escolar, Kaname Matsue, and Yasumasa Nishiura. Hierarchical structures of amorphous solids characterized by persistent homology. Proceedings of the National Academy of Sciences, 113(26):7035\u20137040, 2016.   \n[35] Liam Hodgkinson, Umut \u00b8Sim\u00b8sekli, Rajiv Khanna, and Michael W. Mahoney. Generalization Bounds using Lower Tail Exponents in Stochastic Optimizers. Proceedings of the 39th International Conference on Machine Learning, July 2022.   \n[36] Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk. Training Dynamics of Deep Network Linear Regions. https://arxiv.org/abs/2310.12977v1, October 2023.   \n[37] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic Generalization Measures and Where to Find Them. ICLR 2020, December 2019.   \n[38] Maurice G. Kendall. A new reasure of rank correlation. Biometrika, 1938.   \n[39] Bobak T Kiani, Thien Le, Hannah Lawrence, Stefanie Jegelka, and Melanie Weber. On the hardness of learning under symmetries. arXiv preprint arXiv:2401.01869, 2024.   \n[40] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.   \n[41] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[42] Gady Kozma, Zvi Lotker, and Gideon Stupp. The minimal spanning tree and the upper box dimension. Proceedings of the American Mathematical Society, 134(4):1183\u20131187, 2006.   \n[43] Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.   \n[44] Alex Krizhevsky, Vinod Nair, and Geoffrey E. Hinton. The cifar-10 dataset, 2014.   \n[45] Gregory Leibon, Scott Pauls, Daniel Rockmore, and Robert Savell. Topological structures in the equities market network. Proceedings of the National Academy of Sciences, 105(52):20589\u2013 20594, 2008.   \n[46] Tom Leinster. The magnitude of metric spaces. Documenta Mathematica, 18:857\u2013905, 2013.   \n[47] Katharina Limbeck, Rayna Andreeva, Rik Sarkar, and Bastian Rieck. Metric space magnitude for evaluating the diversity of latent representations, 2024.   \n[48] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[49] Xuanyuan Luo, Luo Bei, and Jian Li. Generalization Bounds for Gradient Methods via Discrete and Continuous Prior, October 2022.   \n[50] Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction, January 2023.   \n[51] German Magai. Deep neural networks architectures from the perspective of manifold learning. In 2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI), pages 1021\u20131031. IEEE, 2023.   \n[52] German Magai and Anton Ayzenberg. Topology and geometry of data manifold in deep learning. arXiv preprint arXiv:2204.08624, 2022.   \n[53] Pertti Mattila. Geometry of Sets and Measures in Euclidean Spaces. Cambridge University Press, 1999.   \n[54] Pertti Mattila, Manuel Moran, and Jose-manual Rey. Dimension of a measure. Studia mathematica 142 (3), 2000.   \n[55] Mark W Meckes. Positive definite metric spaces. Positivity, 17(3):733\u2013757, 2013.   \n[56] Mark W Meckes. Magnitude, diversity, capacities, and dimensions of metric spaces. Potential Analysis, 42(2):549\u2013572, 2015.   \n[57] Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints. In Proceedings of the 31st Conference On Learning Theory. arXiv, July 2017.   \n[58] Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim. Topology of deep neural networks. Journal of Machine Learning Research, 21(184):1\u201340, 2020.   \n[59] Gergely Neu, Gintare Karolina Dziugaite, Mahdi Haghifam, and Daniel M. Roy. InformationTheoretic Generalization Bounds for Stochastic Gradient Descent, August 2021.   \n[60] Monica Nicolau, Arnold J Levine, and Gunnar Carlsson. Topology based data analysis identifies a subgroup of breast cancers with a unique mutational proflie and excellent survival. Proceedings of the National Academy of Sciences, 108(17):7265\u20137270, 2011.   \n[61] Nina Otter, Mason A Porter, Ulrike Tillmann, Peter Grindrod, and Heather A Harrington. A roadmap for the computation of persistent homology. EPJ Data Science, 6:1\u201338, 2017.   \n[62] Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson, Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro Li\u00f2, Paolo Di Lorenzo, et al. Position paper: Challenges and opportunities in topological deep learning. arXiv preprint arXiv:2402.08871, 2024.   \n[63] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[64] Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization Error Bounds for Noisy, Iterative Algorithms. 2018 IEEE International Symposium on Information Theory (ISIT), January 2018.   \n[65] Juli\u00e1n Burella P\u00e9rez, Sydney Hauke, Umberto Lupo, Matteo Caorsi, and Alberto Dassatti. Giotto-ph: A Python Library for High-Performance Computation of Persistent Homology of Vietoris-Rips Filtrations, August 2021.   \n[66] David P\u00e9rez-Fern\u00e1ndez, Asier Guti\u00e9rrez-Fandi\u00f1o, Jordi Armengol-Estap\u00e9, and Marta Villegas. Characterizing and measuring the similarity of neural networks with persistent homology. arXiv preprint arXiv:2101.07752, 2021.   \n[67] Yakov B Pesin. Dimension theory in dynamical systems: contemporary views and applications. University of Chicago Press, 2008.   \n[68] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in neural information processing systems, 34:12116\u201312128, 2021.   \n[69] Patrick Rebeschini. Algorithmic fundations of learning, 2020.   \n[70] Bastian Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch, and Karsten Borgwardt. Neural persistence: A complexity measure for deep neural networks using algebraic topology. arXiv preprint arXiv:1812.09764, 2018.   \n[71] Sarav Sachs, Umut \u00b8Sim\u00b8sekli, and Tim van Erven. Generalization Guarantees via Algorithmdependent Rademacher Complexity - preprint. COLT 2023, 2023.   \n[72] Shilan Salim. The $q$ -spread dimension and the maximum diversity of square grid metric spaces. PhD thesis, University of Sheffield, 2021.   \n[73] Benjamin Schweinhart. Fractal dimension and the persistent homology of random geometric complexes. Advances in Mathematics, 372:107291, 2020.   \n[74] Benjamin Schweinhart. Persistent homology and the upper box dimension. Discrete & Computational Geometry, 65(2):331\u2013364, 2021.   \n[75] Milad Sefidgaran, Amin Gohari, Ga\u00ebl Richard, and Umut S\u00b8im\u00b8sekli. Rate-Distortion Theoretic Generalization Bounds for Stochastic Learning Algorithms, June 2022.   \n[76] Milad Sefidgaran and Abdellatif Zaidi. Data-dependent Generalization Bounds via Variable-Size Compressibility, January 2024.   \n[77] Shai Shalev-Schwartz and Shai Ben-David. Understanding Machine Learning - From Theory to Algorithms. Cambridge University Press, 2014.   \n[78] Umut Simsekli, Ozan Sener, George Deligiannidis, and Murat A Erdogdu. Hausdorff dimension, heavy tails, and generalization in neural networks. Advances in Neural Information Processing Systems, 33:5138\u20135151, 2020.   \n[79] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[80] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 32\u201342, 2021.   \n[81] Tim van Erven and Peter Harremo\u00ebs. R\u00e9nyi Divergence and Kullback-Leibler Divergence. IEEE Transactions on Information Theory, 60(7):3797\u20133820, July 2014.   \n[82] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2020.   \n[83] Satoru Watanabe and Hayato Yamana. Topological measurement of deep neural networks using persistent homology. Annals of Mathematics and Artificial Intelligence, 90(1):75\u201392, 2022.   \n[84] Jing Xu, Jiaye Teng, Yang Yuan, and Andrew Yao. Towards Data-Algorithm Dependent Generalization: A Case Study on Overparameterized Linear Regression. Advances in Neural Information Processing Systems, 36:79698\u201379733, December 2023.   \n[85] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR 2017, February 2017.   \n[86] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013 115, February 2021.   \n[87] Afra Zomorodian. Topological data analysis. Advances in applied and computational topology, 70:1\u201339, 2012. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We now provide additional technical details and proofs that are omitted from the paper, followed by experimental evidence complementing our main paper. We organize the appendix as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Appendix A presents additional technical background related to information theory, Rademacher complexity, and the various topological quantities that appear in our work.   \n\u2022 In Appendix B, we present the omitted proofs of all our theoretical results, as well as a few additional theoretical contributions.   \n\u2022 In Appendix C, we show the experimental details needed to reproduce our experiments.   \n\u2022 Finally, Appendix D is dedicated to additional empirical results. ", "page_idx": 15}, {"type": "text", "text": "A Additional technical background ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Information-theoretic quantities ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The following definition is a precise definition of the total mutual information term that appears in our main theoretical results. The reader may consult [81, 35, 22] for further information on this notion. ", "page_idx": 15}, {"type": "text", "text": "Definition A.1 (Total mutual information). Let $X$ and $Y$ be two random elements defined on a probability space $\\textstyle(\\Omega,{\\mathcal{F}},\\operatorname{P})$ (note that the codomains of $X$ and $Y$ may be distinct). We define the total mutual information between $X$ and $Y$ by the following formula: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{I}_{\\infty}(X,Y)=\\log\\left(\\operatorname*{sup}_{A}\\frac{\\mathbb{P}_{X,Y}(A)}{\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Y}(A)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Such a term has already been used in the fractal-based generalization literature [35, 22]. Other works used intricate variants of this total mutual information term [21, 10, 3, 13]. We stress the fact that our proposed bounds are simpler. ", "page_idx": 15}, {"type": "text", "text": "A.2 Rademacher complexity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Rademacher complexity [6, 77] is a central tool in learning theory. As part of our theory uses this notion, we now provide its definition and introduce some notation. ", "page_idx": 15}, {"type": "text", "text": "Definition A.2 (Rademacher complexity on a hypothesis set). Let us fix a dataset $S\\in{\\mathcal{Z}}^{n}$ , a set $\\mathcal{W}\\subset\\mathbb{R}^{d}$ and $\\epsilon=(\\epsilon_{1},...\\,,\\epsilon_{n})$ some iid Rademacher random variable.5 Whenever it is defined, we will call Rademacher complexity of $\\ell$ over $\\mathcal{W}$ the following quantity: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Rad}(\\ell,\\mathcal{W},S):=\\frac{1}{n}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{w\\in\\mathcal{W}}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Rademacher complexity has already been used in [21, Theorem 3.4] to relate the generalization error to the so-called data-dependent fractal dimension. Part of our theory is based on a recent extension of such arguments in the data-dependent setting [22]. ", "page_idx": 15}, {"type": "text", "text": "A.3 Persistent homology ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The goal of this short subsection is to present a few notions of persistent homology, which is necessary for a better understanding of our contributions. ", "page_idx": 15}, {"type": "text", "text": "Persistent homology [24, 14, 11] is an important subfield of TDA, capable of providing myriad of new insights for analysing data by extracting meaningful topological features. It has demonstrated its usefulness in a very diverse set of applications from biology [60, 25], to materials science [34], finance [45], robotics [9], sensor networks [18] and a lot more [61]. The types of datasets which are amenable to this kind of analysis are finite metric spaces (known as point-cloud datasets), images, networks and also level-sets of functions. More recently, several studies have brought to light empirical links between persistent homology and DNNs [70, 17, 66]. In particular, recent studies have related the worst-case generalization error to several concept of intrinsic dimensions defined through persistent homology [10, 21]. As mentioned in the introduction, our goal is to extend these last studies to more practical settings. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "In general, persistent homology is defined for any degree $k\\in\\mathbb{N}$ (denoted $\\mathrm{PH}^{k}$ ). Intuitively, $\\mathrm{PH}^{k}$ keeps track of the number of \u201choles of dimension $k^{\\mathord{\\circ}}$ in a set when looked at different scales. However, in our work and as in [10, 21], we only use $\\mathrm{PH}^{0}$ , whose presentation is simpler. In this section, to avoid harming the readability of the paper, we only present a high-level introduction to $\\mathrm{PH}^{0}$ that is sufficient to understand our work. The interested reader may consult [11, 15, 87] for a more in-depth introduction to persistent homology. ", "page_idx": 16}, {"type": "text", "text": "We first start by introducing briefly homology, which is a classical concept in algebraic topology. We only introduce the most essential concepts for understanding persistent homology. For a more detailed introduction, please consult [33]. ", "page_idx": 16}, {"type": "text", "text": "Definition A.3. A simplicial complex is a set $K$ of finite sets closed under the subset relation: if $\\sigma\\in K$ and $\\tau\\subset\\sigma$ , then $\\tau\\in K$ . ", "page_idx": 16}, {"type": "text", "text": "In the above definition, $\\sigma$ is a simplex (plural simplices) and $\\tau$ is a face of $\\sigma$ , its coface. ", "page_idx": 16}, {"type": "text", "text": "Definition A.4. An abstract simplicial complex $\\kappa$ is a finite collection of simplices where a face of any simplex $\\sigma\\in\\kappa$ is also a simplex in $\\kappa$ . ", "page_idx": 16}, {"type": "text", "text": "Definition A.5. A simplicial $k$ -chain is the formal sum of $k$ -simplices, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}=r_{i}\\sigma_{i},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where each $r_{i}\\in R$ , where $R$ is a fixed commutative ring with additive identity 0 and multiplicative identity 1, and $\\sigma_{i}\\in\\mathcal{K}$ . ", "page_idx": 16}, {"type": "text", "text": "$\\kappa_{k}$ is the set of simplicial $k$ -chains with addition over $R$ , which is an $R$ -module. Then, the set of all $k$ -simplices of the complex $\\kappa$ is a set of generators for $\\kappa_{k}$ . For each generator $\\sigma$ , the boundary of $\\sigma$ is the sum of all $(k-1)$ -faces of $\\sigma$ . ", "page_idx": 16}, {"type": "text", "text": "Definition A.6. The boundary of a $k$ -simplex $\\sigma=(x_{0},\\ldots,x_{k})$ is the $(k-1)$ -chain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{k}(\\sigma)=\\sum_{i=0}^{k}(-1)^{i}(x_{0},\\ldots,\\hat{x_{i}},\\ldots,x_{k}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(x_{0},\\ldots,{\\hat{x_{i}}},\\ldots,x_{k})$ is the $(k-1)$ -simplex spanned by all vertices without $x_{i}$ ", "page_idx": 16}, {"type": "text", "text": "It is common that the coefficients for homology are considered to be restricted to $\\mathbb{Z}_{2}$ , which is the field with 2 elements, 0 and 1, where $1+1=0$ . However, the theory extends to homoogy with coefficeints in any field (and since every field is a ring, the definitions in terms of rings are more general). ", "page_idx": 16}, {"type": "text", "text": "Definition A.7. A chain complex is a sequence of abelian groups $A_{k}$ with homomorphisms (called boundary maps) $\\partial_{k}:A_{k}\\rightarrow A_{k-1}$ , such that $\\partial_{k-1}\\circ\\partial_{k}=0$ for all $k$ . ", "page_idx": 16}, {"type": "text", "text": "We should note that when considering coefficients in $\\mathbb{Z}_{2}$ , a $k$ -chain can be seen as a finite collection of $k$ -simplices. ", "page_idx": 16}, {"type": "text", "text": "Introduce topological invariants: simplicial homology groups and Betti numbers. ", "page_idx": 16}, {"type": "text", "text": "Definition A.8 (Simplicial Homology group). The $n$ -th (simplicial) homology group of a finite simplicial complex $\\kappa$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\nH_{n}=\\ker\\partial_{n}/\\mathrm{im}\\partial_{n+1},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ker and im are the kernel and image respectively of the boundary operator. ", "page_idx": 16}, {"type": "text", "text": "In order to define the simplicial complexes of use in TDA, we need to first understand what a nerve is. ", "page_idx": 16}, {"type": "text", "text": "Definition A.9 (Nerve). A simplicial complex associated to a collection of sets is called a nerve. The sets are the vertices of the complex, and a simplex belongs to a complex iff its vertices have a non-empty intersection, $\\operatorname{Nrv}=\\{\\alpha\\subseteq S\\mid\\cap_{A\\in\\alpha}A\\neq\\varnothing\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Definition A.10 ( C\u02c7ech complex). The C\u02c7ech complex of $X$ for radius $r$ is $\\check{\\mathrm{Cech}}_{r}(X)=\\mathrm{Nrv}\\{B(x,r)\\;|\\;$ $x\\in X\\}$ , where $B(x,r)$ is the closed ball of radius $r\\geq0$ , centered at $x$ . ", "page_idx": 17}, {"type": "text", "text": "In other words, the C\u02c7ech complex is the nerve of the ball neighbourhoods of a set of points $X\\subseteq\\mathbb{R}^{n}$ . The \u02c7Cech complex faithfully captures the topology of the space, but it is not computed in practice due to its high computational cost. Instead, a different complex called Vietoris-Rips (VR) is used due to ease of construction for higher dimensions. It can be shown that the VR complex is not always homotopy equivalent to the C\u02c7ech complex, and therefore it can be seen as an approximation. ", "page_idx": 17}, {"type": "text", "text": "We first need to introduce the notion of a clique complex to explain what the VR is. ", "page_idx": 17}, {"type": "text", "text": "Definition A.11 (Clique complex). The clique complex for a graph $G=(V,E)$ consists of all cliques of $G$ , which are all simplices $\\alpha\\subseteq V$ for which $E$ contains all edges of $\\alpha$ . ", "page_idx": 17}, {"type": "text", "text": "Now we have explicitly states all the necessary components in order to define the main complex used in TDA, the Vietoris-Rips complex. ", "page_idx": 17}, {"type": "text", "text": "Definition A.12 (Vietoris-Rips complex). The Vietoris-Rips complex of $X$ for radius $r$ is the clique complex of the 1-skeleton of the \u02c7Cech complex of $X$ and $r$ , ${\\mathrm{Rips}}_{r}(X)=\\{\\alpha\\in X\\mid||u-v||\\leq2r\\}$ for all $u,v\\in\\alpha$ . ", "page_idx": 17}, {"type": "text", "text": "Now that we have defined the most important complex in TDA, we proceed to explain how we can derive important topological information at multiple scales by introducing the concept of a flitration. ", "page_idx": 17}, {"type": "text", "text": "Definition A.13. Given a simplicial complex $\\kappa$ , a flitration is a totally ordered set of subcomplexes $\\kappa^{i}$ of $\\kappa$ , indexed by nonnegative integers, such that for $i\\leq j,\\kappa^{i}\\subseteq\\dot{\\mathcal{K}}^{j}$ . ", "page_idx": 17}, {"type": "text", "text": "Definition A.14 (Filtered simplicial complex). A simplicial complex, $\\kappa$ , together with a filtration (function $f:K\\rightarrow\\mathbb{R}$ such that $f(\\sigma)\\le f(\\tau)$ whenever $\\sigma$ is a face of $\\tau$ ). The sublevel set at a value $r\\in\\mathbb{R}$ is $f^{-1}(-\\infty,r]$ , which is a subxomplex of $\\kappa$ . Let $r_{0}<r_{1}<\\cdot\\cdot<r_{m}$ be the values of the simplices, and $\\mathcal{K}_{i}=\\dot{f}^{-1}(-\\infty,r_{i}]$ , then we call $K_{0}\\subseteq K_{1}\\subseteq\\cdots\\subseteq K_{m}$ the sublevel set filtration of $f$ . ", "page_idx": 17}, {"type": "text", "text": "When you start with a simplicial complex $\\kappa$ and you filter it according to a filtration $f$ , it is clear that the homology of $\\kappa_{r}$ evolves as the radius $r$ increases. For example, new connected components can be formed, loops can appear or disapper, cavities can form. What persistent homology does, and where the importance of the filtering comes in is that now we have the tools to track the topological changes associated with the different stages of the filtering process, and to associate a lifetime to them (track when a topological feature has first appeared and at which stage of the filtration it will disappear). This essential topological information is recorded in a set of intervals known as barcodes, which can be represented as a multiset of points in $\\mathbb{R}^{2}$ , where the coordinates correspond to the birth and death points of each interval. ", "page_idx": 17}, {"type": "text", "text": "A.3.1 Persistent homology of degree 0 (alternative approach) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the rest of the this section, we only focus on homology in dimension 0, and provide an alternative and perhaps easier to understand interpretation. Please note that the following definition is a simplified and non-standard (though equivalent) definition of $\\mathrm{PH}^{0}$ . ", "page_idx": 17}, {"type": "text", "text": "Definition A.15 (Persistent homology of degree $0\\,(\\mathrm{PH}^{0}))$ . Let $(X,\\rho)$ be a finite metric space and $N$ its cardinality. For each time6 $t\\geq0$ , we construct an undirected graph $G_{t}$ , whose edges are given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall x,y\\in X,\\ \\{x,y\\}\\in G_{t}\\iff\\rho(x,y)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "There exists a finite set of times $0\\;<\\;t_{1}\\cdot\\cdot\\cdot\\;<\\;t_{k}\\;<\\;+\\infty$ such that the number of connected components in $G_{t_{i}}$ changes compared to $G_{t}$ for $t<t_{i}$ . Let $c_{i}$ be the number of connected components in $G_{t_{i}}$ . By convention we set $c_{0}=N$ and $t_{0}=0$ and define $n_{i}:=c_{i}-c_{i-1}$ . $\\mathrm{PH}^{0}$ is then defined as the following multiset (the notation $\\{\\{\\cdot\\}\\}$ denotes multisets): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{PH}^{0}:=\\left\\{\\left\\{\\underbrace{t_{1},\\dots,t_{1}}_{n_{1}\\;\\mathrm{times}},t_{2},\\dots,\\underbrace{t_{k},\\dots,t_{k}}_{n_{k}\\;\\mathrm{times}}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "6We use the term time for the scalar $t$ , as it is classically done in the study of persistent homology. Note that this has nothing to do with the number of iterations appearing in the rest of the paper. ", "page_idx": 17}, {"type": "text", "text": "Remark A.16 (Vietoris-Rips flitration). The above is a simplified high-level definition of $\\mathrm{PH}^{0}$ . More formally, the construction of the family of graphs $G_{t}$ corresponds to the construction of the so-called Vietoris-Rips flitration of $X$ , of which we only kept the simplices of dimension 1, see [11] for more details. ", "page_idx": 18}, {"type": "text", "text": "We now use $\\mathrm{PH}^{0}$ to give the definitions of the quantities of interest in our work. The following is a definition of the quantity $E_{\\alpha}$ already mentioned in section 2, but seen through the lens of persistent homology. As it will be explained in Appendix A.4, these definitions are equivalent. ", "page_idx": 18}, {"type": "text", "text": "Definition A.17 ( $\\alpha$ -weighted lifetime sums). With the same notations as in Definition A.15, we define the $\\alpha$ -weighted lifetime sums as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall\\alpha\\geq0,\\;{\\pmb E}_{\\alpha}^{\\rho}(X):=\\sum_{t\\in\\mathrm{PH}^{0}}t^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark A.18 (\u201cbirth\u201d and \u201cdeath\u201d times). $\\mathrm{PH}^{k}$ is usually defined as a multiset of birth and death times, tracking the appearance and disappearance of \u201choles of dimension $k^{\\mathord{\\circ}}$ during the construction of the Vietoris-Rips flitration of $X$ . In the particular case of $\\mathrm{PH}^{0}$ , all birth times are 0 and the times that we constructed correspond to the death times. ", "page_idx": 18}, {"type": "text", "text": "We end this section by giving the definition of the PH dimension, which has been shown to be theoretically and empirically related to the generalization error of neural networks in prior works [10, 21]. ", "page_idx": 18}, {"type": "text", "text": "Definition A.19 (Persistent homology dimension of degree 0). Given a compact metric space $(X,\\rho)$ , we define the PH dimension of degree 0 by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\dim_{\\operatorname{PH}}^{\\rho}(X):=\\operatorname*{inf}\\left\\{\\alpha\\geq0,\\,\\exists C>0,\\forall A\\subseteq X{\\mathrm{~finite,~}}E_{\\alpha}(A)\\leq C\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It has been shown in [42, 73] that for any compact metric space, the PH dimension defined above is equal to the celebrated upper box-counting dimension [26, 54]. ", "page_idx": 18}, {"type": "text", "text": "A.4 Minimum spanning tree ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The persistent homology dimension used in existing generalization bounds [10, 21] is closely related to another notion of intrinsic dimension, called minimum spanning tree (MST) dimension [42], in the sense that the PH and MST dimensions of bounded metric spaces are identical. The link between persistent homology and MST is even deeper than the equality between the induced dimensions, as noted by [73]. In this section, we define quantities related to MSTs which will play an important role in our proofs. ", "page_idx": 18}, {"type": "text", "text": "In this section let us fix a finite metric space $(X,\\rho)$ . Let us first specify our notations for trees. A tree $\\tau$ on $X$ is a connected undirected graph. We represent $\\tau$ by its set of edges, which are denoted $a\\rightarrow b$ (or equivalently $b\\to a$ as the graph is undirected). For an edge $e$ of the form $a\\rightarrow b$ , we define its length by $|e|=\\rho(a,b)$ . ", "page_idx": 18}, {"type": "text", "text": "Definition A.20 (Minimum spanning tree). Let us define the cost of a tree by the sum of the length of its edges, i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\nE_{1}^{\\mathrm{MST}}(Tilde{T}):=\\sum_{e\\in\\mathcal{T}}|e|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "An MST of $X$ is defined as a tree with minimal cost. A consequence of the greedy algorithm to find such an MST [16] is that an MST $\\tau$ is also minimal for any of the following costs: ", "page_idx": 18}, {"type": "equation", "text": "$$\nE_{\\alpha}^{\\mathrm{MST}}(\\mathcal{T}):=\\sum_{e\\in\\mathcal{T}}|e|^{\\alpha},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\alpha\\geq0$ ", "page_idx": 18}, {"type": "text", "text": "Our interest in this notion comes from several results that are summed up in the following theorem.   \nThe reader can refer to [1, 73, 11] for more details. ", "page_idx": 18}, {"type": "text", "text": "Theorem A.21 (Link between MST and persistent homology). There is a bijection between the two following multisets: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The multiset of the lifetimes in the persistent homology of degree 0 of the Vietoris-Rips complex of $X$ .   \n\u2022 The multiset of the length of the edges of an MST of $X$ . ", "page_idx": 19}, {"type": "text", "text": "Therefore, if we fix some $\\alpha\\geq0$ , the weighted $\\alpha$ -sum associated to the persistent homology of degree 0 of the Vietoris-Rips complex of $X$ is equal to the cost $E_{\\alpha}$ of an MST of $X$ , ie: ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{\\alpha}^{\\mathrm{MST}}({\\mathcal T})=E_{\\alpha}(X).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In all the following, we will use the notation $E_{\\alpha}$ to denote both quantities. ", "page_idx": 19}, {"type": "text", "text": "A.5 Magnitude ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let us restate formally a few standard definitions. The reader may refer to [46, 55, 56] for more details on the notions of magnitude, weighting, and positive definite metric spaces. In this section, we fix a finite metric space $(X,\\rho)$ . Some of the presented concepts will be later extended to pseudometric spaces in Appendix B.2. ", "page_idx": 19}, {"type": "text", "text": "As before, the similarity matrix [46] of $X$ is defined by $M(a,b)=e^{-\\rho(a,b)}$ , for $a,b\\in X$ . We now define weightings and magnitude of $X$ , according to [46, Section 2.1]. ", "page_idx": 19}, {"type": "text", "text": "Definition A.22 (Weighting and magnitude). A weighting of $X$ is a function $\\beta:X\\longrightarrow\\mathbb{R}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall a\\in X,\\ \\sum_{b\\in X}e^{-\\rho(a,b)}\\beta(b)=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If such a weighting exists, the magnitude of $X$ is defined by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{Mag}(X):=\\sum_{b\\in X}\\beta(b).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It is easily seen that this definition is independent of the choice of weighting $\\beta$ . When a weighting exists, we say that $X$ \u201chas magnitude\u201d. ", "page_idx": 19}, {"type": "text", "text": "Based on such a definition, it is natural to inquire, whether such a weighting exists. This question has been studied by several authors [46, 55, 56]. This question appears to be related to the notion of positive definite space, which we now define, according to [46]. ", "page_idx": 19}, {"type": "text", "text": "Definition A.23 (Positive definite space). $X$ is positive definite if the similarity matrix $M$ is positive definite. ", "page_idx": 19}, {"type": "text", "text": "It is clear that positive definite spaces have magnitude. More interestingly, we have the following result, which ensures that most metric spaces considered in this study are positive definite. ", "page_idx": 19}, {"type": "text", "text": "Theorem A.24 ([46, 55]). Let $p\\,\\in\\,[1,2]$ and $d\\geq1$ , every finite subset of $(\\mathbb{R}^{d},\\|\\cdot\\|_{p})$ is positive definite. ", "page_idx": 19}, {"type": "text", "text": "A.6 Covering and packing numbers ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we fix a compact pseudometric space $(X,\\rho)$ and give definitions of covering and packing numbers. These quantities have long been of primary interest in learning theory, in particular through the classical covering arguments for Rademacher complexity [77, 69]. More recently, limits of covering arguments have been leveraged by several authors to derive uniform generalization bounds in terms of fractal dimensions [78, 35, 13, 21, 22], which we aim to improve in this study. ", "page_idx": 19}, {"type": "text", "text": "For $x\\ \\in\\ X$ and $r~>~0$ , we denote the closed ball centered at $x$ and or radius $r$ by $\\bar{B}_{r}(x)\\;:=\\;$ $\\left\\{y\\in X\\;,\\rho(x,y)\\leq r\\right\\}$ . We can now define covering and packing. ", "page_idx": 19}, {"type": "text", "text": "Definition A.25 (Covering number). Let $\\delta>0$ , the covering number $N_{\\delta}^{\\rho}(X)$ is the cardinality of a minimal set of points $N$ such that: ", "page_idx": 19}, {"type": "equation", "text": "$$\nX\\subseteq\\bigcup_{x\\in N}{\\bar{B}}_{\\delta}(x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Remark A.26. There exist several conventions for the definition of such numbers [26, 53, 82], all of which are equivalent up to absolute constants and in particular induce the same fractal dimensions on $X$ (see [26]). ", "page_idx": 19}, {"type": "text", "text": "Definition A.27 (Packing number). Let $\\delta>0$ , the covering number $N_{\\delta}^{\\rho}(X)$ is the cardinality of a maximal set of disjoint closed balls with centers in $X$ . ", "page_idx": 19}, {"type": "text", "text": "A.7 About Johnson-Lindenstrauss lemma ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our implementation of Euclidean-based topological quantities, we use sparse random projections to project the weight vectors from $\\mathbb{R}^{d}$ to a lower dimensional subspace. This is necessary because of memory constraints. Indeed, storing the full trajectory $\\mathcal{W}_{t_{0}\\rightarrow T}\\;\\subset\\;\\mathbb{R}^{d}$ (in our experiments $T-t_{0}=5\\times10^{3},$ ) can become intractable for large models. ", "page_idx": 20}, {"type": "text", "text": "Given a finite set of points $\\mathcal{W}\\subset\\mathbb{R}^{d}$ and $\\epsilon>0$ . Let $\\begin{array}{r}{N\\ge\\mathcal{O}\\left(\\frac{\\log|\\mathcal{W}|}{\\epsilon^{2}}\\right)}\\end{array}$ , Johnson-Lindenstrauss lemma [82, 27] ensures the existence of a linear map $P:\\mathbb{R}^{d}\\longrightarrow\\mathbf{\\dot{R}}^{N}$ such that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall w,w^{\\prime}\\in\\mathcal{W},\\;\\left(1-\\epsilon\\right)\\left\\Vert w-w^{\\prime}\\right\\Vert^{2}\\leq\\left\\Vert P w-P w^{\\prime}\\right\\Vert^{2}\\leq\\left(1+\\epsilon\\right)\\left\\Vert w-w^{\\prime}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In practice, the linear maps suggested by this result can be obtained through subgaussian random projections [82, Section 9.3]. ", "page_idx": 20}, {"type": "text", "text": "In our work, as the purpose of Johnson-Lindenstrauss embeddings is mainly memory optimization, we have to rely on sparse random projections. We use the implementation provided in scikit-learn [63]. More precisely, we used a relative variation $\\epsilon$ of $5\\%$ . ", "page_idx": 20}, {"type": "text", "text": "Finally, it should be noted that these projection techniques were only used for the vision transformer experiments, as the GNNs that we used have a small enough number of parameters to avoid the use of random projections. ", "page_idx": 20}, {"type": "text", "text": "A.8 A note on the connection to Topological Deep Learning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models [62, 31]. Our topological complexity measures can be seen as a direction towards addressing the Open Problem 7 mentioned in [62] concerning the discovery of topological properties of internal representations that are linked to generalization. ", "page_idx": 20}, {"type": "text", "text": "B Omitted proofs of the theoretical results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present the proofs of our main theoretical contributions. We divide our proofs into two groups of subsections: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Sections B.1,B.2 and B.3 focus on the extension (in a very natural way) of the quantities appearing in our bounds in pseudometric spaces. The main outcome of this analysis is the definition of positive magnitude in the pseudometric case. Note that Appendix B.1 is not a contribution of this paper. We placed it in this section to improve the readability of the paper. \u2022 In sections B.4, B.5, B.6 and B.7, we present the proof of our main theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Before, proving our main results, we define the notion of metric identification, which will be used in several of the following subsections. This is the same setting that was used in [21] to naturally extend the persistent homology dimension to pseudometric spaces. ", "page_idx": 20}, {"type": "text", "text": "Definition B.1 (Metric identification). Let $(X,\\rho)$ be a pseudometric space. We can define an equivalence relation on $X$ by $a\\sim b\\iff\\rho(a,b)=0$ . The associated quotient space, which is denoted $X/{\\sim}$ is a metric space for the naturally induced metric, which we still denote $\\rho$ .7 We will also use the canonical projection, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi:X\\longrightarrow X/{\\sim}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "These notations will be used throughout the text. ", "page_idx": 20}, {"type": "text", "text": "B.1 Persistent homology and MST in pseudometric spaces ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this short subsection, we first restate results proven in [21], regarding persistent homology in pseudometric spaces. The main result is the following proposition, which has been proven inside the proof of [21, Lemma B.9]. ", "page_idx": 20}, {"type": "text", "text": "Proposition B.2 ([22]). Let $(X,\\rho)$ be a finite pseudometric space and $\\alpha\\geq0$ , then we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{{\\cal E}}_{\\alpha}(X)=\\pmb{{\\cal E}}_{\\alpha}\\left(X/{\\sim}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the pseudometric $\\rho$ (and its metric identification) have been omitted from the notation. ", "page_idx": 21}, {"type": "text", "text": "Based on Theorem A.21, the above result is also true when $E_{\\alpha}$ represents the cost of a MST of $X$ . ", "page_idx": 21}, {"type": "text", "text": "B.2 Magnitude in pseudometric spaces ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we fix $(X,\\rho)$ a finite pseudometric space. We denote by $X/{\\sim}$ its metric identification and by $\\pi:X\\longrightarrow X/{\\sim}$ the canonical projection. ", "page_idx": 21}, {"type": "text", "text": "We directly extend Definition A.22 to the pseudometric case. In order for this definition to make sense in our context, we first need to verify that it provides a well-posed definition of magnitude. This follows from the following lemma. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.3. We assume that the finite pseudometric space $(X,\\rho)$ has magnitude. Then magnitude is independent of the choice of weighting. ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof is straightforward and identical to the metric case. Let $\\beta,\\beta^{\\prime}$ be two weightings, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{a\\in X}\\beta(a)=\\sum_{a\\in X}\\sum_{b\\in X}e^{-\\rho(a,b)}\\beta^{\\prime}(b)\\beta(a)=\\sum_{b\\in X}\\beta^{\\prime}(b)\\sum_{a\\in X}e^{-\\rho(a,b)}\\beta(a)=\\sum_{b\\in X}\\beta^{\\prime}(b).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the following theorem, we show that magnitude is invariant through metric identification. ", "page_idx": 21}, {"type": "text", "text": "Theorem B.4 (Invariance of magnitude through metric identification). $X$ has magnitude if and only $i f\\,X/{\\sim}$ has magnitude, in which case we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{Mag}(X)=\\mathbf{Mag}\\left({X}/{\\sim}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We decompose $X$ into equivalence classes as: ", "page_idx": 21}, {"type": "equation", "text": "$$\nX=\\coprod_{\\bar{a}\\in^{X}/\\sim}\\bar{a}=:\\coprod_{i\\in I}\\bar{a_{i}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\amalg$ denotes disjoint union and the points $(a_{i})_{i\\in I}\\in X^{I}$ represent each equivalence class. We denote  by $\\bar{a}$ the equivalence class of $a\\in X$ . ", "page_idx": 21}, {"type": "text", "text": "Let $\\beta:X\\longrightarrow\\mathbb{R}$ be any function. We have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall a\\in X,\\ \\sum_{b\\in X}e^{-\\rho(a,b)}\\beta(b)=\\sum_{i\\in I}e^{-\\rho(\\bar{a},\\bar{a}_{i})}\\sum_{b\\in\\bar{a}_{i}}\\beta(b).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\Longrightarrow:$ If $X$ has magnitude, then we take $\\beta$ to be a weighting of $X$ , we define: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall\\bar{a}\\in X/{\\sim},\\;\\bar{\\beta}(\\bar{a}):=\\sum_{b\\in\\bar{a}}\\beta(b).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Equation (11), $\\bar{\\beta}$ is a weighting of $X/{\\sim}$ . ", "page_idx": 21}, {"type": "text", "text": "$\\Longleftarrow:$ if $\\bar{\\beta}$ is a weighting of $X/{\\sim}$ , then we define: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall a\\in X,\\;\\beta(a):=\\frac{1}{|\\bar{a}|}\\bar{\\beta}(\\bar{a}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $|\\bar{a}|$ denotes the cardinality of $\\bar{a}$ . By Equation (11), $\\beta$ is a weighting of $X$ . ", "page_idx": 21}, {"type": "text", "text": "B.3 Definition of positive magnitude in the pseudometric case ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let us extend our new notion of positive magnitude in finite pseudometric spaces. This is a rather complicated task. Indeed we need to ensure that the positive magnitude is independent of the choice of weighting, which is not true in general. For this reason, we restrict our definition to pseudometric spaces whose metric identification is positive definite and we choose one particular weighting. ", "page_idx": 22}, {"type": "text", "text": "Definition B.5 (Positive magnitude in finite pseudometric spaces). Let $(X,\\rho)$ be a finite pseudometric space whose metric identification $X/{\\sim}$ is positive definite. Let $\\bar{\\beta}:X/{\\sim}\\longrightarrow\\bar{\\mathbb{R}}$ be a weighting of $X/{\\sim}$ , then we define the positive magnitude of $X$ , denoted PMag, by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathrm{PMag}}(X)=\\sum_{\\bar{x}\\in{\\cal X}/\\sim}\\bar{\\beta}(\\bar{x})_{+},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $x_{+}:=\\operatorname*{max}(x,0)$ denotes the positive part of $x$ . We will say that $X$ admits a positive magnitude if its metric identification $X/{\\sim}$ is positive definite. ", "page_idx": 22}, {"type": "text", "text": "Note that $X/{\\sim}$ admits a unique weighting because it is positive definite. However, $X$ still admits several weightings in general. The above definition ensures that the definition of positive magnitude is independent of any choice of weighting. For the need of our proofs, we will need to introduce weightings in pseudometric spaces, whose sums of positive parts yield the positive magnitude. This is possible by using the following definition, which corresponds to a \u201cgood\u201d choice of weighting in finite pseudometric spaces. ", "page_idx": 22}, {"type": "text", "text": "Definition B.6 (Canonical weighting). Let $(X,\\rho)$ be a finite pseudometric space whose metric identification $X/{\\sim}$ is positive definite. Let $\\bar{\\beta}:\\,x/{\\sim}\\longrightarrow\\mathbb{R}$ be a weighting of $X/{\\sim}$ , we define the canonical weighting $\\bar{\\beta}^{0}:X\\longrightarrow\\mathbb{R}$ on $X$ by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall a\\in X,\\;\\beta^{0}(a):=\\frac{1}{|\\pi(a)|}\\bar{\\beta}(\\pi(a)),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pi:X\\longrightarrow X/{\\sim}$ is the canonical surjection. ", "page_idx": 22}, {"type": "text", "text": "The following lemma is then obvious but crucial to some of our theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.7. With the notation of the previous definition, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathrm{PMag}}(X)=\\sum_{x\\in X}\\beta^{0}(x)_{+}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The next proposition is a consequence of Theorem A.24, it shows that the pseudometrics considered in practice in our work (and in our experiments) admit a positive magnitude. ", "page_idx": 22}, {"type": "text", "text": "Proposition B.8. Let $p\\in[1,2]$ and $S\\in{\\mathcal{Z}}^{n}$ , then every finite subset of $(\\mathbb{R}^{d},\\rho_{S}^{(p)})$ admits a positive magnitude, and therefore it also has a canonical weighting. ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\mathcal{W}:=\\{w_{1},\\ldots,w_{N}\\}$ be a finite set in $\\mathbb{R}^{d}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|L_{S}(w)-L_{S}(w^{\\prime})\\|_{p}=n^{1/p}\\rho_{S}^{(p)}(w,w^{\\prime}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, if we denote by $\\bar{w}$ the equivalence class of $w$ in the metric identification, it is clear that $\\bar{w}=\\bar{w}^{\\prime}\\iff L_{S}(w)=L_{S}(w^{\\prime})$ . Hence, the map $\\varphi_{S}:=n^{-1/p}L_{S}$ naturally extends to an isometry between metric spaces: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{W}/{\\sim}\\stackrel{\\sim}{\\longrightarrow}\\varphi_{S}(\\mathcal{W})\\underset{\\mathrm{finite}}{\\subset}\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Theorem A.24, the finite set $\\varphi_{S}(\\mathcal{W})$ is positive definite, hence it is also the case of $w/\\sim$ . Therefore $\\mathcal{W}$ admits a positive magnitude by definition. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B.4 Warm-up: covering bounds ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The following is deduced from the transcription of the results of [22] to our setting. It is the starting point of our persistent homology-based analysis. ", "page_idx": 22}, {"type": "text", "text": "Theorem B.9. Let $\\rho$ be a pseudometric on $\\mathbb{R}^{d}$ . Suppose that Assumption $^{\\,l}$ holds and that $\\ell$ is $(q,L,\\rho)$ -Lipschitz, for $q\\geq1$ . Then, for all $\\delta>0$ , with probability at least $1-\\zeta$ over $\\mu_{z}^{\\otimes n}\\otimes\\mu_{u}^{\\otimes\\infty}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\leq i\\leq T}G_{S}(w_{i})\\leq2L\\delta+2B\\sqrt{\\frac{2\\log N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\rightarrow T})}{n}}+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The proof of this theorem will be given in the next subsection. Before discussing this proof, a few remarks are in order. ", "page_idx": 23}, {"type": "text", "text": "Covering bounds, such as B.9 have been used in [78, 13, 10, 21] to introduce fractal dimensions (more precisely through the notion of upper box-counting dimension) into the generalization bounds. This is done via the following definition of the aforementioned upper box-counting dimension: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\overline{{\\mathrm{dim}}}}_{\\mathrm{B}}^{\\rho}(X):=\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{sup}_{\\log(1/\\delta)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By using a similar procedure, we see that our framework could be used to introduce intrinsic dimensions associated to a wide range of pseudometrics, as soon as they satisfy a $(q,L,\\rho)$ -Lipschitz continuity assumption. ", "page_idx": 23}, {"type": "text", "text": "However, arguments based on these intrinsic dimensions only make sense in the limit $T\\rightarrow\\infty$ , which makes little sense in practical settings. To address this issue, we take inspiration from two other notions that are equal to the upper box-counting dimension (and therefore lay the ground of the numerical approximation of this dimension), namely the PH-dimension [42, 73, 10, 21] and the magnitude dimension [55, 3]. Our approach is to replace the intrinsic dimensions by the \u201cintermediary quantities\u201d used to define them. This leads to the results presented in the next two subsection. ", "page_idx": 23}, {"type": "text", "text": "B.5 Proof of Theorem B.9 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Before going to the proof of Theorem B.9, we specify our theoretical setup, which is the one introduced in [22]. In this section, we prove our results in the case $T<+\\infty$ . However, note that one could consider $T=+\\infty$ without much technical difficulties. ", "page_idx": 23}, {"type": "text", "text": "The setup is the following: let $(F(\\mathbb{R}^{d}),\\mathcal{T})$ denote the set of all finite subsets of $\\mathbb{R}^{d}$ , endowed with a $\\sigma$ -algebra $\\tau$ . ", "page_idx": 23}, {"type": "text", "text": "We consider the following probability distribution on $F(\\mathbb{R}^{d})$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall A\\in{\\mathcal{T}},\\,\\pi(A):=\\int_{{\\mathcal{Z}}^{n}}\\rho_{S}(A)d\\mu_{z}^{\\otimes n}(S).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As it is discussed in [22, Section 5.4], we make the following technical measure-theoretic assumption. ", "page_idx": 23}, {"type": "text", "text": "Assumption 2. The probability measure $\\mu_{z}^{\\otimes n}$ is a strictly positive Borel measure. Moreover, for every $A\\in\\mathcal T$ , the map $S\\mapsto\\rho_{S}(A)$ is continuous. ", "page_idx": 23}, {"type": "text", "text": "The following example highlights the fact this is a very mild assumption. ", "page_idx": 23}, {"type": "text", "text": "Example B.10. If the data space $\\mathcal{Z}$ is countable and the data distribution $\\mu_{z}$ has no null mass, then the above assumption is automatically satisfied with respect to the discrete topology. ", "page_idx": 23}, {"type": "text", "text": "Theorem B.9. Let $\\rho$ be a pseudometric on $\\mathbb{R}^{d}$ . Suppose that Assumption $^{\\,l}$ holds and that $\\ell$ is $(q,L,\\rho)$ -Lipschitz, for $q\\geq1$ . Then, for all $\\delta>0$ , with probability at least $1-\\zeta$ over $\\mu_{z}^{\\otimes n}\\otimes\\mu_{u}^{\\otimes\\infty}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\leq i\\leq T}G_{S}(w_{i})\\leq2L\\delta+2B\\sqrt{\\frac{2\\log N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\rightarrow T})}{n}}+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let us fix some $\\zeta\\,\\in\\,(0,1)$ . First note that thanks to Assumption 2, we have that $\\rho_{S}$ is absolutely continuous with respect to $\\pi$ , $\\mu_{z}^{\\otimes n}$ -almost surely. Therefore, we can introduce its RadonNykodym derivative, denoted by $d\\rho_{S}/d\\pi$ . ", "page_idx": 23}, {"type": "text", "text": "Thanks to the above notation, we can apply the data-dependent Rademacher complexity bound of [22, Theorem 10] to obtain that with probability at least $1-\\zeta$ , we have, for any $\\lambda>0$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\leq i\\leq T}\\left(\\mathcal{R}(w_{i})-\\widehat{\\mathcal{R}}_{S}(w_{i})\\right)\\leq2\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\rightarrow T},S)+\\frac{1}{\\lambda}\\left(\\frac{d\\rho_{S}}{d\\pi}(\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)\\right)+\\lambda\\frac{9B^{2}}{8n},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with $\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\to T},S)$ a Rademacher complexity term, defined by: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\to T},S):=\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{w\\in\\mathcal{W}_{t_{0}\\to T}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\right],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\epsilon:=(\\epsilon_{1},\\dots,\\epsilon_{n})$ is a vector of independent centered Bernoulli random variables. ", "page_idx": 24}, {"type": "text", "text": "By [22, Lemma 16], we have almost surely that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{d\\rho_{S}}{d\\pi}(\\mathcal{W}_{t_{0}\\rightarrow T})\\leq\\mathrm{I}_{\\infty}(\\mathcal{W}_{t_{0}\\rightarrow T},S).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, by optimizing the choice of the parameter $\\lambda$ in the above equation, we have that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\leq i\\leq T}\\Big(\\mathcal{R}(w_{i})-\\widehat{\\mathcal{R}}_{S}(w_{i})\\Big)\\leq2\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\rightarrow T},S)+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now perform a covering argument very similar to classical covering arguments for Rademacher complexity[77]. Let us fix some $\\delta>0$ and introduce $(x_{1},\\hdots,x_{N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})})$ the centers of a minimal $\\delta$ -covering of $\\mathcal{W}_{t_{0}\\rightarrow T}$ for pseudometric $\\rho$ . For any $w\\in\\mathcal{W}_{t_{0}\\to T}$ , there exists $j$ such that $\\rho(w,x_{j})\\le\\delta$ . Therefore we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{w\\in\\mathcal{W}_{t_{0}\\to T}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\leq\\displaystyle\\operatorname*{sup}_{1\\leq j\\leq N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(x_{j},z_{i})+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}(\\ell(w,z_{i})-\\ell(x_{j},z_{i}))}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{1\\leq j\\leq N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(x_{j},z_{i})+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}|\\ell(w,z_{i})-\\ell(x_{j},z_{i})|}\\\\ &{\\leq\\displaystyle\\operatorname*{sup}_{1\\leq j\\leq N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(x_{j},z_{i})+n^{-1/q}\\left\\|L_{S}(w)-L_{S}(x_{j})\\right\\|_{q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last line comes from H\u00f6lder\u2019s inequality. ", "page_idx": 24}, {"type": "text", "text": "We can now apply Massart\u2019s lemma on the first term and the $(q,L,\\rho)$ -Lipschitz continuity of $\\ell$ on the second term, this gives us: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\to T},S)\\leq L\\delta+B\\sqrt{\\frac{2\\log N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})}{n}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 24}, {"type": "text", "text": "B.6 Persistent homology bounds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We now present the proofs of our persistent homology-based bounds, ie, the results of section 3.2.   \nThe following lemma is a pseudometric version of a classical result of fractal geometry [26]. ", "page_idx": 24}, {"type": "text", "text": "Lemma B.11 (Covering and packing in pseudometric spaces). Let $(X,\\rho)$ be a pseudometric space, $\\delta>0$ , and $\\left\\{x_{1},\\ldots,x_{P_{\\delta}(X)}\\right\\}$ a maximal $\\delta$ -packing of $X$ for pseudometric $\\rho$ . Then we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\nN_{2\\delta}^{\\rho}(X)\\leq P_{\\delta}^{\\rho}(X).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Let us fix $\\delta>0$ and let $(x_{1},\\ldots,x_{P_{\\delta}^{\\rho}(X)})$ be centers of a maximal packing of $X$ with closed $\\delta$ -balls. Let us assume that: ", "page_idx": 24}, {"type": "equation", "text": "$$\nX\\backslash\\bigcup_{1\\leq i\\leq P_{\\delta}^{\\rho}(X)}\\bar{B}_{2\\delta}(x_{i})\\neq\\emptyset,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so that we can take some $x_{0}$ belonging to the above non-empty set. Now let us fix $i\\in\\{1,\\ldots,P_{\\delta}^{\\rho}(X)\\}$ and $w\\in\\bar{B}_{\\delta}(x_{i})$ . By the triangle inequality and the definition of $w$ and $x_{0}$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underbrace{\\rho(x_{0},x_{i})}_{>2\\delta}\\leq\\rho(x_{0},w)+\\underbrace{\\rho(w,x_{i})}_{\\leq\\delta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we have $\\rho(x_{0},w)\\;>\\;\\delta$ , and hence $\\bar{B}_{\\delta}(x_{i})\\cap\\bar{B}_{\\delta}(x_{0})$ , so that we construct a bigger $\\delta$ -packing, by adding $x_{0}$ to $(x_{1},\\ldots,x_{P_{\\delta}^{\\rho}(X)})$ , which is absurd. ", "page_idx": 25}, {"type": "text", "text": "Therefore, we have $\\begin{array}{r}{X\\backslash\\bigcup_{1\\leq i\\leq P_{\\delta}^{\\rho}(X)}\\bar{B}_{2\\delta}(x_{i})=\\emptyset}\\end{array}$ , hence the result. ", "page_idx": 25}, {"type": "text", "text": "The next lemma asserts that $E_{\\alpha}$ is increasing (with respect to the inclusion of sets), if and only if $\\alpha\\leq1$ . This is the reason why we require $\\alpha\\in[0,1]$ in Theorem 3.4. ", "page_idx": 25}, {"type": "text", "text": "Lemma B.12. Let $(X,\\rho)$ be a non-empty finite pseudometric space, $\\alpha\\in[0,1]$ and $\\delta>0$ . Then we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{\\alpha}^{\\rho}(X)\\geq\\frac{P_{\\delta}^{\\rho}(X)}{2}\\delta^{\\alpha}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We refer to Figure 5 for a graphical illustration of the main technical elements of this proof. ", "page_idx": 25}, {"type": "text", "text": "In the case where $P_{\\delta}^{\\rho}(X)\\,=\\,1$ , the result is obvious. In the rest of the proof we assume that $P_{\\delta}^{\\rho}(X)\\geq2$ ", "page_idx": 25}, {"type": "text", "text": "In all the following, we fix $\\alpha\\in[0,1]$ and $\\delta>0$ . We also denote $P:=P_{\\delta}^{\\rho}(X)$ . Without loss of generality, we can assume $P\\geq2$ . ", "page_idx": 25}, {"type": "text", "text": "We fix $\\tau$ an MST of $X$ , represented by a set of edges denoted $x\\to y$ , with $\\gamma,y\\in X^{2}$ (note that we identify $x\\to y$ and $y\\rightarrow x$ ). It is a classical result that there are $|X|-1$ edges. For an edge $e$ of the form $a\\to b$ , we denote its length by $|e|:=\\rho(a,b)$ . ", "page_idx": 25}, {"type": "text", "text": "For $a,b\\in X$ , with $a\\neq b$ , we denote by $\\{a\\rightarrow b\\}$ the shortest path between $a$ and $b$ . More precisely, we represent it as a list of edges, denoted $a\\,=\\,a_{0}\\,\\rightarrow\\,a_{1}\\cdot\\cdot\\cdot\\,\\rightarrow\\,a_{K}\\,=\\,b$ , for some $K$ . When the context is clear, we identify $\\{\\bar{a}\\to b\\}$ to the set of its edges $a\\rightarrow b$ . ", "page_idx": 25}, {"type": "text", "text": "Let us introduce $(x_{1},\\ldots,x_{P})$ a maximal $\\delta$ -packing of $X$ by closed. ", "page_idx": 25}, {"type": "text", "text": "For every $i\\in\\{1,\\ldots,P\\}$ , as $\\tau$ is connected, there exists $y_{i}\\in X$ such that $y_{i}\\notin\\bar{B}_{\\delta}(x_{i})$ and $y_{i}$ is the only point in the path $\\{x_{i}\\to y_{i}\\}$ that does not belong to the ball $\\bar{B}_{\\delta}(x_{i})$ . ", "page_idx": 25}, {"type": "text", "text": "For each $i$ , we denote $e_{i}$ the only edge in $\\{x_{i}\\to y_{i}\\}$ to which $y_{i}$ belongs, i.e. $e_{i}$ is of the form $z_{i}\\to y_{i}$ , with $z_{i}\\in\\bar{B}_{\\delta}(x_{i})$ . By construction, those edges $e_{i}$ are the only ones that can be shared by several paths $\\{x_{i}\\to y_{i}\\}$ . ", "page_idx": 25}, {"type": "text", "text": "Let us introduce the following set of indices: ", "page_idx": 25}, {"type": "equation", "text": "$$\nI:=\\left\\{i\\in\\{1,\\dots,P\\}\\,,\\,\\forall j\\neq i,\\,e_{i}\\not\\in\\{x_{j}\\to y_{j}\\}\\right\\},\\quad K:=\\{1,\\dots,P\\}\\setminus{I}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let us consider $i\\in K$ . Let us assume that we have $j,j^{\\prime}\\in\\{1,\\ldots,P\\}$ such that $e_{i}\\in\\{x_{j}\\rightarrow y_{j}\\}$ and $e_{i}\\in\\{x_{j^{\\prime}}\\to y_{j^{\\prime}}\\}$ . If we denote $e_{i}$ as $z_{i}\\to y_{i}$ , we have that $z_{i}\\in\\bar{B}_{\\delta}(x_{i})$ , by definition of $y_{i}$ . Therefore, by definition of $y_{j}$ , we have $z_{i}=y_{j}$ (because $\\bar{B}_{\\delta}(x_{i})\\cap\\bar{B}_{\\delta}(x_{j})=\\emptyset)$ . We have similarly $z_{i}=y_{j^{\\prime}}$ and thus $y_{j}=y_{j^{\\prime}}$ . By definition of $y_{j}$ and $y_{j^{\\prime}}$ we also have $y_{i}\\in\\bar{B}_{\\delta}(x_{j})\\cap\\bar{B}_{\\delta}(x_{j^{\\prime}})$ , which is absurd, by definition of packing. We conclude the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall k\\in K,\\ \\exists!j\\neq i,\\ e_{i}\\in\\left\\{x_{j}\\rightarrow y_{j}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $k\\in K$ , we denote the corresponding $j$ by $\\varphi(k)$ . ", "page_idx": 25}, {"type": "text", "text": "By definition of $K$ , it is clear that $\\varphi(k)\\in K$ . Moreover, as $y_{\\varphi(i)}=z_{i}\\in\\bar{B}_{\\delta}(x_{i})$ , this implies that $\\varphi^{2}(i)=i$ . Therefore, we have constructed an involution, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\varphi:K\\longrightarrow K,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "such that $\\forall k\\in K,\\,\\varphi(k)\\neq k.$ . This implies that the cardinality of $K$ is even and that we can write $K=K_{1}\\coprod K_{2}$ , with: ", "page_idx": 25}, {"type": "equation", "text": "$$\n|K_{1}|=|K_{2}|,\\quad\\varphi(K_{1})=K_{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The outcome of this construction is that we now have disjoint paths given by the $(x_{i}\\to y_{i})_{i\\in I}$ and the $(x_{k}\\to x_{\\varphi(k)})_{k\\in K_{1}}$ . Therefore, we get the following lower bound on $E_{\\alpha}(X)$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{\\alpha}(X)\\geq\\sum_{i\\in I}\\sum_{e\\in\\{x_{i}\\rightarrow y_{i}\\}}|e|^{\\alpha}+\\sum_{k\\in K_{1}}\\sum_{e\\in\\{x_{k}\\rightarrow x_{\\varphi(k)}\\}}|e|^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As $\\alpha\\in[0,1]$ , we have that: ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{\\alpha}(X)\\geq\\sum_{i\\in I}\\left(\\sum_{e\\in\\{x_{i}\\rightarrow y_{i}\\}}|e|\\right)^{\\alpha}+\\sum_{k\\in K_{1}}\\left(\\sum_{e\\in\\{x_{k}\\rightarrow x_{\\varphi(k)}\\}}|e|\\right)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the triangle inequality, and by definition of packing, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{\\alpha}(X)\\geq\\sum_{i\\in I}\\delta^{\\alpha}+\\sum_{k\\in K_{1}}\\delta^{\\alpha}=\\delta^{\\alpha}(|I|+|K_{1}|)\\geq\\frac12P_{\\delta}^{\\rho}(X)\\delta^{\\alpha},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 26}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/88b6980dc96743472de79264846a98d8863694157508c59d1a73b16312fe2d96.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 5: Geometric representation of the proof of Lemma B.12. It represents a point cloud $(w_{i})_{i}$ , the centers of the 3 packing balls (blue) and the minimum spanning tree $T$ (red), so that the sum of the lengths of the edges of $T$ is exactly $E_{1}$ , see Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Theorem 3.4. Let $\\rho$ be a pseudometric on $\\mathbb{R}^{d}$ . Supposes that Assumption $^{\\,l}$ holds and that $\\ell$ is $(q,L,\\rho)$ -Lipschitz, for $q\\geq1$ . Then, for all $\\alpha\\in[0,1]$ , with probability at least $1-\\zeta$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\le i\\le T}G_{S}(w_{i})\\le2B\\sqrt{\\frac{2\\log{(1+K_{n,\\alpha}{\\bf E}_{\\alpha}^{\\rho})}}{n}}+\\frac{2B}{\\sqrt{n}}+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\to T})+\\log(1/\\zeta)}{2n}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $K_{n,\\alpha}:=2\\left(2L\\sqrt{n}/B\\right)^{\\alpha}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. For better clarity, we assume $T<+\\infty$ . Let us fix some $\\zeta\\in(0,1)$ , $\\delta>0$ , and $\\alpha\\geq0$ . By Theorem B.9, we have, with probability at least $1-\\zeta$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leqslant i<T}\\Big(\\mathcal{R}(w_{i})-\\widehat{\\mathcal{R}}_{S}(w_{i})\\Big)\\leq2L\\delta+2B\\sqrt{\\frac{2\\log N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\rightarrow T})}{n}}+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now bound the covering number appearing in the above equation. By Lemma B.12, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\cal E}_{\\alpha}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})\\geq2^{-\\alpha-1}\\left[P_{\\delta/2}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})-1\\right]\\delta^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, by Lemma B.11, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{\\alpha}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})\\geq2^{-\\alpha-1}\\left[N_{\\delta}^{\\rho}(\\mathcal{W}_{t_{0}\\to T})-1\\right]\\delta^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now combine this with our generalization bound by choosing the value: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\delta:={\\frac{B}{L{\\sqrt{n}}}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and we get that with probability at least $1-\\zeta$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\operatorname*{sup}_{t_{0}\\leq i\\leq T}\\Big(\\mathcal{R}(w_{i})-\\widehat{\\mathcal{R}}_{S}(w_{i})\\Big)\\leq\\displaystyle\\frac{2B}{\\sqrt{n}}+2B\\sqrt{\\frac{2\\log(1+K_{n,\\alpha}E_{\\alpha}^{\\rho}(\\mathcal{W}_{t_{0}\\rightarrow T}))}{n}}}\\\\ {+\\,3B\\sqrt{\\frac{\\Gamma_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)}{2n}},~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with: ", "page_idx": 27}, {"type": "equation", "text": "$$\nK_{n,\\alpha}:=2\\left({\\frac{2L{\\sqrt{n}}}{B}}\\right)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It leads to the desired result. ", "page_idx": 27}, {"type": "text", "text": "B.7 Proof of the magnitude-based generalization bounds ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma B.13. Let $\\mathcal{W}\\subset\\mathbb{R}^{d}$ be a finite set and $\\epsilon:=(\\epsilon_{1},\\ldots,\\epsilon_{n})$ and $\\rho$ a pseudometric such that $(\\boldsymbol{\\mathscr{W}},\\lambda\\rho)$ admits a positive magnitude (according to Definition B.5) for every $\\lambda>0$ . We assume that $\\ell$ is $(L,q,\\rho)$ -Lipschitz continuous with $q\\in[1,2]$ . Then, for any $\\lambda>0$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\exp\\left\\{\\frac{\\lambda}{n}\\operatorname*{sup}_{w\\in\\mathcal{W}}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\right\\}\\right]\\leq e^{\\frac{\\lambda^{2}B^{2}}{2n}}\\mathbf{PMag}\\left((L\\lambda)\\mathcal{W}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where PMag is the positive magnitude, see Appendix B.3 ", "page_idx": 27}, {"type": "text", "text": "Proof. We first remark that, by H\u00f6lder\u2019s inequality and the $(L,q,\\rho)$ -Lipschitz condition, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall w,w^{\\prime}\\in\\mathcal{W},\\ \\rho_{S}(w,w^{\\prime})\\leq n^{-1/q}\\left\\|L_{S}(w)-L_{S}(w^{\\prime})\\right\\|_{q}\\leq L\\rho(w,w^{\\prime}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let us fix some $\\lambda\\,>\\,0$ . As $(\\boldsymbol{\\mathscr{W}},\\lambda\\rho)$ admits a positive magnitude, we can introduce a canonical weighting $\\beta:\\mathcal{W}\\longrightarrow\\mathbb{R}$ . By definition of a weighting, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall a\\in\\mathcal{W},\\;\\;\\sum_{b\\in\\mathcal{W}}e^{-\\lambda\\rho(a,b)}\\beta(b)=1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, for any $\\epsilon\\in\\{-1,1\\}^{n}$ , we introduce: ", "page_idx": 27}, {"type": "equation", "text": "$$\na_{\\epsilon}:=\\operatorname{argmax}_{a\\in\\mathcal{W}}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(a,z_{i}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "With those notations, we can compute: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1\\leq\\displaystyle\\sum_{b\\in\\mathcal{W}}e^{-\\lambda\\rho(a_{c},b)}\\beta_{+}(b)}\\\\ &{\\leq\\displaystyle\\sum_{b\\in\\mathcal{W}}e^{-\\frac{1}{\\lambda}\\rho_{S}(a_{c},b)}\\beta_{+}(b)}\\\\ &{=\\displaystyle\\sum_{b\\in\\mathcal{W}}\\exp\\left\\{-\\frac{\\lambda}{L n}\\sum_{i=1}^{n}|\\xi(a_{c},z_{i})-\\ell(b,z_{i})|\\right\\}\\beta_{+}(b)}\\\\ &{\\leq\\displaystyle\\sum_{b\\in\\mathcal{W}}\\exp\\left\\{-\\frac{\\lambda}{L n}\\sum_{i=1}^{n}\\epsilon_{i}(\\ell(a_{c},z_{i})-\\ell(b,z_{i}))\\right\\}\\beta_{+}(b)}\\\\ &{=\\exp\\left\\{-\\frac{\\lambda}{L n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(a_{c},z_{i})\\right\\}\\displaystyle\\sum_{b\\in\\mathcal{W}}\\exp\\left\\{\\frac{\\lambda}{L n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(b,z_{i})\\right\\}\\beta_{+}(b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, by dividing by the first term on the right-hand side and using the independence of the $\\epsilon_{i}$ , we deduce that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\epsilon}\\left[\\exp\\left\\{\\displaystyle\\frac{\\lambda}{L n}\\operatorname*{sup}_{w\\in\\mathcal{W}}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\right\\}\\right]\\leq\\mathbb{E}_{\\epsilon}\\left[\\sum_{b\\in\\mathcal{W}}\\exp\\left\\{\\displaystyle\\frac{\\lambda}{L n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(b,z_{i})\\right\\}\\beta_{+}(b)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\sum_{b\\in\\mathcal{W}}\\prod_{i=1}^{n}\\mathbb{E}_{\\epsilon}\\left[e^{\\frac{\\lambda}{L n}\\epsilon_{i}\\ell(b,z_{i})}\\right]\\beta_{+}(b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Hoeffding\u2019s lemma, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{\\epsilon}\\left[\\exp\\left\\{\\frac{\\lambda}{L n}\\operatorname*{sup}_{w\\in\\mathcal{W}}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\right\\}\\right]\\leq e^{\\frac{\\lambda^{2}B^{2}}{2n L^{2}}}\\sum_{b\\in\\mathcal{W}}\\beta_{+}(b)}\\\\ &{}&{=e^{\\frac{\\lambda^{2}B^{2}}{2n L^{2}}}\\mathbf{PMag}\\left(\\lambda\\mathcal{W}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The result follows by the change of variable $\\lambda=\\Lambda L$ . ", "page_idx": 27}, {"type": "text", "text": "Theorem 3.5. Let \u03c1 be a pseudometric such that $(\\boldsymbol{\\mathscr W},\\lambda\\rho)$ admits a positive magnitude (according to Definition B.5) for every $\\lambda>0$ . We assume that $\\ell$ is $(q,L,\\rho)$ -Lipschitz continuous with $q\\geq1$ . Then, for any $s>0$ , we have with probability at least $1-\\zeta$ that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\leq i\\leq T}G_{S}(w_{i})\\leq\\frac{2}{s}\\log\\mathbf{PM}\\mathbf{ag}^{\\rho}\\left(L s\\mathcal{W}_{t_{0}\\rightarrow T}\\right)+s\\frac{B^{2}}{n}+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}\\big(S,\\mathcal{W}_{t_{0}\\rightarrow T}\\big)+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. The beginning of the proof is completely similar to the proof of B.9 up to Equation (13). More precisely, we have that with probability at least $1-\\zeta$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\leq i\\leq T}\\Big(\\mathcal{R}(w_{i})-\\widehat{\\mathcal{R}}_{S}(w_{i})\\Big)\\leq2\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\rightarrow T},S)+3B\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Jensen\u2019s inequality, we have, for all $\\lambda>0$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname{Rad}(\\ell,\\mathcal{W}_{t_{0}\\to T},S)\\leq\\frac{1}{\\lambda}\\log\\mathbb{E}_{\\epsilon}\\left[\\exp\\left\\{\\frac{\\lambda}{n}\\operatorname*{sup}_{w\\in\\mathcal{W}_{t_{0}\\to T}}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we can apply Lemma B.13 to write that, for all $s>0$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Rad}(\\ell,\\mathcal{W}_{t_{0}\\to T},S)\\le s\\frac{B^{2}}{2n}+\\frac{1}{s}\\log\\mathbf{PMag}\\left(L s\\mathcal{W}_{t_{0}\\to T}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We deduce that for all $s>0$ , we have with probability at least $1-\\zeta$ that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq i\\leq T}\\Big(\\mathcal{R}(w_{i})-\\widehat{\\mathcal{R}}_{S}(w_{i})\\Big)\\leq s\\frac{B^{2}}{n}+\\frac{2}{s}\\log\\mathbf{PMag}\\left(L s\\mathcal{W}_{t_{0}\\rightarrow T}\\right)+\\sqrt{\\frac{\\mathrm{I}_{\\infty}(S,\\mathcal{W}_{t_{0}\\rightarrow T})+\\log(1/\\zeta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Remark B.14 (Link between magnitude and positive magnitude). Let $\\boldsymbol{\\mathcal{W}}\\subset\\mathbb{R}^{M}$ be a finite set (for some $M$ ), of cardinality $N$ , and $\\rho$ a metric on $\\mathcal{W}$ . If we denote the similarity matrix, for a given value of $s>0$ , by $M_{s}(a,b)=e^{-\\rho(a,b)}$ , then it is clear that: ", "page_idx": 28}, {"type": "equation", "text": "$$\nM_{s}\\underset{s\\rightarrow\\infty}{\\longrightarrow}I_{N}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, by continuity of the inverse, this implies that the weighting associated to $s\\ >\\ 0$ , i.e. $\\beta_{s}:\\mathcal{W}\\to\\mathbb{R}$ , satisfy: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall a\\in\\mathcal{W},\\;\\beta_{s}(a)\\underset{s\\to\\infty}{\\longrightarrow}1.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From this, we first deduce that, for $s\\to\\infty$ , we have $\\mathbf{Mag}^{\\rho}(s\\mathcal{W})\\to N.$ . Moreover, by continuity of the inverse, this means that, up to a certain $s$ , the weighting $(\\beta_{s}(a))_{a\\in\\mathcal{W}}$ only has positive elements. Therefore, this implies that, for $s$ big enough, one has $\\mathbf{Mag}^{\\rho}(s\\mathcal{W})=\\mathbf{PMag}^{\\rho}(s\\mathcal{W})$ . ", "page_idx": 28}, {"type": "text", "text": "Thanks to our definitions for positive magnitude in pseudometric spaces, given in Appendix B.3, this observation extends to the pseudometric case. ", "page_idx": 28}, {"type": "text", "text": "Remark B.15 (Extension to infinite sets). There exist extensions of the definition of magnitude beyond finite sets [55, 56]. More specifically, weightings are then represented by measures on the set. It is clear from the above proofs that we can extend the positive magnitude in this setting and that the proof would follow similar lines. Therefore, our theory provides upper bounds of Rademacher complexity in terms of positive magnitude in more general cases than the one we use in this work. ", "page_idx": 28}, {"type": "text", "text": "In particular, the present reasoning could be extended to compact random sets $\\mathcal{W}$ . The next lemma is the extension of Lemma B.13 to the compact setting. The proof follows very similar lines as Lemma B.13. ", "page_idx": 28}, {"type": "text", "text": "Lemma B.16. Let us fix $S\\in{\\mathcal{Z}}^{n}$ and consider a set $W\\subset\\mathbb{R}^{d}$ that is compact8 with respect to the pseudometric \u03c1S. For every $\\lambda>0$ , we assume that $A$ possesses a weighting $\\mu_{\\lambda}$ (which is a finite measure on $A$ ) with respect to pseudometric $\\lambda\\rho_{S}$ , in the sense of [56, Definition 3.3]. Then we have, for any $\\lambda>0$ : ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\exp\\left\\{\\frac{\\lambda}{n}\\operatorname*{sup}_{a\\in A}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(w,z_{i})\\right\\}\\right]\\leq e^{\\frac{\\lambda^{2}B^{2}}{2n}}\\mathbf{PMag}\\left(\\lambda A\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where PMag is the positive magnitude of the compact space $(A,\\lambda\\rho_{S})$ which, according to [56], can be defined as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\bf P M a g}\\,(\\lambda A)=(\\mu_{\\lambda})_{+}\\,(A),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\left(\\mu_{\\lambda}\\right)_{+}$ denotes the positive part of the measure $\\mu_{\\lambda}$ . ", "page_idx": 29}, {"type": "text", "text": "C Additional Experimental Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we give additional details regarding the models, datasets, and hyperparameters used in our experiments. ", "page_idx": 29}, {"type": "text", "text": "C.1 Experimental setting ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "C.1.1 Vision Transformers Architecture and implementation details ", "text_level": 1, "page_idx": 29}, {"type": "table", "img_path": "6U5fCHIWOC/tmp/1c7cb88ce2ec06acfaac32beafde4df0df7e917725489f367f022150e9b47aa7.jpg", "table_caption": ["Table 2: Architecture details for the vision transformers (taken from [29]). WS refers to Window Size. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "The design of the ViT has been modified to accommodate for the small datasets as per [68]. Our implementation is based on the [29], which is based on the timm library with the architecture parameters presented in Table 2. The implementation of Swin is based on the Swin-Transformer libarary and the implementation of CaiT is predominantly based on the timm library with some modifications. The full version can be found in the supplementary code. ", "page_idx": 29}, {"type": "text", "text": "Instead of training from scratch, which is extremely time-consuming, we used the pre-trained weights available from the GitHub repository of the paper [29], we further fintetuned them for 100 epochs on the dataset CIFAR10 or CIFAR100 to achieve the optimum performance reported in the paper [29]. Then we verified that the finetuned weights achieved $100\\%$ training performance, and then they were the starting point of our computational framework. We ran the transformer experiments on 18 NVIDIA 2080Ti GPUs, and the graph experiments on 18 Intel Xeon Silver 4114 CPUs. ", "page_idx": 29}, {"type": "text", "text": "C.1.2 GNN Architecture and implementation details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We will briefly talk about the details of GraphSage [32] and GatedGCN [12], prior works we use in our experiments. GraphSage [32] is an improvement over the GCN (Graph ConvNets) model [41] and it incorporates each node\u2019s own features from the previous layer in an explicit way by the update equation: ", "page_idx": 29}, {"type": "equation", "text": "$$\nh_{i}^{l+1}=\\mathrm{ReLU}\\big(U^{l}\\mathrm{Concat}(h_{i}^{l},\\mathrm{Mean}_{j\\in N_{i}}h_{j}^{l})\\big),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $N_{i}$ is the neighbourhood of node $i$ , $h_{i}^{l}$ is the feature vector and $U^{l}\\,\\in\\,\\mathbb{R}^{d\\times2d}$ . We use the graph-pooling version of GraphSage, with the following update equation: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{i}^{l+1}=\\mathrm{ReLU}(U^{l}\\mathrm{Concat}(h_{i}^{l},\\mathrm{Max}_{j\\in N_{i}}\\mathrm{ReLU}(V^{l}h_{j}^{l}))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\nh_{i}^{l+1}=h_{i}^{l}+\\ensuremath{\\mathrm{ReLU}}(\\ensuremath{\\mathrm{BN}}(U^{l}h_{i}^{l}+\\sum_{j\\in N_{i}}e_{i j}^{l}\\odot V^{l}h_{i}^{l})),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $U^{l},V^{l}\\ \\in\\ \\mathbb{R}^{d\\times d}$ , $\\odot$ is the Hadamard product, and the edge gates $e_{i j}^{l}$ have the following definitions: ", "page_idx": 30}, {"type": "equation", "text": "$$\ne_{i j}^{l}=\\frac{\\sigma(\\hat{e}_{i j}^{l})}{\\sum_{j^{\\prime}\\in N_{i}}\\sigma(\\hat{e}_{i j}^{l})+\\epsilon},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{e}_{i j}^{l}=\\hat{e}_{i j}^{l-1}+\\mathrm{ReLU}(\\mathrm{BN}(A^{l}h_{i}^{l-1}+B^{l}h_{i}^{l-1}+C^{l}\\hat{e}_{i j}^{l-1})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\sigma$ is the sigmid funciton, $\\epsilon$ is a small constant for numerical stability, $A^{l},B^{l},C^{l}\\in\\mathbb{R}^{d\\times d}$ , and BN stands for Batch Normalization. ", "page_idx": 30}, {"type": "text", "text": "We used the code provided by [23], which relies on the $\\mathtt{d g l}$ library implementation of GraphSage and GatedGCN. We trained GraphSage and GatedGCN until $100\\%$ training accuracy, following the setup in [23]. All experiments were ran on 18 Intel Xeon Silver 4114 CPUs. Each experiment (one fixed batch size and learning rate) was run on a single CPU and 18 experiments were run on the server at any given time (on different CPUs). ", "page_idx": 30}, {"type": "text", "text": "C.2 Hyperparameter details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Hyperparameters shared among experiments.. For the Vision Transformers experiments, we varied the learning rate range $[10^{-5},\\bar{1}0^{-3}]$ , and batch size in the range [8, 256]. For the graph experiments, $[10^{-\\overline{{6}}},10^{-4}]$ , and batch size in the range [8, 256]. For all experiments, we used 0.1 proportion of the training data for the computation of the pseudo matrix, apart from CaiT and Swin on CIFAR100, where we used 0.09 proportion of the training data due to memory constraints. All experiments use a $6\\times6$ grid of hyperparameters which is specified as follows. ", "page_idx": 30}, {"type": "text", "text": "ViT on CIFAR10. We selected 6 values for the learning rate in the range $[10^{-5},10^{-3}]$ , and the batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 30}, {"type": "text", "text": "ViT on CIFAR100. We selected 6 values for the learning rate in the range $[10^{-5},10^{-3}]$ , and the batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 30}, {"type": "text", "text": "CaiT on CIFAR10. We selected 6 values for the learning rate in the range $[10^{-5},10^{-3}]$ , batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 30}, {"type": "text", "text": "CaiT on CIFAR100. We selected 6 values for the learning rate in the range $[10^{-5},10^{-3}]$ , batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $9\\%$ (see section 4). ", "page_idx": 30}, {"type": "text", "text": "Swin on CIFAR10. We selected 6 values for the learning rate in the range $[10^{-5},10^{-3}]$ , batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 30}, {"type": "text", "text": "Swin on CIFAR100. We selected 6 values for the learning rate in the range $[10^{-5},10^{-3}]$ , batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $9\\%$ (see section 4). ", "page_idx": 30}, {"type": "text", "text": "GatedGCN. We selected 6 values for the learning rate in the range $[10^{-6},10^{-4}]$ , the batch size between [8, 256] and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). We note that for due to time constraints, the experiments with batch sizes of 8 and 256 for the Euclidean metric were not complete. ", "page_idx": 30}, {"type": "text", "text": "GraphSage. We selected 6 values for the learning rate in the range $[10^{-6},10^{-4}]$ , the batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 30}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/a28155875287a91dec2e2439fc31a23e7b6c6c484fc17ea7a397cffcac541b5b.jpg", "img_caption": ["Figure 6: Comparison of topological complexities for different models, datasets, and pseudometrics. The results are a visual representation of some results from Table 1, they complete Fig. 1.c in the main par of the paper. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "ViT on CIFAR10 (Adam). We selected 6 values for the learning rate in the range $[10^{-5},10^{-3}]$ , and the batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 31}, {"type": "text", "text": "ViT on CIFAR10 (SGD). We selected 6 values for the learning rate in the range $[5\\times10^{-3},10^{-1}]$ , and the batch size between [8, 256], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 31}, {"type": "text", "text": "ViT on CIFAR10 (RMSprop). We selected 6 values for the learning rate in the range $[10^{-6},10^{-3}]$ , and the batch size between [8, 512], and data proportion for the computation of the pseudo-distance $(\\rho_{S})$ of $10\\%$ (see section 4). ", "page_idx": 31}, {"type": "text", "text": "D Additional experimental results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we present additional empirical results, in addition to what was already presented in the main part of this document. We divide this section into three parts. Additional graphical representation of our main experimental results is presented in Appendix D.1. Then, we quickly explore in Appendix D.2 additional ablation studies and comparison of our proposed topological complexities with a complexity notion that is more standard in the literature, namely gradient variance [37]. In Appendix D.3 we report additional experiments based on vision transformers and in Appendix D.4 we include additional illustrations of the GNN experiments. ", "page_idx": 31}, {"type": "text", "text": "D.1 Additional graphical representations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we include additional bar plots, shown in Figure 6, which are meant to provide more visual support to understand the results of Table 1. Figure 6 completes the bar plots shown in Figure 1. ", "page_idx": 31}, {"type": "text", "text": "D.2 Further ablations and comparison with other complexity metrics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "D.2.1 About the final accuracy gap and the worst accuracy gap ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Our main theoretical results, presented in section 3, apply to the worst-case generalization error over the trajectory, i.e. on the quantity $\\begin{array}{r}{\\operatorname*{sup}_{t_{0}\\leq k\\leq T}\\Big(\\mathcal{R}(w_{k})-\\widehat{\\mathcal{R}}_{S}(w_{k})\\Big)}\\end{array}$ . However, computing this quantity over the whole trajectory may be extremely expensive as it requires evaluating the model on the whole dataset at each iteration (this is a similar problem to the one encountered for the computation of the data-dependent distance matrices, discussed in section 4). Previous studies on worst-case TDA-inspired generalization bounds circumvented this issue by reporting the final accuracy gap as the \u201cgeneralization error\u201d in their experiments (as it is the case in our work, most existing experiments consist of classification tasks). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "In our work, we argue that the true worst-case generalization error may however have a different behavior than the final accuracy gap. In order to estimate this quantity in a computationally friendly way, we used the following procedure: we periodically estimated the test accuracy during the training, computed its minimum value $\\mathrm{\\acc{_{\\mathrm{{test-worst}}}}}$ and substracted it from the final train accuracy (acctrain-final) to obtain the \u201cgeneralization gap\u201d ${\\widehat{G}}_{S}$ reported in our main experiments, i.e., ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{G}_{S}:=\\mathrm{acc}_{\\mathrm{train-final}}-\\mathrm{acc}_{\\mathrm{test-worst}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that in addition to being a good proxy to the true error appearing in our theory, the above quantity could be of independent experimental interest. ", "page_idx": 32}, {"type": "text", "text": "In order to assess that our main conclusions remain valid if the final accuracy gap is used instead of ${\\widehat{G}}_{S}$ , we present here a few additional experiments using the final accuracy gap as a generalization measure (it is denoted Accuracy gap in the figures.) In the case of a ViT on CIFAR10, this is shown in Figure 7 and Figure 8. We observe that our proposed topological complexities also correlate very well with the final accuracy gap, and outperform the previously proposed PH dimensions [10, 21]. ", "page_idx": 32}, {"type": "text", "text": "In addition to these findings, we make two additional new observations. First, the Ph dim, while outperformed by our proposed metric, has better granulated Kendall\u2019s coefficients when compared to the final accuracy gap than the worst generalization error ( $\\Psi$ goes from 0.20 to 0.36). This may explain why we observed poor performance of PH-dim in Figure 4a. Second, we observe that the correlation seems to be slightly less good with the final accuracy gap, especially for high learning rates, which seems to be similar behavior to what was reported in [21]. ", "page_idx": 32}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/28816ebbdced2185633804e34c6a6a6ed2fb3e2a2bdfb2760db0587ee0e2542d.jpg", "img_caption": ["Figure 7: ViT on CIFAR10 with $\\rho_{S}$ -pseudometric, using the final accuracy gap as a generalization measure. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/177bbcdc35641258b0cddafbd1edc545a539df92831400309a5cb5180a25085a.jpg", "img_caption": ["Figure 8: ViT on CIFAR10 with 01-pseudometric, using the final accuracy gap as a generalization measure. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "D.2.2 Sensitivity to the scale parameter in magnitude experiments ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "As is explicitly shown by Theorem 3.5, using (positive) magnitude as a topological complexity requires choosing the scale parameter $s>0$ . In our main experiments, we experimented with both $s={\\sqrt{n}}$ (justified to obtain the expected $1/\\sqrt{n}$ in the generalization bound) and $s=0.01$ (in order to compare with using a small value for $s$ ). We can see in Table 1 that both settings give relatively satisfactory results. Note that in our setting we have ${\\sqrt{n}}\\approx223.6$ . ", "page_idx": 33}, {"type": "text", "text": "We present in Figure 9 the observed correlation between positive magnitude and generalization error for several intermediary values of $s$ . This experiment was made with a ViT on the CIFAR10 dataset, using the ADAM optimizer. We observe a relative stability of the correlation $\\Psi$ with respect to $s$ . In this particular case, the correlation is extremely stable for higher values of $s$ while it displays more variability for smaller values of $s$ . Further experiments would be necessary to understand whether this behavior is general and could then lead to the discovery of more stable magnitude-related complexities, which we leave for future work. ", "page_idx": 33}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/7914d904a18c6f3a28270ea9d27637c35753f0207b535a484167b3973dabe6f5.jpg", "img_caption": ["Figure 9: Sensitivity analysis of the scale parameter $s$ for positive magnitude $\\mathbf{PMag}(s\\mathcal{W}_{t_{0}\\to\\tau})$ . Experiment made with ViT on CIFAR10 and ADAM optimizer. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "D.2.3 Comparison with gradient variance as a generalization measure ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this short subsection, we investigate the performance comparison of our proposed topological complexities with the more widely used gradient variance, which appears for instance in [37]. ", "page_idx": 33}, {"type": "text", "text": "In this experiment, conducted with a ViT on the CIFAR10 dataset and the ADAM optimizer, we observe very similar performance between $E_{1}$ , $\\mathbf{PMag}(\\sqrt{n}\\mathcal{W}_{t_{0}\\to\\tau})$ and the gradient variance. Note that the fact that $E_{1}$ and $\\mathbf{PMag}(\\sqrt{n}\\mathcal{W}_{t_{0}\\to\\tau})$ yield similar correlation was already observed on Table 1. This tends to suggest that these three complexity measures may be able to capture similar aspects of the geometry around a local minimum. ", "page_idx": 33}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/28f9bd2f6f5a4d15b5e499d0c0296927ccfe0084b4251d8d44cd3ce50850b96b.jpg", "img_caption": ["Figure 10: Comparison of the granulated Kendall coefficients of topological complexities vs gradient variance (GV). $\\psi_{\\mathrm{LR}}$ is denoted GKC (LR), $\\psi_{\\mathrm{{BS}}}$ is denoted GKC (BS) and the averaged coefficient $\\Psi$ is denoted Avg GKC. Experiment made with ViT on CIFAR10 and ADAM optimizer. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Remark D.1. It should be noted that the primary goal of the introduced topological complexity measure is not to outperform existing measures such as a gradient variance but rather to demonstrate the empirical importance of the topology of the trajectory for generalization error. ", "page_idx": 34}, {"type": "text", "text": "D.3 Vision Transformers - additional experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We compare the performance of the different metrics by using the granulated Kendall\u2019s coefficients introduced in [37]. The experiments presented here use 3 different Vision Transformers (ViT [79], CaiT [80], Swin [48]) on CIFAR10 and CIFAR100. As a baseline, we use the $\\dim_{\\operatorname{PH}}$ introduced in [10] and the data-dependent dimension with the pseudometric $\\dim_{\\operatorname{PH}}$ from [21]. ", "page_idx": 34}, {"type": "text", "text": "Here we present the full results on each dataset and model. They can be found in Table 4 for CaiT and CIFAR10, 6 for Swin and CIFAR10, 3 for ViT and CIFAR100 and 5 for CaiT and CIFAR100. The plots from each experiment for every computed quantity can be found in (the remaining 3 quantities for ViT and CIFAR10). ", "page_idx": 34}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/4c75e1d6aa917dbcc113647cbff0371c3742cc8cb20adf3f0398e6fb8750eff0.jpg", "img_caption": ["Figure 11: ViT on CIFAR10 with $\\rho_{S}$ "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/56ede54f01f1059ed98fb381a549bf43790944a51de356c62787c7104c4e12fb.jpg", "img_caption": ["Figure 12: ViT on CIFAR10 with $\\Vert\\cdot\\Vert_{2}$ "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/6a94040a856f786505e9442411bb65ffe213dda9e62ca98609af59ec27b2290f.jpg", "img_caption": ["Figure 13: ViT on CIFAR10 with 01-pseudometric "], "img_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "6U5fCHIWOC/tmp/a19fb925b07370ff583bf4d0e2bbe5f3fb261be12c6ab817a8bc25bd8eff3bb6.jpg", "table_caption": ["Table 3: Correlation coefficients for all quantities for ViT model and CIFAR100 dataset. The corresponding plots are presented in Figures 14, Figure 15 and Figure 16. "], "table_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/2a307a7ff3bae9f95c92b4669cec11c2bd874310720f197799c09ca2c715b91e.jpg", "img_caption": ["Figure 14: ViT on CIFAR100 with $\\rho_{S}$ "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/16bee0b5d1ac898d5e00df0cd866c5a68dad55b2da9c85457b0676df7127933d.jpg", "img_caption": ["Figure 15: ViT on CIFAR100 with $\\Vert\\cdot\\Vert_{2}$ "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/86e61e8c44da36571fffdf15233766ef7d82ef41682f4d3394b853d6b1b68e23.jpg", "img_caption": ["Figure 16: ViT on CIFAR100 with 01-pseudometric "], "img_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "6U5fCHIWOC/tmp/65dc4f4e1fec552b126c09017966a18f136c31dc253b47e578b09f35a5e55043.jpg", "table_caption": ["Table 4: Correlation coefficients for all quantities for CaiT model and CIFAR10 dataset. The corresponding plots can be seen in Figures 17, 18 and 19. "], "table_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/1928fbc4c0ded96d4e18f881510b1f64f71f1ad98a37a114e98dc554f4e59c54.jpg", "img_caption": ["Figure 17: CaiT on CIFAR10 with $\\rho_{S}$ -pseudometric. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/741b486805388dc97dd27798c13394dd59cef4f00fb9d2f12a98629c396ea852.jpg", "img_caption": ["Figure 18: CaiT on CIFAR10 with $\\Vert\\cdot\\Vert_{2}$ distance. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/de72041f5efde7ddae49b6770215cbc8cd3e1c42f7a63f214dcc92840fdea777.jpg", "img_caption": ["Figure 19: CaiT on CIFAR10 with 01-pseudometric. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/bd8ed25be8e859cca741207fa329c3fd881443ff6226ab4e3f1394683e4e8c20.jpg", "img_caption": ["Figure 20: CaiT on CIFAR100 with $\\rho_{S}$ -pseudometric. "], "img_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "6U5fCHIWOC/tmp/309f5cc5cc271eb5b3e49be23b26429da9b2fc47643d3688443dc453108b3d67.jpg", "table_caption": ["Table 5: Correlation coefficients for all quantities for CaiT model and CIFAR100 dataset. The corresponding plots can be seen in 20, 21 and 22 "], "table_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/2eb32e2c173f50ee286cd479ce294d5dc4e9fc2f7b8ed8b607b606b3ee6f0118.jpg", "img_caption": ["Figure 21: CaiT on CIFAR100 with $\\|\\cdot\\|_{2}$ . "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/ba74f9d0fcfc1ef83cc5bcab73473eca254ba9aa3ec2a969d24adb2fa16b0e1f.jpg", "img_caption": ["Figure 22: CaiT on CIFAR100 with 01-pseudometric. "], "img_footnote": [], "page_idx": 40}, {"type": "table", "img_path": "6U5fCHIWOC/tmp/698fbca056b9802be74c407835a2e7013136b82a6f10874f4c0ad2e84444abed.jpg", "table_caption": ["Table 6: Correlation coefficients for all quantities for Swin model and CIFAR10. The corresponding plots are in Figure 23, 24 and 25. "], "table_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/b8fd62f91de6cbac8a29066338f7ba3dced2033e54963a50082ccf9351046c26.jpg", "img_caption": ["Figure 23: Swin on CIFAR10 with $\\rho_{S}$ -pseudometric. "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/27edd18d907aafbf569026479eca4a62a7fc37bc262df4f3ee9826ae3d4e8a28.jpg", "img_caption": ["Figure 24: Swin on CIFAR10 with $\\|\\cdot\\|_{2}$ . "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/a8c4caa34c2231518c2879bf0c76ffb8afc86f0d29eb5a5ee62e1e99d66f7b0a.jpg", "img_caption": ["Figure 25: Swin on CIFAR10 with 01-pseudometric. "], "img_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "6U5fCHIWOC/tmp/c6e1a7b93468e31b534efd88163fb8024031c55e9be21e5fb7649d29ea18a3fd.jpg", "table_caption": ["Table 7: Correlation coefficients for all quantities for Swin model and CIFAR100. See Figures 26, 27 and 28 for the corresponding plots. "], "table_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/3b7ff88c53d42247b3a3dc2a4f0422d5b080d0c3cd3f6f370dc7bb71b2e97bb0.jpg", "img_caption": ["Figure 26: Swin on CIFAR100 with $\\rho_{S}$ -pseudometric. "], "img_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/26a24683eec34d580d630b290c6e80ca74c623b0b5244ca7b089b59bd3f34352.jpg", "img_caption": ["Figure 27: Swin on CIFAR100 with $\\|\\cdot\\|_{2}$ . "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/51d53bee9f1692f3bfecc8a04ef1cb82b51ecb8aac97a1f6670635ea8b451430.jpg", "img_caption": ["Figure 28: Swin on CIFAR100 with 01. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "D.4 Graph Neural Networks \u2013 Additional Experiments ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In Table 1, we already presented the correlation coefficients for all quantities for the GNN models considered in our study (GraphSage, GatedGCN) [23] (we have selected the models which achieve $100\\%$ training accuracy)) and Graph-MNIST. We can observe a nice correlation, outperforing dim-PH in most experiments. As it was observed for the transformer-based experiments, the correlation seems to be better for the data-dependent-metrics. This is an important fact, as no sparse random projection was used to compute the Euclidean distance matrices in the GNN experiments (it was not necessary as these models have less parameters than the tramsformers considered above). This shows that the fact the data-dependent pseudometrics outperform the Euclidean distance also happens in the absence of these projections. It also shows that all quantities seem to yield better correlations in the absence of random projections, at least in the GNN expsriments. ", "page_idx": 44}, {"type": "text", "text": "The corresponding plots for GatedGCN can be seen in Figure 32 with the pseudometric, Figure 33 for the Euclidean and 34 for 01. The plots for GraphSage are reported in Figure 29, Figure 30 and Figure 31. ", "page_idx": 44}, {"type": "text", "text": "We can observe a strong correlation on these figures, outperforing dim-PH in most cases. As it was observed for the transformer-based experiments, the correlation seems to be better for the data-dependent-metrics. This is an important fact, as no sparse random projection was used to compute the Euclidean distance matrices in the GNN experiments9. This shows that data-dependent pseudometrics outperform the Euclidean distance also in the absence of these projections. In addition, all quantities seem to yield better correlations in the absence of random projections, at least in the GNN expsriments. ", "page_idx": 44}, {"type": "text", "text": "Interestingly, a few failure cases can be seen on these plots. Indeed, $\\mathbf{Mag}(0.01)$ and $\\mathbf{PMag}(0.01)$ seem to be almost constant and near 1. This indicates that the scale choice $s=0.01$ was not suited for these experiments; this behavior was already reflected in Table 1 through very low Kendall\u2019s coefficients, indicating the absence of meaningful correlation. However, $\\mathbf{Mag}({\\sqrt{n}})$ and $\\mathbf{PMag}({\\sqrt{n}})$ provide significantly better correlation, which supports our main claims, as $s=\\sqrt{n}$ has been argued in section 3.3 to be a particulary relevant choice of scale factor. ", "page_idx": 44}, {"type": "text", "text": "Note finally that the PH-dim plots for the 01-pseudometric failed to produce numbers in these graphs experiments (this is why they are either missing or look irrelevant). As before, we gave away this fact in Table 1 by imposing our granulated Kendall\u2019s coefficients implementation to return zeros in the absence of correlation, hence the small numbers observed in this case. That being said, this behavior should not be seen as an issue. Indeed, PH-dim with 01-pseudometric consists (in theory) in estimating the dimension of a subset of a discrete hypercube, which is always 0. The reason we still reported PH-dim for this pseudometric is for consistence and to test the implementation of [10, 21] in this non-standard setting; it is however not theoretically grounded. ", "page_idx": 44}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/87d41826692641f5cb6bdb05240eea8905828107914bfc18c5a97f657be265b7.jpg", "img_caption": ["Figure 29: GraphSage on MNIST with $\\rho_{S}$ -pseudometric. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/c31bcdf26f444149c6b1d053cecc01dff12569480b13f4dbe7cafd9c72164605.jpg", "img_caption": ["Figure 30: GraphSage on MNIST with $\\Vert\\cdot\\Vert_{2}$ . "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/d6bb2ba7ba41fa1286024a2c75cb827c0763d4ffef01b0ae23ce6c9c5a5d7735.jpg", "img_caption": ["Figure 31: GraphSage on MNIST with 01. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/0cd064ceb7bd676d43b160ff44f7996db9d90ab7140b3d2213b1a258e515b204.jpg", "img_caption": ["Figure 32: GatedGCN on MNIST with $\\rho_{S}$ -pseudometric. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/9680e024af0db136cf8a5d019cd5a85b341db89c0b5557aec839badfe5d35955.jpg", "img_caption": ["Figure 33: GatedGCN on MNIST with $\\|\\cdot\\|_{2}$ . "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "6U5fCHIWOC/tmp/67c368c8ef243c25adc4b9f77db6d53d573b915a6dc8cdd3f6e961669205ebcf.jpg", "img_caption": ["Figure 34: GatedGCN on MNIST with 01. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We clearly stated our contributions in the introduction and abstract. These contributions are then detailed in the main part of the paper. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We discuss several theoretical and empirical limitations of our work in Sections 3, 4 and 5. Moreover, a paragraph following the conclusion is dedicated to highlighting the most important limitations and discussing future works. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: All our assumptions are clearly stated. We additinoally include technical measure-theoretic considerations in B.5, showing the level of rigour in our mathematical analysis. It should be noted that all the proofs of the theoretical results are presented in the appendix. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The models, datasets and optimizers of all our epxeimrents are clearly stated. Moreover, all the details of the hyperparameters we used for both training the neural networks and evaluating our topological complexities are clearly stated in Sections 5 and C.2. Moreover, we specify the exact algorithms that we use to compute the proposed topological complexities. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 48}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We provide the essential part of our code as part of the supplementary material. This code submission contains a README.md file that contains instructions to first test the code and then run experiments. The code will be made publicly available upon acceptance of the paper. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We provide in Sections 5 and C.2 the full details of hyperparameters and optimizers, models and datasets used to train the neural networks in our experiments. Moreover, all hyperparameters used to evaluate our proposed topological complexities are explicitely mentioned in the paper, this includes in particular the number of iteratiosn in the trajectory, the proportion of the data used to evaluate the data-dependent distance matrices. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [No] ", "page_idx": 50}, {"type": "text", "text": "Justification: In our paper, we conducted experiments on a variety of models (vision transformers, graph neural networks) that are significantly bigger than the ones previously used in the trajectory-dependent generalization literature. As the cost of these experiments is extremely high, both in terms of computational time (it requires evaluating the model on a large part of the data at each iterations in order to compute the distance matrices) and memory (storing the trajectories). Moreover, for each (model - dataset) pair, we run the experiments on a grid of hyperparameters, corresponding to 36 different set of hyperparameters each time. Therefore, we were not able to perform numerous epxeriments for each model/dataset. However, the fact that our experiments have good performance accross a variety of models and datasets and on a grid of hyperparameters. Therefore, while we do not explicitly study the statistical significance of the resented results, our extensive set of experiments does support the significance of these results. Moreover, it should be noted that we partially base our empirical study on the granulated Kendall\u2019s coefficients, which are more relevant than the classical Kendall\u2019s coefficient to capture causal relationships. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We specified in Section C the compute resources we used to run our experiments. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: Our paper follows the NeurIPS Code of Ethics. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We included a broader impact statement following the conclusion, at the end of the main part of this document. As understading the generalization properties of modern neural networks is key to trustworthy AI systems, we believe that our work may inspire future research having a positive societal impact. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our submitted code only allows one to compute various topological complexities and compare them to the generalization error observed when training deep learning models. There is no risk that this could be misued. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 51}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: Our algorithms to evaluate the topological complexities that we propose, as well as the baselines, make use of existing libraries and codes that are clearly stated in Sections 4 and 5. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: We include, as part of the submission, a version of our code that contains the algorithms described in the paper, along with documentation provided in a README.md flie. Our code will be made publicly available and will be clearly documented regarding what are the existing and new assets, as it is done in the paper. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: Our paper includes neither crowdsourcing experiments nor research with human subjects. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: Our paper does not include research with human subjects. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]