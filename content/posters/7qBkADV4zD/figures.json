[{"figure_path": "7qBkADV4zD/figures/figures_2_1.jpg", "caption": "Figure 1: (a) Reconstruction evolution when increasing inference iterations. (b) Hidden states trajectory for 5 consecutive input points with the first two principal components. Details in A.1. (c) Cumulative explained variance for all hidden states. (d) Evolution of different dimensions of hidden states (represented by colors) over iterations. (e) Mean delta activation for different dimensions (represented by colors). The colored solid areas indicate the standard deviation from different inputs. (f) Histogram of converged dimensions (blue) at i-th iteration and evolution of the model prediction MSE (red).", "description": "This figure visualizes several key observations supporting the concept of heterogeneous convergence in deep equilibrium models.  Subfigure (a) shows the model's reconstruction improving with more iterations. (b) displays the hidden state trajectory using PCA, indicating lower dimensionality than expected.  (c) further supports this with cumulative explained variance. (d) and (e) illustrate the varying convergence speeds across different dimensions of the hidden state. Finally, (f) shows how many dimensions converge at each iteration and the overall model error.", "section": "3 Observations and motivation"}, {"figure_path": "7qBkADV4zD/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Convolution type of DeltaDEQ. The input I\u00af\u00b9 from the previous iteration is stored and subtracted to create the sparse \u2206\u0399. White represents zero. (b) For sparse convolution, in theory, all zero entries in the feature map can be skipped; in practice, this is more feasible on hardware [1, 15] when the entire activation patch is fully sparse. The complete formulation and pseudo-code are given in A.2 and RNN type DeltaDEQ is illustrated in A.5.1.", "description": "This figure illustrates the mechanism of DeltaDEQ, a method to leverage heterogeneous convergence for accelerating deep equilibrium iterations.  Panel (a) shows how the input from the previous iteration (I<sup>t-1</sup>) is stored and subtracted from the current iteration's input (I<sup>t</sup>) to create a sparse delta (\u0394I). This sparsity increases with each iteration. Panel (b) details how this sparse delta is used in sparse convolutions, allowing for computational savings by skipping computations on zero entries.  The figure highlights the efficiency gains achievable through this method.", "section": "Methods"}, {"figure_path": "7qBkADV4zD/figures/figures_5_1.jpg", "caption": "Figure 1: (a) Reconstruction evolution when increasing inference iterations. (b) Hidden states trajectory for 5 consecutive input points with the first two principal components. Details in A.1. (c) Cumulative explained variance for all hidden states. (d) Evolution of different dimensions of hidden states (represented by colors) over iterations. (e) Mean delta activation for different dimensions (represented by colors). The colored solid areas indicate the standard deviation from different inputs. (f) Histogram of converged dimensions (blue) at i-th iteration and evolution of the model prediction MSE (red).", "description": "This figure visualizes several key observations about the heterogeneous convergence phenomenon in deep equilibrium models. Subfigure (a) shows the model's reconstruction improving with more inference iterations. Subfigure (b) displays the trajectory of hidden states in a 2D PCA projection, demonstrating that similar inputs lead to nearby hidden state trajectories. Subfigure (c) shows that a small number of principal components capture most of the variance in hidden states. Subfigures (d) and (e) illustrate the heterogeneous convergence, where some dimensions converge much faster than others. Subfigure (f) presents a histogram of converged dimensions at each iteration and the model's mean squared error (MSE) over time, further demonstrating that the convergence speed varies across dimensions.", "section": "Observations and motivation"}, {"figure_path": "7qBkADV4zD/figures/figures_6_1.jpg", "caption": "Figure 4: FLOPs reduction and task accuracy (PSNR) at different inference delta threshold.", "description": "This figure shows the relationship between computation saving (in terms of FLOPs reduction), reconstruction quality (measured by PSNR), and the inference delta threshold.  The results are averaged over three runs, with standard deviations represented as shaded areas.  The plot demonstrates a trade-off; increasing the threshold leads to greater FLOPs reduction but potentially at the cost of slightly lower PSNR. The figure illustrates that a range of thresholds provides substantial computation savings while maintaining high reconstruction quality.", "section": "5.1 Implicit neural representation"}, {"figure_path": "7qBkADV4zD/figures/figures_15_1.jpg", "caption": "Figure 5: Illustration of saving mechanisms of DeltaDEQ. Fig. Fully connected Wz.zi type of computation skip. Entire columns of MACs at zero entries of \u2206zi can be skipped and the sparsity of zi grows with iteration i.", "description": "This figure illustrates how DeltaDEQ saves computation.  In a standard RNN layer, a matrix-vector multiplication (MxV) is performed using the full weight matrix (Wz) and the state vector (zi). DeltaDEQ modifies this by calculating the change (\u2206zi) in the state vector between iterations. Only the non-zero elements of \u2206zi participate in the MxV operation, significantly reducing computations.  The cached results from the previous iteration are added to further improve efficiency. The figure shows how the sparsity of \u2206zi increases as the fixed-point iteration converges, leading to substantial computational savings.", "section": "A.5.1 Illustration for RNN type of DeltaDEQ"}, {"figure_path": "7qBkADV4zD/figures/figures_15_2.jpg", "caption": "Figure 3: Original image and reconstructions with INR network.", "description": "This figure shows a comparison of the original image and reconstructions obtained using the INR (Implicit Neural Representation) network with different methods. It compares the reconstruction quality of the DEQ-Fourier method and the proposed DeltaDEQ-Fourier method.  The PSNR (Peak Signal-to-Noise Ratio) and FLOPs (floating-point operations) for each method are provided, demonstrating the computational savings achieved by the DeltaDEQ-Fourier method.", "section": "5.1 Implicit neural representation"}, {"figure_path": "7qBkADV4zD/figures/figures_15_3.jpg", "caption": "Figure 4: FLOPs reduction and task accuracy (PSNR) at different inference delta threshold.", "description": "This figure shows the relationship between computation saving (FLOPs Reduction), reconstruction quality (PSNR), and the inference delta threshold. The x-axis represents the inference delta threshold, while the y-axis shows both FLOPs reduction and PSNR. Two lines represent different DeltaDEQ model architectures: DeltaDEQ-Fourier and DeltaDEQ-Siren. The figure demonstrates the impact of different inference delta thresholds on the model's performance and computation cost. Each line represents the mean of three runs, and shaded areas show standard deviations.", "section": "5.1 Implicit neural representation"}, {"figure_path": "7qBkADV4zD/figures/figures_17_1.jpg", "caption": "Figure 1: (a) Reconstruction evolution when increasing inference iterations. (b) Hidden states trajectory for 5 consecutive input points with the first two principal components. Details in A.1. (c) Cumulative explained variance for all hidden states. (d) Evolution of different dimensions of hidden states (represented by colors) over iterations. (e) Mean delta activation for different dimensions (represented by colors). The colored solid areas indicate the standard deviation from different inputs. (f) Histogram of converged dimensions (blue) at i-th iteration and evolution of the model prediction MSE (red).", "description": "This figure visualizes the heterogeneous convergence phenomenon observed in deep equilibrium models. Subfigure (a) shows the model's reconstruction improving with more iterations. Subfigures (b) and (c) show the dimensionality reduction of hidden states using PCA. Subfigures (d) and (e) show how different dimensions of hidden states converge at different speeds. Subfigure (f) shows the number of converged dimensions and the model's MSE over iterations.", "section": "Observations and motivation"}, {"figure_path": "7qBkADV4zD/figures/figures_18_1.jpg", "caption": "Figure 9: Illustration of the evolution of activation values in the original DEQ-flow network [4] along the fixed-point iterations when processing 5 consecutive pairs of input frames in the Sintel [8] dataset. The startings of a new pair are marked with vertical dashed lines. Each pair is processed with 60 iterations. Different activation sites converge at different speeds and the fourth pair takes more iterations to reach convergence, due to the large motion in the input space.", "description": "This figure shows the evolution of activation values in the original DEQ-flow network during fixed-point iterations while processing five consecutive pairs of input frames from the Sintel dataset. Each pair undergoes 60 iterations. The figure highlights that different activation sites converge at varying speeds, with the fourth pair requiring more iterations due to significant motion within the input space.", "section": "A.6 DeltaDEQ for optical flow"}, {"figure_path": "7qBkADV4zD/figures/figures_18_2.jpg", "caption": "Figure 9: Illustration of the evolution of activation values in the original DEQ-flow network [4] along the fixed-point iterations when processing 5 consecutive pairs of input frames in the Sintel [8] dataset. The startings of a new pair are marked with vertical dashed lines. Each pair is processed with 60 iterations. Different activation sites converge at different speeds and the fourth pair takes more iterations to reach convergence, due to the large motion in the input space.", "description": "This figure shows the evolution of activation values in the original DEQ-flow network during fixed-point iterations while processing 5 consecutive pairs of input frames from the Sintel dataset.  Each pair undergoes 60 iterations. The start of each new pair is indicated by dashed vertical lines. The figure highlights the heterogeneous convergence, where different activation sites converge at varying speeds; this is particularly evident in the fourth pair, which requires more iterations to converge due to significant motion within the input space.", "section": "A.6 DeltaDEQ for optical flow"}, {"figure_path": "7qBkADV4zD/figures/figures_19_1.jpg", "caption": "Figure 10: Architecture illustration for RAFT [53], DEQ flow [4] and our DeltaDEQ.", "description": "The figure shows the architecture of RAFT, DEQ-RAFT, and DeltaDEQ for optical flow. It highlights the feature encoder and context encoder, which are common to all three methods. The key difference lies in the update block, which iteratively refines the flow prediction.  The DeltaDEQ version incorporates a delta rule to accelerate the computationally intensive update block.", "section": "A.6.2 DeltaDEQ conversion details for the RAFT architecture"}]