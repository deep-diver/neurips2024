[{"type": "text", "text": "FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Asadullah Hill Galib, Pang-Ning Tan, and Lifeng Luo Michigan State University Emails: {galibasa, ptan, lluo} $@$ msu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series generation is a crucial aspect of data analysis, playing a pivotal role in learning the temporal patterns and their underlying dynamics across diverse fields. Conventional time series generation methods often struggle to capture extreme values adequately, diminishing their value in critical applications such as scenario planning and risk management for healthcare, finance, climate change adaptation, and beyond. In this paper, we introduce a conditional diffusion model called FIDE to address the challenge of preserving the distribution of extreme values in generative modeling for time series. FIDE employs a novel high-frequency inflation strategy in the frequency domain, preventing premature fade-out of the extreme values. It also extends the traditional diffusion-based model, enabling the generation of samples conditioned on the block maxima, thereby enhancing the model\u2019s capacity to capture extreme events. Additionally, the FIDE framework incorporates the Generalized Extreme Value (GEV) distribution within its generative modeling framework, ensuring fidelity to both block maxima and overall data distribution. Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generative models [18, 10, 13] have revolutionized the AI landscape, demonstrating their broad applicability across diverse domains, including computer vision and natural language processing. Such models are designed to learn the underlying data distribution and exhibit resilience to overftiting while promoting automatic feature extraction. Diffusion-based models [12, 19], in particular, have emerged as a popular generative AI method due to their capability to generate realistic, high-quality data. This paper examines the application of diffusion-based models for time series generation. In particular, we investigate the following issue: How well do existing diffusion models preserve the fidelity of extreme values (i.e., tail distribution) of the original time series? ", "page_idx": 0}, {"type": "text", "text": "The modeling of extreme values in time series is essential for informed decision-making across diverse applications, including weather forecasting, earthquake prediction, and disease outbreak detection. Effective generative modeling of these extremes is important as it aids in learning the underlying data distribution, facilitating data augmentation, and improving uncertainty estimation, all of which are crucial for developing robust risk management strategies and enhancing disaster preparedness measures. While there has been growing research on applying diffusion models for time series [20, 2], their ability to preserve the distribution of extreme values remains largely underexplored. In this study, we examine how effectively diffusion models preserve extreme values in the form of block maxima [4], defined as the peak value within a specified time window. ", "page_idx": 0}, {"type": "text", "text": "To illustrate the difficulty of modeling the distribution of block maxima, Figure 1 shows the result of applying the Denoising Diffusion Probabilistic Model (DDPM) [12] to a synthetic AR(1) dataset. ", "page_idx": 0}, {"type": "image", "img_path": "5HQhYiGnYb/tmp/3708e71c4b7876d262066822548e8e043fced4244973f6a912cc345d4d353581.jpg", "img_caption": ["(a) All Values Distribution (b) Block Maxima Distribution "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparing the distributions of all values and block maxima values for real and generated samples using DDPM [12] when applied to the synthetic AR(1) dataset. ", "page_idx": 1}, {"type": "text", "text": "While DDPM shows proficiency in generating samples that closely align with the overall data distribution (left diagram), it struggles to preserve the distribution of block maxima values (right diagram) when the generated time series is partitioned into disjoint time windows. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we identify the key shortcomings of existing diffusion models that hamper their ability to accurately model block maxima values. We then present a novel framework to overcome this limitation. Our key observation is that unusually large block maxima values, often linked to abrupt temporal changes, are strongly associated with high-frequency components of the time series. As the diffusion-based generative model gradually introduces noise with a linearly increasing variance schedule, it slowly diminishes the long-term trends (low-frequency components) of the time series while quickly attenuating the high-frequency components. These high-frequency components are crucial for reproducing extreme block maxima values. This limitation hampers the accurate representation of the block maxima, necessitating the development of new techniques. ", "page_idx": 1}, {"type": "text", "text": "To address this challenge, we propose an end-to-end diffusion model framework termed FIDE. First, to mitigate the rapid dissipation of high-frequency components in the diffusion model, we introduce a novel high-frequency inflation strategy within the frequency domain. This strategic augmentation ensures the sustained emphasis on block maxima, preventing their premature fadeout. We further employ a conditional diffusion-based generative modeling approach to guide the time series generation by conditioning on their block maxima. To enhance the preservation of the block maxima distribution while learning the overall data distribution, we extend the conventional framework with a regularization term in the loss function based on the negative log-likelihood of the Generalized Extreme Value (GEV) distribution. Using these strategies, we empirically show that our approach effectively addresses the challenges of learning the overall data distribution while simultaneously preserving the block maxima distribution. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider a time series dataset $\\mathscr{D}\\,=\\,\\{\\mathbf{x}_{m,0}\\}_{m=1}^{M}$ comprising of $M$ samples, where each sample $\\mathbf{x}_{m,0}=(x_{m,0}^{1},x_{m,0}^{2},\\cdot\\cdot\\cdot,x_{m,0}^{T})$ is a univariate time series of finite length $T$ . Let $\\mathbf{f}_{m,0}\\in\\mathbb{R}^{T}$ be the Fourier coefficients, whose $k$ -th frequency component is obtained by applying the following discrete Fourier transform on $\\mathbf{x}_{m,0}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\nf_{m,0}^{k}=\\sum_{t=1}^{T}x_{m,0}^{t}\\;e^{-i2\\pi t k/T}=\\sum_{t=1}^{T}\\left[x_{m,0}^{t}\\cos\\left({\\frac{2\\pi t k}{T}}\\right)-i\\cdot x_{m,0}^{t}\\sin\\left({\\frac{2\\pi t k}{T}}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The time series can be recovered from its Fourier coefficients using the following inverse discrete Fourier transform: ", "page_idx": 1}, {"type": "equation", "text": "$$\nx_{m,0}^{t}=\\frac{1}{T}\\sum_{k=1}^{T}f_{m,0}^{k}\\;e^{i2\\pi t k/T}=\\frac{1}{T}\\sum_{k=1}^{T}\\left(f_{m,0}^{k}\\cos\\left(\\frac{2\\pi t k}{T}\\right)+i\\cdot f_{m,0}^{k}\\sin\\left(\\frac{2\\pi t k}{T}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For brevity, we will drop the sample subscript $m$ when it is clear from the context. Let, $\\begin{array}{r}{\\omega_{k}=\\frac{2\\pi k}{T}}\\end{array}$ be the $k$ -th frequency in Fourier transform. ", "page_idx": 1}, {"type": "text", "text": "Given a sample $\\mathbf{x}_{\\mathrm{0}}$ , let $y_{0}$ be its corresponding block maxima value, where $y_{0}=\\operatorname*{max}_{\\tau\\in\\{1,\\cdots,T\\}}x_{0}^{\\tau}$ . The distribution of the block maxima values is governed by the Generalized Extreme Value (GEV) ", "page_idx": 1}, {"type": "image", "img_path": "5HQhYiGnYb/tmp/c5334e1096d6bddbe92b74884e73226c84b41fa266ab886c71d8de9458b94522.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Removal of high-frequency components from daily temperature time series significantly alters the magnitude of its block maxima value (at time step 20), as evidenced by its high residual. ", "page_idx": 2}, {"type": "text", "text": "distribution, whose cumulative distribution function is given as follows [4]: ", "page_idx": 2}, {"type": "equation", "text": "$$\nG(y)=\\exp\\bigg\\{-\\bigg[1+\\xi(\\frac{y-\\mu}{\\sigma})\\bigg]^{-1/\\xi}\\bigg\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mu$ (location), $\\sigma$ (scale), and $\\xi$ (shape) are the distribution parameters. Given $M$ independent block maxima values, denoted as $\\{y_{1,0},\\bar{y}_{2,0},\\cdot\\cdot\\cdot,y_{M,0}\\}$ , with the cumulative distribution function given by Equation (3), the distribution parameters can be estimated using the maximum likelihood approach by minimizing the following negative log-likelihood function: ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\log\\mathcal{L}_{\\mathrm{GEV}}(\\mu,\\sigma,\\xi)=M\\log\\sigma+\\left(\\frac{1}{\\xi}+1\\right)\\sum_{i=1}^{M}\\log\\left[1+\\xi\\,\\frac{y_{i,0}-\\mu}{\\sigma}\\right]+\\sum_{i=1}^{M}\\left(1+\\xi\\frac{y_{i,0}-\\mu}{\\sigma}\\right)^{-1/\\xi}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 On the Rapid Dissipation of Block Maxima in Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "While diffusion models have demonstrated remarkable capabilities in learning complex data distributions, a significant challenge arises in accurately capturing the distribution of block maxima values, as evidenced by Figure 1. Addressing this shortcoming is crucial for enhancing the performance and applicability of these models across various domains. In this section, we delve into the root cause of this phenomenon and present insightful observations that shed light on the underlying issue. ", "page_idx": 2}, {"type": "text", "text": "Our first key observation reveals a connection between block maxima with abrupt changes and the high-frequency components of many real-world time series. Block maxima, often characterized by their rarity and abrupt temporal changes, are intrinsically linked to the high-frequency components of the data. This relationship is observed in many real-world datasets, where the block maxima values do not typically evolve smoothly but rather emerge through large deviations from their adjacent values. ", "page_idx": 2}, {"type": "text", "text": "To illustrate this, consider the real-world temperature time series depicted in Figure 2. In this plot, we first transform the time series into its Fourier domain, obtaining its frequency components, and selectively zeroing out its top-5 highest frequency components. We then reconstruct the time series via its inverse Fourier transform and compute the difference between the original and reconstructed time series. The recovered signal exhibits a notable distortion around the block maxima value, as evidenced by the larger residual at time step 20, where the block maxima value occurs. This suggests that the removal of high-frequency components of a time series has a significant impact on the accurate representation of block maxima values. A more detailed analysis supporting this argument is given in Appendix B. ", "page_idx": 2}, {"type": "image", "img_path": "5HQhYiGnYb/tmp/df327bff76c847145af8e327555a8f65b95bb4edd5c7dd0f707a1eddfd5b0135.jpg", "img_caption": ["Figure 3: A comparison of the effects of noise addition by existing DDPM versus high-frequency inflation on the block maxima of generated samples. ", "", "(b) Effect of High Frequency Inflation "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Our second key observation unveils a concerning behavior of diffusion models: the addition of noise diminishes high frequency components, i.e., block maxima, at a faster rate compared to other values in the signal. As diffusion-based generative models gradually introduce noise characterized by a linearly increasing variance scheduler, they inadvertently attenuate the signals associated with high-frequency components. These components, as established in our first observation, are crucial for accurately reproducing block maxima. Concurrently, the models effectively capture the long-term trends and low-frequency components, which are conducive to learning the overall data distribution. However, the high frequency components dissipate more rapidly, hindering the model\u2019s ability to adeptly learn the distribution of the block maxima values. ", "page_idx": 3}, {"type": "text", "text": "Figure 3a illustrates this phenomenon. By tracking the evolution of residuals, or the differences between the original and perturbed time series generated by DDPM, we observe a discernible pattern: block maxima dissipate at a faster rate compared to other values, as evidenced by the higher residuals associated with these extreme points. Notably, in the early iterations highlighted by the green circle, the substantially higher residual suggests that the block maxima signal is rapidly transformed into noise, outpacing the dissipation rate of other values. This behavior poses a formidable challenge for diffusion models in effectively capturing the distributions of the block maxima values. ", "page_idx": 3}, {"type": "text", "text": "To substantiate our observations, the theorem below offers a rigorous justification for the rapid dissipation of block maxima during the forward process of the diffusion model (see Appendix C for proof and details). Let $\\mathbf{x}_{\\mathrm{0}}$ be an input sample and ${\\bf x}_{n}$ be the perturbed sample after $n$ iterations of the forward process, where $x_{n}^{t}=x_{n-1}^{t}+\\epsilon_{n}^{t}$ and $\\epsilon_{n}^{t}\\sim\\mathbb{N}(0,\\sigma_{\\epsilon_{n}^{t}}^{2})$ is Gaussian noise. Due to the linearity of the Fourier transform operator $\\mathcal{F}$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}(\\mathbf{x}_{n})=\\mathcal{F}(\\mathbf{x}_{n-1})+\\mathcal{F}(\\epsilon_{n})\\quad\\Longrightarrow\\quad f_{n}^{k}=f_{n-1}^{k}+\\mathcal{E}_{n}^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Under certain mild assumptions (see Appendix $C$ ), the ratio of high-frequency and low-frequency components after perturbation during the forward process of the diffusion model is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{lim}_{k\\to k_{\\mathrm{max}}}|f_{n}^{k}|^{2}}{\\operatorname*{lim}_{k\\to0}|f_{n}^{k}|^{2}}=\\delta\\ll1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $k_{\\mathrm{max}}$ is the index of the maximum frequency and $\\delta=f_{n}^{k_{\\mathrm{max}}}$ , which is generally close to 0. ", "page_idx": 3}, {"type": "text", "text": "In short, our findings shed light on a fundamental limitation of diffusion models while modeling block maxima and underscore the need for a more tailored approach to preserve its distribution. ", "page_idx": 3}, {"type": "text", "text": "4 Proposed Framework: FIDE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the detailed methodology of our proposed approach, addressing the challenges associated with capturing extreme values of time series within diffusion-based generative models. Figure 4 provides an overview of the FIDE framework. ", "page_idx": 3}, {"type": "image", "img_path": "5HQhYiGnYb/tmp/1c5e63cfb79344489d22e948e80f64542bee237dd40f8702b57fe6930ec8fc3e.jpg", "img_caption": ["Figure 4: Proposed FIDE framework for generating time series with extreme events "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 High Frequency Components Inflation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In order to counteract the rapid decay of high-frequency components in the frequency domain while adding noise in the forward process of DDPM, we present a strategy for high-frequency inflation. Let $\\mathbf{f}_{0}=\\mathbf{F}\\mathbf{F}\\mathbf{T}\\left(\\mathbf{x}_{0}\\right)$ denote the vector of Fourier coefficients resulting from applying the discrete Fourier transform to the time series $\\mathbf{x}_{\\mathrm{0}}$ . These coefficients are arranged in ascending order from lowest to highest frequency. Consequently, the last $\\kappa$ elements of $\\mathbf{f}_{0}$ correspond to the coefficients associated with the $\\kappa$ highest frequencies. Our goal is to inflate the top- $\\kappa$ frequency components of $\\mathbf{f}_{\\mathrm{0}}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\Gamma^{i}}&{=}&{\\left\\{1,\\begin{array}{l l}{\\mathrm{~if~}i\\leq\\kappa}\\\\ {\\gamma,}&{\\mathrm{if~}i>T-\\kappa}\\end{array}\\right.\\mathrm{~and~}\\;\\overline{{\\mathbf{f}}}_{0}=\\mathbf{{T}}\\odot\\mathbf{f}_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma>1$ is the inflation weight and $\\odot$ denotes the element-wise multiplication. ", "page_idx": 4}, {"type": "text", "text": "With the modified coefficients $\\overline{{\\mathbf{f}}}_{0}$ , the inverse Fourier transform (IFFT) is applied to get the modified time series, $\\overline{{\\mathbf{x}}}_{0}=\\mathbf{IFFT}\\left(\\overline{{\\mathbf{f}}}_{0}\\right)$ , containing the inflated high-frequency components. Here, the highfrequency components are inflated by $\\gamma>1$ . The following theorem shows how this inflation strategy helps the high-frequency components (block maxima) diminish less rapidly in the diffusion forward process compared to before (see Appendix C for proof and details). ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $\\overline{{f_{n}^{k}}}$ be the Fourier coefficient after inflating high-frequency components with a factor of $\\gamma$ such that $\\gamma>1$ . Let $\\delta=f_{n}^{k_{\\mathrm{max}}}$ be the Fourier coefficient of the maximum frequency before inflation, and $\\delta^{\\prime}=\\delta\\cdot\\gamma=\\overline{{f_{n}^{k_{\\mathrm{max}}}}}$ be the Fourier coefficient of the maximum frequency after inflation. Then, using Lemma $^{\\,l}$ and under certain mild assumptions (see Appendix $C$ ), the ratio of high-frequency and low-frequency components after inflation and perturbations is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname*{lim}_{k\\to k_{\\operatorname*{max}}}|{\\overline{{f_{n}^{k}}}}|^{2}}{\\operatorname*{lim}_{k\\to0}|{\\overline{{f_{n}^{k}}}}|^{2}}}=\\delta\\cdot\\gamma\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, by applying high-frequency inflation, the high-frequency components including abrupt block maxima will be preserved by a factor of $\\gamma$ compared to the previous case. We can see the effects of this inflation empirically as well. Figure 3b shows how inflating the high-frequency components helps in preserving the block maxima values for longer iterations of the diffusion model. This enables the block maxima after high-frequency inflation to dissipate at a similar rate compared to other values in the earlier iterations. The diffusion model will have more iterations to capture the block maxima signal. ", "page_idx": 5}, {"type": "text", "text": "4.2 Forward Process ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use the inflated time series $\\overline{{\\mathbf{x}_{0}}}$ as input time series to be perturbed during the forward process instead of $\\mathbf{x}_{\\mathrm{0}}$ . By adopting $\\overline{{\\mathbf{x}_{\\mathrm{0}}}}$ as the reference for the unperturbed sample, we ensure that the denoising diffusion process takes advantage of the enhanced representation provided by the inflated high-frequency components. This nuanced adjustment contributes to the efficacy of our proposed framework in capturing and preserving essential information during the diffusion process. ", "page_idx": 5}, {"type": "text", "text": "4.3 Conditional Reverse Diffusion Process ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To enable the generation of samples conditioned on block maxima, we extend the conventional diffusion model to a conditional model. Here, the reverse process is conditioned on block maxima $y_{0}$ . Grounded in extreme value theory [4], the block maxima values $\\{y_{0}\\}$ are governed by the Generalized Extreme Value (GEV) distribution, distinctly diverging from the distribution of all values $\\mathbf{x_{0}}\\sim p_{\\theta}(\\mathbf{x}_{0})$ . This mandates a strategic shift in our learning objective. Rather than marginally targeting $p_{\\theta}(\\mathbf{x}_{0})$ , our objective now extends to mastering the joint distribution $p_{\\theta}(\\mathbf{x}_{0},y_{0})$ , driven by a nuanced understanding of the unique characteristics inherent in extreme events and their crucial impact on the overall distribution. We formally extend the diffusion model\u2019s marginal distribution to a joint distribution in the following theorem (see Appendix $\\mathbf{C}$ for proof and details). ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Consider an extension of the conventional diffusion model from learning a marginal distribution $p_{\\theta}(\\mathbf{x}_{0})$ to a joint distribution $p_{\\theta}(\\mathbf{x}_{0},y_{0})$ conditioned on block maxima $y_{0}$ . In this context, the variational lower bound can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\log p_{\\theta}(\\mathbf{x}_{0},y_{0})\\leq\\mathbb{E}q\\Big[\\log\\frac{q(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0})}{p_{\\theta}(\\mathbf{x}_{0:N})}\\Big]-\\log p_{\\theta}(y_{0})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "First, we adopt $\\overline{{\\mathbf{x}_{0}}}$ as the reference for the unperturbed sample $\\mathbf{x}_{\\mathrm{0}}$ as discussed in the previous subsection. After reparameterization and ignoring the weighting term, as suggested by [12], the first term of the variation lower bound can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DDPM}}=\\mathbb{E}_{\\overline{{x_{n}}},\\overline{{\\epsilon_{n}}},n,y_{0}}\\left\\|\\widetilde{\\epsilon_{n}}(\\overline{{\\mathbf{x}_{n}}},n,y_{0})-\\overline{{\\epsilon_{n}}}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Additionally, considering a Generalized Extreme Value (GEV) distribution for block maxima, the second term is simplified as $\\log{\\mathcal{L}}_{\\mathrm{GEV}}(\\mu,\\sigma,\\xi)$ , as defined in Eq. 4. ", "page_idx": 5}, {"type": "text", "text": "The preceding theorem establishes a clear link between the variational lower bound and an interpretable objective loss function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\log p_{\\theta}(\\mathbf{x}_{0},y_{0})\\le\\mathcal{L}_{\\mathrm{DDPM}}-\\lambda\\log\\mathcal{L}_{\\mathrm{GEV}}(\\mu,\\sigma,\\xi):=L\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, ${\\mathcal{L}}_{\\mathrm{DDPM}}$ represents the expected reconstruction error between actual and estimated noise, and $\\log{\\mathcal{L}}_{\\mathrm{GEV}}(\\mu,\\sigma,\\xi)$ captures the negative log-likelihood of the block maxima governed by the GEV distribution. ", "page_idx": 5}, {"type": "text", "text": "4.4 GEV Distribution Enforcement Module ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To enforce fidelity on both the block maxima and overall data distribution, we incorporate the Generalized Extreme Value (GEV) distribution within the DDPM framework following Theorem 3. We first fit a GEV distribution using maximum log-likelihood estimation with all the block maxima $(y_{0})$ values in the training data. The ftited GEV distribution is parameterized by $\\mu,\\sigma$ , and $\\xi$ , denoted as $\\theta_{\\mathrm{gev}}=\\{\\mu,\\sigma,\\xi\\}$ . Using the conditional diffusion process, the estimated noise is given by $\\widetilde{\\epsilon}_{n}(\\overline{{\\mathbf{x}_{n}}},n,y_{0})$ . Consequently, the estimated denoised sample can be obtained as: $\\widetilde{\\mathbf{x}}_{0}=\\overline{{\\mathbf{x}_{n}}}-\\widetilde{\\epsilon}_{n}$ . Then, utilizing the fitted GEV distribution, the log-likelihood of the estimated denoised block maxima, $\\widetilde{y}_{0}\\;=\\;\\operatorname*{max}_{\\tau\\in\\{1,\\cdots,T\\}}\\widetilde{\\mathbf{x}}_{0}^{\\tau}$ , is calculated. This negative log-likelihood, $-\\log\\mathcal{L}_{\\mathrm{GEV}}(\\mu,\\sigma,\\xi,\\widetilde{y}_{0})$ , is f inally incorporated in t o the loss function of training. ", "page_idx": 6}, {"type": "text", "text": "4.5 Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Algorithm 1 summarizes the pseudocode for training and Algorithm 2 summarizes the pseudocode for the sampling step of FIDE. The overall loss function $\\mathcal{L}_{\\mathtt{F I D E}}$ is constructed by combining two key components: the DDPM loss ${\\mathcal{L}}_{\\mathrm{DDPM}}$ and the negative log-likelihood of the Generalized Extreme Value (GEV) distribution $-\\mathcal{L}_{\\mathrm{GEV}}$ . The formulation is expressed as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{{\\mathcal{L}}_{\\mathtt{F I D E}}}&{=}&{\\mathbb{E}_{\\overline{{\\mathbf{x}}}_{n},\\overline{{\\epsilon}}_{n},n,y_{0}}\\left\\|\\widetilde{\\epsilon}_{n}(\\overline{{\\mathbf{x}_{n}}},n,y_{0})-\\overline{{\\epsilon_{n}}}\\right\\|_{2}^{2}-\\lambda\\cdot\\log{\\mathcal{L}}_{\\mathtt{G E V}}(\\mu,\\sigma,\\xi,\\widetilde{y}_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ is a hyperparameter controlling the influence of the GEV distribution on the loss. ", "page_idx": 6}, {"type": "text", "text": "In this context, ${\\mathcal{L}}_{\\mathrm{DDPM}}$ evaluates the mean squared difference between the estimated noise term $\\widetilde{\\epsilon}_{n}$ and the true noise term $\\epsilon_{n}$ within the conditional diffusion process. Its purpose is to guide the generative model towards effectively capturing the underlying data distribution. The second element, $-\\log\\mathcal{L}_{\\mathrm{GEV}}(\\mu,\\sigma,\\xi,\\widetilde{y}_{0})$ , encapsulates the negative log-likelihood of the GEV distribution. This component assesses ho  w well the fitted GEV distribution aligns with the estimated block maxima values $\\widetilde{y}_{0}$ derived from the denoised samples. Here, the log-likelihood has a negative sign to indicate a minimization objective, aligning with the overall goal of minimizing the loss function. ", "page_idx": 6}, {"type": "table", "img_path": "5HQhYiGnYb/tmp/d7d720137b95d81af905b4743e766cc58c9a3d1e51bc7e41a19d455654d4caff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experimental Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have performed extensive experiments to evaluate the performance of our FIDE framework. All the code and datasets used in this paper are available at https://github.com/galib19/FIDE. The datasets used are described in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "We compared our proposed framework against various generative models: (1) GAN-based: We utilize two GAN-based approaches as our baselines. The first approach is Conditional GAN (cGAN [15]), which introduces conditional information to the training process, enabling targeted generation based on specified conditions. The second baseline is TimeGAN [21], which is a generative model designed specifically for time-series generation. (2) VAE-based: We employ beta-VAE [11], conditional betaVAE [16], and TimeVAE [6] as baseline methods for comparison. Both beta-VAE and conditional beta-VAE incorporate a specific disentanglement objective to encourage the model to learn more interpretable and factorized representations while TimeVAE [6] promotes interpretability. (3) Flowbased: We use normalizing flows-based approaches such as RealNVP [7] and Fourier-Flows [1] as our baseline methods. (4) Diffusion-based: We consider two baselines for comparison, namely, the denoising diffusion probabilistic model (DDPM) [12] and time series diffusion model called Diffusion-TS [22]. ", "page_idx": 6}, {"type": "table", "img_path": "5HQhYiGnYb/tmp/5d125082643bf576759596622f0af9b97abf67473f35a516479434fb3b46f9be.jpg", "table_caption": ["Table 1 Comparison of generated samples\u2019 block maxima distribution metrics and predictive score using the various methods. Bold and Underlined entries denote the best and second-best result "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We partitioned each dataset into training, validation, and testing, according to a 8:1:1 ratio. We repeated the experiments 5 times. Prior to applying the various algorithms, the time series data is standardized to have zero mean and unit variance. The encoder component of our framework employs a 3-layer transformer architecture, accompanied by fully connected layers. The training was facilitated using the Adam optimizer. For all the methods, we perform extensive hyperparameter tuning on the length of the embedding vector, the number of hidden layers, the number of nodes, the learning rate, and the batch size. The optimal hyperparameters were determined using the Ray Tune framework, integrating an Asynchronous Successive Halving Algorithm (ASHA) scheduler to enable early stopping. All experiments were conducted on NVIDIA T4 GPU. ", "page_idx": 7}, {"type": "text", "text": "To assess the effectiveness of the proposed framework, we utilize four metrics: Jensen-Shannon (JS) Divergence, KL Divergence, CRPS (Continuous Rank Probability Score), and Predictive Score. The first three metrics examine how well the generated samples fit the original data distribution. The fourth metric, Predictive Score [21], evaluates the generative model\u2019s ability to replicate the temporal characteristics of the original data. This is done by training an LSTM-based sequence model for time series forecasting using the synthetic samples produced by each generative model. The model\u2019s performance is measured by its mean absolute error (MAE) on the original test data, providing insight into how well the generative model preserves the temporal patterns of the data. In short, the evaluation focuses on forecasting block maxima on the test dataset using the model trained on generated data. ", "page_idx": 7}, {"type": "table", "img_path": "5HQhYiGnYb/tmp/b800e050a5a0dc6a76a33d2957eaf7df03fe2ab25279f2ef7ae11deb365f8461.jpg", "table_caption": ["Table 2 Comparison of the generated sample distribution for all values using KL divergence and CRPS metrics. Bold and Underlined entries denote the best and second-best result. For the JS Divergence and Predictive Score metrics, the results are given in Appendix E. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "5HQhYiGnYb/tmp/521168bf7d238cf991bb70676a5505f24e54872a7b9d892af732ad45f631dfc0.jpg", "img_caption": ["Figure 5: Comparison of block maxima distribution and all values distribution for real and generated samples using the proposed FIDE model and DDPM [12] when applied to the synthetic AR(1) dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 1 compares the performance of FIDE against the various baselines in terms of their ability to capture the block maxima distribution for 5 diverse datasets (AR1, Stock, Energy, Temperature, and ECG). In terms of the distribution metrics (JS divergence, KL divergence, and CRPS), FIDE consistently achieves the best results, providing evidence of FIDE\u2019s superior performance in preserving the block maxima distribution. For the Predictive Score metric, FIDE achieves the best results in 3 out of 5 datasets and ranks second in the remaining 2 datasets. To further illustrate FIDE\u2019s capabilities, Figures 5-(a) and (b) compare the distribution of block maxima values generated by DDPM [12] and FIDE for the AR(1) dataset. Note that, while DDPM struggles to capture the block maxima distribution accurately, FIDE generates samples that more faithfully preserve the fidelity of the distribution. This improvement is particularly noticeable in the upper tail behavior, which is critical for applications that require precise modeling of extreme block maxima values. This superior performance is not surprising as it directly results from our method\u2019s emphasis on block maxima distribution, achieved through the introduction of frequency inflation, conditional generation based on block maxima, and incorporation of the GEV distribution into the generative modeling framework. ", "page_idx": 8}, {"type": "text", "text": "As FIDE prioritizes the accurate modeling of block maxima, we have also evaluated its efficacy in capturing the distribution of all (block maxima and non-block maxima) values in time series. The results are shown in Tables 2 and 4 (in Appendix E). Note that FIDE achieves comparable performance to state-of-the-art methods like DDPM [12] and Diffusion-TS [22]. This is further illustrated by the distribution plots of all values for DDPM and FIDE given in Figure 5-(c) and (d). The results in Table 2 also show that FIDE consistently outperforms VAE-based, GAN-based, and Flow-based alternatives. For Predictive Score, while TimeGAN and TimeVAE show marginally better results, FIDE maintains competitive performance against other baseline methods. These results suggest minimal performance degradation when applying FIDE to time series data. Despite its emphasis on block maxima values, this does not significantly compromise its ability to model the overall distribution. This positions FIDE as a robust and versatile generative model for capturing extreme values in time series. ", "page_idx": 8}, {"type": "table", "img_path": "5HQhYiGnYb/tmp/47ee1ab3d8ac253cb14433b887664f1d59419aa5e09d1ea9d28de1d9d307128a.jpg", "table_caption": ["Table 3 Ablation Study of generated samples\u2019 block maxima distribution metrics and predictive score using the proposed FIDE model and without individual component of the model "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In our ablation study depicted in Table 3, we systematically assessed the individual contributions of each component within our proposed framework. By selectively deactivating elements such as the GEV loss, conditional block maxima input, and high-frequency inflation module, we observed consistent performance degradation across all scenarios. Notably, the absence of the conditional block maxima input significantly impacted the Jenson-Shannon Divergence and KL Divergence metrics, while the lack of the GEV loss had the most pronounced effect on the CRPS metric. Surprisingly, the predictive score remained relatively resilient to the deactivation of any single component, suggesting a degree of redundancy or compensatory mechanisms among the remaining components. Overall, our ablation study highlights the indispensable role of each component in achieving optimal performance in our model. In summary, our findings underscore the holistic importance of the individual components, with their synergistic interplay contributing to the overall effectiveness of FIDE. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This framework examines the challenges of applying diffusion models to capture extreme values in time series. Through a comprehensive exploration of the constraints within current diffusion-based models, the proposed FIDE framework addresses these limitations by introducing a novel strategy to maintain high-frequency components of the time series. FIDE extends conventional diffusion models to enable conditional generation of block maxima by integrating a loss function based on the generalized extreme value (GEV) distribution. The superiority of the framework over various baseline methods is validated through rigorous experiments on both synthetic and real-world data. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the U.S. National Science Foundation under grant IIS-2006633. Any use of trade, firm, or product names are for descriptive purposes only and do not imply endorsement by the U.S. Government. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ahmed Alaa, Alex James Chan, and Mihaela van der Schaar. 2021. Generative Time-series Modeling with Fourier Flows. In International Conference on Learning Representations. [2] Marin Bilo\u0161, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan G\u00fcnnemann. 2023. Modeling Temporal Data as Continuous Functions with Stochastic Process Diffusion. In International Conference on Machine Learning. PMLR, 2452\u20132470. [3] Luis Candanedo. 2017. Appliances Energy Prediction. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5VC8G.   \n[4] Stuart Coles, Joanna Bawa, Lesley Trenner, and Pat Dorazio. 2001. An introduction to Statistical Modeling of Extreme Values. Vol. 208. Springer. [5] Jonathan Crabb\u00e9, Nicolas Huynh, Jan Stanczuk, and Mihaela van der Schaar. 2024. Time Series Diffusion in the Frequency Domain. arXiv preprint arXiv:2402.05933 (2024).   \n[6] Abhyuday Desai, Cynthia Freeman, Zuhui Wang, and Ian Beaver. 2021. TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation. arXiv preprint arXiv:2111.08095 (2021).   \n[7] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2016. Density Estimation using Real NVP. arXiv preprint arXiv:1605.08803 (2016).   \n[8] Vincent Fortuin, Dmitry Baranchuk, Gunnar R\u00e4tsch, and Stephan Mandt. 2020. GP-VAE: Deep Probabilistic Time Series Imputation. In International conference on artificial intelligence and statistics. PMLR, 1651\u20131661.   \n[9] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. 2000. PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. circulation 101, 23 (2000), e215\u2013e220.   \n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. Advances in neural information processing systems 27 (2014).   \n[11] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2016. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In International conference on learning representations.   \n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. Advances in neural information processing systems 33 (2020), 6840\u20136851.   \n[13] Diederik P Kingma and Max Welling. 2013. Auto-encoding Variational Bayes. arXiv preprint arXiv:1312.6114 (2013).   \n[14] Andrew McDonald, Pang-Ning Tan, and Lifeng Luo. 2022. COMET Flows: Towards Generative Modeling of Multivariate Extremes and Tail Dependence. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. 3328\u20133334.   \n[15] Olof Mogren. 2016. C-RNN-GAN: Continuous Recurrent Neural Networks with Adversarial Training. arXiv preprint arXiv:1611.09904 (2016).   \n[16] Alexander Nikitin, Letizia Iannucci, and Samuel Kaski. 2023. TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series. arXiv preprint arXiv:2305.11567 (2023).   \n[17] Australian Bureau of Meteorology. 2016. https://www.kaggle.com/datasets/ suprematism/daily-minimum-temperatures   \n[18] Danilo Rezende and Shakir Mohamed. 2015. Variational Inference with Normalizing Flows. In International conference on machine learning. PMLR, 1530\u20131538.   \n[19] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-based Generative Modeling through Stochastic Differential Equations. arXiv preprint arXiv:2011.13456 (2020).   \n[20] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation. Advances in Neural Information Processing Systems 34 (2021), 24804\u201324816.   \n[21] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. 2019. Time-series Generative Adversarial Networks. Advances in neural information processing systems 32 (2019).   \n[22] Xinyu Yuan and Yan Qiao. 2024. Diffusion-TS: Interpretable Diffusion for General Time Series Generation. arXiv preprint arXiv:2403.01742 (2024).   \n[23] Mingda Zhang. 2018. Time Series: Autoregressive models AR, MA, ARMA, ARIMA. University of Pittsburgh (2018). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Time series generation has been a subject of extensive research, leveraging a variety of statistical [23] and machine-learning [16, 21] techniques to capture temporal dependencies and complexities within data. Generative methods, including Generative Adversarial Networks (GANs) [10], Variational Autoencoders (VAEs) [13], normalizing flows [18], and diffusion-based approaches [12, 19], have demonstrated efficacy in time series generation and garnered interest due to their ability to learn underlying data distributions for data generation. Normalizing flows are constrained by their computational complexity, limited expressiveness, and suboptimal sample quality, thereby restricting their capacity for effective modeling. Numerous works have delved into enhancing GANs, introducing variations like RcGAN [15] and TimeGAN [21], which have demonstrated improvements in generating realistic time series data. TimeGAN [21] specifically adopts a GAN architecture to generate time-series data, employing an encoder and decoder to transform a time-series sample into latent vectors. However, GAN-based generative models are susceptible to issues like mode collapse and unstable behavior during training. While VAEs have not been extensively applied to synthetic time series generation, their effectiveness in addressing related challenges, such as time series imputation [8], suggests their potential utility in this domain. Diffusion-based models are also gaining traction for their ability to generate high-quality data such as images and videos, bypassing the challenges associated with discriminator networks in GANs and avoiding the artifact-prone lower-dimensional latent spaces of VAEs. There are a couple of diffusion-based works [20, 2] that have been employed for time series, but they are specifically designed for discriminative tasks. ", "page_idx": 12}, {"type": "text", "text": "While generative AI for time series offers numerous advantages, it has not been extensively explored, especially in terms of modeling extreme values. The difficulty of modeling extremes using generative models such as normalizing flows [14] has been recognized in previous research. Studies by Wiese et al. (2019) and Jaini et al. (2020) highlight the inability of normalizing flows to accurately capture heavy-tailed marginal distributions. Specifically, these studies show that any attempt to map heavy-tailed distributions to light-tailed distributions (e.g., Gaussian) cannot maintain Lipschitzboundedness. However, this challenge remains largely unexplored within the realm of diffusion models. ", "page_idx": 12}, {"type": "text", "text": "B Relationship between Abrupt Block Maxima and High Frequency Components ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section presents the relationship between abrupt block maxima and high frequency components of a time series. ", "page_idx": 12}, {"type": "image", "img_path": "5HQhYiGnYb/tmp/3cedc5580c5c4d4e4aa5b1ac58b6ce2595813869a6336cc735df3a40beeba43a.jpg", "img_caption": ["Figure 6: Percentile distribution of first order derivatives for the block maxima values in different time series datasets. Observe that the derivatives tend to exhibit elevated percentile values. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Definition 1 (Abrupt Block Maxima). Let, $\\mathbf{x}_{0}\\in\\mathbb{R}^{T}$ be a time series of length $T$ and $y_{0}\\equiv x_{0}^{\\tau}=$ $\\operatorname*{max}_{t\\in1,\\ldots,T}x_{0}^{t}$ be its block maxima, where $\\tau$ is the time step of the block maxima. Then, $x_{0}^{\\tau}$ is considered an abrupt block maxima $\\begin{array}{r}{i f\\,\\frac{d x}{d t}|_{t=\\tau}>\\rho,}\\end{array}$ , where $\\rho$ is a threshold. ", "page_idx": 12}, {"type": "image", "img_path": "5HQhYiGnYb/tmp/0bd20ef7703af7ac41eabf1442bcf2b8f619521d11f56116e4408b5e2257bfee.jpg", "img_caption": ["Figure 7: The summation of high frequency terms for abrupt (A) changes is consistently higher than for smooth (S) changes. (A) denotes abrupt changes, (S) denotes smooth changes. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "We argue that the block maxima values in real-world time series often exhibit an abrupt change behavior compared to the non-block maxima values. To substantiate this, we conduct an empirical analysis across five distinct datasets, wherein we assess the percentile distribution of the first-order derivatives associated with the block maxima values, as depicted in Figure 6. Specifically, given a time series, we first partition it into a set of disjoint time windows and compute the block maxima value within each window along with the first-order difference, $\\Delta x^{t}=x_{0}^{t}-\\bar{x}_{0}^{t-1}$ , for each time step $t$ . We then compute the percentile of $\\Delta x^{\\tau}$ associated with the block maxima $\\boldsymbol{x}_{0}^{\\tau}$ of the window and plot its distribution using a boxplot as shown in Figure 6. Our findings affirm the conjecture that the block maxima tends to exhibit an elevated value for its time derivative (with a median larger than $70\\%$ of all first-order differences), thereby indicating a notable association between block maxima occurrences and the abrupt changes in a time series. ", "page_idx": 13}, {"type": "text", "text": "More importantly, the abrupt changes are strongly influenced by the high frequency components of the time series. This can be observed by differentiating the inverse Fourier transform shown in Equation 2 and decomposing the derivative into low and high frequency components: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{d x_{m,0}^{t}}{d t}}&{=}&{\\displaystyle\\frac{1}{T}\\sum_{k=1}^{T}f_{m,0}^{k}\\frac{d}{d t}\\left[e^{i2\\pi t k/T}\\right]=\\frac{1}{T}\\sum_{k=1}^{T}\\frac{i2\\pi k}{T}f_{m,0}^{k}e^{i2\\pi t k/T}=\\frac{1}{T}\\sum_{k=1}^{T}i\\omega_{k}f_{m,0}^{k}e^{i\\omega_{k}t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\omega_{k}=\\frac{2\\pi k}{T}}\\end{array}$ . Let $\\kappa$ be the threshold index for dividing the frequencies into low $(k\\leq\\kappa)$ and high $[k>\\kappa)$ frequency components. Then, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d x_{m,0}^{t}}{d t}=\\frac{1}{T}\\sum_{l=1}^{\\kappa}i\\omega_{l}f_{m,0}^{l}e^{i\\omega_{l}t}+\\frac{1}{T}\\sum_{h=\\kappa+1}^{T}i\\omega_{h}f_{m,0}^{h}e^{i\\omega_{h}t}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To illustrate the impact of high frequency components on the abrupt changes, we compute the value of the second term in Equation (7) for time steps with abrupt1 (A) and non-abrupt (S) changes for various datasets. The results shown in Fig. 7 suggest that the sum of high frequency terms for abrupt changes is consistently higher than the sum of high frequency terms for non-abrupt changes. In essence, the abrupt block maxima values often manifest as high-frequency components in the Fourier domain as they introduce sharp transitions in the time domain signal. This explains the high residual shown in Fig. 2 for the block maxima when the high frequency components are zeroed out. ", "page_idx": 13}, {"type": "text", "text": "C Theoretical Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $\\epsilon_{n}^{t}$ be the Gaussian noise added in the diffusion step $n$ during the forward process of the diffusion model and $\\boldsymbol{x}_{0}^{t}$ be the original value in the time series. The perturbed signal at diffusion step $n$ can be expressed as: $x_{n}^{t}=x_{n-1}^{t}+\\epsilon_{n}^{t}$ . Let $\\sigma_{n}^{2}$ and $\\sigma_{\\epsilon_{n}}^{2}$ be the variances of perturbed time series and noise respectively at diffusion step $n$ while $\\sigma_{n-1}^{2}$ is the variance of the time series at diffusion step $n-1$ ", "page_idx": 13}, {"type": "text", "text": "Note that $\\sigma_{\\epsilon_{n}}^{2}$ increases linearly according to a linear noise scheduler as the diffusion step increases. Let $S_{n}^{f}(\\omega_{k})$ and $S_{n}^{\\varepsilon}(\\omega_{k})$ denote the power spectral density (PSD) for the $k$ -th frequency component of the perturbed time series and noise respectively for the diffusion step $n$ , where $\\begin{array}{r}{\\omega_{k}=\\frac{2\\pi k}{T}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Our theoretical analysis is based on the following assumptions: ", "page_idx": 14}, {"type": "text", "text": "Assumption 1. An abrupt block maxima is linked to the high-frequency components of the time series. ", "page_idx": 14}, {"type": "text", "text": "$(P S D)$ m fpotri odinf f2u.s ioTnh es tneopi $n$ , $\\epsilon_{n}^{t}$ e .i,s $\\forall k:S_{n}^{\\mathcal{E}}(\\omega_{k})\\approx\\sigma_{\\epsilon_{n}}^{2}$ .m process with a constant power spectral density ", "page_idx": 14}, {"type": "text", "text": "Assumption 3. The power spectral density (PSD) of the perturbed time series for diffusion step $n$ can be modeled using the following generalized Gaussian function: ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{n}^{f}(\\omega_{k})=\\sigma_{n}^{2}\\cdot\\exp(-\\alpha_{n}\\vert\\omega_{k}\\vert^{\\beta_{n}})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\alpha_{n}$ is a scaling factor, and $\\beta_{n}$ is a shape parameter. ", "page_idx": 14}, {"type": "text", "text": "Remark 1. The rationale and supporting evidence for Assumption $^{\\,l}$ is presented in Section 3. ", "page_idx": 14}, {"type": "text", "text": "Remark 2. Assumption 2 is intuitive as the Gaussian noise used in the diffusion model has approximately constant PSD over the frequency range of interest. ", "page_idx": 14}, {"type": "text", "text": "Remark 3. Assumption 3 is reasonable as for most real-world time series, the energy spectrum is localized at the lower frequency $(\\omega_{k}\\,\\approx\\,0,$ ), also known as the fundamental frequency, which quickly decays with increasing frequency $(\\omega_{k}\\ \\rightarrow\\ \\omega_{m a x})\\ I5J$ . Therefore, we use the generalized Gaussian function to model this decaying behavior. Note that, the exponential decay behavior will eventually transition into a uniform distribution according to the diffusion model\u2019s forward process. Consequently, the shape $\\beta_{n}$ of the distribution function (Eqn 13) is not constant; instead, it evolves with the diffusion step n to remain consistent with the diffusion model\u2019s forward process. Initially, when n is small, $1\\leq\\beta_{n}\\leq2,$ , representing an exponential decay. However, as $n\\rightarrow N$ , where $N$ is the final diffusion step, $\\beta_{n}\\to\\infty$ , and the PSD transitions to a uniform distribution. This transformation occurs because, according to the forward process of the diffusion model, as $n$ increases, Gaussian noise with linearly increasing variance is added to the perturbed time series, making the signal increasingly noise-like until it becomes white noise at step $N$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 1. The difference between the variance of the perturbed time series $x_{n}^{t}$ and the variance of the noise $\\epsilon_{n}^{t}$ is equal to a constant $\\zeta$ such that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}-\\sigma_{\\epsilon_{n}}^{2}=\\zeta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. First, we have the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{n}^{t}=x_{n-1}^{t}+\\epsilon_{n}^{t}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying the variance operator to both sides of Equation 15 and utilizing the property that the variance of a sum of independent random variables is the sum of their individual variances, we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}=\\sigma_{n-1}^{2}+\\sigma_{\\epsilon_{n}}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\sigma_{\\epsilon_{n}}^{2}$ denotes the variance of the noise at step $n$ . Recursively applying Equation 16, we can express the variance of the perturbed time series at step $n$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}=\\sigma_{0}^{2}+\\sum_{i=1}^{n}\\sigma_{\\epsilon_{i}}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Subtracting $\\sigma_{\\epsilon_{n}}^{2}$ from both sides of Equation 17, we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{n}^{2}-\\sigma_{\\epsilon_{n}}^{2}=\\sigma_{0}^{2}+\\sum_{i=1}^{n-1}\\sigma_{\\epsilon_{i}}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the difference between the variance of the perturbed time series and the variance of the noise at step n is equal to the constant \u03b6 =  in=\u221211 \u03c3\u03f52i, which completes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "The lemma establishes a crucial bound on the difference between the variances of the perturbed time series and the noise, which is leveraged in the subsequent theorem to analyze the behavior of the Fourier transform of the perturbed time series at low and high frequencies. ", "page_idx": 14}, {"type": "text", "text": "We now provide the proof for Theorem 1 in the main paper. ", "page_idx": 14}, {"type": "text", "text": "Proof for Theorem 1 Using Lemma 1 and Assumptions 1, 2, and 3, we can prove the theorem as follows: For low frequencies, i.e., $\\omega_{k}\\rightarrow0$ , taking the limit as $\\omega_{k}\\rightarrow0$ on the expression for the signal power spectral density $S_{n}^{f}(\\omega)$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to0}S_{n}^{f}(\\omega_{k})=\\operatorname*{lim}_{k\\to0}\\sigma_{n}^{2}\\cdot\\exp(-\\alpha_{n}|\\omega_{k}|^{\\beta_{n}})=\\sigma_{n}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $\\exp(-\\alpha_{n}|0|^{\\beta_{n}})=\\exp(0)=1$ . Therefore, as $k\\rightarrow0$ , $S_{n}^{f}(\\omega_{k})=|f_{n}^{k}|^{2}$ approaches the variance $\\sigma_{n}^{2}$ . So, we can write: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to0}S_{n}^{f}(\\omega_{k})=\\operatorname*{lim}_{k\\to0}|f_{n}^{k}|^{2}=\\sigma_{n}^{2}=\\zeta+\\sigma_{\\epsilon_{n}}^{2}=\\sigma_{0}^{2}+\\sum_{i=1}^{n-1}\\sigma_{\\epsilon_{i}}^{2}+\\sigma_{\\epsilon_{n}}^{2}=\\sigma_{0}^{2}+\\sum_{i=1}^{n}\\sigma_{\\epsilon_{i}}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, for high frequencies, i.e., $k\\rightarrow k_{\\mathrm{max}}$ , taking the limit as $\\omega\\rightarrow\\omega_{\\mathrm{max}}$ on the expression for the signal power spectral density $S_{n}^{f}(\\omega_{k})$ (Eq. (13)), we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to k_{\\mathrm{max}}}S_{n}^{f}(\\omega_{k})=\\operatorname*{lim}_{k\\to k_{\\mathrm{max}}}\\sigma_{n}^{2}\\cdot\\exp(-\\alpha_{n}|\\omega|^{\\beta_{n}})\\to\\sigma_{n}^{2}\\cdot\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\delta=f_{n}^{k_{\\operatorname*{max}}}\\ll1$ ", "page_idx": 15}, {"type": "text", "text": "As $k\\rightarrow k_{\\mathrm{max}}$ , $S_{n}^{f}(\\omega_{k})$ approaches $\\sigma_{n}^{2}\\cdot\\delta$ . We can also write: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to k_{\\mathrm{max}}}|f_{n}^{k}|^{2}=\\delta\\cdot\\left(\\zeta+\\sigma_{\\epsilon_{n}}^{2}\\right)=\\delta\\cdot\\left(\\sigma_{0}^{2}+\\sum_{i=1}^{n}\\sigma_{\\epsilon_{i}}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking the ratio of high-frequency and low-frequency components after perturbations yields: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{lim}_{k\\to k_{\\operatorname*{max}}}|f_{n}^{k}|^{2}}{\\operatorname*{lim}_{k\\to0}|f_{n}^{k}|^{2}}=\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thereby, high-frequency components or abrupt block maxima dissipate rapidly compared to lowfrequency components or smooth changes. Our findings shed light on a fundamental limitation of diffusion models while modeling block maxima and underscore the need for tailored approaches to preserve the block maxima consistent with the other values and to address the accurate representation of block maxima distributions. ", "page_idx": 15}, {"type": "text", "text": "We now provide the proof for Theorem 2 in the main paper. ", "page_idx": 15}, {"type": "text", "text": "Proof for Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Using Lemma 1 and Assumptions 1, 2, and 3, we can prove the theorem as follows: For low frequencies, i.e., $\\omega_{k}\\rightarrow0$ , taking the limit as $\\omega_{k}\\rightarrow0$ on the expression for the signal power spectral density $\\overline{{S_{n}^{f}}}(\\omega)$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to0}\\overline{{S_{n}^{f}}}(\\omega_{k})=\\operatorname*{lim}_{k\\to0}\\sigma_{n}^{2}\\cdot\\exp(-\\alpha_{n}|\\omega_{k}|^{\\beta_{n}})=\\sigma_{n}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $\\exp(-\\alpha_{n}|0|^{\\beta_{n}})=\\exp(0)=1$ . Therefore, as $k\\rightarrow0$ , $\\overline{{S_{n}^{f}}}(\\omega_{k})=|f_{n}^{k}|^{2}$ approaches the variance $\\sigma_{n}^{2}$ . So, we can write: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to0}\\overline{{S_{n}^{f}}}(\\omega_{k})=\\operatorname*{lim}_{k\\to0}|\\overline{{f_{n}^{k}}}|^{2}=\\sigma_{n}^{2}=\\zeta+\\sigma_{\\epsilon_{n}}^{2}=\\sigma_{0}^{2}+\\sum_{i=1}^{n-1}\\sigma_{\\epsilon_{i}}^{2}+\\sigma_{\\epsilon_{n}}^{2}=\\sigma_{0}^{2}+\\sum_{i=1}^{n}\\sigma_{\\epsilon_{i}}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, for high frequencies, i.e., $k\\rightarrow k_{\\mathrm{max}}$ , taking the limit as $\\omega\\rightarrow\\omega_{\\mathrm{max}}$ on the expression for the signal power spectral density $\\overline{{S_{n}^{f}}}(\\omega_{k})$ (Eq. (13)), we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to k_{\\mathrm{max}}}\\overline{{S_{n}^{f}}}(\\omega_{k})=\\operatorname*{lim}_{k\\to k_{\\mathrm{max}}}\\sigma_{n}^{2}\\cdot\\exp(-\\alpha_{n}|\\omega|^{\\beta_{n}})\\to\\sigma_{n}^{2}\\cdot\\delta^{\\prime}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\delta^{\\prime}=\\delta\\cdot\\gamma$ ", "page_idx": 16}, {"type": "text", "text": "As $k\\rightarrow k_{\\mathrm{max}},\\overline{{S_{n}^{f}}}(\\omega_{k})$ approaches $\\sigma_{n}^{2}\\cdot\\delta\\cdot\\gamma$ . We can also write: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to k_{\\mathrm{max}}}|\\overline{{f_{n}^{k}}}|^{2}=\\delta\\cdot\\gamma\\cdot\\left(\\zeta+\\sigma_{\\epsilon_{n}}^{2}\\right)=\\delta\\cdot\\gamma\\cdot\\left(\\sigma_{0}^{2}+\\sum_{i=1}^{n}\\sigma_{\\epsilon_{i}}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking the ratio of high-frequency and low-frequency components after perturbations yields: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname*{lim}_{k\\to k_{\\operatorname*{max}}}|{\\overline{{f_{n}^{k}}}}|^{2}}{\\operatorname*{lim}_{k\\to0}|{\\overline{{f_{n}^{k}}}}|^{2}}}=\\delta\\cdot\\gamma\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof for Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The proof begins by expressing the negative log-likelihood of the joint distribution $-\\log p_{\\theta}(\\mathbf{x}_{0},y_{0})$ in terms of conditional probabilities: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log p_{\\theta}(\\mathbf{x}_{0},y_{0})=-\\log p_{\\theta}(\\mathbf{x}_{0}|y_{0})\\cdot p_{\\theta}(y_{0})=-\\log p_{\\theta}(\\mathbf{x}_{0}|y_{0})-\\log p_{\\theta}(y_{0})}\\\\ &{\\phantom{-\\log p_{\\theta}(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0})}\\leq D_{\\mathrm{KL}}(q(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0})|\\|p_{\\theta}(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0}))}\\\\ &{\\phantom{-\\log p_{\\theta}(\\mathbf{x}_{0}|y_{0})}-\\log p_{\\theta}(y_{0})}\\\\ &{\\phantom{-\\log p_{\\theta}(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0})}=\\mathbb{E}_{q}\\bigg[\\log\\frac{q(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0})}{p_{\\theta}\\left(\\mathbf{x}_{0:N}\\right)}+\\log p_{\\theta}(\\mathbf{x}_{0},y_{0})\\bigg]}\\\\ &{\\phantom{-\\log p_{\\theta}(y_{0})}-\\log p_{\\theta}(y_{0})}\\\\ &{\\phantom{-\\log p_{\\theta}(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0})}=\\mathbb{E}_{q}\\bigg[\\log\\frac{q(\\mathbf{x}_{1:N}|\\mathbf{x}_{0},y_{0})}{p_{\\theta}\\left(\\mathbf{x}_{0:N}\\right)}\\bigg]-\\log p_{\\theta}(y_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We performed our experiments using the following datasets. (1) Synthetic Data (AR2): AR(2) dataset comprises synthetic time series data generated using an autoregressive model of order 2. (2) Financial Data (Stocks): It features continuous-valued and aperiodic sequences, such as daily historical Google stocks data spanning from 2004 to 2019. We consider the adjusted closing price data for this work. (3) Energy Data (Appliance Energy): The UCI Appliances energy prediction dataset [3] encompasses multivariate, continuous-valued measurements. We consider appliance energy data for analysis. (4) Weather/Climate Data (Daily Minimum Temperature): This dataset [17] comprises daily minimum temperatures in Melbourne, Australia, from 1981 to 1990. (5) Medical Data (ECG5000: Congestive Heart Failure): The original dataset [9] for \"ECG5000\" originates from a 20-hour long electrocardiogram (ECG) obtained from the Physionet database. Specifically, it is derived from the BIDMC Congestive Heart Failure Database (chfdb), with the record labeled as \"chf07.\" The processed data encompasses 5,000 heartbeats randomly selected from the original dataset. ", "page_idx": 16}, {"type": "text", "text": "E Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 4 reports the evaluation of all values in time series using JS Divergence and predictive score, comparing the performance of our proposed method against baseline methods. ", "page_idx": 16}, {"type": "text", "text": "Table 4 Comparison of generated samples\u2019 (all values) JS Divergence and Predictive Score using the baselines methods. Bold and Underlined entries denote the best and second-best result. ", "page_idx": 17}, {"type": "table", "img_path": "5HQhYiGnYb/tmp/cceb129b786ecc993ec4cb02d6285a926006b7f82a2056e7671bfe6a5bbe18ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: See 1. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: See 6. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See C. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See 5. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See 5. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See 5. In all Tables, we report the standard deviation with 5 runs. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See 5. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: It does not use user data and it verifies all the information provided. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: NA ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See 5 and A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See 5. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: NA ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]