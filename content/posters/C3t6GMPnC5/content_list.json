[{"type": "text", "text": "Mamba State-Space Models Can Be Strong Downstream Learners ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Mamba [22] state-space models (SSMs) have recently outperformed state-of-the  \n2 art (SOTA) Transformer large language models (LLMs) in various tasks and been   \n3 widely adapted. However, Mamba\u2019s downstream learning capabilities remain ei  \n4 ther unexplored\u2013e.g., mixed-precision (MPFT) and parameter-efficient fine-tuning   \n5 (PEFT)\u2013or under-evaluated\u2013e.g., in-context learning (ICL). For the latter, recent   \n6 works [45, 19] reported Mamba\u2019s ICL rivals SOTA Transformer LLMs using non  \n7 standard benchmarks. In contrast, we show that on standard benchmarks, pretrained   \n8 Mamba models achieve only $38\\%$ of the ICL performance improvements (over   \n9 zero-shot) of comparable Transformers.   \n10 Enabling MPFT and PEFT in Mamba architectures is challenging due to recurrent   \n11 dynamics and highly customized CUDA kernels, respectively. However, we prove   \n12 that Mamba\u2019s recurrent dynamics are robust to small input changes using dynamical   \n13 systems theory. Empirically, we show that performance changes in Mamba\u2019s   \n14 inference and fine-tuning due to mixed-precision align with Transformer LLMs.   \n15 Furthermore, we show that targeting key memory buffers in Mamba\u2019s customized   \n16 CUDA kernels for low-rank adaptation regularizes SSM parameters, thus achieving   \n17 parameter efficiency while retaining speedups. We show that combining MPFT and   \n18 PEFT enables up to 2.15 times more tokens-per-second and $65.5\\%$ reduced per  \n19 token-memory compared to full Mamba fine-tuning, while achieving up to $81.5\\%$   \n20 of the ICL performance improvements (over zero-shot) of comparably fine-tuned   \n21 Transformers. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 Innovating on previous state-space models (SSMs) [23, 11], Mamba [22] has been recently proposed   \n24 as an accurate, sub-quadratic alternative to Transformer large language models (LLMs). Mamba was   \n25 initially shown to greatly outperform comparable Transformer LLMs [5] across a large number of   \n26 standard natural language benchmarks. Subsequently, pretrained Mamba models have been widely   \n27 adapted across different data modalities [42, 65, 36, 46, 37], tasks [60, 62, 48, 63, 57, 37, 2], and   \n28 architectures [1, 45, 40].   \n29 However, despite such rapid and widespread adaptation, evaluation of Mamba\u2019s ability to perform   \n30 standard downstream learning abilities exhibited by Transformer-based LLMs have either not been   \n31 extensively conducted on standard natural benchmarks or are completely lacking. For instance, while   \n32 recent works [45, 19, 30] have evaluated Mamba\u2019s ability to perform in-context learning (ICL), such   \n33 studies focused extensively on either non-natural tasks [30, 17] or non-standard benchmarks [25].   \n34 Furthermore, evaluation of Mamba\u2019s mixed-precision fine-tuning (MPFT) and performance efficient   \n35 fine-tuning (PEFT) capabilities are currently lacking. For the former, MPFT (and, by extension,   \n36 mixed-precision inference) are made difficult due to potential sensitivities of Mamba\u2019s recurrent   \n37 dynamics, where [21, 29] suggest full precision (FP32) is required to perform stable training. For   \n38 the latter, PEFT via standard low-rank adaptation (LoRA) [28] is made difficult within Mamba\u2019s   \n39 SSM layer (referred to herein as the MambaBlock) due highly customized SSM CUDA kernels which   \n40 provide competitive performance to attention-based speedups [10] at the cost of standard adapter   \n41 support. However, PEFT and MPFT are arguably two of the most widely utilized techniques for LLM   \n42 alignment [53] and customization [55], and are typically combined to drastically decrease hardware   \n43 demands needed to fine-tune modern LLMs [12].   \n44 Herein, we extensively explore Mamba\u2019s downstream learning capabilities across standard natural   \n45 benchmarks. For ICL, we show that, in contrast to recent non-standard studies showing Mamba   \n46 models rival state-of-the-art (SOTA) LLMs of similar parameter counts, the pretrained benefits of   \n47 Mamba few-shot learning are significantly less than comparable Transformer LLMs across   \n48 standard natural benchmarks; averaged across the benchmarks and parameter counts in Table 1,   \n49 Mamba models only achieve ${\\bf38\\%}$ of the performance improvements (relative to zero-shot)   \n50 of comparable Transformer models from the Pythia suite [5]. However, we show in the sequel   \n51 that Mamba models can more than halve this gap through efficient fine-tuning, achieving as   \n52 much as $81.5\\%$ of the average few-shot learning improvement (relative to zero-shot) of comparable   \n53 Transformers.   \n54 For MPFT, we leverage theory from dynamical systems to show that small input changes in a   \n55 MambaBlock do not lead to exponentially deviating outputs. Empirically, we validate this theoretical   \n56 result; compared to full-precision, deviations due to mixed-precision for Mamba inference and   \n57 fine-tuning are on par with those demonstrated by Transformer LLMs (Section 6). For PEFT, we   \n58 show that by targeting the largest memory buffer exploited by Mamba\u2019s highly customized CUDA   \n59 kernels, LoRA may be used for extremely efficient fine-tuning, while simultaneously regularizing   \n60 the majority of Mamba\u2019s SSM parameters via weight tying. We show that this leads to extremely   \n61 efficient PEFT, resulting in up to 2.15 times faster training and $65.5\\%$ reduced memory compared to   \n62 the largest evaluated Mamba model without MPFT or PEFT. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "63 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "64 Downstream learning for LLMs. Since the release of the Transformer architecture [54], attention  \n65 based LLMs have exhibited several downstream learning abilities\u2013in particular, PEFT, MPFT, and   \n66 ICL\u2013which allow the rapid adaptation of foundation models towards specific applications. PEFT using   \n67 adapters [24] allows a large pretrained model to be efficiently adapted for a particular downstream   \n68 task by freezing the full model and training only a small number of extra parameters. Arguably the   \n69 most widely used such PEFT method is LoRA [28], which injects trainable low-rank matrices into   \n70 Transformer layers to approximate weight updates.   \n71 To further decrease the computational demands necessary for LLM fine-tuning and inference, MPFT   \n72 via mixed-precision (i.e., FP16 or BF16) [31, 43] and quantized low-precision [12] have proven   \n73 effective strategies to reduce GPU memory and runtime requirements without deleterious effects on   \n74 downstream performance [12, 59]. Additionally, mixed-precision approaches have paved the way for   \n75 hardware-aware optimizations within the self-attention module [10], greatly mitigating the quadratic   \n76 complexity of Transformer LLMs. Together, PEFT and MPFT have created a rich ecosystem with   \n77 which varying combinations of these approaches may be used to meet the computational constraints   \n78 of a given training system. We note that post-fine-tuning quantization approaches [13] may be further   \n79 used to decrease Transformer LLM computational demands, but such approaches are not considered   \n80 in this work.   \n81 ICL provides an adaptable alternative to fine-tuning. Rather than fine-tune the LLM directly, ICL   \n82 augments a prompt with $n$ relevant examples (called shots) preceding the query of interest. Given   \n83 sufficiently large models and pretraining data [8, 58], Transformer LLMs have proven adept at   \n84 learning new concepts on the fly provided such few-shot prompting. However, it is worth noting   \n85 that ICL inference time increases dramatically as the number of shots grows (due to self-attention\u2019s   \n86 quadratic complexity) and PEFT (when possible) is known to produce more accurate downstream   \n87 learning results [8, 41]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Table 1: In-context learning performance for pretrained Mamba and Pythia models. Models are collected into parameter classes for head-to-head comparison using the groupings in [22]. Model checkpoints were evaluated on all benchmarks and few-shot settings using the LM evaluation harness from Eleuther AI [16]. LAMBADA zero-shot is more effective for the model sizes considered (further discussed in [61, 8]) and thus excluded from few-shot performance averages. Highlighted in bold is the top-performing few-shot learner per benchmark and model grouping. ", "page_idx": 2}, {"type": "table", "img_path": "C3t6GMPnC5/tmp/4e8691ffe0586eb7890132d9c83e47633d7d01c7de5dccf19627608f54e97dba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "88 State-space Models. Structured state-space sequence (S4) models [23, 14] are SSMs which leverage   \n89 linear time-invariant (LTI) systems to combine the computational advantages of Transformers\u2013i.e.,   \n90 highly parallelizable training\u2013and recurrent neural networks (RNNs)\u2013i.e., subquadratic autoregressive   \n91 inference using recurrency. Within the S4 layer, an input signal is discretized and LTI parameters   \n92 representing the input\u2019s latent dynamics are learned. Owing to the S4 block\u2019s latent dynamics being   \n93 LTI, the S4 block\u2019s output may be thus compactly represented as a single convolution between the   \n94 input and an SSM convolution kernel (a matrix whose entries are products of LTI learnable parameters   \n95 resulting from unrolling the state-space equations). However, despite hardware efficiency and   \n96 long-dependency-modeling improvements, LTI-based S4 models remained inferior to Transformers   \n97 of comparable parameter-sizes for natural language tasks, even when augmenting S4 layers with   \n98 attention-layers for hybrid architectures [22].   \n99 Innovating on these previous S4 approaches, Mamba utilizes time-varying parameters to model   \n100 latent dynamics, thus broadening the ability to capture nuanced changes evolving in discrete-time.   \n101 Without LTI dynamics, however, the input-output representation via the SSM convolution kernel is no   \n02 longer applicable, thus voiding previous hardware-aware S4 optimizations [14]. To enable hardware   \n103 efficiency with time-varying SSM parameters, [22] thus introduced extensively customized CUDA   \n104 kernels which implement highly parallelized prefix sums to compute recurrent states. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "105 3 Mamba state-space models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "106 For model dimension $d$ and maximum input sequence length $T$ , the MambaBlock defines state-space   \n107 parameters $\\mathbf{A},\\mathbf{B}_{t},\\mathbf{C}_{t},\\Delta_{t}\\in\\mathbb{R}^{d\\times d}$ for $\\bar{t}\\,\\in\\,\\{1,\\bar{,}\\dots,T\\}$ . The matrix $\\Delta_{t}$ controls the discrete step  \n108 size. Given an input sequence $\\mathbf{u}_{1},\\dots,\\mathbf{u}_{T}\\in\\mathbb{R}^{d}$ , the following linear mapping through latent states   \n109 $\\ensuremath{\\mathbf{{x}}}_{1},\\dots,\\ensuremath{\\mathbf{{x}}}_{T}\\in\\mathbb{R}^{d}$ is used to produce the output $\\mathbf{y}_{1},\\dots,\\mathbf{y}_{T}\\in\\bar{\\mathbb{R}}^{d}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}_{t}=\\bar{\\mathbf{A}}_{t}\\pmb{x}_{t-1}+\\bar{\\mathbf{B}}_{t}\\mathbf{u}_{t}}\\\\ &{\\mathbf{y}_{t}=\\bar{\\mathbf{C}}_{t}\\pmb{x}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "110 where $\\bar{\\Delta}_{t}=\\mathsf{s o f t p l u s}\\big(\\mathrm{Linear}(\\Delta_{t})\\big)\\in\\mathbb{R}^{d\\times d}$ , $\\bar{\\mathbf{A}}_{t}\\,=\\,\\exp\\left(\\bar{\\Delta}_{t}\\mathbf{A}\\right)$ and $\\bar{\\mathbf{B}}_{t}\\,=\\,\\mathbf{A}^{-1}(\\bar{\\mathbf{A}}-\\mathbf{I})\\mathbf{B}_{t}$ . In   \n111 practice, $\\mathbf{A},\\mathbf{B}_{t},\\mathbf{C}_{t}$ and $\\Delta_{t}$ are diagonal matrices.   \n112 Hardware-aware optimizations. As matrices $\\mathbf{B}_{t},\\mathbf{C}_{t}$ and $\\Delta_{t}$ are time-varying, S4 optimizations via   \n113 the SSM convolution kernel [11] are no longer applicable. However, by diagonality, each dimension   \n114 may be computed in parallel. Furthermore, the recurrence along every dimension is a prefix sum (also   \n115 called a scan), which is highly parallelizable [7]. [15] thus capitalizes on this through extensively   \n116 customized CUDA kernels wherein the majority of temporal variables are carefully laid out in a large   \n117 buffer of GPU memory and manipulated. Instantiated as a PyTorch linear layer\u2019s weight matrix, this   \n118 memory buffer $\\mathbf{W}\\in\\dot{\\mathbb{R}}^{n\\times3d}$ is used to store and access the diagonal elements of $\\mathbf{B}_{t},\\mathbf{C}_{t}$ and $\\Delta_{t}$ for   \n119 all $t\\in\\{1,\\ldots,T\\}$ , such that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{W}[t-1,:d]=\\mathbf{diag}(\\Delta_{t}),\\mathbf{W}[t-1,d:2d]=\\mathbf{diag}(\\mathbf{B}_{t}),\\mathbf{W}[t-1,2d:3d]=\\mathbf{diag}(\\mathbf{C}_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "120 where $\\begin{array}{r}{\\mathbf{\\dot{W}}[0,:d]=\\mathtt{d i a g}(\\Delta_{1}),\\mathbf{W}[n-1,d:2d]=\\mathtt{d i a g}(\\mathbf{B}_{T}),}\\end{array}$ , and so on. ", "page_idx": 3}, {"type": "text", "text": "121 The customized Mamba prefix scan kernel heavily relies on this memory layout to optimize the   \n122 access pattern of W in Equations 5 and 6.We note that, rather than adjusting Mamba\u2019s low-level   \n123 CUDA kernels themselves to integrate LoRA within the highly optimized prefix scan, we can instead   \n124 directly target W. Doing so, we have the following, where the proof is available in Appendix A.   \n125 Theorem 1. Consider the weight matrix W of a MambaBlock from Equation 3. Targeting W for   \n126 LoRA during fine-tuning ties adaptation weights across $\\mathbf{B}_{t},\\mathbf{C}_{t}$ and $\\Delta_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "127 4 Stable dynamics in the MambaBlock ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "128 The Mamba foundation models were pretrained in full FP32 precision. Consequently, official Mamba   \n129 implementations have cautioned against fine-tuning or training in reduced precision [21, 29], with   \n130 potential sensitivities of MambaBlock recurrent dynamics remaining an open question. We answer   \n131 the latter using theory from dynamical systems. For Mamba\u2019s discrete dynamic system in Equations 5   \n132 and 6, define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{t}=F_{\\theta}(\\pmb{x}_{t-1},\\mathbf{u}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "133 where $\\theta$ denotes the time-varying parameters described in Section 3. For input sequence $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{T}$   \n134 and initial latent state vector $\\scriptstyle x_{0}$ , we thus write ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb x}_{T}=F_{\\theta}(F_{\\theta}(\\,.\\,.\\,.\\,F_{\\theta}({\\pmb x}_{0},{\\bf u}_{1}))):=F_{\\theta}^{T-1}({\\pmb x}_{0},{\\bf u}_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "135 The rate of divergence between two scalar $\\varepsilon$ -close inputs to a discrete dynamical system is bounded   \n136 by the system\u2019s maximal Lyapunov exponent $\\lambda_{\\mathtt{m a x}}$ [44]. Given $\\lambda_{\\operatorname*{max}}$ and two initial values $(x_{0},\\mathbf{u}_{1})$   \n137 and $(x_{0}+\\varepsilon,\\mathbf{u}_{1}+\\varepsilon)$ , the maximum deviation between these points grows as [33, 50]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}|F_{\\theta}^{N}(\\mathbf{x}_{0},\\mathbf{u}_{1})-F_{\\theta}^{N}(\\mathbf{x}_{0}+\\varepsilon,\\mathbf{u}_{1}+\\varepsilon)|\\in\\mathcal{O}(\\varepsilon\\exp\\left(N\\lambda_{\\mathtt{m a x}}\\right)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "138 Thus, when $\\lambda_{\\tt m a x}\\,>\\,0$ , nearby trajectories exponentially separate and, when $\\lambda_{\\mathtt{m a x}}\\,\\leqslant\\,0$ , nearby   \n139 trajectories ultimately converge to the same fixed point or periodic cycles. ", "page_idx": 4}, {"type": "text", "text": "140 The maximal Lyapunov exponent is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}:=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\log\\left\\|\\prod_{t=0}^{T}\\frac{\\partial\\pmb{x}_{t}}{\\partial\\pmb{x}_{t-1}}\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "141 where $\\|\\|_{2}$ denotes the spectral norm for matrices.\u203a For an arb\u203aitrary MambaBlock, we prove the   \n142 following:   \n143 Theorem 2. Let $({\\boldsymbol x}_{t-1},{\\bf u}_{t})$ be the latent state and input at an arbitrary time $t\\in\\{1,\\ldots,T\\}$ within $a$   \n144 MambaBlock. Then small changes $({\\boldsymbol{x}}_{t-1}+{\\boldsymbol{\\varepsilon}},\\mathbf{u}_{t}+{\\boldsymbol{\\varepsilon}})$ produce deviations which are exponentially non  \n145 increasing over discrete-time. That is, max $|\\dot{F}_{\\theta}^{N}(x_{t-1},\\mathbf{u}_{t})-F_{\\theta}^{N}(x_{t-1}+\\varepsilon,\\mathbf{u}_{t}+\\varepsilon)|\\in\\dot{\\mathcal{O}}(\\varepsilon\\exp\\dot{(N\\zeta)}),$ ,   \n146 for some scalar $\\zeta\\leqslant0$ .   \n147 The proof of Theorem 2 is available in Appendix B, where the maximal Lyapunov exponent for an   \n148 arbitrary MambaBlock is first proven to be non-positive. The main result subsequently follows.   \n149 Consequences for automatic mixed-precision. During a forward pass, automatic mixed-precision   \n150 (AMP) saves time and memory by computing forward activations in half-precision (FP16 or BF16).   \n151 During a backward pass, AMP computes gradients in half-precision and up-casts to full-precision   \n152 prior to updating. In contrast to full-precision fine-tuning, MPFT within the MambaBlock thus results   \n153 in small differences to the inputs $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{T}$ fed into the SSM scan (which are passed through a   \n154 SwiGLU), $\\bar{\\Delta}_{t}$ (which is passed through a softplus), and the gradients calculated during training.   \n155 For a discrete dynamical system with $\\lambda_{\\tt m a x}\\,>\\,0$ , changes due to AMP compound after repeated   \n156 expansion of the recurrent state, thus leading to exponential deviations between quantities calculated   \n157 using mixed- versus full-precision. We note that Transformers are not recurrent, and thus not   \n158 susceptible to such issues. Yet, just as differences introduced by quantization/mixed-precision produce   \n159 output differences in Transformer results, differences are expected in Mamba results using different   \n160 precision strategies. However, by Theorem 2, such differences do not exponentially compound over   \n161 discrete-time within the MambaBlock. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "162 5 Related Work ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 Several recent works [45, 19, 30, 40] have studied Mamba\u2019s ability to perform ICL. However, none   \n164 of these have extensively studied Mamba\u2019s ICL capabilities either on standard NLP benchmarks or on   \n165 pure MambaBlock foundation models. In particular, foundational Mamba models\u2019 ICL abilities were   \n166 tested in [45] to learn simple function classes (e.g., logistic regression and decision trees [17]) and in   \n167 [19] to learn non-standard NLP benchmarks (i.e., task vectors [25]). While [45, 19] report Mamba\u2019s   \n168 ICL abilities rival SOTA Transformers, their utilized benchmarks were proposed as supplemental   \n169 ICL studies after Transformer LLMs\u2019 success on standard NLP benchmarks [8]. Indeed, direct   \n170 evaluation of Mamba foundation models on standard NLP benchmarks does not lead to higher gains   \n171 over zero-shot performance relative to comparable Transformer LLMs (demonstrated in Table 1).   \n172 Lyapunov exponents have previously been considered for classic RNN structures (e.g., vanilla   \n173 RNNs, LSTMs, GRUs, PLRNNs, etc.) [44, 56], to determine when such models exhibit chaotic   \n174 dynamics and the impact on the exploding/vanishing gradient phenomena\\*. For more recent S4 neural   \n175 models, [18] used Hurwitz matrices to characterize the numerical stability of linear time-invariant   \n176 (LTI) S4 models. However, such analysis is not applicable to time-varying models, such as Mamba,   \n177 nor does it characterize the effects of sensitive dependence on initial conditions (e.g., divergence of   \n178 two $\\varepsilon$ close inputs). To the best of our knowledge, no previous works have used Lyapunov exponents   \n179 to explore the effects of mixed-precision on recurrent neural models or Mamba architectures.   \n180 As in [22], the majority of subsequent Mamba works have focused on pretraining MambaBlocks using   \n181 full precision [65, 62, 1, 40]. Notably, the official implementation of Jamba [40], the Transformer  \n182 Mamba hybrid, supports mixed- and 8-bit precision, but avoids MambaBlocks when applying such   \n183 quantization [32]. Similarly, the official Mamba sources advise using full precision within the   \n184 MambaBlock [29, 21], cautioning against using mixed-precision due to potential recurrent sensitivities.   \n185 To the best of our knowledge, no existing works have either theoretically explored the effects small   \n186 input changes (e.g., due to mixed-precision) have on Mamba\u2019s recurrent dynamics, empirically   \n187 explored such effects downstream impact on fine-tuning and inference, or explored pure Mamba   \n188 networks fine-tuning abilities relative to Transformer LLMs. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "189 6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "190 To demonstrate the implications of Theorem 2, we explore the performance difference between   \n191 running inference with full-precision pretrained weights and using mixed-precision (FP16 and BF16)   \n192 weights. Model performance is measured as percent accuracy using the MMLU [26] dataset.   \n193 The difference in model performance is reported as the mean divergence (i.e., absolute difference)   \n194 between the original full-precision and respective mixed-precision model, averaged over {0, 1, 3,   \n195 5}-shot percent accuracy. Thus, a divergence greater than one denotes an average difference   \n196 greater than one entire percentage of accuracy.   \n197 Mamba pretrained checkpoints are compared to pretrained Transformer models of similar parameter   \n198 counts and no more than ${\\sim}300\\mathrm{B}$ total pretraining tokens (Pythia [5], OLMo [20] 336B-token   \n199 checkpoint, and Phi 1.5 [39]). We note that Pythia and Mamba models were both pretrained using   \n200 the same corpus [15], allowing the fairest comparison between SSMs and Transformers. To limit   \n201 extraneous numerical effects within experiments (e.g., due to parameter aggregation across multiple   \n202 GPUs), all models were run using a single GPU (Nvidia A10G, 24 GB total memory). All models   \n203 were evaluated using the LM evaluation harness from Eleuther AI [16]. Further experimental details   \n204 are available in Appendix C. The results are available in Table 2.   \n205 From Table 2, inferencing in Pythia using FP16 and BF16 result in an average 0.13 and 0.41 full  \n206 precision divergence, respectively. Mamba displays similar averages in comparison: inferencing in   \n207 Mamba using FP16 and BF16 result in an average 0.10 and 0.48 divergence, respectively. Interestingly,   \n208 both SSM and Transformer architectures exhibit large divergence spikes\u2013i.e., mean divergence greater   \n209 than a percentage point\u2013when using BF16, which occurs once for Mamba and Phi 1.5 models and   \n210 twice for Pythia models. In the following, we show that such spikes may be mitigated for Mamba   \n211 SSMs by combining mixed-precision with parameter-efficient adapters during fine-tuning.   \n212 Non-divergent Mamba fine-tuning. We next explore the implications of Theorem 2 on fine-tuning,   \n213 wherein mixed-precision is especially critical; MPFT combined with PEFT adapters have been shown   \n214 to drastically reduce Transformer fine-tuning times [12]. We are thus interested in the divergence   \n215 between Mamba models fully fine-tuned (i.e., no adapters, all model weights are trained) in full  \n216 precision and models fine-tuned using mixed-precision and/or PEFT adapters. We focus on utilizing   \n217 LoRA [28], which is arguably the most widely used PEFT framework for LLMs.   \n218 Using the Alpaca dataset [51], Mamba 160M, 410M, and 790M models are fine-tuned for three epochs   \n219 with a maximum sequence length of 512. We denote the targeting of all linear layers (ALL) for LoRA   \n220 as ALL LoRA, the targeting of a subset of linear layers (SLL) for LoRA as SLL LoRA, and no adapters   \n221 as Full (i.e., full fine-tuning). Both ALL and SLL LoRA adapt the large memory buffer described in   \n222 Theorem 1.   \nEach fine-tuning run occurred on a single A10G GPU. To further limit extraneous numerical effects,   \n224 the same batch size is used for all FP32, FP16, and BF16 experiments for a given model size. While   \nthis leads to hardware underutilization (i.e., non-saturated GPU memory for mixed-precision and   \n226 LoRA experiments), this is necessary to guarantee no divergence is due to differences in parameter   \n227 update schedules. For comparison, Pythia 160M, 410M, and 1B models are fine-tuned using the   \n228 same experimental setup. The training recipe for all models was adapted from [53], with the   \n229 AdamW_torch optimizer and a cosine annealing schedule. Further experimental details are   \n230 available in Appendix C.   \n231 For each Mamba and Pythia model, Figure 1 shows the mean divergence calculated between the   \n232 respective FP32 Full and mixed-precision ALL/SLL LoRA fine-tuned models, averaged over $\\{0,\\,1,\\,3$ ,   \n233 5}-shot MMLU accuracy. Across mixed-precisions and adapter settings, Mamba displays comparable   \n234 divergences to Pythia models. E.g., for FP16, Mamba demonstrates an average divergence of 0.1,   \ncompared to 0.14 for Pythia. Similarly, for BF16, Mamba demonstrates an average divergence   \nof 0.18, compared to 0.28 for Pythia. Importantly, Mamba models do not exhibit large deviation   \n237 spikes after fine-tuning (in contrast to Pythia models).   \n238 Hardware throughput and memory-utilization improvements. With comparable divergences   \n239 to Transformers and stable dynamics, we show that MPFT and PEFT may be used to significantly   \n240 increase GPU-training throughput for Mamba SSMs. To demonstrate such improvements, we utilize   \n241 the previous fine-tuning settings for the Alpaca dataset. However, we now adjust the batch size to   \n242 maximize throughput per MPFT and PEFT configuration.   \nFor each MPFT and PEFT configuration, the average tokens-per-second (ATPS) is calculated as the   \n244 total tokens used for fine-tuning divided by total training time, and the maximum memory-per-token   \n245 (MMPT) is calculated as the maximum GPU memory utilization incurred (over the entire fine-tuning   \n246 run) divided by the total number of tokens in each mini-batch. Results are plotted in Figure 6.   \nBoth throughput and memory utilization improve as the number of Mamba parameters increases   \nin Figure 6. Compared to the full-precision full fine-tuning of Mamba 790M (the largest model   \n249 supported by an A10G\u2019s memory capacity), evaluated MPFT and PEFT combinations result in   \n250 an average 2.15 times more training tokens-per-second while reducing per-token memory   \n251 utilization by an average $62.7\\%$ . Across all model sizes, evaluated MPFT and PEFT combinations   \n252 result in an average 1.74 times more training tokens-per-second while reducing per-token memory   \n253 utilization by an average $47.2\\%$ compared to respective full-precision fine-tuned runs. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "C3t6GMPnC5/tmp/5cd0acbb2c65cc2d91e5792f813bfc534cd4c02c49b4ea1a2a244c53a213c035.jpg", "table_caption": ["Table 2: Mean full-precision (FP32) divergence in MMLU performance for mixed-precision inference. Divergence is averaged over $\\{0,\\,1,\\,3,\\,5\\}$ -shot performance. Pretrained checkpoints are used for Mamba (M), Pythia (P), OLMo [20], and Phi-1.5 [39] (Phi) models. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "C3t6GMPnC5/tmp/444febe6fc6009a85aaa21bee6de3c4ae15f2d04f5c2af34a93c2238e4cf4408.jpg", "img_caption": ["Figure 1: Mean full-precision (FP32) divergence in MMLU performance for Mamba and Pythia models. Models are fine-tuned over the Alpaca dataset [51] using different combinations of MPFT and PEFT. Full fine-tuning (i.e., no PEFT adapters) is denoted as Full. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "254 6.1 Fine-tuning narrows the ICL gap between Mamba and Transformers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "255 We next explore how MPFT and PEFT affect Mamba ICL performance. All Mamba pretrained   \n256 models are instruction fine-tuned using ALL LoRA and the OpenHermes dataset [52] (which consists   \n257 of 242,000 supervised samples). We use the training recipe of [53], which includes BF16 utilization. ", "page_idx": 6}, {"type": "image", "img_path": "C3t6GMPnC5/tmp/8494800ad293606e7a7017b1b5766b479bc81f0ad91d193f47d1f1646360a360.jpg", "img_caption": ["Figure 2: Timing and memory usage calculated Mamba model-sizes and PEFT combinations. Each model was trained using the Alpaca dataset [51] dataset for three epochs and maximum sequence length 512. For each PEFT combination, the batch size was tuned to maximize GPU occupancy. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "258 Performance is evaluated using the datasets from Table 1\u2013HellaSwag [64], PIQA [6], Arc-E [9], 259 Arc-C [9], and WinoGrande [49]\u2013and report the average improvement percentage of $\\{1,3,5\\}$ -shot 260 versus 0-shot (AIPSS). For comparison, Pythia pretrained models are instruction fine-tuned using the same training recipe and ALL LoRA (i.e., all Pythia linear layers are adapted). ", "page_idx": 7}, {"type": "image", "img_path": "C3t6GMPnC5/tmp/4651e32033c419513828f0dbe7e1ecc206685e909ff7535188eadaec586d2645.jpg", "img_caption": ["Figure 3: Fine-tuning narrows the ICL gap between Mamba and Pythia. ALL LoRA models were instruction fine-tuned on the OpenHermes [52] dataset for one epoch. Performance is reported as the average improvement percentage of $\\{1,3,5\\}$ -shot versus 0-shot over five standard benchmarks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "261 ", "page_idx": 7}, {"type": "text", "text": "262 Figure 3 displays AIPSS for pretrained and instruction fine-tuned Mamba and Pythia models. As   \n263 previously noted, pretrained Mamba models do not display similar ICL ability as comparable Pythia   \n264 models on the evaluated standard NLP benchmarks. In particular, Mamba 2.8B, the largest pretrained   \n265 Mamba model, displays inconsistent zero-shot improvements as the number of shots increase.   \n266 However, after fine-tuning, all Mamba models larger than Mamba 130M consistently improve in ICL   \n267 performance as the number of shots increase. Compared to Mamba pretrained models, which are only   \n268 capable of $38\\%$ of the AIPSS compared to similar pretrained Pythia models, fine-tuned ALL LoRA   \n269 Mamba models are capable of $81.5\\%$ of the AIPSS compared to similarly fine-tuned Pythia models.   \n270 Fine-tuning robustness. We show that Mamba is robust to the choice of PEFT hyperparemters. We   \n271 conduct an extensive hyperparameter search across the learning rate, LoRA dimension, and number of   \n272 warmup steps. From the Cartesian-product of these three parameters, 150 hyperparameter configura  \n273 tions were sampled and used to fine-tune Mamba 370M over the Openhermes dataset. For comparison,   \n274 Pythia 410M is similarly fine-tuned using the same set of 150 hyperparameter configurations.   \n275 The MMLU 5-shot performance for each of the 150 Mamba and Pythia fine-tuned models is displayed   \n276 in 6.1. Pythia 410M is capable of higher performance than Mamba 370M, where the average accuracy   \n277 for the former and the latter are $26.5\\%$ and $24.8\\%$ , respectively. However, Mamba 370M is much more   \n278 robust to the choice of hyperparameters, with a difference of $1.5\\%$ between the minimum $(23.3\\%)$   \n279 and maximum $(24.8\\%)$ . In contrast, Pythia 410M fine-tuned models display a large performance   \n280 difference of $4.7\\%$ between the minimum $(22.9\\%)$ and maximum $(27.6\\%)$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "C3t6GMPnC5/tmp/01b485bf9022e64b885d5ea6e616194bfcb38a51553d1743d0f97433171e867c.jpg", "img_caption": ["Figure 4: Fine-tuning hyperparameter search for OpenHermes. Each point is a different hyperparameter configuration. SLL LoRA was used for both models. The $x$ -axis is the learning rate, the $y$ -axis is resulting MMLU 5-shot performance, bubble size is the LoRA dimension, and the color is the number of warmup steps $\\in\\{0,1\\mathrm{k,2}\\mathrm{k}\\}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "281 7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "282 We\u2019ve extensively explored Mamba\u2019s downstream learning capabilities. Using dynamical systems   \n283 theory, we\u2019ve shown that Mamba\u2019s recurrent dynamics are robust to small input perturbations (contrary   \n284 to the current understanding of Mamba\u2019s recurrent sensitivities). We\u2019ve extensively confirmed this   \n285 result, showing that: a) Mamba inference is robust to changes due to mixed-precision, (b) Mamba   \n286 inference differences due to mixed-precision align with Transformers, (c) Mamba fine-tuning is robust   \n287 to changes due to mixed-precision and PEFT, and (d) differences in downstream performance for   \n288 Mamba due to MPFT and PEFT can be more robust than Transformers. Using both MPFT and PEFT,   \n289 we\u2019ve shown that instruction fine-tuning Mamba SSMs greatly narrows the previously observed ICL   \n290 gap, going from only $38\\%$ (post pretraining) up to $81.5\\%$ (post fine-tuning) of the ICL abilities of   \n291 similar Transformers. Furthermore, we\u2019ve shown that combining MPFT and PEFT can more than   \n292 halve training time and nearly triple memory efficiency for Mamba models.   \n293 There are significant avenues for future work. In particular, adapting Mamba\u2019s CUDA kernels to   \n294 support more aggressive low-precision PEFT methods [12] would further decrease the hardware   \n295 needed to train Mamba models, while providing additional speedups. Furthermore, while the largest   \n296 pure Mamba model contains 2.8B parameters, the training speedups and improved memory utilization   \n297 described herein may be applied to more efficiently pretrain larger pure Mamba SSMs (e.g., 7B   \n298 parameters and greater), where Mamba models may better manifest emergent abilities previously   \n299 displayed by Transformers (or even manifest previously unobserved abilities).   \n300 Limitations. While we explored the use of LoRA for Mamba models, many other PEFT adapters   \n301 exist [41, 38, 27, 35]. Furthermore, while mixed-precision using FP16 and BF16 were explored,   \n302 lower-precision methods exist [12] (which may be enabled by adapting Mamba\u2019s highly customized   \n303 CUDA kernels). Both are interesting directions for future work. Finally, our timing and memory   \n304 usage experiments using Alpaca did not consider the largest two Mamba models (1.4B and 2.8B) due   \n305 to their exceeding A10G memory capacity for FP32 full fine-tuning.   \n306 Broader Impact. The Mamba models considered are all LLMs, and thus have the same potential   \n307 positive and negative societal impacts as other LLMs (e.g., hallucinations). Furthermore, fine-tuning   \n308 is known to possibly erode existing LLM guardrails, and thus our methods may be adapted for this   \n309 fine-tuning use case (as is the case for all PEFT and MPFT methods). However, our work improves the   \n310 quality of Mamba models for downstream applications, which may be adapted for all positive LLM   \n311 applications in society (e.g., personal assistants, task automation, code completion, etc.). Finally, our   \n312 work decreases the computational constraints required to train and inference Mamba SSMs, which   \n313 has implications for green ML (e.g., decreased CO2 emissions, positive climate change impact, etc.).   \n314 410 GPU days were used to produce the results for this paper. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "315 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "316 [1] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts   \n317 for state-space models. arXiv preprint arXiv:2402.01771, 2024.   \n318 [2] Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space models.   \n319 arXiv preprint arXiv:2402.08678, 2024.   \n320 [3] Nils Bertschinger and Thomas Natschl\u00e4ger. Real-time computation at the edge of chaos in recurrent neural   \n321 networks. Neural computation, 16(7):1413\u20131436, 2004.   \n322 [4] Nils Bertschinger, Thomas Natschl\u00e4ger, and Robert Legenstein. At the edge of chaos: Real-time computa  \n323 tions and self-organized criticality in recurrent neural networks. Advances in neural information processing   \n324 systems, 17, 2004.   \n325 [5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric   \n326 Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia:   \n327 A suite for analyzing large language models across training and scaling. In International Conference on   \n328 Machine Learning (ICML), pages 2397\u20132430. PMLR, 2023.   \n329 [6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical common  \n330 sense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34,   \n331 pages 7432\u20137439, 2020.   \n332 [7] Guy E Blelloch. Prefix sums and their applications. 1990.   \n333 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind   \n334 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.   \n335 Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n336 [9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind   \n337 Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint   \n338 arXiv:1803.05457, 2018.   \n339 [10] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient   \n340 exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359,   \n341 2022.   \n342 [11] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry   \n343 hippos: Towards language modeling with state space models. In Proceedings of the 11th International   \n344 Conference on Learning Representations (ICLR), 2023.   \n345 [12] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of   \n346 quantized llms. Advances in Neural Information Processing Systems, 36, 2024.   \n347 [13] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization   \n348 for generative pre-trained transformers. 2023.   \n349 [14] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry   \n350 hungry hippos: Towards language modeling with state space models. In International Conference on   \n351 Learning Representations (ICLR), 2023.   \n352 [15] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,   \n353 Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language   \n354 modeling. arXiv preprint arXiv:2101.00027, 2020.   \n355 [16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster,   \n356 Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris   \n357 Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang,   \n358 Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,   \n359 12 2023.   \n360 [17] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in  \n361 context? a case study of simple function classes. Advances in Neural Information Processing Systems   \n362 (NeurIPS), 35:30583\u201330598, 2022.   \n363 [18] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It\u2019s raw! audio generation with state-space   \n364 models. In International Conference on Machine Learning, pages 7616\u20137633. PMLR, 2022.   \n365 [19] Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba capable of   \n366 in-context learning? arXiv preprint arXiv:2402.03170, 2024.   \n367 [20] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh   \n368 Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language   \n369 models. arXiv preprint arXiv:2402.00838, 2024.   \n370 [21] Albert Gu and Tri Dao. Mamba Precision Guidance. \"https://github.com/state-spaces/mamba#   \n371 precision\", 2023. \"Accessed: 2024-04-25\".   \n372 [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint   \n373 arXiv:2312.00752, 2023.   \n374 [23] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state   \n375 spaces. In International Conference on Learning Representations (ICLR), 2022.   \n376 [24] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified   \n377 view of parameter-efficient transfer learning. In International Conference on Learning Representations   \n378 (ICLR), 2021.   \n379 [25] Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. In The 2023   \n380 Conference on Empirical Methods in Natural Language Processing, 2023.   \n381 [26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob   \n382 Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning   \n383 Representations, 2020.   \n384 [27] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges  \n385 mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International   \n386 conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \n387 [28] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and   \n388 Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,   \n389 2021.   \n390 [29] Huggingface. Mamba PEFT. \"https://huggingface.co/docs/transformers/en/model_doc/   \n391 mamba#peft-finetuning\", 2024. \"Accessed: 2024-04-25\".   \n392 [30] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me: Transformers   \n393 are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024.   \n394 [31] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth   \n395 Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of   \n396 bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.   \n397 [32] AI 21 Labs. Jamba PEFT. \"https://huggingface.co/ai21labs/Jamba-v0.1\", 2024. \"Accessed:   \n398 2024-04-25\".   \n399 [33] Tanguy Laffargue, Khanh-Dang Nguyen Thu Lam, Jorge Kurchan, and Julien Tailleur. Large deviations of   \n400 lyapunov exponents. Journal of Physics A: Mathematical and Theoretical, 46(25):254002, 2013.   \n401 [34] Thomas Laurent and James von Brecht. A recurrent neural network without chaos. In Proceedings of the   \n402 11th International Conference on Learning Representations (ICLR), 2017.   \n403 [35] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.   \n404 arXiv preprint arXiv:2104.08691, 2021.   \n405 [36] Kai Li and Guo Chen. Spmamba: State-space model is all you need in speech separation. arXiv preprint   \n406 arXiv:2404.02063, 2024.   \n407 [37] Lincan Li, Hanchen Wang, Wenjie Zhang, and Adelle Coster. Stg-mamba: Spatial-temporal graph learning   \n408 via selective state space model. arXiv preprint arXiv:2403.12418, 2024.   \n409 [38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv   \n410 preprint arXiv:2101.00190, 2021.   \n411 [39] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.   \n412 Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \n413 [40] Opher Lieber, Barak Lenz, Hofti Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked   \n414 Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language   \n415 model. arXiv preprint arXiv:2403.19887, 2024.   \n416 [41] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A   \n417 Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances   \n418 in Neural Information Processing Systems, 35:1950\u20131965, 2022.   \n419 [42] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan   \n420 Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024.   \n421 [43] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris   \n422 Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In   \n423 International Conference on Learning Representations (ICLR), 2018.   \n424 [44] Jonas Mikhaeil, Zahra Monfared, and Daniel Durstewitz. On the difficulty of learning chaotic dynamics   \n425 with rnns. Advances in Neural Information Processing Systems, 35:11297\u201311312, 2022.   \n426 [45] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook   \n427 Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context   \n428 learning tasks. International Conference on Machine Learning (ICML), 2024.   \n429 [46] Changsheng Quan and Xiaofei Li. Multichannel long-term streaming neural speech enhancement for static   \n430 and moving speakers. arXiv preprint arXiv:2403.07675, 2024.   \n431 [47] Ant\u00f4nio H Ribeiro, Koen Tiels, Luis A Aguirre, and Thomas Sch\u00f6n. Beyond exploding and vanishing   \n432 gradients: analysing rnn training using attractors and smoothness. In International conference on artificial   \n433 intelligence and statistics (AISTATS), pages 2370\u20132380. PMLR, 2020.   \n434 [48] Jiacheng Ruan and Suncheng Xiang. Vm-unet: Vision mamba unet for medical image segmentation. arXiv   \n435 preprint arXiv:2402.02491, 2024.   \n436 [49] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial   \n437 winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n438 [50] Hiroki Sayama. Introduction to the modeling and analysis of complex systems. Open SUNY Textbooks,   \n439 2015.   \n440 [51] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,   \n441 and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.   \n442 com/tatsu-lab/stanford_alpaca, 2023.   \n443 [52] Teknium. Openhermes. \"https://huggingface.co/datasets/teknium/openhermes\", 2024. \"Ac  \n444 cessed: 2024-04-25\".   \n445 [53] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada,   \n446 Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation   \n447 of lm alignment. arXiv preprint arXiv:2310.16944, 2023.   \n448 [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz   \n449 Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,   \n450 30, 2017.   \n451 [55] Kushala VM, Harikrishna Warrier, Yogesh Gupta, et al. Fine tuning llm for enterprise: Practical guidelines   \n452 and recommendations. arXiv preprint arXiv:2404.10779, 2024.   \n453 [56] Ryan Vogt, Maximilian Puelma Touzel, Eli Shlizerman, and Guillaume Lajoie. On lyapunov exponents   \n454 for rnns: Understanding information propagation using dynamical systems tools. Frontiers in Applied   \n455 Mathematics and Statistics, 8:818799, 2022.   \n456 [57] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence   \n457 modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024.   \n458 [58] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,   \n459 Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.   \n460 Transactions on Machine Learning Research, 2022.   \n461 [59] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for   \n462 deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602, 2020.   \n463 [60] Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, and Guibo Luo. Promamba: Prompt  \n464 mamba for polyp segmentation. arXiv preprint arXiv:2403.13660, 2024.   \n465 [61] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning   \n466 as implicit bayesian inference. In International Conference on Learning Representations (ICLR), 2021.   \n467 [62] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. Segmamba: Long-range sequential modeling   \n468 mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560, 2024.   \n469 [63] Yijun Yang, Zhaohu Xing, and Lei Zhu. Vivim: a video vision mamba for medical video object segmenta  \n470 tion. arXiv preprint arXiv:2401.14168, 2024.   \n471 [64] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really   \n472 finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational   \n473 Linguistics, pages 4791\u20134800, 2019.   \n474 [65] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision   \n475 mamba: Efficient visual representation learning with bidirectional state space model. International   \n476 Conference on Machine Learning (ICML), 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "477 A Proof of weight-tying using LoRA in the MambaBlock ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "478 Due to the low-level nature of Mamba\u2019s prefix scan optimizations (discussed in Section 3), standard   \n479 use of LoRA adapters is made difficult within Mamba\u2019s SSM-layer. E.g., while $B_{t},C_{t}$ and $\\Delta_{t}$ are   \n480 conceptually PyTorch linear layers, their bundling in a contiguous memory block and careful manip  \n481 ulation makes appending a LoRA adapter on any of these invidiual matrices non-trivial (particularly,   \n482 while respecting the highly specialized layout of each LoRA adapters targeted layer). However, we   \n483 note that the overall design of the MambaBlock\u2019s hardware optimizations may be leveraged to both   \n484 efficiently learn the parameter-space for the majority of time-varying parameters (thus achieving   \n485 PEFT) and regularize parameters during training (thus improving fine-tuning generalization).   \n486 Theorem 1. Consider the weight matrix W of a MambaBlock from Equation 3. Targeting W for   \n487 LoRA during fine-tuning ties adaptation weights across $\\mathbf{B}_{t},\\mathbf{C}_{t}$ and $\\Delta_{t}$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "488 Proof. Let $r$ be the specified LoRA dimension. Targeting this matrix for LoRA results in the adapter ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{W}}=\\mathbf{W}+\\mathbf{W}^{\\prime}}\\\\ {=\\mathbf{W}+\\mathbf{U}\\mathbf{V},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "489 where $\\mathbf{U}\\in\\mathbb{R}^{n\\times r}$ , $\\mathbf{V}\\in\\mathbb{R}^{r\\times3d}$ , and $\\mathbf{W}$ is frozen during fine-tuning. Thus, for index $\\left[i,j\\right]$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{W}^{\\prime}[i,j]=\\sum_{k=0}^{r-1}\\mathbf{U}[i,k]\\mathbf{V}[k,j].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 Recall the form of $\\mathbf{W}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{W}[t-1,:d]=\\mathbf{diag}(\\Delta_{t}),\\mathbf{W}[t-1,d:2d]=\\mathbf{diag}(\\mathbf{B}_{t}),\\mathbf{W}[t-1,2d:3d]=\\mathbf{diag}(\\mathbf{C}_{t}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "491 where $\\mathbf{W}[0,:d]=\\operatorname{diag}(\\Delta_{1}),\\mathbf{W}[n-1,d:2d]=\\operatorname{diag}(\\mathbf{B}_{T})$ , and so on. For index $\\left[t-1,j\\right]$ , we   \n492 thus have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\tilde{\\mathbf{W}}[t-1,j]=\\mathbf{W}[t-1,j]+\\mathbf{W}^{\\prime}[t-1,j]}}\\\\ &{}&{\\displaystyle=\\mathbf{W}[t-1,j]+\\sum_{k=0}^{r-1}\\mathbf{U}[t-1,k]\\mathbf{V}[k,j].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "493 Thus, the weights $\\mathbf{U}[t-1,:]$ are tied for any parameter $\\tilde{\\mathbf{W}}[t-1,j],j\\in\\{1,\\dots,3d\\}$ , which are used   \n494 to adapt parameters $\\Delta_{1},\\mathbf{B}_{t}$ , and $\\mathbf{C}_{t}$ . ", "page_idx": 12}, {"type": "text", "text": "496 B Mamba stable dynamics proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "497 Recall the state-space parameters and equations for the MambaBlock; $\\mathbf{A},\\mathbf{B}_{t},\\mathbf{C}_{t},\\Delta_{t}\\in\\mathbb{R}^{d\\times d}$ for   \n498 $t\\in\\{1,\\dots,n\\}=\\overline{{[n]}}$ . Given an input sequence $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{n}\\,\\in\\,\\mathbb{R}^{d}$ , the following linear mapping   \n499 through latent states $\\bar{\\mathbf{x}}_{1},\\ldots,\\mathbf{x}_{n}\\in\\mathbb{R}^{d}$ is used to produce the output $\\mathbf{y}_{1},\\ldots,\\mathbf{y}_{n}\\in\\mathbb{R}^{d}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}_{t}=\\bar{\\mathbf{A}}_{t}\\pmb{x}_{t-1}+\\bar{\\mathbf{B}}_{t}\\mathbf{u}_{t}}\\\\ &{\\mathbf{y}_{t}=\\bar{\\mathbf{C}}_{t}\\pmb{x}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "500 where $\\bar{\\pmb{\\Delta}}_{t}=\\mathsf{s o f t p l u s}(\\mathtt{L i n e a r}(\\pmb{\\Delta}_{t}))\\in\\mathsf{\\Gamma}^{d\\times d}$ , $\\bar{\\mathbf{A}}_{t}=\\exp{(\\bar{\\mathbf{A}}_{t}\\mathbf{A})},\\bar{\\mathbf{B}}_{t}=\\mathbf{A}^{-1}(\\bar{\\mathbf{A}}-\\mathbf{I})\\mathbf{B}_{t}$ , and is the   \n501 set of non-negative real numbers. In practice, $\\mathbf{A},\\mathbf{B}_{t},\\mathbf{C}_{t}$ and $\\Delta_{t}$ are diagonal matrices. ", "page_idx": 13}, {"type": "text", "text": "502 Furthermore, recall the following definitions: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}=F_{\\theta}(\\pmb{x}_{t-1},\\mathbf{u}_{t})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "503 where $\\theta$ denotes the aforementioned time-varying parameters. For input sequence $\\mathbf{u}_{t},\\ldots,\\mathbf{u}_{T}$ and   \n504 initial latent state value $\\scriptstyle{x_{0}}$ , we thus write ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb x}_{T}=F_{\\theta}(F_{\\theta}(\\,.\\,.\\,.\\,F_{\\theta}({\\pmb x}_{0},{\\bf u}_{1}))):=F_{\\theta}^{T-1}({\\pmb x}_{0},{\\bf u}_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "505 We first prove that, given two scalar $\\varepsilon$ -close inputs to a MambaBlock, their deviations do not grow   \n506 exponentially as the number of recurrences increases (Lemma 1). The main result in the paper is   \n507 subsequently proved.   \n508 Lemma 1. For input $(x_{0},\\mathbf{u}_{1})$ to a MambaBlock, small changes $\\left(x_{0}+\\varepsilon,\\mathbf{u}_{1}+\\varepsilon\\right)$ produce deviations   \n509 which are exponentially non-increasing over discrete-time. That is, max $|F_{\\theta}^{N}(\\stackrel{\\cdot}{{\\bf x}_{0}},{\\bf u}_{1})-F_{\\theta}^{N}(\\stackrel{\\cdot}{{\\bf x}_{0}}+$   \n510 $\\varepsilon,\\mathbf{u}_{1}+\\varepsilon)|\\in\\mathcal{O}(\\varepsilon\\exp\\left(N\\zeta\\right))$ , for some scalar $\\zeta\\leqslant0$ .   \n511 Proof. Firstly, we note that within the MambaBlock, $A$ is stored in log-space followed by a negative   \n512 exponentiation prior to use. Thus, $\\mathbf{A}\\in{\\mathfrak{d}}\\times{\\mathfrak{d}}$ , where is the set of non-positive real numbers. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "513 Recall that for the maximum deviation, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}|F_{\\theta}^{N}(\\mathbf{x}_{0},\\mathbf{u}_{1})-F_{\\theta}^{N}(\\mathbf{x}_{0}+\\varepsilon,\\mathbf{u}_{1}+\\varepsilon)|\\in\\mathcal{O}(\\varepsilon\\exp\\left(N\\lambda_{\\mathtt{m a x}}\\right)).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "514 where the maximal Lyapunov exponent $\\lambda_{\\mathtt{m a x}}$ is defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}:=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\log\\left\\|\\prod_{t=0}^{T}\\frac{\\partial\\pmb{x}_{t}}{\\partial\\pmb{x}_{t-1}}\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "515 and $\\|\\|_{2}$ denotes the spectral norm for matrices. ", "page_idx": 13}, {"type": "text", "text": "516 Thus, to complete the proof, it suffices to show that $\\lambda_{\\mathtt{m a x}}\\leqslant0$ . Recall that $\\mathbf{A}$ and $\\bar{\\Delta}_{t}$ are diagonal.   \n517 From Equation 5, we thus have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\lambda_{\\operatorname*{max}}=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\log\\left\\|\\prod_{t=0}^{T}\\widehat{\\sigma}\\mathbf{x}_{t-1}\\right\\|_{2}}\\\\ {\\displaystyle=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\log\\left\\|\\prod_{t=0}^{T}\\exp\\left(\\bar{\\Delta}_{t}\\mathbf{A}\\right)\\right\\|_{2}}\\\\ {\\displaystyle=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\log\\left\\|\\exp\\sum_{t=0}^{T}(\\bar{\\Delta}_{t}\\mathbf{A})\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "518 Let $i$ be the dimension which corresponds to the output of the spectral norm, i.e., $\\textit{i}=$   \n519 $\\begin{array}{r l}{\\operatorname*{irgmax}_{j=1,\\dots,d}\\{\\exp\\sum_{t=0}^{T}(\\bar{\\Delta}_{t}[j,j]{\\bf A}[j,\\bar{j}])\\}}&{{}}\\end{array}$ . We thus have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{max}}=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\log\\left\\Vert\\exp\\displaystyle\\sum_{t=0}^{T}(\\bar{\\Delta}_{t}\\mathbf{A})\\right\\Vert_{2}}\\\\ &{\\qquad=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\log\\exp\\displaystyle\\sum_{t=0}^{T}(\\bar{\\Delta}_{t}[i,i]\\mathbf{A}[i,i])}\\\\ &{\\qquad=\\mathbf{A}[i,i]\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T}\\bar{\\Delta}_{t}[i,i]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "521 Theorem 2. Let $({\\boldsymbol x}_{t-1},{\\bf u}_{t})$ be the latent state and input at an arbitrary time $t\\in[1,T]$ within $a$   \n522 MambaBlock. Then small changes $(x_{t-1}+\\varepsilon,\\mathbf{u}_{t}+\\varepsilon)$ produce deviations which are exponentially   \n523 decreasing over discrete-time, i.e., m $\\mathrm{ax}\\,|F_{\\theta}^{N}({\\mathbf x}_{0},\\mathbf{u}_{1})\\stackrel{\\cdot\\,\\cdot\\,}{-F_{\\theta}^{N}}({\\mathbf x}_{0}+\\varepsilon,\\mathbf{u}_{1}+\\varepsilon)|\\in\\mathcal{O}(\\varepsilon\\exp\\!\\left(N\\zeta\\right))$ , for   \n524 some scalar $\\zeta\\leqslant0$ .   \n525 Proof. Let $\\tau(t)$ be a function that maps time values such that $\\tau(t)\\in[1,T-t]$ and $\\tau(t)=1,\\tau(t+$   \n526 $1)\\,=\\,2,\\dots,\\dot{\\tau}(t+T)\\,=\\,T-t$ . Then $\\mathbf{B}_{\\tau(t)},\\mathbf{C}_{\\tau(t)},\\Delta_{\\tau(t)}$ define a new MambaBlock with inputs   \n527 $\\mathbf{u}_{\\tau(t)},\\ldots,\\mathbf{u}_{\\tau(t+T)}$ and subsequent recurrent states $\\mathbf{\\boldsymbol{x}}_{\\tau(t)},\\ldots,\\mathbf{\\boldsymbol{x}}_{\\tau(t+T)}.$ Applying Lemma 1 to this   \n528 MambaBlock with $(\\mathbf{\\boldsymbol{x}}_{\\tau(t)-1},\\mathbf{\\boldsymbol{u}}_{\\tau(t)})$ completes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "529 C Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "530 All model checkpoints were evaluated on all benchmarks and few-shot settings using the LM   \n531 evaluation harness from Eleuther AI [16], version 0.4.2. Pythia and Mamba Huggingface check  \n532 points were used for all inference and fine-tuning experiments, e.g., EleutherAI/pythia-160m   \n533 and state-spaces/mamba-130m-hf for the smallest respective models. All fine-tuning experi  \n534 ments were run using package versions Transformers 4.40.0.dev0, Accelerate 0.28.0, TRL   \n535 0.8.1, PyTorch $2\\,.\\,2\\,.\\,1{+}\\mathtt{c u}121$ , and PEFT 0.10.0.   \n536 For MPFT, Flash Attention 2.0 [10] via flash_attn 2.5.7 was used for Pythia mod  \n537 els. For FP16 and BF16 inference results, Flash Attention 2.0 was used for both Pythia   \n538 and OLMo models. For OLMo results, the 336B-token checkpoint was used by specifying   \n539 revision $\\equiv$ step80000-tokens336B.   \n540 Outside of the OpenHermes hyperparameter search, all Alpaca and OpenHermes fine-tuning exper  \n541 iments used the following training recipe (adapted from [53]): AdamW_torch optimizer, cosine   \n542 annealing schedule, no gradient accumulation, maximum norm of 1.0 for gradient clipping, and no   \n543 warmup steps. Training epochs used for all Alpaca and OpenHermes experiments were three and   \n544 one, respectively. For both Pythia and Mamba models, the learning rate and LoRA dimension $r$ were   \n545 scaled to improve performance of smaller models (per-model values listed in Table 3).   \n546 For SLL LoRA, targeted Mamba layers were $\\{{\\tt x}_{-}{\\tt p r o j}$ , embeddings, in_proj, out_proj};   \n547 x_proj is the large MambaBlock memory buffer which, when targeted   \n548 by LoRA, regularizes the majority of SSM parameters during fine-tuning   \n549 through weight tying (Theorem 1). Pythia targeted SLL LoRA layers were   \n550 {dense, embed_in, query_key_value, dense_h_to_4h,dense_4h_to_h}, chosen to   \n551 balance performance across model sizes. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "552 All experiments in Tables 1 and 2, Figures 1 and 6 were run using a signle-GPU Nvidia A10G (24 553 GB total memory). For Pythia and Mamba ALL LoRA experiments in Figure 3, all experiments were 554 run on an A10G, except for Mamba 2.8B, which exceeded A10G memory capacity and was run on an Nvidia H100 (80 GB total memory). ", "page_idx": 14}, {"type": "table", "img_path": "C3t6GMPnC5/tmp/8ced980b3eb4a946923c0842da3ba73086d139b5d5bed45a58f4f49135226151.jpg", "table_caption": ["Table 3: Learning rate and LoRA dimension $r$ values "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "555 ", "page_idx": 14}, {"type": "text", "text": "556 For the hyperparameter search results in Figure 6.1, all experiments were run using 8 H100 GPUs.   \n557 SLL LoRA was used for Mamba and Pythia models. The range of hyperparameter values was as   \n558 follows: ", "page_idx": 14}, {"type": "text", "text": "560 ", "page_idx": 14}, {"type": "text", "text": "\u2022 learning ra $\\mathbf{t}\\in\\{1e-7,2e-7,5e-7,1e-6,2e-6,5e-6,1e-5,2e-5,5e-5,1e-6,5e-6,2e+6,6e-7,6e+8,6e-9\\}$ $4,2e-4,5e-4,1e-3,2e-3,5e-3\\}$ ", "page_idx": 14}, {"type": "text", "text": "561 \u2022 LoRA dimension $r\\in\\{16,32,64,128,256\\}$   \n562 \u2022 warmup steps $\\in\\{0,1000,2000\\}$ ", "page_idx": 15}, {"type": "text", "text": "563 All other hyperparameters followed previous experiments. ", "page_idx": 15}, {"type": "text", "text": "564 The Alpaca dataset is freely available for download at ttps://huggingface.co/datasets/   \n565 tatsu-lab/alpaca under open-source license CC-by-NC 4.0. The OpenHermes dataset is freely   \n566 available for download at https://huggingface.co/datasets/teknium/OpenHermes-2.5 un  \n567 der open-source license MIT, Apache 2.0, CC. ", "page_idx": 15}, {"type": "text", "text": "568 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "569 1. Claims   \n570 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n571 paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Justification: All claims made in the abstract and introduction are directly derived from theoretical and experimental results presented in the main paper. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "585 2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Limitations of experimental results are described in the limitations section, under Discussion. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "617 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "18 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n19 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes]   \nJustification: All theoretical results list any underlying assumptions in the main text and full proofs are available in the supplementary.   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "634 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "635 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n636 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n637 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Relevant experimental results are detailed in the main text, with extensive details for all experiments further elaborated upon in the supplementary. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "673 5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "674 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n675 tions to faithfully reproduce the main experimental results, as described in supplemental   \n676 material?   \n677 Answer: [No]   \n678 Justification: While we currently answer no, and provide enough detail to reproduce our   \n679 experiments, we are actively working towards packaging our code for release. All datasets   \n680 are already open source, with licenses listed in the supplementary material.   \n681 Guidelines:   \n682 \u2022 The answer NA means that paper does not include experiments requiring code.   \n683 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n684 public/guides/CodeSubmissionPolicy) for more details.   \n685 \u2022 While we encourage the release of code and data, we understand that this might not be   \n686 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n687 including code, unless this is central to the contribution (e.g., for a new open-source   \n688 benchmark).   \n689 \u2022 The instructions should contain the exact command and environment needed to run to   \n690 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n691 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n692 \u2022 The authors should provide instructions on data access and preparation, including how   \n693 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n694 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n695 proposed method and baselines. If only a subset of experiments are reproducible, they   \n696 should state which ones are omitted from the script and why.   \n697 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n698 versions (if applicable).   \n699 \u2022 Providing as much information as possible in supplemental material (appended to the   \n700 paper) is recommended, but including URLs to data and code is permitted.   \n701 6. Experimental Setting/Details   \n702 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n703 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n704 results?   \n705 Answer: [Yes]   \n706 Justification: All datasets are open source, and all experimental hyperparameters are specified   \n707 in the paper. All results are fully reproducible with these details.   \n708 Guidelines:   \n709 \u2022 The answer NA means that the paper does not include experiments.   \n710 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n711 that is necessary to appreciate the results and make sense of them.   \n712 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n713 material.   \n714 7. Experiment Statistical Significance   \n715 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n716 information about the statistical significance of the experiments?   \n717 Answer: [No]   \n718 Justification: The paper does not report statistical significance.   \n719 Guidelines:   \n720 \u2022 The answer NA means that the paper does not include experiments.   \n721 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n722 dence intervals, or statistical significance tests, at least for the experiments that support   \n723 the main claims of the paper.   \n724 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n725 example, train/test split, initialization, random drawing of some parameter, or overall   \n726 run with given experimental conditions).   \n727 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n728 call to a library function, bootstrap, etc.)   \n729 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n730 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n731 of the mean.   \n732 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n733 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n734 of Normality of errors is not verified.   \n735 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n736 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n737 error rates).   \n738 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n739 they were calculated and reference the corresponding figures or tables in the text.   \n740 8. Experiments Compute Resources   \n741 Question: For each experiment, does the paper provide sufficient information on the com  \n742 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n743 the experiments?   \n744 Answer: [Yes]   \n745 Justification: The paper details (at length) the hardware requirements necessary to run each   \n746 experiment. Environmental requirements are available as experimental details both in the   \n747 main text and supplementary.   \n748 Guidelines:   \n749 \u2022 The answer NA means that the paper does not include experiments.   \n750 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n751 or cloud provider, including relevant memory and storage.   \n752 \u2022 The paper should provide the amount of compute required for each of the individual   \n753 experimental runs as well as estimate the total compute.   \n754 \u2022 The paper should disclose whether the full research project required more compute   \n755 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n756 didn\u2019t make it into the paper).   \n757 9. Code Of Ethics   \n758 Question: Does the research conducted in the paper conform, in every respect, with the   \n759 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n760 Answer: [Yes]   \n761 Justification: The work detailed in the paper conforms to all aspect of the NeurIPS Code of   \n762 Ethics.   \n763 Guidelines:   \n764 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n765 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n766 deviation from the Code of Ethics.   \n767 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n768 eration due to laws or regulations in their jurisdiction).   \n769 10. Broader Impacts   \n770 Question: Does the paper discuss both potential positive societal impacts and negative   \n771 societal impacts of the work performed?   \n772 Answer: [Yes]   \n773 Justification: The societal impact of this work is addressed in the Discussion section.   \nGuidelines:   \n775 \u2022 The answer NA means that there is no societal impact of the work performed.   \n776 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n777 impact or why the paper does not address societal impact.   \n778 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n779 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n780 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n781 groups), privacy considerations, and security considerations.   \n782 \u2022 The conference expects that many papers will be foundational research and not tied   \n783 to particular applications, let alone deployments. However, if there is a direct path to   \n784 any negative applications, the authors should point it out. For example, it is legitimate   \n785 to point out that an improvement in the quality of generative models could be used to   \n786 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n787 that a generic algorithm for optimizing neural networks could enable people to train   \n788 models that generate Deepfakes faster.   \n789 \u2022 The authors should consider possible harms that could arise when the technology is   \n790 being used as intended and functioning correctly, harms that could arise when the   \n791 technology is being used as intended but gives incorrect results, and harms following   \n792 from (intentional or unintentional) misuse of the technology.   \n793 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n794 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n795 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n796 feedback over time, improving the efficiency and accessibility of ML).   \n797 11. Safeguards   \n798 Question: Does the paper describe safeguards that have been put in place for responsible   \n799 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n800 image generators, or scraped datasets)?   \n801 Answer: [No]   \n802 Justification: The work does not aim to release pretrained models or datasets.   \n803 Guidelines:   \n804 \u2022 The answer NA means that the paper poses no such risks.   \n805 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n806 necessary safeguards to allow for controlled use of the model, for example by requiring   \n807 that users adhere to usage guidelines or restrictions to access the model or implementing   \n808 safety filters.   \n809 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n810 should describe how they avoided releasing unsafe images.   \n811 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n812 not require this, but we encourage authors to take this into account and make a best   \n813 faith effort.   \n814 12. Licenses for existing assets   \n815 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n816 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n817 properly respected?   \n818 Answer: [Yes]   \n819 Justification: Extensive lengths were made to cite all original authors for any and all utilized   \n820 code/data/work.   \n821 Guidelines:   \n822 \u2022 The answer NA means that the paper does not use existing assets.   \n823 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n824 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n825 URL.   \n826 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n827 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n828 service of that source should be provided.   \n829 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n30 package should be provided. For popular datasets, paperswithcode.com/datasets   \n31 has curated licenses for some datasets. Their licensing guide can help determine the   \n32 license of a dataset.   \n33 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n34 the derived asset (if it has changed) should be provided.   \n835 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n836 the asset\u2019s creators.   \n37 13. New Assets   \n838 Question: Are new assets introduced in the paper well documented and is the documentation   \n839 provided alongside the assets?   \n40 Answer: [NA]   \n841 Justification: The paper currently does not release source code. However, as previously   \n42 mentioned, we are actively working to remedy this.   \n843 Guidelines:   \n44 \u2022 The answer NA means that the paper does not release new assets.   \n884456 \u2022 sRuebsemairscshioernss  svhioau sltdr uccotmurmedu ntiecamtpe ltahtee sd. etTaihliss  oifn tchlue ddeast adseetta/iclos dae/bmouotd terl aaisn ipnagr,t  loifc ethnesier,   \n847 limitations, etc.   \n848 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n849 asset is used.   \n850 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n851 create an anonymized URL or include an anonymized zip file.   \n52 14. Crowdsourcing and Research with Human Subjects   \n53 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n854 include the full text of instructions given to participants and screenshots, if applicable, as   \n55 well as details about compensation (if any)?   \n56 Answer: [NA]   \n57 Justification: [TODO]   \n858 Guidelines:   \n859 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n860 human subjects.   \n861 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n862 tion of the paper involves human subjects, then as much detail as possible should be   \nincluded in the main paper.   \n64 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n865 or other labor should be paid at least the minimum wage in the country of the data   \n66 collector.   \n867 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n868 Subjects   \n886790 Question: Does the paper describe potential risks incurred by study participants, whether   \nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n871 approvals (or an equivalent approval/review based on the requirements of your country or   \n872 institution) were obtained?   \n73 Answer: [NA]   \n74 Justification: [TODO]   \n875 Guidelines:   \n876 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n77 human subjects. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]