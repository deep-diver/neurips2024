[{"figure_path": "C3t6GMPnC5/figures/figures_6_1.jpg", "caption": "Figure 1: Mean full-precision (FP32) divergence in MMLU performance for Mamba and Pythia\nmodels. Models are fine-tuned over the Alpaca dataset [51] using different combinations of MPFT\nand PEFT. Full fine-tuning (i.e., no PEFT adapters) is denoted as Full.", "description": "This figure compares the performance divergence between full-precision fine-tuning and mixed-precision fine-tuning with and without PEFT (Parameter-Efficient Fine-Tuning) on both Mamba and Pythia models.  The MMLU (Massive Multitask Language Understanding) dataset was used to evaluate the performance difference, averaged across different shot settings.  The goal is to show that even with mixed-precision and PEFT, the performance of Mamba models remains stable and comparable to that of Pythia models.", "section": "6 Experiments"}, {"figure_path": "C3t6GMPnC5/figures/figures_7_1.jpg", "caption": "Figure 1: Mean full-precision (FP32) divergence in MMLU performance for Mamba and Pythia models. Models are fine-tuned over the Alpaca dataset [51] using different combinations of MPFT and PEFT. Full fine-tuning (i.e., no PEFT adapters) is denoted as Full.", "description": "The figure shows the mean divergence in MMLU performance (across 0, 1, 3, and 5-shot settings) between full-precision fine-tuning and different combinations of mixed-precision fine-tuning (MPFT) and parameter-efficient fine-tuning (PEFT) using LoRA adapters for both Mamba and Pythia models.  The x-axis shows different model sizes, and the bars represent different fine-tuning approaches. The divergence is a measure of how much the model's performance changes when using mixed-precision and PEFT compared to full-precision fine-tuning.", "section": "Experiments"}, {"figure_path": "C3t6GMPnC5/figures/figures_7_2.jpg", "caption": "Figure 3: Fine-tuning narrows the ICL gap between Mamba and Pythia. ALL LORA models were instruction fine-tuned on the OpenHermes [52] dataset for one epoch. Performance is reported as the average improvement percentage of {1, 3, 5}-shot versus 0-shot over five standard benchmarks.", "description": "This figure compares the in-context learning (ICL) performance of pretrained and fine-tuned Mamba and Pythia models across five standard natural language benchmarks.  The y-axis shows the percentage improvement in performance from zero-shot to few-shot (1, 3, and 5-shot) settings. The figure demonstrates that while pretrained Mamba models lag behind Pythia models in ICL, fine-tuning with ALL LORA significantly closes the performance gap, bringing Mamba's ICL performance closer to that of Pythia.", "section": "6.1 Fine-tuning narrows the ICL gap between Mamba and Transformers"}, {"figure_path": "C3t6GMPnC5/figures/figures_8_1.jpg", "caption": "Figure 4: Fine-tuning hyperparameter search for OpenHermes. Each point is a different hyperparameter configuration. SLL LoRA was used for both models. The x-axis is the learning rate, the y-axis is resulting MMLU 5-shot performance, bubble size is the LoRA dimension, and the color is the number of warmup steps \u2208 {0, 1k, 2k}.", "description": "This figure displays the result of a hyperparameter search for fine-tuning Mamba and Pythia models on the OpenHermes dataset using the SLL LoRA method.  The x-axis represents the learning rate, the y-axis shows the resulting MMLU 5-shot accuracy, the size of the bubbles indicates the LoRA dimension, and the color corresponds to the number of warmup steps (0, 1000, or 2000). The figure helps to visualize the impact of different hyperparameter choices on the model's performance and to identify the optimal settings for both Mamba and Pythia.", "section": "6 Experiments"}]