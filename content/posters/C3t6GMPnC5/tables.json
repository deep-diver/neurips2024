[{"figure_path": "C3t6GMPnC5/tables/tables_2_1.jpg", "caption": "Table 1: In-context learning performance for pretrained Mamba and Pythia models. Models are collected into parameter classes for head-to-head comparison using the groupings in [22]. Model checkpoints were evaluated on all benchmarks and few-shot settings using the LM evaluation harness from Eleuther AI [16]. LAMBADA zero-shot is more effective for the model sizes considered (further discussed in [61, 8]) and thus excluded from few-shot performance averages. Highlighted in bold is the top-performing few-shot learner per benchmark and model grouping.", "description": "This table compares the in-context learning performance of pretrained Mamba and Pythia models across various benchmarks (LAMBADA, HellaSwag, PIQA, Arc-E, Arc-C, WinoGrande).  Models are grouped by parameter size, and performance is evaluated using 0, 1, 3, and 5-shot settings. LAMBADA zero-shot results are excluded due to their superiority at the evaluated model sizes. The table highlights the top-performing model for each benchmark and parameter group, showing that Pythia generally outperforms Mamba in few-shot learning on standard benchmarks.", "section": "1 Introduction"}, {"figure_path": "C3t6GMPnC5/tables/tables_5_1.jpg", "caption": "Table 1: In-context learning performance for pretrained Mamba and Pythia models. Models are collected into parameter classes for head-to-head comparison using the groupings in [22]. Model checkpoints were evaluated on all benchmarks and few-shot settings using the LM evaluation harness from Eleuther AI [16]. LAMBADA zero-shot is more effective for the model sizes considered (further discussed in [61, 8]) and thus excluded from few-shot performance averages. Highlighted in bold is the top-performing few-shot learner per benchmark and model grouping.", "description": "This table compares the in-context learning performance of pretrained Mamba and Pythia models across various benchmarks.  Models are grouped by parameter size, and performance is measured using metrics like accuracy and perplexity for different numbers of shots (few-shot learning).  The LAMBADA zero-shot results are excluded due to their high effectiveness for the model sizes shown.  The table highlights the best-performing model within each parameter group for each benchmark.", "section": "1 Introduction"}, {"figure_path": "C3t6GMPnC5/tables/tables_14_1.jpg", "caption": "Table 1: In-context learning performance for pretrained Mamba and Pythia models. Models are collected into parameter classes for head-to-head comparisons using the groupings in [22]. Model checkpoints were evaluated on all benchmarks and few-shot settings using the LM evaluation harness from Eleuther AI [16]. LAMBADA zero-shot is more effective for the model sizes considered (further discussed in [61, 8]) and thus excluded from few-shot performance averages. Highlighted in bold is the top-performing few-shot learner per benchmark and model grouping.", "description": "This table compares the in-context learning performance of pretrained Mamba and Pythia models across various benchmarks.  Models are grouped by parameter size for easier comparison.  The table shows the performance (accuracy and perplexity) for different numbers of shots (0, 1, 3, 5), allowing assessment of few-shot learning capabilities.  LAMBADA zero-shot results are excluded because they're superior to few-shot results in this model size range. The best performing model for each benchmark and parameter size is highlighted.", "section": "1 Introduction"}]