{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-MM-DD", "reason": "This paper is foundational to the field of in-context learning, introducing the concept of large language models as few-shot learners."}, {"fullname_first_author": "Samuel M\u00fcller", "paper_title": "Transformers can do Bayesian inference", "publication_date": "2021-MM-DD", "reason": "This paper provides a Bayesian interpretation of in-context learning, which forms the theoretical basis for the current work's approach."}, {"fullname_first_author": "Sang Michael Xie", "paper_title": "An explanation of in-context learning as implicit Bayesian inference", "publication_date": "2021-MM-DD", "reason": "This paper offers another Bayesian perspective on in-context learning, providing further theoretical support for the current paper's methodology."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-MM-DD", "reason": "This paper introduces Llama-2, a large language model used in the empirical evaluation of the proposed hallucination rate estimation method."}, {"fullname_first_author": "Marta Garnelo", "paper_title": "Conditional neural processes", "publication_date": "2018-MM-DD", "reason": "This paper introduces conditional neural processes, a model architecture that is similar to the one used in the synthetic regression experiments of the current work."}]}