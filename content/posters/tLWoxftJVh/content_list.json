[{"type": "text", "text": "Consistency Purification: Effective and Efficient Diffusion Purification towards Certified Robustness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiquan ${\\bf L i}^{1*}$ Zhongzhu Chen2\u2217 Kun $\\mathbf{Jin}^{2*}$ Jiongxiao Wang1\u2217 Jiachen Lei3 Bo Li4 Chaowei Xiao1 ", "page_idx": 0}, {"type": "text", "text": "1University of Wisconsin-Madison; 2 University of Michigan-Ann Arbor;   \n3California Institute of Technology ;4University of Illinois Urbana-Champaign ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion Purification, purifying noised images with diffusion models, has been widely used for enhancing certified robustness via randomized smoothing. However, existing frameworks often grapple with the balance between efficiency and effectiveness. While the Denoising Diffusion Probabilistic Model (DDPM) offers an efficient single-step purification, it falls short in ensuring purified images reside on the data manifold. Conversely, the Stochastic Diffusion Model effectively places purified images on the data manifold but demands solving cumbersome stochastic differential equations, while its derivative, the Probability Flow Ordinary Differential Equation (PF-ODE), though solving simpler ordinary differential equations, still requires multiple computational steps. In this work, we demonstrated that an ideal purification pipeline should generate the purified images on the data manifold that are as much semantically aligned to the original images for effectiveness in one step for efficiency. Therefore, we introduced Consistency Purification, an efficiency-effectiveness Pareto superior purifier compared to the previous work. Consistency Purification employs the consistency model, a one-step generative model distilled from PF-ODE, thus can generate on-manifold purified images with a single network evaluation. However, the consistency model is designed not for purification thus it does not inherently ensure semantic alignment between purified and original images. To resolve this issue, we further refine it through Consistency Fine-tuning with LPIPS loss, which enables more aligned semantic meaning while keeping the purified images on data manifold. Our comprehensive experiments demonstrate that our Consistency Purification framework achieves state-of-the-art certified robustness and efficiency compared to baseline methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models were first proposed for high-quality image generation [1, 2, 3, 4, 5] and have been extended to generative tasks across various modalities, including audio [6, 7, 8], video [9, 10], and 3D object [11, 12, 13]. A diffusion model for image generation typically involves two key processes: (1) a forward diffusion process, which transforms the source image into an isotropic Gaussian by gradually adding Gaussian noise, and (2) the reverse diffusion process, which uses a Deep Neural Network (DNN) to perform iterative denoising starting from random Gaussian noise. ", "page_idx": 0}, {"type": "text", "text": "Due to the inherent denoising capability of diffusion models, there have been widely applied to improve the robustness of DNNs. This enhancement is achieved by Diffusion Purification [14, 15, 16, 17, 18], which purifies the network inputs to reduce the effects of various types of unforeseen corruptions or adversarial attacks. Among these, one particularly suitable and effective scenario of purification is to improve certified robustness through randomized smoothing [19] for image classification tasks. This method guarantees a tight robustness in the $\\ell_{2}$ norm with a smoothed classifier. However, many previous works [19, 20, 21, 22, 23, 24] have shown that it still requires retraining with Gaussian augmented examples for each noise level to optimize the smoothed classifier. Diffusion models, capable of purifying Gaussian perturbed images before classification, can be seamlessly integrated with any base classifier to produce a smoothed classifier for arbitrary noise levels. This integration has been demonstrated to effectively enhance certified robustness, as supported by numerous studies [18, 25, 26, 27]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "tLWoxftJVh/tmp/8a08012d77b69964a53576fcda44aa1792875e18b6d0312af5a87a71021add91.jpg", "img_caption": ["Figure 1: An illustration of Consistency Purification framework. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, current diffusion purification for certified robustness via randomized smoothing still faces significant trade-offs between efficiency and effectiveness. Although Denoising Diffusion Probabilistic Model (DDPM) [28] only requires one single network evaluation in the purification process [25], it generates the mean of the posterior data distribution conditioned the noisy sample, which does not necessarily locate on the data manifold and may exhibit ambiguity during classification. To further improve diffusion purification, various methods such as DensePure [26], Local Smoothing [27] and Noised Diffusion Classifiers [29] are applied. However, these methods are considerably less efficient as they require multiple times of the computational costs compared to one-step DDPM. Another promising approach involves using the Probability Flow Ordinary Differential Equation (PF-ODE) [3]. It has offered a method to accelerate the sampling process [4] and achieved a closer distribution to the original data, well balancing efficiency and effectiveness. However, several computational steps are still needed to solve the ODE numerically. ", "page_idx": 1}, {"type": "text", "text": "To find a Pareto superior solution in terms of efficiency and effectiveness, we introduce a new framework, Consistency Purification, which integrating consistency models into diffusion purification with Consistency Fine-tuning. The consistency model is a novel category of diffusion models that learns the trajectory of the PF-ODE that transits the data distribution to the noisy distribution. It is trained to map any point along this trajectory back to its starting point. This property is desirable for diffusion purification, as it allows images with any scale of Gaussian noise to be directly purified to the clean images. Distilled from a pre-trained diffusion model by simulating the PF-ODE trajectory, the consistency model can generate high-quality in-distribution images in a single step, thereby ensuring both efficiency and effectiveness. However, since consistency models are primarily trained for image generation, it may not suffice to guarantee that the purified image that maintains the same semantic meaning as the original image. To address this issue, we propose adding a Consistency Fine-tuning step into the purification framework, which further fine-tunes the consistency model using Learned Perceptual Image Patch Similarity (LPIPS) [30] loss, aiming to minimize the perceptual differences between the purified and original images, thereby ensuring better semantic alignment, while at the same time, ensuring the purified images still lie on the data manifold. ", "page_idx": 1}, {"type": "text", "text": "We show that Consistency Purification is Pareto superior compared to baselines from two aspects. First of all, compared with effective methods like DensePure [26], Local Smoothing [27] and Noised Diffusion Classifiers [29], Consistency Purification is much more efficient since it enables single-step purification. Secondly, compared with efficient method like onestep-DDPM [25], we provide both theoretical analysis and experiment results to support the effectiveness improvement of Consistency Purification. In Example 3.1, we show an one-dimensional example demonstrating that consistency model can generate on-manifold purified samples while onestep-DDPM does not have this property. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In Theorem 3.3, we show an important theoretical result that given a purifier, the lower the transport from the original distribution to the purified distribution (a measure of distance between probability distributions, see [31]), the higher the probability that the purified sample is sufficiently close to the original sample, and thus the better purification outcomes. Our experiment results verify that both the integration of consistency model in Consistency Purification and the further Consistency Fine-tuning decreases such transport and achieves better semantic alignment between purified samples and original samples. ", "page_idx": 2}, {"type": "text", "text": "Beyond the validation of our theory, we conduct comprehensive experiments to demonstrate the empirical improvements of Consistency Purification. Compared to various baseline settings, our approach has shown significant improvements, achieving an average $5\\%$ gain in performance over the previous onestep-DDPM under the same cost with single-step purification. These observations underscore our success in finding a Pareto superior diffusion purification framework in both efficiency and effectiveness for certified robustness. ", "page_idx": 2}, {"type": "text", "text": "2 Backgrounds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Randomized Smoothing [19]. Randomized smoothing is designed to certify the robustness of a given classifier under $\\ell_{2}$ norm perturbations. Given a base classifier $f$ and an input $\\textbf{\\em x}$ , randomized smoothing first defines the smoothed classifier by $g(\\pmb{x})=\\arg\\operatorname*{max}_{c}\\mathbb{P}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}I)}(f(\\pmb{x}+\\pmb{\\epsilon})=c)$ , where $\\sigma$ is the noise level, which controls the trade-off between robustness and accuracy. [19] shows that $g(x)$ induces the certifiable robustness for $\\textbf{\\em x}$ under the $\\ell_{2}$ norm with radius $R$ , where $\\begin{array}{r}{R=\\frac{\\sigma}{2}\\left(\\Phi^{-1}(p_{A})-\\Phi^{-1}(p_{B})\\right)}\\end{array}$ , where $p_{A}$ and $p_{B}$ are the probability of the most probable class and \u201crunner-up\u201d class respectively; $\\Phi$ is the inverse of the standard Gaussian CDF. The $p_{A}$ and $p_{B}$ can be estimated with arbitrarily high confidence via the Monte Carlo method. ", "page_idx": 2}, {"type": "text", "text": "Continuous-Time Diffusion Model [3]. The diffusion model has two components: the diffusion process followed by the reverse process. Given an input random variable $\\pmb{x}_{0}\\sim p$ , the diffusion process adds isotropic Gaussian noises to the data so that the diffused random variable at time $t$ is $\\mathbf{\\bar{\\boldsymbol{x}}}_{t}=\\sqrt{\\alpha_{t}}(\\mathbf{\\boldsymbol{x}}_{0}+\\bar{\\mathbf{\\boldsymbol{\\epsilon}}_{t}})$ , s.t., $\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{t}^{2}\\pmb{I})$ , and $\\sigma_{t}^{2}=(1-\\alpha_{t})/\\alpha_{t}$ , and we denote $\\pmb{x}_{t}\\sim p_{t}$ . The forward diffusion process can also be defined by the stochastic differential equation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}=D(\\pmb{x},t)\\mathrm{d}t+G(t)\\mathrm{d}\\pmb{w},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{x}_{0}\\sim p$ , $D:\\mathbb{R}^{d}\\times\\mathbb{R}\\mapsto\\mathbb{R}^{d}$ is the drift coefficient and typically has the form $D({\\pmb x},t)=D(t){\\pmb x}$ . $G:\\mathbb{R}\\mapsto\\mathbb{R}$ is the diffusion coefficient, $\\mathrm{d}t$ is an infinitesimal time step, and $\\pmb{w}(t)\\in\\mathbb{R}^{i}$ is the standard Wiener process. ", "page_idx": 2}, {"type": "text", "text": "The reverse process exists and removes the added noise by solving the reverse-time SDE [32] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}=[D(t)\\pmb{x}-G(t)^{2}\\nabla_{\\hat{x}}\\log p_{t}(\\pmb{x})]\\mathrm{d}t+G(t)\\mathrm{d}\\overline{{\\pmb{w}}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{t}(\\pmb{x})$ denotes the marginal distribution at time $t$ , and $\\overline{{\\pmb{w}}}(t)$ is a reverse-time standard Wiener process. [3] defined the probability flow ODE (PF ODE) which has the same marginal distribution as reverse-SDE but can be solved much faster ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\pmb{x}=\\left[D(t)\\pmb{x}-\\frac{1}{2}G(t)^{2}\\nabla_{\\pmb{x}}\\log p_{t}(\\pmb{x})\\right]\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As shown in [4], the perturbation kernel of SDE has the general form ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{0t}(\\pmb{x}(t)\\mid\\pmb{x}(0))=\\mathcal{N}\\left(\\pmb{x}(t);\\pmb{s}(t)\\pmb{x}(0),\\pmb{s}(t)^{2}\\sigma(t)^{2}\\mathbf{I}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{s(t)=\\exp\\left(\\int_{0}^{t}f(\\xi)\\mathrm{d}\\xi\\right)}\\end{array}$ and $\\begin{array}{r}{\\sigma(t)=\\sqrt{\\int_{0}^{t}\\frac{g(\\xi)^{2}}{s(\\xi)^{2}}\\;\\mathrm{d}\\xi}}\\end{array}$ . Under this formulation, PF-ODE can written as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\pmb{x}=\\left[\\frac{\\dot{s}(t)}{s(t)}\\pmb{x}-s(t)^{2}\\dot{\\sigma}(t)\\sigma(t)\\nabla_{\\pmb{x}}\\log p\\left(\\frac{\\pmb{x}}{s(t)};\\sigma(t)\\right)\\right]\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\cdot$ denotes the time derivative and $\\textstyle p\\left({\\frac{x}{s(t)}};\\sigma(t)\\right)$ denotes the marginal distribution at time $t$ . In our context, we use the $E D M$ parameter [4] where $s(t)=1$ and $\\sigma(t)=t$ which gives us a probability flow ODE ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}=-t\\nabla_{\\pmb{x}}\\log p_{t}(\\pmb{x})\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We use $\\{\\boldsymbol{x}_{t}\\}_{t\\in[0,1]}$ and $\\{\\hat{x}_{t}\\}_{t\\in[0,1]}$ to denote the diffusion process and the reverse process generated by SDE and reverse-SDE respectively, which follow the same distribution. We also use $\\{\\tilde{\\mathbf{\\alpha}}_{t}\\}_{t\\in}$ [0,1] to denote the reverse process generated by PF-ODE, which has the same marginal distribution as $\\{\\boldsymbol{x}_{t}\\}_{t\\in[0,1]}$ and $\\{\\hat{\\pmb{x}}_{t}\\}_{t\\in[0,1]}$ given $t$ . ", "page_idx": 3}, {"type": "text", "text": "Discrete-Time Diffusion Model (DDPM [28]). DDPM constructs a discrete Markov chain $\\{x_{0},x_{1},\\cdot\\cdot\\cdot,x_{i},\\cdot\\cdot\\cdot,x_{N}\\}$ as the forward process for the training data $\\textbf{\\textit{x}}\\sim\\textbf{\\textit{p}}$ , such that $\\mathbb{P}(\\pmb{x}_{i}|\\pmb{x}_{i-1})\\,=\\,\\mathcal{N}(\\pmb{x}_{i};\\sqrt{1-\\beta_{i}}\\pmb{x}_{i-1},\\beta_{i}I)$ , where $0\\;<\\;\\beta_{1}\\;<\\;\\beta_{2}\\;<\\;\\cdot\\,\\cdot\\;<\\;\\beta_{N}\\;<\\;1$ are predefined noise scales such that $\\pmb{x}_{N}$ approximates the Gaussian white n\u221aoise. Denote $\\begin{array}{r}{\\overline{{\\alpha}}_{i}=\\prod_{i=1}^{N}(1-\\beta_{i})}\\end{array}$ , we have $\\mathbb{P}(\\pmb{x}_{i}|\\pmb{x}_{0})=\\mathcal{N}(\\pmb{x}_{i};\\sqrt{\\overline{{\\alpha}}_{i}}\\pmb{x}_{0},(1-\\overline{{\\alpha}}_{i})\\pmb{I})$ , i.e., $\\pmb{x}_{t}(\\pmb{x}_{0},\\epsilon)=\\sqrt{\\overline{{\\alpha}}_{i}}\\pmb{x}_{0}+(1-\\overline{{\\alpha}}_{i})\\pmb{\\epsilon},\\epsilon\\sim\\mathcal{N}(\\pmb{0},I)$ . ", "page_idx": 3}, {"type": "text", "text": "The reverse process of DDPM learns a reverse direction variational Markov chain $p_{\\theta}(\\mathbf{x}_{i-1}|\\mathbf{x}_{i})=$ $\\mathcal{N}(\\pmb{x}_{i-1};\\pmb{\\mu}_{\\theta}\\bar{(\\pmb{x}_{i},i)},\\Sigma_{\\theta}(\\pmb{x}_{i},i))$ . [28] defines $\\epsilon_{\\theta}$ as a function approximator to predict $\\epsilon$ from $\\pmb{x}_{i}$ such that $\\begin{array}{r}{\\mu_{\\theta}(x_{i},i)=\\frac{1}{\\sqrt{1-\\beta_{i}}}\\left(x_{i}-\\frac{\\beta_{i}}{\\sqrt{1-{\\overline{{\\alpha}}}_{i}}}\\epsilon_{\\theta}(x_{i},i)\\right)}\\end{array}$ . Then the reverse time samples are generated by x\u02c6i \u2212\u221a1\u03b2\u2212i\u03b1i \u03f5\u03b8\u2217(x\u02c6i, i) + \u221a\u03b2i\u03f5, \u03f5 \u223cN(0, I), and the optimal parameters \u03b8\u2217are obtained by s $\\begin{array}{r}{\\mathrm{olving}\\;\\theta^{*}:=\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{x_{0},\\epsilon}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\sqrt{\\overline{{\\alpha_{i}}}}x_{0}+(1-\\overline{{\\alpha}}_{i}),i)\\|_{2}^{2}\\right]}\\end{array}$ . [28] also provided a one-step approximate reconstruction of $\\pmb{x}_{0}$ from any $\\pmb{x}_{t}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{0}\\approx\\hat{\\pmb{x}}_{0}=\\left(\\pmb{x}_{t}-\\sqrt{1-\\overline{{\\alpha}}_{t}}\\pmb{\\epsilon}_{\\theta}(\\pmb{x}_{t})\\right)/\\sqrt{\\overline{{\\alpha}}_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "(onestep-DDPM) ", "page_idx": 3}, {"type": "text", "text": "Consistency Model [33]. Given a solution trajectory of PF-ODE, the consistency model is defined as $D:({\\bf{\\boldsymbol{x}}}_{t},t)\\mapsto{\\bf{\\boldsymbol{x}}}_{\\epsilon}$ . The model exhibits the property of self-consistency, ensuring that its outputs are consistent for arbitrary pairs of $\\left(\\boldsymbol{x}_{t},t\\right)$ from the same PF-ODE trajectory; specifically, $\\bar{D(\\pmb{x}_{t},t)}=$ $D(x_{t^{\\prime}},t^{\\prime})$ for all $t,t^{\\prime}\\in[\\epsilon,T]$ . As shown by the definition, consistency models are suitable for oneshot denoising, allowing for the recovery of $\\pmb{x}_{\\epsilon}$ from any noisy input $\\pmb{x}_{t}$ in one network evaluation. Two distinct training strategies can be employed for training the consistency models: distillation mode and isolation mode. The primary distinction lies in whether the models distill the knowledge from pre-trained diffusion models or train from initial parameters. According to the experiments reported in [33], consistency models trained in the distillation mode have been shown to outperform those trained in isolation mode for generating high-quality images. Consequently, our paper only considers consistency models trained in the distillation mode. ", "page_idx": 3}, {"type": "text", "text": "3 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide theoretical explanations on the advantages of Consistency Purification, with a focus on its purification performance improvement in terms of certified robustness over [25]. ", "page_idx": 3}, {"type": "text", "text": "As demonstrated in [3], PF-ODE maintains the marginal distribution of reverse-SDE, thereby establishing a deterministic mapping between the noisy distribution $\\pmb{x}_{t}$ and the data distribution $\\scriptstyle x_{0}$ . In other words, PF-ODE guarantees that the purified sample lies on the data manifold, unlike onestep-DDPM, which lacks this assurance. We present here a simple one dimensional example for illustration. ", "page_idx": 3}, {"type": "text", "text": "Example 3.1. Consider a one-dimensional space with a data set consisting of two samples $\\{y_{1},y_{2}\\}$ , where $\\begin{array}{r}{y_{1}=1}\\end{array}$ and ${\\pmb y}_{2}=-1$ . The distribution can be represented as a mixture of Dirac delta distributions: $\\begin{array}{r}{p_{d a t a}({\\pmb x})=\\frac{1}{2}\\left(\\delta({\\pmb x}-{\\pmb y}_{1})+\\delta({\\pmb x}-{\\pmb y}_{2})\\right)}\\end{array}$ . By setting $s(t)=1$ and $\\sigma(t)=t$ in perturbationkernel, the distribution at time $t$ becomes: $\\begin{array}{r}{p_{t}(x)=\\frac{1}{2t\\sqrt{2\\pi}}\\bigl(e^{-\\frac{1}{2}\\left(\\frac{x-1}{t}\\right)^{2}}+e^{-\\frac{1}{2}\\left(\\frac{x+1}{t}\\right)^{2}}\\bigr)}\\end{array}$ . Then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\mathrm{d}\\log p_{t}\\left(x\\right)}{\\mathrm{d}x}}&{=\\frac{-\\left(\\frac{x-1}{t}\\right)e^{-\\frac{1}{2}\\left(\\frac{x-1}{t^{2}}\\right)^{2}}-\\left(\\frac{x+1}{t}\\right)e^{-\\frac{1}{2}\\left(\\frac{x+1}{t^{2}}\\right)^{2}}}{2t\\sqrt{2\\pi}p_{t}\\left(x\\right)}}\\\\ &{=\\;-\\;\\frac{x}{t^{2}}+\\frac{e^{-\\frac{1}{2}\\left(\\frac{x-1}{t}\\right)^{2}}-e^{-\\frac{1}{2}\\left(\\frac{x+1}{t}\\right)^{2}}}{e^{-\\frac{1}{2}\\left(\\frac{x-1}{t}\\right)^{2}}+e^{-\\frac{1}{2}\\left(\\frac{x+1}{t}\\right)^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From the derivative formula $\\frac{\\operatorname{d}\\log p_{t}(\\pmb{x})}{\\operatorname{d}\\!\\pmb{x}}$ , it\u2019s evident that $\\pmb{x}\\,=\\,0$ is an equilibrium point, and the right-hand side expression is Lipschitz continuous around $\\pmb{x}=0$ by $L$ \u2019H\u00f4pital\u2019s rule. Thus, according to the Picard-Lindel\u00f6f theorem, any trajectory starting on either side of $\\pmb{x}=0$ will not cross this point. As PF-ODE drives $p_{t}(\\pmb{x})$ closer to the Dirac delta distribution $p_{d a t a}(\\pmb{x})$ as t approaches zero, any initial point on positive/negative side of $\\pmb{x}=0$ will eventually approach 1 or $-1$ , i.e., the data manifold. Furthermore, in this example, $P F$ -ODE generates not only a purified sample on the data manifold but also closest to the noisy sample. This property is desirable as it establishes a relatively large \"robust\" neighborhood around each true data point, which implies high certified robustness and a significant certified radius, which will be further discussed later. With the consistency model, we do not need to solve the ODE but rather directly map the noisy sample to either $1/-1$ depending on its location relative to $\\pmb{x}=0$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "For comparison, given any $\\textbf{\\em x}$ and $t$ , the onestep-DDPM will output a posterior mean that is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{e^{-\\frac{1}{2}\\left(\\frac{x-1}{t}\\right)^{2}}-e^{-\\frac{1}{2}\\left(\\frac{x+1}{t}\\right)^{2}}}{e^{-\\frac{1}{2}\\left(\\frac{x-1}{t}\\right)^{2}}+e^{-\\frac{1}{2}\\left(\\frac{x+1}{t}\\right)^{2}}}=\\frac{e^{\\frac{2x}{t^{2}}}-1}{e^{\\frac{2x}{t^{2}}}+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The posterior mean will be near 1 or $-1$ only when $t$ is sufficiently small compared to $\\lVert x\\rVert$ . Otherwise, it deviates from the data manifold. In the case when $t$ is large, the posterior mean will be close to zero, locating in an ambiguous classification region. In adversarial purification [25, 26, 14], we typically select t based on the variance of the noise added to the data sample rather than using an very small t. This practice helps avoid significant deviations in the posterior mean estimation due to the imperfect estimation of score/noise. With a very small $t$ , even a slight bias in score/noise estimation can lead to a substantial deviation, resulting in a denoised sample even farther from the data manifold represented by $p_{d a t a}(\\pmb{x})$ . ", "page_idx": 4}, {"type": "text", "text": "Additionally, PF-ODE is deterministic, eliminating the overhead of majority voting required when using reverse-SDE as a purifier [26]. The consistency model, which reduces ODE solving to a one-step mapping, further ensures purification has the same efficiency as onestep-DDPM while keeping the in-distribution property. ", "page_idx": 4}, {"type": "text", "text": "Though the consistency model enjoys both in-distribution property and one-step efficiency, it does not guarantee that the purified sample has the same semantic meaning as the original sample. This is because the derivation of PF-ODE only guarantees a mapping between noisy distribution and data distribution, which is sufficient for generation, but not enough for denoising purposes. ", "page_idx": 4}, {"type": "text", "text": "To address this concern, we first delineate the desired characteristics of the purifier. As evidenced in prior works [14, 25, 26, 34], an ideal purifier should yield a purified output situated within a proximate vicinity of the original input. It is generally presumed that such purified outputs retain the semantic meaning of the original inputs with a high probability. The disparity in semantic consistency between the noisy input and the purified output generated by PF-ODE arises due to the proximity of the purified output to other samples. In this regard, we propose quantifying this disparity through the notion of transport between the data distribution and the purified distribution, derived by introducing Gaussian perturbations to the data distribution and subsequently applying denoising via PF-ODE. Given an original sample $\\textbf{\\em x}$ , Gaussian noise $\\epsilon$ , and purifier $d$ , the mapping in the transport process is defined as $T:x\\rightarrow d(\\mathbf{x}+\\epsilon)$ , which is probabilistic. We aim to demonstrate that a diminished transport between the data distribution and the purified distribution is conducive to a higher likelihood of the purified output being situated in proximity to the original sample, thereby preserving its semantic meaning. ", "page_idx": 4}, {"type": "text", "text": "We will leverage the following definition. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2. Given the data distribution $p$ , Gaussian noise $\\epsilon$ , timestep $t$ , and a purifier $d$ , we define $\\pi_{t}:x\\rightarrow d({\\pmb x}+t\\epsilon)$ and the \u201ctransport\" under $g_{t}$ between the data distribution and purified distribution as $\\begin{array}{r}{T_{\\pi_{t}}(p):=\\int||\\mathbf x-\\pi_{t}(\\mathbf x)||\\cdot\\dot{p}(\\pmb x)d\\pmb x}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Intuitively, transport measures the distance between the original and purified samples, which should be small by an effective purifier. Below, we quantify this intuition and present our main theorem. See the detailed proof in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3. Given the transport $T_{\\pi_{t}}(p)$ between the data distribution $p$ and the corresponding purified distribution under $g_{t}$ , then for any $r\\,>\\,0$ , the probability that the distance between the original sample $\\textbf{\\em x}$ and purified sample $\\hat{\\pmb{x}}=\\pi_{t}(\\pmb{x})$ is larger than $r$ is upper bounded by $\\frac{T_{\\pi_{t}}\\left(p\\right)}{r}$ ", "page_idx": 4}, {"type": "text", "text": "Remark 3.4. By Theorem 3.3, the efficacy of the purifier hinges on two crucial factors: the transport $T_{\\pi_{t}}(p)$ and the radius $r$ . A theoretically perfect purifier would yield zero transport; however, this is unattainable due to the inherent randomness of $g_{t}$ . Typically, we can optimize the parameter $t$ to minimize the transport, denoted as $\\begin{array}{r}{T^{*}=\\operatorname*{min}_{t}\\frac{T_{\\pi_{t}}(p)}{r}}\\end{array}$ tT\u03c0tr(p). In the context of classification tasks, the selection of $r$ also depends on the robustness of the classifier; a more robust classifier allows a larger $r$ to be chosen, thereby guarantee better purification efficacy. ", "page_idx": 4}, {"type": "image", "img_path": "tLWoxftJVh/tmp/9081987e756d0655c4fdf991f8b66c7cdac1526f89f4f84f00fa69095cf6b20c.jpg", "img_caption": ["Figure 2: Transport between purified images and clean images with noise level $\\sigma\\in$ $\\{\\bar{0}.25,0.5,0.75,1.0\\}$ . ", "Table 1: FID between purified and clean images on CIFAR-10 test set using different types of fine-tuning loss functions with noise level $\\sigma\\in$ $\\{0.25,0.5,1.0\\}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "tLWoxftJVh/tmp/356ed76b3fa034da61057971419215ce12797d0f7f1086dcb5fe7731cd7a3406.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "For ensuring consistency in semantic meaning between the original and purified samples, it is insufficient merely to minimize their distance; it is also necessary that the purified sample resides on the data manifold, which is the in-distribution property we previously mentioned. To concurrently achieve both objectives, rather than solely focusing on minimizing the Euclidean distance between the original and purified samples, we opt to minimize the Learned Perceptual Image Patch Similarity (LPIPS) loss between them. This strategy aids in mitigating the risk of the purified sample deviating from the data manifold, thereby preserving semantic meaning. In Table 1, we show that using LPIPS is better than $\\ell_{1}$ and $\\ell_{2}$ loss for Consistency Fine-tuning when we want to guarantee the generated images are in-distribution, where lower FID scores indicate better in-distribution properties. ", "page_idx": 5}, {"type": "text", "text": "Figure 2 validates the effectiveness of Consistency Purification based on our results in Theorem 3.3, it shows that both the integration of consistency model in Consistency Purification and the further Consistency Fine-tuning can decrease the transport from the original distribution to the purified distribution. Specifically, we can see that Consistency Purification achieves a lower average distance from the purified sample to the original sample compared with onestep-DDPM, and Consistency Fine-tuning further decreases this average distance, indicating both components result in a lower transport and thus a better semantic alignment between purified samples and original samples. ", "page_idx": 5}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We propose our framework, Consistency Purification, with a further improved version using Consistency Fine-tuning. ", "page_idx": 5}, {"type": "text", "text": "4.1 Consistency Purification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We introduce Consistency Purification, directly applying consistency model as a purifier to integrate with a base classifier into smoothed classifier for randomized smoothing. ", "page_idx": 5}, {"type": "text", "text": "Following Diffusion Denoised Smoothing outlined in [25], it is necessary to establish a mapping between Gaussian noise augmented images required by randomized smoothing and the noised image in the ODE trajectory of consistency model. For a given consistency model purifier $D_{\\theta}$ , any noisy input $\\pmb{x}_{t}\\sim\\mathcal{N}(\\pmb{x},t^{2}\\pmb{\\dot{I}})$ can be recovered to the trajectory\u2019s start $\\pmb{x}_{\\epsilon}$ by directly passing it through the model with time $t$ : $\\pmb{x}_{\\epsilon}=D_{\\theta}(\\pmb{x}_{t},t)$ . ", "page_idx": 5}, {"type": "text", "text": "When comparing this to the image augmented with additive Gaussian noise ${\\pmb x}_{r s}\\,\\sim\\,\\mathcal{N}({\\pmb x},\\sigma^{2}{\\pmb I})$ , which is required by randomized smoothing, we observe that $\\pmb{x}_{r s}$ and $\\pmb{x}_{t}$ share the same formula when $t=\\sigma$ . However, since the variances $\\sigma\\in\\{\\sigma_{i}\\}_{i=1}^{m}$ may not be used during the training of the consistency model, we empirically select the nearest time step $t$ from the discrete time steps used in training for each $\\sigma$ . ", "page_idx": 5}, {"type": "text", "text": "For the entire time horizon $[\\epsilon,T]$ with $N-1$ sub-interval boundaries $t_{1}=\\epsilon<t_{2}<\\cdot\\cdot\\cdot<t_{N}=T$ , the time steps used in training are computed by: $t_{i}=(\\epsilon^{1/\\rho}+\\left.^{i-1}/{}_{N-1}(T^{1/\\rho}-\\epsilon^{1/\\rho})\\right)^{\\rho}$ , where $\\rho=7$ . ", "page_idx": 5}, {"type": "text", "text": "tGiimvee ns ttehpe a rfioarn Cceo $\\sigma$ iosft eGnacuy sPsiuarni fnieodi sSe muoseotd hiinn gr abnyd .corresponding $t_{\\sigma}^{\\ast}$ $\\begin{array}{r}{t_{\\sigma}^{*}=\\{t_{i}|\\sigma\\in(\\frac{t_{i}-1+t_{i}}{2},\\frac{t_{i}+t_{i+1}}{2}]\\}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "4.2 Consistency Fine-tuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To optimize the consistency model for aligning semantic meanings during purification, we fine-tune the purifier $D_{\\theta}$ by minimizing the following loss function: $\\begin{array}{r}{\\mathcal{L}_{\\theta}=\\mathbb{E}\\|\\pmb{x}-D_{\\theta}(\\pmb{x}_{\\sigma},t_{\\sigma}^{*})\\|_{\\mathrm{LPIPS}}}\\end{array}$ , where the expectation is taken with $\\pmb{x}\\sim p_{d a t a}$ , $\\bar{\\sigma^{\\mathrm{~\\,~}}}\\sim\\mathcal{U}\\{\\sigma_{i}\\}_{i=1}^{m}$ , $\\pmb{x}_{\\sigma}\\sim\\mathcal{N}(\\pmb{x},\\sigma^{2}\\pmb{I})$ . Here LPIPS denotes the distance computed by the Learned Perceptual Image Patch Similarity [30]. $p_{d a t a}$ represents the distribution of the training data, from which clean images $\\textbf{\\em x}$ are sampled. $\\mathcal{U}\\{\\sigma_{i}\\}_{i=1}^{m}$ denotes the uniform distribution over $m$ different noise scales $\\sigma_{i}$ used for randomized smoothing. Typically, we select the scale set $\\sigma_{i}\\in\\{0.25,0.5,1.0\\}$ , which is commonly used to compute the certified radius via randomized smoothing. ", "page_idx": 6}, {"type": "text", "text": "After obtaining the fine-tuned consistency model purifier $D_{\\theta^{*}}$ ,it can replace the original model used in Consistency Purified Smoothing to purify any noised image $x_{r s}$ with Gaussian variance $\\sigma_{i}$ , resulting in the final purified image $\\pmb{x}_{p}$ by $\\pmb{x}_{p}=D_{\\theta^{*}}(\\pmb{x}_{r s},t_{\\sigma_{i}}^{*})$ . ", "page_idx": 6}, {"type": "text", "text": "We present the detailed algorithm of our Consistency Purification in Appendix A ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we begin by detailing the experimental settings, followed by our main results.   \nAdditionally, we conduct ablation studies to further demonstrate the effectiveness of our framework.   \nAll experiments are conducted with $1\\times$ NVIDIA RTX A5000 24GB GPU. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. We evaluate the Consistency Purification framework on both CIFAR-10 [35] and ImageNet64 [36]. CIFAR-10 contains $32\\times32$ pixel images across 10 different categories while ImageNet-64 includes $64\\times64$ pixel images across 1000 categories. Due to limited computational resources, we select 500 test images for CIFAR-10 from the 10,000 CIFAR-10 test set, choosing every 20th example in sequence (e.g., the 1st, 21st, 41st, etc.). Similarly, for the ImageNet-64 dataset, we sample 500 test examples from its 50,000 test examples using a fixed interval of 100. ", "page_idx": 6}, {"type": "text", "text": "Consistency Purification. For CIFAR-10, to demonstrate the effectiveness of Consistency Purification, we first perform purification with a public unconditional consistency model [37]. After that, to further improve the performance, we fine-tune the model with noise levels $\\sigma$ sampling from $\\{0.25,0.5,1.0\\}$ , shown as the ( $\\scriptstyle\\left({\\frac{1}{2}}+{\\frac{1}{2}}\\right)$ Consistency Fine-tuning). However, currently there is no publicly available unconditional consistency model checkpoint for the ImageNet dataset that can be used directly for purification purposes. The only available model is the conditional consistency model on ImageNet-64. Thus, here we trained an unconditional consistency model on ImageNet-64, initializing it with the existing conditional consistency model checkpoint. Details of the training process are included in Appendix C. Additionally, we also conduct Consistency Fine-tuning on ImageNet-64 model with noise levels $\\sigma\\in\\{0.05,0.\\dot{1}5,0.25\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Baselines. For comparative analysis of CIFAR-10, we conduct baseline experiments under various settings. The first baseline involves onestep-DDPM, where we employ the 50-M unconditional improved diffusion models from [2] utilizing the one-shot denoising method [25] for purification. Given that our consistency model is distilled from an EDM model [4], we include EDM as our baselines, applying both one-shot denoising (onestep-EDM) and ODE solver (PF-ODE EDM) for purification. Additionally, we include the recent advancement in diffusion purification methods, Diffusion Calibration, as a baseline following [38], which fine-tunes the diffusion model with the guidance of classifier WideResNet28-10 to improve the purification accuracy under the specific classifier. While for ImageNet-64, due to the lack of public unconditional EDM model, we only include the comparison baseline with onestep-DDPM. ", "page_idx": 6}, {"type": "text", "text": "Randomized Smoothing Settings. We set $N~=~10000$ for both CIFAR-10 and ImageNet as the number of sampling times used in randomized smoothing. We compute the certified radius for each test example at three different noise levels $\\sigma\\,\\in\\,\\{0.25,0.5,1.0\\}$ for CIFAR-10 and $\\sigma\\in$ $\\{0.05,0.15,0.25\\}$ for ImageNet-64. Then we calculate the proportion of test examples whose radius exceeds a specific threshold $\\epsilon$ . The highest accuracy among these noise levels is reported as the certified accuracy at $\\epsilon$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Classifiers. For the classifier used after purification for CIFAR-10, we employ ViT-B/16 model [39], which is pretrained on ImageNet-21k [36] and finetuned on CIFAR-10 dataset. In our ablation studies, we also use ResNet [40] and WideResNet [41] trained on CIFAR-10. For ImageNet-64, we make up-sampling on the $64\\!\\times\\!64$ images and directly apply ViT-B/16 as the classifier. ", "page_idx": 7}, {"type": "text", "text": "5.2 Main Results. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We present the certified accuracy of Consistency Purification for both CIFAR-10 and ImageNet64 dataset, with the results presented in Table 2. We also include the purification steps which decide whether the purifier needs multiple evaluation times through the networks (Multi Steps) other than a single network evaluation (One Step). As observed from Table 2, Consistency Purification significantly outperforms onestep-DDPM for both CIFAR-10 and ImageNet-64 with even higher certified accuracy with Consistency Fine-tuning. Besides, for CIFAR-10, the results also suggest the effectiveness of Consistency Purification with Consistency Fine-tuning when compared with more baseline methods such as onestep-EDM, PF-ODE EDM and Diffusion Calibration. For the detailed certified accuracy evaluation of fine-grained $\\epsilon$ at different noise levels $\\sigma$ , we present the results in Figure 3 compared with the onestep-DDPM setting. All results have demonstrated that Consistency Purification is able to certify robustness with both efficiency and effectiveness. ", "page_idx": 7}, {"type": "table", "img_path": "tLWoxftJVh/tmp/46973f395651c4dabd3f4bd688c6b5562bbcb2a5db92a9bd4c412a6695854600.jpg", "table_caption": ["Table 2: Certified Accuracy of Consistency Purification for CIFAR-10 and ImageNet-64. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "tLWoxftJVh/tmp/9156e7359743a6f8f808c57ce4c446d22f7995fb74cc5753897845adf29ead40.jpg", "img_caption": ["Figure 3: Certified Accuracy of Consistency Purification with fine-grained radius $\\epsilon$ . The left figure shows results on CIFAR-10, the right figure shows results on ImageNet-64. The lines demonstrate the certified accuracy of various radius $\\epsilon$ under different Gaussian noise levels $\\sigma$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "To better illustrate the significant improvement in certified robustness brought by Consistency Purification, we present visualizations of images after diffusion purification in Figure 4 for CIFAR10 at a noise level of $\\sigma\\ =\\ 0.5$ , compared with the onestep-DDPM approach. As shown, our method produces significantly higher-quality purified images than onestep-DDPM. Furthermore, these purified images achieve a notably higher classification accuracy when evaluated by the same classifier. Additional visualization examples for ImageNet-64 are included in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "tLWoxftJVh/tmp/3986e8f3b4b298c6bac565f5cf9b0211bf89cb593d4bfd69f5cd2c757b197692.jpg", "img_caption": ["(a) Purified images by onestep-DDPM "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "tLWoxftJVh/tmp/2abb848f0e45a3259796088ce33bbe6762d98e6fd136c75a5e0d0c7d4cffea1c.jpg", "img_caption": ["(b) Purified images by Consistency Purification "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Visualization of purified images after the diffusion purification by applying onestep-DDPM and Consistency Purification on CIFAR-10 with $\\sigma=0.5$ noise level. Identical noise patterns are applied to images at corresponding locations. A green border indicates that the purified image is correctly classified, while a red border denotes misclassification by the classifier. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Studies. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct various ablation studies to evaluate the effectiveness of our proposed method. ", "page_idx": 8}, {"type": "text", "text": "Comparison with Non-Diffusion-based Baselines. To compare Consistency Purification with various non-diffusion-based approaches, we conducted additional experiments to compute the certified accuracy under three non-diffusion-based methods [19, 22, 42]. [19] first proposed training a classifier with noisy images to ensure certified robustness. Subsequent works [22, 42] ", "page_idx": 8}, {"type": "table", "img_path": "tLWoxftJVh/tmp/d9991e03673cda18f20864054ad70ebd1309ca8ff2b70b5c1dc0490bf3eed864.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "build on [19]\u2019s methodology, attempting to Table 3: Certified Accuracy of Consistency Purification enhance the smoothed classifier by adding compared with non-diffusion-based baseline methods. prediction consistency regularization, or incorporating per-sample bias. The experimental results presented in Tabel 3 show that our method surpasses all previous non-diffusion-based methods in achieving higher certified accuracy, particularly with a significantly high clean performance at $\\epsilon=0.0$ . Furthermore, in contrast to non-diffusion-based methods, which incur significant costs by requiring additional fine-tuning of robust classifiers for each specific noise level, our method can be applied directly to any off-the-shelf classifiers, significantly broadening its practical applications. ", "page_idx": 8}, {"type": "text", "text": "Fine-tuning Loss Functions. To further demonstrate that LPIPS loss is the best choice considering both on-manifold purification and semantic meaning alignment, we assess the certified accuracy of Consistency Purification using different loss functions during Consistency Fine-tuning. Instead of LPIPS distance between the clean and purified images as the loss function, we experiment with $\\ell_{1}$ and $\\ell_{2}$ distances. Results in Table 4 indicate that Consistency Purification with LPIPS loss achieves the highest Certified Accuracy. In contrast, fine-tuning with $\\ell_{1}$ and $\\ell_{2}$ distances compromises the purification performance for certification. This demonstrates that fine-tuning with LPIPS loss function effectively aligns semantic meanings, whereas $\\ell_{1}$ or $\\ell_{2}$ distances may hurt them. ", "page_idx": 8}, {"type": "text", "text": "Noise Levels Sampling Schedules during Consistency Fine-tuning. In our experiments of Consistency Fine-tuning, we simply select the same sampling schedules of noise levels $\\sigma~\\sim$ $\\mathcal{U}\\{0.25,0.5,1.0\\}$ , uniformly sampling $\\sigma$ used in randomized smoothing. To empirically demonstrate its effectiveness, we compare this approach with continuous sampling schedules where $\\sigma\\sim\\mathcal{U}[0,1]$ . Results presented in Table 5 show that our discrete sampling schedule achieves higher certified accuracy. This indicates that fine-tuning with a discrete scale, aligned with the noise levels used in randomized smoothing, enhances certified robustness. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Certified Accuracy of Consistency Purification with different loss functions during fine-tuning for CIFAR-10. \"- -\" represents the setting without fine-tuning. ", "page_idx": 9}, {"type": "text", "text": "Table 5: Certified Accuracy of Consistency Purification with continuous and discrete sampling schedules during fine-tuning for CIFAR-10. \"- -\" represents the setting without fine-tuning. ", "page_idx": 9}, {"type": "table", "img_path": "tLWoxftJVh/tmp/9f5a05443a882f882941fc8544d4559538333c2407cd6f6d1f6eb35d204bca6b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "tLWoxftJVh/tmp/4eee53de6fc1c6d93fdb44c39e5c99ebd754b2c89060f2b98d0849b2beadb5a3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Generalizability with Different Classifiers. We compute certified accuracy with various classifiers to test if our framework maintains its effectiveness with arbitrary classifiers. The results, presented in Table 6, compare Consistency Fine-tuning with Diffusion Calibration, an alternative method to fine-tune diffusion models for improving the certified robustness. When evaluated across different classifiers, including ViT-B/16, ResNet56, and WideNet28-10, our method outperforms Diffusion Calibration except certified accuracy at $\\epsilon=0.0$ on WRN28-10 model. It is worth noting that the Diffusion Calibration, which requires a specific classifier for guidance during fine-tuning, exhibits limitations, only achieving comparable performance with the guidance classifier WRN28-10. This demonstrates the advantages of Consistency Fine-tuning in generalizing across different classifiers. ", "page_idx": 9}, {"type": "text", "text": "Fine-tuning Classifier vs. Fine-tuning Diffusion Model. A potential concern with Consistency Fine-tuning is the higher certified accuracy and lower training cost associated with Fine-tuning the Classifier (CLS-FT) compared to our approach of Fine-tuning the Diffusion Model (DM-FT). However, our experiments, as shown in Table 7, indicate that DM-FT does not confilct with CLS-FT; rather, combining these two methods achieves even higher certified accuracy. On another hand, although CLS-FT yield slightly higher certified accuracy than DM-FT, its requirement for fine-tuning a specific classifier compromises the natural property of diffusion purification frameworks with arbitrary off-the-shelf classifiers, thus limiting the practical applicability. ", "page_idx": 9}, {"type": "table", "img_path": "tLWoxftJVh/tmp/a5c08585565f4697829b358af8f59e8a251fcb46025e946b74d4443aaaf98966.jpg", "table_caption": ["Table 6: Certified Accuracy of Consistency Finetuning with different classifiers on CIFAR-10. The guidance classifier used in Diffusion Calibration is WideResNet28-10. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "tLWoxftJVh/tmp/002b034b4c9a7c29d05ceae84c6f4b5c2e27247f11c909ceafd7bc3e32135b84.jpg", "table_caption": ["Table 7: Certified Accuracy of Fine-tuning the Diffusion Model (DM-FT) compared with Finetuning the Classifier (CLS-FT) in diffusion purification frameworks on CIFAR-10. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced Consistency Purification, a novel framework proposed to enhance certified robustness via randomized smoothing. By incorporating consistency models into diffusion purification approach and further refining them through Consistency Fine-tuning, our empirical experiments have demonstrate the framework\u2019s ability to achieve high certified robustness efficiently with one single network evaluation for purification. ", "page_idx": 9}, {"type": "text", "text": "Limitations. A notable limitation of our study is that our empirical results do not include computing certified robustness of high-resolution images such as ImageNet $256\\!\\times\\!256$ . This constraint is due to the absence of publicly available checkpoints for the consistency model at this resolution. Additionally, training a consistency model for ImageNet $256\\!\\times\\!256$ would require huge computing resources, which are currently beyond our affordability. However, our framework is designed for adaptability and could be easily extended to ImageNet $256\\!\\times\\!256$ once these checkpoints become available. As a result, our empirical evaluations in this paper are limited to the CIFAR-10 and ImageNet $64\\!\\times\\!64$ datasets. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[2] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \n[3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[4] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022. [6] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2020.   \n[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, 2020.   \n[8] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Gradtts: A diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pages 8599\u20138608. PMLR, 2021.   \n[9] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633\u20138646, 2022.   \n[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \n[11] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2837\u20132845, 2021.   \n[12] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra. Holodiffusion: Training a 3d diffusion model using 2d images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18423\u201318433, 2023.   \n[13] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022.   \n[14] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification. In International Conference on Machine Learning (ICML), 2022.   \n[15] Shutong Wu, Jiongxiao Wang, Wei Ping, Weili Nie, and Chaowei Xiao. Defending against adversarial audio via diffusion model. In The Eleventh International Conference on Learning Representations, 2022.   \n[16] Jiachen Sun, Jiongxiao Wang, Weili Nie, Zhiding Yu, Zhuoqing Mao, and Chaowei Xiao. A critical revisit of adversarial robustness in 3d point cloud recognition with diffusion-driven purification. In International Conference on Machine Learning, pages 33100\u201333114. PMLR, 2023.   \n[17] Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, and Hongfei Fu. Guided diffusion model for adversarial purification. arXiv preprint arXiv:2205.14969, 2022.   \n[18] Quanlin Wu, Hang Ye, and Yuntian Gu. Guided diffusion model for adversarial purification from random noise. arXiv preprint arXiv:2206.10875, 2022.   \n[19] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1310\u20131320. PMLR, 09\u201315 Jun 2019.   \n[20] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. Advances in Neural Information Processing Systems, 32, 2019.   \n[21] Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certified radius. arXiv preprint arXiv:2001.02378, 2020.   \n[22] Jongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed classifiers. Advances in Neural Information Processing Systems, 33:10558\u201310570, 2020.   \n[23] Mikl\u00f3s Z Horv\u00e1th, Mark Niklas M\u00fcller, Marc Fischer, and Martin Vechev. Boosting randomized smoothing with variance reduced classifiers. arXiv preprint arXiv:2106.06946, 2021.   \n[24] Jongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Do-Guk Kim, and Jinwoo Shin. Smoothmix: Training confidence-calibrated smoothed classifiers for certified robustness. Advances in Neural Information Processing Systems, 34:30153\u201330168, 2021.   \n[25] Nicholas Carlini, Florian Tramer, J Zico Kolter, et al. (certified!!) adversarial robustness for free! arXiv preprint arXiv:2206.10550, 2022.   \n[26] Chaowei Xiao, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Weili Nie, Mingyan Liu, Anima Anandkumar, Bo Li, and Dawn Song. Densepure: Understanding diffusion models for adversarial robustness. In The Eleventh International Conference on Learning Representations, 2022.   \n[27] Jiawei Zhang, Zhongzhu Chen, Huan Zhang, Chaowei Xiao, and Bo Li. {DiffSmooth}: Certifiably robust learning via diffusion models and local smoothing. In 32nd USENIX Security Symposium (USENIX Security 23), pages 4787\u20134804, 2023.   \n[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.   \n[29] Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang, Hang Su, and Jun Zhu. Your diffusion model is secretly a certifiably robust classifier. arXiv preprint arXiv:2402.02316, 2024.   \n[30] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[31] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[32] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[33] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n[34] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In NeurIPS, 2018.   \n[35] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[36] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[37] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pages 32211\u201332252. PMLR, 2023.   \n[38] Jongheon Jeong and Jinwoo Shin. Multi-scale diffusion denoised smoothing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[39] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[40] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[41] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks, 2017.   \n[42] Mikl\u00f3s Z Horv\u00e1th, Mark Niklas M\u00fcller, Marc Fischer, and Martin Vechev. Robust and accurate\u2013 compositional architectures for randomized smoothing. arXiv preprint arXiv:2204.00487, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Consistency Purification Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide detailed descriptions of Consistency Purification in the following algorithms. Algorithm 1 presents the function of Consistency Fine-tuning and Consistency Purification respectively. Algorithm 2 shows the randomized smoothing algorithm from [19] with applying Consistency Purification to do prediction and compute the certified radius. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Consistency Fine-tuning and Consistency Purification ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: Consistency model purifier $D_{\\theta}$ where $\\theta$ represents the model parameters. Noise levels used   \nin randomized smoothing $\\{\\sigma_{i}\\}_{i=1}^{m}$ . Arbitrary classification model $f_{\\mathrm{clf}}$ . Fine-tuning learning rate   \n$\\eta$ .   \n1: function CONSISTENCYFINE-TUNING $(D_{\\theta})$   \n2: repeat   \n3: sample $x\\in$ Training Dataset, $\\sigma\\in\\{\\sigma_{i}\\}_{i=1}^{m}$   \n4: $\\begin{array}{r l}&{x_{\\sigma}\\gets x+\\mathcal{N}(0,\\sigma^{2}\\overset{\\mathcal{C}}{I})}\\\\ &{t_{\\sigma}^{*}\\gets\\mathrm{GETTIMESTEP}(\\sigma)}\\\\ &{\\mathcal{L}\\gets\\mathrm{LPIPS}(x,D_{\\theta}(x_{\\sigma},t^{*}))}\\\\ &{\\theta\\gets\\theta-\\eta\\nabla_{\\theta}\\mathcal{L}}\\end{array}$   \n5:   \n6:   \n7:   \n8: until convergence   \n9: return $D_{\\theta}$   \n10: end function   \n11: function CONSISTENCYPURIFICATION $(f_{\\mathrm{clf}},x,\\sigma)$   \n12: $\\begin{array}{r l}&{t_{\\sigma}^{*}\\leftarrow\\mathrm{GETTIMESTEP}(\\sigma)}\\\\ &{\\pmb{x}_{r s}\\leftarrow\\pmb{x}+\\mathcal{N}(0,\\sigma^{2}I)}\\\\ &{\\pmb{x}_{p}\\leftarrow D_{\\theta^{*}}(\\pmb{x}_{r s},t_{\\sigma}^{*})}\\\\ &{y\\leftarrow f_{\\mathrm{clf}}(\\pmb{x}_{p})}\\end{array}$   \n13:   \n14:   \n15:   \n16: return $y$   \n17: end function   \n18: function GETTIMESTEP $(\\sigma)$   \n19: $\\begin{array}{r l}&{t_{i}\\gets(\\epsilon^{1/\\rho}+\\frac{i-1}{N-1}(T^{1/\\rho}-\\epsilon^{1/\\rho}))^{\\rho}\\;\\mathrm{for}\\;i\\in\\{1,\\dots,N\\}}\\\\ &{t_{\\sigma}^{*}\\gets\\mathrm{find}\\;\\{t_{i}|\\sigma\\in\\left(\\frac{t_{i-1}+t_{i}}{2},\\frac{t_{i}+t_{i+1}}{2}\\right]\\}}\\end{array}$   \n20:   \n21: return $t_{\\sigma}^{\\ast}$   \n22: end function ", "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 3.3. Given the transport $T_{\\pi_{t}}(p)$ between the data distribution $p$ and the corresponding purified distribution under $g_{t}$ , then for any $r\\,>\\,0$ , the probability that the distance between the original sample $\\textbf{\\em x}$ and purified sample $\\hat{\\pmb{x}}=\\pi_{t}(\\pmb{x})$ is larger than $r$ is upper bounded by $\\frac{T_{\\pi_{t}}\\left(p\\right)}{r}$ ", "page_idx": 13}, {"type": "text", "text": "Proof. We can leverage the Markov\u2019s inequality. Because ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[\\|\\pmb{x}-\\hat{\\pmb{x}}\\|]\\|=\\int_{\\|\\pmb{x}-\\hat{\\pmb{x}}\\|\\leq r}\\|\\pmb{x}-\\hat{\\pmb{x}}\\|\\cdot p(\\pmb{x})d\\pmb{x}+\\int_{\\|\\pmb{x}-\\hat{\\pmb{x}}\\|>r}\\|\\pmb{x}-\\hat{\\pmb{x}}\\|\\cdot p(\\pmb{x})d\\pmb{x}}\\\\ {\\geq\\int_{\\|\\pmb{x}-\\hat{\\pmb{x}}\\|>r}\\|\\pmb{x}-\\hat{\\pmb{x}}\\|\\cdot p(\\pmb{x})d\\pmb{x}}\\\\ {\\geq\\int_{\\|\\pmb{x}-\\hat{\\pmb{x}}\\|>r}r\\cdot p(\\pmb{x})d\\pmb{x}}\\\\ {=r\\cdot P(\\|\\pmb{x}-\\hat{\\pmb{x}}\\|>r),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\|\\pmb{x}-\\hat{\\pmb{x}}\\|>r)\\leq\\frac{\\mathbb{E}[\\|\\pmb{x}-\\hat{\\pmb{x}}\\|]}{r}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Randomized Smoothing [19] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: Sampling times for prediction $n$ . Sampling times for certification $N$ . Significant confidence   \nlevel $\\alpha$ . Function LOWERCONFBOUND $(k,n,1-\\alpha)$ returns a one-sided $(1{-}\\alpha)$ lower confidence   \ninterval for the Binomial parameter $p$ given that $k\\sim$ Binomial $(n,p)$ .   \n1: function PREDICT $(f_{\\mathrm{clf}},\\mathbf{\\boldsymbol{x}},\\sigma,n,\\alpha)$   \n2: counts $\\gets0$   \n3: for $i\\in\\{1,2,\\dots,n\\}$ do   \n4: y \u2190CONSISTENCYPURIFICATION $(f_{\\mathrm{clf}},x,\\sigma)$   \n5: counts[y] $\\leftarrow$ counts[y] + 1   \n6: end for   \n7: $\\hat{y}_{A},\\hat{y}_{B}\\gets$ top two labels in counts   \n8: $n_{A}$ $_4,n_{B}\\gets\\mathrm{counts}[\\hat{y}_{A}],\\mathrm{counts}[\\hat{y}_{B}]$   \n9: if BINOMTEST $\\begin{array}{r}{\\cdot(n_{A},n_{A}+n_{B},\\frac{1}{2})\\leq\\alpha}\\end{array}$ then   \n10: return $\\hat{y}_{A}$   \n11: else   \n12: return Abstain   \n13: end if   \n14: end function   \n15:   \n16: function CERTIFY $(f_{\\mathrm{clf}},\\mathbf{\\boldsymbol{x}},\\sigma,n,N,\\alpha)$   \n17: counts $0\\leftarrow0$   \n18: for $i\\in\\{1,2,\\dots,n\\}$ do   \n19: $\\begin{array}{r l}&{\\imath\\gets\\boldsymbol{\\mathrm{1}}^{\\iota},\\boldsymbol{\\mathrm{2}},\\cdot\\cdot\\cdot,\\imath\\iota_{\\boldsymbol{\\mathrm{J}}}\\mathbf{\\delta}_{\\boldsymbol{\\mathrm{u}}\\boldsymbol{\\mathrm{0}}}}\\\\ &{\\imath\\gets\\boldsymbol{\\mathrm{CONSISTENCYPURIFICATION}}(f_{\\mathrm{clf}},\\pmb{x},\\sigma)}\\\\ &{\\boldsymbol{\\mathrm{counts0}}[\\boldsymbol{\\mathrm{y}}]\\gets\\boldsymbol{\\mathrm{counts0}}[\\boldsymbol{\\mathrm{y}}]+1}\\end{array}$   \n20:   \n21: end for   \n22: $\\hat{y}_{A}\\leftarrow$ top label in counts0   \n23: counts $\\gets0$   \n24: fo $\\begin{array}{r l}&{\\mathrm{\\Lambda}^{\\mathrm{{mus}}}\\gets\\cup}\\\\ &{\\mathrm{\\Lambda}^{i}\\in\\{1,2,\\dots,N\\}\\;{\\mathbf d o}}\\\\ &{\\mathrm{\\Lambda}^{y}\\gets\\mathrm{CoNSISTENCYPURIFICATION}(f_{\\mathrm{clf}},{\\mathbf x},\\sigma)}\\\\ &{\\mathrm{\\Lambda}\\mathrm{counts}[{\\mathrm{y}}]\\gets\\mathrm{counts}[{\\mathrm{y}}]+1}\\end{array}$   \n25:   \n26:   \n27: end for   \n28: $\\underline{{p_{A}}}\\gets\\mathrm{LowERCONFBOUND}\\big(\\mathrm{counts}[\\hat{y}_{A}],N,1-\\alpha\\big)$   \n29: if $\\begin{array}{r}{\\underline{{p_{A}}}>\\frac{1}{2}}\\end{array}$ then   \n30: return prediction $\\hat{y}_{A}$ and radius $\\sigma\\Phi^{-1}(\\underline{{p_{A}}})$   \n31: else   \n32: return Abstain   \n33: end if   \n34: end function ", "page_idx": 14}, {"type": "text", "text": "C Training Unconditional Consistency Model for ImageNet-64 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We train an unconditional consistency model for ImageNet-64 from the public available conditional version by transiting the class embedding layers to a learnable token, initialization with average class embeddings. For each model forwarding, this token will be combined with the time embeddings for computation. After that, we train the conditional consistency model, initialized with the unconditional model\u2019s parameters, on ImageNet-64 training set for 120k steps. ", "page_idx": 14}, {"type": "text", "text": "D Purified Images Visualization for ImageNet-64 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present visualization images after diffusion purification by applying onestep-DDPM and Consistency Purification for ImageNet-64 under the noise level $\\sigma=0.25$ in Figure 5. ", "page_idx": 15}, {"type": "image", "img_path": "tLWoxftJVh/tmp/00f6a195c57dd0d8ccd7e04b66289984a12d7f85bdb803c54c590eec7c2008f2.jpg", "img_caption": ["(a) Purified images by onestep-DDPM "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "tLWoxftJVh/tmp/731566eace347fac193044caf781fb58b4d8f5c2e438458fe83ee51bb8e0601d.jpg", "img_caption": ["(b) Purified images by Consistency Purification "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 5: Visualization of purified images after the diffusion purification by applying onestep-DDPM and Consistency Purification on ImageNet-64 with $\\sigma=0.25$ noise level. Identical noise patterns are applied to images at corresponding locations. A green border indicates that the purified image is correctly classified, while a red border denotes misclassification by the classifier. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We have included our paper\u2019s contributions and scope in the abstract and introduction. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have discussed the limitaitons of our work after Conclusion Section. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide detailed assumptions in Section 3 and the proof of theorem in Appendix B.   \nGuidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. ", "page_idx": 16}, {"type": "text", "text": "\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All experiments in our paper can be reproducible with all disclosed information shown in our experiment section.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We have submitted our code as the Supplementary Material Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not ", "page_idx": 17}, {"type": "text", "text": "including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We have include the detailed experiment settings in Section 5.1 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [No] Justification: Error bars are not reported because it would be too computationally expensive for the large language model fine-tuning process.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] Justification: We include the details about the experiments compute resources in Section 5. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have checked the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: There is no societal impact of our work performed ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA]   \nJustification: Our paper does not have such risks.   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We have included licenses of original owners in our code and made citations in our papers.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?   \nAnswer: [NA]   \nJustification: Our paper does not release new assets.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "L Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]