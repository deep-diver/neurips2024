[{"type": "text", "text": "Right this way: Can VLMs Guide Us to See More to Answer Questions? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Li Liu\u2217 Diji Yang\u2217 Sijia Zhong Kalyana Suma Sree Tholeti Lei Ding Yi Zhang Leilani H. Gilpin \u2020 University of California, Santa Cruz {lliu112,dyang39,szhong16,ktholeti,lding25,yiz,lgilpin}@ucsc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating \u201cwhere to know\u201d scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans. Our dataset and code are available at: https://github.com/LeoLee7/Directional_guidance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, Vision-Language Models (VLMs) have made significant strides in general multimodal tasks such as visual recognition and Visual Question Answering (VQA) [1, 49]. This progress has opened up a vast potential for various applications, including enhancing visual accessibility for visually impaired individuals [4, 20], supporting decision-making in autonomous systems [50, 32], enabling interactive technologies [39], etc. Despite these advances, VLMs still fall short of human capabilities. Humans can intuitively assess whether the available information is sufficient to answer a question and seek additional details when necessary [36, 11]. In contrast, VLMs typically tend to provide direct, single-response outputs even when information is insufficient to answer the question accurately. This limitation reduces their effectiveness in real-world applications [13]. To address this issue, recent studies have explored ways to teach VLMs to assess information sufficiency [42]. These studies aim to have VLMs either provide concrete answers or label questions as unanswerable, using benchmark datasets from real user questions like VizWiz [20]. ", "page_idx": 0}, {"type": "text", "text": "However, a significant gap remains in handling unanswerable cases: deciding what actions to take when VLMs identify a question to be unanswerable. Humans naturally possesses the ability to seek additional details when faced with unanswerable questions \u2014 a challenge often encountered in real-world VQA tasks due to poor image quality, ambiguous questions, or loss of context [3, 9]. To the best of our knowledge, no existing benchmarks focused on \u201cwhat to do\u201d after the model identifies information insufficiency. This active process of information acquisition, fundamental to human cognition, has not been replicated in VLMs and remains largely unexplored. ", "page_idx": 0}, {"type": "image", "img_path": "7ANmKBfP88/tmp/cc7c9c2cba18b7e23d046345e9901118ca7a679f8cceb705c3c160d045246af7.jpg", "img_caption": ["Figure 1: The examples of the Directional Guidance task. The model utilizes self-knowledge to distinguish between known and unknown information and provides guidance on where to find more information. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To narrow the gap between VLMs and human intelligence, we suggest going beyond improving accuracy on answer generation or merely deciding on information sufficiency. Instead, we focus on enhancing the model\u2019s capability to provide constructive feedback when encountering unanswerable questions. In response to this challenge, we introduce a novel VQA task aimed at providing Directional Guidance, which aligns with real-world needs, particularly for visually impaired individuals. As indicated in previous studies [9], a common issue is that many images taken by visually impaired users are ill-framed. Our task aims to guide users on how to reframe their images during the interactive VQA process. This task evaluates the model\u2019s ability to understand visual direction and determine a potential direction to obtain more relevant information. ", "page_idx": 1}, {"type": "text", "text": "Moreover, to empower VLM with such guiding capability, we propose an automatic VQA data augmentation framework. This framework begins by prompting a pretrained VLM to filter a set of answerable questions from the given VQA dataset. The corresponding images are then perturbed using predefined rules that crop relevant visual information, making it more challenging for the model to answer the questions correctly. Finally, the VLM is fine-tuned using this augmented dataset, with the task of providing Directional Guidance on resolving the predefined perturbations. This approach simulates information inadequacy scenarios and holds promising potential for enhancing the model\u2019s ability to guide users in acquiring relevant information. ", "page_idx": 1}, {"type": "text", "text": "To validate the effectiveness of the approach, we contribute a manually labeled test set containing the Directional Guidance for real-world unanswerable datasets with images taken by visually impaired individuals. Our experiments on three popular open-source VLMs show significant improvements in the models\u2019 performance on the Directional Guidance task after fine-tuning with our synthetic training data. Notably, the best-performing model outperforms GPT-4o (CoT) [31] by $3\\%$ accuracy score. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this study are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Directional Guidance task: We define a novel VQA task. As shown in Figure 1, the proposed task assesses the model\u2019s ability to identify the information sufficiency and provide Directional Guidance when needed.   \n\u2022 Directional Guidance dataset: We create a human-labeled test set to benchmark the guidance-providing capability of VLMs.   \n\u2022 Directional Guidance framework: We propose a data-efficient framework for training models on the Directional Guidance task. This framework includes synthetic training data generation and model fine-tuning, which can be generalized with any VQA dataset with grounding information. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Directional Visual Understanding. Many studies have identified that current VLMs struggle to interpret and understand spatial relationships within an input image, especially on fundamental visual concepts like relative directions [15, 40]. This ability is important for interactive VQA applications like autonomous agents [18, 17], visual navigation, and assistive technologies designed for visually impaired individuals [24, 13]. To enhance VLMs\u2019 capability to understand directional relationships, researchers construct extensive training data [44], add assistive visual prompt [47, 30], or include collaborative VLMs to communicate and ensemble their decisions [8]. However, these methods often require heavy data collection or introduce additional models. Previous studies have investigated visual learning through simulation [18], but they rely on virtual interactive environments that may not accurately reflect real-world scenarios. Another trend involves generating training data by asking questions about directional relationships in existing images [29, 27], but this also requires additional involvement of advanced models. In our study, we aim to improve the model\u2019s directional understanding with simple data augmentation methods, using images collected from real users. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Assistive technology for visually impaired individuals. Over the past decade, applications like VizWiz [4] and Be My Eyes [13] have used real-time video connections to enhance visual accessibility. VLMs present a more accessible and responsive solution to satisfy the user\u2019s needs, as they can provide immediate responses when given a photo-query pair. However, as noted in [9], visually impaired users often face challenges in capturing clear images. In real-world conditions, many images suffer from quality issues such as blurriness, obstructions, and improper exposure, making them difficult to recognize. These issues often result in divergence in human annotations [9, 3]. Moreover, even when the images are clear, the questions may still be difficult to answer due to the off-framing of the target objects [9]. Addressing these challenges typically requires multiple rounds of queries and adjustments to properly frame the key object. For VLMs, these difficulties may be amplified because their training data typically lack examples of unrecognizable images. Additionally, to align with the multiple adjustment interaction offered by human operators in Be My Eyes applications, VLMs need to offer honest and effective guidance to navigate to target objects. ", "page_idx": 2}, {"type": "text", "text": "Self-knowledge. Self-knowledge refers to the model\u2019s ability to recognize what is known and unknown [22]. When confronted with unanswerable questions due to ambiguity or insufficient information, VLMs/LLMs often generate hallucinated responses [26, 46]. Previous research has introduced methods to help LLMs understand limitations regarding unknowns [48, 2, 34, 41]. Subsequent studies, such as [12], have explored explaining unanswerability by constructing known and unknown datasets through data augmentation and refining base models with a self-curation method. For VLMs, [25] presents a robust visual instruction tuning dataset that includes negative instructions at different semantic levels, i.e. nonexistent object manipulation, existent object manipulation, and knowledge manipulation, all implemented by GPT-4. Although these studies validate the beneftis of data augmentation, they have focused on generating negative or unknown data primarily within the language modality. Instead, our study extends this exploration into multimodal data by incorporating the vision modality. ", "page_idx": 2}, {"type": "text", "text": "3 The Cognitive Question: From What\u2019s Unknown and Where to Know ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To understand how a statistical model conceptualizes the world, one effective approach is to draw an analogy to human cognition. In the meta task in NLP (i.e., Question-Answering as other core NLP tasks can be transformed into QA), human cognitive processes in problem-solving and learning are multifaceted, involving not just the retrieval of stored information but also the recognition of one\u2019s knowledge boundaries and the strategic acquisition of new knowledge. To simulate these processes, we propose a hierarchical cognitive process pattern comprising three levels: ", "page_idx": 2}, {"type": "text", "text": "1. Response Generation (knowing what\u2019s known): At the foundational level, the model utilizes its existing knowledge base and basic analysis capabilities to generate responses to queries. This process mirrors the human cognitive function of retrieving known information from memory, akin to recall or recognition tasks in cognitive psychology[28, 35]. It reflects the model\u2019s ability to combine available information into coherent answers. ", "page_idx": 2}, {"type": "text", "text": "2. Awareness of Knowledge Limits (knowing what\u2019s unknown): The second level reflects the model\u2019s metacognitive ability to evaluate its own knowledge state, recognizing when it lacks sufficient information to answer a question accurately [14, 36, 37]. This awareness is crucial for intellectual honesty and mirrors the human cognitive process of monitoring and evaluating one\u2019s understanding and capabilities, a key aspect of metacognition [51, 38, 11]. ", "page_idx": 2}, {"type": "text", "text": "3. Knowledge Acquisition Direction (knowing where to know the unknown): At the most advanced level, the model identifies pathways for acquiring new knowledge when existing information is insufficient. This ability to seek out and engage in learning opportunities mirrors the human cognitive strategies for addressing knowledge gaps, such as identifying resources, formulating questions, or modifying learning strategies. It signifies the model\u2019s capacity for self-guided learning and adaptation, similar to strategic learning and problemsolving in human cognition. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As mentioned in Section 1, most existing works on VLMs cognitive questions are focused on the first two levels [42, 15, 5], and the third level is mostly under-explored. We argue that the challenges lie in the difficulty of collecting suitable data for benchmark and training data: there are few VQA samples that exhibit both awareness of knowledge limits and knowledge acquisition direction. Therefore, in our study, we focus on benchmark dataset curation and training data generation. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Directional Guidance Task ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We define our Directional Guidance task as follows: in the context of VQA, given an image-question pair $<I,Q>$ , the model $M$ should determine whether the image needs to be reframed. To be specific, if the target object is only partially visible and not sufficient to answer the question, the model should give clear guidance for the reframing direction (left, right, up, or down). Otherwise, the model should inform whether the question is already answerable (no need to change) or remains unanswerable even with potential reframing (none of the other options). This task mirrors real-world scenarios where visually impaired individuals need guidance to position their cameras correctly through many attempts. Although the target object might be only partially visible on each attempt, with continuous adjustments under guidance, the user can always capture a better view and finally have a better chance to get the question answered. This task goes beyond simply detecting the ill-framing issue of the image: it assesses whether the framing issue impacts the model\u2019s ability to answer the specific question posed. For example, reframing may not be necessary if the question can be directly answered with the available visual information even if the image is ill-framed. We regard these guide responses as an additional output that complements the original VQA answering process. ", "page_idx": 3}, {"type": "text", "text": "This task exemplifies three levels of the hierarchical cognitive pattern discussed in Section 3. Instead of a binary classification of answerable/unanswerable as proposed in [9], this task emphasizes the model\u2019s ability to effectively utilize available visual information. It requires the model to assess what is known and determine where to acquire extra information, standing in the transition from unanswerable to answerable. ", "page_idx": 3}, {"type": "text", "text": "4.2 Directional Guidance Dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Benchmark dataset. To evaluate model performance in our task setting, we created a benchmark dataset derived from VizWiz dataset families [20, 7]. The VizWiz dataset consists of real VQA queries collected from visually impaired individuals [20]. From this dataset, we used all the unanswerable samples (1.4k) from the validation set as the training set may have potential leakage issues during the pre-training process of VLMs. We invited 26 human annotators to identify ill-framed photos and label the most promising direction to move the camera, by which the reframing action could potentially help to answer the question (more details are available in Appendix A.1). After cleaning and re-evaluation, we collected 291 samples where reframing could potentially lead to an answer, and 230 samples unlikely to be answered even with reframing. The rest samples are where the human annotators have disagreements. The details of the data collection are presented in the Supplementary Materials. To ensure a balanced distribution in the test set, we randomly selected 300 samples from the VizWiz-grounding test set and simulated the case where the current image already has sufficient information to answer the question, under the assumption that the visual evidence could be theoretically grounded in the image. Combining those three groups, we get a high-quality Directional Guidance benchmark dataset including 821 samples. Despite the size of the dataset being relatively small, this reflects the inherent challenge of the task, where ill-framed images are rare in standard VQA datasets but commonly seen in real-world scenarios. Furthermore, our dataset\u2019s diversity and comprehensiveness make it suitable for evaluating model performance on the target task, providing a valuable foundation for future studies. ", "page_idx": 3}, {"type": "image", "img_path": "7ANmKBfP88/tmp/b9313dbdd9f1b521c592006fddba7f00bf17a81629a585eb6e04745a0075410d.jpg", "img_caption": ["Figure 2: The training set generation framework. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Training dataset. We propose a data augmentation process to simulate the ill-framed samples, instead of collecting ad-hoc images that suit the task. Initially, we take all the training samples from a dataset pool - the validation set of VizWiz-grounding dataset [7] as it includes the visual groundings for each answerable VQA query. With that visual grounding information, manual perturbations have been applied to simulate ill-framing. Specifically, we identify the bounding box surrounding the target object and divide it into 10 zones, horizontally and vertically. We then choose a specific zone for cropping, resulting in an image that has some missing information while retaining a part of the target object. With a series of perturbations, we observe the consistency of the model\u2019s response to the initial VQA query and capture the cases where an ill-framing issue impacts the question-answering. As the VizWiz is an open-ended task, we use precision as the evaluation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathrm{Precision}}\\,=\\frac{\\sum_{w}|P(w)\\cap T(w)|}{\\sum_{w}|P(w)|}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$P(w)$ and $T(w)$ denote a word from the model prediction and from the ground-true answer. Precision calculates how many words in the predictions also appear in the ground-truth answer, and we set a threshold $e$ to identify the correctness. Following [19], only non-stop words have been taken into consideration. Figure 2 and algorithm 1 outline the process of generating training data with guidance labels. ", "page_idx": 4}, {"type": "text", "text": "Another crucial case in the benchmark test set involves samples that remain unanswered even after adjusting the camera. One more data argumentation technique has been placed: we mismatch the questions and images from the same dataset pool to create new pairs with different semantic information. Most questions in the original dataset pool are generic, as a highly frequent question is \u201cWhat is this?\u201d without semantic information. Correspondingly, our GPT-4 enabled argumentation helped rephrase the paired question and answer. For example, given an image $I_{i}$ with the question $Q_{i}$ \u201cWhat is this?\u201d and an answer $A_{i}$ \u201claptop,\u201d the new question $Q_{i}^{\\prime}$ will be rephrased to \u201cWhat\u2019s the color of this laptop?.\u201d Then, we mismatch the $Q_{i}^{\\prime}$ with another irrelevant image $I_{j}$ to form a new pair. This augmentation generates complex, real-world queries where straightforward answers are infeasible, compelling models to learn deeper semantic information. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Generate synthetic training set with data augmentation ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "7ANmKBfP88/tmp/25aa307b11d6970181d7ed8ab75ec6f75ebfa2e4f9911ff4d816cfdb17f00957.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.3 Experiment settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Model Selection. To verify the feasibility and effectiveness of our approach for different model architectures and sizes, we analyze the experiments of four mainstream open-source large models with different sizes, including: LLaVA-1.5 [27], InstructBlip [10], GPT-4o [31], and CLIP [33]. First, we benchmark the test set on the LLaVA-1.5, InstructBlip, and GPT-4o on the zero-shot setting. A series of prompts has been designed to test their zero-shot performances, serving as our baselines. Next, we generate a training dataset using algorithm 1 and apply LoRA [21] fine-tuning on the open-sourced models. We anticipate the effectiveness of our proposed training framework will be reflected by the improvement of model performance compared with the zero-shot baseline. ", "page_idx": 5}, {"type": "text", "text": "Task format. To quantitatively analyze the model\u2019s ability to provide guidance, we format the task with a basic VQA multiple choice template: \u201c<image>{Original_Question} To improve the image and answer the question, how should the camera be moved? A.Leave it unchanged. B.Left. C.Right. D.Up. E.Down. F.None of the other options.\u201d Each option reflects the model\u2019s decision of Directional Guidance: The leave it unchanged option indicates that the current image contains all the necessary information to answer the question. The four directional options suggest that the relevant object is only partially visible, and further image adjustment is needed. The None of the other options implies that moving the camera will not help because the question is inherently unanswerable, i.e. due to the ambiguity, or the relevant object is absent from the current image. We use the F1 score and accuracy as the evaluation metrics and also analyze the confusion matrix of the different options. ", "page_idx": 5}, {"type": "text", "text": "Zero-shot prompt setting. For the zero-shot baseline, we enhance the basic template with additional instructions and explanations tailored for each model. We designed two prompt settings to accommodate their varying capabilities. The first setting is a single-round query where the model makes predictions from six options directly. The second setting is a two-round prompt, following the Chain-of-Thought [43] process. This two-round prompt decomposes the tasks and works as follows: Initially, we prompt the model to determine if the target object is fully present in the image, partially visible, or if the question is unanswerable. The corresponding options are: leave it unchanged, reframe, and none of the other options. If the model indicates that the target object is only partially visible, we then ask it to decide a specific direction for movement: left, right, up, or down. To ensure reproducibility, we include all prompts we used in Supplementary Materials A.5. ", "page_idx": 5}, {"type": "text", "text": "Fine-tune setting. In our training framework, we utilize data augmentation to generate potential samples with guidance labels. We assess the consistency of the model\u2019s predictions before and after perturbations and categorize the samples into two groups. Samples where the model fails to predict post-perturbation are considered positive, and their Directional Guidance labels are assigned one of four directions: left, right, up, or down. Conversely, samples where the model maintains correct predictions are labeled as negative, with the Directional Guidance label set to leave it unchanged. Upon analyzing these groups, we observed that negative samples predominated the generated training set. To ensure a balanced distribution within the training dataset, we under-sampled the negative samples to align with the average count of the four directional categories. We also adjusted the number of None of the other options samples to achieve an even distribution across the entire training set. ", "page_idx": 5}, {"type": "text", "text": "After generating the training set, we format the new pairs into a standardized instruction fine-tuning layout: each sample, comprising $<\\ I^{\\prime}$ , question $>$ , is supplemented with option choices and instructions. Since the task requires models to respond with a single letter, the prediction process is equivalent to a classification task. Following the settings in [27], the loss is only computed on the token for the chosen letter and the $<\\,e o s\\,>$ (end of the sentence) token. Also, to prevent the model from memorizing the letter distribution, we randomly shuffle the association between letters and options, ensuring each letter (from A to F) is paired with an option randomly in each training sample. When fine-tuning each model, most training configurations follow the officially suggested settings, and more training details are presented in Supplementary Materials A.2. ", "page_idx": 5}, {"type": "text", "text": "None-generative Models. Since the task has been simplified to a classification problem, we also investigate whether a non-generative model with a simpler architecture could suffice. Accordingly, we add a linear probe layer onto CLIP and perform a classification head, using a vision encoder (CLIPViT-L-336px) aligned with LLaVA-1.5 and a text encoder in the default setting. We concatenate the image feature and the text feature as the input for the classification head. Since the CLIP model can not generate open-ended answers, we use the synthetic training dataset generated by LLaVA-1.5 13b. ", "page_idx": 5}, {"type": "image", "img_path": "7ANmKBfP88/tmp/f12dc30c4f75341185e28ec644a32ac91c5866093888850653dfecd591349053.jpg", "img_caption": ["Figure 3: The distribution of four directions in our benchmark dataset (a) and examples (b-e). The upper caption is the Directional Guidance label and the lower caption is the original question. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "7ANmKBfP88/tmp/a9489f0f293985389c42216d2dd39f01cfb1db857b6866d1cfda18d0100eaad5.jpg", "img_caption": ["Figure 4: The heatmaps of the model\u2019s prediction. (a1)-(a4) shows the baseline performance under zero-shot setting, and (b1)-(b4) shows the performances of fine-tuned models. \u2018O\u2019 denotes the class leave it unchanged, and $\\mathbf{\\nabla}\\cdot\\mathbf{X}^{\\bullet}$ denotes the class none of the other options. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Directional Guidance benchmark dataset and baseline performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Fig. 3 (a) shows the distribution of four directions in our benchmark dataset. The horizontal directions are the most common, with left at $38.5\\%$ and right at $29.6\\%$ . The figure displays four typical samples from each direction. We also identified a frequent scenario where users need to take another photo and attempt a different question, as shown in Fig. 3 (e). This pattern reveals a common challenge for visually impaired individuals: without continuous guidance, the user and assistant can easily lose the context of the original VQA. These findings emphasize the importance of providing clear, sequential dialogue-based guidance for effectively adjusting the camera position. ", "page_idx": 6}, {"type": "text", "text": "As mentioned in Section 4.3, we use different prompt settings for each group of models that suit their capabilities. For the 7b models, we use the two-round prompt because these models benefit from a more structured, step-by-step approach, which helps them handle the task more effectively. In contrast, we tested the LLaVA-1.5 13b and GPT-4o model with a single-round of prompting to see if they were capable of this task. The prediction results are presented in Fig. 4, from (a1) - (a4). We observe that three open-sourced models (LLaVA-1.5 7b/13b, and Instructblip 7b) show similar behaviors: these models tend to avoid predicting the reframing cases and mistakenly categorize them as either leave it unchanged or none of the other options. With the two-round prompt, LLaVA- $1.5\\,7\\mathtt{b}$ and InstructBlip 7b make some correct predictions in the left and right categories. However, the correct and incorrect predictions are nearly balanced, with frequent misclassifications in the opposite direction. For example, the number of true left predictions is equivalent to the number of erroneous left predictions that were intended to be right. For GPT-4o, there are fewer errors in categorizing reframing cases into the wrong categories, and more cases within the reframing category are correctly predicted. However, contradictory predictions also occur frequently within the reframing cases. The result demonstrates that all the models are generally incapable of accurately predicting the reframing cases under zero-shot prompt settings. However, every baseline model performs well in the none of the other options category. We present the examples from each model in the Supplementary material A.4 to visualize the model\u2019s pretrained capability on the Directional Guidance Task. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Model\u2019s performance after fine-tuning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We present the heat maps of the fine-tuned model predictions in Fig. 4. By comparing the prediction results between the zero-shot baselines and the fine-tuned models, we observe significant and consistent improvements in prediction performance, demonstrating the effectiveness and generalizability of our proposed method. Although there is considerable potential to improve the overall accuracy, the fine-tuned models reduce confusion between reframing, leaving it unchanged(O), and none of the other options(X). The fine-tuned models are more likely to provide directional guidance on the reframing cases. Moreover, the predictions within reframing cases show noticeable improvement, as indicated by the clear diagonal line in the heat map. Another interesting finding is the substantial reduction in wrong predictions with opposite directions (e.g., predicting an up case as down). This clarity is meaningful, as it lowers the chance of users receiving conflicting guidance, thereby enhancing safety and efficiency in real-world applications. Overall, the fine-tuned models reduce errors across all options, showing significant improvement in both cross-category and within reframing predictions. ", "page_idx": 7}, {"type": "text", "text": "To evaluate our training framework\u2019s sensitivity to different settings, we conducted groups of comprehensive experiments. We used three metrics to quantitatively assess the model\u2019s performance: overall F1 score, overall Accuracy, and Accuracy on the reframing cases denoted as ACC(F). The metrics for the baseline models are presented in Table 1. In some baseline experiments, we found that the zero-shot setting did not always ensure a standard output format. In such cases, we performed post-processing and excluded samples with predictions that did not fall within our options. The total number of excluded samples was fewer than 10, and this only occurred in the zero-shot baseline models. The results, as shown in Table 1, include a combination of different settings from two aspects: varying perturbation ranges and the impact of shuffling letters and options in the training data. Regarding the choice to shuffle, we observed that randomly mixing letters and options does not consistently enhance performance. For instance, with a perturbation range of 0.1-0.9, the unshuffled approach often outperformed the shuffled version, while with a perturbation range of 0.3-0.7, shuffilng generally resulted in inferior performance. ", "page_idx": 7}, {"type": "text", "text": "The CLIP with linear probing method achieves comparable performance with the zero-shot performance of InstructBlip, but it\u2019s still not able to provide informative guidance (the accuracy for random choice for a six classification task is $16.6\\%)$ ). This suggests that simple CLIP-based encoders, lacking integration with a language model, may not be sufficient for this task. While the task largely relies on the model\u2019s perception of salient features, the contextual information within questions is also essential. For instance, the VizWiz dataset includes many generic questions such as \u2018What is the color of this?\u2019 Many of these questions are answerable even though the photos are heavily ill-framed. Since these questions do not concern spatial details, the appropriate guidance is to leave it unchanged. This underlines a key distinction between our task and other image quality detection tasks, especially those focusing on ill-framing solely on image modality. ", "page_idx": 7}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we analyze the effect of different settings, including perturbation range and shuffling operations, on the generation of training data. A detailed heatmap of the model\u2019s predictions is presented in Supplementary Materials with Figure 6. The perturbation range determines the crop ratios used to generate the training samples, ranging from 0.1 (minimal crop) to 0.9 (maximum crop). We observed that positive Guidance samples tend to cluster at high crop ratios, while lower ratios often correspond to negative samples (where the Guidance is leave it unchanged). Therefore, a range of 0.3-0.7 leads to a more balanced selection of training data, while a range of 0.1-0.9 provides a more comprehensive and varied dataset. This setting can affect the model\u2019s performance due to the balance between the diversity and complexity of the generated training samples, and the trade-off works as follows: when the perturbation becomes more severe (e.g., at a ratio of 0.9), images are aggressively corrupted. This increases the chance of obtaining positive Guidance samples, as the model is more likely to fail in predicting these heavily perturbed samples, which it could have predicted accurately without perturbation. However, it also results in significant information loss and greater challenges in detecting objects, making it a harder sample to learn. Conversely, a moderate perturbation ratio results in less aggressive cropping, allowing the model to access more information and better respond to the original question. However, this can lead to fewer positive Guidance samples, as the perturbation does not sufficiently challenge the predictions. The differences with the shuffling settings could be attributed to the regularization effect, which prevents the model from memorizing fixed patterns and increases training difficulty, especially when training data is scarce. In scenarios with less training data, shuffling acts as a form of data augmentation, increasing the diversity of training examples and making the model more robust. However, shuffilng might add unnecessary complexity to a larger training dataset derived from a wider perturbation range, making it harder for the model to learn effectively. In such cases, the unshuffled approach allows the model to quickly identify and leverage consistent patterns, facilitating faster and more efficient learning processes. ", "page_idx": 7}, {"type": "table", "img_path": "7ANmKBfP88/tmp/8e80aceacf1315f11b4daca3837d655c6958549e30edc4e4382c7ece1eb94426.jpg", "table_caption": ["Table 1: Model\u2019s performance with different settings. F1 and ACC denote the F1 score and accuracy score, respectively. ACC(F) refers to the accuracy of reframing directions, excluding the categories \u2018Leave it unchanged\u2019 and \u2018None of the other options.\u2019N/A indicates not applicable experiments due to limitations on the model\u2019s accessibility or incompatibility with the experiment design. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Ablation Study To gain a more comprehensive understanding of how different perturbation ranges affect model performance, we conducted a more fine-grained ablation study with LLaVA1.5-7b. Specifically, we evaluated perturbation ranges of 0.1-0.3, 0.3-0.5, 0.5-0.7, and 0.7-0.9, alongside our main experiments of 0.1-0.9 and 0.3-0.7. The results, presented in Table 2, reveal that as the perturbation range increases from 0.1-0.3 to 0.5-0.7, both overall F1 scores and overall accuracy show substantial improvements, stabilizing around 0.49. However, at the highest perturbation range of 0.7-0.9, we observe a slight decrease in the ACC(F) metric, suggesting that overly aggressive perturbations may introduce excessive complexity, hindering the model\u2019s ability to accurately identify relevant objects. Notably, the broader range of 0.1-0.9 achieves the highest overall F1 and accuracy scores, suggesting that a wide perturbation range strikes an effective balance between data diversity and sample complexity. Additionally, all perturbation ranges demonstrate improvements in ACC(F), with enhanced reframing direction performance as perturbation increases, except at the highest range. These findings support our initial discussion by emphasizing the trade-off between data diversity and the complexity of perturbed samples. Future work could explore optimized strategies for selecting perturbation ranges, potentially employing dynamic or adaptive methods to further improve model performance based on specific dataset characteristics. ", "page_idx": 8}, {"type": "table", "img_path": "7ANmKBfP88/tmp/00d41c16cb0dd18186c960c11854fc4430d3b5b7eea1812b5b25eb42fa577eed.jpg", "table_caption": ["Table 2: Model performance under different perturbation ranges with LLaVA-1.5 7b. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Limitation and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we focused specifically on guiding image reframing directions as a proof of concept. Although reframing is one of the most common needs when assisting visually impaired individuals, some other aspects that impact the VQA process could also be explored, such as orientation, exposure, and focus. Our data augmentation framework can be extended to these aspects and generate training data in a similar way, and we plan to explore this in future work. Second, the directional guidance has been simplified to a classification task on directions, which may not fully capture the complexity of real-world scenarios. For example, effective reframing might require combining multiple directions\u2014such as moving both up and left\u2014or even zooming out. A more informative guidance in practice would also consider additional parameters like the magnitude of the reframing action. Those complexities can confuse the model, leading to inaccurate evaluations. To enhance clarity and reduce ambiguity in our benchmark dataset, we included only cases that received consistent annotations from multiple annotators, which resulted in a limited size in our benchmark test set. Additionally, our preliminary experiments are designed to validate the effectiveness of our proposed framework rather than to maximize the model\u2019s performance. Consequently, the current method cannot fully guarantee the reliability of the model\u2019s prediction, and it still requires cautious deployment in highrisk scenarios. Moving forward, we aim to refine the task design and data generation framework, adapting more effectively to complex, real-world applications. In addition, theoretically, our guidance framework can be extended to more general and quantitative scenarios. By simulating spatial drift, we can customize the ratio of drift and produce synthetic training data with quantitative values. This potential extension would allow models to identify not just the direction but also the extent of camera movement required. This makes such quantitative guidance particularly meaningful in applications such as robotics, where precise tracking of target objects is crucial, for example, in calculating the ground truth for the extent of movement needed [16]. Furthermore, we expect our unsupervised data generation framework to alleviate the pressing data needs of studies exploring LLM or VLM for spatial or temporal reasoning tasks [6, 45, 23]. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced a novel task and benchmark dataset within the context of Visual Question Answering (VQA) aimed at improving the self-knowledge of Vision-Language Models (VLMs). Our task specifically evaluates how well VLMs can assess the sufficiency of visual information and determine the necessary actions to reframe an image to obtain additional information. To address the challenge of limited training data, we proposed an automated framework that generates synthetic data by simulating unanswerable scenarios through perturbations applied to answerable cases. Our results show that current high-performing VLMs, including LLaVA and GPT-4o, struggle with this task, revealing a gap in their ability to handle incomplete or ambiguous visual inputs. However, when fine-tuned with the synthetic training data generated by our framework, the models significantly outperformed the zero-shot baseline on real-world data. ", "page_idx": 9}, {"type": "text", "text": "This study highlights the importance of self-knowledge in VLMs, particularly their ability to recognize the boundaries of available information and take appropriate actions when confronted with incomplete or misleading data. By mimicking human cognitive processes, our approach presents a promising solution for enhancing models\u2019 self-knowledge and robustness, especially in real-world applications that require accurate and adaptive responses. This is particularly relevant for assistive technologies, such as those designed for visually impaired individuals, where providing timely and effective guidance is essential. As VLMs continue to evolve, the ability to recognize their knowledge boundaries and make informed decisions will be critical for their successful deployment in dynamic environments. Future work can explore additional strategies for refining this self-knowledge, potentially leading to more robust models capable of learning in complex, uncertain scenarios. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We sincerely appreciate all collaborators who contributed to this project for their invaluable insights and support. Special thanks to our human labelers for their diligent efforts in creating the benchmark dataset. We are also grateful to Prof. David Lee and Prof. James Davis for their expert guidance, constructive feedback, and encouragement throughout this research. ", "page_idx": 10}, {"type": "text", "text": "We thank the VizWiz community for providing an inspiring platform that significantly influenced our work. We acknowledge the use of the VizWiz VQA and VizWiz Grounding datasets(both under CC-BY 4.0)). Our model development was based on LLaVA (Apache-2.0 license) and InstructBLIP (in compliance with the license terms of LLaMA and Vicuna). ", "page_idx": 10}, {"type": "text", "text": "This material is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-24-1-0149. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV), 2015. [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2023. [3] Nilavra Bhattacharya, Qing Li, and Danna Gurari. Why does a visual question have different answers? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4271\u20134280, 2019. [4] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333\u2013342, 2010. [5] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Jiaqi Wang. Dualfocus: Integrating macro and micro perspectives in multi-modal large language models. arXiv preprint arXiv:2402.14767, 2024. [6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14455\u201314465, 2024. [7] Chongyan Chen, Samreen Anjum, and Danna Gurari. Grounding answers for visual questions asked by visually impaired people. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19098\u201319107, June 2022. [8] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Large language models are visual reasoning coordinators. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Tai-Yin Chiu, Yinan Zhao, and Danna Gurari. Assessing image quality issues for real-world problems. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3646\u20133656, 2020.   \n[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] Andreas Demetriou and Smaragda Kazi. Self-awareness in g (with processing efficiency and reasoning). Intelligence, 34(3):297\u2013317, 2006. ", "page_idx": 10}, {"type": "text", "text": "[12] Yang Deng, Yong Zhao, Moxin Li, See-Kiong Ng, and Tat-Seng Chua. Gotcha! don\u2019t trick me with unanswerable questions! self-aligning large language models for responding to unknown questions. arXiv preprint arXiv:2402.15062, 2024. ", "page_idx": 11}, {"type": "text", "text": "[13] Be My Eyes. Introducing be my eyes virtual volunteer, 2023. Accessed: 2024-05-21.   \n[14] John H Flavell. Metacognition and cognitive monitoring: A new area of cognitive\u2013 developmental inquiry. American psychologist, 34(10):906, 1979.   \n[15] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024.   \n[16] Yunhao Ge, Yihe Tang, Jiashu Xu, Cem Gokmen, Chengshu Li, Wensi Ai, Benjamin Jose Martinez, Arman Aydin, Mona Anvari, Ayush K Chakravarthy, et al. Behavior vision suite: Customizable dataset generation via simulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22401\u201322412, 2024.   \n[17] Akshay Gopalkrishnan, Ross Greer, and Mohan Trivedi. Multi-frame, lightweight & efficient vision-language models for question answering in autonomous driving. arXiv preprint arXiv:2403.19838, 2024.   \n[18] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4089\u20134098, 2018.   \n[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.   \n[20] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608\u20133617, 2018.   \n[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[22] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.   \n[23] Dohwan Ko, Ji Soo Lee, Wooyoung Kang, Byungseok Roh, and Hyunwoo J Kim. Large language models are temporal and causal reasoners for video question answering. arXiv preprint arXiv:2310.15747, 2023.   \n[24] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635\u2013651, 2023.   \n[25] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023.   \n[26] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024.   \n[27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[28] George Mandler. Recognizing: The judgment of previous occurrence. Psychological Review, 87:252\u2013271, 1980. ", "page_idx": 11}, {"type": "text", "text": "[29] Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and Laurens Van Der Maaten. Learning by asking questions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11\u201320, 2018. ", "page_idx": 12}, {"type": "text", "text": "[30] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872, 2024.   \n[31] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[32] SungYeon Park, MinJae Lee, JiHyuk Kang, Hahyeon Choi, Yoonah Park, Juhwan Cho, Adam Lee, and DongKyu Kim. Vlaad: Vision and language assistant for autonomous driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 980\u2013987, 2024.   \n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789, 2018.   \n[35] Henry L. Roediger and Andrew C. Butler. The critical role of retrieval practice in long-term retention. Trends in Cognitive Sciences, 15:20\u201327, 2011.   \n[36] Gregory Schraw and David Moshman. Metacognitive theories. Educational psychology review, 7:351\u2013371, 1995.   \n[37] Nick Shannon and Bruno Frischherz. Metathinking: The Art and Practice of Transformational Thinking. Management for Professionals. Springer International Publishing, Cham, Switzerland, 1st edition, 2020.   \n[38] Zhen Tan, Jie Peng, Tianlong Chen, and Huan Liu. Tuning-free accountable intervention for llm deployment\u2013a metacognitive approach. arXiv preprint arXiv:2403.05636, 2024.   \n[39] Gemini Team. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[40] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[41] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval augmentation for large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10303\u201310315, Singapore, December 2023. Association for Computational Linguistics.   \n[42] Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, and Yanfeng Wang. Mmsap: A comprehensive benchmark for assessing self-awareness of multimodal large language models in perception. arXiv preprint arXiv:2401.07529, 2024.   \n[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[44] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, et al. Q-instruct: Improving low-level visual abilities for multi-modality foundation models. arXiv preprint arXiv:2311.06783, 2023.   \n[45] Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. Large language models can learn temporal reasoning. arXiv preprint arXiv:2401.06853, 2024.   \n[46] Diji Yang, Kezhen Chen, Jinmeng Rao, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. Tackling vision language tasks through learning inner monologues. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19350\u201319358, 2024.   \n[47] Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visual prompting. Advances in Neural Information Processing Systems, 36, 2024.   \n[48] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuan-Jing Huang. Do large language models know what they don\u2019t know? In Findings of the Association for Computational Linguistics: ACL 2023, pages 8653\u20138665, 2023.   \n[49] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and Yang: Balancing and answering binary visual questions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.   \n[50] Xingcheng Zhou, Mingyu Liu, Bare Luka Zagar, Ekim Yurtsever, and Alois C Knoll. Vision language models in autonomous driving and intelligent transportation systems. arXiv preprint arXiv:2310.14414, 2023.   \n[51] Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrievalaugmented large language models. arXiv preprint arXiv:2402.11626, 2024. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix / supplementary material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Benchmark Dataset Collection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We invited 26 human annotators to label the 1.4k unanswerable samples extracted from the validation set of VizWiz VQA dataset[20]. We divided the dataset evenly, assigning each annotator a segment to work on. Each sample has an image and corresponding question from the original dataset. We include several sub-tasks for each VQA question to categorize how the images could be improved to better answer the question: annotators were required to choose options from the following categories\u2014 Reframing, Other Actions, or No Way to Answer. In the Reframing category, annotators specified the direction \u2014 left, right, up, down\u2014for a reframing action that might reveal an answer. The \u2018Other Actions\u2019 includes the actions that are beyond the previous four actions, such as zoom in/out, rotation, and adjusting exposure. The \u2018No Way to Answer\u2019 category is used to indicate cases where there is unlikely to yield additional information by taking any actions. We also asked the annotators to summarize their selected options with one sentence, which can be open-ended and qualitative comments. This is used for sanity checks and helps our further cleaning. After the initial round of annotations, we engaged four additional annotators (validators) to review the labels, noting any errors or disagreements. These validators conducted two rounds of evaluation and discussion before finalizing the benchmark dataset. After the validation, we select the Reframing and \u2018No Way to Answer\u2019 categories to join our benchmark dataset. The instructions given to annotators are: ", "page_idx": 14}, {"type": "text", "text": "Task Overview ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Thank you for joining this task! In Visual-Question-Answering (VQA), some image-question pairs are marked \"unanswerable\" due to insufficient information in the image. Our goal is to determine if specific camera adjustments or other actions could potentially make these questions answerable. ", "page_idx": 14}, {"type": "text", "text": "For each image-question pair, your task is to identify if any adjustments or guidance could potentially make the question answerable. Please select the most applicable option from the drop-down cells and provide a brief explanation in the summary column. Below is a description of each option. ", "page_idx": 14}, {"type": "text", "text": "Reframing: Choose \"Left, Right, Up\", or \"Down\" if moving the camera in a specific direction could reveal information to answer the question. Choose \"Leave it unchanged\" if the current image already contains all the information needed to answer the question. ", "page_idx": 14}, {"type": "text", "text": "Other Actions: Select \"Zoom in/out, Rotate\", or \"Adjust exposure\" if these camera settings might improve visibility or clarify the image content. ", "page_idx": 14}, {"type": "text", "text": "No Way to Answer: Choose this option if no adjustment would help, such as cases with lost context, ambiguity, or obstructions. ", "page_idx": 14}, {"type": "text", "text": "Summary Column: In the summary column, provide a one-sentence explanation of your choice to clarify the reasoning behind your selection. ", "page_idx": 14}, {"type": "image", "img_path": "7ANmKBfP88/tmp/2904bceaf83b93a51244f788f50c45d702cdc877602a46568dc0d2a5f63f9a6e.jpg", "img_caption": ["Figure 5: A screenshot of the annotation work. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Training details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We run experiments on an Ubuntu 22.04.4 LTS system equipped with 4\\*NVIDIA RTX A6000 GPU and AMD Ryzen 24-Cores CPU. ", "page_idx": 15}, {"type": "text", "text": "The training details for LLaVA models follow the default settings in [27], with a training epoch of 3. The training details for InstructBlip-7b are shown as follows: ", "page_idx": 15}, {"type": "table", "img_path": "7ANmKBfP88/tmp/746effa096a6c44dfb23fc5084a981f19932e72f17ecfe09f87d7bf73067b830.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "7ANmKBfP88/tmp/435ca39d2eeaa2be5c19515d3b5118a560a76fc5c62d5c445299e22bdcbae8f5.jpg", "img_caption": ["A.3 The model\u2019s performance under different settings ", "Figure 6: The heatmap of model\u2019s predictions under different settings. The $p$ denotes the perturbation ranges we applied when generating synthetic data. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "7ANmKBfP88/tmp/d1864f958b26cb9216504e6e291c46a447d07585b39afade0b7d71c3514f9b10.jpg", "img_caption": ["A.4 Examples of VLMs performances with zero-shot settings ", "Figure 7: The examples of the VLMs\u2019 zero-shot performance on our Directional Guidance benchmark dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "7ANmKBfP88/tmp/8347f2025152381b588a1836f28681b3419fbfe705809df6f871de7606798d6d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "7ANmKBfP88/tmp/7e785b991852b28a942ee62e7596322622bd9a6d6f37f14d3e3eb1851576928c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "7ANmKBfP88/tmp/72972e448c863510fa715737b151c022322dc45be244ec6b4681335d74820ce8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 10: The examples of the VLMs\u2019 zero-shot performance on our Directional Guidance benchmark dataset. ", "page_idx": 20}, {"type": "text", "text": "INSTRUCTBLIP-7B: TWO-ROUND PROMPT ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Round 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u201c\u2018Image question pair Image: <Image> Question: {QUESTION}\u201d\u2019 ", "page_idx": 21}, {"type": "text", "text": "You\u2019re an assistant who helps us adjust the view of the image to better answer the given question. Given the image and question pair, your options are as follows: ", "page_idx": 21}, {"type": "text", "text": "A: \u2018framing\u2019 - If the given image contains partial needed information and the camera can move in a certain direction to gather more information for better answering the given question. ", "page_idx": 21}, {"type": "text", "text": "B: \u2018leave it unchanged\u2019 - if the image contains enough information to answer the given question, or if the given question is already answered, choose this option. ", "page_idx": 21}, {"type": "text", "text": "C: \u2018none of the other options\u2019 - If the previous options don\u2019t fit, or the given question is not related to the given image, or the given question cannot be answered, choose this option. ", "page_idx": 21}, {"type": "text", "text": "- If the given question is unrelated to the given image, select C: \u2019none of the other options\u2019. ", "page_idx": 21}, {"type": "text", "text": "- If there is no way to use the given image for answering the given question, select C: \u2019none of the other options\u2019. ", "page_idx": 21}, {"type": "text", "text": "Please only output the selected option for the given image and question pair. ", "page_idx": 21}, {"type": "text", "text": "A short answer to the question is: ", "page_idx": 21}, {"type": "text", "text": "Round 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u201c\u2018Image question pair Image: <Image> Question: {QUESTION}\u201d\u2019 ", "page_idx": 21}, {"type": "text", "text": "In the previous setting, the result is framing, which means that the image has part of the needed information for answering, and the camera can move to the corresponding direction to better answer the question. ", "page_idx": 21}, {"type": "text", "text": "You\u2019re an assistant who helps us move the camera in direction to better answer the given question. Given the image and question pair, your options are as follows: ", "page_idx": 21}, {"type": "text", "text": "D: \u2018left\u2019 - if the image is truncated on the left and the camera framing should be moved left to better answer the question based on the given image, choose this option. ", "page_idx": 21}, {"type": "text", "text": "E: \u2018right\u2019 - if the image is truncated on the right and the camera framing should be moved right to better answer the question based on the given image, choose this option. ", "page_idx": 21}, {"type": "text", "text": "F: \u2018up\u2019 - if the image is truncated on top and the camera framing should be moved up to better answer the question based on the given image, choose this option. ", "page_idx": 21}, {"type": "text", "text": "G: \u2018down\u2019 - if the image is truncated on the bottom and the camera framing should be moved down to better answer the question based on the given image, choose this option. ", "page_idx": 21}, {"type": "text", "text": "Please only output the selected option for direction. ", "page_idx": 21}, {"type": "text", "text": "A short answer to the question is: ", "page_idx": 21}, {"type": "text", "text": "\u2014\u2013 GPT4: SINGLE-ROUND PROMPT \u2014\u2014", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "You are an assistive technology specializing in visual question answering, i.e., the task of providing a natural language answer to a question about a given image. To better answer the question {QUESTION} based on the given image, please choose one of the six options (A. \u2018Leave it unchanged\u2019, B. \u2018Up\u2019, C. \u2018Left\u2019, D. \u2018Right\u2019, E. \u2018Down\u2019, F. \u2018None of the other options\u2019) on the camera framing. The definitions of each of the options are given below: ", "page_idx": 22}, {"type": "text", "text": "- A: \u2018Leave it unchanged\u2019 - The question can be answered based on the given image without the need for changing camera framing and there is visible text or complete object in the image to answer the question. The image is clear and shows the object in question without any truncation or need for reframing. The entire object is visible and identifiable. ", "page_idx": 22}, {"type": "text", "text": "- B: \u2018None of the other options\u2019 - The question cannot be answered based on the given image, even with a change in camera framing, or the question seems to be unrelated to the content of the image provided, or the question is incomplete or the question is unrelated to the image or there is no visible text on the image to answer the question. ", "page_idx": 22}, {"type": "text", "text": "If the answer to the question, i.e., visible text or object is partially visible in the image or the specific text content is not clear due to the angle and quality of the image or the piece of text is partially obscured, and the necessary details to answer the question are not discernible, please choose one of the below camera framing accordingly. ", "page_idx": 22}, {"type": "text", "text": "- C: \u2018Up\u2019 - The camera framing should be moved up to better answer the question based on the given image.   \n- D: \u2018Left\u2019 - The camera framing should be moved left to better answer the question based on the given image.   \n- E: \u2018Right\u2019 - The camera framing should be moved right to better answer the question based on the given image.   \n- F: \u2018Down\u2019 - The camera framing should be moved down to better answer the question based on the given image. ", "page_idx": 22}, {"type": "text", "text": "Output format required is: ", "page_idx": 22}, {"type": "text", "text": "- Option and its value. ", "page_idx": 22}, {"type": "text", "text": "GPT4: TWO-ROUND PROMPT ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Round 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "You are an assistive technology specializing in visual question answering, i.e., the task of providing a natural language answer to a question about a given image. To better answer the question {QUESTION} based on the given image, please choose one of the given options (A. \u2018Leave it unchanged\u2019, B. \u2018None of the other options\u2019, C. \u2018Move camera\u2019) on the camera framing. The definitions of each of the options are given below: ", "page_idx": 22}, {"type": "text", "text": "- A: \u2018Leave it unchanged\u2019 - The question can be answered based on the given image without the need for changing camera framing and there is visible text or complete object in the image to answer the question. The image is clear and shows the object in question without any truncation or need for reframing. The entire object is visible and identifiable. ", "page_idx": 22}, {"type": "text", "text": "- B: \u2018None of the other options\u2019 - The question cannot be answered based on the given image, even with a change in camera framing, or the question seems to be unrelated to the content of the image provided, or the question is incomplete or the question is unrelated to the image or there is no visible text on the image to answer the question. ", "page_idx": 22}, {"type": "text", "text": "- C: \u2018Move camera\u2019 - If the answer to the question i.e., visible text or object is partially visible in the image or the specific text content is not clear due to the angle and quality of the image or the piece of text is partially obscured, and the necessary details to answer the question are not discernible. ", "page_idx": 22}, {"type": "text", "text": "Output format required is: ", "page_idx": 22}, {"type": "text", "text": "- Option and its value,   \n- Reason for choosing that option,   \n- If option A \u2018Leave it unchanged\u2019 is selected, give answer to question based on the given image. ", "page_idx": 22}, {"type": "text", "text": "Round 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "You are an assistive technology specializing in visual question answering, i.e., the task of providing a natural language answer to a question about a given image. To better answer the question {QUESTION} based on the given image or previous context {RESULT}, please choose one of the given four options (A. \u2018Up\u2019, B. \u2018Left\u2019, C. \u2018Down\u2019, D. \u2018Right\u2019) on the camera framing. The definitions of each of the options are given below: ", "page_idx": 22}, {"type": "text", "text": "If the answer to the question i.e., visible text or object is partially visible in the image or the specific text content is not clear due to the angle and quality of the image or the piece of text is partially obscured, and the necessary details to answer the question are not discernible, please choose one of the below camera framing accordingly. ", "page_idx": 22}, {"type": "text", "text": "- A: \u2018Up\u2019 - The camera framing should be moved up to better answer the question based on the given image.   \n- B: \u2018Left\u2019 - The camera framing should be moved left to better answer the question based on the given image.   \n- C: \u2018Down\u2019 - The camera framing should be moved down to better answer the question based on the given image.   \n- D: \u2018Right\u2019 - The camera framing should be moved right to better answer the question based on the given image. ", "page_idx": 22}, {"type": "text", "text": "Output format required is: ", "page_idx": 22}, {"type": "text", "text": "- Option and its value, - Reason for choosing that option. ", "page_idx": 22}, {"type": "text", "text": "LLAVA-1.5-13b: SINGLE-ROUND PROMPT \u2014", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "USER: <image> ", "page_idx": 23}, {"type": "text", "text": "{QUESTION} ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To improve the image and answer the above question, how should the framing should be changed in order to answer the question? Please select the most suitable ways to adjust the framing based on the following description: ", "page_idx": 23}, {"type": "text", "text": "Select \u2018none of the other options\u2019 if there is no correct way to change the framing for answering the above question, which includes: the question is not related to the image or the question has grammar error. If the Question is a sentence instead of a real question, select \u2018none of the other options\u2019. ", "page_idx": 23}, {"type": "text", "text": "Select \u2018leave it unchanged\u2019 if the above question can be answered without adjusting the framing and the question has no related with the image, otherwise choose \u2018none of the other options\u2019. ", "page_idx": 23}, {"type": "text", "text": "If the question is neither \u2018none of the other options\u2019 or \u2018leave it unchanged\u2019, and if the image has part of the needed information for answering and the camera can move to the corresponding direction to better answer the question, choose the framing option which you believe is the most suitable in order to answer the question above. ", "page_idx": 23}, {"type": "text", "text": "Options:   \nleft   \nup   \ndown   \nright ", "page_idx": 23}, {"type": "text", "text": "Please double check to make sure that you are sure about your answer and make sure that you give same priorities to each option, and give me the option only. ", "page_idx": 23}, {"type": "text", "text": "ASSISTANT: ", "page_idx": 23}, {"type": "text", "text": "LLaVA-1.5-7b TWO-ROUND PROMPT\u2014Round 1", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "USER: <image> ", "page_idx": 23}, {"type": "text", "text": "{QUESTION} ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In order to improve the image to answer the given question, please choose which category this question-image pair belongs to and the priorities of each option are the same: \u2018none of the other options\u2019 or \u2018framing\u2019 or \u2018leave it unchanged\u2019. The definitions are given below: ", "page_idx": 23}, {"type": "text", "text": "Select \u2018none of the other options\u2019 if there is no correct direction to move the camera to answer the above question, which includes: the question is unrelated to the image, or the question has a grammar error. ", "page_idx": 23}, {"type": "text", "text": "Select \u2018framing\u2019 if the image has part of the needed information for answering and the camera can move to the corresponding direction to better answer the question. ", "page_idx": 23}, {"type": "text", "text": "Select \u2018leave it unchanged\u2019 if the image contains enough information for answering the above question without moving the camera to better answer the question.   \n- If the question is unrelated to the image, select \u2018none of the other options\u2019.   \n- If there is no answering the question, select \u2018none of the other options\u2019. ", "page_idx": 23}, {"type": "text", "text": "Please answer the selected category above and double check the answer. Make sure that you choose \u2018none of the other options\u2019 and \u2018framing\u2019 and \u2018leave it unchanged\u2019 based on their definitions and the priority of these options are the same. ", "page_idx": 23}, {"type": "text", "text": "ASSISTANT: ", "page_idx": 23}, {"type": "text", "text": "Output format required is: ", "page_idx": 23}, {"type": "text", "text": "- Option and its value,   \n- Reason for choosing that option,   \n- If option A \u2018Leave it unchanged\u2019 is selected, give answer to question based on the given image. ", "page_idx": 23}, {"type": "text", "text": "Round 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "USER: <image> ", "page_idx": 23}, {"type": "text", "text": "{QUESTION} ", "page_idx": 23}, {"type": "text", "text": "In the previous setting, the result is \u2018framing\u2019 which means that the image has part of the needed information for answering, and the camera can move to the corresponding direction to better answer the question. Please choose the most suitable one of the four options for moving camera for better answering the question by the directions: left, right, up, down. The definitions are given below: ", "page_idx": 23}, {"type": "text", "text": "left - if the image is truncated on left and the camera framing should be moved left to better answer the question based on the given image. ", "page_idx": 23}, {"type": "text", "text": "right - if the image is truncated on right and the camera framing should be moved right to better answer the question based on the given image. ", "page_idx": 23}, {"type": "text", "text": "up - if the image is truncated on top and the camera framing should be moved up to better answer the question based on the given image. ", "page_idx": 23}, {"type": "text", "text": "down - if the image is truncated on down and the camera framing should be moved down to better answer the question based on the given image. ", "page_idx": 23}, {"type": "text", "text": "Answer the selected direction only. ", "page_idx": 23}, {"type": "text", "text": "ASSISTANT: ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In the abstract and introduction of our paper, we outline three key claims: First, we identify a significant gap between how humans and VLMs handle situations where there isn\u2019t enough visual information to answer a question. Humans look for more information to help them answer, while VLMs often either make up an answer or simply say it is unanswerable. Second, we present a benchmark dataset that clearly shows the gap. Third, we demonstrate the effectiveness of our proposed novel framework that\u2019s designed to mitigate the gap. The framework effectiveness uses data augmentation to mimic the human cognitive process, thereby improving the VLMs\u2019 ability to handle insufficient information. Our experimental results and contributions align with these claims and also demonstrate the generalizability. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We analyzed the limitations in section 7. We presented the limitations of our task setting and the sensitivity of the proposed method. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 24}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We don\u2019t have theoretical results. We are presenting the empirical results. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We present the steps of collecting the benchmark dataset in Supplementary Materials A.1. We also illustrate the details of our proposed methodology with Algorithm 1. We present the models we used in Section 4.3-Model Selection, and the training details are in Section 4.3-Fine-tune setting and Supplementary Materials. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We released the code and full benchmark dataset through our project page (link). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The experimental settings are presented in Section 4.3 and Supplementary Material Section A.2. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We didn\u2019t include the statistical significance analysis. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We present the computer resources in Supplementary Materials Section A.2. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research presented in the paper conforms to the NeurIPS Code of Ethics in all respects ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we discussed the potential societal impacts of our work in the introduction part. We also analyzed the potential negative societal impacts in Section 7, pointing out the limitations regarding reliability and the risk of the model providing erroneous information. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We introduced the double verification process of collecting benchmark dataset in the Supplementary Materials (Section A.1). ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The dataset and code package we used are properly credited. We noted and followed their license and terms of use. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We released the new assets and documents through the project url. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We present the human annotation instructions and screenshot in Supplementary material A.1. In our annotation process, all participants received fair compensation based on the average market rate for similar studies. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our research is pending on an IRB exemption. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]