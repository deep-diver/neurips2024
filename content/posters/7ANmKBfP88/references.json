{"references": [{"fullname_first_author": "Stanislaw Antol", "paper_title": "VQA: Visual Question Answering", "publication_date": "2015-00-00", "reason": "This foundational paper introduced the Visual Question Answering (VQA) task, which is central to the current work."}, {"fullname_first_author": "Jeffrey P Bigham", "paper_title": "Vizwiz: nearly real-time answers to visual questions", "publication_date": "2010-00-00", "reason": "This paper introduced the VizWiz dataset, a key benchmark dataset used in evaluating VQA models' performance and addressing challenges with visually impaired individuals, which are relevant to the current work."}, {"fullname_first_author": "Danna Gurari", "paper_title": "Vizwiz grand challenge: Answering visual questions from blind people", "publication_date": "2018-00-00", "reason": "This paper presented the VizWiz Grand Challenge, a significant advancement in VQA that focuses on real-world applications and the challenges faced by visually impaired users, directly influencing the direction and scope of the current work."}, {"fullname_first_author": "Chongyan Chen", "paper_title": "Grounding answers for visual questions asked by visually impaired people", "publication_date": "2022-00-00", "reason": "This paper directly addresses the challenge of VQA with ill-framed images from visually impaired people and proposes to ground the answers to improve model accuracy; therefore, it is highly relevant to the current work."}, {"fullname_first_author": "Yash Goyal", "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering", "publication_date": "2017-00-00", "reason": "This paper highlighted the importance of image understanding in VQA and proposed methods to improve visual reasoning in models, directly influencing the current work's focus on model ability in image understanding"}]}