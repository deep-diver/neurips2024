[{"figure_path": "7ANmKBfP88/figures/figures_1_1.jpg", "caption": "Figure 1: The examples of the Directional Guidance task. The model utilizes self-knowledge to distinguish between known and unknown information and provides guidance on where to find more information.", "description": "This figure shows three examples of the Directional Guidance task.  The task challenges the model to determine if there is enough information in an image to answer a question, and if not, to provide guidance on how to improve the image by moving the camera. The examples highlight scenarios where the model successfully identifies information gaps and suggests appropriate camera adjustments (e.g., moving to the right), situations where the model correctly assesses sufficient information (no action needed), and cases where the model appropriately acknowledges that even with adjustments, the answer cannot be derived.", "section": "1 Introduction"}, {"figure_path": "7ANmKBfP88/figures/figures_4_1.jpg", "caption": "Figure 2: The training set generation framework.", "description": "The figure illustrates the process of generating the training dataset for the Directional Guidance task. It starts with an image and a question from the VizWiz-grounding dataset. The image is then perturbed by applying predefined rules that crop relevant visual information. This is done by identifying the bounding box surrounding the target object and dividing it into 10 zones, horizontally and vertically. The specific zone for cropping is chosen. This results in an image with missing information that is then fed to a pretrained VLM along with the original question. The VLM is prompted to provide the corresponding answer, and the consistency of the model's response is evaluated. The cases where ill-framing impacts the question-answering are captured. The synthetic training data is generated by these steps, paired with their corresponding guidance. The purpose is to simulate ill-framed scenarios and enhance the model's ability to provide directional guidance when faced with insufficient visual information.", "section": "4 Method"}, {"figure_path": "7ANmKBfP88/figures/figures_4_2.jpg", "caption": "Figure 2: The training set generation framework.", "description": "The figure illustrates the process of generating synthetic training data for the Directional Guidance task.  It starts with selecting answerable questions and their corresponding images from an existing VQA dataset.  These images are then perturbed by applying predefined cropping rules, simulating real-world scenarios with insufficient visual information. A pretrained VLM is used to assess the model's ability to answer the questions correctly before and after these perturbations. Based on the model's performance, corresponding guidance labels ('leave it unchanged', 'left', 'right', 'up', 'down') are generated, creating the training data. This process ensures that the synthetic data effectively reflects the challenge of identifying information insufficiency and providing directional guidance.", "section": "4 Method"}, {"figure_path": "7ANmKBfP88/figures/figures_6_1.jpg", "caption": "Figure 3: The distribution of four directions in our benchmark dataset (a) and examples (b-e). The upper caption is the Directional Guidance label and the lower caption is the original question.", "description": "This figure shows the distribution of the four directions (up, down, left, right) in the benchmark dataset created for evaluating the Directional Guidance task.  The pie chart (a) visually represents the percentage of each direction in the dataset. The images (b-e) provide example questions and the corresponding Directional Guidance labels. These examples showcase the varying scenarios in which directional guidance might be necessary to improve image quality for a visually impaired user.", "section": "5 Results"}, {"figure_path": "7ANmKBfP88/figures/figures_6_2.jpg", "caption": "Figure 4: The heatmaps of the model's prediction. (a1)-(a4) shows the baseline performance under zero-shot setting, and (b1)-(b4) shows the performances of fine-tuned models. 'O' denotes the class leave it unchanged, and 'X' denotes the class none of the other options.", "description": "This figure presents heatmaps visualizing the performance of four different vision-language models (LLaVA-1.5 7B, LLaVA-1.5 13B, InstructBlip 7B, and CLIP with linear probing) on a directional guidance task in a visual question answering setting.  The heatmaps compare the models' performance before (zero-shot, a1-a4) and after (fine-tuned, b1-b4) being fine-tuned using a synthetic training dataset. Each heatmap shows the confusion matrix for the models' predictions, illustrating the model's ability to accurately predict the necessary direction to adjust camera framing to better answer unanswerable questions. The 'O' in the heatmaps indicates the model correctly identified the question as answerable without reframing and 'X' represents cases where the model determined that even reframing would not resolve the issue. The differences in heatmap patterns between the zero-shot baseline and fine-tuned models highlight the effectiveness of the proposed fine-tuning strategy in improving directional guidance prediction.", "section": "5 Results"}, {"figure_path": "7ANmKBfP88/figures/figures_15_1.jpg", "caption": "Figure 5: A screenshot of the annotation work.", "description": "This figure shows a screenshot of the annotation interface used in the study. The interface presents an image and a question to the annotators, and it asks them to choose from several options to indicate if any adjustments or guidance could potentially make the question answerable. The options include reframing the image by moving the camera (left, right, up, or down), taking other actions (like zooming in/out, rotating, or adjusting exposure), or indicating that there is no way to answer the question.", "section": "A.1 Benchmark Dataset Collection"}, {"figure_path": "7ANmKBfP88/figures/figures_16_1.jpg", "caption": "Figure 4: The heatmaps of the model's prediction. (a1)-(a4) shows the baseline performance under zero-shot setting, and (b1)-(b4) shows the performances of fine-tuned models. 'O' denotes the class leave it unchanged, and 'X' denotes the class none of the other options.", "description": "This figure presents heatmaps illustrating the performance of four different vision-language models (LLaVA-1.5 7B, LLaVA-1.5 13B, InstructBlip 7B, and CLIP with linear probing) on a directional guidance task.  The heatmaps show the model's predictions for directional guidance (up, down, left, right, leave unchanged, or none of the other options) under two settings: zero-shot and fine-tuned. The zero-shot setting represents the model's performance without any fine-tuning, while the fine-tuned setting demonstrates the improvement after training with synthetic data generated by the proposed framework.  Each cell in the heatmap represents the number of times a particular prediction was made for a given actual label.  The darker the color, the higher the frequency of that prediction.", "section": "5 Results"}, {"figure_path": "7ANmKBfP88/figures/figures_17_1.jpg", "caption": "Figure 1: The examples of the Directional Guidance task. The model utilizes self-knowledge to distinguish between known and unknown information and provides guidance on where to find more information.", "description": "This figure shows three examples of the Directional Guidance task.  The task involves a Vision-Language Model (VLM) assessing whether it has enough information to answer a question. If not, it must provide guidance on how to adjust the image (e.g., move the camera) to get more information.  The examples illustrate different scenarios: a question about a room number that's partially visible; a question about an object where the image is not sufficient to provide an answer; and a question about the color of a shirt that is difficult to ascertain from the given image. The VLM provides a text response suggesting how to improve the image in order to give an answer or states that it cannot answer the question.", "section": "4 Method"}, {"figure_path": "7ANmKBfP88/figures/figures_18_1.jpg", "caption": "Figure 3: The distribution of four directions in our benchmark dataset (a) and examples (b-e). The upper caption is the Directional Guidance label and the lower caption is the original question.", "description": "This figure shows the distribution of directional guidance labels in the benchmark dataset, illustrating the frequency of each direction (left, right, up, down).  It also provides example images (b-e) representing each direction, with the original question posed to the visually impaired user and the corresponding Directional Guidance given by a human annotator. This demonstrates the real-world scenarios and the task challenge.", "section": "5 Results"}, {"figure_path": "7ANmKBfP88/figures/figures_19_1.jpg", "caption": "Figure 3: The distribution of four directions in our benchmark dataset (a) and examples (b-e). The upper caption is the Directional Guidance label and the lower caption is the original question.", "description": "This figure presents the distribution of directional guidance labels in the benchmark dataset, showing that 'left' and 'right' are the most common directions.  It also includes example image-question pairs to illustrate different scenarios and the corresponding Directional Guidance labels given by human annotators. The examples highlight the varying image qualities and the challenges associated with capturing clear and appropriately framed images for visually impaired individuals.", "section": "5 Results"}, {"figure_path": "7ANmKBfP88/figures/figures_20_1.jpg", "caption": "Figure 3: The distribution of four directions in our benchmark dataset (a) and examples (b-e). The upper caption is the Directional Guidance label and the lower caption is the original question.", "description": "This figure shows the distribution of the four directions (left, right, up, down) in the Directional Guidance benchmark dataset.  The pie chart (a) visualizes the proportion of each direction. The images (b-e) provide examples of real-world VQA queries from visually impaired individuals, showcasing scenarios where image reframing is necessary to provide accurate answers. Each image is accompanied by the original question and its corresponding Directional Guidance label, indicating the optimal direction for camera adjustment.", "section": "5 Results"}]