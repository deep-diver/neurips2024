[{"figure_path": "7Hb03vGcJk/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of the proposed Slot-VLM model with other state-of-the-art video question answering (VQA) models.  It shows the accuracy and average score on three benchmark datasets (MSVD-QA, MSRVTT-QA, ActivityNet-QA).  The table also indicates the instruction data used for training each model, the method used to connect vision features with the language model (LLM), and the number of video tokens used.  The results highlight Slot-VLM's superior performance.", "section": "4 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_7_1.jpg", "caption": "Table 2: Ablation studies on the effectiveness of our Slot-VLM. We compare our schemes powered by slot attention with the schemes powered by Q-Former under our framework. The FLOPs and number of parameters for the reduced token generation module are presented.", "description": "This table presents the ablation study comparing the effectiveness of Slot-VLM against other models using Q-Former.  It shows the accuracy and score on two datasets (In-domain and MSVD-QA) for different model configurations: using only the spatial branch, only the temporal branch, and both branches, with each branch using either slot attention or Q-Former.  The FLOPs (floating point operations) and the number of parameters are also provided for comparison.", "section": "4.4 Ablation Studies"}, {"figure_path": "7Hb03vGcJk/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM with other state-of-the-art video question answering (QA) models.  It shows the accuracy and average score achieved by each model on three benchmarks: MSVD-QA, MSRVTT-QA, and ActivityNet-QA.  The table also specifies the instruction data used for training each model, the method used to connect vision features and the LLM, and the number of video tokens used.", "section": "4 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_14_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM against other state-of-the-art video question answering (QA) models.  It highlights the different datasets and instruction tuning methods used by each model,  along with their performance scores (accuracy and average score) across three QA benchmarks (MSVD-QA, MSRVTT-QA, ActivityNet-QA).  The table emphasizes that Slot-VLM achieves state-of-the-art results despite using significantly less instruction data than most comparable models.", "section": "4 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_15_1.jpg", "caption": "Table 5: Ablation study on the influence of high spatial resolution for the Object-Slots branch and high frame rate for the Event-Slots branch. Object-Slot-VLM (4 \u00d7 4) denotes the spatial resolution is reduced from 16 \u00d7 16 to 4 \u00d7 4. Event-Slot-VLM (T/8) denotes the frame rate is reduced by a factor of 8.", "description": "This table presents the ablation study results on the impact of spatial resolution in the Object-Slots branch and frame rate in the Event-Slots branch of the proposed Slot-VLM model.  It shows the performance (accuracy and average score) on the In-domain and MSVD-QA datasets when reducing either the spatial resolution or temporal sampling rate.  This helps to understand the contribution of each branch to the overall model performance.", "section": "4.4 Ablation Studies"}, {"figure_path": "7Hb03vGcJk/tables/tables_15_2.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM with other state-of-the-art video question answering (QA) models.  It shows the accuracy and average scores on three benchmark datasets (MSVD-QA, MSRVTT-QA, ActivityNet-QA).  The table also indicates the LLM used (Vicuna-7B), the size and type of training data for each model, and the method used to connect vision features and the LLM.  The performance of Slot-VLM is highlighted, showing its competitive results against other methods.", "section": "4.3 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_15_3.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM against other state-of-the-art video question answering (QA) methods.  Key aspects of the comparison include the LLM used (all use Vicuna-7B), the amount of pre-training and instruction-tuning data, the method used to connect vision features to the LLM, and the resulting accuracy scores across three benchmark datasets (MSVD-QA, MSRVTT-QA, ActivityNet-QA).  The table highlights Slot-VLM's superior performance and efficiency.", "section": "4 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_16_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM with other state-of-the-art video question answering (QA) models.  It shows the accuracy and average score achieved by each model on three benchmarks: MSVD-QA, MSRVTT-QA, and ActivityNet-QA.  Key aspects compared include the instruction data used during training and the method used to connect vision features with the Language Model (LLM).", "section": "4 Experiments"}, {"figure_path": "7Hb03vGcJk/tables/tables_16_2.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM against other state-of-the-art video question answering (QA) models.  It shows the accuracy and average score achieved by each model on three benchmark datasets (MSVD-QA, MSRVTT-QA, ActivityNet-QA).  The table also indicates the amount of training data used (both video and image instruction pairs) and the method employed to connect visual features with the Language Language Model (LLM).", "section": "4 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_17_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM with other state-of-the-art video question answering (VQA) models.  It highlights the model's accuracy on three benchmark datasets (MSVD-QA, MSRVTT-QA, ActivityNet-QA), the type of LLM used (Vicuna-7B), the instruction data used for training, and the method used to connect vision features with the LLM.  The table indicates that Slot-VLM achieves state-of-the-art performance, even with less instruction data compared to other models.", "section": "4 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_17_2.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM against other state-of-the-art video question answering (QA) models.  It highlights the different datasets used for pre-training and instruction tuning, the method used to connect vision features with the large language model (LLM), and the resulting accuracy scores on three benchmark datasets (MSVD-QA, MSRVTT-QA, and ActivityNet-QA).  The table shows that Slot-VLM achieves competitive or superior performance compared to other models.", "section": "4 Comparison with the State-of-the-Arts"}, {"figure_path": "7Hb03vGcJk/tables/tables_21_1.jpg", "caption": "Table 1: Comparison with the state-of-the-art methods for video QA. All these models use Vicuna-7B as the LLM. Different methods may use different datasets for pre-training. Moreover, for the instruction tuning, different methods adopt different instruction data as illustrated in the second column. For example, 11K(V)+5.1M(I) denotes the instruction data comprises about 11,000 pairs of video instructions pairs and 5.1 million pairs of image instructions. Connector denotes the method for connecting the vision features and the LLM. See Table 4 for the number of video tokens.", "description": "This table compares the performance of Slot-VLM against other state-of-the-art video question answering (QA) models.  It highlights the different datasets and instruction data used for training each model, along with the method used to connect visual features to the large language model (LLM).  The table shows accuracy and average scores on three benchmark datasets: MSVD-QA, MSRVTT-QA, and ActivityNet-QA.", "section": "4 Comparison with the State-of-the-Arts"}]