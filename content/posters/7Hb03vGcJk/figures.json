[{"figure_path": "7Hb03vGcJk/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of methods for aligning visual features with LLM. Previous methods (a) and (b) leverage pooling or Q-Former to aggregate visual tokens, where each generated token contains coupled semantics. In contrast, we propose to generate semantically decoupled object-centric tokens as illustrated in (c), and event-centric tokens as illustrated in (d), to align with the LLM.", "description": "This figure illustrates different methods for aligning visual features with a large language model (LLM).  (a) and (b) show existing methods using pooling or a Q-Former, resulting in tokens with mixed semantics.  The proposed method (c and d) generates semantically separated object-centric and event-centric tokens, improving alignment with the LLM.", "section": "1 Introduction"}, {"figure_path": "7Hb03vGcJk/figures/figures_2_1.jpg", "caption": "Figure 2: Flowchart of our proposed Slot-VLM for video understanding. Slot-VLM consists of a frozen image encoder, a learnable Object-Event Slots module (i.e., OE-Slots module), a projection layer, and a frozen LLM. The image encoder encodes the input video of T frames into a sequence of image features, resulting in extensive (H \u00d7 W \u00d7 T) video tokens. In order to obtain semantically decoupled and compact (reduced) video tokens as the vision context for aligning with LLM, our OE-Slots module learns to aggregate those tokens to object-centric tokens and event-centric tokens through the Object-Slots branch and the Event-Slots branch, respectively. The Object-Slots branch operates at low frame rate (t \u226aT) but high spatial resolution in order to capture spatial objects through slot attention on each sampled frame. The Event-Slots branch operates at high frame rate but low spatial resolution (m = h \u00d7 w, where h < H, w < W) in order to capture temporal dynamics through slot attention over each spatial position. The learned slots (tokens) from two branches are projected and inputted to LLM for video reasoning, together with the text.", "description": "This figure illustrates the architecture of Slot-VLM, a framework for video-language modeling.  It shows how the input video is processed through an image encoder to extract video tokens.  These tokens are then fed into an Object-Event Slots module which has two branches: one for object-centric slots (high spatial resolution, low frame rate) and one for event-centric slots (high frame rate, low spatial resolution).  The resulting slots are then projected and combined with text input before being passed to a Large Language Model (LLM) for video reasoning.", "section": "3 Proposed Slot-VLM"}, {"figure_path": "7Hb03vGcJk/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of spatial attention masks from the Object-Slots branch for two video examples, respectively. We have t = 8 frames as shown in 8 rows, indexed by i, where i = 1, ..., t. The first column shows the original frame. The second to the ninth columns show the cross attention mask (from slot attention) for the N\u00b0 = 8 object-centric slots Oi = {0i,1,..., oi,No}. We can see that even though not perfectly segmented, some meaningful slots have been formed. For example, the slots marked by red, purple, green, and blue in the first video (left) correspond to 'background', 'human body', 'head', and 'barbell'. Note that the slots in a frame is unordered and exchangeable.", "description": "This figure visualizes the spatial attention masks from the Object-Slots branch of the Slot-VLM model.  It shows how the model attends to different regions of the input frames to generate object-centric slots. Each row represents a frame, and each column represents a different slot. The color intensity of each cell indicates the attention weight. The examples provided demonstrate that the model learns to focus on semantically meaningful regions, such as background, human body parts, and objects.", "section": "4.5 Visualization"}, {"figure_path": "7Hb03vGcJk/figures/figures_9_1.jpg", "caption": "Figure 4: Visualization of temporal attention mask for m = h \u00d7 w = 16 spatial positions from (a) our Event-Slots branch and (b) Temporal-QFormer-VLM, respectively. For simplicity, we also refer to slot as query here. For the k-th spatial position, we denote the set of learned temporal queries by Ek. Take the 13-th spatial position of the query set E13 as an example (as marked by red box in (a) and blue box in (b)). For this spatial position, the models generate Ne = 8 slots/queries by aggregating the temporal visual tokens. The attention masks for E13 are denoted by a map of T rows and Ne columns, with the visibility indicating which queries this temporal position belongs to. The higher the visibility, the greater the affinity between this temporal position and the query. We can see that in our Slot-VLM, similar contents tend to be allocated to the same slot, i.e., different slots capture different contents (events) and present decoupled semantics. In contrast, in Temporal-QFormer-VLM, different contents are usually assigned to the same query or are uniformly assigned to different queries. Note that for Temporal-QFormer-VLM, we only show the mask of one head to save space, where similar observations can be found from other heads. A glimpse of the original video can be found in Figure 5. See Figure 10 for the enlarged visualization of E13.", "description": "This figure visualizes the temporal attention masks from the Event-Slots branch of Slot-VLM and compares it with the Temporal-QFormer-VLM. It shows that Slot-VLM effectively groups similar temporal contents into the same slot, demonstrating disentangled semantics. In contrast, Temporal-QFormer-VLM shows less clear separation of semantics.", "section": "4.5 Visualization"}, {"figure_path": "7Hb03vGcJk/figures/figures_9_2.jpg", "caption": "Figure 5: A glimpse of the original video used in Figure 4. For visualization purpose, we only show the frames down-sampled at a factor of 8, which is 1/8fps.", "description": "This figure shows a glimpse of the original video used to generate Figure 4, which visualizes the temporal attention masks.  The frames shown here are down-sampled to 1/8th of the original frame rate for visualization purposes.", "section": "4.5 Visualization"}, {"figure_path": "7Hb03vGcJk/figures/figures_18_1.jpg", "caption": "Figure 6: Visualization of spatial attention masks from the Q-Former in BLIP2 for two images in (a) and (b) respectively. We show the learned query masks for the 12 heads in 12 rows, respectively. In each row, we show the masks for the 32 queries. Note that the first column show the original image repeated by 12 times. There is no obvious evidence that different queries have learned decoupled semantics.", "description": "This figure visualizes spatial attention masks from the Q-Former in BLIP2, a visual language model. It shows the attention weights of 12 heads across 32 queries for two different images. The goal is to analyze if the Q-Former learns decoupled semantics for these queries, meaning if individual queries focus on distinct semantic aspects of the image. The visualization reveals that there's no clear evidence of decoupled semantics, suggesting that queries capture mixed semantic information.", "section": "G.1 Visualization of Q-Former Attention Maps from BLIP2"}, {"figure_path": "7Hb03vGcJk/figures/figures_19_1.jpg", "caption": "Figure 3: Visualization of spatial attention masks from the Object-Slots branch for two video examples, respectively. We have t = 8 frames as shown in 8 rows, indexed by i, where i = 1, ..., t. The first column shows the original frame. The second to the ninth columns show the cross attention mask (from slot attention) for the N\u00b0 = 8 object-centric slots Oi = {0i,1,\u00b7\u00b7\u00b7, 0i,No}. We can see that even though not perfectly segmented, some meaningful slots have been formed. For example, the slots marked by red, purple, green, and blue in the first video (left) correspond to 'background', 'human body', 'head', and 'barbell'. Note that the slots in a frame is unordered and exchangeable.", "description": "This figure visualizes spatial attention masks from the Object-Slots branch of the Slot-VLM model for two video examples. Each row represents one of the 8 frames used, and each column represents one of the 8 object-centric slots. The visualization shows how the model attends to different parts of the image to form these slots, demonstrating the capacity of the model to extract semantically meaningful features from the input images.", "section": "4.5 Visualization"}, {"figure_path": "7Hb03vGcJk/figures/figures_19_2.jpg", "caption": "Figure 8: Visualization of spatial attention masks from (a) the stage 1 pre-training, and (b) the stage 2 after instruction tuning. We have t = 8 frames as shown in 8 rows, indexed by i, where i = 1, ..., t, respectively. The first column shows the original frame. The second to the ninth columns show the cross attention mask (from slot attention) for the N\u00b0 = 8 object-centric slots Oi = {0i,1,\u06f0\u06f0\u06f0, Oi,No}. Interestingly, we can see that after the instruction tuning, the learned slots are much more decoupled, where a spatial position usually contributes to multiple slots in stage 1 but only contributes to a very few slots in stage 2.", "description": "This figure visualizes spatial attention masks from the Object-Slots branch at two different training stages: pre-training and instruction tuning.  It shows how the attention mechanism changes after instruction tuning, resulting in more decoupled and semantically meaningful object-centric slot representations.", "section": "4.5 Visualization"}, {"figure_path": "7Hb03vGcJk/figures/figures_20_1.jpg", "caption": "Figure 9: Visualization of spatial attention masks from (a) the stage 1 pre-training, and (b) the stage 2 after instruction tuning. We have t = 8 frames as shown in 8 rows, indexed by i, where i = 1, ..., t, respectively. The first column shows the original frame. The second to the ninth columns show the cross attention mask (from slot attention) for the N\u00b0 = 8 object-centric slots Oi = {0i,1,..., Oi,No}. Interestingly, we can see that after the instruction tuning, the learned slots are much more decoupled, where a spatial position usually contributes to multiple slots in stage 1 but only contributes to a very few slots in stage 2.", "description": "This figure visualizes spatial attention masks from the Object-Slots branch, comparing stage 1 (pre-training) and stage 2 (after instruction tuning).  It shows how instruction tuning leads to more decoupled slot representations, where each spatial position is less likely to contribute to multiple slots.", "section": "4.5 Visualization"}]