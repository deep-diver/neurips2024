[{"type": "text", "text": "Efficiently Learning Significant Fourier Feature Pairs for Statistical Independence Testing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yixin Ren1, Yewei Xia1,4, Hao Zhang3,\u2217, Jihong Guan2, Shuigeng Zhou1,\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Shanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai, China 2Department of Computer Science and Technology, Tongji University, Shanghai, China 3SIAT, Chinese Academy of Sciences, Shenzhen, China 4Machine Learning Department, MBZUAI, Abu Dhabi, UAE {yxren21, ywxia23}@m.fudan.edu.cn, h.zhang10@siat.ac.cn jhguan@tongji.edu.cn, sgzhou@fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose a novel method to efficiently learn significant Fourier feature pairs for maximizing the power of Hilbert-Schmidt Independence Criterion (HSIC) based independence tests. We first reinterpret HSIC in the frequency domain, which reveals its limited discriminative power due to the inability to adapt to specific frequency-domain features under the current inflexible configuration. To remedy this shortcoming, we introduce a module of learnable Fourier features, thereby developing a new criterion. We then derive a finite sample estimate of the test power by modeling the behavior of the criterion, thus formulating an optimization objective for significant Fourier feature pairs learning. We show that this optimization objective can be computed in linear time (with respect to the sample size $n$ ), which ensures fast independence tests. We also prove the convergence property of the optimization objective and establish the consistency of the independence tests. Extensive empirical evaluation on both synthetic and real datasets validates our method\u2019s superiority in effectiveness and efficiency, particularly in handling high-dimensional data and dealing with large-scale scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Testing for independence is a crucial and challenging task in machine learning and statistics, with wide-range applications in causal inference [16, 31], feature selection [6] and deep learning [23, 42]. Its primary objective is to determine whether two random variables, $X$ and $Y$ are independent, based on the observations of the underlying joint distribution $\\mathbb{P}_{X Y}$ . While traditional independence tests, such as Pearson\u2019s correlation coefficient [9] and Kendall\u2019s $\\tau$ , can only detect monotonic relationships between low-dimensional variables, more modern tests [26, 43, 7, 25, 27, 35, 19, 20] aim to deal with complex non-linear interactions in much more challenging higher-dimensional space [45, 29]. ", "page_idx": 0}, {"type": "text", "text": "One class of nonlinear dependence measures [3, 15] aims to capture distributional characteristics using kernel embeddings [13], primarily derived from the cross-covariance operators in the reproducing kernel Hilbert space (RKHS). Among them, Hilbert-Schmidt Independence Criterion (HSIC) [14] is the most popular one. It utilizes the squared Hilbert-Schmidt norm to detect dependence and exhibits outstanding performance across various data contexts by choosing suitable kernels. On the other hand, some other fundamental nonlinear dependence measures employ characteristic functions to detect the smoothed discrepancy between the joint distribution and the product of marginals. By employing appropriate characteristic functions, the statistic [39, 40] computes the covariance between distances of variable pairs. It has been demonstrated that these distance-based methods are equivalent to HSIC with specific kernels [33]. However, all these measures suffer from the drawback of requiring quadratic time (w.r.t. the sample size $n$ ) to compute the feature covariance and necessitating fixed kernel or distance functions, rendering them impractical on large-scale datasets due to the unaffordable time cost and lacking flexibility in handling complex scenarios. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these challenges, a multitude of works grounded on these measures have emerged. Upon HSIC, [44] proposes some linear-time tests including a block-averaged statistic, a statistic with Nystr\u00f6m approximation, and one with finite-dimensional feature mappings using random Fourier features (RFF) [28]. For convenience, these tests are referred to as BHSIC, NyHSIC, and FHSIC, respectively. FHSIC and NyHSIC are observed to have a considerable advantage over BHSIC. However, a remaining drawback of these methods is that the features are not learnable. Therefore, these methods lack enough adaptability to complex settings, thus leading to performance degradation. ", "page_idx": 1}, {"type": "text", "text": "In addition to time efficiency, another research direction [1, 30] aims to make independence tests adaptive to better capturing distributional distinctions. These methods either select/combine appropriate kernels from a predefined set or learn parameterized kernels. Nonetheless, their criteria still inherit the quadratic time complexity of HSIC, thus cannot be readily applied to large-scale data. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, some approaches [17, 32] try to address both challenges simultaneously. For instance, HSICAgg [32] suggests combining several kernels from a predefined set (e.g. kernels with different preset bandwidths) and aggregating the test results for improving performance. Additionally, an incomplete $U$ -statistic of HSIC is proposed to ensure computational efficiency. Nevertheless, selecting from a predefined set of kernels imposes limitations on flexibility, and in cases where scaling optimization is required on each dimension, the number of kernel pairs escalates exponentially. Also, NFSIC [18] proposes to combine a time-efficient technique called analytic kernel embeddings [8, 17] and learn the important local distributional features. However, its learning objective is merely a lower bound of test power and demands a substantial number of samples to ensure accuracy. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel test method that flexibly learns distributional features while maintaining high efficiency. We first reinterpret HSIC from a frequency-domain perspective, then we point out its potential shortcomings with an elaborate example and indicate corresponding improvement directions. Finally an central optimization objective is derived by directly modeling test power, which can be computed in linear time while maximizing the test performance. Comparing with [30] that also addresses the kernel learning problem in independence testing with a time/space complexity of ${\\mathcal{O}}(n^{2})$ , our criteria for learning are designed to have a complexity of ${\\mathcal{O}}(n)$ for both space and time. Consequently, the whole test framework can efficiently handle large-scale data. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In summary, the contributions of the work are as follows: 1) We propose a novel approach that efficiently learns significant Fourier feature pairs for maximizing the power of HSICbased independence tests. 2) We design an optimization objective that can be computed in linear time, which is derived by directly modeling test power. 3) We theoretically establish the non-asymptotic convergence property of the optimization objective and demonstrate the consistency of our method. 4) We conduct extensive experiments on both synthetic and real data, showcasing its superiority in effectiveness and efficiency in handling high-dimensional data (e.g. image data) and addressing large-scale scenarios. ", "page_idx": 1}, {"type": "text", "text": "Outline. The rest of the paper is organized as follows: Sec. 2 reviews HSIC-based statistical independence tests. Sec. 3 reinterprets HSIC from a frequency-domain perspective, and explain its potential shortcomings with an elaborate example and indicate corresponding improvement directions. Sec. 4 designs an optimization objective by directly modeling test power, which can be computed in linear time. Sec. 5 presents the theoretical analysis and Sec. 6 evaluates the performance of the proposed method on synthetic and real dataset. We conclude the paper in Sec. 7. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We begin by introducing notions and reviewing the hypothesis testing framework for independence tests. Let $\\mathcal X\\times\\mathcal X$ be separable metric space, typically $\\dot{\\mathbb R}^{d_{x}}\\times\\mathbb R^{d_{y}}$ . $\\mathbb{P}_{X Y}$ denotes a Borel probability measure defined on $\\mathcal X\\times\\mathcal X$ , while $\\mathbb{P}_{X}$ and $\\mathbb{P}_{Y}$ denote the respective marginal distributions. Given $n$ independent and identically distributed (i.i.d) samples $Z:=(X,Y)=\\{(x_{i},y_{i})\\}_{i=1}^{n}$ with distribution $\\mathbb{P}_{X Y}$ , we aim to test whether $X,Y$ are independent (i.e., $X\\perp\\!\\!\\!\\perp Y)$ . This corresponds to a hypothesis testing problem formulated as $\\mathcal{H}_{0}:\\mathbb{P}_{X Y}=\\mathbb{P}_{X}\\mathbb{P}_{Y}$ versus $\\mathcal{H}_{1}:\\mathbb{P}_{X Y}\\neq\\mathbb{P}_{X}\\mathbb{P}_{Y}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The testing procedure is as follows: First, define the statistic $\\rho$ and calculate its estimated value using the samples. Then, choose a significance level $\\alpha$ (typically set to 0.05), which represents the probability that the sampling of $\\rho$ under $\\mathcal{H}_{0}$ is at least as extreme as the observed value. Finally, the null hypothesis $\\mathcal{H}_{0}$ is rejected if the $p$ -value is not greater than $\\alpha$ . ", "page_idx": 2}, {"type": "text", "text": "Two types of errors may occur in this procedure. Type I error occurs when $\\mathcal{H}_{0}$ is falsely rejected, while Type II error happens when $\\mathcal{H}_{0}$ is incorrect but not rejected. A good test [43] needs to control Type I error within $\\alpha$ while maximizing the testing power (1\u2212Type II error rate). ", "page_idx": 2}, {"type": "text", "text": "For independence tests, a commonly used statistic is HSIC, defined as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 1. [14]. Let $\\mathcal{F}$ be an RKHS with kernel $k:\\mathcal{X}\\times\\mathcal{X}\\mapsto\\mathbb{R}$ and $\\mathcal{G}$ be a second RKHS on $\\boldsymbol{\\wp}$ with kernel $l:\\mathcal{Y}\\times\\mathcal{Y}\\mapsto\\mathbb{R}$ , the HSIC between $X$ and $Y$ , denoted as $H S I C(X,Y)$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{E}\\big[k(X,X^{\\prime})l(Y,Y^{\\prime})\\big]\\!+\\!\\mathbf{E}\\big[k(X,X^{\\prime})\\big]\\mathbf{E}\\big[l(Y,Y^{\\prime})\\big]\\!-\\!2\\mathbf{E}_{X^{\\prime}Y^{\\prime}}\\big[\\mathbf{E}_{X}k(X,X^{\\prime})\\mathbf{E}_{Y}l(Y,Y^{\\prime})\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(X^{\\prime},Y^{\\prime})$ is a independent copy of $(X,Y)$ . An estimator of $H S I C(X,Y)$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nH S I C_{b}(Z):=\\frac{1}{n^{2}}\\sum_{i,j}k_{i j}l_{i j}+\\frac{1}{n^{4}}\\sum_{i,j,q,r}k_{i j}l_{q r}-2\\frac{1}{n^{3}}\\sum_{i,j,q}k_{i j}l_{i q}=\\frac{1}{n^{2}}T r(\\mathbf{K}\\mathbf{H}\\mathbf{L}\\mathbf{H}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $k_{i j}:=k(x_{i},x_{j})$ , $l_{i j}:=l(y_{i},y_{j})$ are the entries of the $n\\times n$ kernel matrices $\\mathbf{K}$ , L respectively, $\\mathbf{H}=\\mathbf{I}-{\\textstyle\\frac{1}{n}}\\mathbf{1}\\mathbf{1}^{T}$ is the centering matrix and 1 is a vector of ones. ", "page_idx": 2}, {"type": "text", "text": "3 Revisiting HSIC from Frequency Domain Perspective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We denote $\\mathcal{F}$ as the Fourier transform, and ${\\mathcal{F}}^{-1}$ as its inverse. When the kernels $k,l$ are translationinvariant, i.e., there exist functions $\\psi,\\psi_{k},\\psi_{l}$ such that for all $(x,x^{\\prime})\\in\\mathcal{X}\\times\\mathcal{X}$ and $(y,y^{\\prime})\\in\\mathcal{y}\\times\\mathcal{y}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi(x-x^{\\prime},y-y^{\\prime})=\\psi_{k}(x-x^{\\prime})\\psi_{l}(y-y^{\\prime})=k(x,x^{\\prime})l(y,y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, according to the results of [36, Corollary 4], the HSIC with function $\\psi$ can be formulated as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{HSIC}(X,Y)=\\int_{\\mathbb{R}^{d_{x}}\\times\\mathbb{R}^{d_{y}}}\\left|\\phi_{\\mathbb{P}_{X Y}}(\\omega)-\\phi_{\\mathbb{P}_{X}\\mathbb{P}_{Y}}(\\omega)\\right|^{2}(\\mathcal{F}^{-1}\\psi)(\\omega)d\\omega,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\omega=(\\omega_{x},\\omega_{y})\\in\\mathbb{R}^{d_{x}}\\times\\mathbb{R}^{d_{y}},\\omega_{x},\\omega_{y}$ are the frequencies of $X$ and $Y$ respectively, and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\phi_{\\mathbb{P}_{X Y}}(\\omega):=\\int e^{-i(\\omega_{x}^{T}x+\\omega_{y}^{T}y)}d\\mathbb{P}_{X Y},\\;\\phi_{\\mathbb{P}_{X}\\mathbb{P}_{Y}}(\\omega):=\\left(\\int e^{-i\\omega_{x}^{T}x}d\\mathbb{P}_{X}\\right)\\left(\\int e^{-i\\omega_{y}^{T}y}d\\mathbb{P}_{Y}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "are the characteristic functions of $\\mathbb{P}_{X Y}$ and $\\mathbb{P}_{X}\\mathbb{P}_{Y}$ , respectively. Intuitively, Eq. (4) means that HSIC can be understood as the difference between the joint distribution and the product of the marginal distributions in the frequency domain, with different weights $(\\mathcal{F}^{-1}\\psi)(\\omega)$ being attached to different frequencies, which are determined by the kernel function. When ${\\mathcal{F}}^{-1}{\\mathcal{\\psi}}$ is almost everywhere nonzero, it can be shown that the kernel is characteristic [36, 10]. The characteristic condition ensures that the criterion is discriminative for discrepancies at almost all frequencies. However, with inappropriate choices of ${\\mathcal{F}}^{-1}{\\psi}$ , the differences may not be significant enough. We explain this with an example: ", "page_idx": 2}, {"type": "text", "text": "Example. Consider the Sinusoid model that $\\mathcal{X}\\times\\mathcal{Y}:=\\,[-\\pi,\\pi]^{2}$ and $(X,Y)\\;\\sim\\;p_{x y}(x,y)\\;\\propto\\;$ $1+\\sin(\\omega_{0}x)\\sin(\\omega_{0}y)$ , where $p_{x y}$ is the probability density function and $\\omega_{0}$ is a positive integer. Combining Eq. (5), we can calculate that $\\bar{\\phi}_{\\mathbb{P}_{X}\\mathbb{P}_{Y}}(\\omega)=\\delta(\\bar{\\omega_{x}})\\delta(\\omega_{y})$ and $\\phi_{\\mathbb{P}_{X Y}}(\\omega)\\bar{=}\\,\\delta(\\omega_{x})\\delta(\\omega_{y}\\bar{)}+$ $[\\delta(\\omega_{x}+\\omega_{0})+\\delta(\\omega_{x}-\\omega_{0})][\\delta(\\omega_{y}+\\omega_{0})+\\delta(\\stackrel{\\cdot}{\\omega}_{y}-\\omega_{0})]$ , where $\\delta$ is the Dirac delta function, thus the differ\u221aence between them only relies on the frequency $\\omega_{0}$ . When the Gaussian kernels with width $\\sqrt{2}\\lambda_{x}$ and $\\sqrt{2}\\lambda_{y}$ are used, i.e., $k(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{2}^{2}/(4\\lambda_{x}^{2})),l(y,y^{\\prime})=\\exp(-\\|y-y^{\\prime}\\|_{2}^{2}/(4\\lambda_{y}^{2}))$ , then the inverse Fourier transform of $\\psi$ is $;\\,(\\mathcal{F}^{-1}\\psi)(\\omega_{x},\\omega_{y})\\,=\\,\\pi^{-1}\\lambda_{x}\\lambda_{y}\\exp(-(\\lambda_{x}^{2}\\omega_{x}^{2}+\\lambda_{y}^{2}\\omega_{y}^{2}))$ . Hen\u221ace $\\mathrm{HSIC}(X,Y)\\,=\\,4\\pi^{-1}\\lambda_{x}\\lambda_{y}\\exp(-(\\lambda_{x}^{2}+\\lambda_{y}^{2})\\omega_{0}^{2})$ whose maximum is taken at $\\lambda_{x}^{*}=\\lambda_{y}^{*}=$ $1/(\\sqrt{2}\\omega_{0})$ , indicating that the widths need to be adjusted to focus on some specific frequencies. If the common setting [14, 44] is adapted, which uses mid-widths (i.e., the median distance does not change with $\\omega_{0}$ since the marginal distributions do not change with $\\omega_{0}$ ), then the criterion will exponentially decline to 0 as $\\omega_{0}$ increases. In contrast, the criterion using the adaptive optimization width $(1/\\omega_{0},1/\\omega_{0})$ decreases at a rate of $\\mathcal{O}(\\omega_{0}^{-2})$ , which is a considerable improvement. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "This example illustrates the loss of the discriminatory power of the criterion when an inappropriate ${\\mathcal{F}}^{-1}{\\psi}$ is chosen. The discriminatory power of the criterion heavily impacts the sample size required for the test to obtain significant results in practice, and existing inflexible configurations may lead to inadequate test power in the presence of reasonably large sample sizes. Consequently, it is important to design learnable ${\\mathcal{F}}^{-1}{\\psi}$ . To this end, we subsequently design a learnable objective and let it be optimized in a data-driven manner. Before this, we provide an approach to make the criterion be computed efficiently. This can be achieved by sampling in the frequency domain. Formally, a finite-dimensional approximation in the frequency domain of the integral in Eq. (4) is given as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{HSIC}_{\\omega}(X,Y):=\\frac{1}{D_{x}D_{y}}\\sum_{i=1}^{D_{x}}\\sum_{j=1}^{D_{y}}\\bigl|\\phi_{\\mathbb{P}_{X Y}}(\\omega_{x;i},\\omega_{y;j})-\\phi_{\\mathbb{P}_{X}\\mathbb{P}_{Y}}(\\omega_{x;i},\\omega_{y;j})\\bigr|^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{\\omega_{x;i}\\}_{i=1}^{D_{x}},\\{\\omega_{y;j}\\}_{j=1}^{D_{y}}$ are sampled independently with the measure $\\mathcal{F}^{-1}\\psi_{k},\\mathcal{F}^{-1}\\psi_{l}$ , respectively. Note that ${\\mathcal{F}}^{-1}{\\psi}$ is a product measure, i.e., ${\\mathcal{F}}^{-1}\\psi\\,=\\,({\\mathcal{F}}^{-1}\\psi_{k})\\otimes({\\mathcal{F}}^{-1}\\psi_{l})$ . This type of approximation is also called random Fourier features (RFF) [28] that had been applied to various kernel algorithms. We will incorporate this technique to efficiently perform computation later. ", "page_idx": 3}, {"type": "text", "text": "4 Learning Significant Fourier Feature Pairs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 HSIC with Learnable Fourier Feature Pairs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To design ${\\mathcal{F}}^{-1}{\\psi}$ , we need to make sure that supp $(\\mathcal{F}^{-1}\\psi)=\\mathbb{R}^{d_{x}}\\times\\mathbb{R}^{d_{y}}$ to meet the characteristic condition and that its integral over the full space is 1 to ensure it is a probability measure. Also, for practical utility, ${\\mathcal{F}}^{-1}{\\psi}$ should embody a familiar probability density function, facilitating sampling procedures. Fortunately, a versatile array of options emerges through the judicious selection of kernels 2 with adjustable parameters. Take kernel $k$ as an example, some commonly used kernels are listed in Tab. 1, and their inverse Fourier transforms are listed simultaneously. Additionally, to be able to apply gradient-based optimization techniques, we invoke a method that disentangle the sampled objects and the learnable parameters. Specifically, we leverage a variable transform $\\tau_{\\theta_{k}}$ (a bijection function parameterized with $\\theta_{k}$ ) to convert the probability measure ${\\mathcal{F}}^{-1}\\psi_{k}$ into a simple distribution (e.g. a standard Gaussian distribution) $p_{k}(\\omega)$ . Simultaneously, we relocate the learnable component onto $X$ . Consequently, we can focus on learning parameterized transformations $\\tau_{\\theta_{k}}$ and simplifying the computation by enabling sampling directly from $p_{k}(\\omega)$ . ", "page_idx": 3}, {"type": "table", "img_path": "BEiqNQZIky/tmp/2fe190be05e4afffb1114faf2c82665c89da607ff894249547de88fdbb938028.jpg", "table_caption": ["Table 1: Some popular kernels (parameterized by $\\sigma,\\Sigma)$ with corresponding density functions. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Remark. The above scheme provides a broader form for designing. The mapping $\\tau_{\\theta_{k}}$ can be viewed as a feature extractor, which makes it possible to flexibly combine models (e.g., neural network) thus incorporating deep kernel [24] into the framework. Also, it should be noted that the single kernel example can also be extended to multi-kernel setting [11] by executing the procedure for each kernel. ", "page_idx": 3}, {"type": "text", "text": "Next, we obtain the learnable independence criterion and utilize the sampling technique as in Eq. (6) to compute efficiently. Note that for simplicity, we take the same value for both $D_{x}$ and $D_{y}$ in Eq. (6) by default. By the definition, the kernel function can be expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi_{k}\\left(\\mathcal{T}_{\\theta_{k}}x-\\mathcal{T}_{\\theta_{k}}x^{\\prime}\\right)=\\mathcal{F}[\\mathcal{F}^{-1}\\psi_{k}(\\omega)]=\\int e^{-i\\omega^{T}(\\mathcal{T}_{\\theta_{k}}x-\\mathcal{T}_{\\theta_{k}}x^{\\prime})}p_{k}(\\omega)d\\omega.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By applying the frequency sampling technique, we obtain the approximation as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{k}^{(\\omega)}\\left(\\mathcal{T}_{\\theta_{k}x}-\\mathcal{T}_{\\theta_{k}x}^{\\prime}\\right):=\\frac{2}{D}\\sum_{j=1}^{D/2}e^{-i\\omega_{k;j}^{T}(\\mathcal{T}_{\\theta_{k}x}-\\mathcal{T}_{\\theta_{k}}x^{\\prime})}=\\frac{2}{D}\\sum_{j=1}^{D/2}\\cos\\left(\\omega_{k;j}^{T}(\\mathcal{T}_{\\theta_{k}x}-\\mathcal{T}_{\\theta_{k}x^{\\prime}})\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\{\\omega_{k;j}\\}_{j=1}^{D/2}$ are sampled independently with distribution $p_{k}(\\omega)$ and the last equation is because the kernel function is real. To get a more computationally tractable form, we define ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Lambda_{k}(x):=\\sqrt{\\frac{2}{D}}\\left[\\cos(\\omega_{1}^{T}{\\mathcal T}_{\\theta_{k}}x),\\sin(\\omega_{1}^{T}{\\mathcal T}_{\\theta_{k}}x),...,\\cos(\\omega_{D/2}^{T}{\\mathcal T}_{\\theta_{k}}x),\\sin(\\omega_{D/2}^{T}{\\mathcal T}_{\\theta_{k}}x)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "called learnable RFF of $k$ then Eq. (8) becomes $\\psi_{k}^{(\\omega)}\\left(\\mathcal{T}_{\\theta_{k}}x-\\mathcal{T}_{\\theta_{k}}x^{\\prime}\\right)=\\Lambda_{k}(x)\\Lambda_{k}(x^{\\prime})^{T}$ . The expression with a similar form is also given in [44], with the difference that we have added learnable parts. For $Y$ , we define the corresponding symbols by substituting $k$ for $l$ and $x$ for $y$ . Also, for convenience, we default to keeping $Y$ and $X$ the same number of samples $D$ from here on. Then the HSIC with learnable RFF pairs can be obtained by replacing $k,l$ in Eq. (1) to $\\psi_{k}^{(\\omega)},\\psi_{l}^{(\\omega)}$ . Also, the corresponding estimator with sample $Z$ can be obtained by replacing $\\mathbf{K},\\mathbf{L}$ in Eq. (2) to the matrices $\\mathbf{A}_{X}\\mathbf{A}_{X}^{\\dagger},\\mathbf{A}_{Y}\\breve{\\mathbf{A}}_{Y}^{T}$ , where $\\begin{array}{r}{\\mathbf{A}_{X}:=[\\bar{\\Lambda}_{k}(x_{1});...;\\Lambda_{k}(x_{n})]_{n\\times D}}\\end{array}$ and so as define for $\\pmb{\\Lambda}_{Y}$ . As a result, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{HSIC}_{\\omega}(Z):=\\frac{1}{n^{2}}\\mathrm{Tr}(\\mathbf{\\Lambda}_{X}\\mathbf{\\Lambda}_{X}^{T}\\mathbf{H}\\mathbf{\\Lambda}_{Y}\\mathbf{A}_{Y}^{T}\\mathbf{H})=\\frac{1}{n^{2}}\\mathrm{Tr}(\\mathbf{\\Lambda}_{X}^{T}\\mathbf{H}\\mathbf{\\Lambda}_{Y}\\mathbf{A}_{Y}^{T}\\mathbf{H}\\mathbf{\\Lambda}_{X})=\\frac{1}{n^{2}}\\|\\mathbf{\\Lambda}_{X\\boldsymbol{c}}^{T}\\mathbf{A}_{Y\\boldsymbol{c}}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{A}_{X c}\\;:=\\;\\mathbf{H}\\mathbf{A}_{X},\\mathbf{A}_{Y c}\\;:=\\;\\mathbf{H}\\mathbf{A}_{Y}$ . The time complexity is analyzed as follows. Since the computation of the mapping $\\boldsymbol{\\mathcal{T}_{\\boldsymbol{\\theta}_{k}}}\\boldsymbol{x}$ depends on the specific design, here we default to analyzing the kernel case shown in Tab. 1. In this case, computing $\\Lambda_{X},\\Lambda_{Y}$ requires $\\mathcal{O}\\big(n D(d_{x}+d_{y})\\big)$ time. Then calculate $\\Lambda_{X c},\\Lambda_{Y c}$ cost $\\mathcal{O}(n D)$ . After that, calculate $\\mathrm{HSIC}_{\\omega}(Z)$ cost $\\mathcal{O}(n D^{2})$ . Hence, the overall time complexity is $\\mathcal{O}\\big(n D(d_{x}+d_{y}+D)\\big)$ , i.e. the running time is linear with $n$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Linear-time Optimization Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we model the behavior of $\\mathrm{HSIC}_{\\omega}(Z)$ to obtain an optimization objective for maximizing the power of the test. By utilizing the property that $\\mathrm{HSIC}_{\\omega}(Z)$ is a ${\\mathrm{V}}.$ -statistic, we can extend the results [14, Theorem 1, 2] for $\\mathrm{HSIC}_{\\omega}(Z)$ , as shown in the following proposition with the proof given in the Appendix. To simplify, we denote $\\left({x_{i},y_{i}}\\right)$ as $z_{i}$ to represent the $i$ -th sample and denote $\\psi_{k}^{(\\omega)}\\left(T_{\\theta_{k}}x_{t}-T_{\\theta_{k}}x_{u}\\right)$ as $k_{t u}^{(\\omega)}$ and $\\psi_{l}^{(\\omega)}\\,({\\mathcal T}_{\\theta_{l}}y_{t}-{\\mathcal T}_{\\theta_{l}}y_{u})$ as $l_{t u}^{(\\omega)}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 (Asymptotics). Let $\\begin{array}{r}{h_{i j q r}^{(\\omega)}:=\\frac{1}{4!}\\sum_{(t,u,v,w)}^{(i,j,q,r)}k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{v w}^{(\\omega)}-2k_{u v}^{(\\omega)}l_{t v}^{(\\omega)},}\\end{array}$ ere the $(t,u,v,w)$ drawn without replacement from $(i,j,q,r)$ Under the null hypothesis $\\mathcal{H}_{0}$ , $H S I C_{\\omega}(Z)$ coverages in distribution to ", "page_idx": 4}, {"type": "equation", "text": "$$\nn H S I C_{\\omega}(Z)\\stackrel{d}{\\rightarrow}\\sum_{l=1}^{\\infty}\\lambda_{l}\\chi_{1l}^{2},\\;\\;\\lambda_{l}g_{l}(z_{j})=\\int_{z_{i},z_{q},z_{r}}h_{i j q r}^{(\\omega)}g_{l}(z_{i})d F_{z_{i},z_{q},z_{r}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\chi_{11}^{2},\\chi_{12}^{2},...$ are independent $\\chi_{1}^{2}$ variates and $\\lambda_{l}$ is the solution to the eigenvalue problem as in the right of Eq. $(I I)$ . Also, under the alternative $\\mathcal{H}_{1}$ , $H S I C_{\\omega}(Z)$ converges in distribution as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n^{\\frac{1}{2}}\\Big(H S I C_{\\omega}(Z)-\\mathbf{E}_{Z}H S I C_{\\omega}(Z)\\Big)\\frac{d}{\\omega}\\mathcal{N}(0,\\sigma_{\\omega}^{2}),\\ \\sigma_{\\omega}^{2}:=16\\Big[\\mathbf{E}_{i}(\\mathbf{E}_{j,q,r}h_{i j q r}^{(\\omega)})^{2}-\\big(\\mathbf{E}_{Z}h_{i j q r}^{(\\omega)}\\big)^{2}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the simplified notation $\\mathbf{E}_{j,q,r}:=\\mathbf{E}_{z_{j},z_{q},z_{r}}$ and $\\mathbf{E}_{Z}:=\\mathbf{E}_{z_{i},z_{j},z_{q},z_{r}}$ ", "page_idx": 4}, {"type": "text", "text": "According to Proposition 1, the power of the test with $\\mathrm{HSIC}_{\\omega}$ can be formulated by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{H}_{1}}\\left(n\\mathrm{HSIC}_{\\omega}(Z)>r_{\\omega}\\right)\\rightarrow\\Phi\\left(\\frac{n\\mathbf{E}_{Z}\\mathrm{HSIC}_{\\omega}(Z)-r_{\\omega}}{\\sqrt{n}\\sigma_{\\omega}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Phi$ is the standard normal CDF and $r_{\\omega}$ is the threshold, i.e. $(1-\\alpha)$ -quantile of distribution given in Eq. (11) that exactly controls Type I error rate to the nominal\u221a level $\\alpha$ . Hence, to maximize the power of the test, a natural criterion is $[n\\mathbf{E}_{Z}\\mathrm{HSIC}_{\\omega}(Z)-r_{\\omega}]/(\\sqrt{n}\\sigma_{\\omega})$ . Next, we provide its estimation which can be computed in linear time. ", "page_idx": 4}, {"type": "text", "text": "We first consider obtaining the estimator of the numerator part. For the term $\\mathbf{E}_{Z}\\mathbf{H}\\mathbf{S}\\mathbf{IC}_{\\omega}(Z)$ , we can estimate it with $\\mathrm{HSIC}_{\\omega}(\\bar{Z})$ as in Eq. (10). The estimation of the threshold $r_{\\omega}$ poses a challenge, primarily stemming from the lack of an explicit expression for the distribution of the infinite sum of chi-square variables. One avenue to address this challenge involves employing the permutation method [2, 38] to simulate the distribution under $\\mathcal{H}_{0}$ . However, this method necessitates a significant number of shuffles to accurately approximate the distribution. Furthermore, even with the implementation of parallel schemes, it incurs memory costs proportional to the number of permutations, rendering it impractical for resource-constrained scenarios. Here, we adopt a lightweight approach in practice, leveraging the gamma approximation as proposed by [14]. A gamma distribution is uniquely determined by its first and second-order moments. For these two moments, we present their corresponding linear-time estimators in Theorem 1. As a result, we can obtain the $(1-\\alpha)$ -quantile of the gamma distribution, denoted as $\\widehat{c_{\\alpha}}$ , with estimated parameters $\\gamma:=\\mathcal{E}_{0}^{2}/\\mathcal{V}_{0},\\beta:=\\mathcal{V}_{0}/\\bar{\\mathcal{E}_{0}}$ in linear time. Formally, with the term ${\\mathcal{E}}_{0}$ an d $\\lvert\\lambda_{0}$ defined in Theorem 1, $\\widehat{c_{\\alpha}}$ is calculated by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}_{0}:n\\mathrm{HSIC}_{\\omega}(Z)\\sim\\frac{x^{\\gamma-1}e^{-x/\\beta}}{\\beta^{\\gamma}\\Gamma(\\gamma)},\\gamma=\\frac{\\mathcal{E}_{0}^{2}}{\\mathcal{V}_{0}},\\;\\beta=\\frac{\\mathcal{V}_{0}}{\\mathcal{E}_{0}},\\;\\;\\;\\int_{0}^{\\frac{\\overline{{\\omega}}_{\\alpha}}{\\beta}}\\frac{x^{\\gamma-1}e^{-x}}{\\Gamma(\\gamma)}d x=1-\\alpha,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Gamma(\\cdot)$ is the gamma function. By combining the way to estimate the gradients of $\\widehat{c_{\\alpha}}$ [30], we enable it for gradient-based optimization with automatic differentiation framework. As  a  result, we obtain a linear-time differentiable estimator of the numerator part. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Linear-Time Estimators). Under $\\mathcal{H}_{0}$ , the estimators of mean and variance with bias of $O(n^{-1})$ to ${\\bf E}_{Z}[n H S I C_{\\omega}(Z)]$ and $W\\!a r_{Z}[n H S I C_{\\omega}(Z)].$ , denote as ${\\mathcal{E}}_{0}$ and ${\\dot{\\nu}}_{0}$ , respectively, are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathfrak{z}_{0}:=\\frac{[\\mathbf{1}^{T}\\mathbf{A}_{X c}^{\\cdot2}\\mathbf{1}][\\mathbf{1}^{T}\\mathbf{A}_{Y c}^{\\cdot2}\\mathbf{1}]}{(n-1)^{2}},\\mathcal{V}_{0}:=\\frac{2n(n-4)(n-5)}{(n-1)(n-2)(n-3)}\\frac{[\\mathbf{1}^{T}(\\mathbf{A}_{X c}^{T}\\mathbf{A}_{X c})^{\\cdot2}\\mathbf{1}][\\mathbf{1}^{T}(\\mathbf{A}_{Y c}^{T}\\mathbf{A}_{Y c})^{\\cdot2}\\mathbf{1}]}{n^{4}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $()^{\\cdot2}$ is the entry-wise matrix power. Both ${\\mathcal{E}}_{0}$ and ${\\dot{\\nu}}_{0}$ can be calculated in $\\mathcal{O}(n D^{2})$ time. ", "page_idx": 5}, {"type": "text", "text": "FToo r ctahlec ruelamtaei $\\sigma_{\\omega}$ ,e  tehseti smtraatei gith twfiotrh $\\widehat{\\sigma}_{\\omega}$ dt hwat $\\begin{array}{r}{\\widehat{\\sigma}_{\\omega}^{2}:=16\\big[\\frac{1}{n}\\sum_{i}(\\frac{1}{n^{3}}\\sum_{j,q,r}h_{i j q r}^{(\\omega)})^{2}\\!-\\!\\mathrm{HSIC}_{\\omega}^{2}(Z)\\big]}\\end{array}$ j,q,r h $\\sum_{j,q,r}h_{i j q r}^{(\\omega)}$ $h_{i j q r}^{(\\omega)}$ total $O(n^{4})$ of computation. Here we provide a way to enable it to be calculated in linear time by obtaining a matrix expression. The main result is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{j,q,r}h_{i j q r}^{(\\omega)}=\\frac{1}{2}\\left[n\\mathbf{1}^{T}\\mathbf{A}\\mathbf{1}+n^{2}(\\mathbf{A}\\mathbf{1})_{i}+(\\mathbf{1}^{T}\\mathbf{C})\\mathbf{B}_{i}+(\\mathbf{1}^{T}\\mathbf{B})\\mathbf{C}_{i}-n\\mathbf{E}_{i}-n\\mathbf{F}_{i}-n\\mathbf{D}_{i}-\\mathbf{1}^{T}\\mathbf{D}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the definition of variables $\\mathbf{A}$ to $\\mathbf{F}$ with the calculation cost are given in the Fig. 1 and the derivation of Eq. (16) is given in the Appendix. By checking the complexity of the remaining matrix operations in Eq. (16), all the elements with index $i$ can be calculated in $\\mathcal{O}(n D^{2})$ . Combining the results obtained before that $\\mathrm{HSIC}_{\\omega}(Z)$ can also be calculated in $\\mathcal{O}(n D^{2})$ , thus calculating the term $\\widehat{\\sigma}_{\\omega}$ cost $\\mathcal{O}(n D^{2})$ time. As a result, we obtain the overall linear-time optimization objective $J:=[\\mathrm{HSIC}_{\\omega}(Z)-\\widehat{c_{\\alpha}}/n]/\\widehat{\\sigma}_{\\omega}$ , which is a clear contrast to the existing quadratic-time schemes [30]. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underbrace{\\left[\\Delta_{X}^{T}\\Delta_{Y}\\right]_{[\\mathrm{J},\\mathrm{S},\\mathrm{D}]}\\longrightarrow\\left[\\left\\{(\\Delta_{X}^{T}\\Delta_{Y})\\Delta_{Y}^{T}\\right\\}_{[\\mathrm{J},\\mathrm{Sm}]}\\right]_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~}}\\underbrace{\\left(\\mathbf{A}_{X}\\odot\\left(\\mathbf{A}_{X}^{T}\\Delta_{Y}^{T}\\right)_{[\\mathrm{J},\\mathrm{N}]}\\right)}_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~}}\\Bigg\\{\\underbrace{\\mathbf{D}:=\\left[\\mathbf{B}\\odot\\mathbf{C}\\right]_{[\\mathrm{max}]}}\\quad\\left\\{\\begin{array}{l l l}{{\\displaystyle\\mathrm{Im}}\\in\\mathbf{O}\\mathrm{m}\\mathrm{pol}\\mathrm{poik}\\mathrm{ply};}&{\\left(\\mathcal{O}(n P^{2})\\right)}&{\\cdots\\mathrm{or}\\mathrm{~\\infty~-~\\infty~-~\\infty~}}\\\\ {{\\displaystyle\\mathrm{Im}}_{\\Omega}:\\mathrm{Im}_{X}:\\mathrm{co}_{i,\\mathrm{-~\\infty~-~\\infty~-~\\infty~}}}&{\\left(\\mathcal{O}(n P^{2})\\right)}&{\\cdots\\mathrm{o~-~\\infty~-~\\infty~}}\\end{array}\\right.}\\underbrace{\\left[\\begin{array}{l l l}{-\\partial_{Y}\\mathbf{A}_{Y}}&{\\mathrm{Im}}&{\\mathrm{Im}}\\\\ {\\mathrm{J}_{\\Omega}\\mathbf{A}_{Y}}&{\\mathrm{Sm}}&{\\mathrm{Im}}\\end{array}\\right]_{[\\mathrm{J},\\mathrm{Sm}]}}_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~}}\\underbrace{\\left(\\mathcal{O}(n P^{2})\\right)}_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~}}\\Bigg\\}\\underbrace{\\left[\\begin{array}{l l l}{\\mathbf{A}_{X}^{T}}&{\\mathrm{Im}}&{\\mathrm{Im}}\\\\ {\\mathbf{B}_{Y}^{T}}&{\\mathrm{Sm}}&{\\mathrm{Im}}\\end{array}\\right]_{[\\mathrm{J},\\mathrm{Sm}]}}_{\\longrightarrow}\\Bigg\\{\\underbrace{\\left[\\begin{array}{l l l}{\\mathbf{A}_{Y}^{T}\\mathbf{A}_{Y}}&{\\mathrm{Im}}&{\\mathrm{Im}}&{\\mathrm{S m \n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Figure 1: The diagram shows the definition of the quantities in Eq. (16), with styles representing the time complexity of the computational process in the current box. $\\odot$ : the element-wise product. ", "page_idx": 5}, {"type": "text", "text": "4.3 The Overall Learning Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After obtaining the differentiable optimization objective $J$ , we can perform the training process end-to-end. In this process, the overfitting issues may happen especially with insufficient samples, which could influence both Type I and II errors. If we use the same sample for testing, the Type I errors may be uncontrollable [22] when the overfitting issues happen. To address this, we adopt the split scheme as in [24, 18] to allow our tests to maintain validity (controllable Type I errors). The split ratio is set to 0.5 to facilitate the balance between the two. Apart from controlling Type I errors, we still want to mitigate overfitting issues as much as possible in order to generalize the optimized ", "page_idx": 5}, {"type": "text", "text": "Fourier feature pairs on test data thus improving the power of our tests. To this end, we select smooth function classes to control the model complexity (e.g., as measured by the VC dimension), specifically in this paper, we consider the two classes in Tab. 1 and implement them for experiments later. One is the choice of Gaussian classes that optimize the global scale, and the other we consider Mahalanobis classes and set $\\Sigma$ to be $\\mathrm{diag}(\\sigma_{1},...,\\sigma_{d})$ for optimization, which corresponds to optimizing the scale in each dimension (which allows to capture high-frequency signals such as the edge in the image). These smooth choices also bring the advantage of interpretability [30] and it is experimentally proven that this simple choice is already able to handle most of the cases in different settings. ", "page_idx": 6}, {"type": "text", "text": "More discussion about split strategy. Currently, there are two major classes of approaches for adaptive independence tests. One involves selecting kernels from a finite/countable set (discrete scenario) and the other involves performing kernel parameter searches in a continuous space (continuous scenario). For the former case, some methods [22, 32] control Type I errors by applying techniques from the selective inference literature without data splitting. However, these methods cannot be directly applied to a continuous scenario due to the uncountable set of kernels involved. To the best of our knowledge, both our scheme and existing methods [24, 18] rely on data splitting for the continuous case. Designing methods to control Type I errors in the continuous case without sample splitting remains a challenging and significant problem for future research. ", "page_idx": 6}, {"type": "text", "text": "Algorithm. Our algorithm is outlined in Alg. 1. As a pre-processing step, we split the data into the training data $Z^{t r}$ and the testing data $Z^{t e}$ (Line 1). The test contains two phases: 1) We learn the Fourier feature pairs with Adam [21] optimizer using full batches on $Z^{t r}$ (Lines 2-7). 2) With the learned Fourier feature pairs, we calculate the test statistic and threshold (Lines 8-10) to determine the independence (Lines 11) on $Z^{t e}$ . The overall time complexity is $\\mathcal{O}(T n D(d_{x}+d_{y}+D))$ and the space cost is $\\mathcal{O}\\big(n(d_{x}+d_{y}+D)\\big)$ for storing the data as well as the Fourier feature pairs. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 The learning and testing framework ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: samples $Z$ of $X,Y$ , significance level $\\alpha$ , the number of Fourier feature $D$ .   \nOutput: $X\\perp\\!\\!\\!\\perp Y$ or $X$ \u0338\u22a5\u22a5 $Y$ .   \n1: Split the data as $Z=Z^{t r}\\cup Z^{t e}$ . Sampling $\\{\\omega_{j}\\}_{j=1}^{D/2}=\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ with $p(\\omega)$ . 2: \u25c1Learning significant Fourier feature pairs on $Z^{t r}$ .   \n3: Initialize parameters $\\theta_{k},\\theta_{l}$ , set learning rate $\\epsilon$ , and set iteration steps $T$ .   \n4: for $t=1,2,...,T$ do   \n$\\mathbf{\\cal{A}}_{X},\\mathbf{\\cal{A}}_{Y}$ $\\theta_{k},\\theta_{l}$ $\\left\\{\\omega_{j}\\right\\}_{j=1}^{D/2}$ 6: Calculate criterion $J$ with $\\mathbf{\\cal{A}}_{X},\\mathbf{\\cal{A}}_{Y}$ then optimize $J$ with $(\\theta_{k},\\theta_{l})\\gets(\\theta_{k},\\theta_{l})+\\bar{\\epsilon}\\nabla_{(\\theta_{k},\\theta_{l})}J,$ . 7: end for   \n8: After training, obtain optimized parameters $\\theta_{k}^{*},\\theta_{l}^{*}$ .   \n9: $\\vartriangleleft$ Testing with learned Fourier feature pairs on $Z^{t e}$ .   \n10: Calculate the statistic nteHSIC\u03c9(Zte), thresholdc\u03b1(Zte) with parameters \u03b8k\u2217, \u03b8l\u2217 and {\u03c9j}jD=/12. 11: Return $X$ \u0338\u22a5\u22a5 $Y$ if $\\widehat{c_{\\alpha}}(Z^{t e})\\leq n^{t e}\\mathrm{HSIC}_{\\omega}(Z^{t e})$ holds, otherwise $X$ \u22a5\u22a5 $Y$ . ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first give the uniform bound results over a ball in parameter space which guarantees the convergence of our optimizing objective thus ensuring its effectiveness in modeling test power. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Uniform Bound). Let $\\theta_{k},\\theta_{l}$ parameterize $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ in Banach spaces of dimension $d_{k},d_{l}$ . And $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ are Lipschitz to the parameters $\\theta_{k},\\theta_{l}$ with the non-negative constant $L_{k},L_{l}$ , respectively. Let $\\Theta_{c}$ be a set of $(\\theta_{k},\\theta_{l})$ for which $\\sigma_{\\omega}\\ge c>0$ with a positive constant c and $\\lVert{\\boldsymbol{\\theta}}_{k}\\rVert\\leq R_{{\\boldsymbol{\\theta}}_{k}},\\lVert{\\boldsymbol{\\theta}}_{l}\\rVert\\leq$ $R_{\\theta_{l}}$ . Let $r$ denote the threshold, i.e., $(1-\\alpha)$ -quantile for the distribution in Eq. $(I I)$ and $r^{(n)}$ be the threshold with sample size n. Let $\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ be the samplings of frequency with the sampling number $D$ . Also, we define $R_{\\omega_{k}}:=\\operatorname*{sup}_{j}\\|\\omega_{k;j}\\|,R_{\\omega_{l}}:=\\operatorname*{sup}_{j}\\|\\omega_{l;j}\\|,d_{s}:=\\operatorname*{max}\\{d_{k},d_{l}\\}$ and $\\xi_{\\omega}:=H S I C_{\\omega}(Z)$ . Then with probability at least $1-\\delta$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(\\theta_{k},\\theta_{l})\\in\\Theta_{c}}\\left|\\frac{\\xi_{\\omega}-r_{\\omega}^{(n)}/n}{\\hat{\\sigma}_{\\omega}}-\\frac{\\mathbf{E}_{Z}\\xi_{\\omega}-r_{\\omega}/n}{\\sigma_{\\omega}}\\right|\\sim\\mathcal{O}\\left(\\left[\\sqrt{\\frac{1}{n}\\log\\frac{1}{\\delta}+d_{s}\\frac{\\log n}{n}}+\\frac{R_{\\omega_{k}}L_{k}+R_{\\omega_{l}}L_{l}}{\\sqrt{n}}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Next, we show the consistency of the tests, i.e. the power of the test tends to 1 as the sample size increases. Let the U-statistic of $\\mathrm{HSIC}_{\\omega}(Z^{t e})$ be $\\mathrm{HSIC}_{\\omega}^{(u)}(Z^{t e})$ , then we have the following results. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Consistency). Let $\\theta_{k}^{*},\\theta_{l}^{*}$ be the parameters after learning, $Z^{t e}$ be the test samples of size $m$ , when $\\mathbf{E}_{Z}H\\mathbf{S}I C_{\\omega}^{(u)}(Z^{t e})>0,$ , then the probability of the Type II error ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(T\\!y p e\\,I I\\,e r r o r\\big)\\!=\\mathbb{P}_{\\mathcal{H}_{1}}\\big(m H S I C_{\\omega}\\big(Z^{t e}\\big)\\le r_{\\omega}^{(m)}|\\theta_{k}^{*},\\theta_{l}^{*}\\big)\\!\\sim\\mathcal{O}(m^{-1/2}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Let the mapping functions with learned parameters $\\theta_{k}^{*},\\theta_{l}^{*}$ be $\\tau_{\\theta_{k}^{*}},\\tau_{\\theta_{l}^{*}}$ , and the corresponding range space be compact subsets of $\\mathbb{R}^{d_{T_{x}}},\\mathbb{R}^{d_{T_{y}}}$ , respectively. Also, the diameters of two range spaces are denoted by diam $(\\tau_{\\theta_{k}^{*}}),d i a m(T_{\\theta_{l}^{*}}).$ , respectively. Let $\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ be the frequency samplings with their second moment denoted by $\\sigma_{\\omega_{k}}^{2}:=\\mathbf{E}_{p_{k}(\\omega)}[\\omega_{k;j}^{T}\\omega_{k;j}]$ , $\\sigma_{\\omega_{l}}^{2}:=\\mathbf{E}_{p_{l}(\\omega)}[\\omega_{l;j}^{T}\\omega_{l;j}]$ . Additionally, we denote $\\xi_{u}:=H S I C(X,Y)$ , then under $\\mathcal{H}_{1}$ , we have ${\\bf E}_{Z}H S I C_{\\omega}^{(u)}(Z^{t e})>0$ with any constant probability when $\\begin{array}{r}{D=\\Omega\\biggl(\\frac{d\\tau_{x}+d\\tau_{y}}{\\xi_{u}^{2}}\\log\\frac{\\sigma_{\\omega_{k}}d i a m(\\mathcal{T}_{\\theta_{k}^{*}})+\\sigma_{\\omega_{l}}d i a m(\\mathcal{T}_{\\theta_{l}^{*}})}{\\xi_{u}}\\biggr)}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "This result can be understood in two parts. The first one is about consistency, i.e., the Type II error rate tends to 0 at the rate of $m^{-1/2}$ when condition $\\mathbf{E}_{Z}\\mathrm{HSIC}_{\\omega}^{(u)}\\big(Z^{t e}\\big)>0$ holds. The second part provides the condition when $\\mathbf{E}_{Z}\\mathrm{HSIC}_{\\omega}^{(u)}(Z^{t e})>0$ holds, which requires sufficiently many frequency samplings. The theorem shows that the large value of the criterion $\\mathrm{HSIC}(X,\\dot{Y})$ helps to reduce the required $D$ . According to the results discussed in Sec. 3, there is an improvement in the criterion by finding the more significant features and thus helps to reduce the required $D$ . To summarize, the significant features further help to guarantee the consistency of the test under the efficient requirements (smaller $D$ ). All proofs as well as additional results are given in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "6 Performance Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare the following tests: distance-based statistic dCor [39], the original HSIC QHSIC [14], the copula-based method RDC [26], the three variants of HSIC NyHSIC [44], FHSIC [44], BHSIC [44] and HSICAgg [32], NFSIC [18] as introduced in Sec. 1. Among them, dCor and QHSIC are ${\\mathcal{O}}(n^{2})$ tests. RDC is calculated in $O(n\\log n)$ time and the rest are ${\\mathcal{O}}(n)$ tests. A detailed description of the comparing methods is given in the Appendix. For our methods, We provide two variants as mentioned in Sec. 4.3. We name the Gaussian class case as LFHSIC-G, and name the Mahalanobis class case (and set $\\Sigma$ as a diagonal matrix) LFHSIC-M. Additionally, for the comparative methods [30] that are relevant to us, due to their high time overhead and therefore inability to handle some settings of evaluation, we separately provide a comparison with our method under certain feasible experimental settings, the results are given in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "Experimental setup. The significance level $\\alpha$ is set to 0.05. We use Gaussian kernels for both $X$ and $Y$ in all kernel-based methods. And QHSIC, RDC, NyHSIC, FHSIC, BHSIC are all with the kernel width being set to the Euclidean distance median of the samples. The number of random features $D$ for FHSIC, LFHSIC-G/M, the number of induced variables for NyHSIC, the block size for BHSIC as well as the number of sub-diagonals $R$ for HSICAgg are all kept consistent as recommended in [44, 32] for fair evaluation. Parameter settings for the rest of the methods follow the defaults in the code. More details of the setups are given in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "Evaluation protocol. We evaluate on four synthetic datasets [18, 30] and two real datasets [44, 30]. Synthetic datasets consist of Sine Dependency (SD), Sinusoid (Sin), Gaussian Sign (GSign), and independent subspace analysis (ISA) dataset [14]. On real data, we introduce high-dimensional image data and another music dataset to evaluate the capability of all methods in different data scenarios. Unless otherwise specified, we perform 100 repeated randomized experiments and report the average result of test power as default. More details of the generating process of each dataset and the details of the evaluation (including running time) are provided in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "6.1 Results on Synthetic Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Settings of SD, Sin, and GSign Dataset. The Sin data corresponds to the example in Sec. 3 that requires the method to focus on differences in specific frequencies. In SD, $Y$ is dependent solely on the first two dimensions of $X$ . In contrast, in GSign, $Y$ is independent of any proper subset of $X$ but dependent on $X$ as a whole. Therefore, it requires the method to learn important local/global features based on the characteristics of the data to improve the test power. For SD and GSign, we set the dimension of $X$ as 4 and 5, respectively, and the dimension of $Y$ is 1 for both. For Sin, we set the frequency parameter $\\omega=5$ . For calculating the Type I error rate, we evaluate using samples $n=2000)$ obtained by permutation for all three datasets. ", "page_idx": 7}, {"type": "image", "img_path": "BEiqNQZIky/tmp/64613af118a62cb5ff605778299b4090e8c5e698d395d0ea1b5f796b888f767b.jpg", "img_caption": ["Figure 2: Top: ( $D=100)$ ). Below: $D=500)$ ). Left: The average Type I error rate on SD, Sin, and GSign datasets. The other three plots: The results of average test power on these three datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "BEiqNQZIky/tmp/74c9de495ed5993cd19d3dc44e760006b1ecf56694c61d4670411f8d188d17fd.jpg", "img_caption": ["Figure 3: The average test power v.s. the rotation angle of each method on the ISA dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Performance. The results for $D=100$ and $D=500$ are shown in Fig. 2. Except for NFSIC and BHSIC, all the other methods succeed in controlling the Type I error rate $\\leq0.05$ . LFHSIC-M/G, NFHSIC, and HSICAgg perform much better than other methods due to their ability to obtain more appropriate kernels/features for testing. LFHSIC-G/M performs on both settings of $D$ and has a more significant advantage over the others when $D$ is small, implying the optimization objective can still be successfully optimized and the criterion is still powerful under high-speed requirements. In addition, as the sample size increases the test power of LFHSIC-G/M is gradually converging to 1 in both settings, which corroborates the results of Theorem 3. ", "page_idx": 8}, {"type": "text", "text": "Settings of ISA Dataset (Large Scale). We set dimension (of both $X,Y)$ and sample size as $d\\,=\\,16$ , $n\\,=\\,10000$ and $d\\,=\\,32,n\\,=\\,60000$ , then evaluate the average test power with angle parameter $\\theta\\in[0,\\pi/4]$ . Note that a larger angle signifies stronger dependency. The quadratic-time methods are not involved in the evaluation due to their inability to handle large-scale settings. For HSICAgg under the challenging setting $n=60000,d=32.$ , the memory space required for parallel implementation leads to memory overflow and hence the results are not given. ", "page_idx": 8}, {"type": "text", "text": "Performance. The results for $D=300$ and $D=500$ are shown in Fig. 3. The results obtained at $\\theta\\,=\\,0$ reflect the Type I error rate. All methods successfully control the Type I error rate $\\leq$ 0.05. LFHSIC-M stably outperforms other methods significantly as the angle increases. Method (LFHSIC-G) that simply optimizes the global bandwidth performs worse as $d$ increases, corroborating the need for more flexible kernel designs for more challenging tasks. Furthermore, comparing the results under different settings of $D$ , our method LFHSIC-M performs consistently well and exhibits progressively better performance as $D$ increases. ", "page_idx": 8}, {"type": "image", "img_path": "BEiqNQZIky/tmp/ef18ebac24f8abc4b337a055ed6a21c37c449adaef4f4b1486a719ee76ed54d1.jpg", "img_caption": ["Figure 4: The results on two real data. Left: 3DShapes. Right two: MSD Dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.2 Results on Real Data ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Settings of Two Real Data. The first real dataset used is a high-dimensional image dataset 3Dshapes as in [30]. In our experiments, we vectorize image $X$ to a vector with dimension $64\\!\\times\\!64\\!\\times\\!3=12,$ 288. The sample size is set as 128. We add standard Gaussian noise ${\\mathcal{N}}(0,1)$ to the angle $Y$ to make the setting more challenging. The Type I error rate is evaluated by the samples obtained by permutation. Besides, we consider the Million Song Data (MSD) as the second real dataset. The first dimension represents the year of release of each song and is referred to as variable $Y$ . The remaining 90- dimensional features (e.g., mean timbre and timbre covariance) constitute variable $X$ . We follow the recommended setting [44], i.e., disturbing each entry of the $X$ with an independent Gaussian noise $\\mathcal{N}(0,1000)$ . For this dataset MSD, in order to fully utilize the data, we randomly select $n\\in\\{500,1000\\}$ samples as the training set and other $n$ samples from the remaining data 100 times for the evaluation and obtain the average result. The above training and testing processes are repeated 10 times to evaluate the robustness of the optimization scheme. ", "page_idx": 9}, {"type": "text", "text": "Performance. The results of two real data with $D=10$ are presented in Fig. 4. For the results on 3Dshapes (shown in the left of Fig. 4), all methods except BHSIC and NFSIC control the Type I error well. The linear-time test has relatively lower power compared to the quadratic-time test except for LFHSIC-M, proving that its more significant features obtained in high-dimensional scenarios enable it to achieve outstanding performance even in scenarios with high approximation requirements $\\(D\\,=\\,10)$ ). Similar conclusions can drawn from the MSD dataset (shown in the right of Fig. 4). Additionally, the results for NFSIC and LFHSIC-G/M with different sample sizes indicate increased robustness of the optimization as the sample size increases (reflected in the reduction of variance), and the more flexible design also contributes to this (comparing LFHSIC-M and LFHSIC-G), thus can be more effectively applied to real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel method to efficiently learn significant Fourier feature pairs for maximizing the power of HSIC-based independence tests. By integrating a learnable Fourier feature module, we improve the flexibility of existing configurations and design a new criterion. The proposed linear-time optimization objective accurately models the power of the test and can be trained end-toend in a data-driven manner, ensuring both effectiveness and efficiency. Both theoretical results and experimental results show the effectiveness of our proposed method. Future work includes further improving the sampling method in the frequency domain. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Natural Science Foundation (NSFC) (62372116 and 62472415), and National Key Research and Development Program of China (2021YFC3340302 and 2021YFC3300304). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Albert, M., Laurent, B., Marrel, A., and Meynaoui, A. (2022). Adaptive test of independence based on hsic measures. The Annals of Statistics, 50(2):858\u2013879.   \n[2] Arcones, M. A. and Gine, E. (1992). On the bootstrap of u and v statistics. The Annals of Statistics, pages 655\u2013674.   \n[3] Bach, F. R. and Jordan, M. I. (2002). Kernel independent component analysis. Journal of machine learning research, 3(Jul):1\u201348.   \n[4] Bertin-Mahieux, T. (2011). YearPredictionMSD. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C50K61.   \n[5] Burgess, C. and Kim, H. (2018). 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/.   \n[6] Camps-Valls, G., Mooij, J., and Scholkopf, B. (2010). Remote sensing feature selection by kernel dependence measures. IEEE Geoscience and Remote Sensing Letters, 7(3):587\u2013591.   \n[7] Chatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536):2009\u20132022.   \n[8] Chwialkowski, K. P., Ramdas, A., Sejdinovic, D., and Gretton, A. (2015). Fast two-sample testing with analytic representations of probability measures. Advances in Neural Information Processing Systems, 28.   \n[9] Cohen, I., Huang, Y., Chen, J., Benesty, J., Benesty, J., Chen, J., Huang, Y., and Cohen, I. (2009). Pearson correlation coefficient. Noise reduction in speech processing, pages 1\u20134.   \n[10] Fukumizu, K., Gretton, A., Sch\u00f6lkopf, B., and Sriperumbudur, B. K. (2008). Characteristic kernels on groups and semigroups. Advances in neural information processing systems, 21.   \n[11] G\u00f6nen, M. and Alpayd\u0131n, E. (2011). Multiple kernel learning algorithms. The Journal of Machine Learning Research, 12:2211\u20132268.   \n[12] Gretton, A. (2015). A simpler condition for consistency of a kernel independence test. arXiv preprint arXiv:1501.06103.   \n[13] Gretton, A., Borgwardt, K., Rasch, M., Sch\u00f6lkopf, B., and Smola, A. (2006). A kernel method for the two-sample-problem. Advances in neural information processing systems, 19.   \n[14] Gretton, A., Fukumizu, K., Teo, C., Song, L., Sch\u00f6lkopf, B., and Smola, A. (2007). A kernel statistical test of independence. Advances in neural information processing systems, 20.   \n[15] Gretton, A., Smola, A., Bousquet, O., Herbrich, R., Belitski, A., Augath, M., Murayama, Y., Pauls, J., Sch\u00f6lkopf, B., and Logothetis, N. (2005). Kernel constrained covariance for dependence measurement. In International Workshop on Artificial Intelligence and Statistics, pages 112\u2013119. PMLR.   \n[16] Hoyer, P., Janzing, D., Mooij, J. M., Peters, J., and Sch\u00f6lkopf, B. (2008). Nonlinear causal discovery with additive noise models. Advances in neural information processing systems, 21.   \n[17] Jitkrittum, W., Szab\u00f3, Z., Chwialkowski, K. P., and Gretton, A. (2016). Interpretable distribution features with maximum testing power. Advances in Neural Information Processing Systems, 29.   \n[18] Jitkrittum, W., Szab\u00f3, Z., and Gretton, A. (2017). An adaptive test of independence with analytic kernel embeddings. In International Conference on Machine Learning, pages 1742\u20131751. PMLR.   \n[19] Kalinke, F. and Szabo, Z. (2024). The minimax rate of hsic estimation for translation-invariant kernels. arXiv preprint arXiv:2403.07735.   \n[20] Kim, I. and Schrab, A. (2023). Differentially private permutation tests: Applications to kernel methods. arXiv preprint arXiv:2310.19043.   \n[21] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.   \n[22] K\u00fcbler, J., Jitkrittum, W., Sch\u00f6lkopf, B., and Muandet, K. (2020). Learning kernel tests without data splitting. Advances in Neural Information Processing Systems, 33:6245\u20136255.   \n[23] Li, Y., Pogodin, R., Sutherland, D. J., and Gretton, A. (2021). Self-supervised learning with kernel dependence maximization. Advances in Neural Information Processing Systems, 34:15543\u201315556.   \n[24] Liu, F., Xu, W., Lu, J., Zhang, G., Gretton, A., and Sutherland, D. J. (2020). Learning deep kernels for non-parametric two-sample tests. In International conference on machine learning, pages 6316\u20136326. PMLR.   \n[25] Liu, L., Pal, S., and Harchaoui, Z. (2022). Entropy regularized optimal transport independence criterion. In International Conference on Artificial Intelligence and Statistics, pages 11247\u201311279. PMLR.   \n[26] Lopez-Paz, D., Hennig, P., and Sch\u00f6lkopf, B. (2013). The randomized dependence coefficient. Advances in neural information processing systems, 26.   \n[27] Podkopaev, A. and Ramdas, A. (2024). Sequential predictive two-sample and independence testing. Advances in Neural Information Processing Systems, 36.   \n[28] Rahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines. Advances in neural information processing systems, 20.   \n[29] Ramdas, A., Reddi, S. J., P\u00f3czos, B., Singh, A., and Wasserman, L. (2015). On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29.   \n[30] Ren, Y., Xia, Y., Zhang, H., Guan, J., and Zhou, S. (2024). Learning adaptive kernels for statistical independence tests. In International Conference on Artificial Intelligence and Statistics, pages 2494\u20132502. PMLR.   \n[31] Ren, Y., Zhang, H., Xia, Y., Guan, J., and Zhou, S. (2023). Multi-level wavelet mapping correlation for statistical dependence measurement: methodology and performance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37(5), pages 6499\u20136506.   \n[32] Schrab, A., Kim, I., Guedj, B., and Gretton, A. (2022). Efficient aggregated kernel tests using incomplete ${\\boldsymbol u}$ -statistics. Advances in Neural Information Processing Systems, 35:18793\u201318807.   \n[33] Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fukumizu, K. (2013). Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The annals of statistics, pages 2263\u20132291.   \n[34] Serfling, R. J. (2009). Approximation theorems of mathematical statistics. John Wiley & Sons.   \n[35] Shekhar, S., Kim, I., and Ramdas, A. (2023). A permutation-free kernel independence test. Journal of Machine Learning Research, 24(369):1\u201368.   \n[36] Sriperumbudur, B. K., Gretton, A., Fukumizu, K., Sch\u00f6lkopf, B., and Lanckriet, G. R. (2010). Hilbert space embeddings and metrics on probability measures. The Journal of Machine Learning Research, 11:1517\u20131561.   \n[37] Sutherland, D. J. and Schneider, J. (2015). On the error of random fourier features. arXiv preprint arXiv:1506.02785.   \n[38] Sutherland, D. J., Tung, H.-Y., Strathmann, H., De, S., Ramdas, A., Smola, A., and Gretton, A. (2016). Generative models and model criticism via optimized maximum mean discrepancy. arXiv preprint arXiv:1611.04488.   \n[39] Sz\u00e9kely, G., Rizzo, M., and Bakirov, N. (2007). Measuring and testing dependence by correlation of distances. Annals of Statistics, 35(6):2769\u20132794.   \n[40] Sz\u00e9kely, G. J. and Rizzo, M. L. (2013). The distance correlation t-test of independence in high dimension. Journal of Multivariate Analysis, 117:193\u2013213.   \n[41] Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press.   \n[42] Wang, Z., Zhan, Z., Gong, Y., Shao, Y., Ioannidis, S., Wang, Y., and Dy, J. (2023). Dualhsic: Hsicbottleneck and alignment for continual learning. In International Conference on Machine Learning, pages 36578\u201336592. PMLR.   \n[43] Zhang, K., Peters, J., Janzing, D., and Sch\u00f6lkopf, B. (2012). Kernel-based conditional independence test and application in causal discovery. arXiv preprint arXiv:1202.3775.   \n[44] Zhang, Q., Filippi, S., Gretton, A., and Sejdinovic, D. (2018). Large-scale kernel methods for independence testing. Statistics and Computing, 28:113\u2013130.   \n[45] Zhang, T., Zhang, Y., and Zhou, T. (2024). Statistical insights into hsic in high dimensions. Advances in Neural Information Processing Systems, 36. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix Organization ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 Section A: List of Symbols and Notations.   \n\u2022 Section B: Assumptions.   \n\u2022 Section C: Some Auxiliary Lemma.   \n\u2022 Section D: Proof of Proposition 1.   \n\u2022 Section E: Proof of Theorem 1.   \n\u2022 Section F: Calculation of Eq. (16).   \n\u2022 Section G: Proof of Theorem 2.   \n\u2022 Section H: Proof of Theorem 3.   \n\u2022 Section I: Smoothness of Optimization Objective.   \n\u2022 Section J: Details of Experiment Setup.   \n\u2022 Section K: Additional Experiment Results.   \n\u2022 Section L: Limitations and Broader Impacts. ", "page_idx": 12}, {"type": "text", "text": "A List of Symbols and Notations ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "BEiqNQZIky/tmp/1274eb6501299b538254bb50e53b09e95d482ef9db8d0db3db16c6bde9304795.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Assumptions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The following are the assumptions required. We denote the parameter spaces of $\\theta_{k},\\theta_{l}$ as $\\Theta_{k},\\Theta_{l}$ . ", "page_idx": 12}, {"type": "text", "text": "(a) The mapping functions $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ are Lipschitz to the parameters $\\theta_{k},\\theta_{l}$ , i.e. for all $x\\in\\mathcal{X},y\\in\\mathcal{Y}$ and for all $\\theta_{k},\\theta_{k}^{\\prime}\\in\\Theta_{k},\\theta_{l}\\overset{\\cdot\\cdot}{,}\\theta_{l}^{\\prime}\\in\\Theta_{l}$ , ", "page_idx": 12}, {"type": "text", "text": "$\\begin{array}{r}{\\|\\mathcal{T}_{\\theta_{k}}(x)-\\mathcal{T}_{\\theta_{k}^{\\prime}}(x)\\|\\le L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|,\\ \\|\\mathcal{T}_{\\theta_{l}}(y)-\\mathcal{T}_{\\theta_{l}^{\\prime}}(y)\\|\\le L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "with the nonnegative Lipschitz constant $L_{k},L_{l}$ . ", "page_idx": 12}, {"type": "text", "text": "(b) The range of the mapping functions $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ are bounded. ", "page_idx": 12}, {"type": "text", "text": "(c) The parameters $\\theta_{0},\\theta_{1}$ lie in Banach spaces of dimension $d_{k},d_{l}$ respectively. Also, the parameters $\\theta_{k},\\theta_{l}$ are bounded by $R_{\\theta_{k}},R_{\\theta_{l}}$ respectively, i.e., $\\lVert{\\boldsymbol{\\theta}}_{k}\\rVert\\leq R_{{\\boldsymbol{\\theta}}_{k}},\\lVert{\\boldsymbol{\\theta}}_{l}\\rVert\\leq R_{{\\boldsymbol{\\theta}}_{l}}$ . ", "page_idx": 12}, {"type": "text", "text": "C Some Auxiliary Lemma ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 A Useful Expression ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "$h_{i j q r}^{(\\omega)}$ .e  sWueb sseiqmupelnift yp irto bofy.  sBetyt itnhge i, , $\\begin{array}{r}{h_{i j q r}^{(\\omega)}=\\frac{1}{4!}\\sum_{(t,u,v,w)}^{(i,j,q,r)}k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{v w}^{(\\omega)}-\\bar{2}k_{t u}^{(\\omega)}l_{t v}^{(\\omega)}}\\end{array}$ $t=i$ $u=i$ $v=i$ and $w=i$ in turn. Then we can show that $h_{i j q r}^{(\\omega)}$ is equal to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{4!}\\sum_{(u,v,w)}^{(j,q,r)}\\bigl(k_{i u}^{(\\omega)}l_{i u}^{(\\omega)}+k_{i u}^{(\\omega)}l_{v w}^{(\\omega)}-2k_{i u}^{(\\omega)}l_{i v}^{(\\omega)}\\bigr)+\\frac{1}{4!}\\sum_{(t,v,w)}^{(j,q,r)}\\bigl(k_{t u}^{(\\omega)}l_{t i}^{(\\omega)}+k_{t i}^{(\\omega)}l_{v w}^{(\\omega)}-2k_{t i}^{(\\omega)}l_{t v}^{(\\omega)}\\bigr)}}\\\\ {{\\displaystyle+\\frac{1}{4!}\\sum_{(t,u,w)}^{(j,q,r)}\\bigl(k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{i w}^{(\\omega)}-2k_{t u}^{(\\omega)}l_{t i}^{(\\omega)}\\bigr)+\\frac{1}{4!}\\sum_{(t,u,v)}^{(j,q,r)}\\bigl(k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{v i}^{(\\omega)}-2k_{t u}^{(\\omega)}l_{t v}^{(\\omega)}\\bigr).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the definition, $k_{t u}^{(\\omega)}:=\\psi_{k}^{(\\omega)}\\left(\\mathcal{T}_{\\theta_{k}}x_{t}-\\mathcal{T}_{\\theta_{k}}x_{u}\\right)=\\Lambda_{X}(x_{t})\\Lambda_{X}(x_{u})^{T}$ is symmetric, i.e. $k_{t u}^{(\\omega)}=k_{u t}^{(\\omega)}$ And so as $l_{t u}^{(\\omega)}$ . Hence we can merge the identical items (marked with the same color). As a result, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{h_{i j q r}^{(\\omega)}=\\frac{1}{4!}\\sum_{(u,v,w)}^{(j,q,r)}(2k_{i u}^{(\\omega)}l_{i u}^{(\\omega)}+2k_{i u}^{(\\omega)}l_{v w}^{(\\omega)}-2k_{i u}^{(\\omega)}l_{i v}^{(\\omega)})-\\frac{1}{4!}\\sum_{(t,v,w)}^{(j,q,r)}(2k_{t i}^{(\\omega)}l_{t v}^{(\\omega)})}}\\\\ &{}&{+\\,\\frac{1}{4!}\\sum_{(t,u,w)}^{(j,q,r)}(2k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+2k_{t u}^{(\\omega)}l_{i w}^{(\\omega)}-2k_{t u}^{(\\omega)}l_{t i}^{(\\omega)})-\\frac{1}{4!}\\sum_{(t,u,v)}^{(j,q,r)}(2k_{t u}^{(\\omega)}l_{t v}^{(\\omega)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We will use Eq. (20) many times in subsequent proofs. ", "page_idx": 13}, {"type": "text", "text": "C.2 Properties of Learnable Random Fourier Feature ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Under the assumption (a), RFFs $\\psi_{k}^{(\\omega)},\\psi_{l}^{(\\omega)}$ are Lipschitz to the parameters $\\theta_{k},\\theta_{l}$ . Formally, ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. (Lipschitz Property of Fourier Feature). Let $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ be the mapping functions of $X,Y$ that are Lipschitz to the parameters $\\theta_{k},\\theta_{l}$ with the non-negative constant $L_{k},L_{l}$ , respectively. Let $\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ be the samplings of frequency with the sampling number $D$ . Also, we define $R_{\\omega_{k}}\\,:=\\,\\operatorname*{sup}_{j}\\|\\omega_{k;j}\\|,R_{\\omega_{l}}\\,:=\\,\\operatorname*{sup}_{j}\\|\\omega_{l;j}\\|$ , then for the RFFs \u03c8(k\u03c9), \u03c8l(\u03c9)with mapping functions $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ and frequency samplings $\\bar{\\{(\\omega_{k;j},\\omega_{l;j})\\}}_{j=1}^{D/2}$ , for all $(x,x^{\\prime})\\in\\mathcal{X}\\times\\mathcal{X},(y,y^{\\prime})\\in\\mathcal{Y}\\times\\mathcal{Y}$ and for all $\\theta_{k},\\theta_{k}^{\\prime}\\in\\Theta_{k},\\theta_{l},\\theta_{l}^{\\prime}\\in\\Theta_{l},$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\psi_{k}^{(\\omega)}(\\Delta_{x,x^{\\prime}})-\\psi_{k}^{(\\omega)}(\\Delta_{x,x^{\\prime}}^{\\prime})\\|\\leq2R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|,}\\\\ &{\\|\\psi_{l}^{(\\omega)}(\\Delta_{y,y^{\\prime}})-\\psi_{l}^{(\\omega)}(\\Delta_{y,y^{\\prime}}^{\\prime})\\|\\leq2R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Delta_{x,x^{\\prime}}:=\\mathcal{T}_{\\theta_{k}}x-\\mathcal{T}_{\\theta_{k}}x^{\\prime}$ , $\\Delta_{x,x^{\\prime}}^{\\prime}:=\\mathcal{T}_{\\theta_{k}^{\\prime}}x-\\mathcal{T}_{\\theta_{k}^{\\prime}}x^{\\prime}$ and $\\Delta_{y,y^{\\prime}},\\Delta_{y,y^{\\prime}}^{\\prime}$ are defined by analogy. ", "page_idx": 13}, {"type": "text", "text": "Proof. We prove the result for $\\psi_{k}^{(\\omega)}$ only since the proof for $\\psi_{l}^{(\\omega)}$ can be obtained in the same way. We start by recall the definition $\\begin{array}{r}{\\psi_{k}^{(\\omega)}(\\Delta_{x,x^{\\prime}}):=\\frac{2}{D}\\sum_{j=1}^{D/2}\\cos(\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}})}\\end{array}$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\psi_{k}^{(\\omega)}(\\Delta_{x,x^{\\prime}})-\\psi_{k}^{(\\omega)}(\\Delta_{x,x^{\\prime}}^{\\prime})\\|=\\Big\\|\\displaystyle\\frac{2}{D}\\sum_{j=1}^{D/2}\\cos(\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}})-\\displaystyle\\frac{2}{D}\\sum_{j=1}^{D/2}\\cos(\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}}^{\\prime})\\Big\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2}{D}\\sum_{j=1}^{D/2}\\|\\cos(\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}})-\\cos(\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}}^{\\prime})\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since the cosine function is bounded by 1, by the mean value theorem, for fixed $j$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\cos(\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}})-\\cos(\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}}^{\\prime})\\|\\leq|\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}}-\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}}^{\\prime}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then according to the Cauchy\u2013Schwarz inequality, ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}}-\\omega_{k;j}^{T}\\Delta_{x,x^{\\prime}}^{\\prime}|\\leq\\|\\omega_{k;j}\\|\\cdot\\|\\Delta_{x,x^{\\prime}}-\\Delta_{x,x^{\\prime}}^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the definition of $\\Delta_{x,x^{\\prime}}$ and the Lipschitz property of the mapping functions $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta_{x,x^{\\prime}}-\\Delta_{x,x^{\\prime}}^{\\prime}\\|=\\|(\\mathcal{T}_{\\theta_{k}}x-\\mathcal{T}_{\\theta_{k}}x^{\\prime})-(\\mathcal{T}_{\\theta_{k}^{\\prime}}x-\\mathcal{T}_{\\theta_{k}^{\\prime}}x^{\\prime})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathcal{T}_{\\theta_{k}}x-\\mathcal{T}_{\\theta_{k}^{\\prime}}x\\|+\\|\\mathcal{T}_{\\theta_{k}}x^{\\prime}-\\mathcal{T}_{\\theta_{k}^{\\prime}}x^{\\prime}\\|\\leq2L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining the above results, we complete the proof. ", "page_idx": 14}, {"type": "text", "text": "Under the assumption (b), we can obtain the uniform convergence property as follows. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. (Uniform Convergence of Fourier Features). Let the mapping function of $X,Y$ with parameters $\\theta_{k},\\theta_{l}$ be $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ , and the corresponding range space be a compact subset of $\\mathbb{R}^{d_{T_{x}}},\\mathbb{R}^{d_{T_{y}}}$ , respectively. Also, the diameter of two range spaces is denoted by diam $(\\mathcal{T}_{\\theta_{k}}),d i a m(\\mathcal{T}_{\\theta_{l}})$ , respectively. Let $\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ be the samplings of frequency with the sampling number $D$ , then for the $R F F s$ with mapping functions $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ and frequency samplings $\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal{X}}|\\Lambda_{k}(x)^{T}\\Lambda_{k}(x^{\\prime})-k(x,x^{\\prime})|\\ge\\epsilon\\right]\\le2^{8}\\left(\\frac{\\sigma_{\\omega_{k}}d i a m(\\mathcal{T}_{\\theta_{k}})}{\\epsilon}\\right)^{2}\\exp\\left(-\\frac{D\\epsilon^{2}}{4(d_{\\mathcal{T}_{x}}+2)}\\right),}\\\\ {\\mathbb{P}\\left[\\operatorname*{sup}_{y,y^{\\prime}\\in\\mathcal{Y}}|\\Lambda_{l}(y)^{T}\\Lambda_{l}(y^{\\prime})-l(y,y^{\\prime})|\\ge\\epsilon\\right]\\le2^{8}\\left(\\frac{\\sigma_{\\omega_{l}}d i a m(\\mathcal{T}_{\\theta_{l}})}{\\epsilon}\\right)^{2}\\exp\\left(-\\frac{D\\epsilon^{2}}{4(d_{\\mathcal{T}_{y}}+2)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second moment of frequency samplings $\\sigma_{\\omega_{k}}^{2}:=\\mathbf{E}_{p_{k}(\\omega)}[\\omega_{k;j}^{T}\\omega_{k;j}],\\,\\sigma_{\\omega_{l}}^{2}:=\\mathbf{E}_{p_{l}(\\omega)}[\\omega_{l;j}^{T}\\omega_{l;j}].$ ", "page_idx": 14}, {"type": "text", "text": "Proof. Based on the derivation of RFFs in Sec. 4.1, We can view it as if the frequency sampling process is performed after the range space is obtained. Since the convergence bounds of the sampling process can be obtained directly through the results of [28, Claim 1], by replacing the input space in [28, Claim 1] to the range space here, then this part of the proof can be completed. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Remark. Combining the technique in [37], the constants in bounds can be further improved. ", "page_idx": 14}, {"type": "text", "text": "C.3 Approximation Error Bound ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $\\mathrm{HSIC}_{\\omega}^{(u)}(Z)$ , also denoted as \u03be(\u03c9u ), be the U-statistic that corresponding to HSIC\u03c9(Z), i.e., HSIC(\u03c9u )(Z) := (n1)4 (i,j,q,r)\u2208i4n hijqr . The population value of $\\mathrm{HSIC}_{\\omega}^{(u)}(Z)$ is given by $\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}$ which can be viewed as the result obtained after a frequency sampling approximation on $\\mathrm{HSIC}(X,Y)$ is performed. The bound of approximation error is given by the following Lemma. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3. (Approximation Error Bound). For simplify, we denote $\\Lambda_{k}(x)^{T}\\Lambda_{k}(x^{\\prime}),\\Lambda_{l}(y)^{T}\\Lambda_{l}(y^{\\prime})$ as $k^{(\\omega)}(x,x^{\\prime}),l^{(\\omega)}(y,y^{\\prime})$ , respectively. Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}-H S I C(X,Y)|\\leq4\\cdot\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal{X},y,y^{\\prime}\\in\\mathcal{Y}}|k^{(\\omega)}(x,x^{\\prime})l^{(\\omega)}(y,y^{\\prime})-k(x,x^{\\prime})l(y,y^{\\prime})|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We first represent $\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}$ in the form corresponding to Eq. (1), i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{E}}_{Z}\\xi_{\\omega}^{(u)}={\\mathbf{E}}_{X X^{\\prime}Y Y^{\\prime}}\\big[k^{(\\omega)}(X,X^{\\prime})l^{(\\omega)}(Y,Y^{\\prime})\\big]\\!+\\!{\\mathbf{E}}_{X X^{\\prime}}\\big[k^{(\\omega)}(X,X^{\\prime})\\big]{\\mathbf{E}}_{Y Y^{\\prime}}\\big[l^{(\\omega)}(Y,Y^{\\prime})\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\ 2{\\mathbf{E}}_{X^{\\prime}Y^{\\prime}}\\big[{\\mathbf{E}}_{X}k^{(\\omega)}(X,X^{\\prime}){\\mathbf{E}}_{Y}l^{(\\omega)}(Y,Y^{\\prime})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking one of the items as an example and comparing it to the corresponding item in Eq. (1), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\big|\\mathbf{E}_{X^{\\prime}Y^{\\prime}}\\big[\\mathbf{E}_{X}k^{(\\omega)}(X,X^{\\prime})\\mathbf{E}_{Y}l^{(\\omega)}(Y,Y^{\\prime})\\big]-\\mathbf{E}_{X^{\\prime}Y^{\\prime}}\\big[\\mathbf{E}_{X}k(X,X^{\\prime})\\mathbf{E}_{Y}l(Y,Y^{\\prime})\\big]\\big|}\\\\ &{\\leq\\!\\int_{X^{\\prime},Y^{\\prime}}\\!\\int_{X}\\int_{Y}|k^{(\\omega)}(X,X^{\\prime})l^{(\\omega)}(Y,Y^{\\prime})-k(X,X^{\\prime})l(Y,Y^{\\prime})|d\\mathbb{P}_{X}d\\mathbb{P}_{Y}d\\mathbb{P}_{X^{\\prime}Y^{\\prime}}}\\\\ &{\\leq\\!\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal{X},y,y^{\\prime}\\in\\mathcal{Y}}|k^{(\\omega)}(x,x^{\\prime})l^{(\\omega)}(y,y^{\\prime})-k(x,x^{\\prime})l(y,y^{\\prime})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The results can be obtained for the other terms in a similar way, which completes the proof. ", "page_idx": 14}, {"type": "text", "text": "D Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we give a proof of the Proposition 1. We first restate the Proposition 1 here. ", "page_idx": 15}, {"type": "text", "text": "Proposition 1 (Asymptotics). Let $\\begin{array}{r}{h_{i j q r}^{(\\omega)}:=\\frac{1}{4!}\\sum_{(t,u,v,w)}^{(i,j,q,r)}k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{v w}^{(\\omega)}-2k_{u v}^{(\\omega)}l_{t v}^{(\\omega)}}\\end{array}$ , where the sum represents all ordered quadruples $(t,u,v,w)$ drawn without replacement from $(i,j,q,r)$ . Then, Under the null hypothesis $\\mathcal{H}_{0}$ , $H S I C_{\\omega}(Z)$ coverages in distribution to ", "page_idx": 15}, {"type": "equation", "text": "$$\nn H S I C_{\\omega}(Z)\\stackrel{d}{\\rightarrow}\\sum_{l=1}^{\\infty}\\lambda_{l}\\chi_{1l}^{2},\\;\\;\\lambda_{l}g_{l}(z_{j})=\\int_{z_{i},z_{q},z_{r}}h_{i j q r}^{(\\omega)}g_{l}(z_{i})d F_{z_{i},z_{q},z_{r}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\chi_{11}^{2},\\chi_{12}^{2}.$ , ... are independent $\\chi_{1}^{2}$ variates and $\\lambda_{l}$ is the solution to the eigenvalue problem as in the right of Eq. $(30)$ . Also, under the alternative $\\mathcal{H}_{1}$ , $H S I C_{\\omega}(Z)$ converges in distribution as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n^{\\frac{1}{2}}\\Big(H S I C_{\\omega}(Z)-\\mathbf{E}_{Z}H S I C_{\\omega}(Z)\\Big)\\frac{d}{\\omega}\\mathcal{N}(0,\\sigma_{\\omega}^{2}),~\\sigma_{\\omega}^{2}=16\\Big[\\mathbf{E}_{i}(\\mathbf{E}_{j,q,r}h_{i j q r}^{(\\omega)})^{2}-\\big(\\mathbf{E}_{Z}h_{i j q r}^{(\\omega)}\\big)^{2}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with the simplified notation $\\mathbf{E}_{j,q,r}:=\\mathbf{E}_{z_{j},z_{q},z_{r}}$ and $\\mathbf{E}_{Z}:=\\mathbf{E}_{z_{i},z_{j},z_{q},z_{r}}.$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof is mainly based on [34, Chapter 5]. A proof for a similar result has been given in [14, Theorem 1,2]. The difference is that we consider the asymptotic distributions of $\\mathrm{HSIC}_{\\omega}(Z)$ that used learnable RFF thus is a function of frequency samplings while they consider $\\mathrm{HSIC}_{b}(Z)$ . Thus some steps need to be modified. ", "page_idx": 15}, {"type": "text", "text": "Step 1: we show that $\\mathrm{HSIC}_{\\omega}(Z)$ is a ${\\mathrm{V}}.$ -statistic, this can be done since it can be expressed as $\\begin{array}{r}{\\mathrm{HSIC}_{\\omega}(Z)=\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}^{(\\omega)}}\\end{array}$ . To facilitate the conclusions in [34] for the U-statistic, we define the U-statistic HSIC(\u03c9u )(Z) :=(n1)4 (i,j,q,r)\u2208in that corresponds to $\\mathrm{HSIC}_{\\omega}(Z)$ . ", "page_idx": 15}, {"type": "text", "text": "Step 2: Then we prove the result under $\\mathcal{H}_{0}$ . To begin with, we first show that $\\mathbf{E}_{j,q,r}h_{i j q r}^{(\\omega)}=0$ with $(i,j,q,r)\\in\\mathbf{i}_{4}^{n}$ under $\\mathcal{H}_{0}$ . According to Eq. (20), we can calculate $\\mathbf{E}_{j,q,r}h_{i j q r}^{(\\omega)}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}_{j,q,r}h_{i j q r}^{(\\omega)}=\\frac{2}{4!}\\displaystyle\\sum_{(u,v,w)}^{(j,q,r)}\\mathbf{E}_{u,v,w}(k_{i u}^{(\\omega)}l_{i u}^{(\\omega)}+k_{i u}^{(\\omega)}l_{v w}^{(\\omega)}-k_{i u}^{(\\omega)}l_{i v}^{(\\omega)})-\\frac{2}{4!}\\displaystyle\\sum_{(t,v,w)}^{(j,q,r)}\\mathbf{E}_{t,v,w}(k_{t i}^{(\\omega)}l_{t v}^{(\\omega)})}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}+\\frac{2}{4!}\\displaystyle\\sum_{(t,u,w)}^{(j,q,r)}\\mathbf{E}_{t,u,w}(k_{t u}^{(\\omega)}l_{t v}^{(\\omega)}+k_{t u}^{(\\omega)}l_{i w}^{(\\omega)}-k_{t u}^{(\\omega)}l_{t i}^{(\\omega)})-\\frac{2}{4!}\\displaystyle\\sum_{(t,u,v)}^{(j,q,r)}\\mathbf{E}_{t,u,v}(k_{t u}^{(\\omega)}l_{t v}^{(\\omega)})}\\\\ &{\\phantom{x x x x x x}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "rwehaedraeb ilwitey ,d ewfein ew mrepqliufirieed  andodtiitoionns $\\mathbf{E}_{u,v,w}^{i\\neq u\\neq v\\neq w}$ : iacnatde $\\mathbf{E}_{x}k_{i}^{(\\omega)}:=\\mathbf{E}_{u}^{i\\neq u}k_{i u}^{(\\omega)}$ $\\mathbf{E}_{x}k^{(\\omega)}:=\\mathbf{E}_{t,u}^{t\\neq u}k_{t u}^{(\\omega)}$ $Y$ is defined by analogy). Under $\\mathcal{H}_{0}$ , $X$ and $Y$ are independence. Hence ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2\\mathbf{E}_{j,q,r}h_{i j q r}^{(\\omega)}=\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y}l_{i}^{(\\omega)}+\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}-\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y}l_{i}^{(\\omega)}-\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}}\\\\ {+\\;\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}+\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l_{i}^{(\\omega)}-\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l_{i}^{(\\omega)}-\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then combining the results [34, Section 5.5.2], we can prove Eq. (30). ", "page_idx": 15}, {"type": "text", "text": "Step 3: Next we prove the asymptotic distribution under $\\mathcal{H}_{1}$ . We only need to show that $\\lvert\\mathrm{HSIC}_{\\omega}(Z)-$ $\\mathrm{HSIC}_{\\omega}^{(u)}(Z)|\\sim{\\mathcal O}(1/n)$ . By the definition of $k_{t u}^{(\\omega)},l_{t u}^{(\\omega)}$ , we can check that $|k_{t u}^{(\\omega)}|\\leq1,|l_{t u}^{(\\omega)}|\\leq1$ for all $t,u$ , thus $|h_{i j q r}^{(\\omega)}|\\leq4$ for all $i,j,q,r$ . Hence we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathrm{HSIC}_{\\omega}(Z)-\\mathrm{HSIC}_{\\omega}^{(u)}(Z)|\\le\\frac{n^{4}-(n)_{4}}{n^{4}}\\cdot4+\\Big(\\frac{1}{(n)_{4}}-\\frac{1}{n^{4}}\\Big)\\cdot(n)_{4}\\cdot4\\sim\\mathcal{O}(1/n).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining the results [34, Section 5.5.1], we can prove Eq. (31). ", "page_idx": 15}, {"type": "text", "text": "E Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we give a proof of the Theorem 1. We first restate the Theorem 1 here. ", "page_idx": 16}, {"type": "text", "text": "Theorem 1 (Linear-Time Estimators). Under $\\mathcal{H}_{0}$ , the estimators of mean and variance with bias of $O(n^{-1})$ to ${\\bf E}_{Z}[n H S I C_{\\omega}(Z)]$ and $W\\!a r_{Z}[n H S I C_{\\omega}(Z)]$ , denote as ${\\mathcal{E}}_{0}$ and ${\\dot{\\nu}}_{0}$ , respectively, are given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{z}_{0}:=\\frac{[\\mathbf{1}^{T}\\mathbf{A}_{X_{C}}^{\\cdot2}\\mathbf{1}][\\mathbf{1}^{T}\\mathbf{A}_{Y_{C}}^{\\cdot2}\\mathbf{1}]}{(n-1)^{2}},\\mathcal{V}_{0}:=\\frac{2n(n-4)(n-5)}{(n-1)(n-2)(n-3)}\\frac{[\\mathbf{1}^{T}(\\mathbf{1}_{X_{C}}^{T}\\mathbf{A}_{X_{C}})^{\\cdot2}\\mathbf{1}][\\mathbf{1}^{T}(\\mathbf{A}_{Y_{C}}^{T}\\mathbf{A}_{Y_{C}})^{\\cdot2}\\mathbf{1}]}{n^{4}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $()^{\\cdot2}$ is the entrywise matrix power. Both $\\mathcal{E}_{0}$ and $\\protect\\nu_{0}$ can be calculated in $\\mathcal{O}(n D^{2})$ time. ", "page_idx": 16}, {"type": "text", "text": "Proof. We first prove the part of the mean. Recall the definition of $\\mathrm{HSIC}_{\\omega}(Z)$ , we have $\\mathrm{HSIC}_{\\omega}(Z)=$ $\\begin{array}{r}{\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}^{(\\omega)}}\\end{array}$ . Hence $\\begin{array}{r}{\\mathbf{E}_{Z}[\\mathrm{HSIC}_{\\omega}(Z)]=\\frac{1}{n^{4}}\\sum_{i,j,q,r}\\mathbf{E}_{Z}h_{i j q r}^{(\\omega)}}\\end{array}$ . When $(i,j,q,r)\\in\\mathbf{i}_{4}^{n}$ , then we can show that under H0, Ei,j,q,rh $\\mathbf{E}_{i,j,q,r}h_{i j q r}^{(\\omega)}=\\mathbf{\\Gamma}0$ by performing the same analysis as in Eqs. (32) and (33). Then we consider the case where exactly two elements of $i,j,q,r$ are the same, for a total of $6n(n-1)(n-2)$ terms. By the symmetry of $h_{i j q r}^{(\\omega)}$ , the expectation of these terms all take the same value, and here we take $h_{i i q r}^{(\\omega)}$ as an example. According to Eq. (20), we can represent $h_{i i q r}^{(\\omega)}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{{h_{i i q r}^{(\\omega)}=\\frac{2}{4!}\\sum_{(u,v,w)}^{2}(k_{i u}^{(\\omega)}l_{i u}^{(\\omega)}+k_{i u}^{(\\omega)}l_{v w}^{(\\omega)}-k_{i u}^{(\\omega)}l_{i v}^{(\\omega)})-\\frac{2}{4!}\\sum_{(t,v,w)}^{(i,q,r)}(k_{t i}^{(\\omega)}l_{t v}^{(\\omega)})}}}\\\\ &{}&{\\mathrm{~}}&{\\mathrm{~}}&{\\mathrm{~}}\\\\ &{}&{\\mathrm{~}}&{\\mathrm{~}+\\frac{2}{4!}\\sum_{(t,u,w)}^{(i,q,r)}(k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{i w}^{(\\omega)}-k_{t u}^{(\\omega)}l_{t i}^{(\\omega)})-\\frac{2}{4!}\\sum_{(t,u,v)}^{(i,q,r)}(k_{t u}^{(\\omega)}l_{t v}^{(\\omega)}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Under $\\mathcal{H}_{0},X$ and $Y$ are independence. Take the expectation on both sides, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{12\\mathbf{E}_{i,q,r}h_{i i q r}^{(\\omega)}=(2+4\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)})+(2\\mathbf{E}_{y}l^{(\\omega)}+4\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)})}\\\\ &{\\qquad\\qquad\\qquad-\\left(2\\mathbf{E}_{x}k^{(\\omega)}+2\\mathbf{E}_{y}l^{(\\omega)}+2\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}\\right)-(2\\mathbf{E}_{y}l^{(\\omega)}+4\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)})}\\\\ &{\\qquad\\qquad\\qquad+\\left(6\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}\\right)+\\left(2\\mathbf{E}_{x}k^{(\\omega)}+4\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}\\right)}\\\\ &{\\qquad\\qquad\\qquad-\\left(2\\mathbf{E}_{x}k^{(\\omega)}+4\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}\\right)-\\left(6\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}\\right)}\\\\ &{\\qquad\\qquad\\qquad=2(1-\\mathbf{E}_{x}k^{(\\omega)}-\\mathbf{E}_{y}l^{(\\omega)}+\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we define additional notation $\\mathbf{E}_{x}k^{(\\omega)}:=\\mathbf{E}_{t,u}^{t\\ne u}k_{t u}^{(\\omega)}$ (the notation for $Y$ is defined by analogy) and use $k_{t t}^{\\omega}=l_{t t}^{\\omega}=1$ . Hence in this case, the sum of the contributions of all terms to ${\\bf E}_{Z}[n\\mathrm{HSIC}_{\\omega}(Z)]$ is $(1-\\overleftarrow{\\mathbf{E}_{x}}k^{(\\omega)}-\\mathbf{E}_{y}l^{(\\omega)}+\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)})+\\mathcal{O}(1/n)$ . For the remaining terms, i.e., the case where at least three of $i,j,q,r$ are equal, combined with the boundedness of hi(j\u03c9q)r, we can conclude that the sum of their contributions is ${\\mathcal{O}}(1/n)$ . As a result, we have shown that ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf E}_{Z}[n\\mathrm{HSIC}_{\\omega}(Z)]=(1-{\\bf E}_{x}k^{(\\omega)})(1-{\\bf E}_{y}l^{(\\omega)})+{\\mathcal O}(n^{-1}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The unbiased estimators of $\\mathbf{E}_{x}k^{(\\omega)},\\mathbf{E}_{y}l^{(\\omega)}$ are given by $\\mathbf{1}^{T}(\\mathbf{A}_{X}\\mathbf{A}_{X}^{T}-\\mathbf{I}_{n})\\mathbf{1},\\mathbf{1}^{T}(\\mathbf{A}_{Y}\\mathbf{A}_{Y}^{T}-\\mathbf{I}_{n})\\mathbf{1}$ , respectively. Hence under $\\mathcal{H}_{0}$ , we obtain the estimator of mean with bias of $O(n^{-1})$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[1-\\frac{\\mathbf{1}^{T}(\\mathbf{\\Lambda}_{X}\\mathbf{A}_{X}^{T}-\\mathbf{I}_{n})\\mathbf{1}}{n(n-1)}\\right]\\left[1-\\frac{\\mathbf{1}^{T}(\\mathbf{\\Lambda}_{Y}\\mathbf{A}_{Y}^{T}-\\mathbf{I}_{n})\\mathbf{1}}{n(n-1)}\\right]=\\frac{[\\mathbf{1}^{T}\\mathbf{\\Lambda}_{X_{c}}^{\\mathbf{2}}\\mathbf{1}][\\mathbf{1}^{T}\\mathbf{\\Lambda}_{Y_{c}}^{\\mathbf{2}}\\mathbf{1}]}{(n-1)^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we prove the part of the variance. We start by calculating $\\mathbf{Var}_{Z}[n\\mathrm{HSIC}_{\\omega}^{(u)}(Z)]$ , where the Ustatistic HSIC(\u03c9u )(Z) :=(n1)4 (i,j,q,r)\u2208i4n ijqr . According to the results [34, Section 5.2.1, Lemma A], we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Var}[\\mathrm{HSIC}_{\\omega}^{(u)}(Z)]=\\binom{n}{4}^{-1}\\sum_{c=1}^{4}\\binom{4}{c}\\binom{n-4}{4-c}\\zeta_{c}=\\frac{4\\binom{n-4}{3}}{\\binom{n}{4}}\\zeta_{1}+\\frac{6\\binom{n-4}{2}}{\\binom{n}{4}}\\zeta_{2}+\\mathcal{O}(n^{-3}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\zeta_{1}:=\\mathbf{E}_{i}\\left(\\mathbf{E}_{j,q,r}h_{i j q r}^{(\\omega)}\\right)^{2}$ and $\\zeta_{2}:=\\mathbf{E}_{i,j}\\left(\\mathbf{E}_{q,r}h_{i j q r}^{(\\omega)}\\right)^{2}$ . Under $\\mathcal{H}_{0}$ , when $(i,j,q,r)\\in\\mathbf{i}_{4}^{n}$ , we can show that Ej,q,rhi(j\u03c9q)r For calculatin fboyc upse rofno rtmhie ntge rthme .l ysWise  auss ei nE Eq.q s(.2 (03) 2a) gaanind, (33), thus $\\zeta_{1}=0$ . $\\zeta_{2}$ $\\mathbf{E}_{q,r}h_{i j q r}^{(\\omega)}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{12h_{i j q r}^{(\\omega)}=\\displaystyle\\sum_{(u,v,w)}^{(j,q,r)}\\big(k_{i u}^{(\\omega)}l_{i u}^{(\\omega)}+k_{i u}^{(\\omega)}l_{v w}^{(\\omega)}-k_{i u}^{(\\omega)}l_{i v}^{(\\omega)}\\big)-\\displaystyle\\sum_{(t,v,w)}^{(j,q,r)}\\big(k_{t i}^{(\\omega)}l_{t v}^{(\\omega)}\\big)}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{(t,u,w)}^{(j,q,r)}\\big(k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{i w}^{(\\omega)}-k_{t u}^{(\\omega)}l_{t i}^{(\\omega)}\\big)-\\displaystyle\\sum_{(t,u,v)}^{(j,q,r)}\\big(k_{t u}^{(\\omega)}l_{t v}^{(\\omega)}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Under $\\mathcal{H}_{0}$ , $X$ and $Y$ are independence. Take the expectation $\\mathbf{E}_{q,r}$ on both sides, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{12\\mathbf{E}_{q,r}h_{i j q r}^{(\\omega)}=(2k_{i j}^{(\\omega)}l_{i j}^{(\\omega)}+4\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y}l_{i}^{(\\omega)})+(2k_{i j}^{(\\omega)}\\mathbf{E}_{y}l^{(\\omega)}+4\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)})}\\\\ &{\\hphantom{(}-(2k_{i j}^{(\\omega)}\\mathbf{E}_{y}l_{i}^{(\\omega)}+2\\mathbf{E}_{x}k_{i}^{(\\omega)}l_{i j}^{(\\omega)}+2\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)})}\\\\ &{\\hphantom{(}-(2k_{i j}^{(\\omega)}\\mathbf{E}_{y l}l_{j}^{(\\omega)}+2\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)}+2\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)})}\\\\ &{\\hphantom{(}-(2k_{x}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)}+2\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)})+2\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)}}\\\\ &{\\hphantom{(}+(4\\mathbf{E}_{x}k_{j}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)}+2\\mathbf{E}_{x}k_{j}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)})+(4\\mathbf{E}_{x}k_{j}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)}+2\\mathbf{E}_{x}k^{(\\omega)}l_{i j}^{(\\omega)})}\\\\ &{\\hphantom{(}-(2\\mathbf{E}_{x}k_{j}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)}+2\\mathbf{E}_{x}k_{j}^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)}+2\\mathbf{E}_{x}k^{(\\omega)}\\mathbf{E}_{y l}^{(\\omega)})}\\\\ &{\\hphantom{(}-(2\\mathbf{E}_{x}k_{j}^{(\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we define additional notation $\\mathbf{E}_{x}k^{(\\omega)}:=\\mathbf{E}_{t,u}^{t\\ne u}k_{t u}^{(\\omega)},\\mathbf{E}_{x}k_{j}^{(\\omega)}:=\\mathbf{E}_{u}^{j\\ne u}k_{j u}^{(\\omega)}$ (the notations for $Y$ .e  Hdeenncotee $k_{c;i j}^{(\\omega)}:=k_{i j}^{(\\omega)}-\\mathbf{E}_{x}k_{i}^{(\\omega)}-\\mathbf{E}_{x}k_{j}^{(\\omega)}+\\mathbf{E}_{x}k^{(\\omega)}$ and $l_{c;i j}^{(\\omega)}:=l_{i j}^{(\\omega)}-\\mathbf{E}_{y}l_{i}^{(\\omega)}-\\mathbf{E}_{y}l_{j}^{(\\omega)}+\\mathbf{E}_{y}l^{(\\omega)}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{Var}[n\\mathrm{HSIC}_{\\omega}^{(u)}(Z)]=\\frac{2n(n-4)(n-5)}{(n-1)(n-2)(n-3)}\\mathbf{E}_{i,j}(k_{c;i j}^{(\\omega)})^{2}\\cdot\\mathbf{E}_{i,j}(l_{c;i j}^{(\\omega)})^{2}+\\mathcal{O}(n^{-1}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since under $\\mathcal{H}_{0}$ , the bias lead by the difference terms between $\\mathrm{HSIC}_{\\omega}^{(u)}(Z)$ and $\\mathrm{HSIC}_{\\omega}(Z)$ vanish faster than Eq. (43), hence the variance of $\\mathrm{HSIC}_{\\omega}(Z)$ is identical. In the following part, we consider the empirical estimate of the leading term in Eq. (43). We estimate $k_{c;i j}^{(\\omega)}$ with $(\\mathbf{\\mathbf{A}}_{X c}\\mathbf{\\mathbf{A}}_{X c}^{T})_{i j}$ , then the estimation of $\\mathbf{E}_{i,j}(k_{c;i j}^{(\\omega)})^{2}$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\sum_{i,j}\\big[\\big(\\Lambda_{k}(x_{i})-\\overline{{\\Lambda}}_{k}\\big)\\big(\\Lambda_{k}(x_{j})-\\overline{{\\Lambda}}_{k}\\big)^{T}\\big]^{2}\\!=\\frac{1}{n^{2}}\\sum_{i,j}\\!\\big(\\mathbf{\\Lambda}\\mathbf{\\Lambda}_{X\\mathcal{C}}\\mathbf{\\Lambda}_{\\Lambda\\mathcal{X}_{\\mathcal{C}}}^{T}\\big)_{i j}^{2}=\\frac{\\mathbf{1}^{T}\\big(\\mathbf{\\Lambda}\\mathbf{\\Lambda}_{X\\mathcal{C}}\\mathbf{\\Lambda}_{X\\mathcal{C}}^{T}\\big)^{\\cdot2}\\mathbf{1}}{n^{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we define notions $\\begin{array}{r}{\\overline{{\\Lambda}}_{k}:=\\frac{1}{n}\\sum_{u=1}^{n}\\Lambda_{k}(x_{u})}\\end{array}$ . Since computing the value of $\\mathbf{1}^{T}(\\mathbf{A}_{X c}\\mathbf{A}_{X c}^{T})^{\\cdot2}\\mathbf{1}$ requires $O(n^{2})$ time complexity, we transform it into a more computationally tractable form. We perform the following calculating as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{1}^{T}\\big(\\mathbf{A}_{X c}\\mathbf{A}_{X c}^{T}\\big)^{:2}\\mathbf{1}=\\mathbf{Tr}\\big(\\mathbf{A}_{X c}\\mathbf{A}_{X c}^{T}\\mathbf{A}_{X c}\\mathbf{A}_{X c}^{T}\\mathbf{\\Phi}\\big)=\\mathbf{Tr}\\big(\\mathbf{A}_{X c}^{T}\\mathbf{A}_{X c}\\mathbf{A}_{X c}^{T}\\mathbf{A}_{X c}\\big)=\\mathbf{1}^{T}\\big(\\mathbf{A}_{X c}^{T}\\mathbf{A}_{X c}\\big)^{:2}\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall the definition of $\\mathbf{A}_{X c}:=[\\mathbf{H}\\mathbf{A}_{X}]_{n\\times D}$ that can be calculated in $\\mathcal{O}(n D)$ time, thus the term $[\\mathbf{A}_{X c}^{T}\\mathbf{A}_{X c}]_{D\\times D}$ can be calculated in $\\bar{\\mathcal{O}}(n D+n D^{2})$ time. As a result, we obtain the estimator $[\\mathbf{1}^{T}(\\mathbf{\\Lambda}_{X c}^{T}\\mathbf{\\Lambda}_{}_{}\\mathbf{\\Lambda}_{X c})^{\\cdot2}\\mathbf{1}][\\mathbf{1}^{T}(\\mathbf{\\Lambda}_{Y c}^{T}\\mathbf{\\Lambda}_{}\\mathbf{\\Lambda}_{Y c})^{\\cdot2}\\mathbf{1}]$ for the term $\\mathbf{E}_{i,j}(k_{c;i j}^{(\\omega)})^{2}\\cdot\\mathbf{E}_{i,j}(l_{c;i j}^{(\\omega)})^{2}$ that can be calculated in $\\mathcal{O}(n D^{2})$ time. The only thing left to do is to determine the bias of the estimator. For readable, we define $\\begin{array}{r}{\\widehat{k}_{i j}^{(\\omega)}:=k_{i j}^{(\\omega)},\\widehat{k}_{i}^{(\\omega)}:=\\frac{1}{n}\\sum_{u}k_{i u}^{(\\omega)}}\\end{array}$ and $\\begin{array}{r}{\\widehat{k}^{\\left(\\omega\\right)}:=\\frac{1}{n^{2}}\\sum_{u,v}k_{u v}^{\\left(\\omega\\right)}}\\end{array}$ , then by removing the terms with $i=j$ , a estimate with difference $O(n^{-1})$ to Eq. (44) is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{n(n-1)}\\sum_{i\\neq j}\\left[\\widehat{k}_{i j}^{(\\omega)}-\\widehat{k}_{i}^{(\\omega)}-\\widehat{k}_{j}^{(\\omega)}+\\widehat{k}^{(\\omega)}\\right]^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By comparing the difference between the expectation of Eq. (46) and $\\mathbf{E}_{i,j}(k_{c;i j}^{(\\omega)})^{2}$ , we can show that this error is bound by $O(1/n)$ . We illustrate this by taking one of the cross terms as an example and the other terms by analogy, as shown in the following, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\Big[\\frac{1}{n(n-1)}\\displaystyle\\sum_{i\\neq j}\\widehat{k}_{i}^{(\\omega)}\\widehat{k}^{(\\omega)}\\Big]\\!=\\frac{1}{n^{3}(n-1)}\\mathbf{E}\\Big[\\displaystyle\\sum_{i}\\sum_{q,r}\\sum_{u}k_{i u}^{(\\omega)}k_{q r}^{(\\omega)}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle=\\frac{1}{(n)_{4}}\\mathbf{E}\\Big[\\displaystyle\\sum_{(i,q,r,u)\\in\\mathbb{i}_{4}^{n}}k_{i u}^{(\\omega)}k_{q r}^{(\\omega)}\\Big]\\!+\\!\\mathcal{O}(n^{-1})=\\mathbf{E}_{x}k_{i}^{(\\omega)}\\mathbf{E}_{x}k^{(\\omega)}+\\mathcal{O}(n^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we can obtain the results for $\\mathbf{E}_{i,j}(l_{c;i j}^{(\\omega)})^{2}$ . As a result, we have shown that $\\lvert\\lambda_{0}$ is a estimator of $\\mathbf{Var}_{Z}[n\\mathrm{HSIC}_{\\omega}(Z)]$ with bias $O(n^{-1})$ thus complete the whole proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "F Calculation of Eq. (16) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we give the computational details of Eq. (16). We mark colors to indicate correspondences. According to Eq. (20), we can calculate j,q,r hi(j\u03c9q)r as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{j,q,r}h_{i j q r}^{(\\omega)}=\\frac{1}{2}\\sum_{u,v,w}(k_{i u}^{(\\omega)}l_{i u}^{(\\omega)}+k_{i u}^{(\\omega)}l_{v w}^{(\\omega)}-k_{i u}^{(\\omega)}l_{i v}^{(\\omega)})-\\frac{1}{2}\\sum_{t,v,w}(k_{t i}^{(\\omega)}l_{t v}^{(\\omega)})}\\\\ {+\\displaystyle\\frac{1}{2}\\sum_{t,u,w}(k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{i w}^{(\\omega)}-k_{t u}^{(\\omega)}l_{t i}^{(\\omega)})-\\frac{1}{2}\\sum_{t,u,v}(k_{t u}^{(\\omega)}l_{t v}^{(\\omega)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can further represent Eq. (48) in matrices form as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i,q,r}h_{i j q r}^{(\\omega)}=\\frac{1}{2}\\Big[n^{2}\\big(\\Lambda_{X}\\Lambda_{X}^{T}\\Lambda_{Y}\\Lambda_{Y}^{T}\\big)_{i,i}+\\big(\\Lambda_{X}\\Lambda_{X}^{T}\\mathbf{1}\\big)_{i}\\big(\\mathbf{1}^{T}\\Lambda_{Y}\\Lambda_{Y}^{T}\\mathbf{1}\\big)-n\\big[\\big(\\Lambda_{X}\\Lambda_{X}^{T}\\mathbf{1}\\big)\\odot\\big(\\mathbf{A}_{Y}\\Lambda_{Y}^{T}\\mathbf{1}\\big)\\big]_{i}}}\\\\ &{\\qquad\\qquad\\qquad+\\;n\\mathrm{Tr}\\big(\\Lambda_{X}\\Lambda_{X}^{T}\\Lambda_{Y}\\Lambda_{Y}^{T}\\big)+\\big(\\mathbf{A}_{Y}\\mathbf{A}_{Y}^{T}\\mathbf{1}\\big)_{i}\\big(\\mathbf{1}^{T}\\mathbf{A}_{X}\\Lambda_{X}^{T}\\mathbf{1}\\big)-n\\big(\\Lambda_{Y}\\Lambda_{Y}^{T}\\mathbf{A}_{X}\\Lambda_{X}^{T}\\mathbf{1}\\big)_{i}}\\\\ &{\\qquad\\qquad-\\;n\\big(\\Lambda_{X}\\Lambda_{X}^{T}\\Lambda_{Y}\\Lambda_{Y}^{T}\\mathbf{1}\\big)_{i}-\\big(\\mathbf{1}^{T}\\Lambda_{X}\\Lambda_{X}^{T}\\Lambda_{Y}\\Lambda_{Y}^{T}\\mathbf{1}\\big)\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, by variable substitution, we obtain the result as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{j,q,r}h_{i j q r}^{(\\omega)}=\\frac{1}{2}\\left[{n}{\\bf{1}}^{T}{\\bf{A}}{\\bf{1}}+{n}^{2}({\\bf{A}}{\\bf{1}})_{i}+({\\bf{1}}^{T}{\\bf{C}}){\\bf{B}}_{i}+({\\bf{1}}^{T}{\\bf{B}}){\\bf{C}}_{i}-{n}{\\bf{E}}_{i}-{n}{\\bf{F}}_{i}-{n}{\\bf{D}}_{i}-{\\bf{1}}^{T}{\\bf{D}}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the definition of variables $\\mathbf{A}$ to $\\mathbf{F}$ with the calculation cost are given in the Fig. 1. For convenience, we re-show the diagram here for reference. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underbrace{\\left[\\Delta_{X}^{T}\\Delta_{Y}\\right]_{[\\mathrm{J},\\mathrm{S},\\mathrm{D}]}\\longrightarrow\\left[\\left\\{(\\Delta_{X}^{T}\\Delta_{Y})\\Delta_{Y}^{T}\\right\\}_{[\\mathrm{J},\\mathrm{Sm}]}\\right]_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~}}\\underbrace{\\left(\\mathbf{A}_{X}\\odot\\left(\\mathbf{A}_{X}^{T}\\Delta_{Y}^{T}\\right)_{[\\mathrm{J},\\mathrm{N}]}\\right)}_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~}}\\Bigg\\{\\underbrace{\\mathbf{D}:=\\left[\\mathbf{B}\\odot\\mathbf{C}\\right]_{[\\mathrm{max}]}}\\quad\\left\\{\\begin{array}{l l l}{{\\displaystyle\\mathrm{Im}}\\in\\mathbf{O}\\mathrm{m}\\mathrm{pol}\\mathrm{poing}\\;\\mathrm{bify};}&{\\quad{\\displaystyle\\mathrm{O}}(m D^{2})}\\\\ {{\\displaystyle\\mathrm{Im}}_{[\\mathrm{J},\\mathrm{Sm}]},\\;{\\displaystyle\\mathrm{Im}}_{[\\mathrm{J},\\mathrm{N}]},\\;{\\displaystyle\\mathrm{D}}\\longrightarrow\\left[\\mathbf{B}:\\left[\\mathbf{A}_{X}^{T}\\mathbf{A}_{1}^{T}\\right]_{[\\mathrm{N}]},\\;{\\displaystyle\\mathrm{C}}:\\left[\\mathbf{A}_{Y}(\\mathbf{A}_{Y}^{T})\\right]_{[\\mathrm{N},\\mathrm{I}]}\\right]_{\\longrightarrow}\\;\\left[\\begin{array}{l l l}{\\displaystyle\\mathrm{f}_{Y}:\\mathbf{m}_{Y}}&{\\quad\\mathrm{~\\all}_{Y}(\\mathbf{A}_{X}^{T}\\mathbf{C})_{[\\mathrm{mbx}]},\\;{\\displaystyle\\mathrm{F}}:=\\left[\\mathbf{A}_{Y}(\\mathbf{A}_{Y}^{T}\\mathbf{B})\\right]_{[\\mathrm{max}]}\\right]}&{\\quad{\\displaystyle\\mathrm{O}}(m)}\\end{array}\\right.}\\Bigg\\}\\underbrace{\\left[\\begin{array}{l}{\\boldsymbol{\\mathrm{O}}(\\mathbf{B})^{T}}\\\\ {\\boldsymbol{\\mathrm{O}}(\\mathbf{B})^{T}}\\end{array}\\right]_{[\\mathrm{Wing}]}}_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~-~\\infty~}}\\underbrace{\\left(\\mathbf{D}^{T}\\right)_{[\\mathrm{Wing}]}}_{\\longrightarrow:\\mathrm{~\\infty~-~\\infty~-~\\infty \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Figure 5: The diagram shows the definition of the quantities, with styles representing the time complexity of the computational process in the current box. $\\odot$ : the element-wise product. ", "page_idx": 18}, {"type": "text", "text": "The computational complexity of each step is illustrated in Fig. 5. We explain some steps here. As a start, recall that the size of $\\mathbf{\\cal{A}}_{X},\\mathbf{\\cal{A}}_{Y}$ are both $n\\times D$ . Therefore a time complexity $\\mathcal{O}(n D^{2})$ is required to compute $\\mathbf{A}_{X}^{T}\\mathbf{A}_{Y}$ by matrix multiplication operation. Further multiplying the obtained $[\\mathbf{A}_{X}^{T}\\mathbf{A}_{Y}]_{D\\times D}$ with $\\mathbf{A}_{Y}^{T}$ requires $\\mathcal{O}(n D^{2})$ time complexity. Next, since both $\\pmb{\\Lambda}_{X}$ and $(\\mathbf{A}_{X}^{T}\\mathbf{A}_{Y}\\mathbf{A}_{Y}^{T})^{T}$ are of size $n\\times D$ , the elemental product operation requires a time complexity of $\\mathcal{O}(n\\dot{D})$ . In a similar way, we can check the time complexity for each remained step. After getting the variables $\\mathbf{A}$ to $\\mathbf{F}$ , since $\\mathbf{1}^{T}\\mathbf{A1}$ , A1 all can be calculated in $\\mathcal{O}(n D)$ and $\\mathbf{1}^{T}\\mathbf{B},\\mathbf{\\dot{1}}^{T}\\mathbf{C},\\mathbf{1}^{T}\\mathbf{\\check{D}}$ all can be calculated in $\\mathcal{O}(n)$ , we conclude that the results with index $i$ in Eq. (50) can be obtained in $O(n D^{2})$ time. ", "page_idx": 18}, {"type": "text", "text": "G Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we give a proof of the Theorem 2. We first restate the Theorem 2 here. ", "page_idx": 19}, {"type": "text", "text": "Theorem 2 (Uniform Bound). Let $\\theta_{k},\\theta_{l}$ parameterize $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ in Banach spaces of dimension $d_{k},d_{l}$ . And $\\tau_{\\theta_{k}},\\tau_{\\theta_{l}}$ are Lipschitz to the parameters $\\theta_{k},\\theta_{l}$ with the non-negative constant $L_{k},L_{l}$ , respectively. Let $\\Theta_{c}$ be a set of $(\\theta_{k},\\theta_{l})$ for which $\\sigma_{\\omega}\\ge c>0$ with a positive constant $c$ and $\\lVert{\\boldsymbol{\\theta}}_{k}\\rVert\\leq R_{{\\boldsymbol{\\theta}}_{k}},\\lVert{\\boldsymbol{\\theta}}_{l}\\rVert\\leq$ $R_{\\theta_{l}}$ . Let $r$ denote the threshold, i.e., $(1-\\alpha)$ -quantile for the distribution in Eq. $(I I)$ and $r^{(n)}$ be the threshold with sample size n. Let $\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ be the samplings of frequency with the sampling number $D$ . Also, we define $R_{\\omega_{k}}:=\\operatorname*{sup}_{j}\\|\\omega_{k;j}^{\\circ}\\|,R_{\\omega_{l}}:=\\operatorname*{sup}_{j}\\|\\omega_{l;j}\\|,d_{s}:=\\operatorname*{max}\\{d_{k},d_{l}\\}$ and $\\xi_{\\omega}:=H S I C_{\\omega}(Z)$ . Then with probability at least $1-\\delta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(\\theta_{k},\\theta_{l})\\in\\Theta_{c}}\\left|\\frac{\\xi_{\\omega}-r_{\\omega}^{(n)}/n}{\\hat{\\sigma}_{\\omega}}-\\frac{\\mathbf{E}_{Z}\\xi_{\\omega}-r_{\\omega}/n}{\\sigma_{\\omega}}\\right|\\sim\\mathcal{O}\\left(\\left[\\sqrt{\\frac{1}{n}\\log\\frac{1}{\\delta}+d_{s}\\frac{\\log n}{n}}+\\frac{R_{\\omega_{k}}L_{k}+R_{\\omega_{l}}L_{l}}{\\sqrt{n}}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We take a similar roadmap of proof as [30] and extend it to our optimization objective. The roadmap of the proof is as follows: we first obtain the convergence results (with sample size $n$ ) for each estimator with fixed parameters $\\theta_{k},\\theta_{l}$ , and then extend the results to the entire parameter space via $\\epsilon$ -net arguments. We begin the proof of the first part, which is based on bounded differences inequality (McDiarmid\u2019s inequality) [41, Theorem 2.9.1]. ", "page_idx": 19}, {"type": "text", "text": "Bound of |\u03be\u03c9 \u2212EZ\u03be\u03c9|. Recall the definition of \u03be\u03c9 := HSIC\u03c9(Z) = n14 i,j,q,r h . By the definition of $k_{t u}^{(\\omega)},l_{t u}^{(\\omega)}$ , we have $|k_{t u}^{(\\omega)}|\\leq1,|l_{t u}^{(\\omega)}|\\leq1$ for all $t,u$ , thus $|h_{i j q r}^{(\\omega)}|\\leq4$ for all $i,j,q,r$ Now we begin by showing the bounded differences property of $h_{i j q r}^{(\\omega)}$ . Concretely, we replace the first sample $z_{1}=(x_{1},y_{1})$ with $z_{1}^{\\prime}=(x_{1}^{\\prime},y_{1}^{\\prime})$ and keep the remaining samples the same. The obtained samples are named as $Z^{\\prime}$ . Then the difference terms between $h_{i j q r}^{(\\omega)}$ )r and the new substitution \u02d8hi(j\u03c9q)r can only happen in the case that at least one of $i,j,q,r$ is equal to $\\mathrm{1}$ . For the case that only one subscript is 1 (here take $i=1$ for example), combining Eq. (20), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Bigl|\\sum_{j,q,r}h_{1j q r}^{(\\omega)}-\\sum_{j,q,r}\\check{h}_{1j q r}^{(\\omega)}\\Bigr|\\le\\frac{2}{4!}(n-1)(n-2)(n-3)\\cdot6\\cdot16=8(n-1)(n-2)(n-3).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The whole contributes of remaining terms that at least two $i,j,q,r$ are less than $\\mathcal{O}(n^{-2})$ , thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Big|\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}^{(\\omega)}-\\frac{1}{n^{4}}\\sum_{i,j,q,r}\\breve{h}_{i j q r}^{(\\omega)}\\Big|\\le4\\cdot\\Big|\\frac{1}{n^{4}}\\sum_{j,q,r}h_{1j q r}^{(\\omega)}-\\frac{1}{n^{4}}\\sum_{j,q,r}\\breve{h}_{1j q r}^{(\\omega)}\\Big|+\\mathcal{O}(n^{-2})=\\mathcal{O}(n^{-1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence $\\mathrm{HSIC}_{\\omega}(Z)$ satisfy the bounded differences property with $O(n^{-1})$ . Using McDiarmid\u2019s inequality, for fixed $\\theta_{0},\\theta_{1}$ , with probability at least $1-\\delta$ , there exist a universal constant $C_{1}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\xi_{\\omega}-\\mathbf{E}_{Z}\\xi_{\\omega}\\right|\\!\\leq C_{1}\\sqrt{\\frac{1}{n}\\log\\frac{2}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Bound of $|r_{\\omega}^{(n)}-r_{\\omega}|$ . As $r_{\\omega}^{(n)}$ is the $(1-\\alpha)$ of the distribution of $n\\xi_{\\omega}$ with sample size $n$ under $\\mathcal{H}_{0}$ , according to the Eq. (53), when $n$ is large enough, there exist a universal constant $C_{2}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n|r_{\\omega}^{(n)}|/n\\leq C_{1}\\sqrt{\\frac{1}{n}\\log\\frac{2}{\\alpha}}+|\\mathbf{E}_{Z}\\xi_{\\omega}|\\leq C_{2}\\sqrt{\\frac{1}{n}\\log\\frac{1}{\\alpha}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequation is because ${\\bf E}_{Z}\\xi_{\\omega}\\,\\sim\\,{\\cal O}(n^{-1})$ under $\\mathcal{H}_{0}$ (see Theorem 1 for a detailed explanation). Hence $|r_{\\omega}^{(n)}|\\sim\\mathcal{O}\\big(\\sqrt{n\\log(1/\\alpha)}\\big)$ . Also, by definition $r_{\\omega}$ is a constant related to $\\alpha$ . ", "page_idx": 19}, {"type": "text", "text": "Bound of $|\\widehat{\\sigma}_{\\omega}^{2}-\\sigma_{\\omega}^{2}|$ . In this part, We first obtain the bound of $|\\widehat{\\sigma}_{\\omega}^{2}-\\mathbf{E}_{Z}\\widehat{\\sigma}_{\\omega}^{2}|$ , then obtain the bound of $|\\mathbf{E}_{Z}\\widehat{\\sigma}_{\\omega}^{2}-\\sigma_{\\omega}^{2}|$ . As before we start by showing the bounded v ariance pr operty of $\\widehat{\\sigma}_{\\omega}^{2}$ . We replace $z_{1}=(x_{1},y_{1})$ with $z_{1}^{\\prime}=(x_{1}^{\\prime},y_{1}^{\\prime})$ and keep the remaining samples the same. The obtained samples are named as $Z^{\\prime}$ . For readable, we denote $\\widehat{\\sigma}_{\\omega}^{2}$ with sample $Z,Z^{\\prime}$ as $\\widehat{\\sigma}_{\\omega}^{2}(Z),\\widehat{\\sigma}_{\\omega}^{2}(Z^{\\prime})$ respectively. ", "page_idx": 19}, {"type": "text", "text": "Recall the definition $\\begin{array}{r}{\\widehat{\\sigma}_{\\omega}^{2}:=16\\big[\\frac{1}{n}\\sum_{i}(\\frac{1}{n^{3}}\\sum_{j,q,r}h_{i j q r}^{(\\omega)})^{2}-\\mathrm{HSIC}_{\\omega}^{2}(Z)\\big].}\\end{array}$ . Since $|h_{i j q r}^{(\\omega)}|\\leq4$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Big|\\frac{1}{n}\\sum_{i}\\Big(\\frac{1}{n^{3}}\\sum_{j,q,r}h_{i j q r}^{(\\omega)}\\Big)^{2}-\\frac{1}{n}\\sum_{i}\\Big(\\frac{1}{n^{3}}\\sum_{j,q,r}h_{i j q r}^{(\\omega)}\\Big)^{2}\\Big|\\leq\\frac{8}{n^{4}}\\sum_{i}\\sum_{j,q,r}\\Big|h_{i j q r}^{(\\omega)}-\\check{h}_{i j q r}^{(\\omega)}\\Big|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Big|\\Big(\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}^{(\\omega)}\\Big)^{2}-\\Big(\\frac{1}{n^{4}}\\sum_{i,j,q,r}\\breve{h}_{i j q r}^{(\\omega)}\\Big)^{2}\\Big|\\leq\\frac{8}{n^{4}}\\sum_{i,j,q,r}\\big|h_{i j q r}^{(\\omega)}-\\breve{h}_{i j q r}^{(\\omega)}\\big|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Again, the difference terms between $h_{i j q r}^{(\\omega)}$ and the new substitution \u02d8hi(j\u03c9q)r can only happen in the case that at least one of $i,j,q,r$ is equal to 1. Hence, Eqs. (55) and (56) are both $\\mathcal{O}(n^{-1})$ . As a result, $\\widehat{\\sigma}_{\\omega}^{2}$ satisfy the bounded differences property with bound $O(n^{-1})$ . Using McDiarmid\u2019s inequality, w i th probability at least $1-\\delta$ , there exist a universal constant $C_{3}$ such that $\\begin{array}{r}{\\left|\\widehat{\\sigma}_{\\omega}^{2}-\\mathbf{E}_{Z}\\widehat{\\sigma}_{\\omega}^{2}\\right|\\leq C_{3}\\sqrt{\\frac{1}{n}\\log\\frac{2}{\\delta}}}\\end{array}$ . Next we obtain the bound of $|\\mathbf{E}_{Z}\\widehat{\\sigma}_{\\omega}^{2}-\\sigma_{\\omega}^{2}|$ . We rewrite $\\mathbf{E}_{Z}\\widehat{\\sigma}_{\\omega}^{2}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{E}_{Z}\\widehat{\\sigma}_{\\omega}^{2}=16\\Big(\\frac{1}{n^{7}}\\sum_{i j q r j^{\\prime}q^{\\prime}r^{\\prime}}\\mathbf{E}[h_{i j q r}^{(\\omega)}h_{i j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}]-\\frac{1}{n^{8}}\\sum_{i j q r i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}\\mathbf{E}[h_{i j q r}^{(\\omega)}h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}]\\Big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By adding further restrictions that $i,i^{\\prime},j,q,r,j^{\\prime},q^{\\prime},r^{\\prime}$ are all different, we can obtain the corresponding expression for $\\sigma_{\\omega}^{2}$ . Hence the difference between them can only happen when at least one subscript in $i^{\\prime},j,q,r,j^{\\prime},q^{\\prime},r^{\\prime}$ is equal to $i$ . Combining $|h_{i j q r}^{(\\omega)}|\\leq4$ , we have $|\\mathbf{E}_{Z}\\widehat{\\sigma}_{\\omega}^{2}-\\sigma_{\\omega}^{2}|\\sim\\mathcal{O}(n^{-1})$ . ", "page_idx": 20}, {"type": "text", "text": "$\\epsilon$ -net arguments. Next, we prove the second part with $\\epsilon$ -net arguments. Take the parameter space $\\Theta_{k}$ of $\\theta_{k}$ as an example. We choose a cover with $\\mathcal{N}(\\Theta_{k},r_{k})$ points $\\{p_{i}\\}_{i=1}^{\\mathcal{N}(\\Theta_{k},r_{k})}$ such that for any point $p\\,\\in\\,\\Theta_{k}$ , we have $\\operatorname*{min}_{i}\\,\\|p-p_{i}\\|\\,\\leq\\,r_{k}$ . According to [41, Proposition 4.2.12], by comparing the volumes, we have $N(\\Theta_{k},r_{k})\\,\\le\\,(4R_{\\Theta_{k}}/r_{k})^{d_{k}}$ . As for the parameter space $\\Theta_{l}$ of $\\theta_{l}$ , we can also obtain a cover with $\\mathcal{N}(\\Theta_{l},r_{l})$ points that $\\mathcal{N}(\\Theta_{k},r_{k})\\,\\leq\\,({4R_{\\Theta_{l}}}/{r_{l}})^{d_{l}}$ . Here, we set $r_{k}\\,=\\,4R_{\\Theta_{k}}/\\sqrt{n},r_{l}\\,=\\,4R_{\\Theta_{l}}/\\sqrt{n}$ , thus $\\mathcal{N}(\\Theta_{k},r_{k})\\,\\leq\\,(\\sqrt{n})^{d_{k}},\\mathcal{N}(\\Theta_{l},r_{l})\\,\\leq\\,(\\sqrt{n})^{d_{l}}$ . Then combining the Lipschitz property as shown in Lemma 4, we have with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{(\\theta_{k},\\theta_{l})\\in\\Theta_{c}}\\left\\vert\\xi_{\\omega}-{\\bf E}_{Z}\\xi_{\\omega}\\right\\vert\\leq C_{1}\\sqrt{\\frac{1}{n}\\log\\frac{2\\ensuremath{N(\\Theta_{k},r_{k})}\\ensuremath{N(\\Theta_{l},r_{l})}}{\\delta}}+8R_{\\omega_{k}}L_{k}\\cdot r_{k}+8R_{\\omega_{l}}L_{l}r_{l}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq C_{1}\\sqrt{\\frac{1}{n}\\log\\frac{2}{\\delta}+(d_{k}+d_{l})\\frac{\\log n}{2n}}+\\frac{32R_{\\omega_{k}}L_{k}R_{\\Theta_{k}}}{\\sqrt{n}}+\\frac{32R_{\\omega_{l}}L_{l}R_{\\Theta_{l}}}{\\sqrt{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence when $n$ is large enough, there exists a positive constant $C_{3}$ , with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(\\theta_{k},\\theta_{l})\\in\\Theta_{c}}\\left\\vert\\xi_{\\omega}-\\mathbf{E}_{Z}\\xi_{\\omega}\\right\\vert\\leq C_{3}\\left[\\sqrt{\\frac{1}{n}\\log\\frac{1}{\\delta}+(d_{k}+d_{l})\\frac{\\log n}{n}}+\\frac{R_{\\omega_{k}}L_{k}R_{\\Theta_{k}}}{\\sqrt{n}}+\\frac{R_{\\omega_{l}}L_{l}R_{\\Theta_{l}}}{\\sqrt{n}}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similar, when $n$ is large enough, there exists a positive constant $C_{4}$ , with probability at least $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(\\theta_{k},\\theta_{l})\\in\\Theta_{c}}\\left\\vert\\hat{\\sigma}_{\\omega}^{2}-\\sigma_{\\omega}^{2}\\right\\vert\\leq C_{4}\\left[\\sqrt{\\frac{1}{n}\\log\\frac{1}{\\delta}+(d_{k}+d_{l})\\frac{\\log n}{n}}+\\frac{R_{\\omega_{k}}L_{k}R_{\\Theta_{k}}}{\\sqrt{n}}+\\frac{R_{\\omega_{l}}L_{l}R_{\\Theta_{l}}}{\\sqrt{n}}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Overall Bound. Now we combine the previously obtained results. We have ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\frac{\\xi_{\\omega}-r_{\\omega}^{(n)}/n}{\\hat{\\sigma}_{\\omega}}-\\frac{\\mathbf{E}_{Z}\\xi_{\\omega}-r_{\\omega}/n}{\\sigma_{\\omega}}\\right|\\leq\\left|\\frac{\\xi_{\\omega}-\\mathbf{E}_{Z}\\xi_{\\omega}}{\\hat{\\sigma}_{\\omega}}\\right|+\\left|\\frac{r_{\\omega}^{(n)}-r_{\\omega}}{n\\hat{\\sigma}_{\\omega}}\\right|+\\left|\\mathbf{E}_{Z}\\xi_{\\omega}-r_{\\omega}/n\\right|\\cdot\\left|\\frac{1}{\\hat{\\sigma}_{\\omega}}-\\frac{1}{\\sigma_{\\omega}}\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since on $\\Theta_{c}$ , $\\sigma_{\\omega}\\geq c$ , according to Eq. (59), we can make $\\widehat{\\sigma}_{\\omega}\\geq c/2$ happen by assigning probability budget $\\delta/2$ when $n$ is large enough. Also, combining E q . (54), $|{\\bf E}_{Z}\\xi_{\\omega}|\\le1$ and $r_{\\omega}$ is a constant related to $\\alpha$ , then with $n$ large enough, there exist positive constants $C_{5},C_{6}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\frac{\\xi_{\\omega}-r_{\\omega}^{(n)}/n}{\\widehat{\\sigma}_{\\omega}}-\\frac{\\mathbf{E}_{Z}\\xi_{\\omega}-r_{\\omega}/n}{\\sigma_{\\omega}}\\right|\\leq\\frac{2}{c}\\left|\\xi_{\\omega}-\\mathbf{E}_{Z}\\xi_{\\omega}\\right|+\\frac{C_{5}}{c}\\sqrt{\\frac{1}{n}\\log\\frac{1}{\\alpha}}+\\frac{C_{6}}{c^{3}}\\left|\\widehat{\\sigma}_{\\omega}^{2}-\\sigma_{\\omega}^{2}\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that we need to pay for probability budget $\\delta/2$ for the above conclusion to hold. Then by taking the supremum on both sides in Eq. (60) and assigning the remained probability budget $\\delta/2$ to Eqs. (58) and (59), we can show that when $n$ is large enough, with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(\\theta_{k},\\theta_{l})\\in\\Theta_{c}}{\\operatorname*{sup}}\\bigg\\vert\\frac{\\xi_{\\omega}-r_{\\omega}^{(n)}/n}{\\widehat{\\sigma}_{\\omega}}-\\frac{\\mathbf{E}_{Z}\\xi_{\\omega}-r_{\\omega}/n}{\\sigma_{\\omega}}\\bigg\\vert}\\\\ &{\\qquad\\quad\\sim\\mathcal{O}\\bigg(\\frac{1}{c^{3}}\\bigg[\\sqrt{\\frac{1}{n}\\log\\frac{1}{\\delta}+(d_{k}+d_{l})\\frac{\\log n}{n}}+\\frac{R_{\\omega_{k}}L_{k}R_{\\Theta_{k}}+R_{\\omega_{l}}L_{l}R_{\\Theta_{l}}}{\\sqrt{n}}\\bigg]\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "thus complete the proof. ", "page_idx": 20}, {"type": "text", "text": "H Proof of Theorem 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we give a proof of the Theorem 3. We first restate the Theorem 3 here. ", "page_idx": 21}, {"type": "text", "text": "Theorem 3 (Consistency). Let $\\theta_{k}^{*},\\theta_{l}^{*}$ be the parameters after learning, $Z^{t e}$ be the testing samples of size $m$ , when $\\mathbf{E}_{Z}H S I C_{\\omega}^{(u)}(Z^{t e})>0,$ , then the probability of Type II error ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(T\\!y p e\\,I I\\,e r r o r\\big)\\!=\\mathbb{P}_{\\mathcal{H}_{1}}\\big(m H S I C_{\\omega}\\big(Z^{t e}\\big)\\le r_{\\omega}^{(m)}|\\theta_{k}^{*},\\theta_{l}^{*}\\big)\\!\\sim\\mathcal{O}(m^{-1/2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let the mapping functions with learned parameters $\\theta_{k}^{*},\\theta_{l}^{*}$ be $\\tau_{\\theta_{k}^{*}},\\tau_{\\theta_{l}^{*}}$ , and the corresponding range space be a compact subset of $\\mathbb{R}^{d_{T_{x}}},\\mathbb{R}^{d_{T_{y}}}$ , respectively. Also, the diameter of two range spaces is denoted by diam $(\\mathcal T_{\\theta_{k}^{*}}),d i a m(\\mathcal T_{\\theta_{l}^{*}}).$ , respectively. Let $\\{(\\omega_{k;j},\\omega_{l;j})\\}_{j=1}^{D/2}$ be the frequency samplings with their second moment denoted by $\\sigma_{\\omega_{k}}^{2}:=\\mathbf{E}_{p_{k}(\\omega)}[\\omega_{k;j}^{T}\\omega_{k;j}]$ , $\\sigma_{\\omega_{l}}^{2}:=\\mathbf{E}_{p_{l}(\\omega)}[\\omega_{l;j}^{T}\\omega_{l;j}]$ . Additionally, we denote $\\xi_{u}:=H S I C(X,Y),$ , then under $\\mathcal{H}_{1}$ , we have ${\\bf E}_{Z}H S I C_{\\omega}^{(u)}(Z^{t e})>0$ with any constant probability when $\\begin{array}{r}{D=\\Omega\\biggl(\\frac{d\\tau_{x}+d\\tau_{y}}{\\xi_{u}^{2}}\\log\\frac{\\sigma_{\\omega_{k}}d i a m(\\mathcal{T}_{\\theta_{k}^{*}})+\\sigma_{\\omega_{l}}d i a m(\\mathcal{T}_{\\theta_{l}^{*}})}{\\xi_{u}}\\biggr)}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof consists of two parts, we first give the rate of convergence of Type $\\mathrm{II}$ error under condition $\\dot{\\mathbf{E}_{Z}}\\mathrm{HSIC}_{\\omega}^{(u)}\\big(Z^{t e}\\big)\\,>\\,0$ , and next for condition $\\mathbf{E}_{Z}\\mathrm{HSIC}_{\\omega}^{(u)}(Z^{t e})\\,>\\,0$ we give a lower bound on the number of frequency samplings required for it to hold. To simplify, we denote the U-statistic $\\mathrm{HSIC}_{\\omega}^{(u)}(Z^{t e})$ as $\\xi_{\\omega}^{(u)}$ in this proof. We begin to prove the first part. With the learned parameters $\\theta_{k}^{*},\\theta_{l}^{*}$ , the probability of the Type $\\mathrm{II}$ error is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\mathrm{Type~II~error}\\Big)\\!=\\mathbb{P}_{\\mathcal{H}_{1}}\\Big(m\\xi_{\\omega}\\le r_{\\omega}^{(m)}|\\theta_{k}^{*},\\theta_{l}^{*}\\Big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combing the result of the difference between $\\xi_{\\omega}$ and $\\xi_{\\omega}^{(u)}$ as shown in Eq. (34), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{\\mathcal{H}_{1}}\\!\\left(m\\xi_{\\omega}\\le r_{\\omega}^{(m)}|\\theta_{k}^{*},\\theta_{l}^{*}\\right)\\!\\le\\mathbb{P}_{\\mathcal{H}_{1}}\\left(m\\xi_{\\omega}^{(u)}\\le r_{\\omega}^{(m)}+C_{0}\\big|\\theta_{k}^{*},\\theta_{l}^{*}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{0}$ is a positive constant. To apply the rate of convergence of the Central Limit Theorem, we rewrite the right equation in Eq. (64) as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{H}_{1}}\\left(\\frac{\\sqrt{m}(\\xi_{\\omega}^{(u)}-\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)})}{4\\sigma_{\\omega}^{1/2}}\\leq\\frac{r_{\\omega}^{(m)}/\\sqrt{m}-\\sqrt{m}\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}+C_{0}/\\sqrt{m}}{4\\sigma_{\\omega}^{1/2}}\\Big|\\theta_{k}^{*},\\theta_{l}^{*}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the standard deviation (defined in Proposition 1) $\\sigma_{\\omega}>0$ under $\\mathcal{H}_{1}$ . Then according to the results in [34, Section 5.5.1 Theorem B], there exist nonnegative constant $C_{1}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\mathrm{Type~II~error}\\Big)\\leq\\Phi\\left(\\frac{r_{\\omega}^{(m)}/\\sqrt{m}-\\sqrt{m}\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}+C_{0}/\\sqrt{m}}{4\\sigma_{\\omega}^{1/2}}\\right)+\\frac{C_{1}\\nu_{h}}{\\sigma_{\\omega}^{3/2}}\\frac{1}{\\sqrt{m}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\nu_{h}:=\\mathbf{E}_{Z}^{i\\neq j\\neq q\\neq r}|h_{i j q r}^{(\\omega)}|^{3}<\\infty$ . When $m$ is large enough, we further have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\mathrm{Type~II~error}\\Big)\\leq\\Phi\\Big(C_{2}-C_{3}\\sqrt{m}\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}+C_{4}/\\sqrt{m}\\Big)+C_{5}\\frac{1}{\\sqrt{m}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{2},C_{3},C_{4}$ are positive constants and using $r^{(m)}\\sim\\mathcal{O}(m^{1/2})$ we prove in Eq. (54). Hence when ${\\bf E}_{Z}\\xi_{\\omega}^{(u)}\\,>\\,0$ , the leading term $\\sqrt{m}\\mathbf{E}_{Z}\\boldsymbol{\\xi}_{\\omega}^{(u)}$ decrease as $m$ increase. Further, to obtain the decrease rate when $m$ is close to infinity, we consider the asymptotic expansion (when $x$ is close to negative infinity) for the function $\\Phi(x)$ as given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Phi(x)=-\\,{\\frac{e^{-x^{2}}}{2x{\\sqrt{\\pi}}}}\\left(1+\\sum_{n=1}^{\\infty}(-1)^{n}{\\frac{1\\cdot3\\cdot5\\cdot\\cdot(2n-1)}{\\left(2x^{2}\\right)^{n}}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "thus $\\Phi\\Bigl(C_{2}-C_{3}\\sqrt{m}{\\bf E}_{Z}\\xi_{\\omega}^{(u)}+C_{4}/\\sqrt{m}\\Bigr)\\sim\\mathcal{O}(m^{-1/2})$ . As a result, the decreasing rate is at least $\\mathcal{O}(m^{-1/2})$ . We have so far completed the first part of the proof, and we next begin the second part of the proof, i.e. obtain the number of frequency samplings required for the condition $\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}>0$ to ", "page_idx": 21}, {"type": "text", "text": "hold. For simplify, we denote $\\Lambda_{k}(x)^{T}\\Lambda_{k}(x^{\\prime}),\\Lambda_{l}(y)^{T}\\Lambda_{l}(y^{\\prime})$ as $k^{(\\omega)}(x,x^{\\prime}),l^{(\\omega)}(y,y^{\\prime})$ , respectively. Then according to Lemma 2, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\underset{x,x^{\\prime}\\in\\mathcal{X}}{\\operatorname*{sup}}|k^{(\\omega)}(x,x^{\\prime})-k(x,x^{\\prime})|\\ge\\epsilon\\right]\\le2^{8}\\left(\\frac{\\sigma_{\\omega_{k}}\\mathrm{diam}(\\mathcal{T}_{\\theta_{k}^{*}})}{\\epsilon}\\right)^{2}\\exp\\left(-\\frac{D\\epsilon^{2}}{4(d_{T_{x}}+2)}\\right),}\\\\ &{\\mathbb{P}\\left[\\underset{y,y^{\\prime}\\in\\mathcal{Y}}{\\operatorname*{sup}}|l^{(\\omega)}(y,y^{\\prime})-l(y,y^{\\prime})|\\ge\\epsilon\\right]\\le2^{8}\\left(\\frac{\\sigma_{\\omega_{l}}\\mathrm{diam}(\\mathcal{T}_{\\theta_{l}^{*}})}{\\epsilon}\\right)^{2}\\exp\\left(-\\frac{D\\epsilon^{2}}{4(d_{T_{y}}+2)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Also, we denote the bounds in Eq. (69) as $\\delta_{x}(\\epsilon,D),\\delta_{y}(\\epsilon,D)$ , respectively. Next we get the bound between $\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}$ and HSIC $(X,Y)$ . According to Lemma 3, the bound is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}-\\mathrm{HSIC}(X,Y)|\\leq4\\cdot\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal{X},y,y^{\\prime}\\in\\mathcal{Y}}|k^{(\\omega)}(x,x^{\\prime})l^{(\\omega)}(y,y^{\\prime})-k(x,x^{\\prime})l(y,y^{\\prime})|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since by the definition, for all $(x,x^{\\prime})\\;\\in\\;\\mathcal{X}\\,\\times\\,\\mathcal{X},(y,y^{\\prime})\\;\\in\\;\\mathcal{Y}\\,\\times\\,\\mathcal{Y}$ , we have $|k^{(\\omega)}(x,x^{\\prime})|\\,\\leq\\,1$ $|l^{(\\omega)}(y,y^{\\prime})|\\leq1,|k(x,x^{\\prime})|\\leq1,|l(y,y^{\\prime})|\\leq1$ . Hence we have   \n$|k^{(\\omega)}(x,x^{\\prime})l^{(\\omega)}(y,y^{\\prime})-k(x,x^{\\prime})l(y,y^{\\prime})|\\leq|k^{(\\omega)}(x,x^{\\prime})-k(x,x^{\\prime})|+|l^{(\\omega)}(y,y^{\\prime})-l(y,y^{\\prime})|.$ (71) Combining the results of Eqs. (70) and (71), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}-\\mathrm{HSIC}(X,Y)|\\le4\\cdot\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal X}|k^{(\\omega)}(x,x^{\\prime})-k(x,x^{\\prime})|+4\\cdot\\operatorname*{sup}_{y,y^{\\prime}\\in\\mathcal y}|l^{(\\omega)}(y,y^{\\prime})-l(y,y^{\\prime})|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the results as shown in Eq. (69) and allocating the probability budget $\\epsilon$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal{X},\\,y,y^{\\prime}\\in\\mathcal{Y}}|\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}-\\mathrm{HSIC}(X,Y)|\\ge\\epsilon\\right]\\le\\delta_{x}(\\epsilon/8,D)+\\delta_{y}(\\epsilon/8,D).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By setting $\\epsilon\\,=\\,\\xi_{u}/2$ , and since $\\xi_{u}\\,>\\,0$ under $\\mathcal{H}_{1}$ , we conclude that $\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}>0$ holds with any constant probability when $\\begin{array}{r}{D=\\Omega\\Big(\\frac{d_{\\mathcal{T}_{x}}+d_{\\mathcal{T}_{y}}}{\\xi_{u}^{2}}\\log\\frac{\\sigma_{\\omega_{k}}\\mathrm{diam}(\\mathcal{T}_{\\theta_{k}^{*}})+\\sigma_{\\omega_{l}}\\mathrm{diam}(\\mathcal{T}_{\\theta_{l}^{*}})}{\\xi_{u}}\\Big)}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "In the proof of Theorem 3, we obtain the convergence bound of the statistic $\\mathbf{E}_{Z}\\xi_{\\omega}^{(u)}$ . Actually, the convergence result for its estimation can also be obtained, as shown in the following Corollary. Corollary 1 (Approximation Error Bound of $\\mathrm{HSIC}_{\\omega}(Z^{t e}))$ . Maintaining the same conditions and notions as in Theorem 3, we have the uniform convergence bound of $H\\bar{S}I\\bar{C}_{\\omega}(Z^{t e})$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{Z^{t e}\\in\\mathcal{X}\\times\\mathcal{Y}}\\vert H S I C_{\\omega}(Z^{t e})-H S I C_{b}(Z^{t e})\\vert\\ge\\epsilon\\right]\\le\\delta_{x}(\\epsilon/8,D)+\\delta_{y}(\\epsilon/8,D).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By the definition, we have for all $\\begin{array}{r}{i,j,q,r,|k_{i j}^{(\\omega)}|\\leq1,|l_{i j}^{(\\omega)}|\\leq1,|k_{i j}|\\leq1,|l_{i j}|\\leq1,}\\end{array}$ , thus ", "page_idx": 22}, {"type": "equation", "text": "$\\begin{array}{r}{k_{i j}^{(\\omega)}l_{q r}^{(\\omega)}-k_{i j}l_{q r}|\\le|k_{i j}^{(\\omega)}-k_{i j}||l_{q r}^{(\\omega)}|+|k_{i j}||l_{q r}^{(\\omega)}-l_{q r}|\\le|k_{i j}^{(\\omega)}-k_{i j}|+|l_{q r}^{(\\omega)}-l_{q r}|.}\\end{array}$ ", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then according to the results as shown in Eq. (69), we have for all $i,j,q,r$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{\\substack{x_{i},x_{j}\\in\\mathcal{X},y_{q},y_{r}\\in\\mathcal{Y}}}|k_{i j}^{(\\omega)}l_{q r}^{(\\omega)}-k_{i j}l_{q r}|\\geq\\epsilon\\right]\\leq\\delta_{x}(\\epsilon/2,D)+\\delta_{y}(\\epsilon/2,D).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recall the definition of $\\begin{array}{r}{\\dot{h}_{i j q r}^{(\\omega)}=\\frac{1}{4!}\\sum_{(t,u,v,w)}^{(i,j,q,r)}k_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}l_{v w}^{(\\omega)}-2k_{u v}^{(\\omega)}l_{t v}^{(\\omega)}}\\end{array}$ and we further define the corresponding hijqr = 41! ((it,,ju,,qv,,rw)) ktultu + ktulvw \u22122kuvltv, then for all i, j, q, r, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{\\substack{x_{i},x_{j}\\in\\mathcal{X},y_{q},y_{r}\\in\\mathcal{Y}}}|h_{i j q r}^{(\\omega)}-h_{i j q r}|\\geq\\epsilon\\right]\\leq\\delta_{x}(\\epsilon/8,D)+\\delta_{y}(\\epsilon/8,D).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "After that, using the expressions that we obtained before, i.e., $\\begin{array}{r}{\\mathrm{HSIC}_{\\omega}(Z^{t e}):=\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}^{(\\omega)}}\\end{array}$ and $\\begin{array}{r}{\\mathrm{HSIC}_{b}(Z^{t e}):=\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}}\\end{array}$ , we obtain the final bound that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{Z^{t e}\\in\\mathcal{X}\\times\\mathcal{Y}}\\vert\\mathrm{HSIC}_{\\omega}(Z^{t e})-\\mathrm{HSIC}_{b}(Z^{t e})\\vert\\ge\\epsilon\\right]\\le\\delta_{x}(\\epsilon/8,D)+\\delta_{y}(\\epsilon/8,D).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and thus complete the proof. ", "page_idx": 22}, {"type": "text", "text": "I Smoothness of Optimization Objective ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We first prove the Lipschitz property for some functions. For ease of reference, we re-list here the definitions of the terms that related to the optimization objective: \u03be\u03c9 := HSIC\u03c9(Z) = n14 i,j,q,r h $\\begin{array}{r}{\\widehat{\\sigma}_{\\omega}^{2}:=16\\Big[\\frac{1}{n}\\sum_{i}(\\frac{1}{n^{3}}\\sum_{j,q,r}h_{i j q r}^{(\\omega)})^{2}-\\mathrm{HSIC}_{\\omega}^{2}(Z)\\Big]}\\end{array}$ and $\\sigma_{\\omega}^{2}:=16\\Big[{\\mathbf{E}}_{i}({\\mathbf{E}}_{j,q,r}h_{i j q r}^{(\\omega)})^{2}-\\big({\\mathbf{E}}_{Z}h_{i j q r}^{(\\omega)}\\big)^{2}\\Big]$ . The Lipschitz property of these terms are shown as follows. ", "page_idx": 23}, {"type": "text", "text": "Lemma 4 (Lipschitz Property of of $\\xi_{\\omega},\\mathbf{E}_{Z}\\xi_{\\omega},\\widehat{\\sigma}_{\\omega}^{2},\\sigma_{\\omega}^{2})$ . Maintaining the same conditions and notions as in Lemma $^{\\,l}$ , we have the following Lipschi t z property ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\xi_{\\omega}(\\theta_{k},\\theta_{l})-\\xi_{\\omega}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq8R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+8R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\\\ &{|{\\bf E}_{Z}[\\xi_{\\omega}(\\theta_{k},\\theta_{l})]-{\\bf E}_{Z}[\\xi_{\\omega}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})]|\\leq8R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+8R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\\\ &{|\\widehat{\\sigma}_{\\omega}^{2}(\\theta_{k},\\theta_{l})-\\widehat{\\sigma}_{\\omega}^{2}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq1024R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+1024R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\\\ &{|\\sigma_{\\omega}^{2}(\\theta_{k},\\theta_{l})-\\sigma_{\\omega}^{2}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq1024R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+1024R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we use the symbol $\\xi_{\\omega}(\\theta_{k},\\theta_{l})$ to denote $\\xi_{\\omega}$ with the parameter $\\theta_{k},\\theta_{l}$ and the others by analogy. ", "page_idx": 23}, {"type": "text", "text": "Proof. We start by obtaining the result of $h_{i j q r}^{(\\omega)}$ for all $i,j,q,r$ . Since for all $i,j,q,r$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\big|k_{i j}^{(\\omega)}(\\theta_{k})l_{q r}^{(\\omega)}(\\theta_{l})-k_{i j}^{(\\omega)}(\\theta_{k}^{\\prime})l_{q r}^{(\\omega)}(\\theta_{l}^{\\prime})\\big|\\!\\leq\\big|k_{i j}^{(\\omega)}(\\theta_{k})-k_{i j}^{(\\omega)}(\\theta_{k}^{\\prime})\\big|\\!+\\!\\big|l_{q r}^{(\\omega)}(\\theta_{l})-l_{q r}^{(\\omega)}(\\theta_{l}^{\\prime})\\big|\\!,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the property $|k_{i j}^{(\\omega)}|\\leq1$ and $|l_{q r}^{(\\omega)}|\\le1$ are used. According the Lemma 1, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big|k_{i j}^{(\\omega)}(\\theta_{k})-k_{i j}^{(\\omega)}(\\theta_{k}^{\\prime})\\big|\\!\\leq2R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|,\\,\\,\\big|l_{q r}^{(\\omega)}(\\theta_{l})-l_{q r}^{(\\omega)}(\\theta_{l}^{\\prime})\\big|\\!\\leq2R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combing the definition tha \u03c9q)r := 41! ((it,,ju,,qv,,rw)) kt(u\u03c9 )lt(u\u03c9) + kt(u\u03c9 )l(v\u03c9w) \u22122 uv l , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\big|h_{i j q r}^{(\\omega)}(\\theta_{k},\\theta_{l})-h_{i j q r}^{(\\omega)}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})\\big|\\!\\leq8R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+8R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Also, combining the definition of $\\begin{array}{r}{\\xi_{\\omega}:=\\mathrm{HSIC}_{\\omega}(Z)=\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}^{(\\omega)}}\\end{array}$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\xi_{\\omega}(\\theta_{k},\\theta_{l})-\\xi_{\\omega}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq8R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+8R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By using | $\\begin{array}{r}{\\mathbf{E}_{Z}[\\xi_{\\omega}(\\theta_{k},\\theta_{l})]-\\mathbf{E}_{Z}[\\xi_{\\omega}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})]|\\leq\\mathbf{E}_{Z}[|\\xi_{\\omega}(\\theta_{k},\\theta_{l})-\\xi_{\\omega}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|],\\mathrm{we~h~}\\mathtt{f}(\\theta_{k},\\theta_{l})=\\xi_{\\omega}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|.}\\end{array}$ ave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbf{E}_{Z}[\\xi_{\\omega}(\\theta_{k},\\theta_{l})]-\\mathbf{E}_{Z}[\\xi_{\\omega}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})]|\\leq8R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+8R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the results of $\\widehat{\\sigma}_{\\omega}^{2},\\sigma_{\\omega}^{2}$ , we first proof the following results. For all $i,j,q,r,i^{\\prime},j^{\\prime},q^{\\prime},r^{\\prime}$ , we hav ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|h_{i j q r}^{(\\omega)}(\\theta_{k},\\theta_{l})h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}(\\theta_{k},\\theta_{l})-h_{i j q r}^{(\\omega)}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})\\right|}\\\\ &{\\leq\\,4\\cdot\\left|h_{i j q r}^{(\\omega)}(\\theta_{k},\\theta_{l})-h_{i j q r}^{(\\omega)}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})\\right|+4\\cdot\\left|h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}(\\theta_{k},\\theta_{l})-h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})\\right|}\\\\ &{\\leq\\,64R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+64R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality holds due to property $|h_{i j q r}^{(\\omega)}|\\leq4$ . Then we use the expression ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\widehat{\\sigma}}_{\\omega}^{2}=16\\Big[\\frac{1}{n^{7}}\\sum_{\\substack{i,j,q,r,j^{\\prime},q^{\\prime},r^{\\prime}}}h_{i j q r}^{(\\omega)}h_{i j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}-\\frac{1}{n^{8}}\\sum_{\\substack{i,j,q,r,i^{\\prime},j^{\\prime},q^{\\prime},r^{\\prime}}}h_{i j q r}^{(\\omega)}h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}\\Big]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and combine the results in Eq. (85). As a result, we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\widehat{\\sigma}_{\\omega}^{2}(\\theta_{k},\\theta_{l})-\\widehat{\\sigma}_{\\omega}^{2}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq1024R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+1024R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In a similar way, we can obtain the corresponding expression of $\\sigma_{\\omega}^{2}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{\\omega}^{2}=16\\Big[{\\bf E}_{i,j,q,r,j^{\\prime},q^{\\prime},r^{\\prime}}h_{i j q r}^{(\\omega)}h_{i j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}-{\\bf E}_{i,j,q,r,i^{\\prime},j^{\\prime},q^{\\prime},r^{\\prime}}h_{i j q r}^{(\\omega)}h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(\\omega)}\\Big].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we obtain a similar result as before, i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\sigma_{\\omega}^{2}(\\theta_{k},\\theta_{l})-\\sigma_{\\omega}^{2}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq1024R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+1024R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which completes the proof. ", "page_idx": 23}, {"type": "text", "text": "Also for the term associated with the estimated threshold (recall that it is computed from the first two moments), we obtain the following properties of $\\mathcal{E}_{0},\\mathcal{V}_{0}$ as defined in Theorem 1. ", "page_idx": 24}, {"type": "text", "text": "Lemma 5 (Lipschitz Property of of $\\mathcal{E}_{0},\\mathcal{V}_{0})$ . Maintaining the same conditions and notions as in Lemma 4, we have the following Lipschitz property ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{E}_{0}(\\theta_{k},\\theta_{l})-\\mathcal{E}_{0}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq2C_{0}R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+2C_{0}R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\\\ &{|\\mathcal{V}_{0}(\\theta_{k},\\theta_{l})-\\mathcal{V}_{0}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq128C_{1}R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+128C_{1}R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the constant $\\begin{array}{r}{C_{0}(n):=\\frac{n^{2}}{(n-1)^{2}}}\\end{array}$ (nn\u22121)2 and C1(n) := $\\begin{array}{r}{C_{1}(n):=\\frac{n(n-4)(n-5)}{(n-1)(n-2)(n-3)}}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. The expression in Theorem 1, while easy to compute, is not suitable for this part of our proof. We begin by obtaining equivalent expressions for $\\mathcal{E}_{0},\\mathcal{V}_{0}$ . According to Eq. (39), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{0}=\\left[1-\\frac{\\mathbf{1}^{T}\\left(\\mathbf{\\Lambda}_{X}\\mathbf{A}_{X}^{T}-\\mathbf{I}_{n}\\right)\\mathbf{1}}{n(n-1)}\\right]\\left[1-\\frac{\\mathbf{1}^{T}\\left(\\mathbf{\\Lambda}_{Y}\\mathbf{A}_{Y}^{T}-\\mathbf{I}_{n}\\right)\\mathbf{1}}{n(n-1)}\\right]}\\\\ &{\\quad=C_{0}\\left[1-\\frac{\\mathbf{1}^{T}\\left(\\mathbf{\\Lambda}_{X}\\mathbf{\\Lambda}_{X}^{T}\\right)\\mathbf{1}}{n^{2}}\\right]\\left[1-\\frac{\\mathbf{1}^{T}\\left(\\mathbf{\\Lambda}_{Y}\\mathbf{A}_{Y}^{T}\\right)\\mathbf{1}}{n^{2}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "And for ${\\dot{\\nu}}_{0}$ , we use Eq. (45) and obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{V}_{0}=\\frac{2C_{1}}{n^{4}}\\big[\\mathrm{Tr}(\\mathbf{A}_{X}\\mathbf{A}_{X}^{T}\\mathbf{H}\\mathbf{A}_{X}\\mathbf{A}_{X}^{T}\\mathbf{H})\\big]\\big[\\mathrm{Tr}(\\mathbf{A}_{Y}\\mathbf{A}_{Y}^{T}\\mathbf{H}\\mathbf{A}_{Y}\\mathbf{A}_{Y}^{T}\\mathbf{H})\\big].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Also, we define hi(jkq)r $\\begin{array}{r}{h_{i j q r}^{(k)}:=\\frac{1}{4!}\\sum_{(t,u,v,w)}^{(i,j,q,r)}k_{t u}^{(\\omega)}k_{t u}^{(\\omega)}+k_{t u}^{(\\omega)}k_{v w}^{(\\omega)}-2k_{u v}^{(\\omega)}k_{t v}^{(\\omega)}}\\end{array}$ that corresponding to $h_{i j q r}^{(\\omega)}$ and also define $\\begin{array}{r}{\\iota_{i j q r}^{(l)}:=\\frac{1}{4!}\\sum_{(t,u,v,w)}^{(i,j,q,r)}l_{t u}^{(\\omega)}l_{t u}^{(\\omega)}+l_{t u}^{(\\omega)}l_{v w}^{(\\omega)}-2l_{u v}^{(\\omega)}l_{t v}^{(\\omega)}}\\end{array}$ ). Then we can further rewrite the term for $X$ as $\\begin{array}{r}{\\frac{1}{n^{2}}\\mathrm{Tr}(\\mathbf{A}_{X}\\mathbf{A}_{X}^{T}\\mathbf{H}\\mathbf{A}_{X}\\mathbf{A}_{X}^{T}\\mathbf{H})=\\frac{1}{n^{4}}\\sum_{i,j,q,r}h_{i j q r}^{(k)}}\\end{array}$ hi(jkq)r and for Y by analogy. Then the properties of hi(j\u03c9q)r can also be obtained for hi(jkq)r and h(l) , e.g., $|h_{i j q r}^{(k)}|\\,\\leq\\,4,|h_{i j q r}^{(l)}|\\,\\leq\\,4$ for all $i,j,q,r$ . Next we start to prove the Lipschitz property of $\\bar{\\mathcal{E}_{0}},\\mathcal{V}_{0}$ . For $\\check{\\mathcal{E}}_{0}$ , according to Eq. (81) and combining the results $0\\leq\\mathbf{1}^{T}(\\mathbf{\\Lambda}\\mathbf{A}_{X}\\mathbf{\\Lambda}_{X}^{T})\\mathbf{1}\\leq n^{\\frac{5}{2}}$ and that for $Y$ , we can show that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{|\\mathcal{E}_{0}(\\theta_{k},\\theta_{l})-\\mathcal{E}_{0}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|\\leq\\displaystyle\\frac{C_{0}}{n^{2}}\\sum_{i,j}\\lvert k_{i j}^{(\\omega)}(\\theta_{k})-k_{i j}^{(\\omega)}(\\theta_{k}^{\\prime})\\rvert+\\displaystyle\\frac{C_{0}}{n^{2}}\\sum_{q,r}\\lvert l_{q r}^{(\\omega)}(\\theta_{l})-l_{q r}^{(\\omega)}(\\theta_{l}^{\\prime})\\rvert}}}\\\\ {{\\leq2C_{0}R_{\\omega_{k}}L_{k}\\cdot\\lVert\\theta_{k}-\\theta_{k}^{\\prime}\\rVert+2C_{0}R_{\\omega_{l}}L_{l}\\cdot\\lVert\\theta_{l}-\\theta_{l}^{\\prime}\\rVert,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{1}^{T}(\\mathbf{A}_{X}\\mathbf{A}_{X}^{T})\\mathbf{1}=\\sum_{i,j}k_{i j}^{(\\omega)}}\\end{array}$ by definition and so as for $Y$ . And for ${\\dot{\\nu}}_{0}$ , we can prove that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\mathcal{V}_{0}(\\theta_{k},\\theta_{l})-\\mathcal{V}_{0}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})|}\\\\ &{\\le\\,\\displaystyle\\frac{2C_{1}}{n^{8}}\\sum_{\\substack{i,j,q,r,i^{\\prime},j^{\\prime},q^{\\prime},r^{\\prime}}}\\left|h_{i j q r}^{(k)}(\\theta_{k},\\theta_{l})h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(l)}(\\theta_{k},\\theta_{l})-h_{i j q r}^{(k)}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})h_{i^{\\prime}j^{\\prime}q^{\\prime}r^{\\prime}}^{(l)}(\\theta_{k}^{\\prime},\\theta_{l}^{\\prime})\\right|}\\\\ &{\\le\\,128C_{1}R_{\\omega_{k}}L_{k}\\cdot\\|\\theta_{k}-\\theta_{k}^{\\prime}\\|+128C_{1}R_{\\omega_{l}}L_{l}\\cdot\\|\\theta_{l}-\\theta_{l}^{\\prime}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequation is obtained similar to Eq. (85), thus completes the proof. ", "page_idx": 24}, {"type": "text", "text": "The following results extend the results in [30] to the more general case (we only restrict the mapping functions to satisfy the Lipschitz property, and thus include the Gaussian kernel case of their proof). ", "page_idx": 24}, {"type": "text", "text": "Theorem 4 (Smoothness of Optimization Objective). Let the sample of size n be $Z$ , and with $a$ small positive constant $c$ , let the set of the parameters be $\\overline{{\\Theta}}_{c}:=\\{(\\theta_{k},\\theta_{l})|\\widehat{\\sigma}_{\\omega}\\geq c,\\mathcal{V}_{0}\\geq c,\\mathcal{E}_{0}\\geq c\\}$ , then there exist a nonnegative constant $L$ such that $\\|\\nabla_{(\\theta_{k},\\theta_{l})}J\\|\\leq L$ on $\\overline{{\\Theta}}_{c},$ , where the optimization objective $J:=[H S I C_{\\omega}(Z)-\\widehat{c_{\\alpha}}/n]/\\widehat{\\sigma}_{\\omega}$ is that we used in practice. ", "page_idx": 24}, {"type": "text", "text": "Proof. According to Lemma 4, we have shown that $\\mathrm{HSIC}_{\\omega}(Z),\\widehat{\\sigma}_{\\omega}$ both ftis the Lipschitz condition. Also, according to Lemma 5, we have shown that $\\mathcal{E}_{0},\\mathcal{V}_{0}$ are also Lipschitz with respect to $\\theta_{k},\\theta_{l}$ . Since the threshold $\\widehat{c_{\\alpha}}$ is completely determined by these two moments, combining the smoothness property of the mapping from two moments to thresholds as analyzed in [30, Theorem 2], we obtain that $\\widehat{c_{\\alpha}}$ is also Lipschitz with respect to $\\theta_{k},\\theta_{l}$ on $\\overline{{\\Theta}}_{c}$ . As a result, we complete the entire proof based on t h e Lipschitz property of composite mappings. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Remark. ${\\mathcal{E}}_{0}$ and $\\lvert\\lambda_{0}$ are positive is almost satisfied in practice since according to the definition only $[\\mathbf{1}^{T}\\mathbf{A}_{X c}^{\\cdot2}\\mathbf{1}][\\mathbf{1}^{T}\\mathbf{A}_{Y c}^{\\cdot2}\\mathbf{1}]$ and $[\\mathbf{1}^{T}(\\mathbf{\\Lambda}_{X c}^{T}\\mathbf{\\Lambda}_{}_{}\\mathbf{\\Lambda}_{X c})^{\\cdot2}\\mathbf{1}][\\mathbf{1}^{T}(\\mathbf{\\Lambda}_{Y c}^{T}\\mathbf{\\Lambda}_{}\\mathbf{\\Lambda}_{Y c})^{\\cdot2}\\mathbf{1}]$ need to be greater than 0. ", "page_idx": 24}, {"type": "text", "text": "J Details of Experiment Setup ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we give an introduction to the comparison methods in our experiments and provide the implementation details of each method. ", "page_idx": 25}, {"type": "text", "text": "J.1 Details of Comparison Methods ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The methods of comparison used in the experiment are described below. ", "page_idx": 25}, {"type": "text", "text": "\u2022 dCor [39]: An independence test that is based on the distance covariance.   \n\u2022 QHSIC [14]: The original quadratic-time HSIC independence test.   \n\u2022 RDC [26]: The randomized dependence coefficient that measures the independence using the canonical correlation between a finite set of random features of the copula.   \n\u2022 NyHSIC [44]: A variant of HSIC that uses the Nystr\u00f6m method to approximate kernels.   \n\u2022 FHSIC [44]: A variant of HSIC that uses the random Fourier feature to approximate kernels.   \n\u2022 BHSIC [44]: A variant of HSIC with the block-based statistic.   \n\u2022 HSICAgg [32]: An aggregated kernel test with the incomplete statistic of HSIC.   \n\u2022 NFSIC [18]: A test uses the normalized version of the finite set independence criterion and chooses features on a hold-out validation set to optimize a lower bound on the test power. ", "page_idx": 25}, {"type": "text", "text": "Below are the GitHub URLs for each comparison method. ", "page_idx": 25}, {"type": "text", "text": "\u2022 dCor: https://pypi.org/project/dcor.   \n\u2022 QHSIC: https://github.com/amber0309/HSIC/blob/master/HSIC.py.   \n\u2022 RDC: https://github.com/lopezpaz/randomized_dependence_coefficient.   \n\u2022 NyHSIC: https://github.com/oxcsml/kerpy/blob/master/independence_testing.   \n\u2022 FHSIC: https://github.com/oxcsml/kerpy/blob/master/independence_testing.   \n\u2022 BHSIC: https://github.com/oxcsml/kerpy/blob/master/independence_testing.   \n\u2022 HSICAgg: https://github.com/antoninschrab/mmdagg/tree/master/mmdagg.   \n\u2022 NFSIC: https://github.com/wittawatj/fsic-test/blob/master/fsic. ", "page_idx": 25}, {"type": "text", "text": "Time Complexity. Among them, dCor and QHSIC are the tests of quadratic complexity with sample size $n$ , i.e., $\\textstyle{\\boldsymbol{\\mathcal{O}}}(n^{2})$ . RDC is calculated in $\\mathcal{O}(n\\log n)$ and the rest are linear-time tests, i.e., $O(n)$ . ", "page_idx": 25}, {"type": "text", "text": "Threshold. For dCor, QHSIC, RDC, NyHSIC, FHSIC, and BHSIC, we permute the samples 100 times to simulate the null distribution and compute the threshold. The thresholds for the remaining methods are obtained by asymptotic null distribution, i.e., we set the test threshold to the $(1-\\alpha)$ - quantile of $\\chi^{2}(J)$ for NFSIC and obtain the test threshold of LFHSIC-G/M by gamma approximation. ", "page_idx": 25}, {"type": "text", "text": "Details of Setup. The number of random features for FHSIC, LFHSIC-G/M, the number of induced variables for NyHSIC, the block size for BHSIC and the number of sub-diagonals $R$ for HSICAgg are all kept consistent as recommended in [44] for fair evaluation. Specifically, we set the number of random mappings in RDC to 20 to ensure compatibility with large-scale datasets. The test location parameter $J$ of NFHSIC is set as default as 10, since it differs from other approximation methods that blindly increasing $J$ may lead to a loss of power as shown in [18] and can significantly escalate time costs due to its cubic time complexity $\\bar{\\mathcal{O}}\\bar{(J^{3})}$ . In the optimization step, for stabilizing the training, in the implementation of NFSIC we determine the initial bandwidth by searching the best from 25 bandwidth combinations (including the median bandwidth combination). For LFHSIC-G/M, to be fair, we perform the same grid search on SD, Sin, and GSign datasets. In other experiments, we still use the median bandwidth as initialization for LFHSIC-G/M. Also, the maximum number of iterations for the optimization is set to 100 for NFSIC and LFHSIC-G/M. The default learning rate of the optimization of LFHSIC-G/M is set as 0.05 in all the experiments. As for HSICAgg, the default implementation of the predefined 25 pairs of bandwidths in its code is used. For synthetic data, we set the split ratio to 0.5 for NFSIC and LFHSIC-G/M, i.e., we randomly sample half of the data for training and use the remaining for independence testing, while the other methods use all data for testing. For real MSD data, we divide a small portion of the data for training and then extract 100 random subsets of the remaining data (disjoint from the training set) for evaluation. ", "page_idx": 25}, {"type": "text", "text": "J.2 Details of Datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The details of the four synthetic datasets and two real datasets are described below. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Sine Dependency (SD): In this model, $X$ follows a $d$ -dimension multivariate normal distribution $\\mathcal{N}_{d}(0,I_{d})$ , and $Y$ is defined as $20\\sin\\!\\big(4\\pi(X_{1}^{2}+X_{2}^{2})\\big)\\!+\\!Z$ , where $X_{i}$ is the $i$ -th dimension of $X$ , and $Z\\sim\\mathcal{N}(0,1)$ represents independent noise. Notably, when $d>2$ , $Y$ exhibits a nonlinear relationship solely with the first two dimensions of $X$ . ", "page_idx": 26}, {"type": "text", "text": "\u2022 Sinusoid $(\\mathbf{Sin})$ : This model introduces a localized alteration in the probability density function $p_{x y}$ over $\\mathcal{X}\\times\\mathcal{Y}:=[-\\pi,\\pi]^{2}$ , specified as $(X,Y)\\sim p_{x y}(x,y)\\propto1+\\sin(\\omega x)\\sin(\\omega y)$ , where $\\omega$ denotes the frequency. Increasing the frequency enhances the similarity between the sampled data and that drawn from Uniform $([-\\pi,\\Bar{\\pi}]^{2})$ , thereby augmenting the challenge of detecting dependency with limited sample sizes. An example visualization is shown on the left of Fig. 6. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Gaussian Sign (GSign): In this model, $X$ follows a $d_{\\cdot}$ -dimension multivariate normal distribution $\\mathcal{N}_{d}(0,I_{d})$ , and $Y$ is expressed as $\\begin{array}{r}{|Z|\\prod_{i=1}^{d}\\operatorname{sgn}(X_{i})}\\end{array}$ , where $\\operatorname{sgn}({\\mathord{\\cdot}})$ represents the sign function, $X_{i}$ denotes the $i$ -th dimension of $X$ ,  and $\\bar{Z}\\sim\\mathcal{N}(0,1)$ is independent of . The challenge lies in $Y$ being independent of any proper subset of $X$ but dependent on $X$ as a whole, underscoring the importance of considering all dimensions of $X$ simultaneously in independence testing. ", "page_idx": 26}, {"type": "text", "text": "\u2022 ISA Dataset. We construct the data through the following steps: First, we generate $n$ i.i.d samples of two univariate random variables with a mixture of Gaussian model, i.e. $\\textstyle{\\frac{1}{2}}{\\mathcal{N}}(-1,0.0{\\dot{1}})+$ $\\scriptstyle{\\frac{1}{2}}{\\mathcal{N}}(1,0.01)$ . Second, we mix these random variables using a rotation matrix parameterized by an angle $\\theta$ , which varies from 0 to $\\pi/4$ . A zero angle implies independence between the data, while a larger angle signifies stronger dependency. Third, we append noise with a distribution of $\\mathcal{N}_{d-1}(0,I_{d-1})$ to each of the mixtures. Finally, we multiply an independent random $d\\!.$ dimensional orthogonal matrix to obtain vectors dependent across all observed dimensions. The resulting random variables $X$ and $Y$ are dependent but uncorrelated. When $d$ is greater than 1, the problem is associated with the independent subspace analysis (ISA) problem [14]. For the case $d=1,\\theta=\\pi/10$ , an example visualization is shown in the middle of Fig. 6. ", "page_idx": 26}, {"type": "text", "text": "\u2022 3DShapes Dataset. This dataset [5] comprises images depicting 3D scenes, complete with additional features like shadows and backgrounds. It encompasses six fundamental latent factors: floor hue, wall hue, object hue, object scale, object shape, and orientation, all adjustable to generate corresponding images. Orientation is treated as a dependency factor for independence testing, where we test the dependency between the image $X$ and its orientation $Y$ . To heighten the challenge, we maintain the object shape as a ball, minimizing the apparent orientation feature compared to other shapes like squares, while randomizing the remaining factors. An example visualization is given on the right side of Fig. 6. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Million Song Dataset. The dataset, a subset of the Million Song Data3 [4], comprises 515, 345 songs with 91-dimensional features. The first dimension represents the release year of each song, designated as variable $Y$ , while the remaining features (e.g., timbre average and timbre covariance) form variable $X$ . Our objective is to identify the dependency between $X$ and $Y$ . ", "page_idx": 26}, {"type": "image", "img_path": "BEiqNQZIky/tmp/df13a442d3b12ad227cf8e8192835b24bf47f057c640e9d200fc85abd2f2a70c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 6: Examples of visualization of samples from different datasets. The two plots on the left correspond to the samples and their contour under Sin dataset $n=5000$ , $w=2,$ ), and ISA dataset $(n=1000,d=1,\\theta=\\pi/10)$ , respectively. Right: a visualization of the causal diagram of the data generation process and some generated examples. ", "page_idx": 26}, {"type": "text", "text": "K Additional Experiment Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide additional experimental results, mainly including the visualization results on the Sin synthetic dataset, the results with more comparing methods (as explained in the main paper, due to the high time overhead therefore do not participate in the evaluation of the main paper) as well as the running time of each method. ", "page_idx": 27}, {"type": "text", "text": "K.1 The visualization results on the Sin model ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We provide the visualization results on the Sin synthetic dataset to illustrate the performance of our optimization objective for the example mentioned in the main paper. The results are shown in Fig. 7, where the setup follows our experiments $(n=2000,\\omega=5,D=100)$ . For visualization, the negative of our optimization objective $J$ is shown. As can be seen, our optimization objective guides to letting the bandwidth adapt to improve the test power, and here we can see that regions with bandwidths around 0.2 (corresponding to the theoretical optimal solution in our main paper) indicate better power, thus corroborating the validity of our optimization objective. Also, notice that the landscape is smooth over a wide range as demonstrated in Theorem 4, and thus contributes to non-convex optimization. ", "page_idx": 27}, {"type": "image", "img_path": "BEiqNQZIky/tmp/a6be8524f7866e60c8509934071b37fd7854508b28e898ba75f55ba47756afd1.jpg", "img_caption": ["Figure 7: The visualization results on the Sin model. Left: the samples with $n=2000,\\omega=5$ . The visualization of the landscape of the negative of our optimization objective $D=100)$ ). "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "K.2 Additional experimental comparisons ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As mentioned in the main paper, the method4[30] is not involved in the comparison because it takes too much time to run in some settings. Here, we compare it with ours under some feasible settings to illustrate the improvement of our method on power-runtime trade-offs. The methods using Gaussian kernel with global width and Gaussian kernel with widths of each dimension (corresponding with ours) are employed, referred to QHSIC-O and QHSIC-W, respectively. For fairness, the same grid search procedure is employed as the initialization of the optimization. We perform the evaluation on the SD data and plot the results of the test power over time as shown in Fig. 8. Also, for our methods, we provide the results under the setting $D=100$ and $D=500$ as in the main paper. The experiments are conducted with the same equipment, specifically a 6-core CPU with a 3080 GPU. ", "page_idx": 27}, {"type": "text", "text": "Results. Our test consistently results in a better power-runtime tradeoff at different $D$ settings. At $D=100$ , one test can be completed in less than a second when the sample size $n=6000$ . As $D$ increases $:D=500\\$ ), the number of samples required to achieve the same power decreases, but the increase in $D$ leads to an overhead in runtime, and overall our test is still completed in a few seconds. In contrast, even though QHSIC-O/W requires fewer samples to reach the same power than our tests, the runtime rises rapidly as the sample size increases. When $n=1000$ , it already takes more than $10s$ to perform a test. When $n=3000$ , it needs nearly a minute to perform a test, which may greatly limit the practical application. ", "page_idx": 27}, {"type": "image", "img_path": "BEiqNQZIky/tmp/92ef725fb773d13a156159b3d735784d52ee9be6af62ae9c4a1d7c74ad471858.jpg", "img_caption": ["Figure 8: Time-power trade-off curves on the SD dataset. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "K.3 Running Time ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this part, we evaluate the running time of each method on ISA datasets $d=10$ . We set $D=100$ and plot the results of the running time versus sample size $n$ as shown in Fig. 9. Shown on the left are the results of tests with $O(n)$ and $O(n\\log n)$ time complexity. Since the quadratic time complexity test cannot handle large-scale inputs of 100,000 sample size (excessive runtime and large memory overhead to store the kernel matrix), we evaluate them separately on the right. The experiments are all conducted on the same equipment, specifically a 14-core CPU with a 4090 GPU. ", "page_idx": 28}, {"type": "image", "img_path": "BEiqNQZIky/tmp/2e9052aaaa93f9afea5b50e860e22da171b8cae4aa4c1c2457ac840cac3a477b.jpg", "img_caption": ["Figure 9: The running time curves with sample size $n$ on the ISA dataset $d=10$ ). "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Results. The experimental results on the left show that our method is faster than other methods with optimizable options (HSICAgg and NFSIC), and can complete a test within 10 seconds even with 100, 000 samples. Even though HSICAgg uses parallelism to optimize the computational efficiency of the scheme, the actual implementation of the parallelism is time-consuming, and hence leads to a high time overhead for a single practical test. For the results on the right, it can be seen that the quadratic complexity methods face a dramatic increase in time overhead as the sample size rises, and this is especially severe for the methods (QHSIC-O/W) that need to be optimized since the optimizing objective needs to perform multiple squared complexity operations. In contrast, our linear-time learning objective allows us to handle huge data samples very efficiently. ", "page_idx": 28}, {"type": "text", "text": "L Limitations and Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Limitations. According to the experimental results in the main paper as well as in the Appendix, no one method is better than the others in all settings, so it is important to choose several appropriate tests for real scenarios and summarize their results in order to obtain a more reliable conclusion. ", "page_idx": 28}, {"type": "text", "text": "Broader Impacts. This work proposes a novel framework for independence testing. The proposed linear-time optimization objective can be trained end-to-end in a data-driven manner, ensuring both effectiveness and efficiency in high-dimensional and large-scale scenarios. This could be beneficial for developing more reliable downstream algorithms in a variety of areas, including causal discovery, feature selection, and deep learning. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 29}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 29}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 29}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 29}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 29}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 29}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: I\u2019ve clearly stated the contribution. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See the Sec. L in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: See Sec. 5 in the main paper. Also, the summarized assumptions and the proofs are provided in Appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide the details to reproduce the main experimental results, please See Sec. J in Appendix, and we also provide the experimental data/code in the supplemental material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provide the experimental data/code in the supplemental material. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 31}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we provide the details. See Sec. J in Appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The experimental results are accompanied by statistical significance tests. See Sec. 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources. See Sec. K in Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: I read it. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: See the Sec. L in the Appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We cite the methods used and list the URLs of the comparison methods. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]