[{"figure_path": "GuY0zB2xVU/figures/figures_2_1.jpg", "caption": "Figure 1: Multi-environment setup for the Kolmogorov PDE. The model is trained on multiple environments with several trajectories per environment (left). At inference, for a new unseen environment it is adapted on one trajectory (right).", "description": "The figure illustrates the training and inference stages for solving parametric PDEs.  The training stage involves multiple environments (each defined by unique parameters), with multiple trajectories sampled within each environment. The inference stage demonstrates the model's ability to adapt to a single trajectory from a completely new, unseen environment.", "section": "3 Motivations: ERM versus adaptive approaches for parametric PDEs"}, {"figure_path": "GuY0zB2xVU/figures/figures_3_1.jpg", "caption": "Figure 2: Comparison of ERM approaches (shades of blue) and Poseidon foundation model (green) with our framework GEPS (red) when increasing the number of training environments.", "description": "The figure compares the performance of several methods for solving parametric PDEs, including classical ERM approaches with different neural network architectures (CNN, FNO, MP-PDE, Transolver) and a pre-trained foundation model (Poseidon), against the proposed GEPS method.  The experiment is performed for in-distribution generalization, where the number of training environments is varied.  The results show that GEPS consistently outperforms other methods, especially as the number of training environments increases, highlighting the effectiveness of adaptive conditioning in handling the diversity of dynamical systems.", "section": "Motivations: ERM versus adaptive approaches for parametric PDEs"}, {"figure_path": "GuY0zB2xVU/figures/figures_4_1.jpg", "caption": "Figure 2: Comparison of ERM approaches (shades of blue) and Poseidon foundation model (green) with our framework GEPS (red) when increasing the number of training environments.", "description": "The figure compares the performance of various neural PDE solvers, including ERM approaches (CNN, FNO, MP-PDE, Transolver) and a foundation model (Poseidon), against the proposed GEPS method, under in-distribution evaluation. The x-axis represents the number of training environments, and the y-axis represents the relative L2 loss on 32 unseen trajectories.  The results show that ERM approaches fail to capture the dynamics of increasing environments while the GEPS method demonstrates superior generalization performance.", "section": "3 Motivations: ERM versus adaptive approaches for parametric PDEs"}, {"figure_path": "GuY0zB2xVU/figures/figures_4_2.jpg", "caption": "Figure 4: Out-distribution generalization on 4 new environments using one trajectory per environment for fine-tuning or adaptation. Models have either been pretrained on 4 environments (left column) or 1024 environments (right columns). Metric is Relative L2 loss.", "description": "This figure compares the out-of-distribution generalization performance of different models (GEPS, CNN, Transolver, and Poseidon) when pretrained on either 4 or 1024 environments.  The models are evaluated on 4 new environments, using only one trajectory from each for adaptation (or fine-tuning for the baselines).  Relative L2 loss is shown for the Gray-Scott and Burgers equations, illustrating GEPS's superior performance in out-of-distribution scenarios.", "section": "3 Motivations: ERM versus adaptive approaches for parametric PDEs"}, {"figure_path": "GuY0zB2xVU/figures/figures_5_1.jpg", "caption": "Figure 5: Our adaptation framework for our data-driven model. Block in blue refers to the data-driven module Ga. Blocks Li in pink refer to the trainable modules. The green block describes the adaptation mechanism for the data-driven component, with WL\u2081 the weights of layer Li. Context vector ce conditions all the layers WL\u2081.", "description": "This figure illustrates the adaptation framework used in the GEPS method for data-driven models.  It shows a series of trainable modules (pink blocks) forming the core of the data-driven model (Ga, blue block).  A key element is the adaptive conditioning mechanism (green block) which updates a context vector (ce) to adapt to new unseen environments. This adaptation modifies the weights of each layer (Li) via a low-rank update using the context vector and pre-trained shared parameters (WL).  The input (ue) is processed sequentially through the modules and the output (ue_t+1) represents the model's prediction for the next time step.", "section": "4 GEPS method"}, {"figure_path": "GuY0zB2xVU/figures/figures_14_1.jpg", "caption": "Figure 6: Visualization of different behaviors for the damped and driven pendulum equation", "description": "This figure visualizes six different behaviors of a damped and driven pendulum equation. Each environment represents a unique set of parameters resulting in distinct pendulum motions.  The plots show the angle \u03b8(t) over time, showcasing variations like underdamped oscillations, overdamped decay, resonance, and more complex behaviors under the influence of both damping and forcing terms.  The figure is key to illustrating the diverse dynamics captured in the paper's experiments, motivating the need for models capable of generalizing across these widely varying conditions.", "section": "B Dataset details"}, {"figure_path": "GuY0zB2xVU/figures/figures_17_1.jpg", "caption": "Figure 7: MAE loss of the PDE parameters for the Pendulum equation during training and adaptation", "description": "The figure shows the mean absolute error (MAE) loss for the PDE parameters of the pendulum equation during both the training and adaptation phases of the GEPS model. The training phase involves learning initial parameters across multiple environments. The adaptation phase focuses on updating parameters for a specific new environment.  The plot illustrates how the MAE decreases over epochs (training iterations) during both training and adaptation, indicating that the model effectively learns and adapts the PDE parameters. The convergence behavior provides insights into the effectiveness of the proposed GEPS model in estimating and adapting PDE parameters for diverse scenarios.", "section": "D.1 PDE parameter estimation"}, {"figure_path": "GuY0zB2xVU/figures/figures_17_2.jpg", "caption": "Figure 7: MAE loss of the PDE parameters for the Pendulum equation during training and adaptation", "description": "This figure shows the mean absolute error (MAE) loss during training and adaptation of the PDE parameters for the Pendulum equation.  The plot displays the convergence of the MAE over epochs. The graph illustrates the effectiveness of the method for estimating PDE parameters both during the initial training phase and the subsequent adaptation phase when presented with new, unseen environments.", "section": "D.1 PDE parameter estimation"}, {"figure_path": "GuY0zB2xVU/figures/figures_18_1.jpg", "caption": "Figure 7: MAE loss of the PDE parameters for the Pendulum equation during training and adaptation", "description": "This figure shows the Mean Absolute Error (MAE) loss for the estimation of PDE parameters during both training and adaptation phases for the Pendulum equation.  The plot displays the MAE over epochs, illustrating the convergence of the parameter estimation process during training and its rapid adaptation when presented with new unseen environments during the adaptation phase.", "section": "D.1 PDE parameter estimation"}, {"figure_path": "GuY0zB2xVU/figures/figures_18_2.jpg", "caption": "Figure 7: MAE loss of the PDE parameters for the Pendulum equation during training and adaptation", "description": "This figure shows the mean absolute error (MAE) loss during the training and adaptation phases for estimating PDE parameters in the pendulum equation.  It illustrates the convergence of the parameter estimation process over training epochs and during the adaptation to new, unseen environments.", "section": "D.1 PDE parameter estimation"}, {"figure_path": "GuY0zB2xVU/figures/figures_19_1.jpg", "caption": "Figure 13: Model size with respect to code dimension", "description": "The figure shows how the model size changes with respect to the code dimension for both CoDa and GEPS on the Gray-Scott PDE.  It demonstrates the impact of the context vector's dimensionality on the overall number of parameters in each model.  GEPS shows significantly smaller model sizes across different context dimensions compared to CoDa.", "section": "D.3.1 Code dimension"}, {"figure_path": "GuY0zB2xVU/figures/figures_20_1.jpg", "caption": "Figure 14: Convergence speed of the model G\u03b8 to adapt to new environments for the Burgers dataset. Less than 100 steps, compared to CoDA which needs 500 steps. For both runs, we used the same learning rate lr = 0.01.", "description": "This figure compares the convergence speed of the proposed GEPS method and the CoDA method for adapting to new environments in the Burgers dataset.  GEPS shows significantly faster convergence, reaching a stable state in under 100 epochs, while CoDA requires approximately 500 epochs.  Both methods used the same learning rate (lr = 0.01). This illustrates GEPS's efficiency in adapting to new contexts.", "section": "D.3.2 Adaptation speed"}, {"figure_path": "GuY0zB2xVU/figures/figures_21_1.jpg", "caption": "Figure 5: Our adaptation framework for our data-driven model. Block in blue refers to the data-driven module Ga. Blocks Li in pink refer to the trainable modules. The green block describes the adaptation mechanism for the data-driven component, with WL\u2081 the weights of layer Li. Context vector ce conditions all the layers WL\u2081.", "description": "This figure illustrates the GEPS adaptation framework.  The framework consists of a data-driven module (blue) and trainable modules (pink).  The adaptation mechanism (green) uses a low-rank approximation with a context vector (ce) to adapt to new environments. The context vector conditions all the layers of the trainable modules, enabling efficient adaptation with minimal parameter updates.", "section": "4 GEPS method"}, {"figure_path": "GuY0zB2xVU/figures/figures_24_1.jpg", "caption": "Figure 16: Comparison of in-distribution and out-distribution predictions of a trajectory on 1D Burgers. The trajectories are predicted from t = 0 (purple) to t = 0.1 (red).", "description": "This figure compares the performance of different models (Ground truth, GEPS, CoDA, CAVIA, and LEADS) in predicting a trajectory for the 1D Burgers equation. It shows both in-distribution (left) and out-of-distribution (right) generalization. The in-distribution results show how well the models predict trajectories with initial conditions from the training data distribution. The out-of-distribution results show how well the models generalize to initial conditions outside of the training data distribution. Each row represents a different model, and each column represents a different time step. The color gradient represents the amplitude of the predicted trajectory, going from purple (low amplitude) to red (high amplitude).", "section": "3.2 Out-of-distribution generalization to new environments for IVP: classical vs. adaptive conditioning approaches"}, {"figure_path": "GuY0zB2xVU/figures/figures_25_1.jpg", "caption": "Figure 17: Prediction per frame for our approach on 2D Gray-Scott for an out-of-distribution trajectory. The trajectory is predicted from t = 0 to t = T'. In our setting, T = 19 and T' = 39.", "description": "This figure compares the performance of different models in predicting the evolution of a 2D Gray-Scott system.  The \"Ground truth\" shows the actual evolution of the system.  The other rows show the predictions by various models (GEPS, CoDA, CAVIA, LEADS) for an out-of-distribution trajectory (meaning the model was not trained on this specific parameter set). The figure illustrates how well each model generalizes to unseen conditions. It is organized to show the initial conditions and predictions at different time steps (t=0 to t=T', where T=19 and T'=39).", "section": "F Qualitative results"}, {"figure_path": "GuY0zB2xVU/figures/figures_26_1.jpg", "caption": "Figure 17: Prediction per frame for our approach on 2D Gray-Scott for an out-of-distribution trajectory. The trajectory is predicted from t = 0 to t = T'. In our setting, T = 19 and T' = 39.", "description": "This figure compares the model's predictions (GEPS, CoDA, CAVIA, LEADS) against the ground truth for a 2D Gray-Scott equation.  It shows the model's performance on an unseen (out-of-distribution) trajectory, demonstrating its ability to generalize to new conditions. The predictions are shown at different timesteps (t=0, t=T, and t=T') illustrating the evolution of the system.", "section": "3.2 Out-of-distribution generalization to new environments for IVP: classical vs. adaptive conditioning approaches"}]