[{"figure_path": "paobkszgIA/tables/tables_7_1.jpg", "caption": "Table 1: Quantitative results of our method compared to existing UDA methods, with both image-based and video-based, evaluated against MVSS [18]. Bold numbers are the best scores, and underline numbers are the second best scores. The IoU (%) of all classes and the average mIoU (%) are presented. Our method outperforms the best existing method by 4.3 mIoU (%) in average, even with the absence of pretrained optical flows (NOOF).", "description": "This table presents a quantitative comparison of the proposed method against several existing unsupervised domain adaptation (UDA) methods for video semantic segmentation.  The comparison is made using the MVSS dataset, focusing on performance under adverse weather conditions.  The table shows the Intersection over Union (IoU) for individual classes and the mean IoU (mIoU), highlighting the improvements achieved by the proposed method, particularly its ability to perform well without relying on pretrained optical flow.", "section": "4.1 Quantitative results"}, {"figure_path": "paobkszgIA/tables/tables_8_1.jpg", "caption": "Table 2: Quantitative results of our method compared to existing UDA methods, with both image-based and video-based, evaluated against MVSS [18]. Bold numbers are the best scores, and underline numbers are the second best scores. The IoU (%) of all classes and the average mIoU (%) are presented. Our method outperforms the best existing method by 5.8 mIoU (%) in average, even with the absence of pretrained optical flows (NOOF).", "description": "This table presents a quantitative comparison of the proposed method against existing unsupervised domain adaptation (UDA) methods for video semantic segmentation.  The comparison uses the MVSS dataset and considers both image-based and video-based approaches.  The key metric is mean Intersection over Union (mIoU), showing performance improvement with the proposed method, particularly in its superior performance compared to other methods without needing pre-trained optical flow.", "section": "4.1 Quantitative results"}, {"figure_path": "paobkszgIA/tables/tables_9_1.jpg", "caption": "Table 3: Ablation studies of our proposed techniques. We can observe that each component independently contributes to the overall improvement in performance.", "description": "This table presents the results of ablation studies conducted to evaluate the individual contribution of each component of the proposed method.  It shows the mIoU achieved by different combinations of the fusion block, temporal teacher, spatial teacher, and temporal augmentation on the VIPER to MVSS adaptation task. The baseline is the Accel model without any of the proposed components.  Each row adds one or more of the proposed components to assess their effect on the overall performance.", "section": "4.3 Ablation studies"}, {"figure_path": "paobkszgIA/tables/tables_12_1.jpg", "caption": "Table 1: Quantitative results of our method compared to existing UDA methods, with both image-based and video-based, evaluated against MVSS [18]. Bold numbers are the best scores, and underline numbers are the second best scores. The IoU (%) of all classes and the average mIoU (%) are presented. Our method outperforms the best existing method by 4.3 mIoU (%) in average, even with the absence of pretrained optical flows (NOOF).", "description": "This table presents a quantitative comparison of the proposed method's performance against existing Unsupervised Domain Adaptation (UDA) methods for video semantic segmentation.  The comparison uses the MVSS dataset and includes both image-based and video-based UDA techniques.  The key metrics are Intersection over Union (IoU) for each class and mean IoU (mIoU) across all classes.  The table highlights that the proposed method achieves superior performance compared to state-of-the-art methods, even without relying on pretrained optical flow.", "section": "4.1 Quantitative results"}, {"figure_path": "paobkszgIA/tables/tables_13_1.jpg", "caption": "Table 5: Quantitative results of our method compared to existing UDA methods, with both image-based and video-based, evaluated against Cityscapes-Seq [7]. Bold numbers are the best scores, and underline numbers are the second best scores. The IoU (%) of all classes and the average mIoU (%) are presented.", "description": "This table presents a quantitative comparison of the proposed method against existing unsupervised domain adaptation (UDA) methods for video semantic segmentation.  The evaluation is performed on the Cityscapes-Seq dataset under ideal weather conditions.  Both image-based and video-based UDA methods are included in the comparison.  The table shows the Intersection over Union (IoU) scores for each class and the mean IoU (mIoU), highlighting the superior performance of the proposed method.", "section": "4.1 Quantitative results"}, {"figure_path": "paobkszgIA/tables/tables_13_2.jpg", "caption": "Table 2: Quantitative results of our method compared to existing UDA methods, with both image-based and video-based, evaluated against MVSS [18]. Bold numbers are the best scores, and underline numbers are the second best scores. The IoU (%) of all classes and the average mIoU (%) are presented. Our method outperforms the best existing method by 5.8 mIoU (%) in average, even with the absence of pretrained optical flows (NOOF).", "description": "This table presents a comparison of the proposed method's performance against other state-of-the-art unsupervised domain adaptation (UDA) methods for video semantic segmentation.  The comparison is made using the MVSS dataset, and the metrics used are Intersection over Union (IoU) for each class and mean IoU (mIoU) overall.  The table highlights that the proposed method surpasses existing methods even without relying on pre-trained optical flow information.", "section": "4.1 Quantitative results"}, {"figure_path": "paobkszgIA/tables/tables_14_1.jpg", "caption": "Table 7: Network Structure of Fusion Block", "description": "This table details the architecture of the Fusion Block, a key component of the proposed model. It shows the sequence of convolutional layers (Conv), deformable convolutional layers (DeformConv), and activation functions (Sigmoid) used for merging feature information from adjacent frames.  The number of channels (C) is a variable depending on the number of classes in the segmentation task.", "section": "3.2 Temporal-Spatial Teacher-Student learning"}]