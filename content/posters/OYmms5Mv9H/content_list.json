[{"type": "text", "text": "Geometric Trajectory Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiaqi Han, Minkai Xu, Aaron Lou, Haotian Ye, Stefano Ermon Stanford University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and develop a novel transition kernel leveraging SE(3)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning for geometric structures is a fundamental task in many natural science problems ranging from particle systems driven by physical laws [1, 26, 43, 2, 15] to molecular dynamics in biochemistry [22, 16, 45, 10]. Modeling such geometric data is challenging due to the physical symmetry constraint [56, 43], making it fundamentally different from common scalar non-geometric data such as images and text. With the recent progress of generative models, many works have been proposed in generating 3D geometric structures like small molecules [66, 42, 21, 64] and proteins [59, 23], showing great promise in solving the equilibrium states of complex systems. ", "page_idx": 0}, {"type": "text", "text": "Despite this success, these existing methods are limited to synthesizing static structures and neglect the fact that important real-world processes evolve through time. For example, molecules and proteins are not static but always varying with molecular dynamics, which plays a vital role in analyzing possible binding activities [8, 20]. In this paper, we aim to study the generative modeling of geometric trajectories with the additional temporal dimension. While this problem is more practical and important, it is highly non-trivial with several significant challenges. First, geometric dynamics in 3D ubiquitously preserve physical symmetry. With global translation or rotation applied to a trajectory of molecular dynamics, the entire trajectory still describes the same dynamics and the generative model should estimate the same likelihood. Second, trajectories inherently contain the correspondence between frames in different timesteps, requiring generative models to hold a high capacity for capturing the temporal correlations. Last, moving from a single structure to a trajectory composed of multiple ones, the distribution we are interested in becomes much higher-dimensional and more diverse, considering both the initial conditions as well as potential uncertainties injected along the evolution of dynamics. ", "page_idx": 0}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/3ae1e4e69ba177ebeaa1dce7a2b37db8b1f8a00ad8f9d8a25c1548beca456319.jpg", "img_caption": ["Figure 1: Overview of GeoTDM. The forward diffusion $q$ gradually perturbs the input while the reverse process $p_{\\theta}$ , parameterized by EGTN, denoises samples from the prior. The condition $\\mathbf{x}_{c}^{[T_{c}]}$ , if available, is leveraged to construct the equivariant prior and as a conditioning signal in EGTN. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To this end, we propose geometric trajectory diffusion models (GeoTDM), a principled method for modeling the temporal distribution of geometric trajectories through diffusion models [49, 52, 53, 18], the state-of-the-art generative model on various domains such as images [6], videos [19], and molecules [64]. Our key innovation lies in designing an equivariant temporal diffusion over geometric trajectories, with the reverse process parameterized by equivariant transition kernels, ensuring the desired physical symmetry of the generated trajectory. To better excavate the complex spatial interactions and temporal correlations, we develop a novel temporal denoising network, where we stack equivariant spatial convolution and temporal attention. Our developments not only guarantee the desirable physical symmetry of the trajectories, but also capture the complex spatial and temporal correspondence encapsulated in the dynamics of geometric systems. Moreover, by leveraging generative modeling, GeoTDM enjoys high versatility in generating diverse yet high-quality geometric trajectories from scratch, performing interpolation and extrapolation, and optimizing noisy trajectory, all under the proposed diffusion framework. ", "page_idx": 1}, {"type": "text", "text": "In summary, we make the following contributions: 1. We present GeoTDM, a novel temporal diffusion model for generating geometric trajectories. We design the diffusion process to meet the critical equivariance in modeling both unconditional and conditional distributions over geometric trajectories. Notably, we also propose a conditional learnable equivariant prior for enhanced flexibility in temporal conditioning. 2. To fulfill the equivariance of the denoising network, we introduce EGTN, a graph neural network that operates on geometric trajectories, which also permits conditioning upon a given trajectory using equivariant cross-attention, making it suitable to serve as the backbone for GeoTDM. 3. We evaluate our GeoTDM on both unconditional and conditional trajectory generation tasks including particle simulation, molecular dynamics, and pedestrian trajectory prediction. GeoTDM can consistently outperform existing approaches on various metrics, with up to $56.7\\%$ lower prediction score for unconditional generation and $16.8\\%$ lower forecasting error for conditional generation on molecular dynamics simulation. We also show GeoTDM successfully performs several additional applications, such as temporal interpolation and trajectory optimization. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Trajectory modeling for geometric systems. Modeling the dynamics of geometric data is challenging since one must capture the interactions between multiple objects. Graph neural networks [11] have emerged as a natural tool to tackle this complexity [26, 41]. Subsequent works [43, 7, 2, 63] discovered equivariance as a critical factor for promoting model generalization. Among these efforts, Radial Fields [27] and EGNN [43] work with equivariant operations between scalars and vectors, while TFN [56] and SE(3)-Transformer [9] generalize to high-order spherical tensors. While considerable progress has been made, they only conduct (time) frame-to-frame prediction, which is subject to error accumulation when performing roll-out inference. Recently, EqMotion [62] approached the problem by learning to predict trajectories. By comparison, our GeoTDM leverages a generative modeling framework, which enables a wider range of tasks such as generation and interpolation. ", "page_idx": 1}, {"type": "text", "text": "Generative models in geometric domain. There is growing interest in developing generative models for geometric data, e.g.molecule generation [42, 66, 21, 64], protein generation [59, 24, 69], and antibody design [30]. Recently, diffusion-based models [21, 64] have been shown to yield superior performance compared to flow-based [42] and VAE-based [65] approaches in many of these tasks. Despite these fruitful achievements, most existing works only produce a snapshot of the geometric system, e.g., a molecule in 3D space, whereas our GeoTDM generalizes to generating a trajectory with multiple frames, e.g., an MD trajectory in 3D. DiffMD [60] specifically tackles MD modeling using Markovian assumption, while GeoTDM directly captures the joint distribution of all frames along the entire trajectory. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Temporal diffusion models. Diffusion models have been recently adapted to handle the natural temporality of data in tasks such as video generation [19, 58, 17], time series forecasting [37, 54], PDE simulation [38], human motion synthesis [55, 71] and pedestrain trajectory forecasting [12]. Distinct from these works, GeoTDM models the temporal evolution of geometric data represented as a geometric graph and maintains the aforementioned vital equivariance constraint. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models. Diffusion models [49, 18, 52, 53] are a type of latent variable generative model that feature a Markovian forward diffusion process and reverse denoising process. The forward process progressively perturbs the input $\\mathbf{x}_{\\mathrm{0}}$ (e.g., image pixe\u221als or molecule coordinates) over $\\tau$ steps using a Gaussian transition kernel $\\bar{q}(\\mathbf{x}_{\\tau}|\\mathbf{x}_{\\tau-1})=\\bar{\\mathcal{N}}(\\bar{\\mathbf{x}}_{\\tau};\\sqrt{1-\\beta_{\\tau}}\\mathbf{x}_{\\tau-1},\\beta_{\\tau}\\mathbf{I})$ . Here, $\\{\\mathbf{x}_{\\tau}\\}_{\\tau=1}^{\\tau}$ are latent variables with the same dimension as the input and $\\beta_{\\tau}$ are predefined using the noise schedule such that $\\mathbf{x}_{\\mathcal{T}}$ is close to being distributed as $\\bar{\\mathcal{N}}(\\mathbf{0},\\mathbf{I})$ . The reverse process maps back from the prior distribution with $p(\\mathbf{x}_{\\mathcal{T}})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ using the kernel $p_{\\pmb\\theta}(\\mathbf x_{\\tau-1}|\\mathbf x_{\\tau})=\\mathcal N(\\mathbf x_{\\tau-1};\\pmb\\mu_{\\pmb\\theta}(\\mathbf x_{\\tau},\\tau),\\sigma_{\\tau}^{2}\\mathbf I)$ , where the variances $\\sigma_{\\tau}^{2}$ are usually fixed and the mean $\\pmb{\\mu_{\\theta}}$ is parameterized by a neural network with parameters $\\pmb{\\theta}$ . The model is trained by optimizing the variational lower bound, defined as $\\begin{array}{r}{\\dot{\\mathbf{\\beta}}_{\\mathrm{clb}}=-\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1})+D_{\\mathrm{KL}}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0})||p(\\mathbf{x}_{T}))+\\sum_{\\tau=2}^{T-1}D_{\\mathrm{KL}}(q(\\mathbf{x}_{\\tau-1}|\\mathbf{x}_{\\tau},\\mathbf{x}_{0})||p_{\\theta}(\\mathbf{x}_{\\tau-1}|\\mathbf{x}_{\\tau})).}\\end{array}$ For training stability, [52, 18] suggest the noise-predic tion objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{simple}}:=\\mathbb{E}_{\\mathbf{x}_{0},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),\\tau}\\lambda(\\tau)\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{x}_{\\tau},\\tau)\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{0}\\sim p_{\\mathrm{data}}$ , $\\tau\\sim\\operatorname{Unif}(1,\\mathcal{T})$ , the weighting factors $\\lambda(\\tau)$ are typically set to 1 to promote sample quality, $\\mathbf{x}_{\\tau}\\,=\\,\\sqrt{\\bar{\\alpha}_{\\tau}}\\mathbf{x}_{0}\\,+\\,\\sqrt{1-\\bar{\\alpha}_{\\tau}}\\epsilon$ with $\\begin{array}{r}{\\bar{\\alpha}_{\\tau}:=\\prod_{s=1}^{\\tau}\\stackrel{}{\\alpha_{s}}=\\dot{\\prod}_{s=1}^{\\tau}(1-\\beta_{s})}\\end{array}$ , and $\\epsilon_{\\theta}$ is a specific parameterization of the mean satisfying \u00b5\u03b8(x\u03c4, \u03c4) =\u221a1\u03b1\u03c4 (x\u03c4 \u2212\u221a1\u03b2\u2212\u03c4\u03b1\u00af\u03c4 . ", "page_idx": 2}, {"type": "text", "text": "Equivariance. Functions. A function $f$ is equivariant w.r.t a group $G$ if $f(g\\cdot\\mathbf{x})=g\\cdot f(\\mathbf{x}),\\forall g\\in G$ Furthermore, $f$ is invariant if $f(g\\cdot\\mathbf{x})\\,=\\,f(\\mathbf{x}),\\forall g\\in G$ [46]. Here we focus on the group SE(3) consisting of all 3D rotations and translations2. Each group element $g\\in{\\mathrm{SE}}(3)$ can be represented by a rotation matrix $\\mathbf{R}$ and a translation $\\mathbf{r}\\in{\\mathbb{R}}^{3}$ . For geometrc graph with node features $\\mathbf{h}$ and coordinates $\\mathbf{x}$ , if ${\\bf h}^{\\prime},{\\bf x}^{\\prime}=f({\\bf h},{\\bf x})$ , we expect $\\mathbf{h}^{\\prime},\\mathbf{R}\\mathbf{x}^{\\prime}+\\mathbf{r}=f(\\mathbf{h},\\mathbf{\\bar{R}}\\mathbf{\\bar{x}}+\\mathbf{r})^{3}$ , i.e., the output node features are invariant while the updated coordinates are equivariant. Distributions. We call a density $p(\\mathbf{x})$ invariant $w.r t$ a group $G$ if $p(g\\cdot\\mathbf{x})=p(\\mathbf{x}),\\forall g\\in G$ . Intuitively, geometries that are rotationally and translationally equivalent should share the same density, since they all refer to the same structure. A conditional distribution $p(\\mathbf{x}|\\mathbf{y})$ is equivariant if $p(g\\cdot\\mathbf{x}|{\\dot{g}}\\cdot\\mathbf{y})=p(\\mathbf{\\dot{x}}|\\mathbf{y}),\\forall g\\in G.$ . Such a property is important in cases where the target distribution is conditioned on some given structures: if the observed geometry is rotated/translated, the target distribution should also rotate/translate accordingly. ", "page_idx": 2}, {"type": "text", "text": "Geometric trajectories and the distributions. We represent a geometric trajectory as $(\\mathbf{x}^{[T]},\\mathbf{h},\\mathcal{E})$ , where $\\mathbf{x}^{[T]}:=\\left[\\mathbf{x}^{(0)},\\mathbf{x}^{(1)},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\mathbf{x}^{(T-1)}\\right]\\in\\mathbb{R}^{T\\times N\\times D_{\\mathbf{x}}}$ is the sequence of temporal geometric coordinates, $\\mathbf{h}\\in\\mathbb{R}^{N\\,\\!\\top}\\mathbf{\\bar{\\times}}D_{\\mathbf{h}}$ is the node feature, and $\\mathcal{E}$ is the set of edges representing the connectivity of the geometric graph. $T$ is the number of time steps and $D_{\\mathbf{x}},D_{\\mathbf{h}}$ refers to the dimension of the coordinate and node feature respectively, with $D_{\\mathbf{x}}$ normally being 2 or 3 depending on the input data. In this work, we are interested in modeling the distribution of geometric trajectories given the configuration of the geometric graph, i.e., $p(\\mathbf{x}^{[T]}|\\mathbf{h},\\mathcal{E})$ . ", "page_idx": 2}, {"type": "text", "text": "Conditioning. Some applications like trajectory forecasting can be viewed as conditional generative tasks, where we seek to model the distribution of trajectories conditioning on certain observed timesteps, i.e., $p(\\mathbf{x}^{[T]}|\\mathbf{x}_{c}^{[T_{c}]},\\mathbf{h},\\mathcal{E})$ where $\\mathbf{x}_{c}^{[T_{c}]}\\in\\mathbb{R}^{T_{c}\\times N\\times D_{\\mathbf{x}}}$ is the provided trajectory in length $T_{c}$ . ", "page_idx": 2}, {"type": "text", "text": "Equivariance for geometric trajectories. Since the dynamics must be invariant to rotation or translation, the distribution of the geometric trajectories should also preserve such symmetry. This is formalized by the following invariance constraint: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathbf{x}^{[T]}\\mid\\mathbf{h},\\boldsymbol{\\mathcal{E}})=p(g\\cdot\\mathbf{x}^{[T]}\\mid\\mathbf{h},\\boldsymbol{\\mathcal{E}}),\\forall g\\in\\mathrm{SE}(3).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g\\cdot\\mathbf{x}^{[T]}:=[\\mathbf{R}\\mathbf{x}^{(0)}+\\mathbf{r},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{R}\\mathbf{x}^{(T-1)}+\\mathbf{r}].$ . The conditional case should instead preserve: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathbf{x}^{[T]}|\\mathbf{x}_{c}^{[T_{c}]},\\mathbf{h},\\mathcal{E})=p(g\\cdot\\mathbf{x}^{[T]}|g\\cdot\\mathbf{x}_{c}^{[T_{c}]},\\mathbf{h},\\mathcal{E}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $g\\in\\mathrm{SE}(3)^{4}$ . Intuitively, if the given trajectory is rotated and/or translated, the distribution of the future trajectory should also rotate and/or translate by exactly the same amount. For simplicity, we omit writing the conditions $\\mathbf{h}$ and $\\mathcal{E}$ henceforth when describing the distributions of trajectories. ", "page_idx": 3}, {"type": "text", "text": "4 Geometric Trajectory Diffusion Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce the machinery of GeoTDM. We first present Equivariant Geometric Trajectory Network (EGTN) in $\\S\\ 4.1$ , a general purpose backbone operating on geometric trajectories while ensuring equivariance. We then present GeoTDM in $\\S\\,^{4.2}$ for both unconditional and conditional generation using EGTN as the denoising network. ", "page_idx": 3}, {"type": "text", "text": "4.1 Equivariant Geometric Trajectory Network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our proposed Equivariant Geometric Trajectory Network (EGTN) is constructed by stacking equivariant spatial aggregation layers and temporal attention layers in an alternated manner, drawing inspirations from spatio-temporal GNNs [70, 61]. In particular, spatial layers characterize the structural interactions within the system and temporal layers model the temporal dependencies along the trajectory. For spatial aggregation, we employ the Equivariant Graph Convolution Layer (EGCL) [43], ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}^{\\prime(t)},\\mathbf{h}^{\\prime(t)}=\\mathrm{EGCL}(\\mathbf{x}^{(t)},\\mathbf{h}^{(t)},\\boldsymbol{\\mathcal{E}}),\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The equivariant message passing is conducted independently for each frame $t\\ \\ \\in\\ \\ [T]\\ \\ :=$ $\\{0,1,\\bar{\\cdot}\\cdot\\cdot,T-1\\}$ , with the goal of passing and fusing the geometric information based on the structure of the graph for each time step. Following such layer, we further develop a temporal layer equipped by self-attention, which has exhibited great promise for sequence modeling [57], to capture the temporal correlations encapsulated in the dynamics. We first compute Eqs. 5-6, where $\\mathbf{q}^{(t)},\\mathbf{k}^{(t,s)},\\mathbf{v}^{(t,s)}$ are the query, key, and value, ", "page_idx": 3}, {"type": "text", "text": "respectively. In detail, ${\\bf q}^{(t)}~=~\\bar{\\varphi_{\\bf q}}({\\bf h}^{(t)}),~{\\bf k}^{(t,s)}~=$ $\\begin{array}{r}{\\mathbf{a}^{(t,s)}=\\frac{\\exp\\big(\\mathbf{q}^{(t)\\top}\\mathbf{k}^{(t,s)}\\big)}{\\sum_{u\\in[T]}\\exp\\big(\\mathbf{q}^{(t)\\top}\\mathbf{k}^{(t,u)}\\big)},}\\\\ {\\mathbf{h}^{\\prime(t)}=\\mathbf{h}^{(t)}+\\displaystyle\\sum_{s\\in[T]}\\mathbf{a}^{(t,s)}\\mathbf{v}^{(t,s)},}\\end{array}$   \n$\\varphi_{\\mathbf{k}}(\\mathbf{h}^{(s)})+\\psi(t-s)$ , and $\\mathbf{v}^{(t,s)}=\\varphi_{\\mathbf{v}}(\\mathbf{h}^{(s)})+\\psi(t-s)$ ,   \nwith $\\psi(t-s)$ being the sinusodial encoding [57] of the   \ntemporal displacement $t-s$ , akin to the relative posi  \ntional encoding [47]. Incorporating such information is ", "page_idx": 3}, {"type": "text", "text": "crucial since the model is supposed to distinguish different time spans between two frames on the trajectory. Moreover, compared with directly encoding the absolute time step, our design is beneficial in that it ensures the temporal shift invariance of physical processes. The update of coordinates reuse the attention coefficients $\\mathbf{a}^{(t,s)}$ and the values $\\mathbf{v}^{(t,s)}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}^{\\prime(t)}=\\mathbf{x}^{(t)}+\\sum_{s\\in[T]}\\mathbf{a}^{(t,s)}\\varphi_{\\mathbf{x}}(\\mathbf{v}^{(t,s)})(\\mathbf{x}^{(t)}-\\mathbf{x}^{(s)}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\varphi_{\\mathbf{x}}$ is an MLP that outputs a scalar to preserve rotation equivariance. The entire network $f_{\\mathrm{EGTN}}$ , with schematic depicted in Fig. 4, is constructed by alternating spatial and temporal layers, enjoying equivariance as desired (proof in Appendix A.4): ", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathbf{Theorem~4.1~(SE(3))-equivariance)},\\ L e t\\ \\mathbf{x}^{\\prime[T]},\\mathbf{h}^{\\prime[T]}\\,=\\,f_{\\mathrm{EGTN}}\\left(\\mathbf{x}^{[T]},\\mathbf{h}^{[T]},\\boldsymbol{\\xi}\\right).\\ T h e t}\\\\ &{\\mathbf{x}^{\\prime[T]},\\mathbf{h}^{\\prime[T]}=f_{\\mathrm{EGTN}}\\left(g\\cdot\\mathbf{x}^{[T]},\\mathbf{h}^{[T]},\\boldsymbol{\\xi}\\right),\\forall g\\in S E(3).}\\end{array}$ n we have $g\\ \\cdot$ ", "page_idx": 3}, {"type": "text", "text": "Geometric conditioning. In certain tasks like trajectory forecasting, we are additionally provided with some partially observed trajectories as side input. In order to leverage their geometric information, we augment the unconditional EGTN with equivariant cross-attention, a conditioning technique tailored for geometric trajectories, and more importantly, guaranteeing the crucial equivariance in Theorem 4.1. In principle, our equivariant cross-attention resembles Eqs. 5-7, but instead computes the attention between the conditioning trajectory x[cTc]and the target x[T ]. In detail, the attention coefficients are recomputed as u\u2208[Tc]\u222a[T ] exp(q(t)\u22a4k(t,u)). The updated node feature h\u2032(t) and coordinate $\\mathbf{x}^{\\prime(t)}$ in Eqs. 6-7 are further renewed by the cross-attention terms, yielding $\\mathbf{h}^{\\prime\\prime(t)}=$ $\\begin{array}{r}{\\mathbf{h}^{\\prime(t)}+\\sum_{s\\in[T_{c}]}\\mathbf{a}^{(t,s)}\\mathbf{v}^{(t,s)}}\\end{array}$ and $\\begin{array}{r}{\\mathbf{x}^{\\prime\\prime(t)}=\\mathbf{x}^{\\prime(t)}+\\sum_{s\\in[T_{c}]}\\mathbf{a}^{(t,s)}\\varphi_{\\mathbf{x}}(\\mathbf{v}^{(t,s)})(\\mathbf{x}^{(t)}-\\mathbf{x}_{c}^{(s)})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.2 Geometric Trajectory Diffusion Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.2.1 Unconditional Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For unconditional generation, we seek to model the trajectory distribution subject to the SE(3)- invariance (Eq. 2). To design a diffusion with the reverse marginal conforming to the invariance, we impose certain constraints to the prior and transition kernel, as depicted in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. If the prior $p_{\\mathcal{T}}(\\mathbf{x}_{\\mathcal{T}}^{[T]})$ is $S E(3)$ -invariant, the transition kernels $p_{\\tau-1}(\\mathbf{x}_{\\tau-1}^{[T]}\\quad|$ $\\mathbf{x}_{\\tau}^{[T]}),\\forall\\tau\\ \\in\\ \\{1,\\cdots,T\\}$ are $S E(3)$ -equivariant, then the marginal $p_{\\tau}(\\mathbf{x}_{\\tau}^{[T]})$ at any step $\\tau\\ \\in$ $\\{0,\\cdot\\cdot\\cdot,\\tau\\}$ is also $S E(3)$ -invariant. ", "page_idx": 4}, {"type": "text", "text": "Prior in the translation-invariant subspace. Unfortunately, there is no properly normalized distribution w.r.t. Lebesgue measure on the ambient space $\\mathcal{X}:=\\mathbb{R}^{T\\times N\\times D}$ that permits translationinvariance [43]. We instead build the prior on a translation-invariant subspace $\\mathcal{X}_{\\mathbf{P}}\\subset\\mathcal{X}$ induced by a linear transformation $\\mathbf{P}\\in\\mathcal{X}\\!\\times\\!\\mathcal{X}$ with $\\mathrm{rank}(\\mathbf{P})=(T N\\!-\\!1)D$ [36]. Specifically, we choose the prior to be the projection of the Gaussian $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ in $\\mathcal{X}$ to $\\scriptstyle{\\mathcal{X}}_{\\mathbf{P}}$ by $\\begin{array}{r}{\\mathbf{P}=\\mathbf{I}_{D}\\otimes\\left(\\mathbf{I}_{T N}-\\frac{1}{T N}\\mathbf{1}_{T N}\\mathbf{1}_{T N}^{\\top}\\right)}\\end{array}$ , which cboerirnegs pthoen dcse tnot etrh-eo ff-unmcatsios n( $\\begin{array}{r}{P(\\mathbf{x}^{[T]})=\\mathbf{x}^{[T]}\\!-\\!\\frac{1}{T}\\sum_{t=0}^{T-1}\\operatorname{CoM}(\\mathbf{x}^{(t)})}\\end{array}$ o, tew $\\begin{array}{r}{\\mathrm{CoM}({\\bf x}^{(t)})=\\frac{1}{N}\\sum_{i=1}^{N}{\\bf x}_{i}^{(t)}}\\end{array}$ $t$ $\\widetilde{\\mathbf{x}}:=P(\\mathbf{x})$ distribution is a restricted Gaussian (denoted $\\tilde{\\mathcal{N}}(\\mathbf{0},\\mathbf{I}))$ with the variables supported only on the subspace (see App. A.1), and more importantly, is still isotropic and thus $\\mathrm{SO}(3)$ -invariant. To sample from the prior, one can alternatively sample $\\mathbf{x}^{[T]}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\in\\mathcal{X}$ and then project it to the subspace to obtain the final sample $\\tilde{\\mathbf{x}}^{[T]}=P(\\mathbf{x}^{[T]})\\in\\mathcal{X}_{\\mathbf{P}}$ . ", "page_idx": 4}, {"type": "text", "text": "Transition kernel. To be consistent with the prior, we also parameterize the transition kernel in the subspace $\\scriptstyle{\\mathcal{X}}_{\\mathbf{P}}$ , given by $p_{\\pmb{\\theta}}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\tilde{\\mathbf{x}}_{\\tau}^{[T]})\\,=\\,\\tilde{\\mathcal{N}}(\\tilde{\\mu}_{\\pmb{\\theta}}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau),\\sigma_{\\tau}^{2}\\mathbf{I})$ . In this way, if the mean function $\\tilde{\\mu}_{\\theta}(\\cdot)$ is $\\mathrm{SO}(3)$ -equivariant, then the transition kernel is also guaranteed $\\mathrm{SO}(3)$ -equivariance. As suggested by [18], we re-parameterize $\\begin{array}{r}{\\tilde{\\mu}_{\\boldsymbol{\\theta}}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)=\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left(\\tilde{\\mathbf{x}}_{\\tau}^{[T]}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\bar{\\alpha}_{\\tau}}}\\tilde{\\mathbf{\\epsilon}}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{\\tau}^{[T]},\\tau)\\right)}\\end{array}$ , where $\\tilde{\\epsilon}_{\\theta}=P\\circ\\mathbf{f}_{\\theta}$ , with $\\mathbf{f}_{\\theta}$ being an $\\mathrm{SO}(3)$ -equivariant adaptation of our proposed EGTN, fulfilled by subtracting the input coordinates from the output for translation invariance. The diffusion step $\\tau$ is transformed via time embedding and concatenated to the invariant node features $\\mathbf{h}^{[T]}$ in the input. ", "page_idx": 4}, {"type": "text", "text": "Training and inference. We optimize the VLB for training, which, interestingly, still has a surrogate in the noise-prediction form when specifying the factors $\\lambda(\\tau)$ as 1 (proof in App. A.1): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{uncond}}:=\\mathbb{E}_{\\mathbf{x}_{0}^{[T]},\\tilde{\\epsilon}\\sim\\tilde{\\mathcal{N}}(\\mathbf{0},\\mathbf{I}),\\tau\\sim\\mathrm{Unif}(1,T)}\\left[\\lVert\\tilde{\\epsilon}-\\tilde{\\epsilon}_{\\boldsymbol{\\theta}}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)\\rVert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The inference process is similar to [18] but with additional applications of $P$ in intermediate steps to keep all samples in the subspace $\\scriptstyle{\\mathcal{X}}_{\\mathbf{P}}$ . Details are in Alg. 1 and 2. ", "page_idx": 4}, {"type": "text", "text": "4.2.2 Conditional Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Distinct from the unconditional generation, in the conditional scenario the target distribution should instead be SE(3)-equivariant w.r.t. the given frames, as elucidated in Eq. 3. The following theorem describes the constraints to consider when designing the prior and transition kernel. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.3. If the prior $p_{T}(\\mathbf{x}_{T}^{[T]}|\\mathbf{x}_{c}^{[T_{c}]})$ is $S E(3)$ -equivariant, the transition kernels $p_{\\tau-1}(\\mathbf{x}_{\\tau-1}^{[T]}|\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]})$ , $\\forall\\tau\\;\\in\\;\\{1,\\cdots,T\\}$ are $S E(3)$ -equivariant, the marginal5 $p_{\\tau}(\\mathbf{x}_{\\tau}^{[T]}|\\mathbf{x}_{c}^{[T_{c}]})$ , $\\forall\\tau\\in\\{0,\\cdots,\\tau\\}$ is $S E(3)$ -equivariant. ", "page_idx": 4}, {"type": "text", "text": "Flexible equivariant prior. There are in general many valid choices for the prior while satisfying SE(3)-equivariance. We provide a guidance on distinguishing feasible designs when using Gaussianbased prior in the proposition below. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.4. $\\mathcal{N}(\\mu(\\mathbf{x}_{c}^{[T_{c}]}),\\mathbf{I})$ is $S E(3)$ -equivariant w.r.t. $\\mathbf{x}_{c}^{[T_{c}]}~i f\\mu(\\mathbf{x}_{c}^{[T_{c}]})$ is $S E(3)$ -equivariant. ", "page_idx": 5}, {"type": "text", "text": "Proof is in App. A.2. Notably, the mean of the prior $\\mathbf{x}_{r}^{[T]}:=\\boldsymbol{\\mu}(\\mathbf{x}_{c}^{[T_{c}]})$ naturally serves as an anchor to transit the geometric information in the provided trajectory to the target distribution we seek to model. For instance, one can choose it as a linear combination of the CoMs of the given frames, ia. ep.,r $\\begin{array}{r}{\\mathbf{x}_{r}^{[T]}=\\mathbf{1}_{T\\times N}\\otimes\\sum_{s\\in[T_{c}]}w^{(s)}\\bar{\\mathbf{x}}_{c}^{(s)}}\\end{array}$ ,e  wdhoeerse $\\begin{array}{r}{\\sum_{s\\in[T_{c}]}w^{(s)}=1}\\end{array}$ oarrael  cfoixnesdi sptearnacmy eotfe rtsh ed ettrearjemcitnoerdy and incurs extra effort in optimization, since the model needs to learn to reconstruct the complex structures from points all located at the CoM. In contrast, we propose the following instantiation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{x}_{r}^{(t)}=\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}\\hat{\\mathbf{x}}_{c}^{(s)},\\quad\\mathrm{s.t.}\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}=\\mathbf{1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $t\\,\\in\\,[T]$ , where each $\\mathbf{x}_{r}^{(t)}$ is a point-wise linear combination of $\\hat{\\mathbf{x}}_{c}^{(s)}$ , an $\\mathrm{SE}(3)$ -equivariant transformation of the conditioning frames, with $\\mathbf{w}^{(t,s)}\\,\\in\\,\\mathbb{R}^{N}$ being the weights. We first obtain $\\hat{\\mathbf{x}}_{c}^{[T_{c}]},\\hat{\\mathbf{h}}_{c}^{[T_{c}]}=\\mathbf{f}_{\\eta}(\\mathbf{x}_{c}^{[T_{c}]},\\mathbf{h}_{c}^{[T_{c}]})$ where $\\mathbf{f}_{\\eta}$ is a lightweight two layer EGTN that aims to synthesize the conditional information. The $\\mathbf{w}^{(t,s)}$ is then derived as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{W}_{t,s}=[\\gamma\\otimes\\hat{\\mathbf{h}}_{c}^{[T_{c}]}]_{t,s}\\in\\mathbb{R}^{N},}\\\\ {\\mathbf{w}^{(t,s)}=\\left\\{\\mathbf{W}_{t,s}\\right.}\\\\ {\\left.\\mathbf{1}_{N}-\\sum_{s=0}^{T_{c}-2}\\mathbf{W}_{t,s}\\quad s=T_{c}-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $\\gamma\\in\\mathbb{R}^{T}$ are learnable parameters, and $\\mathbf{w}^{(t,s)}$ is parameterized such that it has a sum of ${\\mathbf{1}}_{N}$ when $s$ goes through $[T_{c}]$ to satisfy the constraint in Eq. 9 for translation equivariance. Interestingly, as we formally illustrated in Theorem A.4, our parameterization of the prior theoretically subsumes the CoM-based priors [21, 13] and the fixed point-wise priors when $\\bar{\\gamma},\\hat{\\mathbf{h}}_{c}^{[T_{c}]}$ , and $\\hat{\\mathbf{x}}_{c}^{[T_{c}]}$ reduce to specific values. Such theoretical result underscores the benefit of our design since it permits the model to dynamically update the prior, leading to better optimization. The parameters $\\eta$ and $\\gamma$ are updated during training with gradients coming from optimizing the variational lower bound. ", "page_idx": 5}, {"type": "text", "text": "Transition kernel. We need to modify the forward and reverse process such that they both match the proposed prior. The forward process is modified as $q(\\mathbf{x}_{\\tau}^{[T]}|\\mathbf{x}_{\\tau-1}^{[T]},\\mathbf{x}_{c}^{[T_{c}]})\\;\\;:=$ $\\mathcal{N}(\\mathbf{x}_{\\tau}^{[T]};\\mathbf{x}_{r}\\,+\\,\\sqrt{1-\\beta_{\\tau}}(\\mathbf{x}_{\\tau-1}^{[T]}\\,-\\,\\mathbf{x}_{r}),\\beta_{\\tau}\\mathbf{I})$ , which ensures $q(\\mathbf{x}_{T}^{[T]}|\\mathbf{x}_{c}^{[T_{c}]})$ matches the equivariant prior ${\\bf x}_{r}$ (proof in App. A.2). The reverse transition kernel is given by $p_{\\tau-1}(\\mathbf{x}_{\\tau-1}^{[T]}|\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]})=$ $\\mathcal{N}(\\mu_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\tau,\\mathbf{x}_{c}^{[T_{c}]}),\\sigma_{\\tau}^{2}\\mathbf{I})$ . Similar to the unconditional case, we also adopt the noise prediction objective by rewriting $\\begin{array}{r}{\\mu_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)=\\mathbf{x}_{r}^{[T]}+\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{x}_{r}^{[T]}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\bar{\\alpha}_{\\tau}}}\\epsilon_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)\\right)}\\end{array}$ . The denoising network $\\epsilon_{\\theta}$ is implemented as an EGTN but with its output subtracted by the input for translation invariance, hence the translation equivariance of $\\pmb{\\mu_{\\theta}}$ . ", "page_idx": 5}, {"type": "text", "text": "Training and inference. Optimizing the VLB of our diffusion yields the following objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cond}}:=\\mathbb{E}_{{\\mathbf{x}}_{0}^{[T]},{\\mathbf{x}}_{c}^{[T_{c}]},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),\\tau\\sim\\mathrm{Unif}(1,T)}\\left[\\|\\epsilon-\\epsilon_{\\theta}({\\mathbf{x}}_{\\tau}^{[T]},{\\mathbf{x}}_{c}^{[T_{c}]},\\tau)\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "after simplification (proof in App. A.2). The training and inference procedures are in Alg. 3 and 4. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate GeoTDM on N-body physical simulation, molecular dynamics, and pedestrian trajectory forecasting, in both conditional $(\\S\\ S.1)$ and unconditional generation $(\\S\\ S.2)$ scenarios. We ablate our core design choices and demonstrate additional use cases in $\\S\\ 5.3$ . ", "page_idx": 5}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/2f5bb37041f0f868a09049b2992dcf4a960740ccb96287f35cd5aa52e810d9d2.jpg", "table_caption": ["Table 1: Conditional generation on N-body.Table 2: Pedestrian trajectory forecasting on ETHResults averaged over 5 runs, std in App. C.4.UCY. Best in bold and second best underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/b4bce08c6debebec61075c94b92c6bcf2a3d4d2bd826f1e5b5217dbe6cb9f14e.jpg", "table_caption": ["Table 3: Conditional trajectory generation on MD17. Results averaged over 5 runs (std in App. C.4). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Conditional Case ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1.1 N-body ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental setup. We adopt three scenarios in the collection of N-body simulation datasets, including 1. Charged Particles [26, 43], where $N\\,=\\,5$ particles with charges randomly chosen between $+1/-1$ are moving under Coulomb force; 2. Spring Dynamics [26], where $N=5$ particles with random mass are connected by springs with a probability of 0.5 between each pairs, and force on the spring follows Hooke\u2019s law; 3. Gravity System [2], where $N=10$ particles with random mass and initial velocity moves driven by gravitational force. For all three datasets, we use 3000 trajectories for training, 2000 for validation, and 2000 for testing. For each trajectory, we use 10 frames as the condition and predict the trajectory for the next 20 frames. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We involve baselines from three families. Frame-to-frame prediction models: Radial Field [27], Tensor Field Network [56], SE(3)-Transformer [9], and EGNN [43]; Deterministic trajectory model: EqMotion [62]; Probabilistic trajectory model: SVAE [67]. Details in App. B.3. ", "page_idx": 6}, {"type": "text", "text": "Metrics. We employ Average Discrepancy Error (ADE) and Final Discrepancy Error (FDE), which are widely adopted for trajectory forecasting [67, 62], given by $\\begin{array}{r l}{\\bar{\\mathbf{A}}\\bar{\\mathbf{D}}\\bar{\\mathbf{E}_{}}({\\mathbf{x}}^{[T]},{\\mathbf{y}}^{[T]})}&{=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{T N}\\sum_{t=0}^{T-1}\\sum_{i=0}^{N-1}\\|\\mathbf{x}_{i}^{(t)}-\\mathbf{y}_{i}^{(t)}\\|_{2}}\\end{array}$ , and $\\begin{array}{r}{\\mathrm{FDE}({\\mathbf x}^{[T]},{\\mathbf y}^{[T]})~=~\\frac{1}{N}\\sum_{i=0}^{N-1}\\|{\\mathbf x}_{i}^{(T-1)}~-~{\\mathbf y}_{i}^{(T-1)}\\|_{2}}\\end{array}$ yi(T \u22121)\u22252. For probabilistic models, we report average ADE and FDE derived from $K=5$ samples. ", "page_idx": 6}, {"type": "text", "text": "Implementation. The input data are processed as geometric graphs. For example, on Charged Particles, the node feature is the charge, and the graph is specified as fully connected without selfloops. We use 6 layers in EGTN with hidden dimension of 128. We use $\\tau=1000$ and the linear noise schedule [18]. More details in App. B.2. ", "page_idx": 6}, {"type": "text", "text": "Results. We present the results in Table 1, with the following observations. 1. Trajectory models generally yield lower error than frame to frame prediction models since they mitigate errors accumulated in iterative roll-out. 2. The equivariant methods, e.g., EGNN, EqMotion, and our GeoTDM significantly improves over the non-equivariant model SVAE, demonstrating the importance of injecting physical symmetry into the modeling of geometric trajectories. 3. By directly modeling the distribution of geometric trajectories with equivariance, GeoTDM achieves the lowest ADE and FDE on all three tasks, showcasing the superiority of the proposed approach. ", "page_idx": 6}, {"type": "text", "text": "5.1.2 Molecular Dynamics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental setup. We employ the MD17 [5] dataset, which contains the DFT-simulated molecular dynamics (MD) trajectories of 8 small molecules, with the number of atoms for each molecule ranging from 9 (Ethanol and Malonaldehyde) to 21 (Aspirin). For each molecule, we construct a training set of 5000 trajectories, and 1000/1000 for validation and testing, uniformly sampled along the time dimension. Different from [62], we explicitly involve the hydrogen atoms which contribute most to the vibrations of the trajectory, leading to a more challenging task. The node feature is the one-hot encodings of atomic number [44] and edges are connected between atoms within three hops measured in atomic bonds [48]. We adopt the same set of baselines as the N-body experiments. ", "page_idx": 6}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/b82c46382a8b9c98668f9cc4d862110587281ccbf5fc355a555ec669a5d52e39.jpg", "table_caption": ["Table 4: MD Trajectory generation results on MD17. Marg, Class, and Pred refer to Marginal score, Classification score, and Prediction score respectively. GeoTDM performs the best on all 8 molecules. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results. As depicted in Table 3, GeoTDM achieves the best performance on all eight molecule MD trajectories, outperforming previos state-of-the-art approach EqMotion. In particular, GeoTDM obtains an improvement of $23.1\\%/15.3\\%$ on average in terms of ADE/FDE, compared with the previous state-of-the-art approach EqMotion, thanks to the probabilistic modeling which is advantageous in capturing the stochasticity of MD simulations. ", "page_idx": 7}, {"type": "text", "text": "5.1.3 Pedestrian Trajectory Forecasting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental setup. We apply our model to ETH-UCY [35, 28] dataset, a challenging and largescale benchmark for pedestrian trajectory forecasting. There are five scenes in total: ETH, Hotel, Univ, Zara1, and Zara2. Following standard setup [14, 67], we use 8 frames (3.2 seconds) as input to predict the next 12 frames (4.8 seconds). The pedestrians are viewed as nodes and their 2D coordinates are extracted from the scenes. Edges are connected for nodes within a preset distance measured from the final frame in the given trajectory. The metrics are minADE/minFDE computed from 20 samples. For baselines, we compare with existing generative models that have been specifically designed for pedestrian trajectory prediction, including GANs: SGAN [14], SoPhie [39]; VAEs: PECNet [32], Traj $^{++}$ [40], BiTraP [68], SVAE [67]; and diffusion: MID [12]. Baseline results are taken from [67]. ", "page_idx": 7}, {"type": "text", "text": "Results. From Table 2, we observe that our GeoTDM obtains the best predictions on 3 out of the 5 scenarios while achieving the lowest average ADE and FDE. It is remarkable since compared with these baselines specifically tailored for the task of pedestrian trajectory forecasting, GeoTDM does not involve special data preprocessing of the trajectories through rotations or translations, does not involve extra auxiliary losses to optimize during training, and does not require task-specific backbones, demonstrating its general effectiveness across different geometric domains. ", "page_idx": 7}, {"type": "text", "text": "5.2 Unconditional Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental setup. For generation we reuse the Charged Particle dataset and the MD17 dataset. We follow the same setup as the conditional case, except that we generate trajectories with length 20 from scratch. We compare with SGAN [14], SVAE [67] (slightly modified to enable generation from scratch), and a VAE-modified version of EGNN [43], dubbed EGVAE (see App. B). The results of SGAN on MD17 is omitted due to mode collapse during training. ", "page_idx": 7}, {"type": "text", "text": "Metrics. We adopt three metrics adapted from time series generation to quantify the generation quality of the geometric trajectories: Marginal scores [34] measure the distance between the empirical probability density functions of the generated samples and the ground truths; Classification scores [25] are computed as the cross-entropy loss given by a trajectory classification model, trained on the task of distinguish whether the trajectory is generated or real; Prediction scores [72] are the MSEs of a train-on-synthetic-test-on-real trajectory prediction model (a 1-layer EqMotion) that takes as input the first half of the trajectories to predict the other half. ", "page_idx": 7}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/3ee12d1db180a5084d70ebc2c4daeb5f3fc31b3bacb8608f76cd3e5823375dcf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: (a) Unconditional generation samples on MD17. GeoTDM generates MD trajectories with much higher quality (see more in App. D). (b) Interpolation. Left: the given initial and final 5 frames. Right: GeoTDM interpolation and GT. (c) Optimization by GeoTDM on predictions of EGNN. Dis(Opt, GT)/Dis(Opt, EGNN) is the distance between optimized trajectories and GT/EGNN. ", "page_idx": 8}, {"type": "text", "text": "Results. Quantitative results are displayed in Table 5 and 4 for N-body and MD17. Notably, GeoTDM delivers samples with much higher quality than the baselines. On Charged Particles, GeoTDM achieves a classification score of 0.556, indicating its generated samples are generally indistinguishable with the ground truths. We observe similar patterns on MD17, where GeoTDM obtains remarkably lower marginal scores, higher classification scores, and lower prediction scores, showcasing its strong capability to model complex distributions of geometric trajectories on various geometric data. Visualizations are in Fig. 5 and more in App. D. ", "page_idx": 8}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/0ffc260e900d61a9843338e0f980a8a0a55ad33bab95c8de63782582be2c753f.jpg", "table_caption": ["Table 5: Unconditional generation results on N-body Charged Particle. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Studies and Additional Use Cases ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablations on diffusion prior. We investigate different priors, including non-equivariant $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ (i.e., DDPM [18]), equivariant but fixed $\\mathrm{CoM}$ prior $\\mathcal{N}(\\mathrm{CoM}(\\mathbf{x}_{c}^{(T_{c}-1)}),\\mathbf{I})$ and point-wise equivariant prior $\\mathcal{N}(\\mathbf{x}_{c}^{(T_{c}-1)},\\mathbf{I})$ . In Table 6 we see that non-equivariant prior leads to significantly worse performance. The CoM prior, though equivariant, is still inferior due to extra overhead in denoising the nodes initialized around the CoM to the original geometry. GeoTDM yields the lowest error due to the flexible learnable prior. ", "page_idx": 8}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/2973f16e62dfb6404b15f6d77944951969019a146d8008f2813a2fca604e950c.jpg", "table_caption": ["Table 6: Ablation studies. The numbers refer to ADE/FDE. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablations on EGTN. We further ablate the designs of the denoising model. 1. Equivariance. We replace all EGCL layers into non-equivariant MPNN [11] layers with same hidden dimension, leading to non-equivariant transition kernels. The performance becomes much worse, verifying the necessity of equivariance. 2. Attention. We substitute the attentions in temporal layers by equivariant convolutions (see App. B). Compared with this variant, GeoTDM enjoys larger capacity with attention and yields lower prediction error especially on Charged Particle where the particles generally move faster. 3. Temporal shift invariance. We employ relative temporal embeddings in attention, which enhances the generalization. Notably, the FDE improves from 0.330 to 0.258 on Charged Particle compared with the absolute temporal embedding. ", "page_idx": 8}, {"type": "text", "text": "Temporal interpolation. GeoTDM is able to perform interpolation as a special case of the conditional case. We demonstrate such capability on Charged Particle. The model is provided with the first 5 and last 5 frames, and the task is to generate the intermediate 20 frames as interpolation. GeoTDM reports an ADE of 0.055 on the test set, while a linear interpolation baseline reports an ADE of 0.171. From the qualitative visualizations in Fig. 2, we clearly see that GeoTDM can capture the complex dynamics and yield high-quality non-linear interpolations between the given initial and final frames. ", "page_idx": 8}, {"type": "text", "text": "Optimization. We further illustrate that GeoTDM can conduct optimization [31, 33] on given trajectories (e.g., those simulated by an EGNN) by simulating $K$ steps through the forward diffusion and then performing the reverse denoising. From Fig. 2 we see the distance between the optimized trajectory and GT gradually decreases as the optimization step grows. This reveals GeoTDM can effectively optimize the given trajectory towards the ground truth distribution. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. Akin to other diffusion models, GeoTDM resorts to multi-step sampling which may require more compute. We present empirical runtime benchmarks and more discussions in App. C.3. ", "page_idx": 9}, {"type": "text", "text": "Conclusion. We present GeoTDM, a diffusion model built over distribution of geometric trajectories. It is designed to preserve the symmetry of geometric systems, achieved by using EGTN, a novel SE(3)-equivariant geometric trajectory model, as the denoising network. We evaluate GeoTDM on various datasets for unconditional generation, interpolation, extrapolation and optimization, showing that it consistently outperforms the baselines. Future works include streamlining GeoTDM and extending it to more tasks such as protein MD, robot manipulation, and motion synthesis. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for the helpful feedback on improving the manuscript. This work was supported by ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), and the CZ Biohub. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. Advances in neural information processing systems, 29, 2016. 1   \n[2] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and physical quantities improve e(3) equivariant message passing. In International Conference on Learning Representations, 2022. 1, 2, 7 [3] Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020 (oc20) dataset and community challenges. Acs Catalysis, 11(10):6059\u20136072, 2021. 24 [4] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. 22   \n[5] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Sch\u00fctt, and Klaus-Robert M\u00fcller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017. 7   \n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021. 2 [7] Weitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Se (3) equivariant graph neural networks with complete local frames. In International Conference on Machine Learning, pages 5583\u20135608. PMLR, 2022. 2   \n[8] Jacob D Durrant and J Andrew McCammon. Molecular dynamics simulations and drug discovery. BMC biology, 9(1):1\u20139, 2011. 1 [9] Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020. 2, 7, 22   \n[10] Johannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations, 2020. 1   \n[11] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017. 2, 9, 22   \n[12] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen Lu. Stochastic trajectory prediction via motion indeterminacy diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17113\u201317122, 2022. 3, 7, 8, 25, 26   \n[13] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. In The Eleventh International Conference on Learning Representations, 2023. 6, 20   \n[14] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2255\u20132264, 2018. 7, 8, 9, 22   \n[15] Jiaqi Han, Wenbing Huang, Hengbo Ma, Jiachen Li, Joshua B. Tenenbaum, and Chuang Gan. Learning physical dynamics with subequivariant graph neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 1   \n[16] Jiaqi Han, Wenbing Huang, Tingyang Xu, and Yu Rong. Equivariant graph hierarchy-based neural networks. Advances in Neural Information Processing Systems, 35:9176\u20139187, 2022. 1, 22   \n[17] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. Advances in Neural Information Processing Systems, 35:27953\u201327965, 2022. 3   \n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020. 2, 3, 5, 7, 9, 18, 21   \n[19] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 2, 3   \n[20] Scott A Hollingsworth and Ron O Dror. Molecular dynamics simulation for all. Neuron, 99(6):1129\u20131143, 2018. 1   \n[21] Emiel Hoogeboom, V\u0131ctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 8867\u20138887. PMLR, 2022. 1, 2, 3, 6, 20   \n[22] Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Equivariant graph mechanics networks with constraints. In International Conference on Learning Representations, 2022. 1   \n[23] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher $\\mathrm{Ng}$ -Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable generative model. Nature, pages 1\u20139, 2023. 1   \n[24] Bowen Jing, Ezra Erives, Peter Pao-Huang, Gabriele Corso, Bonnie Berger, and Tommi Jaakkola. Eigenfold: Generative protein structure prediction with diffusion models, 2023. 2   \n[25] Patrick Kidger, James Foster, Xuechen Li, and Terry J Lyons. Neural sdes as infinite-dimensional gans. In International conference on machine learning, pages 5453\u20135463. PMLR, 2021. 8   \n[26] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018. 1, 2, 7   \n[27] Jonas K\u00f6hler, Leon Klein, and Frank No\u00e9. Equivariant flows: sampling configurations for multi-body systems with symmetric energies. arXiv preprint arXiv:1910.00753, 2019. 2, 7, 22   \n[28] Alon Lerner, Yiorgos Chrysanthou, and Dani Lischinski. Crowds by example. In Computer graphics forum. Wiley Online Library, 2007. 8   \n[29] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022. 24   \n[30] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigenspecific antibody design and optimization with diffusion-based generative models for protein structures. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 3   \n[31] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigenspecific antibody design and optimization with diffusion-based generative models for protein structures. Advances in Neural Information Processing Systems, 35:9754\u20139767, 2022. 9   \n[32] Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal, Kuan-Hui Lee, Ehsan Adeli, Jitendra Malik, and Adrien Gaidon. It is not the journey but the destination: Endpoint conditioned trajectory prediction. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 759\u2013776. Springer, 2020. 7, 8   \n[33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 9   \n[34] Hao Ni, Lukasz Szpruch, Magnus Wiese, Shujian Liao, and Baoren Xiao. Conditional sigwasserstein gans for time series generation. arXiv preprint arXiv:2006.05421, 2020. 8   \n[35] Stefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You\u2019ll never walk alone: Modeling social behavior for multi-target tracking. In 2009 IEEE 12th international conference on computer vision, pages 261\u2013268. IEEE, 2009. 8   \n[36] Calyampudi Radhakrishna Rao. Linear statistical inference and its applications, volume 2. Wiley New York, 1973. 5   \n[37] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pages 8857\u20138868. PMLR, 2021. 3   \n[38] Salva R\u00fchling Cachay, Bo Zhao, Hailey Joren, and Rose Yu. DYffusion: a dynamics-informed diffusion model for spatiotemporal forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 3   \n[39] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, and Silvio Savarese. Sophie: An attentive gan for predicting paths compliant to social and physical constraints. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1349\u20131358, 2019. 7, 8   \n[40] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron $^{++}$ : Dynamically-feasible trajectory forecasting with heterogeneous data. In Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVIII 16, pages 683\u2013700. Springer, 2020. 7, 8, 25   \n[41] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pages 8459\u20138468. PMLR, 2020. 2   \n[42] Victor Garcia Satorras, Emiel Hoogeboom, Fabian Bernd Fuchs, Ingmar Posner, and Max Welling. E(n) equivariant normalizing flows. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. 1, 2, 3   \n[43] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. arXiv preprint arXiv:2102.09844, 2021. 1, 2, 4, 5, 7, 8, 9, 20, 22   \n[44] Kristof Sch\u00fctt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pages 9377\u20139388. PMLR, 2021. 8   \n[45] Kristof T Sch\u00fctt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R M\u00fcller. Schnet\u2013a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24):241722, 2018. 1   \n[46] Jean-Pierre Serre et al. Linear representations of finite groups, volume 42. Springer, 1977. 3   \n[47] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464\u2013468, 2018. 4   \n[48] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In International conference on machine learning, pages 9558\u20139568. PMLR, 2021. 8   \n[49] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015. 2, 3   \n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 24   \n[51] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. 24   \n[52] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 2, 3   \n[53] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020. 2, 3   \n[54] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34:24804\u201324816, 2021. 3   \n[55] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2023. 3   \n[56] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018. 1, 2, 7, 22   \n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4   \n[58] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. MCVD - masked conditional video diffusion for prediction, generation, and interpolation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 3   \n[59] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023. 1, 2   \n[60] Fang Wu and Stan Z Li. Diffmd: a geometric diffusion model for molecular dynamics simulations. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023. 3   \n[61] Liming Wu, Zhichao Hou, Jirui Yuan, Yu Rong, and Wenbing Huang. Equivariant spatiotemporal attentive graph networks to simulate physical dynamics. Advances in Neural Information Processing Systems, 36, 2024. 4   \n[62] Chenxin Xu, Robby T Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eqmotion: Equivariant multi-agent motion prediction with invariant interaction reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1410\u20131420, 2023. 2, 7, 8, 22   \n[63] Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaif,i Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, and Anima Anandkumar. Equivariant graph neural operator for modeling 3d dynamics. In Forty-first International Conference on Machine Learning, 2024. 2   \n[64] Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning. PMLR, 2023. 1, 2, 3, 24, 25, 26   \n[65] Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli, and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel programming. In International Conference on Machine Learning, pages 11537\u201311547. PMLR, 2021. 3   \n[66] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2022. 1, 2, 25, 26   \n[67] Pei Xu, Jean-Bernard Hayet, and Ioannis Karamouzas. Socialvae: Human trajectory prediction using timewise latents. In European Conference on Computer Vision, pages 511\u2013528. Springer, 2022. 7, 8, 9, 22   \n[68] Yu Yao, Ella Atkins, Matthew Johnson-Roberson, Ram Vasudevan, and Xiaoxiao Du. Bitrap: Bi-directional pedestrian trajectory prediction with multi-modal goal estimation. IEEE Robotics and Automation Letters, 6(2):1463\u20131470, 2021. 7, 8   \n[69] Jason Yim, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. SE(3) diffusion model with application to protein backbone generation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 40001\u201340039. PMLR, 23\u201329 Jul 2023. 2   \n[70] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI), 2018. 4   \n[71] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16010\u201316021, 2023. 3   \n[72] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. In International Conference on Machine Learning, pages 42625\u201342643. PMLR, 2023. 8 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proofs 15   \nA.1 Unconditional Case . 15   \nA.2 Conditional Case 17   \nA.3 Optimizable Equivariant Prior . 19   \nA.4 Proof of Theorem 4.1 . 20 ", "page_idx": 14}, {"type": "text", "text": "B More Details on Experiments 21 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Compute Resources 21   \nB.2 Hyper-parameters 21   \nB.3 Baselines . . 22   \nB.4 Model . 22   \nB.5 Evaluation Metrics in the Unconditional Case . 22   \nMore Experiments and Discussions 23   \nC.1 Model Composition for Longer Trajectory . . 23   \nC.2 Number of Diffusion Steps . . 24   \nC.3 Sampling Time . . . 24   \nC.4 Standard Deviations . . 24   \nC.5 More Discussions with Existing Works 25 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Unconditional Case ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We note that, na\u00efvely, a distribution $p(\\mathbf{x}^{[T]})$ can not be translation invariant. In particular, this would imply that $p(\\mathbf{x}^{[T]})=p(\\mathbf{x}^{[T]}+\\mathbf{r})$ for all $\\mathbf{r}\\in\\mathbb{R}^{D}$ , but this would imply that $p(\\mathbf{x}^{[T]})=0$ uniformly, a contradiction. ", "page_idx": 14}, {"type": "text", "text": "Instead, we derive an equivalent invariance condition by restricting $\\operatorname{SE}(D)$ to its maximally compact subgroup. In particular, we note that it is possible to define $\\mathrm{SO}(D)$ -invariant distributions (as this group is compact), and $\\mathrm{SE}(D)/\\mathrm{SO}(D)\\cong\\mathrm{\\bar{T}}.$ , the translation group. The natural way to quotient out our base space $\\mathbb{R}^{T\\times N\\times D}/\\mathbb{T}\\cong\\mathbb{R}^{(T\\times N-1)\\times D}$ is to zero-center our data (along each dimension). ", "page_idx": 14}, {"type": "text", "text": "However, for practical purposes, we will refer to our construction as $\\mathrm{SE}(D)$ -invariant. In particular, since all inputs $\\mathbf{x}\\in\\mathbb{R}^{\\mathbf{\\hat{\\boldsymbol{T}}}\\times\\mathbf{\\boldsymbol{\\hat{N}}}\\times\\boldsymbol{D}}$ are first zero-centered to be projected to $\\mathbb{R}^{'T\\times N\\times D}/\\mathbb{T}$ , the \u201clifted\" unnormalized measure is $\\operatorname{SE}(D)$ invariant. ", "page_idx": 14}, {"type": "text", "text": "$P$ eraon-dc oenutr erreisntgri cotpede r $\\begin{array}{r}{P(\\mathbf{x}^{[T]})\\,=\\,\\mathbf{x}^{[T]}\\,-\\,\\frac{1}{T}\\sum_{t=0}^{T-1}\\operatorname{CoM}(\\mathbf{x}^{(t)})}\\end{array}$ ,c h wciathn CoM(x(t)) = N1 iN=1 x $(T\\times N-1)\\times D$ $\\tilde{\\mathcal{N}}(\\mathbf{y}|\\mathbf{x},\\Sigma)$ be represented in the ambient space as a degenerated Gaussian variable ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{N}}(\\mathbf{y}|\\mathbf{x},\\mathbf{D})=\\frac{1}{(2\\pi)^{2/((T\\times N-1)\\times D)}\\operatorname*{det}^{*}(\\Sigma_{\\mathbf{P}})^{1/2}}\\exp\\left(-\\frac{1}{2}(\\mathbf{y}-\\mathbf{x})^{\\top}\\Sigma_{\\mathbf{P}}^{+}(\\mathbf{y}-\\mathbf{x})\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\boldsymbol{\\Sigma}_{\\mathbf{P}}=\\mathbf{P}\\boldsymbol{\\Sigma}\\mathbf{P}^{\\intercal}$ and $\\Sigma_{\\mathbf{P}}^{+}$ is the pseudo-inverse (and $\\mathrm{{det}^{*}}$ is the determinant restricted to the subspace). Note that $\\mathbf{P}$ is symmetric and idempotent. Then specifically when $\\boldsymbol{\\Sigma}=\\boldsymbol{\\mathbf{I}}$ , we have $\\begin{array}{r}{\\Sigma_{\\mathbf{P}}\\doteq\\mathbf{P}\\mathbf{\\dot{P}}^{\\top}}\\end{array}$ , then $\\pmb{\\Sigma}_{\\mathbf{P}}^{+}=\\mathbf{P}$ as $(\\mathbf{PP}^{\\top})\\mathbf{P}(\\mathbf{PP}^{\\top})=\\mathbf{P}$ , since $\\mathbf{P}\\mathbf{P}=\\mathbf{P}$ and $\\mathbf{P}=\\mathbf{P}^{\\top}$ . ", "page_idx": 14}, {"type": "text", "text": "Base distribution. We require the base distribution to be $\\mathrm{SO}(3)$ -invariant. In practice, we let $p_{T}(\\tilde{\\mathbf{x}}_{T}^{[T]})=\\tilde{\\mathcal{N}}(\\mathbf{0},\\mathbf{I})$ to be the Gaussian distribution in the translation-invariant subspace ", "page_idx": 15}, {"type": "text", "text": "Transition kernel. For the transition kernel, we specify it as $p_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\tilde{\\mathbf{x}}_{\\tau}^{[T]})=\\tilde{\\mathcal{N}}(\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau),\\sigma_{\\tau}^{2}\\mathbf{I})$ . In order to ensure $p_{\\theta}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau}^{[T]})=p_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\tilde{\\mathbf{x}}_{\\tau}^{[T]})$ , it suffices to make $\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)$ an SO(3)- equivariant function. In this way, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p_{\\theta}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau}^{[T]})=\\tilde{N}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]};\\tilde{\\mu}_{\\theta}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau}^{[T]}),\\sigma_{\\tau}^{2}\\mathbf{I}),}&{}\\\\ {=\\tilde{N}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}-\\tilde{\\mu}_{\\theta}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau}^{[T]});\\mathbf{0},\\sigma_{\\tau}^{2}\\mathbf{I}),}&{}\\\\ {=\\tilde{N}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}-\\mathbf{R}\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]});\\mathbf{0},\\sigma_{\\tau}^{2}\\mathbf{I}),}&{}\\\\ {=\\tilde{N}(\\mathbf{R}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}-\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]});\\mathbf{0},\\sigma_{\\tau}^{2}\\mathbf{I}),}&{}\\\\ {=\\tilde{N}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}-\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]});\\mathbf{0},\\sigma_{\\tau}^{2}\\mathbf{I}),}&{}\\\\ {=\\tilde{N}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]};\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]}),\\sigma_{\\tau}^{2}\\mathbf{I}),}&{}\\\\ {=p_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\tilde{\\mathbf{x}}_{\\tau}^{[T]}),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which permits the $\\mathrm{SO}(3)$ -equivariance of the transition kernel. In our implementation, we further re-parameterize $\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)$ as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mu}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)=\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left(\\tilde{\\mathbf{x}}_{\\tau}^{[T]}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\bar{\\alpha}_{\\tau}}}\\tilde{\\epsilon}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we instead ensure $\\tilde{\\epsilon}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)$ to be SO(3)-equivariant and its output should lie in the subspace $\\scriptstyle{\\mathcal{X}}_{\\mathbf{P}}$ . ", "page_idx": 15}, {"type": "text", "text": "We now prove the following proposition, which states that if the base distribution is $\\mathrm{SO(3)}$ -invariant and the transition kernel is $\\mathrm{SO}(3)$ -equivariant, then the marginal at any diffusion time step is also $\\mathrm{SO}(3)$ -invariant. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.1. If the prior $p_{\\mathcal{T}}(\\tilde{\\mathbf{x}}_{\\mathcal{T}}^{[T]})$ is $S O(3)$ -invariant, the transition kernels $p_{\\tau-1}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\quad$ $\\tilde{\\mathbf{x}}_{\\tau}^{[T]}),\\forall\\tau\\in\\{1,\\cdots,T\\}$ are $S O(3)$ -equivariant, then the marginal $p_{\\tau}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]})$ at any time step $\\tau\\in$ $\\{0,\\cdot\\cdot\\cdot,\\tau\\}$ is also $S O(3)$ -invariant. ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof is given by induction. ", "page_idx": 15}, {"type": "text", "text": "Induction base. When $\\tau=\\tau$ , we have the marginal being the prior $p_{\\mathcal{T}}(\\tilde{\\mathbf{x}}_{\\mathcal{T}}^{[T]})$ , which is $\\mathrm{SO}(3)$ - invariant. ", "page_idx": 15}, {"type": "text", "text": "Induction step. Suppose the marginal at diffusion time step $\\tau$ is $\\mathrm{SO}(3)$ -invariant, i.e., $p_{\\tau}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]})=$ $p_{\\tau}(\\mathbf{R}\\tilde{\\mathbf{x}}_{\\tau}^{[T]})$ , then we have the following derivation for the marginal at time step $\\tau-1$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\tau-1}(\\mathbf R\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]})=\\int p_{\\tau-1}(\\mathbf R\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\tilde{\\mathbf{x}}_{\\tau}^{[T]})p_{\\tau}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]})\\mathrm{d}\\tilde{\\mathbf{x}}_{\\tau}^{[T]},}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\int p_{\\tau-1}(\\mathbf R\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\mathbf R\\mathbf R^{-1}\\tilde{\\mathbf{x}}_{\\tau}^{[T]})p_{\\tau}(\\mathbf R\\mathbf R^{-1}\\tilde{\\mathbf{x}}_{\\tau}^{[T]})\\mathrm{d}\\tilde{\\mathbf{x}}_{\\tau}^{[T]},}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\int p_{\\tau-1}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\mathbf R^{-1}\\tilde{\\mathbf{x}}_{\\tau}^{[T]})p_{\\tau}(\\mathbf R^{-1}\\tilde{\\mathbf{x}}_{\\tau}^{[T]})\\mathrm{d}\\tilde{\\mathbf{x}}_{\\tau}^{[T]},}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\int p_{\\tau-1}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\mid\\tilde{\\mathbf{y}}_{\\tau}^{[T]})p_{\\tau}(\\tilde{\\mathbf{y}}_{\\tau}^{[T]})\\operatorname*{det}(\\mathbf R)\\mathrm{d}\\tilde{\\mathbf{y}}_{\\tau}^{[T]},}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =p_{\\tau-1}(\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Notably, for the final step at $\\tau=0$ , the marginal $p_{0}(\\mathbf{R}\\tilde{\\mathbf{x}}_{0}^{[T]})$ is also $\\mathrm{SO(3)}$ -invariant, indicating the final sample from the entire geometric trajectory diffusion process resides in an $\\mathrm{SO}(3)$ -invariant distribution, hence the physical symmetry being well preserved. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Training Procedure of GeoTDM-uncond ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: repeat   \n2: Sample $\\tilde{\\epsilon}^{[T]}\\sim\\tilde{\\mathcal{N}}(\\mathbf{0},\\mathbf{I})^{[T]}$ , \u03c4 \u2208Unif({1, \u00b7 \u00b7 \u00b7 , T }), \u02dcx[\u221aT ] \u223cD\u02dcdata   \n3: $\\tilde{\\mathbf{x}}_{\\tau}^{[T]}\\leftarrow\\sqrt{\\bar{\\alpha}_{\\tau}}\\tilde{\\mathbf{x}}^{[T]}+\\sqrt{1-\\bar{\\alpha}_{\\tau}}\\tilde{\\mathbf{\\epsilon}}^{[T]}$   \n4: Take gradient descent step on $\\nabla_{\\boldsymbol{\\theta}}\\|\\tilde{\\epsilon}^{[T]}-\\tilde{\\epsilon}_{\\boldsymbol{\\theta}}\\bar{(}\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)\\|_{2}^{2}$   \n5: until converged ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Sampling Procedure of GeoTDM-uncond ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: Sample $\\tilde{\\mathbf{x}}_{T}^{[T]}\\sim\\tilde{\\mathcal{N}}(\\mathbf{0},\\mathbf{I})^{[T]}$   \n2: for $\\tau\\gets\\tau,\\cdots\\,,1$ do   \n3: Sample $\\tilde{\\mathbf{z}}_{\\tau}^{[T]}\\sim\\tilde{\\mathcal{N}}(\\mathbf{0},\\mathbf{I})^{[T]}$ if $\\tau>1$ else $\\tilde{\\mathbf{z}}_{\\tau}^{[T]}=\\mathbf{0}$   \n4: $\\begin{array}{r}{\\tilde{\\mathbf{x}}_{\\tau-1}^{[T]}\\leftarrow\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left(\\tilde{\\mathbf{x}}_{\\tau}^{[T]}-\\frac{1-\\alpha_{\\tau}}{\\sqrt{1-\\bar{\\alpha}_{\\tau}}}\\tilde{\\epsilon}_{\\theta}(\\tilde{\\mathbf{x}}_{\\tau}^{[T]},\\tau)\\right)+\\sigma_{\\tau}\\tilde{\\mathbf{z}}_{\\tau}^{[T]}}\\end{array}$   \n5: end for   \n6: return x[0T ] ", "page_idx": 16}, {"type": "text", "text": "Algorithm 3 Training Procedure of GeoTDM-cond ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: repeat   \n2: x[rT ]\u2190EquiPrior\u03b7,\u03b3(x[cTc]) {Eq. 9}   \n3: Sample \u03f5[T ] \u223cN(0, I), \u03c4 \u2208Unif({1, \u00b7 \u00b7 \u00b7 , T }), (x[T ], x[cTc], ) \u223cpdata   \n4: $\\mathbf{x}_{\\tau}^{[T]}\\leftarrow\\sqrt{\\bar{\\alpha}_{\\tau}}(\\mathbf{x}^{[T]}-\\mathbf{x}_{r}^{[T]})+\\mathbf{x}_{r}^{[T]}+\\sqrt{1-\\bar{\\alpha}_{\\tau}}\\epsilon^{[T]}$   \n5: Take gradient descent step on $\\nabla_{\\theta,\\eta,\\gamma}\\big\\|\\epsilon^{[T]}-\\dot{\\epsilon}_{\\theta}\\big(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau\\big)\\big\\|^{2}$   \n6: until converged ", "page_idx": 16}, {"type": "text", "text": "Algorithm 4 Sampling Procedure of GeoTDM-cond ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: $\\mathbf{x}_{r}^{[T]}\\leftarrow\\mathrm{EquiPrior}_{\\eta,\\gamma}(\\mathbf{x}_{c}^{[T_{c}]})$ {Eq. 9}   \n2: Sample $\\mathbf{x}_{\\mathcal{T}}^{[T]}\\sim\\mathcal{N}(\\mathbf{x}_{r}^{[T]},\\mathbf{I}),\\mathbf{x}_{c}^{[T_{c}]}\\sim\\mathcal{D}_{\\mathrm{data}}$   \n3: for $\\tau\\gets\\tilde{\\tau},\\cdots,1$ do   \n4: Sample $\\mathbf{z}_{\\tau}^{[T]}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})^{[T]}$ if $\\tau>1$ else $\\mathbf{z}_{\\tau}^{\\left[T\\right]}=\\mathbf{0}$   \n5: $\\begin{array}{r}{{\\bf x}_{\\tau-1}^{[T]}\\leftarrow\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left({\\bf x}_{\\tau}^{[T]}-{\\bf x}_{r}^{[T]}-\\frac{1-\\alpha_{\\tau}}{\\sqrt{1-\\bar{\\alpha}_{\\tau}}}\\epsilon_{\\theta}({\\bf x}_{\\tau}^{[T]},{\\bf x}_{c}^{[T_{c}]},\\tau)\\right)+{\\bf x}_{r}^{[T]}+\\sigma_{\\tau}{\\bf z}_{\\tau}^{[T]}}\\end{array}$   \n6: end for   \n7: return x0 [T ] ", "page_idx": 16}, {"type": "text", "text": "A.2 Conditional Case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the conditional case, we target on modeling the conditional distribution $p(\\mathbf{x}^{[T]}~\\mid~\\mathbf{x}_{c}^{[T_{c}]})$ . The desired constraint is the following equivariance condition: $p(\\mathbf{x}^{[T]}\\mid\\mathbf{x}_{c}^{[T_{c}]})=p\\big(g\\cdot\\mathbf{x}^{[T]}\\mid g\\cdot\\mathbf{x}_{c}^{[T_{c}]}\\big)$ , for all $g\\in\\mathrm{SE}(3)$ . ", "page_idx": 16}, {"type": "text", "text": "Construction of the equivariant prior. The prior is constructed through Eq. 9. Here we formally show that this guarantees SE(3)-equivariance of the prior. For convenience we repeat Eq. 9 below. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}_{r}^{(t)}=\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}\\hat{\\mathbf{x}}_{c}^{(s)},\\quad\\mathrm{s.t.}\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}=\\mathbf{1},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{x}_{r}^{\\prime(t)}=\\sum_{s\\in[T_{c}]}\\mathbf{w}^{\\prime(t,s)}\\hat{\\mathbf{x}}_{c}^{\\prime(s)},}}\\\\ &{=\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}(\\mathbf{R}\\hat{\\mathbf{x}}_{c}^{(s)}+\\mathbf{r}),}\\\\ &{=\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}(\\mathbf{R}\\hat{\\mathbf{x}}_{c}^{(s)})+\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}\\mathbf{r},}\\\\ &{=\\mathbf{R}\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}\\hat{\\mathbf{x}}_{c}^{(s)}+\\mathbf{r},}\\\\ &{=\\mathbf{R}\\mathbf{x}_{\\delta}^{(t)}+\\mathbf{r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\forall$ rotation matrix $\\mathbf{R}$ and $\\mathbf{r}\\in{\\mathbb{R}}^{3}$ , which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "Base distribution. We propose to leverage the following base distribution. ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\mathcal{T}}(\\mathbf{x}_{\\mathcal{T}}^{[T]}\\mid\\mathbf{x}_{c}^{[T_{c}]})=\\mathcal{N}(\\mathbf{x}_{\\mathcal{T}}^{[T]};\\mathbf{x}_{r}^{[T]},\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{x}_{r}^{[T]}=\\mathrm{EquiPrior}(\\mathbf{x}_{c}^{[T_{c}]})$ is $\\mathrm{SE}(3)$ -equivariant with respect to the condition $\\mathbf{x}_{c}^{[T_{c}]}$ . With such choice, the base distribution above is SE(3)-equivariant, since ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\mathcal{T}}(\\mathbf{R}\\mathbf{x}_{\\mathcal{T}}^{[T]}+\\mathbf{r}\\mid\\mathbf{R}\\mathbf{x}_{c}^{[T]}+\\mathbf{r})=\\mathcal{N}(\\mathbf{R}\\mathbf{x}_{\\mathcal{T}}^{[T]}+\\mathbf{r};\\mathbf{R}\\mathbf{x}_{r}^{[T]}+\\mathbf{r},\\mathbf{I}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathcal{N}(\\mathbf{R}\\mathbf{x}_{\\mathcal{T}}^{[T]};\\mathbf{R}\\mathbf{x}_{r}^{[T]},\\mathbf{I}),}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathcal{N}(\\mathbf{x}_{\\mathcal{T}}^{[T]};\\mathbf{x}_{r}^{[T_{c}]},\\mathbf{I}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last equation is due to $\\operatorname*{det}(\\mathbf{R}^{\\top}\\mathbf{R})=\\mathbf{I},\\|\\mathbf{x}_{T}^{[T]}-\\mathbf{x}_{r}^{[T]}\\|^{2}=\\|\\mathbf{R}\\mathbf{x}_{T}^{[T]}-\\mathbf{R}\\mathbf{x}_{r}^{[T]}\\|^{2}$ , which also gives the proof for Theorem 4.4 by a mild substitution of the notations. ", "page_idx": 17}, {"type": "text", "text": "Transition kernel. The transition kernel is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{\\tau-1}^{[T]}\\mid\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]})=\\mathcal{N}(\\mathbf{x}_{\\tau-1}^{[T]};\\mu_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau),\\sigma_{\\tau}^{2}\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mu_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)$ parameterized to be $\\mathrm{SE}(3)$ -equivariant with respect to its input $\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]}$ In practice, we re-parameterize it as, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)=\\mathbf{x}_{r}^{[T]}+\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{x}_{r}^{[T]}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\bar{\\alpha}_{\\tau}}}\\epsilon_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\epsilon_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)$ is an $\\mathrm{SO}(3)$ -equivariant but translation-invariant function. It is then easy to see that \u00b5\u03b8(x[\u03c4T  ], x[cTc], \u03c4) meets the SE(3)-equivariance as desired. ", "page_idx": 17}, {"type": "text", "text": "Proposition A.2. With such parameterization, optimizing the variational lower bound is equivalent to optimizing the following objective, up to certain re-weighting: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\|\\epsilon_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)-\\epsilon\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We define $\\begin{array}{r l r}{q(\\mathbf{x}_{\\tau}^{[T]}|\\mathbf{x}_{\\tau-1}^{[T]})}&{:=}&{\\mathcal{N}(\\mathbf{x}_{\\tau}^{[T]};\\mathbf{x}_{r}\\:+\\:\\sqrt{1-\\beta_{\\tau}}(\\mathbf{x}_{\\tau-1}^{[T]}\\:-\\:\\mathbf{x}_{r}),\\beta_{\\tau}\\mathbf{I}).}\\end{array}$ , which yields $q(\\mathbf{x}_{\\tau}^{[T]}|\\mathbf{x}_{0}^{[T]})=\\mathcal{N}(\\mathbf{x}_{\\tau}^{[T]};\\mathbf{x}_{r}+\\sqrt{\\bar{\\alpha}_{\\tau}}(\\mathbf{x}_{0}^{[T]}-\\mathbf{x}_{r}),(1-\\bar{\\alpha}_{\\tau})\\mathbf{I}).$ . The proof then generally follows [18] but with all latent variables in [18] being replaced by $\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{x}_{r}^{[T]}$ . Then the terms in the VLB are ", "page_idx": 17}, {"type": "text", "text": "given by, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\tau-1}=D_{\\mathrm{KL}}(q(\\mathbf{x}_{\\tau-1}^{[T]}\\mid\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{0}^{[T]})\\|p_{\\theta}(\\mathbf{x}_{\\tau-1}^{[T]}\\mid\\mathbf{x}_{\\tau}^{[T]})),}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\mathbf{x}_{0}^{[T]},\\epsilon}\\left[\\frac{1}{2\\sigma_{\\tau}^{2}}\\left\\|\\mathbf{x}_{\\tau}^{[T]}+\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left(\\mathbf{x}_{\\tau}^{[T]}(\\mathbf{x}_{0}^{[T]},\\epsilon,\\mathbf{x}_{\\tau}^{[T]})-\\mathbf{x}_{r}^{[T]}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\tilde{\\alpha}_{\\tau}}}\\epsilon\\right)-\\mu_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\tau)\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\mathbf{x}_{0}^{[T]},\\epsilon}\\left[\\frac{1}{2\\sigma_{\\tau}^{2}}\\left\\|\\mathbf{x}_{r}^{[T]}+\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\Bigg(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{x}_{r}^{[T]}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\tilde{\\alpha}_{\\tau}}}\\epsilon\\right)\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.-\\mathbf{x}_{r}^{[T]}-\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\Bigg(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{x}_{r}^{[T]}-\\frac{\\beta_{\\tau}}{1-\\tilde{\\alpha}_{\\tau}}\\epsilon_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{\\tau}^{[T]},\\tau)\\Bigg)\\right\\|^{2}\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n=\\mathbb{E}_{\\mathbf{x}_{0}^{[T]},\\epsilon}\\left[\\frac{\\beta_{\\tau}^{2}}{2\\sigma_{\\tau}^{2}\\alpha_{\\tau}(1-\\bar{\\alpha}_{\\tau})}\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]},\\tau)\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is equivalent to Eq. 39 up to certain re-weighting factors. For $\\begin{array}{r l}{\\mathcal{L}_{T}}&{{}=}\\end{array}$ $D_{\\mathrm{KL}}(q(\\mathbf{x}_{T}^{[T]}|\\Bar{\\mathbf{x}}_{0}^{[T]})\\|p(\\mathbf{x}_{T}^{[T]}))$ , it is does not contribute to the gradient since it is irrelevant to $\\pmb{\\theta}$ , and ${\\bf x}_{r}$ is also cancelled out in computing the KL, thus stopping the gradient from passing to $\\eta$ and $\\gamma$ . $\\boxed{\\begin{array}{r l}\\end{array}}$ ", "page_idx": 18}, {"type": "text", "text": "Analogous to the unconditional case, we have the following proposition, indicating that if the base distribution is $\\mathrm{SE}(3)$ -equivariant and the transition kernel is SE(3)-equivariant, then the marginal is also SE(3)-equivariant. ", "page_idx": 18}, {"type": "text", "text": "Proposition A.3. If the base distribution $p_{T}(\\mathbf{x}_{T}^{[T]}\\mid\\mathbf{x}_{c}^{[T_{c}]})$ is $S E(3)$ -equivariant and the transition kernels $p_{\\tau-1}(\\mathbf{x}_{\\tau-1}^{[T]}\\mid\\mathbf{x}_{\\tau}^{[T]},\\mathbf{x}_{c}^{[T_{c}]})$ of all diffusion steps $\\tau\\in\\{1,\\cdots,\\mathcal{T}\\}$ are $S E(3)$ -equivariant, then the marginal6 $p_{\\tau}(\\mathbf{x}_{\\tau}^{[T]}\\mid\\mathbf{x}_{c}^{[T_{c}]})$ at any diffusion step $\\tau\\in\\{0,\\cdots,\\mathcal{T}\\}$ is $S E(3)$ -equivariant. ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof is similarly given by induction. ", "page_idx": 18}, {"type": "text", "text": "Induction base. When $\\tau=\\tau$ , the distribution is the base distribution $p_{T}(\\mathbf{x}_{T}^{[T]}\\mid\\mathbf{x}_{c}^{[T_{c}]})$ is $\\mathrm{SE}(3)$ - equivariant, as it is designed. ", "page_idx": 18}, {"type": "text", "text": "Induction step. Suppose the marginal at diffusion step $\\tau$ , i.e., $p_{\\tau}(\\mathbf{x}_{\\tau}^{[T]}\\mid\\mathbf{x}_{c}^{[T_{c}]})$ , is SE(3)-equivariant, then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad p_{\\tau-1}(\\mathbf{Rx}_{\\tau-1}^{[T]}+\\mathbf{r}\\mid\\mathbf{Rx}_{\\tau}^{[T]}+\\mathbf{r})}&{\\quad{\\mathrm{(44)}}}\\\\ &{=\\int p_{\\tau-1}(\\mathbf{Rx}_{\\tau-1}^{[T]}+\\mathbf{r}\\mid\\mathbf{x}_{\\tau}^{[T]},\\mathbf{Rx}_{\\tau}^{[T]}+\\mathbf{r})p_{\\tau}(\\mathbf{x}_{\\tau}^{[T]}\\mid\\mathbf{Rx}_{\\tau}^{[T]}+\\mathbf{r})\\mathrm{d}\\mathbf{x}_{\\tau}^{[T]},}&{\\quad{\\mathrm{(45)}}}\\\\ &{=\\int p_{\\tau-1}(\\mathbf{Rx}_{\\tau-1}^{[T]}+\\mathbf{r}\\mid\\mathbf{R}(\\mathbf{R}^{-1}(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{r}))+\\mathbf{r},\\mathbf{Rx}_{\\tau}^{[T]}+\\mathbf{r})p_{\\tau}(\\mathbf{R}(\\mathbf{R}^{-1}(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{r}))+\\mathbf{r}\\mid\\mathbf{Rx}_{\\tau}^{[T]}}\\\\ &{=\\int p_{\\tau-1}(\\mathbf{x}_{\\tau-1}^{[T]}\\mid\\mathbf{R}^{-1}(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{r}),\\mathbf{x}_{\\tau}^{[T]})p_{\\tau}(\\mathbf{R}^{-1}(\\mathbf{x}_{\\tau}^{[T]}-\\mathbf{r})\\mid\\mathbf{x}_{\\tau}^{[T]})\\mathrm{d}\\mathbf{x}_{\\tau}^{[T]},}&{\\quad{\\mathrm{(44)}}}\\\\ &{=\\int p_{\\tau-1}(\\mathbf{x}_{\\tau-1}^{[T]}\\mid\\mathbf{y}_{\\tau}^{[T]},\\mathbf{x}_{\\tau}^{[T]})p_{\\tau}(\\mathbf{y}_{\\tau}^{[T]}\\mid\\mathbf{x}_{\\tau}^{[T]})\\operatorname*{d}\\mathbf{x}_{\\tau}^{[T]}\\mid\\mathbf{x}_{\\tau}^{[T]},}&{\\quad{\\mathrm{(45)}}}\\\\ &{=\\int p_{\\tau-1}(\\mathbf{x}_{\\tau}^{[T]}\\mid\\mathbf{y}_{\\tau}^{[T]},\\mathbf{x}_{\\tau}^{[T]})p_{\\tau}(\\mathbf{y}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "A.3 Optimizable Equivariant Prior ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem A.4. The prior implemented by the parameterization in Eq. 9, 10, and 11 subsumes CoM-based priors and fixed point-wise priors. ", "page_idx": 18}, {"type": "text", "text": "6Here the marginal refers to marginalizing the intermediate states in previous diffusion steps, while still being conditional on the input condition $\\mathbf{x}_{c}^{[T_{c}]}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. We repeat the parameterizations specified by Eq. 9, 10, and 11 below for better readability. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{x}_{r}^{(t)}=\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}\\hat{\\mathbf{x}}_{c}^{(s)},\\quad\\mathrm{s.t.}\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}=\\mathbf{1}_{N},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{W}_{t,s}=[\\gamma\\otimes\\hat{\\mathbf{h}}_{c}^{[T_{c}]}]_{t,s}\\in\\mathbb{R}^{N},}\\\\ {\\mathbf{w}^{(t,s)}=\\left\\{\\mathbf{W}_{t,s}\\right.}\\\\ {\\left.\\mathbf{1}_{N}-\\sum_{s=0}^{T_{c}-2}\\mathbf{W}_{t,s}\\quad s=T_{c}-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We first show $\\mathbf{x}_{r}^{[T]}$ can reduce to the CoM-based priors. Let $\\hat{\\mathbf{x}}_{c}^{(s)}\\,=\\,\\mathrm{CoM}(\\mathbf{x}_{c}^{(s)})$ , $\\begin{array}{r}{\\hat{\\bf h}_{c}^{(s)}\\,=\\,\\frac{1}{T_{c}}{\\bf1}_{N}}\\end{array}$ , \u03b3(t) = 1. In this case, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{r}^{(t)}=\\displaystyle\\sum_{s\\in\\mathbb{R}_{+}^{r}}\\mathbf{w}^{(t,s)}\\hat{\\mathbf{x}}_{c}^{(s)},}\\\\ &{\\qquad=\\displaystyle\\sum_{s\\in\\mathbb{R}_{+}^{r}-1}\\gamma^{(t)}\\frac{1}{T_{c}}\\mathbf{1}_{N}\\mathrm{CoM}(\\mathbf{x}_{c}^{(s)})+(\\mathbf{1}_{N}-\\displaystyle\\sum_{s\\in\\mathbb{R}_{-}^{r}-1}\\gamma^{(t)}\\frac{1}{T_{c}}\\mathbf{1}_{N})\\mathrm{CoM}(\\mathbf{x}_{c}^{(T_{c}-1)}),}\\\\ &{\\qquad=\\displaystyle\\sum_{s\\in\\mathbb{R}_{-}^{r}-1}\\frac{1}{T_{c}}\\mathbf{1}_{N}\\mathrm{CoM}(\\mathbf{x}_{c}^{(s)})+(\\mathbf{1}_{N}-\\displaystyle\\frac{T_{c}-1}{T_{c}}\\mathbf{1}_{N})\\mathrm{CoM}(\\mathbf{x}_{c}^{(T_{c}-1)}),}\\\\ &{\\qquad=\\displaystyle\\sum_{s\\in\\mathbb{R}_{+}^{r}-1}\\frac{1}{T_{c}}\\mathbf{1}_{N}\\mathrm{CoM}(\\mathbf{x}_{c}^{(s)})+\\frac{1}{T_{c}}\\mathbf{1}_{N}\\mathrm{CoM}(\\mathbf{x}_{c}^{(T_{c}-1)}),}\\\\ &{\\qquad=\\displaystyle\\sum_{s\\in\\mathbb{R}_{+}^{r}-1}\\sum_{c}\\mathbf{CoM}(\\mathbf{x}_{c}^{(s)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\frac{1}{T_{c}}\\sum_{s\\in[T_{c}]}{\\bf C o M}({\\bf x}_{c}^{(s)})}\\end{array}$ is the generalization of the CoM-based priors [21, 13] in the multiple frame conditioning scenario, which reduces to $\\mathrm{CoM}(\\mathbf{x}_{c}^{(0)})$ when $T_{c}=1$ . ", "page_idx": 19}, {"type": "text", "text": "To show $\\mathbf{x}_{r}^{[T]}$ can reduce to fixed point-wise priors is straightforward. Let $\\hat{\\mathbf{x}}_{c}^{(s)}=\\mathbf{x}_{c}^{(s)}$ , $\\hat{\\mathbf{h}}^{[T_{c}]}=$ Onehot $(s^{*})\\mathbf{1}_{T_{c}\\times N}$ and $\\gamma^{(t)}=1,\\bar{\\forall}t$ . Then $\\mathbf{w}^{(t,s)}=\\mathrm{Onehot}(s^{*})\\mathbf{1}_{T_{c}\\times N}$ . Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{r}^{(t)}=\\displaystyle\\sum_{s\\in[T_{c}]}\\mathbf{w}^{(t,s)}\\mathbf{x}_{c}^{(s)},}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{s\\in[T_{c}]}\\mathrm{Onehot}(s^{*})\\mathbf{1}_{T_{c}\\times N\\mathbf{X}_{c}^{(s)}},}\\\\ &{\\quad\\quad=\\mathbf{x}_{c}^{(s^{*})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{x}_{c}^{(s^{*})}$ is the point-wise equivariant prior, and $s^{*}\\in[T_{c}]$ is the frame index in the conditioning trajectory for this specific prior. ", "page_idx": 19}, {"type": "text", "text": "We also provide an illustrative comparison of these equivariant priors in Fig. 3. ", "page_idx": 19}, {"type": "text", "text": "A.4 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 4.1 (SE(3)-equivariance of EGTN). $L e t\\,\\mathbf{x}^{\\prime[T]},\\mathbf{h}^{\\prime[T]}=f_{\\mathrm{EGTN}}\\left(\\mathbf{x}^{[T]},\\mathbf{h}^{[T]},\\boldsymbol{\\mathcal{E}}\\right)$ . Then we have $g\\cdot{\\bf x}^{\\prime[T]},\\dot{\\bf h}^{\\prime[T]}=f_{\\mathrm{EGTN}}^{'}\\left(g\\cdot{\\bf x}^{[T]},{\\bf h}^{[T]},\\dot{\\mathcal{E}}\\right),\\forall g\\in S\\dot{E}(3).\\eqno{(10)}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. $f_{\\mathrm{EGTN}}$ is a stack of $L$ EGNN and temporal attention layer in alternated fashion, formally   \nwritten as $f_{\\mathrm{EGTN}}\\,=\\,f_{\\mathrm{attn}}\\circ f_{\\mathrm{EGNN}}\\circ\\cdots\\circ f_{\\mathrm{attn}}\\circ f_{\\mathrm{EGNN}_{\\cdot}}$ . Since the chain of SE(3)-equivariant $L\\times\\left(f_{\\mathrm{attn}}\\circ f_{\\mathrm{EGNN}}\\right)$   \nfunction is also $\\mathrm{SE}(3)$ -equivariant, it suffices to prove $f_{\\mathrm{attn}}$ is $\\mathrm{SE}(3)$ -equivariant, in that the $\\mathrm{SE}(3)$ -   \nequivariance of EGNN directly follows [43]. ", "page_idx": 19}, {"type": "text", "text": "It is directly verified that the attention coefficients $\\mathbf{a}^{(t,s)}\\in\\mathbb{R}$ in Eq. 5 and the query $\\mathbf{q}^{[T]}$ , key $\\mathbf{k}^{[T]}$ , and value $\\dot{\\mathbf{v}}^{[T]}$ are all SE(3)-invariant, since they are derived based on the $\\mathrm{SE}(3)$ -invariant input ", "page_idx": 19}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/65580d37e7dd4d412535bbcd93dd4ad39c69336c058eeb482bb3cdee59bcd212.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 3: An illustration of different equivariant priors. For simplicity in the chart here we only illustrate the case when $N=3$ and $T_{c}=1$ , $T=1$ . ", "page_idx": 20}, {"type": "text", "text": "$\\mathbf{h}^{[T]}$ . This directly leads to the $\\mathrm{SE}(3)$ -invariance of the updated node feature $\\mathbf{h}^{\\prime\\left[T\\right]}$ . For the updated coordinates, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{\\mathrm{tr}}^{\\prime(t)}=\\mathbf{x}_{\\mathrm{tr}}^{(t)}+\\sum_{s\\in[T]}\\mathbf{a}_{\\mathrm{tr}}^{(t,s)}\\varphi_{\\mathbf{x}}(\\mathbf{v}_{\\mathrm{tr}}^{(t,s)})(\\mathbf{x}_{\\mathrm{tr}}^{(t)}-\\mathbf{x}_{\\mathrm{tr}}^{(s)}),}\\\\ &{\\qquad=\\mathbf{R}\\mathbf{x}^{(t)}+\\mathbf{r}+\\sum_{s\\in[T]}\\mathbf{a}^{(t,s)}\\varphi_{\\mathbf{x}}(\\mathbf{v}^{(t,s)})(\\mathbf{R}\\mathbf{x}^{(t)}+\\mathbf{r}-\\mathbf{R}\\mathbf{x}^{(s)}-\\mathbf{r}),}\\\\ &{\\qquad=\\mathbf{R}\\mathbf{x}^{(t)}+\\mathbf{r}+\\mathbf{R}\\left(\\sum_{s\\in[T]}\\mathbf{a}^{(t,s)}\\varphi_{\\mathbf{x}}(\\mathbf{v}^{(t,s)})(\\mathbf{x}^{(t)}-\\mathbf{x}^{(s)})\\right),}\\\\ &{\\qquad=\\mathbf{R}\\left(\\mathbf{x}^{(t)}+\\sum_{s\\in[T]}\\mathbf{a}^{(t,s)}\\varphi_{\\mathbf{x}}(\\mathbf{v}^{(t,s)})(\\mathbf{x}^{(t)}-\\mathbf{x}^{(s)})\\right)+\\mathbf{r},}\\\\ &{\\qquad=\\mathbf{R}\\mathbf{x}^{(t)}+\\mathbf{r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the variables with subscript $\\operatorname{tr}$ refers to their transformed counterparts when the input $\\mathbf{x}^{[T]}$ is transformed into $\\mathbf{R}\\mathbf{x}^{[T]}+\\mathbf{r}$ . Thus it completes the proof of SE(3)-equivariance of the temporal attention $f_{\\mathrm{attn}}$ and hence the entire $f_{\\mathrm{EGTN}}$ . ", "page_idx": 20}, {"type": "text", "text": "B More Details on Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We use Distributed Data Parallel on 4 Nvidia A6000 GPUs to train all the models. The training on NBody and ETH-UCY take around 12 hours while each MD17 training phase takes about a day. Our CPUs were standard intel CPUs. ", "page_idx": 20}, {"type": "text", "text": "B.2 Hyper-parameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We provide the detailed hyper-parameters of GeoTDM in Table 7. We adopt Adam optimizer with betas (0.9, 0.999) and $\\epsilon=\\bar{1}0^{-\\bar{8}}$ . For all experiments, we use the linear noise schedule per [18] with $\\beta_{\\mathrm{start}}=0.02$ and $\\beta_{\\mathrm{end}}=0.0001$ . ", "page_idx": 20}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/66cc89268469cc07e625f5fed91df686be3e51b6b89f97346d8746baef7f3117.jpg", "table_caption": ["Table 7: Hyper-parameters of GeoTDM in the experiments. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.3 Baselines ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the frame-to-frame prediction models, including RF [27], EGNN [43], TFN [56], and SE(3)- Transformer [9], we adopt the implementation in the codebase maintained by [43]. To yield a strong comparison, instead of taking one frame as input to directly predict the final frame, we employ a discretized NeuralODE [4]-style training and inference procedure. In particular, we train the models with position $\\mathbf{x}^{(t)}$ and velocity (computed as the difference of the current and previous frame, i.e., $\\mathbf{v}^{(t)}\\bar{=}\\,\\mathbf{x}^{(t)}-\\mathbf{x}^{(t-1)})$ as input to predict the next velocity $\\hat{\\mathbf{v}}^{(t+1)}$ . The position for the next step is integrated as $\\hat{\\mathbf{x}}^{(t+1)}=\\mathbf{x}^{(t)}\\bar{+}\\hat{\\mathbf{v}}^{(t+\\bar{1})}$ . The training loss is computed as the Mean Squared Error (MSE) between the predicted position \u02c6x(t+1) and the ground truth position x(gtt+ 1). In inference time, a rollout prediction is conducted, which iteratively predict the next step by feeding the predicted position and velocity at the current step, for a total of $T$ steps. We follow the hyper-parameter tuning guideline for these baselines by [16] which conduct a random search over the space spanned by the number of layers in $\\{2,4,6,8\\}$ , the hidden dimension $\\{32,64,128\\}$ , learning rate $\\{5e\\bar{-}3,1e{-}3,5e{-}4,1e{-}4\\}$ , and batch size 32, 64, 128, 256, and select the model with best performance. All models are trained towards convergence with an early-stopping counter of 5, with validation performed every 20 epochs. ", "page_idx": 21}, {"type": "text", "text": "For EqMotion, we directly adopt the code by [62] and their suggested hyper-parameters for the N-body datasets and MD17 datasets. ", "page_idx": 21}, {"type": "text", "text": "For SVAE [67] and SGAN [14], these methods are originally developed for the pedestrian trajectory forecasting task. The backbone model that processes the input trajectory consists of social pooling operation and GRU or LSTM blocks for temporal processing. In order the make them favorable in tackling geometric systems which additionally include node features and edge features, we replace the social pooling operations by MPNNs [11] in both encoder (or discriminator) and decoder (or generator) to synthesize the information on the geometric graph. The temporal module is still kept as GRU for SVAE and LSTM for SGAN, following their original implementations. We also search over the best hyper-parameters which additionally involve the KL-divergence weight in $\\{1,0.1,0.01,0.001\\}$ for SVAE according to the validation ELBO. For EGVAE, we replace the MPNNs in SVAE by EGNN [43], and restructured the latent space of the prior with both equivariant and invariant latent features. By this means, EGVAE is also guaranteed to model an equivariant distribution in the conditional case and an invariant distribution in the unconditional case. ", "page_idx": 21}, {"type": "text", "text": "B.4 Model ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In detail, the EGCL layer [43] is given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf{m}}_{i j}=\\varphi_{{\\bf{m}}}\\left({\\bf{h}}_{i},{\\bf{h}}_{j},\\|{\\bf{x}}_{i}-{\\bf{x}}_{j}\\|,{\\bf{e}}_{i j}\\right),}}\\\\ {~~}\\\\ {{\\displaystyle{\\bf{h}}_{i}^{\\prime}=\\varphi_{{\\bf{h}}}\\left(\\sum_{\\substack{j\\in{\\cal N}(i)}}\\sum_{\\substack{j\\in{\\cal N}(i)}}{\\bf{m}}_{i j}\\right),}}\\\\ {{\\displaystyle~{\\bf{x}}_{i}^{\\prime}={\\bf{x}}_{i}+\\sum_{j\\in{\\cal N}(i)}\\varphi_{{\\bf{x}}}\\left({\\bf{m}}_{i j}\\right)\\left({\\bf{x}}_{i}-{\\bf{x}}_{j}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\varphi_{\\mathbf{m}}$ , $\\varphi_{\\mathbf{h}}$ , and $\\varphi_{\\mathbf{x}}$ are all MLPs. We also provide a schematic of our proposed EGTN in Fig. 4 for better illustration. ", "page_idx": 21}, {"type": "text", "text": "B.5 Evaluation Metrics in the Unconditional Case ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "All these metrics are evaluated on a set of model samples with the same size as the testing set. ", "page_idx": 21}, {"type": "text", "text": "Marginal score is computed as the absolute difference of two empirical probability density functions. Practically, we collect the $x,y,z$ coordinates at each time step marginalized over all nodes in all systems in the predictions and the ground truth (testing set). Then we split the collection into 50 bins and compute the MAE in each bin, finally averaged across all time steps to obtain the score. Note that on MD17, instead of computing the pdf on coordinates, we compute the pdf on the length of the chemical bonds, which is a clearer signal that correlates to the validity of the generated MD trajectory, since during MD simulation the bond lengths are usually stable with very small vibrations. Marginal score gives a broad statistical measurement how each dimension of the generated samples align with the original data. ", "page_idx": 21}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/898f4bbcd4a7f5fa68499e32bdb33f11f1d645f0150c20c6af625301571d444f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 4: Schematic of the proposed EGTN, which alternates the EGCL layer for extracting spatial interactions and the temporal attention layer for modeling temporal sequence. Additional conditional information $\\mathbf{x}_{c}^{[T_{c}]}$ and $\\mathbf{h}_{c}^{[T_{c}]}$ can also be processed using cross-attention. The relative temporal embedding $\\psi(t-s)$ is added to the key and value. DotProd refers to dot product and Softmax is performed over indexes of $s$ . ", "page_idx": 22}, {"type": "text", "text": "Classification score is computed as the cross-entropy loss of a sequence classification model that aims to distinguish whether the trajectory is generated by the model or from the testing set. To be specific, we construct a dataset mixed by the generated samples and the testing set, and randomly split it into $80\\%$ and $20\\%$ subsets for training and testing. Then the model is trained on the training set and the classification score is computed as the cross-entropy on the testing set. We use a 1-layer EqMotion with a classification head as the model. The classification score provided intuition on how difficult it is to distinguish the generated samples and the original data. ", "page_idx": 22}, {"type": "text", "text": "Prediction score is computed as the MSE loss of a train-on-synthetic-test-on-real sequence to sequence model. In detail, we train a 1-layer EqMotion on the sampled dataset with the task of predicting the second half of the trajectory given the first half. We then evaluate the model on the testing set and report the MSE as the prediction score. Prediction score provides intuition on the capability of the generative model on generating synthetic data that well aligns with the ground truth. ", "page_idx": 22}, {"type": "text", "text": "C More Experiments and Discussions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Model Composition for Longer Trajectory ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Since attention is utilized to extract temporal information, the time complexity scales quadratically with the length of the input trajectory, both during training and inference. In practice, we can instead train models on shorter trajectories and compose them during inference for longer trajectories, in both unconditional and conditional cases. For target trajectories with length $T$ , we can first decompose it into $K$ several equal-length7 non-overlapping intervals with time span $\\Delta T$ . Then, for the unconditional case, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\np(\\mathbf{x}^{[T]})=p(\\mathbf{x}^{[\\Delta T]})\\prod_{k=1}^{K-1}p(\\mathbf{x}^{k\\Delta T+[\\Delta T]}\\mid\\mathbf{x}^{(k-1)\\Delta T+[\\Delta T]}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "by assuming mild conditional independence, where $p(\\mathbf{x}^{[\\Delta T]})$ is an unconditional model for trajectory with length $\\Delta T$ , and $p(\\mathbf{x}^{k\\Delta T+\\left[\\Delta T\\right]}\\mid\\mathbf{x}^{(k-1)\\Delta T+\\left[\\Delta\\hat{T}\\right]})$ can be learned by a conditional model for short trajectories. The conditional case directly follows by factorizing into products of conditional distribution over shorter trajectories. ", "page_idx": 22}, {"type": "text", "text": "We provide a demonstration of such technique as gifs in the supplementary file. ", "page_idx": 22}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/6ca624684a61ec2df240ee7bf9977b50e85137388b6dd9c566acd27bae3c7932.jpg", "table_caption": ["Table 8: The effect of diffusion steps in the unconditional generation setting (top) and conditional forecasting setting (bottom). "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.2 Number of Diffusion Steps ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide results in the unconditional generation setting for $\\tau=100$ . The results are in Table 8. Compared with the conditional setting, the unconditional generation is more challenging in that is needs to generate trajectories without any given reference geometries. We observe a drop in performance when $\\tau$ is decreased from 1000 to 100. However, the performance with only 100 diffusion steps is still significantly better than SVAE. ", "page_idx": 23}, {"type": "text", "text": "C.3 Sampling Time ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the table below we display the generation metrics and the inference time per batch with batch size 128 on MD17 Aspirin molecule. We compare GeoTDM with EGVAE, an autoregressive VAE-based method with EGNN as the backbone. Here GeoTDM-100 and GeoTDM-1000 refer to GeoTDM using 100 and 1000 diffusion steps, respectively. ", "page_idx": 23}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/b5830319386a5b3200c1629ffb9e2d2624a1fdffd86cf8655ea1191aa1ca5e1d.jpg", "table_caption": ["Table 9: Sampling runtime comparison on MD17 Aspirin molecule. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We observe that GeoTDM-100 is approximately 10 times slower than EGVAE, since the model requires 100 calls of the denoising network to generate one batch, while EGVAE consumes the same number of calls as the length of the trajectory (20 in this case) due to autoregressive modeling. Although GeoTDM is slower, the gain in performance is significant and the quality of the generated trajectory is remarkably better than that of EGVAE. When further increasing the number of diffusion steps to 1000, the performance becomes better while requiring much more compute. ", "page_idx": 23}, {"type": "text", "text": "However, it is worth noticing that all these deep learning-based methods are significantly faster than traditional methods like DFT, which typically requires hours to even several days to converge depending on the scale of the system, according to OCP [3]. Therefore, although GeoTDM becomes slower than VAEs when using larger number of diffusion steps, it is still much faster than DFT, which indicates its practical value in generating geometric trajectories like molecular dynamics simulation. ", "page_idx": 23}, {"type": "text", "text": "The computation overhead of diffusion models compared with VAEs or GANs has been a well-known issue. We recognize enhancing the efficiency of GeoTDM as an interesting direction of future work, potentially through adopting faster solvers like DDIM [50] or DPMSolver [29], performing consistency distillation [51], or developing latent diffusion models [64] that take advantage of a more compact representation of the spatio-temporal geometric space. ", "page_idx": 23}, {"type": "text", "text": "C.4 Standard Deviations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide the standard deviations in Table 10 and 11. ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/4a9c49cc48bd2343795582d3d1ee896e800dea747af3ae7ae42b61284c10ad45.jpg", "table_caption": ["Table 10: Conditional generation results of GeoTDM on N-body charged particle, spring, and gravity. Results (mean $\\pm$ standard deviation) are computed from 5 samples. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/8bab51ab5c88c3744729e053b013251c9b7a5a83b78fffeb937a48a9d87d600a.jpg", "table_caption": ["Table 11: Conditional generation results of GeoTDM on MD17. Results (mean $\\pm$ standard deviation) are computed from 5 samples. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.5 More Discussions with Existing Works ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Below we discuss the unique challenges for designing GeoTDM compared with MID [12] and geometric diffusion models like GeoDiff [66] and GeoLDM [64], and how we tackle these challenges. ", "page_idx": 24}, {"type": "text", "text": "Modeling geometric trajectories. Although MID can model trajectories, it leverages Trajec$\\mathrm{{tron}++}$ [40] backbone which takes as input the position vectors through a Transformer network. It requires non-trivial effort to incorporate additional node features and edge features into MID, while for GeoTDM, we design a general backbone EGTN that can process geometric trajectories while preserving equivariance. Existing geometric diffusion models (e.g., GeoDiff and GeoLDM) never consider modeling the temporal dynamics and their backbone can only work on static (single-frame) geometric strctures. ", "page_idx": 24}, {"type": "text", "text": "Incorporating equivariance into temporal diffusion. While geometric diffusion models have discussed proper ways to inject equivariance into diffusion models, it is unclear how to preserve equivariance when each hidden variable in the diffusion process has an additional dimension of time. In this work, we formally define equivariance constraint we want to impose on the marginal distribution, and how to design the prior and transition kernel in order to fulfill the constraint, in the context where all hidden variables are geometric trajectories. This is technically very different from existing works (e.g., GeoDiff and GeoLDM) since the dimension of the data is fundamentally different, which leads to different analyses. ", "page_idx": 24}, {"type": "text", "text": "Consideration of both conditional and unconditional generation scenarios. MID is only designed and evaluated in the conditional setting where the task is to forecast the future trajectory given initial frames. GeoDiff and GeoLDM only operate in the unconditional setting where the task is to generate the structure without any initial 3D structure information. In this work, we systematically discuss both unconditional and conditional generation for geometric trajectories, and elaborate on how to design the prior and transition kernel to meet the equivariance constraint. ", "page_idx": 24}, {"type": "text", "text": "Parameterization of the learnable equivariant prior. In the conditional case, we propose to parameterize the equivariant prior with a lightweight EGTN. Such appraoch offers more flexibility in the equivariant prior, enabling optimizing it during training, which is also proved to be able to subsume existing center-of-mass (CoM) based parameterization (see Theorem A.4 in Appendix). Experiments in ablation studies also verify the superiority of such design. ", "page_idx": 24}, {"type": "text", "text": "We summarize the points above in Table 12. ", "page_idx": 24}, {"type": "text", "text": "D More Visualizations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide more visualizations in Fig. 5, 6, 8, 7, 9, and 10. Please refer to their captions for the detailed descriptions. ", "page_idx": 24}, {"type": "table", "img_path": "OYmms5Mv9H/tmp/a041b9591a1b340bbe017a56a33fcc05c8b566afffada9d024b6a6aeda1a910c.jpg", "table_caption": ["Table 12: Technical differences between GeoTDM and existing works. "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/ce527025a3afd8acde7d6b83a88350575b4dd002bea34be0ddf4bcc34258a912.jpg", "img_caption": ["Figure 5: Uncurated samples of GeoTDM on MD17 dataset in the unconditional generation setup. From top-left to bottom-right are trajectories of the eight molecules: Aspirin, Benzene, Ethanol, Malonaldehyde, Naphthalene, Salicylic, Toluene, and Uracil. Five samples are displayed for each molecule. GeoTDM generates high quality samples. It well captures the vibrations and rotating behavior of the methyl groups in Aspirin and Ethanol. The bonds on the benzene ring are also more stable, aligning with findings in chemistry. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/b88e84a844d0af3d1762e6060e40d5a8a3cf40b262b4df0394796a630f30d0d1.jpg", "img_caption": ["Figure 6: Samples from MD17 dataset. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/a7e2450d15c7e94db2db6850d699b4b6f9ad9c020f8984433d59ddc9f1eb9316.jpg", "img_caption": ["Figure 7: Visualization of the diffusion trajectory at different diffusion steps. From top to bottom: Aspirin, Naphthalene, Salicylic, Uracil. For each molecule, the first row shows the unconditional generation process, where the model generates the trajectory from the invariant prior purely from the molecule graph without any conditioning structure. The second row refers to the conditional generation, where the model generates from the equivariant prior, conditioning on some given frames $\\mathbf{x}_{c}^{[T_{c}]}$ . Notably, the equivariant prior (see samples at $\\tau=\\tau$ in each second row) preserves some structural information encapsulated in xcc, thanks to our flexible parameterization. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/954f9a2f5f51a17a18b7f4899e3b359fca7ef989af776b705fd4a1b04497e0f1.jpg", "img_caption": ["Figure 8: Uncurated samples of GeoTDM on MD17 dataset in the conditional forecasting setting. We highlight some regions of interest in red dashed boxes. GeoTDM delivers samples with very high accuracy while also capturing some stochasticity of the molecular dynamics. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/fb7dcca8fce218685125ee03a6a399e130dd4ec03fa0ebc6659d57792d4a4dd4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 9: Visualization of data samples and generated samples by GeoTDM and SVAE in the unconditional setting on Charged Particles dataset. Nodes with color red and blue have the charge of $+1/-1$ , respectively. Best viewed by zooming in. ", "page_idx": 27}, {"type": "image", "img_path": "OYmms5Mv9H/tmp/9da899be868eebb1b0aaf85b42c2e1cd127ad07beefeb8996d4f760577e0e9cb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 10: Visualization of predictions by GeoTDM and EGNN in the conditional setting on Charged Particles dataset. Nodes with color red and blue have the charge of $+1/-1$ , respectively. Best viewed by zooming in. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have provided theoretical and empirical results showing that our proposed GeoTDM is able to work with geometric symmetries through multiple time steps. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have discussed some limitations of GeoTDM, including the slow sampling speed of diffusion models (see Sec. C.3). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes, we have listed out all of our assumptions and have explicated each step for our proofs. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes, we have included all of the relevant details for reproducing our experiments. See App. B. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We upload the code in supplementary file. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See our code and experiment details in App. B. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: See Sec. C.4. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have listed out this type of information in App. B. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have reviewed and followed the NeurIPS code of ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our method is primarily focused on scientific discovery, and there is no societal impact of our work beyond this scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our method does not have a high risk of misuse. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have credited all code and dataset sources. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not introduce any new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This work does not involve human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This work does not involve human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]