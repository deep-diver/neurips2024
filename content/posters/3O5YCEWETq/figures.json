[{"figure_path": "3O5YCEWETq/figures/figures_1_1.jpg", "caption": "Figure 1: Size, time, and accuracy overview of TTMB vs. open-sourced pre-trained TS benchmarks. We plot each model based on its model size and per batch CPU inference time. The X% mentioned for each baseline indicates that the baseline's forecast is X% less accurate compared to TTM's forecast. Full details in Tables [1-5].", "description": "This figure compares the Tiny Time Mixer (TTM) model with other state-of-the-art (SOTA) pre-trained time series models in terms of model size and CPU inference time.  It shows that TTM significantly outperforms the SOTAs in terms of accuracy while being much smaller and faster. The X% values indicate the percentage improvement of TTM over each benchmark model in terms of forecasting accuracy.", "section": "1 Introduction"}, {"figure_path": "3O5YCEWETq/figures/figures_2_1.jpg", "caption": "Figure 2: TTM overview (a) Refer to Sections 2 and 3, (b) Refer to Section 3.1, (c) Refer to Section 3.2", "description": "This figure provides a comprehensive overview of the Tiny Time Mixer (TTM) model architecture and workflow. It is divided into three parts:\n(a) illustrates the components and workflows of TTM. It shows a multi-level architecture with a backbone for feature extraction, a decoder for channel correlation and exogenous signal handling, and a head for forecasting.\n(b) details the TTM backbone architecture. It explains how the backbone uses adaptive patching, diverse resolution sampling, and resolution prefix tuning to handle varied dataset resolutions.\n(c) shows how exogenous information is incorporated into the forecasting process using an exogenous mixer.\nIn summary, this figure provides a detailed view of the TTM model's design, showing how it combines various components and techniques to achieve efficient and accurate multivariate time series forecasting.", "section": "2 TTM Components"}, {"figure_path": "3O5YCEWETq/figures/figures_7_1.jpg", "caption": "Figure 2: TTM overview (a) Refer to Sections 2 and 3, (b) Refer to Section 3.1, (c) Refer to Section 3.2", "description": "This figure provides a comprehensive overview of the Tiny Time Mixer (TTM) model architecture and workflow. Panel (a) illustrates the main components of the TTM model: the backbone, decoder, forecast head, and optional exogenous mixer. It also depicts the pre-training and fine-tuning workflows. Panel (b) details the architecture of the TTM backbone, which consists of multiple levels and blocks of TSMixer units. Adaptive patching, diverse resolution sampling, and resolution prefix tuning are highlighted as key features for handling multi-resolution data. Panel (c) explains the exogenous mixer, which fuses exogenous data into the forecasting process to capture channel correlations and exogenous signals.", "section": "2 TTM Components"}, {"figure_path": "3O5YCEWETq/figures/figures_8_1.jpg", "caption": "Figure 1: Size, time, and accuracy overview of TTMB vs. open-sourced pre-trained TS benchmarks\u00b2. We plot each model based on its model size and per batch CPU inference time. The X% mentioned for each baseline indicates that the baseline's forecast is X% less accurate compared to TTM's forecast. Full details in Tables [1-5].", "description": "This figure compares the performance of Tiny Time Mixers (TTMB) against other open-source pre-trained time series models.  It shows the model size (in millions of parameters) plotted against inference time (per batch in seconds) on a logarithmic scale.  Each point represents a different model, and the percentage values near each point indicate the relative performance improvement of TTMB compared to that specific model.  The figure highlights that TTMB achieves comparable or higher accuracy with a smaller model size and much faster inference time, thus being more efficient.", "section": "1 Introduction"}, {"figure_path": "3O5YCEWETq/figures/figures_8_2.jpg", "caption": "Figure 1: Size, time, and accuracy overview of TTMB vs. open-sourced pre-trained TS benchmarks. We plot each model based on its model size and per batch CPU inference time. The X% mentioned for each baseline indicates that the baseline's forecast is X% less accurate compared to TTM's forecast. Full details in Tables [1-5].", "description": "This figure compares the model size and inference time of the proposed Tiny Time Mixer (TTM) model against other state-of-the-art (SOTA) pre-trained time series forecasting models. It shows that TTM achieves comparable accuracy with a significantly smaller model size and faster inference time.  The X% values represent the accuracy improvement of TTM over the other models.", "section": "1 Introduction"}, {"figure_path": "3O5YCEWETq/figures/figures_8_3.jpg", "caption": "Figure 5: (a) TTM embedding projections across 3 datasets and 3 segments within datasets. (b) Cross-channel attention based explanation.", "description": "This figure visualizes the TTM embeddings from various datasets (weather, traffic, and electricity) using PCA projection. Each dataset is represented by a different color. From each dataset, three distant, non-overlapping, fixed-length time segments (S-1, S-2, S-3) are selected, each depicted with a unique marker shape. The visualization uses the first and second principal components of the TTM embeddings. The inset image focuses on the weather dataset alone, revealing a deeper structure learned by the TTM architecture. The cyclic orbits in the embeddings reflect the seasonal patterns in the data. Both hourly datasets (traffic and electricity) form concentric orbits due to similar seasonal patterns, while the weather data, with its distinct seasonal pattern, shows cyclic orbits in a different sub-dimension. In addition, the cross-channel attention from the fine-tuned model's channel mixing layers reveals feature importance across channels. As shown, the model focuses on channels like weathersit, season, holiday, and temperature to predict bike-rental counts. These attention model weights correlate with the general data characteristics where bike rental demands are heavily influenced by weather and holidays, providing explanation for the fine-tuned model predictions.", "section": "TTM Model Insights & Explainability"}, {"figure_path": "3O5YCEWETq/figures/figures_25_1.jpg", "caption": "Figure 2: TTM overview (a) Refer to Sections 2 and 3, (b) Refer to Section 3.1, (c) Refer to Section 3.2", "description": "This figure provides a comprehensive overview of the Tiny Time Mixer (TTM) model architecture and workflow. It's broken down into three subfigures:\n(a) Shows the TTM components and workflows including the backbone, decoder, forecast head, and exogenous mixer. It also illustrates the pre-training and fine-tuning workflows.\n(b) Details the architecture of the TTM backbone. It consists of multiple levels and blocks which allow for mixing of features within patches, across patches and channels. It highlights elements such as adaptive patching, diverse resolution sampling, and resolution prefix tuning.\n(c) Illustrates the exogenous mixer which combines the model's forecasts with known exogenous values, enabling the model to integrate external information into the forecast predictions.", "section": "2 TTM Components"}, {"figure_path": "3O5YCEWETq/figures/figures_26_1.jpg", "caption": "Figure 1: Size, time, and accuracy overview of TTMB vs. open-sourced pre-trained TS benchmarks. We plot each model based on its model size and per batch CPU inference time. The X% mentioned for each baseline indicates that the baseline's forecast is X% less accurate compared to TTM's forecast. Full details in Tables [1-5].", "description": "This figure compares Tiny Time Mixer (TTMB) with other state-of-the-art (SOTA) pre-trained time series models in terms of model size, inference time, and forecasting accuracy.  Each model is represented by a point on a scatter plot where the x-axis represents model size (in millions of parameters) and the y-axis represents CPU inference time (in seconds). The percentage value next to each SOTA model indicates how much less accurate its forecast is compared to TTMB's. The figure clearly demonstrates the superior performance of TTMB in terms of efficiency and accuracy.", "section": "1 Introduction"}, {"figure_path": "3O5YCEWETq/figures/figures_27_1.jpg", "caption": "Figure 5: (a) TTM embedding projections across 3 datasets and 3 segments within datasets. (b) Cross-channel attention based explanation.", "description": "This figure visualizes the TTM embeddings from various datasets (weather, traffic, and electricity) using PCA projection. Each dataset is represented by a different color. From each dataset, three distant, non-overlapping, fixed-length time segments (S-1, S-2, S-3) are selected, each depicted with a unique marker shape. The visualization uses the first and second principal components of the TTM embeddings. The inset image focuses on the weather dataset alone, revealing a deeper structure learned by the TTM architecture. The cyclic orbits in the embeddings reflect the seasonal patterns in the data. Both hourly datasets (traffic and electricity) form concentric orbits due to similar seasonal patterns, while the weather data, with its distinct seasonal pattern, shows cyclic orbits in a different sub-dimension. In addition, the cross-channel attention from the fine-tuned model's channel mixing layers reveals feature importance across channels. As shown, the model focuses on channels like weathersit, season, holiday, and temperature to predict bike-rental counts. These attention model weights correlate with the general data characteristics where bike rental demands are heavily influenced by weather and holidays, providing explanation for the fine-tuned model predictions.", "section": "TTM Model Insights & Explainability"}]