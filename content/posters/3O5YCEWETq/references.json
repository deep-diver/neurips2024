{"references": [{"fullname_first_author": "Abdul Fatir Ansari", "paper_title": "Chronos: Learning the language of time series", "publication_date": "2024-03-07", "reason": "This paper introduces Chronos, a strong SOTA benchmark model in the domain of time series forecasting, and it directly inspired the creation of the TTM model presented in this work."}, {"fullname_first_author": "Abhimanyu Das", "paper_title": "A decoder-only foundation model for time-series forecasting", "publication_date": "2023-00-00", "reason": "This paper introduced a decoder-only foundation model for time series forecasting, which is also a relevant SOTA benchmark, demonstrating the strength of transfer learning techniques in this field and influencing the development of the TTM."}, {"fullname_first_author": "Vijay Ekambaram", "paper_title": "TSMixer: Lightweight mlp-mixer model for multivariate time series forecasting", "publication_date": "2023-00-00", "reason": "This paper details the TSMixer architecture, the base model on which the TTM model is built, explaining the core features that enhance its speed and performance."}, {"fullname_first_author": "Mononito Goswami", "paper_title": "Moment: A family of open time-series foundation models", "publication_date": "2024-00-00", "reason": "This paper introduced Moment, a significant SOTA benchmark model in time series forecasting, influencing this research by demonstrating the capabilities and challenges of large-scale pre-trained TS models."}, {"fullname_first_author": "Yuqi Nie", "paper_title": "A time series is worth 64 words: Long-term forecasting with transformers", "publication_date": "2023-00-00", "reason": "This paper, another relevant SOTA benchmark in time series forecasting, presents an approach which uses transformers and inspired the usage of patching techniques in TTM, impacting its performance."}]}