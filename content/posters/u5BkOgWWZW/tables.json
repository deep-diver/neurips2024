[{"figure_path": "u5BkOgWWZW/tables/tables_3_1.jpg", "caption": "Table 1: We compare the reference predicates and our learned predicates when clustering the DBPedia dataset. We abbreviate the predicates, e.g. \u201cart\u201d = \u201chas a topic of art\u201d. For each reference, we match it with the learned predicate that achieves the highest F1-score at predicting the reference denotation. We also report the surface similarity (defined in Section 5.2) between the learned predicate and the reference. Our learning algorithm mostly recovers the underlying reference predicates, though it sometimes learns larger/correlated cluster that disagrees with the reference but is still meaningful.", "description": "This table compares the reference predicates (ground truth) against the predicates learned by the model when clustering the DBPedia dataset.  For each reference predicate, the table shows the learned predicate with the highest F1 score (a measure of predictive accuracy) and the surface similarity (a measure of string similarity) between the reference and the learned predicate.  The results show that the model largely recovers the intended predicates, although some learned predicates represent broader, related concepts than the original references.", "section": "5 Experiments"}, {"figure_path": "u5BkOgWWZW/tables/tables_5_1.jpg", "caption": "Table 1: We compare the reference predicates and our learned predicates when clustering the DBPedia dataset. We abbreviate the predicates, e.g. \"art\" = \"has a topic of art\". For each reference, we match it with the learned predicate that achieves the highest F1-score at predicting the reference denotation. We also report the surface similarity (defined in Section 5.2) between the learned predicate and the reference. Our learning algorithm mostly recovers the underlying reference predicates, though it sometimes learns larger/correlated cluster that disagrees with the reference but is still meaningful.", "description": "This table compares the reference predicates (ground truth) with the predicates learned by the model when clustering the DBPedia dataset.  The learned predicates are abbreviated, with the full meaning provided in the caption. For each reference predicate, the table shows the learned predicate with the highest F1-score (the model's accuracy in predicting whether the reference predicate is true), the size of the reference cluster, the size of the cluster produced by the learned predicate, and a surface similarity score between the reference and learned predicates. The results show that the model largely recovers the reference predicates but occasionally creates larger, related clusters that differ slightly from the references.", "section": "5 Experiments"}, {"figure_path": "u5BkOgWWZW/tables/tables_7_1.jpg", "caption": "Table 2: Results on clustering. Ours always outperforms No-Refine and No-Relax, indicating that both continuous relaxation and iterative refinement are helpful. Compared to GoalEx [53], our method is slightly better on all datasets except DBPedia, which we analyze in Table 1.", "description": "This table presents the results of clustering experiments using different methods: Ours, No-Refine, No-Relax, and GoalEx (a specialized method).  The performance is measured by the F1-score and surface similarity. The results demonstrate that the proposed method (Ours) significantly outperforms the baselines (No-Refine and No-Relax) and is comparable to the specialized GoalEx method, highlighting the effectiveness of both continuous relaxation and iterative refinement.  A detailed analysis of the DBPedia dataset results is further provided in Table 1.", "section": "5 Experiments"}, {"figure_path": "u5BkOgWWZW/tables/tables_7_2.jpg", "caption": "Table 3: Our performance on time series (left) and classification (right). Both continuous relaxation and iterative refinement improve the performance (comparing Ours to No-Refine and No-Relax).", "description": "This table presents the performance of the proposed method (Ours) compared to ablative variants: No-Refine (without iterative refinement) and No-Relax (without continuous relaxation).  It evaluates performance across four time series datasets (topic, lang, locat, all) and a classification task, using F1-score and surface similarity as metrics.  The results show consistent improvement with both continuous relaxation and iterative refinement.", "section": "5.3 Experiments on Our Benchmark"}, {"figure_path": "u5BkOgWWZW/tables/tables_16_1.jpg", "caption": "Table 2: Results on clustering. Ours always outperforms No-Refine and No-Relax, indicating that both continuous relaxation and iterative refinement are helpful. Compared to GoalEx [53], our method is slightly better on all datasets except DBPedia, which we analyze in Table 1.", "description": "This table presents the results of clustering experiments using three different methods: the proposed method (\"Ours\"), a method without iterative refinement (\"No-Refine\"), and a method without continuous relaxation (\"No-Relax\").  The performance is measured using F1-score and surface similarity on five datasets (AGNews, DBPedia, NYT, Bills, Wiki).  The results demonstrate that both continuous relaxation and iterative refinement improve performance. A comparison with a specialized explainable clustering method (GoalEx) shows that the proposed method achieves comparable or slightly better performance across most datasets.", "section": "5.3 Experiments on Our Benchmark"}, {"figure_path": "u5BkOgWWZW/tables/tables_16_2.jpg", "caption": "Table 3: Our performance on time series (left) and classification (right). Both continuous relaxation and iterative refinement improve the performance (comparing Ours to No-Refine and No-Relax).", "description": "This table presents the results of experiments on time series and multiclass classification tasks. It compares the performance of three different approaches: (1) a baseline using only prompting; (2) a variant using continuous relaxation but without iterative refinement; and (3) the proposed approach combining both.  The results show that using both continuous relaxation and iterative refinement leads to significant performance gains, indicating the effectiveness of the proposed optimization algorithm. The table also includes results for various subtasks within the time-series modeling (topic, lang, locat) to illustrate the versatility of the model.", "section": "5.3 Experiments on Our Benchmark"}]