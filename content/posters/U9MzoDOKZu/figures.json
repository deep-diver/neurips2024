[{"figure_path": "U9MzoDOKZu/figures/figures_6_1.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder Ery that abstracts recent h-step history \u03bc\u03b5 into a compact task representation z\u012f, and the generalized decoders (R4, Tp) that predict the reward and next state conditioned on z\u012f. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure illustrates the Meta-DT framework.  It shows how a context-aware world model is pre-trained to learn a compact task representation from multi-task data. This representation is then used as a condition for a causal transformer to generate sequences, guiding task-oriented behavior. The figure also highlights a self-guided prompt generation method using past trajectories to select informative segments.", "section": "Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_7_1.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder E\u03c8 that abstracts recent h-step history \u03bct into a compact task representation zi, and the generalized decoders (R\u03c6, T\u03c6) that predict the reward and next state conditioned on zi. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure provides a visual overview of the Meta-DT architecture.  It shows the context-aware world model which learns a compact task representation from historical data.  This representation is then used as a context for a causal transformer which generates task-specific sequences.  A key element is the use of a self-guided prompt, based on the prediction error of the world model, to improve the performance of the causal transformer.", "section": "4 Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_8_1.jpg", "caption": "Figure 4: Test return curves of Meta-DT ablations using Medium datasets. w/o_context removes task representation, w/o_com removes the complementary way, and w/o_prompt removes the prompt.", "description": "This figure shows the ablation study results of the Meta-DT model on three different environments using Medium datasets.  The ablation studies systematically remove different components of the Meta-DT framework to determine their individual contributions to the overall performance. Specifically, it shows the impact of removing the task representation (w/o_context), the complementary prompt generation strategy (w/o_com), and the prompt itself (w/o_prompt).  The results highlight the importance of each component and demonstrate that Meta-DT's performance relies on the synergistic interaction of all its components.", "section": "5.3 Ablation Study"}, {"figure_path": "U9MzoDOKZu/figures/figures_8_2.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder Ery that abstracts recent h-step history \u03bce into a compact task representation zi, and the generalized decoders (R\u03c6, T\u03c6) that predict the reward and next state conditioned on zi. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure provides a comprehensive overview of the Meta-DT architecture. It illustrates the process of pretraining a context-aware world model to extract task-specific information from historical data, using this information to guide sequence generation via a causal transformer, and employing a self-guided prompt to enhance learning by focusing on areas where the world model's predictions are least accurate.", "section": "4 Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_9_1.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder E\u03c8 that abstracts recent h-step history \u03bct into a compact task representation zi, and the generalized decoders (R\u03c6, T\u03c6) that predict the reward and next state conditioned on zi. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure shows a schematic of the Meta-DT architecture.  It illustrates how a context-aware world model is pre-trained to learn a compact task representation from historical data. This representation is then used to condition a causal transformer for sequence generation, guiding the model to produce task-appropriate actions.  A crucial aspect is the use of a self-guided prompt generated from past trajectories to enhance task-specific information and complement the world model.", "section": "4 Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_18_1.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder E\u03c8 that abstracts recent h-step history \u03bcit into a compact task representation zit, and the generalized decoders (R\u03d5, T\u03c6) that predict the reward and next state conditioned on zit. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure illustrates the overall architecture of the Meta-DT model.  It shows how a context-aware world model is used to learn compact task representations from historical data. These representations are then fed into a causal transformer, which generates sequences of actions. A key component is the 'complementary prompt', a trajectory segment selected to maximize prediction error from the world model, aiming to add task-specific information not already captured by the model.", "section": "4 Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_18_2.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder E\u03c8 that abstracts recent h-step history \u03bct into a compact task representation zt, and the generalized decoders (R\u03c6, T\u03c6) that predict the reward and next state conditioned on zt. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure illustrates the Meta-DT framework. A context-aware world model is pretrained to learn a compact task representation from historical data, which is then used as a contextual condition for a causal transformer to generate task-oriented sequences. A self-guided prompt, created from the trajectory segment with the largest prediction error from the world model, provides additional task-specific information. ", "section": "4 Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_21_1.jpg", "caption": "Figure 9: The received return curves averaged over test tasks of Meta-DT with different values of context horizon h using Medium datasets under an aligned few-shot setting.", "description": "This figure shows the performance of Meta-DT with different context horizons (h=4, 6, 8) on three different environments: Point-Robot, Cheetah-Dir, and Ant-Dir.  The x-axis represents the training timesteps (in 1e5 units), and the y-axis represents the cumulative returns. The shaded region around each line represents the standard deviation or error bars. The figure helps to analyze the influence of the hyperparameter 'context horizon' (h) on the performance and stability of Meta-DT.", "section": "Appendix E. Hyperparameter Analysis"}, {"figure_path": "U9MzoDOKZu/figures/figures_21_2.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder Ery that abstracts recent h-step history \u03bct into a compact task representation zt, and the generalized decoders (R\u03c6, T\u03c6) that predict the reward and next state conditioned on zt. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure illustrates the overall architecture of Meta-DT, highlighting the three key components:  a context-aware world model for disentangling task-relevant information, a causal transformer for task-oriented sequence generation, and a complementary prompt design using past trajectories to enhance generalization.  The world model takes recent history as input, encodes it into a task representation, and predicts future rewards and states. This representation is then used as context for the transformer, which generates actions. The prompt is created using a trajectory segment with high prediction error from the world model to further optimize performance.", "section": "4 Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_22_1.jpg", "caption": "Figure 2: The received return curves averaged over test tasks of Meta-DT and baselines using Medium datasets under an aligned few-shot setting.", "description": "The figure shows the learning curves for several algorithms on various benchmark tasks in a few-shot setting using medium-quality datasets.  The x-axis represents the number of timesteps, and the y-axis represents the average return.  The plot shows Meta-DT achieving better performance and lower variance compared to several baselines, including CORRO, CSRO, Prompt-DT, and Generalized DT, demonstrating its superior generalization ability and stability in few-shot scenarios.", "section": "5 Experiments"}, {"figure_path": "U9MzoDOKZu/figures/figures_22_2.jpg", "caption": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder Ery that abstracts recent h-step history \u03bc\u03b5 into a compact task representation z\u012f, and the generalized decoders (R4, Tp) that predict the reward and next state conditioned on z\u012f. Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally.", "description": "This figure shows a schematic overview of the Meta-DT architecture.  It highlights the three key components: a context-aware world model for disentangling task-specific information, a meta decision transformer for sequence modeling, and a complementary prompt generated from past trajectories. The world model learns compact task representations which are then used to condition the transformer's sequence generation, making it task-aware.  The prompt mechanism uses the world model to guide the selection of informative trajectory segments for further improving performance.", "section": "Method"}, {"figure_path": "U9MzoDOKZu/figures/figures_23_1.jpg", "caption": "Figure 2: The received return curves averaged over test tasks of Meta-DT and baselines using Medium datasets under an aligned few-shot setting.", "description": "This figure compares the performance of Meta-DT against four other baselines (Prompt-DT, Generalized DT, CORRO, and CSRO) on six different tasks from the MuJoCo and Point-Robot environments.  The x-axis represents the number of timesteps (in 1e4 increments), and the y-axis shows the average return achieved.  The shaded area around each line represents the standard deviation across multiple trials. The figure demonstrates Meta-DT's superior performance and stability compared to the baselines across multiple tasks in a few-shot learning scenario.", "section": "5 Experiments"}]