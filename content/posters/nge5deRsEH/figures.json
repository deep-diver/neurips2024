[{"figure_path": "nge5deRsEH/figures/figures_1_1.jpg", "caption": "Figure 1: (a) An example of story continuation generated by our Auto-Regressive Decision Trees. We use decision trees and, remarkably, attain results comparable to Transformer-based models in terms of linguistic fluency. (b) The decision process of the decision trees. We visualize part of the tree ensemble, and can observe which word is most relevant for the splitting rule at each node.", "description": "This figure shows an example of story continuation generated by Auto-Regressive Decision Trees (ARDTs).  Part (a) displays a sample story continuation generated by the model, demonstrating its ability to produce fluent and coherent text comparable to Transformer models.  Part (b) visualizes a portion of the ARDT's decision-making process, illustrating how the model uses decision trees to determine the next word in the sequence based on word relevance.", "section": "4 Experiments"}, {"figure_path": "nge5deRsEH/figures/figures_5_1.jpg", "caption": "Figure 1: (a) An example of story continuation generated by our Auto-Regressive Decision Trees. We use decision trees and, remarkably, attain results comparable to Transformer-based models in terms of linguistic fluency. (b) The decision process of the decision trees. We visualize part of the tree ensemble, and can observe which word is most relevant for the splitting rule at each node.", "description": "This figure shows an example of text generation using Autoregressive Decision Trees (ARDTs). Part (a) displays a story continuation generated by the ARDT model, demonstrating its ability to produce fluent and coherent text comparable to transformer-based models. Part (b) visualizes the decision-making process of the ARDT by showing a portion of the decision tree ensemble, highlighting how the model selects the most relevant words for its prediction based on the splitting rules.", "section": "4 Experiments"}, {"figure_path": "nge5deRsEH/figures/figures_5_2.jpg", "caption": "Figure 1: (a) An example of story continuation generated by our Auto-Regressive Decision Trees. We use decision trees and, remarkably, attain results comparable to Transformer-based models in terms of linguistic fluency. (b) The decision process of the decision trees. We visualize part of the tree ensemble, and can observe which word is most relevant for the splitting rule at each node.", "description": "This figure shows an example of story continuation generated by the proposed Autoregressive Decision Trees (ARDTs).  The (a) part demonstrates the model's ability to generate fluent and grammatically correct text comparable to Transformers.  The (b) part visualizes a section of the ARDT's decision-making process, highlighting the word selection at each node.", "section": "4 Experiments"}, {"figure_path": "nge5deRsEH/figures/figures_7_1.jpg", "caption": "Figure 2: The Pipeline of Our Method. (a) Training. First, we employ a Word2Vec model to convert words into embeddings. Next, we utilize a sliding window approach to construct a dataset for training decision trees. Within this window, we performed a weighted average calculation, and the following token after the window was used as the label. (b) Inference. We use our trained Decision Trees for the purpose of next-token prediction.", "description": "This figure illustrates the training and inference pipeline of the proposed Auto-Regressive Decision Trees (ARDTs) model. The training process involves using Word2Vec to generate word embeddings, employing a sliding window to create training data, calculating a weighted average of the embeddings within the window, and using the next token after the window as a label. This data is then used to train an ensemble of decision trees using XGBoost.  The inference process uses the trained model to predict the next token in a sequence, given a current input sequence.  It uses a weighted average of word embeddings and the trained decision trees to make its prediction, selecting the output with the highest similarity score.", "section": "4 Experiments"}, {"figure_path": "nge5deRsEH/figures/figures_16_1.jpg", "caption": "Figure 3: t-SNE van der Maaten (2013) visualization of 20 cluster centers. We selected 20 cluster centers and display 4 words closest to the cluster centers.", "description": "This figure shows a t-SNE visualization of 20 word clusters from the TinyStories dataset.  The t-SNE algorithm reduces the dimensionality of the word embeddings, allowing for a 2D representation where semantically similar words cluster together. The visualization helps to illustrate the interpretability of the decision trees, as splitting rules can be interpreted in terms of semantic similarity to the words representing each cluster.  Four words closest to each cluster center are displayed for better understanding.", "section": "C Interpretability"}, {"figure_path": "nge5deRsEH/figures/figures_17_1.jpg", "caption": "Figure 4: Track the decision-making process within the decision trees. We use 'Lily and Tom loved to play together, and they found' as an the input prompt and generate the next word using our ARDTs. We visualize part of the process within the decision tree. Specifically, we visualized 31 nodes of the first decision tree.", "description": "This figure visualizes a portion of the decision-making process within a single decision tree from an ensemble of Auto-Regressive Decision Trees (ARDTs).  The input prompt is the sentence, \"Lily and Tom loved to play together, and they found.\" The figure shows a section of the tree, highlighting the nodes and decision splits, with the associated feature values and their corresponding branch selections (Y/N). This illustrates how the ARDT determines the next word in a sequence.  The visualization allows one to trace the path the model takes from the root node to a leaf node, which predicts the next word based on the input sequence.", "section": "4 Experiments"}, {"figure_path": "nge5deRsEH/figures/figures_17_2.jpg", "caption": "Figure 5: Feature Importance. We present the feature importance of the top 20 words most closely associated with each cluster, based on their average gain.", "description": "This figure shows the feature importance of the top 20 words in an XGBoost model, ranked by their average gain.  The words represent clusters of semantically similar words, and their importance reflects how much they contribute to the model's ability to make accurate predictions. Words higher on the chart are more important to the model's decision-making process.", "section": "4.1 Setting"}]