[{"figure_path": "buqvMT3B4k/tables/tables_3_1.jpg", "caption": "Table 2: The average PG of the algorithms on the benchmarks. In each row, we highlight in blue (bold) the best constructive (non-constructive) gap. Shapes marked with * are larger than those seen in training by our GM.", "description": "This table compares the performance of various algorithms on two benchmark datasets for the Job Shop Problem (JSP).  It shows the average percentage gap (PG) for each algorithm on different problem sizes (shapes).  The best constructive and non-constructive algorithms are highlighted for each shape, and shapes marked with an asterisk (*) indicate problem sizes larger than those used during the training of the generative model.  The table helps illustrate the relative effectiveness of different approaches, including greedy constructive methods, multiple constructive methods, priority dispatching rules (PDRs), reinforcement learning (RL) approaches, and the proposed generative model.", "section": "5.2 Performance on Benchmarks"}, {"figure_path": "buqvMT3B4k/tables/tables_6_1.jpg", "caption": "Table 2: The average PG of the algorithms on the benchmarks. In each row, we highlight in blue (bold) the best constructive (non-constructive) gap. Shapes marked with * are larger than those seen in training by our GM.", "description": "This table presents the average percentage gap (PG) of different algorithms on two benchmark datasets (Taillard's and Demirkol's) for job shop scheduling.  It compares several approaches: greedy constructive heuristics (simple methods for generating solutions), multiple randomized constructive heuristics (improving solutions by adding randomness), and non-constructive methods (more complex, non-heuristic methods).  The table is organized by shape (size of the problem) and highlights the best-performing algorithm for each shape, categorized as either constructive (building a solution step-by-step) or non-constructive.  Shapes marked with an asterisk (*) indicate problem sizes larger than those seen during the training of the generative model (GM). The table allows comparison of different solution generation strategies.", "section": "5.2 Performance on Benchmarks"}, {"figure_path": "buqvMT3B4k/tables/tables_7_1.jpg", "caption": "Table 3: The average gaps when sampling 128 solutions from architectures trained without and with self-labeling. CLUCL is the model obtained in [25] by training with reward-to-go on random instance shapes (no curriculum learning) and CL is similarly obtained by applying curriculum learning.", "description": "This table compares the average performance gaps of different models on Taillard and Demirkol benchmarks. The models are trained using different methods: reinforcement learning (RL) with and without curriculum learning, and self-labeling.  The results show that the self-labeling approach leads to lower gaps (better performance).", "section": "6.1 Self-Labeling other Architectures"}, {"figure_path": "buqvMT3B4k/tables/tables_14_1.jpg", "caption": "Table 1: The features of a context vector cj \u2208 R11 that describes the status of a job j within a partial solution \u03c0<t. Recall that o(t, j) is the ready operation of job j at step t, po(t, j) its machine, and o(t, j) \u2013 1 its predecessor.", "description": "This table lists 11 features used to create a context vector for each job within a partial solution during the construction of a solution for the Job Shop Problem (JSP).  Each feature provides information about the job's status relative to the current partial solution and its remaining operations, including completion times and comparisons to averages and quartiles.", "section": "4 Methodology: Generative Model & Self-Labeling"}, {"figure_path": "buqvMT3B4k/tables/tables_14_2.jpg", "caption": "Table 2: The average PG of the algorithms on the benchmarks. In each row, we highlight in blue (bold) the best constructive (non-constructive) gap. Shapes marked with * are larger than those seen in training by our GM.", "description": "This table compares the performance of various algorithms on two benchmark datasets (Taillard's and Demirkol's) for the Job Shop Problem (JSP).  It shows the average percentage gap (PG) for each algorithm on different problem instance sizes (shapes). The best-performing constructive (single-solution) and non-constructive (multiple-solution) algorithms are highlighted for each shape.  The asterisk (*) indicates problem shapes larger than those used for training the generative model (GM). This allows assessment of the GM's generalization capabilities.", "section": "5.2 Performance on Benchmarks"}, {"figure_path": "buqvMT3B4k/tables/tables_15_1.jpg", "caption": "Table 2: The average PG of the algorithms on the benchmarks. In each row, we highlight in blue (bold) the best constructive (non-constructive) gap. Shapes marked with * are larger than those seen in training by our GM.", "description": "This table presents the average percentage gap (PG) achieved by various algorithms on two benchmark datasets: Taillard's and Demirkol's.  The algorithms are categorized into greedy constructive, multiple (randomized) constructive, and non-constructive approaches.  The table highlights the best-performing constructive and non-constructive algorithms for each instance shape, offering a comparative analysis of different algorithmic approaches on the JSP problem.", "section": "5.2 Performance on Benchmarks"}, {"figure_path": "buqvMT3B4k/tables/tables_15_2.jpg", "caption": "Table 2: The average PG of the algorithms on the benchmarks. In each row, we highlight in blue (bold) the best constructive (non-constructive) gap. Shapes marked with * are larger than those seen in training by our GM.", "description": "This table presents the average percentage gap (PG) for various algorithms on two benchmark datasets (Taillard and Demirkol).  It compares the performance of several constructive heuristics (greedy and randomized), priority dispatching rules (PDRs), reinforcement learning (RL) approaches, and the proposed generative model (GM).  The table is organized to show separate results for constructive methods and non-constructive methods, highlighting the best performing algorithm for each instance shape in both categories.  The asterisk (*) indicates that some instance shapes are larger than those used in training the generative model.", "section": "5.2 Performance on Benchmarks"}, {"figure_path": "buqvMT3B4k/tables/tables_16_1.jpg", "caption": "Table 8: Average makespan (avg \u00b1 std) and execution time of CP-Sat and our GM (when sampling \u03b2 = 512 solutions) on very large instances.", "description": "This table compares the performance of CP-Sat and the proposed Generative Model (GM) on very large JSP instances.  It shows the average makespan (Cmax) and its standard deviation, along with the execution time for each solver on four different instance shapes. The \"Gap sum from CP-Sat\" row indicates the cumulative difference in makespan between the GM and CP-Sat across all instances.  Negative values indicate that the GM produced solutions with lower makespans (better performance) than CP-Sat.", "section": "5.2 Performance on Benchmarks"}]