[{"figure_path": "h0a3p5WtXU/tables/tables_1_1.jpg", "caption": "Table 1: Summary of existing assumptions on the problem (1) and their limitations. Here S denotes the set of minimizers of f and f* := argmin, fi(x). Unlike earlier conditions, the a-\u03b2-condition is specifically designed to capture local minima and saddle points. NN = Neural Network.", "description": "The table summarizes several existing conditions used in optimization, including QCvx, Aiming, and PL, comparing their definitions and limitations.  It highlights that these conditions often exclude saddle points and local minima and frequently require impractical levels of over-parametrization for neural networks.  In contrast, the newly proposed \u03b1-\u03b2-condition is designed to address these limitations by characterizing loss landscapes that can include saddle points and local minima without requiring excessive over-parametrization.", "section": "2 Related work"}, {"figure_path": "h0a3p5WtXU/tables/tables_6_1.jpg", "caption": "Table 1: Summary of existing assumptions on the problem (1) and their limitations. Here S denotes the set of minimizers of f and f* := argmin, fi(x). Unlike earlier conditions, the a-\u03b2-condition is specifically designed to capture local minima and saddle points. NN = Neural Network.", "description": "This table summarizes several existing conditions used in optimization, including QCvx, Aiming, PL, and the newly proposed \u03b1-\u03b2-condition.  It compares their definitions, limitations, and how they relate to the loss landscape of neural networks. The \u03b1-\u03b2-condition is highlighted for its ability to handle saddle points and local minima without requiring extensive over-parametrization, a major limitation of other methods.", "section": "2 Related work"}, {"figure_path": "h0a3p5WtXU/tables/tables_39_1.jpg", "caption": "Table 2: Summary of how the non-vanishing term \u03b2\u03c3<sub>int</sub> (as appearing e.g. in Eq. (9)) increases or decreases (\u221a) as a function of specific quantities of interest.", "description": "This table shows how the non-vanishing term \u03b2\u03c3<sub>int</sub> in the convergence rate changes depending on the model's width, depth, and batch size.  It summarizes empirical observations regarding the impact of model architecture choices on the convergence guarantees derived using the \u03b1-\u03b2-condition.  The symbol \u221a indicates a decrease in the term, while <sup>2</sup> indicates an increase.", "section": "Experimental validation of the \u03b1-\u03b2-condition"}]