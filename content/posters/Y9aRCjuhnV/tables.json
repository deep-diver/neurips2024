[{"figure_path": "Y9aRCjuhnV/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of policies on synthetic, Junyi, and OLI datasets. E[R] represents the average reward obtained in the final episode of training. Statistic after \u00b1 represents standard error across 30 trials.", "description": "This table compares the performance of different teaching policies (Random, WIQL, Myopic, TW, and EduQate) across three datasets (Synthetic, Junyi, and OLI).  For each policy and dataset, it shows the average reward (E[R]) achieved at the end of training and the intervention benefit (E[IB]) relative to a random policy. The \u00b1 values indicate the standard error across 30 independent trials.  The results demonstrate the superior performance of EduQate compared to other methods across the datasets. The intervention benefit (IB) is calculated as (E<sub>\u03c0</sub>(R(.)) - E<sub>Random</sub>(R(.)))/(E<sub>EduQate</sub>(R(.)) - E<sub>Random</sub>(R(.))), reflecting how much better each policy performs compared to a random policy, relative to EduQate.", "section": "5 Experiment"}, {"figure_path": "Y9aRCjuhnV/tables/tables_11_1.jpg", "caption": "Table 1: Comparison of policies on synthetic, Junyi, and OLI datasets. E[R] represents the average reward obtained in the final episode of training. Statistic after \u00b1 represents standard error across 30 trials.", "description": "This table compares the performance of different policies (Random, Myopic, TW, WIQL, and EduQate) across three datasets (Synthetic, Junyi, and OLI).  It shows the average reward (E[R]) achieved by each policy in the final training episode and the intervention benefit (IB) relative to a random policy. The \u00b1 values indicate the standard error across 30 trials, providing a measure of the variability in the results.  EduQate consistently outperforms other methods across all datasets.", "section": "5 Experiment"}, {"figure_path": "Y9aRCjuhnV/tables/tables_15_1.jpg", "caption": "Table 3: Comparison of policies on synthetic dataset, with different network setups. Note that that as Ntopics increase, the reliability of any algorithms decreases, as seen by the standard deviations of their average IB. EduQate- here refers to the EduQate algorithm without replay buffer.", "description": "This table presents the results of comparing different policies (WIQL, Myopic, TW, and EduQate) on a synthetic dataset with varying network complexities (controlled by the number of topics, Ntopics).  The table shows the average intervention benefit (IB) and its standard deviation for each policy and network setup.  The results highlight how the reliability of the algorithms decreases as the network complexity increases.  EduQate- represents the EduQate algorithm run without the experience replay buffer, showing its performance without this optimization technique.", "section": "E.1 Comparing Different Network Setups"}, {"figure_path": "Y9aRCjuhnV/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of policies on synthetic, Junyi, and OLI datasets. E[R] represents the average reward obtained in the final episode of training. Statistic after \u00b1 represents standard error across 30 trials.", "description": "This table presents a comparison of different reinforcement learning policies (EduQate, Threshold Whittle (TW), WIQL, Myopic, and Random) on three datasets: synthetic, Junyi, and OLI.  For each policy and dataset, the table shows the average reward (E[R]) obtained at the end of training and the intervention benefit (E[IB]) relative to a random policy.  The standard error across 30 trials is also indicated.  The results demonstrate EduQate's superior performance in maximizing rewards across diverse datasets.", "section": "5 Experiment"}, {"figure_path": "Y9aRCjuhnV/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of policies on synthetic, Junyi, and OLI datasets. E[R] represents the average reward obtained in the final episode of training. Statistic after \u00b1 represents standard error across 30 trials.", "description": "This table compares the performance of different teacher policies (EduQate, Threshold Whittle, WIQL, Myopic, and Random) across three datasets (Synthetic, Junyi, and OLI).  The metrics used are the average reward (E[R]) and the Intervention Benefit (IB), which measures the improvement over a random policy.  Standard error across 30 trials is also presented to show statistical significance.", "section": "5 Experiment"}]