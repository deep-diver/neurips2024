[{"type": "text", "text": "TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrei Margeloiu1\u2217, Xiangjian Jiang1\u2217, Nikola Simidjievski2,1, Mateja Jamnik1   \n1Department of Computer Science and Technology, University of Cambridge, UK 2PBCI, Department of Oncology, University of Cambridge, UK {am2770, xj265, ns779, mj201}@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data collection is often difficult in critical fields such as medicine, physics, and chemistry, yielding typically only small tabular datasets. However, classification methods tend to struggle with these small datasets, leading to poor predictive performance. Increasing the training set with additional synthetic data, similar to data augmentation in images, is commonly believed to improve downstream tabular classification performance. However, current tabular generative methods that learn either the joint distribution $p(\\mathbf{x},y)$ or the class-conditional distribution $p(\\mathbf{x}\\mid y)$ often overfit on small datasets, resulting in poor-quality synthetic data, usually worsening classification performance compared to using real data alone. To solve these challenges, we introduce TabEBM, a novel class-conditional generative method using Energy-Based Models (EBMs). Unlike existing tabular methods that use a shared model to approximate all class-conditional densities, our key innovation is to create distinct EBM generative models for each class, each modelling its class-specific data distribution individually. This approach creates robust energy landscapes, even in ambiguous class distributions. Our experiments show that TabEBM generates synthetic data with higher quality and better statistical fidelity than existing methods. When used for data augmentation, our synthetic data consistently leads to improved classification performance across diverse datasets of various sizes, especially small ones. Code is available at https://github.com/andreimargeloiu/TabEBM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many scientific domains within medicine, physics, and chemistry often rely on intricate and challenging data acquisition procedures [5, 50, 4, 32, 76, 12] that typically render smallsize tabular datasets [5, 46]. Using these to train machine learning models that can aid in tasks such as disease diagnosis [52, 37], material property prediction [35], and chemical compound classification [11], can lead to poor performance [74, 52, 37]. In the case of learning tasks which leverage image and text data, a standard remedy to address performance issues due to data scarcity is employing data augmentation techniques [72, 73, 60, 71] that generate additional synthetic samples from existing data. ", "page_idx": 0}, {"type": "image", "img_path": "FmNoFIImZG/tmp/7cff2ef841a5ea97110ac3747c216a4fa5063fcb13eac88e87fbb3205059b02b.jpg", "img_caption": ["Figure 1: Evaluation of TabEBM and other state-of-the-art tabular generative methods across six key metrics (larger area indicates better performance). The results demonstrate that TabEBM excels in data augmentation (utility), with a larger area than all other methods. "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "FmNoFIImZG/tmp/4def02dd7b82e2a746d4f98c59adc104fc7431343a60d4f4ec8f879797508775.jpg", "img_caption": ["Figure 2: An overview of TabEBM. We learn distinct class-specific Energy-Based Models (EBMs) $E_{\\mathrm{blue}}(\\mathbf{x})$ and $E_{\\mathrm{red}}(\\mathbf{x})$ exclusively on the points of their respective class. Each EBM approximates a class-conditional distribution $p(\\mathbf{x}|y)$ . TabEBM allows synthetic data generation by sampling from the estimated distributions for each class $p(\\mathbf{x}|y=\\mathsf{b l u e})$ and $p(\\mathbf{x}|y={\\bar{\\mathrm{red}}})$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, applying data augmentation to tabular data introduces additional challenges, as tabular datasets are often very diverse and lack explicit symmetries [8], such as rotations or translations seen in images. Consequently, existing tabular data augmentation methods often yield mixed results and can even degrade model performance [51, 71, 48], hindering their widespread adoption. ", "page_idx": 1}, {"type": "text", "text": "Tabular augmentation typically involves training generative models to approximate either the joint distribution $p(\\mathbf{x},y)$ [85, 24] or the class-conditional distribution $p(\\mathbf{x}|y)$ [85, 42, 83, 47, 48]. A key challenge of joint distribution methods is maintaining the original training label distribution, as sampling from such generators can produce label distributions that deviate from the original and even fail to generate data for specific classes (see Appendix C for an example). These issues compromise the effectiveness of data augmentation [51] by undermining the label accuracy and distribution. On the other hand, while class-conditional models that learn $p(\\mathbf{x}|y)$ preserve the stratification of the original data, they often employ a shared model to represent all class-conditional densities. This, however, can lead to overfitting, particularly in imbalanced datasets where the model may prioritise more frequent classes [21], ignoring unique features needed for generating label-invariant samples. Additionally, in datasets with limited data, this can lead to mode collapse [68, 70], where the model does not effectively capture the diversity of each class [70], and thus tends to perform poorly in a multi-class setting. ", "page_idx": 1}, {"type": "text", "text": "To address the challenges of class-conditional tabular generation, we introduce TabEBM (Figure 2), a new method for tabular data augmentation utilising Energy-Based Models (EBMs). Our method introduces two innovations: (i) Distinct class-specific models: TabEBM constructs a collection of individual models \u2013 one for each class \u2013 which, by design, enables learning distinct marginal distributions for the inputs associated with each class. This, in turn, enables performing data augmentation while maintaining the original label distribution. (ii) Generative models: we build novel class-specific generators that produce high-quality synthetic data even from extremely few samples. Specifically, we create a surrogate binary classification task for each class and fit it with a pre-trained tabular in-context classifier. We then convert the binary classifier into an EBM, a generative model, without additional training. Using class-specific EBMs makes the energy landscape more robust to class overlaps, compared to using a single shared EBM to approximate the class-conditional distribution. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarised as: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Technical: We propose TabEBM, which is the first generative method to create class-specific EBMs, learning the marginal distribution for each class separately. \u2022 Empirical: We present the first comprehensive analysis of tabular data augmentation across different dataset sizes and use cases beyond predictive performance. Our analysis compares TabEBM with eight leading tabular generative models across various datasets, demonstrating that TabEBM consistently improves data augmentation performance on small datasets, while our generated data demonstrates better statistical fidelity and privacy-preserving properties (Figure 1). \u2022 Library: We release TabEBM as an open-source library, available at https://github.com/ andreimargeloiu/TabEBM. Our library enables off-the-shelf data generation and data augmentation on any tabular dataset without requiring training. Further details are available in Appendix B.5. ", "page_idx": 1}, {"type": "text", "text": "2 TabEBM ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We address classification problems with $C$ classes, denoted by $\\mathcal{V}=\\{1,2,\\dots,C\\}$ . Let $\\{(\\mathbf{x}^{(i)},y_{i})\\}_{i=1}^{N}$ represent a dataset of $N$ samples, each being a $D$ -dimensional vector $\\mathbf{x}^{(i)}\\in\\mathbb{R}^{D}$ , with a corresponding label $y_{i}\\in\\mathcal{V}$ . For each class $c\\in\\mathcal{V}$ , we define $\\mathcal{X}_{c}=\\{\\mathbf{x}^{(i)}\\mid y_{i}=c\\}$ as the subset of samples labelled with class $c$ . Let $f_{\\theta}(\\cdot)$ denote a classifier. The expression $f_{\\theta}(\\mathbf{x})[y]$ represents the (unnormalised) logit assigned to the class $y$ for the input $\\mathbf{x}$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Preliminaries on Energy-Based Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "An Energy-Based Model (EBM) [43] defines a probability density function $p_{\\theta}(\\mathbf{x})$ through an energy function $E({\\bf x})$ . Specifically, the model posits that $p(\\mathbf{x})\\,\\propto\\,e^{-E(\\mathbf{x})}$ , where $E(\\mathbf{x})$ represents the unnormalised negative log-density of the input $\\mathbf{x}$ . In this framework, lower energy values correspond to higher probability densities. This relationship allows EBMs to model distributions by learning to assign lower energy to more probable configurations of $\\mathbf{x}$ and higher energy to less probable ones. ", "page_idx": 2}, {"type": "text", "text": "An important observation is that energy-based models can utilise the same model architectures as standard classification models [29]. Typically, the logits $f_{\\theta}(\\mathbf{x})[y]$ from a classification model define a discriminative distribution through the softmax function, expressed as $p_{\\theta}(y|\\mathbf x)=\\mathrm{softmax}(f_{\\theta}(\\mathbf x)[y])$ . Intriguingly, these same logits can be reinterpreted to define an energy-based model for the joint distribution $p(\\mathbf{x},y)$ . This is achieved by setting the energy function to $\\bar{E}(\\mathbf{x},y)=-f_{\\theta}(\\mathbf{x})$ . Furthermore, the energy function for the marginal distribution $p(\\mathbf x)$ is obtained by marginalising over $p(\\mathbf{x},y)$ , resulting in $E(\\mathbf{x})=-\\mathrm{LogSumExp}_{y^{\\prime}}f_{\\theta}(\\mathbf{x})[y^{\\prime}]$ . ", "page_idx": 2}, {"type": "text", "text": "Such an energy-based model, trained with EBM-specific protocols on multiple classes, is typically used as a classifier, as demonstrated on several computer vision tasks in [29]. In contrast, in this work our focus is the opposite: we propose employing trained classifiers, one for each specific class, as a generative energy-based model for the class-conditional distributions $p(\\mathbf{x}|y)$ . We apply our TabEBM method for generative tasks on tabular data. ", "page_idx": 2}, {"type": "text", "text": "2.2 Distinct Class-Specific Energy-Based Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "TabEBM is a class-conditional generative model $p(\\mathbf{x}|y)$ implemented using a set of EBMs, $\\{E_{1}(\\mathbf{x}),E_{2}(\\mathbf{x}),\\dots,E_{C}(\\mathbf{x})\\}$ . Our approach assumes that the class-conditional density $p(\\mathbf{x}|y=c)$ is best modelled using its class-specific data $\\scriptstyle{\\mathcal{X}}_{c}$ . Thus, for each class $c$ , we construct a class-specific EBM, $E_{c}(\\mathbf{x})$ , using only the data from that class, $\\scriptstyle{\\mathcal{X}}_{c}$ , such that $p(\\mathbf{x}|y=c)\\propto\\exp(-E_{c}(\\mathbf{x}))$ . ", "page_idx": 2}, {"type": "text", "text": "We derive each class-specific EBM $E_{c}(\\mathbf{x})$ by training a classifier on a novel task and reinterpreting its logits. Specifically, for each class $c$ , we propose a surrogate binary classification task to determine if a sample belongs to class $c$ by comparing $\\scriptstyle{\\mathcal{X}}_{c}$ against a set of surrogate negative samples $\\chi_{c}^{\\mathrm{neg}}$ , which we show in Figure 3. Specifically, we generate the negative samples at the corners of a hypercube in $\\bar{R^{D}}$ . For each dimension $d$ , the coordinates of a negative sample are either $\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}\\sigma_{d}$ or $-\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}\\sigma_{d}$ , where $\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}$ is a fixed constant and $\\sigma_{d}$ is the standard deviation of dimension $d$ . For example, in $R^{3}$ , a negative sample might have coordinates $[\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}\\sigma_{1},\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}\\bar{\\sigma}_{2},-\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}\\sigma_{3}]$ Placing the negative samples at the corners of a hypercube ensures they are easily distinguishable from the real data, which is crucial for an accurate energy function (see Appendix D.1.1). This placement is also robust to variations in the number and distance of the negative samples (see Appendices D.1.2 and D.1.3). ", "page_idx": 2}, {"type": "image", "img_path": "FmNoFIImZG/tmp/09367040a4a493f500a9bef19632ca3a9622f38fd52ff1d8cf0d3b8203b93261.jpg", "img_caption": ["Figure 3: The class-specific energy function $E_{c}(\\mathbf{x})$ from the surrogate binary task, where the blue region represents low energy (i.e., high data density). Placing the negative samples in a hypercube distant from the data results in an accurate energy function. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "We create the combined dataset $\\mathcal{D}_{c}$ for the surrogate binary classification task by labelling $\\scriptstyle{\\mathcal{X}}_{c}$ as 1 and $\\chi_{c}^{\\mathrm{neg}}$ as 0: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{D}_{c}=(\\mathcal{X}_{c}\\cup\\mathcal{X}_{c}^{\\mathrm{neg}},\\{1\\}^{|\\mathcal{X}_{c}|}\\cup\\{0\\}^{|\\mathcal{X}_{c}^{\\mathrm{neg}}|})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We then train a binary classifier $f_{\\theta}^{c}(\\cdot)$ on $\\mathcal{D}_{c}$ and use it to construct the class-specific energy $E_{c}(\\mathbf{x})$ for class $c$ . To do this, we reinterpret the logits $\\{f_{\\theta}^{c}(\\mathbf{x})[0],f_{\\theta}^{c}(\\mathbf{x})[1]\\}$ of the trained binary classifier as components of an approximated joint distribution for the surrogate binary task: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{c}(\\mathbf{x},0)=\\frac{\\exp(f_{\\theta}^{c}(\\mathbf{x})[0])}{Z},\\quad p_{c}(\\mathbf{x},1)=\\frac{\\exp(f_{\\theta}^{c}(\\mathbf{x})[1])}{Z}\\quad(Z\\mathrm{~is~the~normalisation~constant})\\quad\\forall\\theta\\in\\mathcal{K}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Next, we derive the approximated distribution $p_{c}(\\mathbf{x})$ by marginalisation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{c}(\\mathbf{x})=p_{c}(\\mathbf{x},0)+p_{c}(\\mathbf{x},1)}}\\\\ {{\\mathrm{}}}\\\\ {{\\qquad=\\frac{\\exp\\left(f_{\\theta}^{c}(\\mathbf{x})[0]\\right)+\\exp\\left(f_{\\theta}^{c}(\\mathbf{x})[1]\\right)}{Z}}}\\\\ {{\\mathrm{}}}\\\\ {{\\qquad=\\frac{\\exp\\left(\\log\\left(\\exp\\left(f_{\\theta}^{c}(\\mathbf{x})[0]\\right)+\\exp\\left(f_{\\theta}^{c}(\\mathbf{x})[1]\\right)\\right)\\right)}{Z}}}\\\\ {{\\rightarrow E_{c}(\\mathbf{x})=-\\log\\left(\\exp(f_{\\theta}^{c}(\\mathbf{x})[0])+\\exp(f_{\\theta}^{c}(\\mathbf{x})[1])\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the binary classifier $f_{\\theta}^{c}(\\cdot)$ in the surrogate binary classification, we use TabPFN [33], a pre-trained tabular in-context model. Note that TabPFN is intended for inference only, with no updates to its parameters (see Section 4 for more details about TabPFN). In this context, \u201ctraining\u201d the TabPFN classifier is analogous to the K-Nearest Neighbour algorithm, which simply performs inference based on a training dataset provided to the model. We apply TabPFN multiple times on separate datasets $\\{\\mathcal{D}_{1},\\mathcal{D}_{2},\\bar{.}\\cdot.\\cdot,\\mathcal{D}_{C}\\}$ to obtain multiple classifiers $\\{\\mathbf{\\dot{{f}}}_{\\theta}^{1},{f}_{\\theta}^{2},\\dots,{f}_{\\theta}^{C}\\}$ . In Section 3.4, we explore why reinterpreting TabPFN\u2019s logits, trained on our surrogate binary tasks, can be useful for estimating an energy function. We emphasise that TabEBM is a general method, capable of using any gradient-based classifier that computes logits (using Equation (3)), and is not limited to TabPFN. ", "page_idx": 3}, {"type": "text", "text": "Generating data with TabEBM involves two steps. First, we sample a class $c$ from the empirical distribution $c\\sim p(y)$ . Then, we sample a data point $\\mathbf{x}$ from the conditional distribution $\\mathbf{x}\\ \\stackrel{.}{\\sim}\\ p(\\mathbf{x}|y\\ =\\ c)$ approximated by the class-specific energy-based model $E_{c}(\\mathbf{x})$ , as outlined in Algorithm 1. We employ Stochastic Gradient Langevin Dynamics (SGLD) [84] to perform this sampling. SGLD is an efficient method for high-dimensional data, combining stochastic gradient descent (SGLD) with Langevin dynamics. The update rule for SGLD at each iteration is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\frac{\\eta}{2}\\nabla E(\\mathbf{x}_{t})+\\epsilon_{t},\\quad\\epsilon_{t}\\sim\\mathcal{N}(0,\\eta\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where a Gaussian noise term $\\epsilon_{t}$ introduces randomness into the sampling process, enhancing the exploration of the distribution. In practice, the step size and the noise standard deviations are often chosen separately, resulting in a biased sampler that allows for faster training. Appendix D.2 further shows that TabEBM is stable to hyperparameters for the sampling process. ", "page_idx": 3}, {"type": "text", "text": "In our method, SGLD performs iterative augmentation. We start by sampling close to real data and iteratively adjust these synthetic data points, steering them towards regions of higher probability under the learned energy model. TabEBM enables sampling from any specified class distribution, including the original class distribution, which is crucial for data augmentation. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 TabEBM sampling from Class-Specific EBM $E_{c}(\\mathbf{x})$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Input: Training data $\\mathcal{X}_{c}$ for class $c$ , step size $\\alpha_{\\mathrm{step}}$ , noise scale $\\alpha_{\\mathrm{noise}}$ , initial perturbation $\\sigma_{\\mathrm{start}}$ , number of steps $\\overline{T}$ Output: Set of synthetic samples for class $c$ ", "page_idx": 3}, {"type": "text", "text": "Initialise a surrogate binary classification task and train the model   \n1: Assign new labels to the samples $\\scriptstyle{\\mathcal{X}}_{c}$ from class $c$ , setting them to class 1   \n2: Generate a set of surrogate negative samples $\\mathcal{X}_{c}^{\\mathrm{neg}}$ and assign them class 0 labels   \n3: Train a binary classifier $f_{\\theta}^{c}$ on the dataset $\\mathcal{D}_{c}=(\\mathcal{X}_{c}\\cup\\mathcal{X}_{c}^{\\mathrm{neg}},\\{1\\}^{|\\mathcal{X}_{c}|}\\cup\\{0\\}^{|\\mathcal{X}_{c}^{\\mathrm{neg}}|})$   \nSynthesise samples using Stochastic Gradient Langevin Dynamics (SGLD)   \n4: Initialise synthetic data points $\\mathbf{x}_{0}^{\\mathrm{synth}}$ by sampling from $\\mathcal{N}(\\ensuremath{\\boldsymbol{\\chi}}_{c},\\ensuremath{\\boldsymbol\\sigma}_{\\mathrm{start}}^{2}\\mathbf{I})$   \n5: for each iteration $t=0,1,\\ldots,T-1$ do   \n6: $\\begin{array}{r l}&{E_{c}({\\bf x}_{t}^{\\mathrm{synth}})=-\\log\\Big(\\!\\exp(f_{\\theta}^{c}({\\bf x}_{t}^{\\mathrm{synth}})[0])+\\exp(f_{\\theta}^{c}({\\bf x}_{t}^{\\mathrm{synth}})[1])\\Big)}\\\\ &{{\\bf x}_{t+1}^{\\mathrm{synth}}={\\bf x}_{t}^{\\mathrm{synth}}-\\alpha_{\\mathrm{step}}\\nabla E_{c}({\\bf x}_{t}^{\\mathrm{synth}})+\\mathcal{N}(0,\\alpha_{\\mathrm{noise}}^{2}{\\bf I})}\\end{array}$   \n7:   \n8: end for   \n9. return ${\\bf x}_{T}^{\\mathrm{synth}}$ as the gen edsyntheticdataforclass ", "page_idx": 3}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate TabEBM by focusing on four research questions: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Data Augmentation Improvement (Q1, Section 3.1): Can TabEBM generate synthetic data that improves the accuracy of downstream predictors via data augmentation? \u2022 Statistical Fidelity (Q2, Section 3.2): Can TabEBM generate synthetic data with high statistical fidelity (i.e., with similar distributions to those of real data)? \u2022 Privacy Preservation (Q3, Section 3.3): Can TabEBM generate synthetic data that finds a competitive trade-off between downstream performance and privacy preservation? \u2022 Understanding TabEBM\u2019s energy formulation (Q4, Section 3.4): Why is TabEBM\u2019s classspecific energy effective, and how do the proposed surrogate tasks influence this? ", "page_idx": 4}, {"type": "text", "text": "Datasets. We utilise eight open-source tabular datasets from OpenML [7] across five domains: Medicine, Chemistry, Engineering, Language and Economics. As TabPFN utilises many small-size OpenML datasets in its meta-validation [33], it can lead to data leakage when evaluating TabEBM. Therefore, to provide fair comparisons, we select six additional leakage-free datasets from UCI [22]. These diverse datasets contain 7 to 77 features and 698 to 5500 samples across 2 to 26 classes. Five datasets contain both numerical and categorical features, while the remaining are numerical only. We further enlarge the evaluation scope by varying the degrees of data availability (i.e., $N_{\\mathrm{real.}}$ ), leading up to 33 different test cases for the eight OpenML datasets. Appendix B.1 provides detailed descriptions. ", "page_idx": 4}, {"type": "text", "text": "Benchmark generators. We compare TabEBM against eight existing tabular data generation methods of eight different categories: (i) a standard interpolation method SMOTE [13]; (ii) a Variational Autoencoders (VAE) based method TVAE [85]; (iii) a Generative Adversarial Networks (GAN) method CTGAN [85]; (iv) a normalising flow model Neural Spine Flows (NFLOW) [24]; (v) a diffusion model TabDDPM [42]; (vi) a tree-based method Adversarial Random Forests (ARF) [83]; (vii) a Graph Neural Network (GNN) based method GOGGLE [47]; and (viii) a Prior-Data Fitted Networks (PFN) based method TabPFGen [48]. Furthermore, we also include a \u201cBaseline\u201d model, where no data augmentation is applied (i.e., only real data is used to train downstream predictors). In Appendix B.6, we detail the settings used for TabEBM and all other generators. ", "page_idx": 4}, {"type": "text", "text": "Downstream predictors. We select six representative downstream predictors, including three standard baselines: Logistic Regression (LR) [16], KNN [27] and MLP [28]; two tree-based methods: Random Forest (RF) [10] and XGBoost [14]; and a PFN method: TabPFN [33]. ", "page_idx": 4}, {"type": "text", "text": "General experimental setup. For each dataset of $N$ samples, we first split it into stratified train and test sets. We create large test sets to reduce the likelihood that the model\u2019s performance is accidentally inflated due to a small, unrepresentative set of samples [69], and thus the test size is computed via $\\begin{array}{r}{N_{\\mathrm{test}}=\\operatorname*{min}\\left(\\frac{N}{2},500\\right)}\\end{array}$ . The full train set approximates the upper bound of the quality of synthetic data, and we call this set \u201coracle\u201d. We subsample the full train set to simulate different levels of data availability, thus the subset size $N_{\\mathrm{real}}$ varies over $\\{20,50,100,200,500\\}$ . We split each subset into stratified training and validation sets with a ratio of 4:1. We provide detailed descriptions of data splitting in Appendix B.2 and preprocessing in Appendix B.3. We repeat the splitting ten times, summing up to 10 runs per subset size. The reported results are averaged by default over ten runs on the test sets. When aggregating results across datasets, we use the average distance to the minimum (ADTM) metric via affine renormalisation between the top-performing and worse-performing models [30, 54]. We provide the evaluation results averaged over six downstream predictors for a general conclusion, and the fine-grained numerical results for each predictor are in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "Data augmentation setup. Given $N_{\\mathrm{real}}$ real samples, we first train generators on the real training data and then generate $N_{\\mathrm{syn}}$ synthetic samples. For training the downstream predictors, we expand the real training split by adding the synthetic samples. The real validation data is used for early stopping, and the real test set is used for evaluating the predictor\u2019s performance. The optimal $N_{\\mathrm{syn}}$ remains an open problem for tabular data [51, 71, 31]. Prior works [47, 48] mainly use synthetic sets with equivalent sizes to the real sets (i.e., $N_{\\mathrm{real}}=N_{\\mathrm{syn}},$ ). However, we observe that $N_{\\mathrm{real}}=N_{\\mathrm{syn}}$ can lead to highly unstable results, especially on small datasets that we investigate. Recent work has used different $N_{\\mathrm{syn}}$ for various generators, such as by applying post-processing [31, 71]. In this work, we want to provide a head-to-head comparison of the effect of data augmentation across subsampled datasets of varying sizes $N_{\\mathrm{real}}\\in\\{20,50,100,200,500\\}$ . Therefore, we perform data augmentation with a large synthetic set ( $\\mathrm{\\Delta}N_{\\mathrm{syn}}=500)$ ) across all splits, and the synthetic data has the same class distribution as the real training data. We provide an illustrative figure of the data splitting setup in Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Classification accuracy $(\\%)$ aggregated over six downstream predictors, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean balanced accuracy of other methods. We bold the highest accuracy for each dataset of different sample size. Our method, TabEBM, consistently outperforms training on real data alone, and achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 5}, {"type": "table", "img_path": "FmNoFIImZG/tmp/b6c6a28fc52f7b35e41851e5eb1fc197fef8ace49d9e0256168b83dd41b64cf0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.1 Data Augmentation Improvement (Q1) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate the effect of using synthetic data for data augmentation by comparing the balanced accuracy of downstream predictors before and after augmentation. Typically, higher classification accuracy (i.e., ACCGenerator) and accuracy improvements (i.e., ACCGenerator \u2212ACCBaseline $>\\mathrm{~0~}$ ) demonstrate the effectiveness of the synthetic data for data augmentation. ", "page_idx": 5}, {"type": "text", "text": "TabEBM effectively improves downstream performance across sample sizes, especially for very low-sample-size regimes. Table 1 and Figure 4 (Left) show that TabEBM exhibits competitive performance in data augmentation, generally achieving the highest downstream accuracy and average rank across most datasets and sample sizes. Notably, TabEBM is the only generator that consistently improves performance across sample sizes. A key observation is that most modern benchmark generators underperform even the Baseline, indicating poor approximated distributions in the low-sample-size regime. Moreover, TabEBM achieves the largest overall performance improvement on six leakage-free UCI datasets, further supporting its effectiveness (see Appendix D.5.2 for details). ", "page_idx": 5}, {"type": "text", "text": "Furthermore, TabEBM is the most widely applicable method among the top three competitive generators on the considered datasets: (i) SMOTE requires at least two samples per class for interpolation, and thus it is not applicable for some datasets, such as the \u201cprotein\u201d dataset $\\mathrm{\\Delta}N_{\\mathrm{real}}=20\\$ ); (ii) TabPFGen cannot scale up to more than ten classes, such as the \u201ccollins\u201d dataset. In addition, TabEBM can stabilise downstream performance, especially when real data is very scarce $\\mathrm{\\DeltaN_{real}=20}$ ): TabEBM leads to smaller standard deviations than Baseline on seven out of eight datasets. ", "page_idx": 5}, {"type": "text", "text": "TabEBM effectively improves downstream performance across any number of classes, especially for more than ten classes. Figure 4 (Right) shows that TabEBM consistently outperforms the ", "page_idx": 5}, {"type": "image", "img_path": "FmNoFIImZG/tmp/5b68c7eb28fcbaf56d868d009d56b258c28659cda02c58d9c2a5bf06f7f1ffec.jpg", "img_caption": ["Figure 4: Mean normalised balanced accuracy improvement $(\\%)$ across different sample sizes (Left) and across datasets with varying numbers of classes (Right). Because TabPFGen is not applicable for datasets with more than ten classes, we plot short bars at zeros for visual clearance. Positive values indicate that the generator improves downstream classification performance. TabEBM generally outperforms benchmark generators across varying sample sizes and number of classes. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Baseline with notable improvements, particularly in datasets with more than ten classes. In contrast, an increased number of classes tends to cause a performance degradation in the benchmark generators. ", "page_idx": 6}, {"type": "text", "text": "TabEBM is robust on imbalanced datasets. For the three binary OpenML datasets (i.e., \u201cbiodeg\u201d, \u201csteel\u201d and \u201cstock\u201d), we adjust the class distribution in the training data to vary the class imbalance, while keeping the test data fixed. Figure 5 shows that TabEBM consistently outperforms Baseline, while the other generators exhibit performance degradation as data imbalance increases. ", "page_idx": 6}, {"type": "text", "text": "TabEBM is computationally efficient. Figure 6 shows the trade-off between accuracy and the time needed for generating stratified synthetic data (for data augmentation). We measure the total duration of (i) training the model and (ii) generating 500 synthetic samples. The results show that TabEBM is practical, as it achieves higher downstream accuracy with lower time costs. ", "page_idx": 6}, {"type": "image", "img_path": "FmNoFIImZG/tmp/13bddfbe5b3862ad15182c69b6d3b27a70e6dd2c0dd1f37752bf2cf2de5f853c.jpg", "img_caption": ["Figure 5: Mean normalised balanced accuracy improvement $(\\%)$ on imbalanced datasets. TabEBM consistently outperforms the Baseline and other generators across different levels of data imbalance. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "FmNoFIImZG/tmp/c38257ed453cfc6eb9468435e3f4779e92a53fb22c8d085c68393166706b2e34.jpg", "img_caption": ["Figure 6: Median data augmentation time vs. mean normalised balanced accuracy. TabEBM achieves higher downstream accuracy while typically operating 3-30 times faster than most other methods. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.2 Statistical Fidelity (Q2) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the fidelity of synthetic data by measuring the similarity of synthetic data to real train data and to real test data (Figure 7). We evaluate this similarity via (i) average inverse of the Kullback\u2013Leibler Divergence (inverse KL) [17], (ii) p-value of Kolmogorov\u2013Smirnov test (KS test) [39] and (iii) p-value of Chi-squared test $(\\chi^{2}$ test) [55]. For full numerical results, including $\\chi^{2}$ test, see Appendix D.6. For all three metrics, a bigger value denotes that synthetic data is more likely to have the same distribution as real data. ", "page_idx": 6}, {"type": "text", "text": "In Figure 7 (a1&a2), TabEBM consistently exhibits the highest accuracy and distribution similarity between real train data and synthetic data, indicating that TabEBM learns the distributions of real train data better than benchmark generators. In Appendix D.6, we further show that TabEBM remains the most competitive method in similarity between real test data and synthetic data. This indicates that TabEBM can extrapolate beyond real train data and thus generate synthetic data from a more general distribution that aligns with both train and test data. This extrapolation ability also explains why TabEBM can outperform Baseline via data augmentation (Section 3.1). ", "page_idx": 6}, {"type": "table", "img_path": "FmNoFIImZG/tmp/3376c4335b4aa06b3a98a32a63400b502fee45d4105a99fcbe79a26c1d924e6a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.3 Privacy Preservation (Q3) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "More broadly, data privacy is a critical concern for organisations and governments handling sensitive data [75]. Privacy-preserving synthetic data allows researchers and practitioners to bypass ethical and logistical issues while enabling model training and testing [38]. We further explore the use of TabEBM-generated data for data sharing, where only synthetic data is accessible for downstream users [75, 89, 86, 23, 42]. In this case, downstream models are trained exclusively on synthetic data. ", "page_idx": 7}, {"type": "text", "text": "Specifically, we evaluate synthetic data via three metrics: (i) balanced accuracy of downstream predictors trained with only synthetic data (i.e., train-on-synthetic, test-on-real [85, 42, 87]); (ii) median Distance to Closest Record (DCR) [88], where a greater DCR denotes synthetic data is less likely to be copied from real data; and (iii) $\\delta$ -presense [62], where a smaller value denotes a lower re-identification risk for real data from synthetic data. Full numerical results are in Appendix D.7. ", "page_idx": 7}, {"type": "text", "text": "Figure 7 (b1&b2) shows that TabEBM consistently finds a better trade-off between accuracy and privacy preservation. Notably, the \u201ctrain-on-synthetic, test-on-real\u201d scenario poses a greater challenge for generators in achieving high accuracy because real data is inaccessible for model training and data augmentation. Despite this difficulty, TabEBM is the only generator that surpasses the overall performance of training on real data (i.e., Baseline). The relatively high DCR for TabEBM indicates that it can extrapolate beyond real train data, aligning with the finding that TabEBM\u2019s synthetic data is statistically similar to real test data (Section 3.2). These results further suggest that TabEBM learns the general distribution of real data, and can generate high-quality synthetic data suitable for various purposes, including privacy preservation. ", "page_idx": 7}, {"type": "image", "img_path": "FmNoFIImZG/tmp/1d394ffbb8136586bf5c57d6a6fdae023946d174abc044b3909475ae86071ac2.jpg", "img_caption": ["Figure 7: (a1&a2): Median inverse KL and KS test vs. mean normalised balanced accuracy improvement $(\\%)$ between real train data and synthetic data. (b1&b2): Median DCR and $\\delta^{.}$ -presence vs. mean normalised balanced accuracy change $(\\%)$ between real train data and synthetic data. Note that \u201caccuracy improvement\u201d is for data augmentation, and \u201caccuracy change\u201d is for data sharing. Complete results with standard deviations are in Appendix D.4. TabEBM generates high-fidelity synthetic data that can also be used for privacy preservation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "3.4 Why is TabEBM effective for estimating Energy-Based Models? (Q4) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Having established that TabEBM excels in data augmentation, we explore why classifier logits can be useful when reinterpreted as a class-conditional energy function. Figure 8 shows the logit distribution of TabPFN trained on surrogate binary tasks and the corresponding energy function of TabEBM (with TabPFN as the binary classifier) as the Euclidean distance from the real data increases. ", "page_idx": 7}, {"type": "text", "text": "We found it essential to place the negative samples far from the real data, since TabPFN, which is pre-trained to approximate Bayesian inference [33], has its confi", "page_idx": 7}, {"type": "text", "text": "Figure 8: (Left) Logit distribution of TabPFN trained on our surrogate binary tasks at increasing distances from the real data (on \u201csteel\u201d). (Right) The corresponding unnormalised density approximated by TabEBM. TabEBM assigns higher density closer to the real data. ", "page_idx": 7}, {"type": "text", "text": "dence influenced by the distance from the training data [53]. Figure 8 (left) shows that TabPFN outputs high logit values near the real data. As the distance from the real data increases, the logit $f(\\bar{\\bf x})$ [1] decreases smoothly until the two logits become similar, making the classifier uncertain (because the class probabilities become equal). Figure 8 (right) shows that TabEBM\u2019s inferred density drops significantly as the maximum logit decreases, because $\\begin{array}{r}{p_{c}(\\mathbf{x})\\propto(\\exp(f(\\mathbf{x})[0])+\\exp(f(\\mathbf{x})[1]))}\\end{array}$ from Equation (3). Since SGLD sampling performs gradient ascent on the density, the TabEBM-generated samples will be close to the real data. These findings are consistent across datasets (see Appendix D.3), where TabPFN\u2019s logits remain positive, with similar ranges and a relatively constant sum as distance increases, warranting further investigation. Overall, TabPFN\u2019s distance-based uncertainty is useful for inferring accurate energy functions within our TabEBM framework. Since TabEBM can be paired with any other gradient-based classifier that produces logits, we leave these extensions for future work. ", "page_idx": 8}, {"type": "text", "text": "4 Discussion & Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Section 3 showed that TabEBM efficiently generates high-fidelity data that can effectively improve the downstream performance via data augmentation. In Table 2, we further provide a summary of tabular data generative models analysed from three important perspectives: (i) Training: the type of distribution that the generators learn (crucial for preserving the original training label distribution), and the training costs associated with learning; (ii) Generation: do the generators employ class-specific models (reflecting their capability to capture unique features essential for label-invariant generation), and do models support stratified generation (crucial for effective data augmentation); (iii) Practicability: the scalability of the generators with respect to the number of classes (a common requirement in real-world multi-class tasks), and consistent downstream performance improvement across different class sizes. ", "page_idx": 8}, {"type": "text", "text": "Generative Models for Tabular Data. The common paradigm for tabular data generation is to adapt Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) [85, 63]. For instance, TableGAN employs a convolutional neural network to optimise the label quality [63], and TVAE is introduced in [85] as a variant of VAE for tabular data. However, these methods learn the joint distribution and thus cannot preserve the stratification of the original data (Appendix C). CTGAN [85] refines the generation to be class-conditional. The recent ARF [83] is an adversarial variant of random forest for density estimation, and GOGGLE [47] enhances VAE by learning relational structure with a Graph Neural Network (GNN). Some recent work focuses on generation with denoising diffusion models [42, 87, 40, 44]. For instance, TabDDPM [42] demonstrates that diffusion models can approximate typical distributions of tabular data. Although these class-conditional models can preserve the label distribution, they struggle to outperform Baseline and standard SMOTE in data augmentation [71, 48]. ", "page_idx": 8}, {"type": "text", "text": "We attribute the performance degradation in current class-conditional models to their reliance on a single shared model to approximate all class-conditional densities. For instance, another promising generative approach uses pre-trained models like Prior-Data Fitted Networks (PFNs), and the recent TabPFGen [48] adapts such models into one shared class-conditional generator. However, TabPFGen\u2019s shared generator can lead to inaccurate density estimates, particularly in high-noise and classimbalance situations (see examples in Appendix C). As noise increases, TabPFGen\u2019s inferred densities fluctuate significantly and diverge from the true data distributions. In contrast, TabEBM uses classspecific EBMs to model each class\u2019s marginal distributions, and the results in Appendix C reveal that our design choice reduces the impact of noise and data imbalance. TabEBM focuses on approximating and generating for one class at a time, remaining unaffected by noise from other classes. Overall, our results demonstrate that TabEBM consistently improves performance across different datasets and sample sizes, outperforming TabPFGen. Moreover, TabPFGen is limited in usability (e.g., it supports only up to ten classes), while TabEBM scales to any number of classes. ", "page_idx": 8}, {"type": "text", "text": "In a broader context, some recent work attempts to adapt Large Language Models (LLMs) for tabular data generation [25, 71, 9]. However, data contamination is an inherent issue with such LLM-based models [19, 36, 18, 49]. As the pre-training data is not typically open-source, these models can have unfair advantages in downstream tasks (i.e., the full real dataset, including the real test data, may have been used for pre-training). Therefore, in this paper, we focus on models without support from LLMs, thus avoiding potential biases from data contamination. ", "page_idx": 8}, {"type": "text", "text": "Data Augmentation (DA) for Tabular Data. DA is an omnipresent technique in computer vision and natural language processing [82, 73, 72, 60, 26, 2]. However, DA for tabular data remains underexplored, and existing methods often perform poorly in real-world tasks, sometimes even reduce performance [51]. Recent studies show that using the same transformations across all classes leads to varied performance impacts [3, 41], indicating that data augmentation effects are class-specific and suggesting that different classes may require distinct augmentations. Given the lack of symmetries in tabular data, we believe this class-dependent effect is even more pronounced. Therefore, we propose TabEBM as a class-specific generative model to produce tailored augmentations for each class. ", "page_idx": 8}, {"type": "table", "img_path": "FmNoFIImZG/tmp/6737940004345d1863cb3453a91ab7099fae81b2db63ffaa1f0940ba82ae2366.jpg", "table_caption": ["Table 2: Comparison of the properties between TabEBM and prior tabular generative methods. TabEBM has novel design rationales of training-free class-specific models, and TabEBM is highly practicable with wide applicability and consistent accuracy improvement. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Prior-fitted Networks (PFNs) for Tabular Data. Recent work proposes to approximate the posterior predictive distribution with transformers [59, 33, 61, 79, 20]. PFNs can be adapted for various purposes by pre-training the transformer with corresponding \u201cprior data\u201d, and then it can make in-context predictions with unseen downstream data. For instance, TabPFN is a variant that is pre-trained on a prior designed for tabular data [33]. We note that prior data is different to synthetic data in this paper. Specifically, prior data refers to manually crafted fake data (e.g., $y=2x$ ) with no real-world semantics. In contrast, synthetic data from generators is expected to have the same semantics as real data. Inspired by TabPFN\u2019s success in small-size classification tasks, TabEBM converts TabPFN into multiple EBMs that learn the marginal distribution for each class. The training-free nature of TabPFN enables TabEBM to generate high-quality tabular data without introducing extra training costs. Additionally, our class-specific design lets TabEBM surpass TabPFN\u2019s limits and scale to more than ten classes. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. TabEBM is a general method that relies on an underlying binary classifier, and as such, its strengths and weaknesses are directly tied to this classifier. We used TabPFN because it is a well-established open-source pre-trained model for tabular data. Therefore, TabEBM inherits some of TabPFN\u2019s limitations, particularly in scaling to a larger number of features. TabEBM can handle datasets with over 1000 samples, overcoming TabPFN\u2019s limitation, as it processes one class at a time. In Appendix D.5.3, we show that TabEBM outperforms other generators on larger datasets, though the performance gains decrease as the sample size increases. Although we implement TabEBM with TabPFN in this paper, we stress that TabEBM is compatible with any classifier that can be adapted into EBMs, as described in Section 2. As foundational models for tabular data evolve [81], new models capable of handling more features and samples are expected. Integrating them into TabEBM will enhance its ability to manage high-dimensional datasets, increasing its versatility and utility. Finally, note that, generators that are limited in modelling multivariate distributions may still perform well on univariate fidelity metrics, which is a standard approach to evaluating such models. However, evaluating their ability to learn more complex, high-order, relationships between features remains an open research question [78], which we leave for future work. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced TabEBM, the first tabular data augmentation method that creates class-specific EBM generators, learning the marginal distribution for each class separately. We also provide the first comprehensive analysis of tabular data augmentation across various dataset sizes. Our results demonstrate that TabEBM improves downstream performance through data augmentation on realworld datasets, outperforming other benchmark generators. The statistical evaluation confirms that TabEBM generates high-fidelity synthetic data, particularly for small datasets. We release our method as an open-source library, allowing users to generate data immediately without additional training. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Francisco Vargas, Randall Balestriero, and Otilia Stretcu for their insightful discussions and valuable input early in the project. NS and MJ acknowledge the support of the U.S. Army Medical Research and Development Command of the Department of Defense; through the FY22 Breast Cancer Research Program of the Congressionally Directed Medical Research Programs, Clinical Research Extension Award GRANT13769713. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the Department of Defense. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hassane Alami, Lysanne Rivard, Pascale Lehoux, Steven J Hoffman, Stephanie Bernadette Mafalda Cadeddu, Mathilde Savoldelli, Mamane Abdoulaye Samri, Mohamed Ali Ag Ahmed, Richard Fleet, and Jean-Paul Fortin. Artificial intelligence in health care: laying the foundation for responsible, sustainable, and inclusive innovation in low-and middle-income countries. Globalization and Health, 16:1\u20136, 2020.   \n[2] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017.   \n[3] Randall Balestriero, Leon Bottou, and Yann LeCun. The effects of regularization and data augmentation are class dependent. Advances in Neural Information Processing Systems, 35:37878\u2013 37891, 2022.   \n[4] Ms Aayushi Bansal, Dr Rewa Sharma, and Dr Mamta Kathuria. A systematic review on data scarcity problem in deep learning: solution and applications. ACM Computing Surveys (CSUR), 54(10s):1\u201329, 2022.   \n[5] Andreas D Baxevanis, Gary D Bader, and David S Wishart. Bioinformatics. John Wiley & Sons, 2020.   \n[6] Anton Bespalov, Thomas Steckler, Bruce Altevogt, Elena Koustova, Phil Skolnick, Daniel Deaver, Mark J Millan, Jesper F Bastlund, Dario Doller, Jeffrey Witkin, et al. Failed trials for central nervous system disorders do not necessarily invalidate preclinical models and drug targets. Nature Reviews Drug Discovery, 15(7):516\u2013516, 2016.   \n[7] B. Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael Gomes Mantovani, Jan N. van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS 2021), 2021.   \n[8] Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[9] Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. In The Eleventh International Conference on Learning Representations, 2022.   \n[10] Leo Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001.   \n[11] Chenjing Cai, Shiwei Wang, Youjun Xu, Weilin Zhang, Ke Tang, Qi Ouyang, Luhua Lai, and Jianfeng Pei. Transfer learning for drug discovery. Journal of Medicinal Chemistry, 63(16):8683\u20138694, 2020.   \n[12] Rees Chang, Yu-Xiong Wang, and Elif Ertekin. Towards overcoming data scarcity in materials science: unifying models and datasets with a mixture of experts framework. npj Computational Materials, 8(1):242, 2022.   \n[13] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321\u2013 357, 2002.   \n[14] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.   \n[15] Tadeusz Ciecierski-Holmes, Ritvij Singh, Miriam Axt, Stephan Brenner, and Sandra Barteit. Artificial intelligence for strengthening healthcare systems in low-and middle-income countries: a systematic scoping review. npj Digital Medicine, 5(1):162, 2022.   \n[16] David R Cox. The regression analysis of binary sequences. Journal of the Royal Statistical Society Series B: Statistical Methodology, 20(2):215\u2013232, 1958.   \n[17] Imre Csisz\u00e1r. I-divergence geometry of probability distributions and minimization problems. The annals of probability, pages 146\u2013158, 1975.   \n[18] Jasper Dekoninck, Mark Niklas M\u00fcller, Maximilian Baader, Marc Fischer, and Martin Vechev. Evading data contamination detection for language models is (too) easy. arXiv preprint arXiv:2402.02823, 2024.   \n[19] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark B. Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. In North American Chapter of the Association for Computational Linguistics, 2024.   \n[20] Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha V Naidu, and Colin White. Forecastpfn: Synthetically-trained zero-shot forecasting. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Georgios Douzas and Fernando Bacao. Effective data generation for imbalanced learning using conditional generative adversarial networks. Expert Systems with applications, 91:464\u2013471, 2018.   \n[22] Dheeru Dua and Casey Graff. Uci machine learning repository, 2017.   \n[23] Larry A Dunning and Ray Kresman. Privacy preserving data sharing with anonymous id assignment. IEEE transactions on information forensics and security, 8(2):402\u2013413, 2012.   \n[24] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Advances in neural information processing systems, 32, 2019.   \n[25] Xi Fang, Weijie Xu, Fiona Anting Tan, Ziqing Hu, Jiani Zhang, Yanjun Qi, Srinivasan H. Sengamedu, and Christos Faloutsos. Large language models (llms) on tabular data: Prediction, generation, and understanding - a survey. Transactions on Machine Learning Research, 2024.   \n[26] Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. A survey of data augmentation approaches for nlp. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968\u2013988, 2021.   \n[27] Evelyn Fix. Discriminatory analysis: nonparametric discrimination, consistency properties, volume 1. USAF school of Aviation Medicine, 1985.   \n[28] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34:18932\u2013 18943, 2021.   \n[29] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In International Conference on Learning Representations, 2019.   \n[30] L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in neural information processing systems, 35:507\u2013520, 2022.   \n[31] Lasse Hansen, Nabeel Seedat, Mihaela van der Schaar, and Andrija Petrovic. Reimagining synthetic tabular data generation through data-centric ai: A comprehensive benchmark. Advances in Neural Information Processing Systems, 36:33781\u201333823, 2023.   \n[32] Mikel Hernandez, Gorka Epelde, Ane Alberdi, Rodrigo Cilla, and Debbie Rankin. Synthetic data generation for tabular health records: A systematic review. Neurocomputing, 493:28\u201345, 2022.   \n[33] Noah Hollmann, Samuel M\u00fcller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In The Eleventh International Conference on Learning Representations, 2023.   \n[34] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3):90\u201395, 2007.   \n[35] Dipendra Jha, Kamal Choudhary, Francesca Tavazza, Wei-keng Liao, Alok Choudhary, Carelyn Campbell, and Ankit Agrawal. Enhancing materials property prediction by leveraging computational and experimental data using deep transfer learning. Nature communications, 10(1):5316, 2019.   \n[36] Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. Investigating data contamination for pre-training language models. arXiv preprint arXiv:2401.06059, 2024.   \n[37] Xiangjian Jiang, Andrei Margeloiu, Nikola Simidjievski, and Mateja Jamnik. Protogate: Prototype-based neural networks with global-to-local feature selection for tabular biomedical data. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024.   \n[38] Hao Jin, Yan Luo, Peilong Li, and Jomol Mathew. A review of secure and privacy-preserving medical data sharing. IEEE access, 7:61656\u201361669, 2019.   \n[39] Marvin Karson. Handbook of methods of applied statistics. volume i: Techniques of computation descriptive methods, and statistical inference. volume ii: Planning of surveys and experiments. im chakravarti, rg laha, and j. roy, new york, john wiley; 1967., 1968.   \n[40] Jayoung Kim, Chaejeong Lee, and Noseong Park. Stasy: Score-based tabular data synthesis. In The Eleventh International Conference on Learning Representations, 2022.   \n[41] Polina Kirichenko, Mark Ibrahim, Randall Balestriero, Diane Bouchacourt, Shanmukha Ramakrishna Vedantam, Hamed Firooz, and Andrew G Wilson. Understanding the detrimental class-level effects of data augmentation. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular data with diffusion models. In International Conference on Machine Learning, pages 17564\u201317579. PMLR, 2023.   \n[43] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energybased learning. Predicting structured data, 1(0), 2006.   \n[44] Chaejeong Lee, Jayoung Kim, and Noseong Park. Codi: Co-evolving contrastive diffusion models for mixed-type tabular synthesis. In International Conference on Machine Learning, pages 18940\u201318956. PMLR, 2023.   \n[45] Guillaume Lema\u00eetre, Fernando Nogueira, and Christos K. Aridas. Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning. Journal of Machine Learning Research, 18(17):1\u20135, 2017.   \n[46] Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabular models. In The Eleventh International Conference on Learning Representations, 2022.   \n[47] Tennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. Goggle: Generative modelling for tabular data by learning relational structure. In The Eleventh International Conference on Learning Representations, 2023.   \n[48] Junwei Ma, Apoorv Dankar, George Stein, Guangwei Yu, and Anthony Caterini. Tabpfgen\u2013 tabular data generation with tabpfn. In NeurIPS 2023 Second Table Representation Learning Workshop, 2023.   \n[49] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157\u2013165, 2022.   \n[50] Bradley A Malin, Khaled El Emam, and Christine M O\u2019Keefe. Biomedical data privacy: problems, perspectives, and recent advances. Journal of the American medical informatics association, 20(1):2\u20136, 2013.   \n[51] Dionysis Manousakas and Serg\u00fcl Ayd\u00f6re. On the usefulness of synthetic tabular data generation. In Data-centric Machine Learning Research (DMLR) Workshop at the 40th International Conference on Machine Learning (ICML), 2023.   \n[52] Andrei Margeloiu, Nikola Simidjievski, Pietro Lio, and Mateja Jamnik. Weight predictor network with feature selection for small sample tabular biomedical data. AAAI Conference on Artificial Intelligence, 2023.   \n[53] Calvin McCarter. What exactly has tabpfn learned to do? In ICLR Blogposts 2024, 2024.   \n[54] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? Advances in Neural Information Processing Systems, 36, 2024.   \n[55] Mary L McHugh. The chi-square test of independence. Biochemia medica, 23(2):143\u2013149, 2013.   \n[56] Daniele Micci-Barreca. A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems. ACM SIGKDD explorations newsletter, 3(1):27\u201332, 2001.   \n[57] Daniel J Mollura, Melissa P Culp, Erica Pollack, Gillian Battino, John R Scheel, Victoria L Mango, Ameena Elahi, Alan Schweitzer, and Farouk Dako. Artificial intelligence in low-and middle-income countries: innovating global health radiology. Radiology, 297(3):513\u2013520, 2020.   \n[58] LaRonda L Morford, Christopher J Bowman, Diann L Blanset, Ingrid B B\u00f8gh, Gary J Chellman, Wendy G Halpern, Gerhard F Weinbauer, and Timothy P Coogan. Preclinical safety evaluations supporting pediatric drug development with biopharmaceuticals: strategy, challenges, current practices. Birth Defects Research Part B: Developmental and Reproductive Toxicology, 92(4):359\u2013380, 2011.   \n[59] Samuel M\u00fcller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In International Conference on Learning Representations, 2022.   \n[60] Alhassan Mumuni and Fuseini Mumuni. Data augmentation: A comprehensive survey of modern approaches. Array, 16:100258, 2022.   \n[61] Thomas Nagler. Statistical foundations of prior-data fitted networks. In International Conference on Machine Learning, pages 25660\u201325676. PMLR, 2023.   \n[62] Mehmet Ercan Nergiz, Maurizio Atzori, and Christopher W Clifton. $\\delta$ -presence. Encyclopedia of Cryptography, Security and Privacy, pages 1\u20135, 2019.   \n[63] Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim. Data synthesis based on generative adversarial networks. Proceedings of the VLDB Endowment, 11(10), 2018.   \n[64] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.   \n[65] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[66] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018.   \n[67] Zhaozhi Qian, Rob Davis, and Mihaela van der Schaar. Synthcity: a benchmark framework for diverse use cases of tabular synthetic data. Advances in Neural Information Processing Systems, 36, 2024.   \n[68] Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18434\u201318443, 2023.   \n[69] Sebastian Raschka. Model evaluation, model selection, and algorithm selection in machine learning. arXiv preprint arXiv:1811.12808, 2018.   \n[70] Vignesh Sampath, I\u00f1aki Maurtua, Juan Jose Aguilar Martin, and Aitor Gutierrez. A survey on generative adversarial networks for imbalance problems in computer vision tasks. Journal of big Data, 8:1\u201359, 2021.   \n[71] Nabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated llm: Synergy of llms and data curation for tabular augmentation in low-data regimes. In Forty-first International Conference on Machine Learning, 2024.   \n[72] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of big data, 6(1):1\u201348, 2019.   \n[73] Connor Shorten, Taghi M Khoshgoftaar, and Borko Furht. Text data augmentation for deep learning. Journal of big Data, 8(1):101, 2021.   \n[74] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion, 81:84\u201390, 2022.   \n[75] Theresa Stadler and Carmela Troncoso. Why the search for a privacy-preserving data sharing mechanism is failing. Nature Computational Science, 2(4):208\u2013210, 2022.   \n[76] Fahim Sufi. Addressing data scarcity in the medical domain: A gpt-based approach for synthetic data generation and feature extraction. Information, 15(5):264, 2024.   \n[77] Haoyuan Sun, Navid Azizan, Akash Srivastava, and Hao Wang. Private synthetic data meets ensemble learning. arXiv preprint arXiv:2310.09729, 2023.   \n[78] Ruibo Tu, Zineb Senane, Lele Cao, Cheng Zhang, Hedvig Kjellstr\u00f6m, and Gustav Eje Henter. Causality for tabular data synthesis: A high-order structure causal benchmark framework. arXiv preprint arXiv:2406.08311, 2024.   \n[79] Jordan Ubbens, Ian Stavness, and Andrew G Sharpe. Gpfn: Prior-data fitted networks for genomic prediction. bioRxiv, pages 2023\u201309, 2023.   \n[80] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. Synthetic data, real errors: how (not) to publish and use synthetic data. In International Conference on Machine Learning, pages 34793\u201334808. PMLR, 2023.   \n[81] Boris van Breugel and Mihaela van der Schaar. Why tabular foundation models should be a research priority. In Forty-first International Conference on Machine Learning, 2024.   \n[82] David A Van Dyk and Xiao-Li Meng. The art of data augmentation. Journal of Computational and Graphical Statistics, 10(1):1\u201350, 2001.   \n[83] David S Watson, Kristin Blesch, Jan Kapar, and Marvin N Wright. Adversarial random forests for density estimation and generative modeling. In International Conference on Artificial Intelligence and Statistics, pages 5357\u20135375. PMLR, 2023.   \n[84] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681\u2013688. Citeseer, 2011.   \n[85] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. Advances in neural information processing systems, 32, 2019.   \n[86] Aiqing Zhang and Xiaodong Lin. Towards secure and privacy-preserving data sharing in e-health systems via consortium blockchain. Journal of medical systems, 42(8):140, 2018.   \n[87] Hengrui Zhang, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Xiao Qin, Christos Faloutsos, Huzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with score-based diffusion in latent space. In The Twelfth International Conference on Learning Representations, 2023.   \n[88] Zilong Zhao, Aditya Kunar, Robert Birke, and Lydia Y Chen. Ctab-gan: Effective table data synthesizing. In Asian Conference on Machine Learning, pages 97\u2013112. PMLR, 2021.   \n[89] Xu Zheng and Zhipeng Cai. Privacy-preserved data sharing towards multiple parties in industrial iots. IEEE Journal on Selected Areas in Communications, 38(5):968\u2013979, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Broader Impact Statement 18 ", "page_idx": 16}, {"type": "text", "text": "B Reproducibility 18 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Datasets 18   \nB.2 Data Splitting 18   \nB.3 Data Preprocessing . . 19   \nB.4 Software and Computing Resources . . 19   \nB.5 TabEBM open-source library . . . 20   \nB.6 Implementation of Generators . . 20   \nB.7 Implementation of Downstream Predictors 20 ", "page_idx": 16}, {"type": "text", "text": "C Limitations of Existing Generative Methods 21 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D Extended Experimental Results 24 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": ".1 Ablations on the distribution of the surrogate negative samples . . 24   \nD.1.1 Ablations on placing the negative samples 24   \nD.1.2 Varying the number of negative samples . . 24   \nD.1.3 Varying the distance of the negative samples . . . . 25   \nD.2 Ablations on the sensitivity to the hyperparameters of SGLD sampling . . 25   \nD.3 Distribution of Logits and Unnormalized Density in TabEBM 26   \nD.4 Complete Trade-off Figures with Error Bars 27   \nD.5 Results on Data Augmentation . . . 28   \nD.5.1 Results on eight OpenML datasets. . . . 28   \nD.5.2 Results on six UCI Datasets . . . . . 34   \nD.5.3 Results on larger sample sizes . . . . 34   \nD.6 Results on Statistical Fidelity . . . . 34   \nD.6.1 Similarity between Real Train Data and Synthetic Data 35   \nD.6.2 Similarity between Real Test Data and Synthetic Data 38   \nD.7 Results on Privacy Preservation . . . . . . 41   \nD.7.1 Downstream Accuracy in Data Sharing . . . . 41   \nD.7.2 DCR Evaluation . . . 48   \nD.7.3 Delta-presence Evaluation . . . 49 ", "page_idx": 16}, {"type": "text", "text": "A Broader Impact Statement ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This paper introduces a novel data augmentation approach, TabEBM, that aims to advance the field of machine learning by addressing challenges in the low-sample-size regime. Furthermore, TabEBM offers an elegant solution to learning the unique features in generating samples for each class, leading to high-fidelity synthetic data that can effectively improve downstream performance. These characteristics can be particularly useful in data-scarce domains like healthcare (e.g., pre-clinical drug evaluation in early-stage clinical trials [6, 58]). Moreover, we also demonstrate that TabEBM is readily applicable for privacy-preserving data sharing in high-stake tasks [89, 77]. ", "page_idx": 17}, {"type": "text", "text": "TabEBM\u2019s impact further extends to enabling broader machine learning applications in data-scarce domains, for instance, facilitating data analysis in clinical scenarios with limited access to data collection techniques. Improving the performance of machine learning models in such applications can further foster the uptake of more sophisticated ML approaches and, ultimately, help improve the quality of healthcare [1, 15, 57]. TabEBM can further facilitate research and enhance machine learning accessibility in various communities across societal and scientific domains. To this end, our work has only been evaluated in a strictly research setting. Further applications of our work in scenarios with sensitive data bear some risks. As TabEBM is a generative model, training models with the resulting generated samples can bias the downstream model. Therefore, this risk, together with other data privacy risks during downstream deployment, must be carefully managed. ", "page_idx": 17}, {"type": "text", "text": "B Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All eight datasets are publicly available on OpenML [7], and their details are listed in Table 3. To ensure consistent stratified data-splitting across all datasets, we remove classes with fewer than 10 samples. For example, the original \u201cenergy\u201d dataset contains 14 classes with fewer than 10 samples, which could result in a validation set lacking samples from these classes, leading to unstratified data splitting. ", "page_idx": 17}, {"type": "table", "img_path": "FmNoFIImZG/tmp/c1abf656c54a397478f9f50b921b4e757b1fa543574cd22ea0f4e9a0d827eb03.jpg", "table_caption": ["Table 3: Details of the eight real-world tabular datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Data Splitting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 9 shows the data splitting setup used across all datasets. Note that data sharing (Section 3.3) shares the same data splitting as data augmentation, except that the \u201cTraining set\u201d and \u201cValidation set\u201d containing real data are no longer used for training the downstream predictors. ", "page_idx": 17}, {"type": "image", "img_path": "FmNoFIImZG/tmp/60a5e840eb4f553af2752783d3105af3ba459d539453991195208546b6433126.jpg", "img_caption": ["Figure 9: Data splitting strategies for data augmentation for all datasets. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3 Data Preprocessing ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Following the procedures presented in prior work [54, 30], we perform preprocessing in two steps. We first compute the required statistics with training data and then transform it. Firstly, we impute the missing values with the mean value for numerical features and the most mode value for categorical features. Secondly, we convert the categorical features into numerical features equal to Leave-one-out Target Statistic [66, 56]. Next, we perform Z-score normalisation for each feature. Specifically, we compute each feature\u2019s mean and standard deviation in the training data and then transform the training samples to have a mean of zero and a variance of one for each feature. Finally, we apply the same transformation to the validation and test data before conducting evaluations. ", "page_idx": 18}, {"type": "text", "text": "B.4 Software and Computing Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Software implementation. (i) For generators: We implemented TabEBM using PyTorch 1.13 [64], an open-source deep learning library with a BSD licence. We implemented SMOTE with Imbalancedlearn [45], an open-source Python library for imbalanced datasets with an MIT licence. For other benchmark generators, we used their open-source implementations in Synthcity [67], a library for generating and evaluating synthetic tabular data with an Apache-2.0 license. (ii) For downstream predictors: We implemented TabPFN with its open-source implementation (https://github.com/ automl/TabPFN). We implemented the other five downstream predictors (i.e., Logistic Regression, KNN, MLP, Random Forest and XGBoost) with their open-source implementation in scikit-learn [65], an open-source Python library under the 3-Clause BSD license. (iii) For result analysis and visualisation: All numerical plots and graphics have been generated using Matplotlib 3.7 [34], a Pythonbased plotting library with a BSD licence. The model architecture was generated using draw.io (https://github.com/jgraph/drawio), a free drawing software under Apache License 2.0. ", "page_idx": 18}, {"type": "text", "text": "We ensure the consistency and reproducibility of experimental results by implementing a uniform pipeline using PyTorch Lightning, an open-source library under an Apache-2.0 licence. We further fixed the random seeds for data loading and evaluation throughout the training and evaluation process. This ensured that TabEBM and all benchmark models were trained and evaluated on the same set of samples. The experimental environment settings, including library dependencies, are specified in the open-source library for reference and reproduction purposes. ", "page_idx": 18}, {"type": "text", "text": "Computing Resources. We trained 140,000 models for evaluations (including over 35,000 of generators and over 10,500 for downstream predictors). All our experiments are run on a single machine from an internal cluster with a GPU Nvidia Quadro RTX 8000 with 48GB memory and an Intel(R) Xeon(R) Gold 5218 CPU with 16 cores (at 2.30GHz). The operating system was Ubuntu 20.4.4 LTS. ", "page_idx": 18}, {"type": "text", "text": "B.5 TabEBM open-source library ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We implemented TabEBM as an extensible library, and the code is available on https://github.com/andreimargeloiu/TabEBM. For practitioners, it offers an easy-to-use, domain-agnostic tool that requires no training, making it particularly suitable for data augmentation, especially in small datasets. For researchers, the library includes the complete implementation of TabEBM, facilitating future extensions and investigations into class-specific energy-based models. ", "page_idx": 19}, {"type": "text", "text": "The library has two core functionalities: ", "page_idx": 19}, {"type": "text", "text": "1. Generate synthetic data: The library can generate data for augmentation. ", "page_idx": 19}, {"type": "text", "text": "from tabebm.TabEBM import TabEBM ", "page_idx": 19}, {"type": "text", "text": "tabebm $=$ TabEBM() augmented_data $=$ tabebm.generate(X_train, y_train, num_samples $=\\!100$ ) $\\%$ augmented_data[class_id] $=$ numpy.ndarray of generated data % for a specific \u2019\u2019class_id\u2018\u2018 ", "page_idx": 19}, {"type": "text", "text": "2. Compute and visualise the energy function: The library allows computation of TabEBM\u2019s energy function and the unnormalised data density. The demo notebook, TabEBM_approximated_density.ipynb, shows the TabEBM-inferred densities under conditions of data noise and class imbalance (thus recreating the plots from Appendix C). ", "page_idx": 19}, {"type": "text", "text": "B.6 Implementation of Generators ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "TabEBM. In all our experiments, the surrogate binary classifier in TabEBM is a pretrained in-context model, TabPFN [33], using the official model weights released by the authors (https://github.com/automl/TabPFN/raw/main/tabpfn/models_diff/ prior_diff_real_checkpoint_n_0_epoch_42.cpkt). We use TabPFN with three ensembles. We use four surrogate negative samples, X cneg, positioned at \u03b1dniesgt = standard deviations from zero, in random corners of a hypercube in $\\mathbb{R}^{D}$ (as explained in Section 2.2), distant from any real data. In Appendix D.1, we show that TabEBM is robust to the distribution of the negative samples. ", "page_idx": 19}, {"type": "text", "text": "We use SGLD [84] for sampling from TabEBM, where the starting points $\\mathbf{x}_{\\mathrm{0}}^{\\mathrm{synth}}$ are initialised by adding Gaussian noise with zero mean and standard deviation $\\sigma_{\\mathrm{start}}~=~0.01$ to a randomly selected sample of the specific class, i.e., xs0ynth\u223cN(Xc, \u03c3s2tartI). For SGLD, we used the following parameters: step size $\\alpha_{\\mathrm{step}}=0.1$ , noise scale $\\alpha_{\\mathrm{noise}}=0.01$ and number of steps . We found TabEBM to be robust to the SGLD settings (see Appendix D.2). ", "page_idx": 19}, {"type": "text", "text": "TabPFGen. We re-implemented TabPFGen [48] by closely following the original paper since no official implementation is available. As recommended in [48], the starting points are initialised by adding Gaussian noise with zero mean and standard deviation of 0.01 to the training points. ", "page_idx": 19}, {"type": "text", "text": "SMOTE. We use the open-source implementation of SMOTE from Imbalanced-learn [45], and the number neighbours $k$ is set within the range of $\\{1,3,5\\}$ . When applicable, we always set the maximum value for nearest neighbours (i.e., $k=5$ ). However, very low-sample-size datasets may not contain sufficient samples for large $k$ . For instance, the \u201cfourier\u201d dataset $\\mathrm{\\DeltaN_{real}}=20\\$ ) only has two samples per class. We set $k=1$ to generate synthetic data with SMOTE in these cases. ", "page_idx": 19}, {"type": "text", "text": "For the other six benchmark generators, we use their open-source implementations in Synthcity [67].   \nFollowing prior studies [87, 80, 71, 48], we use the default settings for all generators. ", "page_idx": 19}, {"type": "text", "text": "B.7 Implementation of Downstream Predictors ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We implemented TabPFN with its official implementation [33] and the other five downstream predictors with the scikit-learn library [65]. Following prior studies [80, 71], we use the default settings for all downstream predictors. ", "page_idx": 19}, {"type": "text", "text": "C Limitations of Existing Generative Methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We showcase three limitations of current generative models: (1) Appendix C shows that models approximating the joint distribution $p(\\mathbf{x},y)$ may fail to preserve the stratification of the real data and even fail to generate samples from specific classes. (2) Appendix C evaluates the approximated class-conditional distributions $p(\\mathbf{x}\\mid\\,y)$ on data with increasing noise levels, and (3) Appendix C evaluates the approximated class-conditional distributions $p(\\mathbf{x}\\mid\\mathbf{\\mu}_{y})$ on data with increasing class imbalance. ", "page_idx": 20}, {"type": "image", "img_path": "FmNoFIImZG/tmp/49ca71f3b9ea9c04af5476f3fa8b5126eb44f8b9e360f9cfe5c203e0f9473cca.jpg", "img_caption": ["Figure 10: Comparison of class distribution between real data and synthetic data from TVAE. We first train TVAE on the \u201cenergy-efficiency\u201d dataset and then randomly generate 10,000 samples with it. We highlight the classes where no synthetic samples are generated. TVAE fails to generate samples for 4 of 23 classes, showing the impracticability to preserve stratification by generative methods that learn joint distribution $p(\\mathbf{x},y)$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "FmNoFIImZG/tmp/b931344392655a33902b316cad70369e111c65778a15ee4337ab97e72ab050a1.jpg", "img_caption": ["(e) Noise level 2 "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 11: Evaluating the approximated class-conditional distributions on data with increasing noise levels. Darker blue indicates a higher assigned probability. TabPFGen uses a single shared energybased model to infer the class-conditional distribution $p(\\mathbf{x}|y)$ . As noise increases, TabPFGen\u2019s probability assignments vary significantly and end up assigning very high probabilities that are far from the real data. For instance, the areas of assigned probability for $p(\\mathbf{x}|y=1)$ completely flip when noise increases from 0.5 to 1. In contrast, our TabEBM uses class-specific energy models, resulting in robust inferred conditionals. TabEBM performs well even under very high noise (see $p(\\mathbf{x}|y=0)$ for noise level 2), while TabPFGen struggles. ", "page_idx": 21}, {"type": "image", "img_path": "FmNoFIImZG/tmp/2b04c7ed2492c58326dd1ab7550bd155743109774c3eb386f04d726cd1b3626b.jpg", "img_caption": ["(d) Class ratio 10:290 "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 12: Evaluating the approximated class-conditional distributions on a toy dataset of 300 samples with varying class imbalances. The two clusters maintain their positions. Darker blue indicates a higher assigned probability. TabPFGen uses a single shared energy-based model to infer the class-conditional distribution $p(\\mathbf{x}|y)$ . As class imbalance increases, TabPFGen starts assigning high probability in areas far from the real data, for instance, in the case of $p(\\mathbf{x}|y=1)$ for class ratio 10:290. In contrast, our TabEBM fits class-specific energy models only on the class-wise data $\\mathcal{X}_{c}=\\{\\mathbf{x}^{(i)}\\mid y_{i}=c\\}$ . This results in very robust inferred conditional distributions even under heavy class imbalance (e.g., see that $p(\\mathbf{x}|y=1)$ remains relatively constant). ", "page_idx": 22}, {"type": "text", "text": "D Extended Experimental Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Ablations on the distribution of the surrogate negative samples ", "page_idx": 23}, {"type": "text", "text": "D.1.1 Ablations on placing the negative samples ", "page_idx": 23}, {"type": "image", "img_path": "FmNoFIImZG/tmp/d846aa97b067c8bd1a5997947dbedf821329f2cab11337f192cb9622b4ea52c9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 13: TabEBM energy $E_{c}(x)$ for different choices of negative samples. The blue region represents low energy, indicating high data density. In (A), TabEBM, with the proposed negative samples placed in a hypercube far from the data, infers an accurate energy surface, resulting in generated data close to the real points. In (B), labelling a random subset of the real data as negative samples leads to a completely inaccurate energy surface. In (C), labelling half of the real points as negative samples reduces density near the decision boundary, as TabPFN assigns low maximal logit due to the high uncertainty. In conclusion, placing negative samples far from the real data results in a robust energy surface. ", "page_idx": 23}, {"type": "text", "text": "Appendix D.1.1 shows TabEBM\u2019s energy $E_{c}(x)$ when varying the selection of the negative samples. TabEBM infers an accurate energy surface with distant negative samples, and the energy surface becomes inaccurate when negative samples resemble real samples. This occurs because TabPFN is uncertain when points of different classes are close, affecting its logits magnitude and making them unsuitable for density estimation. ", "page_idx": 23}, {"type": "text", "text": "D.1.2 Varying the number of negative samples ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We evaluate the impact of the ratio $|\\mathcal{X}_{c}^{\\mathrm{neg}}|:|\\mathcal{X}_{c}|$ between the negative samples $\\chi_{c}^{\\mathrm{neg}}$ and the real samples $|\\mathcal{X}_{c}|$ . We vary $\\left\\lceil\\mathcal{X}_{c}^{\\mathrm{neg}}\\right\\rceil$ while keeping $|\\mathcal{X}_{c}|$ fixed, simulating both balanced and highly imbalanced scenarios. The negative samples are placed in random corners of the hypercube (as described in Section 2), at five standard deviations in each direction (i.e., \u03b1dniesgt $\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}=5\\$ ). To ensure reliable outcomes, we maintained a consistent ratio across all classes, keeping the same proportion of negative samples for each class. ", "page_idx": 23}, {"type": "text", "text": "Table 4 shows the results across six datasets with $N_{\\mathrm{real}}\\,=\\,100$ real samples, demonstrating that TabEBM is robust to imbalances in the surrogate binary tasks. The column with $|\\mathcal{X}_{c}^{\\mathrm{neg}}|=4$ represents the TabEBM results from the main paper, where four negative samples were placed in the corners (as described in Section 2). There are negligible differences in performance, and TabEBM consistently outperforms both the baseline and other generators (as shown in Table 1). ", "page_idx": 23}, {"type": "table", "img_path": "FmNoFIImZG/tmp/b306c20e77502684920d1647d2cc13e91f6a9e02bd2664c030ed3d407ba28c2c.jpg", "table_caption": ["Table 4: Evaluating the impact of varying the ratio $|\\mathcal{X}_{c}^{\\mathrm{neg}}|:|\\mathcal{X}_{c}|$ . We show the test classification accuracy performance $(\\%)$ of TabEBM on data augmentation averaged over six datasets and ten repeats. TabEBM shows consistent performance and outperforms the baseline, regardless of the number of negative samples. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.1.3 Varying the distance of the negative samples ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We assess the effect of varying the distance of negative samples. We use TabEBM with four negative samples positioned randomly at the corners of the hypercube, as outlined in Section 2 (this corresponds to the experimental setup from the main paper). The distance of the negative samples, denoted as $\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}$ is varied. Table 5 demonstrates that TabEBM remains generally robust to changes in this distance, with only small performance variations across different datasets. Importantly, using TabEBM for data augmentation consistently improves performance by approximately $3\\%$ compared to the Baseline, regardless of the distance used. ", "page_idx": 24}, {"type": "table", "img_path": "FmNoFIImZG/tmp/82b4f0d9b4f9a8bf55ff659c0ce881bb59c09b161749b8fb207e393d621203fa.jpg", "table_caption": ["Table 5: Evaluating the impact of varying the distance of the negative samples $\\alpha_{\\mathrm{dist}}^{\\mathrm{neg}}$ across various datasets. We show the test classification accuracy performance $(\\%)$ of TabEBM on data augmentation averaged over six datasets and ten repeats. TabEBM is robust, and optional tuning of the negative samples could slightly improve performance. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.2 Ablations on the sensitivity to the hyperparameters of SGLD sampling ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We vary two key hyperparameters of SGLD on the \u201cbiodeg\u201d binary dataset with $N_{\\mathrm{real}}=100$ : the step size $\\alpha_{\\mathrm{step}}$ and the noise scale $\\alpha_{\\mathrm{noise}}$ . Table 6 shows that TabEBM remains stable with respect to these hyperparameters. Note that smaller values of $\\alpha_{\\mathrm{noise}}$ are expected to perform better because SGLD sampling adds noise at each iteration (see Line 7 in Algorithm 1), thus larger values of $\\alpha_{\\mathrm{noise}}$ will hinder convergence of the SGLD sampler. ", "page_idx": 24}, {"type": "text", "text": "Table 6: Test classification accuracy $(\\%)$ of TabEBM (averaged over six downstream predictors) with different SGLD settings. Increasing $\\alpha_{\\mathrm{noise}}$ (added at each SGLD step) is expected to degrade performance, as it causes the sampling to diverge further from the real data. ", "page_idx": 25}, {"type": "table", "img_path": "FmNoFIImZG/tmp/3819c77a0975f45aa01c10e98e24c9f5889b9b8b52c0bb61f507b8929aa9114b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.3 Distribution of Logits and Unnormalized Density in TabEBM ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "FmNoFIImZG/tmp/001e0605c7f4182579b46680a34ea3e184c90d4a0ad0e9bb84a5d1d5c2578303.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 14: Additional results for Section 3.4. The logit distribution of TabPFN trained on our surrogate binary tasks across four datasets. Starting from the real samples, random points are selected at increasing distances (shown on the ${\\bf X}$ -axis). The top row shows the logit distributions for the surrogate task. Close to the real data, TabPFN outputs a high logit value. As the distance increases, the logits converge due to increased predictive uncertainty, leading to equal class probabilities after applying softmax. Notably, across datasets, TabPFN\u2019s logits are always positive, have similar ranges, and maintain a relatively constant sum as distance increases. The bottom row TabEBM\u2019s unnormalized density, $p_{c}(x)\\,\\propto\\,\\exp(-E_{c}(x))\\,\\rightarrow\\,p_{c}(x)\\,\\propto\\,(\\exp(f(x)[0])+\\exp(f(x)[1]))$ . The density decreases significantly far from the data, becoming negligible. Because sampling using SGLD perform gradient ascent on the density, the TabEBM-generated samples will be similar when using one or both logits. ", "page_idx": 25}, {"type": "image", "img_path": "FmNoFIImZG/tmp/30a531f91a3fd9d40c9ee0658ff2b367b4e6b8fbc3a9525c2c1030a8b7a9a292.jpg", "img_caption": ["Figure 15: $(\\mathbf{a}1\\&2)$ : Median inverse $\\mathrm{KL}$ and KS test vs. mean normalised balanced accuracy improvement $(\\%)$ between real train data and synthetic data. (b1&b2): Median DCR and $\\delta-$ presence vs. mean normalised balanced accuracy change $(\\%)$ between real train data and synthetic data. Note that \u201caccuracy improvement\u201d is for data augmentation, and \u201caccuracy change\u201d is for data sharing. TabEBM generates high-fidelity synthetic data that can also be used for privacy preservation. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.5 Results on Data Augmentation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "D.5.1 Results on eight OpenML datasets. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Table 7: Classification accuracy $(\\%)$ of LR, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We bold the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 27}, {"type": "table", "img_path": "FmNoFIImZG/tmp/185c3a7a5e209d8fa7d2dd3c64ec3d0038cc417af3035509d744bc8b8b27c2c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 8: Classification accuracy $(\\%)$ of KNN, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We bold the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 28}, {"type": "table", "img_path": "FmNoFIImZG/tmp/7fd69592a56cae3ddf0a4f2f21107e132e68abdb66a8f86c54f16932fff68311.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 9: Classification accuracy $(\\%)$ of MLP, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We bold the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 29}, {"type": "table", "img_path": "FmNoFIImZG/tmp/f9f32db9e3e59cf372245e83d2bfff3d62cc90758a46b2d7fd465d7eaa74713c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 10: Classification accuracy $(\\%)$ of RF, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We bold the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 30}, {"type": "table", "img_path": "FmNoFIImZG/tmp/dfa87f731298fe2b60b7f0408b8d0e1eaeacf89776007030fa79078b85dd5cd4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 11: Classification accuracy $(\\%)$ of XGBoost, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We bold the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 31}, {"type": "table", "img_path": "FmNoFIImZG/tmp/57b0fce445dae641787f9bb531edf92a4c5c83284a9e023d9fb02f4f122ab380.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Table 12: Classification accuracy $(\\%)$ of TabPFN, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We bold the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 32}, {"type": "table", "img_path": "FmNoFIImZG/tmp/ce1683fd804f52a129f1f877b028c174eb2d11e65e1b9ca9d405af19bf231023.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "D.5.2 Results on six UCI Datasets ", "text_level": 1, "page_idx": 33}, {"type": "table", "img_path": "FmNoFIImZG/tmp/be4d2b6c4374fbf120006a1d72888f80f123968a0df4fee417bd3a1c014c18cb.jpg", "table_caption": ["Table 13: Details of the six real-world tabular datasets from UCI. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Table 14: Test classification accuracy $(\\%)$ aggregated over six downstream predictors, comparing data augmentation on six leakage-free UCI datasets. Note that \u201cN/A\u201d denotes that a specific generator was not applicable. TabEBM still achieves the best overall performance against benchmark methods. ", "page_idx": 33}, {"type": "table", "img_path": "FmNoFIImZG/tmp/e6e30330fd6996cc6852083425d9a2b87ede24b529963a84c11351fab053b0e1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "D.5.3 Results on larger sample sizes ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Table 15: Test classification accuracy $(\\%)$ aggregated over six downstream predictors, comparing data augmentation with increased real data availability of the \u201ctexture\u201d dataset. Note that \u201cN/A\u201d denotes that a specific generator was not applicable. On larger datasets, TabEBM still outperforms other generators, but training on real data alone appears sufficient. This highlights TabEBM\u2019s usefulness in fields with limited training samples. ", "page_idx": 33}, {"type": "table", "img_path": "FmNoFIImZG/tmp/6188661331e317b2bca4052bd743dbda1bde07ab96c601bbe520b623ef7dda59.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "D.6 Results on Statistical Fidelity ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We aim to provide a fair and coherent comparison between TabEBM and existing methods and thus we follow the widely adopted evaluation process in prior studies. Specifically, we compute the three statistical fidelity metrics with the open-source implementations from the well-established benchmark, Synthcity. We note that the previous studies [87, 67] often operate under the assumption that the issues associated with multiple comparisons are less pronounced in generating low-dimensional tabular data, hence correction methods for multiple hypothesis testing are seldom employed. Following such assumptions, correction methods are not employed in this work. In addition, we would like to point out the imperfection of widely adopted univariate metrics (i.e., Inverse KL, KS test and test) in existing work. However, evaluating generators\u2019 ability to capture the joining feature relationships remains an open research question [78]. We leave this for future work to explore. ", "page_idx": 33}, {"type": "text", "text": "D.6.1 Similarity between Real Train Data and Synthetic Data ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Table 16: Inverse KL between real train data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher fidelity. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators. ", "page_idx": 34}, {"type": "table", "img_path": "FmNoFIImZG/tmp/3415881092407d4a9f6532969dfde1e41a2ef56cd189829adcfbe8c353fa09d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 17: KS test between real train data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std result and average rank across datasets. A higher rank implies higher fidelity. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators. ", "page_idx": 35}, {"type": "table", "img_path": "FmNoFIImZG/tmp/e70b86b7e00fcf1733d0210b9e3a57290d6c238d7881908bda7143704d22ede7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "FmNoFIImZG/tmp/16c4911010c03947b322a93e74f402a7bd4f83d6589a31e94be3d71eb5b25e49.jpg", "table_caption": ["Table 18: $\\chi^{2}$ test between real train data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std result and average rank across datasets. A higher rank implies higher fidelity. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "D.6.2 Similarity between Real Test Data and Synthetic Data ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Table 19: Inverse KL between real test data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std result and average rank across datasets. A higher rank implies higher fidelity. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators. ", "page_idx": 37}, {"type": "table", "img_path": "FmNoFIImZG/tmp/94393c2e93e974e4f2397d4b7796412e1e5c55d4adc014faadd64b9d287dc2ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "Table 20: KS test between real test data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std result and average rank across datasets. A higher rank implies higher fidelity. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators. ", "page_idx": 38}, {"type": "table", "img_path": "FmNoFIImZG/tmp/376e29db835a9a2085651a1722aff5974beb5a8d04a951083b98eb223bfc0f1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "FmNoFIImZG/tmp/4e6106903bd2f1b63440823ae38704145208b988acd13ee75426fb16083c46c7.jpg", "table_caption": ["Table 21: $\\chi^{2}$ test between real test data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std result and average rank across datasets. A higher rank implies higher fidelity. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "D.7 Results on Privacy Preservation ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "D.7.1 Downstream Accuracy in Data Sharing ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Table 22: Classification accuracy $(\\%)$ aggregated over six downstream predictors, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes the inapplicability of a specific generator. Different from Table 1, \u201c\u2212\u201d denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We bold the highest result for each dataset of different sample sizes. TVAE learns the joint distribution $p(\\mathbf{x},y)$ and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 40}, {"type": "table", "img_path": "FmNoFIImZG/tmp/c3da1ca5f9ce26d4d8ebc529e46f7415bc35df38b39c587c316f554bff052de0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "Table 23: Classification accuracy $(\\%)$ of LR, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes the inapplicability of a specific generator. Different from Table 1, \u201c\u2212\u201d denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We bold the highest result for each dataset of different sample sizes. TVAE learns the joint distribution $p(\\mathbf{x},y)$ and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 41}, {"type": "table", "img_path": "FmNoFIImZG/tmp/c73ef77b34d17913710f94ffd6fc0923ea69e32bf21b11ec14c5e7b8735631cb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "Table 24: Classification accuracy $(\\%)$ of KNN, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes the inapplicability of a specific generator. Different from Table 1, \u201c\u2212\u201d denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We bold the highest result for each dataset of different sample sizes. TVAE learns the joint distribution $p(\\mathbf{x},y)$ and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 42}, {"type": "table", "img_path": "FmNoFIImZG/tmp/a55b08edbeb0c435c159028c16c3129d814b1de23b8402f4add5e0dd59e48daa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "Table 25: Classification accuracy $(\\%)$ of MLP, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes the inapplicability of a specific generator. Different from Table 1, \u201c\u2212\u201d denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We bold the highest result for each dataset of different sample sizes. TVAE learns the joint distribution $p(\\mathbf{x},y)$ and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 43}, {"type": "table", "img_path": "FmNoFIImZG/tmp/fcc93e68335042707835f4dd3de109b26e3d293ef90688e155116722aee223b2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "Table 26: Classification accuracy $(\\%)$ of RF, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes the inapplicability of a specific generator. Different from Table 1, \u201c\u2212\u201d denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We bold the highest result for each dataset of different sample sizes. TVAE learns the joint distribution $p(\\mathbf{x},y)$ and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 44}, {"type": "table", "img_path": "FmNoFIImZG/tmp/abf47a9b57107314021c84edd1baed480ccc21c972de66231d33f6e336affdcd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "Table 27: Classification accuracy $(\\%)$ of XGBoost, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes the inapplicability of a specific generator. Different from Table 1, \u201c\u2212\u201d denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We bold the highest result for each dataset of different sample sizes. TVAE learns the joint distribution $p(\\mathbf{x},y)$ and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 45}, {"type": "table", "img_path": "FmNoFIImZG/tmp/7e69ae4dcb0c350eedd703e054959883441dbcf43dd91e5f12cfadcb9cd39535.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "Table 28: Classification accuracy $(\\%)$ of TabPFN, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that \u201cN/A\u201d denotes the inapplicability of a specific generator. Different from Table 1, \u201c\u2212\u201d denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We bold the highest result for each dataset of different sample sizes. TVAE learns the joint distribution $p(\\mathbf{x},y)$ and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators. ", "page_idx": 46}, {"type": "table", "img_path": "FmNoFIImZG/tmp/7f8a84472f4e56f66237a7413cb5a76f2e49fb66081edc440611434e9101e38e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "D.7.2 DCR Evaluation ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Table 29: DCR between real train data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std result and average rank across datasets. A higher rank implies better privacy preservation. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the highest result for each dataset of different sample sizes. Even though ARF and NFLOW show high DCR, our experiments demonstrate that they do not learn the data distribution well, leading to poor downstream accuracy. TabEBM achieves competitive overall DCR against benchmark generators. ", "page_idx": 47}, {"type": "table", "img_path": "FmNoFIImZG/tmp/17c083b5c2855b91d5d6c69ff8aa73dee90ced37a9436c070af5366f9a79a502.jpg", "table_caption": [], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "D.7.3 Delta-presence Evaluation ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Table 30: $\\delta$ -presence between real train data and synthetic data on eight real-world tabular datasets with varied real data availability. We report the mean $\\pm$ std result and average rank across datasets. A higher rank implies better privacy preservation. Note that \u201cN/A\u201d denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We bold the best result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators. ", "page_idx": 48}, {"type": "table", "img_path": "FmNoFIImZG/tmp/95158044a7df8016aa7d323e35141314b201dac8f8e9e2013fe44bfc9f04256b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 48}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] Justification: Section 1 details our research objectives and highlights our contributions. ", "page_idx": 49}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Presented in Section 4. ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Section 2 presents the theoretical results of our proposed method. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Refer to Appendix B, where we provide full details on reproducing the results in the paper. We provide an open-source library of the proposed method. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Refer to Appendix B. All datasets used in this paper are publicly available, and the implementations of benchmark generators are open-source. We also provide an open-source library https://github.com/andreimargeloiu/TabEBM . ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Appendix B provides full descriptions of the experimental setup. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Refer to Section 3, where we provide standard deviations for all tables. Figure 6 and Figure 4 (Right) contain error bars. Due to the page limit, the error bars for all other figures are available in Appendix D. In Section 3.2 and Appendix D.6, we show statistical significance tests of the similarity between real data and synthetic data. ", "page_idx": 49}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Refer to Appendix B.4, where we provide full details on the computation resources used in the paper. ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We carefully check the NeurIPS Code of Ethics, and we confirm that our work follows the Code in every respect. ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Refer to Appendix A, where we include the societal impacts of our work. ", "page_idx": 50}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Refer to Appendix B, where we provide the open-source licenses followed by the creators or original owners of assets. ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We provide the implementation of our method as a python library attached to this submission. We will make it publicly available post-publication. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [No] Justification:[NA] ", "page_idx": 50}]