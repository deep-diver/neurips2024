{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduces a crucial safety fine-tuning method used extensively in the field and is directly relevant to the paper's core investigation."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-05-18", "reason": "This paper presents a significant alternative safety fine-tuning approach (DPO) that the current paper analyzes and compares to other methods."}, {"fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "publication_date": "2021-09-01", "reason": "This foundational paper establishes the effectiveness of instruction fine-tuning, a key precursor to safety fine-tuning, directly impacting the current paper's context."}, {"fullname_first_author": "Sijia Liu", "paper_title": "Rethinking machine unlearning for large language models", "publication_date": "2024-02-08", "reason": "This paper introduces a critical safety fine-tuning method (unlearning) that is central to the comparative analysis and mechanistic study in the current paper."}, {"fullname_first_author": "Maksym Andriushchenko", "paper_title": "Jailbreaking leading safety-aligned LLMs with simple adaptive attacks", "publication_date": "2024-04-01", "reason": "This paper highlights the vulnerability of safety-aligned LLMs to adversarial attacks, a crucial challenge directly addressed by the mechanistic investigation in the current work."}]}