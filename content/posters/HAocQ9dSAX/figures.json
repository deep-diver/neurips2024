[{"figure_path": "HAocQ9dSAX/figures/figures_0_1.jpg", "caption": "Figure 2: The pipeline of our distributed 3D Gaussian Splatting method. 1) We first split the scene into K blocks with similar sizes. Each block is extended to a larger size to construct overlapping parts. 2) Subsequently, we assign views and points into different blocks. The shared local 3D Gaussians (connected by solid lines in the figure) are a copy of the global 3D Gaussians. 3) The local 3D Gaussians are then collected and averaged to the global 3D Gaussians in each consensus step, and the global 3D Gaussians are shared with each block before training all blocks. 4) Finally, we use the final global 3D Gaussians to synthesize novel views.", "description": "This figure illustrates the pipeline of the proposed DOGS method for distributed 3D Gaussian splatting.  It shows how a scene is divided into blocks, how local and global 3D Gaussian models are maintained and updated through consensus, and how the final global model is used for rendering.", "section": "3.2 Distributed 3D Gaussian Consensus"}, {"figure_path": "HAocQ9dSAX/figures/figures_3_1.jpg", "caption": "Figure 2: The pipeline of our distributed 3D Gaussian Splatting method. 1) We first split the scene into K blocks with similar sizes. Each block is extended to a larger size to construct overlapping parts. 2) Subsequently, we assign views and points into different blocks. The shared local 3D Gaussians (connected by solid lines in the figure) are a copy of the global 3D Gaussians. 3) The local 3D Gaussians are then collected and averaged to the global 3D Gaussians in each consensus step, and the global 3D Gaussians are shared with each block before training all blocks. 4) Finally, we use the final global 3D Gaussians to synthesize novel views.", "description": "This figure illustrates the pipeline of the proposed DOGS method.  The scene is first split into K blocks with overlapping regions.  Each block has its own set of local 3D Gaussians.  During training, these local 3D Gaussians are iteratively updated, and their averages are used to update a global 3D Gaussian model.  The global model is then shared with all blocks to maintain consistency. After training, only the global model is used for rendering novel views.", "section": "3.2 Distributed 3D Gaussian Consensus"}, {"figure_path": "HAocQ9dSAX/figures/figures_5_1.jpg", "caption": "Figure 2: The pipeline of our distributed 3D Gaussian Splatting method. 1) We first split the scene into K blocks with similar sizes. Each block is extended to a larger size to construct overlapping parts. 2) Subsequently, we assign views and points into different blocks. The shared local 3D Gaussians (connected by solid lines in the figure) are a copy of the global 3D Gaussians. 3) The local 3D Gaussians are then collected and averaged to the global 3D Gaussians in each consensus step, and the global 3D Gaussians are shared with each block before training all blocks. 4) Finally, we use the final global 3D Gaussians to synthesize novel views.", "description": "This figure illustrates the steps involved in the distributed 3D Gaussian Splatting process.  The scene is first divided into K blocks with overlapping regions to ensure data consistency between neighboring blocks. Views and points are assigned to their respective blocks.  Then, a global 3D Gaussian model is maintained on a master node. Local models are updated iteratively. Local 3D Gaussians are shared and averaged to update the global model, which is then re-distributed. Finally, only the global model is used for rendering.", "section": "3.2 Distributed 3D Gaussian Consensus"}, {"figure_path": "HAocQ9dSAX/figures/figures_7_1.jpg", "caption": "Figure 1: DOGS accelerates 3D GS training on large-scale UrbanScene3D dataset [29] by 6+ times with better rendering quality. Top: 3D Gaussian primitives (8.27M); Bottom: rendered images.", "description": "This figure shows a comparison of the results obtained using the proposed DOGS method and the original 3D Gaussian Splatting (3D GS) method for 3D reconstruction on the large-scale UrbanScene3D dataset. The top row displays the 3D Gaussian primitives used to represent the scene, showing 8.27 million primitives in the DOGS method. The bottom row shows rendered images generated from those primitives, highlighting the improved rendering quality achieved using the DOGS method. The figure demonstrates that the DOGS method significantly accelerates the training process of 3D GS while maintaining high rendering quality. ", "section": "Abstract"}, {"figure_path": "HAocQ9dSAX/figures/figures_8_1.jpg", "caption": "Figure 1: DOGS accelerates 3D GS training on large-scale UrbanScene3D dataset [29] by 6+ times with better rendering quality. Top: 3D Gaussian primitives (8.27M); Bottom: rendered images.", "description": "This figure shows the results of applying the DOGS method to the UrbanScene3D dataset.  The top row displays 8.27 million 3D Gaussian primitives that represent the 3D scene reconstructed by DOGS. The bottom row shows the rendered images generated from these primitives, demonstrating the high-fidelity rendering quality achieved by the method.  The caption highlights that DOGS accelerates the training process by more than 6 times compared to standard 3D Gaussian Splatting.", "section": "Abstract"}, {"figure_path": "HAocQ9dSAX/figures/figures_9_1.jpg", "caption": "Figure 1: DOGS accelerates 3D GS training on large-scale UrbanScene3D dataset [29] by 6+ times with better rendering quality. Top: 3D Gaussian primitives (8.27M); Bottom: rendered images.", "description": "This figure shows the results of applying DOGS to the UrbanScene3D dataset.  The top row displays 8.27 million 3D Gaussian primitives used to represent the scene. The bottom row shows rendered images generated from these primitives, highlighting the improved rendering quality achieved by DOGS.  The caption also emphasizes that DOGS speeds up training by more than 6 times compared to the original 3D Gaussian Splatting method.", "section": "Abstract"}, {"figure_path": "HAocQ9dSAX/figures/figures_9_2.jpg", "caption": "Figure 7: Ablation study of our method. Top: rendered images; Bottom: rendered depths.", "description": "This figure shows the results of an ablation study comparing different versions of the DOGS method.  The top row displays rendered images, while the bottom row shows the corresponding depth maps. The variations compared include versions without consensus (w.o. CS), without self-adaptation (w.o. SD), without over-relaxation (w.o. OR), and the full model. Red boxes highlight areas where differences are most apparent.", "section": "3.3 Improving Convergence Rate"}, {"figure_path": "HAocQ9dSAX/figures/figures_9_3.jpg", "caption": "Figure 8: Importance of the consensus step.", "description": "This figure shows a comparison of the results with and without the consensus step. The red boxes highlight the areas near the boundary between blocks. The results without consensus steps show obvious artifacts near the boundary, which indicates that the consensus step is important for ensuring the consistency of the global 3D Gaussian model.", "section": "3.2 Distributed 3D Gaussian Consensus"}, {"figure_path": "HAocQ9dSAX/figures/figures_17_1.jpg", "caption": "Figure 10: More qualitative comparisons of our method and state-of-the-art methods.", "description": "This figure presents a qualitative comparison of the proposed DOGS method against several state-of-the-art methods, including Mega-NeRF, Switch-NeRF, and the original 3D GS, on various scenes.  Green circles highlight areas for closer examination. The comparison focuses on the visual quality of novel view synthesis, showing the superior detail and clarity achieved by the proposed method.  It demonstrates the impact of distributed training and consensus on large-scale scenes.", "section": "Experiments"}, {"figure_path": "HAocQ9dSAX/figures/figures_17_2.jpg", "caption": "Figure 2: The pipeline of our distributed 3D Gaussian Splatting method. 1) We first split the scene into K blocks with similar sizes. Each block is extended to a larger size to construct overlapping parts. 2) Subsequently, we assign views and points into different blocks. The shared local 3D Gaussians (connected by solid lines in the figure) are a copy of the global 3D Gaussians. 3) The local 3D Gaussians are then collected and averaged to the global 3D Gaussians in each consensus step, and the global 3D Gaussians are shared with each block before training all blocks. 4) Finally, we use the final global 3D Gaussians to synthesize novel views.", "description": "This figure illustrates the pipeline of the DOGS method.  It shows how the scene is divided into blocks, local and global 3D Gaussian models are trained and updated in an alternating fashion using ADMM, and finally, how only the global model is used for inference. The figure highlights the key steps: scene splitting, Gaussian sharing, consensus, and synthesis.", "section": "3.2 Distributed 3D Gaussian Consensus"}, {"figure_path": "HAocQ9dSAX/figures/figures_18_1.jpg", "caption": "Figure 2: The pipeline of our distributed 3D Gaussian Splatting method. 1) We first split the scene into K blocks with similar sizes. Each block is extended to a larger size to construct overlapping parts. 2) Subsequently, we assign views and points into different blocks. The shared local 3D Gaussians (connected by solid lines in the figure) are a copy of the global 3D Gaussians. 3) The local 3D Gaussians are then collected and averaged to the global 3D Gaussians in each consensus step, and the global 3D Gaussians are shared with each block before training all blocks. 4) Finally, we use the final global 3D Gaussians to synthesize novel views.", "description": "This figure illustrates the pipeline of the DOGS method for distributed 3D Gaussian splatting.  It shows how the scene is divided into blocks, how local and global 3D Gaussian models are maintained and updated through consensus, and how the final global model is used for rendering.", "section": "3.2 Distributed 3D Gaussian Consensus"}]