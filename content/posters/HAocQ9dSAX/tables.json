[{"figure_path": "HAocQ9dSAX/tables/tables_7_1.jpg", "caption": "Table 1: Quantitative results of novel view synthesis on Mill19 [51] dataset and Urban-Scene3D [29] dataset. \u2191: higher is better, \u2193: lower is better. The red, orange and yellow colors respectively denote the best, the second best, and the third best results. \u2020 denotes without applying the decoupled appearance encoding.", "description": "This table presents a quantitative comparison of different methods for novel view synthesis on two large-scale urban datasets: Mill19 and UrbanScene3D.  The metrics used are PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity).  Higher PSNR and SSIM values indicate better image quality, while a lower LPIPS value signifies better perceptual similarity.  The table highlights the best performing method (red), second best (orange), and third best (yellow) for each scene.  A dagger symbol (\u2020) denotes results obtained without applying decoupled appearance encoding, allowing for a fair comparison between methods with and without this technique.", "section": "4 Experiments"}, {"figure_path": "HAocQ9dSAX/tables/tables_7_2.jpg", "caption": "Table 2: Quantitative results of novel view synthesis on Mill19 dataset and UrbanScene3D dataset. We present the training time (hh:mm), the number of final points (106), the allocated memory (GB), and the framerate (FPS) during evaluation. \u2020 denotes without applying the decoupled appearance encoding.", "description": "This table presents a quantitative comparison of different methods for novel view synthesis on the Mill19 and UrbanScene3D datasets.  It shows the training time, the number of final 3D Gaussian points, the memory used during training and the frames per second (FPS) achieved during evaluation.  The results help to assess the efficiency and quality of various methods in reconstructing and rendering large-scale scenes.", "section": "4 Experiments"}, {"figure_path": "HAocQ9dSAX/tables/tables_8_1.jpg", "caption": "Table 3: Quantitative results of novel view synthesis on the MatrixCity [24] dataset. \u2191: higher is better, \u2193: lower is better. The red, orange and yellow colors respectively denote the best, the second best, and the third best results.", "description": "This table presents a quantitative comparison of the proposed DOGS method against several state-of-the-art methods for novel view synthesis on the MatrixCity dataset.  It shows the PSNR, SSIM, and LPIPS metrics for both aerial and street views, along with training time, the number of points used in the final model, memory usage, and frames per second (FPS).  Higher PSNR and SSIM values, and lower LPIPS values indicate better performance. The table also highlights the best-performing method for each metric in each scene type with color-coding.", "section": "Experiments"}, {"figure_path": "HAocQ9dSAX/tables/tables_8_2.jpg", "caption": "Table 4: Ablation study of our method.", "description": "This table presents the ablation study of the proposed DOGS method.  It shows the quantitative results (PSNR, SSIM, LPIPS) of novel view synthesis on the Mill19 and UrbanScene3D datasets when different components of the method are removed.  The results are compared against the full model, showing the impact of the 3D Gaussian consensus (CS), self-adaptive penalty parameters (SD), and over-relaxation (OR) on the overall performance.", "section": "4 Experiments"}, {"figure_path": "HAocQ9dSAX/tables/tables_15_1.jpg", "caption": "Table 1: Quantitative results of novel view synthesis on Mill19 [51] dataset and Urban-Scene3D [29] dataset. \u2191: higher is better, \u2193: lower is better. The red, orange and yellow colors respectively denote the best, the second best, and the third best results. \u2020 denotes without applying the decoupled appearance encoding.", "description": "This table presents a quantitative comparison of different methods for novel view synthesis on two large-scale urban datasets: Mill19 and UrbanScene3D.  The metrics used are PSNR, SSIM, and LPIPS.  The table highlights the best performing method (DOGS) across various scenes within each dataset, indicating superior performance in terms of image quality.  The notation for higher is better and lower is better is provided, along with color-coding to visually represent ranking of performance. The note indicates that some results exclude the decoupled appearance encoding technique.", "section": "4 Experiments"}, {"figure_path": "HAocQ9dSAX/tables/tables_16_1.jpg", "caption": "Table 2: Quantitative results of novel view synthesis on Mill19 dataset and UrbanScene3D dataset. We present the training time (hh:mm), the number of final points (10<sup>6</sup>), the allocated memory (GB), and the framerate (FPS) during evaluation. \u2020 denotes without applying the decoupled appearance encoding.", "description": "This table presents a quantitative comparison of different methods for novel view synthesis on two large-scale datasets (Mill19 and UrbanScene3D).  It shows the training time, the number of 3D Gaussian points used in the final model, the memory used (in GB), and the rendering speed (frames per second) achieved by each method. The results highlight the efficiency and quality of DOGS compared to other state-of-the-art techniques.", "section": "4 Experiments"}]