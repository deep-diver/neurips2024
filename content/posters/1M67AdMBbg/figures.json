[{"figure_path": "1M67AdMBbg/figures/figures_2_1.jpg", "caption": "Figure 1: The pipeline of the proposed M\u00b2CRL. For the generated global and local views with different frame rates and spatial sizes, we adopt two different mask strategies: the frame-aggregated attention guided tube mask and the random tube mask. These strategies are integrated with mask reconstruction and contrastive method, enabling the model to simultaneously learn both the pixel-level and discriminative features of the video.", "description": "This figure illustrates the architecture of the proposed Multi-view Masked Contrastive Representation Learning (M\u00b2CRL) framework.  It shows how global and local views of endoscopic videos are processed using different masking strategies (Frame-aggregated Attention Guided Tube Mask and Random Tube Mask) combined with contrastive learning and mask reconstruction. The goal is to learn both fine-grained pixel-level details and holistic discriminative features for improved performance on downstream tasks.", "section": "3 Method"}, {"figure_path": "1M67AdMBbg/figures/figures_6_1.jpg", "caption": "Figure 2: Qualitative results of segmentation and detection tasks. The segmentation results on the left are from the CVC-12k dataset, while the detection results on the right are from the KUMC dataset.", "description": "This figure shows a qualitative comparison of the segmentation and detection results obtained using the proposed M\u00b2CRL method and other state-of-the-art self-supervised pre-training methods on the CVC-12k and KUMC datasets. The left side displays segmentation results, highlighting the model's ability to accurately segment polyp regions, even in challenging scenarios with overlapping or small polyps. The right side shows detection results, demonstrating the method's effectiveness in identifying and localizing polyps with varying sizes and contrasts. The figure visually demonstrates the superior performance of M\u00b2CRL in both tasks, especially in capturing fine-grained details and handling challenging scenarios.", "section": "4.2 Comparison with Prior Work"}, {"figure_path": "1M67AdMBbg/figures/figures_19_1.jpg", "caption": "Figure 2: Qualitative results of segmentation and detection tasks. The segmentation results on the left are from the CVC-12k dataset, while the detection results on the right are from the KUMC dataset.", "description": "This figure shows a qualitative comparison of the segmentation and detection results obtained using different methods on two benchmark datasets for endoscopic video analysis. The left half displays the segmentation results on the CVC-12k dataset, while the right half shows detection results on the KUMC dataset. Each column represents a different method, allowing for a visual comparison of how well each method performs in segmenting polyps and detecting them within endoscopic videos.  The original images and ground truth annotations are also included for reference.", "section": "4.2 Comparison with Prior Work"}, {"figure_path": "1M67AdMBbg/figures/figures_20_1.jpg", "caption": "Figure 2: Qualitative results of segmentation and detection tasks. The segmentation results on the left are from the CVC-12k dataset, while the detection results on the right are from the KUMC dataset.", "description": "This figure shows a qualitative comparison of the segmentation and detection results obtained using different methods on the CVC-12k and KUMC datasets. The left side displays the segmentation results, showcasing how various methods segment polyps in endoscopic images from the CVC-12k dataset. The right side shows the detection performance on images from the KUMC dataset, demonstrating the ability of each method to accurately locate polyps.  The comparison helps to visualize the differences in accuracy and precision of various methods in identifying polyps.", "section": "4.2 Comparison with Prior Work"}, {"figure_path": "1M67AdMBbg/figures/figures_20_2.jpg", "caption": "Figure 5: Illustration of our frame-aggregated attention guided tube masking strategy. We visualize spatial attention map with temporal information for each frame (2nd column), then aggregate attention maps for all frames and select area of high attention. We sample visible patches in this area (3rd column).", "description": "This figure visualizes the frame-aggregated attention guided tube masking strategy used in the paper.  It shows how the model aggregates attention maps across multiple frames to identify regions of high importance.  These regions are then used to guide the sampling of visible patches, ensuring that the most relevant information is used for masked reconstruction and contrastive learning.", "section": "E Case Study"}]