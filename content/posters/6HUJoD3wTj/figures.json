[{"figure_path": "6HUJoD3wTj/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of a few key tasks considered in our work.", "description": "This figure shows three tasks that are considered in the paper.  (a) Index Lookup Task: A model receives a sequence of tokens, then an index, and must output the token at that index.  (b) Bounded Dyck-2 with depth \u2264 2:  The model must decide if a sequence of parentheses is well-balanced, with a maximum nesting depth of 2. (c) Nearest Neighbor Task: The model receives a sequence of input-label pairs, then a query input, and must predict the closest matching label using the nearest-neighbor algorithm.", "section": "3 Index Lookup Task"}, {"figure_path": "6HUJoD3wTj/figures/figures_8_1.jpg", "caption": "Figure 2: Performance of models on the Index Lookup and bounded Dyck task. Labels such as TF-(1, 64) denote Transformers with 1 layer and 64 widths. See Section 6 for more details.", "description": "This figure presents the performance of different recurrent neural network architectures and transformer models on two tasks: Index Lookup and recognizing bounded Dyck languages.  The Index Lookup task involves predicting a token at a given index in a sequence. The bounded Dyck language task involves determining whether a sequence of parentheses is correctly balanced. The heatmaps show the accuracy of different models at various sequence lengths. The line graphs showcase the validation accuracy during training on specific sequence lengths.", "section": "Empirical Analysis"}, {"figure_path": "6HUJoD3wTj/figures/figures_33_1.jpg", "caption": "Figure 3: Performance of Mamba on the Index Lookup task across various lengths and widths. See Section H.1 for more details.", "description": "This figure shows the performance of the Mamba recurrent model on the Index Lookup task for various sequence lengths (20, 50, 75, 100, 200, 400) and hidden state sizes (32, 64, 256, 512, 1024).  The heatmap on the left displays the accuracy for each length and width combination. The line graph on the right shows the validation accuracy curves during training, highlighting the learning progress for different model sizes.  The results indicate that Mamba's performance on the Index Lookup task improves with increasing width, but still significantly lags behind the performance of one-layer Transformers.", "section": "Empirical Analysis"}, {"figure_path": "6HUJoD3wTj/figures/figures_35_1.jpg", "caption": "Figure 2: Performance of models on the Index Lookup and bounded Dyck task. Labels such as TF-(1, 64) denote Transformers with 1 layer and 64 widths. See Section 6 for more details.", "description": "The figure displays the performance of various recurrent neural network models (LSTMs, DSS, RetNet, Mamba) and transformer models (one-layer and two-layer) on the Index Lookup and bounded Dyck tasks.  The left panel shows validation accuracy at different input sequence lengths. The center and right panels present the validation accuracy curves across training iterations for the Index Lookup and Dyck tasks respectively. The results highlight the performance differences between the models across different tasks and sequence lengths.", "section": "6 Empirical Analysis"}]