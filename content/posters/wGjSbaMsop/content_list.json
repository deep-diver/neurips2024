[{"type": "text", "text": "Algorithmic Collective Action in Recommender Systems: Promoting Songs by Reordering Playlists ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Joachim Baumann\u2217 University of Zurich baumann@ifi.uzh.ch ", "page_idx": 0}, {"type": "text", "text": "Celestine Mendler-D\u00fcnner ", "page_idx": 0}, {"type": "text", "text": "ELLIS Institute, T\u00fcbingen MPI for Intelligent Systems, T\u00fcbingen T\u00fcbingen AI Center celestine@tue.ellis.eu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate algorithmic collective action in transformer-based recommender systems. Our use case is a collective of fans aiming to promote the visibility of an underrepresented artist by strategically placing one of their songs in the existing playlists they control. We introduce two easily implementable strategies to select the position at which to insert the song and boost recommendations at test time. The strategies exploit statistical properties of the learner to leverage discontinuities in the recommendations, and the long-tail nature of song distributions. We evaluate the efficacy of our strategies using a publicly available recommender system model released by a major music streaming platform. Our findings reveal that even small collectives (controlling less than $0.01\\%$ of the training data) can achieve up to $40\\times$ more test time recommendations than songs with similar training set occurrences, on average. Focusing on the externalities of the strategy, we find that the recommendations of other songs are largely preserved, and the newly gained recommendations are distributed across various artists. Together, our findings demonstrate how carefully designed collective action strategies can be effective while not necessarily being adversarial. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the ever-evolving landscape of music discovery, the challenge of accessing and sifting through the overwhelming number of tracks released daily has become increasingly difficult. This has resulted in a strong dependence on platforms like Spotify, Deezer, and Apple Music, which distribute and promote music through algorithmic song recommendations. These systems rely on historical data to learn user preferences and predict future content consumption [21, 51, 34, 7, 6]. ", "page_idx": 0}, {"type": "text", "text": "It has been widely documented that music recommendation systems suffer from popularity bias as they tend to concentrate recommendation exposure on a limited fraction of artists, often overlooking new and emerging talent [35, 4, 2, 15, 9, 28]. As the success and visibility of artists are deeply influenced by the algorithms of these platforms, this can lead to a considerable imbalance in the music industry [1, 41] and reinforce existing inequalities [50]. Thus, artists have started to fight for more transparency and fairer payments from online streaming services. The \u201cJustice at Spotify\u201d campaign, launched by the Union of Musicians and Allied Workers [54], has been signed by more than 28,000 artists. At the same time, the International Society for Music Information Retrieval has been arguing for promoting the discovery of less popular artists by recommending \u2018long-tail\u2019 items [3], as have other researchers [11, 53, 16, 40]. ", "page_idx": 0}, {"type": "image", "img_path": "wGjSbaMsop/tmp/de0fb48e75139f3b1e743d45a243a4f7e44ed22ec91feb9b1087c773c8239dfd.jpg", "img_caption": ["Figure 1: By strategically choosing the position at which to insert the target song in a playlist, collectives can achieve a disproportionally high recommendation frequency relative to training set occurrences, compared to naturally occurring songs. Amplification of one corresponds to matching frequencies in train and test. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this work, we explore algorithmic collective action as an alternative means for emerging artists to gain exposure in machine learning-powered recommender systems by mobilizing their fan base. Algorithmic collective action [24] refers to the coordinated effort of a group of platform participants who strategically report the part of the training data they control to influence prediction outcomes. Our work is situated in an emerging literature that recognizes data as a powerful lever for users to promote their interests on digital platforms [57, 24]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study algorithmic collective action in transformer-based recommender systems. As a case study, we consider the task of automatic playlist continuation (APC), which is at the heart of many music streaming services. APC models take a seed playlist (an ordered list of unique songs) as input and recommend songs to follow. They are trained on the universe of playlists stored on the platform. The collective consists of platform users who can modify the subset of playlists they own. The goal of the collective is to promote a less popular artist by increasing the recommendations of their songs at test time. To this end, we consider collective action strategies where participants of the collective agree on a target song $s^{*}$ to strategically place in their playlists. ", "page_idx": 1}, {"type": "text", "text": "We motivate and discuss two strategies to choose the position of $s^{*}$ within any given playlist. Both strategies are derived from a statistical optimality assumption on the recommender and do not require knowledge of the specifics of the model architecture or the model weights. Instead, they use that the model is trained to fti sequential patterns in existing data and build on aggregate song statistics that are feasible to gather from public information. We empirically test our strategies using an industry-scale APC model that has been deployed to provide recommendations for millions of users on Deezer\u2014one of the biggest streaming platforms in the world. To train the model, we use the Spotify Million Playlist Dataset, treating each playlist as a user and randomly sampling a fraction to compose the collective. ", "page_idx": 1}, {"type": "text", "text": "We find that by strategically choosing the position of the target song, collectives can achieve significant over-representation at test time, see Figure 1 for a teaser. We experiment with collectives composed of a random sample of users owning between $0.001\\%$ and $2\\%$ of the training data instances. Interestingly, even tiny user collectives, controlling as few as 60 playlists, can achieve an amplification of up to $25\\times$ , referring to the song\u2019s recommendation frequency relative to the training frequency. This is $40\\times$ more than an average song occurring at the same frequency in the training data. In contrast, placing the song in a fixed position in every playlist is largely ineffective. ", "page_idx": 1}, {"type": "text", "text": "Our strategy satisfies a strict authenticity constraint and thus preserves user experience at training time by design. Interestingly, we find that also at test time recommendations are largely preserved; not only on aggregate but also for members of the collective. As a consequence, the strategies come with small externalities for users, and at the same time, they also have a relatively small effect on model performance. For large collectives controlling $>3\\%$ of the playlists, the effect corresponds to every target song recommendation replacing an otherwise relevant song in less than $15\\%$ of the cases, leaving other recommendations unaltered. Thus, in the hypothetical case where the promoted song is indeed relevant, this could lead to an overall gain in more than $85\\%$ of the cases, even though the total number of test-time recommendations is fixed. Lastly, we show that the newly gained recommendations are taken from artists of diverse popularity without any indication that a specific artist suffers disproportionally. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Taken together, our work demonstrates a first example of collective action in sequential recommender systems. We show how collective action goals can be achieved while largely preserving service quality and user experience. The feasibility of such strategies raises many interesting questions, challenges, and opportunities for future work. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The fairness of recommendation systems on online platforms remains a pressing issue for both content consumers and producers [10, 32, 19, 26]\u2014see Zehlike et al. [61] for a detailed overview. Several recent works study individual strategic users attempting to influence their own recommendations [5, 25, 12, 13]. Other works consider adding antidote data to fight polarization and unfairness [42, 18]. ", "page_idx": 2}, {"type": "text", "text": "Beyond recommender systems, a related line of work centers the users in the study of machine learning systems. Vincent and Hecht [55] call for conscious data contribution, Vincent et al. [56] discuss data strikes, and Vincent et al. [57] emphasize the potential of data levers as a means to gain back power over platforms. Hardt et al. [24] introduce the framework of algorithmic collective action for formally studying coordinated strategies of users against algorithmic systems. They empirically demonstrate the effectiveness of collective action in correlating a signal function with a target label. Sigg et al. [46] inspect collective action at inference time in combinatorial systems. Complementing these findings, we demonstrate that collective action can be effective even without control over samples at inference time. We highlight a so far understudied dimension of algorithmic collective action by discussing and illuminating the externalities of algorithmic collective action strategies. ", "page_idx": 2}, {"type": "text", "text": "At a technical level, our findings most closely relate to shilling attacks, or more broadly, data poisoning attacks [c.f., 49]. Shilling attacks are usually realized by injecting fake user profiles and ratings in order to push the predictions of some targeted items [45, 47]. Due to the fraudulent nature of these attacks, there are little design restrictions on the proflies, and they often come with considerable negative effects for the firm [38, 20]. Data poisoning attacks in recommender systems predominantly focus on collaborative filtering-based models, with a few exceptions; Zhang et al. [62] propose a reinforcement learning-based framework to promote a target item, Yue et al. [58] provide a solution to extract a black-box model\u2019s weights through API queries to then generate fake users for promoting an item, and Yue et al. [59] propose injecting fake items into seemingly real item sequences (at inference time and without retraining) with a gradient-guided algorithm, requiring full access to the model weights. Taking the perspective of collective action, we focus on easy-to-implement strategies that require minimal knowledge of the model and operate under an authenticity constraint to preserve the utility of altered playlists while seamlessly integrating into natural interaction with the platform. ", "page_idx": 2}, {"type": "text", "text": "Further, our work pertains to a broader scholarly literature interested in improving labor conditions for gig workers on digital platforms [e.g., 29, 52], optimizing long-term social welfare in online systems [33], and understanding dynamics in digital marketplaces [27]. The type of strategic data modification we consider falls under the umbrella of adversarial feature feedback loops [39]. Taking advantage of collective strategies to change model outcomes more broadly has been studied in tabular data [17], computer vision [43], and recently in generative AI [44]. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries on automatic playlist continuation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use automatic playlist continuation (APC) as a running example of a sequential recommendation task. APC forms the backbone of major streaming platforms, such as Spotify and Deezer. To formally define the recommendation task, let $S=\\{s_{1},...,s_{n}\\}$ denote the universe of songs, where $n\\geq1$ denotes the number of unique songs. A playlist $p$ is composed of an ordered list of songs selected from $\\boldsymbol{S}$ without replacement. Given a seed playlist $p$ , the firm\u2019s goal is to predict follow-up songs that the user likely listens to. We consider a top- $K$ recommender system that outputs a personalized ordered list of $K\\geq1$ songs. We write ${\\mathrm{Rec}}_{K}(p)$ for the set of $K$ songs recommended for a seed playlist $p$ . ", "page_idx": 2}, {"type": "image", "img_path": "wGjSbaMsop/tmp/c1524d184a16446c47b3e0c8e937fdf6e3960f5d56ea0bc81710780987fb69b0.jpg", "img_caption": ["Figure 2: Imbalance in recommendation distribution. (left) The Lorenz curve shows that $80\\%$ of all recommendations are concentrated among just $10\\%$ of artists. (right) The Spotify track frequency distribution shows the long tail of song frequencies in user-generated playlists: close to $50\\%$ of tracks in playlists occur only once. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.1 Transformer-based recommender ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Over the past years, most large platforms have shifted from relying on collaborative filtering-based models for APC to building deep learning-based recommenders that account for sequential and session-based information [21, 51, 34]. In this work, we focus on transformer-based recommender systems that posit the following structure: Each song $s$ is mapped to a song embedding vector $\\bar{h_{s}}=\\phi(s)$ , where $\\phi$ denotes the embedding function. Each playlist $\\boldsymbol{p}=[s_{1},s_{2},...,s_{L}]$ is mapped to an embedding vector $h_{p}$ by aggregating the embeddings of the songs contained in the playlist as $h_{p}=g(h_{s_{1}},h_{s_{2}},...,h_{s_{L}})$ , where $g$ is a sequence-aware attention function. We assume all playlists have length $L$ smaller than the attention window for the purpose of exposition. At inference time, the recommendation of the next $K$ songs for a given playlist $p$ is determined by evaluating the similarity between the playlist seed $p$ and all potential follow up songs $s\\in S\\setminus p$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{SIM}(s,p):=\\langle h_{s},h_{p}\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the $K$ songs with the largest similarity value are recommended in descending order of similarity. We denote the set of recommended songs as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Rec}_{K}(p)=\\underset{S^{\\prime}\\subseteq S\\setminus p:|S^{\\prime}|=K}{\\arg\\operatorname*{max}}\\ \\sum_{s\\in S^{\\prime}}\\mathrm{SIM}(s,p).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The embeddings $\\phi$ and the attention function $g$ are parameterized by neural networks. They are trained from existing user-generated playlists in a self-supervised manner by repeatedly splitting playlists into a seed context and a target sequence and employing a contrastive loss function for training. ", "page_idx": 3}, {"type": "text", "text": "Statistical abstraction. We do not assume the collective has knowledge of the parameters of either $\\phi$ or $g$ . Instead, the design of the strategy builds on the assumption that sequential, transformer-based models are trained such that $\\mathrm{SIM}(s,p)$ is large for songs $s$ that frequently follow context $p$ in the training data, and small otherwise. This approximately is robust to nuances in hyperparameter choices or architecture design and applies to any sufficiently expressive and well trained model. ", "page_idx": 3}, {"type": "text", "text": "2.2 Typical imbalances in recommendations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "On today\u2019s music streaming platforms, a small number of artists receive the vast majority of recommendations, while the majority receive few or none. This imbalance is illustrated by the Lorenz curve in Figure 2, which is based on recommendations derived from the Deezer model on the Spotify MPD dataset (see Section 4). The Gini coefficient measuring inequality corresponds to 0.87. Streaming and radio statistics reveal an even more severe imbalance: the top $1\\%$ of newly released songs receive $99.99\\%$ of radio plays and $90\\%$ of streams go to just $1\\%$ of artists [9]. ", "page_idx": 3}, {"type": "text", "text": "Considering Figure 1 we can also see that songs with high prevalence in the training data are recommended disproportionately often at test time compared to their training set frequency (referring to the slope of ${\\sim}1.8$ of the blue point cloud). This gain in exposure through the recommender comes at the expense of many low-frequency songs that receive no recommendations at test time, further amplifying existing imbalances. Considering Spotify\u2019s substantial power to influence song consumption among platform users [1], withholding initial exposure for these songs limits their potential to reach a broader audience, significantly impacting an artist\u2019s career. In this work, we focus on collective efforts to boost recommendations for one of these underrepresented songs. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3 Algorithmic collective action for promoting songs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We build on the framework of Hardt et al. [24] and consider collectives that are composed of a fraction $\\alpha\\in(0,1]$ of randomly sampled users on the platform. We assume each user controls a single playlist. Members of the collective can strategically manipulate their playlists. We use $\\mu(\\cdot)$ to describe the strategy of mapping an original playlist to a modified playlist. ", "page_idx": 4}, {"type": "text", "text": "Success and amplification. Let ${\\mathcal P}_{0}$ denote the distribution over playlists. The goal of the collective is to increase the exposure of a target song $s^{*}$ for a randomly sampled playlist from ${\\mathcal P}_{0}$ at test time. We measure the success of collective action as ", "page_idx": 4}, {"type": "equation", "text": "$$\nS(\\alpha):=\\mathrm{E}_{p\\sim\\mathcal{P}_{0}}\\;1\\left[s^{*}\\in\\mathrm{Rec}_{K}(p)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The recommender system $\\mathrm{Rec}_{K}$ is trained on a partially manipulated training dataset $\\mathcal{D}$ , composed of $N$ samples from ${\\mathcal P}_{0}$ , among which $\\alpha N$ have been transformed under $\\mu$ . ", "page_idx": 4}, {"type": "text", "text": "We are particularly interested in measuring the effectiveness of a strategy relative to the effort of the collective. Therefore, we define amplification (Amp) as the fraction of newly gained target recommendations at test time divided by the fraction of manipulated playlists in the training set: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Amp}(\\alpha)={\\frac{1}{\\alpha}}(S(\\alpha)-S(0))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "An amplification of 0 means that the strategy is ineffective, an amplification of 1 means that the song frequency in the training set is proportionally represented in the model\u2019s predictions, and an amplification larger than 1 means that collective action achieves a disproportionate influence on the recommender. In the following, we choose a song $s^{*}$ that does not currently appear in the training data, hence $S(0)=0$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Authenticity constraint ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Participants of the collective are users of the platform. We design collective action strategies under the following authenticity constraint, not to compromise user experience: ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Authenticity constraint). We say a strategy $\\boldsymbol{\\mu}:\\boldsymbol{p}\\rightarrow\\boldsymbol{p}^{\\prime}$ is authentic iff the Levenshtein distance between $p$ and $\\bar{p^{\\prime}}=\\mu(p)$ satisfies $\\mathrm{Lev}(p,p^{\\prime})\\leq1$ for any $p$ . ", "page_idx": 4}, {"type": "text", "text": "The Levhenstein distance [30], also known as edit distance in information theory, counts the number of operations needed to transform one sequence into another. The song insertion strategy we propose in this work is one concrete instantiation of $\\mu$ that satisfies this constraint. More specifically, our strategy consists of inserting an agreed-upon target song $s^{*}$ at a specific position in every playlist $p$ . In contrast, existing adversarial strategies typically perform larger modifications to playlists and would not satisfy this constraint [62, 58]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Algorithmic lever ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The algorithmic lever of the collective is to strategically choose, for each playlist, the position $i^{*}$ at which to insert the target song. Under our probabilistic assumption about the learner, strategically placing $s^{*}$ after $p$ means that the similarity between $p$ and $s^{*}$ is increased. Thus, by choosing the index $i^{*}$ , the collective targets context $p_{i^{*}}^{-}$ referring to the sequence of songs $s_{j}$ in $p$ up to index $j\\,=\\,i$ . Recall that the collective aims to be among the top $K$ songs with high frequency over a randomly sampled context $p$ at test time. Our strategies exploit two different algorithmic levers towards this goal: Indirectly targeting Clusters of similar contexts (InClust) or Directly targeting Low-Frequency contexts (DirLoF). Pseudocode for the two strategies can be found in Figure 3. ", "page_idx": 4}, {"type": "text", "text": "Input: $s^{*}$ , collectively owned playlists $D^{*}=\\{p_{1},p_{2},...,p_{n}\\}\\subseteq D$   \n1: Coordination step:   \n2: (a) $r^{s}\\leftarrow$ for every $s$ in $D^{*}$ pool information to count song frequencies in $D^{*}$ .   \n3: (b) $q^{s}\\gets$ for every $s$ in $D^{*}$ estimate training set song frequency by gathering side information.   \n4: for all playlists $p\\in D^{*}$ do   \n5: Define anchor $s_{0}$ : find song $s_{0}\\in p$ such that (a) $r^{s_{0}}\\geq r^{s}\\;\\forall s\\in p$ or (b) $q^{s_{0}}\\leq q^{s}\\;\\forall s\\in p$   \n6: Insert target song: insert $s^{*}$ (a) before $s_{0}$ or (b) after $s_{0}$   \n7: Store modified playlist   \n8: end for ", "page_idx": 5}, {"type": "image", "img_path": "wGjSbaMsop/tmp/d77706f5a7ba040fa785571afc115e042081c55d23b146bde1d370e7ecfd8408.jpg", "img_caption": ["Figure 3: Song insertion strategies, pseudocode and illustration. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Concentrating effort. Inclusion in the set ${\\mathrm{Rec}}_{K}(p)$ leads to a song\u2019s recommendation for context $p$ at test time. In turn, being ranked in position $K+1$ does not yield any recommendations. Instead, the probability mass in the tails is reallocated to the top $K$ songs at test time. This discontinuity can be exploited by the members of the collective to target specific contexts in a coordinated fashion to increase the likelihood of inclusion. Compared to random song placement, the collective can increase the mass on a particular context by a factor of $L$ . The InClust strategy implements a way for selecting contexts to target, projecting this intuition from the non-parametric setting to the embedding space of the recommender. Namely, it systematically places $s^{*}$ directly before each occurrence of a popular song $s_{0}$ . In that way, it targets the region in the context embedding space around $h_{s_{0}}$ in a coordinated fashion. To implement this strategy, the collective repeatedly determines the most frequent song in their playlists, places $s^{*}$ before every occurrence of this song, and then repeats this with the remaining playlists until all of them are used. ", "page_idx": 5}, {"type": "text", "text": "Strategically exploiting overrepresentation. An alternative lever the collective has available is to strategically target contexts that are overrepresented among the playlists the collective controls. Meaning that the frequency of the context among the playlists owned by the collective is larger than the overall frequency in the training data due to finite sample artifacts. The DirLoF strategy aims to identify such contexts by targeting infrequently occurring songs and exploiting the long-tail nature of the overall song frequency distribution (see Figure 2). The core intuition is that if they manage to target low-frequency contexts, a single song placement might be sufficient to overpower existing signals. To identify low-frequency contexts, the collective uses the frequency of the last song as a proxy. For each playlist, it selects the anchor songs $s_{0}$ with the smallest overall song frequency and places $s^{*}$ right after $s_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Obtaining song statistics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The InClust strategy targets high-frequency contexts, whereas the DirLoF strategy targets lowfrequency contexts. However, there is an important difference when implementing the strategies. InClust can be implemented from only statistics obtained from the songs in playlists the collective owns; all it requires is participants to set up infrastructure for pooling this information, either through an app, an online service, or other means. In contrast, to effectively implement the DirLoF strategy, the collective needs statistics about the full training data to identify the songs that are least popular overall. However, they typically do not have direct access to this information. Instead, as a proxy, they can leverage publicly available user-generated playlists, which are often accessible through official APIs (e.g., Spotify). Additionally, scraping external data sources can provide supplementary information. We evaluate the use of scraped song streams in Section 4.2. ", "page_idx": 5}, {"type": "image", "img_path": "wGjSbaMsop/tmp/4f24c6bc35f776cfc8ce870bb0db195b52d6d8d7b0ea7970396d4f6afa3b7724.jpg", "img_caption": ["Figure 4: Success of our collective action strategies. For tiny collectives DirLoF achieves an amplification of up to $25\\times$ while uncoordinated strategies (Random, AtTheEnd) are mostly ineffective. For larger collectives, InClust outperforms DirLoF. Amplification significantly exceeds 1, implying a disproportional test-time effect due to targeted song placement. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Empirical evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our collective action strategies against a public version of Deezer\u2019s transformers-based APC solution that \u201chas been deployed to all users\u201d [7, p. 472]. To train the model, we use the Spotify Million Playlist Dataset (MPD), which is currently the largest public dataset for APC [14]. It contains one million playlists generated by US Spotify users between 2010 and 2017, with an average length of 66.35 tracks from nearly 300,000 unique artists. ", "page_idx": 6}, {"type": "text", "text": "Model training and evaluation. We use the standard methodologies used in APC for model training and testing.2We start by randomly selecting 20,000 playlists to build a test and validation set of equal sizes. The remaining 980,000 playlists are used for training the model. The collective intervenes by strategically modifying an $\\alpha$ fraction of the playlists composing the training and validation set. We consider collectives of size $\\alpha\\,\\in\\,[0.00001,\\mathrm{~0.02}]$ which corresponds to 10 to 20000 playlists. For evaluation on the test set, every playlist $p$ is split into a seed context and a masked end. The length of the seed context is chosen randomly in [1, 10] for each playlist and models are evaluated by comparing the model\u2019s recommendations based on the seed playlist to the masked ground truth. We employ five-fold cross-validation, using different random seeds for sampling the playlists designated for training, validation, and testing, as well as for selecting the subset controlled by the collective. We use bootstrapped $95\\%$ confidence intervals (CI) over folds when reporting results. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We consider four baseline strategies to compare with our collective strategies, each performing the same number of target song insertions. The Random strategy inserts $s^{*}$ at a Random position in the playlist, Insert $\\mathfrak{Q}i$ inserts the song always at position $i$ in every playlist, the AtTheEnd strategy places $s^{*}$ as the last song of the playlist, and $\\mathtt{R a n d o m@i-j}$ inserts $s^{*}$ at a random position between indices $i$ and $j$ . Unlike our collective strategies, these baselines do not require coordination among participants beyond the shared goal of promoting $s^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "4.1 Success of collective action ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We start by evaluating the success of the proposed strategies, assuming full information about song frequencies in the training set to illustrate the potential. In Figure 4, we plot the amplification for different $\\alpha$ . In particular, we observe that strategic song placement allows very small collectives $(\\alpha\\leq0.1\\%)$ to be successful, whereas Random or fixed placement of $s^{*}$ is ineffective. ", "page_idx": 6}, {"type": "text", "text": "For $\\alpha=0.025\\%$ , the DirLoF strategy achieves amplification of up to 25. In contrast to an average song that naturally occurs in $0.025\\%$ of the playlists, the number of recommendations is $40\\times$ larger, as these low-frequency songs are typically ignored by the recommender. This suggests that collective action could make a tremendous difference for these artists: suppose an artist\u2019s song is streamed ", "page_idx": 6}, {"type": "image", "img_path": "wGjSbaMsop/tmp/85cfa23a32a0c4a8a185bfacd4dc4144120179e4956b73df2ff3595fe3974f56.jpg", "img_caption": ["Figure 5: Information bottleneck. The empirical amplification of the DirLoF strategy decreases with worse song statistics but scraped song streaming counts can serve as a practical solution. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "10,000 times, yielding a revenue of $\\mathbb{S}40$ at a royalty rate of $80.004\\$ per stream [31]; an amplification of 25 would hypothetically increase this revenue to $\\mathbb{S}1$ , 000. While this example is purely illustrative (as actual royalties depend on the platform and payment model used), it emphasizes the link between recommendations and potential revenue. ", "page_idx": 7}, {"type": "text", "text": "For collective sizes of $\\alpha\\:\\geq\\:0.1\\%$ the InClust starts being effective, as it has enough mass to effectively compete with existing signals associated with a cluster of similar context embeddings. As the strategy can target several such clusters at the same time, amplification increases with $\\alpha$ though with diminishing returns, achieving $\\mathrm{Amp}=10$ for $\\alpha\\approx2\\%$ . From Figure 1, we can see that in the regime of $2\\%$ training data frequency, a typical song enjoys an amplification of 1.8. ", "page_idx": 7}, {"type": "text", "text": "We also observe that the success of the random strategy increases with the collective size. This implies that even minimal coordination, in which members agree to all insert the same song $s^{*}$ , independent of the playlist they own, can already lead to significant amplification. Amplification values for the other baselines inserting $s^{*}$ at a fixed position are all close to 0 (see Table 1 in Appendix C.4). ", "page_idx": 7}, {"type": "text", "text": "Robustness to hyperparameters. Our strategies are designed based on a statistical intuition of sequential generation and should not be sensitive to specifics of the model architecture. We demonstrate the robustness with additional experiments where we vary the hyperparameters of the model (see Table 2 in Appendix C.5). However, the design of our strategies relies on the assumption that the model approximates the conditional probabilities in the training data sufficiently well. Accordingly, the effectiveness of the strategy decreases if model training is stopped early (see Table 3). ", "page_idx": 7}, {"type": "text", "text": "Hybrid strategy. Building on these observations, we construct a hybrid strategy that interleaves the two approaches by first using InClust to target indirect anchors that appear at least $\\lambda$ times in the collective and then deviates to DirLoF for playlists where no such anchor is present (we use $\\lambda=10$ ). This corresponds to the dashed line in Figure 4. We come back to this strategy in Section 4.3. ", "page_idx": 7}, {"type": "text", "text": "4.2 DirLoF strategy with approximate song statistics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The DirLoF strategy critically relies on training data song frequency estimates to determine the low-frequency anchor songs. We investigate the strategy\u2019s success with partially available song information in Figure 5. We find that if a collective of size $\\alpha=1\\%$ has access to $1\\%$ of the remaining training data they do not control, they can already achieve $\\approx30\\%$ of the amplification in the full information setting, with $10\\%$ of the data, it is $>50\\%$ of the achievable amplification. ", "page_idx": 7}, {"type": "text", "text": "By default, user-generated playlists on streaming platforms are often publicly accessible, enabling researchers to gather song frequency data through API calls. However, the amount of training data that can be aggregated is limited by the platform\u2019s API rate limits. Alternatively, proxy statistics can be used to increase the fraction of songs for which estimates are available. To illustrate the feasibility of this approach, we implemented a scraper to obtain current stream counts from Spotify. Although these counts are visible in the Spotify browser version, they are not accessible through the Spotify API. The scraped data reflects song popularity as of 2024, which is not ideal given our experiment relies on the much older Spotify MPD dataset (collected between 2010 and 2017). Nonetheless, these counts serve as effective proxies, as Figure 5 impressively shows. ", "page_idx": 7}, {"type": "image", "img_path": "wGjSbaMsop/tmp/77ea8e3f219ba67f0145a57ab0ac1076df7b0bca1e39fc17094cb9f21d431028.jpg", "img_caption": ["Figure 6: Effect of algorithmic collective action on recommendation performance. Performance loss relative to training on clean data for the hybrid / random strategies (solid lines), a conservative adversarial baseline (dashed lines), and an optimistic scenario where $s^{*}$ is treated as relevant (dotted lines). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Despite the temporal gap, a collective of size $\\alpha=1\\%$ can achieve over $85\\%$ of the amplification achievable in a full-information setting simply by using 2024 stream data to approximate past popularity levels. Even a smaller collective of $\\dot{\\alpha}=0.1\\%$ can reach about $50\\%$ of the amplification seen in the full-information scenario. In practice, scraped stream counts are likely to be more accurate proxies, as models are typically trained on more recent data. However, within the scope of our study, it remains impossible to access historical stream counts that would reflect popularity as of the time the playlists were originally generated. Thus our proof of concept should be seen as a lower bound. ", "page_idx": 8}, {"type": "text", "text": "4.3 Internalities and externalities of algorithmic collective action ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now inspect the effect of our strategies on other participants in the system, including the firm, other artists, and the members of the collective. For this investigation, we focus on the hybrid strategy. ", "page_idx": 8}, {"type": "text", "text": "First, we gauge the impact of collective action on the firm. This helps us understand the overall quality degradation of the service and the incentives of the firm to protect against collective action. We compare the performance under a recommender trained on the clean data and a recommender trained on the manipulated data. Figure 6 shows the corresponding loss in performance due to collective action for three different evaluation metrics. We find that our strategy (solid lines) only affects the recommender\u2019s performance marginally. We also show a conservative adversarial baseline (dashed lines), which simulates a scenario where successful collective action results in the first relevant item in playlist recommendations being replaced by the target song while leaving other recommendations unaltered. The considerably larger performance loss of this baseline indicates that our strategy only rarely affects relevant songs. Finally, as a thought experiment, consider $s^{*}$ as a relevant recommendation (dotted lines). Then, collective action even enhances the system\u2019s performance. This reference is meant to illustrate an optimistic scenario where collective action helps the recommender detect underrepresented but emerging and popular artists. ", "page_idx": 8}, {"type": "text", "text": "Second, we inspect the effect of collective action on other artists. To this end, Figure 7 depicts the change in recommendations for individual songs of different popularity. Songs are binned by frequency and the bars indicate variation across songs. The star shows the target song $s^{*}$ , and the corresponding increase in recommendations. We see that recommendations replaced by the target song seem to span songs of all popularity levels. In particular, our strategy does not harm specific songs or artists disproportionally and, as intended, has by far the largest effect on the targeted song $s^{*}$ . ", "page_idx": 8}, {"type": "text", "text": "Finally, we focus on the experience for participants who listen to the playlists. At training time our strategies are designed to only ask for minimal modifications with the goal to preserve user experience for members of the collective. We envision this to be an important factor for incentivizing participation in practice. Non-participating individuals are not affected at this stage. At test time, we find that user recommendations are largely preserved for both participating and non-participating individuals. More precisely, participating in collective action does not deteriorate the fraction of relevant songs participants get recommended, i.e., performance remains equivalent across all three recommendation quality metrics (see Figure 13 in Appendix C.6). ", "page_idx": 8}, {"type": "image", "img_path": "wGjSbaMsop/tmp/e427720f8e758a04918dd1d9247b7cbf7d6ab21a52dc0385df9d34d18d384812.jpg", "img_caption": ["Figure 7: Impact of collective action on other songs. We use $\\alpha=1\\%$ to obtain an upper bound on the effect. $\\Delta R$ denotes the change in the number of recommendations for a song due to collective action. Songs are sorted by their training set frequency and aggregated into 50 evenly spaced bins, whose means are represented by the blue dots with $95\\%$ CI. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work studies how collective action strategies can empower participants to exert a targeted influence on platform-deployed algorithms. By experimenting with an industry-scale transformerbased APC model, we demonstrate how strategically inserting a single song within randomly sampled playlists in the training data, can effectively increase recommendations of that song. Intriguingly, we find that the strategy only minimally interferes with service quality, and the recommendations for other users on the platform are largely preserved. ", "page_idx": 9}, {"type": "text", "text": "The proposed concept of participating in collective action to steer recommender system outcomes is grounded in the idea that users on online platforms should leave their digital traces more consciously. Thereby, their consumption behavior functions as a lever to reclaim some control over the data that platforms use to predict and recommend future content. Our emphasis on authenticity stands in clear contrast to adversarial machine learning techniques, which are often artificially designed and sometimes malicious in intent. ", "page_idx": 9}, {"type": "text", "text": "While altering a single playlist alone has little impact, the true power of algorithmic collective action lies in mobilizing a sufficiently large number of participants around a shared objective. This allows underrepresented artists to gain visibility through coordination. In our case, coordination corresponds to agreeing on a target song and an insertion procedure. The actual implementation of the strategy is possible with very limited technical skills and knowledge of the algorithm. We demonstrate how information for setting the parameters of the strategy can effectively be gathered using web scraping techniques. What we leave for future work is the actual implementation of an app to orchestrate collective action and share all the relevant information with the participants. ", "page_idx": 9}, {"type": "text", "text": "Our work suggests a widely unexplored design space for effective collective action strategies that differ from typical adversarial data poisoning attacks [c.f. 49, 62, 58, 59]. They can offer a powerful data lever to counter existing power imbalances [56, 57], and a community-centric approach to participatory AI [8]. Thus, understanding the role of economic power [23, 22], formalizing incentives [37], as well as quantifying long-term payoffs, dynamics, and equilibria, under collective action promises to be a fruitful direction for future work. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and potential for misuse ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Grounding algorithmic collective action means identifying both its opportunities and challenges. The power that arises from gaining control over the learning algorithm through collective action can also be abused by individuals controlling a substantial number of playlists. Instead of collective goals, these individuals could leverage similar methods to pursue individualistic goals, creating a different incentive structure and potentially posing a risk to the system. Similarly, popular artists could use our strategy to gain additional exposure and reinforce inequalities among artists. Thus, incentive structures will crucially determine the desirability of the resulting market outcome. Designing larger-scale collective action strategies that promote fairness and equity on online platforms as well as mechanisms that disincentivize malicious use remains a crucial open question. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Moritz Hardt for many insightful and formative discussions throughout the course of this work. We would also like to thank Mila Gorecki, Ricardo Dominguez-Olmedo, AnaAndreea Stoica and Andr\u00e9 Cruz for invaluable feedback on the manuscript, and Olawale Salaudeen, Florian Dorner, Stefania Ionescu and Tijana Zrnic for helpful feedback on earlier versions of this work. Celestine Mendler-D\u00fcnner acknowledges financial support from the Hector foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. Aguiar and J. Waldfogel. Platforms, power, and promotion: Evidence from spotify playlists. The Journal of Industrial Economics, 69(3):653\u2013691, 2021.   \n[2] C. Bauer. Allowing for equal opportunities for artists in music recommendation. In Proceedings of the 1st Workshop on Human-Centric Music Information Research Systems, pages 16\u201318, 2019.   \n[3] C. Bauer. Report on the ISMIR 2020 special session: how do we help artists? ACM SIGIR Forum, 54(2), 2020.   \n[4] C. Bauer, M. Kholodylo, and C. Strauss. Music recommender systems: challenges and opportunities for non-superstar artists. In 30th Bled eConference, pages 21\u201332, 2017.   \n[5] O. Ben-Porat and M. Tennenholtz. A game-theoretic approach to recommendation systems with strategic content providers. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[6] W. Bendada, T. Bontempelli, M. Morlon, B. Chapus, T. Cador, T. Bouab\u00e7a, and G. Salha-Galvan. Track Mix Generation on Music Streaming Services Using Transformers. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 112\u2013115, 2023.   \n[7] W. Bendada, G. Salha-Galvan, T. Bouab\u00e7a, and T. Cazenave. A Scalable Framework for Automatic Playlist Continuation on Music Streaming Services. In International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 464\u2013474, 2023.   \n[8] A. Birhane, W. Isaac, V. Prabhakaran, M. Diaz, M. C. Elish, I. Gabriel, and S. Mohamed. Power to the people? opportunities and challenges for participatory ai. In ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, 2022. [9] E. Blake. Data shows 90 percent of streams go to the top 1 percent of artists, 2020. https: //www.rollingstone.com/pro/news/top-1-percent-streaming-1055005.   \n[10] R. Burke. Multisided fairness for recommendation. ArXiv preprint arXiv:1707.00093, 2017.   \n[11] \u00d2. Celma. Music Recommendation and Discovery: The Long Tail, Long Fail, and Long Play in the Digital Music Space. Springer Berlin, Heidelberg, 2010.   \n[12] S. H. Cen, A. Ilyas, J. Allen, H. Li, D. Rand, and A. Madry. Measuring strategization in recommendation: Users adapt their behavior to shape future content. Arxiv preprint arXiv:2405.05596, 2023.   \n[13] S. H. Cen, A. Ilyas, and A. Madry. User strategization and trustworthy algorithms. ArXiv preprint arXiv:2312.17666, 2023.   \n[14] C.-W. Chen, P. Lamere, M. Schedl, and H. Zamani. Recsys Challenge 2018: Automatic Music Playlist Continuation. In ACM Conference on Recommender Systems, pages 527\u2013528, 2018.   \n[15] M. P. Coelho and J. Z. Mendes. Digital music and the \u201cdeath of the long tail\u201d. Journal of Business Research, 101:454\u2013460, 2019.   \n[16] S. Craw, B. Horsburgh, and S. Massie. Music recommendation: Audio neighbourhoods to discover music in the long tail. In Case-Based Reasoning Research and Development, pages 73\u201387. Springer International Publishing, 2015.   \n[17] E. Creager and R. Zemel. Online algorithmic recourse by collective action. ICML Workshop on Algorithmic Recourse, 2023.   \n[18] M. Fang, J. Liu, M. Momma, and Y. Sun. Fairroad: Achieving fairness for recommender systems with optimized antidote data. In ACM Symposium on Access Control Models and Technologies, page 173\u2013184, 2022.   \n[19] A. Ferraro, X. Serra, and C. Bauer. What is fair? exploring the artists\u2019 perspective on the fairness of music streaming platforms. In Human-Computer Interaction, volume 12933, pages 562\u2013584, 2021.   \n[20] I. Gunes, C. Kaleli, A. Bilge, and H. Polat. Shilling attacks against recommender systems: a comprehensive survey. Artificial Intelligence Review, 42:767\u2013799, 2014.   \n[21] C. Hansen, C. Hansen, L. Maystre, R. Mehrotra, B. Brost, F. Tomasi, and M. Lalmas. Contextual and sequential user embeddings for large-scale music recommendation. In ACM Conference on Recommender Systems, page 53\u201362, 2020.   \n[22] M. Hardt and C. Mendler-D\u00fcnner. Performative prediction: Past and future. ArXiv preprint arXiv:2310.16608, 2023.   \n[23] M. Hardt, M. Jagadeesan, and C. Mendler-D\u00fcnner. Performative Power. In Advances in Neural Information Processing Systems, 2022.   \n[24] M. Hardt, E. Mazumdar, C. Mendler-D\u00fcnner, and T. Zrnic. Algorithmic Collective Action in Machine Learning. In International Conference on Machine Learning, volume 202, pages 12570\u201312586, 2023.   \n[25] A. Haupt, D. Hadfield-Menell, and C. Podimata. Recommending to strategic users. ArXiv preprint arXiv:2302.06559, 2023.   \n[26] S. Ionescu, A. Hannak, and N. Pagan. Group fairness for content creators: the role of human and algorithmic biases under popularity-based recommendations. In ACM Conference on Recommender Systems, page 863\u2013870, 2023.   \n[27] M. Jagadeesan, M. I. Jordan, and N. Haghtalab. Competition, alignment, and equilibria in digital marketplaces. AAAI Conference on Artificial Intelligence, 37(5):5689\u20135696, 2023.   \n[28] D. Jannach, L. Lerche, F. Gedikli, and G. Bonnin. What recommenders recommend \u2013 an analysis of accuracy, popularity, and sales diversity effects. In User Modeling, Adaptation, and Personalization, pages 25\u201337. Springer Berlin Heidelberg, 2013.   \n[29] M. H. Jarrahi and W. Sutherland. Algorithmic management and algorithmic competencies: Understanding and appropriating algorithms in gig work. In Information in Contemporary Society, pages 578\u2013589, 2019.   \n[30] V. I. Levenshtein. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10:707, 1966.   \n[31] L. Marshall. \u2018let\u2019s keep music special. f\u2014spotify\u2019: on-demand streaming and the controversy over artist royalties. Creative Industries Journal, 8(2):177\u2013189, 2015.   \n[32] R. Mehrotra, J. McInerney, H. Bouchard, M. Lalmas, and F. Diaz. Towards a fair marketplace: Counterfactual evaluation of the trade-off between relevance, fairness & satisfaction in recommendation systems. In ACM International Conference on Information and Knowledge Management, page 2243\u20132251, 2018.   \n[33] M. Mladenov, E. Creager, O. Ben-Porat, K. Swersky, R. Zemel, and C. Boutilier. Optimizing long-term social welfare in recommender systems: a constrained matching approach. In International Conference on Machine Learning, 2020.   \n[34] D. Moor, Y. Yuan, R. Mehrotra, Z. Dai, and M. Lalmas. Exploiting sequential music preferences via optimisation-based sequencing. In ACM International Conference on Information and Knowledge Management, page 4759\u20134765, 2023.   \n[35] P. M. Napoli. Requiem for the long tail: Towards a political economy of content aggregation and fragmentation. International Journal of Media & Cultural Politics, 12(3):341\u2013356, 2016.   \n[36] S. Oh, B. Ustun, J. McAuley, and S. Kumar. Rank list sensitivity of recommender systems to interaction perturbations. In ACM International Conference on Information & Knowledge Management, page 1584\u20131594, 2022.   \n[37] M. Olson. The logic of collective action: public goods and the theory of groups. Harvard University Press, 1965.   \n[38] M. O\u2019Mahony, N. Hurley, N. Kushmerick, and G. Silvestre. Collaborative recommendation: A robustness analysis. ACM Trans. Internet Technol., 4(4):344\u2013377, 2004.   \n[39] N. Pagan, J. Baumann, E. Elokda, G. De Pasquale, S. Bolognani, and A. Hann\u00e1k. A classification of feedback loops and their relation to biases in automated decision-making systems. In ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, 2023.   \n[40] L. Porcaro, E. G\u00f3mez, and C. Castillo. Assessing the impact of music recommendation diversity on listeners: A longitudinal study. ACM Trans. Recomm. Syst., 2(1), 2024.   \n[41] R. Prey, M. Esteve Del Valle, and L. Zwerwer. Platform pop: disentangling spotify\u2019s intermediary role in the music industry. Information, Communication & Society, 25(1):74\u201392, 2022.   \n[42] B. Rastegarpanah, K. P. Gummadi, and M. Crovella. Fighting fire with fire: Using antidote data to improve polarization and fairness of recommender systems. In ACM International Conference on Web Search and Data Mining, page 231\u2013239, 2019.   \n[43] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[44] S. Shan, J. Cryan, E. Wenger, H. Zheng, R. Hanocka, and B. Y. Zhao. Glaze: Protecting artists from style mimicry by Text-to-Image models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 2187\u20132204, 2023.   \n[45] M. Si and Q. Li. Shilling attacks against collaborative recommender systems: a review. Artificial Intelligence Review, 53:291\u2013319, 2020.   \n[46] D. Sigg, M. Hardt, and C. Mendler-D\u00fcnner. Decline now: A combinatorial model for algorithmic collective action. ArXiv preprint arXiv:2410.12633, 2024.   \n[47] A. P. Sundar, F. Li, X. Zou, T. Gao, and E. D. Russomanno. Understanding shilling attacks and their detection traits: A comprehensive survey. IEEE Access, 8:171703\u2013171715, 2020.   \n[48] D. Thain, T. Tannenbaum, and M. Livny. Distributed computing in practice: the condor experience. Concurrency and Computation: Practice and Experience, 17(2-4):323\u2013356, 2005.   \n[49] Z. Tian, L. Cui, J. Liang, and S. Yu. A comprehensive survey on poisoning attacks and countermeasures in machine learning. ACM Comput. Surv., 55(8), 2022.   \n[50] T. Tofalvy and J. Koltai. \u201cSplendid Isolation\u201d: The reproduction of music industry inequalities in Spotify\u2019s recommendation system. New Media & Society, 25(7):1580\u20131604, 2023.   \n[51] F. Tomasi, J. Cauteruccio, S. Kanoria, K. Ciosek, M. Rinaldi, and Z. Dai. Automatic music playlist generation via simulation-based reinforcement learning. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 4948\u20134957, 2023.   \n[52] C. Toxtli and S. Savage. Designing AI Tools to Address Power Imbalances in Digital Labor Platforms, pages 121\u2013137. Springer International Publishing, 2023.   \n[53] D. Turnbull, L. Barrington, and G. Lanckriet. Five approaches to collecting tags for music. In ISMIR, 2008.   \n[54] Union of Musicians and Allied Workers. Justice at Spotify, mar 2021. URL https:// weareumaw.org/justice-at-spotify.   \n[55] N. Vincent and B. Hecht. Can \"Conscious Data Contribution\" Help Users to Exert \"Data Leverage\" Against Technology Companies? Proc. ACM Hum.-Comput. Interact., 5, 2021.   \n[56] N. Vincent, B. Hecht, and S. Sen. \u201cData Strikes\u201d: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies. In The World Wide Web Conference, pages 1931\u20131943, 2019.   \n[57] N. Vincent, H. Li, N. Tilly, S. Chancellor, and B. Hecht. Data Leverage: A Framework for Empowering the Public in Its Relationship with Technology Companies. In ACM Conference on Fairness, Accountability, and Transparency, pages 215\u2013227, 2021.   \n[58] Z. Yue, Z. He, H. Zeng, and J. McAuley. Black-box attacks on sequential recommenders via data-free model extraction. In ACM Conference on Recommender Systems, page 44\u201354, 2021.   \n[59] Z. Yue, H. Zeng, Z. Kou, L. Shang, and D. Wang. Defending substitution-based proflie pollution attacks on sequential recommenders. In ACM Conference on Recommender Systems, page 59\u201370, 2022.   \n[60] H. Zamani, M. Schedl, P. Lamere, and C.-W. Chen. An Analysis of Approaches Taken in the ACM RecSys Challenge 2018 for Automatic Music Playlist Continuation. ACM Trans. Intell. Syst. Technol., 10(5), 2019.   \n[61] M. Zehlike, K. Yang, and J. Stoyanovich. Fairness in ranking, part II: Learning-to-rank and recommender systems. ACM Comput. Surv., 55(6), 2022.   \n[62] H. Zhang, Y. Li, B. Ding, and J. Gao. Practical data poisoning attack against next-item recommendation. In The Web Conference 2020, page 2458\u20132464, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Song recommendation inequality ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 8 visualizes the track-level distribution of algorithmic exposure with the cumulative share of recommendations (y-axis) plotted against the percentiles of tracks (x-axis). The recommendations are derived from the Deezer model [7] on the Spotify MPD dataset [14]. More precisely, they are based on the outputs generated for a random selection of 10,000 seed playlists for testing, produced by a model that has been trained on the remainder of the dataset, without any collective action\u2014see Section 4 for more details. Similar to the artist-based Lorenz curve in Figure 2, we observe a very high level of inequality with a Gini coefficient of 0.8 (measuring the gap between the line of equality and the Lorenz curve). ", "page_idx": 14}, {"type": "image", "img_path": "wGjSbaMsop/tmp/42bbd9079384ff932a621af943ede1e9fd2e448b6decdbc78f594a3dc1819bb6.jpg", "img_caption": ["Figure 8: The Lorenz curve shows the unequal distribution of recommendations across tracks: $80\\%$ of all recommendations are concentrated among just $15\\%$ of tracks. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All experiments were run as jobs submitted to a centralized cluster, using the open-source HTCondor job scheduler [48]. All jobs utilized the same computing resources: For data preprocessing and performing the data modifications as per a strategic collective action, 1 CPU was used with an allocated 100GB of RAM. In a subsequent step, transformer models were trained using a single NVIDIA A100-SXM4-80GB GPU. For each job, data preprocessing takes roughly 1-2 hours to complete (with coordinated strategies taking longer than uncoordinated ones). Models are trained for 18 epochs, using the optimal hyperparameters provided by Bendada et al. [7], which takes roughly 6 hours. ", "page_idx": 14}, {"type": "text", "text": "A total of 1195 experiments were run: We investigated 10 strategies (including 6 DirLoF strategies with varying levels of song statistics knowledge and 8 simple baselines, as well as 6 different hyperparameter configurations), across 12 collective sizes $(\\alpha)$ , and an additional baseline without collective action $\\left[\\alpha\\right.\\mathrm{~=~}0]$ ) as a reference for the main experiments. For the ablation study, 6 strategies were tested over $13~\\alpha$ values. Each experiment was conducted with five folds using different random seeds. This resulted in approximately 2390 CPU hours and 7170 GPU hours of total compute usage. The complete code is available at https://github.com/joebaumann/ recsys-collectiveaction. ", "page_idx": 14}, {"type": "text", "text": "C Additional experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Ablation study for InClust strategy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We perform an additional empirical investigation to provide insights into the inner workings of the strategy. In particular, the effect of strategic positioning using indirect anchors on the attention function. Recall that the recommender is trained such that $\\mathrm{SI}\\bar{\\mathrm{M}}(s,p)\\,=\\,\\langle h_{s_{0}},g\\left(p\\right)\\rangle$ is large for songs $s$ that frequently follow context $p$ in the training data, and small otherwise. The embedding function $\\phi$ (described in Section 2.1) is insensitive to the song ordering within playlists, and strategic positioning will only surface on the attention function $g$ . ", "page_idx": 14}, {"type": "image", "img_path": "wGjSbaMsop/tmp/d242b1c2137e21b636dc312a6bddac1e3e44c7ce2dfa785c3d5ba94754b42947.jpg", "img_caption": ["Figure 9: Similarities of context embeddings for indirect anchor songs $(s_{0},s_{1}$ , and $s_{2}$ ) and the target song $(s^{*})$ for $\\alpha=1.1\\%$ . Dashed purple lines represent average thresholds to be among the top 50 most similar songs. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "For illustration, we use three indirect anchors corresponding to the three songs that occur most frequently in the playlists of the collective, denoted $s_{0},\\,s_{1}$ , and $s_{2}$ . We compare InClust, where we insert the target song $s^{*}$ before these songs, with Random, where we add $s^{*}$ in the same playlists but at random positions. This ensures we have the same pretrained song embeddings across the two strategies. Playlists that do not contain any of those three songs are not manipulated. We use $p_{s_{i}^{-}}$ to denote the context targeted by using $s_{i}$ as an indirect anchor. ", "page_idx": 15}, {"type": "text", "text": "In Figure 9, we visualize the similarity scores of the songs $(s^{*},s_{0},s_{1}$ , and $s_{2}$ ) with the three context clusters that have been targeted. More specifically, for every anchor song $s_{i}$ , we use all seed contexts $p_{s_{i}^{-}}$ that have been targeted in the training data and compare the similarity score of these contexts with (yes) and without (no) collective action. This results in a distribution over scores, as visualized by the violin plot. The left and the middle panel show that the similarity scores of the anchor songs and their associated contexts are not altered for either of the two strategies (Random and InClust). Furthermore, as can be seen in the right panel, the InCLust strategy is very effective in getting the target song to be among the top $50\\,\\mathrm{most}$ similar tracks for the targeted contexts (corresponding to the probability mass above the purple threshold). In contrast, for the random placement that does not specifically target these contexts, $s^{*}$ generally fails to achieve a ranking in the top 50. ", "page_idx": 15}, {"type": "text", "text": "Targetting multiple context clusters. The InCLust strategy is effective on all three distinct context clusters that are targeted simultaneously with a single target song $s^{*}$ . We confirm the effectiveness of this strategy by assessing its success with respect to a test set, which contains a randomly drawn set of playlists (each split into a seed context and a ground truth) that have not been seen during training. Figure 10 shows that for any number of indirect anchors, the InClust strategy significantly outperforms the Random placement strategy. Furthermore, it also clearly shows that it is possible to effectively target multiple context clusters using the same target song. In conclusion, it is possible to compete with several context clusters simultaneously. ", "page_idx": 15}, {"type": "text", "text": "Overall, this ablation indicates that inserting a single song in a subset of playlists can be effective in associating $s^{*}$ with specific contexts while preserving recommendations for songs $\\boldsymbol{S}\\backslash\\boldsymbol{s}^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "C.2 Information bottleneck ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 11 displays the 100 targeted direct anchors for $\\alpha=0.01\\%$ under different levels of knowledge about the song frequencies in the training set. Less information results in the selection of more frequent songs, as the gap between the estimated probability (relative occurrences of songs in the known fraction of the dataset or estimated using external information) and the true probability (relative occurrences of songs in the entire training data) widens. This difference provides insight into the reduced amplification observed in Figure 5 for a specific value of $\\alpha$ . ", "page_idx": 15}, {"type": "image", "img_path": "wGjSbaMsop/tmp/f2fde301911d165865cd9f54654765de82fae5f45ea9a03e5a4bb3b08460420a.jpg", "img_caption": ["Figure 10: Success with respect to the number of used indirect anchors (IA), in random or coordinated fashion. Each dot or triangle corresponds to a separate training run. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "wGjSbaMsop/tmp/743f59747d71b59d1e6027574619a2568f9d548b2fd98587788dfc93db6f9c3c.jpg", "img_caption": ["Figure 11: Estimated and true probabilities of targeted direct anchors with limited information about training set frequencies $\\alpha=\\bar{0}.01\\%$ ). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 Songs targeted by different strategies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 12 illustrates the anchor song selection for three distinct strategies. The InClust strategy identifies anchors based on their prevalence within the collective, targeting songs frequently listened to by its members. Highlighted in red in the left panel of Figure 12, this method repeatedly employs indirect anchors, relying solely on internal playlist statistics without needing broader song frequency insights. Conversely, the DirLoF strategy targets a specific cluster of direct anchors (resulting in the red-colored cluster of anchors in the bottom left corner of the middle panel in Figure 12) targeting each only once. This method requires external data to ensure the disproportionately represented anchors in the collective match those less prevalent in the training data, as shown on the $\\mathbf{X}$ -axis. The Hybrid strategy, illustrated in the right panel of Figure 12, combines these approaches, targeting both frequently occurring songs within the collective and low-frequency anchors. The parameter $\\lambda$ governs the fraction of the collective targeting indirect anchors (as in the left panel) versus targeting direct anchors (as in the middle panel). Larger $\\lambda$ values result in a larger share of direct anchors being targeted, and smaller values result in keeping more of the frequently targeted indirect anchors, i.e., the large red integers visualized toward the top of the left panel. Note that all strategies target several anchors, ensuring all playlists can be used effectively. ", "page_idx": 16}, {"type": "image", "img_path": "wGjSbaMsop/tmp/bcbebdef8a3268f632503afee34e4cc9b3afe8ebbdb7c5275cb81c8f7712e52c.jpg", "img_caption": ["Figure 12: Songs in playlists controlled by a collective composed of $0.07\\%$ of the training data. Blue dots are songs that are not targeted. Red integers indicate a used anchor song and the number of times it is targeted. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "wGjSbaMsop/tmp/f41aef681ffbbb916384dd06656adc66dd4596c69c83aef308a347c3ec28f7b5.jpg", "table_caption": ["Table 1: Mean Amplification (Std Dev) for additional baseline strategies that do not require coordination. Insert ${\\boldsymbol{\\mathfrak{Q}}}{\\boldsymbol{\\mathit{i}}}$ denotes the insertion of $s^{*}$ at index $i$ and Random@i- $\\cdot j$ denotes the insertion of $s^{*}$ at a random index between $i$ and $j$ . The best and second-best performing strategies are highlighted in bold and underlined, respectively. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "wGjSbaMsop/tmp/ce5137f0e53550ec50bc552b4ec02751fdc55807272a8b52e216bfd78abae6c8.jpg", "table_caption": ["Table 2: Mean Amplification (Std Dev) under different hp configurations $(\\alpha=0.001)$ ). $\\mathrm{\\bfhp^{*}}$ denotes the optimal set of hp reported by Bendada et al. [7] with 8 attention heads (n_heads), a learning rate $({\\tt1r})$ of 1.0, a dropout rate (drop_p) of 0.13, a weight decay (wd) of 1.53e-05, and 18 epochs. We experiment with five alternative hp configurations that are equivalent to $\\mathfrak{h p}^{*}$ except for the following changes: hp1 sets n_heads $=4$ , hp2 sets $\\tt{1r}{=}0.5$ , hp3 sets $\\mathtt{d r o p\\_p=}0.2$ , hp4 sets $\\mathtt{w d=}5\\mathrm{e-}05$ , and hp5 makes all four of these changes. The highest amplification values among hp configurations are highlighted in bold. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "C.4 Additional baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 1 demonstrates that simple baselines, which insert $s^{*}$ at a fixed position, are much less effective than a random insertion. Additionally, inserting $s^{*}$ at random within the first 10 positions of any playlist is much worse than a random insertion at any position. ", "page_idx": 17}, {"type": "text", "text": "C.5 Robustness of collective strategies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 2 shows that the DirLoF strategy is robust against hyperparameter (hp) changes. It consistently outperforms the Random strategy across all configurations. Finally, Table 3 illustrates the effectiveness of DirLoF and Random across different numbers of training epochs for the recommender. Notice that the effectiveness of the DirLoF strategy decreases if model training is stopped early. ", "page_idx": 17}, {"type": "text", "text": "C.6 Internalities and externalities of algorithmic collective action ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we provide more details on the results presented in Section 4.3. ", "page_idx": 17}, {"type": "text", "text": "Metrics for model accuracy. To assess the quality of recommendations we follow Chen et al. [14] and Bendada et al. [7] in using the following three popular performance metrics: R-precision measures the fraction of recommended items present in the masked ground truth, augmented by artist matches. The Normalized Discounted Cumulative Gain (NDCG) measures the ranking quality by rewarding relevant tracks placed higher in the recommendation list. The number of clicks (#C) quantifies how many batches of ten song recommendations (starting with top candidates) are needed to find one relevant track. The Deezer model trained on the unmanipulated data achieves comparable results to the winning solutions of the RecSys 2018 APC challenge along these metrics [60]. ", "page_idx": 17}, {"type": "table", "img_path": "wGjSbaMsop/tmp/e1c11b3521a79b02ba41ef3ba9d28b64a2a6d7fef9d2f37125e03be68d3e6ebb.jpg", "table_caption": ["Table 3: Mean Amplification (Std Dev) for relative to trained epochs $(\\alpha\\,=\\,0.001)$ ). The bestperforming strategies are highlighted in bold. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "wGjSbaMsop/tmp/9b72b93bd28be41e6dd8d462d9b56ea13475305c59f9383b79632ad85514fed0.jpg", "table_caption": ["Table 4: Mean $(\\pm\\,95\\%$ CI) recommendations without collective action $(R_{0})$ , total gained recommendations $(\\Delta R)$ , and gained recommendations in $\\%$ of $R_{0}$ (considering songs that are recommended at least once without collective action) for direct anchors, indirect anchors and others over five folds. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Impact on other artists. Unlike the partially aligned interests of the firm and the collective, the dynamics among artists differ, since boosting recommendations for one artist inevitably reduces the visibility of others. We are interested in understanding who is affected by our strategy. For the purpose of this analysis, we hold the total number of recommendations at inference time constant, making it a zero-sum game. ", "page_idx": 18}, {"type": "text", "text": "To complement Figure 7, in Table 4, we show the effect of collective action (hybrid strategy with $\\alpha=1\\%$ ) on other songs. In line with the result presented in Section 4.3 (showing bins of songs with similar frequencies for just one fold), we do not find any evidence that any other songs experience a systematic change in exposure due to collective action, not even the targeted (in)direct anchor songs. ", "page_idx": 18}, {"type": "text", "text": "User experience of collective participants Collective participants are platform users who continue consuming content on the platform during and after performing strategic actions. To understand the price of collective action, we investigate the downstream effect on their own user experience. After all, users are likely to engage in collective action only if it does not result in significant detriment to their own content consumption experience. While a perfect measure of user satisfaction is outside the scope of this work, we utilize participant\u2019s known preferences as a basis for the experienced quality of recommendations. More precisely, for any collective participant that manipulated their playlist with $h(p)=(p_{i^{*}}^{-},s^{*},p_{i^{*}}^{+})$ , we use the targeted context $\\bar{(p_{i^{*}}^{-})}$ as a seed for the recommender and evaluate the output recommendations using the user-generated continuation of the playlist $(p_{i^{*}}^{+})$ as the ground truth. ", "page_idx": 18}, {"type": "text", "text": "We revisit the collective strategy outlined in the ablation study (Section C.1) to scrutinize the internalities of collective action. We find that the recommendations for collective participants are stable and robust under authentic strategic playlist manipulations. Figure 13 illustrates the precision score\u2014 measuring the similarity of recommendations for different strategies\u2014across attacked contexts. It is around $80\\%$ on average, with little variation across strategies. This is likely attributed to inherent instabilities in the top $K$ recommendations rather than the specifics of the strategies [36]. ", "page_idx": 18}, {"type": "image", "img_path": "wGjSbaMsop/tmp/734a3e2a70e6398d3cc2e8e2722058f27a7e24316676f4ebe4fbc22619e43078.jpg", "img_caption": ["Figure 13: Variation in top 50 song predictions for attacked contexts: Predictions are stable under different collective action strategies. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "wGjSbaMsop/tmp/4df0aa52820b787030a886fd7d63367a66dd89cabe62fbba2e04b9a80479a5b2.jpg", "table_caption": ["Table 5: Average model performance: Predictions are robust under different collective action strategies. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Furthermore, by participating in collective action, users do not affect the variety of songs they get recommended. This stability of model predictions despite the coordinated collective action is shown in Table 5: Inserting $s^{*}$ between $p_{i^{*}}^{-}$ and $p_{i^{*}}^{+}$ does not significantly distort the recommenders ability to predict $p_{i^{*}}^{+}$ from $p_{i^{*}}^{-}$ . Interestingly, random placement of songs reduces the performance slightly more (especially for #C), as in this case, $s^{*}$ can be inserted within the context $p_{i^{*}}^{-}$ by pure chance. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The claims made in the abstract and in the introduction are shown in detail in Section 4. The scope outlined in the abstract and in the introduction corresponds to the framework described in Section 3. Furthermore, Section 1.1 accurately reflects the paper\u2019s contributions. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The limitations of our work are discussed in Section 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The experimental methodology is clearly described in Section 4 and all steps of the experiments are clearly outlined, ensuring the reproducibility of the presented results. Furthermore, the experimental setup is described in detail in Section B. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide open access to the code here: https://github.com/ joebaumann/recsys-collectiveaction. The data used for the experiments is publicly available and referenced in the paper. Furthermore, the hardware setup and the detailed experimental setup are described in Section B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All the training and test details (e.g., data splits, hyperparameters, etc.) are described in Sections 4 and B as well as in the open access code repository. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All experiments were run several times with different seeds and we report error bars (where appropriate) along with a description throughout the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide detailed information on the used computer resources in Section B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The underlying goal of this research is a positive societal impact as clearly described in the introduction as well as in Sections 2.2 and 3. Broader impacts (such as unintended uses) are additionally discussed in Section 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All assets used for this research are properly credited and the licenses are respected. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]