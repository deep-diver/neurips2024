[{"type": "text", "text": "An efficient search-and-score algorithm for ancestral graphs using multivariate information scores ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose a greedy search-and-score algorithm for ancestral graphs, which in  \n2 clude directed as well as bidirected edges, originating from unobserved latent   \n3 variables. The normalized likelihood score of ancestral graphs is estimated in terms   \n4 of multivariate information over relevant subsets of vertices, $_{C}$ , that are connected   \n5 through collider paths confined to the ancestor set of $_{C}$ . For computational effi  \n6 ciency, the proposed two-step algorithm relies on local information scores limited   \n7 to the close surrounding vertices of each node (step 1) and edge (step 2). This   \n8 computational strategy is shown to outperform state-of-the-art causal discovery   \n9 methods on challenging benchmark datasets. ", "page_idx": 0}, {"type": "text", "text": "10 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "11 The likelihood function plays a central role in the selection of a graphical model $\\mathcal{G}$ based on   \n12 observational data $\\mathcal{D}$ . Given $N$ independent samples from $\\mathcal{D}$ , the likelihood $\\mathcal{L}_{\\mathcal{D}\\mid\\mathcal{G}}$ that they might   \n13 have been generated by the graphical model $\\mathcal{G}$ is given by [1], ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}|\\mathcal{G}}=\\frac{1}{Z_{\\mathcal{D},\\mathcal{G}}}\\exp\\left(-N H(p,q)\\right)\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "14 where $\\begin{array}{r}{H(p,q)=-\\sum_{\\mathbfit{x}}p(\\mathbfit{x})\\log q(\\mathbf{x})}\\end{array}$ is the cross-entropy between the empirical probability distribu  \n15 tion $p(x)$ of the observed data $\\mathcal{D}$ and the theoretical probability distribution $q(x)$ of the model $\\mathcal{G}$ and   \n16 $Z_{D,\\mathcal{G}}$ a data- and model-dependent factor ensuring proper normalization condition for finite dataset. In   \n17 short, Eq.1 results from the asymptotic probability that the $N$ independent samples, $\\pmb{x}^{(1)},\\cdot\\cdot\\cdot,\\pmb{x}^{(N)}$ ,   \n18 are drawn from the model distribution, $q(x)$ , i.e. $\\begin{array}{r}{\\mathcal{L}_{\\mathcal{D}|\\mathcal{G}}\\equiv q(\\pmb{x}^{(1)},\\cdot\\cdot\\cdot\\cdot,\\pmb{x}^{(N)})\\stackrel{\\cdot}{=}\\prod_{i}q(\\pmb{x}^{(i)})}\\end{array}$ , rather   \n19 than the empirical distribution, $p(x)$ . This leads to, $\\begin{array}{r}{\\log\\mathcal{L}_{\\mathcal{D}|\\mathcal{G}}=\\sum_{i}\\log q(\\mathbf{\\boldsymbol{x}}^{(i)})}\\end{array}$ , which converges   \n20 towards $\\begin{array}{r}{N\\sum_{x}p({\\pmb x})\\log q({\\pmb x})~=~-N\\,H(p,q)}\\end{array}$ in the large sample size limit, $N\\ \\rightarrow\\ \\infty$ , with   \n21 $\\log Z_{\\mathcal{D},\\mathcal{G}}=\\mathcal{O}(\\log N)$ .   \n22 The structural constraints of the model $\\mathcal{G}$ translate into the factorization form of the theoretical   \n23 probability distribution, $q(x)$ [2\u20136]. In particular, the probability distribution of Bayesian networks   \n24 (BN) factorizes in terms of conditional probabilities of each variable given its parents, as $q_{\\mathrm{BN}}(x)=$   \n25 $\\textstyle\\prod_{i}q(x_{i}|\\mathbf{p^{a}}_{X_{i}})$ , where $\\mathbf{pa}_{X_{i}}$ denote the values of the parents of node $X_{i}$ in $\\mathcal{G}$ , $\\mathbf{Pa}_{X_{i}}$ . For Bayesian   \n26 networks, the factors of the model distribution, $q(x_{i}|\\mathbf{pa}_{X_{i}})$ , can be directly estimated with the   \n27 empirical conditional probabilities of each node given its parents as, $q(x_{i}|\\mathbf{\\dot{p}a}_{X_{i}})\\,\\equiv\\,p(x_{i}|\\mathbf{pa}_{X_{i}})$ ,   \n28 leading to the well known estimation of the likelihood function in terms of conditional entropies   \n29 $\\begin{array}{r}{H(X_{i}|\\mathbf{Pa}_{X_{i}})=-\\sum_{x}p(x_{i},\\mathbf{pa}_{X_{i}})\\log p(x_{i}|\\mathbf{pa}_{X_{i}})}\\end{array}$ , ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}|\\mathcal{G}_{\\mathrm{BN}}}\\!=\\frac{1}{Z_{\\mathcal{D},\\mathcal{G}_{\\mathrm{BN}}}}\\:\\mathrm{exp}\\left(-\\:N{\\sum_{X_{i}\\in V}^{\\mathrm{vertices}}}H(X_{i}|\\mathbf{P}\\mathbf{a}_{X_{i}})\\right)\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "30 This paper concerns the experimental setting for which some variables of the underlying Bayesian   \n31 model are not observed. This frequently occurs in practice for many applications. We derive an   \n32 explicit likelihood function for the class of ancestral graphs, which include directed as well as   \n33 bidirected edges, arising from the presence of unobserved latent variables. Tian and Pearl 2002 [7]   \n34 showed that the probability distribution of such graphs factorizes into c-components including subsets   \n35 of variables connected through bidirected paths (i.e. containing only bidirected edges). Richardson   \n36 2009 [6] later proposed a refined factorization of the model distribution of the broader class of acyclic   \n37 directed mixed graphs in terms of conditional probabilities over \u201chead\u201d and \u201ctail\u201d subsets of variables   \n38 within each ancestrally closed subsets of vertices. However, unlike with Bayesian networks, the   \n39 contributions of c-components or head-and-tail factors to the likelihood function cannot simply be   \n40 estimated in terms of empirical distribution $p(x)$ , as shown below. This leaves the likelihood function   \n41 of ancestral graphs difficult to estimate from empirical data, in general, although iterative methods   \n42 have been developped when the data is normally distributed [8\u201313].   \n43 The present paper provides an explicit decomposition of the likelihood function of ancestral graphs   \n44 in terms of multivariate cross-information over relevant $\\because a c$ -connected\u2019 subsets of variables, Figs. 1.,   \n45 which do not rely on the head-and-tail factorization but coincide with the parametrizing sets [14]   \n46 derived from the head-and-tail factorization. It suggests a natural estimation of these revelant   \n47 contributions to the likelihood function in terms of empirical distribution $p(x)$ . This result extends   \n48 the likelihood expression of Bayesian Networks (Eq. 2) to include the effect of unobserved latent   \n49 variables and enables the implementation of a greedy search-and-score algorithm for ancestral graphs.   \n50 For computational efficiency, the proposed two-step algorithm relies on local information scores   \n51 limited to the close surrounding vertices of each node (step 1) and edge (step 2). This computational   \n52 strategy is shown to outperform state-of-the-art causal discovery methods on challenging benchmark   \n53 datasets. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "54 2 Theoretical results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "55 2.1 Multivariate cross-entropy and cross-information ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "56 The theoretical result of the paper (Theorem 1) is expressed in terms of multivariate cross-information   \n57 derived from multivariate cross-entropies through the Inclusion-Exclusion Principle. The same   \n58 expressions can be written between multivariate information and multivariate entropies by simply   \n59 substituting $q(\\{x_{i}\\})$ with $p(\\{x_{i}\\})$ in the equations below and will be used to estimate the likelihood   \n60 function of ancestral graphs (Proposition 3). ", "page_idx": 1}, {"type": "text", "text": "61 As recalled above, the cross-entropy between $m$ variables, $V=\\{X_{1},\\cdot\\cdot\\cdot,X_{m}\\}$ , is defined as, ", "page_idx": 1}, {"type": "equation", "text": "$$\nH(V)\\;=\\;-\\sum_{\\{x_{i}\\}}p(x_{1},\\cdot\\cdot\\cdot\\;,x_{m})\\log q(x_{1},\\cdot\\cdot\\cdot\\;,x_{m})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "$^{62}_{{63}}$ where $p(\\{x_{i}\\})$ is the empirical joint probability distribution of the variables $\\{X_{i}\\}$ and $q(\\{x_{i}\\})$ the 64 joint probability distribution of the model. Bayes formula, $q(\\{x_{i}\\},\\{y_{j}\\})=q(\\{\\dot{x_{i}}\\}|\\{y_{j}\\})\\,\\bar{q}(\\{y_{j}\\})$ , 65 directly translates into the definition of conditional cross-entropy through the decomposition, ", "page_idx": 1}, {"type": "equation", "text": "$$\nH(\\{X_{i}\\},\\{Y_{j}\\})=H(\\{X_{i}\\}|\\{Y_{j}\\})+H(\\{Y_{j}\\})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "66 Multivariate (cross) information, $\\boldsymbol{I}(\\boldsymbol{V})\\equiv\\boldsymbol{I}(X_{1};\\cdot\\cdot\\cdot;X_{m})$ , are defined from multivariate (cross)   \n67 entropies through Inclusion-Exclusion formulas over all subsets of variables [15\u201318] as, ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r l}{I(X)}&{=}&{H(X)}\\\\ {I(X;Y)}&{=}&{H(X)+H(Y)-H(X,Y)}\\\\ {I(X;Y;Z)}&{=}&{H(X)+H(Y)+H(Z)-H(X,Y)-H(X,Z)-H(Y,Z)+H(X,Y,Z)}\\\\ {I(V)}&{=}&{-\\displaystyle\\sum_{S\\subseteq V}(-1)^{|S|}H(S)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "68 where the semicolon separators are needed to distinguish multipoint (cross) information from joint   \n69 variables as $\\{X,Z\\}$ in $\\bar{I(\\{X,Z\\};Y)}=I(X;Y)\\!+\\!I\\bar{(Z;Y)}\\!-\\!I(\\bar{X};Y;Z)$ . Below, implicit separators   \n70 between non-conditioning variables in multivariate (cross) information will always correspond to   \n71 semicolons, e.g. as in $I(V)$ in Eq. 5. Unlike multivariate (cross) entropies, which are always positive,   \n72 $H(X_{1},\\cdot\\cdot\\cdot,X_{k})\\geqslant0$ , multivariate (cross) information, $I(X_{1};\\cdot\\cdot\\cdot;X_{k})$ , can be positive or negative   \n73 for $k\\geqslant3$ , while they remain always positive for $k<3$ , i.e. $I(X;Y)\\geqslant0$ and $\\bar{I(X)}\\geqslant0$ .   \n74 In turn, multivariate (cross) entropies can be expressed through the Principle of Inclusion-Exclusion   \n75 into the same expression form but in terms of multivariate (cross) information, ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{{\\cal H}(V)}}&{{=}}&{{-\\displaystyle\\sum_{S\\subseteq V}(-1)^{|S|}{\\cal I}(S),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "76 Conditional multivariate (cross) information $I(V|Z)$ are defined similarly as multivariate (cross)   \n77 information $I(V)$ but in terms of conditional (cross) entropies as, ", "page_idx": 2}, {"type": "equation", "text": "$$\nI(V|Z)\\;\\;=\\;\\;-\\sum_{S\\subseteq V}(-1)^{|S|}H(S|Z)\\,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "78 Eqs. 5 & 7 lead to a decomposition rule relative to a variable $Z$ , Eq. 8, which can be conditioned   \n79 on a set of joint variables, $\\mathbf{\\bar{A}}=\\{A_{1},\\cdots,A_{m}\\}$ , with implicit comma separators for conditioning   \n80 variables in Eq. 9, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{I(V)\\;=\\;I(V|Z)+I(V;Z)}}\\\\ {{I(V|A)\\;=\\;I(V|Z,A)+I(V;Z|A)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "81 Alternatively, conditional (cross) information, such as $I(X;Y|A)$ , can be expressed in terms of   \n82 non-conditional (cross) entropies using Eq. 4, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I(X;Y|A)\\!\\!\\!}&{=\\,H(X|A)+H(Y|A)-H(X,Y|A)}\\\\ {\\!\\!\\!}&{=\\,H(X,A)+H(Y,A)-H(X,Y,A)-H(A)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "83 which can in turn be expressed in terms of non-conditional (cross) information as, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{^{\\cdot}(X;Y|A)=I(X;Y)-\\cdots(-1)^{k}\\displaystyle\\sum_{i_{1}<\\cdots<i_{k}}I(X;Y;A_{i_{1}};\\cdots;A_{i_{k}})+\\cdots(-1)^{m}I(X;Y;A_{1};\\cdots;A_{m})}}\\\\ {{\\qquad\\qquad\\qquad\\qquad x,\\displaystyle\\sum_{s^{\\prime}\\subset S}\\displaystyle(-1)^{|s^{\\prime}|}I(S^{\\prime}),}}\\\\ {{\\qquad\\qquad\\qquad\\qquad\\qquad(11)\\displaystyle\\sum_{s^{\\prime}\\subset S}\\displaystyle(11)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "84 where $S=\\{X,Y\\}\\cup A$ . This corresponds, up to an opposite sign, to all (cross) information terms   \n85 including both $X$ and $Y$ in the expression of the multivariate (cross) entropy, $H(X,Y,A)$ , Eq. 6. ", "page_idx": 2}, {"type": "text", "text": "86 2.2 Graphs and connection criteria ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "87 2.2.1 Directed mixed graphs and ancestral graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "88 Two vertices are said to be adjacent if there is an edge (of any type) between them, $X\\!*\\!\\!-*Y$ , where   \n89 $^*$ stands for any (head or tail) end mark. $X$ and $Y$ are said to be neighbors if $X-Y$ , parent and   \n90 child if $X\\rightarrow Y$ and spouses if $X\\longleftrightarrow Y$ in $\\mathcal{G}$ .   \n91 A path in $\\mathcal{G}$ is a sequence of distinct vertices $V_{1},\\ldots,V_{n}$ consecutively adjacent in $\\mathcal{G}$ , as,   \n92 $V_{1}{*}\\!\\!\\!-\\!\\!\\!{*}V_{2}{*}\\!\\!\\!-\\!\\!\\!{*}\\cdots{*}\\!\\!\\!-\\!\\!\\!-\\!\\!\\!{*}V_{n-1}{*}\\!\\!\\!-\\!\\!\\!-\\!\\!\\!{*}V_{n}$ . In particular, a collider path between $V_{1}$ and $V_{n}$ has the form   \n93 $V_{1}*\\to V_{2}\\longleftrightarrow\\cdots\\longleftrightarrow V_{n-1}\\longleftrightarrow\\psi_{n}$ and a directed path corresponds to $V_{1}\\to V_{2}\\to\\cdots\\to V_{n}$ .   \n94 $X$ is called an ancestor of $Y$ and $Y$ a descendant of $X$ if $X=Y$ or there is a directed path from   \n95 $X$ to $Y$ , $X\\rightarrow\\cdot\\cdot\\cdot\\rightarrow Y$ . $\\mathbf{An}_{\\mathcal{G}}(Y)$ denotes the set of ancestors of $Y$ in $\\mathcal{G}$ . By extension, for any   \n96 subset of vertices, $C\\subseteq V$ , $\\mathbf{An}_{\\mathcal{G}}(C)$ denotes the set of ancestors for all $Y\\!\\in\\!C$ in $\\mathcal{G}$ .   \n97 A directed mixed graph is a vertex-edge graph ${\\mathcal{G}}=(V,E)$ that can contain two types of edges:   \n98 directed $(\\rightarrow)$ and bidirected $(\\longleftrightarrow)$ edges.   \n99 A directed cycle occurs in $\\mathcal{G}$ when $X\\in\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(Y)$ and $X\\leftarrow Y$ . An almost directed cycle occurs   \n100 when $X\\in\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(Y)$ and $X\\longleftrightarrow Y$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "101 Definition 1. An ancestral graph is a directed mixed graph: ", "page_idx": 2}, {"type": "text", "text": "102 $i)$ without directed cycles;   \n103 $i i,$ without almost directed cycles.   \n104 An ancestral graph is said to be maximal if every missing edge corresponds to a structural indepen  \n105 dence. If an ancestral graph $\\mathcal{G}$ is not maximal, there exists a unique maximal ancestral graph $\\bar{\\mathcal G}$ by   \n106 adding bidirected edges to $\\mathcal{G}$ [8].   \n108 Let us now define ancestor collider connecting paths or ac-connecting paths, which entail simpler   \n109 path connecting criterion than the traditional m-connecting criterion, discussed in the Appendix A.   \n110 Yet, ac-connecting paths and ac-connected subsets will turn out to be directly relevant to character  \n111 ize the likelihood decomposition and Markov equivalent classes of ancestral graphs.   \n112 Definition 2. [ac-connecting path] An $a c$ -connecting path between $X$ and $Y$ given a subset of   \n113 variables $_{C}$ (possibly including $X$ and $Y$ ) is a collider path, $X*\\to Z_{1}\\longleftrightarrow\\cdots\\longleftrightarrow Z_{K}\\longleftarrow*Y$ ,   \n114 with all $Z_{i}\\in{\\mathbf{\\bar{A}}}{\\mathbf{n}}_{\\mathcal{G}}(\\{X,Y\\}\\cup\\bar{C})$ , that is, with $Z_{i}$ in $_{C}$ or connected to $\\{X,Y\\}\\cup C$ by an ancestor   \n115 path, i.e. $Z_{i}\\to\\cdot\\cdot\\cdot\\to T$ with $T\\in\\{X,Y\\}\\cup C$ .   \n116 Definition 3. [ac-connected subset] A subset $_{C}$ is said to be $a c$ -connected if \u2200X $,Y\\in C$ , $X$ and   \n117 $Y$ are connected (through any type of edge) or there is an $a c$ -connecting path between $X$ and $Y$   \n118 given $_{C}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "119 2.3 Likelihood decomposition of ancestral graphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "120 Theorem 1. [likelihood of ancestral graphs] The cross-entropy $H(p,q)$ and likelihood $\\mathcal{L}_{\\mathcal{D}\\mid\\mathcal{G}}$ of an   \n121 ancestral graph $\\mathcal{G}$ is decomposable in terms of multivariate cross-information, $I(C)$ , summed over   \n122 all ac-connected subsets of variables, $_{C}$ (Definition 3), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{\\mathrm{ac-connected}}}\\\\ {{}}&{{}}&{{\\displaystyle=\\ -\\sum_{C\\subseteq V}\\ (-1)^{|C|}I(C)}}\\\\ {{}}&{{}}&{{\\mathcal{L}_{{D|G}}\\ =\\ \\displaystyle\\frac{1}{Z_{{D,\\mathcal{G}}}}\\exp\\left(N\\ \\sum_{C\\subseteq V}\\ (-1)^{|C|}I(C)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "123 where $N$ is the number of iid samples in the dataset $\\mathcal{D}$ and $Z_{\\mathcal{D},\\mathcal{G}}$ a data- and model-dependent   \n124 normalization constant.   \n125 The proof of Theorem 1 is left to the Appendix B. It is based on a partition of the cross-entropy (Eq. 6)   \n126 into cross-information contributions from $a c$ -connected and non- $a c\\cdot$ -connected subsets of variables,   \n127 which do not rely on head-and-tail factorizations. Hu and Evans [14] proposed an equivalent result   \n128 (Proposition 3.3 in [14]) with a proof using head-and-tail decomposition to define parametrizing   \n129 sets, which happen to coincide with the $a c$ -connected sets defined here (Definition 3). Theorem 1   \n130 characterizes in particular the Markov equivalence class of ancestral graphs [8, 19\u201324] as,   \n131 Corollary 2. Two ancestral graphs are Markov equivalent if and only if they have the same ac  \n132 connected subsets of vertices.   \n133 Note, in particular, that Eq. 12 holds for maximal ancestral graphs (MAG), for which all pairs of   \n134 ac-connected variables are connected by an edge, and their Markov equivalent representatives, the   \n135 partial ancestral graphs (PAG) [8, 25\u201327].   \n136 Proposition 3. The likelihood decomposition of ancestral graphs (Eq. 12, Theorem 1) can be   \n137 estimated by replacing the model distribution $q$ by the empirical distribution $p$ in the retained   \n138 multivariate cross-information terms $I(C)$ corresponding to all $a c$ -connected subsets of variables, $_{C}$ .   \n139 Hence, Proposition 3 amounts to estimating all relevant cross-information terms in the likelihood   \n140 function with the corresponding multivariate information terms computed from the available data,   \n141 while assuming by construction that the model distribution obeys all local and global conditional inde  \n142 pendences entailed by the ancestral graph. The corresponding factorization of the model distribution   \n143 can be expressed in terms of empirical distribution, assuming positive distributions, see Appendix C.   \n144 Fig. 1 illustrates the cross-entropy decomposition for a few graphical models in terms of cross  \n145 information contributions from their $a c$ -connected subsets of vertices. In particular, an unshielded   \n146 non-collider (e.g. $X\\rightarrow Z\\rightarrow W$ , Fig. 1A), is less likely (i.e. higher cross-entropy) than an unshielded   \n147 collider or \u2018v-structure\u2019 (e.g. $X\\rightarrow Z\\leftarrow W$ , Fig. 1B), if the corresponding three-point information   \n148 term is negative, $I(X;Z;W)<0$ , in agreement with earlier results [28, 29]. However, this early   \n149 approach, exploiting the sign and magnitude of three-point information to orient v-structures, does   \n150 not include higher order terms involving multiple v-structures, which can lead to orientation confilcts   \n151 between unshielded triples, in practice. Resolving such orientation conflicts requires to include   \n152 information contributions from higher-order ac-connected subgraphs, such as star-like $a c$ -connected   \n153 subsets including three or more parents, Fig. 1C. Similarly, the cross-entropies of collider paths   \n154 involving several colliders also include higher-order terms, as with the simple example of a two  \n155 collider path, Fig. 1D. By contrast, the cross-entropy based on the head-and-tail factorization of the   \n156 same two-collider path, i.e. $q(x,z,y,w)=q(z,{\\bar{y|}}{\\bar{x}},w)q(x)q(w)$ [6], is found to be equivalent to   \n157 the cross-entropy of a Bayesian graph without bidirected edge, Fig. 1E, when estimated with the   \n158 empirical distribution $p(.)$ , see Appendix C. This observation illustrates the difficulty to estimate the   \n159 likelihood functions of ancestral graphs using head-and-tail factorization.   \n160 Further examples of graphical models, Figs. 1F-I, show the relative simplicity of the decomposition   \n161 with only few (non-trivial) $a c$ -connected contributing subsets $_{C}$ with $|C|\\geqslant3$ , as compared to   \n162 the much larger number of non- ${\\it a c}$ -connected non-contributing subsets, that cancel each other by   \n163 construction due to conditional independence constraints of the underlying model. Note, in particular,   \n164 that most contributing multivariate information $I(C)$ only concern direct connections or collider   \n165 paths within a single component subgraph induced by $_{C}$ (solid line edges in Fig. 1). However,   \n166 occasionally, collider paths extending beyond $_{C}$ into $\\mathbf{A}\\dot{\\mathbf{n}}_{\\mathcal{G}}(C)\\setminus C$ (marked with wiggly edges) with   \n167 corresponding ancestor path(s) (marked with dashed edges) do occur, as shown in Fig. 1G.   \n168 In addition, the present information-theoretic decomposition of the likelihood of ancestral graphs   \n169 can readily distinguish their Markov equivalence classes according to Corollary 2. For instance, the   \n170 ancestral graphs of Fig. 1F and Fig. 1G, despite sharing the same edges and the same unshielded   \n171 collider $\\langle X\\rightarrow Z\\leftarrow T\\rangle$ ), turn out not to be Markov equivalent, as discussed in [24]. Indeed, their   \n172 cross-entropy decompositions differ by two $a c$ -connected contributing terms: a three-point cross   \n173 information $I(X;Y;T)$ with a collider path not confined in $_{C}$ (i.e. $X\\,\\sim\\,Z\\,\\Longleftrightarrow\\,T\\,\\longleftrightarrow\\,Y$ and   \n174 corresponding ancestor path $Z\\ \\--\\ \\ Y)$ and a four-point information term $I(X;Y;Z;T)$ due to   \n175 the two-collider path $(X\\to Z\\longleftrightarrow T\\longleftrightarrow Y)$ ). More quantitatively, it shows that the graph of   \n176 Fig. 1G with a two-collider path is more likely than the graph of Fig. 1F whenever $I(X;\\bar{Y};\\bar{T})\\;-$   \n177 $I(X;Y;Z;T)=I(X;Y;T\\vert Z)=I(X;Y\\vert Z)-I(X;Y\\vert Z,T)<0$ . Finally, the Markov equivalent   \n178 graphs of Fig. 1H and Fig. 1I, also due to [24], illustrate the fact that the actual ancestor collider path   \n179 between unconnected pairs does not need to be unique nor conserved between Markov equivalent   \n180 graphs (as long as their cross-entropies share the same multivariate cross-information decomposition). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "Z2f4Laqi8U/tmp/8b9b902cda0af7b70d3115427e15e9f30e3783a48aee38163cfc1647a617fecf.jpg", "img_caption": ["Figure 1: Cross-entropy decomposition of ancestral graphs. Examples of cross-entropy decomposition of ancestral graphs (red edges, lhs) in terms of relevant multivariate cross-information contributions $I(C)$ with $C\\subseteq V$ (red nodes, rhs). Simple graphs: (A) without unshielded colliders, $(\\mathbf{B})$ with a single or non-overlapping unshielded colliders, $(\\pmb{\\complement})$ with overlapping unshielded colliders through three or more (conditionally) independent parents or $({\\bf D})$ through a two-(or more)-collider path. (E) Bayesian graph corresponding to the head-and-tail factorization of the two-collider path in (D) estimated using the empirical distribution $p(.)$ , see Appendix C. (F) Simple Bayesian graph not Markov equivalent to an ancestral graph (G) sharing the same edges and unshielded collider [24]. Solid black edges correspond to direct connections or collider paths confined to the corresponding $a c$ -connected subset $_{c}$ , while wiggly edges indicate collider paths extending beyond $_{c}$ yet indirectly connected to $_{c}$ by an ancestor path, marked with dashed edges, see Definition 2. By contrast, graphs H and I illustrate the fact that collider paths may not be unique nor conserved between two Markov equivalent graphs (i.e. sharing the same cross-information terms) [24]. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "181 3 Efficient search-and-score causal discovery using local information scores ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "182 The likelihood estimation of ancestral graphs (Theorem 1 and Proposition 3) enables the implemen  \n183 tation of a search-and-score algorithm for this broad class of graphs, which has attracted a number   \n184 of contributions recently [11\u201313, 30\u201332]. Our specific objective is not to develop an exact method   \n185 limited to simple graphical models with a few nodes and small datasets but to implement an efficient   \n186 and reliable heuristic method applicable to more challenging graphical models and large datasets.   \n187 Indeed, search-and-score structure learning methods need to rely on heuristic rather than exhaustive   \n188 search, in general, given that the number of ancestral graphs grows super-exponentially as the number   \n189 of vertices increases. This can be implemented for instance with a Monte Carlo algorithmic scheme   \n190 with random restarts, which efficiently probes relevant graphical models. Here, we opt, instead, to   \n191 use the prediction of an efficient hybrid causal discovery method, MIIC [29, 33, 34], as starting point   \n192 for a subsequent search-and-score approach based on the proposed likelihood estimation of ancestral   \n193 graphs (Eq. 12 and Proposition 3).   \n194 Moreover, while the likelihood decomposition of ancestral graphs may involve extended ac-connected   \n195 subsets of variables, as illustrated in Fig. 1, we aim to implement a computationally efficient search  \n196 and-score causal discovery method based on approximate local scores limited to the close surrounding   \n197 vertices of each node and edge. Yet, while MIIC only relies on unshielded triple scores, the novel   \n198 search-and-score extension, MIIC_search&score, uses also higher-order local information scores to   \n199 compare alternative subgraphs, as detailed below.   \n200 The proposed method is shown to outperform MIIC and other state-of-the-art causal discovery   \n201 methods on challenging datasets including latent variables. ", "page_idx": 5}, {"type": "text", "text": "202 3.1 MIIC, an hybrid causal discovery method based on unshielded triple scores ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "203 MIIC is an hybrid causal discovery method combining constraint-based and information-theoretic   \n204 frameworks [29, 35]. Unlike traditional constraint-based methods [4, 5], MIIC does not directly   \n205 attempt to uncover conditional independences but, instead, iteratively substracts the most significant   \n206 three-point (conditional) information contributions of successive contributors, $A_{1},\\,A_{2},\\,...,\\,A_{n}$ , from ", "page_idx": 5}, {"type": "text", "text": "207 the mutual information between each pair of variables, $I(X;Y)$ , as, ", "page_idx": 6}, {"type": "equation", "text": "$$\nI(X;Y)-I(X;Y;A_{1})-I(X;Y;A_{2}|A_{1})-\\cdot\\cdot-I(X;Y;A_{n}|\\{A_{i}\\}_{n-1})=I(X;Y|\\{A_{i}\\}_{n})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "208 where $I(X;Y;A_{k}|\\{A_{i}\\}_{k-1})\\;>\\;0$ is the positive information contribution from $A_{k}$ to $I(X;Y)$   \n209 [28, 36]. Conditional independence is eventually established when the residual conditional mutual   \n210 information on the right hand side of Eq. 13, $I(X;Y|\\{A_{i}\\}_{n})$ , becomes smaller than a complexity   \n211 term, i.e. $k_{X;Y|\\{A_{i}\\}}(\\bar{N})\\geqslant I(X;Y|\\{A_{i}\\}_{n})\\geqslant0$ , which dependents on the considered variables and   \n212 sample size $N$ .   \n213 This leads to an undirected skeleton, which MIIC then (partially) orients based on the sign and   \n214 amplitude of the regularized conditional 3-point information terms [28, 29]. In particular, negative   \n215 conditional 3-point information terms, $I(X;Y;Z|\\{A_{i}\\})\\!<\\!0$ , correspond to the signature of causality   \n216 in observational data [28] and lead to the prediction of a v-structure, $X\\rightarrow Z\\leftarrow Y$ , if $X$ and $Y$   \n217 are not connected in the skeleton. By contrast, a positive conditional 3-point information term,   \n218 $I(X;Y;Z|\\{A_{i}\\})\\!>\\!0$ , implies the absence of a v-structure and suggests to propagate the orientation   \n219 of a previously directed edge $X\\rightarrow Z-Y$ as $X\\rightarrow Z\\rightarrow Y$ .   \n220 In practice, MIIC\u2019s strategy to circumvent spurious conditional independences significantly improves   \n221 recall, that is, the fraction of correctly recovered edges, compared to traditional constraint-based   \n222 methods [28, 29]. Yet, MIIC only relies on unshielded triple scores to reliably uncover significant   \n223 contributors and orient v-structures, as outlined above. MIIC has been recently improved to ensure   \n224 the consistency of the separating set in terms of indirect paths in the final skeleton or (partially)   \n225 oriented graphs [37, 34] and to improve the reliably of predicted orientations [33, 34].   \n226 The predictions of this recent version of MIIC, which include three type of edges (directed, bidirected   \n227 and undirected), have been used as starting point for the subsequent local search-and-score method   \n228 implemented in the present paper. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "229 3.2 New search-and-score method based on higher-order local information scores ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "230 Starting from the structure predicted by MIIC, as detailed above, MIIC_search&score method   \n231 proceeds in two steps. ", "page_idx": 6}, {"type": "text", "text": "232 3.2.1 Step 1: Node scores for edge orientation priming and edge removal ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "233 The first step consists in minimizing a node score corresponding to the local normalized log likelihood   \n234 of each node w.r.t. its possible parents or spouses amongst the connected nodes predicted by MIIC.   \n235 To this end, the node score assesses the conditional entropy of each node w.r.t. a selection of   \n236 parents, spouses or neighbors, $\\mathbf{P}\\mathbf{a}_{x_{i}}^{\\prime}\\subseteq\\mathbf{P}\\mathbf{a}_{x_{i}}\\cup\\mathbf{S}\\mathbf{p}_{x_{i}}\\cup\\mathbf{N}\\mathbf{e}_{x_{i}}$ , and a factorized Normalized Maximum   \n237 Likelihood (fNML) regularization [28], see Appendix $\\mathrm{D}$ for details, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Score}_{\\mathrm{n}}(X_{i})=H(X_{i}|\\mathbf{Pa}_{{x}_{i}}^{\\prime})+\\frac{1}{N}\\sum_{j}^{q_{x}_{i}}\\log C_{n_{j}}^{r_{x_{i}}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "238 where $q_{x_{i}}$ corresponds to the combination of levels of $\\mathbf{Pa}_{x_{i}}^{\\prime},$ while $r_{x_{i}}$ is the number of levels of $X_{i}$ ,   \n239 and $n_{j}$ the number of samples corresponding to a particular combination of levels $j$ in each summand,   \n240 with $\\textstyle\\sum_{j}n_{j}=N$ , the total number of samples. $\\log{\\mathcal{C}}_{n_{j}}^{r_{x_{i}}}$ is the fNML regulatization cost summed   \n241 over all combinations of levels, $q_{x_{i}}$ , [38, 39], see Appendix D.   \n242 This first algorithm is looped over each node, priming the orientations of their surrounding edges (as   \n243 directed, bidirected or undirected), until convergence. Edges without orientation priming at either   \n244 extremity are removed at the end of Step 1. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "245 3.2.2 Step 2: Edge orientation scores ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "246 The second step consists in minimizing an edge orientation score corresponding to the local normal  \n247 ized log likelihood of each edge w.r.t. its nodes\u2019 parents and spouses inferred in Step 1. To this end,   \n248 the edge score assesses the conditional information and a fNML complexity cost with respect to the   \n249 type of orientation, given three sets of parents and spouses of $X$ and $Y$ , i.e. $\\mathbf{\\dot{P}a}_{\\scriptscriptstyle X\\!\\ Y}^{\\prime}=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X}\\!\\stackrel{\\!\\scriptscriptstyle\\mathbf{\\T}}{\\cup}\\!\\mathbf{S}\\mathbf{p}_{\\scriptscriptstyle X}\\!\\stackrel{\\!\\scriptscriptstyle\\mathbf{\\bigwedge}}{Y}$ ,   \n250 $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y\\!\\backslash X}^{\\prime}=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y}\\cup\\mathbf{S}\\mathbf{p}_{\\scriptscriptstyle Y}\\backslash X$ and $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X Y}^{\\prime}=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X\\backslash Y}^{\\prime}\\cup\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y\\backslash X}^{\\prime}$ with their corresponding combinations of   \n251 levels, $q_{y\\backslash x},q_{x\\backslash y}$ and $q_{x y}$ . These orientation scores, listed in Table 1, include symmetrized fNML com  \n252 plexity terms to enforce Markov equivalence, if $X$ and $Y$ share the same parents or spouses (excluding   \n253 $X$ and $Y$ ), see Appendix D. Indeed, all three scores become equals if $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle{Y\\!\\:\\backslash}X}^{\\prime}=\\mathbf{\\bar{P}}\\mathbf{a}_{\\scriptscriptstyle{X\\!\\:\\backslash}Y}^{\\prime}=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle{X Y}}^{\\prime}$   \n254 implying also the same combinations of parent and spouse levels, $q_{y\\backslash x}=q_{x\\backslash y}=q_{x y}$ .   \n255 This second algorithm is looped over each edge to compute an orientation score decrement, given the   \n256 orientations of its surrounding edges. The orientation change corresponding to the largest orientation   \n257 score decrement is then chosen at each iteration until convergence or until a limit cycle is reached   \n258 and stopped at the lowest sum of local orientation scores. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 1: Local scores for the orientation of a single directed or bidirected edge. ", "page_idx": 7}, {"type": "table", "img_path": "Z2f4Laqi8U/tmp/f61c1032f8e372e4b78c3bafada3f33c9f5abf30cc8fb6790bfb87dd7ff31344.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "259 4 Experimental results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "260 We first tested whether MIIC_search&score orientation scores (Table 1) effectively predicts bidirected   \n261 orientations on three simple ancestral models, Fig. 3, when the end nodes do not share the same   \n262 parents (Fig. 3, Model 1), share some parents (Fig. 3, Model 2) or when the bidirected edge is part of   \n263 a longer than two-collider paths (Fig. 3, Model 3). The prediction of the edge orientation scores are   \n264 summarized in Table 3, Appendix E, and show good predictions for large enough datasets.   \n265 Beyond these simple examples, focussing on the discovery of bidirected edges in small toy models   \n266 of ancestral graphs, we also analyzed more challenging benchmarks from the bnlearn repository   \n267 [40], Fig. 2. They concern ancestral graphs obtained by hiding up to $20\\%$ of variables in Bayesian   \n268 Networks of increasing complexity (number of nodes and parameters), such as Alarm (37 nodes, 46   \n269 links, 509 parameters), Insurance (27 nodes, 52 links, 984 parameters), and Barley (48 nodes, 84   \n270 links, 114,005 parameters). We then assessed causal discovery performance in terms of Precision,   \n271 $T P/(T P+F\\bar{P})$ , and Recall, $T P/(T P+F N)$ , relative to the theoretical PAGs, while counting as   \n272 false positive $(F P)$ , all correctly predicted edges but without or with a different orientation as the   \n273 directed or bidirected edges of the PAG.   \n274 Fig. 2 compares MIIC_search&score performance to MIIC results used as starting point for   \n275 MIIC_search&score and to FCI [41]. MIIC and MIIC_search&score settings were set as described   \n276 in section 3 above. The open-source MIIC R package (v1.5.2, GPL-3.0 license) was obtained at   \n277 https://github.com/miicTeam/miic_R_package. FCI from the python causal-learn package   \n278 (v0.1.3.8, MIT license) [41] was obtained at https://github.com/py-why/causal-learn and   \n279 run with $\\mathrm{G^{2}}$ -conditional independence test and default parameter $\\alpha=0.05$ .   \n280 Overall, MIIC_search&score is found to outperform MIIC in terms of edge precision with little to no   \n281 decrease in edge recall, Fig. 2, demonstrating the benefti of MIIC_search&score\u2019s rationale to improve   \n282 MIIC predictions by extending MIIC information scores from unshielded triples to higher-order   \n283 information contributions. These originate from $a c$ -connected subsets including nodes with more than   \n284 two parents or spouses, or ac-connected subsets including two-collider paths. MIIC_search&score is   \n285 also found to outperform FCI on complex ancestral benchmark networks with many parameters, such   \n286 as Barley (114,005 parameters), Fig. 2. However, FCI is found to reach similar or better precision   \n287 scores on easier benchmarks with fewer parameters (i.e. Alarm and Insurance), although its recall   \n288 remains usually lower than MIIC_search&score, especially at small sample size, as expected for a   \n289 purely constraint-based causal discovery approach.   \n290 Importantly, the benchmark PAGs used to score the causal discovery results with increasing propor  \n291 tions of latent variables, Fig. 2, include not only bidirected edges originating from hidden common   \n292 causes but also additional directed or undirected edges arising, in particular, from indirect effects of   \n293 hidden variables with observed parents. Irrespective of their orientations, all these additional edges   \n294 originating from indirect effects of hidden variables generally correspond to weaker effects (i.e. lower   \n295 mutual information of indirect effects due to the Data Processing Inequality) and are more difficult to   \n296 uncover than the edges of the original graphical model without hidden variables. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Z2f4Laqi8U/tmp/0d44e82b93d8dc7d12344b15be4e73326c9068bb48443b41f729e6e03866c21d.jpg", "img_caption": ["Figure 2: Benchmark results on ancestral graphs of increasing complexity. Benchmark results on ancestral graphs obtained by hiding $0\\%$ , $5\\%$ , $10\\%$ or $20\\%$ of variables in Bayesian Networks of increasing complexity (see main text): Alarm (lhs), Insurance (middle), and Barley (rhs). MIIC_search&score results are compared to MIIC results used as starting point for MIIC_search&score and FCI [41]. Causal discovery performance is assessed in terms of Precision and Recall relative to the theoretical PAGs, while counting as false positive all correctly predicted edges but without or with a different orientation as the directed or bidirected edges of the PAG. Error bars $(\\pm\\sigma)$ : standard deviations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "297 5 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "298 The main limitation of the paper concerns the local scores used in the search-and-score algorithm,   \n299 which are limited to ac-connected subsets of vertices with a maximum of two-collider paths.   \n300 While this approach could be extended to higher-order information contributions including three  \n301 or more collider paths, it allows for a simple two-step search-and-score scheme at the level of   \n302 individual nodes (step 1) and edges (step 2), as detailed in section 3. This already shows a significant   \n303 improvement in causal discovery performance (i.e. combing good precision and good recall on   \n304 challenging benchmarks) as compared to existing state-of-the-art methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "305 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "306 [1] D. Koller, N. Friedman, Probabilistic Graphical Models: Principles and Techniques (MIT Press, 2009).   \n307 [2] J. Pearl, A. Paz, Graphoids: A graph-based logic for reasoning about relevance relations, or when would x   \n308 tell you more about y if you already know z, Tech. rep., UCLA Computer Science Department (1985).   \n309 [3] J. Pearl, Probabilistic reasoning in intelligent systems (Morgan Kaufmann, San Mateo, CA, 1988).   \n310 [4] J. Pearl, Causality: models, reasoning and inference (Cambridge University Press, 2009), second edn.   \n311 [5] P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction, and Search (MIT press, , 2000), second edn.   \n312 [6] T. S. Richardson, A factorization criterion for acyclic directed mixed graphs, Proceedings of the Twenty  \n313 Fifth Conference on Uncertainty in Artificial Intelligence, UAI $^{\\circ9}$ (AUAI Press, Arlington, VA, USA,   \n314 2009), p. 462\u2013470.   \n315 [7] J. Tian, J. Pearl, A general identification condition for causal effects, Proceedings of the National Confer  \n316 ence on Artificial Intelligence (Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999,   \n317 2002), pp. 567\u2013573.   \n318 [8] T. Richardson, P. Spirtes, Ancestral graph markov models. Ann. Statist. 30, 962\u20131030 (2002).   \n319 [9] M. Drton, M. Eichler, T. S. Richardson, Computing maximum likelihood estimates in recursive linear   \n320 models with correlated errors. Journal of Machine Learning Research 10, 2329\u20132348 (2009).   \n321 [10] R. J. Evans, T. S. Richardson, Maximum likelihood fitting of acyclic directed mixed graphs to binary   \n322 data, Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI\u201910 (AUAI Press,   \n323 Corvallis, OR, USA, 2010).   \n324 [11] S. Triantafillou, I. Tsamardinos, Score-based vs constraint-based causal learning in the presence of   \n325 confounders, CFA@UAI (2016).   \n326 [12] K. Rantanen, A. Hyttinen, M. J\u00e4rvisalo, Maximal ancestral graph structure learning via exact search,   \n327 Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, C. de Campos,   \n328 M. H. Maathuis, eds. (PMLR, 2021), vol. 161 of Proceedings of Machine Learning Research, pp. 1237\u2013   \n329 1247.   \n330 [13] T. Claassen, I. G. Bucur, Greedy equivalence search in the presence of latent confounders, Proceedings of   \n331 the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, J. Cussens, K. Zhang, eds. (PMLR,   \n332 2022), vol. 180 of Proceedings of Machine Learning Research, pp. 443\u2013452.   \n333 [14] Z. Hu, R. Evans, Faster algorithms for markov equivalence, Proceedings of the 36th Conference on Uncer  \n334 tainty in Artificial Intelligence (UAI), J. Peters, D. Sontag, eds. (PMLR, 2020), vol. 124 of Proceedings of   \n335 Machine Learning Research, pp. 739\u2013748.   \n336 [15] W. J. McGill, Multivariate information transmission. Trans. of the IRE Professional Group on Information   \n337 Theory (TIT) 4, 93-111 (1954).   \n338 [16] H. K. Ting, On the amount of information. Theory Probab. Appl. 7, 439-447 (1962).   \n339 [17] T. S. Han, Multiple mutual informations and multiple interactions in frequency data. Information and   \n340 Control 46, 26-45 (1980).   \n341 [18] R. W. Yeung, A new outlook on shannon\u2019s information measures. IEEE transactions on information theory   \n342 37, 466\u2013474 (1991).   \n343 [19] P. Spirtes, T. Richardson, A polynomial time algorithm for determinint dag equivalence in the presence of   \n344 latent variables and selection bias, Proceedings of the 6th International Workshop on Artificial Intelligence   \n345 and Statistics (1996).   \n346 [20] T. Richardson, Markov properties for acyclic directed mixed graphs. Scandinavian Journal of Statistics 30,   \n347 145-157 (2003).   \n348 [21] R. A. Ali, T. S. Richardson, Markov equivalence classes for maximal ancestral graphs, Proceedings of the   \n349 Eighteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201902 (Morgan Kaufmann Publishers   \n350 Inc., San Francisco, CA, USA, 2002), pp. 1\u20139.   \n351 [22] R. A. Ali, T. S. Richardson, P. Spirtes, J. Zhang, Towards characterizing markov equivalence classes for   \n352 directed acyclic graphs with latent variables, Proceedings of the Fifteenth Conference on Uncertainty in   \n353 Artificial Intelligence, UAI\u201905 (Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2005).   \n354 [23] J. Tian, Generating markov equivalent maximal ancestral graphs by single edge replacement, Proceedings   \n355 of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201905 (Morgan Kaufmann Publishers   \n356 Inc., San Francisco, CA, USA, 2005).   \n357 [24] R. A. Ali, T. S. Richardson, P. Spirtes, Markov equivalence for ancestral graphs. Ann. Statist. 37, 2808\u20132837   \n358 (2009).   \n359 [25] T. Richardson, P. Spirtes, Scoring ancestral graph models, Tech. rep. (1999). Available as Technical Report   \n360 CMU-PHIL 98.   \n361 [26] J. Zhang, A characterization of markov equivalence classes for directed acyclic graphs with latent variables,   \n362 Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI\u201907 (Morgan   \n363 Kaufmann Publishers Inc., San Francisco, CA, USA, 2007).   \n364 [27] J. Zhang, On the completeness of orientation rules for causal discovery in the presence of latent confounders   \n365 and selection bias. Artif. Intell. 172, 1873-1896 (2008).   \n366 [28] S. Affeldt, H. Isambert, Robust reconstruction of causal graphical models based on conditional 2-point and   \n367 3-point information, Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,   \n368 UAI 2015, July 12-16, 2015, Amsterdam, The Netherlands (2015), pp. 42\u201351.   \n369 [29] L. Verny, N. Sella, S. Affeldt, P. P. Singh, H. Isambert, Learning causal networks with latent variables from   \n370 multivariate information in genomic data. PLoS Comput. Biol. 13, e1005662 (2017).   \n371 [30] B. Andrews, G. F. Cooper, T. S. Richardson, P. Spirtes, The m-connecting imset and factorization for admg   \n372 models, Preprint (2022). Arxiv 2207.08963.   \n373 [31] Z. Hu, R. J. Evans, Towards standard imsets for maximal ancestral graphs. Bernoulli 30 (2024).   \n374 [32] Z. Hu, R. Evans, A fast score-based search algorithm for maximal ancestral graphs using entropy, Preprint   \n375 (2024). Arxiv 2402.04777.   \n376 [33] V. Cabeli, H. Li, M. da C\u00e2mara Ribeiro-Dantas, F. Simon, H. Isambert, Reliable causal discovery based on   \n377 mutual information supremum principle for finite datasets, WHY21, 35rd Conference on Neural Information   \n378 Processing Systems (NeurIPS, 2021).   \n379 [34] M. d. C. Ribeiro-Dantas, H. Li, V. Cabeli, L. Dupuis, F. Simon, L. Hettal, A.-S. Hamy, H. Isambert,   \n380 Learning interpretable causal networks from very large datasets, application to 400, 000 medical records of   \n381 breast cancer patients. iScience 27, 109736 (2024).   \n382 [35] V. Cabeli, L. Verny, N. Sella, G. Uguzzoni, M. Verny, H. Isambert, Learning clinical networks from medical   \n383 records based on information estimates in mixed-type data. PLoS Comput. Biol. 16, e1007866 (2020).   \n384 [36] S. Affeldt, L. Verny, H. Isambert, 3off2: A network reconstruction algorithm based on 2-point and 3-point   \n385 information statistics. BMC Bioinformatics 17 (2016).   \n386 [37] H. Li, V. Cabeli, N. Sella, H. Isambert, Constraint-based causal structure learning with consistent separating   \n387 sets. Advances in Neural Information Processing Systems (NeurIPS) 32 (2019).   \n388 [38] P. Kontkanen, P. Myllym\u00e4ki, A linear-time algorithm for computing the multinomial stochastic complexity.   \n389 Inf. Process. Lett. 103, 227\u2013233 (2007).   \n390 [39] T. Roos, T. Silander, P. Kontkanen, P. Myllym\u00e4ki, Bayesian network structure learning using factorized   \n391 nml universal models, Proc. 2008 Information Theory and Applications Workshop (ITA-2008) (IEEE Press,   \n392 2008).   \n393 [40] M. Scutari, Learning Bayesian Networks with the bnlearn R Package. J. Stat. Softw. 35, 1\u201322 (2010).   \n394 [41] Y. Zheng, B. Huang, W. Chen, J. Ramsey, M. Gong, R. Cai, S. Shimizu, P. Spirtes, K. Zhang, Causal-learn:   \n395 Causal discovery in python. Journal of Machine Learning Research 25, 1\u20138 (2024).   \n396 [42] Y. M. Shtarkov, Universal sequential coding of single messages. Problems of Information Transmission 23,   \n397 3\u201317 (1987).   \n398 [43] J. Rissanen, I. Tabus, Adv. Min. Descrip. Length Theory Appl. (MIT Press, 2005), pp. 245\u2013264.   \n399 [44] W. Szpankowski, Average case analysis of algorithms on sequences (John Wiley & Sons, , 2001).   \n400 [45] P. Kontkanen, W. Buntine, P. Myllym\u00e4ki, J. Rissanen, H. Tirri, Efficient computation of stochastic   \n401 complexity. in: C. Bishop, B. Frey (Eds.) Proceedings of the Ninth International Conference on Artificial   \n402 Intelligence and Statistics, Society for Artificial Intelligence and Statistics 103, 233\u2013238 (2003).   \n403 [46] P. Kontkanen, Computationally efficient methods for mdl-optimal density estimation and data clustering,   \n404 Ph.D. thesis (2009).   \n405 [47] D. M. Chickering, A Transformational Characterization of Equivalent Bayesian Network Structures, UAI   \n406 \u201995: Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (Morgan   \n407 Kaufmann, 1995), pp. 87\u201398.   \n408 [48] M. Kalisch, M. M\u00e4chler, D. Colombo, M. H. Maathuis, P. B\u00fchlmann, Causal inference using graphical   \n409 models with the r package pcalg. J. Stat. Softw. 47, 1\u201326 (2012). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "410 Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "411 A Preliminaries: connection and separation criteria ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "412 A.1 $_m$ -connection vs $m^{\\star}$ -connection criteria ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "413 An ancestral graph can be interpreted as encoding a set of conditional indepencence relations by   \n414 a graphical criterion, called $m$ -separation, based on the concept of $m$ -connecting paths, which   \n415 generalizes the separation criteria of Markov and Bayesian networks to ancestral graphs.   \n416 Definition 4. $\\cdot m$ -connecting path] A path $\\pi$ between $X$ and $Y$ is $m$ -connecting given a (possibly   \n417 empty) subset $C\\subseteq V$ (with $X,Y\\notin C)$ if:   \n418 $i)$ its non-collider(s) are not in $_{C}$ , and   \n419 ii) its collider(s) are in $\\mathbf{An}_{\\mathcal{G}}(C)$ .   \n420 Definition 5. $[m$ -separation criterion] The subsets $\\pmb{A}$ and $_B$ are said to be $m$ -separated by $_{C}$ , noted   \n421 $A\\bot_{m}B|C$ , if there is no $m$ -connecting path between any vertex in $\\pmb{A}$ and any vertex in $_B$ given $_{C}$ .   \n422 The probabilistic interpretation of ancestral graph is given by its global and pairwise Markov properties   \n423 (which are equivalent [8]): if $\\pmb{A}$ and $_B$ are $m$ -separated by $_{C}$ , then $\\pmb{A}$ and $_B$ are conditionally   \n424 independent given $_{C}$ and $\\forall X\\ \\in\\ A$ and $\\forall Y\\,\\in\\,B$ , there is a probability distribution $P$ faithful   \n425 to $\\mathcal{G}$ such that their conditional mutual information vanishes, i.e. $I_{P}(\\dot{X_{}};Y|C)\\,=\\,0$ , also noted   \n426 $X\\perp\\!\\!\\!\\perp\\!\\!\\!\\!\\perp Y Y|C$ .   \n427 However, as discussed above, the proof of Theorem 1 will require to introduce a weaker $m^{\\prime}$ -connection   \n428 criterion defined below.   \n429 Definition 6. $[m^{\\prime}$ -connecting path] A path $\\pi$ between $X$ and $Y$ is $m^{\\prime}$ -connecting given a subset   \n430 $C\\subseteq V$ (with $X,Y$ possibly in $_{C}$ ) if:   \n431 $i)$ its non-collider(s) are not in $_{C}$ , and   \n432 $i i_{\\prime}$ ) its collider(s) are in $\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(\\{X,Y\\}\\cup C)$ .   \n433 Note, in particular, that an $m$ -connecting path is necessary an $m^{\\prime}$ -connecting path but that the   \n434 converse is not always true. For example, the path $X\\!\\rightarrow\\!Z\\!\\longleftrightarrow\\!T\\!\\longleftrightarrow\\!\\!\\!\\!\\!\\!\\longrightarrow\\!\\!\\!\\!\\!\\!\\!Y$ in Fig. 1G (with $Z\\rightarrow Y$ ) is   \n435 an $m^{\\prime}$ -connecting path given $T$ (as $\\bar{Z^{}}\\!\\in\\!\\mathbf{A}\\mathbf{n}\\bar{g^{}}\\!(\\{X,Y\\}\\cup T),$ but not an $m$ -connecting path given $T$   \n436 (as $Z\\not\\in\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(T\\bar{)})$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "437 However, Richardson and Spirtes 2002 [8] have shown the following lemma, ", "page_idx": 12}, {"type": "text", "text": "438 Lemma 4. [Corollary 3.15 in [8]] In an ancestral graph $\\mathcal{G}$ , there is a $m^{\\prime}$ -connecting path $\\pmb{\\mu}$ between   \n439 $X$ and $Y$ given $_{C}$ if and only if there is a (possibly different) $m$ -connecting path $\\pi$ between $X$ and   \n440 $Y$ given $_{C}$ .   \n441 Hence, Lemma 4 implies that $m^{\\prime}$ -separation and $m$ -separation criteria are in fact equivalent, as an   \n442 absence of $m^{\\prime}$ -connecting paths implies an absence of $m$ -connecting paths and vice versa. This   \n443 enables to reformulate the $m$ -separation criterion above as,   \n444 Definition 7. $[m^{\\prime}$ -separation (and $m$ -separation) criteria] The subsets $\\pmb{A}$ and $_B$ are said to be $m^{\\prime}$ -   \n445 separated (or $m$ -separated) by $_{C}$ , if all paths from any $X\\in A$ to any $Y\\in B$ have either ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "447 $i i_{\\prime}$ ) a collider not in $\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(\\{X,Y\\}\\cup C)$ . ", "page_idx": 12}, {"type": "text", "text": "448 The probabilistic interpretation of an ancestral graph is given by its (global) Markov property: if $\\pmb{A}$   \n449 and $_B$ are $m$ -separated (or $m^{\\prime}$ -separated) by $_{C}$ , then $\\pmb{A}$ and $_B$ are conditionally independent given   \n450 $_{C}$ , noted as, $A\\perp_{m}B|C$ . ", "page_idx": 12}, {"type": "text", "text": "451 A.2 ac-connecting paths and ac-connected subsets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "452 Let us now recall the definition of ancestor collider connecting paths or ac-connecting paths,   \n453 which is directly relevant to characterize the likelihood decomposition and Markov equivalent classes   \n454 of ancestral graphs (Theorem 1). We give here a different yet equivalent definition of $a c$ -connecting   \n455 paths as defined in the main text (Definition 2) in order to underline the similarities and differencies   \n456 with the notion of $m^{\\prime}$ -connecting path (Definition 6).   \n457 Definition 8. [ac-connecting path] A path $\\pi$ between $X$ and $Y$ is an $a c$ -connecting path given a   \n458 subset $C\\subseteq V$ (with $X$ and $Y$ possibly in $_{C}$ ) if:   \n459 i) $\\pi$ does not have any noncollider, and   \n460 $i i_{\\prime}$ ) its collider(s) are in $\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(\\{X,Y\\}\\cup C)$ .   \n461 Hence, more simply (following Definition 2 in the main text), an $a c$ -connecting path given $_{C}$ is a   \n462 collider path, $X*\\to Z_{1}\\leftrightarrow\\cdot\\cdot\\cdot\\leftrightarrow Z_{K}\\leftarrow*Y$ , with all $Z_{i}\\in\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(\\{X,Y\\}\\cup C)$ , i.e. with $Z_{i}$ in $_{C}$ or   \n463 connected to $\\{X,Y\\}\\cup C$ by an ancestor path, $Z_{i}\\to\\cdot\\cdot\\cdot\\to T$ with $T\\in\\{X,Y\\}\\cup C$ .   \n464 Definition 9. [ac-separation criterion] The subsets $\\pmb{A}$ and $_B$ are said to be ac-separated by $_{C}$ if there   \n465 is no $a c$ -connecting path between any vertex in $\\pmb{A}$ and any vertex in $_B$ given $_{C}$ .   \n466 Previous definitions and Lemma 4 readily lead to the following corollary between the different   \n467 connection and separation criteria: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "468 Corollary 5. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "469 i) $m$ -connecting path $\\pi\\,\\Longrightarrow\\,m^{\\prime}$ -connecting path $\\pi$   \n470 $i i$ ) ac-connecting path $\\pi\\,\\Longrightarrow\\,m^{\\prime}$ -connecting path $\\pi$   \n471 $i i i)$ $m$ -separation $\\Longleftrightarrow\\ m^{\\prime}$ -separation   \n472 $i\\nu_{.}$ ) $m/m^{\\prime}$ -separation $\\Longrightarrow$ ac-separation   \n473 Finally, we recall the notion of $\\mathbf{\\delta}_{a c}$ -connected subset (Definition 3 in the main text), which is central   \n474 for the decomposition of the likelihood of ancestral graphs (Theorem 1): A subset $_{C}$ is said to be   \n475 $a c$ -connected if $\\forall X$ , $Y\\in C$ , there is an $a c$ -connecting path between $X$ and $Y$ w.r.t. $_{C}$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "476 B Proof of Theorem 1. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "477 In order to prove that the likelihood function of an ancestral graph, Eq. 12, contains all and only the   \n478 $a c$ -connected subsets of vertices in $\\mathcal{G}$ (Definition 3), we will first show (i) that all non-ac-connected   \n479 subsets $S^{\\prime}$ are included in a cancelling combination of multivariate information terms $I(X;Y|A)=0$ ,   \n480 with $X,Y\\in S^{\\prime}$ and $S^{\\prime}\\subseteq S=\\{X,Y\\}\\cup A$ . Conversely, we will then show $(\\romannumeral2)$ that cancelling   \n481 combinations of multivariate information terms associated to pairwise conditional independence,   \n482 $\\begin{array}{r}{I(X;Y|A)=\\sum_{S^{\\prime}\\subseteq S}^{X,Y\\in S^{\\prime}}(-1)^{|S^{\\prime}|}I(S^{\\prime})=0}\\end{array}$ do not contain any $a c$ -connected subset $S^{\\prime}$ . Finally, we   \n483 will prove $(i i i)$ that the information terms which appear in multiple cancelling combinations from   \n484 different pairwise independence constraints do not modify the multivariate information decomposition   \n485 of the likelihood function of ancestral graphs, Eq. 12, as these shared/overlapping terms in fact all   \n486 cancel through more global Markov independence relationships involving higher order (three or more   \n487 points) vanishing multivariate information terms, such as $I(\\dot{\\boldsymbol{X}};\\boldsymbol{Y};\\boldsymbol{Z}|\\dot{\\boldsymbol{A}})=\\dot{\\boldsymbol{0}}$ .   \n488 $^{i)}$ Let\u2019s first prove that all non-ac-connected subsets $S^{\\prime}$ are included in at least one cancelling   \n489 combination of multivariate information terms, $I(X;Y|A)=0$ , with $X$ , $Y\\!\\in\\!S^{\\prime}$ and $S^{\\prime}\\!\\subseteq\\{X,Y\\}\\cup\\!{\\bar{A}}$ .   \n490 If $S^{\\prime}$ is a non- $a c$ -connected subset, there is at least one disconnected pair $X$ and $Y$ for which each   \n491 path $\\pi_{j}$ between $X$ and $Y$ contains either some collider(s) not in $\\mathbf{An}_{\\mathcal{G}}(S^{\\prime})$ or, if all colliders along   \n492 $\\pi_{j}$ are in ${\\bf A n}_{\\mathcal{G}}(S^{\\prime})$ , there must be some non-collider(s) at node(s) $Z_{j}$ but not necessarily in $S^{\\prime}$ . Let\u2019s   \n493 define $S\\,=\\,S^{\\prime}\\cup_{j}\\,Z_{j}$ . $X$ and $Y$ can be shown to be $m$ -separated given $S\\setminus\\{X,Y\\}$ , as for each   \n494 path $\\pi_{j}$ between $X$ and $Y$ , its non-collider(s) are in $\\boldsymbol{S}$ at node(s) $Z_{j}$ (when all collider(s) along $\\pi_{j}$   \n495 are in $S^{\\prime}$ ) or there is some collider(s) not in ${\\bf A n}_{\\mathcal{G}}(S^{\\prime})$ , which are not in ${\\bf A n}_{\\mathcal{G}}(S^{\\prime})$ either. The latter   \n496 statement is proven by contradiction assuming that there is a collider at $Z\\not\\in\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(S^{\\prime})$ such that   \n497 $Z\\in\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(\\bar{S})$ . There is therefore a directed path $Z\\to\\cdots\\to W$ with $W\\in S$ . Hence, $\\mathring{W}\\in S^{\\prime}$ or   \n498 there is a noncollider at $W\\in\\mathbf{Z}_{j}$ which is on a path $\\pi_{j}$ between $X$ and $Y$ along which all colliders   \n499 are in $\\mathbf{An}_{\\mathcal{G}}(S^{\\prime})$ by construction of $\\boldsymbol{S}$ . This leads by induction to $Z\\to\\cdot\\cdot\\cdot\\to W\\to\\cdot\\cdot\\cdot\\to T$ where   \n500 $T\\in S^{\\prime}$ and thus $\\dot{Z}\\in\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(S^{\\prime})$ , which is a contradiction. Hence, all non- $a c$ -connected subsets $S^{\\prime}$   \n501 are included in a cancelling combination of multivariate information terms $I(X;Y|A)=0$ , with   \n502 $X,Y\\in S^{\\prime}$ and $S^{\\prime}\\subseteq S=\\left\\{X,Y\\right\\}\\cup A$ .   \n503 ii) Conversely, we will now show that cancelling combinations of multivariate information terms   \n504 associated to pairwise conditional independence, $\\begin{array}{r}{I(X;Y|A)=\\sum_{S^{\\prime}\\subseteq S}^{X,Y\\in S^{\\prime}}(-1)^{|S^{\\prime}|}I(S^{\\prime})=0}\\end{array}$ , do   \n505 not contain any $a c$ -connected subset $S^{\\prime}$ .   \n506 We will prove it by contradiction assuming that there exists a subset $W\\subseteq A$ , such that $S^{\\prime}=$   \n507 $\\{X,Y\\}\\cup W$ is $a c$ -connected. In particular, there should be an $a c$ -connecting path between $X$ and $Y$   \n508 confined to $\\mathbf{An}_{\\mathcal{G}}(S^{\\prime})$ and thus to $\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(S)\\supseteq\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(S^{\\prime})$ , which is an $m^{\\prime}$ -connecting path between $X$   \n509 and $Y$ given $\\pmb{A}$ , contradicting the above hypothesis of $m^{\\prime}$ -separation given $\\pmb{A}$ , i.e. $\\dot{I}(X;Y|A)=0$ .   \n510 The use of $m^{\\prime}$ -separation, i.e. the absence of $m^{\\prime}$ -connecting paths with colliders in $\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(S)$ rather   \n511 than $m$ -connecting paths with colliders in $\\mathbf{A}\\mathbf{n}_{\\mathcal{G}}(A)$ , is necessary here, see Definitions 4 and 6. Hence,   \n512 no ac-connected subset $S^{\\prime}$ is included in cancelling combinations of multivariate information terms   \n513 associated to pairwise conditional independence, $\\begin{array}{r}{\\bar{I}(X;Y|A)=\\sum_{{\\pmb S}^{\\prime}\\subseteq{\\pmb S}}^{X,Y\\in{\\pmb S}^{\\prime}}(-1)^{|{\\pmb S}^{\\prime}|}I({\\pmb S}^{\\prime})=0.}\\end{array}$ .   \n514 iii) Finally, we will show that the information terms which appear in multiple cancelling combina  \n515 tions from different pairwise independence constraints do not modify the multivariate information   \n516 decomposition of the likelihood function of ancestral graphs, Eq. 12, as these shared/overlapping   \n517 terms in fact all cancel through more global Markov independence relationships involving higher   \n518 order (three or more points) vanishing multivariate information terms, such as $I(X;Y;Z|A)=0$ .   \n519 This result requires to use an ordering of the nodes, $X_{k}\\succ X_{j}\\succ X_{i}$ , that is compatible with the   \n520 directed edges of the ancestral graph assumed to have no undirected edges, i.e. $X_{j}\\notin\\mathbf{A}\\mathbf{n}(X_{i})$ if   \n521 $X_{j}\\succ X_{i}$ . Under this ordering, higher order nodes $X_{k}\\succ X_{i}\\succ X_{j}$ can be a priori excluded from all   \n522 separating sets $A_{i j}$ of pairs of lower order nodes, i.e. if $I(X_{i};X_{j}|\\mathbf{\\bar{A}}_{i j})=0$ then $X_{k}\\notin A_{i j}$ .   \n523 In particular, the two pairwise conditional independence relations $I(X_{k};X_{\\ell}|{\\cal A}_{k\\ell})=0$ , with $X_{\\ell}\\succ$   \n524 $X_{k}$ , and $I(X_{i};X_{j}|\\bar{\\pmb{A}_{i j}})=0$ , with $X_{j}\\succ X_{i}$ , do not share any multivariate information terms, if   \n525 $X_{\\ell}\\neq X_{j}$ . Indeed, as $I(X_{k};X_{\\ell}|\\mathbf{A}_{k\\ell})$ contains all information terms including both $X_{k}$ and $X_{\\ell}$ as   \n526 well as every subset (possibly empty) of $A_{k\\ell}$ , none of them includes $X_{j}$ if $X_{\\ell}\\succ X_{j}$ . Therefore   \n527 $I(X_{k};X_{\\ell}|\\dot{A_{k\\ell}})$ does not contain any information term of $I(X_{i};X_{j}|{\\cal A}_{i j})$ which contains both $X_{i}$ and   \n528 $X_{j}$ as well as every subset (possibly empty) of $A_{i j}$ . This property eliminates all multiple counting of   \n529 multivariate informations terms shared if $X_{\\ell}\\neq\\check{X_{j}}$ . Note that this result does not hold in general for   \n530 ancestral graphs including undirected edges.   \n531 Hence, the issue of redundant multivariate information terms in the likelihood decomposition, Eq. 12,   \n532 is related to the conditional independences of two or more pairs, $\\{X_{i},X_{r}\\},\\{X_{j},X_{r}\\},...,\\{X_{\\ell},\\mathbf{\\bar{\\calX}}_{r}\\}$ ,   \n533 sharing the same higher order node, $X_{r}$ . However, this situation also entails a more global Markov   \n534 independence constraint between $X_{r}$ and $\\{X_{i},X_{j},\\cdot\\cdot\\cdot,X_{\\ell}\\}$ , given a separating set $\\pmb{A}$ , which can be   \n535 decomposed into more local independence constraints using the chain rule and the decomposition   \n536 rules of multivariate information (Eq. 9), ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{0}&{=\\phantom{\\big(}I(\\{X_{i};X_{j},\\cdots,X_{\\ell}\\};X_{r}|\\cal{A})}\\\\ {0}&{=\\phantom{\\big(}I(X_{i};X_{r}|\\cal{A})+I(X_{j};X_{r}|\\cal{A},X_{i})\\big)+\\big[I(X_{k};X_{r}|\\cal{A},X_{i},X_{j})\\big]+\\cdots+I(X_{\\ell};X_{r}|\\cal{A},\\cdots)}\\\\ {=\\phantom{\\big(}I(X_{i};X_{r}|\\cal{A})+I(X_{j};X_{r}|\\cal{A})-I(X_{i};X_{j};X_{r}|\\cal{A})\\big)}\\\\ &{+\\big[I(X_{k};X_{r}|\\cal{A},X_{i})-I(X_{j};X_{k};X_{r}|\\cal{A},X_{i})\\big]+\\cdots+I(X_{\\ell};X_{r}|\\cal{A},\\cdots)}\\\\ &{+\\big[I(X_{i};X_{r}|\\cal{A})+I(X_{j};X_{r}|\\cal{A})-I(X_{i};X_{j};X_{r}|\\cal{A})\\big)}\\\\ &{+\\big[I(X_{k};X_{r}|\\cal{A})-I(X_{j};X_{k};X_{r}|\\cal{A})-I(X_{i};X_{k};X_{r}|\\cal{A})+I(X_{i};X_{j};X_{k};X_{r}|\\cal{A})\\big]+\\cdots.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "537 where all the conditional multivariate information terms vanish by induction due to the non  \n538 negativity of (conditional) mutual information. In particular, the conditional multivariate in  \n539 formation terms in the last expression, i.e. between $X_{r}$ and each subset of $\\{X_{i},X_{j},\\cdot\\cdot\\cdot,X_{\\ell}\\}$   \n540 given the separating set $\\pmb{A}$ , all vanish. This result can be readily extended to any subsets   \n541 $\\{X_{r},X_{s},\\cdot\\cdot\\cdot,X_{z}\\}$ (conditionally) independent of $\\{X_{i},X_{j},\\cdot\\cdot\\cdot,X_{\\ell}\\}$ given a separating set $\\pmb{A}$ ,   \n542 i.e. $I(\\{X_{i},X_{j},\\cdot\\cdot\\cdot,X_{\\ell}\\}$ ; $\\{X_{r},\\bar{X_{s}},\\cdot\\cdot\\cdot\\,,\\bar{X}_{z}\\}|A)\\,=\\,\\bar{0}$ . Hence, as the final conditional multivari  \n543 ate cross information terms of the decomposition all vanish while not sharing any subsets of variables,   \n544 it proves the absence of redundancy and a global cancellation of non- $a c$ -connected subsets (from   \n545 pairwise and higher order conditional independence relations) in the likelihood function of ancestral   \n546 graphs without undirected edges, Eq. 12.   \n547 Hence, only $a c$ -connected subsets effectively contribute to the cross-entropy of an ancestral graph   \n548 with only directed and bidirected edges, Eq. 12. \u25a1 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "549 C Factorization of the probability distribution of ancestral graphs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "550 C.1 Factorization resulting from Theorem 1 and Proposition 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "551 Before presenting the factorization of the model distribution of ancestral graphs resulting from   \n552 Theorem 1 and Proposition 3, it is instructive to obtain an equivalent factorization for Bayesian   \n553 graphs, assuming a positive empirical distributions, $\\begin{array}{r}{p(x_{1},\\cdot\\cdot\\cdot,\\cdot_{m}^{\\cdot})=\\prod_{i=1}^{m}p(x_{i}|x_{i-1},\\cdot\\cdot\\cdot\\cdot,x_{1}^{\\cdot})>0,}\\end{array}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{q(x_{1},\\cdots,x_{m})}&{=}&{\\displaystyle\\prod_{i=1}^{m}q(x_{i}|\\mathbf{pa}_{x_{i}})=\\prod_{i=1}^{m}p(x_{i}|\\mathbf{pa}_{x_{i}})}\\\\ &{=}&{\\displaystyle p(x_{1},\\cdots,x_{m})\\prod_{i=1}^{m}\\frac{p(x_{i}|\\mathbf{pa}_{x_{i}})}{p(x_{i}|x_{i-1},\\cdots,x_{1})}}\\\\ &{=}&{\\displaystyle p(x_{1},\\cdots,x_{m})\\prod_{i=1}^{m}\\frac{p(x_{i}|\\mathbf{pa}_{x_{i}})p(x_{i-1}\\backslash\\mathbf{pa}_{x_{i}}|\\mathbf{pa}_{x_{i}})}{p(x_{i},x_{i-1}\\backslash\\mathbf{pa}_{x_{i}}|\\mathbf{pa}_{x_{i}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "554 This leads to the following alternative expressions for the cross-entropy $\\begin{array}{r l}{H(p,q)}&{{}=}\\end{array}$   \n555 $\\begin{array}{r}{-\\sum_{\\mathbf{x}}p(\\mathbf{x})\\log q(\\mathbf{x})}\\end{array}$ in terms of multivariate entropy and information, which only depend on the   \n556 empirical joint distribution $p(x)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{H(p,q)}&{=}&{\\displaystyle\\sum_{i=1}^{m}H(x_{i}|\\mathbf{Pa}_{X_{i}})}\\\\ &{=}&{\\displaystyle H(X_{1},\\cdot\\cdot\\cdot\\cdot,X_{m})+\\sum_{i=1}^{m}I(X_{i};X_{i-1}\\backslash\\mathbf{Pa}_{X_{i}}|\\mathbf{Pa}_{X_{i}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "557 where $\\begin{array}{r}{\\sum_{i=1}^{m}I(X_{i};{\\pmb X}_{i-1}\\backslash{\\bf P a}_{X_{i}}|{\\bf P a}_{X_{i}})}\\end{array}$ can be decomposed, using the chain rule and Eq. 11, into   \n558 unconditional multivariate information terms, which exactly cancel all the multivariate information   \n559 of the non- $a c$ -connected subsets of variables in the multivariate entropy decomposition, Eq. 6.   \n560 Note, however, that this result obtained for Bayesian networks requires an explicit factorization of the   \n561 global model distribution, $q(x)$ , in terms of the empirical distribution, $p({\\pmb x})$ , which is not known and   \n562 presumably does not exist, in general, for ancestral graphs.   \n563 Alternatively, assuming that the empirical and model distributions are positive $(\\forall{\\pmb x},p({\\pmb x})\\;>\\;0$ ,   \n564 $q({\\pmb x})>0)$ , it is always possible to factorize them into factors associated to each (cross) information   \n565 term in the (cross) entropy decomposition, Eq. 6, as, ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{q({\\pmb x})}&{=}&{\\displaystyle\\prod_{i=1}^{m}q({\\pmb x}_{i})\\times\\prod_{i<j}^{m}\\frac{q({\\pmb x}_{i},{\\pmb x}_{j})}{q({\\pmb x}_{i})q({\\pmb x}_{j})}\\times\\prod_{i<j<k}^{m}\\frac{q({\\pmb x}_{i},{\\pmb x}_{j},{\\pmb x}_{k})q({\\pmb x}_{i})q({\\pmb x}_{j})q({\\pmb x}_{k})}{q({\\pmb x}_{i},{\\pmb x}_{j})q({\\pmb x}_{i},{\\pmb x}_{k})q({\\pmb x}_{j},{\\pmb x}_{k})}\\times\\cdots}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "566 where all the marginal distributions over a subset of variables, e.g. $\\begin{array}{r}{q(x_{i},x_{j},x_{k})=\\sum_{\\ell\\neq i,j,k}q(\\pmb{x})}\\end{array}$ or   \n567 $\\begin{array}{r}{p(x_{i},x_{j},x_{k})=\\sum_{\\ell\\neq i,j,k}p(\\pmb{x})}\\end{array}$ , cancel two-by-two by construction.   \n568 This can be illustrated on a simple example of a two-collider path including one bidirected edge,   \n569 $X\\to Z\\longleftrightarrow Y\\leftarrow W$ (Fig. 1D), valid for $q(.)$ and $p(.)$ alike, ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{q(x,z,y,w)\\;=\\;q(x)\\;q(z)\\;q(y)\\;q(w)}\\\\ &{\\times\\;\\frac{q(x,z)}{q(x)\\;q(z)}\\;\\frac{q(z,y)}{q(z)\\;q(y)}\\;\\frac{q(y,w)}{q(y)\\;q(w)}\\;\\frac{q(x,y)}{q(x)\\;q(y)}\\;\\frac{q(x,w)}{q(x)\\;q(w)}\\;\\frac{q(z,w)}{q(z)\\;q(w)}}\\\\ &{\\times\\;\\frac{q(x)\\;q(z)\\;q(y)\\;q(x,z,y)}{q(x,z)\\;q(x,y)\\;q(z,y)}\\;\\frac{q(z)\\;q(y)\\;q(w)\\;q(z,y,w)}{q(z,y)\\;q(z,w)\\;q(y,w)}}\\\\ &{\\times\\;\\frac{q(x)\\;q(z)\\;q(w)\\;q(x,z,w)}{q(x,z)\\;q(x,w)\\;q(z,w)}\\;\\frac{q(x)\\;q(y)\\;q(w)\\;q(x,y,w)}{q(x,y)\\;q(x,w)\\;q(y,w)}}\\\\ &{\\times\\;\\frac{q(x,z)\\;q(z,y)\\;q(y,w)}{q(x,z,y)\\;q(x,z,w)\\;q(x,y,w)\\;q(z,y,w)\\;q(x,z,y,w)}}\\\\ &{\\times\\;\\frac{q(x,z)\\;q(z,y)\\;q(y,w)\\;q(x,y)\\;q(x,w)\\;q(z,w)\\;q(x,z,y,w)}{q(x,z,y)\\;q(x,z,w)\\;q(x,y,w)\\;q(z,y,w)\\;q(x)\\;q(y)\\;q(z)\\;q(w)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "570 where all individual distribution marginals on subsets of variables, e.g. $q(x),q(x,z),q(x,z,y)$ (or   \n571 $p(x),p(x,z),p(x,z,y))$ , cancel two-by-two by construction, except $q(x,z,y,w)$ (or $p(x,z,y,w))$ .   \n572 In addition and only for the model distribution $q(.)$ , all ratios in gray in Eq. 18 also cancel due to   \n573 Markov independence relations across non- $a c$ -connected subsets (see proof of Theorem 1). This   \n574 leaves a truncated factorization retaining all and only the $a c$ -connected subsets of variables in the   \n575 graph, which we propose to estimate on empirical data by substituting the remaining $q(.)$ terms by   \n576 their empirical counterparts $p(.)$ , see Proposition 3. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "577 This leads to the following global factorization for $q(.)$ in terms of $p(.)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{q(x,z,y,w)~\\equiv~p(x)~p(y)~p(w)~\\frac{p(x,z)}{p(x)\\,p(z)}~\\frac{p(z,y)}{p(z)\\,p(y)}~\\frac{p(y,w)}{p(y)\\,p(y)}}\\\\ &{~~\\times~\\frac{p(x)\\,p(z)\\,p(y)\\,p(x,z,y)}{p(x,z)\\,p(x,y)\\,p(z,y)}~\\frac{p(z)\\,p(y)\\,p(y)\\,p(z,y,w)}{p(z,y)\\,p(z,w)\\,p(y,w)}}\\\\ &{~~\\times~\\frac{p(x,z)\\,p(z,y)\\,p(y,w)\\,p(x,y)\\,p(x,w)\\,p(z,w)\\,p(x,z,y,w)}{p(x,z,y)\\,p(x,z,w)\\,p(x,y)\\,p(z,y)\\,p(x)}}\\\\ &{~~=~p(x,z,y,w)~\\frac{p(x)\\,p(y)}{p(x,y)}~\\frac{p(x)\\,p(w)}{p(x,w)}~\\frac{p(z)\\,p(w)}{p(z,w)}}\\\\ &{~~~\\times~\\frac{p(x,z)\\,p(x,w)\\,p(z,w)}{p(x)\\,p(z)\\,p(w)\\,p(x,z,w)}~\\frac{p(x,y)\\,p(x,w)\\,p(y,w)}{p(x)\\,p(y)\\,p(w)\\,p(x,y,w)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "578 where the terms in gray have been passed to the lhs of Eq. 18 applied to $p(.)$ . This ultimately   \n579 leads to the analog of the Bayesian Network factorization in Eq. 15 but for the two-collider path,   \n580 $X\\rightarrow Z\\longleftrightarrow Y\\leftarrow W$ (Fig. 1D), ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(x,z,y,w)~\\equiv~p(x,z,y,w)~{\\frac{p(x)\\,p(w)}{p(x,w)}}~{\\frac{p(z|x)\\,p(w|x)}{p(z,w|x)}}~{\\frac{p(x|w)\\,p(y|w)}{p(x,y|w)}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "581 where the last three factors \u201ccorrect\u201d the expression of $p(x,z,y,w)$ for the three (conditional)   \n582 independences entailed by the underlying graph, that is, $X\\perp W,Z\\perp W|X$ , and $X\\perp Y|W$ . ", "page_idx": 16}, {"type": "text", "text": "583 C.2 Relation to the head-and-tail factorizations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "584 The head-and-tail factorizations of the model distribution of an acyclic directed mixed graph, intro  \n585 duced by Richardson 2009 [6], enable the parametrization of the joint probability distribution with   \n586 independent parameters for ancestrally closed subsets of vertices.   \n587 For instance, the head-and-tail factorizations of the simple two-collider path including one bidirected   \n588 edge, $X\\to Z\\longleftrightarrow Y\\leftarrow W$ , introduced above, Fig. 1D, are [6], ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r c l}{q(x,w)}&{=}&{q(x)\\,q(w)}\\\\ {q(x,z)}&{=}&{q(z|x)\\;q(x)}\\\\ {q(y,w)}&{=}&{q(y|w)\\;q(w)}\\\\ {q(x,z,w)}&{=}&{q(z|x)\\;q(x)\\,q(w)}\\\\ {q(x,y,w)}&{=}&{q(y|w)\\;q(w)\\,q(x)}\\\\ {q(x,z,y,w)}&{=}&{q(z,y|x,w)\\;q(x)\\,q(w)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "589 Importantly, these head-and-tail factorizations imply additional relations such as $q(y|w)=q(y|x,w)$   \n590 (i.e. $X~\\perp~Y|W)$ obtained by comparing the last two relations in Eq. 21 after marginalizing   \n591 $q(x,z,y,w)$ over $z$ . However, such implicit conditional independence relations are not verified   \n592 by the empirical distribution $p(.)$ in general and prevent the estimation of the head-and-tail factoriza  \n593 tions by substituting the rhs $q(.)$ terms in Eq. 21 with their empirical counterparts $p(.)$ , as in the case   \n594 of Bayesian networks, Eq. 15.   \n595 Indeed, while the head-and-tail factorization relations, Eq. 21, obey the local and global Markov   \n596 independence relations entailed by the graphical model, Fig. 1D, leading to the cancellation of all   \n597 factors associated to non- ${\\it a c}$ -connected subsets in gray in Eq. 18, the remaining head-and-tail factors   \n598 cannot be readily estimated with the empirical distribution $p(.)$ .   \n599 In particular, the cross-entropy of the two-collider path of interest, Fig. 1D, obtained with the head  \n600 and-tail factorizations corresponds to1 $H(p,q)\\!=\\!-\\sum p(x,z,y,w)\\log q(z,y|x,w)\\,q(x)\\,q(w)$ . Then,   \n601 estimating the $q(.)$ terms with their $p(.)$ counterparts leads to the cross-entropy of a Bayesian graph,   \n602 Fig. 1E, with a different Markov equivalent class than the ancestral graph of interest, Fig. 1D. A   \n603 similar discrepancy is obtained with a c-component factorization which leads to the cross-entropy of   \n604 the Bayesian graph of Fig. 1E without edge $X\\rightarrow Y$ , corresponding to a different Markov equivalence   \n605 class than the previous two graphs, Figs. 1D & E.   \n606 These examples illustrate the difficulty to exploit the $\\mathrm{\\bfc}$ -component or head-and-tail factorizations to   \n607 estimate the likelihood of ancestral graphs including bidirected edge(s). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "608 D Node and edge scores based on Normalized Maximum Likelihood criteria ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "609 Search-and-score methods based on likelihood estimates need to properly account for finite sample   \n610 size, as cross-entropy minimization leads to ever more complex models, resulting in model overftiting   \n611 for finite datasets. While BIC regularization is valid in the asymptotic limit of very large datasets, it   \n612 tends to overestimate finite size corrections, leading to lower recall, in general. In order to better take   \n613 into account finite sample size, we used instead the (universal) Normalized Maximum Likelihood   \n614 (NML) criteria [42, 43, 38, 39], which amounts to normalizing the likelihood function over all   \n615 possible datasets with the same number $N$ of samples.   \n616 Node score. We first used the factorized Normalized Maximum Likelihood (fNML) complexity [38,   \n617 39] to define a local score for each node $X_{i}$ , which extends the decomposable likelihood of Bayesian   \n618 graphs given each node\u2019s parents, Eq. 2, to all non-descendant neighbors, $\\mathbf{Pa}_{x_{i}}^{\\prime}$ , ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{l l l}{{\\mathcal{L}}_{\\mathcal{D}|\\mathcal{G}\\mathbf{x}_{\\mathrm{i}}}=\\,e^{-N.\\operatorname{Score}_{\\mathbf{n}}(X_{i})}}&{=}&{{\\cfrac{e^{\\displaystyle-N H(X_{i}|\\mathbf{Pa}^{\\prime}\\mathbf{\\hat{\\varepsilon}}_{X_{i}})}}{\\displaystyle\\sum_{|\\mathbb{D}^{\\prime}|=N}e^{\\displaystyle-N H(X_{i}|\\mathbf{Pa}^{\\prime}\\mathbf{\\hat{\\varepsilon}}_{X_{i}})}}}}\\\\ &{=}&{e^{-N H(X_{i}|\\mathbf{Pa}^{\\prime}\\mathbf{\\hat{\\varepsilon}}_{X_{i}})-\\sum_{j}^{q_{i}}\\log C_{n_{j}}^{r_{i}}}}\\\\ &{=}&{e^{N\\sum_{j}^{q_{i}}\\sum_{k}^{r_{i}}{\\frac{n_{j k}}{N}}\\log\\left({\\frac{n_{j k}}{n_{j}}}\\right)-\\sum_{j}^{q_{i}}\\log C_{n_{j}}^{r_{i}}}}\\\\ &{=}&{\\displaystyle\\prod_{j}^{q_{i}}{\\frac{\\prod_{k}^{r_{i}}\\,\\left({\\frac{n_{j k}}{n_{j}}}\\right)^{n_{j k}}}{\\mathcal{C}_{n_{j}}^{r_{i}}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "619 where $n_{j k}$ corresponds to the number of data points for which $X_{i}$ is in its $k$ th state and its non  \n620 descendant neighbors in their $j$ th state, with $\\begin{array}{r}{n_{j}=\\sum_{k}^{r_{i}}n_{j k}}\\end{array}$ . The universal normalization constant $\\mathcal{C}_{n}^{r}$   \n621 is then computed by summing the numerator ove r all possible partitions of the $n$ data points into a   \n622 maximum of $r$ subsets, $\\ell_{1}+\\ell_{2}+\\dots+\\ell_{r}=n$ with $\\ell_{k}\\geqslant0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{C}_{n}^{r}=\\sum_{\\ell_{1}+\\ell_{2}+\\cdots+\\ell_{r}=n}\\frac{n!}{\\ell_{1}!\\ell_{2}!\\cdot\\cdot\\cdot\\ell_{r}!}\\prod_{k=1}^{r}\\left(\\frac{\\ell_{k}}{n}\\right)^{\\ell_{k}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "623 which can in fact be computed in linear-time using the following recursion [38], ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{C}_{n}^{r}=\\mathcal{C}_{n}^{r-1}+\\frac{n}{r-2}\\mathcal{C}_{n}^{r-2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "624 with $\\mathcal{C}_{n}^{1}\\,=\\,1$ for all $n$ and applying Eq. 30 below for $r\\,=\\,2$ . However, for large $n$ and $r$ , $\\mathcal{C}_{n}^{r}$   \n625 computation tends to be numerically unstable, which can be circumvented by implementing the   \n626 recursion on parametric complexity ratios ${\\cal D}_{n}^{r}\\,=\\,{\\mathcal{C}}_{n}^{r}/{\\mathcal{C}}_{n}^{r-1}$ rather than parametric complexities   \n627 themselves [35] as, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mathcal D_{n}^{r}}}&{{=}}&{{{\\displaystyle1+\\frac{n}{(r-2)\\mathcal D_{n}^{r-1}}}}}\\\\ {{\\log\\mathcal C_{n}^{r}}}&{{=}}&{{{\\displaystyle\\sum_{k=2}^{r}\\log\\mathcal D_{n}^{k}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "628 for $r\\geqslant3$ , with $\\mathcal{C}_{n}^{1}=1$ and $\\mathcal{C}_{n}^{2}=\\mathcal{D}_{n}^{2}$ , which can be computed directly with the general formula,   \n629 Eq. 26, for $r=2$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal{C}}_{n}^{2}=\\sum_{h=0}^{n}\\left({n\\atop h}\\right)\\left({\\frac{h}{n}}\\right)^{h}\\left({\\frac{n-h}{n}}\\right)^{n-h}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "630 or its Szpankowski approximation for large $n$ (needed for $n>1000$ in practice) [44\u201346], ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\mathcal C}_{n}^{2}=\\sqrt{\\frac{n\\pi}{2}}\\left(1+\\frac{2}{3}\\sqrt{\\frac{2}{n\\pi}}+\\frac{1}{12n}+\\mathcal O\\left(\\frac{1}{n^{3/2}}\\right)\\right)}}\\\\ {{\\displaystyle{\\simeq\\sqrt{\\frac{n\\pi}{2}}\\exp\\left(\\sqrt{\\frac{8}{9n\\pi}}+\\frac{3\\pi-16}{36n\\pi}\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "631 ", "page_idx": 18}, {"type": "text", "text": "632 This leads to the following local score for each node $X_{i}$ , which is minimized over alternative   \n633 combinations of non-descendant neighbors, $\\mathbf{P}\\mathbf{a}_{x_{i}}^{\\prime}\\subseteq\\mathbf{P}\\mathbf{a}_{x_{i}}\\cup\\mathbf{S}\\mathbf{p}_{x_{i}}\\cup\\mathbf{N}\\mathbf{e}_{x_{i}}$ , in the first step of the   \n634 local search-and-score algorithm (step 1) detailed in the main text, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Score}_{\\mathrm{n}}(X_{i})=H(X_{i}|\\mathbf{Pa}_{{x}_{i}}^{\\prime})+\\frac{1}{N}\\sum_{j}^{q_{x}_{i}}\\log C_{n_{j}}^{r_{x_{i}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "635 Edge scores. We then defined several edge scores to optimize the orientation of each edge, $X-Y$ ,   \n636 given its close surrounding vertices.   \n637 To this end, we first introduced a local score for node pairs which simply sums the node scores, Eq. 33,   \n638 for each node. The resulting pair scores are listed in Table 2 for unconnected node pairs and for pairs   \n639 of nodes connected by a directed edge, where $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X\\!\\backslash Y}^{\\prime}=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X}\\cup\\mathbf{S}\\mathbf{p}_{\\scriptscriptstyle X}\\backslash Y$ and $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y\\!\\backslash X}^{\\prime}\\!=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y}\\!\\cup\\!\\mathbf{S}\\bar{\\mathbf{p}_{\\scriptscriptstyle Y}}\\!\\rangle X$   \n640 with their corresponding combinations of levels, $q_{y\\backslash x}$ and $q_{x\\backslash y}$ .   \n641 Then, edge scores for directed edges, $X\\rightarrow Y$ and $Y\\rightarrow X$ , are defined w.r.t. to the edge removal   \n642 score, $X\\neq Y$ , by substracting the pair scores of unconnected pairs to the pair scores of directed   \n643 edges, leading to the following edge orientation scores, ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "Z2f4Laqi8U/tmp/a871a4e487903df000ebafe3bb224980fe0175d7e4950fc8b5dbde018e46203f.jpg", "table_caption": ["Table 2: Local scores for node pairs "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{Score}(X\\to Y)}&{=}&{-I(X;Y|{\\bf P a}_{\\mathrm{\\tiny~{\\tiny~{\\itx}}}\\mathrm{\\tiny~{\\itx}}}^{\\prime})+\\frac{1}{N}\\Big(\\begin{array}{c}{\\!\\!\\!\\!\\begin_{array}{c}{\\!\\!\\!q_{y\\mathrm{\\tiny{\\itx}}}r_{x}}\\end{array}\\!\\!\\!\\!\\begin{array}{c}{q_{y\\mathrm{\\tiny{\\itx}}}}\\\\ {\\sum_{j}\\log\\mathcal{C}_{n_{j}}^{r_{y}}-\\sum_{j}\\log\\mathcal{C}_{n_{j}}^{r_{y}}}\\end{array}\\!\\!\\!\\begin{array}{c}{q_{y\\mathrm{\\tiny{\\it~{\\itx}}}}}\\\\ {\\vdots}\\end{array}}\\\\ {\\mathrm{Score}(Y\\rightarrow X)}&{=}&{-I(X;Y|{\\bf P a}_{\\mathrm{\\tiny{\\itx}}\\mathrm{\\tiny{\\itv}}}^{\\prime})+\\frac{1}{N}\\Big(\\begin{array}{c c}{\\!\\!\\!\\!\\begin{array}{c}{q_{x\\mathrm{\\tiny{y}}}r_{y}}\\end{array}\\!\\!\\!\\begin{array}{c}{q_{x\\mathrm{\\tiny{y}}}}\\\\ {\\sum_{j}\\log\\mathcal{C}_{n_{j}}^{r_{x}}-\\sum_{j}\\log\\mathcal{C}_{n_{j}}^{r_{x}}}\\end{array}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "644 However, if $r_{x}\\ \\neq\\ r_{y}$ , the fNML complexities of these orientation scores are not identical for   \n645 Markov equivalent edge orientations between nodes sharing the same parents (or spouses) [47],   \n646 $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle{Y\\backslash X}}^{\\prime}=\\mathbf{\\bar{P}}\\mathbf{a}_{\\scriptscriptstyle{X\\backslash Y}}^{\\prime}=\\mathbf{P}\\mathbf{\\bar{a}}^{\\prime}$ and $q_{y\\backslash x}=q_{x\\backslash y}$ , despite sharing the same conditional mutual information, ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\begin{array}{l l l}{I(X;Y|\\mathbf{Pa}^{\\prime})}&{=}&{{\\frac{1}{2}}{\\Big(}H(X|\\mathbf{Pa}^{\\prime})+H(Y|\\mathbf{Pa}^{\\prime},X){\\Big)}+{\\frac{1}{2}}{\\Big(}H(X|\\mathbf{Pa}^{\\prime},Y)+H(Y|\\mathbf{Pa}^{\\prime}){\\Big)}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "647 This suggests to symmetrize the fNML complexities for edge orientation scores by averaging them   \n648 over each directed orientation, as for the conditional information in Eq. 36, leading to the proposed   \n649 fNML complexity for directed edges given in Table 1 in the main text.   \n650 For bidirected edges, the proposed local orientation score accounts for all $a c$ -connected subsets in   \n651 close vicinity of the bidirected edge, which concerns all subsets including either $X$ and any combi  \n652 nation (possibly void) of parents or spouses different from $Y$ (i.e. corresponding to the information   \n653 contributions $H(X|\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X\\scriptscriptstyle Y}^{\\prime}))$ or $Y$ and any combination of parents or spouses different from $X$   \n654 (i.e. corresponding to the information contributions $H(Y|\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y\\!\\setminus\\!x}^{\\prime}))$ or, else, including both nodes $X$   \n655 and $Y$ plus any combination of their parents or spouses, corresponding to the following information   \n656 contribution, $\\dot{-}I(X;Y|\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X Y}^{\\prime})$ , where $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X Y_{\\cdot}}^{\\prime}=\\mathbf{\\bar{P}a}_{\\scriptscriptstyle X Y}^{\\prime}\\cup\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y\\backslash X}^{\\prime}$ . This last term, $-I(\\bar{X};Y|{\\bf P a}_{{}_{X Y}}^{\\prime})$ ,   \n657 contains all the remaining information contributions once the bidirected orientation score is given   \n658 relative to the edge removal score (Table 2) as for the two directed orientation scores, above. Finally,   \n659 the symmetrized fNML complexity associated with a bidirected edge should be computed with   \n660 the whole set of conditioning parents or spouses, $\\mathbf{Pa}_{x Y}^{\\prime}$ , as indicated in Table 1. Note that this   \n661 bidirected orientation score becomes also Markov equivalent to the two directed orientation scores,   \n662 as required, when the nodes share the same parents and spouses, i.e. $\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X Y}^{\\prime}=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle Y\\!\\backslash X}^{\\prime}=\\mathbf{P}\\mathbf{a}_{\\scriptscriptstyle X\\!\\backslash Y}^{\\prime}$ and   \n663 $q_{x y}=q_{y\\backslash x}=q_{x\\backslash y}$ in Table 1. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "664 E Toy models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "665 Fig. 3 shows three simple ancestral models used to test MIIC_search&score orientation scores   \n666 (Table 1) to effectively predict bidirected orientations when the end nodes do not share the same   \n667 parents (Model 1), share some parents (Model 2) or when the bidirected edge is part of a longer than   \n668 two-collider paths (Model 3).   \n669 The data is generated from the theoretical DAG using the rmvDAG function in the pcalg package   \n670 [48]. Each node follows a normal distribution, and the data is discretized using bnlearn\u2019s discretize   \n671 function using Hartemink\u2019s pairwise mutual information method [40]. For these toy models, the edge   \n672 orientation scores are computed assuming the correct parents of each node.   \n673 The prediction of the edge orientation scores are summarized in Table 3 in $\\%$ of replicates displaying   \n674 directed edges (wrong) or bidirected edge (correct) as a function of increasing dataset size $N$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "Z2f4Laqi8U/tmp/a5ca10117f92a35de2da553bd0c5a47906724a6daf1e7b8835e07e4b0d8e2f38.jpg", "img_caption": ["Figure 3: Simple ancestral graphs. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "Z2f4Laqi8U/tmp/038d9596fa3cb55ace2a853d97a7b903763cd294c64abb4a41e5f3c02a36e8c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "675 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "676 1. Claims   \n677 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n678 paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims of the paper are supported by the theoretical and experimental results shown in Figs. 1 & 2, respectively. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "692 2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have added a Discussion & Limitation section at the end of the paper. The main limitation of the experimental results is the fact that we did not have sufficient time to perform many dataset replicates of the benchmark ancestral graphs. While the obtained statistics already support our main experimental results, we intend to perform more dataset replicates for the final version of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "727 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "28 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n29 a complete (and correct) proof?   \n30 Answer: [Yes]   \n31 Justification: For the theoretical results (notably Theorem 1) we provide the full set of   \n32 assumptions (section 2 and Appendix A)and a complete proof (Appendix B).   \n33 Guidelines:   \n34 \u2022 The answer NA means that the paper does not include theoretical results.   \n35 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n36 referenced.   \n37 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n38 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n39 they appear in the supplemental material, the authors are encouraged to provide a short   \n40 proof sketch to provide intuition.   \n41 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n42 by formal proofs provided in appendix or supplemental material.   \n43 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n744 4. Experimental Result Reproducibility ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We provided the full description of the experiments run in the paper (sections 2 & 3 and Appendix D). The open-source code reproducing the experimental results presented in the paper will be provided with the camera-ready version of the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 21}, {"type": "text", "text": "779 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n780 authors are welcome to describe the particular way they provide for reproducibility.   \n781 In the case of closed-source models, it may be that access to the model is limited in   \n782 some way (e.g., to registered users), but it should be possible for other researchers   \n783 to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "84 5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "85 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n86 tions to faithfully reproduce the main experimental results, as described in supplemental   \n87 material? ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not include a new code with the initial submission, as it is not yet properly packaged at submission time, but we definitely intend to release this open-source code including proper annotation and userguide with the final camera-ready version of the paper. MIIC and FCI open-source packages used for benchmark comparison are already published and available on public servers. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "814 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "15 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n16 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n17 results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provided the full description of the experiments run in the paper (sections 2 3 and Appendix D). ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "827 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "828 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n829 information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "830 Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "831 Justification: The 1-sigma error bars are plotted in Fig. 2. While these statistics already   \n832 support our experimental results, we intend to perform more dataset replicates for the   \n833 final version of the paper, which we did not have sufficient time to perform by the time of   \n834 submission. This should reduce some error bars, in particular, those for the results displaying   \n835 large error bars.   \n836 Guidelines:   \n837 \u2022 The answer NA means that the paper does not include experiments.   \n838 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n839 dence intervals, or statistical significance tests, at least for the experiments that support   \n840 the main claims of the paper.   \n841 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n842 example, train/test split, initialization, random drawing of some parameter, or overall   \n843 run with given experimental conditions).   \n844 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n845 call to a library function, bootstrap, etc.)   \n846 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n847 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n848 of the mean.   \n849 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n850 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n851 of Normality of errors is not verified.   \n852 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n853 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n854 error rates).   \n855 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n856 they were calculated and reference the corresponding figures or tables in the text.   \n857 8. Experiments Compute Resources   \n858 Question: For each experiment, does the paper provide sufficient information on the com  \n859 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n860 the experiments?   \n861 Answer: [Yes]   \n862 Justification: The computer resource used for all experiments is a simple laptop with intel i7   \n863 processors, 12 cores and 16 threads.   \n864 Guidelines:   \n865 \u2022 The answer NA means that the paper does not include experiments.   \n866 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n867 or cloud provider, including relevant memory and storage.   \n868 \u2022 The paper should provide the amount of compute required for each of the individual   \n869 experimental runs as well as estimate the total compute.   \n870 \u2022 The paper should disclose whether the full research project required more compute   \n871 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n872 didn\u2019t make it into the paper).   \n873 9. Code Of Ethics   \n874 Question: Does the research conducted in the paper conform, in every respect, with the   \n875 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n876 Answer: [Yes]   \n877 Justification: The paper does not use or produce sensitive data nor concern potentially   \n878 harmful applications.   \n879 Guidelines:   \n880 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n881 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n882 deviation from the Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid", "page_idx": 24}, {"type": "text", "text": "884 eration due to laws or regulations in their jurisdiction).   \n885 10. Broader Impacts   \n886 Question: Does the paper discuss both potential positive societal impacts and negative   \n887 societal impacts of the work performed?   \n888 Answer: [Yes]   \n889 Justification: The paper does not use or produce sensitive data nor concern potentially   \n890 harmful applications.   \n891 Guidelines:   \n892 \u2022 The answer NA means that there is no societal impact of the work performed.   \n893 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n894 impact or why the paper does not address societal impact.   \n895 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n896 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n897 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n898 groups), privacy considerations, and security considerations.   \n899 \u2022 The conference expects that many papers will be foundational research and not tied   \n900 to particular applications, let alone deployments. However, if there is a direct path to   \n901 any negative applications, the authors should point it out. For example, it is legitimate   \n902 to point out that an improvement in the quality of generative models could be used to   \n903 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n904 that a generic algorithm for optimizing neural networks could enable people to train   \n905 models that generate Deepfakes faster.   \n906 \u2022 The authors should consider possible harms that could arise when the technology is   \n907 being used as intended and functioning correctly, harms that could arise when the   \n908 technology is being used as intended but gives incorrect results, and harms following   \n909 from (intentional or unintentional) misuse of the technology.   \n910 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n911 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n912 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n913 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "15 Question: Does the paper describe safeguards that have been put in place for responsible   \n16 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n17 image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not use or produce sensitive data nor concern potentially harmful applications. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "932 12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "933 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n934 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n935 properly respected?   \n936 Answer: [Yes]   \n937 Justification: We have credited all previously published resources (including license details)   \n938 used in the paper.   \n939 Guidelines:   \n940 \u2022 The answer NA means that the paper does not use existing assets.   \n941 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n942 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n943 URL.   \n944 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n945 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n946 service of that source should be provided.   \n947 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n948 package should be provided. For popular datasets, paperswithcode.com/datasets   \n949 has curated licenses for some datasets. Their licensing guide can help determine the   \n950 license of a dataset.   \n951 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n952 the derived asset (if it has changed) should be provided.   \n953 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n954 the asset\u2019s creators.   \n955 13. New Assets   \n956 Question: Are new assets introduced in the paper well documented and is the documentation   \n957 provided alongside the assets?   \n958 Answer: [NA]   \n959 Justification: We do not include a new code with the initial submission, as it is not yet   \n960 properly packaged at submission time, but we definitely intend to release this open-source   \n961 code including proper annotation and userguide with the final camera-ready version of the   \n962 paper.   \n963 Guidelines:   \n964 \u2022 The answer NA means that the paper does not release new assets.   \n965 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n966 submissions via structured templates. This includes details about training, license,   \n967 limitations, etc.   \n968 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n969 asset is used.   \n970 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n971 create an anonymized URL or include an anonymized zip file.   \n972 14. Crowdsourcing and Research with Human Subjects   \n973 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n974 include the full text of instructions given to participants and screenshots, if applicable, as   \n975 well as details about compensation (if any)?   \n976 Answer: [NA]   \n977 Justification: The paper does not involve crowdsourcing nor research with human subjects   \n978 Guidelines:   \n979 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n980 human subjects.   \n981 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n982 tion of the paper involves human subjects, then as much detail as possible should be   \n983 included in the main paper.   \n984 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n985 or other labor should be paid at least the minimum wage in the country of the data   \n986 collector.   \n987 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n988 Subjects   \n989 Question: Does the paper describe potential risks incurred by study participants, whether   \n990 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n991 approvals (or an equivalent approval/review based on the requirements of your country or   \n992 institution) were obtained?   \n993 Answer: [NA]   \n994 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n995 Guidelines:   \n996 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n997 human subjects.   \n998 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n999 may be required for any human subjects research. If you obtained IRB approval, you   \n000 should clearly state this in the paper.   \n001 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n002 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n003 guidelines for their institution.   \n004 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n005 applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]