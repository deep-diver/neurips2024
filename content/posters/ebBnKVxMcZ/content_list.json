[{"type": "text", "text": "Confidence Calibration of Classifiers with Many Classes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adrien Le $\\mathbf{Coz^{1,2,3}}$ St\u00e9phane Herbin2,3 Faouzi Adjed1 1IRT SystemX 2ONERA - DTIS 3 Paris-Saclay University adrien.2mrvb@passinbox.com stephane.herbin@onera.fr faouzi.adjed@irt-systemx.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods. Our code can be accessed at the following link: https://github.com/allglc/tva-calibration. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The considerable performance increase of modern deep neural networks (DNNs) and their potential deployment in real-world applications has made reliably estimating the probability of wrong decisions a key concern. When such components are expected to be embedded in safety-critical systems (e.g., medical or transportation), estimating this probability is crucial to mitigate catastrophic behavior. One way to address this question is to treat it as an uncertainty quantification problem [2, 12], where the uncertainty value computed for each prediction is considered as a confidence. This confidence can be used to reject uncertain decisions proposed by the DNN [13], for out-of-distribution detection [22], or to control active learning [34] or reinforcement learning based systems [76]. When confidence values reliably reflect the true probability of correct decisions, i.e., their accuracy, a predictive system is said to be calibrated. In this case, confidence values can be used as a reliable control for decision-making. ", "page_idx": 0}, {"type": "text", "text": "We are interested in producing an uncertainty indicator for decision problems where the input is high dimensional and the decision space large, typically classifiers with tens to thousands of classes. For this kind of problem, DNNs are common predictors, and their outputs can be used to provide an uncertainty value at no cost, i.e., without necessitating heavy estimation such as Bayesian sampling [15] or ensemble methods [33]. Indeed, most neural architectures for classification instantiate their decision as a softmax layer, where the maximum value can be interpreted as the maximum of the posterior probability and, therefore, as a confidence. Unfortunately, uncertainty values computed in this way are often miscalibrated. DNNs have been shown to be over-confident [17], meaning their confidence is higher than their accuracy: predictions with $90\\%$ confidence might be correct only $80\\%$ of the time. A later study [44] suggests that model architecture impacts calibration more than model size, pre-training, and accuracy. For ImageNet classifiers, the accuracy and the number of model parameters are not correlated to calibration, but model families are [11]. ", "page_idx": 0}, {"type": "text", "text": "These studies show that it is difficult to anticipate the calibration level of confidence values computed directly from DNNs and exhibit the benefits of a complementary post-processing calibration. This calibration process can be seen as a learning step that exploits data from a calibration set, distinct from the training set, and is used to learn a function that maps classifier outputs into better-calibrated values. This process is typically lightweight and decoupled from the issue of improving model performance. A standard baseline for post-processing calibration is Temperature Scaling [17], where the penultimate logit layer is scaled by a coefficient optimized on the calibration set. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Many post-processing calibration methods have been developed for binary classification models [50, 69, 70]. Applying these methods to multiclass classifiers requires some adaptation. One standard approach reformulates the multiclass setting into many One-versus-All binary problems (one per class) [70]. One limitation of this approach is that it does not scale well. When the number of classes is large, the calibration data is divided into highly unbalanced subsets that do not contain enough positive examples to solve the One-versus-All binary problems. Other methods based on Platt scaling [50] involve learning a set of parameters whose size grows with the number of classes. For problems with many classes, they tend to overfit, as we demonstrate in this work. ", "page_idx": 1}, {"type": "text", "text": "The main idea of our work is to reformulate the multiclass confidence estimation into a single binary problem. This problem can be phrased as the unique question: \"Is the prediction correct?\". In this formulation, the confidence score is defined as the maximum class probability of the binary problem that outputs 1 if the predicted class is correct and 0 otherwise. The intent is that the confidence score accurately describes whether the prediction is correct, regardless of the class. We show that this novel approach, which we call Top-versus-All (TvA), significantly improves the performance of standard calibration methods: Temperature and Vector Scaling [17], Dirichlet Calibration [29], Histogram Binning [69], Isotonic Regression [70], Beta Calibration [28], and Bayesian Binning into Quantiles [46]. We also introduce a simple regularization for Vector Scaling or Dirichlet Calibration that mitigates overftiting when the number of classes is high relative to the calibration data size. We conduct experiments on multiple image and text classification datasets and many pre-trained models. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are the following: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We discuss four issues of the standard approach to confidence calibration. ", "page_idx": 1}, {"type": "text", "text": "\u2022 To solve these issues, we develop the Top-versus-All approach to confidence calibration of multiclass classifiers, transforming the problem into a single binary classifier\u2019s calibration. This straightforward reformulation enables more efficient use of existing calibration methods, achieved with minimal modifications to the methods\u2019 original algorithms. \u2022 Applied to scaling methods for calibration (such as Temperature Scaling), TvA allows the use of the binary cross-entropy loss, which is more efficient in decreasing the confidence of wrong predictions and leads to stronger gradients in the case of Temperature Scaling. Applied to binary methods for calibration (such as Histogram Binning), TvA significantly improves their performance and makes them accuracy-preserving. \u2022 We demonstrate our approach\u2019s scalability and generality with extensive experiments on image classification with state-of-the-art models for complex datasets and on text classification with Pre-trained Language Models (PLMs) and Large Language Models (LLMs). ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Calibration There are various notions of multiclass calibration. One can consider confidence [17], class-wise [28], top- $^r$ [19], top-label [18], decision [75], projection smooth [16], or strong [60, 65] calibration. For recent surveys, we refer to [10] and [63]. In this work, we focus on confidence calibration and not on the calibration of the full probability vector. Indeed, confidence calibration is useful for many applications that only require a single confidence value: selective classification [13], out-of-distribution detection [22], or active learning [34]. For these applications, stronger notions of calibration are both difficult and useless. Also, class-wise calibration metrics do not appropriately scale to large numbers of classes, a setting we consider in this work, as explained in Appendix E. ", "page_idx": 1}, {"type": "text", "text": "Metrics Several metrics have been proposed to quantify calibration error. The most common is the Expected Calibration Error (ECE) [46] (see Equation 2). ECE has flaws: the estimation quality is influenced by the binning scheme, and it is not a proper scoring rule [14, 60, 48]. Despite its flaws, it remains the standard comparison metric for confidence calibration. Variants of ECE have also been developed: classwise-ECE [29], ECE with equal mass bins [48, 44], or top-label-ECE, which adds a conditioning on the predicted class [18]. The Brier score [4] is also used to measure calibration. The proximity-informed expected calibration error (PIECE) evaluates the miscalibration due to proximity bias [68]. We mainly use the standard ECE in this work, and the Appendix contains more metrics. ", "page_idx": 1}, {"type": "text", "text": "Training calibrated networks Several solutions have been proposed in the literature to improve calibration by training neural networks in specific ways, generally by making use of a new loss term [31, 58, 26, 6, 5]. While these methods directly optimize calibration during the training phase of the networks, they require a high development time, often compromise accuracy, and are not adapted to pre-trained foundation models. That is why we prefer to focus on calibrating already-trained models. ", "page_idx": 2}, {"type": "text", "text": "Post-processing (or post-hoc) calibration methods Another approach is to calibrate alreadytrained models. This lowers the development time by decoupling accuracy optimization and calibration. In this paper, we divide post-hoc calibration methods into two categories: scaling and binary. Scaling methods are derived from Platt scaling [50] and optimize some parameters to scale the logits. Temperature Scaling [17] is a popular simple post-processing calibration method. The logits vector is scaled by a coefficient, which modifies the probability vector. Vector Scaling [17] is more expressive and has good performance in many cases [17, 48, 29]. Matrix Scaling can also be considered for more expressiveness but is difficult to apply without overftiting [17]. Dirichlet Calibration [29] proposes a regularization strategy for Matrix Scaling. [72] developed Ensemble Temperature Scaling. Scaling can be combined with binning [30]. Besides logits or probabilities, features can also be used [35]. Another family of methods tackles binary classification. We designate them as binary methods. Histogram Binning [69] divides the prediction into $B$ bins according to the predicted probability. For each bin, a calibrated probability is computed from the calibration data. The probability becomes discrete: it can only take $B$ values. With some modifications, it outperforms scaling methods [18, 49]. Isotonic Regression [70] learns a piecewise constant function to remap probabilities. Bayesian Binning into Quantiles [46] brings Bayesian model averaging to Histogram Binning. Beta Calibration [28] uses a beta distribution to obtain a calibration mapping. ", "page_idx": 2}, {"type": "text", "text": "Our work reformulates the multiclass calibration problem and allows more efficient use of all these calibration methods, with little to no change in their algorithms. ", "page_idx": 2}, {"type": "text", "text": "Multiclass to Binary Using binary calibration methods for a multiclass classifier requires adapting the multiclass setting. This is usually done with a One-versus-All approach [70, 17]. The multiclass setting is decomposed into $L$ One-versus-All independent problems: one binary problem for each class. [18] introduce the notion of top-label calibration, i.e., confidence calibration with additional conditioning on the predicted class (top-label). They describe a general multiclass-to-binary framework to develop top-label calibrators. [6] derive $L(\\dot{L}-1)/2$ pairwise binary problems. The approach requires training the classifier from scratch, and its performance decreases with the number of classes. Our work tackles this multiclass-to-binary research problem. Contrary to [6], the One-versus-All approach [70] and top-label calibrators [18], our approach works well for problems with many classes. The methods I-Max [49] and IRM [72] use a shared class-wise strategy to compute a single calibrator. The calibrator is applied to all class probabilities separately, so the class probabilities ranking and prediction might change. In contrast, TvA applied to binary methods rescales the confidence after the class prediction is made. I-Max and IRM consider the full class probabilities vectors, while TvA only considers confidence values. Also, they build on top of Histogram Binning and Isotonic Regression, respectively, while we apply our approach to many calibration methods. A concurrent work building on an intuition similar to ours derives a calibration method based on a Correctness-Aware Loss [38]. Appendix C discusses the differences between our approach and others. ", "page_idx": 2}, {"type": "text", "text": "3 Problem setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Confidence calibration of a classifier We consider the classification problem where an input $x$ is associated with a class label $y\\in\\mathcal{y}=\\{1,2,...,L\\}$ . The neural network classifier $f$ provides a class prediction from a final softmax layer $\\sigma$ that transforms intermediate logits $z$ into probabilities. The classifier prediction is the most probable class $\\hat{y}=\\arg\\operatorname*{max}_{k\\in\\mathcal{Y}}f_{k}(x)$ with $f_{k}(x)$ referring to the probability of class $k$ , and the confidence score defined as $s=\\operatorname*{max}_{k\\in\\mathcal{Y}}f_{k}(x)$ . Note that we use the term confidence to denote the maximum class probability. With $y$ the real label, we consider the confidence calibration definition from [17] that says that the classifier $f$ is calibrated if: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(\\hat{y}=y|s=p)=p,\\quad\\forall p\\in[0,1]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the probability is over the data distribution. Equation (1) expresses that the probability of being correct when the confidence is around $p$ is indeed $p$ . For instance, if we consider the set of predictions with a confidence of $90\\%$ , they should be correct $90\\%$ of the time. The conditional probability of (1) is not rigorously defined mathematically (the event $\\{s=p\\}$ has zero probability), and interval-based empirical estimators are often used to define metrics capable of evaluating how well (1) is satisfied. This is the case of ECE, which approximates the calibration error by partitioning the confidence distribution into $B$ bins. The absolute difference between the accuracy and confidence is computed for each subset of data in the bins. The final value is a weighted sum of the differences of each bin. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{ECE}=\\sum_{b=1}^{B}{\\frac{n_{b}}{N}}|\\mathrm{acc}(b)-\\mathrm{conf}(b)|\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $n_{b}$ is the number of samples in bin $b$ , $N$ is the total number of samples, $\\operatorname{acc}(b)$ is the accuracy in bin $b$ , and conf $(b)$ is the average confidence in bin $b$ . ECE can be interpreted visually by looking at diagrams such as those of Figure 1: ECE computes the sum of the red bars (difference between bin accuracy and average confidence) weighted by the proportion of samples in the bin. ", "page_idx": 3}, {"type": "text", "text": "Post-processing calibration methods We are considering the scenario where a classifier has already been trained, and the objective is to enhance its calibration. Post-processing calibration methods aim to remap the classifier probabilities to better-calibrated values without modifying the classifier. They typically use a calibration set different from the training set to optimize parameters or learn a function. We note the calibration data $D_{c a l}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ . We focus on post-processing calibration because it enables better utilization of off-the-shelf models and separates model training (optimized for accuracy) from calibration. These advantages significantly reduce the development cost of obtaining a well-performing and well-calibrated model, contrary to optimizing calibration during training. We categorize the post-processing calibration techniques considered in this paper into two groups: scaling methods and binary methods. ", "page_idx": 3}, {"type": "text", "text": "3.2 Issues related to current approaches ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Behavior of current scaling methods Scaling methods for calibration optimize one or more coefficients that scale the logits vector to minimize on calibration data the cross-entropy loss defined as $\\begin{array}{r}{l_{C E}\\,=\\,-\\sum_{k=1}^{L}1_{k=y}\\cdot\\log(f_{k}(x))\\,=\\,-\\log(f_{y}(x))}\\end{array}$ . Minimizing $l_{C E}$ therefore increases the probability of the true class. We can distinguish two cases to understand what happens during the optimization: whether the prediction $\\hat{y}$ is correct or not. In the first case, the confidence score is $s=f_{y}(x)$ : minimizing $l_{C E}$ increases the confidence $f_{y}(x)$ . In the second case, the prediction is incorrect, which implies that $f_{y}(x)<s$ . Minimizing $l_{C E}$ increases the probability of the true class $f_{y}(x)$ but does not directly change the confidence (because $s\\neq f_{y}(\\bar{x)})$ . Instead, the confidence (which was attributed to a wrong class) is indirectly lowered through the softmax normalization. ", "page_idx": 3}, {"type": "text", "text": "$\\hookrightarrow$ Issue 1: Cross-entropy loss only indirecly lowers confidence in wrong predictions. ", "page_idx": 3}, {"type": "text", "text": "We identified another issue of some scaling methods. By design, the number of parameters optimized by Vector Scaling and Dirichlet Calibration grows with the number of classes. When the number of classes is high, these methods overfit the calibration set as shown in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "$\\hookrightarrow$ Issue 2: Vector Scaling and Dirichlet Calibration overfit calibration sets with many classes. ", "page_idx": 3}, {"type": "text", "text": "One-versus-All approach for binary methods The One-versus-All (OvA) calibration approach [70] allows adapting calibration methods for binary classifiers to multiclass classifiers. To do so, it decomposes the calibration of multiclass classifiers into sets of $L$ binary calibration problems: one for each class $k$ . For each problem, the considered probability is $f_{k}(x)$ , and the associated label $1_{y=k}\\in\\{0,1\\}$ . When calibrating a classifier from data, each binary problem is highly imbalanced with a ratio between positive and negative examples equal to $\\frac{1}{L-1}$ if the classes are equally sampled. For instance, for ImageNet, the ratio is 1/999: out of 25000 examples, only 25 have a positive label. ", "page_idx": 3}, {"type": "text", "text": "\u2192Issue 3: OvA approach leads to highly imbalanced binary problems. ", "page_idx": 3}, {"type": "text", "text": "At test time, each of the $L$ class probabilities is calibrated by a separate calibration model. The resulting probability vector can be normalized to ensure a unit norm. Because each probability is calibrated independently, their ranking can change, thus modifying the predicted class. In Table 9, we see that accuracy is often negatively impacted in practice. ", "page_idx": 3}, {"type": "text", "text": "$\\hookrightarrow$ Issue 4: OvA approach can change the predicted class and negatively impact the accuracy. ", "page_idx": 3}, {"type": "text", "text": "4 Top-versus-All approach to confidence calibration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "ebBnKVxMcZ/tmp/59e8e2136c2af36683433bf8ecd598e789d11bd365a30e4d4c71431e0d6880de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 General presentation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the calibration definition (1) and the standard ECE metrics, only the confidence, i.e., the maximal probability, reflects the likelihood of making an accurate prediction. The probabilities of other classes are not taken into account. However, the standard approach to calibration uses the entire set of probabilities, not just confidence, which introduces unnecessary complexity. We aim to simplify the process by reformulating the problem of calibrating multiclass classifiers into a single binary problem. This problem can be phrased as: \"Is the prediction correct?\". In this setting, we do not calibrate the predicted probabilities vector but only a scalar: the confidence. The remaining probabilities are discarded. This is equivalent to calibrating a surrogate binary classifier that predicts whether the class prediction is correct. Since this correctness classifier only considers the maximal probability versus all others, we call our approach Top-versus-All (TvA). ", "page_idx": 4}, {"type": "text", "text": "Replacing the standard approach by TvA is straightforward. Given the standard calibration data $\\bar{D_{c a l}}\\;=\\;\\overline{{{\\bf\\zeta}}}\\{(x_{i},y_{i})\\}_{i=1}^{N}$ , we add a few data preprocessing steps. First, compute the class predictions $\\hat{y}$ and their correctness: $\\boldsymbol{y}^{b}~=~\\boldsymbol{1}_{\\hat{y}=y}$ . Second, create the surrogate binary classifier $f^{b}(x)=\\operatorname*{max}_{k\\in\\mathcal{Y}}f_{k}(x)$ . Finally, build the calibration set for the surrogate binary classifier: ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{c a l}^{\\mathrm{TvA}}=\\{(x_{i},y_{i}^{b})\\}_{i=1}^{N}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After this preprocessing, we choose a standard calibration function $g$ , e.g., Temperature Scaling, to calibrate the surrogate binary classifier. The learning of the calibration function follows its original underlying algorithm but uses the modified calibration data $D_{c a l}^{\\mathrm{TvA}}$ . The learned calibration function is then applied to the confidences of the original multiclass classifier. Algorithm 1 describes our approach. In the Appendix, Algorithm 3 provides more details and highlights differences with the standard approach of Algorithm 2. ", "page_idx": 4}, {"type": "text", "text": "After this general presentation, we explain how TvA impacts the two categories of calibration methods, scaling and binary, in Subsections 4.2 and 4.3, respectively. We also justify its behavior. ", "page_idx": 4}, {"type": "text", "text": "4.2 Top-versus-All approach for scaling methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Because our Top-versus-All setting reformulates the calibration of multiclass classifiers into a binary problem, the natural loss is the binary cross-entropy: ", "page_idx": 4}, {"type": "equation", "text": "$$\nl_{B C E}=-\\left(y^{b}\\cdot\\log s+(1-y^{b})\\cdot\\log(1-s)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Minimizing this loss results in confidence estimates that more accurately describe the probability of being correct, regardless of the $L-1$ less likely class predictions. Using the binary cross-entropy as a calibration loss makes an important difference compared to the usual multiclass cross-entropy. The cross-entropy loss takes into account the probability of the correct class, while with TvA the binary cross-entropy takes into account the probability of the predicted class (i.e., the confidence). ", "page_idx": 4}, {"type": "text", "text": "As for the standard approach, only two cases are possible. When the prediction is correct, $l_{B C E}=$ $-\\log(s)\\,=\\,-\\log(\\dot{f_{y}}\\bar{)}\\,=\\,l_{C E}$ . We get the same result as the cross-entropy loss: minimizing it directly increases the confidence. But when the prediction is incorrect, $l_{B C E}=-\\log(1-s)\\neq l_{C E}$ . Minimizing the loss now directly decreases the confidence. This is a key difference compared to using the multiclass cross-entropy loss. ", "page_idx": 5}, {"type": "text", "text": "The impact of the reformulation can be seen for Temperature Scaling, which optimizes a coefficient $T$ that scales the logits $z_{k}$ . The reformulation generates stronger gradients when the prediction is incorrect: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial l_{B C E}}{\\partial T}\\right|>\\left|\\frac{\\partial l_{C E}}{\\partial T}\\right|~\\mathrm{for}~s>0.5\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\begin{array}{r}{\\frac{\\partial l_{B C E}}{\\partial T}=\\frac{1}{T^{2}}\\cdot\\frac{1}{s-1}\\cdot(\\operatorname*{max}_{k}z_{k}-\\sum_{k}z_{k}\\cdot f_{k})}\\end{array}$ and $\\begin{array}{r}{\\frac{\\partial l_{C E}}{\\partial T}=\\frac{1}{T^{2}}\\left(z_{y}-\\sum_{k}z_{k}\\cdot f_{k}\\right)}\\end{array}$ . See Appendix D for the mathematical calculations. Because for interesting problems, the confidence verifies $s>0.5$ most of the time (as shown in Figure 1), our approach strengthens the gradients. The optimization of the temperature $T$ is more efficient as confident incorrect predictions are more heavily penalized. This effect is not mitigated by the choice of learning rate, which does not vary with $s$ . Applying standard Temperature Scaling usually results in overconfident probabilities, but our approach limits this overconfidence. This is verified experimentally in Table 8, which displays the average confidences for TS without and with TvA. ", "page_idx": 5}, {"type": "text", "text": "$\\hookrightarrow$ Solution for Issue 1: Use the binary cross-entropy loss resulting from TvA approach. ", "page_idx": 5}, {"type": "text", "text": "Regularization of scaling methods Overfitting of Vector Scaling and Dirichlet Calibration can be reduced with a simple L2 regularization that penalizes the coefficients of the vector $v$ that are far from the reference value 1. ", "page_idx": 5}, {"type": "equation", "text": "$$\nl_{r e g}(v)=\\frac{1}{L}\\sum_{i=1}^{L}(v_{i}-1)^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This regularization allows these methods to take advantage of their additional expressiveness without being subject to overfitting. The loss for Vector Scaling becomes $l_{B C E}+\\lambda l_{r e g}(v)$ where $\\lambda$ is a hyperparameter. The loss for Dirichlet Calibration uses additional matrix regularization terms. $\\lambda$ is the only additional hyperparameter introduced by our method, and it applies only to Vector Scaling and Dirichlet Calibration. ", "page_idx": 5}, {"type": "text", "text": "$\\hookrightarrow$ Solution for Issue 2: Use L2 regularization. ", "page_idx": 5}, {"type": "text", "text": "4.3 Top-versus-All approach for binary methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our TvA approach replaces the One-versus-All approach to apply binary methods to the multiclass setting. TvA transforms the multiclass setting into a single binary problem that uses the binary calibration dataset (3). In this dataset, the proportion of positive labels equals the classifier\u2019s accuracy a. The ratio between negative and positive examples is (1\u2212aaN)N $\\begin{array}{r}{\\frac{(1-a)N}{a N}=\\frac{1}{a}-\\overline{{1}}}\\end{array}$ = a1 \u22121. For a classifier with 80% accuracy on ImageNet and a calibration dataset of 25000 examples, there are 5000 negative and 20000 positive examples (ratio of 1/4). This is still a bit imbalanced but orders of magnitude smaller than the class-wise binary calibration datasets of the One-versus-All approach (ratio of 1/999). ", "page_idx": 5}, {"type": "text", "text": "$\\hookrightarrow$ Solution for Issue 3: By not dividing the calibration data into class-wise datasets, the TvA approach yields a much better balanced binary calibration problem. ", "page_idx": 5}, {"type": "text", "text": "The Top-versus-All approach operates on confidence alone, not the full class probabilities vector. This means that the class prediction is already done, and the ranking of the class probabilities does not change, unlike with One-versus-All. The classifier\u2019s prediction and accuracy are unaffected. This scheme allows decoupling accuracy improvements (during training time) and calibration (during post-processing calibration), thus avoiding compromises and reducing development time. ", "page_idx": 5}, {"type": "text", "text": "$\\hookrightarrow$ Solution for Issue 4: By operating on confidence alone, the Top-versus-All approach does not impact the classifier\u2019s prediction or accuracy for binary methods applied to the multiclass scenario. ", "page_idx": 5}, {"type": "image", "img_path": "ebBnKVxMcZ/tmp/1d89712eb1230c5d056c5dc6e3c61ccfa23d31f05a637a7485a648472b3891c3.jpg", "img_caption": ["Figure 1: Reliability diagrams for ResNet-50 and ViT-B/16 when using Temperature Scaling (TS), Vector Scaling (VS), and Histogram Binning (HB) on ImageNet. The subscript $\\mathrm{TvA}$ signifies that the TvA reformulation was used, and $\\mathrm{reg}$ means our regularization (6) was applied. Red bars show the differences between bin accuracy (blue bar) and accuracy for perfect calibration (dashed red line). As the methods improve the calibration, these differences are reduced and the average confidence (vertical dotted line) will get closer to the global accuracy (vertical dashed line). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and models For image classification, we used the datasets CIFAR-10 (C10) and CIFAR100 (C100) [27] with 10 and 100 classes respectively, ImageNet (IN) [7] with 1000 classes, and ImageNet-21K (IN21K) [54] with 10450 classes. For text classification, we used Amazon Fine Foods $(A F F)$ [43] and DynaSent $(D F)$ [51] for sentiment analysis with 3 classes, MNLI [66] for natural language inference with 3 classes, and Yahoo Answers (YA) [73] for topic classification on 10 classes. Experiment results are averaged over five random seeds that randomly split the concatenation of the original validation and test sets into calibration and test sets. ", "page_idx": 6}, {"type": "text", "text": "We used the following models for image classification: ResNet [21], Wide-ResNet-26-10 (WRN) [71], DenseNet-121 [24], MobileNetV3 (MN3) [23], ViT [9], ConvNeXt [41], EfficientNet [56, 57], Swin [40, 39], and CLIP [52] which matches input images to text descriptions in a shared embedding space, assigning labels based on the highest similarity score. For text classification, we used the PLMs RoBERTa [37] and T5 [53]. ", "page_idx": 6}, {"type": "text", "text": "More details about datasets, calibration sets sizes, and model weights can be seen in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "Baselines Our Top-versus-All $\\mathrm{(}_{\\mathrm{TvA}})$ reformulation and regularization $\\mathrm{(_{reg})}$ can be applied to different calibration methods. We have tested the following scaling methods: Temperature Scaling (TS) and Vector Scaling (VS) [17], and Dirichlet Calibration $(D C)$ [29] with the best-performing variant Dir-ODIR, which regularizes off-diagonal and bias coefficients. We also tested the following binary methods: Histogram Binning (HB) [69] using for each case the best-performing variant between equal-mass or equal-size bins, Isotonic Regression (Iso) [70], Beta Calibration (Beta) [28], and Bayesian Binning into Quantiles (BBQ) [46]. For comparison, we include methods with state-ofthe-art results on problems with many classes: I-Max [49] and IRM [72]. See Appendix C for more details on these methods. More details on code implementations can be seen in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "Table 1: ECE in $\\%$ (lower is better). The subscript $\\mathrm{TvA}$ denotes that our reformulation was applied to the calibration method. IRM and I-Max are competing methods. The best method for a given model is in bold. Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Values are averaged over five random seeds. Results are averaged over models of the same family. Detailed results for all models can be seen in Tables 5 and 6 of the Appendix. ", "page_idx": 7}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/a0e6960fe98a9ca4222bd6f53541008858b99c3a476b25b41442cef9f7316af3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Top-versus-All ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For visual qualitative results, Figure 1 displays reliability diagrams [47]. We observe that initially, ResNet-50 is highly underconfident and ViT-B/16 a bit underconfident. Applying TS and VS solves the underconfidence and makes the models slightly overconfident. TvA improves these methods, and the average confidence gets closer to the accuracy. $\\mathrm{HB_{TvA}}$ makes the calibration almost perfect. ", "page_idx": 7}, {"type": "text", "text": "Table 1 shows the results of applying the Top-versus-All reformulation to several calibration methods. For clarity, results are averaged over families of models (models based on the same architecture) and the full results are available in Tables 5 and 6 of the Appendix. In most cases, the TvA reformulation significantly lowers the ECE by dozens of percent. Without TvA, binary methods often perturb the prediction and degrade the classifier\u2019s accuracy (see Table 9), making them inapplicable in a practical setting. TvA solves the issue as it only scales the confidence (after the prediction is made) and makes binary methods outperform scaling methods. ", "page_idx": 7}, {"type": "text", "text": "Improvements due to TvA are consistent across models. However, exceptions are observed for CLIP: it is the model family with the lowest ECE pre-calibration, but the highest ECE post-calibration for ImageNet. CLIP\u2019s multimodal training regime, zero-shot adaptation as a classifier, and very large training dataset might cause this different behavior. CLIP\u2019s low ECE was also observed in [44, 11]. [64] specifically tackles the calibration of fine-tuned CLIP, a setting not considered here. ", "page_idx": 7}, {"type": "text", "text": "We also found that DC is sensitive to hyperparameter tuning, and its performance is usually not much better than VS, which is consistent with [29]. In some cases, the optimization diverges, leading to very poor results, e.g., for CLIP on ImageNet. ", "page_idx": 7}, {"type": "text", "text": "Improvements due to TvA are also consistent across datasets, although they tend to increase with the number of classes. Improvements on ImageNet are usually better than on CIFAR-100, whose improvements are usually better than on CIFAR-10. This is notable with e.g., TS or HB. The magnitude of improvement is usually higher for binary methods, e.g., HB, than scaling methods, e.g., TS, especially for many classes. This indicates that Issue 3 is more serious than Issue 1. ", "page_idx": 7}, {"type": "text", "text": "For text datasets with only three classes (AFF, DS, and MNLI), TS does not benefit from TvA, but other methods do, despite the small number of classes. According to [5], TS is among the best calibration methods for the text classification tasks considered here, even compared to ones that retrain the model. Even so, our method $\\mathrm{HB_{TvA}}$ significantly outperforms it. ", "page_idx": 7}, {"type": "text", "text": "Some methods\u2019 current implementations could not handle the large scale of ImageNet-21K, resulting in out-of-memory errors written as \"err.\" in the Table. For I-Max and IRM, this is because they consider the full probability vectors while TvA efficiently uses data by considering only confidence values. Indeed, TvA handles this scale without difficulty. ", "page_idx": 8}, {"type": "text", "text": "Additional results are included in Appendix H. Tables 5 and 6 contain the full results for ECE, with the standard deviations in Table 7. Table 8 reveals that ImageNet networks are mostly underconfident. This is aligned with [11] and goes against previous knowledge on overconfidence, which was initially believed to be linked to network size [17]. Table 9 provides the accuracies after calibration. Table 10 exhibits that ECE with equal-mass bins has similar values as standard ECE. Table 11 shows that TvA mostly lowers the Brier score, except for Iso, which has the lowest score overall. ", "page_idx": 8}, {"type": "text", "text": "Calibration methods can also be applied to Large Languages Models (LLMs) using In-Context Learning (ICL) to tackle text classification tasks [77, 20, 25, 78, 1]. The primary goal of these methods is to improve model accuracy. TvA was not designed for this objective, but it can still be applied on top of an existing method that improves the accuracy. TvA then lowers the calibration error while keeping the accuracy gain. Results for GPT-J [62] and Llama-2 [59] are in Table 12. ", "page_idx": 8}, {"type": "text", "text": "To summarize the results for practical use, our experiments show that Histogram Binning (within the TvA or I-Max setting) is the best calibration method overall, providing ECE values mostly below $1\\%$ . This is the method we advise using. However, suppose the underlying application requires a confidence with continuous values, e.g., to rank the predictions for selective classification. In that case, we advise using a method that improves the AUROC, shown in Appendix G, such as TS or Iso. ", "page_idx": 8}, {"type": "text", "text": "5.3 Solving overfitting with regularization and TvA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "On ImageNet, VS and DC overfit the calibration set, degrading the calibration on the test set. The lower performance of VS relative to TS indicates this overftiting. As visualized in Figure 2, combining the binary cross-entropy loss used in the TvA reformulation and an additional regularization term prevents overfitting. We fixed the value $\\lambda=0.01$ as it works well across models. Initializing the vector coefficients to $\\textstyle{\\frac{1}{T}}$ with $T$ obtained by $\\mathrm{TS}_{\\mathrm{TvA}}$ helps further improve performance. ", "page_idx": 8}, {"type": "text", "text": "5.4 Influence of the calibration set size ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The size of the calibration set influences the performance of the different methods, as seen in Figure 3. TS and $\\mathrm{TS}_{\\mathrm{TvA}}$ do not benefit from more data due to their low expressiveness. VS does not improve the ECE because of the overftiting problem. In contrast, $\\mathrm{VS}_{\\mathrm{reg}_{\\mathrm{-}}\\mathrm{TvA}}$ beneftis from more calibration data. With enough data $(\\approx15000)$ , it outperforms $\\mathrm{TS}_{\\mathrm{TvA}}$ . Binary methods using the standard One-versus-All approach have poor performance and need a large amount of data to be competitive. Using TvA, they get excellent performance with little data. ", "page_idx": 8}, {"type": "image", "img_path": "ebBnKVxMcZ/tmp/2f381f135ad6c69ee8a6100dfe1e067832a387d4704896abd02f25ff0864693e.jpg", "img_caption": ["Figure 2: Test ECE evolution during training with ResNet-50 on ImageNet. The combination of regularization and TvA prevents overftiting of Vector Scaling. Temperature Scaling with TvA is shown for reference. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ebBnKVxMcZ/tmp/d3655057dd06f35c1785ea7d185f8529ddfee70961f2107de134931c32cb1bb8.jpg", "img_caption": ["Figure 3: Influence of the calibration set size for ResNet-101 on ImageNet. Binary methods at the top and scaling methods at the bottom. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our approach tackles confidence calibration and is unlikely to improve performance for stronger notions of calibration, such as class-wise calibration. However, confidence calibration is useful for many practical cases, such as selective classification [13], out-of-distribution detection [22], or active learning [34]. Also, calibration improvements are less significant for problems with few classes $(\\le10)$ ) than for problems with many classes, but our approach still provides the best results. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Reducing the miscalibration of neural networks is essential to improve trust in their predictions. This can be done after the model training with an optimization using calibration data. However, many current calibration methods do not scale well to complex datasets: binary methods under the One-versus-All setting do not have enough per-class calibration data, and scaling methods are inefficient. We demonstrate that reformulating the confidence calibration of multiclass classifiers as a single binary problem significantly improves the performance of baseline calibration techniques. The competitiveness of scaling methods is increased, and binary methods use per-class calibration data more efficiently without altering the model\u2019s accuracy. In short, our TvA reformulation enhances many existing calibration methods with little to no change in their algorithm. Extensive experiments with state-of-the-art image classification models on complex datasets and with text classification demonstrate our approach\u2019s scalability and generality. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Tahar Nabil, Houssem Ouertatani, and Pol Labarbarie for their constructive feedback on the paper draft.   \nThis work has been supported by the French government under the \"France 2030\u201d program, as part of the SystemX Technological Research Institute.   \nThis work was granted access to the HPC/AI resources of IDRIS under the allocation 2024- AD011013372R1 made by GENCI. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Momin Abbas, Yi Zhou, Parikshit Ram, Nathalie Baracaldo, Horst Samulowitz, Theodoros Salonidis, and Tianyi Chen. Enhancing in-context learning via linear probe calibration. arXiv preprint arXiv:2401.12406, 2024. [2] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243\u2013297, December 2021. [3] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS \u201924). ACM, April 2024.   \n[4] Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1\u20133, 1950.   \n[5] Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A Close Look into the Calibration of Pre-trained Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1343\u20131367, Toronto, Canada, 2023. Association for Computational Linguistics.   \n[6] Jiacheng Cheng and Nuno Vasconcelos. Calibrating Deep Neural Networks by Pairwise Constraints. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13709\u2013 13718, 2022. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, June 2009. [8] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[10] Telmo Silva Filho, Hao Song, Miquel Perello-Nieto, Raul Santos-Rodriguez, Meelis Kull, and Peter Flach. Classifier Calibration: A survey on how to assess and improve predicted class probabilities. Machine Learning, 112(9):3211\u20133260, September 2023.   \n[11] Ido Galil, Mohammed Dabbah, and Ran El-Yaniv. What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers? In The Eleventh International Conference on Learning Representations, February 2023.   \n[12] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in deep neural networks. Artificial Intelligence Review, July 2023.   \n[13] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. Advances in neural information processing systems, 30, 2017.   \n[14] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359\u2013378, 2007.   \n[15] Ethan Goan and Clinton Fookes. Bayesian neural networks: An introduction and survey. Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet Chair, Fall 2018, pages 45\u201387, 2020.   \n[16] Parikshit Gopalan, Lunjia Hu, and Guy N Rothblum. On computationally efficient multi-class calibration. arXiv preprint arXiv:2402.07821, 2024.   \n[17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[18] Chirag Gupta and Aaditya Ramdas. Top-label calibration and multiclass-to-binary reductions. In International Conference on Learning Representations, January 2022.   \n[19] Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, and Richard Hartley. Calibration of Neural Networks using Splines. In International Conference on Learning Representations, December 2021.   \n[20] Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. Prototypical calibration for few-shot learning of language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[22] Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. In International Conference on Learning Representations, 2017.   \n[23] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1314\u20131324, 2019.   \n[24] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[25] Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, and Kang Liu. Generative calibration for in-context learning. arXiv preprint arXiv:2310.10266, 2023.   \n[26] Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C Mozer, and Becca Roelofs. Soft calibration objectives for neural networks. Advances in Neural Information Processing Systems, 34:29768\u201329779, 2021.   \n[27] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.   \n[28] Meelis Kull, Telmo M. Silva Filho, and Peter Flach. Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. Electronic Journal of Statistics, 11(2):5052\u20135080, January 2017.   \n[29] Meelis Kull, Miquel Perello Nieto, Markus K\u00e4ngsepp, Telmo Silva Filho, Hao Song, and Peter Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. Advances in neural information processing systems, 32, 2019.   \n[30] Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. Advances in Neural Information Processing Systems, 32, 2019.   \n[31] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings. In Proceedings of the 35th International Conference on Machine Learning, pages 2805\u20132814. PMLR, July 2018.   \n[32] Fabian K\u00fcppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In The IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020.   \n[33] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.   \n[34] Mingkun Li and I.K. Sethi. Confidence-based active learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(8):1251\u20131261, August 2006.   \n[35] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Taking a Step Back with KCal: Multi-Class KernelBased Calibration for Deep Neural Networks. In The Eleventh International Conference on Learning Representations, September 2022.   \n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n[37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[38] Yuchi Liu, Lei Wang, Yuli Zou, James Zou, and Liang Zheng. Optimizing calibration by gaining aware of prediction correctness. arXiv preprint arXiv:2404.13016, 2024.   \n[39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12009\u201312019, 2022.   \n[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[41] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022.   \n[42] TorchVision maintainers and contributors. Torchvision: Pytorch\u2019s computer vision library. https: //github.com/pytorch/vision, 2016.   \n[43] Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In Proceedings of the 22nd International Conference on World Wide Web, WWW \u201913, page 897\u2013908, New York, NY, USA, 2013. Association for Computing Machinery.   \n[44] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the Calibration of Modern Neural Networks. In Advances in Neural Information Processing Systems, volume 34, pages 15682\u201315694. Curran Associates, Inc., 2021.   \n[45] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania. Calibrating Deep Neural Networks using Focal Loss. In Advances in Neural Information Processing Systems, volume 33, pages 15288\u201315299. Curran Associates, Inc., 2020.   \n[46] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. Proceedings of the AAAI Conference on Artificial Intelligence, 29(1), February 2015.   \n[47] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd International Conference on Machine Learning, ICML \u201905, pages 625\u2013632, New York, NY, USA, August 2005. Association for Computing Machinery.   \n[48] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In CVPR workshops, volume 2, 2019.   \n[49] Kanil Patel, William H Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang. Multi-class uncertainty calibration via mutual information maximization-based binning. In International Conference on Learning Representations, 2020.   \n[50] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999.   \n[51] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. DynaSent: A dynamic benchmark for sentiment analysis. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2388\u20132404, Online, August 2021. Association for Computational Linguistics.   \n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[54] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.   \n[55] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew $\\textrm{Y N g}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.   \n[56] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[57] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International conference on machine learning, pages 10096\u201310106. PMLR, 2021.   \n[58] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[59] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[60] Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas Sch\u00f6n. Evaluating model calibration in classification. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3459\u20133467. PMLR, 2019.   \n[61] Ellen M. Voorhees and Dawn M. Tice. Building a question answering test collection. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201900, page 200\u2013207, New York, NY, USA, 2000. Association for Computing Machinery.   \n[62] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.   \n[63] Cheng Wang. Calibration in deep learning: A survey of the state-of-the-art. arXiv preprint arXiv:2308.01222, 2023.   \n[64] Shuoyuan Wang, Jindong Wang, Guoqing Wang, Bob Zhang, Kaiyang Zhou, and Hongxin Wei. Openvocabulary calibration for vision-language models. arXiv preprint arXiv:2402.04655, 2024.   \n[65] David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classification: A unifying framework. Advances in neural information processing systems, 32, 2019.   \n[66] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.   \n[67] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020. Association for Computational Linguistics.   \n[68] Miao Xiong, Ailin Deng, Pang Wei W Koh, Jiaying Wu, Shen Li, Jianqing Xu, and Bryan Hooi. Proximityinformed calibration for deep neural networks. Advances in Neural Information Processing Systems, 36:68511\u201368538, 2023.   \n[69] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages 609\u2013616, San Francisco, CA, USA, June 2001. Morgan Kaufmann Publishers Inc.   \n[70] Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201902, pages 694\u2013699, New York, NY, USA, July 2002. Association for Computing Machinery.   \n[71] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.   \n[72] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In International conference on machine learning, pages 11117\u201311128. PMLR, 2020.   \n[73] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \n[74] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \n[75] Shengjia Zhao, Michael Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating predictions to decisions: A novel approach to multi-class calibration. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 22313\u201322324. Curran Associates, Inc., 2021.   \n[76] Xujiang Zhao, Shu Hu, Jin-Hee Cho, and Feng Chen. Uncertainty-based Decision Making Using Deep Reinforcement Learning. In 2019 22th International Conference on Information Fusion (FUSION), pages 1\u20138, July 2019.   \n[77] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pages 12697\u201312706. PMLR, 2021.   \n[78] Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. arXiv preprint arXiv:2309.17249, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B discusses the broader impacts of the work. ", "page_idx": 15}, {"type": "text", "text": "C discusses the proposed approach in more details, and compares with other methods. D contains a theoretical justification for Top-versus-All in the case of Temperature Scaling. E discusses the limits of classwise-ECE and top-label-ECE for a high number of classes. F describes implementation details, including the computing time in Table 3. G shows the impact of different calibration methods on selective classification. H provides additional results: full ECE results in Table 5 and 6, standard deviations in Table 7, confidences in Table 8, accuracies in Table 9, equal-mass bins ECE in Table 10, Brier score in Table 11, experiments for in-context learning of LLMs in Table 12. ", "page_idx": 15}, {"type": "text", "text": "B Broader impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our reformulation of the confidence calibration of multiclass classifiers as a binary problem is both simple and general. It has several beneftis. On the theoretical side, it might lead to new perspectives on the confidence calibration problem and the development of new calibration methods. On the practical side, existing calibration methods can be adapted to our problem reformulation by adding just a few lines of code. This is an easy and quick way to improve the calibration of classification models. Better-calibrated models are more trustworthy: potential incorrect predictions are more easily identifiable and preventable. However, this also comes with potential risks. The knowledge that a model is well-calibrated might lead to undue trust in the system and the tendency to overlook prediction errors. Even well-calibrated models are not entirely reliable, and developers and users must remember this. Post-processing calibration requires data not included in the training set, which leaves less data available for a thorough evaluation of the model. Calibration does not fix biases in the data. Finally, we tested the calibration improvement only on in-distribution data, but real systems might receive out-of-distribution data (e.g., an image of a new class) or adversarial examples. For such inputs, the classifier predicted probabilities (and thus the confidence) are unreliable, even for well-calibrated models, and a pipeline to filter such data is necessary. ", "page_idx": 15}, {"type": "text", "text": "C Details on the method ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/4e7118abd9b4f430b3d8e39f75df659667d07420a7eac3439783d72392d7a163.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Comparison with the standard approach Algorithm 2 describes the standard approach to postprocessing calibration, and Algorithm 3 describes our approach in more details and shows in blue the differences with the standard approach. Our approach adds a preprocessing step to keep only the confidences instead of the full probabilities vector. It can be seen as creating a surrogate \"correctness\" classifier and its associated calibration data. The calibrator is learned for the surrogate classifier and applied to the original classifier at inference time. Also, we add regularization for some scaling methods and we have only one binary calibrator instead of one per class. ", "page_idx": 16}, {"type": "text", "text": "Comparison with IRM and I-Max IRM [72] and I-Max [49] are, like TvA, multiclass-to-binary reductions. This is why TvA cannot be applied on top of them: they already transform the multiclass problem into a binary one using a different strategy. ", "page_idx": 16}, {"type": "text", "text": "The shared class-wise strategy of [49] and the data ensemble strategy of [72] are described very briefly in subsections 3.2 and 3.3.2 of their respective papers and not rigorously justified. Our understanding is that these two strategies do exactly the same thing. To build the calibration set, they concatenate all the class probability vectors so that we get a big probability vector of size $N.L$ ( $N$ samples and $L$ classes) as predictions and similarly concatenate the one-hot embedding of the target class (a big vector with $N$ ones and $N.(L-1)$ zeros) as targets. Then, they learn a single calibrator. For each example, this calibrator aims to simultaneously increase the probabilities for the target class (target is 1) and decrease all the other class probabilities (target is 0). The single calibrator is applied to each class probability separately, meaning that the ranking of class probabilities can change, modifying the classifier prediction. ", "page_idx": 16}, {"type": "text", "text": "Our strategy derives from transforming the multiclass calibration into a single binary problem. The intuition is to learn the calibrator on a surrogate binary classifier and apply this calibrator to the original classifier. This binary classifier is built on top of the original classifier (by applying the max function to the class probabilities vector). They thus share their confidence. However, the binary classifier aims to solve a different task: predicting the correctness of the original classifier. To build the calibration set, we concatenate all the confidences (a vector of size N) as predictions and concatenate all the correctnesses as targets (also a vector of size N). The correctness value of a given example is 1 if the class prediction is correct; otherwise, it is 0. Then, we learn a single calibrator, similar to the strategy above. However, there is a key difference: this calibrator aims to increase the probabilities for correct predictions and decrease them for incorrect predictions. Note that our probabilities are all confidences (the maximum class probabilities), meaning we only consider the confidences, which the calibrator directly increases or decreases. In the strategy from [49] and [72], the calibrator has to manage all class probabilities (L times more), even the ones that do not matter, including the lowest class probabilities close to 0. This is less efficient (actually, while this can surely be fixed, the original implementations of IRM and I-Max could not run on ImageNet-21K). This point is closely linked to the analysis of the binary cross entropy loss for scaling methods in Subsection 4.2: when the prediction is incorrect, increasing the probability of the correct class indirectly decreases the confidence (strategy from [49] and [72]) while our strategy directly decreases the confidence. ", "page_idx": 16}, {"type": "text", "text": "I-Max is more complex because it modifies the Histogram Binning algorithm, while our approach does not. Additionally, [35] found that I-Max produces unusable probability vectors. Indeed, they do not sum up to 1, and normalizing them degrades the method\u2019s performance. ", "page_idx": 16}, {"type": "text", "text": "We wrote our paper with practicality and generality in mind. Contrary to [49] and [72], we demonstrate the generality of our strategy by applying it on top of existing calibration baselines of different natures (scaling and binary). One of our main goals is that practitioners can easily and quickly try our TvA approach, using just a few lines of code, which can significantly improve the calibration performance of their existing calibration pipeline while having no impact on the predicted class by design (except for VS and DC). ", "page_idx": 16}, {"type": "text", "text": "Comparison with ProCal Another recent calibration method is ProCal [68]. However, its primary objective differs from ours: it \"focuses on the problem of proximity bias in model calibration, a phenomenon wherein deep models tend to be more overconfident on data of low proximity\". Its goal is to lower the difference in the confidence score values between regions of low and high density, i.e., to make the confidence score independent of a local density indicator called \"proximity.\" There is no theoretical guarantee, however, that minimizing the proximity bias improves the confidence calibration, the focus of our work. Theorem 4.2 about the PIECE metric is a direct consequence of Jensen\u2019s inequality and is true for any random variable D, not necessarily a proximity score. ", "page_idx": 16}, {"type": "text", "text": "Theorem 5.1 is an interesting bias/variance decomposition of the Brier score. However, as this type of decomposition usually states, the error may come from bias (here, a wrong initial calibration) or high estimation variance (which can be related to low density but is not expressed as such in the decomposition). We experimentally compare our approach to the ProCal algorithm using the code provided by its authors and observe in Table 2 that our approach gives much better ECE confidence calibration and, for half of the models, also better PIECE values. ", "page_idx": 17}, {"type": "text", "text": "ProCal aims to achieve three goals: mitigate proximity bias, improve confidence calibration, and provide a plug-and-play method. We share the last two goals. Concerning improving confidence calibration, our approach has better results, as shown in Table 2. Both approaches are plug-and-play, but they apply very differently. ProCal is applied after existing calibration methods to further improve calibration. It thus does not solve any of the four issues we identified (e.g., cross-entropy loss is still inefficient, and One-versus-All still leads to highly imbalanced problems). Our Top-versus-All approach is a reformulation of the calibration problem that uses a surrogate binary classifier. Existing approaches are applied to this surrogate classifier, which is how the four issues are solved. We do not propose a new method but a new way of applying existing methods. Our approach does not introduce new hyperparameters (except in the particular case of regularizing scaling methods). ProCal introduces several new hyperparameters, such as the choice of the distance, the number of KNN neighbors, or a shrinkage coefficient. ", "page_idx": 17}, {"type": "text", "text": "Comparison with Correctness-Aware Loss A concurrent work [38] builds a calibration method on top of an intuition similar to ours: binarize the calibration problem. However, what the authors do with this intuition differs vastly from our approach. They derive a Correctness-Aware Loss (Eq. 7 of their paper), which is almost the standard binary cross-entropy loss we use for scaling methods but without a logarithm. They use this loss to learn a separate model that predicts a sample-wise temperature coefficient. This is a new calibration method, which is not straightforward to implement due to the numerous hyperparameters (network architecture, image transformations...). It also requires multiple inferences at test-time, which can be problematic in some production models. Our approach is, again, not a calibration method but a general reformulation of the calibration problem that enhances existing methods. By looking at their Table 1, they get an ECE of 2.22 on ImageNet (in-distribution), while our approach achieves values around 0.5 for most models in our paper\u2019s Table 1. Their method, contrary to TvA, improves the AUROC, but in our understanding, it seems mostly due to the use of image transformations, not from their proposed loss. Their method seems to work best in out-of-distribution scenarios, which is not the main objective of our paper. However, these good results for AUROC and out-of-distribution scenarios make this method complementary to our approach, and combining the two in some way could be promising. ", "page_idx": 17}, {"type": "text", "text": "Table 2: ECE, MCE, ACE, and PIECE in $\\%$ . The experimental setting is the one used for Table 4 of [68]. The baselines are no calibration (conf), Temperature Scaling (TS), Multi Isotonic Regression (MIR), and Histogram Binning (HB). The ProCal calibration method [68] is applied after one of the baselines, as symbolized by the $\"+\"$ symbol. Our approach, Top-versus-All, changes what the baselines optimize and is symbolized by \"TvA\". We apply it for TS, Isotonic Regression (Iso), and HB. The best values for each model and metric are in bold. ", "page_idx": 18}, {"type": "text", "text": "Overall, $\\mathrm{HB}_{\\mathrm{TvA}}$ is the best calibration method as it always gets the lowest ECE and ACE. Our TvA approach lowers PIECE and even achieves the lowest value for half of the models. ", "page_idx": 18}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/b5d3ac394f9f894e9e2645137373674e4f5b3ba7c91bb04695d2f8ba9c6a1c2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Theoretical justification for Top-versus-All in the case of Temperature Scaling ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We define $L$ the number of classes, $f_{k}(x)$ the classifier estimated probability for class $k$ and data sample $x,\\,y$ the correct class, and the confidence $s(x):=\\operatorname*{max}_{k}\\,f_{k}(x)$ . The cross-entropy loss is $\\begin{array}{r}{l_{C E}(x,y)=-\\sum_{k=1}^{L}1\\{k=y\\}\\cdot\\log(f_{k}(x))=-\\log(f_{y}(x))}\\end{array}$ . Because the last layer of the classifier is a softmax function, $\\begin{array}{r}{f_{y}(x)=\\frac{e^{z_{y}}}{\\sum_{k}{e^{z_{k}}}}}\\end{array}$ with $z$ the logits vector. Note that we omit the writing variable in the following for clarity. ", "page_idx": 19}, {"type": "text", "text": "Temperature scaling optimizes a coefficient $T>0$ that scales the logits vector. Predicted probabilities become $\\begin{array}{r}{f_{y}(x)=\\frac{\\bar{e^{z\\overline{{y}}/T}}}{\\sum_{k}{e^{z_{k}/T}}}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Let us first develop the standard cross-entropy loss when temperature scaling is applied: ", "page_idx": 19}, {"type": "equation", "text": "$$\nc E=-\\log(f_{y})=-\\log(\\frac{e^{z_{y}/T}}{\\sum_{k}e^{z_{k}/T}})=-\\left(\\log(e^{z_{y}/T})-\\log(\\sum_{k}e^{z_{k}/T})\\right)=-\\frac{z_{y}}{T}+\\log(\\sum_{k}e^{z_{k}/T}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us compute its gradient: ", "page_idx": 19}, {"type": "text", "text": "f the chain rule on the second term. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{d\\sigma_{C}}{d T}=\\frac{\\gamma}{T^{2}}+\\frac{\\partial\\sigma_{C}(\\gamma_{+}\\sigma^{2})}{\\partial\\gamma_{+}\\sigma^{2}},\\frac{\\sigma^{2}\\sum_{k}\\in\\sigma^{2}\\Gamma}{T^{2}}\\,}&{\\frac{\\gamma}{\\sigma(T)}=\\frac{\\gamma}{T^{2}}}\\\\ &{=\\frac{\\gamma}{T^{2}}+\\frac{\\gamma}{\\sum_{k}+\\sigma^{2}}\\Gamma\\,\\sum_{k}\\frac{\\partial\\sigma_{C}(k,\\gamma)T}{\\partial\\gamma_{-}\\sigma^{2}}}\\\\ &{=\\frac{\\gamma}{T^{2}}+\\frac{\\gamma}{\\sum_{k}+\\sigma^{2}}\\Gamma\\,\\sum_{k}\\frac{\\partial\\sigma_{C}(k,\\gamma)T}{\\partial\\gamma_{-}\\sigma^{2}}}\\\\ &{=\\frac{\\gamma}{T^{2}}+\\frac{\\gamma}{\\sum_{k}+\\sigma^{2}}\\Gamma\\,\\sum_{k}\\frac{\\gamma}{\\Gamma(k)}\\frac{\\Gamma(\\gamma_{-}\\sigma^{2})}{\\Gamma(k)}(e^{2})^{\\gamma_{-}}}\\\\ &{=\\frac{\\gamma}{T^{2}}+\\frac{1}{\\sum_{k}+\\sigma^{2}}\\Gamma\\,\\sum_{k}\\frac{\\gamma}{\\Gamma(k)}e^{-2\\gamma_{-}k}(e^{2})^{\\gamma_{-}}}\\\\ &{=\\frac{\\gamma}{T^{2}}+\\frac{1}{\\sum_{k}+\\sigma^{2}}\\Gamma\\,\\sum_{k}\\frac{\\gamma}{\\Gamma(k)}-\\frac{\\gamma}{\\Gamma(k)}e^{-2\\gamma_{-}k}}\\\\ &{=\\frac{\\gamma}{T^{2}}\\left(\\gamma_{-}-\\frac{\\sum_{k}\\,\\gamma_{k}+\\sigma^{2}\\Gamma}{\\gamma_{-}\\sigma^{2}}\\right)}\\\\ &{=\\frac{1}{T^{2}}\\left(\\gamma_{-}-\\frac{\\sum_{k}\\,\\gamma_{k}+\\sigma^{2}\\Gamma}{\\gamma_{-}\\sigma^{2}}\\right)}\\\\ &{=\\frac{1}{T^{2}}\\left(\\gamma_{-}-\\sum_{k}\\,\\gamma_{k}+\\gamma_{-}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For our TvA approach, the problem becomes binary. The classification output becomes the confidence $s(x)=\\operatorname*{max}_{j\\in\\mathcal{Y}}f_{j}(x)$ and the ground truth label becomes a binary representation of the prediction correctness: $y^{b}=1_{\\hat{\\mathrm{y}}=\\mathrm{y}}$ with $\\hat{y}(x)=\\arg\\operatorname*{max}_{k\\in\\mathcal{Y}}f_{k}(x)$ and 1 the indicator function. The loss we use is the binary cross entropy $l_{B C E}(x,y)=-\\big(y^{b}\\cdot\\log s(x)+(1-y^{b})\\cdot\\log(1-s(x))\\big)$ ", "page_idx": 19}, {"type": "text", "text": "Let us compute the gradient: ", "page_idx": 20}, {"type": "text", "text": "\u2202lBCE $\\begin{array}{r l}{1}&{=\\frac{\\partial u_{x}}{\\partial x_{1}}\\frac{\\partial u_{y}}{\\partial x_{2}}\\frac{\\partial u_{z}}{\\partial y_{3}}}\\\\ &{=-\\left(\\nu\\frac{\\partial}{\\partial x_{1}}+(1-\\nu)\\frac{\\partial^{2}}{\\partial x_{2}}\\right)\\frac{\\partial u_{z}}{\\partial y_{3}}}\\\\ &{=-\\left(\\nu\\frac{\\partial^{2}}{\\partial x_{1}}-\\frac{\\partial}{\\partial x_{2}}+\\nu\\frac{\\partial^{2}}{\\partial x_{3}}\\right)\\frac{\\partial u_{z}}{\\partial x_{1}}}\\\\ &{\\qquad-\\frac{u_{x}}{\\partial x_{2}}\\frac{\\partial u_{z}}{\\partial x_{3}}}\\\\ &{=-\\frac{u_{x}-x_{1}}{\\partial x_{1}}\\frac{\\partial u_{x}}{\\partial x_{2}}\\frac{\\partial u_{x}}{\\partial x_{3}\\partial x_{4}}\\alpha u_{x}-u_{y}\\frac{u_{x}+x_{2}}{\\partial x_{3}}\\frac{\\partial u_{y}}{\\partial x_{4}\\partial y_{3}}\\frac{\\partial u_{x}-u_{y}-u_{z}}{\\partial x_{5}\\partial y_{4}}}\\\\ &{\\quad=\\frac{u_{x}-x_{2}}{\\partial x_{1}}\\frac{\\partial u_{y}}{\\partial x_{2}}\\frac{\\partial u_{x}-u_{y}-u_{z}}{\\partial x_{3}\\partial y_{3}}\\alpha u_{x}-u_{y}\\frac{u_{x}-u_{z}}{\\partial x_{4}}\\tau\\frac{\\partial u_{x}}{\\partial y_{3}}}\\\\ &{\\quad=\\frac{u_{x}-x_{1}}{\\partial x_{1}}\\frac{\\partial u_{y}}{\\partial x_{2}}\\frac{\\partial u_{x}-u_{y}-u_{z}}{\\partial x_{3}\\partial y_{4}}\\alpha u_{x}}\\\\ &{\\quad=\\frac{u_{x}-x_{2}}{\\partial x_{1}}\\frac{\\partial u_{y}-u_{y}}{\\partial x_{3}\\partial y_{1}}\\frac{\\partial u_{z}-u_{z}-u_{z}-u_{z}}{\\partial x_{1}\\partial y_{3}}\\frac{\\partial u_{z}-u_{z}}{\\partial x_{2}\\partial y_{4}}\\alpha u_{y}}\\\\ &{\\quad=\\frac{u_{x}-x_{1}}{\\partial x_{1}}\\frac{$ \u2202T $\\begin{array}{r l}{{\\mathbf x}}&{=\\frac{-x}{\\sqrt{3}},\\quad\\sum_{k=1}^{{\\infty}}\\mathbf{b}(\\mathbf{\\Phi}_{k})=\\cos x\\cos x\\cos x\\cos x\\cos x\\cos(\\mathbf{b}+\\mathbf{b})-\\frac{x}{\\sqrt{3}}\\cos x\\sin^{2}(\\mathbf{b})}\\\\ &{=\\frac{-x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}},\\quad\\cos^{2}x\\cos x\\sin^{2}(\\mathbf{b}-\\mathbf{b})-\\frac{x}{\\sqrt{3}}\\cos x\\sin^{2}(\\mathbf{b})}\\\\ &{=\\frac{-x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}},\\quad\\alpha\\cos^{2}x\\sin^{2}(\\mathbf{b})}\\\\ &{=-\\frac{x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}}\\cos x\\sin^{2}(\\mathbf{b})-\\frac{x}{\\sqrt{3}}\\cos x\\cos x^{2}\\sin x\\sin^{2}(\\mathbf{b})}\\\\ &{=-\\frac{x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}},\\quad\\alpha\\cos^{2}x\\sin^{2}(\\mathbf{b})}\\\\ &{=\\frac{-x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}}-\\frac{x}{\\sqrt{3}}\\cos x\\sin^{2}(\\mathbf{b})\\frac{-x}{\\sqrt{3}}+\\frac{x}{\\sqrt{3}}\\sin x\\sin^{2}(\\mathbf{b})\\sin^{2}(\\mathbf{b})}\\\\ &{\\quad-\\frac{x}{\\sqrt{3}}\\cos x\\sin^{2}(\\mathbf{b})}\\\\ &{=\\frac{-x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}}-\\frac{x}{\\sqrt{3}}\\cos x\\sin^{2}(\\mathbf{b})-\\frac{x}{\\sqrt{3}}\\sin x\\sin^{2}(\\mathbf{b})}\\\\ &{=\\frac{-x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}}-\\frac{x}{\\sqrt{3}}\\cos x\\sin^{2}(\\mathbf{b})}\\\\ &{=\\frac{-x}{\\sqrt{3}},\\quad\\frac{\\sin^{2}(\\mathbf{b})}{\\sqrt{3}}-\\frac{x}{\\sqrt{3}}\\sin x\\sin^{2$ T2 s (8) ", "page_idx": 20}, {"type": "text", "text": "- First case, the prediction is correct: $y^{b}=1$ and $z_{m}\\,=\\,z_{y}$ . Let us inject these in (8): $\\begin{array}{r}{\\frac{\\partial l_{B C E}}{\\partial T}=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{T^{2}}\\cdot\\left(z_{y}-\\sum_{k}z_{k}\\cdot f_{k}\\right)=\\frac{\\partial l_{C E}}{\\partial T}}\\end{array}$ \u2202l\u2202CTE . We thus get the same gradient as the standard cross-entropy loss. ", "page_idx": 20}, {"type": "text", "text": "- Second case, the prediction is incorrect: $\\boldsymbol{y}^{b}~=~0$ and $z_{m}\\ >\\ z_{y}$ . (8) becomes: $\\begin{array}{r}{\\frac{\\partial l_{B C E}}{\\partial T}}\\\\ {\\frac{\\partial T}{\\partial T}}\\end{array}=$ $\\begin{array}{r}{\\frac{1}{T^{2}}\\cdot\\frac{s}{s-1}\\cdot(z_{m}-\\sum_{k}z_{k}\\cdot f_{k})}\\end{array}$ . By comparing to (7), we have the term $\\begin{array}{r}{\\frac{1}{T^{2}}\\cdot\\left(z_{m}-\\sum_{k}z_{k}\\cdot\\bar{f}_{k}\\right)>}\\end{array}$ $\\begin{array}{r}{\\frac{1}{T^{2}}\\left(z_{y}-\\sum_{\\boldsymbol{k}}z_{\\boldsymbol{k}}\\cdot f_{\\boldsymbol{k}}\\right)=\\frac{\\partial l_{C E}}{\\partial T}}\\end{array}$ and the remaining part of (8) $|\\frac{s}{s-1}|>1$ when $s>0.5$ . ", "page_idx": 20}, {"type": "text", "text": "So to recapitulate, $\\begin{array}{r}{|\\frac{\\partial l_{B C E}}{\\partial T}|>|\\frac{\\partial l_{C E}}{\\partial T}|}\\end{array}$ when $s>0.5$ , which corresponds to the vast majority of data points as the classifier gets better calibrated. This is shown in Figure 1. ", "page_idx": 20}, {"type": "text", "text": "We also have $\\begin{array}{r}{\\operatorname*{lim}_{s\\to1}|{\\frac{s}{s-1}}|\\,=\\,\\infty}\\end{array}$ . In practice, $s$ is not close enough to 1 to generate exploding gradients, so it just means that as confidences for wrong predictions gets higher, so does the gradient to reduce the confidences. ", "page_idx": 20}, {"type": "text", "text": "The conclusion is that for correct predictions, our approach does not change the optimization, but for incorrect predictions, the gradient is stronger and penalizes more heavily confident predictions that are wrong. This is also proven experimentally by looking at Table 8 where we see that average confidences of temperature scaling with the TvA approach $\\mathrm{(TS_{TvA})}$ ) are lower than the ones using the standard approach (TS), for almost all networks. This makes the average confidences closer to the accuracy, showing reduced overconfidence. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "ebBnKVxMcZ/tmp/6826f9e3afbdea6ca1886abee8b0fe6f85eaa5d84b88cb72231b578070905777.jpg", "img_caption": ["E Limits of classwise-ECE and top-label-ECE for a high number of classes ", "Figure 4: Histogram of class probabilities for 3 random classes, for ViT-16/B on ImageNet. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Let us define the ECE for class $j$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{ECE}_{j}=\\sum_{b=1}^{B}\\frac{n_{b}}{N}|\\mathrm{acc}(b,j)-\\mathrm{conf}(b,j)|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The difference compared to (2) is that now $\\operatorname{acc}(b,j)$ corresponds to the proportion of class $j$ in the bin. Also, conf $\\left(b,j\\right)$ now is the average probability given to class $j$ for all samples in the bin. Then, classwise-ECE [29] takes the average for all classes: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{ECE}_{\\mathrm{cw}}=\\sum_{j=1}^{L}\\mathrm{ECE}_{j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Classwise-ECE considers the full probabilities vectors: all the class probabilities for each prediction.   \nHowever, this metric does not scale to large numbers of classes. Let us see why with an example. ", "page_idx": 21}, {"type": "text", "text": "Let us use a test set of $N$ samples, $N/L$ for each of the $L$ classes (the dataset is balanced), and a high-accuracy classifier fairly calibrated. The classifier predicts $N$ probability vectors of length $L$ . Predicted probabilities for class $j$ are all the values of the vector at dimension $j$ . Because the classifier has a high accuracy and is fairly calibrated, around $N/L$ values are close to 1 (corresponding to mostly correct predictions), and the remaining ones, around $N-N/L$ , are close to 0 (because the predicted class is not class $j$ , and the predicted probability is high for another class). ", "page_idx": 21}, {"type": "text", "text": "To compute $\\mathrm{ECE}_{j}$ with equal size 15 bins, the predicted probabilities for class $j$ are partitioned into 15 bins. The first bin (with probabilities close to 0), contains $n_{1}\\approx N-N/L$ samples while the last one (with probabilities close to 1) contains $n_{B}\\approx N/L$ samples. The remaining bins are usually even more empty. That means that the calibration error in the first bin is weighted $n_{1}/n_{B}=L-1$ times more than the last one. For the 1000 classes of ImageNet, $L-1=999$ . Figure 4 shows the number of samples $(n_{b})$ in each bin for an ImageNet classifier. ", "page_idx": 21}, {"type": "text", "text": "Because the impact of the calibration error in the bin with the high probabilities is negligible relative to the bin with the low probabilities, the classwise-ECE mostly measures whether probabilities close to 0 are calibrated. We argue this is not what we are interested in: what matters more is the calibration of higher values of the probabilities. ", "page_idx": 21}, {"type": "text", "text": "Top-label ECE [18] is another interesting metric that does not scale to large numbers of classes either. Top-label-ECE divides data into subsets according to the predicted class, computes the ECEs of these subsets, and averages them. For an ImageNet test set of 25000 samples (25 per class), data is divided into 1000 subsets of $\\approx25$ samples each (the classifier is high-accuracy, most of the time the predicted class is equal to the true class). The ECE is computed for each subset containing only 25 samples. To compute the ECE, samples are typically partitioned into 15 bins. The number of samples per bin does not allow a correct estimation of the average confidence or accuracy. ", "page_idx": 21}, {"type": "text", "text": "F Implementation details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Models weights ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 Model weights for CIFAR are from [45].   \n\u2022 Model weights for ImageNet come from torchvision [42].   \n\u2022 Model weights for ImageNet-21K are from [54].   \n\u2022 CLIP weights are from OpenAI\u2019s Hugging Face.   \n\u2022 Original weights for T5 and RoBERTa come from the Transformers library [67]. The models are fine-tuned for each task using prompt-based learning [36]. For more details, see [5]. \u2022 We used the model GPT-J https://huggingface.co/EleutherAI/gpt-j-6b (Apache", "page_idx": 22}, {"type": "text", "text": "2.0 license) and Llama-2 https://huggingface.co/meta-llama/Llama-2-13b (license). ", "page_idx": 22}, {"type": "text", "text": "F.2 Datasets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 CIFAR-10 (C10) and CIFAR-100 (C100) [27] contain 60000 32x32 images corresponding to 10 and 100 classes, respectively. Data is split into subsets of 45000/5000/10000 images for train/validation/test. We concatenate the original validation and test sets, and randomly split that into a calibration set of size 5000, and a test set of size 10000.   \n\u2022 ImageNet (IN) [7] contains 1.3 million images from 1000 classes. Following [17], we randomly split the original validation test of size 50000 into a calibration set and a test set, both of size 25000.   \n\u2022 ImageNet-21K (IN21K) [54], in its winter21 version, contains 11 million images in the train set, and 522500 in the test set (50 for each of the 10450 classes). We randomly split the test set into equal-sized calibration and test set (261250 samples each, 25 per class).   \n\u2022 Amazon Fine Foods [43] is a collection of customer reviews for fine foods sold on Amazon. Reviews are categorized into bad, neutral, and good. The original validation set size is 78741 and test size 91606. We randomly split them into 78741 samples for calibration and 91606 for test.   \n\u2022 DynaSent [51] is a dynamic benchmark for sentiment analysis consisting of sentences annotated as positive, neutral, and negative. The original validation set size is 11160 and test size 4320. We randomly split them into 11160 samples for calibration and 4320 for test.   \n\u2022 MNLI [66] contains pairs of sentences labeled as contradiction, neutral, and entailment. The original validation set size is 19635 and test size 9815. We randomly split them into 19635 samples for calibration and 9815 for test.   \n\u2022 Yahoo Answers (YA) [73] contains question-answers pairs corresponding to 10 different topics. The original validation set size is 14000 and test size 60000. We randomly split them into 14000 samples for calibration and 60000 for test.   \n\u2022 TREC [61] contains questions categorized into 6 classes. The training set contains 5500 labeled questions, and the test set contains another 500.   \n\u2022 SST-5 [55] contains 11855 sentences corresponding to 5 sentiments (from very negative to very positive).   \n\u2022 DBpedia [74] contains text for topic classification with 14 classes. The training set contains 560000 samples, and the test set 5000. ", "page_idx": 22}, {"type": "text", "text": "F.3 Code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 We used the library netcal [32] (Apache-2.0 license) for their implementation of binary methods for calibration and adapted their reliability diagrams code. For HB, we tested equalsize and equal-mass bins, and chose the best variant for each case. All hyperparameters were kept at their default values (10 bins for HB).   \n\u2022 We took inspiration from the official implementation of temperature scaling: https:// github.com/gpleiss/temperature_scaling (MIT license).   \n\u2022 We took inspiration from the official implementation of Dirichlet calibration: https: //github.com/dirichletcal/experiments_dnn (MIT license).   \n\u2022 We used the official implementation of I-Max: https://github.com/boschresearch/ imax-calibration (AGPL-3.0 license).   \n\u2022 We used the official implementation of IRM: https://github.com/zhang64-llnl/ Mix-n-Match-Calibration (MIT license).   \n\u2022 For evaluation, we used codes from https://github.com/JeremyNixon/ uncertainty-metrics-1 (Apache-2.0 license) and https://github.com/IdoGalil/ benchmarking-uncertainty-estimation-performance (MIT license).   \n\u2022 We used PyTorch 2.0.0 [3] (BSD-style license).   \n\u2022 We used CIFAR models from [45] https://github.com/torrvision/focal_ calibration (MIT license).   \n\u2022 We used ImageNet-21K models [54] https://github.com/Alibaba-MIIL/ ImageNet21K (MIT license).   \n\u2022 We used CLIP models from HuggingFace\u2019s Transformers library [67] (Apache-2.0 license).   \n\u2022 We used pretrained language models from Transformers [67] and calibration codes from [5] https://github.com/lifan-yuan/PLMCalibration (MIT license).   \n\u2022 We used code from https://github.com/mominabbass/LinC for the calibration of LLMs using ICL, itself built upon https://github.com/tonyzhaozh/ few-shot-learning (Apache-2.0 license). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Table 3: Computing time (in seconds) of the calibration on ImageNet, using one NVIDIA V100 GPU. The first column denotes the data preprocessing time, which includes computing the model logits for all calibration examples. Post-hoc calibration methods do not usually require much computing power compared to classifier training. ", "page_idx": 23}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/ec44ea8e75b3baeaaf18423b536d68ab3e08f13c82d0feb1860f1ad94b15f58c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "G Impact on selective classification ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Selective classification aims to improve a model\u2019s prediction performance by trading-off coverage: a reject option allows to discard data that might result in wrong predictions, thus improving the accuracy on the remaining data. A strong standard baseline uses thresholding on the maximum softmax probability outputted by the classifier [13]. Improving confidence calibration means uncertainty is better quantified and should result in better selective classification. ", "page_idx": 23}, {"type": "text", "text": "Results in Table 1 show the superiority of Histogram Binning (applied with the right framework) in reducing the calibration error ECE. Unfortunately, it does not translate into improvements in selective classification. AUROC is a standard metric for selective classification [11]. Table 4 shows that Histogram Binning actually degrades the AUROC, while the best method is Isotonic Regression. Our TvA framework does not significantly impact the AUROC. ", "page_idx": 23}, {"type": "text", "text": "This paper addresses confidence calibration, usually measured by ECE. AUROC is a global rankbased metric for selective classification: it relies on the relative values of the scores, not their absolute values. Even though calibration and selective classification are related, improvement in calibration does not directly translate to better selective classification. This has been clearly demonstrated experimentally by [11]. ", "page_idx": 23}, {"type": "text", "text": "A good example of that difference is the behavior of $\\mathrm{HB}_{\\mathrm{TvA}}$ : it is the best calibration method overall but actually degrades the AUROC in most cases. Such a difference can be explained by the fact that selective classification benefits from a continuous score able to discriminate between certain and uncertain examples finely, but HB quantizes the confidences into, e.g., 10 different values. ", "page_idx": 23}, {"type": "text", "text": "Table 4: AUROC in $\\%$ (higher is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Improvements from the uncalibrated model are colored in blue and degradations in orange. ", "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/e5cb0636e9704fa1d6b9151e64f68e50ce79e285519b673b549761a630ae1388.jpg", "table_caption": ["(a) CIFAR-10 "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/acadfb1e9bf728b5fa7263757e5a1e1ee3cbfa52043555da23c4232355b98592.jpg", "table_caption": ["(b) CIFAR-100 "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/e5631feab461f02a5c5ebb851599cd51e5ac8b6adb665a14496b787105202316.jpg", "table_caption": ["(c) ImageNet "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/a9ce23135e662896fc05938d83b67ea668df2bb797c22f98d0369d76213463cd.jpg", "table_caption": ["(d) ImageNet-21K "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/f64ea679eb997922336747d427994222cba30de3ab18611deb1748aba8aeb8c5.jpg", "table_caption": ["(e) Amazon Fine Foods "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/108411cb99321b56fb117580135d58efb0ab106fc98df6bf19da641313e09be9.jpg", "table_caption": ["(f) DynaSent "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/86afdb5e9d5c1a6f963f36366bf8ce6a66c399a4931b761ce53257b237f5a961.jpg", "table_caption": ["(g) MNLI "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/41faf7c98dfb3c79e1a3f7a2872342db13160d0fd685abb54d835b5f47851222.jpg", "table_caption": ["(h) Yahoo Anwsers "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "H Additional results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 5: ECE in $\\%$ (lower is better, best in bold) \u2013 full results for image classification datasets. Averages on 5 seeds. Mean relative improvements from TvA are shown (negative values for reductions of ECE). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Values are averaged over five random seeds. ", "page_idx": 25}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/e067a6d8af1b08aa7e206bab8a58a61a63511289e3bb84624c3e14d26b872bc3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 6: ECE in $\\%$ (lower is better, best in bold) \u2013 full results for text classification datasets. Averages on 5 seeds. Mean relative improvements from TvA are shown (negative values for reductions of ECE). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Values are averaged over five random seeds. ", "page_idx": 26}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/ee352ce4e19cbfad8fc67e825d8af62509eb629e264f04517f1c2554fd985930.jpg", "table_caption": ["(a) Amazon Fine Foods "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 7: Standard deviations of ECE in $\\%$ for 5 seeds. ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/6fccec6a28f1e7d3aeb5756027075ff41db6016a4f5d4d1c73fbac82f93aef07.jpg", "table_caption": ["(a) CIFAR-10 "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/27ac5918c14d912fc49c4a33d303c7829e3c5c25709c098446d316ce7ece6bb7.jpg", "table_caption": ["(b) CIFAR-100 "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/7521bb76592a6c87bc716603039dbb28878704075782e92f6148ac504316be23.jpg", "table_caption": ["(c) ImageNet "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/3c1557801c57d8d676ca38e71c42d41659ccea5f678c1bf7e4896f4c0358f126.jpg", "table_caption": ["(d) ImageNet-21K "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/c6dab3c95d1b3a06389932131326e7988721f86f1a5862e2c8bfee4dc0703a5a.jpg", "table_caption": ["(e) Amazon Fine Foods "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/25c721c6b41bc433056566d1d8e0b40974a7fe25246c0d91c7ba0f395236ac60.jpg", "table_caption": ["(f) DynaSent "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/caca77029af02928477688ab9a38cc386b75d418028ab47863a36f852e88d05b.jpg", "table_caption": ["(g) MNLI "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/e4a423d47116ccad5b51d1ff551abdff8e437520b2b5237b1129caa3230d8bec.jpg", "table_caption": ["(h) Yahoo Answers "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 8: Average confidence in $\\%$ . Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Overconfidence (average confidence $>$ accuracy) is shown in violet and underconfidence (average confidence $<$ accuracy) in brown. ", "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/177f52e9d9972f48486fe7929b98dde987aa187ac22800f0817a93bf6ee6f969.jpg", "table_caption": ["(a) CIFAR-10 "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/b913a5114267f857a538469152fd72180af9375ac943c7cb842ed4e9d7d38fc8.jpg", "table_caption": ["(b) CIFAR-100 "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/fc11fd0b92a86aa251f930897bce360b32c09e2e3da0e348adb1cc63caddf5f9.jpg", "table_caption": ["(c) ImageNet "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/5c8265df31822fff7e8692f4ff893f86b3cbe10afe9c5ae9b5b2b318f1b1fa25.jpg", "table_caption": ["(d) ImageNet-21K "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/c4234145bd01990bce14b4a37c8dd97d912133c62ae00424c40138e3b2c8469a.jpg", "table_caption": ["(e) Amazon Fine Foods "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "(f) DynaSent ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/921ef2116fbcd627a278ccf1b9b09924ba85010b2e67fe000823cd169c26cb6e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/70f34080c8250458f493f12421266308f8aebe5312700c0cf988df9044cb41a7.jpg", "table_caption": ["(g) MNLI "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/d9e6c7fc86a9e83b14b56baa57da10be59700981d80166a8307599bc9ad62401.jpg", "table_caption": ["(h) Yahoo Answers "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 9: Accuracy in $\\%$ (higher is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. Because classifiers can be well calibrated when not accurate (by having low accuracy and low confidence), it is important to monitor the accuracy. It is even better when the methods preserve the accuracy by design. ", "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/6312619c477083fecf1cf8aa961b7573dba8e0e7901db5b418d988bc0f295ee0.jpg", "table_caption": ["(a) CIFAR-10 "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/07631c17afd326c5eb1e48e6897be1130794269d203c195dd0bdc4291c770591.jpg", "table_caption": ["(b) CIFAR-100 "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/d6f6c4531379964319e22952b07a8e96567f559641ad5fadf4213bda3d9724d0.jpg", "table_caption": ["(c) ImageNet "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/72ff33ed25dcafc047ede09db5b673d5d0fd5ed5b19424c337770946c70deaa8.jpg", "table_caption": ["(d) ImageNet-21K "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/9fc2b61f332624ede797f4ba9e50cc60b0843251fef345bc921e2e244fad6282.jpg", "table_caption": ["(e) Amazon Fine Foods "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "(f) DynaSent ", "text_level": 1, "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/0dd1599f269d95b9dc7772e2ce926f7eb0e003f01f5d73617c41a76a3c191d56.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/4d7470ae17b12a0a90edfe9c3a78678b9a8e22e16232228c5e950ddeb1c94be3.jpg", "table_caption": ["(g) MNLI "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/6921d5c07f6581bac23b80eba4b7ef205a2527fd3f32c27c797041556cb56a2e.jpg", "table_caption": ["(h) Yahoo Answers "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 10: ECE with 15 equal mass bins in $\\%$ (lower is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. ", "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/f9bfc632863dbf6f056f8b5e5cd6e177000a14672c9ace86df63310f855a5762.jpg", "table_caption": ["(a) CIFAR-10 "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "(b) CIFAR-100 ", "text_level": 1, "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/368a002287e8c8bde06a4ae2f7e5507c2f0901f7942048265831d1165f744a49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "(c) ImageNet ", "text_level": 1, "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/c880c5f6ef5ea983b236f6c0af3b540d5cdca79b03b64b8b00af602e7fe7033b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/bb39f77a7fd9b85059a7c19a4b166ea546524bf6e8cc5e460294bf294d2f3460.jpg", "table_caption": ["(d) ImageNet-21K "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/6eba6530ac970051077f7f370bfa925e51d03ad376d01edca8452ba49935391a.jpg", "table_caption": ["(e) Amazon Fine Foods "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "(f) DynaSent ", "text_level": 1, "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/355c011b7afa239809221a74e905541bd2ef4600fb0251ef154ab6e54fc1985e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/57335b55ea9dcb772e61cad116266b59f18dae58163dbe70441f20360394bf4a.jpg", "table_caption": ["(g) MNLI "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/e0ad34539bb8382d650ea27a9fd059e9554d51b99d262fc0ddcbd2b78e84335d.jpg", "table_caption": ["(h) Yahoo Answers "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 11: Brier score of the predicted class in $10^{-2}$ (lower is better). Methods in purple impact the model prediction, potentially degrading accuracy; methods in teal do not. ", "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/d7a1331a576d2cb2863d1b1adc0d91d83bb6c0118350f9c8e9d8ee22e238289c.jpg", "table_caption": ["(a) CIFAR-10 "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "(b) CIFAR-100 ", "text_level": 1, "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/cd99d68a4738bc30e937e8f867513f95c6ceafb820697f23764f1572555ef9f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/18f459879853dc34251e85ae14983848a492e8a92db121ca3776e46a9cd9d4ce.jpg", "table_caption": ["(c) ImageNet "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/1af94aece54f5e996f98eccf18cb47af1c7b7e1dc21aee945e3e4f0bdf36d804.jpg", "table_caption": ["(d) ImageNet-21K "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/e5d7ca1444aea8b448eb1a5f09c9fecbee2c617565a8fce1a5892e3fe05cc453.jpg", "table_caption": ["(e) Amazon Fine Foods "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "(f) DynaSent ", "text_level": 1, "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/f38474713085e25dbcf81a4dbec955425b2e6fb0a892c504c2bcf2f021765007.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/6f17a3134864f57e1c2fada50ca657db18ecb1b90da4017265bfdb2cd7dd0767.jpg", "table_caption": ["(g) MNLI "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/a6e5f43a3d203d9eb8f02b7adde5c436263472b5a35b38ced0de4fe9974dcd26.jpg", "table_caption": ["(h) Yahoo Answers "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ebBnKVxMcZ/tmp/d0d6aad5c631c3190656480701a69f32aeb442c441944259d7d686f1d48712d8.jpg", "table_caption": ["Table 12: Calibration methods applied to in-context learning of LLMs. Accuracy and ECE are in $\\%$ "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Large Language Models (LLMs) exhibit an in-context learning (ICL) capability, meaning they can learn from just a few examples in the context. It works by constructing a prompt that includes input-output pairs demonstrating the considered task, followed by a query for a new input. See [8] for a survey. Recent works develop calibration methods whose main goal is to improve the performance of ICL for LLMs, without requiring a complicated model fine-tuning. [77] uses a customized variant of Platt scaling (more specifically, Vector Scaling). Their method infers good values of the vector scaling parameters in a data-free procedure. The idea is that for a \"content-free\" input, e.g., \"N/A\", the calibrated probability has a $50\\%$ chance (for a binary classification task) of removing a bias toward the positive or negative class. In our paper, we denote this method as ConC. [1] builds on top of this work but uses a calibration set to learn the scaling parameters by minimizing the cross-entropy loss. This can be considered as Matrix Scaling. We denote this method as LinC. [78] proposes a per-class normalization of the probabilities on a given batch. [25] estimates the in-context model label marginal p(y) from limited data and uses it to calibrate the model probabilities. Paper [20] uses a Gaussian mixture model. ", "page_idx": 32}, {"type": "text", "text": "In our experiments, we have tested a two-step calibration. First, we use the state-of-the-art method LinC to maximize the accuracy by learning scaling parameters on a calibration set. Then, we apply $\\mathrm{HB_{TvA}}$ to scale the confidences to lower the calibration error ECE, while preserving the accuracy gains. We use the same calibration set for the two methods. LinC performance depends on hyperparameter values, but to keep the experiments simple, we fixed the following values: 100 epochs, a learning rate of 0.001, and 300 calibration samples. It means that the reported performance of LinC is suboptimal and could be enhanced even more. We used the same experimental setting as [1]. We used the models GPT-J with 6B parameters [62] and Llama-2 with 13B parameters [59]. The text classification datasets are TREC [61] for question classification with 6 classes, SST-5 [55] for sentiment analysis with 5 classes, and DBpedia [74] for topic classification with 14 classes. The 0-shot, 1-shot, 4-shot, and 8-shot learning settings were tested. Five different sets of 300 test samples were randomly selected, and results are averaged over 5 seeds. We evaluated the accuracy and ECE for each configuration. Please see Table 12 for the results. In most cases, ${\\mathrm{LinC}}{+}{\\mathrm{HB}}_{\\mathrm{TvA}}$ achieves the best accuracy and ECE. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our main claims in the abstract and introduction are that our approach solves the failure of many calibration methods when applied to problems with many classes and that we conduct extensive experiments. This reflects our analyses in Sections 3 and 4 and experimental results in Section 5 and Appendices G and H. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Section 6 is a separate \"Limitations\" section in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No strong theoretical result, but mathematical calculations are in Appendix D. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We use publicly available models and data and experiments details are provided in Section 5 and Appendix F. Our provided code can be used to reproduce our results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We use publicly available models and data and provide a clean code implementation aiming for easy reuse and reproducibility. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Details are provided in Section 5, Appendix F; and in the provided code. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All our tables show values averaged over five random seeds that generate different calibration and test datasets, as mentioned in Section 5. Standard deviations are reported in Table 7. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Table 3 in the Appendix provides computing times and the GPU model used. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The research conducted in the paper does conform with the NeurIPS Code of Ethics and we preserve anonymity. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Appendix B discusses the broader impacts of the work. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We believe that improved confidence calibration does not have a high risk of misuse. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The papers producing the used datasets are cited. The code implementations used are cited and the URLs are provided in Appendix F. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our provided code is well documented. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]