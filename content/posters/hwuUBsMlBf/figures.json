[{"figure_path": "hwuUBsMlBf/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Each MLP expert within the MoE block during the visual instruction tuning stage is initialized from the corresponding pre-trained MLP in CuMo. Right: CuMo outperforms strong open-sourced models such as Mini-Gemini and LLaVA-NeXT, as well as the private MM1 model.", "description": "The figure demonstrates the architecture of CuMo's co-upcycled Mixture-of-Experts (MoE) blocks, highlighting how each expert is initialized from pre-trained MLP blocks.  The right panel shows a radar chart comparing CuMo's performance against other state-of-the-art multimodal LLMs across various benchmarks, showcasing CuMo's superior performance.", "section": "1 Introduction"}, {"figure_path": "hwuUBsMlBf/figures/figures_3_1.jpg", "caption": "Figure 2: Architecture of CuMo. CuMo incorporates sparse Top-K MoE blocks into the CLIP vision encoder and vision-language MLP connector, thereby improving the multimodal LLM capabilities from the vision side. Skip connections are omitted for simplicity. Further implementation details are provided in Section 3.2.", "description": "The figure illustrates the architecture of CuMo, a multimodal large language model.  It shows how sparse Top-K Mixture-of-Experts (MoE) blocks are integrated into both the CLIP vision encoder and the MLP connector. This improves the model's ability to process visual information and enhances its multimodal capabilities. The diagram simplifies the architecture by omitting skip connections for clarity.  More detailed implementation information can be found in Section 3.2 of the paper.", "section": "3.2 CuMo Architecture"}, {"figure_path": "hwuUBsMlBf/figures/figures_4_1.jpg", "caption": "Figure 3: Training Stages of CuMo. The first stage involves pre-training the MLP for better alignment. Subsequently, the pre-finetuning stage trains all parameters as a warm-up before the next stage. Finally, the MLP experts within each MoE block are initialized from the weights of the corresponding MLP block, followed by training all parameters in the visual instruction tuning stage.", "description": "This figure illustrates the three-stage training process of CuMo.  The first stage is pre-training the MLP, followed by pre-fine tuning of all parameters.  The final stage is visual instruction tuning, where the MLP experts in the MoE blocks are initialized from the pre-trained MLP weights before training.", "section": "3 Method"}, {"figure_path": "hwuUBsMlBf/figures/figures_7_1.jpg", "caption": "Figure 4: Expert distributions of MoE blocks in CLIP. We select layers from CLIP and summarize the activated experts during inference.", "description": "This figure visualizes the distribution of activated experts within the Mixture-of-Experts (MoE) blocks of the CLIP vision encoder during inference.  It shows the percentage of times each expert is activated across different layers of the CLIP model for a specific benchmark (MME).  The even distribution across experts demonstrates the effectiveness of auxiliary loss functions in balancing expert utilization during training.", "section": "4.4 More Analysis"}, {"figure_path": "hwuUBsMlBf/figures/figures_8_1.jpg", "caption": "Figure 1: Left: Each MLP expert within the MoE block during the visual instruction tuning stage is initialized from the corresponding pre-trained MLP in CuMo. Right: CuMo outperforms strong open-sourced models such as Mini-Gemini and LLaVA-NeXT, as well as the private MM1 model.", "description": "The figure displays two key aspects of CuMo.  The left panel illustrates the co-upcycling initialization of Mixture-of-Experts (MoE) blocks, showing how each expert within the MoE is initialized using a pre-trained Multi-Layer Perceptron (MLP) from CuMo. The right panel presents a performance comparison of CuMo against other state-of-the-art multimodal LLMs (Mini-Gemini, LLaVA-NeXT, and a private model MM1) across several benchmarks, demonstrating the superior performance of CuMo.", "section": "1 Introduction"}, {"figure_path": "hwuUBsMlBf/figures/figures_15_1.jpg", "caption": "Figure 1: Left: Each MLP expert within the MoE block during the visual instruction tuning stage is initialized from the corresponding pre-trained MLP in CuMo. Right: CuMo outperforms strong open-sourced models such as Mini-Gemini and LLaVA-NeXT, as well as the private MM1 model.", "description": "The left panel of the figure shows the architecture of a co-upcycled Mixture-of-Experts (MoE) block used in CuMo. Each expert in the MoE block is initialized from a corresponding pre-trained Multi-Layer Perceptron (MLP) block. The right panel shows a comparison of CuMo's performance against other state-of-the-art multimodal LLMs on various benchmarks. CuMo outperforms other models across different benchmarks.", "section": "1 Introduction"}]