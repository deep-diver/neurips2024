{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-31", "reason": "This paper introduces Flamingo, a foundational visual language model that significantly influenced the development of multimodal LLMs and is frequently cited in related research."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "publication_date": "2023-08-31", "reason": "This paper presents Qwen-VL, a highly competitive open-source multimodal LLM that serves as a strong benchmark against which other models are compared."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-31", "reason": "This paper introduces Visual Instruction Tuning (VIT), a crucial training technique for multimodal LLMs that significantly improves their performance on various visual tasks and is widely adopted."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-12-31", "reason": "This seminal paper introduces the Mixture-of-Experts (MoE) layer, a critical architectural component used in CuMo to improve scalability and efficiency, and it has had a substantial influence on the scaling of LLMs in general."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-12-31", "reason": "This paper introduces CLIP, a widely used pre-trained vision encoder that is leveraged as a component in numerous multimodal LLMs, including CuMo, showcasing its importance in the field."}]}