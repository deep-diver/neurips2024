{"references": [{"fullname_first_author": "Peter Auer", "paper_title": "Near-optimal regret bounds for reinforcement learning", "publication_date": "2008-01-01", "reason": "This paper provides foundational theoretical results on reinforcement learning, which are relevant to the problem of exploration addressed in the target paper."}, {"fullname_first_author": "Adri\u00e0 Puigdom\u00e8nech Badia", "paper_title": "Never give up: Learning directed exploration strategies", "publication_date": "2020-01-01", "reason": "This paper introduces a novel exploration strategy in reinforcement learning that the target paper compares against, highlighting its importance in the field."}, {"fullname_first_author": "Marc Bellemare", "paper_title": "Unifying count-based exploration and intrinsic motivation", "publication_date": "2016-01-01", "reason": "This paper presents a unified approach to exploration in reinforcement learning, which provides context and a comparison point for the method developed in the target paper."}, {"fullname_first_author": "L\u00e9onard Blier", "paper_title": "Learning successor states and goal-dependent values: A mathematical viewpoint", "publication_date": "2021-01-01", "reason": "This paper offers a mathematical perspective on successor states, a key concept used in the target paper's proposed algorithm."}, {"fullname_first_author": "Diana Borsa", "paper_title": "Universal successor features approximators", "publication_date": "2018-01-01", "reason": "This paper introduces successor features, which are used by the target paper, making it an important foundational reference."}]}