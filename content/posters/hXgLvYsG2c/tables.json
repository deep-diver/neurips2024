[{"figure_path": "hXgLvYsG2c/tables/tables_7_1.jpg", "caption": "Table 2: The change of ranks of layers in the fifth encoder block.", "description": "This table shows how the ranks of layers in the fifth encoder block change during the training process using CoMERA.  It compares the ranks before training, after the early stage of training, and after the late stage of training. The late-stage training uses different target compression ratios (0.8, 0.5, and 0.2), resulting in varying degrees of rank reduction.  Notice that some layers end up with all ranks at zero, indicating that these layers were effectively removed by the model during the rank-adaptive training process.", "section": "5 Training Results"}, {"figure_path": "hXgLvYsG2c/tables/tables_8_1.jpg", "caption": "Table 3: Training results on the DLRM model with a batch size 10,000.", "description": "This table presents a comparison of the training results for the Deep Learning Recommendation Model (DLRM) using both uncompressed and CoMERA methods.  It shows the accuracy, normalized cross-entropy (CE) loss, model size, and peak memory usage for each method.  The results demonstrate that CoMERA achieves similar accuracy with a significantly smaller model size and lower peak memory consumption compared to the uncompressed method.", "section": "5.3 Comparison with GaLore and LTE"}, {"figure_path": "hXgLvYsG2c/tables/tables_13_1.jpg", "caption": "Table 1: Result of Transformer on MNLI of batch size 128.", "description": "This table presents the results of training a transformer model on the MNLI dataset using different methods: uncompressed training and CoMERA with varying target compression ratios (0.8, 0.5, 0.2).  For each method, it shows the validation accuracy, the total model size (in MB), and the compressed size of the tensorized layers (in MB). The table demonstrates the effectiveness of CoMERA in compressing the model while maintaining high accuracy.", "section": "5 Training Results"}, {"figure_path": "hXgLvYsG2c/tables/tables_16_1.jpg", "caption": "Table 4: Tensorized setting for the Transformer model in COMERA", "description": "This table shows the tensorization settings used for different layers in the Transformer model within the COMERA framework.  It specifies the format (TTM or TT) employed for compressing the weight matrices, the original linear shape of the weight matrices, the reshaped tensor shape after applying the chosen tensor decomposition, and the rank (a hyperparameter affecting the compression level) used for each layer type.", "section": "5.1 A Medium-Size Transformer with Six Encoders"}, {"figure_path": "hXgLvYsG2c/tables/tables_17_1.jpg", "caption": "Table 5: Speed-up of mixed-precision computation on tensor-compressed linear layers.", "description": "This table presents the speedup achieved by using mixed-precision computation (FP8-mix) compared to using full precision (FP32) for both tensor-vector and matrix-vector multiplication in CoMERA. Different input tensor shapes (b, m, n) are tested, where b represents the batch size, m is the input dimension, and n is the output dimension. The speedup is calculated as the ratio of the execution time of FP32 to the execution time of the respective mixed-precision method. The table shows that significant speedups can be obtained by utilizing mixed-precision computation, especially for larger tensor shapes.", "section": "A.11 Discussion: Mixed-Precision COMERA"}, {"figure_path": "hXgLvYsG2c/tables/tables_18_1.jpg", "caption": "Table 6: Training results of mixed-precision CoMERA on DLRM (batch size=10,000).", "description": "This table presents the training results of the DLRM model using two different precision methods: FP32 CoMERA (full precision) and FP8/FP32 mixed-precision CoMERA.  The results show the accuracy and normalized cross-entropy (CE) loss achieved by each method.  The data is for a batch size of 10,000.", "section": "5.2 A DLRM Model with 4-GB Model Size"}]