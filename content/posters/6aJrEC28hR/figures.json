[{"figure_path": "6aJrEC28hR/figures/figures_9_1.jpg", "caption": "Figure 1. We assess the tightness of our theoretical results on a regression problem on a synthetic data toy example consisting of two weighted circulant graphs. See Appendix D.1 for details. (Left) Numerical stability bound C(h) (dashed) and stability metrics ||h(T)||op (solid) with respect to input signal perturbation as a function of the number of epochs for both the standard (1-layer) GtNN (orange) and (1-layer) stable GtNN (blue). (Middle) Similar plot for the stability metrics with respect to the graph perturbation ||h(W) \u2013 h(Z)||op and its upper bound (Lemma 12 part 2 and 3b). For this plot we take W = T, and Z is a random perturbation from T with ||Z1 - W1||op \u2248 ||Z2 - W2||op \u2248 0.33. (Right) For all four models, compute the 2-norm of the vector of output perturbations from Equation (1) over the test set for various sizes of graph perturbation (||T1-W1||op + ||T2-W2||op)/2, where the additive graph perturbation T1 \u2013 W1 and T2 \u2013 W2 are symmetric matrices with iid Gaussian entries. In addition, each Tj and Wj are normalized such that ||Tj||op \u2264 1 and ||Wj||op \u2264 1 for j = 1,2, so they are nonexpansive operator-tuple networks. (All) We observe that adding stability constraints does not affect the prediction performance: the testing R squared value for GtNN is 0.6866, while for stable GtNN is 0.6543.", "description": "The figure shows the stability of the GtNN model against input signal and graph perturbations. The left and middle panels show the stability metrics (||h(T)||op) and their theoretical bounds (C(h)) for both standard and stable GtNN models. The right panel shows the output perturbation with respect to various graph perturbation sizes, for all four models (one-layer, one-layer stable, two-layer, two-layer stable). The results indicate that the stable GtNN model has better stability without compromising prediction accuracy.", "section": "7 Experimental data and numerical results"}, {"figure_path": "6aJrEC28hR/figures/figures_9_2.jpg", "caption": "Figure 2: This is an experiment on the MovieLens 100k database, a collection of movie ratings given by a set of 1000 users [39] to 1700 movies. Using collaborative filtering techniques [19] we extract two weighted graphs that we use to predict ratings of movies by user from a held out test set. See details in Appendix D.3. We report the mean squared error (MSE) in the test set as a function of the number of training iterations (Left) from 0 to 500 and (Right) from 0 to 1500 for the movie recommendation system experiments. We compare the two models GtNN on the tuple of two graphs (2ONN) and GNN on the best single graph between those two (GNN) on various ridge-regularized versions (the legend contains the values of the chosen regularization constants).", "description": "The figure shows the mean squared error (MSE) for a movie recommendation task on the MovieLens 100k dataset. Two graphs are extracted from the data using collaborative filtering.  The performance of two models is compared: GtNN (using both graphs) and GNN (using the best single graph).  The left panel shows iterations 0-500, the right 0-1500. Different regularization parameters are tested for both models.", "section": "Experimental data and numerical results"}, {"figure_path": "6aJrEC28hR/figures/figures_21_1.jpg", "caption": "Figure 3: Mean squared error (MSE) on the test set (with testing graph of size n = 300) as a function of the number of training epochs for (Left) (1-layer) GtNN and (Middle) (1-layer) stable GtNN. In both plots we depict the performance of five different models, trained with graphs of sizes m = 100, 150, 200, 250, 300 respectively. (Right) Comparison of testing MSE between (1-layer) GtNN (blue) and (1-layer) stable GtNN (orange) for training graphs of size m = 100 as a function of the number of epochs.", "description": "This figure presents the results of experiments evaluating the transferability of graph neural networks.  The left and middle panels show the mean squared error (MSE) on a test set (graph size n=300) for both standard and stable GtNN models, each trained on graphs of varying sizes (m=100, 150, 200, 250, 300).  The right panel compares the MSE of standard and stable GtNN models trained on graphs of size m=100, highlighting the impact of stability constraints on performance.", "section": "D.2 Experiments on transferability for sparse graph tuples"}]