{"references": [{"fullname_first_author": "Maksym Andriushchenko", "paper_title": "A modern look at the relationship between sharpness and generalization", "publication_date": "2023-00-00", "reason": "This paper provides a current overview of the relationship between sharpness and generalization, a key concept in the study of neural network training."}, {"fullname_first_author": "Maksym Andriushchenko", "paper_title": "Why do we need weight decay in modern deep learning?", "publication_date": "2023-00-00", "reason": "This paper investigates the necessity of weight decay in modern deep learning, a technique closely related to the impact of learning rates and generalization."}, {"fullname_first_author": "Maksym Andriushchenko", "paper_title": "SGD with large step sizes learns sparse features", "publication_date": "2023-00-00", "reason": "This paper directly addresses the impact of large learning rates and sparsity, a central theme explored in the main paper."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Practical recommendations for gradient-based training of deep architectures", "publication_date": "2012-00-00", "reason": "This paper offers practical advice on training deep architectures, providing a foundational context for understanding the optimization challenges involved in using large learning rates."}, {"fullname_first_author": "Maxim Kodryan", "paper_title": "Training scale-invariant neural networks on the sphere can happen in three regimes", "publication_date": "2022-00-00", "reason": "This paper introduces a taxonomy of neural network training regimes based on learning rate, which serves as a theoretical foundation for the empirical analysis in the main paper."}]}