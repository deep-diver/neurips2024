[{"type": "text", "text": "Efficiently Parameterized Neural Metriplectic Systems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Metriplectic systems are learned from data in a way that scales quadratically in both   \n2 the size of the state and the rank of the metriplectic data. Besides being provably   \n3 energy conserving and entropy stable, the proposed approach comes with approxi  \n4 mation results demonstrating its ability to accurately learn metriplectic dynamics   \n5 from data as well as an error estimate indicating its potential for generalization to   \n6 unseen timescales when approximation error is low. Examples are provided which illustrate performance in the presence of both full state information as well as when   \n8 entropic variables are unknown, confirming that the proposed approach exhibits superior accuracy and scalability without compromising on model expressivity. ", "page_idx": 0}, {"type": "text", "text": "10 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "11 The theory of metriplectic, also called GENERIC, systems [1, 2] provides a principled formalism   \n12 for encoding dissipative dynamics in terms of complete thermodynamical systems that conserve   \n13 energy and produce entropy. By formally expressing the reversible and irreversible parts of state   \n14 evolution with separate algebraic brackets, the metriplectic formalism provides a general mechanism   \nfor maintaining essential conservation laws while simultaneously respecting dissipative effects.   \n16 Thermodynamic completeness implies that any dissipation is caught within a metriplectic system   \n17 through the generation of entropy, allowing for a holistic treatment which has already found use in   \n18 modeling fluids [3, 4], plasmas [5, 6], and kinetic theories [7, 8].   \n19 From a machine learning point of view, the formal separation of conservative and dissipative effects   \n20 makes metriplectic systems highly appealing for the development of phenomenological models. Given   \n2 data which is physics-constrained or exhibits some believed properties, a metriplectic system can be   \n22 learned to exhibit the same properties with clearly identifiable conservative and dissipative parts. This   \n23 allows for a more nuanced understanding of the governing dynamics via an evolution equation which   \n24 reduces to an idealized Hamiltonian system as the dissipation is taken to zero. Moreover, elements   \n25 in the kernel of the learned conservative part are immediately understood as Casimir invariants,   \n26 which are special conservation laws inherent to the phase space of solutions, and are often useful   \n27 for understanding and exerting control on low-dimensional structure in the system. On the other   \n28 hand, the same benefti of metriplectic structure as a \u201cdirect sum\u201d of reversible and irreversible parts   \n29 makes it challenging to parameterize in an efficient way, since delicate degeneracy conditions must   \n30 be enforced in the system for all time. In fact, there are no methods at present for learning general   \n31 metriplectic systems which scale optimally with both dimension and the rank of metriplectic data\u2014an   \n32 issue which this work directly addresses.   \n33 Precisely, metriplectic dynamics on the finite or infinite dimensional phase space $\\mathcal{P}$ are generated by a   \n34 free energy function(al) $F:\\mathcal{P}\\rightarrow\\mathbb{R}$ , $F=E{+}S$ defined in terms of a pair $E$ $\\backslash,S:\\mathcal{P}\\rightarrow\\mathbb{R}$ representing   \n35 energy and entropy, respectively, along with two algebraic brackets $\\{\\cdot\\bar{,}\\cdot\\},[\\cdot,\\cdot]:C^{\\infty}(\\mathscr{P})\\times\\bar{C}^{\\infty}(\\mathscr{P})\\stackrel{{,}}{\\to}$   \n36 $C^{\\infty}({\\mathcal{P}})$ which are bilinear derivations on $C^{\\infty}({\\mathcal{P}})$ with prescribed symmetries and degeneracies   \n37 $\\{S,\\cdot\\}\\;=\\;[E,\\cdot]\\;=\\;0$ . Here $\\{\\cdot,\\cdot\\}$ is an antisymmetric Poisson bracket, which is a Lie algebra   \n38 realization on functions, and $[\\cdot,\\cdot]$ is a degenerate metric bracket which is symmetric and positive   \n39 semi-definite. When $\\mathcal P\\subset\\mathbb{R}^{n}$ for some $n\\,>\\,0$ , these brackets can be identified with symmetric   \n40 matrix fields $L:{\\mathcal{P}}\\to\\operatorname{Skew}_{n}(\\mathbb{R}),M:{\\mathcal{P}}\\to\\operatorname{Sym}_{n}(\\mathbb{R})$ satisfying $\\{F,G\\}\\,=\\,\\nabla F\\cdot\\dot{L}\\nabla G$ and   \n41 $[F,G]=\\nabla F\\cdot M\\nabla G$ for all functions $F,G\\in C^{\\infty}(\\mathcal{P})$ and all states $\\pmb{x}\\in\\mathcal{P}$ . Using the degeneracy   \n42 conditions along with $\\nabla x=I$ and abusing notation slightly then leads the standard equations for   \n43 metriplectic dynamics, ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\dot{\\boldsymbol{x}}=\\left\\{\\boldsymbol{x},\\boldsymbol{F}\\right\\}+\\left[\\boldsymbol{x},\\boldsymbol{F}\\right]=\\left\\{\\boldsymbol{x},\\boldsymbol{E}\\right\\}+\\left[\\boldsymbol{x},\\boldsymbol{S}\\right]=\\boldsymbol{L}\\boldsymbol{\\nabla}E+\\boldsymbol{M}\\boldsymbol{\\nabla}S,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "44 which are provably energy conserving and entropy producing. To see why this is the case, recall that   \n45 $L^{\\top}=-L$ . It follows that the infinitesimal change in energy satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\dot{E}=\\dot{x}\\cdot\\nabla E=L\\nabla E\\cdot\\nabla E+M\\nabla S\\cdot\\nabla E=-L\\nabla E\\cdot\\nabla E+\\nabla S\\cdot M\\nabla E=0,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "46 and therefore energy is conserved along the trajectory of $\\textbf{\\em x}$ . Similarly, the fact that $M^{\\intercal}\\,=\\,M$ is   \n47 positive semi-definite implies that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\dot{S}=\\dot{\\boldsymbol{x}}\\cdot\\boldsymbol{\\nabla}S=L\\boldsymbol{\\nabla}E\\cdot\\boldsymbol{\\nabla}S+M\\boldsymbol{\\nabla}S\\cdot\\boldsymbol{\\nabla}S=-\\boldsymbol{\\nabla}E\\cdot\\boldsymbol{L}\\boldsymbol{\\nabla}S+M\\boldsymbol{\\nabla}S\\cdot\\boldsymbol{\\nabla}S={|\\boldsymbol{\\nabla}S|}_{M}^{2}\\geq0,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "48 so that entropy is nondecreasing along $\\textbf{\\em x}$ as well. Geometrically, this means that the motion of a   \n49 trajectory $\\textbf{\\em x}$ is everywhere tangent to the level sets of energy and transverse to those of entropy,   \n50 reflecting the fact that metriplectic dynamics are a combination of noncanonical Hamiltonian $M=$   \n51 0) and generalized gradient ${\\boldsymbol{L}}\\;=\\;\\mathbf{0},$ ) motions. Note that these considerations also imply the   \n52 Lyapunov stability of metriplectic trajectories, as can be seen by taking $E$ as a Lyapunov function.   \n53 Importantly, this also implies that metriplectic trajectories which start in the (often compact) set   \n54 $\\hat{K}\\stackrel{\\cdot}{=}\\{{\\pmb x}\\,|\\,\\dot{E}({\\pmb x})\\leq E({\\pmb x}_{0})\\}$ remain there for all time.   \n55 In phenomenological modeling, the entropy $S$ is typically chosen from Casimirs of the Poisson   \n56 bracket generated by $\\textbf{\\emph{L}}$ , i.e. those quantities $C\\in C^{\\infty}(\\mathcal{P})$ such that $\\pmb{L}\\nabla C=\\mathbf{0}$ . On the other hand,   \n57 the method which will be presented here, termed neural metriplectic systems (NMS), allows for all of   \n58 the metriplectic data ${\\pmb{L}},{\\pmb{M}},E,S$ to be approximated simultaneously, removing the need for Casimir   \n59 invariants to be known or assumed ahead of time. The only restriction inherent to NMS is that the   \n60 metriplectic system being approximated is nondegenerate (c.f. Definition 3.1), a mild condition   \n61 meaning that the gradients of energy and entropy cannot vanish at any point $\\pmb{x}\\in\\mathcal{P}$ in the phase space.   \n62 It will be shown that NMS alleviates known issues with previous methods for metriplectic learning,   \n63 leading to easier training, superior parametric efficiency, and better generalization performance.   \n64 Contributions. The proposed NMS method for learning metriplectic models offers the following   \n65 advantages over previous state-of-the-art: (1) It approximates arbitrary nondegenerate metriplectic   \n66 dynamics with optimal quadratic scaling in both the problem dimension $n$ and the rank $r$ of the   \n67 irreversible dynamics. (2) It produces realistic, thermodynamically consistent entropic dynamics   \n68 from unobserved entropy data. (3) It admits universal approximation and error accumulation results   \n69 given in Proposition 3.7 and Theorem 3.9. (4) It yields exact energy conservation and entropy stability   \n70 by construction, allowing for effective generalization to unseen timescales. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "71 2 Previous and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 Previous attempts to learn metriplectic systems from data separate into \u201chard\u201d and \u201csoft\u201d constrained   \n73 methods. Hard constrained methods enforce metriplectic structure by construction, so that the   \n74 defining properties of metriplecticity cannot be violated. Conversely, methods with soft constraints   \n75 relax some aspects of metriplectic structure in order to produce a wider model class which is easier to   \n76 parameterize. While hard constraints are the only way to truly guarantee appropriate generalization   \n77 in the learned surrogate, the hope of soft constrained methods is that the resulting model is \u201cclose   \n78 enough\u201d to metriplectic that it will exhibit some of the favorable characteristics of metriplectic   \n79 systems, such as energy and entropy stability. Some properties of the methods compared in this work   \n80 are summarized in Table 1.   \n81 Soft constrained methods. Attempts to learn metriplectic systems using soft constraints rely on   \n82 relaxing the degeneracy conditions $L\\nabla S=M\\nabla E=\\mathbf{0}$ . This is the approach taken in [9], termed   \n83 SPNN, which learns an almost-metriplectic model parameterized with generic neural networks   \n84 through a simple $L^{2}$ penalty term in the training loss, $\\mathcal{L}_{\\mathrm{pen}}=|L\\nabla E|^{2}+|M\\nabla S|^{2}$ . This widens   \n85 the space of allowable network parameterizations for the approximate operators $L,M$ . While   \n86 the resulting model violates the first and second laws of thermodynamics, the authors show that   \n87 reasonable trajectories are still obtained, at least when applied within the range of timescales used   \n88 for training. A similar approach is taken in [10], which targets larger problems and develops an   \n89 almost-metriplectic model reduction strategy based on the same core idea.   \n90 Hard constrained methods. Perhaps the first example of learning metriplectic systems from data   \n91 was given in [11] in the context of system identification. Here, training data is assumed to come from   \n92 a finite element simulation, so that the discrete gradients of energy and entropy can be approximated   \n93 as $\\nabla E(\\pmb{x})=A\\pmb{x}$ , $\\nabla S(\\pmb{x})=B\\pmb{x}$ . Assuming a fixed form for $\\textbf{\\emph{L}}$ produces a constrained learning   \n94 problem for the constant matrices $M,A,B$ which is solved to yield a provably metriplectic surrogate   \n95 model. Similarly, the work [12] learns $M,E$ given $L,S$ by considering a fixed block-wise decoupled   \n96 form which trivializes the degeneracy conditions, i.e. $\\pmb{L}=[\\star\\ \\mathbf{0};\\mathbf{0}\\ \\mathbf{0}]$ and $M=[\\mathbf{0}~\\mathbf{0};\\mathbf{0}\\star]$ . This   \n97 line of thought is continued in [13] and [14], both of which learn metriplectic systems with neural   \n98 network parameterizations by assuming this decoupled block structure. A somewhat broader class   \n99 of metriplectic systems are considered in [15] using tools from exterior calculus, with the goal of   \n100 learning metriplectic dynamics on graph data. This leads to a structure-preserving network surrogate   \n101 which scales linearly in the size of the graph domain, but also cannot express arbitrary metriplectic   \n102 dynamics due to the specific choices of model form for $L,M$ .   \n103 A particularly inspirational method for learning general metriplectic systems was given in [16] and   \n104 termed GNODE, building on parameterizations of metriplectic operators developed in [17]. GNODE   \n105 parameterizes learnable reversible and irreversible bracket generating matrices $L,M$ in terms of state  \n106 independent tensors $\\pmb{\\xi}\\in(\\mathbb{R}^{n})^{\\otimes3}$ and $\\zeta\\,\\in\\,(\\mathbb{R}^{n})^{\\otimes4}$ : for $1\\leq\\alpha,\\beta,\\gamma,\\mu,\\nu\\leq n$ , the authors choose   \n107 $\\begin{array}{r}{L_{\\alpha\\beta}\\mathbf{\\dot{(}}x)=\\sum_{\\gamma}\\xi_{\\alpha\\beta\\gamma}\\mathbf{\\bar{\\partial}}^{\\gamma}S}\\end{array}$ and $\\begin{array}{r}{M_{\\alpha\\beta}({\\pmb x})=\\sum_{{\\mu},{\\nu}}\\zeta_{\\alpha{\\mu},\\beta{\\nu}}\\partial^{{\\mu}}E\\partial^{{\\nu}}E}\\end{array}$ , where $\\partial^{\\alpha}F=\\partial F/\\partial x_{\\alpha},$ $\\xi$ is to  \n108 tally antisymmetric, and $\\zeta$ is symmetric between the pairs $(\\alpha,\\mu)$ and $(\\beta,\\nu)$ but antisymmetric within   \n109 each of these pairs. The key idea here is to exchange the problem of enforcing degeneracy conditions   \n110 $L\\nabla E=M\\bar{\\nabla}S=\\mathbf{0}$ in matrix fields $L,M$ with the problem of enforcing symmetry conditions in   \n111 tensor fields $\\xi,\\zeta$ , which is comparatively easier but comes at the expense of underdetermining the   \n112 problem. In GNODE, enforcement of these symmetries is handled by a generic learnable 3-tensor   \n113 $\\tilde{\\pmb{\\xi}}\\in(\\mathbb{R}^{n})^{\\otimes3}$ along with learnable matrices $D\\in\\mathrm{Sym}_{r}(\\mathbb{R})$ and $\\mathbf{A}^{s}\\in\\operatorname{Skew}_{n}(\\mathbb{R})$ for $1\\leq s\\leq r\\leq n$ ,   \n114 leading to the final parameterizations $\\begin{array}{r}{\\xi_{\\alpha\\beta\\gamma}=\\frac{1}{3!}\\Big(\\tilde{\\xi}_{\\alpha\\beta\\gamma}-\\tilde{\\xi}_{\\alpha\\gamma\\beta}+\\tilde{\\xi}_{\\beta\\gamma\\alpha}-\\tilde{\\xi}_{\\beta\\alpha\\gamma}+\\tilde{\\xi}_{\\gamma\\alpha\\beta}-\\tilde{\\xi}_{\\gamma\\beta\\alpha}\\Big)}\\end{array}$ and   \n115 $\\begin{array}{r}{\\zeta_{\\alpha\\mu,\\beta\\nu}=\\sum_{s,t}\\Lambda_{\\alpha\\mu}^{s}D_{s t}\\Lambda_{\\beta\\nu}^{t}}\\end{array}$ . Along with learnable energy and entropy functions $E,S$ parameterized   \n116 by multi-layer perceptrons (MLPs), the data $L,M$ learned by GNODE guarantees metriplectic   \n117 structure in the surrogate model and leads to successful learning of metriplectic systems in some   \n118 simple cases of interest. However, note that this is a highly redundant parameterization requiring   \n119 ${\\binom{n}{3}}^{*}+r{\\binom{n}{2}}\\,+\\,{\\binom{r+1}{2}}\\,+\\,2$ learnable scalar functions, which exhibits $O(n^{3}+r n^{2})$ scaling in the   \n120 problem size because of the necessity to compute and store $\\binom{n}{3}$ entries of $\\xi$ and $r\\left({n\\atop2}\\right)$ entries of $\\pmb{\\Lambda}$   \n121 Additionally, the assumption of state-independence in the bracket generating tensors $\\xi,\\zeta$ is somewhat   \n122 restrictive, limiting the class of problems to which GNODE can be applied.   \nA related approach to learning metriplectic dynamics with hard constraints was seen in [18], which   \nproposed a series of GFINN architectures depending on how much of the information ${\\pmb{L}},{\\pmb{M}},E,S$   \n125 is assumed to be known. In the case that $L,M$ are known, the GFINN energy and entropy are   \n126 parameterized with scalar-valued functions $f\\circ P_{\\mathrm{ker}A}$ where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ $E$ or $S$ ) is learnable and   \n127 $P_{\\ker A}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ is orthogonal projection onto the kernel of the (known) operator $\\pmb{A}$ $\\textbf{\\em L}$ or $M_{\\sun}$ ).   \n128 It follows that the gradient $\\nabla(f\\circ P_{\\mathrm{ker}}A)=P_{\\mathrm{ker}A}\\nabla f(P_{\\mathrm{ker}A})$ lies in the kernel of $\\pmb{A}$ , so that the   \n129 degeneracy conditions are guaranteed at the expense of constraining the model class of potential ener  \n130 gies/entropies. Alternatively, in the case that all of $\\pmb{L},\\pmb{M},E,S$ are unknown, GFINNs use learnable   \n131 scalar functions $f$ for $E,S$ parameterized by MLPs as well as two matrix fields $Q^{E},Q^{S}\\in\\mathbb{R}^{r\\times n}$   \n132 with rows given by $\\pmb q_{s}^{f}\\;=\\;\\bar{\\left(S_{s}^{f}\\nabla f\\right)^{\\intercal}}$ for learnable skew-symmetric matrices $S_{s}^{f}$ , $1~\\le~s~\\le~r$ ,   \n133 $f=E,S.$ . Along with two triangular $(r\\times r)$ matrix fields $T_{L},T_{M}$ , this yields the parameterizations   \n134 ${\\bf\\dot{\\cal L}}(x)\\;=\\;Q^{S}(\\bar{x^{\\phantom{\\dagger}}})^{\\dagger}(T_{L}(x)^{\\dagger}-{\\bar{T}}_{L}(x))Q^{S}(x)$ and ${\\cal M}({\\pmb x})\\,\\,=\\,\\,\\bar{\\cal Q}^{E}({\\pmb x})^{\\dagger}(T_{M}({\\pmb x})^{\\dagger}T_{M}({\\pmb x}))Q^{E}({\\pmb x})$ ,   \n135 which necessarily satisfy the symmetries and degeneracy conditions required for metriplectic struc  \n136 ture. GFINNs are shown to both increase expressivity over the GNODE method as well as decrease   \n137 redundancy, since the need for an explicit order-3 tensor field is removed and the reversible and   \n138 irreversible brackets are allowed to depend explicitly on the state $\\textbf{\\em x}$ . However, GFINNs still exhibit   \n139 cubic scaling through the requirement of $r n(\\stackrel{.}{n^{}}-1)\\stackrel{.}{+}r^{2}+2=\\mathcal{O}\\big(r n^{2}\\big)$ learnable functions, which   \n140 is well above the theoretical minimum required to express a general metriplectic system and leads to   \n141 difficulties in training the resulting models.   \n142 Model reduction. Finally, it is worth mentioning the closely related line of work involving model   \n143 reduction for metriplectic systems, which began with [19]. As remarked there, preserving metriplec  \n144 ticity in reduced-order models (ROMs) exhibits many challenges due to its delicate requirements on   \n145 the kernels of the involved operators. There are also hard and soft constrained approaches: the already   \n146 mentioned [10] aims to learn a close-to-metriplectic data-driven ROM by enforcing degeneracies by   \n147 penalty, while [20] directly enforces metriplectic structure in projection-based ROMs using exterior   \n148 algebraic factorizations. The parameterizations of metriplectic data presented here are related to those   \n149 presented in [20], although NMS does not require access to nonzero components of $\\nabla E,\\nabla S$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "150 3 Formulation and Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "151 The proposed formulation of the metriplectic bracket-generating operators $L,M$ used by NMS is   \n152 based on the idea of exploiting structure in the tensor fields $\\xi,\\zeta$ to reduce the necessary number   \n153 of degrees of freedom. In particular, it will be shown that the degeneracy conditions $L\\nabla S\\;=$   \n154 $M\\nabla E=\\mathbf{0}$ imply more than just symmetry constraints on these fields, and that taking these additional   \n155 constraints into account allows for a more compact representation of metriplectic data. Following   \n156 this, results are presented which show that the proposed formulation is universally approximating on   \n157 nondegenerate systems (c.f. Definition 3.1) and admits a generalization error bound in time. ", "page_idx": 3}, {"type": "text", "text": "158 3.1 Exterior algebra ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "159 Developing these metriplectic expressions will require some basic facts from exterior algebra, of   \n160 which more details can be found in, e.g., [21, Chapter 19]. The basic objects in the exterior algebra   \n161 $\\Lambda V$ over the vector space $V$ are multivectors, which are formal linear combinations of totally   \n162 antisymmetric tensors on $V$ . More precisely, if $I(V)$ denotes the two-sided ideal of the free tensor   \n163 algebra $T(V)$ generated by elements of the form $\\pmb{v}\\otimes\\pmb{v}$ $(\\pmb{v}\\in V)$ , then the exterior algebra is the   \n164 quotient space $\\dot{\\mathsf{\\Omega}}\\Lambda V\\simeq T(\\dot{V})/I(V)$ equipped with the antisymmetric wedge product operation $\\wedge$   \n165 This graded algebra is equipped with natural projection operators $P^{k}:{\\textstyle\\bigwedge}V\\to{\\textstyle\\bigwedge}^{k}V$ which map   \n166 between the full exterior algebra and the $k^{\\mathrm{th}}$ exterior power of $V$ , denot ed $\\textstyle\\bigwedge^{k}V$ , whose elements   \n167 are homogeneous $k$ -vectors. More generally, given an $n$ -manifold $M$ with t angent bundle $T M$ , the   \n168 exterior algebra $\\Lambda(T M)$ is the algebra of multivector fields whose fiber over $x\\in M$ is given by   \n169 $\\Lambda T_{x}M$ .   \n170 For the present purposes, it will be useful to develop a correspondence between bivectors $\\mathsf{B}\\in\\textstyle\\bigwedge^{2}(\\mathbb{R}^{n})$   \n171 and skew-symmetric matrices $B\\in\\operatorname{Skew}_{n}(\\mathbb{R})$ , which follows directly from Riesz representation in   \n172 terms of the usual Euclidean dot product. More precisely, supposing that $e_{1},...,e_{n}$ are the standard   \n173 basis vectors for $\\mathbb{R}^{n}$ , any bivector $\\mathsf{B}\\in\\mathsf{\\Lambda}^{2}T\\mathbb{R}^{n}$ can be represented as $\\begin{array}{r}{\\mathsf{B}=\\sum_{i<j}B^{i j}\\pmb{e}_{i}\\wedge\\pmb{e}_{j}}\\end{array}$ where   \n174 $B^{i j}\\in\\mathbb{R}$ denote the components of B. Define a grade-lowering action of bivectors on vectors through   \n175 right contraction (see e.g. Section 3.4 of [22]), expressed for any vector $\\pmb{v}$ and basis bivector $e_{i}\\wedge e_{j}$   \n176 as $(e_{i}\\wedge e_{j})\\cdot v=(e_{j}\\cdot{\\bar{v}})e_{i}-(e_{i}\\cdot v)e_{j}$ . It follows that the action of $\\textsf{B}$ is equivalent to ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{B}\\cdot v=\\sum_{i<j}B^{i j}\\big((e_{j}\\cdot v)e_{i}-(e_{i}\\cdot v)e_{j}\\big)=\\sum_{i<j}B^{i j}v_{j}e_{i}-\\sum_{j<i}B^{j i}v_{j}e_{i}=\\sum_{i,j}B^{i j}v_{j}e_{i}=B v,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "177 where $B^{\\boldsymbol{\\uptau}}\\,=\\,-\\pmb{B}\\,\\in\\,\\mathbb{R}^{n\\times n}$ is a skew-symmetric matrix representing $\\textsf{B}$ , and we have re-indexed   \n178 under the second sum and applied that $\\mathbf{\\check{B}}^{i j}=-B^{j i}$ for all $i,j$ . Since the kernel of this action is   \n179 the zero bivector, it is straightforward to check that this string of equalities defines an isomorphism   \n180 $M:{\\textstyle\\bigwedge}^{2}\\mathbb{R}^{n}\\,\\to\\,\\operatorname{Skew}_{n}(\\mathbb{R})$ from the $2^{\\mathrm{nd}}$ exterior power of $\\mathbb{R}^{n}$ to the space of skew-symmetric   \n181 $(n\\times n)$ -matrices over $\\mathbb{R}$ : in what follows, we will write $B\\simeq\\mathsf{B}$ rather than $B=\\mathcal{M}(\\mathsf{B})$ for notational   \n182 convenience. Note that a correspondence in the more general case of bivector/matrix fields follows in   \n183 the usual way via the fiber-wise extension of $\\mathcal{M}$ . ", "page_idx": 3}, {"type": "text", "text": "184 3.2 Learnable metriplectic operators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "185 It is now possible to explain the proposed NMS formulation. First, note the following key definition   \n186 which prevents the consideration of unphysical examples.   \n187 Definition 3.1. A metriplectic system on $K\\subset\\mathbb{R}^{n}$ generated by the data ${\\pmb{L}},{\\pmb{M}},E,S$ will be called   \n188 nondegenerate provided $\\nabla E$ , $\\nabla S\\neq\\mathbf{0}$ for all $\\pmb{x}\\in K$ .   \n189 With this, the NMS parameterizations for metriplectic operators are predicated on an algebraic result   \n190 proven in Appendix A.   \n191 Lemma 3.2. Let $K\\,\\subset\\,\\mathbb{R}^{n}$ . For all $\\pmb{x}\\in\\cal{K}$ , the operator $\\pmb{L}:\\,\\boldsymbol{K}\\,\\rightarrow\\,\\mathbb{R}^{n\\times n}$ satisfies $L^{\\top}\\,=\\,-L$   \n192 and $\\pmb{L}\\nabla S=\\mathbf{0}$ for some $S:K\\rightarrow\\mathbb{R}$ , $\\nabla S\\neq\\mathbf{0},$ , provided there exists a non-unique bivector field   \n193 $\\mathsf{A}:U\\to\\Lambda^{2}\\mathbb{R}^{n}$ and equivalent matrix field $A\\simeq\\mathsf{A}$ such that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{L}\\simeq\\left(\\mathsf{A}\\wedge\\frac{\\nabla S}{\\left|\\nabla S\\right|^{2}}\\right)\\cdot\\nabla S=\\mathsf{A}-\\frac{1}{\\left|\\nabla S\\right|^{2}}\\pmb{A}\\nabla S\\wedge\\nabla S.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "194 Similarly, for all $\\pmb{x}\\in K$ a positive semi-definite operator $M:K\\rightarrow\\mathbb{R}^{n\\times n}$ satisfies $M^{\\intercal}\\,=\\,M$   \n195 and $M\\nabla E=\\mathbf{0}$ for some $E:K\\rightarrow\\mathbb{R}$ , $\\nabla E\\neq\\mathbf{0},$ , provided there exists a non-unique matrix-valued   \n196 $\\pmb{B}:K\\rightarrow\\mathbb{R}^{n\\times r}$ and symmetric matrix-valued $D:K\\rightarrow\\mathbb{R}^{r\\times r}$ such that $r\\leq n$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle{M=\\sum_{s,t}D_{s t}\\left(b^{s}\\land\\frac{\\nabla E}{\\left|\\nabla E\\right|^{2}}\\right)\\cdot\\nabla E\\,\\otimes\\,\\left(b^{t}\\land\\frac{\\nabla E}{\\left|\\nabla E\\right|^{2}}\\right)\\cdot\\nabla E}}\\\\ &{}&{\\displaystyle{=\\sum_{s,t}D_{s t}\\left(b^{s}-\\frac{b^{s}\\cdot\\nabla E}{\\left|\\nabla E\\right|^{2}}\\nabla E\\right)\\left(b^{t}-\\frac{b^{t}\\cdot\\nabla E}{\\left|\\nabla E\\right|^{2}}\\nabla E\\right)^{\\intercal}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "197 where $b^{s}$ denotes the $s^{\\mathrm{th}}$ column of $_B$ . Moreover, using $\\begin{array}{r}{P_{f}^{\\perp}\\;=\\;\\left(I-\\frac{\\nabla f\\,\\nabla f^{\\intercal}}{|\\nabla f|^{2}}\\right)}\\end{array}$ to denote the   \n198 orthogonal projector onto $\\mathrm{Span}(\\nabla f)^{\\perp}$ , these parameterizations of $L,M$ are equivalent to the   \n199 matricized expressions $L=P_{S}^{\\perp}A P_{S}^{\\perp}$ and $M=P_{E}^{\\perp}B D B^{\\intercal}P_{E}^{\\perp}$ .   \n200 Remark 3.3. Observe that the projections appearing in these expressions are the minimum necessary   \n201 for guaranteeing the symmetries and degeneracy conditions necessary for metriplectic structure. In   \n202 particular, conjugation by $P_{f}^{\\perp}$ respects symmetry and ensures that both the input and output to the   \n203 conjugated matrix field lie in $\\mathrm{Span}(\\nabla f)^{\\perp}$ .   \n204 Lemma 3.2 demonstrates specific parameterizations for $L,M$ that hold for any nondegenerate   \n205 metriplectic data and are core to the NMS method for learning metriplectic dynamics. While   \n206 generally underdetermined, these expressions are in a sense maximally specific given no additional   \n207 information, since there is nothing available in the general metriplectic formalism to determine the   \n208 matrix fields $A,B D B^{\\intercal}$ on $\\operatorname{Span}(\\nabla S),\\operatorname{Span}(\\nabla E)$ , respectively. The following Theorem, also   \n209 proven in Appendix A, provides a rigorous correspondence between metriplectic systems and these   \n210 particular parameterizations.   \n211 Theorem 3.4. The data ${\\pmb{L}},{\\pmb{M}},E,S$ form a nondegenerate metriplectic system in the state variable   \n212 $\\textbf{\\em x}\\in~K$ if and only if there exist a skew-symmetric $A\\,:\\,K\\,\\to\\,\\operatorname{Skew}_{n}(\\mathbb{R}),$ , symmetric postive   \n213 semidefinite $D:K\\to\\operatorname{Sym}_{r}(\\mathbb{R}),$ , and generic $\\pmb{B}:K\\rightarrow\\mathbb{R}^{n\\times r}$ such that ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{\\mathbf{x}}=L\\boldsymbol{\\nabla}E+M\\boldsymbol{\\nabla}S=P_{S}^{\\perp}A P_{S}^{\\perp}\\boldsymbol{\\nabla}E+P_{E}^{\\perp}B D B^{\\intercal}P_{E}^{\\perp}\\boldsymbol{\\nabla}S.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "214 Remark 3.5. Note that the proposed parameterizations for $L,M$ are not one-to-one but properly   \n215 contain the set of valid nondegenerate metriplectic systems in $E,S$ , since the Jacobi identity on $\\textbf{\\emph{L}}$   \n216 necessary for a true Poisson manifold structure is not strictly enforced. For $1\\leq\\,i,j,k\\,\\leq\\,n$ , the   \n217 Jacobi identity is given in components as $\\begin{array}{r}{\\sum_{\\ell}L_{i\\ell}\\partial^{\\ell}L_{j k}+L_{j\\ell}\\dot{\\partial}^{\\ell}L_{k i}+L_{k\\ell}\\partial^{\\ell}L_{i j}=0.}\\end{array}$ . However, this   \n218 requirement is not often enforced in algo rithms for learning general metriplectic (or even symplectic)   \n219 systems, since it is considered subordinate to energy conservation and it is well known that both   \n220 qualities cannot hold simultaneously in general [23]. ", "page_idx": 4}, {"type": "text", "text": "221 3.3 Specific parameterizations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "222 Now that Theorem 3.4 has provided a model class which is rich enough to express any desired   \n223 metriplectic system, it remains to discuss what NMS actually learns. First, note that it is unlikely to   \n224 be the case that $E,S$ are known a priori, so it is beneficial to allow these functions to be learnable   \n225 alongside the governing operators $L,M$ . For simplicity, energy and entropy $E,S$ are parameterized   \n226 as scalar-valued MLPs with tanh activation, although any desired architecture could be chosen for   \n227 this task. The skew-symmetric matrix field $A\\,=\\,A_{\\mathrm{tri}}\\,-\\,A_{\\mathrm{tri}}^{\\top}$ used to build $\\textbf{\\emph{L}}$ is parameterized   \n228 through its strictly lower-triangular part $A_{\\mathrm{tri}}$ using a vector-valued MLP with output dimension $\\binom{n}{2}$ ,   \n229 which guarantees that the mapping $A_{\\mathrm{tri}}\\mapsto A$ above is bijective. Similarly, the symmetric matrix   \n230 field $\\bar{D}=K_{\\mathrm{chol}}K_{\\mathrm{chol}}^{\\intercal}$ is parameterized through its lower-triangular Cholesky factor $K_{\\mathrm{chol}}$ , which   \n231 is a vector-valued MLP with output dimension $\\textstyle{\\binom{r+1}{2}}$ . While this choice does not yield a bijective   \n232 mapping $K_{\\mathrm{chol}}\\mapsto D$ unless, e.g., $_{D}$ is assumed to be positive definite with diagonal entries of fixed   \n233 sign, this does not hinder the method in practice. In fact, $_{D}$ should not be positive definite in general,   \n234 as this is equivalent to claiming that $_M$ is positive definite on vectors tangent to the level sets of $E$ .   \n235 Finally, the generic matrix field $_B$ is parameterized as a vector-valued MLP with output dimension   \n236 nr. Remarkably, the exterior algebraic expressions in Lemma 3.2 require less redundant operations   \n237 than the corresponding matricized expressions from Theorem 3.4, and therefore the expressions from   \n238 Lemma 3.2 are used when implementing NMS. Figure 1 summarizes this information.   \n239 Remark 3.6. With these choices, the NMS parameterization of metriplectic systems requires only   \n240 $(1/2){\\bigl(}(n+r)^{2}-(n-r){\\bigr)}+2$ learnable scalar functions, in contrast to ${\\binom{n}{3}}+{\\dot{r}}{\\binom{n}{2}}+{\\binom{r+1}{2}}+2$ for   \n241 the GNODE approach in [16] and $r n(n-1)+r^{2}+2$ for the GFINN approach in [18]. In particular,   \n242 NMS is quadratic in both $n,r$ with no decrease in model expressivity, in contrast to the cubic scaling   \n243 of previous methods. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "3mzFmBPFIX/tmp/2112c66cd9133167ee62ee6a6358b1e80e673d9eb3ac2b16e303da0ef41a8ea2.jpg", "table_caption": ["Table 1: Properties of the metriplectic architectures compared. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "3mzFmBPFIX/tmp/148653a20b48d3494a5fceb86e0eeb5ddf02f8b07da38c08497b3bbf2270c8a7.jpg", "img_caption": ["Figure 1: A visual depiction of the NMS architecture. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "245 3.4 Approximation and error ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "246 Besides offering a compact parameterization of metriplectic dynamics, the expressions used in NMS   \n247 also exhibit desirable approximation properties which guarantee a reasonable bound on state error   \n248 over time. To state this precisely, first note the following universal approximation result proven in   \n249 Appendix A.   \n250 Proposition 3.7. Let $K\\subset\\mathbb{R}^{n}$ be compact and $E,S:K\\rightarrow\\mathbb{R}$ be continuous such that $L\\nabla S=$   \n251 $M\\nabla E=\\mathbf{0}$ and $\\nabla E,\\nabla S\\neq\\mathbf{0}$ for all $\\pmb{x}\\in K$ . For any $\\varepsilon>0$ , there exist two-layer neural network   \n252 functions $\\tilde{E},\\tilde{S}:K\\to\\mathbb{R},\\tilde{L}:K\\to\\operatorname{Skew}_{n}(\\mathbb{R})$ and ${\\tilde{M}}:K\\to\\operatorname{Sym}_{n}(\\mathbb{R})$ such that $\\nabla\\tilde{E}$ , $\\nabla\\tilde{S}\\neq\\mathbf{0}$ on   \n253 $K,\\,{\\tilde{M}}$ is positive semi-definite, $\\tilde{L}\\nabla\\tilde{S}=\\tilde{M}\\nabla\\tilde{E}=\\mathbf{0}$ for all $\\pmb{x}\\in K$ , and each approximate function   \n254 is $\\varepsilon$ -close to its given counterpart on $K$ . Moreover, $i f L,M$ have $k\\geq0$ continuous derivatives on $K$   \n255 then so do $\\tilde{L},\\tilde{M}$ .   \n256 Remark 3.8. The assumption $\\pmb{x}\\in K$ of the state remaining in a compact set $V$ is not restrictive when   \n257 at least one of $E,-S:\\mathbb{R}^{n}\\to\\mathbb{R}$ , say $E$ , has bounded sublevel sets. In this case, letting ${\\pmb x}_{0}={\\pmb x}(0)$ it   \n258 follows from $\\dot{E}\\leq0$ that $\\begin{array}{r}{E(\\pmb{x}(t))\\stackrel{}{=}E(\\pmb{x}_{0})+\\int_{0}^{t}\\dot{E}(\\pmb{x}(\\tau))\\,d\\tau\\leq E(\\pmb{x}_{0})}\\end{array}$ , so that the entire trajectory   \n259 ${\\mathbf{}}x(t)$ lies in the (closed and bounded) compact set $K=\\{\\pmb{x}\\,|\\,E(\\pmb{x})\\leq E(\\pmb{x}_{0})\\}\\subset\\mathbb{R}^{n}$ .   \n260 Leaning on Proposition 3.7 and classical universal approximation results in [24], it is further possible   \n261 to establish the following error estimate also proven in Appendix A which gives an idea of the error   \n262 accumulation rate that can be expected from this method.   \n263 Theorem 3.9. Suppose ${\\pmb{L}},{\\pmb{M}},E,S$ are nondegenerate metriplectic data such that $L,M$ have at   \n264 least one continuous derivative, $E,S$ have Lipschitz continuous gradients, and at least one of $E,-S$   \n265 have bounded sublevel sets. For any $\\varepsilon>0$ , there exist nondegenerate metriplectic data $\\tilde{\\mathbf{\\cal{L}}},\\tilde{M},\\tilde{E},\\tilde{S}$   \n266 defined by two-layer neural networks such that, for all $T>0$ , ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(\\int_{0}^{T}\\lvert x-\\tilde{x}\\rvert^{2}\\,d t\\right)^{\\frac{1}{2}}\\leq\\varepsilon\\bigg\\lvert\\frac{b}{a}\\bigg\\rvert\\sqrt{e^{2a T}-2e^{a T}+T+1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "267 where a, $b\\in\\mathbb{R}$ are constants depending on both sets of metriplectic data and $\\dot{\\tilde{\\mathbf{x}}}=\\tilde{L}(\\tilde{\\mathbf{x}})\\nabla\\tilde{E}(\\tilde{\\mathbf{x}})\\,+$   \n268 $\\tilde{M}(\\tilde{\\pmb{x}})\\nabla\\tilde{S}(\\tilde{\\pmb{x}})$ .   \n269 Remark 3.10. Theorem 3.9 provides a bound on state error over time under the assumption that the   \n270 approximation error in the metriplectic networks can be controlled. On the other hand, notice that   \n271 Theorem 3.9 can also be understood as a generic error bound on any trained metriplectic networks   \n272 $\\tilde{L},\\tilde{M},\\tilde{E},\\tilde{S}$ provided universal approximation results are not invoked in the estimation leading to $\\varepsilon b$   \n273 This result confirms that the error in the state $\\textbf{\\em x}$ for a fixed final time $T$ tends to zero with the   \n274 approximation error in the networks $\\tilde{L},\\tilde{M},\\tilde{E},\\tilde{S}$ , as one would hope based on the approximation   \n275 capabilities of neural networks. More importantly, Theorem 3.9 also bounds the generalization error   \n276 of any trained metriplectic network for an appropriate (and possibly large) $\\varepsilon$ equal to the maximum   \n277 approximation error on $K$ , where the learned metriplectic trajectories are confined for all time.   \n278 With this theoretical guidance, the remaining goal of this work is to demonstrate that NMS is also   \n279 practically effective at learning metriplectic systems from data and exhibits reasonable generalization   \n280 to unseen timescales. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "281 4 Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "282 Similar to previous approaches in [16] and [18], the learnable parameters in NMS are calibrated   \n283 using data along solution trajectories to a given dynamical system. This brings up an important   \n284 question regarding how much information about the system in question is realistically present in   \n285 the training data. While many systems have a known metriplectic form, it is not always the case   \n286 that one will know metriplectic governing equations for a given set of training data. Therefore, two   \n287 approaches are considered in the experiments below corresponding to whether full or partial state   \n288 information is assumed available during NMS training. More precisely, the state $\\pmb{x}=(\\pmb{x}^{o},\\pmb{x}^{u})$ will   \n289 be partitioned into \u201cobservable\u201d and \u201cunobservable\u201d variables, where $\\pmb{x}^{u}$ may be empty in the case   \n290 that full state information is available. In a partially observable system $x^{o}$ typically contains positions   \n291 and momenta while $\\pmb{x}^{u}$ contains entropy or other configuration variables which are more difficult   \n292 to observe during physical experiments. In both cases, NMS will learn a metriplectic system in $\\textbf{\\em x}$   \n293 according to the procedure described in Algorithm 1.   \nAlgorithm 1 Training neural metriplectic systems   \n1: Input: snapshot data $X\\in\\mathbb{R}^{n\\times n_{s}}$ , each column ${\\pmb x}_{s}={\\pmb x}(t_{s},{\\pmb\\mu}_{s})$ , target rank $r\\geq1$ , batch size   \n$n_{b}\\geq1$ .   \n2: Initialize networks $A_{\\mathrm{tri}},B,K_{\\mathrm{chol}},E,S$ , and loss $L=0$   \n3: for step in $N_{\\mathrm{steps}}$ do   \n4: Randomly draw batch $P=\\{(t_{s_{k}},\\mathbf{x}_{s_{k}})\\}_{k=1}^{n_{b}}$   \n5: for $(t,{\\boldsymbol{x}})$ in $P$ do   \n6: Evaluate $A_{\\mathrm{tri}}({\\boldsymbol{x}}),B({\\boldsymbol{x}}),K_{\\mathrm{chol}}({\\boldsymbol{x}}),E({\\boldsymbol{x}}),S({\\boldsymbol{x}})$   \n7: Automatically differentiate $E,S$ to obtain $\\nabla E({\\pmb x}),\\nabla S({\\pmb x})$   \n8: Form $A(\\pmb{x})\\stackrel{!}{=}A_{\\mathrm{tri}}(\\pmb{x})-A_{\\mathrm{tri}}(\\pmb{x})^{\\top}$ and $D(x)=K_{\\mathrm{chol}}(x)K_{\\mathrm{chol}}({\\pmb x})^{\\intercal}$   \n9: Build ${\\pmb L}({\\pmb x}),M({\\pmb x})$ according to Lemma 3.2   \n10: Evaluate $\\dot{\\pmb{x}}=L(\\pmb{x})\\nabla E(\\pmb{x})\\bar{+}M(\\pmb{x})\\nabla S(\\pmb{x})$   \n11: Randomly draw $n_{1},...,n_{l}$ and form $t_{j}=t+n_{j}\\Delta t$ for all $j$   \n12: $\\tilde{\\mathbf{x}}_{1},...,\\tilde{\\mathbf{x}}_{l}=\\mathrm{ODEsolve}(\\dot{\\mathbf{x}},t_{1},...,t_{l})$   \n13: L += l\u22121  j Loss(xj, x\u02dcj)   \n14: end for   \n15: Rescale $L=|\\boldsymbol{P}|^{-1}L$   \n16: Update $A_{\\mathrm{tri}},B,K_{\\mathrm{chol}},E,S$ through gradient descent on $L$ .   \n17: end for   \n294 Note that the batch-wise training strategy in Algorithm 1 requires initial conditions for $\\pmb{x}^{u}$ in the   \n295 partially observed case. There are several options for this, and two specific strategies will be   \n296 considered here. Suppose the data are drawn from the training interval $[0,T]$ with initial state $\\scriptstyle x_{0}$   \n297 and final state ${\\mathbf{}}x_{T}$ . The first strategy sets $\\mathbf{\\boldsymbol{x}}_{0}^{u}=\\mathbf{0}$ , ${\\mathbfit{x}}_{T}^{u}={\\mathbf1}$ (where 1 is the all ones vector), and   \n298 $\\mathbf{\\Delta}\\mathbf{x}_{s}^{u}=\\mathbf{1}/T$ , $0<s<T$ , so that the unobserved states are initially assumed to lie on a straight line.   \n299 The second strategy is more sophisticated, and involves training a diffusion model to predict the   \n300 distribution of $\\pmb{x}^{u}$ given $x^{o}$ . Specific details of this procedure are given in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "301 5 Examples ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "302 The goal of the following experiments is to show that NMS is effective even when entropic information   \n303 cannot be observed during training, yielding superior performance when compared to previous   \n304 methods including GNODE, GFINN, and SPNN discussed in Section 2. The metrics considered   \n305 for this purpose will be mean absolute error (MAE) and mean squared error (MSE) defined in the   \n306 usual way as the average $\\ell^{1}$ (resp. squared $\\ell^{2}$ ) error between the discrete states $\\mathbf{\\Delta}\\mathbf{x},\\tilde{\\pmb{x}}\\in\\mathbb{R}^{n\\times n_{s}}$ . For   \n307 brevity, many implementation details have been omitted here and can be found in Appendix B. An   \n308 additional experiment showing the effectiveness of NMS in the presence of both full and partial state   \n309 information can be found in Appendix C.   \n310 Remark 5.1. To facilitate a more equal parameter count between the compared metriplectic meth  \n311 ods, the results of the experiments below were generated using the alternative parameterization   \n312 $D\\,=\\,K K^{\\intercal}$ where $K:\\dot{K}\\rightarrow\\mathbb{R}^{r\\times r^{\\prime}}$ is full and $r^{\\prime}\\geq r$ . Of course, this change does not affect   \n313 metriplecticity since $_{D}$ is still positive semi-definite for each $\\pmb{x}\\in K$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "314 5.1 Two gas containers ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "315 The first test of NMS involves two ideal gas containers separated by movable wall which is removed   \n316 at time $t_{0}$ , allowing for the exchange of heat and volume. In this example, the motion of the state   \n317 $\\pmb{x}=(q\\quad p\\quad S_{1}\\quad\\mathbf{\\bar{{S}}}_{2})^{\\sf T}$ is governed by the metriplectic equations: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\dot{q}}=\\frac{p}{m},}}\\\\ {{\\displaystyle{\\dot{S}_{1}=\\frac{9N^{2}k_{B}^{2}\\alpha}{4E_{1}({\\pmb x})}\\bigg(\\frac{1}{E_{1}({\\pmb x})}-\\frac{1}{E_{2}({\\pmb x})}\\bigg),\\qquad}}}\\end{array}\\qquad\\begin{array}{l}{{\\displaystyle{\\dot{p}}=\\frac{2}{3}\\bigg(\\frac{E_{1}({\\pmb x})}{q}-\\frac{E_{2}({\\pmb x})}{2L-q}\\bigg),}}\\\\ {{\\displaystyle{\\dot{S}_{2}=-\\frac{9N^{2}k_{B}^{2}\\alpha}{4E_{1}({\\pmb x})}\\bigg(\\frac{1}{E_{1}({\\pmb x})}-\\frac{1}{E_{2}({\\pmb x})}\\bigg),}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "318 where $(q,p)$ are the position and momentum of the separating wall, $S_{1},S_{2}$ are the entropies of the   \n319 two subsystems, and the internal energies $E_{1},E_{2}$ are determined from the Sackur-Tetrode equation   \n320 for ideal gases, $S_{i}/N k_{B}=\\ln\\Bigl(\\hat{c}V_{i}E_{i}^{3/2}\\Bigr),1\\leq i\\leq2$ . Here, $m$ denotes the mass of the wall, $2L$ is   \n321 the total length of the system, and $V_{i}$ is the volume of the $i^{\\mathrm{th}}$ container. As in [16, 25] $N k_{B}=1$ and   \n322 $\\alpha=0.5$ fix the characteristic macroscopic unit of entropy while $\\hat{c}=102.25$ ensures the argument of   \n323 the logarithm defining $E_{i}$ is dimensionless. This leads to the total entropy $S(\\pmb{x})=S_{1}+\\bar{S}_{2}$ and the   \n324 total energy $E({\\pmb x})=\\,\\!\\!{\\sqrt{(1/2m)p^{2}+E_{1}({\\pmb x})+E_{2}({\\pmb x})}}$ , which are guaranteed to be nondecreasing and   \n325 constant, respectively.   \n326 The primary goal here is to verify that NMS can accurately and stably predict gas container dynamics   \n327 without the need to observe the entropic variables $S_{1},S_{2}$ . To that end, NMS has been compared to   \n328 GNODE, SPNN, and GFINN on the task of predicting the trajectories of this metriplectic system   \n329 over time, with results displayed in Table 2. More precisely, given an intial condition $\\pmb{x}_{0}$ and an   \n330 interval $0<t_{\\mathrm{train}}<t_{\\mathrm{valid}}<t_{\\mathrm{test}}$ , each method is trained on partial state information (in the case of   \n331 NMS) or full state information (in the case of the others) from the interval $[0,t_{\\mathrm{train}}]$ and validated on   \n332 $(t_{\\mathrm{train}},t_{\\mathrm{valid}}]$ before state errors in $q,p$ only are calculated on the whole interval $[0,t_{\\mathrm{test}}]$ . As can be   \n333 seen from Table 2 and Figure 2, NMS is remarkably accurate over unseen timescales even in this   \n334 unfair comparison, avoiding the unphysical behavior which often hinders soft-constrained methods   \n335 like SPNN. The energy and instantaneous entropy plots in Figure 2 further confirm that the strong   \n336 enforcement of metriplectic structure guaranteed by NMS leads to correct energetic and entropic   \n337 dynamics for all time. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "338 5.2 Thermoelastic double pendulum ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "339 Next, consider the thermoelastic double pendulum from [26] with 10-dimensional state variable $\\textbf{\\em x}=$   \n340 $(\\pmb{q}_{1}\\quad\\pmb{q}_{2}\\quad\\pmb{p}_{1}\\quad\\pmb{p}_{2}\\quad\\mathcal{S}_{1}\\quad\\mathcal{S}_{2})^{\\intercal}$ , which represents a highly challenging benchmark for metriplectic   \n341 methods. The equations of motion in this case are given for $1\\leq i\\leq2$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\dot{q}_{i}=\\frac{p_{i}}{m_{i}},\\quad\\dot{p}_{i}=-\\partial_{q_{i}}\\bigl(E_{1}(x)+E_{2}(x)\\bigr),\\quad\\dot{S}_{1}=\\kappa\\bigl(T_{1}^{-1}T_{2}-1\\bigr),\\quad\\dot{S}_{2}=\\kappa\\bigl(T_{1}T_{2}^{-1}-1\\bigr),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "342 where $\\kappa>0$ is a thermal conductivity constant (set to 1), $m_{i}$ is the mass of the $i^{\\mathrm{th}}$ spring (also set to   \n343 1) and $T_{i}=\\partial_{S_{i}}E_{i}$ is its absolute temperature. In this case, $\\pmb{q}_{i},\\pmb{p}_{i}\\in\\mathbb{R}^{2}$ represent the position and   \n344 momentum of the $i^{\\mathrm{th}}$ mass, while $S_{i}$ represents the entropy of the $i^{\\mathrm{th}}$ pendulum. As before, the total   \n345 entropy $S(\\pmb{x})=S_{1}+S_{2}$ is the sum of the entropies of the two springs, while defining the internal   \n346 energies $E_{i}({\\pmb x})=(1/2)(\\ln{\\lambda_{i}})^{2}+\\ln{\\lambda_{i}}+e^{S_{i}-\\ln{\\lambda_{i}}}-1,\\lambda_{1}=|\\pmb q_{i}|,\\lambda_{2}=|\\pmb q_{2}-\\pmb q_{1}|.$ , leads to the total   \n347 energy $E({\\pmb x})=(1/2m_{1})|{\\pmb p}_{1}|^{2}+(1/2m_{2})|{\\pmb p}_{2}|^{2}+E_{1}({\\pmb x})+E_{2}({\\pmb x}).$   \n348 The task in this case is prediction across initial conditions. As in [18], 100 trajectories are drawn from   \n349 the ranges in Appendix $\\mathbf{B}$ and integrated over the interval $[0,40]$ with $\\Delta t=0.1$ , with an 80/10/10   \n350 split for training/validation/testing. Here all compared models are trained using full state information.   \n351 As seen in Table 2, NMS is again the most performant, although all models struggle to approximate   \n352 the dynamics over the entire training interval. It is also notable that the training time of NMS is greatly   \n353 decreased relative to GNODE and GFINN due to its improved quadratic scaling; a representative   \n354 study to this effect is given in Appendix D. ", "page_idx": 7}, {"type": "image", "img_path": "3mzFmBPFIX/tmp/b7ffe34feb6467d78a66ad87a81916bf058867cae48fa076cb57181c8ef4866e.jpg", "img_caption": ["Figure 2: The ground-truth and predicted position, momentum, instantaneous entropy, and energies for the two gas containers example in the training (white), validation (yellow), and testing (red) regimes. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "3mzFmBPFIX/tmp/223934972fd81f3cf87b82fe7bd685d7cda1ec02f59944f68303186257b5ef3c.jpg", "table_caption": ["Table 2: Prediction errors for $x^{o}$ measured in MSE and MAE on the interval $[0,t_{\\mathrm{test}}]$ in the two gas containers example (left) and on the test set in the thermoelastic double pendulum example (right). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "355 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "356 Neural metriplectic systems (NMS) have been considered for learning finite-dimensional metriplectic   \n357 dynamics from data. Making use of novel non-redundant parameterizations for metriplectic operators,   \n358 NMS provably approximates arbitrary nondegenerate metriplectic systems with generalization error   \n359 bounded in terms of the operator approximation quality. Benchmark examples have shown that   \n360 NMS is both more scalable and more accurate than previous methods, including when only partial   \n361 state information is observed. Future work will consider extensions of NMS to infinite-dimensional   \n362 metriplectic systems with the aim of addressing its main limitation: the difficulty of scaling NMS   \n363 (among all present methods for metriplectic learning) to realistic, 3-D problems of the size that would   \n364 be considered in practice. A promising direction is to consider the use of NMS in model reduction,   \n365 where sparse, large-scale systems are converted to small, dense systems through a clever choice of   \n366 encoding/decoding. ", "page_idx": 8}, {"type": "text", "text": "367 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "368 [1] Philip J. Morrison. A paradigm for joined hamiltonian and dissipative systems. Physica D: Nonlinear   \n369 Phenomena, 18(1):410\u2013419, 1986.   \n370 [2] Miroslav Grmela and Hans Christian \u00d6ttinger. Dynamics and thermodynamics of complex fluids. i.   \n371 development of a general formalism. Phys. Rev. E, 56:6620\u20136632, Dec 1997.   \n372 [3] P. J. Morrison. Some observations regarding brackets and dissipation. Technical Report PAM-228, Center   \n373 for Pure and Applied Mathematics, University of California, Berkeley, 1984.   \n374 [4] PJ Morrison. Thoughts on brackets and dissipation: old and new. In Journal of Physics: Conference Series,   \n375 volume 169, page 012006. IOP Publishing, 2009.   \n376 [5] Allan N. Kaufman and Philip J. Morrison. Algebraic structure of the plasma quasilinear equations. Physics   \n377 Letters A, 88(8):405\u2013406, 1982.   \n378 [6] Emmanuele Materassi, M.; Tassi. Metriplectic framework for dissipative magneto-hydrodynamics. Physica   \n379 D: Nonlinear Phenomena, 2012.   \n380 [7] Allan N. Kaufman. Dissipative hamiltonian systems: A unifying principle. Physics Letters A, 100(8):419\u2013   \n381 422, 1984.   \n382 [8] Darryl D Holm, Vakhtang Putkaradze, and Cesare Tronci. Kinetic models of oriented self-assembly.   \n383 Journal of Physics A: Mathematical and Theoretical, 41(34):344010, aug 2008.   \n384 [9] Quercus Hern\u00e1ndez, Alberto Bad\u00edas, David Gonz\u00e1lez, Francisco Chinesta, and El\u00edas Cueto. Structure  \n385 preserving neural networks. Journal of Computational Physics, 426:109950, 2021.   \n386 [10] Quercus Hern\u00e1ndez, Alberto Bad\u00edas, David Gonz\u00e1lez, Francisco Chinesta, and El\u00edas Cueto. Deep learning   \n387 of thermodynamics-aware reduced-order models from data. Computer Methods in Applied Mechanics and   \n388 Engineering, 379:113763, 2021.   \n389 [11] David Gonz\u00e1lez, Francisco Chinesta, and El\u00edas Cueto. Thermodynamically consistent data-driven compu  \n390 tational mechanics. Continuum Mechanics and Thermodynamics, 31(1):239\u2013253, 2019.   \n391 [12] D. Ruiz, D. Portillo, and I. Romero. A data-driven method for dissipative thermomechanics. IFAC  \n392 PapersOnLine, 54(19):315\u2013320, 2021.   \n393 [13] Baige Xu, Yuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Learning generic systems using neural   \n394 symplectic forms. In International Symposium on Nonlinear Theory and Its Applications, number A2L-D  \n395 03 in IEICE Proceeding Series, pages 29\u201332. The Institute of Electronics, Information, and Communication   \n396 Engineers (IEICE), 2022.   \n397 [14] Baige Xu, Yuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Equivalence class learning for   \n398 GENERIC systems. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems,   \n399 2023.   \n400 [15] Anthony Gruber, Kookjin Lee, and Nathaniel Trask. Reversible and irreversible bracket-based dynamics   \n401 for deep graph neural networks. In Thirty-seventh Conference on Neural Information Processing Systems,   \n402 2023.   \n403 [16] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Machine learning structure preserving brackets for   \n404 forecasting irreversible processes. Advances in Neural Information Processing Systems, 34:5696\u20135707,   \n405 2021.   \n406 [17] Hans Christian \u00d6ttinger. Irreversible dynamics, onsager-casimir symmetry, and an application to turbulence.   \n407 Phys. Rev. E, 90:042121, Oct 2014.   \n408 [18] Zhen Zhang, Yeonjong Shin, and George Em Karniadakis. Gfinns: Generic formalism informed neural   \n409 networks for deterministic and stochastic dynamical systems. Philosophical Transactions of the Royal   \n410 Society A: Mathematical, Physical and Engineering Sciences, 380(2229):20210207, 2022.   \n411 [19] Hans Christian \u00d6ttinger. Preservation of thermodynamic structure in model reduction. Phys. Rev. E,   \n412 91:032147, Mar 2015.   \n413 [20] Anthony Gruber, Max Gunzburger, Lili Ju, and Zhu Wang. Energetically consistent model reduction for   \n414 metriplectic systems. Computer Methods in Applied Mechanics and Engineering, 404:115709, 2023.   \n415 [21] Loring W. Tu. Differential Geometry: Connections, Curvature, and Characteristic Classes. Springer   \n416 International Publishing, 2017.   \n417 [22] Leo Dorst, Daniel Fontijne, and Stephen Mann. Geometric Algebra for Computer Science: An Object  \n418 oriented Approach to Geometry. Morgan Kaufmann, Amsterdam, 2007.   \n419 [23] Ge Zhong and Jerrold E. Marsden. Lie-poisson hamilton-jacobi theory and lie-poisson integrators. Physics   \n420 Letters A, 133(3):134\u2013139, 1988.   \n421 [24] Xin Li. Simultaneous approximations of multivariate functions and their derivatives by neural networks   \n422 with one hidden layer. Neurocomputing, 12(4):327\u2013343, 1996.   \n423 [25] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Structure-preserving sparse identification of nonlinear   \n424 dynamics for data-driven modeling. In Mathematical and Scientific Machine Learning, pages 65\u201380.   \n425 PMLR, 2022.   \n426 [26] Ignacio Romero. Thermodynamically consistent time-stepping algorithms for non-linear thermomechanical   \n427 systems. International Journal for Numerical Methods in Engineering, 79(6):706\u2013732, 2023/05/14 2009.   \n428 [27] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational   \n429 and applied mathematics, 6(1):19\u201326, 1980.   \n430 [28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint   \n431 arXiv:1412.6980, 2014.   \n432 [29] Xiaocheng Shang and Hans Christian \u00d6ttinger. Structure-preserving integrators for dissipative systems   \n433 based on reversible\u2013irreversible splitting. Proceedings of the Royal Society A, 476(2234):20190446, 2020.   \n434 [30] Haksoo Lim, Minjung Kim, Sewon Park, and Noseong Park. Regular time-series generation using sgm.   \n435 arXiv preprint arXiv:2301.08518, 2023.   \n436 [31] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation,   \n437 23(7):1661\u20131674, 2011.   \n438 [32] Simo S\u00e4rkk\u00e4 and Arno Solin, editors. Applied stochastic differential equations, volume 10. Cambridge   \n439 University Press, 2019.   \n440 [33] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.   \n441 Score-based generative modeling through stochastic differential equations. CoRR, abs/2011.13456, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "442 A Proof of Theoretical Results ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "443 This Appendix provides proof of the analytical results in Section 3 of the body. First, the parameteri  \n444 zations of $L,M$ in terms of exterior algebra are established.   \n445 Proof of Lemma 3.2. First, it is necessary to check that the operators $L,M$ parameterized this way   \n446 satisfy the symmetries and degeneracy conditions claimed in the statement. To that end, recall that   \n447 $\\mathbf{a}\\wedge b\\simeq a b^{\\intercal}-b a^{\\intercal}$ , meaning that $(a b^{\\intercal}-b a^{\\intercal})^{\\intercal}\\simeq b\\wedge a=-a\\wedge b$ . It follows that $A^{\\mathsf{T}}\\simeq{\\tilde{\\mathsf{A}}}=-\\mathsf{A}$   \n448 where $\\tilde{\\mathsf{A}}$ denotes the reversion of A, i.e., $\\begin{array}{r}{\\tilde{\\mathsf{A}}=\\sum_{i<j}A^{i j}\\pmb{e}_{j}\\wedge\\pmb{e}_{i}}\\end{array}$ . Therefore, we may write ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n{\\cal{L}}^{\\top}\\simeq\\tilde{\\mathsf{A}}-\\frac{1}{\\left|\\nabla S\\right|^{2}}\\overline{{{\\cal{A}}\\nabla S\\wedge\\nabla S}}=-\\mathsf{A}+\\frac{1}{\\left|\\nabla S\\right|^{2}}{\\cal{A}}\\nabla S\\wedge\\nabla S\\simeq-{\\cal{L}},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "449 showing that $L^{\\top}=-L$ . Moreover, using that ", "page_idx": 11}, {"type": "equation", "text": "$$\n(b\\wedge c)\\cdot a=-a\\cdot(b\\wedge c)=(a\\cdot c)b-(a\\cdot b)c,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "450 it follows that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\pmb{L}\\nabla S=\\mathbb{A}\\cdot\\nabla S-\\frac{1}{\\left|\\nabla S\\right|^{2}}(\\pmb{A}\\nabla S\\wedge\\nabla S)\\cdot\\nabla S=\\pmb{A}\\nabla S-\\pmb{A}\\nabla S=\\mathbf{0},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "451 since $\\nabla S\\cdot\\pmb{A}\\nabla S=-\\nabla S\\cdot\\pmb{A}\\nabla S=0$ . Moving to the case of $_M$ , notice that $M=D_{s t}\\mathbf{v}^{s}\\otimes\\mathbf{v}^{t}$ for   \n452 a particular choice of $\\pmb{v}$ , meaning that ", "page_idx": 11}, {"type": "equation", "text": "$$\nM^{\\boldsymbol{\\mathsf{T}}}=\\sum_{s,t}D_{s t}\\big(\\boldsymbol{v}^{s}\\otimes\\boldsymbol{v}^{t}\\big)^{\\boldsymbol{\\mathsf{T}}}=\\sum_{s,t}D_{s t}\\boldsymbol{v}^{t}\\otimes\\boldsymbol{v}^{s}=\\sum_{t,s}D_{t s}\\boldsymbol{v}^{s}\\otimes\\boldsymbol{v}^{t}=\\sum_{s,t}D_{s t}\\boldsymbol{v}^{s}\\otimes\\boldsymbol{v}^{t}=M,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "453 since $_{D}$ is a symmetric matrix. Additionally, it is straightforward to check that, for any $1\\le s\\le r$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\boldsymbol{v}^{s}\\cdot\\nabla E=\\left(b^{s}-\\frac{b^{s}\\cdot\\nabla E}{\\left|\\nabla E\\right|^{2}}\\nabla E\\right)\\cdot\\nabla E=\\boldsymbol{b}^{s}\\cdot\\nabla E-\\boldsymbol{b}^{s}\\cdot\\nabla E=0.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "454 So, it follows immediately that ", "page_idx": 11}, {"type": "equation", "text": "$$\nM\\nabla E=\\sum_{s,t}D_{s t}\\big(\\pmb{v}^{s}\\otimes\\pmb{v}^{t}\\big)\\cdot\\nabla E=\\sum_{s,t}D_{s t}\\big(\\pmb{v}^{t}\\cdot\\nabla E\\big)\\pmb{v}^{s}=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "455 Now, observe that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L=A-\\cfrac{1}{\\left|\\nabla S\\right|^{2}}(A\\nabla S(\\nabla S)^{\\mathsf{T}}-\\nabla S(A\\nabla S)^{\\mathsf{T}})}\\\\ &{\\quad=A-\\cfrac{1}{\\left|\\nabla S\\right|^{2}}(A\\nabla S(\\nabla S^{\\mathsf{T}})+\\nabla S(\\nabla S)^{\\mathsf{T}}A)}\\\\ &{\\quad=\\Bigg(I-\\cfrac{\\nabla S(\\nabla S)^{\\mathsf{T}}}{\\left|\\nabla S\\right|^{2}}\\Bigg)A\\Bigg(I-\\cfrac{\\nabla S(\\nabla S)^{\\mathsf{T}}}{\\left|\\nabla S\\right|^{2}}\\Bigg)=P_{S}^{\\perp}A P_{S}^{\\perp},}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "456 since $A^{\\mathsf{T}}=-A$ and hence $\\pmb{v}^{\\top}\\pmb{A}\\pmb{v}=0$ for all $\\pmb{v}\\in\\mathbb{R}^{n}$ . Similarly, it follows that for every $1\\le s\\le r$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\boldsymbol{P}_{E}^{\\perp}\\boldsymbol{b}^{s}=\\boldsymbol{b}^{s}-\\frac{\\boldsymbol{b}^{s}\\cdot\\nabla E}{\\left|\\nabla E\\right|^{2}}\\nabla E,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "457 and therefore $_M$ is expressible as ", "page_idx": 11}, {"type": "equation", "text": "$$\nM=\\sum_{s,t}D_{s t}\\big(P_{E}^{\\perp}b^{s}\\big)\\big(P_{E}^{\\perp}b^{t}\\big)^{\\intercal}=P_{E}^{\\perp}B D B^{\\intercal}P_{E}^{\\perp}.\\,\\perp\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "458 With Lemma 3.2 established, the proof of Theorem 3.4 is straightforward. ", "page_idx": 11}, {"type": "text", "text": "459 Proof of Theorem 3.4. The \u201cif\u201d direction follows immediately from Lemma 3.2. Now, suppose that   \n460 $\\textbf{\\emph{L}}$ and $_M$ define a metriplectic system, meaning that the mentioned symmetries and degeneracy   \n461 conditions hold. Then, it follows from $\\mathbf{\\Delta}L\\nabla S\\,=\\,\\mathbf{0}$ that the projection $P_{S}^{\\perp}L P_{S}^{\\perp}\\,=\\,L$ leaves $\\textbf{\\emph{L}}$   \n462 invariant, so that choosing $\\boldsymbol{A}=\\boldsymbol{L}$ yields $P_{S}^{\\perp}A P_{S}^{\\perp}=L$ . Similarly, from positive semi-definiteness   \n463 and $M\\nabla E\\,=\\,\\mathbf{0}$ it follows that $M=U\\Lambda U^{\\intercal}=P_{E}^{\\perp}U\\Lambda U^{\\intercal}P_{E}^{\\perp}$ for some column-orthonormal   \n464 $\\boldsymbol{U}\\,\\in\\,\\mathbb{R}^{N\\times{r}}$ and positive diagonal $\\mathbf{A}\\,\\in\\,\\mathbb{R}^{r\\times r}$ . Therefore, choosing $B\\,=\\,U$ and $\\pmb{D}=\\pmb{\\Lambda}$ yields   \n465 $M=P_{E}^{\\perp}B D B^{\\dagger}P^{\\perp}$ , as desired. \u53e3   \n466 Looking toward the proof of Proposition 3.7, we also need to establish the following Lemmata which   \n467 give control over the orthogonal projectors $P_{\\tilde{E}}^{\\perp},P_{\\tilde{S}}^{\\perp}$ . First, we recall how control over the $L^{\\infty}$ norm   \n468 |\u00b7 $\\mid_{\\infty}$ of a matrix field gives control over its spectral norm $|\\cdot|$ .   \n469 Lemma A.1. Let $A:K\\to\\mathbb{R}^{n\\times n}$ be a matrix field defined on the compact set $K\\subset\\mathbb{R}^{n}$ with $m$   \n470 continuous derivatives. Then, for any $\\varepsilon>0$ there exists a two-layer neural network $\\tilde{A}:K\\to\\mathbb{R}^{n\\times n}$   \n471 such that $\\operatorname*{sup}_{\\pmb{x}\\in K}\\Bigl|\\pmb{A}-\\tilde{\\pmb{A}}\\Bigr|<\\varepsilon$ and $\\operatorname*{sup}_{\\mathbf{x}\\in K}\\Bigl|\\nabla^{k}A-\\nabla^{k}\\tilde{A}\\Bigr|_{\\infty}<\\varepsilon$ for $1\\leq k\\leq m$ where $\\nabla^{k}$ is the   \n472 (total) derivative operator of order $k$ .   \n473 Proof. This will be a direct consequence of Corollary 2.2 in [24] provided we show that $|{\\cal A}|\\leq c|{\\cal A}|_{\\infty}$   \n474 for some $c>0$ . To that end, if $\\sigma_{1}\\geq...\\geq\\sigma_{r}>0$ $r\\leq n)$ denote the nonzero singular values of   \n475 $A-{\\tilde{A}}$ , it follows that for each $\\pmb{x}\\in K$ , ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Big|A-\\tilde{A}\\Big|=\\sigma_{1}\\le\\sqrt{\\sigma_{1}^{2}+...+\\sigma_{r}^{2}}=\\sqrt{\\sum_{i,j}\\Big|A_{i j}-\\tilde{A}_{i j}\\Big|^{2}}=\\Big|A-\\tilde{A}\\Big|_{F}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "476 On the other hand, it also follows that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A-\\tilde{A}\\Big|_{F}=\\sqrt{\\underset{i,j}{\\sum_{i,j}}\\Big|A_{i j}-\\tilde{A}_{i j}\\Big|^{2}}\\le\\sqrt{\\underset{i,j}{\\sum_{i,j}}\\underset{i,j}{\\operatorname*{max}}\\Big|A_{i j}-\\tilde{A}_{i j}\\Big|}=n\\sqrt{\\underset{i,j}{\\operatorname*{max}}\\Big|A_{i j}-\\tilde{A}_{i j}\\Big|}=n\\Big|A-\\tilde{A}\\Big|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "477 and therefore the desired inequality holds with $c=n$ . Now, for any $\\varepsilon>0$ it follows from [24] that   \n478 there exists a two layer network $\\tilde{A}$ with $m$ continuous derivatives such that $\\begin{array}{r}{\\operatorname*{sup}_{\\mathbf{x}\\in K}\\Bigl|\\mathbf{A}-\\tilde{A}\\Bigr|_{\\infty}<\\varepsilon/n}\\end{array}$   \n479 and $\\begin{array}{r}{\\operatorname*{sup}_{{\\pmb x}\\in{\\cal K}}\\mathopen{}\\mathclose\\bgroup\\left|\\nabla^{k}{\\pmb A}-\\nabla^{k}\\tilde{\\cal A}\\aftergroup\\egroup\\right|_{\\infty}<\\varepsilon/n<\\varepsilon}\\end{array}$ for all $1\\leq k\\leq m$ . Therefore, it follows that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{x}\\in K}\\Bigl|A-\\tilde{A}\\Bigr|\\le n\\operatorname*{sup}_{\\mathbf{x}\\in K}\\Bigl|A-\\tilde{A}\\Bigr|_{\\infty}<n\\frac{\\varepsilon}{n}=\\varepsilon,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "480 completing the argument. ", "page_idx": 12}, {"type": "text", "text": "481 Next, we bound the deviation in the orthogonal projectors $P_{\\tilde{E}}^{\\perp},P_{\\tilde{S}}^{\\perp}$ ", "page_idx": 12}, {"type": "text", "text": "482 Lemma A.2. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be such that $\\nabla f\\neq\\mathbf{0}$ on the compact set $K\\subset\\mathbb{R}^{n}$ . For any $\\varepsilon>0$ ,   \n483 there exists a two-layer neural network $\\tilde{f}:K\\to\\mathbb{R}$ such that $\\nabla\\tilde{f}\\neq\\mathbf{0}$ on $K$ , $\\operatorname*{sup}_{\\pmb{x}\\in K}\\left|f-\\tilde{f}\\right|<$   \n484 $\\varepsilon,\\operatorname*{sup}_{\\pmb{x}\\in K}\\left|\\nabla f-\\nabla\\widetilde{f}\\right|<\\varepsilon,$ , and $\\begin{array}{r}{\\operatorname*{sup}_{\\mathbf{x}\\in K}\\left|P_{f}^{\\perp}-P_{\\tilde{f}}^{\\perp}\\right|<\\varepsilon.}\\end{array}$ .   \n485 Proof. Denote $\\nabla f=\\pmb{v}$ and consider any $\\tilde{v}:K\\rightarrow\\mathbb{R}$ . Since $|\\boldsymbol{v}|\\leq|\\widetilde{\\boldsymbol{v}}|+|\\boldsymbol{v}-\\widetilde{\\boldsymbol{v}}|$ , it follows for all   \n486 $\\pmb{x}\\in K$ that whenever $|v-\\tilde{v}|<(1/2)\\operatorname*{inf}_{x\\in K}|v|$ , ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\tilde{\\boldsymbol{v}}|\\geq|\\boldsymbol{v}|-|\\boldsymbol{v}-\\tilde{\\boldsymbol{v}}|>|\\boldsymbol{v}|-\\frac12\\operatorname*{inf}_{\\boldsymbol{x}\\in K}|\\boldsymbol{v}|>0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 so that $\\tilde{\\pmb{v}}\\neq0$ in $K$ , and since the square function is monotonic, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\boldsymbol{x}\\in K}\\lvert\\tilde{\\boldsymbol{v}}\\rvert^{2}\\geq\\operatorname*{inf}_{\\boldsymbol{x}\\in K}\\biggl(\\lvert\\boldsymbol{v}\\rvert-\\frac{1}{2}\\operatorname*{inf}_{\\boldsymbol{x}\\in K}\\lvert\\boldsymbol{v}\\rvert\\biggr)^{2}=\\frac{1}{4}\\operatorname*{inf}_{\\boldsymbol{x}\\in K}\\lvert\\boldsymbol{v}\\rvert^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "488 On the other hand, we also have $|\\tilde{{\\boldsymbol{v}}}|\\le|{\\boldsymbol{v}}|+|\\tilde{{\\boldsymbol{v}}}-{\\boldsymbol{v}}|<|{\\boldsymbol{v}}|+(1/2)\\operatorname*{inf}_{{\\boldsymbol{x}}\\in K}|{\\boldsymbol{v}}|$ , so that, adding and   \n489 subtracting $\\tilde{\\pmb{v}}\\pmb{v}^{\\top}$ and applying Cauchy-Schwarz, it follows that for all $\\pmb{x}\\in K$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\lvert v v^{\\top}-\\tilde{v}\\tilde{v}^{\\top}\\rvert\\leq\\lvert v-\\tilde{v}\\rvert\\lvert v\\rvert+\\lvert\\tilde{v}\\rvert\\lvert v-\\tilde{v}\\rvert\\leq2\\operatorname*{max}\\{|v|,\\lvert\\tilde{v}\\rvert\\}\\lvert v-\\tilde{v}\\rvert<\\bigg(2\\lvert v\\rvert+\\operatorname*{inf}_{x\\in K}\\lvert v\\rvert\\bigg)\\lvert v-\\tilde{v}\\rvert.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 Now, by Corollary 2.2 in [24], for any $\\varepsilon>0$ there exists a two-layer neural network $\\tilde{f}:K\\to\\mathbb{R}$ such   \n491 that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{x\\in K}}\\Bigl|v-\\nabla\\tilde{f}\\Bigr|<\\operatorname*{min}\\Biggl\\{\\frac{1}{2}\\operatorname*{inf}_{x\\in K}\\bigl|v\\bigr|,\\frac{\\operatorname*{inf}_{x\\in K}\\bigl|v\\bigr|^{2}}{2\\operatorname*{sup}_{x\\in K}\\bigl|v\\bigr|+\\operatorname*{inf}_{x\\in K}\\bigl|v\\bigr|}\\frac{\\varepsilon}{4},\\varepsilon\\Biggr\\}\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "492 and also $\\operatorname*{sup}_{\\pmb{x}\\in K}\\left|f-\\tilde{f}\\right|<\\varepsilon$ . Letting $\\tilde{\\pmb{v}}=\\nabla\\tilde{f}$ , it follows that for all $\\pmb{x}\\in K$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|P_{f}^{\\perp}-P_{\\bar{f}}^{\\perp}\\right|=\\left|\\frac{v v^{\\top}}{\\left|v\\right|^{2}}-\\frac{\\tilde{v}\\tilde{v}^{\\top}}{\\left|\\tilde{v}\\right|^{2}}\\right|\\leq\\frac{|v v^{\\top}-\\tilde{v}\\tilde{v}^{\\top}|}{\\operatorname*{min}\\left\\{|v|^{2},|\\tilde{v}|^{2}\\right\\}}\\leq\\frac{2|v|+\\operatorname*{inf}_{x\\in K}\\lvert v\\rvert}{\\operatorname*{min}\\left\\{|v|^{2},|\\tilde{v}|^{2}\\right\\}}|v-\\tilde{v}|,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "493 and therefore, taking the supremum of both sides and applying the previous work yields the desired   \n494 estimate, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in K}\\Bigl|P_{f}^{\\perp}-P_{\\tilde{f}}^{\\perp}\\Bigr|\\leq4\\frac{2\\operatorname*{sup}_{x\\in K}\\lvert v\\rvert+\\operatorname*{inf}_{x\\in K}\\lvert v\\rvert}{\\operatorname*{inf}_{x\\in K}\\lvert v\\rvert^{2}}\\operatorname*{sup}_{x\\in K}\\lvert v-\\tilde{v}\\rvert<\\varepsilon.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "495 With these intermediate results established, the proof of the approximation result Proposition 3.7   \n496 proceeds as follows.   \n497 Proof of Proposition 3.7. Recall from Theorem 3.4 that we can write $L\\,=\\,P_{S}^{\\perp}(A_{\\mathrm{tri}}-A_{\\mathrm{tri}}^{\\top})P_{S}^{\\perp}$   \n498 and similarly for $\\tilde{L}$ . Notice that, by adding and subtracting $P_{\\tilde{S}}^{\\perp}A_{\\mathrm{tri}}P_{S}^{\\perp}$ and $P_{\\tilde{S}}^{\\perp}\\tilde{A}_{\\mathrm{tri}}P_{S}^{\\perp}$ , it follows   \n499 that for all $\\pmb{x}\\in K$ , ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|P_{S}^{\\perp}A_{\\mathrm{tri}}P_{S}^{\\perp}-P_{\\bar{S}}^{\\perp}\\tilde{A}_{\\mathrm{tri}}P_{\\bar{S}}^{\\perp}\\right|}\\\\ &{\\qquad=\\left|\\big(P_{S}^{\\perp}-P_{\\bar{S}}^{\\perp}\\big)A_{\\mathrm{tri}}P_{S}^{\\perp}+P_{\\bar{S}}^{\\perp}\\Big(A_{\\mathrm{tri}}-\\tilde{A}_{\\mathrm{tri}}\\Big)P_{S}^{\\perp}+P_{\\bar{S}}^{\\perp}\\tilde{A}_{\\mathrm{tri}}\\big(P_{S}^{\\perp}-P_{\\bar{S}}^{\\perp}\\big)\\right|}\\\\ &{\\qquad\\leq\\left|P_{S}^{\\perp}-P_{\\bar{S}}^{\\perp}\\right|\\left|A_{\\mathrm{tri}}\\right|+\\left|A_{\\mathrm{tri}}-\\tilde{A}_{\\mathrm{tri}}\\right|+\\left|\\tilde{A}_{\\mathrm{tri}}\\right|\\left|P_{S}^{\\perp}-P_{\\bar{S}}^{\\perp}\\right|}\\\\ &{\\qquad\\leq2\\operatorname*{max}\\biggr\\{\\left|A_{\\mathrm{tri}}\\right|,\\left|\\tilde{A}_{\\mathrm{tri}}\\right|\\biggr\\}\\left|P_{S}^{\\perp}-P_{\\bar{S}}^{\\perp}\\right|+\\left|A_{\\mathrm{tri}}-\\tilde{A}_{\\mathrm{tri}}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "500 where we have used that $P_{S}^{\\perp},P_{\\tilde{S}}^{\\perp}$ have unit spectral norm. By Lemma A.1, for any $\\varepsilon>0$ there exists   \n501 a two layer neural network $\\tilde{A}_{\\mathrm{tri}}$ such that $\\begin{array}{r}{\\operatorname*{sup}_{\\mathbf{x}\\in K}\\Bigl|A_{\\mathrm{tri}}-\\tilde{A}_{\\mathrm{tri}}\\Bigr|<\\frac{\\varepsilon}{4}}\\end{array}$ , and by Lemma A.2 there exists   \n502 a two-layer network $\\tilde{S}$ with $\\nabla\\tilde{S}\\neq\\mathbf{0}$ on $K$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in K}\\left|P_{S}^{\\perp}-P_{\\tilde{S}}^{\\perp}\\right|<\\operatorname*{min}\\Biggl\\{\\varepsilon,\\operatorname*{max}\\biggl\\{\\operatorname*{sup}_{x\\in K}\\left|A_{\\mathrm{tri}}\\right|,\\operatorname*{sup}_{x\\in K}\\biggl|\\tilde{A}_{\\mathrm{tri}}\\biggr|\\biggr\\}^{-1}\\frac{\\varepsilon}{8}\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "503 It follows that $\\tilde{S},\\nabla\\tilde{S}$ are $\\varepsilon$ -close to $S,\\nabla S$ on $K$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in K}\\Big(2\\operatorname*{max}\\Bigl\\{|A_{\\mathrm{tri}}|,\\Bigl|\\tilde{A}_{\\mathrm{tri}}\\Bigr|\\Bigr\\}\\big|P_{S}^{\\perp}-P_{\\tilde{S}}^{\\perp}\\big|\\Big)<\\frac{\\varepsilon}{4}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "504 Therefore, the estimate ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in K}\\Bigl|L-\\tilde{L}\\Bigr|\\le2\\operatorname*{sup}_{x\\in K}\\Bigl|P_{S}^{\\perp}A_{\\mathrm{tri}}P_{S}^{\\perp}-P_{\\tilde{S}}^{\\perp}\\tilde{A}_{\\mathrm{tri}}P_{\\tilde{S}}^{\\perp}\\Bigr|<2\\Bigl(\\frac{\\varepsilon}{4}+\\frac{\\varepsilon}{4}\\Bigr)=\\varepsilon,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "505 implies that $\\tilde{L}$ is $\\varepsilon$ -close to $\\textbf{\\emph{L}}$ on $K$ as well. ", "page_idx": 13}, {"type": "text", "text": "506 Moving to the case of $_M$ , we see that for all $\\pmb{x}\\in K$ , by writing $M=U\\Lambda U^{\\intercal}=K_{\\mathrm{chol}}K_{\\mathrm{chol}}^{\\intercal}$ for   \n507 $K_{\\mathrm{chol}}=U\\Lambda^{1/2}$ and repeating the first calculation with $K_{\\mathrm{chol}}$ in place of $A_{\\mathrm{tri}}$ and $P_{E}^{\\perp}$ in place of   \n508 $P_{S}^{\\perp}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|P_{E}^{\\perp}K_{\\mathrm{chol}}K_{\\mathrm{chol}}^{\\top}P_{E}^{\\perp}-P_{\\tilde{E}}^{\\perp}\\tilde{K}_{\\mathrm{chol}}\\tilde{K}_{\\mathrm{chol}}^{\\top}P_{\\tilde{E}}^{\\perp}\\Big|}\\\\ &{\\qquad\\le2\\operatorname*{max}\\Big\\{|K_{\\mathrm{chol}}|,\\Big|\\tilde{K}_{\\mathrm{chol}}\\Big|\\Big\\}\\big|P_{E}^{\\perp}-P_{\\tilde{E}}^{\\perp}\\big|+\\Big|K_{\\mathrm{chol}}K_{\\mathrm{chol}}^{\\top}-\\tilde{K}_{\\mathrm{chol}}\\tilde{K}_{\\mathrm{chol}}^{\\top}\\Big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "509 Moreover, if $\\left|K_{\\mathrm{chol}}-\\tilde{K}_{\\mathrm{chol}}\\right|<(1/2)\\operatorname*{inf}_{{\\pmb x}\\in K}|K_{\\mathrm{chol}}|$ for all $\\pmb{x}\\in K$ then similar arguments as used   \n510 in the proof of Lemma A.2 yield the following estimate for all $\\pmb{x}\\in K$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|K_{\\mathrm{chol}}K_{\\mathrm{chol}}^{\\intercal}-\\tilde{K}_{\\mathrm{chol}}\\tilde{K}_{\\mathrm{chol}}^{\\intercal}\\right|\\leq2\\operatorname*{max}\\Bigl\\{|K_{\\mathrm{chol}}|,\\left|\\tilde{K}_{\\mathrm{chol}}\\right|\\Bigr\\}\\Bigl|K_{\\mathrm{chol}}-\\tilde{K}_{\\mathrm{chol}}\\Bigr|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\biggl(2|K_{\\mathrm{chol}}|+\\underset{x\\in K}{\\operatorname*{inf}}|K_{\\mathrm{chol}}|\\biggr)\\left|K_{\\mathrm{chol}}-\\tilde{K}_{\\mathrm{chol}}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "511 As before, we now invoke Lemma A.1 to construct a two-layer lower-triangular network $\\tilde{K}_{\\mathrm{chol}}$ such   \n512 that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in K}\\Big|K_{\\mathrm{chol}}-\\tilde{K}_{\\mathrm{chol}}\\Big|<\\operatorname*{min}\\bigg\\{\\frac{1}{2}\\operatorname*{inf}_{x\\in K}|K_{\\mathrm{chol}}|,\\bigg(2\\operatorname*{sup}_{x\\in K}|K_{\\mathrm{chol}}|+\\operatorname*{inf}_{x\\in K}|K_{\\mathrm{chol}}|\\bigg)^{-1}\\frac{\\varepsilon}{2}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "513 as well as (using Lemma A.2) a network $\\tilde{E}$ satisfying $\\nabla\\tilde{E}\\neq\\mathbf{0}$ on $K$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in K}\\left|P_{E}^{\\perp}-P_{\\tilde{E}}^{\\perp}\\right|<\\operatorname*{min}\\Biggl\\{\\varepsilon,\\operatorname*{max}\\biggl\\{\\operatorname*{sup}_{x\\in K}\\left|K_{\\mathrm{chol}}\\right|,\\operatorname*{sup}_{x\\in K}\\Bigl|\\tilde{K}_{\\mathrm{chol}}\\Bigr|\\biggr\\}^{-1}\\frac{\\varepsilon}{4}\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "514 Again, it follows that $\\tilde{E},\\nabla\\tilde{E}$ are $\\varepsilon$ -close to $E,\\nabla E$ on $K$ , and by the work above we conclude ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in K}\\mathopen{}\\mathclose\\bgroup\\left|M-\\tilde{M}\\aftergroup\\egroup\\right|=\\operatorname*{sup}_{x\\in K}\\mathopen{}\\mathclose\\bgroup\\left|P_{E}^{\\perp}K_{\\mathrm{chol}}K_{\\mathrm{chol}}^{\\intercal}P_{E}^{\\perp}-P_{\\tilde{E}}^{\\perp}\\tilde{K}_{\\mathrm{chol}}\\tilde{K}_{\\mathrm{chol}}^{\\intercal}P_{\\tilde{E}}^{\\perp}\\aftergroup\\egroup\\right|<\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "515 as desired. ", "page_idx": 14}, {"type": "text", "text": "516 It is now possible to give a proof of the error bound in Theorem 3.9. Recall the $L^{2}([0,T])$ error   \n517 metric $\\lVert x\\rVert$ and Lipschitz constant $L_{f}$ , defined for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{n}$ and Lipschitz continuous functions   \n518 $f$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|x\\right\\|^{2}=\\int_{0}^{T}\\!|x|^{2}\\,d t,\\quad|f(x)-f(\\pmb{y})|\\leq L_{f}|\\pmb{x}-\\pmb{y}|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "519 Proof of Theorem 3.9. First, note that the assumption that one of $E,-S$ (without loss of generality,   \n520 say $E$ ) has bounded sublevel sets implies bounded trajectories for the state $\\textbf{\\em x}$ as in Remark 3.8,   \n521 so we may assume $\\textbf{\\em x}\\in~K$ for some compact $K\\,\\subset\\,\\mathbb{R}^{n}$ . Moreover, for any $\\varepsilon\\ >\\ 0$ it follows   \n522 from Proposition 3.7 that there are approximate networks $\\tilde{E},\\tilde{S}$ which are $\\varepsilon$ -close to $E,S$ on $K$ .   \n523 Additionally, it follows that $\\tilde{E},\\tilde{S}$ have nonzero gradients $\\nabla\\tilde{E},\\nabla\\tilde{S}$ which are also $\\varepsilon$ -close to the true   \n524 gradients $\\nabla E,\\nabla S$ on $K$ . This implies that for each $\\pmb{x}\\in K$ , $E=\\tilde{E}+(E-\\tilde{E})\\leq\\tilde{E}+\\varepsilon$ , so it   \n525 follows that the sublevel sets $\\{x\\,|\\,\\tilde{E}({\\pmb x})\\leq m\\}\\subseteq\\{{\\pmb x}\\,|\\,E({\\pmb x})\\leq m+\\varepsilon\\}$ are also bounded. Therefore,   \n526 we may assume (by potentially enlarging $K$ ) that both $\\mathbf{\\boldsymbol{x}},{\\tilde{\\mathbf{\\boldsymbol{x}}}}\\in K$ lie in the compact set $K$ for all time.   \n527 Now, let ${\\pmb y}={\\pmb x}-\\tilde{\\pmb x}$ . The next goal is to bound the following quantity: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\dot{y}|=\\bigg|L(x)\\nabla E(x)+M(x)\\nabla S(x)-\\tilde{L}(\\tilde{x})\\nabla\\tilde{E}(\\tilde{x})-\\tilde{M}(\\tilde{x})\\nabla\\tilde{S}(\\tilde{x})\\bigg|}\\\\ &{\\quad\\quad=\\bigg|\\Big(L(x)\\nabla E(x)-\\tilde{L}(\\tilde{x})\\nabla E(\\tilde{x})\\Big)+\\Big(M(x)\\nabla S(x)-\\tilde{M}(\\tilde{x})\\nabla S(\\tilde{x})\\Big)\\bigg|=:|\\dot{y}_{E}+\\dot{y}_{S}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "528 To that end, notice that by adding and subtracting $L(x)\\nabla E({\\tilde{x}}),{\\tilde{L}}(x)\\nabla E({\\tilde{x}}),{\\tilde{L}}({\\tilde{x}})\\nabla E({\\tilde{x}})$ , it fol  \n529 lows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{y}_{E}=L(\\pmb{x})(\\nabla E(\\pmb{x})-\\nabla E(\\tilde{\\pmb{x}}))+\\Big(L(\\pmb{x})-\\tilde{L}(\\pmb{x})\\Big)\\nabla E(\\tilde{\\pmb{x}})}\\\\ &{\\qquad\\quad+\\Big(\\tilde{L}(\\pmb{x})-\\tilde{L}(\\tilde{\\pmb{x}})\\Big)\\nabla E(\\tilde{\\pmb{x}})+\\tilde{L}(\\tilde{\\pmb{x}})\\Big(\\nabla E(\\pmb{x})-\\nabla\\tilde{E}(\\tilde{\\pmb{x}})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "530 By Proposition 3.7 there exists a two-layer neural network $\\tilde{L}$ with one continuous derivative such   \n531 that $\\bar{\\operatorname*{sup}}_{\\pmb{x}\\in K}\\Big|{\\pmb{L}}-\\tilde{\\pmb{L}}\\Big|\\,<\\,\\varepsilon$ , which implies that $\\tilde{L}$ is Lipschitz continuous with (uniformly well  \n532 approximated) Lipschitz constant. Using this fact along with the assumed Lipschitz continuity of   \n533 $\\nabla E$ and the approximation properties of the network $\\tilde{E}$ already constructed then yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lvert\\dot{y}_{E}\\rvert\\leq\\left(L_{\\nabla E}\\operatorname*{sup}_{x\\in K}\\lvert L\\rvert+L_{\\tilde{L}}\\operatorname*{sup}_{x\\in K}\\lvert\\nabla E\\rvert\\right)\\lvert y\\rvert+\\varepsilon\\bigg(\\operatorname*{sup}_{x\\in K}\\lvert\\tilde{L}\\rvert+\\operatorname*{sup}_{x\\in K}\\lvert\\nabla E\\rvert\\bigg)=:a_{E}\\lvert y\\rvert+\\varepsilon b_{E}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "534 Similarly, by adding and subtracting $M(\\pmb{x})\\nabla S(\\tilde{\\pmb{x}}),\\tilde{M}(\\pmb{x})\\nabla S(\\tilde{\\pmb{x}}),\\tilde{M}(\\tilde{\\pmb{x}})\\nabla S(\\tilde{\\pmb{x}})$ , it follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\pmb y}_{S}=M(\\pmb x)(\\nabla S(\\pmb x)-\\nabla S(\\tilde{\\pmb x}))+\\Big(M(\\pmb x)-\\tilde{M}(\\pmb x)\\Big)\\nabla S(\\tilde{\\pmb x})}\\\\ &{\\qquad\\quad+\\Big(\\tilde{M}(\\pmb x)-\\tilde{M}(\\tilde{\\pmb x})\\Big)\\nabla S(\\tilde{\\pmb x})+\\tilde{M}(\\tilde{\\pmb x})\\Big(\\nabla S(\\tilde{\\pmb x})-\\nabla\\tilde{S}(\\tilde{\\pmb x})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "535 By Proposition 3.7, there exists a two-layer network $\\tilde{M}$ with one continuous derivative such that   \n536 $\\mathrm{sup}_{{\\bf x}\\in{\\\\bar{\\cal K}}}\\Big|M-{\\tilde{M}}\\Big|<\\varepsilon.$ , with $\\tilde{M}$ Lipschitz continuous for the same reason as before. It follows from   \n537 this and supx\u2208K $\\operatorname*{sup}_{\\mathbf{x}\\in K}\\Bigl|\\dot{\\nabla}S-\\nabla\\tilde{S}\\Bigr|<\\varepsilon$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\dot{y}s|\\leq\\left(L_{\\nabla S}\\operatorname*{sup}_{x\\in K}|M|+L_{\\tilde{M}}\\operatorname*{sup}_{x\\in K}|\\nabla S|\\right)|y|+\\varepsilon\\bigg(\\operatorname*{sup}_{x\\in K}\\left|\\tilde{M}\\right|+\\operatorname*{sup}_{x\\in K}|\\nabla S|\\bigg)=:a_{S}|y|+\\varepsilon b_{S}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "538 Now, recall that $\\partial_{t}|\\pmb{y}|=|\\pmb{y}|^{-1}(\\dot{\\pmb{y}}\\cdot\\pmb{y})\\leq|\\dot{\\pmb{y}}|$ by Cauchy-Schwarz, and therefore the time derivative of   \n539 $|y|$ is bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{t}|y|\\leq|{\\dot{y}}_{E}|+|{\\dot{y}}_{S}|=(a_{E}+a_{S})|y|+\\varepsilon(b_{E}+b_{S})=:a|y|+b.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "540 This implies that $\\partial_{t}|{\\pmb y}|-a|{\\pmb y}|\\leq b$ , so multiplying by the integrating factor $e^{-a t}$ and integrating in   \n541 time yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\pmb{y}(t)|\\leq\\varepsilon b\\int_{0}^{t}e^{a(t-\\tau)}\\,d\\tau=\\varepsilon\\frac{b}{a}\\big(e^{a t}-1\\big),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "542 where we used that ${\\pmb y}(0)={\\bf0}$ since the initial condition of the trajectories is shared. Therefore, the   \n543 $L^{2}$ error in time can be approximated by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|y\\right\\|^{2}=\\int_{0}^{T}\\!|y|^{2}\\,d t\\leq\\varepsilon^{2}{\\frac{b^{2}}{a^{2}}}{\\big(}e^{2a T}-2e^{a T}+T+1{\\big)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "544 establishing the conclusion. ", "page_idx": 15}, {"type": "text", "text": "545 B Experimental and Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "546 This Appendix records additional details related to the numerical experiments in Section 5. For each   \n547 benchmark problem, a set of trajectories is manufactured given initial conditions by simulating ODEs   \n548 with known metriplectic structure. For the experiments in Table 2, only the observable variables   \n549 are used to construct datasets, since entropic information is assumed to be unknown. Algorithm 2   \n550 summarizes the training of the dynamics models used for comparison with NMS. ", "page_idx": 15}, {"type": "text", "text": "", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "3mzFmBPFIX/tmp/156dc56e4e399d11587b34a03b3ff725cd0630e3972d116aebde98d0bd56da36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "551 For each compared method, integrating the ODEs is done via the Dormand\u2013Prince method (do  \n552 pri5) [27] with relative tolerance $10^{-\\breve{7}}$ and absolute tolerance $10^{-9}$ . The loss is evaluated by   \n553 measuring the discrepancy between the ground truth observable states $x^{\\mathrm{0}}$ and the approximate observ  \n554 able states $\\tilde{\\pmb{x}}^{0}$ in the mean absolute error (MAE) metric. The model parameters $\\Theta$ (i.e., the weights   \n555 and biases) are updated by using Adamax [28] with an initial learning rate of 0.01. The number of   \n556 training steps is set as 30,000, and the model parameters resulting in the best performance for the   \n557 validation set are chosen for testing. Specific information related to the experiments in Section 5 is   \n558 given in the subsections below.   \n559 For generating the results reported in Table 2, we implemented the proposed algorithm in Python   \n560 3.9.12 and PyTorch 2.0.0. Other required information is provided with the accompanying code. All   \n561 experiments are conducted on Apple M2 Max chips with 96 GB memory. To provide the mean   \n562 and the standard deviation, experiments are repeated three times with varying random seeds for all   \n563 considered methods.   \n565 As mentioned in the body, the two gas container (TGC) problem tests models\u2019 predictive capability   \n566 (i.e., extrapolation in time). To this end, one simulated trajectory is obtained by solving an IVP with   \n567 a known TGC system and an initial condition, and the trajectory of the observable variables is split   \n568 into three subsequences, $[0,\\,t_{\\mathrm{train}}].$ , $(t_{\\mathrm{train}},t_{\\mathrm{val}}]$ , and $(t_{\\mathrm{val}},t_{\\mathrm{test}}]$ for training, validation, and test with   \n569 $0<t_{\\mathrm{train}}<t_{\\mathrm{val}}<t_{\\mathrm{test}}.$ .   \n570 In the experiment, a sequence of 100,000 timesteps is generated using the Runge\u2013Kutta 4th  \n571 order (RK4) time integrator with a step size 0.001. The initial condition is given as $\\textbf{\\em x}=$   \n572 $(1,2,103.2874,103.2874)$ following [29]. The training/validation/test split is defined by $t_{\\mathrm{train}}=20$ ,   \n573 $t_{\\mathrm{val}}\\,=\\,30$ , and $t_{\\mathrm{test}}=100$ . For a fair comparison, all considered models are set to have a similar   \n574 number of model parameters, ${\\sim}2\\mathrm{,}000$ . The specifications of the network architectures are:   \n575 \u2022 NMS: The total number of model parameters is 1959. The functions $A_{\\mathrm{tri}},B,K_{\\mathrm{chol}},E,S$   \n576 are parameterized as MLPs with the Tanh nonlinear activation function. The MLPs pa  \n577 rameterizing $A_{\\mathrm{tri}},B,K_{\\mathrm{chol}},E$ are specified as 1 hidden layer with 10 neurons, and the on   \n578 parameterizing $S$ is specified as 3 hidden layers with 25 neurons.   \n579 \u2022 NODE: The total number of model parameters is 2179. The black-box NODE is param  \n580 eterized as an MLP with the Tanh nonlinear activation function, 4 hidden layers and 25   \n581 neurons.   \n582 \u2022 SPNN: The total number of model parameters is 1954. The functions $E$ and $S$ are parame  \n583 terized as MLPs with the Tanh nonlinear activation function; each MLP is specified as 3   \n584 hidden layers and 20 neurons. The two 2-tensors defining $\\textbf{\\emph{L}}$ and $_M$ are defined as learnable   \n585 $3\\times3$ matrices.   \n586 \u2022 GNODE: The total number of model parameters is 2343. The functions $E$ and $S$ are   \n587 parameterized as MLPs with the Tanh nonlinear activaton function; each MLP is specified   \n588 as 2 hidden layers and 30 neurons. The matrices and 3-tensors required to learn $\\textbf{\\emph{L}}$ and $_M$   \n589 are defined as learnable $3\\times3$ matrices and $3\\times3\\times3$ tensor.   \n590 \u2022 GFINN: The total number of model parameters is 2065. The functions $E$ and $S$ are   \n591 parameterized as MLPs with Tanh nonlinear activation function; each MLP is specified as 2   \n592 hidden layers and 20 neurons. The matrices to required to learn $\\textbf{\\emph{L}}$ and $_M$ are defined as $K$   \n593 learnable $3\\times3$ matrices, where $K$ is set to 2. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "594 B.2 Thermoelastic double pendulum ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "595 The equations of motion in this case are given for $1\\leq i\\leq2$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\dot{q}_{i}=\\frac{p_{i}}{m_{i}},\\quad\\dot{p}_{i}=-\\partial_{q_{i}}\\bigl(E_{1}(x)+E_{2}(x)\\bigr),\\quad\\dot{S}_{1}=\\kappa\\bigl(T_{1}^{-1}T_{2}-1\\bigr),\\quad\\dot{S}_{2}=\\kappa\\bigl(T_{1}T_{2}^{-1}-1\\bigr),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "596 where $\\kappa>0$ is a thermal conductivity constant (set to 1), $m_{i}$ is the mass of the $i^{\\mathrm{th}}$ spring (also set to   \n597 1) and $T_{i}=\\partial_{S_{i}}E_{i}$ is its absolute temperature. In this case, $\\pmb{q}_{i},\\pmb{p}_{i}\\in\\mathbb{R}^{2}$ represent the position and   \n598 momentum of the $i^{\\mathrm{th}}$ mass, while $S_{i}$ represents the entropy of the $i^{\\mathrm{th}}$ pendulum. As before, the total   \n599 entropy $S(\\pmb{x})=S_{1}+S_{2}$ is the sum of the entropies of the two springs, while defining the internal   \n600 energies ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{i}({\\pmb x})=\\frac{1}{2}(\\ln\\lambda_{i})^{2}+\\ln\\lambda_{i}+e^{S_{i}-\\ln\\lambda_{i}}-1,\\quad\\lambda_{1}=|{\\pmb q}_{i}|,\\quad\\lambda_{2}=|{\\pmb q}_{2}-{\\pmb q}_{1}|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "601 leads to the total energy $E({\\pmb x})=(1/2m_{1})|{\\pmb p}_{1}|^{2}+(1/2m_{2})|{\\pmb p}_{2}|^{2}+E_{1}({\\pmb x})+E_{2}({\\pmb x}).$ ", "page_idx": 16}, {"type": "text", "text": "602 The thermoelastic double pendulum experiment tests model prediction across initial conditions. In   \n603 this case, 100 trajectories are generated by varying initial conditions that are randomly sampled from   \n604 [ $0.1,1.1]\\times[-0.1,0.1]\\times[2.1,^{-}2.3]\\times[-0.1,0.1]\\times[-1.9,2.1]\\times[0.9,1.1]\\times[-0.1,0.1]\\times[0.9,1.1]\\times[-0.9,1.0]\\times[0.9,1.1]\\times[-1.9,2.1]$   \n605 $[0.1,0.3]\\subset\\mathbb{R}^{10}$ . Each trajectory is obtained from the numerical integration of the ODEs using an   \n606 RK4 time integrator with step size 0.02 and the final time $T=40$ , resulting in the trajectories of   \n607 length 2,000. The resulting 100 trajectories are split into 80/10/10 for training/validation/test sets. For   \n608 a fair comparison, all considered models are again set to have similar number of model parameters,   \n609 ${\\sim}2\\mathrm{,}000$ . The specifications of the network architectures are:   \n610 \u2022 NMS: The total number of model parameters is 2201. The functions $A,B,K,E,S$ are pa  \n611 rameterized as MLPs with the Tanh nonlinear activation function. The MLPs parameterizing   \n612 are specified as 1 hidden layer with 15 neurons.   \n613 \u2022 NODE: The total number of model parameters is 2005. The black-box NODE is param  \n614 eterized as an MLP with the Tanh nonlinear activation function, 2 hidden layers and 35   \n615 neurons.   \n616 \u2022 SPNN: The total number of model parameters is 2362. The functions $E$ and $S$ are parame  \n617 terized as MLPs with the Tanh nonlinear activation function; each MLP is specified as 3   \n618 hidden layers and 20 neurons. The two 2-tensors defining $\\textbf{\\emph{L}}$ and $_M$ are defined as learnable   \n619 $3\\times3$ matrices.   \n620 \u2022 GNODE: The total number of model parameters is 2151. The functions $E$ and $S$ are   \n621 parameterized as MLPs with the Tanh nonlinear activaton function; each MLP is specified   \n622 as 2 hidden layers and 15 neurons. The matrices and 3-tensors required to learn $\\textbf{\\emph{L}}$ and $_M$   \n623 are defined as learnable $3\\times3$ matrices and $3\\times3\\times3$ tensor.   \n624 \u2022 GFINN: The total number of model parameters is 2180. The functions $E$ and $S$ are   \n625 parameterized as MLPs with Tanh nonlinear activation function; each MLP is specified as 2   \n626 hidden layers and 15 neurons. The matrices to required to learn $\\textbf{\\emph{L}}$ and $_M$ are defined as $K$   \n627 learnable $3\\times3$ matrices, where $K$ is set to 2. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "628 C Additional experiment: Damped nonlinear oscillator ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "629 Consider a damped nonlinear oscillator of variable dimension with state $\\pmb{x}=(\\pmb{q}\\quad\\pmb{p}\\quad S)^{\\intercal}$ , whose   \n630 motion is governed by the metriplectic system ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\dot{\\pmb q}=\\frac{p}{m},\\quad\\dot{\\pmb p}=k\\sin\\pmb q-\\gamma\\pmb p,\\quad\\dot{\\pmb S}=\\frac{\\gamma|\\pmb q|^{2}}{m T}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "631 Here $\\mathbf{q},\\mathbf{p}\\in\\mathbb{R}^{n}$ denote the position and momentum of the oscillator, $S$ is the entropy of a surround  \n632 ing thermal bath, and the constant parameters $m,\\gamma,T$ are the mass, damping rate, and (constant)   \n633 temperature. This leads to the total energy $E(\\pmb{x})=(1/2m)|\\pmb{p}|^{2}-k\\cos\\pmb{q}+T S$ , which is readily   \n634 seen to be constant along solutions ${\\mathbf{}}x(t)$ .   \n635 It is now verified that NMS can accurately and stably predict the dynamics of a nonlinear oscillator   \n636 $\\pmb{x}=(\\pmb{q}\\quad\\pmb{p}\\quad S)^{\\intercal}$ in the case that $n=1,2$ , both when the entropy $S$ is observable as well as when it   \n637 is not. As before, the task considered is prediction in time, although all compared methods NODE,   \n638 GNODE, and $\\mathrm{NMS}_{\\mathrm{known}}$ are now trained on full state information from the training interval, and test   \n639 errors are computed over the full state $\\textbf{\\em x}$ on the extrapolation interval $(t_{\\mathrm{valid}},t_{\\mathrm{test}}]$ , which is $150\\%$   \n640 longer than the training interval. In addition, another NMS model, $\\mathrm{NMS_{\\mathrm{diff}}}$ , was trained using only   \n641 the partial state information $\\pmb{x}^{o}=(\\pmb{q},\\pmb{p})^{\\intercal}$ and tested under the same conditions, with the initial guess   \n642 for $\\pmb{x}^{u}$ generated as in Appendix E. As can be seen in Table 3, NMS is more accurate than GNODE   \n643 or NODE in both the 1-D and 2-D nonlinear oscillator experiments, improving on previous results by   \n644 up to two orders of magnitude. Remarkably, NMS produces more accurate entropic dynamics even   \n645 in the case where the entropic variable $S$ is unobserved during NMS training and observed during   \n646 the training of other methods. This illustrates another advantage of the NMS approach: because of   \n647 the reasonable initial data for $S$ produced by the diffusion model, the learned metriplectic system   \n648 produced by NMS remains performant even when metriplectic governing equations are unknown and   \n649 only partial state information is observed.   \n650 To describe the experimental setup precisely, data is collected from a single trajectory with initial   \n651 condition as $\\pmb{x}=(\\mathbf{2},\\mathbf{0},0)$ following [16]. The path is calculated at 180,000 steps with a time interval   \n652 of 0.001, and is then split into training/validation/test sets as before using $t_{\\mathrm{train}}=60$ , $t_{\\mathrm{val}}=90$ and   \n653 $t_{\\mathrm{test}}=180$ . Specifications of the networks used for the experiments in Table 3 are:   \n654 \u2022 NMS: The total number of parameters is 154. The number of layers for $A_{\\mathrm{tri}},B,K_{\\mathrm{chol}},E,S$   \n655 is selected from $\\{1{,}2{,}3\\}$ and the number of neurons per layer from {5,10,15}. The best   \n656 hyperparameters are 1 hidden layer with 5 neurons for each network function.   \n657 \u2022 GNODE: The total number of model parameters is 203. The number of layers and num  \n658 ber of neurons for each network is chosen from the same ranges as for NMS. The best   \n659 hyperparameters are 1 layer with 10 neurons for each network function.   \n660 \u2022 NODE: The total number of model paramters is 3003. The NODE architecture is formed by   \n661 stacking MLPs with Tanh activation functions. The number of blocks is chosen from {3,4,5}   \n662 and the number of neurons of each MLP from {30,40,50}. The best hyperparameters are 4   \n663 and 30 for the number of blocks and number of neurons, respectively. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "3mzFmBPFIX/tmp/9941f995399811f1e9a4be510f0de5bfc3b4800306f50abd75d3f32f8dbdcefb.jpg", "table_caption": ["Table 3: Experimental results for the benchmark problems with respect to MSE and MAE. The best scores are in boldface. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "664 D Scaling study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "665 To compare the scalability of the proposed NMS architecture design with existing architectures, dif  \n666 ferent realizations of GNODE, GFINN, and NMS are generated by varying the dimension of the state   \n667 variables, $n=\\{1,5,10,15,20,30,50\\}$ . The specifications of these models (i.e., hyperparameters)   \n668 are set so that the number of model parameters is kept similar between each method for smaller values   \n669 of $n$ . For example, for $n=1,5$ the number of model parameters is ${\\sim}20\\small{,}000$ for each architecture.   \n670 The results in Figure 3(a) confirm that GNODE scales cubically in $n$ while both GFINN and NMS   \n671 scale quadratically. Note that only a constant scaling advantage of NMS over GFINN can be seen   \n672 from this plot, since $r$ is fixed during this study.   \n673 It is also worthwhile to investigate the computational timings of these three models. Consider  \n667754 n, s1 ,o0f0 t0h rea nmdoodme lssa lmisptleeds  aobf osvtaet,e is. ealr ei ngsetannercaetse df.o rT hvaersye isnagm $n=$   \n$\\{1,5,10,15,20,30,50\\}$ $\\{\\pmb{x}^{(i)}\\}_{i=1}^{1,000}$   \n676 are then fed to the dynamics function $\\begin{array}{r}{L(\\pmb{x}^{(i)})\\nabla E(\\pmb{x}^{(i)})+M(\\pmb{\\dot{x}}^{(i)})\\nabla S(\\pmb{x}^{(i)})}\\end{array}$ for $i=1,\\ldots,1000$ ,   \n677 and the computational wall time of the function evaluation via PyTorch\u2019s profiler API is measured.   \n678 The results of this procedure are displayed in Figure 3(b). Again, it is seen that the proposed NMSs   \n679 require less computational resources than GNODEs and GFINNs.   \n681 Recent work in [30] suggests the benefits of performing time-series generation using a diffusion   \n682 model. This Appendix describes how this technology is used to generate initial conditions for the   \n683 unobserved NMS variables in the experiments corresponding to Table 3. More precisely, we describe   \n684 how to train a conditional diffusion model which generates values for unobserved variables $\\pmb{x}^{u}$ given   \n685 values for the observed variables $x^{o}$ .   \n686 Training and sampling: Recall that diffusion models add noise with the following stochastic   \n687 differential equation (SDE): ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "3mzFmBPFIX/tmp/4a2f8c2659c2b73c53b5eb8064e1df7ed04f1a67bbea8699f7b8fe02c154c291.jpg", "img_caption": ["Figure 3: A study of the scaling behavior of GNODE, GFINN, and NMS. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\mathbf{x}(t)=\\mathbf{f}(t,\\mathbf{x}(t))d t+g(t)d\\mathbf{w},\\quad t\\in[0,1],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "688 where $\\mathbf{w}\\,\\in\\,\\mathbb{R}^{\\mathrm{dim}(\\mathbf{x})}$ is a multi-dimensional Brownian motion, $\\mathbf{f}(t,\\cdot)\\,:\\,\\mathbb{R}^{\\mathrm{dim}(\\mathbf{x})}\\,\\to\\,\\mathbb{R}^{\\mathrm{dim}(\\mathbf{x})}$ is a   \n689 vector-valued drift term, and $g:[0,1]\\rightarrow\\mathbb{R}$ is a scalar-valued diffusion function. ", "page_idx": 19}, {"type": "text", "text": "690 For the forward SDE, there exists a corresponding reverse SDE: ", "page_idx": 19}, {"type": "equation", "text": "$$\nd\\mathbf{x}(t)=[\\mathbf{f}(t,\\mathbf{x}(t))-g^{2}(t)\\nabla_{\\mathbf{x}(t)}\\log p(\\mathbf{x}(t))]d t+g(t)d\\bar{\\mathbf{w}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "691 which produces samples from the initial distribution at $t=0$ . This formula suggests that if the score   \n692 function, $\\nabla_{\\mathbf{x}(t)}{\\log{p(\\mathbf{x}(t))}}$ , is known, then real samples from the prior distribution $p(\\mathbf{x})\\sim\\mathcal{N}(\\mu,\\sigma^{2})$   \n693 can be recovered, where $\\mu,\\sigma$ vary depending on the forward SDE type. ", "page_idx": 19}, {"type": "text", "text": "694 In order for a model $M_{\\theta}$ to learn the score function, it has to optimize the following loss: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(\\theta)=\\mathbb{E}_{t}\\{\\lambda(t)\\mathbb{E}_{\\mathbf{x}(t)}[\\left|\\left|M_{\\theta}(t,\\mathbf{x}(t))-\\nabla_{\\mathbf{x}(t)}\\log p(\\mathbf{x}(t))\\right|\\right|_{2}^{2}]\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "695 where $t$ is uniformly sampled over $[0,1]$ with an appropriate weight function $\\lambda(t)\\,:\\,[0,1]\\,\\rightarrow\\,\\mathbb{R}$   \n696 However, using the above formula is computationally prohibitive. Thanks to [31], this loss can be   \n697 substituted with the following denoising score matching loss: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L^{*}(\\theta)=\\mathbb{E}_{t}\\{\\lambda(t)\\mathbb{E}_{\\mathbf{x}(0)}\\mathbb{E}_{\\mathbf{x}(t)|\\mathbf{x}(0)}[\\left|\\left|M_{\\theta}(t,\\mathbf{x}(t))-\\nabla_{\\mathbf{x}(t)}\\log p(\\mathbf{x}(t)|\\mathbf{x}(0))\\right|\\right|_{2}^{2}]\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "698 Since score-based generative models use an affine drift term, the transition kernel $p(\\mathbf{x}(t)|\\mathbf{x}(0))$ follows   \n699 a certain Gaussian distribution [32], and therefore the gradient term $\\nabla_{\\mathbf{x}(t)}\\log p(\\mathbf{x}(t)|\\mathbf{x}(0))$ can be   \n700 analytically calculated.   \n701 Experimental details On the other hand, the present goal is to generate unobserved variables $\\pmb{x}^{u}$   \n702 given values for the observed variables $\\pmb{x}^{o}=(\\pmb{q},\\pmb{p})$ , i.e., conditional generation. Therefore, our model   \n703 has to learn the conditional score function, $\\overbar{\\nabla}_{\\pmb{x}^{u}(t)}\\log p(\\pmb{x}^{u}(t)|\\pmb{x}^{\\bar{o}})$ . For example, in the damped   \n704 nonlinear oscillator case, $S(t)$ is initialized as a perturbed $t\\in[0,1]$ , from which the model takes the   \n705 concatenation of $\\mathbf{\\boldsymbol{q}},\\mathbf{\\boldsymbol{p}},S(t)$ as inputs and learns conditional the score function $\\nabla_{S(t)}\\log(S(t)|\\mathbf{q},\\pmb{p})$ .   \n706 For the experiments in Table 3, diffusion models are trained to generate $\\pmb{x}^{u}$ variables on three   \n707 benchmark problems: the damped nonlinear oscillator, two gas containers, and thermolastic double   \n708 pendulum. On each problem, representative parameters such as mass or thermal conductivity are   \n709 varied, with the total number of cases denoted by $N$ . Full trajectory data of length $T$ is then generated   \n710 using a standard numerical integrator (e.g., dopri5), before it is evenly cut into $\\lfloor T/L\\rfloor$ pieces of   \n711 length $L$ . Let $V,U$ denote the total number of variables and the number of unobserved variables,   \n712 respectively. It follows that the goal is to generate $U$ unobserved variables given $V-U$ observed   \n713 ones, i.e., the objective is to generate data of shape $(N T/L,L,U)$ conditioned on data of shape   \n714 $(N T/L,L,V\\mathrm{~-~}U)$ . After the diffusion model has been trained for this task, the output data is   \n715 reshaped into size $(N,T,U)$ , which is used to initialize the NMS model. Note that the NODE and   \n716 GNODE methods compared to NMS in Table 3 use full state information for their training, i.e.,   \n717 ${\\pmb x}^{u}={\\boldsymbol{\\mathcal{Q}}}$ in these cases, making it comparatively easier for these methods to learn system dynamics.   \n718 As in other diffusion models e.g. [33], a U-net architecture is used, modifying 2-D convolutions to   \n719 1-D ones and following the detailed hyperparameters described in [33]. Note the following probability   \n720 flow ODE seen in [33]: ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\nd\\mathbf{x}(t)=\\left[\\mathbf{f}(t,\\mathbf{x}(t))-\\frac{1}{2}g^{2}(t)\\nabla_{\\mathbf{x}(t)}\\log p(\\mathbf{x}(t))\\right]d t,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "721 Although models trained to mimic the probability flow ODE do not match the perofrmance of the   \n722 forward SDE\u2019s result in the image domain, the authors of [30] observe that the probability flow ODE   \n723 outperforms the forward SDE in the time-series domain. Therefore, the probability flow ODE is used   \n724 with the default hyperparameters of [33].   \n726 The checklist is designed to encourage best practices for responsible machine learning research,   \n727 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n728 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n729 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n730 towards the page limit.   \n731 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n732 each question in the checklist:   \n733 \u2022 You should answer [Yes] , [No] , or [NA] .   \n734 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n735 relevant information is Not Available.   \n736 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n737 The checklist answers are an integral part of your paper submission. They are visible to the   \n738 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n739 (after eventual revisions) with the final version of your paper, and its final version will be published   \n740 with the paper.   \n741 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n742 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n743 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n744 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n745 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n746 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n747 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n748 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n749 please point to the section(s) where related material for the question can be found. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "750 IMPORTANT, please: ", "page_idx": 20}, {"type": "text", "text": "751 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n752 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n753 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n754 1. Claims   \n755 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n756 paper\u2019s contributions and scope?   \n757 Answer: [Yes]   \n758 Justification: The claims made in the abstract and contributions paragraph at the end of the   \n759 introduction are justified in detail throughout the rest of the paper.   \n760 Guidelines:   \n761 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n762 made in the paper.   \n763 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n764 contributions made in the paper and important assumptions and limitations. A No or   \n765 NA answer to this question will not be perceived well by the reviewers.   \n766 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n767 much the results can be expected to generalize to other settings.   \n768 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n769 are not attained by the paper.   \n770 2. Limitations   \n771 Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "772 Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "773 Justification: Limitations are discussed in the Conclusion section.   \n774 Guidelines:   \n775 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n776 the paper has limitations, but those are not discussed in the paper.   \n777 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n778 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n779 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n780 model well-specification, asymptotic approximations only holding locally). The authors   \n781 should reflect on how these assumptions might be violated in practice and what the   \n782 implications would be.   \n783 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n784 only tested on a few datasets or with a few runs. In general, empirical results often   \n785 depend on implicit assumptions, which should be articulated.   \n786 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n787 For example, a facial recognition algorithm may perform poorly when image resolution   \n788 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n789 used reliably to provide closed captions for online lectures because it fails to handle   \n790 technical jargon.   \n791 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n792 and how they scale with dataset size.   \n793 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n794 address problems of privacy and fairness.   \n795 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n796 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n797 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n798 judgment and recognize that individual actions in favor of transparency play an impor  \n799 tant role in developing norms that preserve the integrity of the community. Reviewers   \n800 will be specifically instructed to not penalize honesty concerning limitations.   \n801 3. Theory Assumptions and Proofs   \n802 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n803 a complete (and correct) proof?   \n804 Answer: [Yes]   \n805 Justification: All theoretical results are clearly stated along with the necessary assumptions.   \n806 All formal arguments are complete and contained in the Appendix.   \n807 Guidelines:   \n808 \u2022 The answer NA means that the paper does not include theoretical results.   \n809 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n810 referenced.   \n811 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n812 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n813 they appear in the supplemental material, the authors are encouraged to provide a short   \n814 proof sketch to provide intuition.   \n815 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n816 by formal proofs provided in appendix or supplemental material.   \n817 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n818 4. Experimental Result Reproducibility   \n819 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n820 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n821 of the paper (regardless of whether the code and data are provided or not)?   \n822 Answer: [Yes]   \n823 Justification: All information necessary to implement the proposed architecture is included   \n824 in the body of the manuscript. In addition, all relevant experimental details are included in   \n825 the Appendix. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Code for running the proposed algorithm is included in the supplemental material and will be released publicly upon publication. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 22}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized ", "page_idx": 23}, {"type": "text", "text": "882 versions (if applicable).   \n883 \u2022 Providing as much information as possible in supplemental material (appended to the   \n884 paper) is recommended, but including URLs to data and code is permitted.   \n885 6. Experimental Setting/Details   \n886 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n887 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n888 results?   \n889 Answer: [Yes]   \n890 Justification: All relevant experimental details are presented in the Appendix at an appropri  \n891 ate level of detail.   \n892 Guidelines:   \n893 \u2022 The answer NA means that the paper does not include experiments.   \n894 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n895 that is necessary to appreciate the results and make sense of them.   \n896 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n897 material.   \n898 7. Experiment Statistical Significance   \n899 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n900 information about the statistical significance of the experiments?   \n901 Answer: [Yes]   \n902 Justification: All experiments in the body contain means and standard deviations as the   \n903 initialization is varied.   \n904 Guidelines:   \n905 \u2022 The answer NA means that the paper does not include experiments.   \n906 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n907 dence intervals, or statistical significance tests, at least for the experiments that support   \n908 the main claims of the paper.   \n909 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n910 example, train/test split, initialization, random drawing of some parameter, or overall   \n911 run with given experimental conditions).   \n912 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n913 call to a library function, bootstrap, etc.)   \n914 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n915 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n916 of the mean.   \n917 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n918 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n919 of Normality of errors is not verified.   \n920 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n921 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n922 error rates).   \n923 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n924 they were calculated and reference the corresponding figures or tables in the text.   \n925 8. Experiments Compute Resources   \n926 Question: For each experiment, does the paper provide sufficient information on the com  \n927 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n928 the experiments?   \n929 Answer: [Yes]   \n930 Justification: All necessary information is included in the Appendix.   \n931 Guidelines:   \n932 \u2022 The answer NA means that the paper does not include experiments.   \n933 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n934 or cloud provider, including relevant memory and storage.   \n935 \u2022 The paper should provide the amount of compute required for each of the individual   \n936 experimental runs as well as estimate the total compute.   \n937 \u2022 The paper should disclose whether the full research project required more compute   \n938 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n939 didn\u2019t make it into the paper).   \n940 9. Code Of Ethics   \n941 Question: Does the research conducted in the paper conform, in every respect, with the   \n942 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n943 Answer: [Yes]   \n944 Justification: This is explained in the \"broader impacts\" section.   \n945 Guidelines:   \n946 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n947 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n948 deviation from the Code of Ethics.   \n949 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n950 eration due to laws or regulations in their jurisdiction).   \n951 10. Broader Impacts   \n952 Question: Does the paper discuss both potential positive societal impacts and negative   \n953 societal impacts of the work performed?   \n954 Answer: [Yes]   \n955 Justification: This paper investigates a novel machine learning method tailored to physics  \n956 based simulations and fundamental science. While subsequent applications of this work may   \n957 have societal impact, the research presented here is strictly foundational and only serves to   \n958 improve the production of physically realistic dynamics from data.   \n959 Guidelines:   \n960 \u2022 The answer NA means that there is no societal impact of the work performed.   \n961 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n962 impact or why the paper does not address societal impact.   \n963 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n964 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n965 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n966 groups), privacy considerations, and security considerations.   \n967 \u2022 The conference expects that many papers will be foundational research and not tied   \n968 to particular applications, let alone deployments. However, if there is a direct path to   \n969 any negative applications, the authors should point it out. For example, it is legitimate   \n970 to point out that an improvement in the quality of generative models could be used to   \n971 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n972 that a generic algorithm for optimizing neural networks could enable people to train   \n973 models that generate Deepfakes faster.   \n974 \u2022 The authors should consider possible harms that could arise when the technology is   \n975 being used as intended and functioning correctly, harms that could arise when the   \n976 technology is being used as intended but gives incorrect results, and harms following   \n977 from (intentional or unintentional) misuse of the technology.   \n978 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n979 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n980 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n981 feedback over time, improving the efficiency and accessibility of ML).   \n11. Safeguards ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "982 ", "page_idx": 24}, {"type": "text", "text": "983 Question: Does the paper describe safeguards that have been put in place for responsible   \n984 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n985 image generators, or scraped datasets)?   \n986 Answer: [NA]   \n987 Justification: N/A   \n988 Guidelines:   \n989 \u2022 The answer NA means that the paper poses no such risks.   \n990 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n991 necessary safeguards to allow for controlled use of the model, for example by requiring   \n992 that users adhere to usage guidelines or restrictions to access the model or implementing   \n993 safety filters.   \n994 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n995 should describe how they avoided releasing unsafe images.   \n996 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n997 not require this, but we encourage authors to take this into account and make a best   \n998 faith effort.   \n999 12. Licenses for existing assets   \n1000 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1001 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1002 properly respected?   \n1003 Answer: [NA]   \n1004 Justification: N/A   \n1005 Guidelines:   \n1006 \u2022 The answer NA means that the paper does not use existing assets.   \n1007 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1008 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1009 URL.   \n1010 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1011 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1012 service of that source should be provided.   \n1013 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1014 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1015 has curated licenses for some datasets. Their licensing guide can help determine the   \n1016 license of a dataset.   \n1017 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1018 the derived asset (if it has changed) should be provided.   \n1019 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1020 the asset\u2019s creators.   \n1021 13. New Assets   \n1022 Question: Are new assets introduced in the paper well documented and is the documentation   \n1023 provided alongside the assets?   \n1024 Answer: [NA]   \n1025 Justification: N/A   \n1026 Guidelines:   \n1027 \u2022 The answer NA means that the paper does not release new assets.   \n1028 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1029 submissions via structured templates. This includes details about training, license,   \n1030 limitations, etc.   \n1031 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1032 asset is used.   \n1033 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1034 create an anonymized URL or include an anonymized zip file.   \n36 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n37 include the full text of instructions given to participants and screenshots, if applicable, as   \n38 well as details about compensation (if any)?   \n39 Answer: [NA]   \n40 Justification: N/A   \n41 Guidelines:   \n42 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n43 human subjects.   \n44 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n45 tion of the paper involves human subjects, then as much detail as possible should be   \n46 included in the main paper.   \n47 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n48 or other labor should be paid at least the minimum wage in the country of the data   \n49 collector. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 26}, {"type": "text", "text": "Subjects   \nQuestion: Does the paper describe potential risks incurred by study participants, whether   \nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \napprovals (or an equivalent approval/review based on the requirements of your country or   \ninstitution) were obtained?   \nAnswer: [NA]   \nJustification: N/A   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]