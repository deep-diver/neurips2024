[{"figure_path": "Jj2PEAZPWk/figures/figures_1_1.jpg", "caption": "Figure 1: Visual comparisons of mainstream weakly supervised point cloud semantic segmentation paradigms and our DGNet. The solid and dashed lines represent the network forward process and the loss function, respectively.", "description": "This figure compares four different weakly supervised point cloud semantic segmentation methods, including contrastive learning/perturbation consistency, self-training, similarity metric, and the proposed Distribution Guidance Network (DGNet).  Each method is illustrated with a diagram showing the flow of data through the network and the type of loss function used.  The figure highlights the differences in how these methods leverage sparse annotations to guide the learning process.  DGNet, in particular, is shown to utilize a distribution alignment branch to refine the feature embeddings, leading to improved segmentation performance.", "section": "1 Introduction"}, {"figure_path": "Jj2PEAZPWk/figures/figures_4_1.jpg", "caption": "Figure 2: Structure of Distribution Guidance Network.", "description": "The figure illustrates the architecture of the Distribution Guidance Network (DGNet), which consists of two main branches: the weakly supervised learning branch and the distribution alignment branch. The weakly supervised learning branch takes sparse annotations as input and learns semantic embeddings from the point cloud data. This branch also provides robust initialization for the distribution alignment branch. The distribution alignment branch dynamically aligns the embedding distribution to the mixture of von Mises-Fisher distributions (moVMF). This is achieved using a nested Expectation-Maximum (EM) algorithm, which alternates between updating the network parameters and the moVMF parameters. The moVMF parameters help in characterizing the latent feature space and enforcing alignment. Three loss functions are used to guide the training: cross-entropy loss (LICE) for the weakly supervised learning branch, vMF loss (LVMF) for aligning the embedding distribution to moVMF, and discriminative loss (LDIS) for ensuring distinct decision boundaries between the different categories. A consistency loss (LCON) is also used to impose consistency between the segmentation predictions from the weakly supervised branch and the posterior probabilities from the distribution alignment branch.", "section": "3.3 Distribution Guidance Network"}, {"figure_path": "Jj2PEAZPWk/figures/figures_6_1.jpg", "caption": "Figure 3: DGNet provides segmentation predictions from the weakly supervised learning branch and explains it probabilistically by posterior probabilities from the distribution alignment branch.", "description": "This figure shows a pipeline of the proposed DGNet architecture. First, a point cloud is fed into the DGNet which consists of two branches: the weakly supervised learning branch and the distribution alignment branch. The weakly supervised learning branch generates a prediction for the point cloud. Simultaneously, the distribution alignment branch provides a probabilistic explanation of the prediction by calculating posterior probabilities. The posterior probabilities show the probability of each point belonging to a specific class, providing insights into the prediction's confidence.", "section": "3.3 Distribution Guidance Network"}, {"figure_path": "Jj2PEAZPWk/figures/figures_8_1.jpg", "caption": "Figure 4: Visual comparisons between baseline and our DGNet on S3DIS Area 5 at 0.01% label rate.", "description": "This figure presents a qualitative comparison of the segmentation results obtained using PointNeXt (a baseline method) and DGNet (the proposed method) on the S3DIS Area 5 dataset.  The comparison is performed using a 0.01% label rate, indicating a very sparse annotation setting.  The figure shows several point cloud scenes from S3DIS Area 5, alongside their respective ground truth segmentations, PointNeXt segmentations, and DGNet segmentations. Each scene includes a region highlighted with a blue box, highlighting specific areas where the visual difference between the two methods is significant.  This visual comparison aims to demonstrate the improvements in segmentation accuracy achieved by DGNet, even under extremely sparse annotation conditions.", "section": "4 Experimental Analysis"}]