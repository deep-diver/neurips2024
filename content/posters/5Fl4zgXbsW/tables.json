[{"figure_path": "5Fl4zgXbsW/tables/tables_7_1.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the performance comparison of various question selection algorithms on intra-class ranking consistency.  It shows the results for different algorithms (including those using collaborative ability estimation) across two datasets (NIPS-EDU and JUNYI) and varying numbers of test steps. The bold values indicate statistically significant improvements over the baseline.", "section": "5 Experiments"}, {"figure_path": "5Fl4zgXbsW/tables/tables_7_2.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the performance comparison of different question selection algorithms on intra-class ranking consistency.  It shows the results for both the original IRT-based ability estimation and the proposed CCAT's collaborative ability estimation method. The table is split into subsections for different IRT estimation methods (GD and MCMC) and shows results across various testing steps (5, 10, 15, 20).  The bold numbers highlight statistically significant improvements over baseline performance.", "section": "5 Experiments"}, {"figure_path": "5Fl4zgXbsW/tables/tables_8_1.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the results of an experiment comparing different question selection algorithms on their ability to produce consistent intra-class rankings.  The algorithms are tested with and without using collaborative ability estimation (CCAT). The results show the ranking consistency performance for various algorithms across two datasets (NIPS-EDU and JUNYI) for different numbers of test steps (5, 10, 15, and 20).  The bold values indicate a statistically significant improvement over the baseline.", "section": "5 Experiments"}, {"figure_path": "5Fl4zgXbsW/tables/tables_15_1.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the performance comparison of different question selection algorithms on intra-class ranking consistency.  It shows the results for two datasets (NIPS-EDU and JUNYI) across various test steps (5, 10, 15, and 20). The comparison includes both traditional methods and those incorporating collaborative ability estimation (indicated by '-C').  The table highlights the improvement in consistency offered by the CCAT framework, particularly when using collaborative ability estimation.", "section": "5.1 Expermental Setup"}, {"figure_path": "5Fl4zgXbsW/tables/tables_16_1.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the performance comparison of different question selection algorithms on intra-class ranking consistency.  It shows the results for various algorithms (including those using collaborative ability estimation) across different numbers of test steps (5, 10, 15, 20).  The metrics are reported for two datasets, NIPS-EDU and JUNYI, with and without employing collaborative ability estimation.  The bold font highlights statistically significant improvements compared to the baseline (Random).", "section": "5.1 Expermental Setup"}, {"figure_path": "5Fl4zgXbsW/tables/tables_16_2.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the performance comparison of various question selection algorithms on intra-class ranking consistency.  It shows results for different algorithms (including those using collaborative ability estimation) across two datasets (NIPS-EDU and JUNYI) and varying numbers of test steps (5, 10, 15, 20).  The bold numbers highlight statistically significant improvements over the baseline.", "section": "5.1 Expermental Setup"}, {"figure_path": "5Fl4zgXbsW/tables/tables_16_3.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the performance comparison of various question selection algorithms on the intra-class ranking consistency metric.  It compares the performance of algorithms using traditional IRT ability estimation with those using the collaborative ability estimation method introduced in the CCAT framework. The results are shown for different numbers of testing steps (5, 10, 15, and 20) and on two datasets (NIPS-EDU and JUNYI).  The boldfaced numbers indicate statistically significant improvements over baseline methods.", "section": "5.1 Expermental Setup"}, {"figure_path": "5Fl4zgXbsW/tables/tables_17_1.jpg", "caption": "Table 5: The Detail Performance of different question selection algorithms on JUNYI. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the detailed performance of different question selection algorithms on the JUNYI dataset. It shows the intra-class and inter-class ranking consistency performance with and without collaborative ability estimation for various test steps (5, 10, 15, 20).  The results are broken down by different question selection algorithms (Random, FSI, KLI, MAAT, BECAT, CCAT (without collaborative estimation), Random-C, FSI-C, KLI-C, MAAT-C, BECAT-C, and CCAT) and whether collaborative ability estimation was used.  Statistical significance is indicated by bold font, highlighting improvements over the baseline.", "section": "5.2 Results and Discussion"}, {"figure_path": "5Fl4zgXbsW/tables/tables_17_2.jpg", "caption": "Table 1: The Performance of Different Question Selection Algorithms on Intra-class Ranking Consistency. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. CCAT (w/o C) means using the question selection algorithm but estimating the ability by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the performance comparison of various question selection algorithms on intra-class ranking consistency.  It shows results for two datasets (NIPS-EDU and JUNYI) and different numbers of test steps (5, 10, 15, and 20). The algorithms are compared both with and without using the collaborative ability estimation method introduced in the CCAT framework.  The bold values indicate statistically significant improvements over the baseline.", "section": "5.1 Expermental Setup"}, {"figure_path": "5Fl4zgXbsW/tables/tables_17_3.jpg", "caption": "Table 5: The Detail Performance of different question selection algorithms on JUNYI. Algorithm X-C means use algorithm X for question selection but use collaborative ability estimation proposed in CCAT as the testing result instead of the abilities estimated by IRT. The bold font represents a significant improvement in statistics compared to the baseline.", "description": "This table presents the detailed performance of different question selection algorithms on the JUNYI dataset.  It shows the intra-class and inter-class ranking consistency performance for various algorithms, with and without collaborative ability estimation.  The results are broken down by the IRT estimation method used (GD and MCMC) and the number of test steps (5, 10, 15, and 20).  Bold values indicate statistically significant improvements compared to a baseline.", "section": "5.2 Results and Discussion"}]