{"references": [{"fullname_first_author": "Filippo Amato", "paper_title": "Artificial neural networks in medical diagnosis", "publication_date": "2013-01-01", "reason": "It establishes the context of DNNs' use in safety-critical applications, highlighting the need for verification."}, {"fullname_first_author": "Ian J Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2014-12-01", "reason": "It introduces the concept of adversarial examples, a major motivation for the paper's focus on robustness verification."}, {"fullname_first_author": "Aleksander Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2018-01-01", "reason": "It presents adversarial training, a key technique relevant to the paper's exploration of robustness and the limitations of existing methods."}, {"fullname_first_author": "Timon Gehr", "paper_title": "AI2: Safety and robustness certification of neural networks with abstract interpretation", "publication_date": "2018-01-01", "reason": "It provides a baseline non-relational verification method against which the paper's relational approach is compared and improved upon."}, {"fullname_first_author": "Rudy Bunel", "paper_title": "Branch and bound for piecewise linear neural network verification", "publication_date": "2020-01-01", "reason": "Its Branch and Bound approach is directly used and improved in the paper's core algorithm"}]}