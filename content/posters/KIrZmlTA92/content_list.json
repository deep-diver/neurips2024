[{"type": "text", "text": "Data-Driven Discovery of Dynamical Systems in Pharmacology using Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Samuel Holt\u2217 University of Cambridge sih31@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Zhaozhi Qian\u2217\u2020 Elm UK zqian@elm.sa ", "page_idx": 0}, {"type": "text", "text": "James Weatherall AstraZeneca ", "page_idx": 0}, {"type": "text", "text": "Mihaela van der Schaar University of Cambridge mv472@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The discovery of dynamical systems is crucial across a range of fields, including pharmacology, epidemiology, and physical sciences. Accurate and interpretable modeling of these systems is essential for understanding complex temporal processes, optimizing interventions, and minimizing adverse effects. In pharmacology, for example, precise modeling of drug dynamics is vital to maximize therapeutic efficacy while minimizing patient harm, as in chemotherapy. However, current models, often developed by human experts, are limited by high cost, lack of scalability, and restriction to existing human knowledge. In this paper, we present the Data-Driven Discovery $(D3)$ framework, a novel approach leveraging Large Language Models (LLMs) to iteratively discover and refine interpretable models of dynamical systems, demonstrated here with pharmacological applications. Unlike traditional methods, D3 enables the LLM to propose, acquire, and integrate new features, validate, and compare dynamical systems models, uncovering new insights into pharmacokinetics. Experiments on a pharmacokinetic Warfarin dataset reveal that D3 identifies a new plausible model that is well-ftiting, highlighting its potential for precision dosing in clinical applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The discovery of dynamical systems models plays a fundamental role across various domains, including pharmacology, epidemiology, and physical systems. In pharmacology, pharmacokinetic (PK) models are essential for understanding and predicting the time course of drug absorption, distribution, metabolism, and excretion in the body, which is crucial for optimizing therapeutic efficacy, minimizing toxicity, and personalized treatment regimens in diseases such as cancer, cardiovascular disorders, and infectious diseases [Gabrielsson and Weiner, 2001, Hedaya, 2012]. For example, cardiovascular disorders requiring Warfarin therapy affect tens of millions of individuals globally [Lee and Klein, 2013], highlighting the critical need for superior models to understand the dynamic impact of drugs and precision dosing, thereby assisting both doctors and patients. ", "page_idx": 0}, {"type": "text", "text": "The goal of PK modeling is to discover an underlying closed-form ordinary differential equation (ODE) $f$ from a dataset of observed patient trajectories. This problem is of significant interest to the machine learning (ML) community, as evidenced by previous non-interpretable ML modeling approaches aimed at developing better PK models [Chan and van der Schaar, 2022]. Such datasets are collected from expensive human clinical trials, necessitating the careful selection of which features to acquire and analyze during these trials [Guidance, 2010]. Pharmacometricians then leverage this data, using their existing knowledge to select appropriate pharmacokinetic models and employ standard statistical tools to infer the parameters of these models. For instance, they often fti a single compartmental PK model, a closed-form ODE model with 3-5 parameters, to the data [Chen and Abuassba, 2021]. This process is iterative, refining the models until the most accurate one that generalizes well to new patient trajectories is identified. However, this traditional model discovery approach is often ad-hoc and fundamentally limited by the human expert\u2019s time, experience, and implicit assumptions about the underlying pharmacokinetic processes. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Discovering interpretable pharmacokinetic models is traditionally performed by human experts through a scientific process of proposing, collecting, and validating models. Recently, there has been a growing call for artificial intelligence-driven methods to enhance pharmacological modeling [Ryan et al., 2024, Singh et al., 2023, Cheng et al., 2022]. This established process involves iterative steps, each presenting distinct challenges in constructing accurate pharmacokinetic dynamics models. ", "page_idx": 1}, {"type": "text", "text": "Selecting appropriate temporal models involves complex decisions on model class, state and feature variables, parameters, and their relationships, far exceeding the simplicity of standard pharmacometrics models like single, double, or triple compartmental models [Chen and Abuassba, 2021]. This complexity demands aligning model intricacy with data availability, where simple models suit small datasets and complex models fti larger ones. Acquiring the right features is crucial, as it must be done with limited prior information, unlike feature selection, which evaluates pre-existing features [Li et al., 2017]. The evaluation must identify inadequate models and explain why, requiring diverse and timeintensive tools beyond a single metric like validation MSE. These challenges are interrelated: strategic data acquisition and thorough evaluation are essential for effective modeling. Despite advancements in tackling these challenges independently, temporal modeling remains manual, limiting speed and scalability, underscoring the need for integrated and automated systems to enhance efficiency. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we develop the Data-Driven Discovery $(D3)$ framework, leveraging Large Language Models (LLMs) to iteratively discover and refine interpretable dynamics models, directly relevant to pharmacology, epidemiology and ecology applications. D3 features three agents: Modeling, Feature Acquisition, and Evaluation, which collaborate iteratively. The Modeling Agent uses LLMs\u2019 code generation and natural language understanding to explore the model space by generating hypothetical models. The Data Acquisition Agent utilizes LLMs\u2019 zero-shot and few-shot learning to optimize data acquisition based on summary statistics, text descriptions, and prior knowledge. The Evaluation Agent conducts comprehensive evaluations by computing refined validation MSE metrics, and alignment with prior process understanding, providing precise feedback to improve the other agents\u2019 performance. ", "page_idx": 1}, {"type": "text", "text": "Contributions: $\\textcircled{\\scriptsize{1}}$ We propose the Data-Driven Discovery $(D3)$ framework, a novel approach leveraging Large Language Models (LLMs) to iteratively discover and refine interpretable dynamics models, advancing pharmacokinetic modeling (Section 3). $\\circledcirc$ D3 overcomes the challenges of uncovering interpretable dynamical systems by using LLMs to explore vast model spaces and integrate unstructured data, producing models with few parameters that rival black box neural networks. D3 accurately discovers dynamics models across pharmacology, epidemiology, and ecology, matching the accuracy of existing methods while enhancing interpretability. It discovers a new, more accurate PK model for Warfarin, validated by expert pharmacometricians. We also gain insight into D3\u2019s ability to iteratively improve its models, selectively acquire features, and discover precise dynamics models. ", "page_idx": 1}, {"type": "text", "text": "2 The vast model space for temporal modeling ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we introduce the model space that D3 searches through, which contains a diverse range of temporal models with varying degrees of refinement. Our focus is on Ordinary Differential Equation (ODE) models, which is one of the most widely used methods for modeling temporal dynamics, which include pharmacokinetic, physiological and epidemiological dynamics [Auger et al., 2008]. We are interested in modeling the evolution of a set of state variables $\\mathbf{x}_{n}(t)\\in\\overline{{\\mathbb{R}}}^{D}$ for individuals $n=1,\\ldots,N$ over the time horizon $t\\in[0,T]$ . The state variables represent the target variables that the human experts are interested in modeling; and there may exist other feature variables that can help predict the evolution of state variables. We assume we are provided with a dataset of individuals trajectories such that $\\mathcal{D}=\\{(\\mathbf{x}_{n}(t),\\mathbf{a}_{n}(t))\\mid n=1,\\ldots,N,\\ t\\in[0,T]\\}$ , where $\\mathbf{a}_{n}(t)\\in\\mathbb{R}^{K}$ denotes the observed individual-level features for individual. We can categorize ODE models into three levels $({\\bf R}1-{\\bf R}3)$ based on their capacity for refinement. ", "page_idx": 1}, {"type": "text", "text": "R1: Refinement through initial conditions ${\\bf x}_{n}(0)$ . This level involves the simplest form of ODEs, commonly referenced in scientific literature [Schiesser, 2014]. These equations are defined over state variables $\\mathbf{\\dot{x}}(t)\\in\\mathbb{R}^{D}$ , incorporate global parameters $\\theta$ , and involve a function $f:\\mathbb{R}^{D+1}\\rightarrow\\mathbb{R}^{D}$ . The time $t$ spans the interval $[0,T]$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\dot{\\mathbf{x}}_{n}(t)=f(\\mathbf{x}_{n}(t),t,\\theta),\\forall n\\in[N]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Considering $N$ individuals, each represented by the trajectory ${\\bf x}_{n}(t)$ for ${n=1,\\ldots,N}$ , all follow the equation above. According to the uniqueness theorem for initial value problems in ODEs [Lindel\u00f6f, 1894], any differences among individuals\u2019 trajectories can be attributed solely to variations in the initial conditions ${\\bf x}_{n}(0)$ , under common regularity conditions on $f$ . ", "page_idx": 2}, {"type": "text", "text": "R2: Refinement through observed features ${\\bf a}_{n}$ . Suppose we have access to a set of individual-level feature variables ${\\bf a}_{n}$ . We can enhance the model\u2019s refinement by incorporating these features into the ODE to enable unique dynamics for different individuals: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\mathbf{x}}_{n}(t)=f(\\mathbf{x}_{n}(t),t,\\mathbf{a}_{n}(t),\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "R3: Refinement through acquired features $\\mathbf{h}_{n}$ . Often, not all relevant variables are initially measured and available for analysis. In such cases, one may consider acquiring additional features $\\mathbf{h}_{n}(t)\\in\\mathbb{R}^{J}$ to be integrated into the ODE. Here $\\mathbf{h}_{n}(t)$ is a collection of $J$ features from the set of all acquirable features $\\mathcal{H}$ , where $J\\leq|\\mathcal{H}|$ . This third level of refinement reflects one data acquisition challenge, i.e., identifying and collecting informative features $\\mathbf{h}_{n}(t)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\dot{{\\bf x}}_{n}(t)=f({\\bf x}_{n}(t),t,{\\bf a}_{n}(t),{\\bf h}_{n}(t),\\theta)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "After selecting the level of refinement (R1-R3), it is necessary to determine the functional form of the ODE $f$ , which defines the interactions among various variables and parameters that govern the dynamics. There are two dominant approaches to parameterize $f$ : using a concise closed-form white-box equation $f\\in\\mathcal{M}_{C}$ or employing a neural network $f\\,\\in\\,\\mathcal{M}_{N}$ . The former approach is prevalent in the field of symbolic regression [Billard and Diday, 2002], while the latter is utilized in Neural ODEs [Chen et al., 2018]. Additionally, $f$ can incorporate both a closed-form component and a neural component, resulting in a hybrid ODE model. Nevertheless, all three approaches involve searching within large combinatorial spaces\u2014closed-form equations, neural architectures, and both\u2014presenting significant computational challenges. ", "page_idx": 2}, {"type": "text", "text": "3 Data-Driven Discovery (D3) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to uncover the true underlying pharmacokinetic dynamical system, denoted as $f^{*}$ . Our goal is to continually approximate $f^{*}$ as accurately as possible using proposed models $f_{\\theta}$ , with parameters $\\theta$ . For simplicity, we will refer to these models as $f$ in all subsequent references, omitting the $\\theta$ term. To appropriately identify the refinement model level needed from the vast model space discussed above, we now introduce the Data-Driven Discovery $(D3)$ framework, as depicted in Figure 1. This consists of three LLM agents: the Modeling Agent $G$ , the Feature Acquisition Agent $A$ , and the Evaluation Agent $E$ . These three agents work together to form the model improvement loop and the data acquisition loop. The implementation details of the Agents are provided in Appendix F. ", "page_idx": 2}, {"type": "text", "text": "3.1 Inputs to Data-Driven Discovery (D3) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To begin the process, D3 requires a clearly defined description of the system in natural language, that details the modeling task, of creating a well-fitting model of either a white-box model or a hybrid model; that is a white-box model with a black-box neural network ftited to the residuals of the white-box model. D3 supports both discovering only white-box models and hybrid models, and we leave this choice up to the end user to decide for their application. Specifically, a user must provide a dataset of individual\u2019s trajectories $\\mathcal{D}$ , and a system description and the names and descriptions of any features that exist within the dataset, which can include the ranges of those features. Prior information can be added here by the practitioner into the system description if it is available. We now discuss how the three agents of Modeling, Feature Acquisition and Evaluation interact to discover iteratively better-fitting $f$ models. We provide full implementation details for the framework in Appendix F. ", "page_idx": 2}, {"type": "text", "text": "3.2 Modeling Agent $G$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "D3\u2019s Modeling Agent $G$ is tasked to iteratively propose and refine the temporal model $f_{i}$ , where $i=1,\\dots,I$ indicates the iteration. We design $G$ following the three principles below. ", "page_idx": 2}, {"type": "table", "img_path": "KIrZmlTA92/tmp/71eebd019e96546a3d1b7ddd12e64da5071a9e2b67361b2dfd291e4913b09786.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Representing the model $f_{i}$ as code. As we discussed in Section 1, one key challenge in evidencebased refinement is the vast space of possible ODE models, ranging from closed-form ODEs to complex neural Neural ODEs and hybrid models. As a prerequisite to finding the most suitable model, we need a consistent and flexible way to represent these diverse models such that they can be trained and validated on data. As such, D3 represents the model $f_{i}$ as code in Python language. Specifically, each model is defined as a class derived from \u2018nn.Module\u2019 in PyTorch [Paszke et al., 2017] that specifies the free parameters and the computation logic. The Modeling Agent $G$ leverages an LLM to generate the model code $f_{i}$ in each step. ", "page_idx": 3}, {"type": "text", "text": "Informing generation with context $c$ . Effectively searching through the vast model space presents another challenge. Thankfully, in many applications there exists rich context $c$ about prior works and domain knowledge. However, this rich source of information was previously locked in unstructured documents and texts, making it hard for automated algorithms to make use of it. By leveraging the LLM\u2019s capability to understand unstructured documents, the Modeling Agent $G$ can now access and leverage this information, therefore proposing more informed models and searching more effectively. In this work, we consider the provision of the following categories of problem context of {system description} and {feature description} (Appendix F). ", "page_idx": 3}, {"type": "text", "text": "Iterative model improvements based on rich verbal feedback $r_{i-1}$ and memory $s_{i-1}$ . To enhance the quality of the generated model $f_{i}$ , we have designed the agent $G$ to iteratively improve the model based on the presence of any evaluation verbal feedback and its knowledge of the previously generated models and how they performed on a validation subset of given dataset $\\mathcal{D}$ of trajectories. At each iteration $i$ , the Evaluation Agent $E$ evaluates the previous model $f_{i-1}$ using the validation data subset of $\\mathcal{D}_{i}$ and provides rich verbal feedback $r_{i-1}$ (c.f. Figure 1), which forms the foundation for the next generated model $f_{i}$ . The Modelling Agent $G$ utilizes memory $s_{i-1}$ to track the top- $\\cdot\\mathbf{k}$ best-performing models so far, along with their associated evaluation feedback, here their respective losses. Further details on evaluation and data acquisition are discussed in the following section. This iterative process allows for continuous refinement and optimization of models based on their past performance and targeted feedback. ", "page_idx": 3}, {"type": "text", "text": "In summary, the Modeling Agent $G$ encapsulates a comprehensive range of model structures through computer code $f_{i}$ and efficiently explores the vast model space by integrating unstructured prior knowledge $c$ , and utilizing data-informed feedback $r_{i-1}$ along with memory $s_{i-1}$ . The operation of $G$ can be mathematically represented as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{i}=G(c,f_{i-1},\\mathcal D_{i},r_{i-1},s_{i-1})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Feature Acquisition Agent $A$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "D3\u2019s Feature Acquisition Agent $A$ is tasked with iteratively proposing new features for acquisition. These features are selected based on their potential to improve the next model\u2019s performance in modeling the target state ${\\bf x}_{n}(t)$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{i}=A(c,f_{i},\\mathcal{D}_{i},r_{i-1},s_{i-1})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $h_{i}$ is the next feature to acquire. Specifically, when a feature is acquired we make the assumption, that it is acquired for all individuals, therefore at the next iteration the dimension of the individual-level features $\\mathbf{a}_{n}(t)\\in\\mathbb{R}^{K},n=1,\\ldots,N$ increases by one, to $\\mathbf{a}_{n}(t)\\in\\mathbb{R}^{K+1},n=$ $1,\\ldots,N$ . It is possible to do this in clinical trials, where pharmacokinetic datasets of patients are collected [Dziura et al., 2013], for example acquiring the individual\u2019s ages, or other lab biomarker measurements or tests [Derraik et al., 2021]. ", "page_idx": 4}, {"type": "text", "text": "Estimating the value of the feature. We leverage the value of information [Feltham, 1968] framework to provide a principled way to determine which feature, if any to acquire next. This follows as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV(h_{i})=\\mathbb{E}[L(f_{i},h_{i},\\mathcal{D}_{i-1})]-\\mathbb{E}[L(f_{i},\\mathcal{D}_{i-1})]-l(h_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the first two terms capture the improvements in validation loss $L$ when model $f_{i}$ is trained on the new dataset that includes the new feature, that is $\\mathcal{D}=\\{(\\mathbf{x}_{n}(t),\\mathbf{a}_{n}(t),\\mathbf{h}_{n}(t))|\\forall n\\in[N]\\}$ and $l(h_{i})$ represents the cost for acquiring $h_{i}$ . Estimating $V(h_{i})$ presents a statistical challenge and a computational challenge. Firstly, in practice, we often do not have access to $h_{i}$ to begin with (as the goal is to acquire new features)\u2014hence we need a way to estimate $\\mathbb{E}[L(f_{i},h_{i},\\mathscr{D}_{i-1})]$ without fully accessing $h_{i}$ . Secondly, even when $h_{i}$ is available, computing the two loss terms involves ftiting models twice which can be computationally challenging. ", "page_idx": 4}, {"type": "text", "text": "To address these challenges, the Feature Acquisition Agent $A$ predicts the value of feature $V(h_{i})$ from the available information about $h_{i}$ , as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{V}(h_{i})=g(c_{h_{i}},T(h_{i}),\\mathcal{D}_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c_{h_{i}}$ represents the unstructured prior information about the feature $h_{i}$ (e.g. data descriptions and metadata), $T(h_{i})$ represents the available summary statistics (e.g. range and type of values from a small subset of data $h_{i}$ ), $\\mathcal{D}_{i-1}$ represents the existing data, and $g$ is the prediction function. ", "page_idx": 4}, {"type": "text", "text": "The prediction challenge above corresponds to zero-shot or few-shot learning in ML. In the zero-shot case, we need to predict $\\hat{V}(h_{i})$ without having access to any \u201clabel\u201d $V(h)$ , for some feature $h$ . In the few-shot case, we have access to a small number of feature-label pairs, $\\left\\{{{c_{h_{i}}},T(h_{i}),h_{i-1}},{V(h_{i-1})}\\right\\}$ to learn the prediction function $g$ . The Feature Acquisition Agent $A$ leverages LLM\u2019s capability in zero-shot and few-shot learning to address this estimation challenge. Specifically, we inform the LLM by providing high-level statistics $T(h_{i})$ and a description of the feature $c_{h_{i}}$ in the form of the feature name to warm-start the zero-shot learning. When the $h_{i}$ has been included in the data, we inform the LLM about the validation loss associated with those data $v(h_{i})$ and add it to the few-shot examples to guide the estimation. ", "page_idx": 4}, {"type": "text", "text": "Context, feedback and memory. Similar to $G,\\,A$ is also able to leverage unstructured problem context $c$ , evaluation feedback $r_{i-1}$ , and memory $s_{i-1}$ to guide the data acquisition proposal. ", "page_idx": 4}, {"type": "text", "text": "3.4 The Evaluation Agent $E$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The Evaluation Agent $E$ is vital for iteratively improving the Modeling Agent $G$ and Feature Acquisition Agent $A$ . It provides feedback $r_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{i}=E(c,f_{i},D_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Feedback $r_{i}$ can be numeric, such as validation loss or mean squared error (MSE) of the model $f_{i}$ on the validation subset $\\mathcal{D}_{i}$ , or more refined metrics like validation loss per target dimension in $D$ . It can also be textual, offering detailed suggestions for model improvements, including clinically plausible modifications generated by an LLM [Shinn et al., 2024]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Evaluating dynamical systems is complex due to intricate interactions and temporal dependencies. The Evaluation Agent dynamically assesses both model performance and plausibility, ensuring models are both accurate and interpretable. This comprehensive evaluation is crucial for developing sophisticated, clinically relevant pharmacokinetic models that address real-world complexities. ", "page_idx": 5}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Table 1: Comparison with related works in addressing the Modeling and Data Acquisition Challenges. An empty field means not applicable. Refinement Level: the level of refinement (R1 - R3) the method can capture. Class of $f$ : the class of ODE model $f$ (closed-form $\\mathcal{M}_{C}$ , neural networks $\\mathcal{M}_{N_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ ). Context $c$ : whether the method leverages unstructured contextual information to guide search. Sample $N$ : whether the method is able to acquire new samples. Feature h: whether the method can acquire new feature variables. Goal: whether the goal of data acquisition is to improve training, evaluation, or both. ", "page_idx": 5}, {"type": "table", "img_path": "KIrZmlTA92/tmp/0b93ebdc98e1d9cbaaf7012b14c5bfacc0df8b72e1b06643f051006040c0ae2c.jpg", "table_caption": [], "table_footnote": ["Our work focuses on autonomously learning temporal models while acquiring data, with several relevant research strands, as summarized in Table 1, which is expanded in Appendix A. "], "page_idx": 5}, {"type": "text", "text": "ODE learning methods. Symbolic regression methods like SINDy and D-CODE [Koza, 1994, Brunton et al., 2016, Qian et al., 2022] can discover closed-form ODEs $f\\,\\in\\,\\mathcal{M}_{C}$ using genetic algorithms. They can incorporate features ${\\bf a}_{n}$ (R2 refinement) but struggle with many variables (e.g., ${>}20$ ) due to computational complexity. Neural ODEs use neural networks $f\\in\\mathcal{M}_{N}$ to handle many variables [Chen et al., 2018, Dupont et al., 2019, Zaytar and El Amrani, 2016, Devlin et al., 2018, Sehovac and Grolinger, 2020], but have numerous free parameters, risking overfitting with insufficient data. D3 captures $f\\in\\mathcal{M}_{C}\\cup\\mathcal{M}_{N}$ and supports R1 - R3 refinement, adapting based on data availability. Both Symbolic regression and Neural ODEs require manual context incorporation $c$ via hyperparameters and do not address Data Acquisition Challenges. ", "page_idx": 5}, {"type": "text", "text": "AI for automated modeling. AI Feynman [Udrescu and Tegmark, 2020] enhances symbolic regression by using the physical units of variables, which provide additional constraints to narrow the search space. However, its utility outside physics is limited, as units in other fields carry less information. Eureqa [Ma et al., 2023] applies LLMs to model the reward function in reinforcement learning, integrating unstructured context and representing models as code. Unlike D3, Eureqa cannot automatically refine models, acquire new data, or easily apply to learning temporal dynamics like ODEs. ", "page_idx": 5}, {"type": "text", "text": "Data Acquisition. Methods like Active Feature Acquisition (AFA) [Ma et al., 2018, Gong et al., 2019] measure additional features on existing samples to improve performance. Active Learning [Sebastiani and Wynn, 2000, Settles, 2009, Sener and Savarese, 2017, Imberg et al., 2020] and Active Testing [Lowell et al., 2018, Kossen et al., 2021] acquire new samples to increase training or evaluation data size while keeping features constant. These methods do not address the zero-shot setting, where data acquisition decisions must be made before observing any target data (Equation 7). Additionally, most Active Learning methods focus on acquiring labels for supervised learning, which is not directly applicable to temporal modeling tasks. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments and Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we demonstrate that D3 can discover well-ftiting pharmacokinetic dynamical system models for a range of diverse PK datasets, including an epidemiological and ecological dataset. ", "page_idx": 5}, {"type": "text", "text": "Benchmark Datasets. Our evaluation encompasses six real-world datasets with clinical relevance, each originating from either real-world data or highly accurate simulators developed by human experts. ", "page_idx": 5}, {"type": "text", "text": "Three datasets are based on a state-of-the-art biomedical Pharmacokinetic-Pharmacodynamic (PKPD) model of lung cancer tumor growth, which simulates the combined effects of chemotherapy and radiotherapy in lung cancer [Geng et al., 2017] (Equation (11)). This model has been widely utilized in previous research [Bica et al., 2020, Seedat et al., 2022, Melnychuk et al., 2022]. Specifically, we employ this bio-mathematical model to generate three variations: lung cancer without treatment (Lung Cancer), lung cancer treated with chemotherapy (Lung Cancer (with Chemo.)), and lung cancer treated with both chemotherapy and radiotherapy (Lung Cancer (with Chemo. & Radio.)). Additionally, we utilize an intricate COVID-19 epidemic agent-based simulator (COVID-19) [Kerr et al., 2021], to provide an epidemiological dataset. Another dataset comes from an ecological model simulating a microcosm of algae, flagellate, and rotifer populations (Plankton Microcosm), replicating a three-species prey-predator experimental system [Hiltunen et al., 2013]. Finally, we include a real Pharmacokinetic (PK) dataset of Warfarin patients (Warfarin) [Janssen et al., 2022]. Detailed information about all benchmark datasets is provided in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Benchmark Methods. To evaluate the performance of D3 and establish its competitive performance, we conduct comparisons with leading modeling methods for ODEs. Specifically, we benchmark against advanced black-box models that have many parameters, such as neural ODEs with action inputs, known as DyNODE [Chen et al., 2018, Alvarez et al., 2020]. Also in this class are recurrent neural networks (RNN) and a state-of-the-art transformer model (Transformer). In addition, we include white-box transparent dynamical systems models identified through equation discovery techniques, such as Sparse Identification of Nonlinear Dynamics (SINDy) [Brunton et al., 2016]. Moreover, D3 supports two discovery modes, of which we compare against both, discovering only white-box models (D3-white-box) and discovering hybrid models (D3-hybrid)3. We also perform ablations of D3, of zero-shot generated model from D3 as (ZeroShot) and the same model with optimized parameters (ZeroOptim). Detailed descriptions of the implementations, hyperparameters, and experimental procedures for these benchmarks can be found in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. To assess the performance of our benchmark methods, we use the mean squared error (MSE) on a held-out test dataset of state-action trajectories. This evaluation is conducted over ten runs, each initialized with different random seeds. We report the average MSE from these runs along with their $95\\%$ confidence intervals. Further details can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "6 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conducted a comprehensive evaluation of our benchmark methods across all datasets, as tabulated in Table 2. We observe that D3 can discover well-fitting dynamical system models, achieving low mean squared error in test predictions on the held-out test dataset of individual trajectories. Crucially, it can discover concise closed-form equation white-box models, of 5-15 parameters that can outperform some of the standard white-box and black-box modeling methods, indicating it is discovering well-fitting underlying equations for the respective systems of interest. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Evaluating Method Performance. We report the test prediction mean squared error (MSE) on held-out datasets across benchmarks. D3 consistently has the lowest error. Results are averaged over ten random seeds with $95\\%$ confidence intervals. ", "page_idx": 6}, {"type": "table", "img_path": "KIrZmlTA92/tmp/c544e0338691ed4a88e3de71dacb229dc85adf2ff19fbf5d602ac96d7b15b4d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "6.1 Case Study: Discovering Novel PK Models for Warfarin ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "PK models of warfarin, a widely used anticoagulant, are crucial due to its extensive use in treating deep vein thrombosis, pulmonary embolism, and stroke prevention in atrial fibrillation and patients with mechanical heart valves, with millions of prescriptions globally [Lee and Klein, 2013]. We applied D3 to a public dataset of 33 patient trajectories [Janssen et al., 2022], discovering a novel PK model for warfarin that outperforms existing literature (Table 3). We compare the test MSE of the discovered model against existing models and provide pharmacologist feedback. Full model details and results from running D3-hybrid on the same dataset are in Appendix G. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Existing Wafarin PK Model. The standard pharmacokinetic (PK) warfarin model from the literature [Lv et al., 2017, Hamberg, 2013] is the following: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{\\frac{d C}{d t}=k_{a}\\cdot D-k_{e}\\cdot C,}}\\\\ {\\displaystyle{k_{e}=k_{e,\\mathrm{base}}+k_{e,\\mathrm{age}}\\cdot A+k_{e,\\mathrm{sex}}\\cdot S}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "table", "img_path": "KIrZmlTA92/tmp/cb58bae667ca5a3da6bb0af43f16254ef4c95d40f14b18a9e98d9d11131650b2.jpg", "table_caption": ["Table 3: Warfarin Modeling Comparison "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "where $C$ is the concentration of warfarin, $D$ is the dosage administered, $A$ represents the age of the patient, $S$ denotes the sex of the patient (1 for male, 0 for female), $k_{a}$ is the absorption rate, $k_{e,\\mathrm{base}}$ is the base elimination rate, $k_{e,\\mathrm{age}}$ represents the decrease in elimination rate per year increase in age, and $k_{e,\\mathrm{sex}}$ is the difference in elimination rate between sexes. This standard model achieves a test loss of 0.646. Whereas D3 can discover both a white-box model with a test loss of 0.39 and a hybrid model with a test loss of 0.271, which both outperform the standard model. Of particular relevance is the white-box model as it is fully interpretable by pharmacometricians, whilst still being a precise model. ", "page_idx": 7}, {"type": "text", "text": "New Discovered PK Warfarin Model. D3-white-box discovered a new warfarin PK white-box model with a test loss of 0.39, of the following: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d C}{d t}=\\sqrt{D}-k_{\\mathrm{eff}}\\cdot\\frac{C}{K_{m}+C},}\\\\ {\\displaystyle k_{\\mathrm{eff}}=k_{e,\\mathrm{base}}+k_{e,\\mathrm{age}}\\cdot(A-\\overline{{A}})+k_{e,\\mathrm{sex}}\\cdot(S-\\overline{{S}})}\\\\ {\\displaystyle\\qquad+\\,k_{\\mathrm{decay}}\\cdot C+k_{d s}\\cdot D\\cdot(S-\\overline{{S}})}\\\\ {\\displaystyle\\qquad+\\,k_{a s}\\cdot(A-\\overline{{A}})\\cdot(S-\\overline{{S}})+k_{a d}\\cdot D\\cdot(A-\\overline{{A}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the additional parameters $k_{\\mathrm{decay}}$ represent the natural decay rate of warfarin concentration and $K_{m}$ is the Michaelis constant indicating the warfarin concentration at which the metabolism rate is half its maximum. The parameters $k_{\\mathrm{ds}}$ , $k_{\\mathrm{as}}$ , and $k_{\\mathrm{da}}$ are the interaction terms for dosage-sex, age-sex, and dosage-age, respectively, and $\\overline{{A}}$ and $\\overline{S}$ are the sample population means for the covariates of age and sex. This new PK model includes the original parameters along with additional interaction terms and transformations, enhancing the model\u2019s complexity and accuracy. ", "page_idx": 7}, {"type": "text", "text": "The model discovered by D3 introduces key innovations in warfarin pharmacokinetics. Square Root Transformation for Dosage Effect: This non-linear transformation moderates high doses, enhancing sensitivity to dosage variations, unlike standard models. Natural Decay Term: Proportional to the current warfarin concentration, it better reflects elimination kinetics and improves time-based predictions. Michaelis-Menten Saturation Kinetics: Addresses metabolic pathway saturation at higher concentrations, crucial for warfarin. Interaction Terms (dosage-sex, age-sex, dosage-age): Capture complex interactions between patient-specific factors, providing a nuanced understanding of drug dynamics. These advancements make D3 a powerful tool for enhancing pharmacokinetic predictions in clinical settings. The discovered hybrid model is detailed in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Expert Clinical Commentary. We sought feedback from expert pharmacologists on the discovered model4: Prof. Eoin McKinney, Clinician. \u201cThis model is significant, as consortiums are dedicated to improving Warfarin modeling [Consortium, 2009]. The model adds novel components, such as the Michaelis component for time-varying changes and novel interaction terms like age-sex.\u201d Jean-Baptiste Woillard, Pharmacologist. \u201cThe model is promising and pharmacokinetically plausible. The next step is to apply D3 to other clinically relevant PK drug datasets.\u201d Richard Peck, Clinical Pharmacologist. \u201cThis model is reasonable and potentially superior. It represents a significant advance in clinical pharmacology by automatically identifying robust PK models.\u201d ", "page_idx": 7}, {"type": "text", "text": "6.2 Insight Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section provides an in-depth analysis of D3\u2019s effectiveness related to its benchmark counterparts. ", "page_idx": 7}, {"type": "text", "text": "Can D3 perform feature acquisition and leverage the LLM prior information to perform this better? To explore the feature acquisition performance of D3, we showcase this component working in full, by comparing it against the baseline of a random feature acquirer policy, a null policy that just uses the existing features and our approach of D3 that leverages the LLM to quantify the value of information for features, where no training data exists for them in the dataset. We observe, as in Figure 2, that D3 converges the fastest, and achieves the overall highest performance, whereas the other feature acquisition methods fall short. Experimental details are in Appendix G.2. ", "page_idx": 7}, {"type": "image", "img_path": "KIrZmlTA92/tmp/de3def72ffff887495ced5ebca6e1c7bc193e73e6ae8a0288e1feee47e81e6e8.jpg", "img_caption": ["Figure 2: Feature Acquisition. D3 performing adaptive feature acquisition in the Lung Cancer (with Chemo. & Radio.) dataset. We observe that D3 still achieves the lowest test prediction error. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "KIrZmlTA92/tmp/21d720ec88675d55fec80fd25ceada83a58b27ed62079db2bce4969f7a150a5d.jpg", "img_caption": ["Figure 3: D3 iteratively discovers better models $f_{i}$ . Validation MSE of the model generated in each iteration, showing the best-generated model (Top-1 model)\u2014additionally with a few of the models labeled with their model descriptions, and features acquired at the generation. D3 can effectively acquire and integrate new features, validate, and compare models to achieve a better-fitting model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Can D3 evolve its modular model to fit the system best? We analyze this from an empirical point of view to determine if D3 can correctly evolve the generated model and reduce its prediction error over subsequent generations. We observe that D3 can indeed understand, reason, and iterate the generated code representation of the model to incorporate a better fitting model, as observed in Figure 3. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Summary. In this paper, we proposed Data-Driven Discovery $(D3)$ framework, to iteratively discover and refine interpretable models of pharmacological dynamics, where it has the ability to acquire and integrate new features, validate, and compare pharmacological dynamical systems models, thereby uncovering new insights into pharmacokinetic and physiological processes. Specifically, applied to a real Warfarin PK dataset we were able to discover a better performing Warfarin PK model and provide new insights into what an optimal PK model for Warfarin may need to possess, such as additional Michaelis components. ", "page_idx": 8}, {"type": "text", "text": "Limitations & Future work. There exist limitations to the current approach. First, the LLM discovery framework is an initial framework, and the utility of the LLM to generate better models could be enhanced, tree-based generation strategies such as the tree of thought [Yao et al., 2024], or graph of thought [Besta et al., 2024]. Specifically, it relies on a capable enough LLM, that can use tools, where we specifically used GPT4 as the underlying LLM. Moreover, we make the assumption that when D3 acquires a new feature it acquires that feature for all the individuals within the existing dataset, we leave for future work to consider applications where this assumption may not always be true. Furthermore, we provide to the LLM a system description from a user, future works could explore avoiding the user to provide this, and instead leveraging a form of automatic retrieval augmented generation [Lewis et al., 2020]. ", "page_idx": 8}, {"type": "text", "text": "Broader Impact and Ethical Considerations. Principally D3 aims to discover interpretable models of pharmacological dynamics. However, the final discovered models should always be checked by appropriate human experts and validated in additional held-out datasets before any clinical use. Furthermore, D3 is a tool, which could be misused by a malicious user with unethical system descriptions as input to discover a potentially biased model. Moreover, D3 leverages LLMs and their feedback as an integral component, however, LLMs are prone to hallucinations, thereby motivating any LLM-generated outputs that are shown to the user should have a content filter applied to them. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We extend our gratitude to the anonymous reviewers, area and program chairs, members of the van der Schaar lab, and Andrew Rashbass for their valuable feedback and suggestions. SH gratefully acknowledges the sponsorship and support of AstraZeneca. ZQ did this work whilst a postdoc in the van der Schaar lab. This work was funded by Microsoft\u2019s Accelerate Foundation Models Academic Research initiative. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Victor M Martinez Alvarez, Rares\u00b8 Ros\u00b8ca, and Cristian G Fa\u02d8lcu\u00b8tescu. Dynode: Neural ordinary differential equations for dynamics modeling in continuous control. arXiv preprint arXiv:2009.04278, 2020. ", "page_idx": 9}, {"type": "text", "text": "Pierre Auger, Pierre Magal, and Shigui Ruan. Structured population models in biology and epidemiology, volume 1936. Springer, 2008.   \nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682\u201317690, 2024.   \nIoana Bica, Ahmed M. Alaa, James Jordon, and Mihaela van der Schaar. Estimating counterfactual treatment outcomes over time through adversarially balanced representations. In International Conference on Learning Representations, 2020.   \nLynne Billard and Edwin Diday. Symbolic regression analysis. In Classification, clustering, and data analysis: recent advances and applications, pages 281\u2013288. Springer, 2002.   \nWillem Bonnaff\u00e9 and Tim Coulson. Fast ftiting of neural ordinary differential equations by bayesian neural gradient matching to infer ecological interactions from time-series data. Methods in Ecology and Evolution, 14(6):1543\u20131563, 2023.   \nSteven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):3932\u20133937, 2016.   \nAlex Chan and Mihaela van der Schaar. Synthetic model combination: An instance-wise approach to unsupervised ensemble learning. Advances in Neural Information Processing Systems, 35: 27797\u201327809, 2022.   \nBing Chen and Adnan OM Abuassba. Compartmental models with application to pharmacokinetics. Procedia Computer Science, 187:60\u201370, 2021.   \nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \nLimei Cheng, Yuchi Qiu, Brian J Schmidt, and Guo-Wei Wei. Review of applications and challenges of quantitative systems pharmacology modeling and machine learning for heart failure. Journal of pharmacokinetics and pharmacodynamics, pages 1\u201312, 2022.   \nInternational Warfarin Pharmacogenetics Consortium. Estimation of the warfarin dose with clinical and pharmacogenetic data. New England Journal of Medicine, 360(8):753\u2013764, 2009.   \nJos\u00e9 GB Derraik, Wason Parklak, Benjamin B Albert, Kongsak Boonyapranai, and Kittipan Rerkasem. Fundamentals of data collection in clinical studies: simple steps to avoid \u201cgarbage in, garbage out\u201d. The International Journal of Lower Extremity Wounds, 20(3):183\u2013187, 2021. ", "page_idx": 9}, {"type": "text", "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. ", "page_idx": 10}, {"type": "text", "text": "Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "James D Dziura, Lori A Post, Qing Zhao, Zhixuan Fu, and Peter Peduzzi. Strategies for dealing with missing data in clinical trials: from design to analysis. The Yale journal of biology and medicine, 86(3):343, 2013. ", "page_idx": 10}, {"type": "text", "text": "Gerald A Feltham. The value of information. The accounting review, 43(4):684\u2013696, 1968. ", "page_idx": 10}, {"type": "text", "text": "Johan Gabrielsson and Daniel Weiner. Pharmacokinetic and pharmacodynamic data analysis: concepts and applications. 2001. ", "page_idx": 10}, {"type": "text", "text": "Changran Geng, Harald Paganetti, and Clemens Grassberger. Prediction of Treatment Response for Combined Chemo- and Radiation Therapy for Non-Small Cell Lung Cancer Patients Using a Bio-Mathematical Model. Scientific Reports, 7(1):13542, October 2017. ISSN 2045-2322. doi: 10.1038/s41598-017-13646-z. ", "page_idx": 10}, {"type": "text", "text": "Wenbo Gong, Sebastian Tschiatschek, Sebastian Nowozin, Richard E Turner, Jos\u00e9 Miguel Hern\u00e1ndezLobato, and Cheng Zhang. Icebreaker: Element-wise efficient information acquisition with a bayesian deep latent gaussian model. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "Alex Graves, Santiago Fern\u00e1ndez, and J\u00fcrgen Schmidhuber. Multi-dimensional recurrent neural networks. In International conference on artificial neural networks, pages 549\u2013558. Springer, 2007. ", "page_idx": 10}, {"type": "text", "text": "FDA Draft Guidance. Adaptive design clinical trials for drugs and biologics. Biotechnol Law Rep, 29 (2):173, 2010. ", "page_idx": 10}, {"type": "text", "text": "Anna-Karin Hamberg. Pharmacometric models for individualisation of warfarin in adults and children. PhD thesis, Acta Universitatis Upsaliensis, 2013. ", "page_idx": 10}, {"type": "text", "text": "Mohsen A Hedaya. Basic pharmacokinetics. CRC Press, 2012. ", "page_idx": 10}, {"type": "text", "text": "Teppo Hiltunen, LE Jones, SP Ellner, and Nelson G Hairston Jr. Temporal dynamics of a simple community with intraguild predation: an experimental test. Ecology, 94(4):773\u2013779, 2013. ", "page_idx": 10}, {"type": "text", "text": "Samuel Holt, Alihan H\u00fcy\u00fck, Zhaozhi Qian, Hao Sun, and Mihaela van der Schaar. Neural laplace control for continuous-time delayed systems. In International Conference on Artificial Intelligence and Statistics, pages 1747\u20131778. PMLR, 2023a. ", "page_idx": 10}, {"type": "text", "text": "Samuel Holt, Zhaozhi Qian, and Mihaela van der Schaar. Deep generative symbolic regression. In The Eleventh International Conference on Learning Representations, 2023b. URL https: //openreview.net/forum?id=o7koEEMA1bR. ", "page_idx": 10}, {"type": "text", "text": "Samuel Holt, Alihan H\u00fcy\u00fck, and Mihaela van der Schaar. Active observing in continuous-time control. Advances in Neural Information Processing Systems, 36, 2024a. ", "page_idx": 10}, {"type": "text", "text": "Samuel Holt, Tennison Liu, and Mihaela van der Schaar. Automatically learning hybrid digital twins of dynamical systems. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id $\\equiv$ SOsiObSdU2. ", "page_idx": 10}, {"type": "text", "text": "Henrik Imberg, Johan Jonasson, and Marina Axelson-Fisk. Optimal sampling in unbiased active learning. In International Conference on Artificial Intelligence and Statistics, pages 559\u2013569. PMLR, 2020.   \nAlexander Janssen, Frank C Bennis, and Ron AA Math\u00f4t. Adoption of machine learning in pharmacometrics: An overview of recent implementations and their considerations. Pharmaceutics, 14(9): 1814, 2022.   \nKrzysztof Kacprzyk, Samuel Holt, Jeroen Berrevoets, Zhaozhi Qian, and Mihaela van der Schaar. ODE discovery for longitudinal heterogeneous treatment effects inference. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=pxI5IPeWgW.   \nCliff C Kerr, Robyn M Stuart, Dina Mistry, Romesh G Abeysuriya, Katherine Rosenfeld, Gregory R Hart, Rafael C N\u00fa\u00f1ez, Jamie A Cohen, Prashanth Selvaraj, Brittany Hagedorn, et al. Covasim: an agent-based model of covid-19 dynamics and interventions. PLOS Computational Biology, 17(7): e1009149, 2021.   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nJannik Kossen, Sebastian Farquhar, Yarin Gal, and Tom Rainforth. Active testing: Sample-efficient model evaluation. In International Conference on Machine Learning, pages 5753\u20135763. PMLR, 2021.   \nJohn R Koza. Genetic programming as a means for programming computers by natural selection. Statistics and computing, 4:87\u2013112, 1994.   \nSiddharth Krishna Kumar. On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863, 2017.   \nMing Ta Michael Lee and Teri E Klein. Pharmacogenetics of warfarin: challenges and opportunities. Journal of human genetics, 58(6):334\u2013338, 2013.   \nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459\u20139474, 2020.   \nJundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino, Jiliang Tang, and Huan Liu. Feature selection: A data perspective. ACM computing surveys (CSUR), 50(6):1\u201345, 2017.   \nErnest Lindel\u00f6f. Sur l\u2019application de la m\u00e9thode des approximations successives aux \u00e9quations diff\u00e9rentielles ordinaires du premier ordre. Comptes rendus hebdomadaires des s\u00e9ances de l\u2019Acad\u00e9mie des sciences, 116(3):454\u2013457, 1894.   \nDavid Lowell, Zachary C Lipton, and Byron C Wallace. Practical obstacles to deploying active learning. arXiv preprint arXiv:1807.04801, 2018.   \nChunxiao Lv, Changxiao Liu, Zhuhua Yao, Xiumei Gao, Lanjun Sun, Jia Liu, Haibo Song, Ziqiang Li, Xi Du, Jinxia Sun, et al. The clinical pharmacokinetics and pharmacodynamics of warfarin when combined with compound danshen: a case study for combined treatment of coronary heart diseases with atrial fibrillation. Frontiers in Pharmacology, 8:826, 2017.   \nChao Ma, Sebastian Tschiatschek, Konstantina Palla, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Sebastian Nowozin, and Cheng Zhang. Eddi: Efficient dynamic discovery of high-value information with partial vae. arXiv preprint arXiv:1809.11142, 2018.   \nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023.   \nValentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. Causal transformer for estimating counterfactual outcomes. In International Conference on Machine Learning, pages 15293\u201315329. PMLR, 2022.   \nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.   \nZhaozhi Qian, Krzysztof Kacprzyk, and Mihaela van der Schaar. D-CODE: Discovering closed-form ODEs from observed trajectories. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $=$ wENMvIsxNN.   \nDavid K Ryan, Rory H Maclean, Alfred Balston, Andrew Scourfield, Anoop D Shah, and Jack Ross. Artificial intelligence and machine learning for clinical pharmacology. British Journal of Clinical Pharmacology, 90(3):629\u2013639, 2024.   \nWilliam E Schiesser. Computational mathematics in engineering and applied science: ODEs, DAEs, and PDEs. CRC press, 2014.   \nPaola Sebastiani and Henry P Wynn. Maximum entropy sampling and optimal bayesian experimental design. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 62(1):145\u2013157, 2000.   \nNabeel Seedat, Fergus Imrie, Alexis Bellot, Zhaozhi Qian, and Mihaela van der Schaar. Continuoustime modeling of counterfactual outcomes using neural controlled differential equations. arXiv preprint arXiv:2206.08311, 2022.   \nLjubisa Sehovac and Katarina Grolinger. Deep learning for load forecasting: Sequence to sequence recurrent neural networks with attention. Ieee Access, 8:36411\u201336426, 2020.   \nOzan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489, 2017.   \nBurr Settles. Active learning literature survey. 2009.   \nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \nShruti Singh, Rajesh Kumar, Shuvasree Payra, and Sunil K Singh. Artificial intelligence and machine learning in pharmacological research: Bridging the gap between data and drug discovery. Cureus, 15(8), 2023.   \nSilviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631, 2020.   \nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \nMohamed Akram Zaytar and Chaker El Amrani. Sequence to sequence weather forecasting with long short-term memory recurrent neural networks. International Journal of Computer Applications, 143(11):7\u201311, 2016. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Additional Related Work 15 ", "page_idx": 13}, {"type": "text", "text": "B Benchmark Dataset Environment Details 15 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Cancer PKPD 15   \nB.2 COVID-19 16   \nB.3 Plankton Microcosm 17   \nB.4 PK Wafarin Dataset 17   \nC Evaluation Metrics 17   \nC.1 Model Optimization Losses 18 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "D Pharmacologists Feedback Statements 18 ", "page_idx": 13}, {"type": "text", "text": "E Benchmark Method Implementation Details 19 ", "page_idx": 13}, {"type": "text", "text": "D3 Implementation Details 20   \nF.1 D3 Pseudocode 20   \nF.2 Training Models 20   \nF.3 System Description 21   \nF.4 D3 Prompt Templates 21   \nF.5 D3 System Description Prompts . . 21   \nG Additional Experiments 21   \nG.1 Warfarin Case Study Additional Results 21   \nG.2 Insight Experiment: Feature Acquisition Setup 23   \nG.3 Other Datasets Discovered White-Box Models 32 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We expand on the related work contained within the main paper. ", "page_idx": 14}, {"type": "text", "text": "ODE learning methods. Symbolic regression methods [Koza, 1994, Kacprzyk et al., 2024, Holt et al., 2023b] such as SINDy and D-CODE [Brunton et al., 2016, Qian et al., 2022] can automatically discover closed form ODEs $f\\in\\mathcal{M}_{C}$ through genetic algorithms. In principle, they can incorporate features ${\\bf a}_{n}$ , achieving refinement R2, but they struggle with many variables (e.g., more than 20) due to computational complexity. On the other hand, Neural ODE leverages neural networks $f\\,\\in\\,\\mathcal{M}_{N}$ to capture a large number of variables [Chen et al., 2018, Dupont et al., 2019, Zaytar and El Amrani, 2016, Devlin et al., 2018, Sehovac and Grolinger, 2020, Holt et al., 2022, 2023a, 2024a]. However, Neural ODEs involve many free parameters and cannot automatically reduce the level of refinement when there is insufficient training data, leading to overfitting. In comparison, D3 can capture $f\\in\\mathcal{M}_{C}\\cup\\mathcal{M}_{N}$ and $\\mathrm{R}1\\boldsymbol{\\mathrm{~-~}}\\mathrm{R}3$ levels of refinement; it can further adapt the level of refinement based on data availability. Furthermore, in both Symbolic regression and Neural ODE, human experts need to manually incorporate the problem context $c$ through the specification of many hyperparameters. Finally, neither of these methods assumes fixed training data and does not address the Data Acquisition Challenges. ", "page_idx": 14}, {"type": "text", "text": "AI for automated modeling. AI Feynman [Udrescu and Tegmark, 2020] improves upon the symbolic regression methods by leveraging a specific type of information $c$ , i.e. the physical unit of each variable (e.g., meter). In applications in physics, knowing the units provides additional constraints on the variable\u2019s relationship, thus helping narrow down the search space. However, its utility outside physical sciences has been limited. More recently, Eureqa [Ma et al., 2023] applies LLMs to modeling the reward function of reinforcement learning algorithms. Eureqa is similar to D3 in its ability to integrate unstructured context $c$ and represent the model as code. However, Eureqa cannot automatically adjust the level of refinement or acquire new data and it does not easily apply to learning temporal dynamics, e.g. ODEs. Furthermore, other LLM code generation [Holt et al., 2024b,c] approaches cannot acquire new data. ", "page_idx": 14}, {"type": "text", "text": "Data Acquisition. Methods have been developed to address specific data acquisition challenges. Active Feature Acquisition (AFA) attempts to measure additional feature variables h on existing training samples to improve predictive performance [Ma et al., 2018, Gong et al., 2019]. On the other hand, Active Learning [Sebastiani and Wynn, 2000, Settles, 2009, Sener and Savarese, 2017, Imberg et al., 2020] and Active Testing [Lowell et al., 2018, Kossen et al., 2021] attempt to acquire new samples to increase the size of training or evaluation data $N$ while keeping the features constant. Neither field has proposed a method to address the zero-shot setting, where the data acquisition decision must be made before observing any target data (Equation 7). Moreover, most existing methods in Active Learning focus on acquiring labels for supervised learning given unlabelled samples, which is not immediately applicable to temporal modeling tasks. ", "page_idx": 14}, {"type": "text", "text": "B Benchmark Dataset Environment Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the following, we present six clinically relevant datasets, each of which is either sourced from real-world data or generated from a high-fidelity simulator developed by domain experts. ", "page_idx": 14}, {"type": "text", "text": "B.1 Cancer PKPD ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Three of our environments are derived from a state-of-the-art biomedical PharmacokineticPharmacodynamic (PKPD) model of lung cancer tumor growth, used to simulate the combined effects of chemotherapy and radiotherapy in lung cancer [Geng et al., 2017]. This model has been extensively used in other works [Seedat et al., 2022, Bica et al., 2020, Melnychuk et al., 2022]. We use this bio-mathematical lung cancer model to create three variations: no treatments (Lung Cancer), chemotherapy only (Lung Cancer (with Chemo.)), and both chemotherapy and radiotherapy (Lung Cancer (with Chemo. & Radio.)). For each model, we sample a respective dataset. Below, we detail the general case of Lung Cancer (with Chemo. & Radio.), which originates from the general Cancer PKPD Model, and then describe the variations. ", "page_idx": 14}, {"type": "text", "text": "Cancer PKPD Model. This model simulates the combined effects of chemotherapy and radiotherapy on lung cancer tumor growth [Geng et al., 2017], as shown in Equation (11). The model predicts tumor volume $x(t)$ over time $t$ in days post-diagnosis. The model includes two binary treatments: (1) radiotherapy $u_{t}^{r}$ and (2) chemotherapy $u_{t}^{c}$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d\\boldsymbol{x}(t)}{d t}=\\big(\\underbrace{\\rho\\log\\bigg(\\frac{K}{x(t)}\\bigg)}_{\\mathrm{Tumor~growth}}-\\underbrace{\\beta_{c}C(t)}_{\\mathrm{Chemotherapy}}-\\underbrace{(\\alpha_{r}d(t)+\\beta_{r}d(t)^{2})}_{\\mathrm{Radiotherapy}}\\big)\\boldsymbol{x}(t)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The parameters $K,\\rho,\\beta_{c},\\alpha_{r},\\beta_{r}$ for each simulated patient are specified in Geng et al. [2017] and summarized in Table 4. Additionally, the chemotherapy drug concentration $C(t)$ follows an exponential ", "page_idx": 15}, {"type": "table", "img_path": "KIrZmlTA92/tmp/ca11c2a41c7f864ba0af5e0787ff633f1271d51dee8bf3a5485c1126f21b6893.jpg", "table_caption": ["Table 4: Cancer PKPD parameter values. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "decay with a half-life of one day: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d C(t)}{d t}=-0.5C(t)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The chemotherapy binary action represents increasing the $C(t)$ concentration by $5.0\\mathrm{mg/m^{3}}$ of Vinblastine given at time $t$ . The radiotherapy concentration $d(t)$ represents 2.0 Gy fractions of radiotherapy given at timestep $t$ , where Gy is the Gray ionizing radiation dose. ", "page_idx": 15}, {"type": "text", "text": "Time-dependent confounding. We introduce time-varying confounding by modeling chemotherapy and radiotherapy assignment as Bernoulli random variables. The probabilities $p_{c}$ and $p_{r}$ depend on tumor diameter as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{c}(t)=\\sigma\\left(\\frac{\\gamma_{c}}{D_{\\operatorname*{max}}}(\\bar{D}(t)-\\delta_{c})\\right)\\qquad\\qquad p_{r}(t)=\\sigma\\left(\\frac{\\gamma_{r}}{D_{\\operatorname*{max}}}(\\bar{D}(t)-\\delta_{r})\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $D_{\\mathrm{max}}=13\\mathrm{cm}$ is the maximum tumor diameter, $\\delta_{c}=\\delta_{r}=D_{\\mathrm{max}}/2$ , and $\\bar{D}(t)$ is the average tumor diameter. The parameters $\\gamma_{c}$ and $\\gamma_{r}$ control the extent of time-varying confounding, with $\\gamma_{c}=\\gamma_{r}=2$ . ", "page_idx": 15}, {"type": "text", "text": "Sampling datasets. Using the Cancer PKPD model, we sample $N\\,=\\,1,000$ trajectories, corresponding to $N=10,000$ patients. Initial tumor volumes are sampled from a uniform distribution $x(0)\\sim\\mathcal{U}(0,1149)$ , and patient trajectories are forward simulated for 60 days using the Cancer PKPD Equation (11) and the action policy of Equation (13), employing a Euler stepwise solver. This process generates one dataset sample. We repeat this with independent random seeds to create $D_{\\mathrm{train}},D_{\\mathrm{val}},D_{\\mathrm{test}}$ . For each benchmark method run with a random seed, we re-sample the datasets. Each variation includes either chemotherapy, both chemotherapy and radiotherapy, or neither. We provide further details of this dataset\u2019s system description and variable descriptions in Appendix F.5. ", "page_idx": 15}, {"type": "text", "text": "B.2 COVID-19 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We utilize the sophisticated epidemic agent-based simulator COVASIM [Kerr et al., 2021] to model COVID-19 epidemics. This advanced simulator is capable of simulating both non-pharmaceutical interventions (such as lockdowns, social distancing, and school closures) and pharmaceutical interventions (such as vaccinations). In this agent-based simulator, each agent represents an individual within the population, and can be in one of several states: susceptible to COVID-19, exposed, infectious, or recovered (including deaths). ", "page_idx": 15}, {"type": "text", "text": "We use COVASIM with its default parameter settings provided by the open-source implementation 5. COVASIM simulates a population of individuals, and to ensure accuracy, we simulate 24 countries, collecting trajectories for each. For each simulation, we use a population size of 1, 000, 000 individuals, simulating each individual separately (disabling simulation rescaling). The simulation begins with a random number of individuals initially infected with COVID-19, $I(\\bar{0})=\\mathcal{U}(10,000,100,\\bar{000})$ , and proceeds for 60 days. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "We repeat this process with independent random seeds to generate $D_{\\mathrm{train}},D_{\\mathrm{val}},D_{\\mathrm{test}}$ . For each benchmark method run with a different random seed, we re-sample the datasets. Detailed descriptions of this dataset\u2019s system and variables are provided in the prompt template outlined in Appendix F.5. ", "page_idx": 16}, {"type": "text", "text": "B.3 Plankton Microcosm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This subsection describes an ecological model of a microcosm consisting of algae, flagellate, and rotifer populations, replicating an experimental three-species prey-predator system [Hiltunen et al., 2013]. We use the dataset provided by [Bonnaff\u00e9 and Coulson, 2023]6. The dataset contains a single trajectory of 102 time steps. We split the data into training, validation, and test sets with proportions of $70\\%$ , $15\\%$ , and $15\\%$ , respectively, ensuring that the splits maintain the chronological order to preserve temporal causality. ", "page_idx": 16}, {"type": "text", "text": "Detailed descriptions of this dataset\u2019s system and variables are provided in the prompt template outlined in Appendix F.5. ", "page_idx": 16}, {"type": "text", "text": "B.4 PK Wafarin Dataset ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we describe the real PK Wafarin dataset from a clinical trial that is publicly available [Janssen et al., 2022]. We use the NOMEN dataset available at https://github.com/Janssena/ SI-AIEP-paper. This publicly available dataset consists of 32 patients who received warfarin in a clinical trial, which was originally designed to determine how to predict drug concentrations in patients. The dataset includes a total of 251 warfarin concentration measurements, with a median of six measurements per patient. Each patient received a single dose of warfarin at $t=0$ , and measurements were taken at $\\bar{t}\\in\\{0.25,0.5,1.0,2.0,4.0,6.0,12.0,\\bar{2}4.0,48.0,72.0,96.0,120.0\\}.$ The available covariates in the dataset are patient weight, age, and sex. We follow the dataset\u2019s original pre-processing code, which is provided at https://github.com/Janssena/SI-AIEP-paper. We split the data into training, validation, and test sets with proportions of $70\\%$ , $15\\%$ , and $15\\%$ , respectively, ensuring that the splits maintain the chronological order to preserve temporal causality. This dataset is released under a GPL-3.0 license. ", "page_idx": 16}, {"type": "text", "text": "Detailed descriptions of this dataset\u2019s system and variables are provided in the prompt template outlined in Appendix F.5. ", "page_idx": 16}, {"type": "text", "text": "C Evaluation Metrics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We utilize mean squared error (MSE) to assess the benchmark methods on a separate test dataset comprising individual trajectories, denoted as $\\mathcal{D}_{\\mathrm{test}}$ . This assessment is based on the loss defined in Equation (14) and reported as MSE. The metrics are averaged over ten runs with different random seeds, and we provide these averages along with their $95\\%$ confidence intervals. Specifically we provide the mean confidence interval7. For each random seed, we independently generate new training, validation, and test datasets when a simulator is available. Specifically, for each simulation, we ensure that the validation and test datasets contain the same number of trajectories as the training set. Each baseline model is trained on the training dataset, with early stopping applied using the validation dataset when supported by the method. Subsequently, we evaluate the performance of each baseline on the test dataset. This entire procedure is repeated for each random seed. All experiments and training were conducted using a single Intel Core i9-12900K CPU $@$ 3.20GHz, 64GB RAM, and an Nvidia RTX3090 GPU with 24GB of memory. ", "page_idx": 16}, {"type": "text", "text": "C.1 Model Optimization Losses ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We evaluate the optimization loss using mean squared error (MSE) on a dataset $\\mathcal{D}$ , and also consider a component-wise MSE loss for more detailed analysis. ", "page_idx": 17}, {"type": "text", "text": "MSE Loss. Specifically, we optimize the following mean squared error objective: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta,\\mathcal{D})=\\frac{1}{N\\times T}\\sum_{n=1}^{N}\\sum_{t_{i}=0}^{T^{n}}\\left\\|f_{\\theta}(\\mathbf{x}_{n}(t_{i}),\\mathbf{a}_{n}(t_{i}),t_{i})\\Delta t-\\mathbf{y}_{n}(t_{i})\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $N\\times T$ represents the total number of state-action pairs in the dataset. The goal is to find the parameters $\\theta^{*}$ that minimize this loss, i.e., $\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}(\\theta,\\mathcal{D}_{\\mathrm{train}})$ . We optimize $\\theta$ using stochastic gradient descent with the Adam optimizer [Kingma and Ba, 2014], although other optimization algorithms, such as black box optimizers, could also be employed. ", "page_idx": 17}, {"type": "text", "text": "MSE Loss per component. To gather detailed quantitative statistics on the performance of the trained system model, we collect the validation loss per component. Using $(j)$ to denote the predictions for the $j^{t h}$ component, we define: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\omega_{j}(\\theta^{*},{\\mathcal{D}}_{\\mathrm{val}})=\\frac{1}{N_{\\mathrm{val}}\\times T}\\sum_{n=1}^{N_{\\mathrm{val}}}\\sum_{t_{i}=0}^{T_{\\mathrm{val}}^{n}}\\left(f_{\\theta}(\\mathbf{x}_{n}(t_{i}),\\mathbf{a}_{n}(t_{i}),t_{i}){\\Delta}t-\\mathbf{y}_{n,j}(t_{i})\\right)^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "cWoem apgugtree tghaet e mtehaens ev sacliadlaatri ovanl ildosasti aosn $\\begin{array}{r}{v=\\frac{1}{m}\\overset{\\cdot}{\\sum}_{j=1}^{m}\\dot{\\omega_{j}}\\big(\\theta^{*},\\mathcal{D}_{\\mathrm{val}}\\big)}\\end{array}$ . a vector $\\omega=[\\omega_{1},\\omega_{2},\\ldots,\\omega_{m}]$ , and ", "page_idx": 17}, {"type": "text", "text": "D Pharmacologists Feedback Statements ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We sought feedback from three expert pharmacologists on the discovered model. They have validated and commented on the discovered model and the impact of such a tool as D3. ", "page_idx": 17}, {"type": "text", "text": "Prof. Eoin Mckinney, Clinician. \u201cThis model is significant, as consortiums are dedicated to improving Warfarin modeling [Consortium, 2009]. The updated PK model (Equation (10)) adds complexity with interaction terms and introduces two new variables: the Michaelis constant and the natural decay rate of warfarin concentration. The Michaelis constant, which varies between individuals or over time due to factors like temperature and $\\mathrm{pH}$ , is presumably estimated from observed data. The natural decay rate is likely derived from prior reports but also varies between individuals. The model has proposed minimal-cost new features (a constant and interaction terms), but it\u2019s worth noting the genotype of certain warfarin-metabolizing enzymes (CYP2C9 and VKORC1) dictates clearance rates and levels, though the model did not recommend measuring these, possibly due to the associated costs. The principal clinical relevance of the improved model is better prediction of individual warfarin doses. Numerous algorithms attempt this, supported by a consortium focused on dose-prediction models. The data used, including public datasets, supports demonstrating the clinical utility of the improved PK model, highlighting the relevance of this problem despite newer alternatives to warfarin.\u201d ", "page_idx": 17}, {"type": "text", "text": "Jean-Baptiste Woillard, Pharmacologist. \u201cThe model is promising and pharmacokinetically plausible. I have data for mycophenolic acid, which has a challenging, complex absorption profile that would be interesting to test. The challenge extends to PK/PD, linking exposure and effect (e.g., tumor growth). Monolix, based on the SAEM algorithm, includes relevant datasets such as the PK/PD Warfarin dataset and a PK dataset for tacrolimus in heart transplants, which could serve as valuable benchmarks for comparison. Additionally, TMDD models, where clearance changes with tumor size, are of particular interest. It would be beneficial to see performance comparisons with these datasets using your approach.\u201d ", "page_idx": 17}, {"type": "text", "text": "Richard Peck, Clinical Pharmacologist. \u201cThis model is reasonable and potentially superior. It represents a significant advance in clinical pharmacology by automatically identifying robust PK models. While I am not a modeler or an expert in warfarin modeling, the discovered model appears suitable for your purpose. The most interesting aspect of this work is that D3 identified an alternative model that could be superior. While improving PK models for warfarin might not have much clinical use since warfarin dosing is monitored by INR rather than concentration, the ability to automatically identify robust PK models is a tremendous advance. Publishing about DI3 in clinical pharmacology literature would be significant. I haven\u2019t seen anything like this before. However, to show superiority, consider studying a drug with a more challenging PK. For such a publication, involving an expert pharmacometrician would be beneficial.\u201d ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "E Benchmark Method Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To evaluate the performance of D3 and establish its competitive performance, we conduct comparisons with leading modeling methods for ODEs. Specifically, we benchmark against advanced black-box models that have many parameters, such as neural ODEs with action inputs, known as DyNODE [Chen et al., 2018, Alvarez et al., 2020]. Also in this class are recurrent neural networks (RNN) and a state-of-the-art transformer model (Transformer). In addition, we include white-box transparent dynamical systems models identified through equation discovery techniques, such as Sparse Identification of Nonlinear Dynamics (SINDy) [Brunton et al., 2016]. Moreover, D3 supports two discovery modes, of which we compare against both, discovering only white-box models (D3-white-box) and discovering hybrid models (D3-hybrid). We also perform ablations of D3, of zero-shot generated model from D3 as (ZeroShot) and the same model with optimized parameters (ZeroOptim). ", "page_idx": 18}, {"type": "text", "text": "DyNODE DyNODE is a neural network-based model that integrates control inputs into the neural ordinary differential equation (ODE) framework [Chen et al., 2018], as outlined by Alvarez et al. [2020]. Our implementation of DyNODE features a 3-layer Multilayer Perceptron (MLP) with hidden layers consisting of 128 units each, utilizing tanh activation functions. We initialize weights using the Xavier method [Kumar, 2017]. For consistency and competitiveness, we apply the same objective, optimizer, and hyperparameters as used in D3. Specifically, we utilize the Adam optimizer [Kingma and Ba, 2014] with a learning rate of 0.01, a batch size of 1,000, and early stopping with a patience of 20. The model is trained for 2,000 epochs to ensure convergence. ", "page_idx": 18}, {"type": "text", "text": "RNN Recurrent Neural Networks (RNNs) [Graves et al., 2007] serve as a standard benchmark for autoregressive time series next-step prediction. In our implementation, the input dataset is normalized according to the training dataset. The model consists of a Gated Recurrent Unit (GRU) RNN that maps the state-action dimension to a hidden dimension of 250 across two layers. This hidden representation is then passed through a linear layer to convert it back to the state dimension, enabling next-step prediction. For a fair comparison, we employ the same objective, optimizer, and hyperparameters used in D3. Specifically, we utilize the Adam optimizer [Kingma and Ba, 2014] with a learning rate of 0.01, a batch size of 1,000, and early stopping with patience of 20 epochs. The model is trained for 2,000 epochs to ensure convergence. ", "page_idx": 18}, {"type": "text", "text": "Causal Transformer The Causal Transformer represents a cutting-edge model designed for estimating counterfactual outcomes, as detailed by [Melnychuk et al., 2022]. Given its intricate structure, which includes three distinct transformer networks for processing covariates, past treatments, and past outcomes, we opted for a streamlined approach more suitable for our datasets and task domains. Specifically, we employed a single transformer to model past outcomes. This implementation utilizes a standard transformer encoder, where the input data is normalized to the training dataset. The state-action input dimensions are encoded into a 250-dimensional embedding vector via a linear layer, followed by the application of a standard positional encoder [Melnychuk et al., 2022]. This data is then processed through a transformer encoder layer with a head size of 10 and a dropout rate of 0.1. The output from this layer is passed through a linear layer to reconstruct the next state, matching the state dimension size. Training this model involves the AdamW optimizer [Kingma and Ba, 2014] with a learning rate of 0.00005, complemented by a step learning rate scheduler with a step size of 1.0 and gamma of 0.95. Gradient clipping is applied at 0.7, and the model is trained with a batch size of 1,000. Early stopping is used with patience of 20 epochs, and training is conducted for up to 2,000 epochs to ensure convergence. ", "page_idx": 18}, {"type": "text", "text": "SINDy Sparse Identification of Nonlinear Dynamics (SINDy) [Brunton et al., 2016] is a data-driven methodology designed to uncover the governing equations of a dynamical system directly from time-series data, resulting in a transparent, closed-form mathematical model. The SINDy algorithm operates by performing sparse regression iteratively on a library of candidate functions to find the most parsimonious and accurate representation of the system\u2019s dynamics. ", "page_idx": 18}, {"type": "text", "text": "In our approach, we employ a second-order polynomial library, $\\mathcal{L}=\\{1,x_{0},x_{1},x_{0}x_{1}\\}$ , as the feature library. To calculate the time derivatives from the input time-series data, we use finite difference approximations of first order. We maintain the alpha parameter at 0.5 for all experiments, with a sparsity threshold of 0.02, except for the COVID-19 dataset where it is set to $1\\times\\dot{1}0^{-5}$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Data-Driven Discovery (D3) We defer to Appendix F for implementation-specific details. Specifically, ZeroShot and ZeroOptim are ablations of D3, where these use the exact same setup, hyperparameters, and prompts. First, ZeroShot generates one model without optimizing it\u2019s parameters $\\theta$ to the training dataset split, thereby assessing the model\u2019s output loss directly from the LLM. Second, ZeroOptim re-uses the same setup as ZeroShot, however now optimizes the parameters to the data, using the training setup described in Appendix F.2. ", "page_idx": 19}, {"type": "text", "text": "F D3 Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The data-driven discovery method follows the framework as outlined in Section 3. We present pseudocode in Appendix F.1, how the code-generated models $f_{\\theta}$ are trained in Appendix F.2, prompt templates in Appendix F.4, system description prompts in Appendix F.5 for each dataset. Specifically, we find a top-K, where $K=16$ is sufficient. Additionally, we use the LLM of GPT4-1106-Preview, with a temperature of 0.7. ", "page_idx": 19}, {"type": "text", "text": "F.1 D3 Pseudocode ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "KIrZmlTA92/tmp/758a14a36c41e1b5a0e9ae33d6840aa7e361551004dc7da2b33823300b106d42.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.2 Training Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Upon generation by the Modelling LLM Agent, the model $f$ is output as code, specifically a PyTorch [Paszke et al., 2017] neural network module. This code is then executed and the module is trained on the provided training dataset. The Modelling LLM Agent observes a code skeleton within the system description context c, examples of which are given in Appendix F.5. The code skeleton must be a \u2018torch.nn.Module\u2018 named \u2018StateDifferential\u2018, with initialized parameters and a forward function that computes the state differential using the state and action as input variables. The LLM completes, but does not alter, the skeleton, simplifying text processing, module execution, and model training. ", "page_idx": 19}, {"type": "text", "text": "The model is trained using the standard MSE loss function Equation (14), optimized with the Adam optimizer [Kingma and Ba, 2014], using a learning rate of 0.01, a batch size of 1,000, and early stopping with a patience of 20 epochs. Training proceeds for up to 2,000 epochs to ensure convergence and fair comparison. ", "page_idx": 19}, {"type": "text", "text": "After training, the validation MSE and per-component MSE (validation loss per state output dimension) are computed Equation (15). The trained model is appended back into $s_{i}$ with a string representation including initialized parameter values. Providing these optimized parameters aids the LLM in suggesting effective initial values in subsequent generations, which are then refined through further optimization. ", "page_idx": 19}, {"type": "text", "text": "Computational Efficiency. The computational efficiency of D3 overall arises from two sources: Using a Large Language Model (LLM) to generate the completions of the respective agents, and fitting the model, we now discuss each of these separately. First, using a capable enough LLM such as GPT4, for performing the completions for the agents can be computationally demanding, however, this computer costs with the computational cost per token of the underlying LLM that is used, and scales in the aspect of data with the number of tokens required to process, which we could envisage arises from having the LLM utilize a dataset with many features, each with their own textual description scaling the number of tokens in the input that the LLM has to process. One mitigation for this is to use LLMs that can handle larger context window sizes, and or be more selective about the input tokens fed into the LLM, such as restricting the number of in-context examples in the top- $\\cdot\\mathbf{k}$ examples, by reducing k, a hyperparameter. Second, we represent the models with parameters as a PyTorch module and use Pytorch to optimize the parameters, using a standard ML pipeline to train neural networks using stochastic gradient descent. Such approaches should scale with the number of input features, and parameters, however as with training any large eventual parameter model will scale as well as any other neural network-based approach in the same data pipeline. We note that more complicated distributed neural network training paradigms exist, however emphasize the focus of this work was on an initial framework, and leave the specific implementation for scale for future work. ", "page_idx": 20}, {"type": "text", "text": "F.3 System Description ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our method is initiated by an expert who provides a structured prompt that provides the system {system description} and {feature description}. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The system description provides a holistic description of the system, specifying the state variables $x(t)$ and actions $a(t)$ that are used to model the system. The variables are described semantically. \u2022 The feature description define features, names, units, and their ranges.   \nIn addition to these components, the structured prompt includes a skeleton code program {skeleton program} to instruct the LLM to synthesize executable code in a pre-determined format. We refer to this prompt as the context $c$ . ", "page_idx": 20}, {"type": "text", "text": "F.4 D3 Prompt Templates ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For all prompt templates used, please see the code https://github.com/samholt/ DataDrivenDiscovery. ", "page_idx": 20}, {"type": "text", "text": "F.5 D3 System Description Prompts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By following our proposed system requirements format Appendix F.3 we constructed prompts for each of the datasets that we evaluated against, which are provided in the code, https://github. com/samholt/DataDrivenDiscovery. ", "page_idx": 20}, {"type": "text", "text": "G Additional Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Warfarin Case Study Additional Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Hybrid PKPD Warfarin Model with Neural Network Integration. This model incorporates a simplified pharmacokinetic-pharmacodynamic (PKPD) structure with additional neural network complexity, achieving a validation loss of 0.276. The model is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d C}{d t}=\\frac{F\\cdot D}{V}-k_{c}\\cdot C\\cdot(1+\\beta_{a}\\cdot(A-40))\\cdot(1+\\beta_{s}\\cdot(S-0.5))+\\mathrm{MLP}(C,D,A,S)-R,}\\\\ &{\\quad R=\\lambda\\cdot(|k_{c}|+|F|+|V|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $F$ represents the bioavailability of the drug, $V$ denotes the volume of distribution, $k_{c}$ is the clearance rate of warfarin, $\\beta_{a}$ is the slope of the age effect on drug clearance, $\\beta_{s}$ is the offset for the sex effect on drug clearance, $\\mathrm{MLP}(C,D,A,S)$ is a multi-layer perceptron output, representing complex relationships in the data captured by the neural network, and $R$ is a regularization term to prevent overfitting, with $\\lambda$ being the regularization weight. The addition of a neural network ", "page_idx": 20}, {"type": "text", "text": "allows the model to learn nonlinear relationships directly from data, providing improved predictive performance. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Existing Wafarin PK Model. Test MSE Loss: 0.6461. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Parameters f o r a b s o r p t i o n and e l i m i n a t i o n r a t e s s e l f . $\\mathtt{k\\_a}=\\mathtt{n n}$ . Parameter ( t o r c h . t e n s o r $(0.1)$ ) # Absorption r a t e s e l f . k_e_base $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Base e l i m i n a t i o n r a t e # Modifiers f o r age and sex on e l i m i n a t i o n r a t e s e l f . age_modifier $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22120.001) ) # Decrease in e l i m i n a t i o n r a t e per year i n c r e a s e in age s e l f . sex_modifier $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 2 ) ) # D i f f e r e n c e in e l i m i n a t i o n r a t e between sexes def forward ( s e l f , w a r f a r i n _ c o n c e n t r a t i o n , warfarin_dosage , p a t i e n t _ a g e , p a t i e n t _ s e x ) : # C a l c u l a t e the e l i m i n a t i o n r a t e a d j u s t e d f o r age and sex k_e $=$ s e l f . k_e_base $^+$ s e l f . age_modifier $^*$ p a t i e n t _ a g e $^+$ s e l f . sex_modifier $^\\ast$ p a t i e n t _ s e x # D i f f e r e n t i a l eq u at i on f o r w a r f a r i n c o n c e n t r a t i o n d _ w a r f a r i n _ c o n c e n t r a t i o n _ d t $=$ s e l f . k_a $^*$ warfarin_dosage \u2212 k_e \\* w a r f a r i n _ c o n c e n t r a t i o n r e t u r n ( d _ w a r f a r i n _ c o n c e n t r a t i o n _ d t , ) ", "page_idx": 21}, {"type": "text", "text": "D3-white-box. Test MSE Loss: 0.39 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the PKPD model s e l f . c l e a r a n c e _ r a t e $=$ nn . Parameter ( t o r c h . t e n s o r (0.33197110891342163) ) s e l f . a g e _ e f f e c t _ b a s e $=$ nn . Parameter ( t o r c h . t e n s o r (0.011119960807263851) ) s e l f . s e x _ e f f e c t _ b a s e $=$ nn . Parameter ( t o r c h . t e n s o r (0.781970202922821) ) s e l f . dosage_power $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 5 ) ) # Adjusted to a square r o o t t r a n s f o r m a t i o n s e l f . n a t u r a l _ d e c a y $=$ nn . Parameter ( t o r c h . t e n s o r (0.12728933990001678) ) s e l f . m i c h a e l i s _ c o n s t a n t $=$ nn . Parameter ( t o r c h . t e n s o r (10.865914344787598) ) s e l f . d o s a g e _ s e x _ i n t e r a c t i o n $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 5 ) ) s e l f . a g e _ s e x _ i n t e r a c t i o n $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22120.09479257464408875) ) s e l f . d o s a g e _ a g e _ i n t e r a c t i o n $=$ nn . Parameter ( t o r c h . t e n s o r (0.0010000000474974513) ) def forward ( s e l f , w a r f a r i n _ c o n c e n t r a t i o n : t o r c h . Tensor , warfarin_dosage : t o r c h . Tensor , p a t i e n t _ a g e : t o r c h . Tensor , p a t i e n t _ s e x : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor ] : # C a l c u l a t e the change in w a r f a r i n c o n c e n t r a t i o n over time a g e _ e f f e c t $=$ s e l f . a g e _ e f f e c t _ b a s e $^\\ast$ ( p a t i e n t _ a g e \u2212 42) s e x _ e f f e c t $=$ s e l f . s e x _ e f f e c t _ b a s e \\* ( p a t i e n t _ s e x \u2212 0 . 5 ) d o s a g e _ e f f e c t $=$ t o r c h . s q r t ( warfarin_dosage ) # Using square r o o t based on dosage_power decay_term $=$ s e l f . n a t u r a l _ d e c a y $*$ w a r f a r i n _ c o n c e n t r a t i o n d o s a g e _ s e x _ i n t e r a c t i o n $=$ s e l f . d o s a g e _ s e x _ i n t e r a c t i o n \\* warfarin_dosage \\* ( p a t i e n t _ s e x \u2212 0 . 5 ) a g e _ s e x _ i n t e r a c t i o n $=$ s e l f . a g e _ s e x _ i n t e r a c t i o n $^*$ ( p a t i e n t _ a g e \u2212 42) $*$ ( p a t i e n t _ s e x \u2212 0 . 5 ) d o s a g e _ a g e _ i n t e r a c t i o n $=$ s e l f . d o s a g e _ a g e _ i n t e r a c t i o n $^*$ warfarin_dosage $^\\ast$ ( p a t i e n t _ a g e \u2212 42) michaelis_menten_term $=$ w a r f a r i n _ c o n c e n t r a t i o n / ( s e l f . m i c h a e l i s _ c o n s t a n t $^+$ w a r f a r i n _ c o n c e n t r a t i o n ) d _ w a r f a r i n _ c o n c e n t r a t i o n _ _ d t $=$ d o s a g e _ e f f e c t \u2212 ( s e l f . c l e a r a n c e _ r a t e $^+$ a g e _ e f f e c t $^+$ s e x _ e f f e c t $^+$ decay_term $^+$ d o s a g e _ s e x _ i n t e r a c t i o n $^+$ a g e _ s e x _ i n t e r a c t i o n $^+$ d o s a g e _ a g e _ i n t e r a c t i o n ) $^*$ michaelis_menten_term r e t u r n ( d _ w a r f a r i n _ c o n c e n t r a t i o n _ _ d t , )   \noptimized_parameters $=$ { \u2019 c l e a r a n c e _ r a t e \u2019 : 1.1111963987350464 , \u2019 ag e_ ef fe c t_ ba se \u2019 : 0.012109620496630669 , \u2019 s e x _ e f f e c t _ b a s e \u2019 : 1.5611952543258667 , \u2019 dosage_power \u2019 : 0.5 , \u2019 natural_decay \u2019 : 0.14641410112380981 , \u2019 m i c h a e l i s _ c o n s t a n t \u2019 : 10.195984840393066 , d o s a g e _ s e x _ i n t e r a c t i o n \u2019 : 0.05000000074505806 , \u2019 a g e _ s e x _ i n t e r a c t i o n \u2019 : \u22120.09380292147397995 , \u2019 d o s a g e _ a g e _ i n t e r a c t i o n \u2019 : 0.0010000000474974513}   \nWhite box model with r e i n t r o d u c e d i n t e r a c t i o n terms , square r o o t t r a n s f o r m a t i o n f o r dosage e f f e c t , n a t u r a l decay term , and Michaelis \u2212Menten s a t u r a t i o n k i n e t i c s ", "page_idx": 21}, {"type": "text", "text": "D3-hybrid. Test MSE Loss: 0.271 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the PKPD model with r e a s o n a b l e i n i t i a l values s e l f . c l e a r a n c e _ r a t e $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 2 ) ) s e l f . b i o a v a i l a b i l i t y $=$ nn . Parameter ( t o r c h . t e n s o r ( 3 . 0 ) ) s e l f . v o l u m e _ o f _ d i s t r i b u t i o n = nn . Parameter ( t o r c h . t e n s o r ( 3 7 . 0 ) ) # S i m p l i f i e d l i n e a r age and sex e f f e c t s s e l f . a g e _ e f f e c t _ s l o p e $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 1 ) ) s e l f . s e x _ e f f e c t _ o f f s e t $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # S i m p l i f i e d MLP a r c h i t e c t u r e s e l f . mlp $=$ nn . S e q u e n t i a l ( nn . Linear (4 , 10) , nn . LeakyReLU ( ) , nn . Linear (10 , 1) ) # L1 R e g u l a r i z a t i o n s e l f . r e g u l a r i z a t i o n _ w e i g h t = nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 1 ) . abs ( ) ) ", "page_idx": 21}, {"type": "text", "text": "def forward ( s e l f , w a r f a r i n _ c o n c e n t r a t i o n : t o r c h . Tensor , warfarin_dosage : t o r c h . Tensor , p a t i e n t _ a g e : t o r c h . Tensor , p a t i e n t _ s e x : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor ] : # C a l c u l a t e the change in w a r f a r i n c o n c e n t r a t i o n over time a b s o r p t i o n $=$ s e l f . b i o a v a i l a b i l i t y $^\\ast$ warfarin_dosage / s e l f . v o l u m e _ o f _ d i s t r i b u t i o n e l i m i n a t i o n $=$ s e l f . c l e a r a n c e _ r a t e \\* w a r f a r i n _ c o n c e n t r a t i o n # Linear e f f e c t of age and sex a g e _ e f f e c t $=$ s e l f . a g e _ e f f e c t _ s l o p e $*$ ( p a t i e n t _ a g e \u2212 40) s e x _ e f f e c t $=$ s e l f . s e x _ e f f e c t _ o f f s e t \\* ( p a t i e n t _ s e x \u2212 0 . 5 ) a d j u s t e d _ c l e a r a n c e $=$ e l i m i n a t i o n $^*$ (1 + a g e _ e f f e c t ) $*$ ( $1+$ s e x _ e f f e c t ) # MLP f o r c a p t u r i n g complex r e l a t i o n s h i p s mlp_input $=$ t o r c h . s t a c k ( ( w a r f a r i n _ c o n c e n t r a t i o n , warfarin_dosage , p a t i e n t _ a g e , p a t i e n t _ s e x ) , dim =1) mlp_output $=$ s e l f . mlp ( mlp_input ) . squeeze ( ) # R e g u l a r i z a t i o n term r e g u l a r i z a t i o n $=$ s e l f . r e g u l a r i z a t i o n _ w e i g h t $^\\ast$ ( s e l f . c l e a r a n c e _ r a t e . abs ( ) $^+$ s e l f . b i o a v a i l a b i l i t y . abs ( ) $^+$ s e l f . v o l u m e _ o f _ d i s t r i b u t i o n . abs ( ) ) d _ w a r f a r i n _ c o n c e n t r a t i o n _ _ d t $=$ a b s o r p t i o n \u2212 a d j u s t e d _ c l e a r a n c e $^+$ mlp_output \u2212 r e g u l a r i z a t i o n r e t u r n ( d _ w a r f a r i n _ c o n c e n t r a t i o n _ _ d t , ) ", "page_idx": 21}, {"type": "text", "text": "G.2 Insight Experiment: Feature Acquisition Setup ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For this experiment, we adapted the Lung Cancer (with Chemo. & Radio.) dataset simulator, to form a new variation of this dataset, where D3 starts with two features, which are the main state ${\\bf x}(t)$ features that it intends to model, and can optionally acquire new features ${\\bf a}(t)$ from a set of possible 22 features, which are listed below. Of importance, two features directly affect the underlying behaviour of this dynamical system, these being radiotherapy_dosage and chemotherapy_dosage, and the rest of the features we model as random white noise features, hence have no effect on the underling system, however D3 is unaware of this, and must use the feature description, from the features name to determine relative value of improved performance that it can attain by acquiring that feature, and propose new features to acquire along with their expected improvement. ", "page_idx": 22}, {"type": "text", "text": "Specifically, in this setup, we allow the feature acquistion agent to select from the following set of features at each iteration: ", "page_idx": 22}, {"type": "text", "text": "[ blood_type , smoking_status , p a t i e n t _ a g e , p a t i e n t _ w e i g h t , p a t i e n t _ h e i g h t , sex , radiotherapy_dosage , alcohol_consumption , p r e v i o u s _ c a n c e r _ t r e a t m e n t s , c o m o r b i d i t i e s , genetic_markers , chemotherapy_dosage , f a m i l y _ h i s t o r y _ o f _ c a n c e r , performance_status , d i e t a r y _ h a b i t s , p h y s i c a l _ a c t i v i t y _ l e v e l , environmental_exposure , medication_adherence , p s y c h o l o g i c a l _ s t r e s s _ l e v e l s , socioeconomic_status , r e s i d e n t i a l _ l o c a t i o n , support_network ] . ", "page_idx": 22}, {"type": "text", "text": "We also make the assumption that each feature that can be acquired has equal cost $l$ of acquisition, and that the agent has an unlimited feature acquisition budget, however these settings can be readily changed depending on the specific application. ", "page_idx": 22}, {"type": "text", "text": "To provide comparable baselines, we compare D3 against ablations of itself, which is a random feature acquirer Feature Acquisition Agent, that selects a feature randomly for acquisition, and a null, or no Feature Acquisition Agent, that only observes at all iterations the target states ${\\mathbf x}(t)$ to model. We observe experimentally that D3 that can leverage the LLM to quantify the value of information for features, where no training data exists for them in the dataset. We observe, as in Figure 2, that D3 converges the fastest, and achieves the overall highest performance, whereas the other feature acquisition methods fall short. ", "page_idx": 22}, {"type": "text", "text": "We provide part of the logs from one of the random seed runs, including part of it due to the logs being extensive. ", "page_idx": 22}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e \u221210 or l e s s . ", "page_idx": 23}, {"type": "text", "text": "\\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l ", "page_idx": 23}, {"type": "text", "text": "Skeleton code to f i l l in : ", "page_idx": 23}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 23}, {"type": "text", "text": "Useful to know : ", "page_idx": 23}, {"type": "text", "text": "\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 23}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 23}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:26:22 ,185 ,185 m u l t i p r o c e s s i n g INFO [ Pr o gr e ss : Step 1/0 | R e t r i e s : 0/30 | Token Capacity Used : 0.80% Tokens remaining 126980] ", "page_idx": 23}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:27:26 ,602 ,602 m u l t i p r o c e s s i n g INFO [ Function Call ] [ c o m p l e t e _ S t a t e D i f f e r e n t i a l _ c o d e ] { \" S t a t e D i f f e r e n t i a l _ c o d e \" : \" import t o r c h \\ nimport t o r c h . nn as nn \\ nfrom typing import Tuple \\ n \\ n c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : \\ n def _ _ i n i t _ _ ( s e l f ) : \\ n super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) \\ n # Define the parameters f o r the tumor growth model \\ n s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r $(0.1)$ ) # Tumor growth r a t e \\ n s e l f . beta = nn . Parameter ( t o r c h . t e n s o r $(\\,0\\,.\\,0\\,1\\,)$ ) # Drug e f f i c a c y $\\backslash\\mathfrak{n}\\backslash\\mathfrak{n}$ def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : \\ n # Tumor growth model based on Gompertzian growth \\ n d_tumor_volume__dt $=$ s e l f . alpha $^*$ t o r c h . log (1170.861 / tumor_volume ) $^\\ast$ tumor_volume \\ n # Drug e f f e c t on tumor volume \\ n d_tumor_volume__dt $-=$ s e l f . beta $*$ chemotherapy_drug_concentration \\* tumor_volume \\ n \\ n # Assuming c o n s t a n t drug c o n c e n t r a t i o n decay \\ n drug_decay_rate $\\qquad\\qquad\\qquad\\qquad=\\quad0.1\\backslash\\ n$ d_chemotherapy_drug_concentration__dt $=$ \u2212drug_decay_rate $^\\ast$ chemotherapy_drug_concentration \\ n \\ n r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) \" , \" c o d e _ d e s c r i p t i o n \" : \" This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume and a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n . The model i n c l u d e s parameters f o r the tumor growth r a t e and drug e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . \" , \" m o d e l _ i n p u t _ v a r i a b l e s \" : [ \" tumor_volume \" , \" chemotherapy_drug_concentration \" ] ", "page_idx": 23}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:27:33 ,445 ,445 m u l t i p r o c e s s i n g INFO Early st op pi n g t r i g g e r e d a t epoch 1090   \nMainProcess | 2024\u221205\u221220 07:27:33 ,454 ,454 m u l t i p r o c e s s i n g INFO [D3\u2212f e a t u r e \u2212s e l e c t | Cancer \u2212random \u2212f e a t u r e s | 12][ Generation 0] | Top F i t n e s s : 43.21586608886719 | Num Programs : 1 | Mean F i t n e s s : 43.21586608886719 | F i t n e s s e s : [43.21586608886719] | Current Gen Val Loss : 43.21586608886719 ", "page_idx": 23}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:27:33 ,454 ,454 m u l t i p r o c e s s i n g INFO [D3\u2212f e a t u r e \u2212s e l e c t | Cancer \u2212random \u2212f e a t u r e s | 12][GEN RESULT] { \u2019 method \u2019 : \u2019D3\u2212f e a t u r e \u2212s e l e c t \u2019 , \u2019 env_name \u2019 : \u2019 Cancer \u2212random \u2212f e a t u r e s \u2019 , \u2019 seed \u2019 : 12 , \u2019 generation \u2019 : 0 , \u2019 t o p _ f i t n e s s \u2019 : 43.21586608886719 , \u2019 num_programs \u2019 : 1 , \u2019 mean_fitness \u2019 : 43.21586608886719 , \u2019 f i t n e s s e s \u2019 : [43.21586608886719] , \u2019 c u r r e n t _ g e n _ v a l _ l o s s \u2019 : 43.21586608886719} ", "page_idx": 23}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:27:33 ,455 ,455 m u l t i p r o c e s s i n g INFO [ System ] O b j e c t i v e : Write code to c r e a t e an e f f e c t i v e d i f f e r e n t i a l e qu a ti o n s i m u l a t o r f o r a given t a s k Please note t h a t the code should be f u l l y f u n c t i o n a l . No p l a c e h o l d e r s . ", "page_idx": 23}, {"type": "text", "text": "You must a c t autonomously and you w i l l r e c e i v e no human i n p u t a t any s t a g e . You have to r e t u r n as output the complete code f o r completing t h i s task , and c o r r e c t l y improve the code to c r e a t e the most a c c u r a t e and r e a l i s t i c s i m u l a t o r p o s s i b l e . You always w r i t e out the code c o n t e n t s . You always i n d e n t code with t a b s . ", "page_idx": 23}, {"type": "text", "text": "You cannot v i s u a l i z e any g r a p h i c a l output . You e x i s t within a machine . The code can i n c l u d e black box multi \u2212l a y e r p e r c e p t i o n s where r e q u i r e d . ", "page_idx": 23}, {"type": "text", "text": "Use the f u n c t i o n s provided . When c a l l i n g f u n c t i o n s only provide a RFC8259 compliant JSON r e q u e s t f o l l o w i n g t h i s format without d e v i a t i o n . ", "page_idx": 23}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:27:33 ,455 ,455 m u l t i p r o c e s s i n g INFO [ User ] You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r ", "page_idx": 23}, {"type": "text", "text": "System D e s c r i p t i o n : ", "page_idx": 23}, {"type": "text", "text": "P r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model ", "page_idx": 23}, {"type": "text", "text": "Here you must model the s t a t e d i f f e r e n t i a l of tumor_volume , and chemotherapy_drug_concentration ; with the i n p u t a c t i o n s of ", "page_idx": 23}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s \\* Volume of the tumor with u n i t s $\\operatorname{cm}\\wedge3$ \\* C o n c e n t r a t i o n of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathrm{mg}/\\mathrm{m}^{\\wedge}3$ ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "The time u n i t s i s in days ", "page_idx": 23}, {"type": "text", "text": "A d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of \\* tumor_volume : [0.01433 , 1170.861] \\* chemotherapy_drug_concentration : [0 , 9.9975] ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "The t r a i n i n g d a t a s e t c o n s i s t s of 1000 p a t i e n t s , where each p a t i e n t i s observed f o r 60 days ", "page_idx": 23}, {"type": "text", "text": "Modelling goals : ", "page_idx": 23}, {"type": "text", "text": "\\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 23}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : ", "page_idx": 23}, {"type": "text", "text": "\\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e \u221210 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018 \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 24}, {"type": "text", "text": "Useful to know : ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n$*$ You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch .   \n\u2018 \u2018 \u2018 ", "page_idx": 24}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 24}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:27:33 ,455 ,455 m u l t i p r o c e s s i n g INFO [ User ] You g e n e r a t e d the f o l l o w i n g code completions , which then had t h e i r parameters optimized to the t r a i n i n g d a t a s e t . Please r e f l e c t on how you can improve the code to minimize the v a l i d a t i o n l o s s to 1e\u22126 or l e s s . The code examples are d e l i n e a t e d by ###. ", "page_idx": 24}, {"type": "text", "text": "Here are your p r ev i ou s i t e r a t i o n s the b e s t programs g e n e r a t e d . i f you have exhausted white box models , i . e . when a white box model r e p e a t s with the same val l o s s and then only add black box models to the white box models : \u2018 \u2018 \u2018   \nI t e r a t i o n 0. Best Val Loss : 43.21586608886719. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume and a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n . The model i n c l u d e s parameters f o r the tumor growth r a t e and drug e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . ", "page_idx": 24}, {"type": "text", "text": "Please l i s t in order of ranked importance ( with the f i r s t f e a t u r e to be a c qu i r e d f i r s t ) and the p e r c e n t a g e of expected improvement of p o t e n t i a l f e a t u r e s to a c q u i r e next , such t h a t the v a l i d a t i o n l o s s i s minimized to 1e\u22126 or l e s s . The p o s s i b l e a d d i t i o n a l f e a t u r e s to s e l e c t are : blood_type , smoking_status , p a t i e n t _ a g e , p a t i e n t _ w e i g h t , p a t i e n t _ h e i g h t , sex radiotherapy_dosage , alcohol_consumption , p r e v i o u s _ c a n c e r _ t r e a t m e n t s , c o m o r b i d i t i e s , genetic_markers , chemotherapy_dosage , f a m i l y _ h i s t o r y _ o f _ c a n c e r , performance_status , d i e t a r y _ h a b i t s , p h y s i c a l _ a c t i v i t y _ l e v e l , environmental_exposure , medication_adherence , p s y c h o l o g i c a l _ s t r e s s _ l e v e l s , socioeconomic_status , r e s i d e n t i a l _ l o c a t i o n , support_network . ", "page_idx": 24}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:27:33 ,455 ,455 m u l t i p r o c e s s i n g INFO [ Pr o gr e ss : Step 1/0 | R e t r i e s : 0/30 | Token Capacity Used : 1.06% Tokens remaining 126639] ", "page_idx": 24}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:28:04 ,902 ,902 m u l t i p r o c e s s i n g INFO [LLM RESPONSE MESSAGE]   \nMainProcess | 2024\u221205\u221220 07:28:04 ,903 ,903 m u l t i p r o c e s s i n g INFO [ Function Call ] [ f e a t u r e _ a c q u i s i t i o n ] { \" f e a t u r e s \" : [ { \"name \" : \" r a d i o t h e r a p y _ d o s a g e \" , \" importance_score \" : 0.25 } , \"name \" : \" chemotherapy_dosage \" \" importance_score \" : 0.20 } , { \"name \" : \" genetic_markers \" , \" importance_score \" : 0.15 \"name \" : \" p a t i e n t _ a g e \" , \" importance_score \" : 0.10 } \"name \" : \" smoking_status \" \" importance_score \" : 0.10 \"name \" : \" c o m o r b i d i t i e s \" \" importance_score \" : 0.05 \"name \" : \" p e r f o r m a n c e _ s t a t u s \" \" importance_score \" : 0.05 } , { \"name \" : \" medication_adherence \" \" importance_score \" : 0.05 \"name \" : \" p s y c h o l o g i c a l _ s t r e s s _ l e v e l s \" \" importance_score \" : 0.05   \nMainProcess | 2024\u221205\u221220 07:28:04 ,903 ,903 m u l t i p r o c e s s i n g INFO [ S e l e c t e d new f e a t u r e ] SELECTED $:$ r a d i o t h e r a p y _ d o s a g e Now using : [ tumor_volume \u2019 , \u2019 chemotherapy_drug_concentration \u2019 , \u2019 radiotherapy_dosage \u2019 ]   \nMainProcess | 2024\u221205\u221220 07:28:04 ,905 ,905 m u l t i p r o c e s s i n g INFO [ System ]   \nO b j e c t i v e : Write code to c r e a t e an e f f e c t i v e d i f f e r e n t i a l e qu a ti o n s i m u l a t o r f o r a given t a s k ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Please note t h a t the code should be f u l l y f u n c t i o n a l . No p l a c e h o l d e r s . ", "page_idx": 24}, {"type": "text", "text": "You must a c t autonomously and you w i l l r e c e i v e no human i n p u t a t any s t a g e . You have to r e t u r n as output the complete code f o r completing t h i s task , and c o r r e c t l y improve the code to c r e a t e the most a c c u r a t e and r e a l i s t i c s i m u l a t o r p o s s i b l e .   \nYou always w r i t e out the code c o n t e n t s . You always i n d e n t code with t a b s .   \nYou cannot v i s u a l i z e any g r a p h i c a l output . You e x i s t within a machine . The code can i n c l u d e black box multi \u2212l a y e r p e r c e p t i o n s where r e q u i r e d . ", "page_idx": 24}, {"type": "text", "text": "Use the f u n c t i o n s provided . When c a l l i n g f u n c t i o n s only provide a RFC8259 compliant JSON r e q u e s t f o l l o w i n g t h i s format without d e v i a t i o n . ", "page_idx": 25}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:28:04 ,905 ,905 m u l t i p r o c e s s i n g INFO [ User ] You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r ", "page_idx": 25}, {"type": "text", "text": "System D e s c r i p t i o n : \u2018 \u2018 \u2018   \nP r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model ", "page_idx": 25}, {"type": "text", "text": "Here you must model the s t a t e d i f f e r e n t i a l of tumor_volume , and chemotherapy_drug_concentration ; with the i n p u t a c t i o n s of r a d i o t h e r a p y _ d o s a g e . ", "page_idx": 25}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s :   \n$*$ Volume of the tumor with u n i t s $\\operatorname{cm}\\wedge3$   \n\\* C o n c e n t r a t i o n of the chemotherapy drug v i n b l a s t i n e with u n i t s mg/m^3   \n\\* Dosage of the r a d i o t h e r a p y with u n i t s Gy ", "page_idx": 25}, {"type": "text", "text": "The time u n i t s i s in days . ", "page_idx": 25}, {"type": "text", "text": "A d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of \\* tumor_volume : [0.01433 , 1170.861] \\* chemotherapy_drug_concentration : [0 , 9.9975] r a d i o t h e r a p y _ d o s a g e : [0 , 2 . 0 ] ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "The t r a i n i n g d a t a s e t c o n s i s t s of 1000 p a t i e n t s , where each p a t i e n t i s observed f o r 60 days ", "page_idx": 25}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data . ", "page_idx": 25}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018 \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e \u221210 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 25}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018 \u2018 \u2018 c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) def _ _ i n i t _ _ ( s e l f ) : ", "page_idx": 25}, {"type": "text", "text": "super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 25}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 25}, {"type": "text", "text": "Useful to know : \u2018 \u2018 \u2018   \n$*$ You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s . ", "page_idx": 25}, {"type": "text", "text": "\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t . \\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to \\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 1 out of 20. ", "page_idx": 25}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:28:04 ,905 ,905 m u l t i p r o c e s s i n g INFO [ User ] You g e n e r a t e d the f o l l o w i n g code completions , which then had t h e i r parameters optimized to the t r a i n i n g d a t a s e t . Please r e f l e c t on how you can improve the code to minimize the v a l i d a t i o n l o s s to 1e\u22126 or l e s s . The code examples are d e l i n e a t e d by ###. ", "page_idx": 25}, {"type": "text", "text": "Here are your p r ev i ou s i t e r a t i o n s the b e s t programs g e n e r a t e d . Use i t to see i f you have exhausted white box models , i . e . when a white box model r e p e a t s with the same val l o s s and then only add black box models to the white box models : \u2018 \u2018 \u2018   \nI t e r a t i o n 0. Best Val Loss : 43.21586608886719. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration . Model d e s c r i p t i o n This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume and a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n . The model i n c l u d e s parameters f o r the tumor growth r a t e and drug e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . ", "page_idx": 25}, {"type": "text", "text": "Here are the top code completions so f a r t h a t you have generated , s o r t e d f o r the lowest v a l i d a t i o n l o s s l a s t : ", "page_idx": 25}, {"type": "text", "text": "Val Loss : 43.2 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 79 , chemotherapy_drug_concentration val l o s s : 7 . 4 2 ) I t e r a t i o n : 0 ", "page_idx": 25}, {"type": "text", "text": "### ", "page_idx": 25}, {"type": "text", "text": "import t o r c h import t o r c h . nn as nn from typing import Tuple c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Tumor growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 1 ) ) # Drug e f f i c a c y ", "page_idx": 25}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration $:$ t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # Tumor growth model based on Gompertzian growth d_tumor_volume $\\_\\mathrm{dt}\\ =$ s e l f . alpha $*$ t o r c h . log (1170.861 / tumor_volume ) $^*$ tumor_volume # Drug e f f e c t on tumor volume d_tumor_volume__dt $-=$ s e l f . beta $*$ chemotherapy_drug_concentration $^*$ tumor_volume ", "page_idx": 25}, {"type": "text", "text": "# Assuming c o n s t a n t drug c o n c e n t r a t i o n decay ", "page_idx": 25}, {"type": "text", "text": "drug_decay_rate $=~0.1$ ", "page_idx": 25}, {"type": "text", "text": "r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 26}, {"type": "text", "text": "optimized_parameters $=$ { \u2019 alpha \u2019 : 0.00944874994456768 , \u2019 beta \u2019 : 0.031064828857779503} ### ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Code Skeleton : \u2018 \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) \u2212> Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 26}, {"type": "text", "text": "Please r e f l e c t on how you can improve the code to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e , and be i n t e r p r e t a b l e . Use the code s k e l e t o n above to provide the code . Please a l s o s p e c i f y only the i n p u t parameters t h a t you want to i n c l u d e in the model in from the s k e l e t o n code . Think step \u2212by\u2212s t e p . Where a p p l i c a b l e use the values of the optimized parameters to reason how the code can be improved to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e . This i s f o r g e n e r a t i n g new code f o r the next i t e r a t i o n 1 out of 20. ", "page_idx": 26}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:28:04 ,905 ,905 m u l t i p r o c e s s i n g INFO [ Pr o gr e ss : Step 1/0 | R e t r i e s : 0/30 | Token Capacity Used : 1.42% | Tokens remaining 126184] ", "page_idx": 26}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:29:41 ,577 ,577 m u l t i p r o c e s s i n g INFO [LLM RESPONSE MESSAGE]   \nMainProcess | 2024\u221205\u221220 07:29:41 ,577 ,577 m u l t i p r o c e s s i n g INFO [ Function Call ] [ c o m p l e t e _ S t a t e D i f f e r e n t i a l _ c o d e ] { \" S t a t e D i f f e r e n t i a l _ c o d e \" : \" import t o r c h \\ nimport t o r c h . nn as nn \\ nfrom typing import Tuple \\ n \\ n c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : \\ n def _ _ i n i t _ _ ( s e l f ) : \\ n super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) \\ n # Define the parameters f o r the tumor growth model \\ n s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r (0.00944874994456768) ) # Optimized tumor growth r a t e \\ n s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (0.031064828857779503) ) # Optimized drug e f f i c a c y \\ n # Define the parameter f o r the r a d i o t h e r a p y e f f e c t \\ n s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Radiotherapy e f f i c a c y \\ n \\ n def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : \\ n # Tumor growth model based on Gompertzian growth \\ n d_tumor_volume__dt $=$ s e l f . alpha $^*$ t o r c h . log (1170.861 / tumor_volume ) $^*$ tumor_volume \\ n # Drug e f f e c t on tumor volume \\ n d_tumor_volume__dt $-=$ s e l f . beta $*$ chemotherapy_drug_concentration $^*$ tumor_volume \\ n # Radiotherapy e f f e c t on tumor volume \\ n d_tumor_volume__dt $-=$ s e l f . gamma $^*$ r a d i o t h e r a p y _ d o s a g e $^\\ast$ tumor_volume \\ n \\ n # Assuming c o n s t a n t drug c o n c e n t r a t i o n decay \\ n drug_decay_rate $\\qquad\\qquad\\qquad=\\quad0.1\\,\\backslash\\,{\\bf n}$ d_chemotherapy_drug_concentration__dt = \u2212drug_decay_rate $^*$ chemotherapy_drug_concentration \\ n \\ n r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) \" , \" c o d e _ d e s c r i p t i o n \" : \" This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume , a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n , and i n c l u d e s the e f f e c t of r a d i o t h e r a p y dosage . The model i n c l u d e s parameters f o r the tumor growth rate , drug e f f i c a c y , and r a d i o t h e r a p y e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . \" , \" m o d e l _ i n p u t _ v a r i a b l e s \" : [ \" tumor_volume \" , \" chemotherapy_drug_concentration \" , \" r a d i o t h e r a p y _ d o s a g e \" ]   \nMainProcess | 2024\u221205\u221220 07:29:50 ,478 ,478 m u l t i p r o c e s s i n g INFO Early st op pi n g t r i g g e r e d a t epoch 1110   \nMainProcess | 2024\u221205\u221220 07:29:50 ,489 ,489 m u l t i p r o c e s s i n g INFO [D3\u2212f e a t u r e \u2212s e l e c t | Cancer \u2212random \u2212f e a t u r e s | 12][ Generation 1] | Top F i t n e s s : 7.463240623474121 | Num Programs : 2 | Mean F i t n e s s : 25.339553356170654 | F i t n e s s e s : [7.463240623474121 , 43.21586608886719] | Current Gen Val Loss : 7.463240623474121   \nMainProcess | 2024\u221205\u221220 07:29:50 ,489 ,489 m u l t i p r o c e s s i n g INFO [D3\u2212f e a t u r e \u2212s e l e c t | Cancer \u2212random \u2212f e a t u r e s | 12][GEN RESULT] { \u2019 method \u2019 : \u2019D3\u2212f e a t u r e \u2212s e l e c t \u2019 , \u2019 env_name \u2019 : \u2019 Cancer \u2212random \u2212f e a t u r e s \u2019 , \u2019 seed \u2019 : 12 , \u2019 generation \u2019 : 1 , \u2019 t o p _ f i t n e s s \u2019 : 7.463240623474121 , \u2019 num_programs \u2019 : 2 , \u2019 mean_fitness \u2019 : 25.339553356170654 , \u2019 f i t n e s s e s \u2019 : [7.463240623474121 , 43.21586608886719] , c u r r e n t _ g e n _ v a l _ l o s s \u2019 : 7.463240623474121}   \nMainProcess | 2024\u221205\u221220 07:29:50 ,490 ,490 m u l t i p r o s s i n g INFO [ System ] ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "O b j e c t i v e : Write code to c r e a t e an e f f e c t i v e d i f f e r e n t i a l e qu a ti o n s i m u l a t o r f o r a given t a s k Please note t h a t the code should be f u l l y f u n c t i o n a l . No p l a c e h o l d e r s . ", "page_idx": 26}, {"type": "text", "text": "You must a c t autonomously and you w i l l r e c e i v e no human i n p u t a t any s t a g e . You have to r e t u r n as output the complete code f o r completing t h i s task , and c o r r e c t l y improve the code to c r e a t e the most a c c u r a t e and r e a l i s t i c s i m u l a t o r p o s s i b l e . You always w r i t e out the code c o n t e n t s . You always i n d e n t code with t a b s . ", "page_idx": 26}, {"type": "text", "text": "You cannot v i s u a l i z e any g r a p h i c a l output . You e x i s t within a machine . The code can i n c l u d e black box multi \u2212l a y e r p e r c e p t i o n s where r e q u i r e d . ", "page_idx": 26}, {"type": "text", "text": "Use the f u n c t i o n s provided . When c a l l i n g f u n c t i o n s only provide a RFC8259 compliant JSON r e q u e s t f o l l o w i n g t h i s format without d e v i a t i o n . ", "page_idx": 26}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:29:50 ,490 ,490 m u l t i p r o c e s s i n g INFO [ User ] You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r ", "page_idx": 26}, {"type": "text", "text": "System D e s c r i p t i o n : \u2018 ", "page_idx": 26}, {"type": "text", "text": "P r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model ", "page_idx": 26}, {"type": "text", "text": "Here you must model the s t a t e d i f f e r e n t i a l of tumor_volume , and chemotherapy_drug_concentration ; with the i n p u t a c t i o n s of ", "page_idx": 26}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s : \\* Volume of the tumor with u n i t s $\\operatorname{cm}\\wedge3$ \\* C o n c e n t r a t i o n of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathrm{mg}/\\mathrm{m}^{\\wedge}3$ ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "The time u n i t s i s in days ", "page_idx": 26}, {"type": "text", "text": "A d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of \\* tumor_volume : [0.01433 , 1170.861] \\* chemotherapy_drug_concentration : [0 , 9.9975] ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "The t r a i n i n g d a t a s e t c o n s i s t s of 1000 p a t i e n t s , where each p a t i e n t i s observed f o r 60 days ", "page_idx": 26}, {"type": "text", "text": "Modelling goals : ", "page_idx": 26}, {"type": "text", "text": "\\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 26}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : ", "page_idx": 26}, {"type": "text", "text": "\\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e \u221210 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 26}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018 \u2018 \u2018 c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 27}, {"type": "text", "text": "Useful to know : \u2018 \u2018 \u2018   \n\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 27}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 27}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:29:50 ,490 ,490 m u l t i p r o c e s s i n g INFO [ User ] You g e n e r a t e d the f o l l o w i n g code completions , which then had t h e i r parameters optimized to the t r a i n i n g d a t a s e t . Please r e f l e c t on how you can improve the code to minimize the v a l i d a t i o n l o s s to 1e\u22126 or l e s s . The code examples are d e l i n e a t e d by ###. ", "page_idx": 27}, {"type": "text", "text": "Here are your p r ev i ou s i t e r a t i o n s the b e s t programs g e n e r a t e d . Use i t to see i f you have exhausted white box models , i . e . when a white box model r e p e a t s with the same val l o s s and then only add black box models to the white box models : \u2018 \u2018 \u2018   \nI t e r a t i o n 0. Best Val Loss : 43.21586608886719. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume and a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n . The model i n c l u d e s parameters f o r the tumor growth r a t e and drug e f f i c a c y , ", "page_idx": 27}, {"type": "text", "text": "I t e r a t i o n 1. Best Val Loss : 7.463240623474121. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration , r a d i o t h e r a p y _ d o s a g e . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume , a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n , and i n c l u d e s the e f f e c t of r a d i o t h e r a p y dosage . The model i n c l u d e s parameters f o r the tumor growth rate , drug e f f i c a c y , and r a d i o t h e r a p y e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . ", "page_idx": 27}, {"type": "text", "text": "Please l i s t in order of ranked importance ( with the f i r s t f e a t u r e to be a c qu i r e d f i r s t ) and the p e r c e n t a g e of expected improvement of p o t e n t i a l f e a t u r e s to a c q u i r e next , such t h a t the v a l i d a t i o n l o s s i s minimized to 1e\u22126 or l e s s . The p o s s i b l e a d d i t i o n a l f e a t u r e s to s e l e c t are : blood_type , smoking_status , p a t i e n t _ a g e , p a t i e n t _ w e i g h t , p a t i e n t _ h e i g h t , sex alcohol_consumption , p r e v i o u s _ c a n c e r _ t r e a t m e n t s , c o m o r b i d i t i e s , genetic_markers , chemotherapy_dosage , f a m i l y _ h i s t o r y _ o f _ c a n c e r , performance_status , d i e t a r y _ h a b i t s , p h y s i c a l _ a c t i v i t y _ l e v e l , environmental_exposure , medication_adherence , p s y c h o l o g i c a l _ s t r e s s _ l e v e l s , socioeconomic_status , r e s i d e n t i a l _ l o c a t i o n , support_network . ", "page_idx": 27}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:29:50 ,490 ,490 m u l t i p r o c e s s i n g INFO [ Pr o gr e ss : Step 1/0 | R e t r i e s : 0/30 | Token Capacity Used : 1.14% Tokens remaining 126539] ", "page_idx": 27}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:30:23 ,219 ,219 m u l t i p r o c e s s i n g INFO [LLM RESPONSE MESSAGE] MainProcess | 2024\u221205\u221220 07:30:23 ,219 ,219 m u l t i p r o c e s s i n g INFO [ Function Call ] [ f e a t u r e _ a c q u i s i t i o n ] { {\" name \" : \" chemotherapy_dosage \" , \" importance_score \" : 0.25} , {\" name \" : \" genetic_markers \" , \" importance_score \" : 0.20} , {\" name \" : \" p a t i e n t _ a g e \" , \" importance_score \" : 0.15} , {\" name \" : \" p r e v i o u s _ c a n c e r _ t r e a t m e n t s \" , \" importance_score \" : 0.10} , {\" name \" : \" c o m o r b i d i t i e s \" , \" importance_score \" : 0.10} , {\" name \" : \" smoking_status \" , \" importance_score \" : 0.05} , {\" name \" : \" p e r f o r m a n c e _ s t a t u s \" , \" importance_score \" : 0.05} , {\" name \" : \" medication_adherence \" , \" importance_score \" : 0.05} , {\" name \" : \" p s y c h o l o g i c a l _ s t r e s s _ l e v e l s \" , \" importance_score \" : 0.05} ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:30:23 ,219 ,219 m u l t i p r o c e s s i n g INFO [ S e l e c t e d new f e a t u r e ] SELECTED : chemotherapy_dosage Now using : [ \u2019 tumor_volume \u2019 , \u2019 chemotherapy_drug_concentration \u2019 , \u2019 radiotherapy_dosage \u2019 , \u2019 chemotherapy_dosage \u2019 ]   \nMainProcess | 2024\u221205\u221220 07:30:23 ,222 ,222 m u l t i p r o c e s s i n g INFO [ System ]   \nO b j e c t i v e : Write code to c r e a t e an e f f e c t i v e d i f f e r e n t i a l e qu a ti o n s i m u l a t o r f o r a given t a s k   \nPlease note t h a t the code should be f u l l y f u n c t i o n a l . No p l a c e h o l d e r s . ", "page_idx": 27}, {"type": "text", "text": "You must a c t autonomously and you w i l l r e c e i v e no human i n p u t a t any s t a g e . You have to r e t u r n as output the complete code f o r completing t h i s task , and c o r r e c t l y improve the code to c r e a t e the most a c c u r a t e and r e a l i s t i c s i m u l a t o r p o s s i b l e . You always w r i t e out the code c o n t e n t s . You always i n d e n t code with t a b s . ", "page_idx": 27}, {"type": "text", "text": "You cannot v i s u a l i z e any g r a p h i c a l output . You e x i s t within a machine . The code can i n c l u d e black box multi \u2212l a y e r p e r c e p t i o n s where r e q u i r e d . ", "page_idx": 27}, {"type": "text", "text": "Use the f u n c t i o n s provided . When c a l l i n g f u n c t i o n s only provide a RFC8259 compliant JSON r e q u e s t f o l l o w i n g t h i s format without d e v i a t i o n . ", "page_idx": 27}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:30:23 ,225 ,225 m u l t i p r o c e s s i n g INFO [ User ] You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r ", "page_idx": 27}, {"type": "text", "text": "System D e s c r i p t i o n : \u2018 \u2018 \u2018   \nP r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model ", "page_idx": 27}, {"type": "text", "text": "Here you must model the s t a t e d i f f e r e n t i a l of tumor_volume , and chemotherapy_drug_concentration ; with the i n p u t a c t i o n s of radiotherapy_dosage , chemotherapy_dosage . ", "page_idx": 27}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s : ", "page_idx": 27}, {"type": "text", "text": "\\* Volume of the tumor with u n i t s cm^3 \\* C o n c e n t r a t i o n of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathrm{mg}/\\mathrm{m}^{\\wedge}3$ \\* Dosage of the r a d i o t h e r a p y with u n i t s Gy ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\\* Dosage of the chemotherapy drug v i n b l a s t i n e with u n i t s mg/m^3 ", "page_idx": 27}, {"type": "text", "text": "The time u n i t s i s in days ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "A d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of \\* tumor_volume : [0.01433 , 1170.861] \\* chemotherapy_drug_concentration : [0 , 9.9975] \\* r a d i o t h e r a p y _ d o s a g e : [0 , 2 . 0 ] \\* chemotherapy_dosage : [0 , 5 . 0 ] ", "page_idx": 27}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 28}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018 \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e \u221210 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 28}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt )   \nUseful to know : \u2018 \u2018 \u2018   \n$*$ You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 2 out of 20. ", "page_idx": 28}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:30:23 ,225 ,225 m u l t i p r o c e s s i n g INFO [ User ] You g e n e r a t e d the f o l l o w i n g code completions , which then had t h e i r parameters optimized to the t r a i n i n g d a t a s e t . Please r e f l e c t on how you can improve the code to minimize the v a l i d a t i o n l o s s to 1e\u22126 or l e s s . The code examples are d e l i n e a t e d by ###. ", "page_idx": 28}, {"type": "text", "text": "Here are your p r ev i ou s i t e r a t i o n s the b e s t programs g e n e r a t e d . Use i t to see i f you have exhausted white box models , i . e . when a white box model r e p e a t s with the same val l o s s and then only add black box models to the white box models : \u2018 \u2018   \nI t e r a t i o n 0. Best Val Loss : 43.21586608886719. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume and a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n . The model i n c l u d e s parameters f o r the tumor growth r a t e and drug e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t .   \nI t e r a t i o n 1. Best Val Loss : 7.463240623474121. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration , r a d i o t h e r a p y _ d o s a g e . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume , a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n , and i n c l u d e s the e f f e c t of r a d i o t h e r a p y dosage . The model i n c l u d e s parameters f o r the tumor growth rate , drug e f f i c a c y , and r a d i o t h e r a p y e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . ", "page_idx": 28}, {"type": "text", "text": "Here are the top code completions so f a r t h a t you have generated , s o r t e d f o r the lowest v a l i d a t i o n l o s s l a s t ", "page_idx": 28}, {"type": "text", "text": "Val Loss : 43.2 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 79 , chemotherapy_drug_concentration val l o s s : 7 . 4 2 ) I t e r a t i o n : 0 ", "page_idx": 28}, {"type": "text", "text": "### ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "import t o r c h import t o r c h . nn as nn from typing import Tuple ", "page_idx": 28}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Tumor growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 1 ) ) # Drug e f f i c a c y ", "page_idx": 28}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # Tumor growth model based on Gompertzian growth d_tumor_volume__dt $=$ s e l f . alpha \\* t o r c h . log (1170.861 / tumor_volume ) $^*$ tumor_volume # Drug e f f e c t on tumor volume d_tumor_volume__dt $-=$ s e l f . beta \\* chemotherapy_drug_concentration $^*$ tumor_volume # Assuming c o n s t a n t drug c o n c e n t r a t i o n decay drug_decay_rate $=~0.1$ d_chemotherapy_drug_concentration__dt $=$ \u2212drug_decay_rate \\* chemotherapy_drug_concentration r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 28}, {"type": "text", "text": "optimized_parameters $=$ { \u2019 alpha \u2019 : 0.00944874994456768 , \u2019 beta \u2019 : 0.031064828857779503} ### ", "page_idx": 28}, {"type": "text", "text": "Val Loss : 7.46 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 7.51 , chemotherapy_drug_concentration val l o s s : 7 . 4 2 ) I t e r a t i o n : 1 ", "page_idx": 28}, {"type": "text", "text": "### ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "import t o r c h", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "import t o r c h . nn as nn from typing import Tuple ", "page_idx": 28}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) ", "page_idx": 28}, {"type": "text", "text": "# Define the parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r (0.00944874994456768) ) # Optimized tumor growth r a t e ", "page_idx": 29}, {"type": "text", "text": "s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (0.031064828857779503) ) # Optimized drug e f f i c a c y # Define the parameter f o r the r a d i o t h e r a p y e f f e c t s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Radiotherapy e f f i c a c y def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentratio $:$ t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # Tumor growth model based on Gompertzian growth d_tumor_volume__dt $=$ s e l f . alpha $*$ t o r c h . log (1170.861 / tumor_volume ) \\* tumor_volume # Drug e f f e c t on tumor volume d_tumor_volume__dt $-=$ s e l f . beta $*$ chemotherapy_drug_concentration $^*$ tumor_volume # Radiotherapy e f f e c t on tumor volume d_tumor_volume__dt $-=$ s e l f . gamma \\* r a d i o t h e r a p y _ d o s a g e \\* tumor_volume # Assuming c o n s t a n t drug c o n c e n t r a t i o n decay drug_decay_rate $=~0.1$ d_chemotherapy_drug_concentration__dt $=$ \u2212drug_decay_rate \\* chemotherapy_drug_concentration r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 29}, {"type": "text", "text": "optimized_parameters $=$ { \u2019 alpha \u2019 : 0.025857042521238327 , \u2019 beta \u2019 : 0.025862518697977066 , \u2019gamma \u2019 : 0.03730243816971779} ### ", "page_idx": 29}, {"type": "text", "text": "", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Code Skeleton : \u2018 \u2018 c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor chemotherapy_dosage : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 29}, {"type": "text", "text": "Please r e f l e c t on how you can improve the code to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e , and be i n t e r p r e t a b l e . Use the code s k e l e t o n above to provide the code . Please a l s o s p e c i f y only the i n p u t parameters t h a t you want to i n c l u d e in the model in from the s k e l e t o n code . Think step \u2212by\u2212s t e p . Where a p p l i c a b l e use the values of the optimized parameters to reason how the code can be improved to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e . This i s f o r g e n e r a t i n g new code f o r the next i t e r a t i o n 2 out of 20. ", "page_idx": 29}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:30:23 ,225 ,225 m u l t i p r o c e s s i n g INFO [ Pr o gr e s s : Step 1/0 | R e t r i e s : 0/30 | Token Capacity Used : 1.87% | Tokens remaining 125608] ", "page_idx": 29}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:33:55 ,671 ,671 m u l t i p r o c e s s i n g INFO [LLM RESPONSE MESSAGE]   \nMainProcess | 2024\u221205\u221220 07:33:55 ,671 ,671 m u l t i p r o c e s s i n g INFO [ Function Call ] [ c o m p l e t e _ S t a t e D i f f e r e n t i a l _ c o d e ] { \" S t a t e D i f f e r e n t i a l _ c o d e \" : \" import t o r c h \\ nimport t o r c h . nn as nn \\ nfrom typing import Tuple \\ n \\ n c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : \\ n def _ _ i n i t _ _ ( s e l f ) : \\ n super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) \\ n # Define the parameters f o r the tumor growth model \\ n s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r (0.025857042521238327) ) # Optimized tumor growth r a t e \\ n s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (0.025862518697977066) ) # Optimized drug e f f i c a c y \\ n # Define the parameter f o r the r a d i o t h e r a p y e f f e c t \\ n s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r (0.03730243816971779) ) # Optimized r a d i o t h e r a p y e f f i c a c y \\ n # Define the parameter f o r the chemotherapy e f f e c t \\ n s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Chemotherapy dosage e f f i c a c y \\ n \\ n def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor ) \u2212> Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : \\ n # Tumor growth model based on Gompertzian growth \\ n d_tumor_volume__dt = s e l f . alpha $^*$ t o r c h . log (1170.861 / tumor_volume ) \\* tumor_volume \\ n # Drug e f f e c t on tumor volume \\ n d_tumor_volume__dt $-=$ s e l f . beta \\* chemotherapy_drug_concentration \\* tumor_volume \\ n # Radiotherapy e f f e c t on tumor volume \\ n d_tumor_volume__dt $-=$ s e l f . gamma $^*$ r a d i o t h e r a p y _ d o s a g e $*$ tumor_volume \\ n # Chemotherapy dosage e f f e c t on tumor volume \\ n d_tumor_volume__dt $-=$ s e l f . d e l t a \\* chemotherapy_dosage \\* tumor_volume \\ n \\ n # Assuming c o n s t a n t drug c o n c e n t r a t i o n decay \\ n drug_decay_rate $\\qquad\\qquad\\qquad=\\quad0.1\\,\\backslash\\,{\\bf n}$ d_chemotherapy_drug_concentration__dt = \u2212drug_decay_rate \\* chemotherapy_drug_concentration \\ n \\ n r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) \" , \" c o d e _ d e s c r i p t i o n \" : \" This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume , a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n , and i n c l u d e s the e f f e c t s of both r a d i o t h e r a p y and chemotherapy dosages . The model i n c l u d e s parameters f o r the tumor growth ra te , drug e f f i c a c y , r a d i o t h e r a p y e f f i c a c y , and chemotherapy dosage e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . \" , \" m o d e l _ i n p u t _ v a r i a b l e s \" : [ \" tumor_volume \" , \" chemotherapy_drug_concentration \" , \" r a d i o t h e r a p y _ d o s a g e \" , \" chemotherapy_dosage \" ]   \nMainProcess | 2024\u221205\u221220 07:34:06 ,547 ,547 m u l t i p r o c e s s i n g INFO Early st op p in g t r i g g e r e d a t epoch 1120   \nMainProcess | 2024\u221205\u221220 07:34:06 ,560 ,560 m u l t i p r o c e s s i n g INFO [D3\u2212f e a t u r e \u2212s e l e c t | Cancer \u2212random \u2212f e a t u r e s | 12][ Generation 2] | Top F i t n e s s : 6.335662364959717 | Num Programs : 3 | Mean F i t n e s s : 19.00492302576701 | F i t n e s s e s : [6.335662364959717 , 7.463240623474121 , 43.21586608886719] | Current Gen Val Loss : 6.335662364959717   \nMainProcess | 2024\u221205\u221220 07:34:06 ,560 ,560 m u l t i p r o c e s s i n g INFO [D3\u2212f e a t u r e \u2212s e l e c t | Cancer \u2212random \u2212f e a t u r e s | 12][GEN RESULT] { \u2019 method \u2019 : \u2019D3\u2212f e a t u r e \u2212s e l e c t \u2019 , \u2019 env_name \u2019 : \u2019 Cancer \u2212random \u2212f e a t u r e s \u2019 , \u2019 seed \u2019 : 12 , \u2019 generation \u2019 : 2 , \u2019 t o p _ f i t n e s s \u2019 : 6.335662364959717 , \u2019 num_programs \u2019 : 3 , \u2019 mean_fitness \u2019 : 19.00492302576701 , \u2019 f i t n e s s e s \u2019 : [6.335662364959717 , 7.463240623474121 , 43.21586608886719] , \u2019 c u r r e n t _ g e n _ v a l _ l o s s \u2019 : 6.335662364959717}   \nMainProcess | 2024\u221205\u221220 07:34:06 ,561 ,561 m u l t i p r o c e s s i n g INFO [ System ]   \nO b j e c t i v e : Write code to c r e a t e an e f f e c t i v e d i f f e r e n t i a l e qu a ti o n s i m u l a t o r f o r a given t a s k .   \nPlease note t h a t the code should be f u l l y f u n c t i o n a l . No p l a c e h o l d e r s . ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "You must a c t autonomously and you w i l l r e c e i v e no human i n p u t a t any s t a g e . You have to r e t u r n as output the complete code f o r completing t h i s task , and c o r r e c t l y improve the code to c r e a t e the most a c c u r a t e and r e a l i s t i c s i m u l a t o r p o s s i b l e . You always w r i t e out the code c o n t e n t s . You always i n d e n t code with t a b s . You cannot v i s u a l i z e any g r a p h i c a l output . You e x i s t within a machine . The code can i n c l u d e black box multi \u2212l a y e r p e r c e p t i o n s where ", "page_idx": 29}, {"type": "text", "text": "Use the f u n c t i o n s provided . When c a l l i n g f u n c t i o n s only provide a RFC8259 compliant JSON r e q u e s t f o l l o w i n g t h i s format without d e v i a t i o n . ", "page_idx": 29}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:34:06 ,561 ,561 m u l t i p r o c e s s i n g INFO [ User ] You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r ", "page_idx": 29}, {"type": "text", "text": "System D e s c r i p t i o n : \u2018 \u2018 \u2018   \nP r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model ", "page_idx": 29}, {"type": "text", "text": "Here you must model the s t a t e d i f f e r e n t i a l of tumor_volume , and chemotherapy_drug_concentration ; with the i n p u t a c t i o n s of ", "page_idx": 29}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s :   \n\\* Volume of the tumor with u n i t s $\\operatorname{cm}\\wedge3$   \n\\* C o n c e n t r a t i o n of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathrm{mg}/\\mathrm{m}^{\\wedge}3$ ", "page_idx": 29}, {"type": "text", "text": "The time u n i t s i s in days . ", "page_idx": 29}, {"type": "text", "text": "A d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of : \\* tumor_volume : [0.01433 , 1170.861] ", "page_idx": 30}, {"type": "text", "text": "\\* chemotherapy_drug_concentration : [0 , 9.9975] ", "page_idx": 30}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 30}, {"type": "text", "text": "The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e \u221210 or l e s s . The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 30}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018 \u2018 \u2018 c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentratio $:$ t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 30}, {"type": "text", "text": "Useful to know : \u2018 \u2018 \u2018   \n$*$ You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 30}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 30}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:34:06 ,561 ,561 m u l t i p r o c e s s i n g INFO [ User ] You g e n e r a t e d the f o l l o w i n g code completions , which then had t h e i r parameters optimized to the t r a i n i n g d a t a s e t . Please r e f l e c t on how you can improve the code to minimize the v a l i d a t i o n l o s s to 1e\u22126 or l e s s . The code examples are d e l i n e a t e d by ###. ", "page_idx": 30}, {"type": "text", "text": "Here are your p r ev i ou s i t e r a t i o n s the b e s t programs g e n e r a t e d . Use i t to see i f you have exhausted white box models , i . e . when a white box model r e p e a t s with the same val l o s s and then only add black box models to the white box models : \u2018 \u2018 \u2018   \nI t e r a t i o n 0. Best Val Loss : 43.21586608886719. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume and a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n . The model i n c l u d e s parameters f o r the tumor growth r a t e and drug e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . ", "page_idx": 30}, {"type": "text", "text": "I t e r a t i o n 1. Best Val Loss : 7.463240623474121. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration , r a d i o t h e r a p y _ d o s a g e . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume , a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n , and i n c l u d e s the e f f e c t of r a d i o t h e r a p y dosage . The model i n c l u d e s parameters f o r the tumor growth rate , drug e f f i c a c y , and r a d i o t h e r a p y e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . ", "page_idx": 30}, {"type": "text", "text": "I t e r a t i o n 2. Best Val Loss : 6.335662364959717. S e l e c t e d F e a t u r e s : tumor_volume , chemotherapy_drug_concentration , radiotherapy_dosage , chemotherapy_dosage . Model d e s c r i p t i o n : This i s a white box model t h a t uses a Gompertzian growth model f o r the tumor volume , a simple decay model f o r the chemotherapy drug c o n c e n t r a t i o n , and i n c l u d e s the e f f e c t s of both r a d i o t h e r a p y and chemotherapy dosages . The model i n c l u d e s parameters f o r the tumor growth rate , drug e f f i c a c y , r a d i o t h e r a p y e f f i c a c y , and chemotherapy dosage e f f i c a c y , which can be optimized based on the observed t r a i n i n g d a t a s e t . ", "page_idx": 30}, {"type": "text", "text": "Please l i s t in order of ranked importance ( with the f i r s t f e a t u r e to be a c qu i r e d f i r s t ) and the p e r c e n t a g e of expected improvement of p o t e n t i a l f e a t u r e s to a c q u i r e next , such t h a t the v a l i d a t i o n l o s s i s minimized to 1e\u22126 or l e s s . The p o s s i b l e a d d i t i o n a l f e a t u r e s to s e l e c t are : blood_type , smoking_status , p a t i e n t _ a g e , p a t i e n t _ w e i g h t , p a t i e n t _ h e i g h t , sex alcohol_consumption , p r e v i o u s _ c a n c e r _ t r e a t m e n t s , c o m o r b i d i t i e s , genetic_markers , f a m i l y _ h i s t o r y _ o f _ c a n c e r , performance_status , d i e t a r y _ h a b i t s , p h y s i c a l _ a c t i v i t y _ l e v e l , environmental_exposure , medication_adherence , p s y c h o l o g i c a l _ s t r e s s _ l e v e l s , socioeconomic_status , r e s i d e n t i a l _ l o c a t i o n , support_network . ", "page_idx": 30}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:34:06 ,561 ,561 m u l t i p r o c e s s i n g INFO [ Pr o gr e ss : Step 1/0 | R e t r i e s : 0/30 | Token Capacity Used : 1.23% | Tokens remaining 126425] ", "page_idx": 30}, {"type": "text", "text": "MainProcess | 2024\u221205\u221220 07:34:55 ,703 ,703 m u l t i p r o c e s s i n g INFO [LLM RESPONSE MESSAGE] MainProcess | 2024\u221205\u221220 07:34:55 ,704 ,704 m u l t i p r o c e s s i n g INFO [ Function Call ] [ f e a t u r e _ a c q u i s i t i o n ] { \" f e a t u r e s \" : [ ", "page_idx": 30}, {"type": "text", "text": "\"name \" : \" genetic_markers \" \" importance_score \" : 0.25 \"name \" : \" p r e v i o u s _ c a n c e r _ t r e a t m e n t s \" \" importance_score \" : 0.2 \"name \" : \" c o m o r b i d i t i e s \" \" importance_score \" : 0.15 \"name \" : \" p a t i e n t _ a g e \" \" importance_score \" : 0.1 \"name \" : \" smoking_status \" \" importance_score \" : 0.1 \"name \" : \" p e r f o r m a n c e _ s t a t u s \" \" importance_score \" : 0.05 ", "page_idx": 30}, {"type": "text", "text": "\"name \" : \" medication_adherence \" , \" importance_score \" : 0.05 } \"name \" : \" p s y c h o l o g i c a l _ s t r e s s _ l e v e l s \" , \" importance_score \" : 0.05 } , { \"name \" : \" environmental_exposure \" , \" importance_score \" : 0.05 } ", "page_idx": 31}, {"type": "text", "text": "G.3 Other Datasets Discovered White-Box Models ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In the main paper we presented the best Warfarin discovered white-box model, however after running D3-white-box across all the other datasets, we include a few of the white-box models that it discovered and generated for each other respective dataset used that we compared against. Of particular interest, is that these white-box models fti the dataset well, and can even outperform some of the other neural network based baselines\u2014we highlight that with the right functional form of the white-box model, we can achieve better performance than a largely over-parameterized neural network model, that has signficiantly more parameters (such as an RNN with 18,000 parameters), than a white-box model with 5-15 parameters. ", "page_idx": 31}, {"type": "text", "text": "Lung Cancer ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Val Loss : 0.352 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 0.352) I t e r a t i o n : 7   \n###   \n\u2018 \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the tumor growth model # I n i t i a l i z e parameters with values c l o s e r to the optimized values from i t e r a t i o n 6 s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 2 8 ) ) s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (1000.33) ) # I n i t i a l i z e gamma as a small p o s i t i v e value s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 1 ) ) # I n i t i a l i z e d e l t a with a value c l o s e r to the optimized value from i t e r a t i o n 6 s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 7 0 5 ) ) # I n t r o d u c e e p s i l o n to r e p r e s e n t a new b i o l o g i c a l term , e . g . , immune response s e l f . e p s i l o n $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 1 ) ) def forward ( s e l f , tumor_volume : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor ] : # Ensure tumor_volume i s non\u2212n e g a t i v e tumor_volume $=$ t o r c h . clamp ( tumor_volume , min =0) # C a l c u l a t e the growth term , n e c r o s i s term , a n g i o g e n e s i s term , and new b i o l o g i c a l term growth_term $=$ s e l f . alpha \\* tumor_volume \\* (1 \u2212 tumor_volume / s e l f . beta ) n e c r o s i s _ t e r m $=$ \u2212s e l f . gamma $^\\ast$ tumor_volume angiogen esis_term $=$ s e l f . d e l t a $^\\ast$ t o r c h . s q r t ( tumor_volume ) immune_response_term $=$ \u2212s e l f . e p s i l o n $^*$ tumor_volume # Rate of change of tumor volume d_tumor_volume__dt $=$ growth_term $^+$ n e c r o s i s _ t e r m $^+$ angiog enesis_term $^+$ immune_response_term r e t u r n ( d_tumor_volume__dt , )   \noptimized_parameters = { \u2019 alpha \u2019 : 0.0026573685463517904 , \u2019 beta \u2019 : 1000.9459838867188 , \u2019gamma \u2019 : \u22120.007219356019049883 , \u2019 de l t a 0.7442551851272583 , \u2019 epsilon \u2019 : \u22120.007219356019049883} ", "page_idx": 31}, {"type": "text", "text": "### ", "page_idx": 31}, {"type": "text", "text": "Lung Cancer (with Chemo.) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Val Loss : 2.45 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 4.91 , chemotherapy_drug_concentration val l o s s : 2.68 e \u221206) I t e r a t i o n : 0   \n###   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the model s e l f . k_growth $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) s e l f . k_decay $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) s e l f . k_chemo_effect $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentratio $:$ t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # C a l c u l a t e the r a t e of change of tumor volume d_tumor_volume__dt $=$ s e l f . k_growth $*$ tumor_volume \u2212 s e l f . k_chemo_effect $^*$ chemotherapy_drug_concentration $^*$ tumor_volume # C a l c u l a t e the r a t e of change of chemotherapy drug c o n c e n t r a t i o n d_chemotherapy_drug_concentration__dt $=$ chemotherapy_dosage \u2212 s e l f . k_decay $*$ chemotherapy_drug_concentration r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt )   \noptimized_parameters = { \u2019 k_growth \u2019 : 0.04456980526447296 , \u2019 k_decay \u2019 : 0.4996906816959381 , \u2019 k_chemo_effect \u2019 : 0.027509577572345734} ", "page_idx": 31}, {"type": "text", "text": "Lung Cancer (with Chemo. & Radio.) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Val Loss : 1.42 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 2.85 , chemotherapy_drug_concentration val l o s s : 1.96 e \u221211) I t e r a t i o n : 0 ", "page_idx": 31}, {"type": "text", "text": "### ", "page_idx": 32}, {"type": "text", "text": "import t o r c h import t o r c h . nn as nn from typing import Tuple ", "page_idx": 32}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the model s e l f . k_growth $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) s e l f . k_decay $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) s e l f . k_chemo_effect $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22120.05) ) s e l f . k _ r a d i o _ s e n s i t i v i t y $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22120.02) ) ", "page_idx": 32}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # C a l c u l a t e the d i f f e r e n t i a l of tumor volume d_tumor_volume__dt $=$ s e l f . k_growth $*$ tumor_volume \u2212 s e l f . k_chemo_effect $^*$ chemotherapy_drug_concentration $^*$ tumor_volume s e l f . k _ r a d i o _ s e n s i t i v i t y $^*$ r a d i o t h e r a p y _ d o s a g e \\* tumor_volume # C a l c u l a t e the d i f f e r e n t i a l of chemotherapy drug c o n c e n t r a t i o n d_chemotherapy_drug_concentration__dt $=$ chemotherapy_dosage \u2212 s e l f . k_decay $*$ chemotherapy_drug_concentration ", "page_idx": 32}, {"type": "text", "text": "optimized_parameters $=\\{\\mathrm{~}^{\\cdot}\\,\\mathbf{k}.$ _growth \u2019 : 0.04546591639518738 , \u2019 k_decay \u2019 : 0.49999910593032837 , \u2019 k_chemo_effect \u2019 : 0.027269458398222923 , k _ r a d i o _ s e n s i t i v i t y \u2019 : 0.04899420961737633} ", "page_idx": 32}, {"type": "text", "text": "### ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Plankton Microcosm ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Val Loss : 5.76 e \u221206 ( Where the val l o s s per dimension i s p r e y _ p o p u l a t i o n val l o s s : 1.18 e \u221205 , i n t e r m e d i a t e _ p o p u l a t i o n val l o s s : 1.37 e \u221206 , t o p _ p r e d a t o r s _ p o p u l a t i o n val l o s s : 4.07 e \u221206) I t e r a t i o n : 13 ", "page_idx": 32}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the model with i n i t i a l values c l o s e to optimized parameters s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 4 ) ) # Prey growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 2 3 ) ) # Prey death r a t e due to p r e d a t i o n s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 2 3 ) ) # I n t e r m e d i a t e p r e d a t o r growth r a t e s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 3 ) ) # I n t e r m e d i a t e p r e d a t o r death r a t e s e l f . e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 6 ) ) # Top p r e d a t o r growth r a t e s e l f . t h e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22120.12) ) # Top p r e d a t o r death r a t e s e l f . kappa $=$ nn . Parameter ( t o r c h . t e n s o r ( 2 . 2 ) ) # Prey c a r r y i n g c a p a c i t y s e l f . sigma = nn . Parameter ( t o r c h . t e n s o r ( 1 . 4 ) ) # I n t e r m e d i a t e p r e d a t o r c a r r y i n g c a p a c i t y s e l f . rho $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 7 ) ) # Top p r e d a t o r c a r r y i n g c a p a c i t y s e l f . omega $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 2 ) ) # E f f e c t of top p r e d a t o r s on i n t e r m e d i a t e p r e d a t o r m o r t a l i t y s e l f . p s i $\\r=\\textrm{\\textmu m}$ . Parameter ( t o r c h . t e n s o r ( 0 . 1 2 ) ) # E f f e c t of prey abundance on i n t e r m e d i a t e p r e d a t o r growth s e l f . phi = nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 3 ) ) # Nonlinear i n t e r a c t i o n term f o r prey and i n t e r m e d i a t e p r e d a t o r s s e l f . xi $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 3 ) ) # Nonlinear i n t e r a c t i o n term f o r i n t e r m e d i a t e p r e d a t o r s and top p r e d a t o r s def forward ( s e l f , p r e y _ p o p u l a t i o n : t o r c h . Tensor , i n t e r m e d i a t e _ p o p u l a t i o n : t o r c h . Tensor , t o p _ p r e d a t o r s _ p o p u l a t i o n : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # Prey growth l i m i t e d by c a r r y i n g c a p a c i t y d_prey_population__dt $=$ s e l f . alpha $*$ p r e y _ p o p u l a t i o n $^*$ (1 \u2212 p r e y _ p o p u l a t i o n / s e l f . kappa ) \u2212 s e l f . beta $^*$ p r e y _ p o p u l a t i o n $^\\ast$ i n t e r m e d i a t e _ p o p u l a t i o n # I n t e r m e d i a t e p r e d a t o r growth with n o n l i n e a r i n t e r a c t i o n term d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t $=$ s e l f . gamma \\* i n t e r m e d i a t e _ p o p u l a t i o n \\* (1 \u2212 i n t e r m e d i a t e _ p o p u l a t i o n / s e l f . sigma ) $^\\ast$ ( s e l f . p s i \\* p r e y _ p o p u l a t i o n ) \u2212 s e l f . d e l t a \\* i n t e r m e d i a t e _ p o p u l a t i o n \\* t o p _ p r e d a t o r s _ p o p u l a t i o n \u2212 s e l f . phi \\* p r e y _ p o p u l a t i o n \\*\\*2 \\* i n t e r m e d i a t e _ p o p u l a t i o n # Top p r e d a t o r growth with e f f e c t of i n t e r m e d i a t e p r e d a t o r s and n o n l i n e a r i n t e r a c t i o n term d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t $=$ s e l f . e t a \\* t o p _ p r e d a t o r s _ p o p u l a t i o n \\* (1 \u2212 t o p _ p r e d a t o r s _ p o p u l a t i o n / s e l f . rho ) \u2212 s e l f . t h e t a $^*$ t o p _ p r e d a t o r s _ p o p u l a t i o n $^\\ast$ i n t e r m e d i a t e _ p o p u l a t i o n \u2212 s e l f . xi $^*$ i n t e r m e d i a t e _ p o p u l a t i o n \\*\\*2 $^*$ t o p _ p r e d a t o r s _ p o p u l a t i o n r e t u r n ( d_prey_population__dt , d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t , d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t )   \n\u2018 \u2018 \u2018   \noptimized_parameters = { \u2019 alpha \u2019 : 0.04999999329447746 , \u2019 beta \u2019 : 0.23999999463558197 , \u2019gamma \u2019 : 0.239999920129776 , \u2019 de l t a \u2019 : ", "page_idx": 32}, {"type": "text", "text": "0.13999998569488525 , \u2019 eta \u2019 : 0.05000030994415283 , \u2019 t he t a \u2019 : \u22120.1299992799758911 , \u2019 kappa \u2019 : 2.2099997997283936 , \u2019 sigma \u2019 :   \n1.3900007009506226 , \u2019 rho \u2019 : 0.6900002956390381 , \u2019omega \u2019 : 0.019999999552965164 , \u2019 psi \u2019 : 0.12999995052814484 , \u2019 phi \u2019 :   \n0.020000003278255463 , \u2019 xi \u2019 : 0.020000090822577477} ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Yes, in this paper we introduce the data-driven discovery (D3) framework, verify it experimentally against existing modeling ML methods, and verify the new novel discovered PK model on Wafarin with pharmacometrician experts. We detail the framework in Section 3, provide full implementation details in Appendix F, provide the experimental results to verify our core claims of discovering well-fitting models in Section 6, and our discovered Wafarin PK model as a case study in Section 6.1. We further discuss key assumptions of the method in Section 3 and outline important limitations in Section 7, with the \u201climitations & future work section\u201d. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have a dedicated limitations section in Section 7, titled \u201climitations & future work section\u201d. We also discuss the key assumptions of the method when we introduce it, such as in Section 3, for example in Section 3.3 we state the assumptions made, and how relevant they are in clinical practice, where D3 is designed to work. We also discuss the computational efficiency of D3 in Appendix F. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 33}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not include any theoretical results, therefore this question is not applicable. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: To ensure thorough reproducibility, in addition to D3\u2019s description within the main paper (Section 3), we include full implementation details of D3 in Appendix F. To fully reproduce the experiments we detail the exact datasets, and how they were preprocessed or generated in Appendix B, how the respective evaluation metrics were computed in Appendix C, along with the corresponding full implementation details of the implemented benchmark baseline methods in Appendix E. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Code is available at https://github.com/samholt/ DataDrivenDiscovery. Regarding each dataset, we provide full implementation details to either re-recreate the dataset or links to exactly where the dataset comes from, this is all detailed in the extensive Appendix B. To reproduce the experimental results we provide detailed implementations of D3 in Appendix F, the evaluation metrics in Appendix C and the baseline implementation details in Appendix E. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide this full information within the dataset implementation section in Appendix B, evaluation section Appendix C, and the training details for each method in the respective implementation section of Appendices $\\boldsymbol{\\mathrm E}$ and F. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We accompany results with error bars in the main results, as shown in Section 6. We explain how these are computed, and that they performed over random seed runs, which is provided in the main experimental results section, Section 6, and expanded in the evaluation metrics section, Appendix C. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide full details of the compute used in Appendix C, and further detail the LLM used in Appendix F and for the respective baselines in Appendix E. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, and we provide an explicit ethical consideration discussion section in Section 7. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes, and we provide an explicit broader impact discussion section in Section 7. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: It is not applicable, however, we attempt to cover such discussions in the \u201cBroader Impact and Ethical Considerations\u201d section in Section 7. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 37}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes we provide full creditiation to the original creators of the synthetic datasets, and or provide the links directly alongside licenses for the public datasets used within the paper, and detail all of this in Appendix B. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not release any new assets, therefore this is not applicable. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not perform any crowdsourcing, or research directly with human subjects, therefore this is not applicable. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: This is not applicable. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]