[{"figure_path": "TY9VoSZZIA/figures/figures_6_1.jpg", "caption": "Figure 1: (a) Difference between 2sED and lower 2sED can not be appreciated, which means that the second is a tight lower bound of the first. Here, the lower 2sED and 2sED of MLP 54-16-7 using 100 Covertype samples and 100 vectors of parameters for the Monte Carlo estimation of FN is shown; (b)Lower 2sED and 2sED of CNN 7-5|10-50-34-10 using 100 MNIST samples and 100 vectors of parameters for the Monte Carlo estimation of FN are extremely close.", "description": "This figure compares the two-scale effective dimension (2sED) and its lower bound for two different models (MLP and CNN) on two datasets (Covertype and MNIST).  The results show that the lower bound is very close to the actual 2sED, making the lower bound a useful approximation for model selection.", "section": "7 Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_7_1.jpg", "caption": "Figure 4: Impact of \u03c3\u00b2 on ds for MLP 54-16-7. The 2sED is estimated with a fixed seed varying \u03c3\u00b2.", "description": "This figure shows the impact of the variance (\u03c3\u00b2) on the two-scale effective dimension (2sED) for a specific MLP model (MLP 54-16-7).  The 2sED is calculated for different values of \u03c3\u00b2, keeping the random seed fixed to maintain consistency in the estimation.  The x-axis represents the covering radius (\u03b5), and the y-axis shows the 2sED values.", "section": "7 Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_8_1.jpg", "caption": "Figure 3: (a) Estimated lower 2sED of three different MLP architectures using 100 Covertype samples and 100 different vectors of parameters for the Monte Carlo estimation of FN; (b) Estimated lower 2sED of three different CNN architectures using 100 CIFAR10 samples and 100 different vectors of parameters for the Monte Carlo estimation of FN; (c) Training loss plots of MLPs on 10000 random Covertype samples using Adam with learning rate 1e-3 and a batch size 64; (d) Training loss plots of CNNs on CIFAR10 using Adam optimizer with learning rate 1e-3 and a batch size 512;(e) Training loss plot of MLPs on 100000 Covertype samples using Adam optimizer with learning rate 1e-\u00b3 and a batch size 64; (f) Training loss plots of CNNs on augmented CIFAR10 (double the original size) using Adam optimizer with learning rate 1e-3 and a batch size 512.", "description": "This figure shows the estimated lower 2sED for three different MLP and CNN architectures using 100 Covertype and CIFAR10 samples, respectively.  It also displays training loss plots for these models trained on 10000 and 100000 Covertype samples, and on CIFAR10 using Adam optimizer with specific learning rate and batch size. The plots visualize the relationship between the lower 2sED and the training loss, indicating how the complexity measure correlates with the model's performance.", "section": "7 Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_21_1.jpg", "caption": "Figure 4: Impact of \u03c3\u00b2 on d\u03c2 for MLP 54-16-7. The 2sED is estimated with a fixed seed varying \u03c3\u00b2.", "description": "This figure shows the impact of the variance (\u03c3\u00b2) of the stochastic perturbation added to the MLP model on the two-scale effective dimension (2sED). The 2sED is computed for different values of \u03c3\u00b2, and the results are plotted against the covering radius (\u03b5).  The plot demonstrates how the 2sED changes as the stochasticity of the model varies.", "section": "7 Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_21_2.jpg", "caption": "Figure 5: Impact of \u03c3\u00b2 on d\u03c2 for CNN 7-5|10-50-34-10. The 2sED is estimated with a fixed seed varying \u03c3\u00b2.", "description": "This figure shows how the choice of variance (\u03c3\u00b2) in the stochastic approximation of the CNN model affects the two-scale effective dimension (2sED).  The x-axis represents the covering radius (\u03b5), and the y-axis represents the 2sED value. Multiple lines are plotted, each corresponding to a different value of \u03c3\u00b2. The plot demonstrates the stability of the 2sED measure across various variance levels, suggesting robustness to the level of stochasticity introduced into the model.", "section": "7 Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_22_1.jpg", "caption": "Figure 6. This figure plots the lower 2sED of CNNs estimated using 100 MNIST samples and 100 vectors of parameters for the Monte Carlo approximation.", "description": "This figure shows the lower 2sED (two-scale effective dimension) for three different CNN (convolutional neural network) architectures. The lower 2sED is a measure of model complexity.  The x-axis represents the covering radius (\u03b5), a parameter used in the calculation.  The y-axis shows the lower 2sED values.  The plot illustrates how the complexity (lower 2sED) changes as a function of the covering radius for the three different CNN models. Each model exhibits a unique pattern of complexity change.", "section": "Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_22_2.jpg", "caption": "Figure 7: Training loss plots of CNNs on MNIST using Adam with learning rate 1e-3 and a batch size 256.", "description": "This figure shows the training loss curves for three different CNN architectures trained on the MNIST dataset using the Adam optimizer with a learning rate of 1e-3 and a batch size of 256.  The x-axis represents the number of training epochs, and the y-axis shows the training loss. The three CNN architectures have different numbers and sizes of convolutional layers, which is reflected in their training loss curves.  The curves demonstrate how the training loss decreases over time for each architecture, indicating the model's learning progress.", "section": "7 Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_23_1.jpg", "caption": "Figure 7: Training loss plots of CNNs on MNIST using Adam with learning rate 1e-3 and a batch size 256.", "description": "This figure shows the training loss curves for three different CNN architectures trained on the MNIST dataset using the Adam optimizer with a learning rate of 1e-3 and a batch size of 256.  The x-axis represents the number of epochs (training iterations), and the y-axis represents the training loss. The three CNNs differ in their architectures (number and size of convolutional layers). The plot shows how the training loss decreases over epochs for each CNN, indicating the training progress. By comparing the curves, one can gain insights into the relative training efficiency and convergence speed of the different architectures.", "section": "7 Experiments"}, {"figure_path": "TY9VoSZZIA/figures/figures_23_2.jpg", "caption": "Figure 3: (a) Estimated lower 2sED of three different MLP architectures using 100 Covertype samples and 100 different vectors of parameters for the Monte Carlo estimation of FN; (b) Estimated lower 2sED of three different CNN architectures using 100 CIFAR10 samples and 100 different vectors of parameters for the Monte Carlo estimation of FN; (c) Training loss plots of MLPs on 10000 random Covertype samples using Adam with learning rate 1e-3 and a batch size 64; (d) Training loss plots of CNNs on CIFAR10 using Adam optimizer with learning rate 1e-3 and a batch size 512;(e) Training loss plot of MLPs on 100000 Covertype samples using Adam optimizer with learning rate 1e-3 and a batch size 64; (f) Training loss plots of CNNs on augmented CIFAR10 (double the original size) using Adam optimizer with learning rate 1e-3 and a batch size 512.", "description": "This figure shows the estimated lower 2sED for three different MLP and CNN architectures using 100 Covertype and CIFAR10 samples, respectively.  It also displays the training loss plots for these models trained on 10000 and 100000 Covertype samples and augmented CIFAR10 datasets using the Adam optimizer. The results illustrate the relationship between the lower 2sED and the training loss across different models and datasets.", "section": "7 Experiments"}]