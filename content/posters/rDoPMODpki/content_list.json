[{"type": "text", "text": "KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pengcheng Jiang Lang Cao Cao Xiao\u2020 Parminder Bhatia\u2020 Jimeng Sun Jiawei Han ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "University of Illinois at Urbana-Champaign \u2020GE HealthCare {pj20, langcao2, jimeng, hanj}@illinois.edu danicaxiao@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of $14.4\\%$ , $13.5\\%$ , and $11.9\\%$ in the Hits $@10$ metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of $12.6\\%$ , $6.7\\%$ , and $17.7\\%$ compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings. ", "page_idx": 0}, {"type": "image", "img_path": "rDoPMODpki/tmp/7b46e1ff079a540d19b16376f15c739e60552a7fb6b11591784577eb0753d817.jpg", "img_caption": ["Figure 1: \u201cFine-tune LLM with KG\u201d vs \u201cFine-tune KG with LLM\u201d. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Knowledge graph (KG) is a powerful tool for representing and storing structured knowledge, with applications spanning a wide range of domains, such as question answering [1, 2, 3, 4], recommendation systems [5, 6, 7], drug discovery [8, 9, 10], and clinical prediction [11, 12, 13]. Constituted by entities and relations, KGs form a graph structure where nodes denote entities and edges represent relations among them. To facilitate efficient reasoning and knowledge discovery, knowledge graph embedding (KGE) methods [14, 15, 16, 17, 18, 19, 20, 21] have emerged, aiming to derive low-dimensional vector representations of entities and relations while preserving the graph\u2019s structural integrity. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While current KGE methods have shown success, many are limited to the graph structure alone, neglecting the wealth of open-world knowledge surrounding entities not explicitly depicted in the KG, which is manually created in most cases. This oversight inhibits their capacity to grasp the complete semantics of entities and relations, consequently resulting in suboptimal performance across downstream tasks. For instance, a KG might contain entities such as \u201cAlbert Einstein\u201d and \u201cTheory of Relativity\u201d, along with a relation connecting them. However, the KG may lack the rich context and background information about Einstein\u2019s life, his other scientific contributions, and the broader impact of his work. In contrast, pre-trained language models (PLMs) and LLMs, having been trained on extensive literature, can provide a more comprehensive understanding of Einstein and his legacy beyond the limited scope of the KG. While recent studies have explored fine-tuning PLMs with KG triples [22, 23, 24, 25, 26, 27, 28, 29], this approach is subject to several limitations. Firstly, the training and inference processes are computationally expensive due to the large number of parameters in PLMs, making it challenging to extend to more knowledgeable LLMs. Secondly, the fine-tuned PLMs heavily rely on the restricted knowledge captured by the KG embeddings, limiting their ability to fully leverage the extensive knowledge contained within the language models themselves. As a result, these approaches may not adequately capitalize on the potential of LLMs to enhance KG representations, as illustrated in Fig. 1. Lastly, small-scale PLMs (e.g., BERT) contain outdated and limited knowledge compared to modern LLMs, requiring re-training to incorporate new information, which hinders their ability to keep pace with the rapidly evolving nature of today\u2019s language models. ", "page_idx": 1}, {"type": "text", "text": "To address the limitations of current approaches, we propose KG-FIT (Knowledge Graph FIneTuning), a novel framework that directly incorporates the rich knowledge from LLMs into KG embeddings without the need for fine-tuning the LMs themselves. The term \u201cfine-tuning\u201d is used because the initial entity embeddings are from pre-trained LLMs, initially capturing global semantics. ", "page_idx": 1}, {"type": "text", "text": "KG-FIT employs a two-stage approach: (1) generating entity descriptions from the LLM and performing LLM-guided hierarchy construction to build a semantically coherent hierarchical structure of entities, and (2) fine-tuning the KG embeddings by integrating knowledge from both the hierarchical structure and textual embeddings, effectively merging the open-world knowledge captured by the LLM into the KG embeddings. This results in enriched representations that integrate both global knowledge from LLMs and local knowledge from KGs. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of KG-FIT are outlined as follows: ", "page_idx": 1}, {"type": "text", "text": "(1) We introduce a method for automatically constructing a semantically coherent entity hierarchy using agglomerative clustering and LLM-guided refinement.   \n(2) We propose a fine-tuning approach that integrates knowledge from the hierarchical structure and pre-trained text embeddings of entities, enhancing KG embeddings by incorporating open-world knowledge captured by the LLM.   \n(3) Through an extensive empirical study on benchmark datasets, we demonstrate significant improvements in link prediction accuracy over state-of-the-art baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Structure-Based Knowledge Graph Embedding. Knowledge Graph Embedding methods that rely solely on graph structure aim to learn low-dimensional vector representations of entities and relations while preserving the graph\u2019s structural properties. TransE [14] models relations as translations in the embedding space. DistMult [15] is a simpler model that uses a bilinear formulation for link prediction. ComplEx [16] extends TransE to the complex domain, enabling the modeling of asymmetric relations. ConvE [17] employs a convolutional neural network to model interactions between entities and relations. TuckER [18] utilizes a Tucker decomposition to learn embeddings for entities and relations jointly. RotatE [19] represents relations as rotations in a complex space, which can capture various relation patterns, and HAKE [21] models entities and relations in an implicit hierarchical and polar coordinate system. These structure-based methods have proven effective in various tasks [30] but do not leverage the rich entity information available outside the KG itself. ", "page_idx": 1}, {"type": "text", "text": "PLM-Based Knowledge Graph Embedding. Recent studies have explored integrating pre-trained language models (PLMs) with knowledge graph embeddings to leverage the semantic information captured by PLMs. KG-BERT [22], PKGC [28], TagReal [29], and KG-LLM [31] train PLMs/LLMs with a full set of classification data and prompts. However, these approaches are computationally expensive due to the need to iterate over all possible positive/negative triples. LMKE [26] and SimKGC [27] adopt contrastive learning frameworks to tackle issues like expensive negative sampling and enable efficient learning for text-based KGC. KG-S2S [24] and KGT5 [25] employ sequenceto-sequence models to generate missing entities or relations in the KG. StAR [23] and CSProm-KG [32] fuse embeddings from graph-based models and PLMs. However, they are limited to small-scale PLMs and do not leverage the hierarchical and clustering information reflecting the LLM\u2019s knowledge of entities. Fully LLM prompting-based methods [33, 34] are costly and not scalable. In contrast, our proposed KG-FIT approach can be applied to any LLM, incorporating its knowledge through a semantically coherent hierarchical structure of entities. This enables efficient exploitation of the extensive knowledge within LLMs, while maintaining the efficiency of structure-based methods. ", "page_idx": 1}, {"type": "image", "img_path": "rDoPMODpki/tmp/52ee395ac9c5e2826965929bbcca0b2bb6f74ce90b4d0978ae51ec40c8ef4839.jpg", "img_caption": ["Figure 2: Overview of KG-FIT. Input and Output are highlighted at each step. Step 1: Obtain text embeddings for all entities in the KG, achieved by merging word embeddings with description embeddings retrieved from LLMs. Step 2: Hierarchical clustering is applied iteratively to all entity embeddings over various distance thresholds, monitored by a Silhouette scorer to identify optimal clusters, thus constructing a seed hierarchy where each leaf node represents a cluster of semantically similar entities. Step 3: Leveraging LLM guidance, the seed hierarchy is iteratively refined bottom-up through a series of suggested actions, aiming for a more accurate organization of KG entities with LLM\u2019s knowledge. Step 4: Use the refined hierarchy along with KG triples and the initial entity embeddings to fine-tune the embeddings under a series of distance constraints. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 KG-FIT Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present KG-FIT (as shown in Fig. 2), a framework for fine-tuning KG embeddings leveraging external hierarchical structures and textual information based on open knowledge. This framework comprises two primary components: (1) LLM-Guided Hierarchy Construction: This phase establishes a semantically coherent hierarchical structure of entities, initially constructing a seed hierarchy and then refining it using LLM-guided techniques, and (2) Knowledge Graph Fine-Tuning: This stage enhances the KG embeddings by integrating the constructed hierarchical structure, textual embeddings, and multiple constraints. The two stages combined to enrich KG embeddings with open-world knowledge, leading to more comprehensive and contextually rich representations. Below we present more tecnical details. A table of notations is placed in Appendix L. ", "page_idx": 2}, {"type": "text", "text": "3.1 LLM-Guided Hierarchy Construction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We initiate the process by constructing a hierarchical structure of entities through agglomerative clustering, subsequently refining it using an LLM to enhance semantic coherence and granularity. ", "page_idx": 2}, {"type": "text", "text": "Step 1: Entity Embedding Initialization is the first step of this process, where we are given a set of entities $\\mathcal{E}=\\{e_{1},\\ldots,e_{|\\mathcal{E}|}\\}$ within a KG, and will enrich their semantic representations by generating descriptions using an LLM. Specifically for each entity $e_{i}$ , we prompt the LLM with a template (e.g., Briefly describe [entity] with the format \u201c[entity] is a [description]\u201d. (detailed in Appendix E.1)) prompting it to describe the entity from the KG dataset, thereby yielding a concise natural language description $d_{i}$ . Subsequently, the entity embedding $\\mathbf{v}_{i}^{e}\\in\\mathbb{R}^{\\mathrm{dim}(f)}$ and description embedding $\\mathbf{v}_{i}^{d}\\;=\\;f(d_{i})\\;\\in\\;\\mathbb{R}^{\\dim(f)}$ are obtained using an embedding model $f$ and concatenated to form the enriched entity representation $\\mathbf{v}_{i}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}=[\\mathbf{v}_{i}^{e};\\mathbf{v}_{i}^{d}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Step 2: Seed Hierarchy Construction follows after entity embedding initialization. Here we choose agglomerative hierarchical clustering [35] over flat clustering methods like ${\\bf K}$ -means [36] for establishing the initial hierarchy. This choice is based on the robust hierarchical information provided by agglomerative clustering, which serves as a strong foundation for LLM refinement. Using this hierarchical structure reduces the need for numerous LLM iterations to discern relationships between flat clusters, thereby lowering computational costs and complexity. Agglomerative clustering balances computational efficiency with providing the LLM a meaningful starting point for refinement. The clustering process operates on enriched entity representations $\\mathbf{V}\\,=\\,\\{\\bar{\\mathbf{v}_{1}},\\hdots,\\mathbf{v}_{l}\\}\\,\\in\\,\\mathbb{R}^{|\\mathcal{E}|\\times2\\mathrm{dim}(f)}$ using cosine distance and average linkage. The optimal clustering threshold $\\tau^{*}$ is determined by maximizing the silhouette score [37] $S^{*}$ across a range of thresholds $[\\tau_{\\operatorname*{min}},\\tau_{\\operatorname*{max}}]\\in[0,1]$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau_{\\mathrm{optim}}=\\arg\\operatorname*{max}_{\\tau\\in[\\tau_{\\mathrm{min}},\\tau_{\\mathrm{max}}]}S^{*}(\\mathbf{V},\\mathrm{labels}_{\\tau})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{labels}_{\\tau}$ are the clustering results at threshold $\\tau$ . This ensures that the resulting clusters are compact and well-separated based on semantic similarity. The constructed hierarchy forms a fully binary tree where each leaf node represents an entity. We use a top-down algorithm (detailed in Appendix F.1) to replace the first encountered entity with its cluster based on the optimal threshold $\\tau_{\\mathrm{optim}}$ . This process eliminates other entity leaves within the same cluster, forming the seed hierarchy $\\mathcal{H}_{\\mathrm{seed}}$ , where each leaf node is a cluster of entities defined by labels $\\tau_{\\mathrm{optm}}$ . ", "page_idx": 3}, {"type": "text", "text": "Step 3: LLM-Guided Hierarchy Refinement (LHR) is then applied to improve the quality of the knowledge representation. As the seed hierarchy $\\mathcal{H}_{\\mathrm{seed}}$ is a binary tree, which may not optimally represent real-world entity knowledge, we further refine it using the LLM. The LLM transforms the seed hierarchy into the LLM-guided refined hierarchy $\\mathcal{H}_{\\mathrm{LHR}}$ through actions described below: ", "page_idx": 3}, {"type": "text", "text": "i. Cluster Splitting: For each leaf cluster $C_{\\mathrm{original}}\\in\\mathcal{C}_{\\mathrm{leaf}}(\\mathcal{H}_{\\mathrm{seed}}).$ , the LLM recursively splits it into two subclusters using the prompt $\\mathcal{P}_{\\mathrm{SPLIT}}$ (Fig. 8 in Appendix E.2): ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{\\mathrm{split}}=\\mathrm{LLM}(\\mathcal{P}_{\\mathrm{SPLIT}}(C_{\\mathrm{original}})),\\quad C_{\\mathrm{original}}\\to C_{\\mathrm{split}}=\\{C_{1},C_{2},\\ldots,C_{k}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C_{i}=\\{e_{1}^{i},e_{2}^{i},\\ldots,e_{|C_{i}|}^{i}\\},\\sum_{i=1}^{k}|C_{i}|=|C_{\\mathrm{original}}|=|C_{\\mathrm{split}}|,k\\;\\mathrm{inf}$ s the total number of subclusters after recursively splitting $C_{\\mathrm{original}}$ in a binary manner. This procedure iterates until LLM indicates no further splitting or each subcluster has minimal entities, resulting in an intermediate hierarchy $\\mathcal{H}_{\\mathrm{split}}$ . ", "page_idx": 3}, {"type": "text", "text": "ii. Bottom-Up Refinement: In the bottom-up refinement phase, the LLM iteratively refines the intermediate hierarchy $\\mathcal{H}_{\\mathrm{split}}$ produced by the cluster splitting step. The refinement process starts from the leaf level of the hierarchy and progresses upwards, considering each parent-child triple $(P_{*},P_{l},P_{r})$ , where $P_{*}$ represents the grandparent cluster, and $P_{l}$ and $P_{r}$ represent the left and right child clusters of $P_{*}$ , respectively. Let $\\{C_{1}^{l},C_{2}^{l},C_{3}^{l},\\dots,C_{|P_{l}|}^{l}\\}$ denote the children of $P_{l}$ , and $\\{C_{1}^{r},C_{2}^{r},C_{3}^{r},...,C_{|P_{r}|}^{r}\\}$ denote the children of $P_{r}$ . ", "page_idx": 3}, {"type": "text", "text": "For each parent-child triple, the LLM is prompted with $\\mathcal{P}_{\\mathrm{REFINE}}(P_{*},P_{l},P_{r})$ (Fig. 9 in Appendix E.2), which provides the names and entities of the grandparent and child clusters. The LLM then suggests a refinement action to update the triple based on its understanding of the relationships between the clusters. The refinement options are: ", "page_idx": 3}, {"type": "text", "text": "1. NO UPDATE: The triple remains unchanged, i.e., $(P_{*}^{\\prime},P_{l}^{\\prime},P_{r}^{\\prime})=(P_{*},P_{l},P_{r})$ . ", "page_idx": 3}, {"type": "text", "text": "2. PARENT MERGE: All the children of $P_{l}$ and $P_{r}$ are merged into the grandparent cluster $P_{*}$ , resulting in $P_{*}^{\\prime}=\\{C_{1}^{l},C_{2}^{l},C_{3}^{l},\\ldots,C_{|P_{l}|}^{l},C_{1}^{r},C_{2}^{r},C_{3}^{r},\\ldots,\\bar{C}_{|P_{r}|}^{r}\\}$ . The original child clusters $P_{l}$ and $P_{r}$ are removed from the hierarchy. ", "page_idx": 3}, {"type": "text", "text": "3. LEAF MERGE: $P_{*}^{\\prime}=\\{e_{1},e_{2},\\dots,e_{p}\\},P_{l}^{\\prime}=\\emptyset,P_{r}^{\\prime}=\\emptyset$ , where $\\{e_{1},e_{2},\\ldots,e_{p}\\}=P_{l}\\cup P_{r}$ . ", "page_idx": 3}, {"type": "text", "text": "4. INCLUDE: One of the child clusters is absorbed into the other, while the grandparent cluster remains unchanged. This can happen in two ways: ", "page_idx": 4}, {"type": "text", "text": "The LLM determines the most appropriate refinement action based on the semantic similarity and hierarchical relationships between the clusters. The refinement process continues bottom-up, iteratively updating the triples until the root of the hierarchy is reached. The resulting refined hierarchy is denoted as $\\mathcal{H}_{\\mathrm{LHR}}$ . We place more details of the process in Appendix E.2 and F.2. ", "page_idx": 4}, {"type": "text", "text": "3.2 Global Knowledge-Guided Local Knowledge Graph Fine-Tuning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Step 4: KG-FIT fine-tunes the knowledge graph embeddings by incorporating the hierarchical structure, text embeddings, and three main constraints: the hierarchical clustering constraint, text embedding deviation constraint, and link prediction objective. ", "page_idx": 4}, {"type": "text", "text": "Initialization of Entity and Relation Embeddings: To integrate the initial text embeddings $(\\mathbf{v}_{i}\\in$ $\\mathbb{R}^{\\dim(f)})$ into the model, the entity embedding $\\mathbf{e}_{i}\\,\\in\\,\\mathbb{R}^{n}$ is initialized as a linear combination of a random embedding $\\mathbf{e}_{i}^{\\prime}\\,\\in\\,\\mathbb{R}^{n}$ and the sliced text embedding $\\mathbf{v}_{i}^{\\prime}\\,=\\,\\bigl[\\mathbf{v}_{i}^{e}\\lbrack\\colon\\,\\frac{n}{2}\\rbrack;\\mathbf{v}_{i}^{d}\\lbrack\\colon\\,\\frac{n}{2}\\bigr]\\bigr]\\,\\in\\,\\mathbb{R}^{n}$ . The relation embeddings, on the other hand, are initialized randomly: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{e}_{i}=\\rho\\mathbf{e}_{i}^{\\prime}+(1-\\rho)\\mathbf{v}_{i}^{\\prime},\\quad\\mathbf{r}_{j}\\sim N(0,\\psi^{2})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\rho$ is a hyperparameter controlling the ratio between the random embedding and the sliced text embedding. $\\mathbf{r}_{j}\\in\\mathbb{R}^{m}$ is the embedding of relation $j$ , and $\\psi$ is a hyperparameter controlling the standard deviation of the normal distribution $N$ . This initialization ensures that the entity embeddings start close to their semantic descriptions but can still adapt to the structural information in the KG during training. The random initialization of relation embeddings allows the model to flexibly capture the structural information and patterns specific to the KG. ", "page_idx": 4}, {"type": "text", "text": "Hierarchical Clustering Constraint: The hierarchical constraint integrates the structure and relationships derived from the adaptive agglomerative clustering and LLM-guided refinement process. This optimization enhances the embeddings for hierarchical coherence and distinct semantic clarity. The revised constraint consists of three tailored components: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{hier}}=\\sum_{e_{i}\\in E}\\Big(\\underbrace{\\lambda_{1}d(\\mathbf{e}_{i},\\mathbf{c})}_{C l u s t e r\\;C o h e s i o n}-\\lambda_{2}\\sum_{C^{\\prime}\\in S_{m}(C)}\\frac{d(\\mathbf{e}_{i},\\mathbf{c}^{\\prime})}{|S_{m}(C)|}-\\lambda_{3}\\sum_{j=1}^{h-1}\\frac{\\beta_{j}(d(\\mathbf{e}_{i},\\mathbf{p}_{j+1})-d(\\mathbf{e}_{i},\\mathbf{p}_{j}))}{h-1}\\Big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where: $e_{i}$ and $\\mathbf{e}_{i}$ represent the entity and its embedding. $C$ is the cluster that entity $e_{i}$ belongs to. ${\\mathcal{S}}_{m}(C)$ represents the set of neighbor clusters of $C$ where $m$ is the number of nearest neighbors (determined by lowest common ancestor (LCA) [38] in the hierarchy). c and $\\mathbf{c}^{\\prime}$ denote the cluster embeddings of $C$ and $C^{\\prime}$ , which is computed by averaging all the embedding of entities under them (i.e., $\\begin{array}{r}{\\mathbf{c}=\\frac{\\mathbf{\\bar{\\alpha}}_{1}}{|C|}\\sum_{e_{i}\\in C}\\mathbf{e}_{i}\\in\\mathbb{R}^{n})}\\end{array}$ . $\\mathbf{p}_{j}$ and $\\mathbf{p}_{j+1}$ are the embeddings of the parent nodes along the path from the entity (at depth $h$ ) to the root, indicating successive parent nodes in ascending order. Each parent node is computed by averaging the cluster embeddings under it $\\begin{array}{r}{(\\mathbf{p}=\\frac{1}{|P|}\\sum_{C_{i}\\in P}\\mathbf{c}_{i}\\in\\mathbb{R}^{n})}\\end{array}$ $d(\\cdot,\\cdot)$ is the distance function used to measure distances between embeddings. As higher levels of abstraction encompass a broader range of concepts, and thus a strict maintenance of hierarchical distance may be less critical at these levels, we introduce $\\beta_{j}=\\beta_{0}\\cdot e^{-\\phi j}$ where $\\beta_{0}$ is the initial weight for the closest parent, typically a larger value, $\\phi$ is the decay rate, a positive constant that dictates how rapidly the importance decreases. $\\lambda_{1},\\,\\lambda_{2}$ , and $\\lambda_{3}$ are hyperparameters. In Eq 5, Inter-level Cluster Separation aims to maximize the distance between an entity and neighbor clusters, enhancing the differentiation and reducing potential overlap in the embedding space. This separation ensures that entities are distinctly positioned relative to non-member clusters, promoting clearer semantic divisions. Hierarchical Distance Maintenance encourages the distance between an entity and its parent nodes to be proportional to their respective levels in the hierarchy, with larger distances for higher-level parent nodes. This reflects the increasing abstraction and decreasing specificity, aligning the embeddings with the hierarchical structure of the KG. Cluster Cohesion enhances intra-cluster similarity by minimizing the distance between an entity and its own cluster center, ensuring that entities within the same cluster are closely embedded, maintaining the integrity of clusters. ", "page_idx": 4}, {"type": "text", "text": "Semantic Anchoring Constraint: To preserve the semantic integrity of the embeddings, we introduce the semantic anchoring constraint, which is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{anc}}=-\\sum_{e_{i}\\in\\mathcal{E}}d(\\mathbf{e}_{i},\\mathbf{v}_{i}^{\\prime})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{E}$ is the set of all entities, $\\mathbf{e}_{i}$ is the fine-tuned embedding of entity $e_{i},\\,\\mathbf{v}_{i}^{\\prime}$ is the sliced text embedding of entity $e_{i}$ , and $d(\\cdot,\\cdot)$ is a distance function. This constraint is crucial for large clusters, where the diversity of entities may cause the fine-tuned embeddings to drift from their original semantic meanings. This is also important when dealing with sparse KGs, as the constraint helps prevent overftiting to the limited structural information available. By acting as a regularization term, it mitigates overfitting and enhances the robustness of the embeddings [39]. ", "page_idx": 5}, {"type": "text", "text": "Score Function-Based Fine-Tuning: KG-FIT is a general framework applicable to existing KGE models [14, 15, 16, 17, 18, 19, 20]. These models learn low-dimensional vector representations of entities and relations in a KG, aiming to capture the semantic and structural information within the KG itself. In our work, we perform link prediction to enhance the model\u2019s ability to accurately predict relationships between entities within the KG. Its loss is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{link}}=-\\sum_{(e_{i},r,e_{j})\\in\\mathcal{D}}\\Bigl(\\log\\sigma(\\gamma-f_{r}(\\mathbf{e}_{i},\\mathbf{e}_{j}))-\\frac{1}{|\\mathcal{N}_{j}|}\\sum_{n_{j}\\in\\mathcal{N}_{j}}\\log\\sigma(\\gamma-f_{r}(\\mathbf{e}_{i},\\mathbf{e}_{n_{j}}))\\Bigr)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{D}$ is the set of all triples in the KG, $\\sigma$ is sigmoid function. $f_{r}(\\cdot,\\cdot)$ is the scoring function (detailed in Appendix $\\underline{{\\mathbf{G}}}$ and $\\underline{{\\mathbf{K}}}_{\\phantom{\\,}\\,}$ ) defined by the chosen KGE model that measures the compatibility between the head entity embedding ${\\bf{e}}_{i}$ and the tail entity embedding $\\mathbf{e}_{j}$ given the relation $r,\\mathcal{N}_{j}$ is the set of negative tail entities sampled for the triple $(e_{i},r,e_{j})$ , ${\\bf{e}}_{n_{j}}$ is the embedding of the negative tail entity $n_{j}$ , and $\\gamma$ is a margin hyperparameter. The link prediction-based fine-tuning minimizes the scoring function for the true triples $(e_{i},r,e_{j})$ while maximizing the margin between the scores of true triples and negative triples $(e_{i},r,n_{j})$ . This encourages the model to assign higher scores to positive (true) triples and lower scores to negative triples, thereby enriching the embeddings with the local semantics in KG. ", "page_idx": 5}, {"type": "text", "text": "Training Objective: The objective function of KG-FIT integrates three constraints: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\zeta_{1}\\mathcal{L}_{\\mathrm{hier}}+\\zeta_{2}\\mathcal{L}_{\\mathrm{anc}}+\\zeta_{3}\\mathcal{L}_{\\mathrm{link}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\zeta_{1},\\zeta_{2}$ , and $\\zeta_{3}$ are hyperparameters that assign weights to the constraints. ", "page_idx": 5}, {"type": "text", "text": "Note: During fine-tuning, the time complexity per epoch is $O((|\\mathcal{E}|+|\\mathcal{T}|)\\cdot n)$ where $\\vert\\mathcal{E}\\vert$ is the number of entities, $|\\tau|$ is the number of triples, and $n$ is the embedding dimension. In contrast, classic PLM-based methods [22, 28, 29, 23] have a time complexity of $O(|T|\\cdot L\\cdot n_{\\mathrm{PLM}})$ per epoch during fine-tuning, where $L$ is the average sequence length and $n_{\\mathrm{PLM}}$ is the hidden dimension of the PLM. This is typically much higher than KG-FIT\u2019s fine-tuning time complexity, as $|{\\mathcal{T}}|\\cdot L\\gg(|{\\mathcal{E}}|+|{\\mathcal{T}}|)$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We describe our experimental setup as follows. ", "page_idx": 5}, {"type": "text", "text": "Datasets. We consider datasets that encompass various domains and sizes, ensuring comprehensive evaluation of the proposed model. Specifically, we consider three datasets: (1) FB15K237 [40] $(C C\\,B Y\\,4.0)$ is a subset of Freebase [41], a large collaborative knowledge base, focusing on common knowledge; (2) YAGO3-10 ", "page_idx": 5}, {"type": "table", "img_path": "rDoPMODpki/tmp/0d84d5cc7b6ef6319d7f9747a01d3835d06b8d02d3fe885f3be64af02a07bee0.jpg", "table_caption": ["Table 2: Datasets statistics. #Ent./#Rel: number of entities/relations. #Train/#Valid/#Test: number of triples contained in the training/validation/testing set. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "[42] is a subset of YAGO [43] $(C C B Y\\,4.0)$ , which is a large knowledge base derived from multiple sources including Wikipedia, WordNet, and GeoNames; (3) PrimeKG [44] (CC0 1.0) is a biomedical KG that integrates 20 biomedical resources, detailing 17,080 diseases through 4,050,249 relationships. Our study focuses on a subset of PrimeKG, extracting 106,000 triples from the whole set, with processing steps outlined in Appendix B. Table 2 shows the statistics of these datasets. ", "page_idx": 5}, {"type": "table", "img_path": "rDoPMODpki/tmp/ee25a7b03bea05687e4545ec7c1e57d1c6a4554f088784a10657f6088231cbac.jpg", "table_caption": ["Table 1: Link Prediction Performance Comparison. Results are averaged values (of ten runs for FB15K237/PrimeKG and of three runs for YAGO3-10) of head/tail entity predictions. Top-3 results for each metric are highlighted. \u201c\\*\u201d indicates the results taken from method\u2019s original paper. KG-FIT consistently outperforms both PLM-based models and structure-based base models across all datasets and metrics, demonstrating its effectiveness in incorporating open-world knowledge from LLMs for enhancing KG embeddings. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Metrics. Following previous works, we use Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits $\\boldsymbol{\\@}\\mathbf{N}$ $(\\mathrm{H@N})$ to evaluate link prediction. MR measures the average rank of true entities, lower the better. MRR averages the reciprocal ranks of true entities, providing a normalized measure less sensitive to outliers. Hits $\\mathbb{\\mathrm{(}}\\mathbb{\\alpha}\\mathrm{\\mathbf{N}}$ measures the proportion of true entities in the top $N$ predictions. ", "page_idx": 6}, {"type": "text", "text": "Baselines. To benchmark the performance of our proposed model, we compared it against the stateof-the-art PLM-based methods including KG-BERT [22], StAR [23], PKGC [28], C-LMKE [26], KGT5 [25], KG-S2S [24], SimKGC [27], and CSProm-KG [32], and structure-based methods including TransE [14], DistMult [15], ComplEx [16], ConvE [17], TuckER [18], pRotatE [19], RotatE [19], and HAKE [20]. ", "page_idx": 6}, {"type": "text", "text": "Experimental Strategy: For most PLM-based models, due to their high training cost, we use the results reported in their respective papers.. We reproduce PKGC [28] for all three datasets, SimKGC [27] and CSProm-KG [32] for PrimeKG. In addition, we evaluate the capabilities of LLM embeddings (TE-3-S/L: text-embedding-3-small/large) for zero-shot link prediction by ranking the cosine similarity between $({\\bf e}_{i}+{\\bf r})$ and $\\mathbf{e}_{j}$ . For structure-based KGE models, we assess and present their best performance using optimal settings (shown in Table 12). For KG-FIT, we provide a detailed hyperparameter study in Table 13. We use OpenAI\u2019s GPT-4o as the LLM for entity description generation and for LLM-guided hierarchy refinement. text-embedding-3-large is used for entity embedding initialization, which preserves semantics with flexible embedding slicing [45]. We set $\\tau_{\\mathrm{min}}=0.15$ and $\\tau_{\\operatorname*{max}}=0.85$ for seed hierarchy construction. The values of $\\tau_{\\mathrm{optim}}$ for FB15K-237, ", "page_idx": 6}, {"type": "text", "text": "YAGO3-10, and PrimeKG are 0.52, 0.49, and 0.33, respectively. The statistics of the seed and LHR hierarchies are placed in Appendix F.3. For LCA, we designate the root as the grandparent (i.e., two levels above) source cluster node and set $m=5$ . We set $\\rho=0.5$ , $\\beta_{0}=1.2$ , $\\phi=0.4$ , and use cosine distance for fine-tuning. Filtered setting [14, 19] is applied for link prediction evaluation. Hyperparameter studies and computational cost are detailed in Appendix I and $\\underline{{\\mathrm{H}}}$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments to evaluate the performance on link prediction. The results in Table $\\underline{{1}}$ are averaged values from multiple runs with random seeds: ten runs for FB15K-237 and PrimeKG, and three runs for YAGO3-10. These averages reflect the performance of head/tail entity predictions. ", "page_idx": 7}, {"type": "text", "text": "Main Results. Table 1 shows that our KG-FIT framework consistently outperforms state-of-the-art PLM-based and traditional structure-based models across all datasets and metrics. This highlights KG-FIT\u2019s effectiveness in leveraging LLMs to enhance KG embeddings. Specifically, KG-FITHAKE surpasses CSProm-KG by $6.3\\%$ and HAKE by $6.1\\%$ on FB15K-237 in Hits $@10$ ; KG-FITRotatE outperforms CS-PromKG by $7.0\\%$ and TuckER by $4.6\\%$ on YAGO3-10; KG-FITComplEx exceeds PKGC by $11.0\\%$ and ComplEx by $5.8\\%$ on PrimeKG. Additionally, with LLM-guided hierarchy refinement (LHR), KG-FIT achieves performance gains of $12.6\\%$ , $6.7\\%$ , and $17.8\\%$ compared to the base models, and $3.0\\%$ , $1.9\\%$ , and $2.2\\%$ compared to KG-FIT with seed hierarchy, on FB15K-237, YAGO3-10, and PrimeKG, respectively. All these findings highlight the effectiveness of KG-FIT for significantly improving the quality of structure-based KG embeddings. ", "page_idx": 7}, {"type": "text", "text": "Figure $\\underline{{3}}$ further illustrates the robustness of KG-FIT, showing superior validation performance across training steps compared to the corresponding base models, indicating its ability to fix both overfitting and underfitting issues of some structure-based KG embedding models. ", "page_idx": 7}, {"type": "image", "img_path": "rDoPMODpki/tmp/f664fb68782db552358a0efeadb933fe55567dabf756ef0e2297ae89bc437715.jpg", "img_caption": ["Figure 3: KG-FIT can mitigate overfitting (upper) and underfitting (lower) of structure-based models. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Effect of Constraints. We conduct an ablation study to evaluate the proposed constraints in Eq. 5 and 6, with results summarized in Table 4. This analysis underscores the importance of each constraint: (1) Hierarchical Distance Maintenance is crucial for both datasets. Its removal significantly degrades performance across all metrics, highlighting the necessity of preserving the hierarchical structure in the embedding space. (2) Semantic Anchoring proves more critical for the denser YAGO3-10 graph, where each cluster contains more entities, making it harder to distinguish between them based solely on cluster cohesion. The sparser FB15K-237 dataset is less impacted by the absence of this constraint. Similar to the semantics anchoring, the removal of (3) Inter-level Cluster Separation significantly affects the denser YAGO3-10 more than FB15K-237. Without this constraint, entities in YAGO3-10 may not be well-separated from other clusters, whereas FB15K-237 is less influenced. Interestingly, removing (4) Cluster Cohesion has a larger impact on the sparser FB15K-237 than on YAGO3-10. This difference suggests that sparse graphs rely more on the prior information provided by entity clusters, while denser graphs can learn this information more effectively from their abundant data. ", "page_idx": 7}, {"type": "text", "text": "Effect of Knowledge Sources. We explore the impact of the quality of LLM-refined hierarchies and pre-trained text embeddings on final performance, as illustrated in Figures 4 and 5. The results indicate that hierarchies constructed and text embeddings retrieved from more advanced LLMs consistently lead to improved performance. This finding underscores KG-FIT\u2019s capacity to leverage and evolve with ongoing advancements in LLMs, effectively utilizing the increasingly comprehensive entity knowledge captured by these models. ", "page_idx": 7}, {"type": "table", "img_path": "rDoPMODpki/tmp/8bbcc216083776fd411c79ab9a7c8d01e7feb5a4d96c32e742588ac393eda021.jpg", "table_caption": ["Table 3: Model efficiency on PrimeKG. T/Ep and Inf denote training time/epoch and inference time. KG-FIT outperforms all the PLMbased models. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Efficiency Evaluation. We evaluate the efficiency performance in Table $\\underline{{3}}$ . While pure structure-based models are the fastest, our model significantly outperforms all PLM-based models in both training and inference speed, consistent with our previous analysis. It achieves 12 times the training speed of CSProm-KG, the fastest PLM-based method. Moreover, KG-FIT can integrate knowledge from any LLMs, unlike previous methods that are limited to small-scale PLMs, underscoring its superiority. ", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 4: Ablation study for the proposed constraints. SA, HDM, ICS, CC denote Semantic Anchoring, Hierarchical Distance Maintenance, Inter-level Cluster Separation, and Cluster Cohesion, respectively. We use TransE and HAKE as the base models for KG-FIT on FB15K-237 and YAGO3-10, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "rDoPMODpki/tmp/c01e3efdda27724dbef9dcdfea224b3917e57f8ad600c0e74d6da33bc4d76bfa.jpg", "img_caption": ["Figure 4: KG-FIT on FB15K-237 with different hierarchy types. None indicates no hierarchical information input. Seed denotes the seed hierarchy. $G3.5/G4$ denotes the LHR hierarchy constructed by GPT-3.5/4o. LHR hierarchies outperform the seed hierarchy, with more advanced LLMs constructing higher-quality hierarchies. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "rDoPMODpki/tmp/2e65774fe57454b2ac5105b2c0031b7cf660fbe586ba04ed21d65dda67c34826.jpg", "img_caption": ["Figure 5: KG-FIT on FB15K-237 with different text embedding. BT, RBT, ada2, and te3 are BERT, RoBERTa, text-embedding-ada-002, and text-embedding-3-large, respectively. Seed hierarchy is used for all settings. It is observed that pre-trained text embeddings from LLMs are substantially better than those from small PLMs. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Visualization. Figure 6 demonstrates the effectiveness of KG-FIT in capturing both global and local semantics. The embeddings generated by KG-FIT successfully preserve the global semantics at both intra- and inter-levels. Additionally, KG-FIT excels in representing local semantics compared to the original HAKE model. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we introduced KG-FIT, a novel framework for enhancing knowledge graph (KG) embeddings by leveraging the wealth of open-world knowledge captured by large language models (LLMs). KG-FIT seamlessly integrates LLM-derived entity knowledge into the KG embedding process through a two-stage approach: LLM-guided hierarchy construction and global knowledgeguided local KG fine-tuning. By constructing a semantically coherent hierarchical structure of entities and incorporating this hierarchical knowledge along with textual information during fine-tuning, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on benchmark datasets demonstrate the superiority of KG-FIT over state-of-the-art methods, highlighting its effectiveness in integrating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings. A key advantage of KG-FIT is its flexibility to incorporate knowledge from any LLM, enabling it to evolve and improve with ongoing advancements in language models. This positions KG-FIT as a powerful and future-proof framework for knowledge-infused learning on graphs. Moreover, the enriched KG embeddings produced by KG-FIT have the potential to boost performance on a wide array of downstream tasks, such as question answering, recommendation systems, and drug discovery, among others. Our code and data are available at https://github.com/pat-jj/KG-FIT. ", "page_idx": 8}, {"type": "image", "img_path": "rDoPMODpki/tmp/f2509d32ca288688758fe5ded89a5d65cea398e0e40ce3c972d87d45ff290c28.jpg", "img_caption": ["Figure 6: Visualization of Entity Embedding (left to right: initial text embedding, HAKE embedding, and KG-FITHAKE embedding). Upper (local): Embeddings $(\\mathrm{dim}{=}2048)$ of <Maraviroc, drug_effect, CAA (Coronary artery atherosclerosis) $\\mid>$ and <Cladribine, drug_effect, Exertional dyspnea>, two parent-child triples selected from PrimeKG, in polar coordinate system. In the polar coordinate system, the normalized entity embedding \u00afe is split to $\\mathbf{e_{1}}=\\bar{\\mathbf{e}}[:\\,\\frac{n}{2}]$ and $\\mathbf{e_{2}}={\\dot{\\mathbf{e}}}{\\left[{\\frac{n}{2}}+1\\right]}$ :] where $n$ is the hidden dimension, which serves as values on the $\\mathbf{X}$ -axis and y-axis, respectively, which is consistent with Zhang et al. [20]\u2019s visualization strategy. Lower (global): t-SNE plots of different embeddings of sampled entities, with colors indicating clusters (e.g., Maraviroc belongs to the HIV Drugs cluster). Triangles indicate the positions of \u25b2Maraviroc, \u25b2CAA, \u25b2Cladribine, and \u25b2Exertional dyspnea. Observations: While the initial text embeddings capture global semantics, they fail to delineate local parent-child relationships within the KG, as seen in the intermingled polar plots. In contrast, HAKE shows more distinct grouping by modulus on the polar plots, capturing hierarchical local semantics, but fails to adequately capture global semantics. Our KG-FIT, notably, incorporates prior information from LLMs and is fine-tuned on the KG, maintains global semantics from pre-trained text embeddings while better capturing local KG semantics, demonstrating its superior representational power across local and global scales. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although KG-FIT outperforms state-of-the-art PLM-based models on the FB15K-237, YAGO3- 10, and PrimeKG datasets, it does not outperform pure PLM-based methods on a lexical dataset WN18RR, as shown in Table 5. This limitation is discussed with details in Appendix C. As a future work, we will explore the integration of contrastive learning into the KG-FIT framework to enhance its capability to capture semantic relationships more effectively. ", "page_idx": 9}, {"type": "text", "text": "Moreover, KG-FIT\u2019s performance is influenced by the quality of the constructed hierarchy, particularly the seed hierarchy. To address this, we propose an automatic selection of the optimal binary tree based on the silhouette score. However, if the initial clustering is suboptimal, it may result in a lower-quality hierarchy that affects KG-FIT\u2019s performance. Additionally, the bottom-up refinement process in our proposed LHR approach updates each parent-child triple with only a single operation (within four), which prioritizes efficiency and simplicity over performance. In future work, we plan to explore cost-efficient methods for refining the hierarchy that integrate multiple operations for each triple update, striking a better balance between efficiency and performance. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research was supported in part by US DARPA INCAS Program No. HR0011-21-C0165 and BRIES Program No. HR0011-24-3-0325, National Science Foundation IIS-19-56151, the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 105\u2013113, 2019.   \n[2] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. arXiv preprint arXiv:2104.06378, 2021.   \n[3] Weiguo Zheng, Hong Cheng, Lei Zou, Jeffrey Xu Yu, and Kangfei Zhao. Natural language question/answering: Let users talk with the knowledge graph. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 217\u2013226, 2017.   \n[4] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, and Jure Leskovec. Deep bidirectional language-knowledge graph pretraining. Advances in Neural Information Processing Systems, 35:37309\u201337323, 2022.   \n[5] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In The world wide web conference, pages 2022\u20132032, 2019.   \n[6] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. Knowledge graph convolutional networks for recommender systems. In The world wide web conference, pages 3307\u20133313, 2019.   \n[7] Janneth Chicaiza and Priscila Valdiviezo-Diaz. A comprehensive survey of knowledge graphbased recommender systems: Technologies, development, and contributions. Information, 12(6):232, 2021.   \n[8] Sameh K Mohamed, V\u00edt Nov\u00e1c\u02c7ek, and Aayah Nounu. Discovering protein drug targets using knowledge graph embeddings. Bioinformatics, 36(2):603\u2013610, 08 2019.   \n[9] Pengcheng Jiang, Cao Xiao, Tianfan Fu, and Jimeng Sun. Bi-level contrastive learning for knowledge-enhanced molecule representations, 2024.   \n[10] Payal Chandak, Kexin Huang, and Marinka Zitnik. Building a knowledge graph to enable precision medicine. Scientific Data, 10(1):67, 2023.   \n[11] Junyi Gao, Chaoqi Yang, Joerg Heintz, Scott Barrows, Elise Albers, Mary Stapel, Sara Warfield, Adam Cross, and Jimeng Sun. Medml: fusing medical knowledge and machine learning models for early pediatric covid-19 hospitalization and severity prediction. Iscience, 25(9), 2022.   \n[12] Ricardo MS Carvalho, Daniela Oliveira, and Catia Pesquita. Knowledge graph embeddings for icu readmission prediction. BMC Medical Informatics and Decision Making, 23(1):12, 2023.   \n[13] Pengcheng Jiang, Cao Xiao, Adam Richard Cross, and Jimeng Sun. Graphcare: Enhancing healthcare predictions with personalized knowledge graphs. In The Twelfth International Conference on Learning Representations, 2024.   \n[14] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[15] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014.   \n[16] Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International conference on machine learning, pages 2071\u20132080. PMLR, 2016.   \n[17] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[18] Ivana Bala\u017eevi\u00b4c, Carl Allen, and Timothy Hospedales. Tucker: Tensor factorization for knowledge graph completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5185\u20135194, 2019.   \n[19] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In International Conference on Learning Representations, 2018.   \n[20] Zhanqiu Zhang, Jianyu Cai, Yongdong Zhang, and Jie Wang. Learning hierarchy-aware knowledge graph embeddings for link prediction. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3065\u20133072, 2020.   \n[21] Yushi Bai, Zhitao Ying, Hongyu Ren, and Jure Leskovec. Modeling heterogeneous hierarchies with relation-specific hyperbolic cones. Advances in Neural Information Processing Systems, 34:12316\u201312327, 2021.   \n[22] Liang Yao, Chengsheng Mao, and Yuan Luo. Kg-bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193, 2019.   \n[23] Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, and Yi Chang. Structureaugmented text representation learning for efficient knowledge graph completion. In Proceedings of the Web Conference 2021, pages 1737\u20131748, 2021.   \n[24] Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. Knowledge is flat: A seq2seq generative framework for various knowledge graph completion. In Proceedings of the 29th International Conference on Computational Linguistics, pages 4005\u20134017, 2022.   \n[25] Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. Sequence-to-sequence knowledge graph completion and question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2814\u20132828, 2022.   \n[26] Xintao Wang, Qianyu He, Jiaqing Liang, and Yanghua Xiao. Language models as knowledge embeddings. arXiv preprint arXiv:2206.12617, 2022.   \n[27] Liang Wang, Wei Zhao, Zhuoyu Wei, and Jingming Liu. Simkgc: Simple contrastive knowledge graph completion with pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4281\u20134294, 2022.   \n[28] Xin Lv, Yankai Lin, Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, and Jie Zhou. Do pre-trained models benefti knowledge graph completion? a reliable evaluation and a reasonable approach. Association for Computational Linguistics, 2022.   \n[29] Pengcheng Jiang, Shivam Agarwal, Bowen Jin, Xuan Wang, Jimeng Sun, and Jiawei Han. Text augmented open knowledge graph completion via pre-trained language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11161\u201311180, 2023.   \n[30] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems, 33(2):494\u2013514, 2022.   \n[31] Liang Yao, Jiazhen Peng, Chengsheng Mao, and Yuan Luo. Exploring large language models for knowledge graph completion. arXiv preprint arXiv:2308.13916, 2023.   \n[32] Chen Chen, Yufei Wang, Aixin Sun, Bing Li, and Kwok-Yan Lam. Dipping plms sauce: Bridging structure and text for effective knowledge graph completion via conditional soft prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11489\u201311503, 2023.   \n[33] Yanbin Wei, Qiushi Huang, Yu Zhang, and James Kwok. KICGPT: Large language model with knowledge in context for knowledge graph completion. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8667\u20138683, Singapore, December 2023. Association for Computational Linguistics.   \n[34] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, and Xueqi Cheng. Lpnl: Scalable link prediction with large language models. arXiv preprint arXiv:2401.13227, 2024.   \n[35] Daniel M\u00fcllner. Modern hierarchical, agglomerative clustering algorithms. arXiv preprint arXiv:1109.2378, 2011.   \n[36] K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433\u2013439, 1999.   \n[37] Peter J. Rousseeuw. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20:53\u201365, 1987.   \n[38] Baruch Schieber and Uzi Vishkin. On finding lowest common ancestors: Simplification and parallelization. SIAM Journal on Computing, 17(6):1253\u20131262, 1988.   \n[39] Jay Pujara, Eriq Augustine, and Lise Getoor. Sparsity and noise: Where knowledge graph embeddings fall short. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1751\u20131756, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.   \n[40] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd workshop on continuous vector space models and their compositionality, pages 57\u201366, 2015.   \n[41] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250, 2008.   \n[42] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: a core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web, WWW \u201907, page 697\u2013706, New York, NY, USA, 2007. Association for Computing Machinery.   \n[43] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pages 697\u2013706, 2007.   \n[44] Payal Chandak, Kexin Huang, and Marinka Zitnik. Building a knowledge graph to enable precision medicine. Nature Scientific Data, 2023.   \n[45] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:30233\u201330249, 2022.   \n[46] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341, 1995.   \n[47] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661\u201318673, 2020.   \n[48] Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song, Zhiguo Wang, and Dong Yu. Crosslingual knowledge graph alignment via graph matching neural network. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3156\u20133161, 2019.   \n[49] Bayu Distiawan Trisedya, Jianzhong Qi, and Rui Zhang. Entity alignment between knowledge graphs using attribute embeddings. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 297\u2013304, 2019.   \n[50] Qingheng Zhang, Zequn Sun, Wei Hu, Muhao Chen, Lingbing Guo, and Yuzhong Qu. Multiview knowledge graph embedding for entity alignment. arXiv preprint arXiv:1906.02390, 2019.   \n[51] Hailin Wang, Ke Qin, Rufai Yusuf Zakari, Guoming Lu, and Jin Yin. Deep neural network-based relation extraction: an overview. Neural Computing and Applications, pages 1\u201321, 2022.   \n[52] Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, and Houfeng Wang. Learning entity representation for entity disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 30\u201334, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Ethics and Broader Impacts 15 ", "page_idx": 13}, {"type": "text", "text": "B PrimeKG Dataset Processing & Subset Construction 15 ", "page_idx": 13}, {"type": "text", "text": "C Results on WN18RR Dataset 16 ", "page_idx": 13}, {"type": "text", "text": "D Supplemental Implementation Details 17 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 KG-FIT 17   \nD.2 Baseline Implementation 18   \nD.2.1 PLM-based Methods 18   \nD.2.2 Sturcture-based Methods 18   \nE Prompts 19   \nE.1 Prompt for Entity Description 19   \nE.2 Prompts for LLM-Guided Hierarchy Refinement 19 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "F Details of KG-FIT Hierarchy Construction 22 ", "page_idx": 13}, {"type": "text", "text": "F.1 Seed Hierarchy Construction 22   \nF.2 LLM-Guided Hierarchy Refinement 23   \nF.3 Statistics of Constructed Hierarchies 24   \nF.4 Examples of LLM-Guided Hierarchy Refinement 25 ", "page_idx": 13}, {"type": "text", "text": "G Score Functions 26 ", "page_idx": 13}, {"type": "text", "text": "H Computational Cost 27 ", "page_idx": 13}, {"type": "text", "text": "H.1 Hardware and Software Configuration 27   \nH.2 Cost of Close-Source LLM APIs 27   \nI Hyperparameter Study 27   \nDownstream Applications of KG-FIT 29   \nK Interpreting Score Functions in KG-FIT 30   \nL Notation Table 33 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Ethics and Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "KG-FIT is a framework for enhancing knowledge graph embeddings by incorporating open knowledge from large language models. As a foundational research effort, KG-FIT itself does not have direct societal impacts. However, the enriched knowledge graph embeddings produced by KG-FIT could potentially be used in various downstream applications, such as question answering, recommendation systems, and drug discovery. ", "page_idx": 14}, {"type": "text", "text": "The broader impacts of KG-FIT depend on how the enhanced knowledge graph embeddings are used. On the positive side, KG-FIT could lead to more accurate and comprehensive knowledge representations, enabling better performance in beneficial applications like medical research and personalized recommendations. However, as with any powerful technology, there is also potential for misuse. The enriched knowledge could be used to build systems that spread disinformation or make unfair decisions that negatively impact specific groups. ", "page_idx": 14}, {"type": "text", "text": "To mitigate potential negative impacts, we encourage responsible deployment of KG-FIT and the enhanced knowledge graph embeddings it produces. This includes carefully monitoring downstream applications for fairness, robustness, and truthfulness. Additionally, when releasing KG-FIT-enhanced knowledge graph embeddings, we recommend providing usage guidelines and deploying safeguards to prevent misuse, such as gated access and safety filters, as appropriate. ", "page_idx": 14}, {"type": "text", "text": "As KG-FIT relies on large language models, it may inherit biases present in the language models\u2019 training data. Further research is needed to understand and mitigate such biases. We also encourage future work on building knowledge graph embeddings that are more inclusive and less biased. ", "page_idx": 14}, {"type": "text", "text": "Overall, while KG-FIT is a promising framework for advancing knowledge graph embeddings, it is important to consider its limitations and potential broader impacts. Responsible development and deployment will be key to realizing its beneftis while mitigating risks. We are committed to fostering an open dialogue on these issues as KG-FIT and related technologies progress. ", "page_idx": 14}, {"type": "text", "text": "B PrimeKG Dataset Processing & Subset Construction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We describe the process of constructing a subset of the PrimeKG1 [44] (version: V2, license: CC0 1.0) dataset. The goal is to create a highly-focused subset while leveraging additional information from the entire knowledge graph to assess KG embedding models\u2019 abilities on predicting drug-disease relationships. ", "page_idx": 14}, {"type": "text", "text": "The dataset construction process involves several detailed steps to ensure the creation of balanced training, validation, and testing sets from PrimeKG. ", "page_idx": 14}, {"type": "text", "text": "Validation/Testing Set Creation: We begin by selecting triples from the original PrimeKG where the type of the head entity is \"drug\" and the type of the tail entity is \"disease.\" This selection yields a subset containing 42,631 triples, 2,579 entities, and 3 relations (\"contraindication,\" \"indication,\" \"off-label use\"). From this subset, we randomly select 3,000 triples each for the validation and testing sets, ensuring no overlap between the two. ", "page_idx": 14}, {"type": "text", "text": "Training Set Creation: We first extract the unique entities present in the validation and testing sets.   \nThese entities are used to ensure comprehensive coverage in the training set. ", "page_idx": 14}, {"type": "text", "text": "For each triple in the validation/testing set, we search for triples involving either its head or tail entity across the entire PrimeKG dataset, for the training set construction: ", "page_idx": 14}, {"type": "text", "text": "Step 1 involves searching for 1-hop triples within the specified relations (\"contraindication,\" \"indication,\" \"off-label use\"). ", "page_idx": 14}, {"type": "text", "text": "Step 2 randomly enriches the training set with triples involving other relations, with a limit of up to 10 triples per entity. ", "page_idx": 14}, {"type": "text", "text": "Step 3 involves removing redundant triples and any triples that are symmetric to those in the validation/testing set, enhancing the challenge posed by the dataset. If the training set contains fewer than 100,000 triples, we return to Step 2 and continue the process until the desired size is achieved. ", "page_idx": 14}, {"type": "text", "text": "Following our methodology, we construct a subset of the PrimeKG dataset that emphasizes drug-disease relations while incorporating broader context from the entire dataset. This subset is then split into training, validation, and testing sets, ensuring proper files are generated for training and evaluating knowledge graph embedding (KGE) models. The dataset contains 10,344 entities with 11 types of relations: disease_phenotype_positive, drug_effect, indication, contraindication, disease_disease, disease_protein, disease_phenotype_negative, exposure_disease, drug_protein, off-label use, drug_drug. The testing and validation sets include three specific relations (\"contraindication,\" \"indication,\" \"off-label use\"). ", "page_idx": 15}, {"type": "text", "text": "C Results on WN18RR Dataset ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "rDoPMODpki/tmp/68a9afbe33532aacb7fbbbfeb8de1eb52a95753374f8510e96253cd1ac87f94b.jpg", "table_caption": ["Table 5: Link Prediction Results on WN18RR. Results are averaged values of ten independent runs of head/tail entity predictions. Top-6 results for each metric are highlighted in bold. \u201c\\*\u201d indicates the results taken from the method\u2019s original paper. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "In this section, we present our results and findings on the WN18RR dataset. WN18RR [17] is derived from WordNet [46], a comprehensive lexical knowledge graph for the English language. This subset addresses the test leakage problems identified in WN18. The statistics of WN18RR are shown in Table 6. $\\tau_{\\mathrm{optim}}$ we found for the seed hierarchy construction on WN18RR is 0.44. ", "page_idx": 15}, {"type": "text", "text": "Our experiments reveal that although KG-FIT did not surpass the performance of C-LMKE and SimKGC, its integration with PKGC [28], employing KG-FIT as a recall model followed by reranking with PKGC, yielded comparable results to these leading models. ", "page_idx": 15}, {"type": "text", "text": "Several factors contribute to these observations: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "(1) WN18RR, being a lexical dataset, beneftis significantly from pre-trained language model (PLM) based methods, which assimilate extensive lexical knowledge during fine-tuning. KG-FIT, relying on static knowledge graph embeddings, lacks this capability. However, as shown in Table $\\underline{{5}}$ , its combination with PKGC leverages the strengths of both models, resulting in markedly improved performance. ", "page_idx": 15}, {"type": "text", "text": "Table 6: Statistics of WN18RR. #Ent./#Rel: number of entities/relations. #Train/#Valid/#Test: number of triples contained in the set. ", "page_idx": 16}, {"type": "table", "img_path": "rDoPMODpki/tmp/b86e7bb857c939f5ad9e22b023deb4d910ce1654d02b682b6f1e658f16d96a68.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "(2) WN18RR has a very close semantic similarity between the head and tail entities. The dataset is full of lexical relations such as \u201chypernym\u201d, \u201cmember_meronym\u201d, \u201cverb_group\u201d, etc. C-LMKE and SimKGC, both utilizing contrastive learning [47] with PLMs, effectively exploit this semantic proximity during training. This ability to discern subtle semantic nuances contributes to their superior performance over other PLM-based approaches. ", "page_idx": 16}, {"type": "text", "text": "(3) SimKGC implements a \u201cGraph-based Re-ranking\u201d strategy that narrows the candidate pool to the $\\mathbf{k}$ -hop neighbors of the source entity from the training set. This approach intuitively boosts performance by focusing on more relevant candidates. For a fair comparison, our experiments did not adopt this specific setting across all datasets. In Table 5, we showcase how this strategy could boost the performance for KG-FIT as well. ", "page_idx": 16}, {"type": "text", "text": "As a future work, we will explore the integration of contrastive learning into the KG-FIT framework to enhance its capability to capture semantic relationships more effectively. ", "page_idx": 16}, {"type": "text", "text": "D Supplemental Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide more implementation details of both KG-FIT and baseline models, to improve the reproducibility of our work. ", "page_idx": 16}, {"type": "text", "text": "D.1 KG-FIT ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Pre-computation. In our KG-FIT framework, there is a pre-computation step to avoid overhead during the fine-tuning phase. The data we need to pre-compute includes: ", "page_idx": 16}, {"type": "text", "text": "1. Cluster embeddings c: These are computed by averaging the all initial entity embeddings $\\left(\\mathbf{v}_{i}\\right)$ within each cluster.   \n2. Neighbor cluster IDs $S_{m}(C)$ in Eq. 5): These are computed using the lowest common ancestor (LCA) approach, where we set the ancestor as the grandparent (i.e., two levels above the node) and search for at most $m=5$ neighbor clusters.   \n3. Parent node IDs: These represent the node IDs along the path from a cluster (leaf node) to the root of the hierarchy. ", "page_idx": 16}, {"type": "text", "text": "The pre-computed data is then used to efficiently locate the embeddings of clusters $(\\mathbf{c},\\mathbf{c}^{\\prime})$ and parent nodes $(\\mathbf{p})$ for the hierarchical clustering constraint during fine-tuning. With this pre-computation, we significantly speed up the training process, as the necessary information is readily available and does not need to be calculated on-the-fly. ", "page_idx": 16}, {"type": "text", "text": "Distance Function. We employ cosine distance as the distance metric for computing the hierarchical clustering constraint (Eq. 5) and the semantic anchoring constraint (Eq. 6). Cosine distance effectively captures the semantic similarity between embeddings, making it well-suited for maintaining the semantic meaning of the entity embeddings during fine-tuning. Cosine distance is also invariant to the magnitude of the embeddings, allowing the fine-tuned embeddings to adapt their magnitude based on the link prediction objective while preserving their semantic orientation with respect to the frozen reference embeddings. Moreover, cosine distance is independent of the link prediction objective, focusing on preserving the semantic properties of the embeddings. ", "page_idx": 16}, {"type": "text", "text": "Alternative distance metrics, such as Euclidean distance or L1 norm, were considered but deemed much slower in KG-FIT framework, aligning with the descriptions in OpenAI\u2019s document on the embedding models2. ", "page_idx": 16}, {"type": "text", "text": "Constraint Computation Options. During fine-tuning, each training step involves a batch composed of one positive triple and $b$ negative samples, where $b$ is the negative sampling size. We provide two options for computing the constraints along with the link prediction score: ", "page_idx": 16}, {"type": "text", "text": "1. Full Constraint Computation: This option computes the distances for both the positive triple and the negative triples. For the positive triple, distances are computed for both the source and target entities. For the negative triples, distances are computed only for the source entities. The advantage of this option is that it allows the model to converge in fewer steps and generally achieves better performance after fine-tuning. However, the drawback is that the computation for each batch is nearly doubled, as positive and negative triples are separately fed into the model for score computation. ", "page_idx": 17}, {"type": "text", "text": "2. Partial Constraint Computation: This option computes the distances only for the source entities in the negative batch. It is faster than the full constraint computation option and can achieve comparable results based on our observations. ", "page_idx": 17}, {"type": "text", "text": "D.2 Baseline Implementation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.2.1 PLM-based Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "PKGC. For PKGC on FB15K-237, we use the templates the authors constructed and released 3 for FB15K-237-N, and use RoBERTa-Large as the base pre-trained langauge model. For YAGO3-10 and PrimeKG, we manually created templates for relations. For example, we converted the relation \u201cisLeaderOf\u201d to \u201c[X] is a leader of [Y].\u201d for YAGO3-10 and converted the relation \u201cdrug_effect\u201d \u201cdrug [X] has effect [Y]\u201d. We choose TuckER as PKGC\u2019s backbone KGE recall model with hyperparameter $\\mathcal{X}=100$ for its overall great performance across all datasets. This means that we select top 50 results from TuckER and feed the shuffled entities into PKGC for re-ranking. We set batch size as 256 and run on 1 NVIDIA A6000 GPU for both training and testing. ", "page_idx": 17}, {"type": "text", "text": "SimKGC. For SimKGC on PrimeKG, for fairness, we do not use the \u201cgraph-based re-ranking\u201d strategy introduced in their paper [27], which adds biases to the scores of known entities (in the training set) within $k$ -hop of the source entity. The released code4was used for experiments. We set batch size as 256 and run on 1 NVIDIA A6000 GPU for both training and testing. ", "page_idx": 17}, {"type": "text", "text": "CSProm-KG. For CSProm-KG on PrimeKG, we use ConvE as the backbone graph model, which is the best Hits $\\mathbb{\\@N}$ performed model. The other settings we use are the same as what reported in the paper. We use the code5 released by the authors to conduct the experiments. ", "page_idx": 17}, {"type": "text", "text": "D.2.2 Sturcture-based Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "TuckER and ConvE. For TuckER, we use its release code6 to run the experiments. For ConvE, we use its PyTorch implementation in PKGC\u2019s codebase7, provided in the same framework as TuckER. It is worth noting that we do not use their proposed \u201clabel smoothing\u201d setting for fair comparison. ", "page_idx": 17}, {"type": "text", "text": "TransE, DistMult, ComplEx, pRotatE, RotatE. For those KGE models, we reuse the code base8 released by RotatE [19]. As it provides a unified framework and environment to run all those models, enabling a fair comparison. Our code (\u201ccode/model_common.py\u201d) also adapts this framework as the foundation. ", "page_idx": 17}, {"type": "text", "text": "HAKE. HAKE\u2019s code9 was built upon RotatE\u2019s repository described above. In our work, we integrate the implementation of HAKE into our framework, which is also based on RotatE\u2019s. ", "page_idx": 17}, {"type": "text", "text": "The hyperparameters we used are presented in Appendix I. ", "page_idx": 17}, {"type": "image", "img_path": "rDoPMODpki/tmp/fc4f140b0ca235600b5c20df44fe75eaa0c1ae70fd1247a8dd6b531a1e88703a.jpg", "img_caption": ["Figure 7: Prompt for Entity Description. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Prompts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Prompt for Entity Description ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure 7 showcases the prompts we used for entity description. In the prompt, we instruct the LLM to provide a brief and concrete description of the input entity. We include an example, such as \u201capple is a round fruit with red, green, or yellow skin and crisp, juicy flesh\u201d to illustrate that the description should be concise yet cover multiple aspects. Conciseness is crucial to minimize noise. For datasets like FB15K-237 and WN18RR, where descriptions are already available [23], we use the original description as a hint and ask the LLM to output the description in a unified format. ", "page_idx": 18}, {"type": "text", "text": "E.2 Prompts for LLM-Guided Hierarchy Refinement ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Fig. 8 and 9 show the main prompts we used for LLM-Guided Hierarchy Refinement (LHR). ", "page_idx": 18}, {"type": "text", "text": "Prompt for Cluster Splitting $\\mathcal{P}_{\\mathbf{SPLIT}})$ : This prompt is designed to guide the LLM in identifying and splitting a given cluster into meaningful subclusters based on the characteristics of the entities. The prompt works as follows: ", "page_idx": 18}, {"type": "text", "text": "1. Input Entities: The entities from the specified cluster are provided as input. ", "page_idx": 18}, {"type": "text", "text": "2. Analysis and Grouping: The LLM is tasked with analyzing the entities to determine if they can be grouped into distinct subclusters based on common attributes like characteristics, themes, or genres. ", "page_idx": 18}, {"type": "text", "text": "3https://github.com/THU-KEG/PKGC   \n4https://github.com/intfloat/SimKGC   \n5https://github.com/chenchens190009/CSProm-KG   \n6https://github.com/ibalazevic/TuckER   \n7https://github.com/THU-KEG/PKGC/blob/main/TuckER/model.py   \n8https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding   \n9https://github.com/MIRALab-USTC/KGE-HAKE ", "page_idx": 18}, {"type": "image", "img_path": "rDoPMODpki/tmp/27ed8a80ca3a33674e2f90523adbeee4a4d30bc893005bb4317b5b010f61ff6b.jpg", "img_caption": ["Figure 8: Prompt of LLM_SPLIT_CLUSTER. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "3. Sub-cluster Naming: If subclusters are formed, the LLM provides clear and concise names for each subcluster that represent the common attributes of their entities. Each subcluster is given a unique name that uniformly describes its entities, ensuring differentiation between clusters. ", "page_idx": 19}, {"type": "text", "text": "4. Control of Sub-cluster Count: The prompt instructs the LLM to control the number of subclusters between 1 and 5. If the entities are already well-grouped, no further sub-clustering is needed, and the original cluster is returned. ", "page_idx": 19}, {"type": "text", "text": "The output format ensures structured and consistent results, facilitating easy integration into the hierarchy refinement process. ", "page_idx": 19}, {"type": "text", "text": "Prompt for Bottom-Up Refinement (PREFINE): This prompt guides the LLM in refining the parentchild triples within the hierarchy. The refinement process involves several key steps: ", "page_idx": 19}, {"type": "text", "text": "1. Input Clusters: Two clusters, A and B, along with their entities, are provided as input. ", "page_idx": 19}, {"type": "text", "text": "2. Analysis of Clusters: The LLM analyzes the two clusters and their entities to determine the most appropriate update mode. The alignment of update modes to the actions described in the methodology section is as follows: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Update Mode 1 (Create New Cluster C): Aligns with NO UPDATE. These two clusters cannot be merged, and no cluster belongs to any other. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Update Mode 2 (Merge Cluster A and B): Aligns with PARENT MERGE & LEAF MERGE. These two nodes can be merged. The name of the new merged node should be similar to both nodes. ", "page_idx": 19}, {"type": "image", "img_path": "rDoPMODpki/tmp/a192165d69cc66d7f071a3bd2267b34fb50aaa2822b137b08fa4d194e0dd1f1d.jpg", "img_caption": ["Figure 9: Prompt of LLM_UPDATE. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "\u2022 Update Mode 3 & 4 (Cluster A Covers Cluster B): Aligns with INCLUDE where $P^{\\prime}=P\\cup R,L^{\\prime}=$ $L,R^{\\prime}=\\emptyset$ . Cluster B belongs to cluster A. Cluster B is a subcluster of cluster A. The name of cluster A should uniformly describe the entities from both clusters. ", "page_idx": 20}, {"type": "text", "text": "3. Output Format: The output includes the selected update mode and the suggested name for the new or merged cluster, ensuring clarity and consistency in the hierarchy refinement process. ", "page_idx": 20}, {"type": "text", "text": "These prompts, illustrated in Fig. 8 and Fig. 9, are essential for transforming the seed hierarchy into a more refined and accurate LLM-guided hierarchy, enabling better hierarchical representation to be learned by KG-FIT. ", "page_idx": 21}, {"type": "text", "text": "F Details of KG-FIT Hierarchy Construction ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "F.1 Seed Hierarchy Construction ", "page_idx": 21}, {"type": "table", "img_path": "rDoPMODpki/tmp/65282918d41e680a5fe69381b198dc3f38a8d7d994e0eeebe1e8d5fe36bf2da9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "This section details the seed hierarchy construction mentioned in Section 3.1. Algorithm $\\underline{{1}}$ shows the pseudo code for this process. ", "page_idx": 21}, {"type": "text", "text": "The process begins with the agglomerative hierarchical clustering of the enriched entity representations $\\mathbf{V}$ using a range of thresholds $[\\tau_{\\mathrm{min}},\\tau_{\\mathrm{max}}]$ . For each threshold $\\tau$ , the clustering labels $(\\mathrm{labels}_{\\tau})$ ) are obtained, and the silhouette score $(S_{\\tau})$ is calculated. The optimal threshold $\\tau_{\\mathrm{optim}}$ is determined by selecting the threshold that maximizes the silhouette score. ", "page_idx": 21}, {"type": "text", "text": "Next, the initial hierarchy $\\mathcal{H}_{\\mathrm{init}}$ is constructed based on the clustering labels obtained using the optimal threshold $(\\mathrm{labels}_{\\tau_{\\mathrm{optim}}.}$ ). The ConstructBinaryTree function builds a binary tree structure where each leaf node represents an entity. ", "page_idx": 22}, {"type": "text", "text": "The top-down entity replacement algorithm is then applied to the initial hierarchy $\\mathcal{H}_{\\mathrm{init}}$ . It traverses the hierarchy in a top-down manner, starting from the root node. For each node encountered during the traversal: ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the node is a leaf, the entity associated with the node is retrieved using the GetEntity function.   \n\u2022 The cluster to which the entity belongs is obtained using the GetCluster function and the $\\mathrm{\\labels}_{\\tau_{\\mathrm{optim}}}$ .   \n\u2022 If the cluster has already been visited (i.e., it exists in the visited_clusters set), the leaf node is removed from the hierarchy.   \n\u2022 If the cluster has not been visited, the leaf node is replaced with the cluster, and the cluster is added to the visited_clusters set.   \n\u2022 If the node is not a leaf, the algorithm recursively applies the same process to its child nodes. ", "page_idx": 22}, {"type": "text", "text": "This top-down approach ensures that entities are included in their respective clusters as early as possible during the traversal of the hierarchy. By keeping track of the visited clusters, the algorithm avoids duplicating clusters in the hierarchy, resulting in a more compact and coherent representation. ", "page_idx": 22}, {"type": "text", "text": "Finally, the refinement steps (Refine function) are applied to the modified hierarchy to obtain the final seed hierarchy $\\mathcal{H}_{\\mathrm{seed}}$ . The refinement steps include handling empty dictionaries, single-entry dictionaries, and updating cluster assignments. ", "page_idx": 22}, {"type": "text", "text": "The resulting seed hierarchy $\\mathcal{H}_{\\mathrm{seed}}$ represents a hierarchical organization of the entities, where each node is either a cluster containing a list of entities or a sub-hierarchy representing a more finegrained grouping of entities. This seed hierarchy serves as a starting point for further refinement and incorporation of external knowledge in the subsequent steps of the KG-FIT framework. ", "page_idx": 22}, {"type": "text", "text": "F.2 LLM-Guided Hierarchy Refinement ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "LLM-Guided Hierarchy Refinement (LHR) is used to further refine the seed hierarchy, which can better reflect relationships among entities. The process of LLM-Guided Hierarchy Refinement can be divided into two steps: LLM-Guided Cluster Splitting and LLM-Guided Bottom-Up Hierarchy Refinement. ", "page_idx": 22}, {"type": "text", "text": "As described in Algorithm 2, the LLM-Guided Cluster Splitting algorithm is designed to iteratively split clusters in a hierarchical structure with the guidance of a large language model (LLM). The algorithm begins with an initial hierarchy seed $H_{\\mathrm{seed}}$ and outputs a split hierarchy $H_{\\mathrm{split}}$ . Initially, the current cluster ID is set to 0. The recursive procedure RECURSION_SPLIT_CLUSTER is then defined to manage the splitting process. If the root of the current cluster is a list and its length is less than a predefined minimum number of entities in a leaf node (MIN_ENTITIES_IN_LEAF), the function returns, indicating no further splitting is necessary. Otherwise, the root cluster\u2019s entities are passed to the LLM, which names the cluster and splits it into smaller clusters. If the split results in only one cluster, the function returns. For each resulting subcluster, the entities are assigned a new cluster ID, and the splitting process is recursively applied. If the root is not a list, the function iterates over each subcluster and applies the splitting procedure. Finally, the algorithm initiates the recursive splitting on the initial hierarchy seed and returns the modified hierarchy as $H_{\\mathrm{split}}$ . ", "page_idx": 22}, {"type": "text", "text": "The LLM-Guided Bottom-Up Hierarchy Refinement algorithm (Algorithm 3) refines a previously split hierarchy $H_{\\mathrm{split}}$ to produce a more coherent and meaningful hierarchy $H_{\\mathrm{LHR}}$ . This process is also guided by an LLM. The algorithm defines a recursive procedure RECURSION_REFINE_HIERARCHY, which starts by checking if the root of the current cluster is a list. If it is, the LLM is used to name the cluster, and the updated hierarchy is returned. Otherwise, the procedure recursively refines the left and right child nodes. The children of these nodes are then evaluated, and the LLM suggests an update mode based on the names and children of the right and left nodes. Depending on the suggested update mode, the algorithm may add the right node under the left children, the left node under the right children, or merge the left and right clusters. If no update is suggested, the procedure continues. The updated hierarchy is returned after applying the necessary refinements. The algorithm initiates the refinement process on the split hierarchy $H_{\\mathrm{split}}$ and outputs the refined hierarchy $H_{\\mathrm{LHR}}$ . ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 LLM-Guided Cluster Splitting ", "page_idx": 23}, {"type": "text", "text": "1: Input: Hseed   \n2: Output: $\\mathcal{H}_{\\mathrm{split}}$   \n3: current_cluster $\\mathrm{id}\\gets0$   \n4: procedure RECURSION_SPLIT_CLUSTER(root)   \n5: if root is a list then   \n6: if length of root $<$ MIN_ENTITIES_IN_LEAF then   \n7: return   \n8: end if   \n9: cluster_entitie $\\mathrm{\\bf~i}\\leftarrow\\mathrm{\\bfroot}$   \n10: cluster_name $\\leftarrow$ LLM_NAME_CLUSTER(cluster_entities)   \n11: splitted_clusters $\\leftarrow$ LLM_SPLIT_CLUSTER(cluster_name, cluster_entities)   \n12: if length of splitted_clusters $\\scriptstyle==1$ then   \n13: return   \n14: end if   \n15: for each (name, entities) in splitted_clusters do   \n16: root[current_cluster_id] $\\leftarrow$ entities   \n17: current_cluster_id $\\leftarrow$ current_cluster_id $+\\,1$   \n18: RECURSION_SPLIT_CLUSTER(root[current_cluster_id])   \n19: end for   \n20: else   \n21: for each (key, subcluster) in root do   \n22: RECURSION_SPLIT_CLUSTER(subcluster)   \n23: end for   \n24: end if   \n25: end procedure   \n26: RECURSION_SPLIT_CLUSTER(Hseed)   \n27: $\\mathcal{H}_{\\mathrm{split}}\\leftarrow\\mathcal{H}_{\\mathrm{seed}}$   \n28: return Hsplit ", "page_idx": 23}, {"type": "text", "text": "F.3 Statistics of Constructed Hierarchies ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "rDoPMODpki/tmp/c2b68b624c4a66f631e451529afa138c00fa1f95221f8fc4bba2ae56dd01768b.jpg", "table_caption": ["Table 7: Statistics of the hierarchies constructed by KG-FIT on different datasets. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 7 presents the statistics of the hierarchies constructed by KG-FIT on four different datasets: FB15K-237, YAGO3-10, PrimeKG, and WN18RR. The table compares the seed hierarchy (Seed) with the LLM-Guided Refined hierarchy (LHR) to show the changes and improvements after applying the LLM-guided hierarchy refinement process. ", "page_idx": 23}, {"type": "text", "text": "The number of clusters decreases across all datasets after refinement. For instance, in the FB15K-237 dataset, the clusters reduce from 5226 to 5073. Similarly, the number of nodes also decreases; for FB15K-237, nodes go from 10452 to 8987. The number of entities within each cluster sees a slight increase in the average number, with the maximum and minimum values remaining fairly constant. For example, the average number of entities per cluster in FB15K-237 increases from 2.78 to 2.81. The depth of the hierarchies shows a noticeable reduction, with the maximum and average depths decreasing. In FB15K-237, the maximum depth goes from 46 to 40, and the average depth drops from 25.52 to 18.64. The branching factor of nodes also increases slightly, indicating a more interconnected structure; in FB15K-237, the average number of branches per node rises from 2 to 2.3. Similar patterns are observed in other datasets. ", "page_idx": 23}, {"type": "table", "img_path": "rDoPMODpki/tmp/2f9018dcf35c193605541a35fdb8d1df010ea5aa111385bb2f992f72cb537cda.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Overall, Table 7 illustrates that the refinement process effectively reduces the number of clusters and nodes, slightly increases the number of entities per cluster, decreases the depth of the hierarchy, and increases the branching factor. These changes suggest that the refinement process results in a more compact and interconnected hierarchy, better reflecting relationships among entities. The general effect of the refinement is the creation of a more streamlined and coherent hierarchical structure. ", "page_idx": 24}, {"type": "text", "text": "F.4 Examples of LLM-Guided Hierarchy Refinement ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Table 8: An example of LLM-Guided Cluster Splitting. ", "page_idx": 24}, {"type": "table", "img_path": "rDoPMODpki/tmp/fa80c94898757ef8176339fd6f09d9fcbce8a7ec7bcf002e6541e1f95317e65c.jpg", "table_caption": ["Table 9: Examples of LLM-Guided Bottom-Up Hierarchy Refinement. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "\u201cUniversities and Colleges\u201d includes entities such as Princeton University and Harvard College. Additionally, a sub-cluster named \u201cMedical Schools\u201d includes entities such as Yale School of Medicine. The cluster of \u201cUniversities and Colleges\u201d will be divided into the sub-clusters of \u201cUniversities\u201d and \u201cColleges\u201d in the next step. This example shows that LLM can divide the original large cluster into multiple smaller, refined ones. ", "page_idx": 25}, {"type": "text", "text": "Table 9 demonstrates various cases of cluster updates in LLM-Guided Bottom-Up Hierarchy Refinement. Each case lists two clusters (Cluster A and Cluster B) along with their entities and the type of update applied. In some cases, no update is needed, and both clusters remain unchanged. In others, Cluster B is merged into Cluster A, indicating a hierarchical relationship. There are instances where entities from Cluster B are integrated into Cluster A. Additionally, some updates involve Cluster A including all entities of Cluster B, making Cluster B a subset of Cluster A, or vice versa. These examples all show that the LLM correctly fixes the errors in the original seed hierarchy and refines the hierarchical structure to better reflect world knowledge. ", "page_idx": 25}, {"type": "text", "text": "Overall, these examples demonstrate how clusters are refined to better represent the relationships between entities, leading to a more organized and efficient structure. These examples highlight the detailed organization of entities into meaningful sub-clusters, reflecting their natural groupings and relationships. The refinement process not only improves the overall structure but also enhances the clarity and accessibility of the hierarchy. ", "page_idx": 25}, {"type": "text", "text": "G Score Functions ", "text_level": 1, "page_idx": 25}, {"type": "table", "img_path": "rDoPMODpki/tmp/e0bec453bd6105d6ea6e791cfaa9beeb8547703fd3ab5a3af07cea58621bfb79.jpg", "table_caption": ["Table 10: Score functions defined by the KGE methods tested in this work. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "The score functions defined by the structure-based KG embedding methods [14, 15, 16, 17, 18, 19, 20, 21] we tested in this papaer are shown in Table 10. Here is a paragraph explaining each notation in the table, with all explanations inline: ", "page_idx": 25}, {"type": "text", "text": "The table presents the score functions $f_{r}(\\mathbf{h},\\mathbf{t})$ used by various knowledge graph embedding (KGE) methods, where h, r, and t represent the head entity, relation, and tail entity embeddings, respectively. TransE uses a translation-based score function with either L1 or L2 norm, denoted by $\\|\\cdot\\|_{1/2}$ , where the embeddings are in real space $\\mathbb{R}^{k}$ . DistMult employs a bilinear score function with a diagonal relation matrix diag $({\\bf r})$ , and the embeddings are also in $\\mathbb{R}^{k}$ . ComplEx extends DistMult by using complex-valued embeddings in $\\mathbb{C}^{k}$ and takes the real part of the score, denoted by $\\operatorname{Re}(\\cdot)$ . ConvE applies a 2D convolution operation, where $\\overline{{\\mathbf{h}}}$ is the 2D reshaping of $\\mathbf{h},\\ast$ represents the convolution, $\\omega$ is a set of fliters, $\\operatorname{vec}({\\cdot})$ is a vectorization operation, and W is a linear transformation matrix. TuckER uses a Tucker decomposition with a core tensor $\\mathcal{W}$ and relation-specific weights $\\mathbf{w}_{r}\\in\\mathbb{R}^{d_{r}}$ , where $\\times_{n}$ denotes the tensor product along the $n$ -th mode. pRotatE and RotatE employ rotation-based score functions in complex space, with $\\circ$ representing the Hadamard product and $|\\mathbf{r}_{i}|=1$ constraining the relation embeddings to have unit modulus. HAKE combines a modulus part $(\\mathbf{h}_{m},\\mathbf{t}_{m}\\in\\mathbb{R}^{k},\\mathbf{t}_{m}\\in\\mathbb{R}_{+}^{k})$ and a phase part $(\\mathbf{h}_{p},\\mathbf{r}_{p},\\mathbf{t}_{p}\\in[0,2\\pi)^{k})$ in its score function, with a hyperparameter $\\lambda\\in\\mathbb R$ balancing the two parts. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "H Computational Cost ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "H.1 Hardware and Software Configuration ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Based on the dataset size, we hybridly use two machines: ", "page_idx": 26}, {"type": "text", "text": "For FB15K-237, PrimeKG, and WN18RR, experiments are conducted on a machine equipped with two AMD EPYC 7513 32-Core Processors, 528GB RAM, eight NVIDIA RTX A6000 GPUs, and CUDA 12.4 and the NVIDIA driver version 550.76. ", "page_idx": 26}, {"type": "text", "text": "For YAGO3-10, due to its large size, experiments are conducted on a machine equipped with two AMD EPYC 7513 32-Core Processors, 528GB RAM, and eight NVIDIA A100 80GB PCIe GPUs. The system uses CUDA 12.2 and the NVIDIA driver version 535.129.03. ", "page_idx": 26}, {"type": "text", "text": "With a single GPU, it takes about 2.5, 4.5, 2.0, and 1.1 hours for a KG-FIT model to achieve good performance on FB15K-237, YAGO3-10, PrimeKG, and WN18RR, respectively. ", "page_idx": 26}, {"type": "text", "text": "H.2 Cost of Close-Source LLM APIs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The costs of GPT-4o for entity description generation are $\\underline{{\\mathbb{S}3.0}}$ , $\\underline{{\\mathbb{S}24.8}}$ , $\\underline{{\\mathbb{S}2.1}}$ , and $\\underline{{\\mathbb{S}8.6}}$ for FB15K237, YAGO3-10, PrimeKG, and WN18RR, respectively, proportional to their numbers of entities. ", "page_idx": 26}, {"type": "text", "text": "The cost of text embedding models (text-embedding-3-large) for entity embedding initialization was totally about $\\underline{{\\mathbb{S}0.8}}$ to process all the KG datasets. ", "page_idx": 26}, {"type": "text", "text": "The costs of GPT-4o for LLM-Guided Hierarchy Refinement on FB15K-237, YAGO3-10, PrimeKG, and WN18RR are $\\underline{{\\mathbb{S}10.0}}$ , $\\underline{{\\mathbb{S}74.5}}$ , $\\underline{{\\mathbb{S}8.7}}$ , and $\\underline{{\\mathbb{S34.4}}}$ , respectively. This cost is almost proportional to the nodes consisted in the seed hierarchy. ", "page_idx": 26}, {"type": "text", "text": "I Hyperparameter Study ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This section presents a comprehensive hyperparameter study for both structure-based base models and our proposed KG-FIT framework across different datasets. Table 11 outlines the range of hyperparameter values explored during the study. Table 12 showcases the optimal hyperparameter configurations for the base models that yielded the best performance. Similarly, Table 13 presents the best-performing hyperparameter settings for KG-FIT. ", "page_idx": 26}, {"type": "text", "text": "The hyperparameter study aims to provide insights into the sensitivity of the models to various hyperparameters and to identify the optimal configurations that maximize their performance on each dataset. By conducting a thorough exploration of the hyperparameter space, we ensure a fair comparison between the base models and KG-FIT, and demonstrate the robustness and effectiveness of our proposed framework across different settings. ", "page_idx": 26}, {"type": "table", "img_path": "rDoPMODpki/tmp/6a15f3d148ab0b42bd8cf76a7cbe97c44f5bb3887ea4d48705cf7bce8f512ce1.jpg", "table_caption": ["Table 11: Summary of hyperparameters we explored for both base models and KG-FIT. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 12: Best hyperparameters grid-searched for base models on different datasets. ", "page_idx": 27}, {"type": "table", "img_path": "rDoPMODpki/tmp/4e9e786ea97bd92b00dce7959c13df147d172f385d9f588d76fc4d87351a5392.jpg", "table_caption": ["FB15K-237 "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "rDoPMODpki/tmp/59a76b5906fc0f93e39480f101aeb886710be7b5cfeaec7bb6b4e7d6e386bf41.jpg", "table_caption": ["Table 13: Hyperparameters we used for KG-FIT with different base models on different datasets. "], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "rDoPMODpki/tmp/0711127b288afc1fc1fe518aff53cff56d1e78342866dff825b79aeac0771bd0.jpg", "img_caption": ["Figure 10: Applications of KG-FIT. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "The enhanced knowledge graph embeddings produced by KG-FIT can enable improved performance on various downstream tasks. As shown in Fig. 10, potential areas of application include: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Tasks transformed to fundamental KG tasks. KG-FIT\u2019s strong performance on link prediction can directly benefit fundamental KG tasks like knowledge graph-based question answering (KGQA) [1, 2, 4]. For example, given the question \"Can drinking black tea help reduce the risk of cardiovascular diseases?\", a KGQA system powered by KG-FIT embeddings could effectively traversely perform triple classification task for triples such as <black tea, potentially beneficial to, cardiovascular diseases>, <black tea, is a, flavonoids-rich drink>, <flavonoids, may treat, cardiovascular diseases>, and provide an accurate answer based on the classification results. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Entity Matching Across KGs. KG-FIT\u2019s ability to capture both global and local semantics facilitates accurate entity matching across different knowledge graphs [48, 49, 50]. Consider two KGs, A and B, containing information about a company\u2019s products. KG A lists the entity \u201cGlobalTech Inc\u201d launched \u201cSmartVision $3000^{\\circ}$ , which is a product in \u201cSmart Home Devices\u201d, while KG B mentions \"SV3000 Camera\" is released by \"Global Tech Industries\" in the category of \"Home Automation Solution\". By generating semantically rich embeddings that encode textual, hierarchical and relational information, KG-FIT can help identify that \u201cSmartVision $3000^{\\circ}$ and \u201cSV3000 Camera\u201d, \u201cGlobalTech Inc.\u201d and \u201cGlobal Tech Industries\u201d likely refer to the same entities, despite differences in surface form and graph structure. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Retrieval Augmented Generation with Graphs. The hierarchical nature of KG-FIT\u2019s embeddings enables efficient search for relevant information to augment language model-based text generation. In a retrieval augmented generation setup, a language model\u2019s output can be enhanced by retrieving and conditioning on pertinent information. KG-FIT\u2019s embeddings allow for quick identification of relevant entities and relationships via proximity search in the semantic space. Moreover, by traversing the KG-FIT hierarchy, the system can gather additional context about an entity of interest. For instance, if the generation task involves the cardiovascular benefits of black tea, searching the KG-FIT hierarchy may surface related information on flavonoids and antioxidant properties, providing valuable context to guide the language model in producing an informed and factually grounded response. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Other Tasks. KG-FIT\u2019s embeddings can also be leveraged for tasks such as relation extraction [51] and entity disambiguation [52]. By providing high-quality embeddings that encode both local and global information, KG-FIT can improve the accuracy and efficiency of these tasks. For example, in relation extraction, KG-FIT\u2019s embeddings can help identify the most likely relation between two entities given their positions in the hierarchy and their semantic proximity. In entity disambiguation, KG-FIT\u2019s embeddings can be used to disambiguate between multiple entities with the same name by considering their relationships and positions within the knowledge graph hierarchy. ", "page_idx": 28}, {"type": "text", "text": "In summary, KG-FIT\u2019s robust embeddings, capturing both local and global semantics in a hierarchical structure, can significantly enhance a variety of downstream applications, from fundamental KG tasks to entity matching and retrieval augmented generation. By providing semantically rich and efficiently searchable representations of KG entities and relationships, KG-FIT enables knowledge-infused AI systems that can better understand and reason over complex domains. ", "page_idx": 28}, {"type": "text", "text": "K Interpreting Score Functions in KG-FIT ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we analyze how the entity embeddings in KG-FIT are interpreted in the (transitional) score functions defined by different base models. ", "page_idx": 29}, {"type": "text", "text": "Let $\\mathbf{h},\\mathbf{r},\\mathbf{t}\\,\\in\\,\\mathbb{R}^{\\mathrm{dim}}$ denote the head entity, relation, and tail entity embeddings, respectively. In KG-FIT, the entity embeddings $\\mathbf{h}$ and $\\mathbf{t}$ are initialized as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{h}=[\\mathbf{h}_{n};\\mathbf{h}_{d}],\\quad\\mathbf{t}=[\\mathbf{t}_{n};\\mathbf{t}_{d}]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ${\\bf h}_{n},{\\bf t}_{n}\\,\\in\\,\\mathbb{R}^{\\mathrm{dim}/2}$ represent the entity name embeddings and $\\mathbf{h}_{d},\\mathbf{t}_{d}\\,\\in\\,\\mathbb{R}^{\\mathrm{dim}/2}$ represent the entity description embeddings, obtained from the pre-trained text embeddings. ", "page_idx": 29}, {"type": "text", "text": "The inclusion of both $\\mathbf{h}_{n}$ and $\\mathbf{h}_{d}$ (similarly for $\\mathbf{t}_{n}$ and $\\mathbf{t}_{d}$ ) enhances the expressiveness of the entity representation. Starting from these embeddings, KG-FIT effectively captures a more comprehensive understanding of each entity, leading to improved link prediction performance. ", "page_idx": 29}, {"type": "text", "text": "TransE: The TransE score function is defined as: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{r}({\\bf h},{\\bf t})=-\\|{\\bf h}+{\\bf r}-{\\bf t}\\|_{p}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\|\\cdot\\|_{p}$ denotes the $L_{p}$ norm. ", "page_idx": 29}, {"type": "text", "text": "Expanding the score function using the KG-FIT embeddings: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{r}({\\bf h},{\\bf t})=-\\left\\|[{\\bf h}_{n};{\\bf h}_{d}]+{\\bf r}-[{\\bf t}_{n};{\\bf t}_{d}]\\right\\|_{p}}\\\\ &{~~~~~~~~~~~~~~=-\\left\\|[{\\bf h}_{n}+{\\bf r}_{n}-{\\bf t}_{n};{\\bf h}_{d}+{\\bf r}_{d}-{\\bf t}_{d}]\\right\\|_{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathbf{r}=[\\mathbf{r}_{n};\\mathbf{r}_{d}]$ is the relation embedding learned during fine-tuning. ", "page_idx": 29}, {"type": "text", "text": "Interpretation: ", "page_idx": 29}, {"type": "text", "text": "\u2022 $\\mathbf{h}_{n}+\\mathbf{r}_{n}-\\mathbf{t}_{n}$ : This term represents the distance in the embedding space between the head and tail entities\u2019 name embeddings, adjusted by the relation embedding.   \n\u2022 $\\mathbf{h}_{d}+\\mathbf{r}_{d}-\\mathbf{t}_{d}$ : This term represents the distance in the embedding space between the head and tail entities\u2019 description embeddings, adjusted by the relation embedding. ", "page_idx": 29}, {"type": "text", "text": "The TransE score function considers the global semantic information by computing the translation distance between the head and tail entity embeddings, taking into account both the entity name and description embeddings. ", "page_idx": 29}, {"type": "text", "text": "DistMult: In the DistMult model, the score function is defined as the tri-linear dot product between the head entity embedding $\\mathbf{h}$ , the relation embedding $\\mathbf{r}$ , and the tail entity embedding $\\mathbf{t}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{r}(\\mathbf{h},\\mathbf{t})=\\langle\\mathbf{h},\\mathbf{r},\\mathbf{t}\\rangle\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In KG-FIT, the entity embeddings are initialized by concatenating the entity name embedding and the entity description embedding: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{h}=[\\mathbf{h}_{n};\\mathbf{h}_{d}],\\quad\\mathbf{t}=[\\mathbf{t}_{n};\\mathbf{t}_{d}]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, let\u2019s expand the DistMult score function by substituting the KG-FIT entity embeddings: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{f_{r}}({\\bf h},{\\bf t})=\\langle[{\\bf h}_{n};{\\bf h}_{d}],{\\bf r},[{\\bf t}_{n};{\\bf t}_{d}]\\rangle}\\ ~}\\\\ {{\\displaystyle~~~~~~~~=~\\langle{\\bf h}_{n},{\\bf r}_{n},{\\bf t}_{n}\\rangle+\\langle{\\bf h}_{d},{\\bf r}_{d},{\\bf t}_{d}\\rangle}\\ ~}\\\\ {{\\displaystyle~~~~~~~~=\\sum_{i=1}^{\\dim/2}\\left(h_{n,i}\\cdot r_{n,i}\\cdot t_{n,i}\\right)+\\sum_{i=\\dim/2+1}^{\\dim}(h_{d,i}\\cdot r_{d,i}\\cdot t_{d,i})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\mathbf{r}=[\\mathbf{r}_{n};\\mathbf{r}_{d}]$ is the relation embedding learned during fine-tuning. ", "page_idx": 29}, {"type": "text", "text": "Interpretation: ", "page_idx": 29}, {"type": "text", "text": "\u2022 $\\langle\\mathbf{h}_{n},\\mathbf{r}_{n},\\mathbf{t}_{n}\\rangle$ : This term captures the multiplicative interaction between the head entity\u2019s name embedding, the relation embedding, and the tail entity\u2019s name embedding. ", "page_idx": 29}, {"type": "text", "text": "\u2022 $\\langle\\mathbf{h}_{d},\\mathbf{r}_{d},\\mathbf{t}_{d}\\rangle$ : This term captures the multiplicative interaction between the head entity\u2019s description embedding, the relation embedding, and the tail entity\u2019s description embedding. ", "page_idx": 30}, {"type": "text", "text": "The DistMult score function captures the global semantic information by computing the tri-linear dot product between the head entity, relation, and tail entity embeddings. The dot product considers the interactions between the entity name embeddings and the entity description embeddings separately, allowing the model to capture the global semantic relatedness and attributional similarities. ", "page_idx": 30}, {"type": "text", "text": "ComplEx: In the ComplEx model, the score function is defined as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{r}(\\mathbf{h},\\mathbf{t})=\\mathrm{Re}(\\langle\\mathbf{h},\\mathbf{r},\\bar{\\mathbf{t}}\\rangle)}\\\\ &{\\qquad\\qquad=\\langle\\mathrm{Re}(\\mathbf{h}),\\mathrm{Re}(\\mathbf{r}),\\mathrm{Re}(\\mathbf{t})\\rangle+\\langle\\mathrm{Im}(\\mathbf{h}),\\mathrm{Re}(\\mathbf{r}),\\mathrm{Im}(\\mathbf{t})\\rangle}\\\\ &{\\qquad\\qquad\\quad+\\,\\langle\\mathrm{Re}(\\mathbf{h}),\\mathrm{Im}(\\mathbf{r}),\\mathrm{Im}(\\mathbf{t})\\rangle-\\langle\\mathrm{Im}(\\mathbf{h}),\\mathrm{Im}(\\mathbf{r}),\\mathrm{Re}(\\mathbf{t})\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\operatorname{Re}(\\cdot)$ and $\\operatorname{Im}(\\cdot)$ denote the real part and imaginary part of a complex number, and $\\bar{\\mathbf{t}}$ represents the complex conjugate of $\\mathbf{t}$ . ", "page_idx": 30}, {"type": "text", "text": "In KG-FIT, the entity embeddings are initialized as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\bf h}={\\bf h}_{n}+i{\\bf h}_{d},\\quad{\\bf t}={\\bf t}_{n}+i{\\bf t}_{d}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathbf{h}_{n},\\mathbf{t}_{n}\\ \\in\\ \\mathbb{R}^{\\mathrm{dim}/2}$ represent the entity name embeddings (real part) and $\\mathbf{h}_{d},\\mathbf{t}_{d}\\,\\in\\,\\mathbb{R}^{\\mathrm{dim}/2}$ represent the entity description embeddings (imaginary part). ", "page_idx": 30}, {"type": "text", "text": "The relation embedding $\\mathbf{r}$ is also a complex-valued vector: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{r}=\\mathbf{r}_{r}+i\\mathbf{r}_{i}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathbf{r}_{r},\\mathbf{r}_{i}\\in\\mathbb{R}^{\\mathrm{dim}/2}$ are learned embeddings that capture the intricate semantics of the relation in the complex space. ", "page_idx": 30}, {"type": "text", "text": "Thus, the score function using the KG-FIT embeddings becomes: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{r}(\\mathbf{h},\\mathbf{t})=\\mathrm{Re}(\\langle\\mathbf{h},\\mathbf{r},\\bar{\\mathbf{t}}\\rangle)}\\\\ &{\\qquad\\qquad=\\langle\\mathbf{h}_{n},\\mathbf{r}_{r},\\mathbf{t}_{n}\\rangle+\\langle\\mathbf{h}_{d},\\mathbf{r}_{r},\\mathbf{t}_{d}\\rangle+\\langle\\mathbf{h}_{n},\\mathbf{r}_{i},\\mathbf{t}_{d}\\rangle-\\langle\\mathbf{h}_{d},\\mathbf{r}_{i},\\mathbf{t}_{n}\\rangle}\\\\ &{\\qquad\\qquad=\\mathbf{h}_{n}\\circ\\mathbf{r}_{r}\\circ\\mathbf{t}_{n}+\\mathbf{h}_{d}\\circ\\mathbf{r}_{r}\\circ\\mathbf{t}_{d}+\\mathbf{h}_{n}\\circ\\mathbf{r}_{i}\\circ\\mathbf{t}_{d}-\\mathbf{h}_{d}\\circ\\mathbf{r}_{i}\\circ\\mathbf{t}_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Interpretation: ", "page_idx": 30}, {"type": "text", "text": "\u2022 $\\mathbf{h}_{n}\\circ\\mathbf{r}_{r}\\circ\\mathbf{t}_{n}$ and $\\mathbf h_{d}\\circ\\mathbf r_{r}\\circ\\mathbf t_{d}$ : These terms represent the fundamental interactions between the head and tail entity name embeddings modulated by the real part of the relation. They capture symmetric relationships where the semantic integrity of the relation is maintained irrespective of the direction. ", "page_idx": 30}, {"type": "text", "text": "\u2022 $\\mathbf{h}_{n}\\circ\\mathbf{r}_{i}\\circ\\mathbf{t}_{d}$ and $-\\mathbf{h}_{d}\\circ\\mathbf{r}_{i}\\circ\\mathbf{t}_{n}$ : These cross-terms incorporate the imaginary part of the relation embedding, introducing a unique capability to model antisymmetric relations. The inclusion of the imaginary components allows the score function to account for relations where the direction or the orientation between entities significantly alters the meaning or the context of the relation. ", "page_idx": 30}, {"type": "text", "text": "RotatE: In the RotatE model, each relation is represented as a rotation in the complex plane. The score function is defined as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{r}(\\mathbf{h},\\mathbf{t})=-\\|\\mathbf{h}\\circ\\mathbf{r}-\\mathbf{t}\\|}\\\\ &{\\qquad\\qquad=\\left(\\mathrm{Re}(\\mathbf{h})\\circ\\mathrm{Re}(\\mathbf{r})-\\mathrm{Im}(\\mathbf{h})\\circ\\mathrm{Im}(\\mathbf{r})-\\mathrm{Re}(\\mathbf{t})\\right)+}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\mathrm{Re}(\\mathbf{h})\\circ\\mathrm{Im}(\\mathbf{r})+\\mathrm{Im}(\\mathbf{h})\\circ\\mathrm{Re}(\\mathbf{r})-\\mathrm{Im}(\\mathbf{t})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In RotatE with KG-FIT embeddings, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\bf h}={\\bf h}_{n}+i{\\bf h}_{d},\\quad{\\bf t}={\\bf t}_{n}+i{\\bf t}_{d}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and the relation embedding is: ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\bf r}=\\cos(\\theta_{r})+i\\sin(\\theta_{r})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\theta_{r}$ is the learned rotation angle for the relation. ", "page_idx": 30}, {"type": "text", "text": "Expanding the RotatE score function using KG-FIT embeddings, we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{r}(\\mathbf{h},\\mathbf{t})=-\\|\\mathbf{h}\\circ\\mathbf{r}-\\mathbf{t}\\|}\\\\ &{\\qquad\\qquad=-\\left[(\\mathbf{h}_{n}\\circ\\cos(\\theta_{r})-\\mathbf{h}_{d}\\circ\\sin(\\theta_{r})-\\mathbf{t}_{n})+(\\mathbf{h}_{n}\\circ\\sin(\\theta_{r})+\\mathbf{h}_{d}\\circ\\cos(\\theta_{r})-\\mathbf{t}_{d})\\right]}\\\\ &{\\qquad\\qquad=-\\left[(\\mathbf{h}_{n}\\circ\\cos(\\theta_{r})-\\mathbf{t}_{n})+(\\mathbf{h}_{d}\\circ\\cos(\\theta_{r})-\\mathbf{t}_{d})+(\\mathbf{h}_{n}\\circ\\sin(\\theta_{r}))-(\\mathbf{h}_{d}\\circ\\sin(\\theta_{r}))\\right]_{\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Interpretation: ", "page_idx": 31}, {"type": "text", "text": "\u2022 $\\mathbf{h}_{n}\\circ\\cos(\\theta_{r})-\\mathbf{t}_{n}$ and $\\mathbf h_{d}\\circ\\cos(\\theta_{r})-\\mathbf t_{d}$ : These terms represent the rotated head entity name and description embeddings respectively, which are then compared with the corresponding tail entity name and description embeddings. The cosine of the relation angle $\\theta_{r}$ scales the embeddings, effectively capturing the strength of the relationship between the entities. \u2022 ${\\bf h}_{n}\\circ\\sin(\\theta_{r})$ and $-\\mathbf{h}_{d}\\circ\\sin(\\theta_{r})$ : These terms introduce a phase shift in the entity embeddings based on the relation angle. The sine of the relation angle $\\theta_{r}$ allows for modeling more complex interactions between the head and tail entities, considering both the name and description information. ", "page_idx": 31}, {"type": "text", "text": "pRotatE: In the pRotatE model, the modulus of the entity embeddings is constrained such that $\\left|h_{i}\\right|=\\left|t_{i}\\right|=C$ , and the distance function is defined as: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{r}({\\bf h},{\\bf t})=-2C\\left\\|\\sin\\left(\\frac{\\theta_{h}+\\theta_{r}-\\theta_{t}}{2}\\right)\\right\\|_{1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\theta_{h},\\,\\theta_{r}$ , and $\\theta_{t}$ represent the phases of the head entity, relation, and tail entity embeddings, respectively. For KG-FIT embeddings, the entity embeddings are complex and represented as: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{h}=[\\mathbf{h}_{n};\\mathbf{h}_{d}],\\quad\\mathbf{t}=[\\mathbf{t}_{n};\\mathbf{t}_{d}]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The phases can be calculated as: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\theta_{h}=\\mathrm{arg}([\\mathbf{h}_{n};\\mathbf{h}_{d}]),\\quad\\theta_{t}=\\mathrm{arg}([\\mathbf{t}_{n};\\mathbf{t}_{d}]),\\quad\\theta_{r}=\\mathrm{arg}([\\mathbf{r}_{1};\\mathbf{r}_{2}])\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ${\\bf r}_{1},{\\bf r}_{2}\\in\\mathbb{R}^{\\mathrm{dim}/2}$ are the learned relation embeddings. ", "page_idx": 31}, {"type": "text", "text": "Thus, the pRotatE score function using KG-FIT embeddings becomes: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{r}(\\mathbf{h},\\mathbf{t})=-2C\\left\\|\\sin\\left(\\frac{\\mathrm{arg}([\\mathbf{h}_{n};\\mathbf{h}_{d}])+\\mathrm{arg}([\\mathbf{r}_{1};\\mathbf{r}_{2}])-\\mathrm{arg}([\\mathbf{t}_{n};\\mathbf{t}_{d}])}{2}\\right)\\right\\|_{1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Interpretation: ", "page_idx": 31}, {"type": "text", "text": "In pRotatE with KG-FIT, the entity phases $\\theta_{h}$ and $\\theta_{t}$ are computed using both the name and description embeddings, while the relation phase $\\theta_{r}$ is learned through the relation embeddings $\\mathbf{r}_{1}$ and $\\mathbf{r}_{2}$ . ", "page_idx": 31}, {"type": "text", "text": "The model aims to minimize the phase difference $\\begin{array}{r}{\\left\\|\\sin\\left(\\frac{\\theta_{h}+\\theta_{r}-\\theta_{t}}{2}\\right)\\right\\|_{1}}\\end{array}$ for valid triples, considering both the entity name and description information. This allows pRotatE to capture the complex interactions between entities and relations in the knowledge graph. ", "page_idx": 31}, {"type": "text", "text": "HAKE: In the HAKE (Hierarchy-Aware Knowledge Graph Embedding) model, entities are embedded into polar coordinate space to capture hierarchical structures. The score function is a combination of radial and angular distances: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{r}(\\mathbf{h},\\mathbf{t})=-\\alpha\\lVert\\mathbf{h}_{\\mathrm{mod}}\\circ\\mathbf{r}_{\\mathrm{mod}}-\\mathbf{t}_{\\mathrm{mod}}\\rVert_{2}-\\beta\\left\\lVert\\sin\\left(\\frac{\\mathbf{h}_{\\mathrm{phase}}+\\mathbf{r}_{\\mathrm{phase}}-\\mathbf{t}_{\\mathrm{phase}}}{2}\\right)\\right\\rVert_{1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In KG-FIT, $\\mathbf{h}_{\\mathrm{mod}}=\\mathbf{h}_{d}$ , $\\mathbf{t}_{\\mathrm{mod}}=\\mathbf{t}_{d}$ , ${\\bf h}_{\\mathrm{phase}}\\,=\\,\\arg({\\bf h}_{n})$ , and $\\mathbf{t}_{\\mathrm{phase}}=\\arg(\\mathbf{t}_{n})$ . The learned relation embedding is $\\mathbf{r}=[\\mathbf{r}_{n};\\mathbf{r}_{d}]$ . ", "page_idx": 31}, {"type": "text", "text": "Thus, the HAKE score function using KG-FIT embeddings becomes: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{r}(\\mathbf{h},\\mathbf{t})=-\\alpha\\lVert\\mathbf{h}_{d}\\circ\\mathbf{r}_{d}-\\mathbf{t}_{d}\\rVert_{2}-\\beta\\left\\lVert\\sin\\left(\\frac{\\arg(\\mathbf{h}_{n})+\\arg(\\mathbf{r}_{n})-\\arg(\\mathbf{t}_{n})}{2}\\right)\\right\\rVert_{1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Interpretation: ", "page_idx": 31}, {"type": "text", "text": "In HAKE with KG-FIT, the entity description embeddings are used to determine the modulus, while the entity name embeddings are used to determine the phase. This approach seamlessly utilizes the information from both types of embeddings from pre-trained language models. ", "page_idx": 31}, {"type": "text", "text": "L Notation Table ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Table 14 provides a comprehensive list of the notations used throughout this paper, along with their corresponding descriptions. This table serves as a quick reference to help readers better understand the concepts presented in our work. ", "page_idx": 32}, {"type": "table", "img_path": "rDoPMODpki/tmp/e622171a97f643dc33abe525e990a71f7d1c76b8d6fa8fa95d4c76c8432defcc.jpg", "table_caption": ["Table 14: Notations and Descriptions in KG-FIT "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Section 4. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Appendix A. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: the paper does not include theoretical results Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Please refer to Section 4 and Appendix I to reproduce our results. We also release all the code and data. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We release our code and data in the uploaded zipped supplementary materials. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Section 4 and Appendix I Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Section 4. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: In Appendix H. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. The research conducted in this paper conform with it. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: In Appendix A. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: In Appendix A. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Section 4 and Appendix B. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: In this paper, we provide a subset of PrimeKG, which is described in Appendix B, and we do not release new assets. We shared an anonymized URL to our code and data. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]