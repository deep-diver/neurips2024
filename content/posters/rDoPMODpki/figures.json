[{"figure_path": "rDoPMODpki/figures/figures_0_1.jpg", "caption": "Figure 1: \"Fine-tune LLM with KG\" vs \"Fine-tune KG with LLM\"", "description": "This figure compares two different approaches for integrating knowledge graphs (KGs) with large language models (LLMs): (1) Fine-tuning an LLM with KG data, typically using classification data based on KG facts; and (2) Fine-tuning a KG with LLM-derived knowledge. The chart visually demonstrates the superior performance of the second approach (KG-FIT) on the FB15K-237 and PrimeKG benchmark datasets, measured by average Hits@10 in link prediction tasks.  The figure highlights the effectiveness of leveraging LLMs to enrich KG embeddings and improve downstream task performance.", "section": "1 Introduction"}, {"figure_path": "rDoPMODpki/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of KG-FIT. Input and Output are highlighted at each step. Step 1: Obtain text embeddings for all entities in the KG, achieved by merging word embeddings with description embeddings retrieved from LLMs. Step 2: Hierarchical clustering is applied iteratively to all entity embeddings over various distance thresholds, monitored by a Silhouette scorer to identify optimal clusters, thus constructing a seed hierarchy where each leaf node represents a cluster of semantically similar entities. Step 3: Leveraging LLM guidance, the seed hierarchy is iteratively refined bottom-up through a series of suggested actions, aiming for a more accurate organization of KG entities with LLM's knowledge. Step 4: Use the refined hierarchy along with KG triples and the initial entity embeddings to fine-tune the embeddings under a series of distance constraints.", "description": "This figure presents a detailed overview of the KG-FIT framework, a novel method for enhancing knowledge graph (KG) embeddings by incorporating open-world knowledge from large language models (LLMs). It illustrates the four main steps involved: entity embedding initialization, seed hierarchy construction, LLM-guided hierarchy refinement, and global knowledge-guided local KG fine-tuning. Each step shows the input, process, and output, clarifying how KG-FIT leverages LLMs to build a semantically coherent entity hierarchy and integrate it into the KG embedding process. The figure highlights the iterative nature of the framework, particularly in the hierarchy construction and refinement stages, and emphasizes how this iterative approach ensures the accuracy and effectiveness of the final KG embeddings.", "section": "3 KG-FIT Framework"}, {"figure_path": "rDoPMODpki/figures/figures_7_1.jpg", "caption": "Figure 1: \"Fine-tune LLM with KG\" vs \"Fine-tune KG with LLM\".", "description": "This figure compares two different approaches to integrating knowledge graphs (KGs) with large language models (LLMs).  The left side shows the traditional approach of first fine-tuning an LLM with KG data (using classification data), and then applying the fine-tuned LLM to KG tasks like link prediction.  The right side depicts KG-FIT's novel approach, which fine-tunes the KG embeddings using information from an LLM to improve their expressiveness and informativeness.  The bar charts illustrate the comparative performance (average Hits@10 on FB15K-237 and PrimeKG datasets) of the PLM-based approach, the KGE-based approach, and the KG-FIT approach, highlighting the improved performance achieved by KG-FIT.", "section": "1 Introduction"}, {"figure_path": "rDoPMODpki/figures/figures_8_1.jpg", "caption": "Figure 2: Overview of KG-FIT. Input and Output are highlighted at each step. Step 1: Obtain text embeddings for all entities in the KG, achieved by merging word embeddings with description embeddings retrieved from LLMs. Step 2: Hierarchical clustering is applied iteratively to all entity embeddings over various distance thresholds, monitored by a Silhouette scorer to identify optimal clusters, thus constructing a seed hierarchy where each leaf node represents a cluster of semantically similar entities. Step 3: Leveraging LLM guidance, the seed hierarchy is iteratively refined bottom-up through a series of suggested actions, aiming for a more accurate organization of KG entities with LLM's knowledge. Step 4: Use the refined hierarchy along with KG triples and the initial entity embeddings to fine-tune the embeddings under a series of distance constraints.", "description": "This figure provides a visual overview of the KG-FIT framework, highlighting the input and output of each step in the process.  It begins with entity embedding initialization using LLMs, followed by seed hierarchy construction via agglomerative clustering.  The hierarchy is then refined using LLM guidance, leading to a final fine-tuning step that integrates this refined hierarchical structure with the knowledge graph for enhanced KG embeddings.", "section": "3 KG-FIT Framework"}, {"figure_path": "rDoPMODpki/figures/figures_8_2.jpg", "caption": "Figure 1: \"Fine-tune LLM with KG\" vs \"Fine-tune KG with LLM\".", "description": "This figure compares two approaches to integrating knowledge graphs (KGs) with large language models (LLMs). The left side shows the traditional approach of fine-tuning an LLM with KG data (classification data). The right side presents the KG-FIT approach, which fine-tunes a KG with LLM-derived knowledge (entity descriptions and entity hierarchy).  The bar chart visually represents the average Hits@10 score on FB15K-237 and PrimeKG datasets, demonstrating that KG-FIT significantly outperforms the traditional approach in link prediction.", "section": "1 Introduction"}, {"figure_path": "rDoPMODpki/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization of Entity Embedding (left to right: initial text embedding, HAKE embedding, and KG-FITHAKE's embedding). Upper (local): Embeddings (dim=2048) of <Maraviroc, drug_effect, CAA (Coronary artery atherosclerosis)> and <Cladribine, drug_effect, Exertional dyspnea>, two parent-child triples selected from PrimeKG, in polar coordinate system. In the polar coordinate system, the normalized entity embedding \u0113 is split to e\u2081 = \u0113[:n/2] and e\u2082 = \u0113[n/2 + 1:] where n is the hidden dimension, which serves as values on the x-axis and y-axis, respectively, which is consistent with Zhang et al. [20]'s visualization strategy. Lower (global): t-SNE plots of different embeddings of sampled entities, with colors indicating clusters (e.g., Maraviroc belongs to the HIV Drugs cluster). Triangles indicate the positions of Maraviroc, CAA, Cladribine, and Exertional dyspnea. Observations: While the initial text embeddings capture global semantics, they fail to delineate local parent-child relationships within the KG, as seen in the intermingled polar plots. In contrast, HAKE shows more distinct grouping by modulus on the polar plots, capturing hierarchical local semantics, but fails to adequately capture global semantics. Our KG-FIT, notably, incorporates prior information from LLMs and is fine-tuned on the KG, maintains global semantics from pre-trained text embeddings while better capturing local KG semantics, demonstrating its superior representational power across local and global scales.", "description": "This figure visualizes the entity embeddings generated by three different methods: initial text embedding, HAKE embedding, and KG-FIT with HAKE embedding. It shows both local and global comparisons, highlighting how KG-FIT enhances the representation by combining both global semantics from LLMs and local semantics from the KG.  The upper section uses polar plots to compare embeddings of specific entities related in parent-child triples in PrimeKG, visualizing local semantic relations. The lower section presents t-SNE plots for global comparison. KG-FIT demonstrates better capturing of hierarchical local semantics within the KG, without losing global semantic understanding from pretrained text embeddings.", "section": "Visualization"}, {"figure_path": "rDoPMODpki/figures/figures_18_1.jpg", "caption": "Figure 1: \"Fine-tune LLM with KG\" vs \"Fine-tune KG with LLM\"", "description": "This figure compares two different approaches to integrating knowledge graphs (KGs) with large language models (LLMs). The left side (\"Fine-tune LLM with KG\") shows a traditional approach where the LLM is first fine-tuned with KG data (e.g., classification data), and then used for KG-related tasks.  The right side (\"Fine-tune KG with LLM\") depicts the KG-FIT approach, where the LLM provides knowledge to refine the KG embeddings, resulting in improved performance.  The bar charts illustrate the difference in average Hits@10 performance on FB15K-237 and PrimeKG datasets, demonstrating the superiority of KG-FIT.", "section": "1 Introduction"}, {"figure_path": "rDoPMODpki/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of KG-FIT. Input and Output are highlighted at each step. Step 1: Obtain text embeddings for all entities in the KG, achieved by merging word embeddings with description embeddings retrieved from LLMs. Step 2: Hierarchical clustering is applied iteratively to all entity embeddings over various distance thresholds, monitored by a Silhouette scorer to identify optimal clusters, thus constructing a seed hierarchy where each leaf node represents a cluster of semantically similar entities. Step 3: Leveraging LLM guidance, the seed hierarchy is iteratively refined bottom-up through a series of suggested actions, aiming for a more accurate organization of KG entities with LLM's knowledge. Step 4: Use the refined hierarchy along with KG triples and the initial entity embeddings to fine-tune the embeddings under a series of distance constraints.", "description": "This figure illustrates the KG-FIT framework's four steps.  It starts by initializing entity embeddings using LLMs (Step 1), then constructs a seed hierarchy via agglomerative clustering (Step 2). This hierarchy is refined using LLM suggestions (Step 3) before a final fine-tuning step integrates the refined hierarchy and textual information with KG triples to enhance the KG embeddings (Step 4).", "section": "3 KG-FIT Framework"}, {"figure_path": "rDoPMODpki/figures/figures_20_1.jpg", "caption": "Figure 2: Overview of KG-FIT. Input and Output are highlighted at each step. Step 1: Obtain text embeddings for all entities in the KG, achieved by merging word embeddings with description embeddings retrieved from LLMs. Step 2: Hierarchical clustering is applied iteratively to all entity embeddings over various distance thresholds, monitored by a Silhouette scorer to identify optimal clusters, thus constructing a seed hierarchy where each leaf node represents a cluster of semantically similar entities. Step 3: Leveraging LLM guidance, the seed hierarchy is iteratively refined bottom-up through a series of suggested actions, aiming for a more accurate organization of KG entities with LLM's knowledge. Step 4: Use the refined hierarchy along with KG triples and the initial entity embeddings to fine-tune the embeddings under a series of distance constraints.", "description": "This figure provides a visual overview of the KG-FIT framework, illustrating its four main steps: (1) Entity Embedding Initialization, which combines LLM-generated descriptions with existing embeddings; (2) Seed Hierarchy Construction, which uses agglomerative clustering to create an initial hierarchy; (3) LLM-Guided Hierarchy Refinement, which refines the hierarchy using LLM suggestions; and (4) Global Knowledge-Guided Local KG Fine-Tuning, which integrates hierarchical and textual information to fine-tune KG embeddings.", "section": "3 KG-FIT Framework"}, {"figure_path": "rDoPMODpki/figures/figures_28_1.jpg", "caption": "Figure 1: \"Fine-tune LLM with KG\" vs \"Fine-tune KG with LLM\"", "description": "This figure compares two different approaches for integrating knowledge graphs (KGs) with large language models (LLMs).  The left side shows the traditional approach of fine-tuning an LLM with KG data, typically using classification tasks on KG triples. The right side depicts the KG-FIT approach, which fine-tunes KG embeddings using knowledge from the LLM. The bar charts visually represent the average Hits@10 scores on FB15K-237 and PrimeKG datasets, illustrating the performance difference between the two approaches.  The KG-FIT approach shows significantly better performance in link prediction compared to the traditional method.  This highlights KG-FIT's ability to leverage the knowledge within an LLM and to better integrate this with KG data.", "section": "1 Introduction"}]