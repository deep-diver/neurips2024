[{"figure_path": "f4v7cmm5sC/figures/figures_1_1.jpg", "caption": "Figure 1: Processes of very different nature (seem to) feature similar jump processes. Left: State values (blue circles) recorded from the discrete flashing ratchet process (black line). Right: Current signal (blue line) recorded from the viral potassium channel KcvMT35, together with one possible coarse-grained representation (black line).", "description": "This figure shows two examples of dynamic processes that can be modeled using Markov jump processes (MJPs). The left panel shows a discrete flashing ratchet process, a simple model of a Brownian motor. The right panel shows current recordings from a viral potassium channel. The key takeaway is that even though these systems are very different, after a coarse graining step, their dynamics can be described by similar MJPs. This is a motivation for the authors' work to infer MJPs from various kinds of data.", "section": "1 Introduction"}, {"figure_path": "f4v7cmm5sC/figures/figures_2_1.jpg", "caption": "Figure 2: Foundation Inference Model (FIM) for MJP. Left: Graphical model of the FIM (synthetic) data generation mechanism. Filled (empty) circles represent observed (unobserved) random variables. The light-blue rectangle represents the continuous-time MJP trajectory, which is observed discretely in time. See main text for details regarding notation. Right: Inference model. The network 41 is called K times to process K different time series. Their outputs is first processed by the attention network \u03a9\u2081 and then by the FNNs $1, $2 and 3 to obtain the estimates F, log Var F and 10, respectively.", "description": "This figure illustrates the Foundation Inference Model (FIM) for Markov Jump Processes (MJPs). The left panel shows the graphical model for synthetic data generation, which involves generating MJP trajectories, observation times, and noise, resulting in a dataset of observed MJPs.  The right panel depicts the inference model architecture, where an attention network processes K different time series to produce a global representation. This representation is then passed through feed-forward networks to estimate the intensity rate matrix (F), variance of F, and initial distribution (\u03c00) of the hidden MJP.", "section": "3 Foundation Inference Models"}, {"figure_path": "f4v7cmm5sC/figures/figures_6_1.jpg", "caption": "Figure 3: Illustration of the six-state discrete flashing ratchet model. The potential V is switched on and off at rate r. The transition rates for, foff allow the particle to propagate through the ring.", "description": "This figure illustrates the six-state discrete flashing ratchet model.  The model consists of a ring of six states representing different potential energy levels for a particle.  The potential is periodically switched on and off at rate *r*.  When the potential is on, the particle transitions between states 0, 1, and 2 with rates *f<sub>ij</sub><sup>on</sup>*. When the potential is off, the particle transitions between states 3, 4, and 5 with rates *f<sub>ij</sub><sup>off</sup>*. The transitions between the 'on' and 'off' states (0-3, 1-4, 2-5) occur at rate *r*. The potential difference between adjacent states is *V* or *2V*, depending on the state.", "section": "4.1 The Discrete Flashing Ratchet (DFR): A Proof of Concept"}, {"figure_path": "f4v7cmm5sC/figures/figures_7_1.jpg", "caption": "Figure 4: Zero-shot inference of DFR process. Left: master eq. solution PMJP(x, t) as time evolves, wrt. the (averaged) FIM-inferred rate matrix is shown in black. The ground-truth solution is shown in blue. Right: Total entropy production computed from FIM (over a time-horizon T = 2.5 [a.u.]). The model works remarkably well for a continuous range of potential values.", "description": "The left panel shows the time evolution of the probability distribution over the six states of the discrete flashing ratchet process. The black lines represent the prediction by the Foundation Inference Model (FIM), while the blue lines represent the ground truth. The right panel shows the total entropy production computed from the FIM's prediction as a function of the potential value (V).  Both plots demonstrate that FIM accurately infers the dynamics of the DFR process across a range of potential values.", "section": "4.1 The Discrete Flashing Ratchet (DFR): A Proof of Concept"}, {"figure_path": "f4v7cmm5sC/figures/figures_15_1.jpg", "caption": "Figure 5: Distributions of the number of jumps per trajectory. We used the same distributions as the training set and sampled up to time 10. The figures are based on 1000 processes with 300 paths per process.", "description": "This figure shows the distribution of the number of jumps observed in 1000 simulated Markov jump processes (MJPs), each with 300 paths, up to time 10.  The distributions are displayed for different state space dimensions (2D to 6D). The distributions are similar to those used in the training set, demonstrating the effectiveness of the data generation method in creating a representative dataset.", "section": "Synthetic Data Generation Model"}, {"figure_path": "f4v7cmm5sC/figures/figures_16_1.jpg", "caption": "Figure 6: Distributions of the relaxation times. We also report the percentage of processes that converge into an oscillating distribution (OP) and the percentage of processes that have a relaxation time which is larger than the maximum sampling time (NCP) of our training data (given by tend = 10). The figures are based on 1000 processes.", "description": "The figure shows the distributions of relaxation times for Markov jump processes with state spaces of different dimensions.  The red dashed line indicates the maximum sampling time used during training. The percentages of processes that converge to oscillating distributions (OP) and those exceeding the maximum sampling time (NCP) are also provided for each dimensionality. The distributions illustrate the range of relaxation times observed in the simulated data.", "section": "3.1 Synthetic Data Generation Model"}, {"figure_path": "f4v7cmm5sC/figures/figures_20_1.jpg", "caption": "Figure 7: Impact of Hyperparameters on RMSE. The figure shows four line plots illustrating the effect of hyperparameters on model RMSE. The first plot shows RMSE increases with larger 41 hidden sizes, being lowest at 256. The second plot indicates lower RMSE with a larger 41 architecture size ([2x128]). The third plot shows minimal RMSE impact from 42 architecture size. The fourth plot shows RMSE stability across different \u03a9\u2081 hidden sizes, with slight variations based on 41. This highlights the importance of tuning 41 and 41 for optimal performance.", "description": "This figure presents an ablation study evaluating the impact of different hyperparameter settings on the RMSE of the model.  It shows how changes in the hidden size of the path encoder (\u03c8\u2081), the architecture size of \u03c8\u2081, and \u03c8\u2082, and the hidden size of the attention network (\u03a9\u2081) affect the model's performance. The results suggest that the path encoder and its first feed-forward layer (\u03c6\u2081) are particularly sensitive to hyperparameter changes, while the impact of the attention network is less pronounced.", "section": "E Ablation Studies"}, {"figure_path": "f4v7cmm5sC/figures/figures_22_1.jpg", "caption": "Figure 8: Time-Average Hellinger distance for varying potentials on the DFR. The plot shows the Hellinger distance to a target dataset that was sampled from a DFR with V = 1 on a grid of 50 points between 0 and 2.5. The means and standard deviations were computed by sampling 100 histograms per dataset. As expected, the distance decreases as the voltage gets closer to the voltage of the target dataset. We also remark that the scale of the distances gets smaller as one takes more paths into account and converge to the distance of the solutions of the master equation.", "description": "This figure shows the average Hellinger distance between the model's predictions and the ground truth for different values of the potential V. The average Hellinger distance is computed using 100 histograms for each potential value.  As expected, the distance decreases as the potential V approaches the target value of 1. The error bars represent the standard deviation of the Hellinger distances.", "section": "F.1 Hellinger Distance"}, {"figure_path": "f4v7cmm5sC/figures/figures_23_1.jpg", "caption": "Figure 9: Comparison of the classifications between KMeans (left) and NeuralMJP (right).", "description": "This figure compares the clustering results of the Alanine Dipeptide dataset using two different methods: KMeans and NeuralMJP.  It visually demonstrates how each method groups the data points into different clusters (representing different conformational states). The figure is crucial in the context of the paper because it shows how the choice of coarse-graining method (KMeans vs NeuralMJP) can influence the subsequent analysis and inference of Markov jump processes (MJPs). The differences in clustering observed in Figure 9 lead to differences in the learned MJP models, highlighting the impact of the preprocessing step on downstream inference results.", "section": "F.2 Alanine Dipeptide"}, {"figure_path": "f4v7cmm5sC/figures/figures_24_1.jpg", "caption": "Figure 1: Processes of very different nature (seem to) feature similar jump processes. Left: State values (blue circles) recorded from the discrete flashing ratchet process (black line). Right: Current signal (blue line) recorded from the viral potassium channel KcvMT35, together with one possible coarse-grained representation (black line).", "description": "This figure shows two examples of time series data exhibiting jump processes. The left panel shows data from a discrete flashing ratchet process, illustrating the discrete jumps between states. The right panel shows a current signal from a viral potassium channel, also demonstrating jumps between different levels of activity. This figure highlights that seemingly different systems, after coarse-graining, can exhibit similar jump-process dynamics.", "section": "1 Introduction"}, {"figure_path": "f4v7cmm5sC/figures/figures_27_1.jpg", "caption": "Figure 11: Classification of the protein folding dataset into a Low and a High state. The GMM-Classifier has learned a decision boundary close to the radius 2.", "description": "This figure shows the classification of a protein folding dataset into two states, Low and High, using a Gaussian Mixture Model (GMM). The GMM classifier learns a decision boundary close to a radius of 2. The plot likely displays the radius values on the y-axis and time or simulation steps on the x-axis. Each point represents a data point from the dataset, with different colors (or shapes) possibly indicating the Low and High states. This visualization helps understand how well the GMM classifier separates the two states based on the radius feature.", "section": "F.5 Modeling Protein Folding through Bistable Dynamics"}]