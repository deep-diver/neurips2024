[{"type": "text", "text": "What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Given the remarkable capabilities of large language models (LLMs), there has   \n2 been a growing interest in evaluating their similarity to the human brain. One   \n3 approach towards quantifying this similarity is by measuring how well a model   \n4 predicts neural signals, also called \"brain score\". Internal representations from   \n5 LLMs achieve state-of-the-art brain scores, leading to speculation that they share   \n6 computational principles with human language processing. This inference is only   \n7 valid if the subset of neural activity predicted by LLMs reflects core elements   \n8 of language processing. Here, we question this assumption by analyzing three   \n9 neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward. Second, we explain the surprising result that untrained LLMs have higher-than-expected brain scores by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the   \n25 importance of deconstructing what LLMs are mapping to in neural signals. ", "page_idx": 0}, {"type": "text", "text": "10   \n11   \n12   \n13   \n14   \n15   \n16   \n17   \n18   \n19   \n20   \n21   \n22   \n23   \n24 ", "page_idx": 0}, {"type": "text", "text": "26 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "27 Recent developments in large language models (LLMs) have led many to wonder whether LLMs   \n28 process language like humans do. Whereas LLMs acquire many abstract linguistic generalizations, it   \n29 remains unclear to what extent their internal machinery bears resemblance to the human brain [1]. A   \n30 number of studies have attempted to answer this question through the framework of neural encoding   \n31 [2\u20134]. Within this framework, an LLM\u2019s internal representations of some linguistic stimuli are used   \n32 to predict brain activity during comprehension of the same stimuli. Results have been uniformly   \n33 positive, showing that LLM representations are highly effective at predicting neural signals [5, 6].   \n34 In one impactful study, authors evaluated the brain scores of 43 models on three neural datasets [2].   \n35 They found that GPT2-XL [7] achieved the highest brain score and, in one neural dataset, accounted   \n36 for $100\\%$ of the \"explainable\" neural variance (i.e., taking into account the noise inherent in the data)   \n37 [8]. This result was interpreted as evidence that the brain may be optimizing for the same objective   \n38 as GPT2, namely, next-word prediction. Surprisingly, the authors further found that untrained (i.e.   \n39 randomly initialized) LLMs predict neural activity well, leading to speculations that the transformer   \n40 architecture biases computations to be more brain-like. The finding that untrained LLMs predict   \n41 neural signals significantly above chance has been replicated in other studies [9, 4, 10].   \n42 More generally, many studies have compared models to brain activity and concluded that high   \n43 prediction performance reveals correspondence between some interesting aspect of the model and   \n44 biological linguistic processing [4, 11\u201314]. One issue with this approach is that it assumes that the   \n45 subset of neural activity predicted by a model reflects core processes of the human language system   \n46 [15]. However, this assumption is not necessarily true. For example, a recent paper found that, when   \n47 participants listen to stories, the fMRI signal includes an initial ramping, positional artifact [16].   \n48 It is likely that LLMs which contain absolute positional embeddings would be able to predict this   \n49 ramping signal, whereas a simpler model such as a static word embedding (e.g. GloVe, [17]) would   \n50 not, leading to exaggerated differences between LLMs and GloVe due to reasons of little theoretical   \n51 interest. This issue relates to a more general trend in machine learning research: a complex algorithm   \n52 solves a task, but it is later discovered that the key innovation was a very simple component of the   \n53 algorithm [18]. Analogous to Weinberger [18], without attempting to rigorously deconstruct the   \n54 mapping between LLMs and brains, it is possible to draw erroneous conclusions about the brain\u2019s   \n55 mechanisms for processing language.   \n56 We analyze the same three neural datasets used in [2]. These include the Pereira fMRI dataset, where   \n57 participants read short passages [8]; the Fedorenko electrocorticography (ECoG) dataset, where   \n58 participants read isolated sentences [19]; and the Blank fMRI dataset, where participants listened to   \n59 short stories [20]. As in Schrimpf et al. [2], we focus our analyses on the Pereira dataset. In order to   \n60 deconstruct the mapping between LLMs and the brain, we follow Reddy and Wehbe [21] and de Heer   \n61 et al. [22] by building a set of predictors that describe simple features of the linguistic input, and   \n62 gradually add features that increase in complexity. Our goal is to find the simplest set of features   \n63 which account for the greatest portion of the mapping between LLMs and brains. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "64 2 Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "65 2.1 Experimental data ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "66 For all three neural datasets, we used the same version as used by [2]. For additional details, refer to   \n67 A.1.   \n68 Pereira (fMRI): The Pereira dataset is composed of two experiments. Experiment 1 (EXP1) consists   \n69 of 96 passages each containing 4 sentences, with $n=9$ participants. Experiment 2 (EXP2) consists of   \n70 72 passages each consisting of 3 or 4 sentences, with $n=6$ participants. Passages in each experiment   \n71 were evenly divided into 24 semantic categories which were not related across experiments (4   \n72 passages per category in EXP1, and 3 passages per category in EXP2). A single fMRI scan (TR)   \n73 was taken after visual presentation of each sentence. Unless otherwise noted, we focus our results   \n74 on voxels from within the \"language network\" in the main paper. EXP1 was a $384\\times92450$ matrix   \n75 (number of sentences $\\times$ number of voxels) and EXP2 was a $243\\times60100$ matrix. All analyses were   \n76 conducted separately for each experiment.   \n77 Fedorenko (ECoG): Participants $\\mathrm{~\\textit~{~n~}~}=5$ ) read 52 sentences of length 8 words. A total of 97   \n78 language-responsive electrodes were used across 5 participants: 47, 8, 9, 15, and 18, for participants   \n79 1 through 5, respectively. Neural activity was temporally averaged across the full presentation of each   \n80 word after extracting high gamma, and the entire dataset was a $416\\times97$ matrix.   \n81 Blank (fMRI): The dataset consisted of 5 participants listening to 8 stories from the publicly   \n82 available Natural Stories Corpus [23]. An fMRI scan was taken every 2 seconds, resulting in a total   \n83 of 1317 TRs across the 8 stories. fMRI BOLD signals were averaged across voxels within each   \n84 functional region of interest (fROI). There were 60 fROIs across all 5 participants, resulting in a   \n85 $1317\\times60$ matrix.   \n87 We focus our analyses on GPT2-XL [7], as it was shown to be the best-performing model on the   \n88 Pereira dataset [10, 24, 2]. GPT2 is an auto-regressive transformer model, meaning that it can   \n89 only attend to current and past inputs, trained on next token prediction. The XL variant has ${\\sim}1.5\\mathrm{B}$   \n90 parameters and 48 layers. We replicate some of our key findings on Pereira with RoBERTa-Large[25]   \n91 (A.6). RoBERTa is a transformer model with bidirectional attention trained on masked token   \n92 prediction, meaning that it can attend to past and future tokens. The large variant contains $335\\mathrm{M}$   \n93 parameters and 24 layers. Both GPT2 and RoBERTa use learned absolute positional embeddings,   \n94 such that a unique vector corresponding to each token position is added to the input static embeddings. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "95 2.3 LLM feature pooling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "96 Pereira: Each sentence was fed into an LLM, with previous sentences from the same passage also fed   \n97 as input. Since each fMRI scan was taken at the end of the sentence, we converted LLM token-level   \n98 embeddings to sentence-level embeddings by summing across all tokens within a sentence (sum   \n99 pooling). We used the sum pooling method because it is consistent with other neural encoding studies   \n100 [26, 27], and it performed better than taking the representation at the last token which was done in   \n101 [2] A.5.   \n102 Fedorenko: The current and previous tokens from within the same sentence were fed into the LLM   \n103 as context. We converted LLM token-level embeddings to word embeddings, since each word has a   \n104 neural response, by summing across tokens in multi-token words, and leaving single token words   \n105 unmodified.   \n106 Blank: For each story, we fed the current and all preceding tokens up to a maximum context size of   \n107 512 tokens. As in Schrimpf et al. [2], for each TR, we took the representation of the word that was   \n108 closest to being 4 seconds before the TR. For multi-token words, we took the representation of the   \n109 last token of that word. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "110 2.4 Banded ridge regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 We used ridge regression (linear regression with an L2 penalty) to predict activations for each   \n112 voxel/electrode/fROI independently. We did not use \"vanilla\" ridge regression because it applies a   \n113 single L2 penalty for all weights, whereas our analyses use multiple sets of distinct features. In such   \n114 a case, a single penalty causes the regression will be biased against small feature spaces. Moreover,   \n115 different L2 penalties are likely optimal for each feature space. To remedy this, we employed banded   \n116 ridge regression which effectively allows a different L2 penalty to be applied to each feature space   \n117 [28] (for further details, refer to A.2). ", "page_idx": 2}, {"type": "text", "text": "118 2.5 Out of sample $R^{2}$ metric ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "119 We define the brain score of a model as the out-of-sample $R^{2}$ metric $(R_{o o s}^{2})$ [29]. $R_{o o s}^{2}$ quantifies   \n120 how much better a set of features performs at predicting held-out data compared to a model which   \n121 simply predicts the mean of the training data (i.e. a regression with only an intercept term). To be   \n122 precise, given mean squared error (MSE) values from a model using features $M$ and MSE values   \n123 from an intercept only regression $(I)$ , then: ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{o o s}^{2}=1-\\frac{M S E_{M}}{M S E_{I}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 A positive (negative) value indicates that $M$ was more (less) helpful than predicting the mean of   \n125 126 training data. We elected to use and because it is a less biased estimate of test set performance [29]. We use $R_{o o s}^{2}$ over the standard $R^{2}$ because of this clear interpretation $R_{o o s}^{2}$ over Pearson\u2019s   \n127 correlation coefficient $(r)$ because $R_{o o s}^{2}$ can be interpreted as the fraction of variance explained,   \n128 which lends more straightforwardly to estimating how much variance one feature space explains   \n129 over others. Whenever averaging across voxels, we set $R_{o o s}^{2}$ values to be non-negative to prevent   \n130 differences in performance on noisy voxels/electrodes/fROIs from significantly impacting the results.   \n131 We refer to $R_{o o s}^{\\bar{2}}$ as $R^{2}$ throughout the rest of the paper for brevity, and use the notation $\\bar{R}_{M}^{2}$ to refer   \n132 to the performance of features $M$ . ", "page_idx": 2}, {"type": "text", "text": "133 2.6 Selection of best layer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "134 We evaluate the $R^{2}$ for each LLM layer, and select the layer that performs best across vox  \n135 els/electrodes/fROIs. Due to the stochastic nature of untrained LLMs, we selected the best layer for   \n136 10 random seeds and computed the average $R^{2}$ across seeds. When reporting the best layer, we refer   \n137 to layer 0 as the input static layer, and layer 1 as the first intermediate layer. ", "page_idx": 3}, {"type": "text", "text": "138 2.7 Train, validation, and test folds: ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "139 For each dataset, we construct contiguous train-test splits by ensuring neural data from the same   \n140 passage/sentence/story is not included in both train and test data. Due to low sample sizes, we   \n141 employed a nested cross-validation procedure for each dataset (A.3). When computing $R^{2}$ across   \n142 inner or outer folds, we pooled predictions across folds and computed a single $R^{2}$ as recommended by   \n143 Hawinkel et al. [29]. The optimal parameters for banded regression were selected based on validation   \n144 data.   \n145 We created shuffled train-test splits, as done in [2], of the same size as the contiguous train-test splits.   \n146 Unless explicitly noted, all results are performed using contiguous train-test splits. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "147 2.8 Correcting for decreases in test-set performance due to addition of feature spaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "148 It is possible for a \"full\" encoding model to perform worse than a \"sub-model\" (which consists   \n149 of only a subset of the predictors) because we are evaluating performance on a held-out test set   \n150 [22]. To address this problem, in some analyses we select the best performing sub-model for each   \n151 voxel/electrode/fROI which includes a given feature of interest. For instance, to examine how much   \n152 feature space $C$ adds onto features spaces $A$ and $B$ , we select the best sub-model which includes $C$   \n153 and denote it as $A+B+C^{*}$ . More precisely, the $R^{2}$ of $A+B+C^{*}$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{A+B+C^{*}}^{2}=\\operatorname*{max}(R_{C}^{2},R_{A+C}^{2},R_{B+C}^{2},R_{A+B+C}^{2}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "154 2.9 Orthogonal Auto-correlated Sequences Model (OASM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "155 To model temporal auto-correlation in neural activity, we construct a feature matrix for each dataset   \n156 by (i) forming an $n$ -dimensional identity matrix, where $n$ is the total number of time points in the   \n157 dataset (per voxel / electrode / TR), and (ii) applying a Gaussian filter within \"chunks\" along the   \n158 diagonal that correspond to temporally contiguous time points (i.e., within each passage in Pereira,   \n159 each sentence in Fedorenko, and each story in Blank). This generates an auto-correlated sequence for   \n160 each passage/sentence/story that is orthogonal to that of each other passage/sentence/story (A.7). ", "page_idx": 3}, {"type": "text", "text": "161 3 Pereira dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "162 3.1 Shuffled train-test splits are severely affected by temporal auto-correlation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "163 Prior LLM encoding studies using this dataset [24, 2, 10, 30, 11] used shuffled train-test splits. Here,   \n164 we demonstrate that this approach compromises the evaluation of the neural predictivity of LLMs.   \n165 First, we replicated the pattern of neural predictivity across GPT2-XL\u2019s layers reported in [2] and [24]   \n166 when using shuffled splits. Using this procedure, early and late layers perform best and intermediate   \n167 layers perform worst. Strikingly, when using the alternative approach of contiguous train-test splits,   \n168 the opposite pattern is observed: intermediate layers perform best. Across layers, neural predictivity   \n169 using the shuffled method is highly anti-correlated with neural predictivity using the contiguous   \n170 method $r=-929$ in EXP1, $r=-.764$ in EXP2) (Fig. 1a).   \n171 Next, we hypothesized that much of what LLMs might be mapping to when using shuffled splits   \n172 could be accounted for by OASM, a model which only represents within passage auto-correlation   \n173 and between passage orthogonality. OASM out-performed GPT2-XL on both EXP1 and EXP2   \n174 (Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve   \n175 absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption   \n176 of multiple previous studies [2, 11, 10] that performance on this benchmark is an indication of a   \n177 model\u2019s brain-likeness, .   \n178 Moreover, we find that the unique neural variance that GPT2-XL explains over OASM is very small   \n179 relative to what OASM explains alone. To calculate this, we combine OASM with GPT2-XL and   \n180 observe how much neural variance they explain together. To prevent OASM from ever weakening   \n181 the reported performance of GPT2-XL for any voxel, we correct the $R^{2}$ value for each voxel with   \n182 the OASM $\\cdot+$ GPT2-XL model to be at least as high as with GPT2-XL alone (denoted OASM+GPT2-   \n183 $\\mathrm{{XL^{*}}}$ ) (2.8). Even with these corrections, we find that $R_{O A S M+G P T2-X L}^{2}\\,^{\\ast}$ was $13.6\\%$ higher than   \n184 $R_{Q A S M}^{2}$ in EXP1, and $31.5\\%$ higher than $R_{O A S M}^{2}$ in EXP2 (Fig. 1b) ( $\\%$ differences after averaging   \n185 $R^{2}$ across participants). To be clear, this means that any linguistically-driven neural variance that   \n186 GPT2-XL uniquely explains over OASM is far smaller $\\left\\langle13.6\\right\\%$ on EXP1 and $31.5\\%$ on EXP2) than   \n187 what is predicted solely by OASM, a model with no linguistic features that completely lacks the   \n188 ability to generalize to fully held out passages. Thus, it appears that the largest determinant of   \n189 model predictivity on this dataset when using shuffled train-test splits is whether a model contains   \n190 autocorrelated sequences within passages that are orthogonal between passages. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/a10fef3b168fb510d447ec0d5e9f7030c92524cc463eb47723ec30bce09377ed.jpg", "img_caption": ["Figure 1: Comparing different approaches for creating train-test splits in the Pereira dataset. Within each panel, EXP1 results are on the left and EXP2 results are on the right (same formatting in Figure 2,3) (a) $R^{2}$ values across layers for GPT2-XL on shuffled train-test splits (gray) and contiguous (unshuffled) splits (blue). (b) Each dot shows the mean $R^{2}$ value across voxels within a participant, with bars indicating mean $R^{2}$ across participants. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "191 3.2 Untrained LLM neural predictivity is fully accounted for by sentence length and position ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "192 We next sought to deconstruct what explains the neural predictivity of untrained GPT2-XL (GPT2-   \n193 XLU) in the Pereira dataset. We hypothesized that $R_{G P T2-X L U}^{2}$ could be explained by two simple   \n194 features: sentence length (SL) and sentence position within the passage (SP). Sentence length is   \n195 captured by GPT2-XLU because the GELU nonlinearity in the first layer\u2019s MLP transforms normally   \n196 distributed inputs with zero mean into outputs with a non-zero mean. This introduces a non-zero   \n197 mean component to each token\u2019s representation in the residual stream. When these representations   \n198 are sum-pooled, this non-zero mean component accumulates in a way that reflects the sentence length,   \n199 making the length decodable in the intermediate layers (see A.9 for a formal proof). Sentence position   \n200 is encoded within GPT2-XLU due to absolute positional embeddings which, although untrained, still   \n201 result in sentences at the same position having similar representations when tokens are sum-pooled.   \n202 We represent sentence position as a 4-dimensional one-hot vector, where each element corresponds   \n203 to a given position within a passage, and sentence length as the number of words in a passage.   \n204 To obtain representations from GPT2-XLU, we selected the best-performing layer for each of the 10   \n205 untrained seeds. For EXP1 the best performing layer was layer 0 for 6 seeds, layer 1 for 3 seeds (first   \n206 intermediate layer), and layer 19 for one seed. For EXP2 the best layer was layer 1 for 5 seeds, layer   \n207 2 for 4 seeds, and layer 5 for 1 seed.   \n208 We fti a regression using all subsets of the following feature spaces, SL, SP, GPT2-XLU, resulting in   \n209 7 models. For both experiments, $R_{S P+S L}^{2}$ was descriptively higher than all other models, including   \n210 the best-performing model with GPT2-XLU $(\\mathrm{SP+SL+GPT2-XLU})$ (Fig. 2a). Sentence position was   \n211 particularly important in EXP1, and sentence length was particularly important in EXP2. This may   \n212 explain why the static layer often outperformed intermediate layer representations in EXP1 despite   \n213 encoding sentence length more poorly. Overall, these results suggest that, when averaging across   \n214 voxels within the language network in this dataset, GPT2-XLU does not improve neural encoding   \n215 performance over sentence length and position.   \n216 Although GPT2-XLU did not enhance encoding performance when averaging across voxels, there   \n217 may be a subset of voxels where GPT2-XLU does explain significant additional neural vari  \n218 ance. To examine this possibility, we plotted a 2D histogram of voxel-wise $R_{S P+S L}^{2}$ values vs.   \n219 $R_{S P+S L+G P T2-X L U}^{2}$ values in the language network (Fig. 2b). Values were clustered around the   \n220 identity line, and there was no cluster of voxels where $R_{S P+S L+G P T2-X L U}^{2}$ appeared significantly   \n221 higher. Next, for each voxel, we performed a one-sided paired $t$ -test between the squared error   \n222 values obtained over sentences (EXP1: $N=384$ , EXP2: $N=243$ ) between $\\mathrm{SP+SL+}$ GPT-XLU   \n223 and $\\mathrm{SP+SL}$ . Across all functional networks, only $1.26\\%$ (EXP1) and $1.42\\%$ (EXP2) of voxels were   \n224 significantly $\\left(\\alpha\\,=\\,0.05\\right)$ better explained by the GPT2-XLU model before false discovery rate   \n225 (FDR) correction; these numbers dropped to $0.001\\%$ (EXP1) and $0.078\\%$ (EXP2) after performing   \n226 FDR correction within each participant and network [31]. None of the significant voxels after FDR   \n227 correction were inside the language network. Taken together, these results suggest GPT2-XLU does   \n228 not enhance neural prediction performance over sentence length and position even at the voxel level.   \n229 To control for voxels where the neural encoding performance of GPT2-XLU is weakened by the   \n230 addition of $\\mathrm{SP+SL}$ , we compared $\\mathrm{SP+SL^{*}}$ and $\\mathrm{SP+SL+GPT}2{\\cdot}\\mathrm{XLU^{*}}$ . When averaging across voxels,   \n231 $R_{S P+S L}^{2}\\,^{\\ast}$ still exceeded $R_{G P T2-X L U+S P+S L}^{2}\\,^{*}$ (Fig. 2c). Furthermore, the values for $R_{S P+S L}^{2}{}^{*}$   \n232 and R2GP T 2\u2212XLU+SP +SL\\* across brain areas were highly similar in both experiments (Fig. 2d).   \n233 Only $1.18\\%$ (EXP2) of voxels were significantly better explained by the addition   \n234 of GPT2-XLU before FDR correction; $0\\%$ (EXP1) and $0.05\\%$ (EXP2) of voxels were better explained   \n235 after FDR correction (once again, no significant voxels were inside the language network ). Thus, our   \n236 results hold even when controlling for decreases in performance due to the addition of feature spaces. ", "page_idx": 4}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/d1eb7ed875d35c7c7a5b87204db3efc908937487bf2a958082ec53b17a6e7409.jpg", "img_caption": ["Figure 2: For all panels, EXP1 results are on the left and EXP2 results are on the right. (a) Brain score $(R^{2})$ for different combinations of features. Each dot represents $R^{2}$ values averaged across voxels in a single participant, with bars showing mean across participants. (b) 2D histogram of $R^{2}$ values for the best model without GPT2-XLU $\\mathrm{{(SP+SL)}}$ , and the best model with GPT2-XLU $(\\mathrm{GPT2-XLU+SP+SL})$ . The dotted lines show $y=x$ , $y=0$ , and $x\\,=\\,0$ . Values below $y=0$ or left of $x=0$ were clipped when averaging, but are shown here to visualize the full distribution. (c) Same as (a), but after voxel-wise correction; lines connect data-points from the same participant. (d) Glass brain plots showing $R^{2}$ values of $\\mathrm{SP+SL}$ (left) and GPT2-XLU $+\\mathrm{SP}+\\mathrm{SL}$ (right) after voxel-wise correction. Conventions are the same as Figure 1. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "u1b1dJtyxc/tmp/8812a4a4449674d704f023c071bc426e98b4508095eec334d6b18205453ad472.jpg", "table_caption": ["Table 1: Mean $R^{2}$ values (across participants) for each model. For models composed of multiple features, the best sub-model is used which includes the last feature. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "237 3.3 Sentence length, sentence position, and static word embeddings account for the majority 238 of trained LLM encoding performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "239 We next turned to explaining the neural predictivity of the trained GPT2-XL. In addition to sentence   \n240 position and sentence length, we added static word embeddings (WORD). Together, these features   \n241 defined a baseline model which does not account for any form of linguistic processing of words   \n242 in context. We next included three more complex features which involved contextual processing.   \n243 First, we added sense-specific word embeddings from RoBERTa-Large using the LMMS package   \n244 [32]. Sense embeddings contain distinct representations for different senses of the same word (e.g.,   \n245 mouse: computer device, and mouse: rodent). LMMS generates sense embeddings by averaging over   \n246 contextual embeddings corresponding to the same sense of a word (see A.10 for further details).   \n247 Whereas sense embeddings help disambiguate many content words, they do not disambiguate   \n248 pronouns, i.e., do not encode the entities that they refer to. Therefore, our sense embeddings were   \n249 generated for a version of the Pereira text where pronouns were dereferenced (i.e., replaced by   \n250 the words that they referred to). To maintain consistency with these sense embeddings, our static   \n251 word embeddings were created (1) by taking a frequency-weighted average of sense embeddings   \n252 for the same word, where frequency values were obtained from WordNet [33]; and (2) based on the   \n253 dereferenced Pereira texts. Importantly, this means the impact of pronoun dereferencing and word   \n254 and sense embeddings are not decoupled in this study. Finally, we created an abstract representation   \n255 of the syntax of each sentence (SYNT), using an approach highly similar to that of Caucheteux   \n256 et al. [34]: we collected sentences that are syntactically equivalent but semantically dissimilar to the   \n257 original sentence, and averaged their representations from the best layer of GPT2-XL (A.11). We   \n258 selected the best layer based on averaged $R^{2}$ across language voxels on test data (EXP1: layer 21,   \n259 EXP2: layer 16).   \n260 We fit a regression to the fMRI data using all subsets of the feature spaces $\\mathbf{SL+SP},$ , WORD, SENSE,   \n261 SYNT, GPT2-XL, resulting in 64 models. In this list, features are ranked from least to most complex.   \n262 For each feature, we took the model that exhibited the best performance in the language network   \n263 which included that feature but did not include features more complex than it. For instance, values   \n264 reported for $R_{S L+S P+W O R D+S E N S E}^{2}$ were taken from the best model which included SENSE,   \n265 excluding models which included SYNT and GPT2-XL. By doing so, we were able to examine   \n266 the impact of adding more complex features in explaining $\\mathrm{R}2_{G P T2-X L}$ while still accounting for   \n267 decreases in test performance due to adding redundant features. We note that since this procedure is   \n268 not performed at the voxel-level, we do not add a $^*$ to the $R^{2}$ notation.   \n269 Table 1 displays the performance of each model, including GPT2-XL on its own (Fig. 2a, 2b). The   \n270 baseline $\\mathrm{SP+SL+WORD}$ model, which does not account for any form of contextual processing,   \n271 performs $75\\%$ as well as GPT2-XL in EXP1, and outperforms GPT2-XL in EXP2. When adding   \n272 contextual features, namely SENSE and SYNT, our model performs $84.4\\%$ as well as GPT2-XL and   \n273 the full model in EXP1, and better than GPT2-XL and $95.5\\%$ as well as the full model in EXP2,   \n274 indicating that SENSE and SYNT play a modest role in accounting for GPT2-XL brain scores beyond   \n275 simple features in this dataset.   \n276 Similar to previous sections, we perform voxel-wise correction by selecting the best sub-model with   \n277 GPT2-XL and the best sub-model without GPT2-XL for each voxel. We focus only on sentence   \n278 position, sentence length, and static word embeddings because sense and syntax had modest con  \n279 tributions beyond these features. $R_{S P+S L+W O R D}^{2}*$ was 0.028 in EXP1 and 0.048 in EXP2, and   \n280 R2SP +SL+W ORD+GP T 2\u2212XL\\* was 0.036 in EXP1 and 0.056 in EXP2 (mean across participants)   \n281 (Fig. 3c). This indicates that even after controlling for a reduction in GPT2-XL performance from   \n282 the addition of simple features, GPT2-XL only explains an additional $28.57\\%$ (EXP1) and $16.7\\%$   \n283 (EXP2) neural variance over a model composed of features that are all non-contextual. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/5a6dbe6f5d7cb3915479cd2b12711fe77743c1c6baa9d902913c7b3debe5fb19.jpg", "img_caption": ["Figure 3: For all panels, EXP1 results are on the left and EXP2 results are on the right. (a) For each model, we display the sub-model which includes the added feature. Dots represent participants and bars are mean across participants. Grey dashed line is the performance of GPT2-XL alone. (b) 2d histogram comparing full model and full model with GPT2-XL. (c) Same as (a) but after voxel-wise correction for $\\mathrm{SP+SL+WORD}$ and $\\mathrm{SP+SL+WORD+GPT2-XL}.$ . (d) Glass brain plots showing $R^{2}$ values of $\\mathrm{SP+SL+WORD}$ (left) and SP+SL+WORD $^+$ GPT2-XLU (right) after voxel-wise correction. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "284 4 Fedorenko dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "285 4.1 Shuffled train-test splits also impact ECoG datasets, but less than with fMRI ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "286 We first evaluated the impact of shuffled train-test splits on the Fedorenko dataset. Unlike in Pereira,   \n287 the across-layer performance is well correlated between shuffled and contiguous splits $(r=0.622)$   \n288 (Fig. 4a). The OASM model performs $93.1\\%$ as well as GPT2-XL when averaging $R^{2}$ values across   \n289 participants (Fig. 4b). $R_{O A S M+G P T2-X L}^{2}{}^{*}$ was $45.3\\%$ better than OASM, meaning that the unique   \n290 contribution of GPT2-XL is less than half the total contribution of a simple, auto-correlated model.   \n291 Therefore, shuffled train-test splits also impact results on Fedorenko, albeit less than Pereira. This   \n292 may be due to lower autocorrelation of ECoG compared to fMRI. We use contiguous splits for the   \n293 remainder of the Fedorenko analyses. ", "page_idx": 7}, {"type": "text", "text": "294 4.2 Word position explains all of untrained, and most of trained, GPT2-XL brain score ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "295 As noted in [35], there was a strong positional signal in the ECoG dataset during comprehension of   \n296 sentences that is likely related to the construction of sentence meaning. We therefore hypothesized   \n297 that a feature space that accounted for word position (WP) would do well relative to untrained and   \n298 trained GPT2-XL. We generated a simple feature space that encodes word position, such that words   \n299 in nearby positions were given similar representations (A.12). When performing a one-sided paired   \n300 $t$ -test between the squared error predictions of $\\mathrm{WP+GPT}2{\\cdot}\\mathrm{XLU^{*}}$ and WP, three electrodes were   \n301 significantly better explained by the addition of GPT2-XLU before FDR correction, and none were   \n302 better explained after FDR correction within each participant. Moreover, WP performs $86.7\\%$ as well   \n303 as GPT2-XL, and $82.1\\%$ as well as $\\mathrm{WP+GPT}2{\\cdot}\\mathrm{XL}^{*}$ . Our results therefore suggest that the mapping   \n304 between GPT2-XL and neural activity on the Fedorenko dataset is largely driven by positional signals.   \n305 ", "page_idx": 7}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/c12f9925fc049ee8fe92f2b60e3e98b928a85eca0662cfc2771bf2ad5e58426b.jpg", "img_caption": ["Figure 4: (a) Across-layer $R^{2}$ , averaged across electrodes in the Fedorenko dataset, for GPT2-XL with and without shuffled splits. (b) Each dot is a participant, lines connect data-points from the same participant. Bars display mean across participants. (c) and (d) Same guidelines as (b). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "306 5 Blank dataset is predicted at near chance levels ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "307 Lastly, we address the Blank dataset. We find that OASM achieves an $R^{2}$ that is 103.6 times   \n308 larger than that of GPT2-XL when using shuffled splits A.13, demonstrating that such splits are   \n309 massively contaminated by temporal autocorrelation. We next turn to using contiguous splits, and test   \n310 whether GPT2-XL performs better than an intercept only model by applying a one-sided paired $t^{\\prime}$ -test   \n311 between the squared error values obtained from GPT2-XL and the intercept only model ( $N=1317$   \n312 TRs). GPT2-XL predicts 1 fROI significantly better than an intercept only model, and 0 fROIs are   \n313 significantly better after FDR correction. Our results therefore suggest that GPT2-XL performs at   \n314 near chance levels on the version of the Blank dataset used by [2, 10, 11]. ", "page_idx": 8}, {"type": "text", "text": "315 6 Limitations and Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "316 Our study has three main limitations. First, our method of examining how much neural variance   \n317 an LLM predicts over simple features scales poorly when the number of features is large. Second,   \n318 although we attempted to correct for cases where adding features decreases test set performance and   \n319 employed banded regression, fitting regressions with large feature spaces on noisy neural data with   \n320 low sample sizes can lead to poor estimations of the neural variance explained. Finally, we did not   \n321 analyze datasets with large amounts of neural data per participant, for instance [36], in which the gap   \n322 between the neural predictivity of simple and complex features might be much larger.   \n323 In summary, we find that on the Pereira dataset, shuffled splits are heavily impacted by temporal   \n324 autocorrelation, untrained GPT2-XL brain score is explained by sentence length and position, and   \n325 trained GPT2-XL brain score is largely explained by non-contextual features. We find that the   \n326 majority of GPT2-XL brain score on the Fedorenko dataset is accounted for by word position, and   \n327 on the Blank dataset GPT2-XL predicts neural activity at near chance levels. These results suggest   \n328 that (i) brain scores on these datasets should be interpreted with caution; and (ii) more generally,   \n329 analyses using brain scores should be accompanied by a systematic deconstruction of neural encoding   \n330 performance, and an evaluation against simple and theoretically uninteresting features. Only after   \n331 such deconstruction can we be somewhat confident that the neural predictivity of LLMs reflects core   \n332 aspects of human linguistic processing. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "333 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "334 [1] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and   \n335 Evelina Fedorenko. Dissociating language and thought in large language models. Trends Cogn.   \n336 Sci., March 2024.   \n337 [2] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy   \n338 Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language:   \n339 Integrative modeling converges on predictive processing. Proc. Natl. Acad. Sci. U. S. A., 118   \n340 (45), November 2021.   \n341 [3] Mariya Toneva, Tom M Mitchell, and Leila Wehbe. Combining computational controls with   \n342 natural text reveals aspects of meaning composition. Nat Comput Sci, 2(11):745\u2013757, November   \n343 2022.   \n344 [4] Charlotte Caucheteux and Jean-R\u00e9mi King. Brains and algorithms partially converge in natural   \n345 language processing. Commun Biol, 5(1):134, February 2022.   \n346 [5] Shailee Jain and Alexander Huth. Incorporating context into language encoding models for   \n347 fMRI. Adv. Neural Inf. Process. Syst., 31, 2018.   \n348 [6] Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (in   \n349 machines) with natural language-processing (in the brain). Adv. Neural Inf. Process. Syst., pages   \n350 14928\u201314938, May 2019.   \n351 [7] Alec Radford, Jeff Wu, R Child, D Luan, Dario Amodei, and I Sutskever. Language models are   \n352 unsupervised multitask learners. 2019.   \n353 [8] Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel Ritter, Samuel J Gershman, Nancy   \n354 Kanwisher, Matthew Botvinick, and Evelina Fedorenko. Toward a universal decoder of linguistic   \n355 meaning from brain activation. Nat. Commun., 9(1):963, March 2018.   \n356 [9] Alexandre Pasquiou, Yair Lakretz, John Hale, Bertrand Thirion, and Christophe Pallier. Neural   \n357 language models are not born equal to fit brain data, but training helps. July 2022.   \n358 [10] Eghbal A Hosseini, Martin Schrimpf, Yian Zhang, Samuel Bowman, Noga Zaslavsky, and   \n359 Evelina Fedorenko. Artificial neural network language models predict human brain responses   \n360 to language even after a developmentally realistic amount of training. Neurobiol Lang (Camb),   \n361 5(1):43\u201363, April 2024.   \n362 [11] Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut.   \n363 Instruction-tuned LLMs with world knowledge are more aligned to the human brain, 2024.   \n364 URL https://openreview.net/forum?id $\\cdot$ DZ6B5u4vfe.   \n365 [12] Charlotte Caucheteux, Alexandre Gramfort, and Jean-R\u00e9mi King. Evidence of a predictive   \n366 coding hierarchy in the human brain listening to speech. Nat Hum Behav, 7(3):430\u2013441, March   \n367 2023.   \n368 [13] Ariel Goldstein, Eric Ham, Mariano Schain, Samuel Nastase, Zaid Zada, Avigail Dabush,   \n369 Bobbi Aubrey, Harshvardhan Gazula, Amir Feder, Werner K Doyle, Sasha Devore, Patricia   \n370 Dugan, Daniel Friedman, Roi Reichart, Michael Brenner, Avinatan Hassidim, Orrin Devinsky,   \n371 Adeen Flinker, Omer Levy, and Uri Hasson. The temporal structure of language processing in   \n372 the human brain corresponds to the layered hierarchy of deep language models, 2024. URL   \n373 https://openreview.net/forum?id $\\cdot$ 95ObXevgHx.   \n374 [14] Refael Tikochinski, Ariel Goldstein, Yoav Meiri, Uri Hasson, and Roi Reichart. Incremental ac  \n375 cumulation of linguistic context in artificial and biological neural networks. bioRxiv, 2024. doi:   \n376 10.1101/2024.01.15.575798. URL https://www.biorxiv.org/content/early/2024/   \n377 01/17/2024.01.15.575798.   \n378 [15] Jeffrey S Bowers, Gaurav Malhotra, Federico Adolf,i Marin Dujmovi\u00b4c, Milton L Montero,   \n379 Valerio Biscione, Guillermo Puebla, John H Hummel, and Rachel F Heaton. On the importance   \n380 of severely testing deep learning models of cognition. Cogn. Syst. Res., 82:101158, December   \n381 2023.   \n382 [16] Richard Antonello, Aditya R Vaidya, and Alexander G Huth. Scaling laws for language   \n383 encoding models in fMRI. Adv. Neural Inf. Process. Syst., abs/2305.11863, May 2023.   \n384 [17] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for   \n385 word representation. In Proceedings of the 2014 conference on empirical methods in natural   \n386 language processing (EMNLP), pages 1532\u20131543, 2014.   \n387 [18] Kilian Weinberger. On the importance of deconstruction in machine learning research.   \n388 ML-Retrospectives $@$ NeurIPS 2020, 2020. URL https://slideslive.com/38938218/   \n389 the-importance-of-deconstruction.   \n390 [19] Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specificity for high-level   \n391 linguistic processing in the human brain. Proc. Natl. Acad. Sci. U. S. A., 108(39):16428\u201316433,   \n392 September 2011.   \n393 [20] Idan Blank, Nancy Kanwisher, and Evelina Fedorenko. A functional dissociation between   \n394 language and multiple-demand systems revealed in patterns of BOLD signal fluctuations. J.   \n395 Neurophysiol., 112(5):1105\u20131118, September 2014.   \n396 [21] Aniketh Janardhan Reddy and Leila Wehbe. Can fMRI reveal the representation of syntactic   \n397 structure in the brain? Adv. Neural Inf. Process. Syst., 34:9843\u20139856, December 2021.   \n398 [22] Wendy A de Heer, Alexander G Huth, Thomas L Griffiths, Jack L Gallant, and Fr\u00e9d\u00e9ric E   \n399 Theunissen. The hierarchical cortical organization of human speech processing. J. Neurosci.,   \n400 37(27):6539\u20136557, July 2017.   \n401 [23] Richard Futrell, Edward Gibson, Harry J Tily, Idan Blank, Anastasia Vishnevetsky, Steven   \n402 Piantadosi, and Evelina Fedorenko. The natural stories corpus. In Nicoletta Calzolari, Khalid   \n403 Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente   \n404 Maegaard, Joseph Mariani, H\u00e9l\u00e8ne Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis,   \n405 and Takenobu Tokunaga, editors, Proceedings of the Eleventh International Conference on   \n406 Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European   \n407 Language Resources Association (ELRA).   \n408 [24] Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas, and Evelina Fedorenko. Lexical  \n409 Semantic content, not syntactic structure, is the main contributor to ANN-Brain similarity of   \n410 fMRI responses in the language network. Neurobiol Lang (Camb), 5(1):7\u201342, April 2024.   \n411 [25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,   \n412 Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT   \n413 pretraining approach. July 2019.   \n414 [26] Alexander G Huth, Wendy A de Heer, Thomas L Griffiths, Fr\u00e9d\u00e9ric E Theunissen, and Jack L   \n415 Gallant. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532   \n416 (7600):453\u2013458, April 2016.   \n417 [27] Shailee Jain, Vy A Vo, Shivangi Mahto, Amanda LeBel, Javier Turek, and Alexander G Huth.   \n418 Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech.   \n419 Adv. Neural Inf. Process. Syst., 33, October 2020.   \n420 [28] Tom Dupr\u00e9 la Tour, Michael Eickenberg, Anwar O Nunez-Elizalde, and Jack L Gallant. Feature  \n421 space selection with banded ridge regression. Neuroimage, 264:119728, December 2022.   \n422 [29] Stijn Hawinkel, Willem Waegeman, and Steven Maere. Out-of-Sample r2: Estimation and   \n423 inference. Am. Stat., pages 1\u201311.   \n424 [30] Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta, and Bapi   \n425 Surampudi. Neural language taskonomy: Which NLP tasks are the most predictive of fMRI   \n426 brain activity? In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz,   \n427 editors, Proceedings of the 2022 Conference of the North American Chapter of the Association   \n428 for Computational Linguistics: Human Language Technologies, pages 3220\u20133237, Seattle,   \n429 United States, July 2022. Association for Computational Linguistics.   \n430 [31] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: A practical and   \n431 powerful approach to multiple testing. J. R. Stat. Soc. Series B Stat. Methodol., 57(1):289\u2013300,   \n432 1995.   \n433 [32] Daniel Loureiro, Al\u00edpio M\u00e1rio Jorge, and Jose Camacho-Collados. LMMS reloaded:   \n434 Transformer-based sense embeddings for disambiguation and beyond. Artif. Intell., 305:103661,   \n435 April 2022.   \n436 [33] George A Miller. WordNet: a lexical database for english. Commun. ACM, 38(11):39\u201341,   \n437 November 1995.   \n438 [34] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Remi King. Disentangling syntax and   \n439 semantics in the brain with deep networks. March 2021.   \n440 [35] Evelina Fedorenko, Terri L Scott, Peter Brunner, William G Coon, Brianna Pritchett, Gerwin   \n441 Schalk, and Nancy Kanwisher. Neural correlate of the construction of sentence meaning. Proc.   \n442 Natl. Acad. Sci. U. S. A., 113(41):E6256\u2013E6262, October 2016.   \n443 [36] Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai, Bhavin Gupta, Allyson   \n444 Morgenthal, Jerry Tang, Lixiang Xu, and Alexander G Huth. A natural language fMRI dataset   \n445 for voxelwise encoding models. Sci Data, 10(1):555, August 2023.   \n446 [37] Zachary Mineroff, Idan Asher Blank, Kyle Mahowald, and Evelina Fedorenko. A robust   \n447 dissociation among the language, multiple demand, and default mode networks: Evidence from   \n448 inter-region correlations in effect size. Neuropsychologia, 119:501\u2013511, October 2018.   \n449 [38] Jonathan D Power, Alexander L Cohen, Steven M Nelson, Gagan S Wig, Kelly Anne Barnes,   \n450 Jessica A Church, Alecia C Vogel, Timothy O Laumann, Fran M Miezin, Bradley L Schlaggar,   \n451 and Steven E Petersen. Functional network organization of the human brain. Neuron, 72(4):   \n452 665\u2013678, November 2011.   \n453 [39] Thomas Lumley, Paula Diehr, Scott Emerson, and Lu Chen. The importance of the normality   \n454 assumption in large public health data sets. Annu. Rev. Public Health, 23:151\u2013169, 2002.   \n455 [40] Simon Musall, Matthew T Kaufman, Ashley L Juavinett, Steven Gluf, and Anne K Churchland.   \n456 Single-trial neural dynamics are dominated by richly varied movements. Nat. Neurosci., 22(10):   \n457 1677\u20131686, October 2019.   \n458 [41] Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom   \n459 embeddings, convolutional neural networks and incremental parsing. To appear, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "460 A Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "461 A.1 Experimental data ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "462 Pereira: For both experiments, each sentence was visually presented for $4\\,\\mathrm{~s~}$ with $4\\ s$ between   \n463 sentences and an additional $4\\ \\mathrm{s}$ between passages. A single fMRI scan was taken in the interval   \n464 between each sentence. Because fMRI data is noisy, each experiment was repeated three times and   \n465 fMRI data was averaged across the repetitions. A single fMRI scanning session consisted of 8 runs,   \n466 where each run contained 12 passages in EXP1 and 9 passages in EXP2. Participants performed   \n467 a total of 3 scanning sessions. The division of passages into runs and the order of the runs was   \n468 randomized for each participant and scanning session.   \n469 Fedorenko: Participants read sentence on word at a time, and each word was visually displayed for   \n470 450 or $700\\;\\mathrm{ms}$ . For each electrode, high gamma signal was extracted using gaussian filter banks at   \n471 center frequencies ranging from $73-144\\:\\mathrm{Hz}$ , the envelope of the high gamma signal was computed   \n472 through a hilbert-transform, and the envelope was z-scored within each electrode. For each participant,   \n473 language-selective electrodes were selected where the ${\\bf Z}$ -scored envelope of the gamma activity was   \n474 significantly higher during the sentences than a condition where participants read nonword lists.   \n475 Z-scored high gamma activity from these language-selective electrodes were used in subsuquent   \n476 analyses.   \n477 Blank: Text was split into 2 s segments corresponding to each TR, with words that were on the   \n478 boundary being assinged to the later TR. Due to the delay in the hemodynamic response function   \n479 (HRF), neural activity was predicted using stimuli from 2 TRs (4 s) previous.   \n480 Functional localization: For Pereira and Blank, the language network was defined by the following   \n481 procedure [19]. First, voxels were identified in each participant which showed stronger responses   \n482 to sentences compared to lists of non-words (sentences $>$ non-word lists contrast). These voxels   \n483 were then constrained by data-driven language activation maps formed by applying the same contrast   \n484 to many other participants. Finally, the top $10\\%$ of the voxels were selected which showed the   \n485 greatest sentences $>$ non-word lists difference. For Pereira, we perform some analyses using four   \n486 other networks: multiple demand (MD), default mode network (DMN), auditory, and visual network.   \n487 The multiple demand (MD) and default mode network (DMN) networks were defined using the same   \n488 procedure, except that the contrast involved a spatial working memory task, where a hard $>$ easy   \n489 condition contrast was used for MD and a fixation $>$ hard contrast was used for DMN [37]. Auditory   \n490 and visual networks were defined using resting state connectivity [38]. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "491 A.2 Banded ridge regression ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "492 We used a random search method to optimize the banded regression hyperparameters [28]. Banded   \n493 regression has two hyperparameters, $\\gamma$ , which is a vector of shape number of feature spaces that   \n494 determines how much each feature space is scaled, and $\\alpha$ , which is the L2 penalty applied across   \n495 feature spaces. Values for $\\gamma$ are drawn from a Dirichlet distribution and hence sum to 1. Down-scaling   \n496 a certain feature space relative to others is functionally equivalent to assigning a separate L2 penalty   \n497 for each feature space. This is because when a feature space is down-scaled, the L2 magnitude of   \n498 the weights must increase for it to have a meaningful contribution to the predictions, which equates   \n499 to increasing the L2 penalty for that feature space. The optimal $\\gamma$ and $\\alpha$ combination was found   \n500 for each voxel/electrode/fROI by performing a random search over $\\gamma$ values, storing the $\\alpha$ value   \n501 that performed best for that $\\gamma$ on validation data, and then selecting the best performing $\\gamma$ and $\\alpha$   \n502 combination.   \n503 Before starting the random search, we tried all combinations of $\\gamma$ values that removed feature spaces   \n504 (i.e. down-scaled at least one feature space to 0) to ensure the regression had an opportunity to   \n505 remove features which hurt performance. In theory, this should obviate the need for the procedure   \n506 implemented in 2.8. This is because the banded regression procedure can remove feature spaces   \n507 based on validation data, meaning if a model performs worse than a sub-model the banded procedure   \n508 has the opportunity to set the $\\gamma$ value corresponding to the additional feature spaces to 0. However,   \n509 because neural data is noisy and there is often little data per subject, performance on validation data is   \n510 not always indicative of performance on test-data. Therefore it is possible for the banded regression   \n511 procedure to include a feature space (since it helps on validation data), and for this feature space to   \n512 ultimately hurt test set performance, necessitating the correction procedure detailed in 2.8.   \n513 We ran banded ridge regression for a maximum of 1000 random search iterations with early stopping   \n514 if the mean $R^{2}$ did not improve by more than $10^{-4}$ after 50 iterations. We treated feature spaces   \n515 with many dimensions as one features because preliminary results showed this performed better.   \n516 Specifically, we always treated the following feature spaces as one feature space: static word   \n517 embeddings, sense-specific word embeddings, syntactic representations, and GPT2-XL and Roberta  \n518 Large representations. All other features were treated as their own feature space.   \n519 We z-score all features across samples before training regressions, as is standard when using ridge   \n520 regression in neural encoding studies. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "521 A.3 Additional details on train, validation, and test folds ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "522 Pereira: During each outer fold, a single passage from each of the 24 semantic categories from one   \n523 experiment was selected, and half of these passages were designated as the test set. This equated to 8   \n524 test folds for experiment 1 (4 passages per semantic category) and 6 test folds for experiment 2 (3   \n525 passages per semantic category). During each inner fold, we again selected one passage from each   \n526 semantic category, and half of these passages were designated as validation (leading to 7 inner folds   \n527 for experiment 1, and 5 inner folds for experiment 2).   \n528 Fedorenko: For each outer fold, we selected 4 sentences as the test fold, resulting in 13 outer folds.   \n529 For each inner fold, we once again select 4 sentences as the validation set, resulting in 12 inner folds   \n530 per outer fold.   \n531 Blank: For each outer fold, we selected a single story as the test fold, resulting in 8 outer folds. For   \n532 each inner fold, each of the remaining stories served in turn as the validation set, resulting in 7 inner   \n533 folds. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "534 A.4 Justification of statistical tests ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "535 We performed a $t$ -test between squared error values from two models to determine if one model   \n536 performs better than another. While squared error values are not always normally distributed, our   \n537 sample sizes were large (the minimum sample size was 243) and so we still opted to use a t-test over   \n538 a non-parametric alternative [39]. One issue with a t-test is that relies on the assumption that samples   \n539 are not correlated, which is not true for time-series data. However, we note that correlated samples   \n540 leads one to underestimate the standard error of the mean and exaggerate differences between two   \n541 models. Since we only perform one-sided t-tests to examine whether adding GPT2-XL representations   \n542 improves performance, the net impact of this on our results is to overestimate how much GPT2-XL   \n543 contributes over simple features. ", "page_idx": 13}, {"type": "text", "text": "544 A.5 Across layer $R^{2}$ values in the Pereira dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "545 Across layer performances in the Pereira dataset for GPT2-XLU and GPT2-XL when using the sum   \n546 pooling method (Fig. 5a,b) and the last token method (Fig. 5c,d). Performance in language network   \n547 is higher across the board than performance in DMN, MD, and visual networks. We do not show   \n548 auditory network results because participants read passages in Pereira and hence auditory brain scores   \n549 are near 0. Furthermore, performance is lower with the last token method in every case except in   \nEXP1 trained results where the last token method performs slightly better. ", "page_idx": 13}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/33272b80e2140c7a2dc823d48dc75ae71956b16ce71ec1c39491220564324fad.jpg", "img_caption": ["Figure 5: a) Across layer performances in Pereira dataset for GPT2-XLU for each functional network when using the sum-pooling method. EXP1 is on the left, and EXP2 is on the right. b) Same as a but for GPT2-XL, also using the sum-pooling method. c) Same as a but when using the last token method. Dotted grey line shows performance of best layer of GPT2-XLU in language network when sum pooling. d) Same as $\\mathbf{b}$ but when using the last token method. Dotted grey line shows performance of best layer of GPT2-XL in language network when sum pooling. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "550 ", "page_idx": 13}, {"type": "text", "text": "551 A.6 RoBERTa-Large shows similar results as GPT2-XL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "552 To examine whether our results depending on the choice of LLM, we replicated all of our Pereira   \n553 trained analyses with RoBERTa-Large (ROB). The overall trend in results was the same as   \n554 with GPT2-XL (Fig. 6). Namely, $\\mathrm{SP+SL+WORD}$ performed $76.8\\%$ as well as the full model   \n555 $(\\mathrm{SP+SL+WORD+SENSE+SYNT+ROB})$ ) and $80.0\\%$ as well as ROB alone in EXP1, and in EXP2 it   \n556 performed $88.0\\%$ as well as the full model and better than ROB. Furthermore, SENSE and SYNT   \n557 bridge the gap to the full model by a small amount. In sum, our main conclusion that a large amount   \n558 of trained LLM brain score in the Pereira dataset is accounted for by non-contextual features also   \n559 applies to RoBERTa-Large. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/740a672e1954e65241c653b4eb9578c6549d2a2bbf810320bc1cd4333535b141.jpg", "img_caption": ["Figure 6: All panels are the same as Figure 3, except GPT2-XL is replaced with RoBERTa-Large (ROB). "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "560 A.7 Orthogonal autocorrelated sequences model (OASM) hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "561 The width of the Gaussian fliter used for within-block smoothing was $\\sigma=2.2$ in Pereira, $\\sigma=1.8$ in   \n562 Fedorenko, and $\\sigma=1.5$ in Blank. Gaussian widths were determined by sweeping $\\sigma$ across 50 evenly   \n563 spaced values between 0.1 and 5.0 and choosing the best-performing $\\sigma$ for each dataset. ", "page_idx": 14}, {"type": "text", "text": "564 A.8 Shuffled train test splits confound task-relevant and task-irrelevant neural activity ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "565 OASM is a model which clearly lacks any linguistic representations that would allow it generalize to   \n566 fully held-out passages. However, this is is not to say that OASM is not correlated with linguistic   \n567 features. For instance, sentences in a given passage are more semantically related with each other   \n568 than with sentences in other passages. Nonetheless, using shuffled train-test splits almost certainly   \n569 exaggerates the variance explained by a model which, on the basis of semantic similarity, arrives   \n570 at a similar representational structure as OASM. This is because task-irrelevant neural responses   \n571 make up a large fraction of neural activity [40], and shuffled train-test splits allow a model with   \n572 OASM-like representational structure to predict not just the task-relevant neural responses driven   \n573 by the participant reading the passage, but also any task-irrelevant neural activity that was present   \n574 throughout the reading of the passage. Hence, we strongly urge researchers to avoid shuffled train   \n575 test splits when evaluating the neural predictivity of language models, and we surmise that previous   \n576 studies using shuffled train-test splits to compare neural predictivity between models might have   \n577 come to erroneous conclusions.   \n579 Here, we show that the MLP block adds a linearly decodable component with non-zero mean to the   \n580 residual stream in the GPT2 architecture. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "581 Proof : ", "page_idx": 15}, {"type": "text", "text": "582 We denote the $i$ \u2019th input to the MLP block in the first layer of GPT2-XL as $x_{i}$ . The output of the   \n583 MLP block is defined as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nM L P(x_{i})=x_{i}+W_{d}(G E L U(W_{u}(L a y e r N o r m(x_{i}))))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "584 We assume that the elements of $x_{i}$ are normally distributed. For a given $x_{i}$ , it then follows that the   \n585 distribution of elements in LayerNorm ${\\bf\\nabla}_{\\bf\\rho}(x_{i})$ is normal with $\\mu=0$ and $\\sigma=1$ (assuming the standard   \n586 LayerNorm initialization).   \n587 Because $W_{u}$ is initialized from a zero-mean normal distribution, $W_{u}(L a y e r N o r m(x_{i}))$ also has   \n588 zero-mean.   \n589 Note that $G E L U$ is a function for which $\\mathbb{E}[Y]>0$ for $Y$ normally distributed with mean 0. Hence, the   \n590 mean value across elements following the $G E L U$ is non-zero. Let us denote this mean value across   \n591 all elements of $G E L U(W_{u}(L a y e r\\bar{N}o r m(x)))$ ) and across all tokens $x$ as $m$ . Then, for an MLP   \n592 with up-projected dimension $d_{u}$ , we can take the dot product of $G E L U(W_{u}(L a y e r N o r m(x_{i})))$   \n593 and 1 $\\textstyle{\\frac{1}{d_{u}m}}\\times{\\hat{k}}$ , where $\\hat{k}$ is a $d_{u}$ -dimensional vector of 1s. The resulting value will have mean 1. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "However, we cannot decode this value directly from the MLP in practice; first, this vector is downprojected back to the residual stream by $W_{d}$ . Nonetheless, we can still closely approximate it, assuming it is approximately orthogonal to $x_{i}$ , by using the pseudo-inverse of $W_{d}$ . More specifically, we can extract a scalar with mean 1 as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{d_{u}}{d_{d}}}\\times\\frac{1}{d_{u}m}\\times\\hat{k}W_{d}^{\\dagger}M L P(x_{i})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "594 where $d_{d}$ is the down-projected dimension. Because this extracted scalar value is distributed with   \n595 mean 1 across token representations $x_{i}$ , assuming independence of token representations within a   \n596 sentence, the sum of the extracted scalar value across the tokens of a sentence is distributed with   \n597 mean equaling the number of tokens in the sentence. ", "page_idx": 15}, {"type": "text", "text": "598 A.10 LMMS ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "599 LMMS generates a sense embedding for each word by averaging across contextual embeddings (in   \n600 our case from RoBERTa-Large) of that sense derived from a sense-annotated corpus. For words in   \n601 WordNet where labeled senses don\u2019t exist, LMMS sets their sense embeddings equal to the average   \n602 of sense embeddings with the same sense (or same hypernym/lexname if that approach fails). Finally,   \n603 the sense embeddings are averaged together with the gloss embeddings for that sense of the word   \n604 generated using the same LLM. For additional details refer to Loureiro et al. [32]. ", "page_idx": 15}, {"type": "text", "text": "605 A.11 Contextual syntactic representations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "606 Syntactic embeddings are derived by substituting content words (nouns, verbs, adjectives, and   \n607 adverbs) in the original sentences with words from the Generics KB corpus, matching their part-of  \n608 speech and dependency tag via the SpaCy transformer-based tagger [41]. For each sentence in the   \n609 Pereira dataset, we generate 170 new sentences, ensuring the subtree token indices from each token   \n610 match those of the original sentence. The top 100 sentences, selected based on summed surprisal   \n611 with GPT2-XL, are retained. Each sentence\u2019s syntactic embedding is then computed by summing   \n612 token representations within each sentence and then averaging across the 100 sentences. ", "page_idx": 15}, {"type": "text", "text": "613 A.12 Word position feature in Fedorenko dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "614 The primary finding in the paper which first collected the Fedorenko dataset [35] was a ramping of   \n615 neural activity across the words of sentences, where each sentence was 8 words long. Hence, we   \n616 concatenate a linearly ramping 1-dimensional positional signal to an 8-dimensional 1-hot positonal   \n617 signal. Because we expect positional signals to be more simlar between adjacent words than more   \n618 distant words, we apply a Gaussian fliter $\\left.\\sigma=1\\right.$ ) to the 8-dimensional positional signal. The resulting   \n619 feature space, which we refer to as \"word position\" in the main text, is shown for a single sentence in   \n620 the above figure. ", "page_idx": 15}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/41fdbd661ae24cd44b55c2d7aef788362227d6b8f6fe6357668650128c3ee20a.jpg", "img_caption": ["Figure 7: Word Position feature for a single sentence in the Fedorenko dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "621 A.13 OASM and GPT2 Model Comparison on Blank Dataset ", "page_idx": 16}, {"type": "image", "img_path": "u1b1dJtyxc/tmp/34f316360feb335be3b3f15932243f37eadc1216261509899a897b26814840d1.jpg", "img_caption": ["Figure 8: OASM far outperforms GPT2-XL on the Blank dataset, and GPT2-XL does not appear to explain any variance beyond that explained by OASM. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "622 We find that OASM achieves 103.6 times higher neural predictivity than GPT2-XL on the Blank   \n623 dataset when using shuffled train-test splits. There could be several reasons for this. First, it might   \n624 be that the method for pooling representations from GPT2-XL used here 2.3 and in [2, 10, 11]   \n625 did not yield useful enough representations for GPT2-XL to map effectively to the brain data. An   \n626 additional likely culprit is that, of the three datasets we study here, Blank has the greatest potential for   \n627 autocorrelation in temporally adjacent samples. This is because, while the Pereira dataset typically   \n628 has a TR every 8 seconds, the Blank dataset has a TR every 2 seconds. We note that our results here   \n629 are not completely surprising; given that [2, 10] observed untrained GPT2 models perform far better   \n630 than trained models on this dataset, it did not seem likely that GPT2-XL would map onto neural   \n631 representations of linguistic features here. ", "page_idx": 16}, {"type": "text", "text": "632 A.14 Computational Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "633 All analyses were done between 2 machines: One with 2 RTX 3090 GPUs, and another with 1   \n634 RTX 4090 GPU. The most computationally demanding parts of our analyses were ftiting the banded   \n635 ridge regressions used to generate Figure 3, collecting untrained model results across 10 seeds, and   \n636 generating syntactic representations, which each took around 3 hours to complete. ", "page_idx": 17}, {"type": "text", "text": "637 A.15 Dataset Licenses ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "638 The Blank dataset was originally released as part of the Natural Stories Corpus, which is provided   \n639 under the CC BY-NC-SA license [23]. The Pereira dataset is released under the Creative Commons   \n640 License [8]. The version of the Fedorenko dataset used here is provided under the MIT license. All   \n641 datasets used are the same versions as in [2] and can be downloaded using the neural-nlp repository:   \n642 https://github.com/mschrimpf/neural-nlp/tree/master. All datasets were collected with   \n643 IRB approval at their respective institutions. ", "page_idx": 17}, {"type": "text", "text": "644 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "646 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n647 paper\u2019s contributions and scope?   \n648 Answer: [Yes]   \n649 Justification: We support each of the three claims made in the abstract regarding shuffled   \n650 train-test splits, untrained LLM brain scores, and trained LLM brain scores in the Results   \n651 section. These results support the claim that it is important to deconstruct the mapping   \n652 between LLMs and the brain.   \n653 Guidelines:   \n654 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n655 made in the paper.   \n656 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n657 contributions made in the paper and important assumptions and limitations. A No or   \n658 NA answer to this question will not be perceived well by the reviewers.   \n659 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n660 much the results can be expected to generalize to other settings.   \n661 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n662 are not attained by the paper.   \n663 2. Limitations ", "page_idx": 18}, {"type": "text", "text": "664 Question: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification:We discuss the three main limitations in the paper in the section titled \"Limitations and Conclusions\", and additionally include limitations throughout the Appendix (e.g. Justification of statistical tests). ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "695 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "96 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n97 a complete (and correct) proof?   \n98 Answer: [Yes]   \n99 Justification: Our only theoretical result is that the MLP layer introduces a non-zero mean   \n00 component in the residual stream. We provide both a rough sketch in the main paper as well   \n01 as a formal proof.   \n02 Guidelines:   \n03 \u2022 The answer NA means that the paper does not include theoretical results.   \n04 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n05 referenced.   \n06 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n07 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n08 they appear in the supplemental material, the authors are encouraged to provide a short   \n09 proof sketch to provide intuition.   \n10 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n11 by formal proofs provided in appendix or supplemental material.   \n12 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n713 4. Experimental Result Reproducibility   \n14 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n15 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n16 of the paper (regardless of whether the code and data are provided or not)?   \n17 Answer: [Yes]   \n18 Justification: We include all details regarding the following: banded regression proce  \n19 dure, construction of feature spaces, train, validation, and test splits, and selection of   \n20 voxels/electrodes/fROIs in neural data. These are all the elements needed to reproduce our   \n21 results, with the exception of slight variability due to stochasticity in untrained LLM seeds   \n22 and the randoms search process in banded regression.   \n23 Guidelines:   \n24 \u2022 The answer NA means that the paper does not include experiments.   \n25 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n26 well by the reviewers: Making the paper reproducible is important, regardless of ", "page_idx": 19}, {"type": "text", "text": "whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 19}, {"type": "text", "text": "750 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n751 authors are welcome to describe the particular way they provide for reproducibility.   \n752 In the case of closed-source models, it may be that access to the model is limited in   \n753 some way (e.g., to registered users), but it should be possible for other researchers   \n754 to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "55 5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "56 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n57 tions to faithfully reproduce the main experimental results, as described in supplemental   \n8 material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We will release all our code on Github, and all neural datasets are openly available for use. We also provide anonymized code.   \nGuidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "2 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We dedicate sections towards explaining the data splits in the main paper, and the necessary details to run the banded ridge regression in the main paper and Appendix. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "795 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "96 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n97 information about the statistical significance of the experiments?   \n802 Guidelines:   \n803 \u2022 The answer NA means that the paper does not include experiments.   \n804 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n805 dence intervals, or statistical significance tests, at least for the experiments that support   \n806 the main claims of the paper.   \n807 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n808 example, train/test split, initialization, random drawing of some parameter, or overall   \n809 run with given experimental conditions).   \n810 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n811 call to a library function, bootstrap, etc.)   \n812 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n813 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n814 of the mean.   \n815 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n816 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n817 of Normality of errors is not verified.   \n818 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n819 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n820 error rates).   \n821 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n822 they were calculated and reference the corresponding figures or tables in the text.   \n823 8. Experiments Compute Resources   \n824 Question: For each experiment, does the paper provide sufficient information on the com  \n825 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n826 the experiments?   \n827 Answer: [Yes]   \n828 Justification: We provide a section in the appendix describing the GPUs and CPUs used for   \n829 our analyses, and we describe how long each experiment took to run.   \n830 Guidelines:   \n831 \u2022 The answer NA means that the paper does not include experiments.   \n832 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n833 or cloud provider, including relevant memory and storage.   \n834 \u2022 The paper should provide the amount of compute required for each of the individual   \n835 experimental runs as well as estimate the total compute.   \n836 \u2022 The paper should disclose whether the full research project required more compute   \n837 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n838 didn\u2019t make it into the paper).   \n839 9. Code Of Ethics   \n840 Question: Does the research conducted in the paper conform, in every respect, with the   \n841 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n842 Answer: [Yes]   \n843 Justification: We did not conduct any direct interactions with human participants, none of   \n844 the data-related concerns apply for us, and we do not see any direct societal impacts from   \n845 our work. We make our methods clear to the best of our ability and provide anonymized   \n846 code.   \n847 Guidelines:   \n848 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n849 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n850 deviation from the Code of Ethics.   \n851 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n852 eration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "853 10. Broader Impacts ", "page_idx": 21}, {"type": "text", "text": "854 Question: Does the paper discuss both potential positive societal impacts and negative   \n855 societal impacts of the work performed?   \n856 Answer: [NA]   \n857 Justification: We do not develop any novel technology that can be used for good or bad, but   \n858 rather show that some high-profile previous results have been over-interpreted. While our   \n859 results are relevant for the cognitive neuroscience community, we do not see a direct path to   \n860 any larger societal impacts.   \n861 Guidelines:   \n862 \u2022 The answer NA means that there is no societal impact of the work performed.   \n863 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n864 impact or why the paper does not address societal impact.   \n865 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n866 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n867 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n868 groups), privacy considerations, and security considerations.   \n869 \u2022 The conference expects that many papers will be foundational research and not tied   \n870 to particular applications, let alone deployments. However, if there is a direct path to   \n871 any negative applications, the authors should point it out. For example, it is legitimate   \n872 to point out that an improvement in the quality of generative models could be used to   \n873 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n874 that a generic algorithm for optimizing neural networks could enable people to train   \n875 models that generate Deepfakes faster.   \n876 \u2022 The authors should consider possible harms that could arise when the technology is   \n877 being used as intended and functioning correctly, harms that could arise when the   \n878 technology is being used as intended but gives incorrect results, and harms following   \n879 from (intentional or unintentional) misuse of the technology.   \n880 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n881 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n882 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n883 feedback over time, improving the efficiency and accessibility of ML).   \n884 11. Safeguards   \n885 Question: Does the paper describe safeguards that have been put in place for responsible   \n886 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n887 image generators, or scraped datasets)?   \n888 Answer: [NA]   \n889 Justification: We release no new models or datasets, and do not see any potential for our   \n890 results being misused in unsafe ways.   \n891 Guidelines:   \n892 \u2022 The answer NA means that the paper poses no such risks.   \n893 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n894 necessary safeguards to allow for controlled use of the model, for example by requiring   \n895 that users adhere to usage guidelines or restrictions to access the model or implementing   \n896 safety filters.   \n897 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n898 should describe how they avoided releasing unsafe images.   \n899 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n900 not require this, but we encourage authors to take this into account and make a best   \n901 faith effort.   \n902 12. Licenses for existing assets ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "907 Justification: We cite the papers in which all datasets used were first published. We provide   \n908 the licenses for the Blank and Pereira datasets in the supplement (we could not find a license   \n909 for the Fedorenko dataset). We also specify the version of the datasets used and provide a   \n910 link.   \n911 Guidelines:   \n912 \u2022 The answer NA means that the paper does not use existing assets.   \n913 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n914 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n915 URL.   \n916 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n917 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n918 service of that source should be provided.   \n919 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n920 package should be provided. For popular datasets, paperswithcode.com/datasets   \n921 has curated licenses for some datasets. Their licensing guide can help determine the   \n922 license of a dataset.   \n923 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n924 the derived asset (if it has changed) should be provided.   \n925 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n926 the asset\u2019s creators.   \n927 13. New Assets   \n928 Question: Are new assets introduced in the paper well documented and is the documentation   \n929 provided alongside the assets?   \n930 Answer: [NA]   \n931 Justification: We do not release any new assets with this paper.   \n932 Guidelines:   \n933 \u2022 The answer NA means that the paper does not release new assets.   \n934 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n935 submissions via structured templates. This includes details about training, license,   \n936 limitations, etc.   \n937 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n938 asset is used.   \n939 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n940 create an anonymized URL or include an anonymized zip file.   \n941 14. Crowdsourcing and Research with Human Subjects   \n942 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n943 include the full text of instructions given to participants and screenshots, if applicable, as   \n944 well as details about compensation (if any)?   \n945 Answer: [Yes]   \n946 Justification: We use open source datasets where neural data is obtained from consenting   \n947 human adults. Information regarding research protocols is detailed in the references for   \n948 these datasets.   \n949 Guidelines:   \n950 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n951 human subjects.   \n952 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n953 tion of the paper involves human subjects, then as much detail as possible should be   \n954 included in the main paper.   \n955 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n956 or other labor should be paid at least the minimum wage in the country of the data   \n957 collector.   \n958 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n959 Subjects   \n960 Question: Does the paper describe potential risks incurred by study participants, whether   \n961 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n962 approvals (or an equivalent approval/review based on the requirements of your country or   \n963 institution) were obtained?   \n964 Answer: [Yes]   \n965 Justification: All datasets used here were collected with IRB approval at their respective   \n966 institutions, and this is stated in the appendix. We do not collect any data of our own from   \n967 human subjects.   \n968 Guidelines:   \n969 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n970 human subjects.   \n971 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n972 may be required for any human subjects research. If you obtained IRB approval, you   \n973 should clearly state this in the paper.   \n974 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n975 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n976 guidelines for their institution.   \n977 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n978 applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]