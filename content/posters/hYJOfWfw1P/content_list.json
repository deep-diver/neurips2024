[{"type": "text", "text": "Is $O(l o g\\,N)$ practical? Near-Equivalence Between Delay Robustness and Bounded Regret in Bandits and RL ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "P. R. Kumar ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Enoch H. Kang\u2217   \nFoster School of Business   \nUniversity of Washington   \nSeattle, WA 98195 ", "page_idx": 0}, {"type": "text", "text": "Electrical & Computer Engineering Texas A&M University College Station, TX 77843 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Interactive decision making, encompassing bandits, contextual bandits, and reinforcement learning, has recently been of interest to theoretical studies of experimentation design and recommender system algorithm research. One recent finding in this area is that the well-known Graves-Lai constant being zero is a necessary and sufficient condition for achieving bounded (or constant) regret in interactive decision-making. As this condition may be a strong requirement for many applications, the practical usefulness of pursuing bounded regret has been questioned. In this paper, we show that the condition of the Graves-Lai constant being zero is also necessary for a consistent algorithm to achieve delay model robustness when reward delays are unknown (i.e., when feedback is anonymous). Here, model robustness is measured in terms of $\\epsilon$ -robustness, one of the most widely used and one of the least adversarial robustness concepts in the robust statistics literature. In particular, we show that $\\epsilon$ -robustness cannot be achieved for a consistent (i.e., uniformly sub-polynomial regret) algorithm, however small the nonzero $\\epsilon$ value is, when the Grave-Lai constant is not zero. While this is a strongly negative result, we also provide a positive result for linear rewards models (contextual linear bandits, reinforcement learning with linear MDP) that the Grave-Lai constant being zero is also sufficient for achieving bounded regret without any knowledge of delay models, i.e., the best of both the efficiency world and the delay robustness world. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the cost of addressing stochastic and anonymous delayed rewards in Decision-Making with Structured Observations (DMSO) [1, 2], which generalizes interactive decision-making problems, such as structured bandits, contextual bandits, and reinforcement learning2. In many real-life applications of interactive decision-making problems, stochastic and unknown delays in reward make it challenging to attribute the sequence of observed outcomes to previous decisions. In medical treatments, for example, a doctor cannot easily be sure whether a medical outcome is due to the effect of current treatment or due to some other previously taken treatment\u2019s delayed effect. This type of reward delay in decisions is called an \u2018unknown reward delay\u2019 [3] or \u2018delayed anonymous feedback\u2019 $[4,5]^{3}$ . Under this setting, the decision maker never observes the period information for which each reward corresponds to, even after it receives the delayed reward at the later time step. As it is not obvious which decision caused each observed reward, reward attribution becomes a challenge. ", "page_idx": 0}, {"type": "image", "img_path": "hYJOfWfw1P/tmp/fdcf2b89cb86c580772dafc3402076b85521a89210599e9287a8dc5dabf21de0.jpg", "img_caption": ["Figure 1: Examples of misspecification of the reward delay model of decisions $(\\pi_{1},\\pi_{2},\\pi_{3})$ "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Some knowledge (e.g., mean) of the probabilistic distribution of each decision\u2019s reward delay, combined with the careful design of algorithms, may help to resolve this reward attribution problem under stochastic and anonymous delayed rewards [4]. However, those delay models themselves may be misspecified [6]. Therefore, whether we can design an algorithm that is robust to model misspecification becomes a main concern in the problems with stochastic and anonymous delayed rewards. ", "page_idx": 1}, {"type": "text", "text": "One of the most widely used concepts of model misspecification in the robust statistics literature is $\\epsilon$ -robustness [7]. Given a parameter $\\epsilon>0$ and true distribution $D$ , a model distributionD is called an $\\epsilon_{}$ -(general) contamination of $D$ if $d_{T V}(D,\\widehat{D})\\leq\\epsilon$ , where $d_{T V}$ denotes the total variation distance function4. Figure 1 illustrates some examples  of $\\epsilon$ -contamination of the delay models. As $\\epsilon$ -robustness is also one of the weakest (i.e., least adversarial) and most elementary notion of robustness [8], the first question on an algorithm\u2019s delay robustness will be, \u201cup to which $\\epsilon$ the algorithm\u2019s properties are robust to $\\epsilon$ -contamination of delay model misspecification?\u201d. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we prove that no consistent (i.e., uniformly sub-polynomial regret) algorithm for DMSO can be designed to be robust to $\\epsilon_{}$ -contamination of delay model misspecification unless DMSO\u2019s Graves-Lai constant [9, 2, 1] being zero. While this is a strong negative result, we also provide a positive result for linear DMSO problems (linear contextual bandit, reinforcement learning with linear MDPs), showing that the Graves-Lai constant being zero [9, 10, 11] is sufficient for achieving bounded regret without any knowledge of delay models. As the Graves-Lai constant being zero holds if and only if we can achieve bounded regret [2, 1, 10, 11], the results in this paper strongly motivate the practical usefulness of designing learning systems where we can achieve bounded regret. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "While no previous work has studied the link between consistent algorithms and robustness to delay distribution misspecification when reward delays are unknown $\\mathbf{\\bar{\\rho}}=\\mathbf{\\bar{\\rho}}$ anonymous) in problems related to DMSO (e.g, bandit problems and reinforcement learning problems), there has been other work looking at different flavors of delayed anonymous rewards. There were prior studies on delayed rewards [12, 13, 14, 15], but [4] was the first to formalize the unknown stochastic reward delays assumption in interactive decision making problems, which led to the literature on stochastic delayed, anonymous and aggregated feedback (DAAF). While [4] provides a consistent algorithm for stochastic bandits that does not require any knowledge of delay distributions, it requires a strong assumption that the mean of delay distribution is precisely known, which cannot be achieved under $\\epsilon$ -contamination however small $\\dot{\\epsilon}\\,\\mathrm{is}^{5}$ (In Section 4, we show that no algorithm can be consistent when $\\epsilon>0$ ). [16] provides another consistent algorithm for stochastic bandits that also does not require any knowledge of delay distributions and improves [4], but it requires a different assumption that the delayed reward feedback exactly associates the reward and the arm and therefore rewards are not anonymous; [17, 18] make similar assumption for episodic reinforcement learning problems with stochastic delays. ", "page_idx": 1}, {"type": "text", "text": "While our work centers on delayed anonymous rewards\u2014where the learner cannot associate delayed rewards with the actions that generated them\u2014there exists a parallel line of research addressing non-anonymous delays, in which such associations are possible. In this context, several studies have proposed algorithms that account for unrestricted or unbounded delays. [19] developed algorithms for nonstochastic multiarmed bandits with unrestricted delays, achieving robust regret bounds by employing a skipping strategy to manage excessively delayed feedback. [20] adapted Thompson sampling to handle multiarmed bandits with unrestricted delays, extending its applicability to delayed feedback settings without assuming bounded delays. [21] derived near-optimal regret bounds for adversarial MDPs with delayed bandit feedback, addressing the challenges posed by feedback delays in adversarial environments. [22] proposed an optimal algorithm for adversarial bandits experiencing arbitrary delays, establishing regret bounds that hold even when delays are extensive. Building on this foundation, [23] introduced a \"best-of-both-worlds\" algorithm that improves upon [22] by providing both adversarial guarantees and near-optimal stochastic performance without requiring prior knowledge of the maximal delay. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Decision-Making with Structured Observations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The DMSO problem framework generalizes many problems such as bandit problems, contextual bandit problems, and episodic reinforcement learning problems. DMSO is characterized by the environment and its learning protocol, which is described as follows: ", "page_idx": 2}, {"type": "text", "text": "The environment of a DMSO problem framework is specified as a tuple $(\\Pi,{\\mathcal{R}},{\\mathcal{O}},{\\mathcal{F}})$ , where $\\Pi$ denotes the decision space, $\\mathcal{R}$ denotes the reward space, $\\scriptscriptstyle\\mathcal{O}$ denotes the observation space, and $\\begin{array}{r}{\\mathcal{F}=\\prod_{\\pi\\in\\Pi}\\mathcal{F}_{\\pi}}\\end{array}$ denotes the model class where ${\\mathcal{F}}_{\\pi}\\subseteq\\triangle_{\\mathcal{R}\\times\\mathcal{O}}$ (Here, $\\triangle_{E}$ notation means the space of all possible probability distributions over a set $E$ ).6We use $f_{\\pi}$ to refer to an element of ${\\mathcal{F}}_{\\pi}$ , with $f_{\\pi}$ being the $\\pi$ -coordinate of $f\\in\\mathcal F$ . A ground-truth model $f^{\\star}\\in{\\mathcal{F}}$ governs the rewards and the observations based on the decisions made in the rounds. While $f^{\\star}$ is unknown to the learner, it is typically assumed that a set $\\mathcal{F}$ that includes $f^{\\star}$ is known to the learner. Formally, we make the following assumption, which is often called the realizability assumption [24, 25, 26]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1 (Realizability). The learner has access to the model class $\\mathcal{F}$ containing the groundtruth model $f^{\\star}$ . ", "page_idx": 2}, {"type": "text", "text": "$\\circ$ The learning protocol for the DMSO problem consists of $n$ rounds. In round $k\\leq n$ , ", "page_idx": 2}, {"type": "text", "text": "1. The learner makes a decision $\\pi_{k}\\in\\Pi$ . 2. A reward $r_{k}\\in\\mathcal{R}$ and an observation $o_{k}\\in\\mathcal{O}$ are generated, where $(r_{k},o_{k})\\sim f_{\\pi_{k}}^{\\star}\\in\\mathcal{F}_{\\pi_{k}}$ 3. Learner observes $o_{k}$ . If there are reward delays, the learner observes $R_{k}$ , the set of rewards that arrive at the round $k$ . $R_{k}$ is equivalent to $r_{k}$ only if there are no reward delays. ", "page_idx": 2}, {"type": "text", "text": "2.2 Learning Algorithm for DMSO ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given that we characterized the DMSO problem framework, we can now describe a learning algorithm for it. Let hk be the history up to round k, i.e., hk = {(\u03c0j, Rj, oj)}jk=\u221211 where \u03c0j \u2208\u03a0, Rj \u2208R, oj \u2208 $\\scriptscriptstyle\\mathcal{O}$ and $\\mathcal{H}$ be the set of all possible histories of rounds for $k\\geq1$ . A learning algorithm $A$ is defined as an element of $A\\subseteq(\\mathcal{H}\\mapsto\\triangle_{\\Pi})$ , which is a subset of the set of all possible mappings from the history space $\\mathcal{H}$ to the set of all possible distributions over $\\Pi$ . That is, at each round $k$ , given the history $h_{k}\\in\\mathcal{H}$ , a learning algorithm $A\\in{\\mathcal{A}}$ chooses $p_{k}=A(h_{k})\\in\\triangle_{\\Pi}$ . The decision at round $k$ , $\\pi_{k}$ , is sampled from $p_{k}$ . Note that $f\\in\\mathcal F$ , $A\\in{\\mathcal{A}}$ and the round $n$ completely determine the stochastic behavior of the learning protocol up to round $n$ , i.e., they induce a probability distribution we call $P_{f,n,A}[\\cdot]$ over the set of all histories up to round $n$ . We also denote the respective expectation by $\\mathbb{E}_{f,n,A}[\\cdot]$ . When the meaning is clear from the context, we use $P_{f,n}[\\cdot]$ and $\\mathbb{E}_{f,n}[\\cdot]$ instead of $P_{f,n,A}[\\cdot]$ and $\\mathbb{E}_{f,n,A}[\\cdot]$ . ", "page_idx": 2}, {"type": "text", "text": "Given $(r,o)\\sim f_{\\pi}$ , we denote $\\mu_{f_{\\pi}}:=\\mathbb{E}_{f_{\\pi}}[r]$ . Let $\\pi_{f}\\in\\arg\\operatorname*{max}_{\\pi\\in\\Pi}\\mu_{f_{\\pi}}$ denote an optimal decision for the model $f$ . The sub-optimality gap of decision $\\pi$ for model $f$ is defined as $\\Delta_{f}(\\pi):=\\mu_{f_{\\pi_{f}}}-\\mu_{f_{\\pi}}$ When the ground truth model is $f$ , choosing $\\pi_{f}$ at each round until round $n$ yields the largest value of total reward until round $n$ . Therefore, we can measure the optimality of an algorithm $A$ until round $n$ in terms of regret, which is defined by ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{f,A}(n):=\\mathbb{E}_{A,f,n}\\left[\\sum_{k=1}^{n}\\Delta_{f}(\\pi_{k})\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.3 Consistent Learning Algorithm\u2019s Instance-Dependent Regret Lower Bound ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "$\\mathrm{Reg}_{f,A}(n)$ is a quantity that is dependent on the true model instance $f$ . Since the true model $f$ is unknown to the learner a priori, a good learning algorithm must be able to perform well for all possible $f\\in\\mathcal F$ . Therefore, one might want to define the goodness of an algorithm by its capability to achieve the minimal value of $\\mathrm{Reg}_{f,A}(n)$ among all possible algorithms for all the instances $f\\in\\mathcal F$ However, this is not achievable; a bespoke algorithm that always chooses $\\pi_{f}$ will outperform all possible algorithms for the instance $f$ , while suffering linear regret for the instances in ${\\bar{\\mathcal{F}}}\\setminus f$ . ", "page_idx": 3}, {"type": "text", "text": "Since many problems have algorithms that incur sub-polynomial regret for all instances, it is common to exclude algorithms that suffer polynomially increasing regret in some instances. This idea is formalized in the following definition that restricts the space of \u2018interesting\u2019 algorithms. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Graves and Lai (1997) [9]). A learning algorithm $A$ is called consistent if $\\mathrm{Reg}_{f,A}(n)=o\\left(n^{p}\\right)$ holds for every $p>0$ and $f\\in\\mathcal F$ . ", "page_idx": 3}, {"type": "text", "text": "For DMSO problems, it has been recently shown that any consistent algorithm\u2019s instance-dependent regret must satisfy the asymptotic lower bound described in the following theorem [2]. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.3 (Dong and Ma (2022) [2]). Suppose that there are no reward delays. Then for every instance $f\\in\\mathcal F$ , the expected regret of any consistent algorithm $A$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\frac{\\operatorname{Reg}_{f,A}(n)}{\\ln n}\\geq\\mathcal{C}(f)=\\operatorname*{lim}_{n\\to\\infty}\\mathcal{C}(f,n),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{C}(f,n)$ is the solution to the optimization equation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}(f,n)\\triangleq\\underset{w\\in\\mathbb{R}_{+}^{|\\Pi|}}{\\operatorname*{min}}\\sum_{\\pi\\in\\Pi}w_{\\pi}\\Delta_{f}(\\pi)}\\\\ &{\\quad\\quad\\quad\\quad\\:s.t.\\ \\displaystyle\\sum_{\\pi\\in\\Pi}w_{\\pi}D_{\\mathrm{KL}}(f_{\\pi}\\|g_{\\pi})\\geq1\\ ,\\ \\forall g\\in\\mathcal{F}(f)^{c}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\|w\\|_{\\infty}\\leq n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $D_{K L}$ is the KL divergence and $\\mathcal{F}(f):=\\{g\\in\\mathcal{F}\\mid\\pi_{g}=\\pi_{f}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Corollary 2.4. Let $f^{\\star}\\,\\in\\,{\\mathcal{F}}$ be the ground-truth model. For a consistent algorithm to achieve sub-logarithmic regret, $\\mathcal{C}(f^{\\star})=0$ must hold. That is, achievement of sub-logarithmic regret for all possible instances of $\\mathcal{F}$ can be assured a priori only $i f{\\mathcal{C}}(f)=0$ for $f\\in\\mathcal F$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.5 (Wagenmaker and Foster [1]). Expected regret of order ${\\mathcal{C}}(f^{\\star})\\ln n$ can be achieved for DMSO problems without delays. That is, bounded regret can be assured $i f{\\mathcal{C}}(f)=0$ for $f\\in\\mathcal F$ . ", "page_idx": 3}, {"type": "text", "text": "2.4 $\\epsilon$ -contamination and Total Variation Distance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In robust statistics, one of the oldest and the most commonly used concepts for modeling contaminated data is the concept of $\\epsilon$ -contamination [7]. Given a parameter $0<\\epsilon<1$ and original distribution $D$ , a distribution $X$ is called an $\\epsilon$ -additive contamination (or Huber contamination) of $D$ if $X$ is a mixture distribution of $D$ and an unknown arbitrary distribution $E$ , with their selection probabilities being $(1-\\epsilon)$ for $D$ and $\\epsilon$ for $E$ . Furthermore, A distribution $X$ is called an $\\epsilon$ -subtractive contamination of $D$ if it is equivalent to an arbitrary $\\epsilon$ -probability removal from $D$ (and normalization). Finally, we say that a distribution $X$ is a (general) $\\epsilon$ -contamination of $D$ if $X$ can be constructed by removing $\\epsilon$ -probability from $D$ and replacing that $\\epsilon$ with equal mass from some arbitrary distribution $E$ . ", "page_idx": 3}, {"type": "text", "text": "As discussed earlier, the concept of $\\epsilon$ -contamination is closely related to the concept of total variation distance. Given a space of distributions, the total variation distance, denoted as $d_{T V}(\\nu,\\upsilon)$ , is defined as $\\begin{array}{r}{d_{\\mathrm{TV}}(\\nu,\\upsilon):=\\frac{1}{2}\\|\\nu-\\upsilon\\|_{1}=\\operatorname*{sup}_{E\\in\\Sigma}|\\nu(E)-\\upsilon(E)|.}\\end{array}$ , where $\\Sigma$ stands for the measurable sets on which $\\nu$ and $\\upsilon$ are defined. It is well known that $d_{T V}$ is a metric that satisfies interesting properties such as 1) $d_{T V}=0$ if and only if $\\nu=\\upsilon$ and 2) $d_{T V}=1$ if and only if $\\nu$ and $\\upsilon$ are singular, i.e. there exists $E$ such that $\\nu(E)=1$ and $\\boldsymbol{\\upsilon}(E)=0$ . It is also well known that the concept of $\\epsilon$ -contamination is equivalent to the concept of total variation distance [8]; we separately state this property as the following lemma. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Lemma 2.6 ([8]). Given a parameter $0<\\epsilon<1$ , a distribution $X$ is an $\\epsilon$ -contamination of $D$ (and vice versa) if and only if $d_{T V}(X,D)=\\epsilon.$ . ", "page_idx": 4}, {"type": "text", "text": "3 Main Model: $\\epsilon$ -delay Robustness ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Denote the true reward delay distributions of each decision $\\pi\\in\\Pi$ by $D_{\\pi}$ . Every time $\\pi_{k}$ is determined at each round $k$ , $d_{k}\\sim D_{\\pi_{k}}$ is generated along with the generation of $(r_{k},o_{k})\\sim f_{\\pi}$ . While $O_{k}$ is observed immediately at round $k$ , $r_{k}$ is scheduled to arrive at round $d_{k}+k$ . The order of reward arrivals does not necessarily match the order of the reward generation. ", "page_idx": 4}, {"type": "text", "text": "As in [3], we assume the unknown delays setting throughout the paper. In the unknown delays setting, the delay $d_{k}$ is not observed, and therefore attributing rewards to the previous decisions becomes a nontrivial problem. One can only guess from which decision the reward just arrived came based on the history of previous decisions and some information about reward delay distributions. ", "page_idx": 4}, {"type": "text", "text": "As reward delays are not observed, information we know about reward delay distributions is likely to be misspecified. We model this misspecification as $\\epsilon$ -contamination of the true delay distribution models $\\{D_{\\pi}\\}_{\\pi\\in\\Pi}$ resulting in information about $\\{\\hat{D}_{\\pi}\\}_{\\pi\\in\\Pi}$ instead, where $\\hat{D}_{\\pi}$ is an outcome of $\\epsilon_{}$ -contamination of the delay distribution model $D_{\\pi}$ , i.e., $d_{T V}(D_{\\pi},\\hat{D}_{\\pi})\\,\\le\\,\\epsilon$ . Note that $\\epsilon$ -contamination of delay distribution models encompasses many possible misspecification of information about delay distribution. For example, it implies misspecification of mean of delay distribution as $d_{T V}(D_{\\pi},\\hat{D_{\\pi}})>\\epsilon$ implies $|\\mathbb{E}[D]-\\mathbb{E}[\\hat{D}]|>0$ (See discussions in Assumption 4.2 for details). ", "page_idx": 4}, {"type": "text", "text": "We now propose the formal definition of robustness in terms of delay distribution knowledge. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. We say that a consistent algorithm is $\\epsilon$ -delay robust if it is consistent when the given delay distributions are $\\epsilon_{}$ -contaminations of the true delay distributions. ", "page_idx": 4}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Negative Result: Delay Robustness Requires $\\boldsymbol{\\mathcal{C}}(f)=0$ for all $f\\in\\mathcal F$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the definition of $\\epsilon$ -delay robustness provided in Section 3, the main question is when a consistent, $\\epsilon$ -delay robust algorithm exists. The answer is quite negative: unless $\\bar{c}(f^{\\star})=0$ holds, no consistent algorithm can be $\\epsilon$ -delay robust, however small $\\epsilon>0$ is. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Under minor technical assumptions (see Assumptions 4.2-4.5 below), regardless of how small $\\epsilon>0$ is, a consistent learning algorithm can be $\\epsilon$ -delay robust only if $\\mathcal{C}(f^{\\star})=0$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 implies that the concept of consistent algorithm fails even with a very small misspecification of the delay model unless the Graves-Lai constant $\\mathcal{C}(f^{\\star})$ satisfies $\\mathcal{C}(f^{\\star})=0$ . Since we want to design a learning system with existence of a consistent algorithm that works for all instances of $f\\in\\mathcal F$ , we need $\\boldsymbol{\\mathcal{C}}(f)=0$ for $f\\in\\mathcal F$ , which was the necessary and sufficient condition for achieving bounded regret when there were no reward delays (Corollary 2.4 and Theorem 2.5) ", "page_idx": 4}, {"type": "text", "text": "The intuition behind the proof of Theorem 4.1 (See Appendix B for details), is as follows. When the reward delay model is precisely known, i.e., when the reward delay model is not contaminated, we might be able to address this challenge by designing a good algorithm that makes the probability of confusion in reward attribution as small as we want. However, in the case of $\\epsilon$ -contamination of delay models, under minor technical assumptions below (Assumptions 4.2-4.5), we can always provide a delay model contamination that makes the precision of any consistent algorithm\u2019s reward attribution no better than $1-\\delta$ for some $\\delta>0$ . This leads to reward distribution suffering $\\delta^{.}$ -contamination, which makes impossible to design a consistent algorithm. ", "page_idx": 4}, {"type": "text", "text": "The assumptions required for Theorem 4.1 are as follows. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.2. For the family of distributions $\\mathcal{D}_{r|o}:=\\{f_{\\pi}(\\cdot\\mid o)\\mid f\\in\\mathcal{F},\\pi\\in\\Pi,o\\in\\mathcal{O}\\}$ , there exists a function $q(\\delta)$ s.t. for $D_{1},D_{2}\\in\\mathcal{D}_{r|o}$ , $\\left|\\mathbb{E}[D_{1}]-\\mathbb{E}[D_{2}]\\right|\\le q(\\delta)$ implies $d_{T V}(D_{1},D_{2})\\leq\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "For some special families of reward distributions, such $q$ that satisfies Assumption 4.2 is known [8]. (Let $k$ be a constant in what follows) ", "page_idx": 5}, {"type": "text", "text": "\u2022 For the family of Gaussian distributions with standard deviation 1, $q(\\delta)=k\\delta$ .   \n\u2022 For the family of log-concave distributions with standard deviation 1, $q(\\delta)=k\\delta\\log(1/\\delta)$ .   \n\u2022 For the family of distributions with $k$ th moment bounded by 1 for $k\\geq2$ , $q(\\delta)=k\\delta^{1-1/k}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.3 expresses the conditional unimodality in likelihood functions in terms of rewards. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.3. Given ground truth $f\\ \\in\\ {\\mathcal{F}}$ and $g^{1},g^{2}\\ \\in\\ {\\mathcal{F}}$ , for every $\\pi\\,\\in\\,\\Pi$ , $\\mathbb{E}_{g_{\\pi}^{1}}[r|o]\\le$ $\\mathbb{E}_{g_{\\pi}^{2}}[r|o]\\,\\le\\,\\mathbb{E}_{f_{\\pi}}[r|o]$ or $\\mathbb{E}_{g_{\\pi}^{1}}[r|o]\\;\\ge\\;\\mathbb{E}_{g_{\\pi}^{2}}[r|o]\\;\\ge\\;\\mathbb{E}_{f_{\\pi}}[r|o]$ implies $D_{K L}(f_{\\pi}(\\cdot\\mid o),g_{\\pi}^{2}(\\cdot\\stackrel{\\cdot}{\\mid}o))\\ \\leq$ $D_{K L}(f_{\\pi}(\\cdot\\mid o),g_{\\pi}^{1}(\\cdot\\mid o))$ almost everywhere (a.e.). ", "page_idx": 5}, {"type": "text", "text": "Assumptions 4.4 and 4.5 exclude trivial cases where the reward information is not at all needed for the inference of the ground-truth model $f$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.4 (Density of ${\\mathcal{F}}_{\\pi}$ for every $\\pi\\in\\Pi$ ). For every $\\pi\\in\\Pi$ , $\\{g_{\\pi}\\in\\mathcal{F}_{\\pi}\\mid\\mu_{g_{\\pi}}\\geq\\mu_{f_{\\pi_{f}}}\\}\\cap$ $\\{g_{\\pi}\\in\\mathcal{F}_{\\pi}\\ |\\ |\\mathbb{E}_{f_{\\pi}}(r\\mid o)-\\mathbb{E}_{g_{\\pi}}(r\\mid o)|\\leq q(\\delta)\\ a.e.\\}$ is nonempty given $\\delta>0$ . ", "page_idx": 5}, {"type": "text", "text": "Intuitively, $\\{g_{\\pi}\\;\\in\\;{\\mathcal F}_{\\pi}\\;\\mid\\;\\mu_{g_{\\pi}}\\;\\geq\\;\\mu_{f_{\\pi_{f}}}\\}$ is the set of hypotheses in ${\\mathcal{F}}_{\\pi}$ we need to reject, and $\\{g_{\\pi}\\in\\mathcal{F}_{\\pi}\\ |\\ |\\mathbb{E}_{f_{\\pi}}(r\\ |\\ o)-\\mathbb{E}_{g_{\\pi}}(r\\ |\\ o)|\\stackrel{\\cdot}{\\leq}q(\\delta)\\ a.e.\\}$ is the set of hypothesis we cannot reject under contamination of outcomes from decision $\\pi$ . Note that $\\{g_{\\pi}\\in\\mathcal{F}_{\\pi}\\mid\\lvert\\mathbb{E}_{f_{\\pi}}(r\\mid o)-\\mathbb{E}_{g_{\\pi}}(r\\mid o)\\rvert\\le$ $q(\\delta)\\ a.e.\\}\\subseteq\\{g_{\\pi}\\in\\mathcal{F}_{\\pi}\\mid|\\mu_{g_{\\pi}}-\\mu_{f_{\\pi}}|\\leq q(\\delta)\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.5. Let $g_{\\pi}^{o}$ be the marginal distribution of the observation of $g_{\\pi}\\in\\mathcal{F}_{\\pi}$ . There exists $r_{o}>0$ such that for every $\\pi\\in\\Pi$ , $|\\mu_{f_{\\pi}}-\\mu_{g_{\\pi}}|\\leq r_{o}$ implies $f_{\\pi}^{o}=g_{\\pi}^{o}$ a.e.. ", "page_idx": 5}, {"type": "text", "text": "Note that $f_{\\pi}^{o}=g_{\\pi}^{o}\\;a.e$ . if and only if $D_{K L}(f_{\\pi}^{o}||g_{\\pi}^{o})=0$ holds. If $D_{K L}\\big(f_{\\pi}^{o}\\big|\\big|g_{\\pi}^{o}\\big)>0$ , no information on the rewards will be required to reject $g_{\\pi}$ under $f_{\\pi}$ , the true hypothesis for the decision $\\pi$ . On the other hand, in the reinforcement learning problems where reward functions are parametrized independent of the transition model parameters, $r_{0}$ in the Assumption 4.5 is $+\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Positive Result: $\\boldsymbol{\\mathcal{C}}(f)=0$ for all $f\\in\\mathcal F$ enables Super-Robust Bounded Regret ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the previous section, we saw in Theorem 4.1 that $C(f)=0$ for $f\\in\\mathcal F$ is required to assure the existence of a delay-robust consistent algorithm. We also saw that $C(f)=0$ for $f\\in\\mathcal F$ is required to assure sub-logarithmic regret for all possible instances of true $f$ (Theorem 2.3). ", "page_idx": 5}, {"type": "text", "text": "Here, we try to answer when it is sufficient to assure best of both worlds, i.e., bounded regret and robustness to any delay-model miss-specification at the same time. Before answering this question, we need to define and explore two new concepts: cross-informativeness and max-contamination. ", "page_idx": 5}, {"type": "text", "text": "4.2.1 Cross-informativeness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recall that we denote by $P_{f,n,A}$ the distribution of the outcomes of algorithm $A$ for the model instance $f\\in\\mathcal F$ by the nth round. Let us denote the algorithm that always chooses decision $\\pi\\in\\Pi$ as $\\overline{{\\pi}}$ . Then Lemma 4.6 motivates the concept of cross-informativeness. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.6. Suppose that $f\\in\\mathcal F$ is the ground-truth model instance. Then $C(f)=0$ implies that $D_{\\mathrm{KL}}\\left(P_{f,n,\\overline{{\\pi_{f}}}}\\Vert P_{g,n,\\overline{{\\pi_{f}}}}\\right)=\\Omega(n)$ holds for $g\\in\\mathcal{F}(f)^{c}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. Suppose that $\\mathscr{C}(f)~=~0$ . According to equation (3), this implies that there exist $w_{\\pi_{f}}$ and $0\\,\\leq\\,n_{0}\\,<\\,\\infty$ such that for all $n~\\geq~n_{0}$ , $w_{\\pi_{f}}D_{\\mathrm{KL}}(f_{\\pi_{f}}\\|g_{\\pi_{f}})\\,\\geq\\,1,\\forall g\\,\\in\\,\\mathcal{F}(f)^{c}$ . Therefore, DKL Pf,n,\u03c0f \u2225Pg,n,\u03c0f = nDKL(f\u03c0f \u2225g\u03c0f ) \u2265w1\u03c0f holds $\\forall g\\in\\mathcal{F}(f)^{c}$ for $n\\geq n_{0}$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.6 provides an intuitive and straightforward connection between $C(f)=0$ and bounded regret: we can reject all the hypotheses that need a rejection to conclude that $f$ is indeed the ground truth hypothesis $(\\mathcal{F}(f)^{c})$ , simply by exclusively playing $\\pi_{f}$ forever, while incurring zero regret. Lemma 4.6 shows how informative $\\pi_{f}$ is when the true hypothesis is $f$ . When the true hypothesis is not $f,\\pi_{f}$ can be arbitrarily uninformative. ", "page_idx": 5}, {"type": "text", "text": "The natural question now arises is how much cross-informativeness (informativeness of $\\pi_{h}\\in\\Pi$ for the ground truth $f\\in\\mathcal{F}$ when $h\\neq f$ ) is sufficient for us to achieve bounded regret. The key assumption used in this paper is Assumption 4.7, which is later shown to be satisfied for the linear systems (contextual linear bandits, linear MDP) when the conditions implied by $C(f)=0$ for $f\\in\\mathcal F$ are satisfied (Section 5). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Assumption 4.7 (Cross-informativeness). Suppose that $f\\in\\mathcal F$ is the ground-truth model. Then for any $g,h\\in{\\mathcal{F}}$ $:{\\mathcal{F}},D_{\\mathrm{KL}}\\left(P_{f,n,{\\overline{{\\pi_{h}}}}}\\Vert P_{g,n,{\\overline{{\\pi_{h}}}}}\\right)=\\omega(\\ln n)$ holds. ", "page_idx": 6}, {"type": "text", "text": "Note that the cross-informativeness lower bound rate of $\\omega(\\ln n)$ in Assumption 4.7 is a much weaker rate than the lower bound rate $\\Omega(n)$ in Lemma 4.6. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Max-contamination ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Whatever true delay distribution the reward delays follow, the maximum number of reward arrivals from $\\pi$ by the round $k$ is $N_{\\pi}(k)$ , the total number of $\\pi$ decisions by the round $k$ . The maxcontamination of the decision $\\pi^{\\prime}\\in\\Pi$ at round $k$ is defined as $\\begin{array}{r}{\\delta_{\\pi^{\\prime}}^{\\mathrm{max}}(k):=\\operatorname*{min}(\\frac{\\sum_{\\pi\\in\\Pi\\setminus\\pi^{\\prime}}N_{\\pi}(k)}{\\widetilde N(k)},1)}\\end{array}$ where $\\widetilde{N}(k)$ stands for the total number of reward arrivals by round $k$ . Note that the contamination of rew ard arrival at $k$ is bounded by the max-contamination $\\delta_{\\pi^{\\prime}}^{\\mathrm{max}}(k)$ , as the delay distributions of decisions are stationary, i.e., they do not change over time. ", "page_idx": 6}, {"type": "text", "text": "4.2.3 Algorithm Simply-Test-To-Commit (ST2C) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Assumption 4.8. For all $f,g\\in{\\mathcal{F}}$ , $\\pi\\in\\Pi$ , and $o\\in{\\mathcal{O}}$ , $\\begin{array}{r}{\\left|\\ln\\frac{f_{\\pi}}{g_{\\pi}}\\right|<c}\\end{array}$ for some $c>0$ . This implies $D_{K L}(f_{\\pi}(\\cdot\\mid o)\\|g_{\\pi}(\\cdot\\mid o))<\\infty.$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.8 excludes trivially informative cases where $f$ and $g$ are almost immediately dis$(\\operatorname*{sup}_{g\\in\\Pi,\\pi\\in\\Pi,E\\in\\mathcal{E}_{\\pi}}\\,\\frac{\\mathrm{d}f_{\\pi}(\\cdot|o)}{\\mathrm{d}g_{\\pi}(\\cdot|o)}(E))^{-1}$ $o\\ \\in\\ {\\mathcal{O}}$ ${\\mathcal{E}}_{\\pi}$ dUenndoetre  tAhses cuomllpetcitoino n4 .o8f , mweaes ucraanb lwe eslelt-sd feofrin $f_{\\pi}(\\cdot\\mid o)$ $\\beta\\;:=\\;$ and $g_{\\pi}(\\cdot\\mid o)]$ ), as Assumption 4.8 holds if and only if the log-likelihood ratio $\\ln\\frac{f_{\\pi}(\\cdot|o)}{g_{\\pi}(\\cdot|o)}$ is well-defined on the support of $g_{\\pi}$ and is finite a.e.. ", "page_idx": 6}, {"type": "text", "text": "Let $P_{g,k,\\overline{{\\pi}}}^{c}$ indicate the likelihood of $g\\in{\\mathcal{F}}$ that is computed as if all reward arrivals by the round $k$ are from the decision $\\overline{{\\pi}}$ . (Note that this is actually not true, as we allow decision transitions in Algorithm 1.) Note that $\\begin{array}{r}{\\ln\\frac{P_{f,k,\\pi}^{c}}{P_{g,k,\\pi}^{c}}=\\sum_{k=1}^{n}\\ln\\frac{f_{\\pi}^{c}(k)}{g_{\\pi}^{c}(k)}}\\end{array}$ n gf c\u03c0\u03c0c((kk)), where f \u03c0c(k) and gc\u03c0(k) are likelihood of each data assuming that the data is from $\\pi$ . ", "page_idx": 6}, {"type": "text", "text": "We now describe the algorithm Simply-Test-to-Commit (ST2C) as the Algorithm 1 below. ", "page_idx": 6}, {"type": "table", "img_path": "hYJOfWfw1P/tmp/7204ca74745ffdc130d3037412c91a709460ecc1d225fbc6bd7b480b9bb4dfdb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2.4 Analysis of algorithm ST2C ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Later in Section 5, we will show that $\\mathscr{C}(f)\\,=\\,0$ for all $f\\,\\in\\,{\\mathcal{F}}$ , which is a design feature of a learning problem we decide a priori, is sufficient to show that Assumption 4.7 indeed holds for some representative linear problems. In this section, we show that Assumption 4.7 (combined with a technical minor Assumption 4.8) is sufficient to allow Algorithm 1 to achieve bounded regret without any knowledge of delay distribution model. ", "page_idx": 6}, {"type": "text", "text": "The following Lemma 4.9 shows that $\\hat{f}$ stays at incorrect instances for only finite time, except for the periods $\\hat{f}$ arrives at incorrect instances. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.9. Under Assumption 4.7 and 4.8, total number of periods $\\hat{f}$ stays in $\\mathcal{F}\\backslash f^{\\star}$ is finite in expectation, except for the periods wrong transition (transition to ${\\mathcal{F}}\\backslash f^{\\star})$ happens. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.10 shows that the total number of wrong transitions from the correct inferences is finite in expectation. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.10. Under Assumption 4.8, the number of rounds Algorithm $^{\\,l}$ satisfies the event $\\{\\widehat{f}=$ $\\begin{array}{r}{f^{\\star}\\}\\cap\\{\\exists g\\in\\mathcal{F}(f^{\\star})^{c}\\,s.t.\\,\\sum_{k=1}^{n}\\ln\\frac{g_{\\pi_{f}}^{c}(k)}{\\widehat{f}_{\\pi_{f}}^{c}(k)}\\geq2\\ln k+\\sum_{k=1}^{n}\\frac{2}{\\sqrt{\\beta}}\\delta_{\\pi_{\\widehat{f}}}^{\\operatorname*{max}}(k)\\}}\\end{array}$ ) \u22652 ln k+ kn=1\u221a2\u03b2 \u03b4\u03c0m fa x(k)} holds is finite in expectation. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.11. Under Assumption 4.7 and 4.8, the algorithm ST2C (Algorithm $^{\\,l}$ ), which does not require any knowledge of the delay distribution model, achieves a bounded regret \u2206(1+5 W4c(42ec\u22122)22 ) \u03c062 , where $\\mathcal{W}$ is the principal branch Lambert W function [27], $\\Delta$ is the maximum per-period mean reward difference among decisions, and c is from Assumption 4.8. ", "page_idx": 7}, {"type": "text", "text": "Proof. Combining Lemmas 4.9 and 4.10, we can conclude that ${\\widehat{f}}\\notin{\\mathcal{F}}(f^{\\star})$ holds only for a finite number of rounds in expectation. That is, regret is bounded in expectation. For detailed derivation of the bound, see Appendix C.3. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "5 Equivalence of bounded regret and delay robustness in linear systems ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As discussed in Section 4.2, satisfying the cross-informativeness condition introduced in Assumption 4.7 is the key assumption that enables Algorithm 1 to achieve bounded regret with super-robustness to delay. In this section, we show that linear learning problems such as contextual linear bandit and reinforcement learning (RL) with linear MDP indeed satisfies the cross-informativeness condition if $\\boldsymbol{\\mathcal{C}}(f)=0$ for $f\\in\\mathcal F$ . That is, for those problems, the condition $\\cdot{\\mathcal{C}}(f)=0$ for $f\\in{\\mathcal{F}}^{*}$ is not only necessary (Section 4.1), but also sufficient for achieving bounded regret under any level of delay model misspecification. In other words, we can conclude that achieving bounded regret is equivalent to achieving any level of delay robustness for such linear problems discussed in Section 5.1 and 5.2. ", "page_idx": 7}, {"type": "text", "text": "5.1 Contextual linear bandit problem ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Hao, Lattimore, and Szepesvari [10] was the first to characterize when $\\boldsymbol{\\mathcal{C}}(f)=0$ holds for contextual linear bandit problems. In this paper, we follow the notations and settings of [10] as follows: Let\u2019s consider the stochastic $M$ -armed contextual linear bandit with a horizon of $n$ rounds with $M$ arms and a finite $A$ -size set of $k$ -dimensional possible contexts $\\mathcal{X}=\\{\\mathbf{x}_{j}\\}_{j\\in[A]}$ . At each round, a context is sampled according to the unknown distribution $p$ over $\\mathcal{X}$ and then observed. Every time a context is sampled, an arm choice (a decision in MDSO framework) happens. When the sampled context is $\\mathbf{x}_{j}$ and its chosen arm is $m$ , we receive $\\phi_{m}(\\mathbf{x}_{j})^{\\prime}\\theta+\\epsilon$ , where $\\{\\dot{\\phi}_{m}:\\mathbb{R}^{k}\\mapsto\\mathbb{R}^{d}\\}_{m\\in[M]}$ are linear representation functions that are assumed to be precisely known, $\\theta$ is a parameter vector of dimension $d$ that is shared across the arms, and $\\epsilon$ is an i.i.d. random noise that follows a sub-Gaussian distribution with variance proxy $\\sigma^{2}$ . ", "page_idx": 7}, {"type": "text", "text": "Let $\\Theta$ be the set of all parameter vectors, and let $\\theta^{\\star}\\in\\Theta$ be the unknown true parameter. Suppose that ${\\mathcal{C}}(\\theta)=0$ for $\\theta\\in\\Theta$ . Let $m_{j\\theta}$ be an optimal arm for context $j\\in[A]$ when the true parameter is $\\theta$ , i.e., $m_{j\\theta}\\in\\operatorname{argmax}_{m\\in[M]}\\phi_{m}(\\mathbf{x}_{j})^{\\prime}\\theta$ . The following Theorem 5.1 characterizes previous results on when bounded regret can be achieved when there are no reward delays. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 ([10, 28]). Given linear contextual bandit setting described above, when there are no reward delays, bounded regret can be a priori guaranteed to be achieved if and only if $\\left\\{\\phi_{m_{j\\theta}}(\\mathbf{x}_{j})\\mid j\\in A\\right\\}$ spans $\\mathbb{R}^{d}$ for all $\\theta\\in\\Theta$ . ", "page_idx": 7}, {"type": "text", "text": "Note that the condition that $\\left\\{\\phi_{m_{j\\theta}}(\\mathbf{x}_{j})\\mid j\\in A\\right\\}$ spans $\\mathbb{R}^{d}$ for all $\\theta\\in\\Theta^{\\bullet}$ in Theorem 5.1 is easily satisfied a priori when we are given rich enough context set [29]. How does this easily satisfied condition work when there are anonymous delayed rewards? The following theorem of ours shows that $\\'\\{\\phi_{m_{j\\theta}}(\\mathbf{x}_{j})\\mid j\\in A\\}$ spans $\\mathbb{R}^{d}$ for all $\\theta\\in\\Theta$ \u2019 implies that the Assumption 4.7 is satisfied. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Given contextual linear bandit setting described above, Assumption 4.7 $(D_{\\mathrm{KL}}\\left(P_{\\theta^{\\star},n,\\overline{{\\pi_{\\theta}}}}\\vert\\vert P_{\\theta^{\\prime},n,\\overline{{\\pi_{\\theta}}}}\\right)=\\Omega(n)$ holds for $\\theta^{\\prime}\\in\\mathcal{F}(\\theta^{\\star})^{c})$ is satisfied if $\\left\\{\\phi_{m_{j\\theta}}(\\mathbf{x}_{j})\\mid j\\in{\\bar{A}}\\right\\}$ spans $\\mathbb{R}^{d}$ for all $\\theta\\in\\Theta$ . ", "page_idx": 8}, {"type": "text", "text": "See Appendix $\\mathrm{D}$ for the proof of Theorem 5.2. As Assumption 4.8 trivially holds for linear problems [2], Theorem 4.11 (which requires Assumption 4.7 and 4.8 to hold) conclude that our Algorithm 1 achieves bounded regret without any knowledge of delay distribution model if $\\left\\{\\phi_{m_{j\\theta}}(\\mathbf{x}_{j}){\\mathsf{\\bar{\\phi}}}\\right\\}j\\in A\\right\\}$ spans $\\mathbb{R}^{d}$ for all $\\theta\\in\\Theta$ (which is easily satisfied when the context space is rich enough [29]). For contextual linear bandit setting described above, it has been shown that $\\left\\{\\phi_{m_{j\\theta}}(\\mathbf{x}_{j})\\mid j\\in A\\right\\}$ spans $\\mathbb{R}^{d}$ for all $\\theta\\in\\Theta^{\\bullet}$ is equivalent to ${\\mathcal{C}}(\\theta)=0$ for all $\\theta\\in\\Theta^{\\bullet}$ [26, 2, 1]. Since ${\\mathcal{C}}(\\theta)=0$ for all $\\theta\\in\\Theta$ is a necessary condition for a-priori assurance of any-level robustness of a consistent algorithm (Theorem 4.1), we have the following Corollary 5.3, which is a reminiscent of Theorem 5.1 above. ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.3. Given contextual linear bandit setting described above, under any anonymous delayed rewards, bounded regret can be a priori guaranteed to be achieved if and only if $\\left\\{\\phi_{m_{j\\theta}}\\!\\;\\!\\stackrel{\\cdot}{\\left(\\mathbf{x}_{j}\\right)}\\mid j\\in\\dot{\\cal A}\\right\\}$ spans $\\mathbb{R}^{d}$ for all $\\theta\\in\\Theta$ . ", "page_idx": 8}, {"type": "text", "text": "Note that Corollary 5.3 strongly motivates the practical usefulness of bounded regret algorithm design in contextual linear bandit problems, as it is also a necessary and sufficient condition for achieving any level of delay robustness. This condition is indeed not hard to satisfy in the real world; for example, in Spotify, million daily users can be considered a rich enough context for exploring 60,000 new songs uploaded daily [29]. ", "page_idx": 8}, {"type": "text", "text": "5.2 Reinforcement learning with Linear MDP ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Papini et al. [11] was the first to characterize the condition for achieving bounded regret for some popular classes of reinforcement learning with episodic Linear Markov Decision Process (MDP). In this paper, we follow the notations of [11], which are as follows: we are given a time-inhomogenous MDP $\\boldsymbol{M}=\\left(\\boldsymbol{S},\\boldsymbol{\\mathcal{A}},\\boldsymbol{H},\\left\\{\\boldsymbol{r}_{h}\\right\\}_{h=1}^{H},\\boldsymbol{P},\\boldsymbol{\\mu}\\right)$ , where $\\boldsymbol{S}$ is finite state space, $\\boldsymbol{\\mathcal{A}}$ is a finite action space, $H$ is the length of each episode, $\\{r_{h}\\}$ are the reward functions where $r_{h}(s,a)$ the expected reward of a pair $(s,a)\\in S\\times A$ at time-step $h$ , $P:=\\{p_{h}\\}$ are the transition kernels, and $\\mu$ is the initial state distribution. A policy $\\pi=(\\pi_{1},...,\\pi_{H})\\in\\Pi$ is a sequence of per-time-step policies $\\pi_{h}:{\\mathcal{S}}\\rightarrow A$ . For every $h\\;\\in\\;[H]\\;:=\\;\\{1,\\ldots,H\\}$ , we define the state-action value function of a policy $\\pi$ as $\\begin{array}{r}{Q_{h}^{\\pi}(s,a)\\,=\\,r_{h}(s,a)+\\mathbb{E}_{\\pi}\\left[\\sum_{i=h+1}^{H}r_{i}\\left(s_{i},a_{i}\\right)\\right]}\\end{array}$ and $Q_{h}^{\\pi^{\\star}}(s,a)\\,:=\\,Q_{h}^{\\star}(s,a)\\,=\\,\\operatorname*{sup}_{\\pi}Q_{h}^{\\pi}(s,a)\\,=$ $L_{h}Q_{h+1}^{\\star}(s,a)$ where $L_{h}Q_{h+1}^{\\star}(s,a):=r_{h}(s,a)+\\mathbb{E}_{s^{\\prime}\\sim p_{h}(s,a)}$ maxa\u2032 Q\u22c6h+1 (s\u2032, a\u2032) . ", "page_idx": 8}, {"type": "text", "text": "As in [11], we focus on episodic Linear MDP setting with Bellman closure [30], which is more general than many popular Linear MDP settings such as low-rank Linear MDPs [30]. ", "page_idx": 8}, {"type": "text", "text": "Definition 5.4 (Linear MDP with Bellman closure (completeness)[30]). Suppose that we are given a feature map $\\phi_{h}:S\\times\\mathcal{A}\\to\\mathbb{R}^{d_{h}}$ , possibly different at any $h\\in[H]$ , mapping state-action pair $(s,a)$ into a $d_{h}$ -dimensional vector $\\phi_{h}(s,a)$ . For the set of bounded value function $\\;Q_{h}\\,=\\,\\{Q_{h}\\mid\\dot{\\theta}_{h}\\in\\;$ $\\Theta_{h}:Q_{h}(s,a)=\\phi_{h}(s,a)^{\\top}\\theta_{h},\\forall(s,a)\\Big\\}$ and the associated parameter space $\\Theta_{h}~=~\\{\\theta_{h}\\in\\mathbb{R}^{d}$ : $\\left|\\phi_{h}(s,a)^{\\top}\\theta_{h}\\right|\\leq D\\right\\}$ . An MDP is said to satisfy zero Inherent Bellman Error (IBE) (or satisfy Bellman closure) if $\\forall h\\in[H]$ , $\\begin{array}{r}{\\operatorname*{sup}_{Q_{h+1}\\in\\mathcal{Q}_{h+1}}\\operatorname*{inf}_{Q_{h}\\in\\mathcal{Q}_{h}}\\left\\|Q_{h}-L_{h}Q_{h+1}\\right\\|_{\\infty}=0}\\end{array}$ holds. ", "page_idx": 8}, {"type": "text", "text": "The following Theorem 5.5 summarizes a previous result that characterizes when bounded regret can be achieved when there are no reward delays. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.5 (Papini et al. [11]). Denote the optimal policy as $\\pi^{\\star}$ and $\\phi_{h}^{\\star}(s):=\\phi_{h}\\left(s,\\pi_{h}^{\\star}(s)\\right)$ . In Linear MDPs satisfying Bellman closure, the condition that \u2018span $\\{\\phi_{h}^{\\star}(s)\\mid\\forall s\\in S$ , $\\pi^{\\star}$ visits s at $h$ with positive probability $\\boldsymbol{\\beta}=\\mathbb{R}^{d}$ for all $h\\in[H]\\,.$ is sufficient for achieving bounded regret in high probability when there is no unknown reward delay. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.6. Given episodic Linear MDP with Bellman closure described above, Assumption 4.7 $(D_{\\mathrm{KL}}\\,(P_{\\theta^{\\star},n,\\overline{{\\pi_{\\theta}}}}||P_{\\theta^{\\prime},n,\\overline{{\\pi_{\\theta}}}})=\\Omega(n)$ holds for $\\theta^{\\prime}\\in\\mathcal{F}(\\theta^{\\star})^{c})$ is satisfied $i f\\sin\\{\\phi_{h}^{\\star}(s)\\mid\\forall s^{\\prime}\\in{\\cal S},$ $\\pi^{\\star}$ visits s at $h$ with positive probability} $\\mathbf{\\mu}=\\mathbb{R}^{d}$ for all $h\\in[H]$ . ", "page_idx": 8}, {"type": "text", "text": "Again, as Assumption 4.8 trivially holds for linear problems [2], Theorem 4.11 (which requires Assumption 4.7 and 4.8 to hold) conclude that our Algorithm 1 achieves bounded regret without any knowledge of delay distribution model if $\\operatorname{span}\\{\\phi_{h}^{\\star}(s)~|~\\forall s\\in{\\mathcal{S}},\\pi^{\\star}$ visits $s$ at $h$ with positive probability $\\}=\\mathbb{R}^{d}$ for all $h\\in[H]$ . Therefore, we have the following Corollary 5.7. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Corollary 5.7. Given episodic Linear MDP with Bellman closure setting described above, under any anonymous delayed rewards, bounded regret can be a priori guaranteed to be achieved $i f$ $\\operatorname{span}\\{\\phi_{h}^{\\star}(s)\\mid\\forall s\\in S,\\,\\pi^{\\star}$ visits s at $h$ with positive probability} $\\mathcal{l}=\\mathbb{R}^{d}$ for all $h\\in[H]$ . ", "page_idx": 9}, {"type": "text", "text": "Again, note that Corollary 5.7 strongly motivates the practical usefulness of bounded regret algorithm design, as the sufficient condition for it is also sufficient for achieving any level of delay robustness. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we characterize the link between consistent algorithms and delay robustness in interactive decision-making. The first main result states that, for consistent algorithms, the necessary condition for achieving any (small) level of robustness against delay model misspecifications is also sufficient for achieving bounded regret. Viewed from another perspective, this result urges us to revisit the practicality of the instance-dependent regret minimizing algorithm design regime [9, 1] for real-world problems with anonymous delayed rewards. The second main result states vice versa for linear problems, showing that the well-known necessary (and sufficient) condition for bounded regret is also sufficient for designing a consistent algorithm that achieves any (large) level of robustness against delay model misspecifications and bounded regret at the same time. An interesting future research direction raised by our paper is whether it is possible to achieve our second main result without restricting to linear problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This material is based upon work partially supported by the US Army Contracting Command under W911NF-22-1- 0151 and USARO under W911NF2120064, the US National Science Foundation under CNS-2328395 and CMMI-2038625, and the US Office of Naval Research under N00014- 24-1-2615 and N00014-21-1-2385. This work was also partially conducted with support from the Bertauche Transportation Endowment and the Edna Benson PhD Fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Andrew Wagenmaker and Dylan J Foster. \u201cInstance-Optimality in Interactive Decision Making: Toward a Non-Asymptotic Theory\u201d. In: arXiv preprint arXiv:2304.12466 (2023).   \n[2] Kefan Dong and Tengyu Ma. Asymptotic Instance-Optimal Algorithms for Interactive Decision Making 2023. arXiv: 2206.02326 [cs.LG].   \n[3] Bingcong Li, Tianyi Chen, and Georgios B Giannakis. \u201cBandit online learning with unknown delays\u201d. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 993\u20131002.   \n[4] Ciara Pike-Burke et al. \u201cBandits with delayed, aggregated anonymous feedback\u201d. In: International Conference on Machine Learning. PMLR. 2018, pp. 4105\u20134113.   \n[5] Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. \u201cNonstochastic bandits with composite anonymous feedback\u201d. In: Conference On Learning Theory. PMLR. 2018, pp. 750\u2013 773.   \n[6] Siwei Wang, Haoyun Wang, and Longbo Huang. \u201cAdaptive algorithms for multi-armed bandit with composite and anonymous feedback\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. 11. 2021, pp. 10210\u2013 10217.   \n[7] Peter J Huber. Robust statistics. Vol. 523. John Wiley & Sons, 2004.   \n[8] Ilias Diakonikolas and Daniel M Kane. Algorithmic high-dimensional robust statistics. Cambridge University Press, 2023.   \n[9] Todd L Graves and Tze Leung Lai. \u201cAsymptotically efficient adaptive choice of control laws incontrolled markov chains\u201d. In: SIAM journal on control and optimization 35.3 (1997), pp. 715\u2013743.   \n[10] Botao Hao, Tor Lattimore, and Csaba Szepesvari. \u201cAdaptive exploration in linear contextual bandit\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 3536\u20133545.   \n[11] Matteo Papini et al. \u201cReinforcement learning in linear mdps: Constant regret and representation selection\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 16371\u2013 16383.   \n[12] Gergely Neu et al. \u201cOnline Markov decision processes under bandit feedback\u201d. In: Advances in Neural Information Processing Systems 23 (2010).   \n[13] Pooria Joulani, Andras Gyorgy, and Csaba Szepesv\u00e1ri. \u201cDelay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30. 1. 2016.   \n[14] Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. \u201cDelay and cooperation in nonstochastic bandits\u201d. In: Journal of Machine Learning Research 20.17 (2019), pp. 1\u201338.   \n[15] Claire Vernade, Olivier Capp\u00e9, and Vianney Perchet. \u201cStochastic bandit models for delayed conversions\u201d. In: arXiv preprint arXiv:1706.09186 (2017).   \n[16] Tal Lancewicki et al. \u201cStochastic multi-armed bandits with unrestricted delay distributions\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 5969\u20135978.   \n[17] Benjamin Howson, Ciara Pike-Burke, and Sarah Filippi. \u201cOptimism and delays in episodic reinforcement learning\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2023, pp. 6061\u20136094.   \n[18] Nikki Lijing Kuang et al. \u201cPosterior sampling with delayed feedback for reinforcement learning with linear function approximation\u201d. In: Advances in Neural Information Processing Systems 36 (2023), pp. 6782\u20136824.   \n[19] Tobias Sommer Thune, Nicol\u00f2 Cesa-Bianchi, and Yevgeny Seldin. \u201cNonstochastic multiarmed bandits with unrestricted delays\u201d. In: Advances in Neural Information Processing Systems 32 (2019).   \n[20] Han Wu and Stefan Wager. \u201cThompson sampling with unrestricted delays\u201d. In: Proceedings of the 23rd ACM Conference on Economics and Computation. 2022, pp. 937\u2013 955.   \n[21] Tiancheng Jin et al. \u201cNear-optimal regret for adversarial mdp with delayed bandit feedback\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 33469\u201333481.   \n[22] Julian Zimmert and Yevgeny Seldin. \u201cAn optimal algorithm for adversarial bandits with arbitrary delays\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 3285\u20133294.   \n[23] Saeed Masoudian, Julian Zimmert, and Yevgeny Seldin. \u201cA Best-of-both-worlds Algorithm for Bandits with Delayed Feedback with Robustness to Excessive Delays\u201d. In: ICML 2024 Workshop: Foundations of Reinforcement Learning and Control\u2013Connections and Per 2024.   \n[24] Alekh Agarwal et al. \u201cContextual bandit learning with predictable rewards\u201d. In: Artificial Intelligence and Statistics. PMLR. 2012, pp. 19\u201326.   \n[25] Simon Du et al. \u201cBilinear classes: A structural framework for provable generalization in rl\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 2826\u20132836.   \n[26] Dylan J Foster et al. \u201cThe statistical complexity of interactive decision making\u201d. In: arXiv preprint arXiv:2112.13487 (2021).   \n[27] Robert M Corless et al. \u201cOn the Lambert W function\u201d. In: Advances in Computational mathematics 5 (1996), pp. 329\u2013359.   \n[28] Matteo Papini et al. \u201cLeveraging good representations in linear contextual bandits\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 8371\u20138380.   \n[29] Enoch Hyunwook Kang and PR Kumar. \u201cBounded (o (1)) regret recommendation learning via synthetic controls oracle\u201d. In: 2023 59th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE. 2023, pp. 1\u20137.   \n[30] Andrea Zanette et al. \u201cLearning near optimal policies with low inherent bellman error\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 10978\u201310989.   \n[31] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[32] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999. [33] Sergio Verd\u00fa. \u201cTotal variation distance and the distribution of relative information\u201d. In: 2014 Information Theory and Applications Workshop (ITA). IEEE. 2014, pp. 1\u20133. ", "page_idx": 11}, {"type": "text", "text": "A Special Cases of DMSO Framework ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In finite-armed bandit problems, each round is an arm pull. $\\Pi$ is the arm space, and $\\mathcal{R}$ is the space of rewards from arms. Since there is no observation space $\\scriptscriptstyle\\mathcal{O}$ , the model class degenerates to $\\mathcal{F}\\subseteq(\\Pi\\mapsto\\Delta_{\\mathcal{R}})$ .   \n- In contextual bandit problems, each round is an arm pull. $\\Pi$ is the set $\\mathcal{X}\\mapsto A$ ) of all policies, where $\\mathcal{X}$ is the context space and the $\\boldsymbol{\\mathcal{A}}$ is the arm space. The reward space $\\mathcal{R}$ is the space of rewards from arms. The observation space $\\scriptscriptstyle\\mathcal{O}$ is $\\mathcal{X}$ , where the $k$ th round\u2019s observation $o_{k}\\,\\in\\,\\mathcal{O}$ (which results from $\\pi_{k}$ ) is the $k+1$ th round\u2019s context. Since the future context arrival is not affected by previous decisions, the model class degenerates to $\\mathcal{F}\\subseteq(\\Pi\\mapsto\\Delta_{\\mathcal{R}})$ .   \n- In episodic reinforcement learning problems, each round is an episode. $\\Pi$ is the set $(S\\mapsto A)$ ) of all policies, where $\\boldsymbol{S}$ is the space of all possible states and $\\boldsymbol{\\mathcal{A}}$ is the action space. The reward space $\\mathcal{R}$ is the space of value functions at each initial state, and the observation space $\\scriptscriptstyle\\mathcal{O}$ is the set of all possible sequences of action choices, state transitions, and received rewards in one episode. The model class $\\mathcal{F}$ is characterized jointly by the initial state distribution and the transition kernel, which are shared across all the episodes. ", "page_idx": 12}, {"type": "text", "text": "B Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Recall that $P_{f,n,A}[\\cdot]$ denotes the distribution of outcomes of algorithm $A$ on the true model instance $f$ by the round $n$ . We further denote the marginal distribution of $P_{f,n,A}[\\cdot]$ in terms of decision $\\pi$ \u2019s rewards and outcomes by $P_{f,n,A}^{\\pi}[\\cdot]$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. Suppose that the ground-truth model is $f\\,\\in\\,{\\mathcal{F}}$ . Then a consistent algorithm must satisfy $\\begin{array}{r}{(1+o(1))\\ln n\\leq\\sum_{\\pi\\in\\Pi}D_{\\mathrm{KL}}\\left(P_{f,n,A}^{\\pi}\\|P_{g,n,A}^{\\pi}\\right)\\!.}\\end{array}$ for $g\\in\\mathcal{F}(f)^{c}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. According to Dong and Ma (2022) [2], any consistent algorithm $A$ must satisfy $\\left(1\\right.+$ $o(1)\\dot{)}\\ln n\\,\\leq\\,\\bar{D_{\\mathrm{KL}}}\\bar{(P_{f,n,A}||\\bar{P}_{g,n,A})}$ for $g\\,\\in\\,{\\mathcal{F}}(f)^{c}$ . Since the terms involving $A$ (the algorithm used to collect the data) cancel out and the outcomes of decisions are independent of each other, $\\begin{array}{r}{{\\frac{P_{f,n,A}}{P_{g,n,A}}}\\,=\\,\\prod_{\\pi\\in\\Pi}{\\frac{P_{f,n,A}^{\\pi}}{P_{g,n,A}^{\\pi}}}}\\end{array}$ PP  f\u03c0\u03c0,n,A holds. Therefore, the condition of Dong and Ma (2022) becomes (1 + $\\begin{array}{r}{o(1))\\ln n\\leq\\sum_{\\pi\\in\\Pi}D_{\\mathrm{KL}}\\left(P_{f,n,A}^{\\pi}\\|P_{g,n,A}^{\\pi}\\right)\\,\\mathrm{for}\\,g\\in\\mathcal{F}(f)^{c}.}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Lemma B.2. For any $\\epsilon>0$ , $\\epsilon$ -contamination in the delay model of $\\pi^{\\star}$ makes the rewards of decisions $\\pi\\neq\\pi^{\\star}$ suffer $\\delta$ -contamination for some $\\delta>0$ under consistency. ", "page_idx": 12}, {"type": "text", "text": "See Section B.1 for the proof of Lemma B.2. ", "page_idx": 12}, {"type": "text", "text": "Lemma B.3 shows that, under $\\delta$ -reward contaminations in reward distributions of all $\\pi\\;\\neq\\;\\pi^{\\star}$ , choosing optimal decision $\\pi^{\\star}$ alone must be enough to satisfy the condition described in Lemma B.1 and otherwise, we cannot satisfy it. ", "page_idx": 12}, {"type": "text", "text": "Lemma B.3. If the rewards of decisions $\\pi\\,\\in\\,\\Pi\\setminus\\pi_{f}$ suffer $\\delta$ -contamination for some $\\delta\\ >\\ 0$ , consistency of the algorithm requires $\\left(1+o(1)\\right)\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)$ for all $g\\in\\mathcal{F}(f)^{c}$ . ", "page_idx": 12}, {"type": "text", "text": "See Section B.2 for the proof of Lemma B.3. ", "page_idx": 12}, {"type": "text", "text": "The rest of the proof of Theorem 4.1 is immediate from the derivation of [2]\u2019s Theorem 2.3, which is as follows: from the chain rule of divergence, $D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi}||P_{g,n}^{\\pi}\\right)=\\mathbb{E}_{f,n}\\left[N_{\\pi}\\right]D_{\\mathrm{KL}}(f_{\\pi}||g_{\\pi})$ holds for $\\pi\\in\\Pi$ . Defining $w_{\\pi}:=\\mathbb{E}_{f}\\left[N_{\\pi}\\right]/((1+o(1))\\ln\\grave{n})$ , Lemma B.3 implies that $\\boldsymbol{\\mathcal{C}}(f)=0$ by the definition of $\\mathcal{C}(f)$ in the equation (3). Since we don\u2019t know the ground truth $f$ a priori, designing a learning system that assures the existence of a robust algorithm requires $\\boldsymbol{\\mathcal{C}}(f)=0$ for all $f\\in\\mathcal F$ . ", "page_idx": 12}, {"type": "text", "text": "B.1 Proof of Lemma B.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Denote by $N_{\\pi}^{[a,b]}$ the random variable that counts the number of decisions of $\\pi$ between rounds $a$ and $b$ . For consistency, for any small enough $p>0$ , for any $r>0$ , for some $m$ , there must exist a constant $n_{r,p}$ such that for all intervals $[a,b]$ with $b-a\\ge n_{r,p}$ and $a,b>m$ $>m,E[N_{\\pi^{\\star}}^{[a,b]}]\\geq(b-a)-(b-a)^{p}r$ holds. Recall that we denote by $D_{\\pi}$ the true delay model for the decision $\\pi\\in\\Pi$ , and by $\\hat{D}_{\\pi}$ the given model for $D_{\\pi}$ . Note that $\\epsilon$ -contamination means we can arbitrarily choose $\\{D_{\\pi}\\}_{\\pi\\in\\Pi}$ as long as $d_{T V}(D_{\\pi},\\hat{D}_{\\pi})\\leq\\epsilon.$ . Consider the case when $D_{\\pi^{\\star}}=\\hat{D_{\\pi^{\\star}}}+\\epsilon D_{a}-\\epsilon D_{c}$ where $\\begin{array}{r}{P(D_{a}=k)=\\frac{1}{n_{r,p}}}\\end{array}$ for $0\\,\\leq\\,k\\,\\leq\\,n_{r,p}\\,-\\,1$ and 0 for elsewhere, and $D_{c}$ is an arbitrary distribution. For $\\pi\\in\\Pi\\setminus\\pi^{\\star}$ , consider $D_{\\pi}=\\hat{D_{\\pi}}$ . Then for $k\\geq\\operatorname*{max}(n_{r,p},m)$ , ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\{\\land\\mathrm{~reward~arival~at~}\\mathrm{from~}\\pi^{*}\\})}\\\\ &{\\quad=\\frac{\\sum_{i=1}^{k}P\\left(\\{\\pi_{i}^{*}\\mathrm{~sreward~arrives~at~}k\\mathrm{~and~}\\pi_{i}\\neq\\pi^{*}\\}\\right)}{\\sum_{i=1}^{k}P\\left(\\{\\pi_{i}^{*}\\mathrm{~sreward~arrives~at~}k\\}\\right)}}\\\\ &{\\quad\\leq\\frac{|\\Pi|}{|\\Pi|-1+\\sum_{i=1}^{k}P\\left(\\{\\pi_{i}^{*}\\mathrm{~sreward~arrives~at~}k\\mathrm{~and~}\\pi_{i}=\\pi^{*}\\}\\right)}}\\\\ &{\\quad\\leq\\frac{|\\Pi|-1}{|\\Pi|-1+\\frac{\\epsilon}{n_{\\pi,p}}\\sum_{i=k-n_{r,p}}^{k}\\mathbb{E}\\left[1_{\\pi_{i}=\\pi^{*}}\\right]}}\\\\ &{\\quad\\leq\\frac{|\\Pi|-1}{|\\Pi|-1+\\frac{\\epsilon}{n_{\\pi,p}}(n_{r,p}-n_{r,p}^{p}r)}=\\frac{|\\Pi|-1}{|\\Pi|-1+\\epsilon(1-n_{r,p}^{p-1}r)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the inequality in the equation (4) follows from the fact that the delay distribution of each decision sums to one. Therefore, $P(\\{\\mathbf A$ reward arrival at $k$ is $\\begin{array}{r}{\\mathrm{~\\rom~}\\pi^{\\star}\\})>\\delta:=\\frac{\\epsilon(1-n_{r,p}^{p-1}r)}{|\\Pi|-1+\\epsilon(1-n_{r,p}^{p-1}r)}.}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Denote the rewards distributions associated with $D_{\\pi^{\\star}},\\hat{D_{\\pi^{\\star}}},D_{a}$ , and $D_{c}$ as $R_{\\pi^{\\star}},\\hat{R_{\\pi^{\\star}}},R_{a}$ , and $R_{c}$ each. Then ${\\cal R}_{\\pi^{\\star}}=\\hat{\\cal R}_{\\pi^{\\star}}+\\epsilon{\\cal R}_{a}-\\epsilon{\\cal R}_{c}$ must hold, where the mean of $R_{\\pi^{\\star}}$ and $R_{\\pi^{\\star}}$ are supposed to be the same. Since the choice of $R_{a}$ can be arbitrary by choosing $R_{c}$ accordingly, we can conclude that the reward distributions of decisions $\\pi\\neq\\pi^{\\star}$ indeed suffer $\\delta$ -contamination. ", "page_idx": 13}, {"type": "text", "text": "B.2 Proof of Lemma B.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that we use $f_{\\pi}$ to refer to an element of ${\\mathcal{F}}_{\\pi}$ , while $f_{\\pi}$ also denotes the $\\pi$ -coordinate of some $f\\in\\mathcal F$ . Suppose that the ground-truth model is $f\\in\\mathcal F$ . For $g\\in\\mathcal{F}(f)^{c}$ , a consistent algorithm must satisfy ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1+o(1))\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}\\|P_{g,n}\\right)=\\displaystyle\\sum_{\\pi\\in\\Pi}D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi}\\|P_{g,n}^{\\pi}\\right).}\\\\ &{\\quad=D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)+\\displaystyle\\sum_{\\pi\\in\\Pi\\backslash\\pi_{f}}\\mathbb{E}_{f,n}\\left[N_{\\pi}\\right]D_{\\mathrm{KL}}(f_{\\pi}(r,o)\\|g_{\\pi}(r,o))}\\\\ &{\\quad=D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)+\\displaystyle\\sum_{\\pi\\in\\Pi\\backslash\\pi_{f}}\\mathbb{E}_{f,n}\\left[N_{\\pi}\\right]\\left(D_{\\mathrm{KL}}(f_{\\pi}^{\\sigma}(o)\\|g_{\\pi}^{\\sigma}(o))+\\mathbb{E}_{f_{\\pi}}\\left[\\log\\frac{f_{\\pi}(r\\vert o)}{g_{\\pi}(r\\vert o)}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Above, ", "page_idx": 13}, {"type": "text", "text": "\u2022 The inequality in the equation (5) is from Dong and Ma (2022) [2]. \u2022 The equality in the equation (5) follows from the fact that algorithm-related terms cancel out. \u2022 The inequality in the equation (6) is from the Divergence decomposition Lemma [31] \u2022 The inequality in the equation (7) follows from the chain rule of KL divergence [32]. ", "page_idx": 13}, {"type": "text", "text": "Let $\\operatorname*{min}(q(\\delta),r_{o})=q^{\\prime}(\\delta)$ , where $r_{o}$ is defined as in Assumption 4.5. By Assumption 4.4, for every $\\pi\\neq\\pi_{f}$ , there exists a non-empty set $\\mathcal{E}_{\\pi}:=\\left\\{l_{\\pi}\\in\\mathcal{F}_{\\pi}\\ |\\ |E_{f_{\\pi}}\\bar{[r|}o]-E_{l_{\\pi}}\\bar{[r|}o]|\\leq q^{\\bar{\\prime}}(\\delta)\\ a.e.\\right\\}\\cap\\{\\mu_{l_{\\pi}}\\overset{\\cdot}{\\geq}$ $\\mu_{f_{\\pi_{f}}}\\}$ . Hense we can construct $\\mathcal{E}(\\pi)\\,:=\\,\\{l\\,\\in\\,\\mathcal{F}\\,\\mid\\,l_{\\pi}\\,\\in\\,\\mathcal{E}_{\\pi},l_{\\pi^{\\prime}}\\,=\\,f_{\\pi^{\\prime}}$ for $\\pi^{\\prime}\\neq\\pi\\}$ }. Note that $\\mathcal{E}(\\bar{\\pi})\\subseteq\\mathcal{F}(f)^{c}:=\\{g\\in\\mathcal{F}\\mid\\pi_{g}\\neq\\pi_{f}\\}=\\{g\\in\\mathcal{F}\\mid\\exists\\pi\\in\\Pi\\ s.t.$ $\\mu_{g_{\\pi}}\\geq\\mu_{f_{\\pi_{f}}}\\}$ . Therefore, for every ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\pi\\neq\\pi_{f},}\\\\ &{(1+o(1))\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)+\\displaystyle\\sum_{\\pi\\in\\Pi\\backslash\\pi_{f}}\\mathbb{E}_{f,n}\\left[N_{\\pi}(n)\\right]\\mathbb{E}_{f_{\\pi}}\\left[\\log\\frac{f_{\\pi}(r|\\sigma)}{g_{\\pi}(r|\\sigma)}\\right]\\mathrm{~for~}g\\in\\mathcal{E}(\\pi)}\\\\ &{\\quad(\\Rightarrow)\\,\\left(1+o(1)\\right)\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)+\\mathbb{E}_{f,n}\\left[N_{\\pi}(n)\\right]\\mathbb{E}_{f_{\\pi}}\\left[\\log\\frac{f_{\\pi}(r|\\sigma)}{g_{\\pi}(r|\\sigma)}\\right]\\mathrm{~for~}g\\in\\mathcal{E}(\\pi)}\\\\ &{\\quad(\\Rightarrow)\\,\\left(1+o(1)\\right)\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)\\mathrm{~for~}g\\in\\mathcal{E}(\\pi)}\\\\ &{\\quad(\\Rightarrow)\\,\\left(1+o(1)\\right)\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)\\mathrm{~for~}g\\in\\mathcal{E}^{\\prime}(\\pi)}\\\\ &{\\quad(\\Rightarrow)\\,\\left(1+o(1)\\right)\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)\\mathrm{~for~}g\\in\\mathcal{E}^{\\prime}(\\pi)}\\\\ &{\\quad(\\Rightarrow)\\,\\left(1+o(1)\\right)\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)\\mathrm{~for~}g\\in\\left\\{g\\in\\mathcal{F}\\,\\big|\\,\\mu_{g_{\\pi}}\\geq\\mu_{f_{\\pi}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{E}_{\\pi}^{\\prime}\\,:=\\,\\{l_{\\pi}\\,\\in\\,\\mathcal{F}_{\\pi}\\,\\mid\\,\\mu_{f_{\\pi_{f}}}\\,\\le\\,\\mu_{l_{\\pi}}\\}$ and $\\mathcal{E}^{\\prime}(\\pi)\\,:=\\,\\{l\\,\\in\\mathcal{F}\\mid l_{\\pi}\\,\\in\\,\\mathcal{E}_{\\pi}^{\\prime},l_{\\pi^{\\prime}}\\,=\\,f_{\\pi^{\\prime}}$ for $\\pi^{\\prime}\\neq\\pi$ }. Above, ", "page_idx": 14}, {"type": "text", "text": "\u2022 Equation (8) follows from Assumption 4.5 and equation (7).   \n\u2022 The logical implication in equation (9) follows from the definition of $\\mathcal{E}(\\pi)$ .   \n\u2022 The logical implication in equation (10) follows from Assumptions 4.2, 4.3 and 4.4: $|E_{f_{\\pi}}[\\bar{r|}o]-\\bar{E_{g_{\\pi}}}[r|o]|\\,\\le\\,q^{\\prime}\\bar{(\\delta)}\\,\\le\\,q(\\delta)$ a.e. implies $d_{T V}(f_{\\pi}(\\cdot\\ |\\ o),g_{\\pi}(\\cdot\\ |\\ o))\\,<\\,\\delta$ a.e. from Assumption 4.2; therefore, under some $\\delta$ -contamination of $f_{\\pi}(\\cdot\\ \\mid\\ o)$ , the contaminated $\\bar{E_{f_{\\pi}}}[r|o]$ can be farther from the true $E_{f_{\\pi}}[r|o]$ than $E_{g_{\\pi}}[r|o]$ . Therefore, $D_{\\mathrm{KL}}\\left(f_{\\pi}(\\cdot\\mid o)\\parallel g_{\\pi}(\\cdot\\mid o)\\right)\\leq0$ a.e. due to Assumption 4.3, and so $\\begin{array}{r l}{\\mathbb{E}_{f_{\\pi}}\\left[\\log\\frac{f_{\\pi}(r|o)}{g_{\\pi}(r|o)}\\right]=}&{{}}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}_{f_{\\pi}^{o}}\\left[\\mathbb{E}_{f_{\\pi}(r\\mid o)}\\left[\\log\\frac{f_{\\pi}(r\\mid o)}{g_{\\pi}(r\\mid o)}\\right]\\right]=\\mathbb{E}_{f_{\\pi}^{o}}\\left[D_{\\mathrm{KL}}\\left(f_{\\pi}(\\cdot\\mid o)\\parallel g_{\\pi}(\\cdot\\mid o)\\right)\\right]\\le0.}\\end{array}$   \n\u2022 The logical implication in equation (11) follows from Assumption 4.3: Define ${\\hat{\\mathcal{E}}}_{\\pi}:=\\{|\\mu_{f_{\\pi}}-\\mu_{g_{\\pi}}|\\leq q^{\\prime}(\\delta)\\}\\cap\\{\\mu_{g_{\\pi}}\\geq\\mu_{f_{\\mu_{f}}}\\}$ and $\\bar{\\mathcal{E}}(\\bar{\\pi}):=\\{l\\in\\mathcal{F}\\mid l_{\\pi}\\in\\hat{\\mathcal{E}}_{\\pi},l_{\\pi^{\\prime}}=$ $f_{\\pi^{\\prime}}$ for $\\pi^{\\prime}\\neq\\pi\\}$ . Note that $\\hat{\\mathcal{E}}(\\pi)\\,\\subseteq\\,\\mathcal{E}(\\pi)$ . Then for $g^{\\prime}\\,\\in\\,\\mathcal{E}^{\\prime}(\\pi)\\setminus\\hat{\\mathcal{E}}(\\pi)$ and $g\\,\\in\\,{\\hat{\\mathcal{E}}}(\\pi)$ with $(\\mu_{g_{\\pi}}-\\mu_{f_{\\pi}})(\\mu_{g_{\\pi}^{\\prime}}-\\mu_{f_{\\pi}})\\geq0$ , $D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi}||P_{g,n}^{\\pi}\\right)\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi}||P_{g^{\\prime},n}^{\\pi}\\right)$ due to the monotonicity assumption of Assumption 4.3.   \n\u2022 The logical implication in equation (12) follows from the fact that any element in $\\{g\\in{\\mathcal{F}}\\,|\\,$ $\\mu_{g_{\\pi}}\\geq\\mu_{f_{\\pi_{f}}}\\}$ has an element in ${\\mathcal{E}}^{\\prime}$ that is strictly closer to $f$ . ", "page_idx": 14}, {"type": "text", "text": "Since $\\mathcal{F}(f)^{c}:=\\{g\\in\\mathcal{F}\\;|\\;\\pi_{g}\\neq\\pi_{f}\\}=\\{g\\in\\mathcal{F}\\;|\\;\\exists\\pi\\in\\Pi\\;s.t.\\;\\mu_{g_{\\pi}}\\geq\\mu_{f_{\\pi_{f}}}\\}$ , we immediately get $\\left(1+o(1)\\right)\\ln n\\leq D_{\\mathrm{KL}}\\left(P_{f,n}^{\\pi_{f}}\\|P_{g,n}^{\\pi_{f}}\\right)$ for $g\\in\\mathcal{F}(f)^{c}$ . ", "page_idx": 14}, {"type": "text", "text": "C Proof of Lemma 4.9, Lemma 4.10 and Theorem 4.11 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Lemma 4.9 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma C.1 (Upper bounding lemma). $\\begin{array}{r}{k=\\frac{4c^{4}e^{-2}}{\\mathcal{W}(2c^{2})^{2}}}\\end{array}$ satisfies $e^{-{\\frac{n(\\ln n)^{2}}{2c^{2}}}}\\leq k{\\frac{1}{n^{2}}}$ for all $n\\geq1$ , where $\\mathcal{W}$ is the principal branch Lambert W function $I27J$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y=x^{2}\\cdot e^{-\\frac{x+x^{2}}{2}}}\\\\ &{\\quad=x^{2}\\cdot e^{-f(x)}\\ (\\ \\mathrm{by}\\ \\mathrm{sting}\\ f(x)=\\frac{x(\\mathrm{Br}\\cdot x)^{2}}{2c^{2}}}\\\\ &{\\frac{d y}{d x}=\\frac{d}{d x}\\left(z^{2}\\right)\\cdot e^{-f(x)}+x^{2}\\cdot\\frac{d}{d x}\\left(e^{-f(x)}\\right)}\\\\ &{=2x\\cdot e^{-f(x)}+x^{2}\\cdot e^{-f(x)}\\left(-\\left(\\frac{(\\mathrm{Br}\\cdot x)^{2}}{2c^{2}}+\\frac{\\mathrm{Br}}{c^{2}}\\right)\\right)}\\\\ &{\\quad\\cdot\\frac{d}{d x}(e^{-f(x)})=e^{-f(x)}\\cdot(-f(x))\\ \\mathrm{and}}\\\\ &{\\quad\\cdot\\left(\\cdot\\frac{d}{d x}e^{-f(x)}\\right)=\\frac{-f^{\\prime}}{2c^{2}}\\left(\\ \\mathrm{(nn}\\lambda^{2}+2x\\mathrm{In}\\cdot\\frac{1}{x}\\right)=\\frac{(\\mathrm{Br}\\cdot x)^{2}}{2c^{2}}+\\frac{\\mathrm{In}\\,x}{c^{2}}\\right)}\\\\ &{=2x e^{-f(x)}-x^{2}e^{-f(x)}\\left(\\ \\frac{(\\mathrm{Br}\\cdot x)^{2}}{2c^{2}}+\\frac{\\mathrm{Br}}{c^{2}}\\right)}\\\\ &{=e^{-f(x)}\\left(2x-x^{2}\\left(\\ \\frac{(\\mathrm{Br}\\cdot x)^{2}}{2c^{2}}+\\frac{\\mathrm{Br}}{c^{2}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Set the derivative to zero to find the critical points: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{2x-x^{2}\\left({\\cfrac{(\\ln x)^{2}}{2c^{2}}}+{\\cfrac{\\ln x}{c^{2}}}\\right)=0}\\\\ {2=x\\left({\\cfrac{(\\ln x)^{2}}{2c^{2}}}+{\\cfrac{\\ln x}{c^{2}}}\\right){\\mathrm{~if~}}x>0}\\\\ {4c^{2}=x(\\ln x)^{2}+2x\\ln x}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is the uni-modal function with a maximum, as the derivatives are positive on the left side of the critical point and negative on the right side of the critical point. Also, note that $x_{\\mathrm{max}}\\ln x_{\\mathrm{max}}\\leq2c^{2}$ , as $x(\\ln x)^{2}\\geq0$ . This implies that $\\begin{array}{r}{\\bar{x}_{\\mathrm{max}}\\leq\\frac{2c^{2}}{\\mathcal{W}(2c^{2})}}\\end{array}$ , where $\\mathcal{W}$ denote the principal branch Lambert function. Now note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{\\operatorname*{max}}\\,=\\,x_{\\operatorname*{max}}^{2}\\cdot e^{-\\frac{x_{\\operatorname*{max}}\\,(\\ln x_{\\operatorname*{max}})^{2}}{2c^{2}}}}\\\\ &{\\qquad=x_{\\operatorname*{max}}^{2}\\cdot e^{-\\left(2+\\frac{x_{\\operatorname*{max}}}{c^{2}}\\ln x_{\\operatorname*{max}}\\right)}}\\\\ &{\\qquad\\leq e^{-2}\\cdot x_{\\operatorname*{max}}^{2}}\\\\ &{\\qquad\\leq e^{-2}\\left(\\frac{2c^{2}}{\\mathcal{W}\\left(2c^{2}\\right)}\\right)^{2}=\\frac{4c^{4}e^{-2}}{\\mathcal{W}\\left(2c^{2}\\right)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{k=\\frac{4c^{4}e^{-2}}{\\mathcal{W}(2c^{2})^{2}}}\\end{array}$ satisfies $e^{-{\\frac{n(\\ln n)^{2}}{2c^{2}}}}\\leq k{\\frac{1}{n^{2}}}$ for all $n\\geq1$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4.9. Let $f$ be the ground truth model. After each of $\\hat{f}$ \u2019s transition to $g\\in\\mathcal{F}(f)^{c}$ , at period $n$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\{\\displaystyle\\sum_{k=1}^{n}\\ln\\frac{f_{\\pi_{g}}}{g_{\\pi_{g}}}(k)}\\leq2\\ln n+\\displaystyle\\sum_{k=1}^{n}\\frac{2}{\\sqrt{\\beta}}\\delta_{\\pi_{g}}^{\\operatorname*{max}}(k)\\})}\\\\ &{\\quad=P(\\{\\displaystyle\\sum_{k=1}^{n}(\\ln\\frac{f_{\\pi_{g}}}{g_{\\pi_{g}}}+\\ln\\frac{f_{\\pi_{g}}^{c}}{f_{\\pi_{g}}}(k)}+\\ln\\frac{g_{\\pi_{g}}}{g_{\\pi_{g}}^{c}(k)})\\leq2\\ln n+\\displaystyle\\sum_{k=1}^{n}\\frac{2}{\\sqrt{\\beta}}\\delta_{\\pi_{g}}^{\\operatorname*{max}}(k)\\})}\\\\ &{\\quad\\leq P(\\{\\displaystyle\\sum_{k=1}^{n}(\\ln\\frac{f_{\\pi_{g}}}{g_{\\pi_{g}}}+\\ln\\frac{f_{\\pi_{g}}^{c}}{f_{\\pi_{g}}}+\\ln\\frac{g_{\\pi_{g}}}{g_{\\pi_{g}}^{c}(k)})\\leq2\\ln n+\\displaystyle\\frac{2C}{\\sqrt{\\beta}}\\ln n\\})\\mathrm{~for~some~}C}\\\\ &{\\quad\\leq P(\\{\\displaystyle\\sum_{k=1}^{n}\\left(\\ln\\frac{f_{\\pi_{g}}}{g_{\\pi_{g}}}-D_{K L}(f_{\\pi_{g}},g_{\\pi_{g}})\\right)+\\displaystyle\\sum_{k=1}^{n}\\left(\\ln\\frac{f_{\\pi_{g}}^{c}(k)}{f_{\\pi_{g}}}\\right)+\\sum_{k=1}^{n}\\left(\\ln\\frac{g_{\\pi_{g}}}{g_{\\pi_{g}}^{c}(k)}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S}P(\\|\\displaystyle\\sum_{j=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\mathcal{F}_{j}}-D_{K\\ell}(J_{n},J_{n})\\right)+\\displaystyle\\sum_{l=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}(k)}{\\int_{n}}-D_{K\\ell}(J_{n},J_{n}^{\\top}(k))\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{l=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\mathcal{F}_{j}}-D_{K\\ell}(J_{n},\\cdot_{\\ell},\\cdot_{\\ell}(k))\\right)\\leq-\\ln m\\Big)\\mathrm{~for~}\\ge\\ln\\frac{(1)}{m}}\\\\ &{\\leq P(\\|\\displaystyle\\sum_{j=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\mathcal{F}_{j}}-D_{K\\ell}(J_{n},\\cdot_{\\ell},\\cdot_{\\ell})\\right)\\geq-\\ln m\\sum_{k=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\int_{n}}-D_{K\\ell}(J_{n},\\cdot_{\\ell}(k))\\right)}\\\\ &{\\qquad\\geq-\\ln m\\sum_{k=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\mathcal{F}_{j}}-D_{K\\ell}(J_{n},\\cdot_{\\ell},\\cdot_{\\ell}(k))\\right)\\geq-\\ln m\\Big)\\mathrm{~for~}\\geq m_{k}}\\\\ &{=P(\\|\\displaystyle\\sum_{j=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\mathcal{F}_{j}}-D_{K\\ell}(J_{n},\\cdot_{\\ell},\\cdot_{\\ell})\\right)\\leq-\\ln m)\\cup\\displaystyle\\sum_{l=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}(k)}{\\int_{n}}-D_{K\\ell}(J_{n},\\cdot_{\\ell},\\cdot_{\\ell}(k))\\right)}\\\\ &{\\qquad\\leq-\\ln m\\Big)\\left(\\bigcup_{j=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\mathcal{F}_{j}}-D_{K\\ell}(J_{n},\\cdot_{\\ell},\\cdot_{\\ell})\\right)+2\\ln m\\right)\\mathrm{~for~}\\geq m_{k}}\\\\ &{\\qquad\\leq-\\ln m\\Big)\\left(\\bigcup_{j=1}^{m}\\left(\\ln\\frac{\\mathcal{F}_{j}}{\\mathcal{F}_{j}}-D_{K\\ell}(J_{n},\\cdot\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Above, ", "page_idx": 16}, {"type": "text", "text": "\u2022 Equation 17 follows from the fact that $\\delta_{\\pi_{g}}^{\\mathrm{max}}(k)$ decreases with the rate $1/n$ .   \n\u2022 Equation 18 follows from the fact that $n D_{K L}(f_{\\pi_{g}},g_{\\pi_{g}})\\;=\\;D_{K L}(P_{f,n,\\overline{{{\\pi_{g}}}}}||P_{g,n,\\overline{{{\\pi_{g}}}}})\\;=$ $\\omega(\\ln n)$ from the Assumption 4.7.   \n\u2022 Equation 19 follows from the fact that substracting positive value on the left does not change the inequality.   \n\u2022 Equation 20 follows from the fact that the log-likelihood ratios are bounded by constant $c$ due to Assumption 4.8, and thus sub-gaussian random variables with $\\textstyle\\sigma^{2}={\\frac{c^{2}}{4}}$ .   \n\u2022 Equation 21 is from Lemma C.1. ", "page_idx": 16}, {"type": "text", "text": "Therefore, after each time a bad transition to $g\\,\\in\\,{\\mathcal{F}}(f)^{c}$ happens, the event $\\begin{array}{r}{\\{\\sum_{k=1}^{n}\\ln\\frac{f_{\\pi_{g}}^{c}(k)}{g_{\\pi_{g}}^{c}(k)}~\\le}\\end{array}$ $\\begin{array}{r}{2\\ln n\\!+\\!\\sum_{k=1}^{n}\\frac{2}{\\sqrt{\\beta}}\\delta_{\\pi_{g}}^{\\operatorname*{max}}(k)\\}}\\end{array}$ happens only finite many times (more precisely, smaller than $3\\cdot\\frac{4c^{4}e^{-2}}{\\mathcal{W}(2c^{2})^{2}}$ $\\begin{array}{r}{\\frac{\\pi^{2}}{6}=\\frac{2c^{4}e^{-2}\\pi^{2}}{\\mathcal{W}(2c^{2})^{2}})}\\end{array}$ in expectation by the Borel-Cantelli lemma, which implies that the inference will arrive at the correct instance within finite expected time. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C.2 Proof of Lemma 4.10 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 4.10. Suppose that $f$ is the ground truth model. For any $g\\in{\\mathcal{F}}\\backslash f^{\\star}$ , when ${\\hat{f}}=f$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n(n-1)}\\displaystyle\\sum_{p_{j}\\in\\mathbb{N}_{J}}\\frac{\\sigma_{p_{j}(\\cdot,\\cdot,\\cdot,\\cdot,\\xi)}}{\\sigma_{p_{j}(\\cdot,\\cdot,\\cdot,\\xi)}}\\ge2n\\sigma_{j}^{-1}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }-\\sigma_{\\sigma_{\\sigma_{\\sigma_{i}}}}\\\\ &{=F(\\sum{\\frac{\\cdot{\\rho},\\cdot\\cdot,\\cdot}{\\frac{\\rho}{\\rho}}})\\Big(u_{{{\\frac{\\rho}{\\rho}{\\rho}}}}^{\\mathcal{L}}+\\frac{m_{{\\rho}_{\\sigma_{i}}}^{\\mathcal{L}}}{\\rho_{i}(\\cdot,\\cdot,\\cdot,\\xi)}+\\frac{m_{\\rho}^{2}}{\\rho_{i}(\\cdot,\\cdot,\\cdot,\\xi)})\\geq2n\\sigma_{i}+\\displaystyle\\sum_{s=1}^{n}\\sum_{i=1}^{2}g_{i}^{\\sigma_{i}(\\cdot,\\cdot,\\cdot,\\xi)}\\Big(s\\Big)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\displaystyle\\sum_{s=1}^{n}\\Big(u_{{\\frac{\\rho}{\\rho}}}^{\\mathcal{L}}\\xi_{i}^{(0)}-D_{K}(\\xi_{i}(\\cdot,\\cdot,\\cdot,\\phi_{u})\\xi_{s})\\Big)\\geq2n\\sigma_{i}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Above, ", "page_idx": 17}, {"type": "text", "text": "\u2022 Equation 22 follows from the reverse Pinsker\u2019s inequality [33] (total variation distance smaller than $\\delta$ implies KL divergence smaller than \u221a1\u03b2 \u03b4) \u2022 Equation 23 follows from Lemma 4.3 of Dong and Ma (2022) [2], which says that $\\begin{array}{r}{\\bar{P_{Q}}\\left(\\left\\{\\sum_{i=1}^{m}\\ln\\frac{P_{i}}{Q_{i}}\\geq c\\right\\}\\right)\\;\\leq\\;\\exp(-c)}\\end{array}$ , and the fact that the log-likelihood ratios are bounded due to Assumption 4.8, and thus sub-gaussian random variables. ", "page_idx": 17}, {"type": "text", "text": "Therefore, the event holds in total only for finite rounds of $k$ in expectation (more precisely, bounded by $\\begin{array}{r}{(1+\\frac{8c^{4}e^{-2}}{\\mathcal{W}(2c^{2})^{2}})\\frac{\\pi^{2}}{6})}\\end{array}$ by the Borel-Cantelli lemma. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C.3 Proof of Theorem 4.11 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Combining Lemmas 4.9 and 4.10, we can conclude that ${\\widehat{f}}\\notin{\\mathcal{F}}(f^{\\star})$ holds only for a finite number of rounds in expectation. That is, regret is bounded in exp ectation with value \u2206(1 + 5 W4c(42ec\u22122)22 ) ", "page_idx": 17}, {"type": "text", "text": "D Proof of Theorem 5.2 (Linear contextual bandit case) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $\\Theta$ be the set of all parameters, and let $\\theta^{\\star}\\in\\Theta$ be the unknown true parameter. Suppose that $\\mathcal{C}(\\theta)=0$ for $\\theta\\in\\Theta$ . By Theorem 2.3 and Theorem 5.1, $\\{\\phi_{m_{j\\theta}}(x_{j})\\mid j\\in\\dot{A}\\}$ spans $\\mathbb{R}^{d}$ for $\\theta\\in\\Theta$ . Denote $T_{\\mathbf{x}}(n)$ be the number of arrivals of context $\\mathbf{x}\\in\\mathcal{X}$ . ", "page_idx": 18}, {"type": "text", "text": "Then for any $\\tilde{\\theta}\\in\\Theta\\setminus\\{\\theta^{\\star}\\}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}\\left(P_{\\theta^{*},n,\\overline{{\\pi}}\\overline{{\\theta}}}\\lVert P_{\\bar{\\theta},n,\\overline{{\\pi}}\\overline{{\\theta}}}\\rangle=\\frac{1}{2}\\displaystyle\\sum_{x\\in A}\\mathbb{E}\\left[T_{x}(n)\\right]\\langle x,\\theta^{*}-\\widetilde{\\theta}\\rangle^{2}\\right.}\\\\ &{\\quad\\left.=\\frac{1}{2}(\\theta^{*}-\\widetilde{\\theta})^{\\top}\\mathbb{E}\\left[\\displaystyle\\sum_{x\\in A}T_{x}(n)x x^{\\top}\\right](\\theta^{*}-\\widetilde{\\theta})\\right.}\\\\ &{\\quad\\left.=\\frac{1}{2}(\\theta^{*}-\\widetilde{\\theta})^{\\top}n\\mathbb{E}\\left[\\displaystyle\\sum_{x\\in A}\\frac{T_{x}(n)}{n}x x^{\\top}\\right](\\theta^{*}-\\widetilde{\\theta})\\right.}\\\\ &{\\quad\\displaystyle\\geq\\frac{1}{2}\\lVert\\theta^{*}-\\widetilde{\\theta}\\rVert^{2}n\\lambda_{\\mathrm{min}}}\\\\ &{\\quad\\displaystyle=\\Omega(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Above, ", "page_idx": 18}, {"type": "text", "text": "\u2022 The equality in equation (24) is from the divergence decomposition lemma Lattimore and Szepesv\u00e1ri [31]   \n\u2022 $\\lambda_{\\mathrm{min}}$ of equation (25) denotes the smallest eigenvalue for $\\mathbb{E}_{\\mathbf{x}_{j}\\sim p}\\left[\\phi_{m_{j\\theta}}(\\mathbf{x}_{j})\\phi_{m_{j\\theta}}(\\mathbf{x}_{j})^{\\top}\\right]$   \n\u2022 The inequality of equation (25) is from the fact that $\\frac{x^{T}A x}{x^{T}x}$ is larger than the smallest eigenvalue of $A$ .   \n\u2022 The equality of equation (26) comes from the fact that $\\lambda_{\\operatorname*{min}}~>~0$ is equivalent to $\\{\\phi_{m_{j\\theta}}\\bar{(}x_{j})\\ \\vert\\ j\\in A\\}$ spanning $\\mathbb{R}^{d}$ [28]. ", "page_idx": 18}, {"type": "text", "text": "E Proof of Theorem 5.6 (Reinforcement learning with Linear MDP case) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "It is straightforward that the proof of Theorem 5.6 is almost equivalent to the proof of Theorem 5.2, except that the problem here is inferring $\\theta_{h}$ separately for each $h\\in[H]$ . ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a   \ncomplete (and correct) proof?   \nAnswer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?   \nAnswer: [NA]   \nJustification: Theory paper. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA]   \nJustification: Theory paper. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA]   \nJustification: Theory paper. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?   \nAnswer: [NA]   \nJustification: Theory paper. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?   \nAnswer: [NA]   \nJustification: Theory paper. ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?   \nAnswer: [NA]   \nJustification: Learning theory paper that is not bound to practical implications. ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Theory paper. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Theory paper. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?   \nAnswer: [NA]   \nJustification: Theory paper. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA]   \nJustification: Theory paper. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: Theory paper. ", "page_idx": 20}]