[{"figure_path": "7v88Fh6iSM/figures/figures_4_1.jpg", "caption": "Figure 1. Illustration of the posterior q(xt | y) for the Gaussian approximation q(x | xt) when the prior p(x) lies on a manifold. Ellipses represent 95% credible regions of q(x | xt). (A) With Et as heuristic for V[x | xt], any xt whose mean E[x | xt] is close to the plane y = Ax is considered likely. (B) With V[x | xt], more regions are correctly pruned. (C) Ground-truth p(xt | y) and p(x | xt) for reference.", "description": "This figure compares different approximations of the posterior distribution q(xt|y) when the prior p(x) is defined on a manifold.  It illustrates how using the true covariance V[x|xt] (panel B) leads to a more accurate posterior estimate compared to using only the mean E[x|xt] (panel A) or simple heuristics like \u03a3t for V[x|xt]. Panel C shows the ground truth posterior for comparison.", "section": "4.2 Moment Matching Posterior Sampling"}, {"figure_path": "7v88Fh6iSM/figures/figures_4_2.jpg", "caption": "Figure 2. Sinkhorn divergence [69] between the posteriors p(xt | y) and q(xt | y) for different heuristics of V[x | xt] when the prior p(x) lies on 1-d manifolds embedded in R\u00b3. Lines and shades represent the 25-50-75 percentiles for 64 randomly generated manifolds [71] and measurement matrices A \u2208 R1\u00d73. Using V[x | xt] instead of heuristics leads to orders of magnitude more accurate posteriors q(xt | y).", "description": "This figure shows the Sinkhorn divergence between the true posterior distribution and the approximated posterior distribution using different methods for estimating the covariance matrix.  It demonstrates that using the exact covariance (V[x|xt]) significantly improves the accuracy of the posterior approximation compared to using heuristics such as \u03a3t or (\u03a3\u22121t + \u03a3\u22121x)\u22121. The x-axis represents the diffusion coefficient \u03c3t, and the y-axis represents the Sinkhorn divergence.", "section": "4.2 Moment Matching Posterior Sampling"}, {"figure_path": "7v88Fh6iSM/figures/figures_5_1.jpg", "caption": "Figure 3. Illustration of 2-d marginals of the model qk(x) along the EM iterations. The initial Gaussian prior q0(x) leads to a very dispersed first model q1(x). The EM algorithm gradually prunes the density regions which are inconsistent with observations, until it reaches a stationary distribution. The marginals of the final distribution are close to the marginals of the ground-truth distribution.", "description": "This figure shows the evolution of the model's 2D marginals during the Expectation-Maximization (EM) algorithm's iterations.  The initial model is dispersed, but the EM algorithm refines it step by step, improving its consistency with the observations until reaching a stationary distribution resembling the true distribution.", "section": "5.1 Low-dimensional manifold"}, {"figure_path": "7v88Fh6iSM/figures/figures_6_1.jpg", "caption": "Figure 4. FID of q\u03b8k(x) along the EM iterations for the corrupted CIFAR-10 experiment.", "description": "This figure shows the Fr\u00e9chet Inception Distance (FID) scores over the Expectation-Maximization (EM) iterations for the corrupted CIFAR-10 experiment.  Different lines represent different corruption levels (25%, 50%, 75%) and different methods for approximating the posterior covariance (Tweedie's formula, (I+\u03a3t\u207b\u00b9)\u207b\u00b9, \u03a3t). The plot illustrates how the FID score (a measure of generated image quality) evolves as the model is trained using the EM algorithm.", "section": "5.2 Corrupted CIFAR-10"}, {"figure_path": "7v88Fh6iSM/figures/figures_6_2.jpg", "caption": "Figure 3. Illustration of 2-d marginals of the model qk(x) along the EM iterations. The initial Gaussian prior q0(x) leads to a very dispersed first model q1(x). The EM algorithm gradually prunes the density regions which are inconsistent with observations, until it reaches a stationary distribution. The marginals of the final distribution are close to the marginals of the ground-truth distribution.", "description": "This figure shows the evolution of the model's 2D marginal distributions throughout the Expectation-Maximization (EM) algorithm iterations. It starts with a dispersed initial Gaussian prior and gradually refines it by pruning inconsistent regions, converging towards the ground-truth distribution.", "section": "5.1 Low-dimensional manifold"}, {"figure_path": "7v88Fh6iSM/figures/figures_7_1.jpg", "caption": "Figure 6. Examples of posterior samples for accelerated MRI using a diffusion prior trained from k-space observations only. Posterior samples are detailed and present plausible variations, while remaining consistent with the observation. We provide the zero-filled inverse, where missing frequencies are set to zero, as baseline.", "description": "This figure shows examples of posterior samples generated for accelerated MRI using a diffusion prior.  The top row shows the k-space mask, the zero-filled reconstruction (baseline), and two samples generated by the proposed method, along with the ground truth.  The figure demonstrates the method's ability to produce detailed and plausible MRI reconstructions even with missing k-space data.", "section": "5.3 Accelerated MRI"}, {"figure_path": "7v88Fh6iSM/figures/figures_20_1.jpg", "caption": "Figure 7. 1-d and 2-d marginals of the ground-truth distribution p(x) used in the low-dimensional manifold experiment. The distribution lies on a random 1-dimensional manifold embedded in R5.", "description": "This figure shows the 1D and 2D marginal distributions of the ground truth distribution p(x) used in the low-dimensional manifold experiment of the paper.  The distribution is defined on a randomly generated 1-dimensional manifold embedded in a 5-dimensional space (R5). The plots visualize the probability density across different dimensions and pairs of dimensions of the latent variable x, illustrating its structure and distribution along the manifold.", "section": "5.1 Low-dimensional manifold"}, {"figure_path": "7v88Fh6iSM/figures/figures_21_1.jpg", "caption": "Figure 3. Illustration of 2-d marginals of the model qk(x) along the EM iterations. The initial Gaussian prior q0(x) leads to a very dispersed first model q1(x). The EM algorithm gradually prunes the density regions which are inconsistent with observations, until it reaches a stationary distribution. The marginals of the final distribution are close to the marginals of the ground-truth distribution.", "description": "This figure shows the evolution of the model's 2D marginal distributions during the Expectation-Maximization (EM) algorithm iterations.  The initial Gaussian prior is very broad, but the EM process refines it, gradually focusing on regions consistent with the observed data. The final distribution closely matches the ground truth.", "section": "5.1 Low-dimensional manifold"}, {"figure_path": "7v88Fh6iSM/figures/figures_21_2.jpg", "caption": "Figure 3. Illustration of 2-d marginals of the model qk(x) along the EM iterations. The initial Gaussian prior q0(x) leads to a very dispersed first model q1(x). The EM algorithm gradually prunes the density regions which are inconsistent with observations, until it reaches a stationary distribution. The marginals of the final distribution are close to the marginals of the ground-truth distribution.", "description": "This figure shows the evolution of the learned diffusion model's marginal distributions across different EM iterations.  Starting from a diffuse initial prior, the EM algorithm refines the model by focusing the probability mass onto regions that are consistent with the observed data. The final distribution closely matches the ground truth.", "section": "5.1 Low-dimensional manifold"}, {"figure_path": "7v88Fh6iSM/figures/figures_21_3.jpg", "caption": "Figure 3. Illustration of 2-d marginals of the model qk(x) along the EM iterations. The initial Gaussian prior q0(x) leads to a very dispersed first model q1(x). The EM algorithm gradually prunes the density regions which are inconsistent with observations, until it reaches a stationary distribution. The marginals of the final distribution are close to the marginals of the ground-truth distribution.", "description": "This figure shows the evolution of the model's 2D marginal distributions over 32 EM iterations.  Starting from a dispersed initial Gaussian prior, the EM algorithm refines the distribution, progressively removing inconsistencies with observed data.  The final distribution closely matches the ground truth.", "section": "5.1 Low-dimensional manifold"}, {"figure_path": "7v88Fh6iSM/figures/figures_22_1.jpg", "caption": "Figure 11. Example of scan slices from the fastMRI [7, 8] dataset.", "description": "This figure shows example slices from the fastMRI dataset used in the accelerated MRI experiment of the paper.  The images are grayscale and show various knee scans.", "section": "5.3 Accelerated MRI"}, {"figure_path": "7v88Fh6iSM/figures/figures_22_2.jpg", "caption": "Figure 11. Example of scan slices from the fastMRI [7, 8] dataset.", "description": "This figure shows example slices from the fastMRI dataset, which contains knee MRI scans. These images serve as ground truth data for the accelerated MRI experiment described in the paper. The images show the detailed structure and anatomy of the knee.", "section": "5.3 Accelerated MRI"}, {"figure_path": "7v88Fh6iSM/figures/figures_23_1.jpg", "caption": "Figure 11. Example of scan slices from the fastMRI [7, 8] dataset.", "description": "This figure displays example slices from the fastMRI dataset used in the accelerated MRI experiment of the paper.  The images show various knee scans, illustrating the type of data used for training and evaluation of the proposed method.", "section": "5.3 Accelerated MRI"}, {"figure_path": "7v88Fh6iSM/figures/figures_23_2.jpg", "caption": "Figure 6. Examples of posterior samples for accelerated MRI using a diffusion prior trained from k-space observations only. Posterior samples are detailed and present plausible variations, while remaining consistent with the observation. We provide the zero-filled inverse, where missing frequencies are set to zero, as baseline.", "description": "This figure shows the results of applying the proposed method to the accelerated MRI task.  The top row displays the ground truth MRI scans. The second row shows the incomplete k-space observations used as input. The bottom two rows present two different samples from the posterior distribution generated by the model, demonstrating that the model can produce detailed and plausible MRI reconstructions that are consistent with the limited observations.  The zero-filled inverse serves as a baseline to compare against, showcasing the improvement achieved by the proposed method.", "section": "5.3 Accelerated MRI"}, {"figure_path": "7v88Fh6iSM/figures/figures_24_1.jpg", "caption": "Figure 15. Example of samples from the model q\u03b8k (x) after k = 2 EM iterations for the accelerated MRI experiment when the heuristic (I + \u03a3t\u22121)\u22121 is used for V[x | xt]. The samples start to present vertical artifacts due to poor sampling.", "description": "This figure shows samples generated after only two Expectation-Maximization (EM) iterations using a specific heuristic for the covariance matrix.  The result showcases the negative impact of using less accurate heuristics on the quality of the samples, leading to artifacts (vertical lines).  This highlights the importance of the more accurate Tweedie's formula proposed in the paper.", "section": "5.3 Accelerated MRI"}, {"figure_path": "7v88Fh6iSM/figures/figures_24_2.jpg", "caption": "Figure 7. 1-d and 2-d marginals of the ground-truth distribution p(x) used in the low-dimensional manifold experiment. The distribution lies on a random 1-dimensional manifold embedded in R5.", "description": "This figure displays the marginal and 2D marginal distributions of the ground truth data used for the low-dimensional manifold experiment. The data is sampled from a 1-dimensional manifold embedded in 5 dimensions. The plot visually shows the underlying structure of the data used for the experiment.", "section": "5.1 Low-dimensional manifold"}, {"figure_path": "7v88Fh6iSM/figures/figures_27_1.jpg", "caption": "Figure 17. Qualitative evaluation of MMPS with 1 and 5 solver iterations.", "description": "This figure shows a qualitative comparison of the results obtained using MMPS with 1 and 5 solver iterations for four different inverse problems: box inpainting, random inpainting, motion deblurring, and super-resolution.  For each problem, the top row shows the reference image, the second row shows the observation, and subsequent rows display samples generated by MMPS with different numbers of solver iterations (10, 100, and 1000 steps). The figure visually demonstrates the improved image quality achieved by MMPS when increasing the number of solver iterations, particularly for more challenging tasks like motion deblurring.", "section": "E Evaluation of MMPS"}, {"figure_path": "7v88Fh6iSM/figures/figures_27_2.jpg", "caption": "Figure 18. Qualitative evaluation of DPS [21] and DiffPIR [26].", "description": "This figure shows a qualitative comparison of the results obtained by using three different posterior sampling methods: DPS, DiffPIR, and MMPS. For each method, the results obtained with 10, 100, and 1000 sampling steps are shown, for four different image reconstruction tasks: box inpainting, random inpainting, motion deblurring, and super-resolution. By comparing the visual results, one can assess the qualitative performance of each method in generating high-quality images from the noisy or incomplete observations.", "section": "E Evaluation of MMPS"}, {"figure_path": "7v88Fh6iSM/figures/figures_28_1.jpg", "caption": "Figure 19. Qualitative evaluation of IGDM [22] and TMPD [25].", "description": "This figure shows a qualitative comparison of the image reconstruction results obtained using IGDM and TMPD methods for four different inverse problems: box inpainting, random inpainting, motion deblur, and super-resolution.  Each row represents a different inverse problem, with the reference image, the noisy observation, and the reconstruction results for IGDM and TMPD, each with different numbers of sampling steps (10, 100, 1000).  The figure allows for visual comparison of the quality of image reconstruction achieved by the two different methods under varying noise and degradation conditions.", "section": "E Evaluation of MMPS"}]