[{"figure_path": "7v88Fh6iSM/tables/tables_6_1.jpg", "caption": "Table 1. Evaluation of final models trained on corrupted CIFAR-10. Our method outperforms AmbientDiffusion [80] at similar corruption levels. Using heuristics for V[x | xt] instead of Tweedie\u2019s formula greatly decreases the sample quality.", "description": "This table presents the results of evaluating different methods on the corrupted CIFAR-10 dataset.  The metrics used are the Fr\u00e9chet Inception Distance (FID) and the Inception Score (IS), which measure the quality of generated images. Lower FID and higher IS values indicate better image quality.  The table shows that the proposed method ('Ours w/ Tweedie') outperforms AmbientDiffusion, especially at higher corruption levels.  Furthermore, it demonstrates the importance of using the Tweedie\u2019s formula for covariance estimation, as using heuristics leads to significantly poorer results.", "section": "5.2 Corrupted CIFAR-10"}, {"figure_path": "7v88Fh6iSM/tables/tables_18_1.jpg", "caption": "Table 2. Hyperparameters for the low-dimensional manifold experiment.", "description": "This table lists the hyperparameters used in the low-dimensional manifold experiment.  It specifies details of the neural network architecture (MLP type, input and hidden layer dimensions, activation function (SiLU), and normalization (LayerNorm)), optimization settings (Adam optimizer, weight decay, learning rate schedule, gradient clipping), batch size, and the number of optimization steps and EM iterations.", "section": "C Experiment details"}, {"figure_path": "7v88Fh6iSM/tables/tables_19_1.jpg", "caption": "Table 3. Hyperparameters for the corrupted CIFAR-10 and accelerated MRI experiments.", "description": "This table lists the hyperparameters used for the corrupted CIFAR-10 and accelerated MRI experiments.  It details the architecture (U-Net for both), input shape, residual blocks per level, channels per level, attention heads per level, kernel size, activation function (SiLU), normalization (LayerNorm), optimizer (Adam), weight decay, learning rate, gradient norm clipping, EMA decay, dropout rate, augmentation techniques, batch size, epochs per EM iteration, and the number of EM iterations.  These settings are crucial for reproducibility of the experimental results.", "section": "C Experiment details"}, {"figure_path": "7v88Fh6iSM/tables/tables_26_1.jpg", "caption": "Table 4. Quantitative evaluation of MMPS with 1, 3 and 5 solver iterations.", "description": "This table presents a quantitative comparison of the Moment Matching Posterior Sampling (MMPS) method against other state-of-the-art posterior sampling methods across four linear inverse problems: box inpainting, random inpainting, motion deblur, and super resolution.  The evaluation metrics used are LPIPS, PSNR, and SSIM.  The number of solver iterations (1, 3, and 5) for MMPS is also varied to show the impact of increasing computational effort on performance.  The results indicate the relative performance of each method across different tasks and solver iterations.", "section": "E Evaluation of MMPS"}, {"figure_path": "7v88Fh6iSM/tables/tables_26_2.jpg", "caption": "Table 5. Time and memory complexity of MMPS for the 4\u00d7 super resolution task. Each solver iteration increases the time per step by around 16 ms. The maximum memory allocated by MMPS is about 10% larger than DPS [21] and IGDM [22].", "description": "This table presents the computational cost of the MMPS method compared to other methods for a super-resolution task.  It shows the number of vector-Jacobian products (VJPs), time per step, and memory usage for MMPS with varying numbers of solver iterations. The results indicate that while MMPS has a higher computational cost than some baselines, its memory usage is comparable and the increase in time is linear with the number of VJPs.", "section": "E Evaluation of MMPS"}]