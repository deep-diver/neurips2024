[{"figure_path": "1YGgaouVgZ/tables/tables_3_1.jpg", "caption": "Table 1: Comparison with existing work [28]. With a wide network assumption, we improve the existing results from the perspective of data distribution, perturbation design, training time, loss function, and network architecture. Note that the non-bias and leaky-ReLU assumptions of [28] are critical for deriving their results. A detailed comparison can be found in Section 3.4.", "description": "This table compares the authors' work with a prior work ([28]). It highlights improvements in terms of data distribution (any vs. mutually orthogonal), perturbation design (standard gradient-based vs. oracle-based), training time (any vs. infinite), loss function (differentiable, non-decreasing vs. exponential or logistic), network bias (available vs. not available), activation functions (ReLU and Leaky-ReLU vs. Leaky-ReLU), and network width (sufficiently wide but finite vs. any).  The common aspects of both works are also listed at the bottom: binary classification, two-layer network, and gradient flow.", "section": "3.4 Comparison with Prior Work and Limitations"}]