[{"figure_path": "3HpCVZV9it/figures/figures_5_1.jpg", "caption": "Figure 2: Histogram of soft preference labels \\hat{p} in preference dataset simulated with AI feedback from PaLM 2-L, instruction-tuned on Flan dataset. We prepare Reddit TL;DR [69, 49], Anthropic Helpful and Harmless [4], and Plasma Plan [6]. We construct competitive paired samples with winner responses and PaLM 2-L to simulate diverse preference distributions that have a peak around the modest confidence (e.g. \\hat{p} \u2208 [0.7,0.9)).", "description": "This figure displays histograms illustrating the distribution of soft preference labels across six different datasets.  The soft preference labels are simulated using AI feedback from a PaLM 2-L language model.  Each histogram shows the frequency of different soft preference label values (ranging from 0.6 to 1.0), with higher values indicating stronger preference for one response over another.  The datasets used include Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and three versions of the Plasma Plan dataset. The Plasma Plan datasets are further subdivided into 'Plasma Plan', 'Plasma Plan Skewed' and 'Plasma Plan Stairs' to highlight the diversity of label distributions.", "section": "4 Experiments"}, {"figure_path": "3HpCVZV9it/figures/figures_7_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "This figure consists of two parts. The left panel shows scaling factors for the gradient of three different methods (DPO, cDPO, GDPO) as a function of the reward difference. It illustrates how geometric averaging in GDPO adjusts the gradient scale based on soft preference labels, reducing the impact of less confident preferences. The right panel demonstrates a 1D bandit problem with 100 actions, comparing the policy probability mass distribution from different methods. This part demonstrates that GDPO can successfully address the objective mismatch and over-optimization issues faced by other methods by focusing on high reward regions, unlike cDPO which focuses on accurately fitting data distribution, potentially leading to suboptimal solutions.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_8_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The left panel shows how the scaling factor in the gradient calculation changes with different soft preference labels (p) and reward difference values (h(x, y1, y2)) for DPO, cDPO, and GDPO.  GDPO dynamically adjusts the gradient scale based on the confidence of the soft labels. The right panel compares the performance of DPO, cDPO, and GDPO on a 1D bandit problem.  It demonstrates that while cDPO precisely fits the training data distribution, leading to a mode in a low-reward region, DPO and GDPO are better at focusing on higher-reward regions.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_8_2.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The figure demonstrates the effect of weighted geometric averaging on gradient scaling and policy learning in a 1D bandit problem. The left panel shows how GDPO adjusts gradient scaling based on soft preference labels, mitigating over-optimization issues. The right panel compares the action distributions of DPO, cDPO, and GDPO, highlighting GDPO's ability to assign probability mass to high-reward regions, unlike cDPO which accurately fits the data distribution but places its mode in a low-reward area.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_9_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The left panel shows how the scaling factor in the gradient of the loss function changes with the soft preference labels for three different optimization methods: DPO, cDPO, and GDPO.  GDPO adjusts the gradient scale based on the soft preference, effectively minimizing the loss when responses are equally preferred. The right panel illustrates a 1D bandit problem comparing the policies learned by DPO, cDPO, and GDPO. It highlights that while cDPO accurately fits the training data distribution, DPO and GDPO place more probability mass in high-reward regions, showing better alignment with true reward.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_16_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The figure's left panel displays scaling factors influencing the gradient of different objective functions (DPO, cDPO, GDPO) as a function of the reward difference.  GDPO dynamically adjusts the gradient scale based on soft preference labels. The right panel presents a 1-D bandit problem where cDPO accurately models the data distribution but peaks in a low-reward area, while DPO and GDPO assign higher probability mass to high-reward regions.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_17_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The figure on the left shows how the scaling factor in the gradient of three different optimization objectives (DPO, cDPO, and GDPO) changes with the reward difference.  Geometric averaging (GDPO) adapts the gradient scale according to soft preference labels, effectively reducing the gradient's influence when preferences are weak or uncertain. The figure on the right illustrates the performance of the three methods on a 1-D bandit problem, demonstrating GDPO's ability to focus probability mass on high-reward actions, unlike cDPO which fits the data distribution but fails to prioritize high-reward regions.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_21_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The left panel of Figure 1 shows how the gradient scaling factor changes based on the soft preference labels and the reward difference between two outputs.  Geometric averaging (GDPO) dynamically adjusts the gradient scale, reducing its magnitude when outputs are equally preferred and keeping the scale when one output is highly preferred. The right panel illustrates a 1D bandit problem where cDPO precisely fits the data distribution but is stuck in a low-reward region, while DPO and GDPO focus more on high-reward regions.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_22_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The left panel shows how the scaling factor in the gradient of three different objectives (DPO, cDPO, GDPO) varies with the reward difference h(x,y1,y2) and the soft preference label p. GDPO adjusts the scale based on p, reducing the effect from equally good pairs, while DPO and cDPO do not. The right panel illustrates a 1D bandit problem where cDPO accurately fits the data distribution but focuses on a low-reward region, while DPO and GDPO assign probability mass to high-reward regions.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_23_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p\u0302 = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p\u0302 = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "This figure demonstrates the effects of weighted geometric averaging on gradient scaling and policy optimization.  The left panel shows how the scaling factor (\u03c9\u03b8) in the gradient of the loss function adapts to different values of the soft preference label (p\u0302). GDPO reduces the gradient magnitude for labels close to 0.5 (equal preference), while maintaining it for labels close to 1 (strong preference).  The right panel visualizes a 1D bandit problem, comparing the action distributions learned by three different methods (DPO, cDPO, and GDPO). cDPO accurately matches the data distribution but favors a low-reward action. In contrast, DPO and GDPO assign significant probability mass to high-reward actions, showing that geometric averaging helps avoid overfitting and achieve better performance.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_23_2.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The figure compares three different preference optimization methods (DPO, cDPO, and GDPO) in terms of their gradient scaling factors and performance on a 1D bandit problem. The left panel shows that GDPO adjusts the scale of the gradient based on the soft preference labels, effectively mitigating over-optimization. The right panel shows the action distribution of the learned policies of three methods trained on 1D bandit problem with 100 actions.  While cDPO accurately fits the training data, it tends to select actions with low rewards. DPO and GDPO perform better, selecting actions with higher rewards.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_25_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The figure shows two plots. The left plot shows the scaling factors for different preference optimization methods (DPO, cDPO, GDPO) as a function of the reward difference.  Geometric averaging (GDPO) adapts the scale of the gradient based on the soft preference labels. The right plot shows results for a 1-D bandit problem with 100 actions, comparing the learned policy distributions for DPO, cDPO, and GDPO.  It demonstrates that GDPO and DPO can better assign probability mass to high-reward regions compared to cDPO, which fits the data distribution but concentrates probability mass in a low-reward area.", "section": "3 Methods"}, {"figure_path": "3HpCVZV9it/figures/figures_26_1.jpg", "caption": "Figure 1: (Left) Scaling factors \u03c9\u03b8 in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of h(x, y1, y2). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (p = 0.95), the scaling factor of GDPO is almost the same, and small soft labels (p = 0.55) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.", "description": "The left panel of Figure 1 shows how the gradient scaling factors of DPO, cDPO, and GDPO vary with the reward difference (h(x, y1, y2)) and soft preference labels (p).  GDPO adapts its scaling factor based on p, reducing the influence of gradients from pairs with low preference similarity (p \u2248 0.5). The right panel contrasts the learned action distributions of DPO, cDPO, and GDPO in a 1-D bandit problem. It demonstrates that while cDPO accurately models the data distribution, it centers on a low-reward region; whereas DPO and GDPO effectively allocate probability mass to high-reward regions.", "section": "3 Methods"}]