[{"type": "text", "text": "Schur Nets: effectively exploiting local structure for equivariance in higher order graph neural networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qingqi Zhang1\u2217, Ruize $\\mathbf{X}\\mathbf{u}^{2*}$ and Risi Kondor1,2 1Computational and Applied Mathematics 2Department of Computer Science University of Chicago   \n{qingqi,richard1xur,risi}@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Several recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks. In such architectures, to faithfully account for local structure such as cycles, the local operations must be equivariant to the automorphism group of the local environment. However, enumerating the automorphism groups of all subgraphs of interest and finding appropriate equivariant operations for each one of them separately is generally not feasible. In this paper we propose a solution to this problem based on spectral graph theory that bypasses having to determine the automorphism group entirely and constructs a basis for equivariant operations directly from the graph Laplacian. We show that empirically this approach can boost the performance of GNNs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Message pasing neural networks (MPNNs) are the most popular paradigm for building neural networks on graphs [19]. While MPNNs have proved to be remarkably effective in a range of domains from program analysis [1] to drug discovery [21], a series of both empirical [3] and theoretical [43, 10] results have shown that the fact that MPNNs are based on just individual vertices sending messages to their neighbors limits their expressive power. This problem is especially acute in domains such as chemistry, where the presence or absence of specific small structural units (functional groups) directly influences the behavior and properties of molecules. ", "page_idx": 0}, {"type": "text", "text": "Recent papers proposed to address this issue by extending the message paradigm to also allow for message passing between vertices and edges [25] or between subgraphs, treated as single units [2, 16, 4, 49]. However, if the actual messages remain scalars, the added expressive power of these networks is relatively limited. ", "page_idx": 0}, {"type": "text", "text": "Another recent development is higher order message passing networks, where subgraphs communicate with each other via not just scalars, but more complex messages indexed by the individual vertices of the sending and receiving subgraphs [32, 35, 15]. For example, in an organic molecule, the internal state of a benzene ring (six carbon atoms arranged in a cycle) can be represented as a matrix $T\\in\\mathbb{R}^{6\\times c}$ , in which the rows correspond to the individual carbons. If this benzene ring passes a message to a neighboring benzene ring with which it shares an edge, information relating to the two shared carbons can be sent directly to the same atoms in the second ring, whereas information relating to the other vertices has to be treated differently. Hypergraph neural networks [16, 48, 44] and neural networks on more abstract mathematical structures such as simplicial complexes [7, 6] are closely related, since these also involve higher order generalizations of message passing between combinatorial objects interlocked in potentially complicated ways. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The theory of higher order MPNNs builds on the by now substantial literature on permutation equivariance in neural networks from Deep Sets [46], to various works studying higher order permutation equivariant maps [33], and is also related to the category theoretic approach expounded employed Natural Graph Neural Networks [13]. The recently introduced $P$ -tensors formalism provides a flexible framework for implementing message passing in higher order MPNNs and enumerating all possible linear maps between subgraphs [22]. ", "page_idx": 1}, {"type": "text", "text": "One aspect of higher order MPNNs that has garnered relatively little attention, however, is the interaction between the local graph structure and the equivariance constraint. Intuitively, the reason that GNNs must be equivariant to permutations is that when the same, or analogous, graphs are presented to the network with the only difference being that the vertices have been renumbered, the final output must remain the same. However, Thiede et al. [40] observed that at the local level enforcing full permutation equivariance is too restrictive. When considering specific, structurally important subgraphs such as paths or cycles, the operations local to such a subgraph $S$ should really only be equivariant to the group of automorphisms of $S$ , rather than all $|S|$ ! permutations of its vertices. The Autobahn architecture described in [40] explicitly takes the automorpism group of two specific types of subgraphs, cycles and paths, into account and defines a bespoke form of equivariant convolution on them, which is shown to improve performance on molecular datasets like ZINC [27]. ", "page_idx": 1}, {"type": "text", "text": "The drawback of the Autobahn approach is that it requires explicitly identifying the automorphism group of each type of subgraph of interest, and crafting specialized equivariant operations for it based on its representation theory. For more flexible architectures leveraging not just cycles and paths but many other types of subgraphs this quickly becomes infeasible. ", "page_idx": 1}, {"type": "text", "text": "Main contributions. The main contribution of this paper is a simple algorithm based on spectral graph theory for constructing a basis of automorphism equivariant operations on any possible subgraph without explicitly having to determine its automorphism group, let alone derive its irreducible representations. The algorithm is based on imitating the structure of the group theoretical approach to equivariance, which we summarize in Section 3, but bypassing having to use actual group theory. Section 4 describes the algorithm itself and the resulting neural network operations, which we call Schur layers. In Section 5 we show that the introduction of Schur layers does indeed improve the performance of higher order graph neural networks on standard molecular benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Background: equivariance with side information ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\mathcal{G}$ be an undirected graph with vertex set $V=\\{1,2,\\ldots,n\\}$ and edge set $E\\subseteq V\\times V$ represented by the adjacency matrix $\\mathbf{\\bar{A}}\\in\\mathbb{R}^{n\\times n}$ . Graph neural networks (GNNs) address one of two, closely related tasks: (a) given a function $f^{\\mathrm{in}}\\colon V\\stackrel{=}{\\rightarrow}\\mathbb{R}$ on the vertices of $\\mathcal{G}$ , learn to transform it to another function $f^{\\mathrm{out}}\\colon V\\to\\mathbb{R}$ ; (b) given not just one graph, but an entire collection of graphs, learn to map each graph $\\mathcal{G}$ , or rather its adjacency matrix $A$ , to a scalar or vector $\\psi(A)$ that characterizes $\\mathcal{G}$ \u2019s structure. ", "page_idx": 1}, {"type": "text", "text": "In both cases, the critical constraint is that the network must behave appropriately under permuting the order in which the vertices are listed. The group of all permutations of $\\{1,2,\\ldots,n\\}$ is called the symmetric group of degree $n$ and denoted $\\mathbb{S}_{n}$ . Applying a permutation $\\sigma\\in\\mathbb{S}_{n}$ to the vertices changes the adjacency matrix to $A^{\\prime}=\\sigma(A)$ with ", "page_idx": 1}, {"type": "equation", "text": "$$\nA_{i,j}^{\\prime}=A_{\\sigma^{-1}(i),\\sigma^{-1}(j)}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The basic constraint on algorithms that operate o n functions on graphs is that if the input is transformed awlaoyn, $f_{i}^{\\mathrm{out}\\prime}=f_{\\sigma^{-1}(i)}^{\\mathrm{out}}$ .b eTrih nisg  porf otpheer tvye ritsi cceasl,l $f_{i}^{\\mathrm{in}^{\\prime}}=f_{\\sigma^{-1}(i)}^{\\mathrm{in}}$ ,c te.h eFno trhme aolluyt,p dute nmotuisnt\u2032 g t rtahnes fmorapmp tihneg  sfraomme inputs to outputs $\\bar{\\phi}^{'}$ : , equivariance states that the action of permutations on functions given by $f_{i}^{\\prime}=f_{\\sigma^{-1}(i)}$ must commute with $\\phi$ , that is, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\phi(\\sigma(f^{\\mathrm{in}}))=\\sigma(\\phi(f^{\\mathrm{in}}))\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "for any $\\sigma\\in\\mathbb{S}_{n}$ and any input function $f^{\\mathrm{in}}\\in\\mathbb{R}^{V}$ . In contrast, for networks that just learn embeddings ${\\mathcal{G}}\\mapsto\\psi(A)$ , the constraint is invariance to permutations, i.e., $\\psi(A)=\\psi(\\sigma(A)){\\big\\}}$ . In practice, however, the two cases are closely related because most graph embedding networks work with functions defined on the vertices, and then symmetrize over permutations at the very top of the network by using a readout function such as $\\begin{array}{r}{\\psi(\\boldsymbol{\\mathcal{G}})=\\sum_{i}f_{i}^{\\mathrm{out}}}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Enforcing the constraint (1) on the hypothesis space directly would be very limiting, effectively constraining GNNs to be composed of symmetric polynomials of $f^{\\mathrm{in}}$ combined with pointwise nonlinearities. Graph neural networks cleverly get around this problem by using the adjacency matrix itself as side information to guide equivariance. For example, in classical (zeroth order) message passing neural networks (MPNNs), the output of each layer is itself a (vector valued) function on the graph, and in the simplest case the update rule from layer $\\ell$ to layer $\\ell+1$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}^{\\ell+1}=\\eta\\Big(W\\sum_{j\\in\\mathcal{N}(i)}f_{j}^{\\ell}+\\theta\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{N}(i)$ denotes the neighbors of node $i$ in $\\mathcal{G},W$ and $\\theta$ are learnable weight matrices/vectors and $\\eta$ is a suitable nonlinearity. The fact that the summation only extends over the neighbors of vertex $i$ induces a dependence of $\\phi$ on the adjacency matrix $A$ . Hence it would be more accurate to write $\\phi$ as $\\phi_{A}$ , and the equivariance condition as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\phi_{\\sigma(A)}\\big(\\sigma(f^{\\mathrm{in}})\\big)=\\sigma\\big(\\phi_{A}(f^{\\mathrm{in}})\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since vertex permutations transform both $f^{\\mathrm{in}}$ and $A$ , when the vertices are permuted $A$ can implicitly provide information about this to the algorithm allowing it to compensate, even if for fixed $A$ , $\\phi_{A}$ is not equivariant to all $n!$ factorial permutations. Update rules such as (2) work in this way automatically. The adjacency matrix cannot carry information about permutations that leave it invariant however, so $\\phi_{A}$ must still be equivariant to the group of all such permutations, called the automorphism group of $\\mathcal{G}$ and denoted $\\bar{\\mathbf{A}}\\mathbf{u}\\mathbf{t}(\\mathcal{G})$ (or $\\operatorname{Aut}(A))$ . This observation about $A$ acting as a source of side information that can reduce the size of the group that individual GNN operations need to be equivariant to and hence increase the expressive power of the network as a whole motivates much of the rest of this paper. ", "page_idx": 2}, {"type": "text", "text": "2.1 Higher order GNNs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Despite the great success of message passing networks, a long sequence of empirical as well as theoretical results [43, 35, 31, 14, 38] over the last several years have made it clear that the expressive power of algorithms based on simple update rules like (2) is severely limited. In response, the community has turned to extensions of the MPNN paradigm involving message passing not just from vertices to vertices but also between vertices and edges or between carefully selected subgraphs [19, 35, 6, 8]. While these networks maintain the local and equivariant character of earlier MPNNs, they can more faithfully reflect local topological information, and are particularly well suited to domains such as chemistry, where capturing local structures such as functional groups is critical. ", "page_idx": 2}, {"type": "text", "text": "In tandem, researchers have developed higher order MPNN architectures, where the output of individual edge or subgraph \u201cneurons\u201d are not just scalars, but vector or tensor quantities indexed by the vertices of the graph involved in the given substructure. For example, in the chemistry setting, if $\\mathfrak{n}$ is a titular neuron corresponding to a benzene ring in a molecule, the output of n, assuming we have $C$ channels, might be a matrix $\\stackrel{\\triangledown}{T}\\in\\mathbb{R}^{6\\times C}$ or a tensor $T\\in\\mathbb{R}^{6\\times6\\times C}$ , or $\\dot{T}\\in\\mathbb{R}^{6\\times6\\times6\\times C}$ , etc.. In each of these cases the non-channel dimensions correspond to the six atoms making up the ring, and when n communicates with other subgraph-neurons, must be treated accordingly. Such higher order representations offer far greater expressive power since $T$ can effectively represent not just information related to individual vertices, but also relations between pairs of vertices, triples, and so on. The general trend towards higher order message passing is also closely tied to hypergraph neural networks [16], \u201ctopological\u201d neural networks and simplicial complex networks [7, 6]. ", "page_idx": 2}, {"type": "text", "text": "Recently, [22] proposed a general formalism for describing such higher order architectures using so-called $P$ -tensors. In this formalism, given a neuron n attached to a subgraph $S$ with $m$ vertices, if the output of $\\mathfrak{n}$ is a zeroth order $P$ -tensor $T$ with $C$ channels then that means that it is simply a vector $T\\,\\dot{\\in}\\,\\mathbb{R}^{C}$ . The elements of this vector are scalars in the sense that they give a global description of $S$ , which is invariant to permutations of the vertices of $S$ . If $T$ is a first order $P$ -tensor then that means that it is a matrix $T\\in\\mathbb{R}^{m\\times C}$ whose columns transform under permutations of $S$ similarly to how $f^{\\mathrm{in}}$ and $f^{\\mathrm{out}}$ transform under global permutations of the full graph: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sigma\\colon T\\mapsto T^{\\prime}\\phantom{\\;\\;\\;\\sigma\\;\\;\\sigma\\;\\;\\sigma\\;\\;\\sigma}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;T_{i,c}^{\\prime}=T_{\\sigma^{-1}(i),c}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sigma\\in\\mathbb{S}_{m}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If $T$ is a second order $P$ -tensor $T\\in\\mathbb{R}^{m\\times m\\times C}$ , then each slice of it corresponding to a given channel transforms according to the second order action of the symmetric group, like the adjacency matrix: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma\\colon T\\mapsto T^{\\prime}\\qquad\\qquad\\qquad T_{i,j,c}^{\\prime}=T_{\\sigma^{-1}(i),\\,\\sigma^{-1}(j),c}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma\\in\\mathbb{S}_{m}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Continuing this pattern, a $k$ \u2019th order $P$ -tensor $T\\in\\mathbb{R}^{m\\times m\\times\\dots\\times m\\times C}$ transforms under local permutations as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma\\colon T\\mapsto T^{\\prime}\\qquad\\qquad\\qquad T_{i_{1},\\dots,i_{k},c}^{\\prime}=T_{\\sigma^{-1}(i_{1}),\\dots,\\,\\sigma^{-1}(i_{k}),c}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma\\in\\mathbb{S}_{m}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "[22] derived the general rules for equivariant message passing between such $P$ -tensors in the cases that the sending and receiving subgraphs $S$ resp. $S^{\\prime}$ are (a) the same (b) partially overlap (c) are disjoint. In all these cases however it was assumed that $T$ needs to be equivariant to all permutations of the underlying subgraphs $S$ and $S^{\\prime}$ . As we discussed above, this is an overly restrictive condition that limits the extent to which a higher order subgraph neural network can exploit the underlying topology. In the following sections we describe how the $P$ -tensor framework can be extended so that the $S\\mapsto S$ type of message passing steps (called linmaps in the $P$ -tensors nomenclature) are equivariant to just $\\mathrm{Aut}(S)$ rather than the full symmetric group $\\mathbb{S}_{m}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Equivariance to local permutations: the group theoretic approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall that a representation of a finite group $G$ such as $\\mathbb{S}_{m}$ or $\\mathrm{Aut}(S)$ is a (complex) matrix valued function $\\rho\\colon G\\to\\mathbb{C}^{d_{\\rho}\\times d_{\\rho}}$ such that the matrices multiply the same way as the corresponding group elements do ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho(\\sigma_{2}\\sigma_{1})=\\rho(\\sigma_{2})\\rho(\\sigma_{1})\\qquad\\qquad\\qquad\\qquad\\sigma_{1},\\sigma_{2}\\in G.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Two representations $\\rho$ and $\\rho^{\\prime}$ are said to be equivalent if there is an invertible matrix $Q$ such that $\\rho^{\\prime}(\\sigma)\\doteq Q^{-1}\\rho(\\sigma)\\,Q$ for all group elements. A representation of a finite group is said to be reducible if there is some invertible matrix $Q$ that reduces it to a block diagonal form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho(\\sigma)=Q^{-1}\\left(\\frac{\\rho_{1}(\\sigma)\\mid}{\\rho_{2}(\\sigma)}\\right)\\,Q.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Some fundamental results in representation theory tell us that $\\mathrm{G}$ only has a finite number of inequivalent irreducible representations (irreps, for short), these irreps can be chosen to all be unitary, and that any representation of $G$ is reducible to some combination of them [39]. These facts give rise to a type of generalized Fourier analysis on finite groups, consisting of decomposing vectors that $G$ acts on into parts transforming according to the unitary irreps of the group. ", "page_idx": 3}, {"type": "text", "text": "The most general approach to defining $\\mathrm{Aut}(S)$ -equivariant maps for first, second, and higher order subgraph neurons uses exactly this machinery. In particular, defining permutation matrices as usual as ", "page_idx": 3}, {"type": "equation", "text": "$$\n[P_{\\sigma}]_{i,j}=\\left\\{{\\begin{array}{l l}{1}&{{\\mathrm{if}}~\\,\\sigma(j)=i}\\\\ {0}&{{\\mathrm{otherwise}},}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "dropping the channel indices w.l.o.g., and writing our $P$ -tensors in vectorized form $\\overline{{T}}\\,\\overset{\\mathtt{\\Lambda}}{=}\\,\\mathrm{vec}(T)\\in\\mathbb{R}^{m^{k}}$ , we note that (4)\u2013(6) can be written in a unified form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{T^{\\prime}}}=P^{k}(\\sigma)\\,\\overline{{T}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P^{k}(\\sigma)$ is the $k$ -fold Kronecker produc t matrix $P^{k}(\\sigma)=P_{\\sigma}\\otimes P_{\\sigma}\\otimes...\\,P_{\\sigma}$ . Crucially, $P^{k}(\\sigma)$ , regarded as a function $\\operatorname{Aut}(S)\\to\\mathbb{C}^{m^{\\mathbf{i}_{k}}\\times m^{k}}$ is a unitary representation of the automorphism group. ", "page_idx": 3}, {"type": "text", "text": "According to representation theory, $P^{k}$ must then be decomposable into a direct sum of irreps $\\rho_{1},\\ldots,\\rho_{p}$ of $\\operatorname{Aut}(S)$ with corresponding multiplicities $\\underline{{\\kappa}}_{1},\\ldots,\\kappa_{p}$ . The same unitary matrix $Q$ that accomplishes this can also be used to decompose $\\overline{T}$ into a combinations of smaller vectors $(\\pmb{t}_{j}^{i}\\in\\mathbb{R}^{d_{\\rho_{i}}})_{i,j}^{\\star}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ\\overline{{{T}}}=\\bigoplus_{i}\\bigoplus_{j=1}^{\\kappa_{i}}t_{j}^{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where each $t_{j}^{i}$ now transforms independently under the action of the group as $\\pmb{t}_{j}^{i}\\mapsto\\rho_{i}(\\sigma)\\,\\pmb{t}_{j}^{i}$ . Alternatively, stacking all $t_{j}^{i}$ vectors transforming according to the same irrep $\\rho_{i}$ together in a matrix $\\pmb{T}^{i}\\in\\mathbb{R}^{d_{\\rho_{i}}\\times{\\star}_{i}}$ , we arrive at a sequence of matrices $T^{1},T^{2^{\\underline{{\\tau}}}},\\dots,T^{p}$ trasforming as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c c c c}{{{\\bf T}^{1}\\mapsto\\rho_{1}(\\sigma)\\,{\\bf T}^{1}\\qquad}}&{{{\\bf T}^{2}\\mapsto\\rho_{2}(\\sigma)\\,{\\bf T}^{2}\\qquad}}&{{\\ldots\\cdot\\,}}&{{{\\bf T}^{p}\\mapsto\\rho_{p}(\\sigma)\\,{\\bf T}^{p}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Now it is very easy to see how to construct learnable linear operations that are equivariant to these actions: simply multiplying each $T^{i}$ from the right by an apporpriate weight matrix $W^{i}$ . ", "page_idx": 4}, {"type": "text", "text": "In summary, this general, group theoretic approach to constructing automorphism group equivariant linear maps between $k$ \u2019th order $P$ -tensors can be seen as a special case of e.g. [30, 41], and involves the following sequence of steps: ", "page_idx": 4}, {"type": "text", "text": "1. Find the unitary matrix $Q$ that decomposes $P^{k}(\\sigma)=P_{\\sigma}\\otimes...\\otimes P_{\\sigma}$ into a direct product of   \nirreducible representations of $\\operatorname{Aut}(S)$ .   \n2. Use $Q$ to decompose the input $P$ -tensor into a sequence of matrices $T_{\\cdot}^{1},.\\,T^{p}$ transforming as (7).   \n3. Multiply each $T^{i}$ by an appopriate learnable weight matrix $W^{i}\\in\\mathbb{R}^{{\\boldsymbol{\\kappa}}_{i}\\times{\\boldsymbol{\\kappa}}_{i}}$ .   \n4. Use the inverse map $Q^{-1}=Q^{\\dagger}$ to reassemble $T^{\\bar{1}},T^{2},\\ldots,T^{p}$ into the output $P$ -tensor $T^{\\mathrm{out}}$ . ", "page_idx": 4}, {"type": "text", "text": "Notwithstanding its elegance and power, the representation theoretic approach to implementing automorphism group equivariance has some major disdvantages, namely that it requires explicitly determining the automorphishm group of each subgraph of interest, which is in and of itself a combinatorial problem, and explicitly finding its representations, which is also not trivial. The underlying mathematical structure however is important because it forms the basis to generalizing the approach to a much simpler framework in the next section: ", "page_idx": 4}, {"type": "text", "text": "1. We have a collection of (orthogonal) linear maps $\\{P^{k}(\\sigma)\\colon U\\mapsto U\\}_{\\sigma\\in\\operatorname{Aut}(S)}$ (with $U^{}\\!=\\!\\mathbb{R}^{m^{k}})$ that the neuron\u2019s operation needs to be equivariant to.   \n2. $U$ is decomposed into an orthogonal sum of subspaces $U=U_{1}\\oplus...\\oplus U_{p}$ corresponding to the different irreps featured in the decomposition of $P^{k}$ .   \n3. Each $U_{i}$ is further decomposed into an orthogonal sum of subspaces $U_{i}=V_{1}^{i}\\oplus...\\oplus V_{\\kappa_{i}}^{i}$ corresponding to the different columns of the T  matrices.   \n4. The decomposition is such that the $\\{P^{k}(\\sigma)\\}$ maps fix each $V_{j}^{i}$ subspace. Moreover, for a fixed $i$ , $\\{P^{k}(\\sigma)\\}$ acts the same way on each $V_{j}^{i}$ subspace by $\\rho_{i}(\\sigma)$ .   \n5. This structure implies that any linear map that linearly mixes the $V_{1}^{i},\\ldots V_{\\kappa_{i}}^{i}$ subspaces but does not mix information across subspaces with different values of $i$ is equivariant. ", "page_idx": 4}, {"type": "text", "text": "4 Equivariance via spectral graph theory: Schur layers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Instead of the representation theoretical approach described in the previous section, for practical purposes we advocate a simpler way of implementing automorphism group equivariance based on just spectral graph theory. The cornerstones of this approach are the following two theorems. The proofs can be found in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $G$ be a finite group acting on a space $U$ by the linear action $\\{g\\colon U\\to U\\}_{g\\in G}$ . Assume that we have a decomposition of $U$ into a sequence of spaces of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\nU=U_{1}\\oplus...\\oplus U_{p}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "such that each $U_{i}$ is invariant under $G$ in the sense that for any $g\\in G$ and $v\\in U_{i}$ , we have $g(v)\\in U_{i}$ . Let $\\phi\\colon U\\to U$ be a linear map that is a homothety on each $U_{i}$ , i.e., $\\phi(w)\\!=\\!\\alpha_{i}w$ for some fixed scalar $\\alpha_{i}$ for any $w\\in U_{i}$ . Then $\\phi$ is equivariant to the action of $G$ on $U$ in the sense that $\\phi(g(u))=g(\\phi(u))$ for any $u\\in U$ and any $g\\in G$ . ", "page_idx": 4}, {"type": "text", "text": "The representation theoretic result of the previous section actually corresponds to a refinement of this result involving a further decomposition of each $U_{i}$ space into a sequence of smaller subspaces. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $G$ and $U$ be as in Theorem $^{\\,l}$ , but now assume that each $U_{i}$ further decomposes into an orhthogonal sum of subspaces in the form $U_{i}=V_{1}^{i}\\oplus...\\oplus V_{\\kappa_{i}}^{i}$ such that (a) Each $V_{j}^{i}$ subspace is individually invariant by $G$ ; ", "page_idx": 4}, {"type": "text", "text": "$(b)$ For any fixed value of $i$ , the spaces $V_{1}^{i},\\dots,\\dot{V}_{\\kappa_{i}}^{i}$ are isomorphic and there is a set of canonical isomorphisms $\\iota_{j\\rightarrow j^{\\prime}}^{i}\\colon V_{j}^{i}\\rightarrow\\bar{V}_{j^{\\prime}}^{i}$ between them such that ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(\\iota_{j\\to j^{\\prime}}^{i}(v))=\\iota_{j\\to j^{\\prime}}^{i}(g(v))\\qquad\\qquad\\qquad\\qquad\\qquad\\forall\\,v\\in V_{j}^{i}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\phi\\colon U\\to U$ be a map of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi(v)=\\sum_{j^{\\prime}}\\alpha_{j,j^{\\prime}}^{i}\\,\\iota_{j\\to j^{\\prime}}^{i}(v)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nv\\in V_{j}^{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some fixed set of coefficients $\\{\\alpha_{j,j^{\\prime}}^{i}\\}$ . Then $\\phi$ is equivariant to the action of $G$ on $U$ . ", "page_idx": 4}, {"type": "text", "text": "In the matrix language of the previous section, $U_{1},\\ldots,U_{p}$ correspond to the $T^{1},T^{2},\\dots,T^{p}$ matrices, whereas the $V_{1}^{i},\\ldots,V_{\\kappa_{i}}^{i}$ subspaces of $U_{i}$ correspond to individual columns of $T^{i}$ . For any fixed $i$ , the $\\left(\\alpha_{j,j^{\\prime}}^{i}\\right)_{j,j^{\\prime}}$ scalars correspond to the individual matrix entries of the learnable weight matrix $W^{i}$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Automorphism invariance the simple way via spectral graph theory ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It remains to find the actual systems of invariant subspaces that can be plugged into Theorems 1 and 2. The following lemma suggests such a system of subspaces can be found directly from the graph Laplacian, effecitvely bypassing the need for group representation theory. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $S$ be an undirected graph with $m$ vertices, ${\\mathrm{Aut}}_{S}$ its automorphism group, and $L$ its combinatorial graph Laplacian. Assume that $L$ has $p$ distinct eigenvalues $\\lambda_{1},\\ldots,\\lambda_{p}$ and corresponding subspaces $U_{1},\\ldots,U_{p}$ . Then each $U_{i}$ is invariant under the first order action (4) of ${\\mathrm{Aut}}_{S}$ on $\\mathbb{R}^{m}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. Since ${\\mathrm{Aut}}_{S}$ is a subgroup of the full group of vertex permutations , its action on $\\mathbb{R}^{m}$ is just $\\mathbf{v}\\mapsto\\sigma(\\mathbf{v})=P_{\\sigma}\\mathbf{v}$ with $\\sigma\\in\\mathrm{Aut}_{S}$ . $L$ is a real symmetric matrix, so its eigenspaces $U_{1},\\ldots,U_{p}$ are mutually orthogonal and $U_{1}\\oplus...\\oplus U_{p}=\\mathbb{R}^{n}$ . Furthermore, $\\mathbf{v}\\in U_{i}$ if and only if $L{\\mathbf{v}}=\\lambda_{i}{\\mathbf{v}}$ . By definition, ${\\mathrm{Aut}}_{S}$ is the set of permutations that leave the adjacency matrix, and consequently the Laplacian invariant, so, in particular, $P_{\\sigma}L P_{\\sigma^{-1}}=L$ . Therefore, for any $\\mathbf{v}\\in U_{i}$ and any $\\sigma\\in\\mathrm{Aut}_{S}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nL\\underbrace{P_{\\sigma}\\mathbf{v}}_{\\sigma(\\mathbf{v})}=P_{\\sigma}L P_{\\sigma^{-1}}P_{\\sigma}\\mathbf{v}=P_{\\sigma}L\\mathbf{v}=\\lambda_{i}\\underbrace{P_{\\sigma}\\mathbf{v}}_{\\sigma(\\mathbf{v})}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "showing that $\\sigma(\\mathbf{v})\\in U_{i}$ and hence that $U_{i}$ is an invariant subspace. ", "page_idx": 5}, {"type": "text", "text": "The following Corollary puts this lemma to use and gives a surprisingly easy way of creating locally automorphism equivariant neurons. We define a Schur layer as a neural network module that applies this operation to every instance of a given subgraph in the graph, for example, every single benzene ring in a molecule. Clearly, for the sake of global permutation equivariance, the weight matrices for the same subgraph $S$ must be shared across the network. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Consider a GNN on a graph that involves a neuron $\\mathfrak{n}_{S}$ corresponding to a subgraph $S$ with m vertices. Assume that the input of $\\mathfrak{n}_{S}$ is a matrix $T\\in\\mathbb{R}^{m\\times c_{i n}}$ , the rows of which transform covariantly with permutations of $S$ and $c_{i n}$ is the number of channels. Let $L$ be the combinatorial Laplacian of $S$ , $U_{1},\\ldots,U_{p}$ be the eigenspaces of $L$ , and $M_{i}$ an orthognal basis for the i\u2019th eigenspace stacked into an $\\mathbb{R}^{n\\times d i m\\left(U_{i}\\right)}$ dimensional matrix. Then for any collection of learnable weight matrices $W_{1},\\ldots,W_{p}\\in\\mathbb{R}^{c_{i n}\\times c_{o u t}}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi\\colon T\\longmapsto\\sum_{i=1}^{p}M_{i}M_{i}^{\\top}T W_{i}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is a permutation equivariant linear operation. ", "page_idx": 5}, {"type": "text", "text": "The spectral approach also generalizes to higher order permutation equivariance, and in that case we can take advantage of the more refined two-level subspace structure implied by Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Let $S$ , $L$ and the $M_{i}$ \u2019s be as in Corollary $^{\\,l}$ . Given a multi-index $\\mathbf{i}=(i_{1},\\dots,i_{k})\\in$ $\\{1,\\dots,p\\}^{k}$ , we define its type as the tuple $\\mathbf{n}=(n_{1},\\ldots,n_{p})$ , where $n_{j}$ is the number of occurrences of $j$ in i and we define ${\\mathcal{T}}_{\\mathbf{n}}$ as the set of all multi-indices of type n. Assume that the input to neuron $\\mathfrak{n}_{S}$ is a k\u2019th order permutation equivariant tensor $T\\in\\mathbb{R}^{m\\times\\ldots\\times m\\times c_{i n}}$ , as defined in Section 3. For any given i, define the $k$ \u2019th order eigen-projector ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Pi_{\\mathrm{i}}=\\mathcal{P}_{\\mathrm{i}}(M_{i_{1}}^{\\top}\\otimes M_{i_{2}}^{\\top}\\otimes\\ldots\\otimes M_{i_{k}}^{\\top}\\otimes I)\\colon\\mathbb{R}^{m\\times\\ldots\\times m\\times c}\\rightarrow\\mathbb{R}^{m\\times\\ldots\\times m\\times c}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{P}_{\\mathbf{i}}$ is a permutation map that canonicalizes the form of the projection, as defined in the Appendix. Let $T\\circ W$ denote multiplying $T$ by the matrix $W$ only along its last dimension. Then for any collection of weight matrices $\\{W_{\\mathbf{i}^{\\prime},\\mathbf{i}^{\\prime}}\\in\\mathbb{R}^{c_{o u t}\\times c_{i n}}\\}$ the map ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi\\colon T\\longmapsto\\sum_{\\mathbf{n}}\\sum_{\\mathbf{i^{\\prime}}\\in\\mathbb{Z}_{\\mathbf{n}}}\\sum_{\\mathbf{i}\\in\\mathbb{Z}_{\\mathbf{n}}}\\Pi_{\\mathbf{i^{\\prime}}}^{\\top}\\bigl(\\Pi_{\\mathbf{i}}(T\\odot W_{\\mathbf{i^{\\prime}},\\mathbf{i^{\\prime}}})\\bigr)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is a permutation equivariant map. ", "page_idx": 5}, {"type": "text", "text": "As mentioned earlier, this gives sufficient but not necessary conditions for equivariant maps w.r.t. the automorphism group. A discussion of potential gap between our approach and the group theoretical approach can be found in appendix D. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To empirically evaluate Schur net, we implement the first order case in corollary 1 and compare it with other higher order MPNNs. Note that, the theorem only gives the equivariant maps to transform local representation on a subgraph, i.e., $\\phi:T^{\\mathrm{in}}\\mapsto T^{\\mathrm{out}}$ where $T^{\\mathrm{in}}\\,\\in\\,R^{m*c_{\\mathrm{in}}}$ and $T^{\\mathrm{out}}\\,\\in\\,R^{m*c_{\\mathrm{out}}}$ are representations of the subgraph. Another key component that is missing is the message passing between different subgraphs, which as mentioned in the previous section, is already given by [22] ${\\cal P}_{}$ -tensor). In $P$ -tensor framework, the most general linear equivariant message passing is defined based on an extension of equivariant maps by considering different domains. More details can be found in appendix E. We implement our Schur net based on the $P_{\\|}$ -tensor framework but use Schur layer to transform local representation. ", "page_idx": 6}, {"type": "text", "text": "There are several design details about the architecture that are worth mentioning: (1) We chose cycles of lengths three to eight in most of the experiments and also added branched cycles to show our algorithm\u2019s scalability. The reason to do so is in the chemical dataset we used, cycles are indeed the most important functional group to the property to be predicted, other subgraphs such as $m$ -stars, and $m$ -paths didn\u2019t help the performance. (2) While having first-order representation on the cycles, we also maintain 0th-order representation (just scalar) on node and edges, as in [6, 22]. Those node and edge representations captures more elementary information about the graph, such as $k$ -hop neighborhood, and is still important in higher order MPNNs. The representations pass messages with each other by intersection rule as defined in $P$ -tensor framework. (3) As discussed in appendix $\\mathrm{G}$ , we view Schur layer\u2019s operation as a spectral convolution filter [5] applied to the subgraph and the number of channels to indicate how many times it expands the input feature to the output feature. The codes used to run our experiments can be found at https://github.com/risilab/SchurNet. ", "page_idx": 6}, {"type": "text", "text": "5.1 Schur layer improves over Linmaps ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "First of all, we want to show that considering more possible equivariant maps on the subgraph can indeed boost the performance. To this end, we performed controlled experiments to compare Schur layer with the equivariant maps w.r.t. $S_{m}$ (following [22] we called it Linmaps). To make a fair comparison, we use the same architecture including MLPs and message passing defined by $P$ -tensor and only replace the equivariant maps used in Linmaps by what is defined in corollary 1. We didn\u2019t compare with Autobahn [40] because it didn\u2019t use the $P$ -tensor framework in the original paper and it\u2019s hard for us to implement the convolution w.r.t. automorphism group for all the subgraphs we chose. But we note that for the case of the cycles (see table 4), the equivariant maps given by our approach are equal to that given by the group theoretical approach. ", "page_idx": 6}, {"type": "text", "text": "We\u2019ll present the results on the commonly used molecular benchmark ZINC-12K [27] dataset in the main text. Results on TUdatasets as well as runtime comparison can be found in appendix H. The task on ZINC-12K is to regress the logP coefficient of each molecule and the Mean absolute error (MAE) between the prediction and the ground truth is used for evaluation. ", "page_idx": 6}, {"type": "text", "text": "We design experiments to compare Linmaps and Schur layers in various scenarios, to showcase the robustness of the improvement. Table 1 shows under various message passing schemes between edges and cycles, Schur Layer consistently outperforms Linmaps. Those are indications of the added expressive power of extra equivariant maps in Schur layer, and they\u2019re effective in various architectural design settings. Another ablation study regarding different cycle sizes can be found in appendix H. ", "page_idx": 6}, {"type": "table", "img_path": "HRnSVflpgt/tmp/0c76c5da65c019ce2e768a864839f56f5946220912f5e8892a63065247c131c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Comparison between Schur Layer and Linmaps with different message passing schemes. The message passing scheme is a design choice in $P$ -tensor framework, where the user can set when two subgraph\u2019s representations communicate. The mostly common use case is to require at least $k$ vertices in the intersection of two subgraphs for them to communicate. Experiments on ZINC-12k dataset and all scores are test MAE. Cycle sizes of {3,4,5,6,7,8,11} are used. ", "page_idx": 6}, {"type": "text", "text": "Then we studied the possibilities of adding Schur layer in different places of higher-order message passing scheme. In table 2, we observed that the more condensed the higher-order feature is, the more improvement that the Schur Layer brings to us over Linmaps. We attribute the improvements of adding/replacing Schur layer in various scenarios over Linmaps the benefti gained from utilizing the subgraph structure and increased number of equivariant maps. We also tried other ways to use Schur layer, the result is summarized in G. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "HRnSVflpgt/tmp/a8b7139df98ab8c799ce04372d33ae044fc756e6b68efed951da10a753a6df6a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: An experiment demonstrating different ways of using Schur layer. \"Complete Schur Layer\" means that we apply Schur Layer on the incoming messages together with the original cycle representation. \"Linmap SchuLayer\" means that we just apply the Schur Layer on the aggregated subgraph representation feature. \"Simple Schur Layer\" means we directly apply Schur Layer on the subgraph features without any preprocessing. We can observe that as the subgraph information diversifies, Schur layer tends to decouple the dense information better and results in better performance. The test MAE of Linmaps in this table is taken from [22]. ", "page_idx": 7}, {"type": "text", "text": "5.2 Flexibility ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The other advantage of Schur layer is it computes the feature transform only based on the subgraph\u2019s Laplacian, bypassing a difficult step of finding the automorphisms group and irreps of the subgraph it acts on. As discussed in the theory, Schur layer constructs equivariant maps only based on the subgraph\u2019s Laplacian and is applicable directly to any subgraphs, making the implementation much easier when different subgraphs are chosen than the group theoretical approach. This allows it to easily extend to any subgraph templates that\u2019re favorable by the user. To demonstrate this, we augment the subgraphs in the model by all the five and six cycles with one to three branches (including in total 16 non-isomorphic subgraph templates), comparing with baseline model where only the cycle itself is considered. Results can be found in appendix G. ", "page_idx": 7}, {"type": "text", "text": "5.3 Benchmark results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finally, and most importantly, we compare the Schur-Net to several other higher-order MPNNs 1 on ZINC-12k dataset and OGB-HIV dataset [24] in table 3. We included baselines of (1) classical MPNNs: GCN[29], GIN [43], GINE [25], PNA[12], HIMP [17] (2) higher order MPNNs: $N^{2}$ -GNN [15] 2, CIN [6], $P$ -tensors [22] (3) Subgraph-GNNs: DS-GNN(EGO+) and DSS-GNN $\\mathrm{EGO+}$ ) [4], GNN-AK $^{+}$ [49], SUN(EGO $^+$ )[18] (4) Autobahn [40]. ", "page_idx": 7}, {"type": "text", "text": "We find that Schur Net ranked second on ZINC-12K and outperformed all other baselines on OGBHIV dataset. This shows the expressivity of adding more equivariant maps by leveraging the subgraph topology. Furthermore, note that while $N^{2}$ -GNN outperforms Schur Net on ZINC-12K, it\u2019s a second-order model whereas in our experiment, we only used first-order activation. Also, the partial reason Autobahn didn\u2019t perform well is in the original implementation, the author didn\u2019t use $P$ -tensor framework and used only a part of all possible linear message passing schemes. This shows to get the full power of equivariant maps w.r.t. subgraph automorphism group, we need to combine it with a general message passing framework between subgraphs as well. ", "page_idx": 7}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Unlike the representation theoretic approach, the spectral approach is not guaranteed the give the finest possible decomposition into invariant subspaces. Hence, equations like (8) do not necessarily define the most general possible automorphism-equivariant linear maps. In this paper we did not investigate from a theoretical point of view the potential gap in expressivity due to this fact. In general, being able to craft automorphism-equivariant layers of any order for any types of subgraphs opens up a host of possibilities for making GNNs more powerful. Our experiments are limited to some of the simplest cases, such as exploiting cycles and edges. We also only used first order activations. ", "page_idx": 7}, {"type": "table", "img_path": "HRnSVflpgt/tmp/b3aa21a36cf6068ccbb162c916f58433468131ceec36ed575800b46b2eb2f40d.jpg", "table_caption": [], "table_footnote": ["Table 3: Comparison of different models on the ZINC-12K and OGBG-MOLHIV datasets. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Enforcing equivariance to the automorphism group of subgraphs in higher order neural networks seemingly requires the use of advanced tools from group representation theory. This is likely a large part of the reason why automorphism-based architectures such as [40] are not used more commonly in practical applications. In this paper we have shown that a simpler approach based on spectral graph theory, following the same underlying logic as the group theoretic approach but bypassing having to enumerate all irreducible representations of the automorphism group, can lead to an architecture that is almost as expressive. Our algorithm, called Schur Nets, easily generalizes to higher order activations, as well as incorporating other types of side information such as vertex labels. In a practical setting, Schur Nets is easiest to deploy in conjunction with an another algorithm like P\u2013tensors [22] which hides the complexities of the higher order message passing component. The empirical performance of Schur Nets on the ZINC 12K dataset is superiror to all other comparable (non-transformer based) architectures that we are aware of. ", "page_idx": 8}, {"type": "text", "text": "Our approach exposes heretofore unexplored connections between permutation equivariance and spectral GNNs such as [9, 23]. It also highlights the fact that while permutation equivariance is a fundamental constraint on graph neural networks, the key to building high performing, expressive GNNs is to use as much side information as possible, whether it be the adjacency matrix, vertex degrees or something else, to reduce the size of the group that the network needs to be equivariant to. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We would like to thank Andrew Hands for many valuable pieces of advice and much practical assistance that he has given to the experimental side of this project. We also gratefully acknowledge the Toyota Technological Institute of Chicago and the Data Science Institute at the University of Chicago for making their computational resources available for our use, as well as NSF MRI-1828629 for additional infrastructure that was used in the course in this project. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to Represent Programs with Graphs. In International Conference on Learning Representations (ICLR), 2018.   \n[2] Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. Subgraph Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[3] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, and et al. Relational Inductive Biases, Deep Learning, and Graph Networks. 2018.   \n[4] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant Subgraph Aggregation Networks. In International Conference on Learning Representations (ICLR), 2022.   \n[5] Deyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. A Survey on Spectral Graph Neural Networks. 2023.   \n[6] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Li\u00f2, Guido Montufar, and Michael Bronstein. Weisfeiler and Lehman Go Cellular: CW Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[7] Cristian Bodnar, Fabrizio Frasca, Yu Guang Wang, Nina Otter, Guido Mont\u00fafar, Pietro Li\u00f2, and Michael Bronstein. Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks. arXiv:2103.03212, 2021.   \n[8] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. In International Conference on Learning Representations (ICLR), 2020.   \n[9] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally Connected Networks on Graphs. In International Conference on Learning Representations (ICLR), 2014.   \n[10] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can Graph Neural Networks Count Substructures? In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[11] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge Equivariant Convolutional Networks and the Icosahedral CNN. In International Conference on Machine Learning (ICML), 2019.   \n[12] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Velickovic. Principal Neighbourhood Aggregation for Graph Nets. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[13] Pim de Haan, Taco S. Cohen, and Max Welling. Natural Graph Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[14] Nima Dehmamy, Albert-L\u00e1szl\u00f3 Barab\u00e1si, and Rose Yu. Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[15] Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan Zhang, and Yixin Chen. Extending the Design Space of Graph Neural Networks by Rethinking Folklore WeisfeilerLehman. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[16] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph Neural Networks. 33rd AAAI Conference on Artificial Intelligence, 2019.   \n[17] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical Inter-Message Passing for Learning on Molecular Graphs. arXiv:2006.12179, 2020.   \n[18] Fabrizio Frasca, Beatrice Bevilacqua, Michael M. Bronstein, and Haggai Maron. Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[19] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural Message Passing for Quantum Chemistry. International Conference on Machine Learning (ICML), 2017.   \n[20] Martin Grohe. Descriptive Complexity, Canonisation, and Definable Graph Structure Theory. Cambridge University Press, 2017.   \n[21] Rafael G\u00f3mez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Al\u00e1n Aspuru-Guzik. Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. ACS Central Science, 2018.   \n[22] Andrew R. Hands, Tianyi Sun, and Risi Kondor. P-Tensors: A General Framework for Higher Order Message Passing in Subgraph Neural Networks. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics (AISTATS), 2024.   \n[23] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep Convolutional Networks on GraphStructured Data. arXiv:1506.05163, 2015.   \n[24] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[25] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for Pre-Training Graph Neural Networks. In International Conference on Learning Representations (ICLR), 2020.   \n[26] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning (ICML), 2015.   \n[27] John J. Irwin, Khanh G. Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R. Wong, Munkhzul Khurelbaatar, Yurii S. Moroz, John Mayfield, and Roger A. Sayle. ZINC20\u2014A Free Ultrlarge-Scale Chemical Database for Ligand Discovery. Journal of Chemical Information and Modeling, 2020.   \n[28] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations (ICLR), 2015.   \n[29] T. N. Kipf and M. Welling. Semi-Supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations (ICLR), 2017.   \n[30] Risi Kondor and Shubhendu Trivedi. On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups. In International Conference on Machine Learning (ICML), 2018.   \n[31] Andreas Loukas. How Hard is it to Distinguish Graphs with Graph Neural Networks? In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[32] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably Powerful Graph Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[33] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and Equivariant Graph Networks. In International Conference on Learning Representations (ICLR), 2019.   \n[34] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. TUDataset: A Collection of Benchmark Datasets for Learning with Graphs. arXiv:2007.08663, 2020.   \n[35] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks. In Proceedings of the Thirty-Third AAAI Conference, 2019.   \n[36] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance Through ParameterSharing. In International Conference on Machine Learning (ICML), 2017.   \n[37] B.E. Sagan. The Symmetric Group: Representations, Combinatorial Algorithms, and Symmetric Functions. Springer New York, 2013.   \n[38] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation Ratios of Graph Neural Networks for Combinatorial Problems. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[39] Jean-Pierre Serre. Linear Representations of Finite Groups. Springer-Verlag, 1977.   \n[40] Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: Automorphism-Based Graph Neural Nets. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[41] Erik Henning Thiede, Truong-Son Hy, and Risi Kondor. The General Theory of Permutation Equivariant Neural Networks and Higher Order Graph Variational Encoders. arXiv:2004.03990, 2020.   \n[42] Boris Weisfeiler and Andrei Leman. The Reduction of a Graph to a Canonical Form and the Algebra Which Appears Therein. Nauchno-Technicheskaya Informatsiya, 1968.   \n[43] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural Networks. In International Conference on Learning Representations (ICLR), 2019.   \n[44] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar. HyperGCN: A New Method for Training Graph Convolutional Networks on Hypergraphs. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[45] Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-Aware Graph Neural Networks. arXiv:2101.10320, 2021.   \n[46] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R. Salakhutdinov, and Alexander J. Smola. Deep Sets. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[47] Muhan Zhang and Pan Li. Nested Graph Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[48] Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-SAGNN: A Self-Attention Based Graph Neural Network for Hypergraphs. In International Conference on Learning Representations (ICLR), 2020.   \n[49] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness. In International Conference on Learning Representations (ICLR), 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 12}, {"type": "text", "text": "2 Background: equivariance with side information 2   \n2.1 Higher order GNNs . . 3   \n3 Equivariance to local permutations: the group theoretic approach 4   \n4 Equivariance via spectral graph theory: Schur layers 5   \n4.1 Automorphism invariance the simple way via spectral graph theory . . . 6   \n5 Experiments 7   \n5.1 Schur layer improves over Linmaps 7   \n5.2 Flexibility . . 8   \n5.3 Benchmark results 8 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "6 Limitations 8 ", "page_idx": 12}, {"type": "text", "text": "7 Conclusions 9 ", "page_idx": 12}, {"type": "text", "text": "A Additional details 14 ", "page_idx": 12}, {"type": "text", "text": "B Proofs 14   \nB.1 Alternative Proof for Corollary 14   \nC Related works 15   \nD Analysis of Schur layer 16   \nE A brief overview about $P$ -tensor framework 16   \nE.1 Message passing between $P$ -tensors with the same reference domain . . 17   \nE.2 Message passing between $P$ -tensors with the different reference domains . 17   \nF Group Representation Theory Background 18   \nG Ways to Use Schur Neuron 20   \nH More experiment results 21   \nI Architecture and experiment details 22 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The purpose of the permutation map $\\mathcal{P}_{\\mathbf{i}}$ in Theorem 3 is to remap the tensor product space $\\mathbb{R}^{\\mathrm{dim}\\left(\\boldsymbol{\\bar{U}}_{i_{1}}\\right)\\times\\ldots\\times\\mathrm{dim}\\left(\\boldsymbol{U}_{i_{k}}\\right)\\mathbf{\\bar{\\times}}C}$ into a canonical form so that those dimensions that correspond to $i_{j}=1$ are mapped to the first $n_{1}$ slots, those dimensions with $i_{j}=2$ are mapped to the second $n_{2}$ slots, and so on. Given the permutation $\\tau\\colon\\{1,2,\\ldots,k\\}\\to\\{1,2,\\ldots,k\\}$ that canonicalizes the indices in this way, $\\mathcal{P}_{\\mathbf{i}}$ is just the corresponding permutation map, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal P_{\\mathrm{i}}(x_{1}\\otimes...\\,x_{k}\\otimes x_{c})=x_{\\tau(1)}\\otimes..\\,.\\,x_{\\tau(k)}\\otimes x_{c}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 1. Let $\\{g\\downarrow_{U_{i}}:U_{i}\\to U_{i}\\}_{g\\in G}$ be the restriction of the action of $G$ to $U_{i}$ . Since $G$ fixes each $U_{i}$ , the action of $G$ on the full space decomposes in the form ", "page_idx": 13}, {"type": "equation", "text": "$$\ng(u)=\\sum_{i=1}^{p}g\\mathsf{i}_{u_{i}}(u\\mathsf{i}_{U_{i}})\\qquad\\qquad\\qquad\\qquad u\\in U.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now since multiplication by scalars commutes with linear maps, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi(g(u))=\\sum_{i=1}^{p}\\alpha_{i}\\,g\\downarrow_{u_{i}}(u\\downarrow_{U_{i}})=\\sum_{i=1}^{p}g\\downarrow_{u_{i}}(\\alpha_{i}\\,u\\downarrow_{U_{i}})=g(\\phi(u))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for any $u\\in U$ and any $g\\in G$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 2. Each of the $V_{j}^{i}$ subspaces is invariant, hence a homothety on each is an equivariant operation for the same reason as in Theorem 1. In addition, since $G$ acts the same way on any pair of subspaces $(V_{j}^{i},V_{j^{\\prime}}^{i})$ , any map of the form $\\xi_{j,j^{\\prime}}^{i}\\colon V_{j}^{i}V_{j^{\\prime}}^{i}$ that is just a scaling is also equivariant. The composition of equivariant linear maps is an equivariant linear map, hence any map $\\phi$ of the given form is equivariant. \u25a0 ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3. For any multi-index i, the subspace $\\operatorname{Im}(M_{i_{1}}^{\\top}\\otimes M_{i_{2}}^{\\top}\\otimes\\ldots\\otimes M_{i_{k}}^{\\top}\\otimes I_{\\mathrm{c}})$ is invariant to permutations. Further after applying the permutation map $\\mathcal{P}_{\\mathbf{i}}$ , all such subspaces of the same type $\\mathbf{n}$ , the action of ${\\mathbb S}_{k}$ on all such subspaces will be the same. Hence we can directly apply Theorem 2 to prove this theorem. ", "page_idx": 13}, {"type": "text", "text": "B.1 Alternative Proof for Corollary 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we give a more direct proof for Corollary 1. ", "page_idx": 13}, {"type": "text", "text": "Proof. Without loss of generality, we assume $c_{i n}\\;=\\;c_{o u t}\\;=\\;1$ . Since the $M_{i}$ \u2019s and $\\lambda_{i}$ \u2019s are eigenspaces and eigenvectors of the Laplacian $L$ , $L=M\\Lambda M^{T}$ , where $M=[M_{1},\\cdot\\cdot\\cdot\\,,M_{p}]$ and $\\Lambda{\\stackrel{\\_}{=}}\\,\\mathrm{diag}(\\lambda_{1},\\cdot\\cdot\\cdot\\,,\\bar{\\lambda}_{m})$ . ", "page_idx": 13}, {"type": "text", "text": "The neuron $\\mathfrak{n}_{S}$ is a linear transform $\\phi:R^{m}\\rightarrow R^{m}$ , with $\\phi(T)=\\Sigma_{i}^{p}M_{i}M_{i}^{T}T W_{i}=M V M^{T}T,$ where $V=\\mathrm{diag}(W_{1}I_{n_{1}},\\cdot\\cdot\\cdot\\,,W_{p}I_{n_{p}})$ . Here the $W_{i}$ \u2019s are just scalars and $I_{n_{i}}$ is an identity matrix of the same size as the corresponding eigenspace. ", "page_idx": 13}, {"type": "text", "text": "Given a permutation $\\sigma\\in S_{n}$ , we want to show that $\\phi_{\\sigma}(\\sigma\\circ T)=\\sigma\\circ\\phi_{e}(T)$ , which is the definition of equivariance. Note that $\\phi_{\\sigma}$ actually depends on $\\sigma$ , since we\u2019re doing eigenvalue decomposition on the Laplacian of the subgraph transformed by $\\sigma$ . This is the key to proving equivariance. We use $e$ to denote the identity permutation. ", "page_idx": 13}, {"type": "text", "text": "Now $L^{\\sigma}=P_{\\sigma}L P_{\\sigma}^{T}=M_{\\sigma}\\Lambda M_{\\sigma}^{T}$ , where $M_{\\sigma}=P_{\\sigma}M$ , while the eigenvalues remain invariant after permutation and eigenspaces are transformed in the same manner as $L$ . Thus, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{\\sigma}(\\sigma\\circ T)=M_{\\sigma}V M_{\\sigma}^{\\top}P_{\\sigma}T}\\\\ &{\\qquad\\qquad\\qquad=P_{\\sigma}M V M_{\\sigma}^{\\top}P_{\\sigma}^{\\top}P_{\\sigma}T}\\\\ &{\\qquad\\qquad\\qquad=P_{\\sigma}\\,\\phi_{e}(T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Remark 1. The key to the proof is that the transform $\\phi$ depends on an object transformed with permutation $\\sigma$ , specifically. graph Laplacian $L$ . And all we need is that the object $L$ is transformed equivariantly with $\\sigma$ . We can also add side informatino such as node labels or degrees to further constrain the automorphism group and increase the number of distinct eigenvalues. One easy way to do that is to set ${\\cal L}^{\\prime}={\\cal L}+V$ , where $V=d i a g(v_{1},\\cdots\\,,v_{m})$ captures the node labels or degrees, and build the Schur neuron $\\mathfrak{n}_{S}$ from $L^{\\prime}$ . ", "page_idx": 14}, {"type": "text", "text": "C Related works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Higher order MPNNs have become a popular research area in graph representation learning since the seminal work by [33], which characterized the space of equivariant map w.r.t. $\\mathbb{S}_{n}$ for $k$ \u2019th order tensors. The following works have mainly been divided into two streams, the one is based on $k$ -Weisfeiler Leman test ( $\\boldsymbol{k}$ -WL test) [42, 20], for instances, [32] proposed $k$ -order graph neural networks that is as powerful as $k$ -WL test; [35] proposed $k$ -GNN to approximate (or simulate) $k$ -WL test; [15] further designed a $(k,t){\\mathrm{-FWL+}}$ hierarchy that extends the $k$ -WL test and implemented the corresponding neuron version. A common feature of these approaches is they all work on all $k$ -tuples of vertices for $k$ -th order neural networks and thus make their space and time complexity at least $\\theta(n^{k})$ . ", "page_idx": 14}, {"type": "text", "text": "The other line of work is seeking ways to choose subgraphs in the graph, that are representative or contain important information, such as functional groups in a molecule. [7] chooses simplicial complex and [6] further extends this to any cell complex such as cycles. [22] incorporates any subgraph template and uses the equivariant maps defined [33] for local feature transform and devised a high order message passing scheme between those subgraphs, called $P_{\\|}$ -tensor. This is arguably the most general framework in this line of work. This line of work is kind of independent of the $k$ -WL test since the $k$ -WL test is aimed to distinguish any pair of non-isomorphic graphs while chosen subgraphs are meant to work in some specific regions with domain prior. It\u2019s possible to still compare this kind of network with the distinguishing power of $k$ -WL test, for example, [6] shows if they include cycles of size at most $k$ (including nodes and edges), their network are as powerful as WL test. However, such comparisons are not very meaningful to indicate the expressive power of such a domain-specific approach. In our opinion, the ability to learn certain features (such as count cycles of certain sizes or learn some function related to specific functional groups) might be a better criterion. It\u2019ll be an interesting research direction to design suitable criteria for the expressive power of such higher order MPNNs in general. ", "page_idx": 14}, {"type": "text", "text": "Our work follows this line of work since we\u2019re choosing specific subgraphs to learn representation on. But we utilize the subgraph\u2019s automorphism group to devise more possible equivariant maps (none of the aforementioned methods are built on the automorphism group). The only other work to our knowledge that uses the automorphism group is Autobahn [40], which is the first to introduce equivariance to the automorphism group to GNNs. The difference between our work and it lines twofold: (1) Autobahn theoretically introduces equivariant maps w.r.t. automorphism group via generalized convolution and Cosets, which is another group theoretical approach to construct equivariant maps, while we\u2019re using the decomposition to irreps (2) More importantly, Autobahn didn\u2019t mention a practical approach to construct those equivariant maps explicitly, hindering the utilization of more diverse subgraphs. In their experiments, they only use cycles of length five and six and paths of length three to six and construct the equivariant maps potentially by hand. In contrast, we give a simple and efficient way to construct equivariant maps w.r.t. automorphism group by EVD, which is very general to be used by any subgraph. Though our approach doesn\u2019t give the full set of equivariant maps, experiments show improvement over the traditional approach where only equivariant maps w.r.t. $S_{n}$ are used. We believe this algorithm, together with the group theoretical idea about decomposition $R^{m^{k}}$ into stable subspaces and irreps are beneficial to the research community. ", "page_idx": 14}, {"type": "text", "text": "There is a \"third\" type of higher order GNNs that is called Subgraph GNN, which deviates from the original definition of higher order MPNNs but can still be considered as higher order network. In particular, node-based GNNs such as [47, 49, 4, 45, 18] associate each node with a subgraph (by deleting the node, marking the node or extracting the EGO-network) and do MPNN on each subgraph, where they get the resulting representation $X\\in R^{n\\times n}$ that can be considered as a 2nd order tensor representation on the original graph. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "D Analysis of Schur layer ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "First of all, we notice that the equivariant map characterized in 1 (or 3) may not be the full set of equivariant maps w.r.t. $A u t_{S}$ . Because (1) In the decomposition of $U\\,=\\,\\dot{R}^{m^{k}}$ to eigenspaces $U=U_{1}\\oplus\\ldots U_{t}$ , while $U_{i}$ is stable subspaces, it might not be irreducible and finer decomposition may be possible. In other words, there might be two irreps (isomorphic or not) corresponding to the same eigenvalue. (2) The maps defined by 1 didn\u2019t take into account the isomorphic subspaces corresponding to the same type of irrep, thus ignored the possible equivariant maps between ${\\bar{V}}_{j}^{i}$ and $V_{j^{\\prime}}^{i}$ . So, it is of interest to find out how much the gap would be between our approach and the group theoretical approach. We first look at some examples in the first-order case (see table 4). ", "page_idx": 15}, {"type": "table", "img_path": "HRnSVflpgt/tmp/2bf3e4e064fcdd95579972725fcb64710148b563c7802e4b67f22e8689f8e1bd.jpg", "table_caption": [], "table_footnote": ["Table 4: Examples on EVD approach vs group theoretical approach towards # of equivariant maps w.r.t. $A u t_{S}$ . To calculate the decomposition of $R^{m}$ into irreps, one can calculate $\\kappa_{i}=(\\phi|\\chi_{i})$ where $\\phi$ and $\\chi_{i}$ is the character of the first-order action and ith irrep respectively, and (|) is inner product. Another quick approach is to use $\\begin{array}{r}{\\phi(\\sigma)=\\sum_{k}\\kappa_{k}\\chi_{k}(\\sigma)}\\end{array}$ and look at some examples of $\\sigma$ to determine what the $\\kappa_{k}$ should be. "], "page_idx": 15}, {"type": "text", "text": "We note that for cycles in the graph case, there\u2019s no gap. However, when we add branches to the cycle to make the automorphism group smaller, the multiplicities of the irreps increase, e.g., in 5-cycle with one branch case, $S_{2}$ only have two 1-dimensional irreps: trivial and sign representation of permutation, and the first-order action decompose to 4 copies of trivial and 2 copies of sign representation, gives in total $4*4+2*2=20$ possible equivariant maps. However, note that # of irreps (counting multiplicities given by $\\sum_{i}\\kappa_{i})$ is equal to # of distinct eigenvalues, meaning that each eigenspace corresponds to an irreducible subspace and the decomposition of $R^{m}$ provided by eigenspaces is indeed a decomposition to the irreps in all of the cases listed in the table. Therefore, the multiplicities are the key reason for the gap between our EVD approach and the group theoretical approach, since our EVD approach can\u2019t capture the isomorphic property between subspaces. In principle, it is possible to find out which eigenspace is isomorphic to which, but in our current implementation, we didn\u2019t take this into account because we found out that only considering cycles is enough for a very good performance in the datasets we used. ", "page_idx": 15}, {"type": "text", "text": "Generally, it\u2019s complicated to determine the gap between our EVD approach and the group theoretic approach, especially in higher-order cases (where merely determining the $\\kappa_{i}$ \u2019s is tricky), so we leave this into feature exploration. ", "page_idx": 15}, {"type": "text", "text": "E A brief overview about $P$ -tensor framework ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The $P$ -tensor framework is a framework for linear equivariant maps w.r.t. $S_{n}$ both between the same subgraph and across different subgraphs. It is built on the equivariant maps characterized by [33]. ", "page_idx": 15}, {"type": "text", "text": "Definition 1 ( $P$ -tensors). Let $U$ be a finite set of atoms and $\\boldsymbol{D}=\\left(x_{1},\\ldots,x_{d}\\right)$ an ordered subset of $U$ . We say that a $k$ -th order tensor $T\\in\\mathbb{R}^{d\\times d\\times\\cdots\\times d}$ is a $k$ -th order permutation covariant tensor (or $P$ -tensor for short) with reference domain $D$ if under reordering by $\\tau\\in S_{d}\\,D$ it transforms to ", "page_idx": 16}, {"type": "equation", "text": "$$\n[\\tau\\circ T]_{i_{1},i_{2},\\ldots,i_{k}}=T_{\\tau^{-1}(i_{1}),\\ldots,\\tau^{-1}(i_{k})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E.1 Message passing between $P$ -tensors with the same reference domain ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Consider the equivariant maps sending $T$ on $D$ to $T^{\\mathrm{out}}$ on $D$ . The space of equivariant maps w.r.t $S_{m}$ is characterized by [33]: ", "page_idx": 16}, {"type": "text", "text": "Proposition 1. The space of linear maps $\\phi:\\mathbb{R}^{d k_{1}}\\,\\to\\,\\mathbb{R}^{d k_{2}}$ that is equivariant to permutations $\\tau\\in S_{d}$ is spanned by a basis indexed by the partitions of the set $\\{1,2,\\ldots,k_{1}+k_{2}\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Then the authors designed a straightforward way to write the maps explicitly. Specifically, for each partition of the set $\\{1,2,\\ldots,k_{1}+k_{2}\\}$ , there are three parts that determines the equivariant map: (1) summing over specific dimensions or diagonals of $T^{\\mathrm{in}}$ (2) transferring $T^{\\mathrm{in}}$ to $T^{\\mathrm{out}}$ by identifying indices of $T^{\\mathrm{in}}$ with indices of $T^{\\mathrm{out}}$ (3) broadcasting the result along certain dimensions of $T^{\\mathrm{out}}$ . These three operations correspond to the three different types of sets that can occur in a given partition $\\mathcal{P}$ : (1) those that only involve the second $k2$ numbers, (2) those that involve a mixture of the first $k_{1}$ and $k_{2}$ and (3) those only involve the first $k_{1}$ numbers. The type (1) - (3) corresponds to type (1) - (3) of operations with the dimensions in the sets. ", "page_idx": 16}, {"type": "text", "text": "For example, in the case $k_{1}=k_{2}=3$ , the $\\mathcal{P}=\\{\\{1,3\\},\\{2,5,6\\},\\{4\\}\\}$ partition corresponds to (a) summing $\\bar{T}^{\\mathrm{in}}$ along its first dimension (corresponding to $\\{4\\}$ ) (b) transferring the diagonal along the second and third dimension of $T^{\\mathrm{in}}$ to the second dimension of $T^{\\mathrm{out}}$ (corresponding to $\\{2,5,6\\}\\rangle$ ) (c) broadcasting the result along the diagonal of the first and third dimensions (corresponding to $\\{1,3\\}$ ). Explicitly, this gives the equivariant map: ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{a,b,a}^{\\mathrm{out}}=\\sum_{c}T_{c,b,b}^{\\mathrm{in}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "See table 5 for another example for $k_{1}=k_{2}=2$ case. ", "page_idx": 16}, {"type": "table", "img_path": "HRnSVflpgt/tmp/6cb48cbb5a1d58af89ae416a5fba65aa2e7efbe7e819c13ae3f2c759c181fe45.jpg", "table_caption": [], "table_footnote": ["Table 5: The $\\overline{{B(4)=15}}$ possible partitions of the set $\\overline{{\\{1,2,3,4\\}}}$ and the corresponding permutation equivariant linear maps $\\phi:\\mathbb{R}^{k\\times k}\\rightarrow\\mathbb{R}^{k\\times k}$ . "], "page_idx": 16}, {"type": "text", "text": "E.2 Message passing between $P$ -tensors with the different reference domains ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "If $T^{\\mathrm{in}}$ has reference domain $D_{1}$ and $T^{\\mathrm{out}}$ has $D_{2}$ , with $D_{1}\\neq D_{2}$ and $D_{1}\\cap D_{2}\\neq\\emptyset$ . We could have more options corresponding to summing either over the intersection or over $D_{1}$ , and broadcasting either over the intersection or over $D_{2}$ . So the number of maps corresponging to partition of type $(p_{1},p_{2},p_{3})$ is $2^{(p_{1}+p_{3})}$ . Let $D_{1}\\cap D_{2}=d^{\\cap}$ , the previous example would have the following maps in $D_{1}\\neq D_{2}$ case: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{a,b,a}^{\\mathrm{out}}=\\left\\{\\sum_{c=1}^{d^{\\cap}}T_{c,b,b}^{\\mathrm{in}}\\quad a,b\\leq d^{\\cap}\\right.}\\\\ &{\\mathrm{otherwise},\\quad\\left(a\\in\\{1,\\ldots,d^{\\cap}\\},\\ c\\in\\{1,\\ldots,d^{\\cap}\\}\\right)}\\\\ &{T_{a,b,a}^{\\mathrm{out}}=\\left\\{\\sum_{c=1}^{d^{\\cap}}T_{c,b,b}^{\\mathrm{in}}\\quad b\\leq d^{\\cap}\\right.}\\\\ &{\\mathrm{otherwise},\\quad\\left(a\\in\\{1,\\ldots,d_{2}\\},\\ c\\in\\{1,\\ldots,d^{\\cap}\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{a,b,a}^{\\mathrm{out}}=\\left\\{\\sum_{c=1}^{d_{1}}T_{c,b,b}^{\\mathrm{in}}\\quad a,b\\leq d^{\\cap}\\right.}\\\\ &{\\mathrm{otherwise},\\quad\\left(a\\in\\{1,\\dots,d^{\\cap}\\},\\ c\\in\\{1,\\dots,d_{1}\\}\\right)}\\\\ &{T_{a,b,a}^{\\mathrm{out}}=\\left\\{\\sum_{c=1}^{d_{1}}T_{c,b,b}^{\\mathrm{in}}\\quad b\\leq d^{\\cap}\\right.}\\\\ &{\\mathrm{otherwise},\\quad\\left(a\\in\\{1,\\dots,d_{2}\\},\\ c\\in\\{1,\\dots,d_{1}\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "F Group Representation Theory Background ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we explain in detail the decomposition of $k$ -th order permutation to irreps of $S_{m}$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition 2. Let $\\rho_{\\lambda},\\lambda\\vdash m$ be the irreps of $S_{m}$ , $U=R^{m^{k}}$ , $\\sigma\\in S_{m}$ act on $U$ in the manner of equation $^{6}$ , and suppose this action contains irreps $\\rho_{\\lambda_{1}},\\ldots,\\rho_{\\lambda_{p}}$ with multiplicities $\\kappa_{1},\\hdots,\\kappa_{p}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nU=U_{1}\\oplus U_{2}\\oplus\\cdots\\oplus U_{p},\\quad U_{i}=\\bigoplus_{j}^{\\kappa_{i}}V_{i}^{j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first part gives the canonical decomposition of $U$ and the second part further decompose each $U_{i}$ into irreducible subspaces, where $V_{i}^{j}$ correspond to $\\rho_{\\lambda_{i}}$ . In matrix form, ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{\\sigma}^{(k)}=M\\left(\\begin{array}{c c c c}{\\rho_{\\lambda_{1}}(\\sigma)}&{0}&{\\cdots\\cdot}&{0}\\\\ {0}&{\\rho_{\\lambda_{2}}(\\sigma)}&{\\cdots\\cdot}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots\\cdot}&{\\rho_{\\lambda_{p}}(\\sigma)}\\end{array}\\right)M^{T},\\quad f o r\\,a l l\\,\\sigma\\in S_{m}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $M$ is the orthogonal transformation for basis and we abuse $\\rho_{\\lambda_{i}}(\\sigma)$ to denote also the matrix of i-th irrep in the decomposition. ", "page_idx": 17}, {"type": "text", "text": "If we take a closer look at $M$ , we can find that since $P_{\\sigma}^{(k)}$ is under the standard basis of $R^{m^{k}}$ , thus $M$ is just the orthonormal basis of each $U_{i}$ (thus each $V_{i}^{j}$ ) combined together, we denote $M=(M_{1},\\ldots,M_{p})$ with $M_{i}$ a $m^{k}\\times(d_{\\lambda_{i}}*\\kappa_{i})$ dimensional matrix, where $d_{\\lambda_{i}}$ is the degree of $\\rho_{\\lambda_{i}}$ . Therefore, $P_{\\sigma}^{(k)}T$ becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{\\sigma}^{(k)}T=[M_{1},M_{2},\\ldots,M_{p}]\\left(\\begin{array}{c c c c}{\\rho_{\\lambda_{1}}(\\sigma)}&{0}&{\\cdots}&{0}\\\\ {0}&{\\rho_{\\lambda_{2}}(\\sigma)}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{\\rho_{\\lambda_{p}}(\\sigma)}\\end{array}\\right)\\left(\\begin{array}{c}{M_{1}^{T}}\\\\ {M_{2}^{T}}\\\\ {\\vdots}\\\\ {M_{p}^{T}}\\end{array}\\right)\\,T\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The part (1) is a generalized Fourier transform of $T$ to its Fourier components (coordinates under $\\begin{array}{r l}&{\\mathrm{the~orthonormal~basis}\\,\\hat{T}=\\left(\\begin{array}{c}{M_{1}^{T}T}\\\\ {M_{2}^{T}T}\\\\ {\\vdots}\\\\ {M_{p}^{T}T}\\end{array}\\right)\\triangleq\\left(\\begin{array}{c}{B_{1}}\\\\ {B_{2}}\\\\ {\\vdots}\\\\ {\\dot{B}_{p}}\\end{array}\\right)\\mathrm{,~and~the~part~}(2)\\mathrm{~is~the~}i r r e p s\\,\\rho_{\\lambda_{1}},\\ldots,\\rho_{\\lambda_{p}}\\mathrm{~add~}\\rho_{\\lambda_{p}}}\\\\ &{\\mathrm{independently~on~}\\hat{T}\\mathrm{~with~each~component~}B_{i}\\mapsto\\left(\\begin{array}{c c c}{\\rho_{\\lambda_{i}}(\\sigma)}&{\\cdots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\cdots}&{\\rho_{\\lambda_{i}}(\\sigma)}\\end{array}\\right)B_{i}.\\mathrm{~Note~that~therepents}}\\end{array}$ t $\\kappa_{i}$ multiple of $\\rho_{\\lambda_{i}}$ act the same on components of $B_{i}$ correspond to $V_{i}^{j}$ , with a slight abuse of notation, we can think $B_{i}\\in R^{d_{\\lambda_{i}}\\times\\kappa_{i}}$ (instead of $R^{(d_{\\lambda_{i}}*\\kappa_{i})\\times1})$ and write this map as $\\rho_{\\lambda_{i}}(\\sigma)B_{i}$ , the fact that the map by irrep $\\rho_{\\lambda_{i}}$ act independently on each column of $B_{i}$ allow us to identify directly a set of equivariant maps by multiplying $B_{i}$ with matrix $W_{i}\\in R^{\\kappa_{i}\\times\\kappa_{i}}$ to the right, and using associativity of matrix multiplication: $\\rho_{\\lambda_{i}}(\\sigma)(B_{i}W_{i})=(\\rho_{\\lambda_{i}}(\\sigma)B_{i})W_{i}$ . This gives in total of $\\sum_{i}\\bar{\\kappa}_{i}^{2}$ independent equivariant maps, as stated in the main text. The last part of the above equ ation maps Fourier components to their original space. In short, we can write: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{\\sigma}^{(k)}T=\\sum_{i}M_{i}\\rho_{\\lambda_{i}}(\\sigma)\\underbrace{M_{i}^{T}T}_{B_{i}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with the abuse of notation to rearrange elements in $B_{i}$ mentioned above. ", "page_idx": 18}, {"type": "text", "text": "Then we present a detailed version of theorem 2 in the main text. ", "page_idx": 18}, {"type": "text", "text": "Theorem 4 (Necessary and sufficient condition for equivariant map). Let $G$ be a finite group acting on a vector space $U$ by the linear action $\\{g:U\\to U\\}_{g\\in G}$ and assume the action can be decomposed into irreps $\\rho_{1},\\ldots,\\rho_{p}$ with multiplicities $\\kappa_{1},\\hdots,\\kappa_{p}$ and degree $d_{i}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nU=U_{1}\\oplus U_{2}\\oplus\\cdots\\oplus U_{p},\\quad U_{i}=\\bigoplus_{j=1}^{\\kappa_{i}}V_{i}^{j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then $\\phi:U\\rightarrow U$ is an equivariant map w.r.t. this group action if and only if $\\phi$ is of the form: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi(v)=\\sum_{j^{\\prime}}\\alpha_{j,j^{\\prime}}^{i}\\tau_{j\\rightarrow j^{\\prime}}^{i}(v)\\quad f o r\\,v\\in V_{j}^{i}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some fixed set of coefficient $\\{\\alpha_{j,j^{\\prime}}^{i}:i\\in[1,\\ldots,p],j,j^{\\prime}\\in[1,\\ldots,\\kappa_{i}]\\}$ . ", "page_idx": 18}, {"type": "text", "text": "In matrix form, suppose the matrix of $g$ is $R_{g}$ under basis $(e_{i})$ and $d i m(U)=n$ , and it decompose into: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{R_{g}=M\\left(\\begin{array}{c c c c}{{\\displaystyle\\rho_{1}(g)}}&{{0}}&{{\\cdots}}&{{0}}\\\\ {{0}}&{{\\rho_{2}(g)}}&{{\\cdots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{0}}&{{0}}&{{\\cdots}}&{{\\rho_{p}(g)}}\\end{array}\\right)M^{T}}}\\\\ {{=\\displaystyle\\sum_{i}M_{i}\\rho_{i}(g)M_{i}^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then $\\phi:R^{n}\\to R^{n}$ is equivariant if and only if it is of the form: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi(T)=M M^{T}T\\left(\\begin{array}{c c c c}{W_{1}}&{0}&{\\cdots}&{0}\\\\ {0}&{W_{2}}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{W_{p}}\\end{array}\\right)}\\\\ {=\\displaystyle\\sum_{i}M_{i}\\underbrace{M_{i}^{T}T}_{B_{i}}W_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $T\\in R^{n}$ , $W_{i}\\in R^{\\kappa_{i}\\times\\kappa_{i}}$ is the fixed coefficients and $B_{i}$ rearrange to $R^{d_{i}\\times\\kappa_{i}}$ when needed. The coefficients $\\alpha_{j,j^{\\prime}}^{i}$ corresponds to $(W_{i})_{j,j^{\\prime}}$ . 3 ", "page_idx": 18}, {"type": "text", "text": "Proof. Sufficiency: firstly, $\\alpha_{j,j^{\\prime}}^{i}\\tau_{j\\rightarrow j^{\\prime}}^{i}(v)$ is equivariant by definition of isomorphism map. And the equivariance of $\\phi$ follows from the fact that sum of equivariant maps are equivariant. ", "page_idx": 18}, {"type": "text", "text": "Necessity: consider $\\phi$ on $V_{j}^{i}$ and let $W=I m(\\phi|_{V_{j}^{i}})$ the image of $\\phi$ on $V_{j}^{i}$ . Since $\\phi\\circ g=g\\circ\\phi$ and $g(v)\\in V_{j}^{i}$ for any $v\\in V_{j}^{i}$ , we have $g(\\phi(v))=\\phi(g(v))\\in W$ , so $W$ is stable under the action. Thus we can decompose $W$ into irreducible spaces $W=W_{1}\\oplus...W_{p}$ . Apply this decomposition to $\\phi(v)$ for $v\\in V_{j}^{i}$ , we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi(v)=\\phi_{1}(v)+\\ldots+\\phi_{p}(v)\\quad{\\mathrm{where~}}\\phi_{i}(v)\\in W_{i}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In other words $\\phi_{k}=P r o j_{k}\\circ\\phi$ where $k$ is the projection of $W$ onto $W_{k}$ (uniquely defined by the direct sum decomposition). Then $\\phi_{k}:V_{j}^{i}\\to W_{k}$ and $\\phi_{k}\\circ g=g\\circ\\phi_{k}$ . By Schur\u2019s lemma [39], for $\\phi_{k}\\neq0$ , we must have $W_{k}$ isomorphic to $V_{j}^{i}$ and $\\phi_{k}(v)=\\theta_{k}\\tau_{V_{j}^{i},W_{k}}(v)$ . Since only $V_{j^{\\prime}}^{i}$ is isomorphic to $V_{j}^{i}$ , we have $W_{k}=V_{k}^{i}$ and $\\phi_{k}(v)=\\alpha_{j,k}^{i}\\tau_{j,k}(v)$ , where $\\tau_{j,k}(v)$ is the isomorphic map sending $V_{j}^{i}$ to $V_{k}^{i}$ . Thus $\\begin{array}{r}{\\phi(v)=\\sum_{k}\\phi_{k}(v)=\\sum_{k}\\alpha_{j,k}^{i}\\tau_{j,k}(v)}\\end{array}$ for $v\\in V_{j}^{i}$ . ", "page_idx": 18}, {"type": "text", "text": "Finally, by linearity and $U=\\bigoplus_{i,j}V_{j}^{i}$ , the only possible equivariant function $\\phi:U\\rightarrow U$ is given by 11. ", "page_idx": 18}, {"type": "text", "text": "Connection to matrix form: first note that the isomorphism $\\tau_{j\\rightarrow j^{\\prime}}^{i}:V_{j}^{i}\\rightarrow V_{j^{\\prime}}^{i}$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau_{j\\rightarrow j^{\\prime}}^{i}(u_{j,l}^{i})=u_{j^{\\prime},l}^{i}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\{u_{j,l}^{i},l=1,\\ldots,\\kappa_{i}\\}$ and $\\{u_{j^{\\prime},l}^{i},l=1,\\ldots,\\kappa_{i}\\}$ are orthonormal basis for $V_{j}^{i}$ and $V_{j^{\\prime}}^{i}$ respectively such that the action of $g$ is associated with matrix $\\rho_{i}(g)$ . Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\phi(u_{j,l}^{i})=\\sum_{j^{\\prime}}\\alpha_{j,j^{\\prime}}^{i}\\tau_{j\\rightarrow j^{\\prime}}^{i}(u_{j,l}^{i})}}\\\\ &{}&{=\\displaystyle\\sum_{j^{\\prime}}\\alpha_{j,j^{\\prime}}^{i}(u_{j^{\\prime},l}^{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus the matrix of $\\phi$ under basis $\\begin{array}{r c l c r c l}{(u_{j,l}^{i},i}&{=}&{1,\\ldots,p,j}&{=}&{1,\\ldots,\\kappa_{i},l}&{=}&{1,\\ldots,d_{i})}\\end{array}$ is ", "page_idx": 19}, {"type": "text", "text": "G Ways to Use Schur Neuron ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "While Schur neuron can be viewed as a generic transform on any subgraph\u2019s first-order activation, we provide some thoughts and possibilities to instantiate it. ", "page_idx": 19}, {"type": "text", "text": "In our experiment, we primarily view Schur neuron as a way to expand the feature of a subgraph. It can be viewed as an analogue to CNN\u2019s convolution filter and it\u2019s indeed a constrained version of spectral convolution filter [5] on the subgraph. In light of this, we call number of channels of Schur neuron as how many times it expand the output feature versus the input feature. ", "page_idx": 19}, {"type": "text", "text": "Besides, we observe that the linear equivariant map described [33] is a special case of Schur neuron\u2019s map. The two possible linear map in [33] is (1) identity map, corresponding to take all weights $W_{i}=1$ (2) $T\\rightarrow\\Sigma_{i}T_{i}\\mathbb{1}$ , where $\\mathbb{1}$ is all ones vector, corresponding to set $W_{1}=1$ , and $W_{i}=0$ for $i\\neq1$ since $\\mathbb{1}$ is eigenvector of any graph Laplacian with eigenvalue 0. Consequently, we suggest to keep the two basic but important linear maps and augment them with Schur neuron. Empirically we verified this by an experiment only vary the number of channels. In figure 1, we see that further increase the number of channels beyond 4 wouldn\u2019t give us performance gain since cycle 5 and 6 only has 3 and 4 distinct eigenvalues respectively. ", "page_idx": 19}, {"type": "text", "text": "Furthermore, we suggest that number of channels of Schur neuron be proportional or approximately equal to number of distinct eigenvalues of the subgraph it attach to. This is because the later is the number of independent linear maps for Schur neuron. ", "page_idx": 19}, {"type": "image", "img_path": "HRnSVflpgt/tmp/83a2b0d6479ddf4912e394e294c79a8ec2717ea577a371600a3cd76a8a685b03.jpg", "img_caption": ["range to $R^{d_{i}\\times\\kappa_{i}}$ ) and $M$ is just the matrix to transform standard basis $(e_{i})$ to basis $(u_{j,l}^{i})$ . ", "Figure 1: A study on num of channels in Schur layer. Cycle 5 and 6 are included. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Additonally, we\u2019re wondering if we can use Schur layer in place of MLP\u2019s to provide a local structure aware transform to subgraph activations. Therefore, we tried to replace the 2-layer MLP with 2-layer ", "page_idx": 19}, {"type": "text", "text": "Schur layer in the original Linmaps architecture. But it turns out this use case wasn\u2019t as effective as the use as learning new feature. Possibly because Schur layer learns feature itself while MLP transform the learned feature to some desired space. ", "page_idx": 20}, {"type": "table", "img_path": "HRnSVflpgt/tmp/eead7b34da02943863d4f18c2e4bbdc0329d3b0931abd048a9864eec2275c328.jpg", "table_caption": ["Table 6: Comparison about two use cases of Schur layer. Experiment on ZINC-12K. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Finally, regarding flexibility as discussed 5.2, in table 7, we see that adding cycles with branches could provide the Schur Net with more detailed topological information, thus improving the performance. And the code for adding this actually only requires a single definition of those templates without modification to the neural network. ", "page_idx": 20}, {"type": "table", "img_path": "HRnSVflpgt/tmp/636b43efd68ea09cd88c6f07d2d11c71a8630048005467fbb90ecf79baac7b05.jpg", "table_caption": ["Table 7: Flexibility of Schur layer. Experiments on ZINC-12k dataset. All other settings are the same. A smaller network than previous experiments was used. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "H More experiment results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first start with some molecular datasets in TUdataset [34], which is a small but commonly used benchmark for GNN. In table 8, we see that Schur layer consistently improves over Linmaps in various bioinformatics and molecule datasets. ", "page_idx": 20}, {"type": "table", "img_path": "HRnSVflpgt/tmp/3b3557de8790d6ea048dd35fd4bdebee1d9af42a0b2e666ca3f5faf6efa4988d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "In table 9, we compare the runtime of our Schur layer and Linmaps, which shows the extra computational cost of Schur layer wasn\u2019t significant while being able to use more equivariant maps and achieving better accuracy. ", "page_idx": 20}, {"type": "table", "img_path": "HRnSVflpgt/tmp/dab77d303142f917b3c37225f9c1b8eb0300c48b6457a88f5eb74da24d4f642e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 9: Runtime per epoch with hyper-params num_layers $=4$ , rep_ $\\dim=128$ , dropout $=0.0$ , batch_size $=256$ , num of channels $=4$ , cycle_sizes $=3{,}4{,}5{,}6{,}7{,}8$ . This shows Schur Layer didn\u2019t add much computational costs to Linmaps while being more expressive. ", "page_idx": 20}, {"type": "text", "text": "Another ablation study is perform on ZINC-12K dataset. In table 10, we see that Schur layer outperforms Linmaps when certain cycle sizes are considered, especially when only cycles 5 and 6 are chosen as subgraphs in the neural network. ", "page_idx": 20}, {"type": "table", "img_path": "HRnSVflpgt/tmp/a4ba4f76ccd6450af6db754f133312f5d8122cdb63c16f1f97e6cb16c06a8fd3.jpg", "table_caption": [], "table_footnote": ["Table 10: Comparison between Schur Layer and Linmaps with different set of cycles chosen. Experiments on ZINC-12k dataset and all scores are test MAE. "], "page_idx": 21}, {"type": "text", "text": "I Architecture and experiment details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In designing of our architecture, we design a message passing scheme similar to [22] and add Schur layer to each of the convolution layers. A visualization of our architecture can be found in figure ?? In each convolution layer, we maintain 0th-order node representation $(h_{v}\\in R^{|V|*d i m})$ and edge representation $(h_{e}\\in R^{|E|*d i m})$ where we assume the input graph $G=(V,E)$ . We further maintain 1st-order representation on cycles $(h_{D_{k}}\\in R^{(k*(c_{k})*d i m)})$ where $D_{k}$ is cycle of length $\\boldsymbol{\\mathrm{k}}$ and $c_{k}$ is the number of such cycle in $G$ . We did message passing between node and edge representation the same as CIN [6]. The edge and cycle pass message with each other by tranfer maps described by [22] and then the incoming message to cycles as well as the original cycle representation is transformed by Schur layer. When updating the edge representation and cycle representation, we combine the original representation with the incomming message by either concatenation or $\\epsilon$ -add described in GIN [43] and feed it into an MLP to get new representations. ", "page_idx": 21}, {"type": "text", "text": "Hyperparameters For experiments on ZINC-12K and OGB-HIV, we tune representation dimension in $\\{32,64,128\\}$ , and experiment on cycles up to length 18. In the best-performing model, we use representation dimension 128 and cycle lengths from 3 to 8 and number-of-layers is always 4. For Schur layer, we tuned number of channels from 2 to 8 and found 2, 4 are a suitable choice when it is used as an augmentation to Linmaps. For MLPs, we either use 2 to 3 layers depending on how dense the input\u2019s information is. We always use a batchnorm [26] after the Linear layer and then do ReLu activation. For training hyperparameters, we use an init learning rate of 0.01 and use ReduceLROnPlateau scheduler in PyTorch with patience of 10 or 30. We use Adam optimizer [28] for all trainings. We train for 500 or 1000 epochs depending on model size. In particular, in table 10, the models have representation dimension 32, so it is trained to only 500 epochs. All other models are trained 1000 epochs. A batch size of 256 is used for all models. All results are run at least three times to calculate the standard deviation. ", "page_idx": 21}, {"type": "text", "text": "For experiments on TUdataset (table 8), we chose a set of hyper-params by intuition (we didn\u2019t tune them because this experiment is to demonstrate the effectiveness of Schur layer and compare Schur layer with Linmaps under the same experiment setting. we don\u2019t aim to compare with Sota on this experiment). Hyper-params for both Schur Layer and Linmaps: num_layers $=4$ , rep_ $\\dim=32{,}64$ , dropout $=0.5$ , batch_size $=32{,}128\\$ , $\\mathrm{lr}=0.01$ , num of channels $=\\{2{,}4\\}$ , cycle s $\\mathrm{izes}=3{,}4{,}5{,}6$ . We\u2019re training with StepLR scheduler where reduce lr to $1/2$ after every 50 epochs, with a total of 300 epochs. The hyperparams weren\u2019t tuned, we just chose a smaller value for a smaller dataset and a bigger value for bigger datasets by rule-of-thumb. Linmaps is implemented on our own according to the description of $P_{\\|}$ -tensor, then Schur Layer replaces Linmaps operation by Schur operation. We follow the evaluation strategy specified in [43]. ", "page_idx": 21}, {"type": "text", "text": "Used Compute Resources Experiments are all run on one Tesla L4 from PyTorch Lightning Workspace and one NVIDIA RTX 4090. The code is implemented in PyTorch. For the running time, on the ZINC- $.12\\mathrm{k}$ dataset, it takes around $8.53\\pm1.2$ hours to finish training for 1000 epochs with 4 layers and 128 dimension representation on an NVIDIA RTX 4090. We\u2019re running on a desktop with a Ryzen 7900 CPU and 64GB of RAM. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The claims made in the paper relate to specific properties of message passing algorithms. The claims that we make about our own algorithm are accurate. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] . ", "page_idx": 22}, {"type": "text", "text": "Justification: We have a section on limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: For each Lemma and Theorem in the paper we provide a proof. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We document our experiments in detail, including the dataset, the architecture and parameters. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have included the link to a github repository containing all the flies needed to reproduce the experiments. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we give all these details in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: For each of our results on the benchmarks datasets we provide the standard deviation of the performance. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide this information in the Appendix. ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Justification: We have reviewed the code of ethics and our research conforms to it. ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work is foundational/theoretical in nature and we not foresee it having any direct social impacts. Indirectly, it could have impact because higher order automorphismequivariant could be used in drug discovery, for example. Applications with negative social impacts are also possible, but a little more difficult to pinpoint. Space limitations prevented us from discussing this topic in more detail. ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] .   \nJustification: The model itself does not pose such a risk. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide a citation for every baseline model and benchmark dataset that we use. All assets were used in a way that conforms to their published licenses. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not provide new assets. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This research did not involve crowdsourcing or human subjects research. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 23}, {"type": "text", "text": "Justification: This research did not involve crowdsourcing or human subjects research. ", "page_idx": 23}]