[{"type": "text", "text": "Membership Inference Attacks against Large Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhan $\\mathbf{L}\\mathbf{i}^{*}$ Yongtao Wu\u2217 Yihang Chen\u2217\u2020 Francesco Tonin Elias Abad Rocamora Volkan Cevher LIONS, EPFL [first name].[last name]@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ , which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The rise of large language models (LLMs) [9, 60, 45, 11] has inspired the exploration of large models across multi-modal domains, exemplified by advancements like GPT-4 [1] and Gemini [59]. These large vision-language models (VLLMs) have shown promising ability in various multi-modal tasks, such as image captioning [33], image question answering [13, 35], and image knowledge extraction [26]. However, the rapid advancement of VLLMs also causes user concerns about privacy and knowledge leakage. For instance, the image data used during commercial model training may contain private photographs or medical diagnostic records. This is concerning since early work has demonstrated that machine learning models can memorize and leak training data [3, 56, 63]. To mitigate such concerns, it is essential to consider the membership inference attack (MIA) [23, 53], where attackers seek to detect whether a particular data record is part of the training dataset [23, 53]. The study of MIAs plays an important role in preventing test data contamination and protecting data security, which is of great interest to both industry and academia [24, 19, 44]. ", "page_idx": 0}, {"type": "text", "text": "When exploring MIAs in VLLMs, one main issue is the absence of a standardized dataset designed to develop and evaluate different MIA methods, which comes from the large size [16] and multimodality of the training data, and the diverse VLLMs training pipelines [66, 35, 18]. Therefore, one of the main goals of this work is to build an MIA benchmark tailored for VLLMs. ", "page_idx": 0}, {"type": "text", "text": "Beyond the need for a valid benchmark, we lack efficient techniques to detect a single modality in VLLMs. The closest work to ours is [30], which performs MIAs on multi-modal CLIP [46] by detecting whether an image-text pair is in the training set. However, in practice, it is more common to detect a single modality, as we care whether an individual image or text is in the training set. Therefore, we aim to develop a pipeline to detect the single modality from a multi-modal model. Moreover, existing literature on language model MIAs, such as $\\mathbb{M}\\mathrm{in-K}\\%$ [52] and Perplexity [62], mostly are target-based MIAs, which use the next token as the target to compute the prediction probability. However, we can only access the image embedding instead of the image token in VLLMs, and thus only target-free MIAs [48] can be directly applied. ", "page_idx": 0}, {"type": "image", "img_path": "nv2Qt5cj1a/tmp/a7c9cd50e606f7dddaf7abd5863b3281ac35817d97c6d2bf6cf2cf7e9072f9a5.jpg", "img_caption": ["Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ metric: we first get the R\u00e9nyi entropy of each token position, then select the largest $k\\%$ tokens and calculate the average R\u00e9nyi entropy. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Therefore, we first propose a cross-modal pipeline for individual image or description MIAs on VLLMs, which is distinguished from traditional MIAs that only use one modality [61, 62]. We feed the VLLMs with a customized image-instruction pair from the target image or description. We show that we can perform the MIA not only by the image slice but also by the instruction and description slices of the VLLM\u2019s output logits, see Figure 1. Such a cross-modal pipeline enables the usage of text MIA methods on image MIAs. We also introduce a target-free metric that adapts to both image and text MIAs and can be further modified to a target-based way. ", "page_idx": 1}, {"type": "text", "text": "Overall, the contributions and insights can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We release the first benchmark tailored for the detection of training data in VLLMs, called Vision Language MIA (VL-MIA) (Section 4). By leveraging Flickr and GPT-4, we construct VL-MIA that contains two images MIA tasks and one text MIA task for various VLLMs, including MiniGPT-4 [66], LLaVA 1.5 [35] and LLaMA-Adapter V2 [18]. \u2022 We perform the first individual image or description MIAs on VLLMs in a cross-modal manner. Specifically, we demonstrate that we can perform image MIAs by computing statistics from the image or text slices of the VLLM\u2019s output logits (Figure 1 and Section 5.1). \u2022 We propose a target-free MIA metric, MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ , and its modified target-based ModR\u00e9nyi (Section 5.2). We demonstrate their effectiveness on open-source VLLMs and closed-source GPT-4 (Section 6). We achieve an AUC of 0.815 on GPT-4 in image MIAs. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Membership Inference Attack (MIA) aims to classify whether a data sample has been used in training a machine learning model [53]. Keeping training data confidential is a desired property for machine learning models, since training data may contain private information about an individual [62, 24]. Popular MIA methods can be divided into metric-based and shadow model-based MIAs [24]. Metric-based MIAs [62, 48, 57, 52] determine whether a data sample has been used for training by comparing metrics computed from the output of the target model with a threshold. Shadow model-based MIAs need shadow training to mimic the behavior of the target model [53, 48], which is computationally infeasible for LLMs. Therefore, we focus on the metric-based methods in this work. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "MIAs have been extensively researched in various machine learning models, including classification models [38, 58, 10], generative models [20, 22, 7], and embedding models [54, 40]. With the emergence of LLMs, there are also a lot of work exploring MIAs in LLMs [42, 17, 52, 41]. Nevertheless, MIAs for multi-modal models have not been fully explored. [30, 25] perform MIAs using the similarity between the image and the ground truth text, which detects the image-text pair instead of a single image or text sequence. However, detecting an individual image or text is more practical in real-world scenarios and poses additional challenges. To the best of our knowledge, we are the first to perform the individual image or text MIA on VLLMs. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we also consider a more difficult task, to detect the pre-training data from the fine-tuned models, that is, to detect the base LLM pertaining data from VLLMs. First, compared with detecting fine-tuning data [55, 51], pretraining data come from a much larger dataset and are used only once, reducing the potential probability for a successful MIA [27, 32]. In addition, compared to the detection of pretraining data from the pre-trained models [53, 52], catastrophic forgetting [29, 28, 16] in the fine-tuning stage also makes it harder to detect the pre-training data from downstream models. To the best of our knowledge, we are the first to perform pre-training data MIAs on fine-tuned models. ", "page_idx": 2}, {"type": "text", "text": "Large Vision-Language Models (VLLMs) incorporate visual preprocessors into LLMs [9, 60, 45] to manage tasks that require handling inputs from text and image modalities. A foundational approach in this area is represented by CLIP [46], which established techniques for aligning modalities between text and images. Further developments have integrated image encoders with LLMs to create enhanced VLLMs. These models are typically pre-trained on vast datasets of image-text pairs for feature alignment [33, 64, 39, 2], and are subsequently instruction-tuned for specific downstream tasks to refine the end ability. MiniGPT [66, 8], LLaVA [36, 35], and LLaMA Adapter [65, 18] series have demonstrated significant capabilities in understanding and inference in this area. ", "page_idx": 2}, {"type": "text", "text": "3 Problem setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce the main notation and problem settings for MIAs. ", "page_idx": 2}, {"type": "text", "text": "Notation. The token set is denoted by $\\nu$ . A sequence with $L$ tokens is denoted by $X\\ :=$ $(x_{1},x_{2},\\ldots,x_{L})$ , where $x_{i}\\in\\mathcal{V}$ for $i\\ \\in\\ [L]$ . Let $X_{1}\\oplus X_{2}$ be the concatenation of sequence $X_{1}$ and $X_{2}$ . An image token sequence is denoted by $Z$ . In this work, we focus on the VLLM, parameterized by $\\theta$ , where the input is the image $Z$ followed by the instruction text $X_{\\mathrm{ins}}$ , and the output is the description text $X_{\\mathrm{des}}$ . We use $\\mathcal{D}_{\\mathrm{des}}$ and $\\mathcal{D}_{\\mathrm{image}}$ to represent the description training set and image training set, respectively. Detailed notations are summarized in Table 5 of the appendix. ", "page_idx": 2}, {"type": "text", "text": "Attacker\u2019s goal. In this work, the purpose of the attacker is to detect whether a given data point (image $Z$ or description $X_{\\mathrm{des}},$ ) belongs to the training set. We formulate this attack as a binary classification problem. Let $\\mathbf{A}_{\\mathrm{image}}(Z;\\theta):\\to\\{0,1\\}$ and $\\mathbf{A}_{\\mathrm{des}}(X;\\theta):\\rightarrow\\{0,1\\}$ be two binary classification algorithms for image and description respectively, which are implemented by comparing the metric $\\mathtt{S c o r e}(Z\\oplus X_{\\mathrm{ins}}\\oplus X_{\\mathrm{des}};\\theta)$ with some threshold $\\lambda$ . ", "page_idx": 2}, {"type": "text", "text": "When detecting image $Z$ , we feed the model with the target image with a fixed instruction prompt such as \u201cDescribe this image in detail\u201d, denoted as $X_{\\mathrm{ins}}$ . The model then generates the description text $X_{\\mathrm{des}}^{\\prime}$ . The algorithm $\\mathbf{A}_{\\mathrm{image}}^{\\mathsf{-}}(Z;\\theta)$ is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}_{\\mathrm{image}}(Z;\\theta)=\\left\\{\\begin{array}{l l}{1}&{(Z\\in\\mathscr{D}_{\\mathrm{image}}),\\;\\mathrm{if}\\;\\mathbf{Score}(Z\\oplus X_{\\mathrm{ins}}\\oplus X_{\\mathrm{des}}^{\\prime};\\theta)<\\lambda,}\\\\ {0}&{(Z\\notin\\mathscr{D}_{\\mathrm{image}}),\\;\\mathrm{if}\\;\\mathbf{Score}(Z\\oplus X_{\\mathrm{ins}}\\oplus X_{\\mathrm{des}}^{\\prime};\\theta)\\geq\\lambda.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When detecting a description sequence $X_{\\mathrm{des}}$ , we feed the model with an all-black image, denoted as $Z_{\\mathrm{ept}}$ , as the visual input, followed by an empty instruction $X_{\\mathrm{ept}}$ . The algorithm $\\mathbf{A}_{\\mathrm{des}}\\bar{(X_{\\mathrm{des}};\\theta)}$ is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{A}_{\\mathrm{des}}(X_{\\mathrm{des}};\\theta)=\\left\\{\\begin{array}{l l}{1}&{(X_{\\mathrm{des}}\\in\\mathcal{D}_{\\mathrm{des}}),\\mathrm{~if~}\\mathbf{S}\\mathbf{core}(Z_{\\mathrm{ept}}\\oplus X_{\\mathrm{ept}}\\oplus X_{\\mathrm{des}};\\theta)<\\lambda,}\\\\ {0}&{(X_{\\mathrm{des}}\\notin\\mathcal{D}_{\\mathrm{des}}),\\mathrm{~if~}\\mathbf{S}\\mathbf{core}(Z_{\\mathrm{ept}}\\oplus X_{\\mathrm{ept}}\\oplus X_{\\mathrm{des}};\\theta)\\ge\\lambda.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Attacker\u2019s knowledge. We assume a grey-box setting on the target model, where the attacker can query the model by a custom prompt (including an image and text) and have access to the tokenizer, the output logits, and the generated text. The attacker is unaware of the training algorithm and parameters of the target model. ", "page_idx": 3}, {"type": "text", "text": "4 Dataset construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We construct a general dataset: Vision Language MIA (VL-MIA), based on the training data used for popular VLLMs, which, to our knowledge, is the first MIA dataset designed specifically for VLLMs. We present a takeaway overview of VL-MIA in Table 1. We also provide some examples in VL-MIA, see Table 16 in the appendix. The prompts we use for generation can be found in Table 6. ", "page_idx": 3}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/233faf89cadee6e1d4ab2bdb11fcc1b9f8133e75ef1edda7d417ca27c97ace0f.jpg", "table_caption": ["Table 1: Overview of VL-MIA dataset: VL-MIA covers image and text modalities and can be applied for dominant open-sourced VLLMs. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Target models. We perform MIAs open-source VLLMs, including MiniGPT-4 [66], LLaVA-1.5 [35], and LLaMA-Adapter V2.1 [18]. The checkpoints and training datasets of these models are provided and public. The training pipeline of a VLLM encompasses several stages. Initially, an LLM undergoes pre-training using extensive text data such as LLaMA [18]. Meanwhile, a vision preprocessor, e.g., CLIP [46], is pre-trained on a large number of image-text pairs. Subsequently, a VLLM is constructed based on the LLM and the vision preprocessor and is pre-trained using image-text pairs. The final stage involves instruction-tuning the VLLM, which can be performed using either image-text pairs or image-based question-answer data. In the instruction tuning stage of a VLLM, every data entry contains an image, a question, and the corresponding answer to the image. We use the answer text as member data and GPT-4 generated answers under the same question and same image as non-member data. Specifically, for LLaVA 1.5 and LLaMA-Adapter v2, we use the answers in LLaVA 1.5\u2019s instruction tuning as member data. ", "page_idx": 3}, {"type": "text", "text": "VL-MIA/DALL-E. MiniGPT-4, LLaVA 1.5, and LLaMA-Adapter V2 use images from LAION [49], Conceptual Captions 3M [6], Conceptual 12M [6] and SBU captions [43] datasets (collectively referred to as LAION-CCS) as pre-training data. BLIP [33] provides a synthetic dataset with imagecaption pairs for LAION-CCS used in MiniGPT-4 and LLaVA 1.5. We first detect the intersection of the training images used in these three VLLMs. From this intersection, we randomly select a subset to serve as the member data for our benchmark. For the non-member data, we use the corresponding BLIP captions of the member images as prompts to generate images with DALL-E $2^{3}$ . This process yields one-to-one corresponding pairs of the generated images (non-member) and the original member images. Consequently, our dataset comprises an equal number of member images from the LAION-CCS dataset and non-member images generated by DALL\u00b7E, allowing us to evaluate MIA performance comprehensively. We have 592 images in VL-MIA/DALL-E in total. ", "page_idx": 3}, {"type": "text", "text": "VL-MIA/Flickr. MS COCO [34] co-occurs as a widely used dataset in the training data of the target models, so we use the images in this dataset as member data. Given the fact that such member data are collected from Flickr4, we fliter Flickr photos by the year of upload and obtain new photos from January 1, 2024, as non-member data, which are later than the release of the target models. We additionally prepare a set of corrupted versions, where the member images are deliberately corrupted ", "page_idx": 3}, {"type": "text", "text": "to simulate real-world settings. More results of the corrupted versions are discussed in Section 6.5.   \nThis dataset contains 600 images. ", "page_idx": 4}, {"type": "text", "text": "VL-MIA/Text. We prepare text MIA datasets for the VLLMs instruction-tuning stage. LLaVA 1.5 and LLaMA Adapter v2.1 both use the LLaVA-Instruct-150K [35] in instruction-tuning, which consists of multi-round QA conversations. We first select entries with descriptive answers of 64 words. Next, we feed the corresponding questions and images into GPT- $.4^{3}$ , and ask GPT-4 to generate responses of the same length, treating these generated responses as non-member text data. In addition, since MiniGPT4 employs long descriptions of images for instruction-tuning, typically beginning with \u201cthe image\u201d, we prompt GPT to generate descriptions based on the MS COCO dataset, starting with \u201cthis image\u201d to ensure similar data distributions. We also prepare different versions of the datasets by truncating the text into different word lengths such as 16, 32, and 64. Each text dataset contains 600 samples. ", "page_idx": 4}, {"type": "text", "text": "VL-MIA/Synthetic We synthesize two new MIA datasets: VL-MIA/Geometry and VLMIA/Password. The image in the VL-MIA/Geometry consists of a random 4x4 arrangement of geometrical shapes, and the image in the VL-MIA/Password consists of a random 6x6 arrangement of characters and digits from MNIST [15] and EMINST [12]. The associated text for each image represents its content (e.g., specific characters, colors, or shapes), ordered from left to right and top to bottom. Half of the dataset can be selected as the member set for VLLM fine-tuning, with the remainder as the non-member set. This partition ensures that members and non-members are independently and identically distributed, avoiding the latent distribution shift between the members and non-members in current MIA datasets [14]. Our synthetic datasets are ready to use, and can be applied to evaluate any VLLM MIA methods through straightforward fine-tuning. We provide some examples of this dataset in Figure 5 in the Appendix C. ", "page_idx": 4}, {"type": "text", "text": "5 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1 A cross-modal pipeline to detect image ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "VLLMs such as LLaVA and MiniGPT project the vision encoder\u2019s embedding of the image into the feature space of LLM. However, a major challenge for image MIA is that we do not have the ground-truth image tokens. Only the embeddings of images are available, which prevents directly transferring many target-based MIA from languages to images. To this end, we propose a token-level image MIA which calculates metrics based on the output logit of each token position. ", "page_idx": 4}, {"type": "text", "text": "This pipeline consists of two stages, as demonstrated in Figure 1. In generation stage, we provide the model with an image followed by an instruction to generate a textual sequence. Subsequently, in inference stage, we feed the model with the concatenation of the same image, instruction, and generated description text. During the attack, we correspondingly slice the output logits into image, instruction, and description segments, which we use to compute various metrics for MIAs. Our pipeline considers the information from the image, the instructions and the descriptions following the image. In practice, even if there is no access to the logits of the image feature and instruction slice, we can still detect the member image solely from the model generation. We visually describe a prompt example with different slice notations presented in Appendix A.2. ", "page_idx": 4}, {"type": "text", "text": "Our pipeline operates on the principle that VLLMs\u2019 responses always follow the instruction prompt [60], where the images usually precede the instructions and then always precede the descriptions. For causal language models used in VLLMs that predict the probability of the next token based on the past history [45], the logits at text tokens in the sequence inherently incorporate information from the preceding image. ", "page_idx": 4}, {"type": "text", "text": "5.2 MaxR\u00e9nyi MIA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose our MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ , utilizing the R\u00e9nyi entropy of the next-token probability distribution on each image or text token. The intuition behind this method is that if the model has seen this data before, the model will be more confident in the next token and thus have smaller R\u00e9nyi entropy. ", "page_idx": 4}, {"type": "text", "text": "Given a probability distribution $p$ , the R\u00e9nyi entropy [47] of order $\\alpha$ , is defined as $H_{\\alpha}(p)\\;=\\;$ $\\begin{array}{r}{\\frac{1}{1-\\alpha}\\log\\left(\\sum_{j}(p_{j})^{\\alpha}\\right),0\\,<\\,\\alpha\\,<\\,\\infty,\\alpha\\,\\neq\\,1.}\\end{array}$ . $H_{\\alpha}(p)$ is further defined at $\\alpha\\,=\\,1,\\infty$ , as $H_{\\alpha}(p)=$ $\\mathrm{lim}_{\\gamma\\to\\alpha}\\,\\dot{H}_{\\gamma}(p)$ by, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\textbf{\\emph{*}}H_{1}(p)=-\\sum_{j}p_{j}\\log p_{j},\\quad\\bullet\\ H_{\\infty}(p)=-\\log\\operatorname*{max}p_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To be more specific, given a token sequence $X:=(x_{1},x_{2},\\ldots,x_{L})$ , let $p^{(i)}(\\cdot)=\\mathbb{P}(\\cdot|x_{1},\\cdot\\cdot\\cdot\\,,x_{i})$ be the probability of next-token distribution at the $i$ -th token. Let Max- $.{\\mathrm{K}}{\\mathit{\\%}}(X)$ be the top ${\\mathrm{K}}\\%$ from the sequence $X$ with the largest R\u00e9nyi entropies, the MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ score of $X$ equals ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathtt{M a x R e n y i-K}\\%(X)=\\frac{1}{|\\mathrm{Max-K}\\%(X)|}\\sum_{i\\in\\mathrm{Max-K}\\%(X)}H_{\\alpha}(p^{(i)}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When $K=0$ , we define the MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ score to be $\\operatorname*{max}_{i\\in[L-1]}H_{\\alpha}(p^{(i)})$ . When $K=100$ , the MaxR\u00e9nyi- $\\mathrm{K\\%}$ score is the averaged R\u00e9nyi entropy of the sequence $\\overrightharpoon{X}$ . ", "page_idx": 5}, {"type": "text", "text": "In our experiments, we vary $\\alpha=\\textstyle{\\frac{1}{2}},1,2$ , and $+\\infty$ ; $K=0,10,100.$ . As $\\alpha$ increases, the top percentile of distribution $p$ will have more influence on $H_{\\alpha}(p)$ . When $\\alpha\\,=\\,1$ , $H_{1}(p)$ equals the Shannon entropy [50], and our method at $K=100$ is equivalent to the Entropy [48]. When $\\alpha=\\infty$ , we consider the most likely next token probability [31]. In contrast, $\\mathbb{M}\\mathrm{in-K}\\%$ [52] only deals with the target next token probability. When the sequence is generated by the target model deterministically, i.e., when the model always generates the most likely next token, our MaxR\u00e9nyi- $\\mathrm{K\\%}$ at $\\alpha=\\infty$ is equivalent to the $\\tt M i n-K\\%$ . ", "page_idx": 5}, {"type": "text", "text": "We also extend our MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ to the target-based scenarios, denoted by ModR\u00e9nyi. We first consider linearized R\u00e9nyi entropy, $\\begin{array}{r}{\\overline{{H}}_{\\alpha}(p)=\\frac{1}{1-\\alpha}\\left(\\sum_{j}(p_{j})^{\\alpha}-1\\right),0<\\alpha<\\infty,\\alpha\\neq1}\\end{array}$ . $\\overline{{H}}_{\\alpha}(p)$ is also further defined at $\\alpha=1$ , as $\\begin{array}{r}{\\overline{{H}}_{1}(p)=\\operatorname*{lim}_{\\alpha\\rightarrow1}\\overline{{H}}_{\\alpha}(p)=H_{1}(p)}\\end{array}$ . Assuming the next token ID is $y$ , recall that a small entropy value or a large $p_{y}$ value indicates membership, we want our modified entropy to be monotonically decreasing on $p_{y}$ and monotonically increasing on $p_{j},j\\neq y$ . Therefore, we propose the modified R\u00e9nyi entropy on a given next token $\\ensuremath{\\mathrm{ID~}}y$ , denoted by $\\overline{{{H}}}_{\\alpha}(p,y)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{{H}}}_{\\alpha}(p,y)=-\\frac{1}{|\\alpha-1|}\\left((1-p_{y})p_{y}^{|\\alpha-1|}-(1-p_{y})+\\sum_{j\\neq y}p_{j}(1-p_{j})^{|\\alpha-1|}-p_{j}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\alpha\\rightarrow1$ , we have $\\begin{array}{r}{\\overline{{H}}_{1}(p,y)=\\operatorname*{lim}_{\\alpha\\to1}\\overline{{H}}_{\\alpha}(p,y)=-\\sum_{j\\neq y}p_{j}\\log(1-p_{j})-(1-p_{y})\\log p_{y},}\\end{array}$ which is equivalent to the Modified Entropy [58]. In addition, our more general method does not encounter numerical instability in Modified Entropy as $p_{j}\\to0,1$ at $\\alpha\\neq1$ . For simplicity, we let the ModR\u00e9nyi score be the averaged modified R\u00e9nyi entropy of the sequence. ", "page_idx": 5}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct MIAs across three target models using various baselines, MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ , and ModR\u00e9nyi. Experiment setup is provided in Section 6.1. The results on text MIAs and image MIAs are present in Section 6.2 and Section 6.3, respectively. In Section 6.4, we show that the proposed MIA pipeline can also be used in GPT-4. Ablation studies are present in Section 6.5. The versions and base models of VLLMs we use are listed in Table 7 of the appendices. ", "page_idx": 5}, {"type": "text", "text": "6.1 Experimental setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Evaluation metric. We evaluate different MIA methods by their AUC scores. AUC score is the area under the receiver operating characteristic (ROC) curve, which measures the overall performance of a classification model in all classification thresholds $\\lambda$ . The higher the AUC score, the more effective the attack is. In addition to the average-case metric AUC, we also include the worst-case metric, the True Positive Rate at $5\\%$ False Positive Rate ( $\\Gamma\\mathsf{P R@5}\\%\\mathrm{FPR})$ in Appendix D suggested by [5]. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We take existing metric-based MIA methods as baselines and conduct experiments on our benchmark. We use the MIA method from [37], which compares the feature vectors produced by the original image with the augmented image. We use KL-divergence to compare the logit distributions and term it Aug-KL in this paper. We also use Loss attack [62], which is perplexity in the case of language models. Furthermore, we consider ppl/zlib and ppl/lowercase [4], which compare the target perplexity to zlib compression entropy and the perplexity of lowercase texts respectively. [52] proposes Min- $\\cdot K\\%$ method, which calculates the smallest $\\bar{K}\\%$ probabilities corresponding to the ground truth token. Min- $\\cdot\\mathrm{K}\\%$ is currently a state-of-the-art method to detect pre-training data of LLMs. For both $\\tt M i n-K\\%$ and MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ , we vary $K=0,10,100$ . In addition, we consider $K=20$ for $\\tt M i n-K\\%$ as suggested in [52]. We further include the max_prob_gap metric that can represent the extreme confidence in certain tokens by the model. That is, we subtract the second largest probability from the maximum probability in each token position and calculate the mean as metric. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.2 Image MIA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first conduct MIAs on images using VL-MIA/Flickr and VL-MIA/DALL-E in three VLLMs. For the image slice, it is not possible to perform target-based MIAs, because of the absence of ground-truth token IDs for the image. However, our MIA pipeline presented in Figure 1 can still handle target-based metrics by accessing the instruction slice and description slice. ", "page_idx": 6}, {"type": "text", "text": "As demonstrated in Table 2, MaxR\u00e9nyi-K% surpasses other baselines in most scenarios. An $\\alpha$ value of 0.5 yields the best performance in both VL-MIA/Flickr and VL-MIA/DALL-E. As $\\alpha$ increases, performance becomes erratic and generally deteriorates, though it remains superior to all target-based metrics. Overall, target-free metrics outperform target-based metrics for image MIAs. Another interesting observation is that instruction slices result in unstable AUC values, sometimes falling below 0.5 in target-based MIAs. This can be partially explained by the fact the model is more familiar with the member data. As a result, after encountering the first word \u201cDescribe\u201d, the model is more inclined to generate the description directly than generating the following instruction of $X_{\\mathrm{ins}}$ , i.e., \u201cthis image in detail\u201d. This is an interesting phenomenon that we leave to future research. ", "page_idx": 6}, {"type": "text", "text": "The performance of the image MIA model is influenced by its training pipelines. Recall that MiniGPT4 only updates the parameters of the image projection layer in image training, and LLaMA Adapter v2 applies parameter-efficient fine-tuning approaches. In contrast, LLaVA 1.5 training updates both the parameters of the projection layer and the LLM. The inferior performance of MIAs on MiniGPT-4 and LLaMA Adapter compared to LLaVA 1.5 is therefore consistent with [52] that more parameters\u2019 updates make it easier to memorize training data. ", "page_idx": 6}, {"type": "text", "text": "We find that VL-MIA/DALL-E is a more challenging dataset than VL-MIA/Flickr, reflected in the AUC being closer to 0.5. In VL-MIA/DALL-E, each non-member image is generated based on the description of a member image. Therefore, member data have a one-to-one correspondence with non-member data and depict a similar topic, which makes it harder to discern. ", "page_idx": 6}, {"type": "text", "text": "6.3 Text MIA ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/947bfd6452ef7499c5938b2ff833a1fb8a71aa4b7aae8f65d5de3cc6f8e44fa3.jpg", "table_caption": ["Table 3: Text MIA. AUC results on LLaVA "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Text member data might be used in different stages of VLLM training, including the base LLM model pre-training and the later VLLM instruction-tuning. We hypothesize that after the last usage of the member data in its training, the more the model changes, the better the targetfree MIA methods compared to targetbased ones, and vice-versa. The heuristic is that if the model\u2019s parameters have changed a lot, target-free MIA methods, which use the whole distribution to compute statistics, are more robust than target-based methods, which rely on the probability at the next token ID. On the other hand, if the member data are seen in recent fine-tuning, the next token will convey more causal relations in the sequence remembered by the model, and thus target-based ones are better. ", "page_idx": 6}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/72df365fe6cd5d69b1d457fe77dd970ef95855e5425f2ad4c689f55cf054a179.jpg", "table_caption": ["Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \u201cimg\u201d indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\u201d is the concatenation of the instruction slice and description slice. We use an asterisk \u2217in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "MIA on VLLM instruction-tuning texts We detect whether individual description texts appear in the VLLMs instruction-tuning. We use the description text dataset VL-MIA/Text of lengths (32, 64), constructed in Section 4. We present our results in the first column of Table 3. We observe that target-based MIA methods are significantly better than target-free ones, confirming our hypothesis. ", "page_idx": 7}, {"type": "text", "text": "MIA on LLM pre-training texts. We use the WikiMIA benchmark [52], which leverages the Wikipedia timestamp to separate the early Wiki data as the member data, and recent Wiki data as the non-member data. The early Wiki data are used in various LLMs pre-training [60]. We use WikiMIA of different lengths (32, 64, 128, 256), and expect the membership of longer sequences will be more easily identified. We present our results in the second column of Table 3. We observe that on LLaVA, our target-free MIA methods on large $\\alpha$ consistently outperform target-based MIA methods, which again confirms our hypothesis since the base LLM model of LLaVA has full parameter fine-tuning from LLaMA-2 and thus changed a lot. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.4 Image MIA on GPT-4 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we demonstrate the feasibility of image MIAs on the closed-source model GPT-4. Our experiments use two image datasets: VL-MIA/Flickr and VLMIA/DALL-E, detailed in Section 4. We choose GPT-4- vision-preview API, which was trained in 2023 and likely does not see the member data in either dataset. We randomly select 200 images per dataset and prompt GPT-4 to describe them in 64 words. We then apply MIAs based on the generated descriptions. Since GPT-4 can only provide the top-five probabilities at each token position, we can not directly use the proposed MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ that requires the whole probability distribution. To address this issue, we assume the size of the entire token set is 32000 and the probability of the remaining 31995 tokens are uniformly distributed. The AUC results are present in Table 4. We omit the result of perplexity and $\\tt M i n-K\\%$ since they are equivalent to MaxR\u00e9nyi- $\\cdot K\\%$ with $\\alpha=\\infty$ in the greedygenerated setting, as discussed in Section 5.2. Surprisingly, we observe that in VL-MIA/DALL-E, the best-performed method MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ $(\\alpha\\,=\\,0.5)$ ) can achieve an AUC of 0.815. This indicates a high level of effectiveness for MIAs on GPT-4, demonstrating the potential risks of privacy leakage even with closed-source models. ", "page_idx": 8}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/e7788b89c69f8c282ec008a36848fd3c6ac17a3ffb72af696f99fea9612b0123.jpg", "table_caption": ["Table 4: Image MIA on GPT-4. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.5 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Does the length of description affect the image MIA performance? We conduct ablation experiments on LLaVA 1.5 targeting the length of generated description texts with MaxR\u00e9nyi- $10\\%$ . In the generation stage, we restrict the max_new_tokens parameter of the generate function to (32, 64, 128, 256) to obtain description slices of different lengths. As presented in Figure 2a, when the length of the description increases, the AUC of the MIA becomes higher and enters a plateau when max_new_tokens reaches 128. This may be because a shorter text contains insufficient information about the image, and in an excessively long text, words generated later tend to be more generic and not closely related to the image, thereby contributing less to the discriminative information that helps discern the membership. ", "page_idx": 8}, {"type": "text", "text": "Can we still detect corrupted member images? The motivation is to detect whether sensitive images are inappropriately used in VLLM\u2019s training even when the images at hand may get corrupted. We leverage ImageNet-C [21] to generate corrupted versions of member data in VL-MIA/Flickr: Snow, Brightness, JPEG, and Motion_Blur, with the parameters in Table 8. The corrupted examples and corresponding model output generations are demonstrated in Appendix C Table 17 and Table 18. We take MaxR\u00e9nyi- $\\cdot K\\%$ $\\alpha=0.5)$ ) as the attacker and the results of LLaVA are presented in Figure 2b. Corrupted member images make MIAs more difficult, but can still be detected successfully. We also observe that reducing model quality (JPEG) or adding blur (Motion_Blur) degrade MIA performance more than changing the base parameter (Brightness) or overlaying texture (Snow). ", "page_idx": 8}, {"type": "text", "text": "Can we use different instructions? We conduct image MIAs on VL-MIA/Flickr with LLaVA through three different instruction texts: \u201cDescribe this image concisely.\u201d, \u201cPlease introduce this painting.\u201d, and \u201cTell me about this image.\u201d. We present our results in Table 14 of the appendix. Our pipeline successfully detects member images on every instruction, which indicates robustness across different instruction texts. ", "page_idx": 8}, {"type": "image", "img_path": "nv2Qt5cj1a/tmp/690788d7767fe169285bdee85e6ce625326221ad0e0b71045dd0155466e6976f.jpg", "img_caption": ["(a) Ablation study on max_new_tokens. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "nv2Qt5cj1a/tmp/156e6cc9c66b774a8cec85f27e485445b44690ddaf096b7f0cc02413dfb04186.jpg", "img_caption": ["(b) MaxR\u00e9nyi-K% on corrupted images. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2: Ablation study (a) on max_new_tokens with MaxR\u00e9nyi- $10\\%$ . Allowing VLLMs to generate longer descriptions can increase the AUC of \u201cdesp\u201d slices, but we encounter a plateau when max_new_tokens equals 128. (b) on image MIAs against corrupted versions of VL-MIA/Flickr with MaxR\u00e9nyi- $\\mathrm{K\\%}$ $(\\alpha=0.5)$ ). Three levels of corruption are applied to the images: Marginal, Moderate, and Severe. The dotted line indicates the AUC on raw images without corruption. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we take an initial step towards detecting training data in VLLMs. Specifically, we construct a comprehensive dataset to perform MIAs on both image and text modalities. Additionally, we uncover a new pipeline for conducting MIA on VLLMs cross-modally and propose a novel method based on R\u00e9nyi entropy. We believe that our work paves the way for advancing MIA techniques and, consequently, enhancing privacy protection in large foundation models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was carried out when Zhan Li and Yihang Chen were interns in the EPFL LIONS group. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). This research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-24-1-0048. This work was supported by the Swiss National Science Foundation (SNSF) under grant number 200021_205011. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   \n[3] Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX security symposium (USENIX security 19), pages 267\u2013284, 2019.   \n[4] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650, 2021.   \n[5] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy $(S P)$ , pages 1897\u20131914. IEEE, 2022.   \n[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3558\u20133568, 2021.   \n[7] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. Gan-leaks: A taxonomy of membership inference attacks against generative models. In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security, pages 343\u2013362, 2020.   \n[8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.   \n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.   \n[10] Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Labelonly membership inference attacks. In International conference on machine learning, pages 1964\u20131974. PMLR, 2021.   \n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1\u2013113, 2023.   \n[12] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 international joint conference on neural networks (IJCNN), pages 2921\u20132926. IEEE, 2017.   \n[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[14] Debeshee Das, Jie Zhang, and Florian Tram\u00e8r. Blind baselines beat membership inference attacks for foundation models. arXiv preprint arXiv:2406.16201, 2024.   \n[15] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \n[16] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024.   \n[17] Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. Practical membership inference attacks against fine-tuned large language models via self-prompt calibration. arXiv preprint arXiv:2311.06062, 2023.   \n[18] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.   \n[19] Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[20] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. Logan: Membership inference attacks against generative models. arXiv preprint arXiv:1705.07663, 2017.   \n[21] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.   \n[22] Benjamin Hilprecht, Martin H\u00e4rterich, and Daniel Bernau. Monte carlo and reconstruction membership inference attacks against generative models. Proceedings on Privacy Enhancing Technologies, 2019.   \n[23] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W Craig. Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. PLoS genetics, 4(8):e1000167, 2008.   \n[24] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu, and Xuyun Zhang. Membership inference attacks on machine learning: A survey. ACM Computing Surveys (CSUR), 54(11s):1\u201337, 2022.   \n[25] Pingyi Hu, Zihan Wang, Ruoxi Sun, Hu Wang, and Minhui Xue. M 4 i: Multi-modal models membership inference. Advances in Neural Information Processing Systems, 35:1867\u20131882, 2022.   \n[26] Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, and Zhuowen Tu. Bliva: A simple multimodal llm for better handling of text-rich visual questions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2256\u20132264, 2024.   \n[27] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. In International Conference on Machine Learning, pages 10697\u2013 10707. PMLR, 2022.   \n[28] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[29] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[30] Myeongseob Ko, Ming Jin, Chenguang Wang, and Ruoxi Jia. Practical membership inference attacks against large-scale multi-modal models: A pilot study. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4871\u20134881, 2023.   \n[31] Robert Konig, Renato Renner, and Christian Schaffner. The operational meaning of min-and max-entropy. IEEE Transactions on Information theory, 55(9):4337\u20134347, 2009.   \n[32] Klas Leino and Matt Fredrikson. Stolen memories: Leveraging model memorization for calibrated $\\{\\mathrm{White}{-}\\mathrm{Box}\\}$ membership inference. In 29th USENIX security symposium (USENIX Security 20), pages 1605\u20131622, 2020.   \n[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023.   \n[36] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/.   \n[37] Hongbin Liu, Jinyuan Jia, Wenjie Qu, and Neil Zhenqiang Gong. Encodermi: Membership inference against pre-trained encoders in contrastive learning. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, pages 2081\u20132095, 2021.   \n[38] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and Kai Chen. Understanding membership inferences on well-generalized learning models. arXiv preprint arXiv:1802.04889, 2018.   \n[39] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[40] Saeed Mahloujifar, Huseyin A Inan, Melissa Chase, Esha Ghosh, and Marcello Hasegawa. Membership inference on word embedding and beyond. arXiv preprint arXiv:2106.11384, 2021.   \n[41] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "via neighbourhood comparison. In Findings of the Association for Computational Linguistics: ", "page_idx": 12}, {"type": "text", "text": "ACL 2023, pages 11330\u201311343, 2023.   \n[42] Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David K Evans, and Taylor BergKirkpatrick. An empirical analysis of memorization in fine-tuned autoregressive language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1816\u20131826, 2022.   \n[43] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011.   \n[44] Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test set contamination in black-box language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=KS8mIvetg2.   \n[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI, 2018.   \n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[47] Alfr\u00e9d R\u00e9nyi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics, volume 4, pages 547\u2013562. University of California Press, 1961.   \n[48] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models. arXiv preprint arXiv:1806.01246, 2018.   \n[49] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[50] Claude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE mobile computing and communications review, 5(1):3\u201355, 2001.   \n[51] Virat Shejwalkar, Huseyin A Inan, Amir Houmansadr, and Robert Sim. Membership inference attacks against nlp classification models. In NeurIPS 2021 Workshop Privacy in Machine Learning, 2021.   \n[52] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id ${}^{=}{}_{\\!}$ zWqr3MQuNs.   \n[53] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 3\u201318. IEEE, 2017.   \n[54] Congzheng Song and Ananth Raghunathan. Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security, pages 377\u2013390, 2020.   \n[55] Congzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 196\u2013206, 2019.   \n[56] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine learning models that remember too much. In Proceedings of the 2017 ACM SIGSAC Conference on computer and communications security, pages 587\u2013601, 2017.   \n[57] Liwei Song and Prateek Mittal. Systematic evaluation of privacy risks of machine learning models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2615\u20132632, 2021.   \n[58] Liwei Song, Reza Shokri, and Prateek Mittal. Membership inference attacks against adversarially robust deep learning models. In 2019 IEEE Security and Privacy Workshops (SPW), pages 50\u201356. IEEE, 2019.   \n[59] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[61] Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, and Reza Shokri. Enhanced membership inference attacks against machine learning models. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pages 3093\u20133106, 2022.   \n[62] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF), pages 268\u2013282. IEEE, 2018.   \n[63] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3): 107\u2013115, 2021.   \n[64] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023.   \n[65] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.   \n[66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents of the appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Appendix is organized as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 In Appendix A, we provide some supplementary of the main body.   \n\u2022 Our additional experiment results are presented in Appendix B.   \n\u2022 A preview of some examples of VL-MIA can be found in Appendix C.   \n\u2022 We give broader impacts of this work in Appendix F.   \n\u2022 We state the limitation of our work in Appendix G. ", "page_idx": 14}, {"type": "text", "text": "A Supplementaries to the main text ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Detailed notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We summarize the notations of this work in Table 5. ", "page_idx": 14}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/6e82911be1f78ba15e91ba561b8a210a5a05421dd64cf6255813b5f7f7e96c89.jpg", "table_caption": ["Table 5: Main notations. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Explanation and visualization of slices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We give an example of the different slices in the MiniGPT-4 prompt in Figure 3. ", "page_idx": 14}, {"type": "image", "img_path": "nv2Qt5cj1a/tmp/8e73470857cd83202e4f8a0bdfc9b2af8b68d21edbe2396da0fb6b50ffcdd586.jpg", "img_caption": ["Figure 3: Different slices in the prompt. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "nv2Qt5cj1a/tmp/cad13bb997d3ae7f56241e489a95d0430b8128124b6c1cc307f46b75160366fb.jpg", "img_caption": ["Figure 4: A schematic of VLLM. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "As shown in Figure 4, the VLLMs consist of a vision encoder, a text tokenizer, and a language model. The output of the vision encoder (image embedding) have the same embedding dimension $d$ as the text token embedding. ", "page_idx": 15}, {"type": "text", "text": "When feeding the VLLMs with an image and the instruction, a vision encoder transforms this image to $L_{1}$ hidden embeddings of dimension $d$ , denoted by $E_{\\mathrm{image}}\\in\\mathbb{R}^{d\\times L_{1}}$ . The text tokenizer first tokenizes the instruction into $L_{2}$ tokens, and then looks up the embedding matrix to get its $d$ -dimensional embedding $E_{\\mathrm{ins}}\\,\\in\\,\\mathbb{R}^{d\\times L_{2}}$ . The image embedding and the instruction embedding are concatenated as $E_{\\mathrm{img-ins}}=\\left(E_{\\mathrm{image}},E_{\\mathrm{ins}}\\right)\\in\\mathbb{R}^{\\bar{d}\\times\\left(L_{1}+L_{2}\\right)}$ , which are then fed into a language model to perform next token prediction. The cross-entropy loss (CE loss) is calculated based on the predicted token id and the ground truth token id on the text tokens. ", "page_idx": 15}, {"type": "text", "text": "We can see that in this process, image embeddings are directly obtained from the vision encoder and there are no image tokens. There are no causal relations between consecutive image embeddings as well. Threfore, as we stated in Section 1, target-based MIA that requires token ids cannot be directly applied. ", "page_idx": 15}, {"type": "text", "text": "A.3 Datasets construction prompts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present the instruction prompts we use to construct the dataset in Section 4 in Table 6. ", "page_idx": 15}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/a0d9d8cb90a26983621b1e48d23a362f6b965f2be82d958c735762c99c8eee11.jpg", "table_caption": ["Table 6: Different prompts we use for dataset construction. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Additional experimental information ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The versions, base models and training datasets of target VLLMs are listed in Table 7. We run experiments on a single NVIDIA A100 80GB GPU, where the image MIA costs less than 2 hours for one model. ", "page_idx": 16}, {"type": "text", "text": "In our experiments on description text MIA, we do not detect the member text during the VLLM image-text pre-training stage (the 3rd row) because the captions used in pre-training are usually fewer than 10 words and do not contain sufficient information. Therefore, we only detect member text during the VLLM instruction tuning stage. ", "page_idx": 16}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/b415ff5c4ff89b00dc86684343d386b04b55ac012aaf183527b04906c55cebc1.jpg", "table_caption": ["Table 7: Model details used in this work. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.5 Parameters for image corruption ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We utilize the code8 from ImageNet-C [21] to corrupt member images. The related analysis can be found in Section 6.5. Here we provide the corruption parameters in the code in Table 8. ", "page_idx": 16}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/32c4da788576b32329c7b7c1ce12a4e89ff39277a0da5b9920bdb0de11151cfa.jpg", "table_caption": ["Table 8: Values for corruption parameter c in ImageNet-C code. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Additional experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Text MIA: Complete results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to Section 6.3, we present our complete Text MIA results on LLaVA, MiniGPT-4, and LLaMA-Adapter in Table 9 and Table 10. We find that no current method can effectively detect LLM pre-training text corpus from MiniGPT-4, even the AUC on 256-length Wiki text is only 0.6. Apart from that, we note that on the LLaMA-Adapter, target-based methods perform similarly or better than target-free ones. In the following, we explain this phenomenon by examining the training pipelines of two different models. ", "page_idx": 16}, {"type": "text", "text": "The wiki data is used in the pre-training stage of LLaMA 7b, which is further fine-tuned to LLaMA 7b Chat. The base MML of LLAVA is fine-tuned from Vicuna 7b v1.5, which is further fine-tuned from LLaMA 7b Chat. Therefore, the LLM model of LLAVA has changed a lot since wiki data\u2019s last usage, and target-free methods are preferable here. In contrast, LLaMA-Adapter is parameter-efficiently fine-tuned from LLaMA 7b, and the model\u2019s parameters have not changed much since the wiki\u2019s data last usage, and the target-based MIA methods can still retain their performance. In addition, to detect instruction-tuning texts in VLLMs, the model has just repeatedly seen the member data, and the target-based ones are significantly superior to target-free ones. Overall, the phenomena can be explained by our hypothesis. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/62ad70d2fb727e043797b5c7276d5afd7b18b70a53a8738c03a571aef1e45ade.jpg", "table_caption": ["Table 9: MIA on VLLM instruction-tuning texts. We present the MIA on VLLM instruction-tuning texts with description length (32, 64). Complete results of Table 3. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/529148cf9321b514dc1019af020e62187dcfea6eaffd5cabeb6060241c138dc0.jpg", "table_caption": ["Table 10: MIA on LLM pre-training texts. We evaluate on WikiMIA of lengths (32, 64, 128, 256). Complete results of Table 3. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Given a sequence of entropies calculated on each position, it would be natural to ask whether the higher percentile or lower percentile should be used to determine the score of the whole sequence for MIA. In the main paper, we use max, which coincides with $\\tt M i n-K\\%$ [52] under some special cases. We provide more evidence of the advantages of max or min. We define MinR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ as similar to MaxR\u00e9nyi- $\\mathrm{K\\%}$ , but consider the smallest ${\\bf K}\\%$ entropies in the sequence $X$ . ", "page_idx": 18}, {"type": "text", "text": "We conduct the pre-training text MIA on LLAVA. We also use the WiKiMIA [52] benchmark of different lengths (32, 64, 128, 256). We set $K=0$ , 10, 20, since when $K=100$ , MaxR\u00e9nyi- $\\cdot K\\%$ coincides with MinR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ . We observe that usually MinR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ is slightly inferior to MaxR\u00e9nyi- $\\mathrm{K\\%}$ . Therefore, we adopt MaxR\u00e9nyi- $\\cdot K\\%$ in our main paper. ", "page_idx": 18}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/3a4ce7f9772fa0e58872178c271037daf3dfa9cfb8e532d889a2d59362f9d548.jpg", "table_caption": ["Table 11: MIA on LLM pre-training texts. We evaluate on WikiMIA of lengths (32, 64, 128, 256) on LLaVA. We compare MaxR\u00e9nyi with MinR\u00e9nyi. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3 Ablation on extended dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this part, we extend VL-MIA/Flickr and VL-MIA/Text size to 2000, which includes 1000 members and 1000 non-members. From Table 12 and Table 13, we see a similar trend with small-size (600 samples) VL-MIA. Considering this computing source, our main experiments are conducted on small-size datasets. ", "page_idx": 18}, {"type": "text", "text": "B.4 Different instruction texts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct image MIA on VL-MIA/Flickr with LLaVA through three different instruction texts: \u201cDescribe this image concisely.\u201d, \u201cPlease introduce this painting.\u201d, and \u201cTell me about this image.\u201d. We present our results in Table 14 of the appendix. Our pipeline successfully detects member images on every instruction, which indicates robustness across different instruction texts. ", "page_idx": 18}, {"type": "text", "text": "C Dataset examples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Table 16, we give some examples of our proposed VL-MIA datasets. In Table 17 and Table 18, we show the corrupted images used in the ablation stuidies (Section 6.5) and the model\u2019s descriptions of these corrupted images. In Figure 5, we present some examples of our VL-MIA/Synthetic dataset. ", "page_idx": 18}, {"type": "text", "text": "D TPR at ${\\bf5\\%}$ FPR ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide the True Positive Rate (TPR) at $5\\%$ False Positive Rate (FPR) results as supplementary evaluation, as presented in Table 15. We can see a similar trend in Table 2. ", "page_idx": 18}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/a8a56e771e94b3d311d2092e341239ce17eb50460f85e12207b9cbd4f9885739.jpg", "table_caption": ["Table 12: Image MIA on extended VL-MIA/Flickr. AUC results on VL-MIA/Flickr of size 2000 under our pipeline. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/58b1cb61ca10c54bda28c1bcf1f4e771f2061c354dd6177158130debc70e3bda.jpg", "table_caption": ["Table 13: Text MIA on extended VL-MIA/Text. AUC results on LLaVA with 1000 members and 1000 non-members. We detect the text used in VLLM instruction tuning stage for text length equals to [32,64]. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E VLLM pipelines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this part, we investigate the pipelines of vLLMs, to explain why we have access to the text tokens, but only have access to the image embeddings instead of image tokens. ", "page_idx": 19}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/ef48830181429097e70496ab15badbccf917bed443f8e1a155795ee88fe809f1.jpg", "table_caption": ["Table 14: Image MIA. AUC results on VL-MIA/Flickr with LLaVA 1.5 when we change the instruction text. \u201cDescribe\u201d indicates \u201cDescribe this image concisely.\u201d, \u201dPlease\u201d indicates \u201cPlease introduce this painting.\u201d, and \u201cTell\u201d indicates \u201cTell me about this image.\u201d. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "The VLLMs consist of a vision encoder, a text tokenizer, and a language model. The vision encoder and the text tokenizer have the same embedding dimension $d$ . When feeding the VLLMs with an image and the instruction, a vision encoder transforms this image to $L_{1}$ hidden embeddings of dimension $d$ , denoted by $e_{\\mathrm{image}}\\in\\mathbb{R}^{d\\times L_{1}}$ . The text tokenizer first tokenizes the instruction into $L_{2}$ tokens, and then looks up the embedding matrix to get its $d$ -dimensional embedding $e_{\\mathrm{ins}}\\in\\mathbb{R}^{d\\times L_{2}}$ . The image embedding and the instruction embedding are concatenated as $e_{\\mathrm{img-ins}}=(e_{\\mathrm{image}},e_{\\mathrm{ins}})\\in$ $\\mathbb{R}^{d\\times(L_{1}+L_{2})}$ , which are then fed into a language model to generate a description that has $L_{3}$ tokens. The cross-entropy loss (CE loss) is calculated based on the predicted token id and the ground truth token id on the instruction and description tokens. ", "page_idx": 20}, {"type": "text", "text": "We can see that in this process, image embeddings are directly obtained from the vision encoder and there are no image tokens. Threfore, as we stated in Section 1, target-based MIAs that require token ids cannot be directly applied. ", "page_idx": 20}, {"type": "text", "text": "Furthermore, given the ouput logits of the shape $(L_{1}+L_{2}+L_{3})\\times|\\mathcal{V}|$ , where $\\nu$ is the vocabulary set, we can access the logits of the image by the slice $[0:L_{1}]$ , the logits of instruction by the slice $\\left[L_{1}:L_{1}+L_{2}\\right]$ , and the logits of description by the slice $[L_{1}+L_{2}:L_{1}+L_{2}+L_{3}]$ . ", "page_idx": 20}, {"type": "text", "text": "F Broader impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this paper, we present the first multi-modalities MIA benchmark for VLLMs, and propose a novel metric MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ for MIA. We recognize that our research has significant implications for the safety and ethics of VLLMs and may lead to targeted MIA defense by developers. Nevertheless, our findings provide valuable insights into data contamination, which could contribute to the training data split. Additionally, our method empowers individuals to detect their private data within the training dataset, which is essential for ensuring data security. We believe our work can raise awareness about the importance of privacy protection in multi-modal language models. ", "page_idx": 20}, {"type": "text", "text": "G Limitation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The first limitation of this work is that the best methods on the pre-training dataset can only achieve an AUC of 0.688. This is because the detection of pre-training data will become more challenging as the VLLMs are further fine-tuned. We believe increasing the performance for detecting pre-training data would be promising in the future. Secondly, the proposed MaxR\u00e9nyi- $\\cdot\\mathrm{K}\\%$ method requires access to the full probability distribution over the predicted tokens. Although we have demonstrated the feasibility of this approach on GPT-4, where the top-5 probabilities are available, it can be challenging if only the probability of the target token is provided. ", "page_idx": 20}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/c6571fa55b3d40dc5c5752b2ca39b9286e1875422957fdbd81be6d3926701d7b.jpg", "table_caption": ["Table 15: Image MIA. TPR at $5\\%$ FPR results on VL-MIA under our pipeline. \u201cimg\u201d indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst $^+$ desp\u201d is the concatenation of the instruction slice and description slice. We use an asterisk \u2217in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/9cb163a398a3ebfb6bd9c2bc5479c4888bce75a9758cff9b64d1b42310013ac2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "nv2Qt5cj1a/tmp/840c9d633a588f8fc0b23b54e052b76bfd44f33c8381d0d7be40ebdb3cca4708.jpg", "img_caption": ["Figure 5: Examples of new geometry datasets (top), new password datasets (bottom).. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 17: Examples in corrupted versions of VL-MIA/Flickr. ", "page_idx": 23}, {"type": "image", "img_path": "nv2Qt5cj1a/tmp/acc0793a8581da7848ff9ee3dec89e43c3d0d1ac9ec7bca0c26a2de4e40273bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "nv2Qt5cj1a/tmp/857b4459d37b9ed82adf469d6747cd2329c3b014d7f421722cfab8be0a812e21.jpg", "table_caption": ["Table 18: Model generated output of corrupted versions of VL-MIA/Flickr. Take severe severity as an example. (Max_new_tokens $=64\\_$ ) "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The abstract and introduction accurately state our contribution and findings. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations in Appendix G. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There is no assumption or theoretical result in this work. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We disclose all the information needed to reproduce the main experimental results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided open access to the data and code in https://github.com/LIONS-EPFL/VL-MIA. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We introduce the setup of our experiment in Section 6.1. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no randomness in our experiments because we set the temperature of LLM to 0. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We state computer resources-related information in Appendix A.4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics in every respect. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We discuss these in Appendix F. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the safeguards in Appendix F. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We cite every reference we use and CC-BY 4.0 is applicable for our work. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We will provide the datasets on huggingface upon acceptance with comprehensive documentation. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]