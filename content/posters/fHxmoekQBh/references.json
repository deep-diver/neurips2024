{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "This paper introduces Flamingo, a visual language model which serves as a strong baseline and is frequently compared against in the current paper."}, {"fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An open-source framework for training large autoregressive vision-language models", "publication_date": "2023-08-01", "reason": "This paper provides an open-source framework for training large vision-language models, which is relevant to the current paper's focus on improving MLLMs."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-07-01", "reason": "BLIP-2 is a key model in the field of multimodal large language models and is used as a comparison model in the experiments."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-05-01", "reason": "Vision Transformer (ViT) is a foundational model for visual encoding used in many MLLMs and is directly relevant to the current paper's approach."}, {"fullname_first_author": "Yuying Ge", "paper_title": "Planting a seed of vision in large language model", "publication_date": "2023-07-01", "reason": "This paper proposes a method for incorporating visual information into large language models, which is directly related to the core methodology of the current paper."}]}