[{"figure_path": "lbSI1j8m6p/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison in terms of averaged per-task gain over single task backbone (All results are in the form of percentage values %)", "description": "This table presents the performance comparison of different methods for multi-task learning on electronic health records (EHR).  It compares several baselines including hand-crafted and automated methods with the proposed AutoDP framework. The table shows the averaged per-task gain in ROC and AVP metrics for three different settings: using the first 5, 10, and 25 tasks.  Each setting includes different methods such as training one shared model for all tasks, task grouping with one model per group, and variations of AutoDP. The goal is to show the improved performance of AutoDP, especially when optimizing task grouping and model architectures.", "section": "4.2 Performance Evaluation"}, {"figure_path": "lbSI1j8m6p/tables/tables_9_1.jpg", "caption": "Table 2: Ablation results in terms of AVP.", "description": "This table presents the ablation study results, showing the impact of removing key components from the AutoDP framework.  It compares the averaged precision (AVP) performance of AutoDP against variations where progressive sampling, greedy search, or the automated task grouping are replaced with simpler alternatives.  The results highlight the contribution of each component to the overall performance improvement.", "section": "4.4 Ablation Study"}, {"figure_path": "lbSI1j8m6p/tables/tables_13_1.jpg", "caption": "Table 3: Performance of the single task backbone.", "description": "This table presents the performance of a single-task baseline model for each of the 25 prediction tasks. The metrics used to evaluate performance are the Area Under the Receiver Operating Characteristic curve (ROC) and the Averaged Precision (AVP). These metrics are commonly used in binary classification problems to assess the ability of a model to correctly identify positive cases among a set of samples.  The results in this table show the baseline performance of a simple recurrent neural network (RNN) before any optimization techniques are applied.  These results serve as a benchmark to compare against more advanced methods, like the automated multi-task learning framework proposed in the paper.", "section": "4.1 Set Up"}, {"figure_path": "lbSI1j8m6p/tables/tables_13_2.jpg", "caption": "Table 4: Hyperparameter setting.", "description": "This table shows the hyperparameter settings used in the experiments for three different settings: Task @ 5, Task @ 10, and Task @ 25.  The parameters include the number of tasks, the dimension of the feature vector in the surrogate model, the number of nodes in the DAG, parameters related to the progressive sampling strategy (number of initial samples, total number of samples selected during progressive sampling, number of top architectures selected, exploration-exploitation parameter, number of progressive sampling rounds), parameters related to the greedy search method (total iterations, budget for task combinations), and the approximate GPU hours required for each setting.", "section": "4.1 Set Up"}, {"figure_path": "lbSI1j8m6p/tables/tables_14_1.jpg", "caption": "Table 5: Disease Based Grouping.", "description": "This table shows how 25 prediction tasks are grouped based on medical knowledge using GPT-4.  Each group contains related diseases. This grouping is used as a baseline to compare against the automated task grouping method developed in the paper.", "section": "4.1 Set Up"}]