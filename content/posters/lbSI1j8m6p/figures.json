[{"figure_path": "lbSI1j8m6p/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of the proposed AutoDP", "description": "This figure provides a visual overview of the AutoDP framework, illustrating the different stages involved.  Starting with data extraction from MIMIC-IV, it shows the multi-task learning (MTL) procedure, which includes searching for optimal task combinations and architectures. The surrogate model, progressive sampling, and greedy search methods are also highlighted, demonstrating how they contribute to finding the best configuration for multi-task disease prediction. The figure maps these steps to the relevant sections of the paper (Sections 3.1, 3.3, 3.4, and 3.5), allowing for easy cross-referencing.", "section": "3 Methodology"}, {"figure_path": "lbSI1j8m6p/figures/figures_8_1.jpg", "caption": "Figure 2: Histogram of task gains for AutoDP in terms of Averaged Precision.", "description": "This figure shows three histograms visualizing the distribution of per-task gains achieved by the AutoDP model across three different experimental settings: Task @ 5, Task @ 10, and Task @ 25.  Each histogram represents a different number of tasks considered (5, 10, and 25, respectively), illustrating the range and frequency of performance improvements obtained for each individual task compared to single-task baselines. The x-axis represents the percentage gain in averaged precision, while the y-axis shows the frequency of tasks achieving a particular gain. The histograms illustrate the positive gains obtained across all tasks, highlighting the efficacy of the AutoDP framework in enhancing performance compared to single-task models.", "section": "4.2 Performance Evaluation"}, {"figure_path": "lbSI1j8m6p/figures/figures_8_2.jpg", "caption": "Figure 3: Analysis for the number of progressive sampling rounds K1 and the budget of task groups B under the setting of Task @ 25.", "description": "This figure analyzes the impact of two hyperparameters on the performance of the AutoDP model:  K1, which represents the number of progressive sampling rounds during the surrogate model training, and B, which represents the budget of task groups in the greedy search. The left panel shows that increasing K1 leads to performance improvement, but the gains level off after around 25 rounds.  The right panel shows that increasing B (budget for task groups) also yields improved performance that levels off after around 12 groups. This suggests that there are diminishing returns for further increases in either K1 or B, indicating good efficiency in the model's hyperparameter tuning.", "section": "4.3 Hyperparameter & Complexity Analysis"}, {"figure_path": "lbSI1j8m6p/figures/figures_14_1.jpg", "caption": "Figure 1: Overview of the proposed AutoDP", "description": "This figure provides a visual overview of the proposed AutoDP framework. It details the different stages of the framework, starting from data extraction and preprocessing to the final results. The framework consists of several key components: (1) Data Extraction: Extracting EHR time series data. (2) MTL Procedure: Implementing the multi-task learning procedure, including task combination (C) and architecture (A). (3) Progressive Sampling: Progressively sampling from the search space to train a surrogate model that estimates multi-task gains. (4) Surrogate Model: Building a neural network to estimate the MTL gains using the inputs of task combination (C) and architecture (A). (5) Greedy Search: A greedy search approach to efficiently find a near optimal solution for task grouping and architecture. The figure illustrates how the framework integrates these components to efficiently search for the optimal configuration of task grouping and architecture to maximize the multi-task performance gain.", "section": "3 Methodology"}, {"figure_path": "lbSI1j8m6p/figures/figures_15_1.jpg", "caption": "Figure 1: Overview of the proposed AutoDP", "description": "This figure provides a visual overview of the proposed AutoDP framework. It illustrates the various stages of the framework, including data extraction, multi-task learning procedures, progressive sampling, and greedy search.  The flowchart highlights the interaction between these components in finding optimal task grouping and neural architectures.  Specifically, it shows how intermediate results from each stage inform subsequent stages in an iterative process that culminates in the final results.", "section": "3.2 Overview"}]