[{"figure_path": "VPSx3n6ICE/tables/tables_3_1.jpg", "caption": "Table 1: Comparison with existing private inference works.", "description": "This table compares PrivCirNet with other state-of-the-art private inference methods.  It highlights whether each method uses HE encoding optimization, network optimization, and what types of operations are targeted (GEMM, Convolution, etc.).  It also indicates which methods reduce the number of HE rotations and/or HE multiplications, and what specific network optimization techniques are employed.", "section": "Related Works"}, {"figure_path": "VPSx3n6ICE/tables/tables_3_2.jpg", "caption": "Table 2: Comparison between PrivCirNet and previous works that use circulant matrix.", "description": "This table compares PrivCirNet with three other methods that utilize circulant matrices for efficient inference.  It highlights key differences in their application (plaintext convolution vs. private inference), initialization methods for the circulant matrices, whether they use variable block sizes, their block size assignment strategies, and whether they employ customized encoding methods. The table also notes the types of neural networks each method is designed for.", "section": "2 Preliminaries"}, {"figure_path": "VPSx3n6ICE/tables/tables_3_3.jpg", "caption": "Table 3: Accuracy and latency impact of applying block circulant transformation to different layers of MobileNetV2 on Tiny ImageNet. 32 layers are partitioned into 4 groups.", "description": "This table shows the results of applying block circulant transformation with different block sizes to different layers of the MobileNetV2 model on the Tiny ImageNet dataset.  The 32 layers of the model are divided into 4 groups. Each row shows a different layer-wise block size assignment, its corresponding Top-1 accuracy, and the resulting inference latency. This demonstrates how the choice of block size affects both accuracy and efficiency.", "section": "3 PrivCirNet Framework"}, {"figure_path": "VPSx3n6ICE/tables/tables_5_1.jpg", "caption": "Table 4: Theoretical complexity comparison of CirEncode with prior works. The data of GEMM is measured with dimension (d1, d2, d3) = (512,768,3072), and that of convolution is (H, W, C, K, R) = (16, 16, 128, 128, 3). The polynomial degree n = 8192 and block size b = 8.", "description": "This table provides a comparison of the theoretical computational complexity of the CirEncode algorithm with several existing algorithms for both general matrix multiplication (GEMM) and convolution operations.  The comparison is based on the number of homomorphic encryption (HE) multiplications (# HE-Pmult), HE rotations (# HE-Rot), and ciphertexts used.  The table shows that CirEncode significantly reduces the number of HE-Rot and HE-Pmult compared to state-of-the-art methods, particularly when using block circulant matrices, demonstrating the efficiency gains achieved through its novel encoding algorithm.", "section": "3.2 CirEncode: nested encoding for block circulant GEMMS"}, {"figure_path": "VPSx3n6ICE/tables/tables_8_1.jpg", "caption": "Table 5: The number of HE-Rot / HE-Pmult comparisons of different protocols for GEMMs and convolutions with different dimensions.", "description": "This table compares the number of homomorphic encryption (HE) rotations (HE-Rot) and HE multiplications (HE-Pmult) required by different protocols (Neujeans+BSGS, Bolt+BSGS, PrivCirNet (b2), and PrivCirNet (b8)) for general matrix multiplications (GEMMs) and convolutions.  Different dimensions of GEMMs and convolutions are considered, reflecting those found in MobileNetV2, ViT, and ResNet-18 architectures.  The results demonstrate PrivCirNet's efficiency in reducing both HE-Rot and HE-Pmult operations, especially with larger block sizes (b8).", "section": "4.2 Micro-Benchmark on Single GEMM and Convolution"}, {"figure_path": "VPSx3n6ICE/tables/tables_8_2.jpg", "caption": "Table 5: The number of HE-Rot / HE-Pmult comparisons of different protocols for GEMMs and convolutions with different dimensions.", "description": "This table compares the number of homomorphic encryption rotations (HE-Rot) and multiplications (HE-Pmult) required by different protocols (Neujeans+BSGS, Bolt+BSGS, PrivCirNet (b2), PrivCirNet (b8)) for both general matrix multiplications (GEMMs) and convolutions.  The comparison is done for various dimensions of GEMMs and convolutions, representing different layers and operations in deep neural networks, such as MobileNetV2, ViT, and ResNet-18. The numbers illustrate the computational efficiency gains achieved by PrivCirNet, especially with larger block sizes (b8), significantly reducing the number of HE operations compared to the state-of-the-art methods, Bolt and Neujeans.", "section": "4.2 Micro-Benchmark on Single GEMM and Convolution"}, {"figure_path": "VPSx3n6ICE/tables/tables_9_1.jpg", "caption": "Table 6: Accuracy comparison of different block size assignment methods. Latency limitation represents the proportion of latency relative to the original uncompressed model.", "description": "This table compares the Top-1 accuracy results of different block size assignment methods (Uniform, Frobenius, and Loss-aware) for MobileNetV2 and ViT models on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets.  The comparison is performed at various latency limitations (50%, 25%, and 12.5%) relative to the uncompressed model's latency.  The table shows the impact of different block size strategies on model accuracy and reveals how the loss-aware method (PrivCirNet) achieves superior accuracy across various datasets and latency constraints.", "section": "4.4 Ablation Study"}, {"figure_path": "VPSx3n6ICE/tables/tables_9_2.jpg", "caption": "Table 7: Latency comparison of different protocols for GEMMs and convolutions. PrivCirNet use circulant weight with block size b.", "description": "This table compares the latency of different homomorphic encryption (HE) protocols for general matrix multiplications (GEMMs) and convolutions.  It shows the latency achieved by different protocols such as CrypTFlow2, Cheetah, Neujeans, Bolt, and PrivCirNet with different block sizes (b2 and b8). The GEMM dimensions are chosen from various layers of MobileNetV2 and ViT, while convolution dimensions come from ResNet-18.  The comparison highlights the latency reduction achieved by PrivCirNet, especially when using a larger block size.", "section": "4.2 Micro-Benchmark on Single GEMM and Convolution"}, {"figure_path": "VPSx3n6ICE/tables/tables_19_1.jpg", "caption": "Table 4: Theoretical complexity comparison of CirEncode with prior works. The data of GEMM is measured with dimension (d1, d2, d3) = (512,768,3072), and that of convolution is (H, W, C, K, R) = (16, 16, 128, 128, 3). The polynomial degree n = 8192 and block size b = 8.", "description": "This table compares the theoretical computational complexity of the proposed CirEncode algorithm with existing state-of-the-art HE encoding methods (CrypTFlow2, Cheetah, Iron, Bumblebee, Neujeans+BSGS, Bolt+BSGS).  The comparison is performed for both GEMM (General Matrix Multiplication) and convolution operations, considering the number of homomorphic multiplications (# HE-Pmult), homomorphic rotations (# HE-Rot), and the number of ciphertexts used.  The table highlights the significant reduction in computational complexity achieved by CirEncode, especially in terms of HE rotations, due to its utilization of block circulant matrices and the optimized BSGS algorithm.", "section": "3.2 CirEncode: nested encoding for block circulant GEMMs"}, {"figure_path": "VPSx3n6ICE/tables/tables_19_2.jpg", "caption": "Table 8: PrivCirNet evaluation benchmarks.", "description": "This table presents the key characteristics of the different deep neural network models (MobileNetV2, ResNet-18, and ViT) used in the PrivCirNet experiments.  It shows the number of layers in each model, the number of parameters (in millions), the number of multiply-accumulate operations (in billions), and the datasets used for evaluation.  This information is essential for understanding the scope and scale of the experimental results presented in the paper.", "section": "4.1 Experimental Setup"}, {"figure_path": "VPSx3n6ICE/tables/tables_21_1.jpg", "caption": "Table 9: Accuracy and latency results when combining PrivCirNet and DeepReshape (ReLU reduction).", "description": "This table presents the accuracy and latency results obtained by combining PrivCirNet and DeepReshape, a method for optimizing ReLU layers.  Different configurations of DeepReshape are tested, each with a varying percentage of ReLU units removed (-53%, -50%, -72%). The results show the Top-1 accuracy and latency breakdown for linear and non-linear layers for each configuration.", "section": "4.3 End-to-End Inference Evaluation"}]