[{"type": "text", "text": "Automated Label Unification for Multi-Dataset Semantic Segmentation with GNNs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rong Ma,\u2217 Jie Chen,\u2217 Xiangyang Xue, and Jian $\\mathbf{P}\\mathbf{u}^{\\dagger}$ Fudan University rma22@m.fudan.edu.cn, {chenj19,xyxue,jianpu}@fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep supervised models possess significant capability to assimilate extensive training data, thereby presenting an opportunity to enhance model performance through training on multiple datasets. However, confilcts arising from different label spaces among datasets may adversely affect model performance. In this paper, we propose a novel approach to automatically construct a unified label space across multiple datasets using graph neural networks. This enables semantic segmentation models to be trained simultaneously on multiple datasets, resulting in performance improvements. Unlike existing methods, our approach facilitates seamless training without the need for additional manual reannotation or taxonomy reconciliation. This significantly enhances the efficiency and effectiveness of multi-dataset segmentation model training. The results demonstrate that our method significantly outperforms other multi-dataset training methods when trained on seven datasets simultaneously, and achieves state-of-the-art performance on the WildDash 2 benchmark. Our code can be found in https://github.com/Mrhonor/AutoUniSeg. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in computer vision [35, 21] have shown the advantages of large datasets in training robust visual models [2]. However, for deep supervised visual models that rely on annotated data, the collection of such extensive annotated datasets can be prohibitively costly [11]. To address this expense and expand the data available for training, several efforts [4, 23, 30] focus on the challenges of multi-dataset training, enabling the use of diverse datasets to train more robust and generalizable models. ", "page_idx": 0}, {"type": "text", "text": "Models trained on multiple datasets must confront the challenge of reconciling confilcting annotation standards and label spaces. For example, the class road in the BDD dataset [46] can be further divided into several classes in the Mapillary dataset [33]: road, lane marking and crosswalk. Similarly, the Mapillary dataset labels both barrier and curb as distinct classes, while in the IDD dataset [42], they are combined under the single label curb. These conflicts impact the supervised learning of models, as they may be incorrectly penalized for predicting finer-grained classes from other datasets. ", "page_idx": 0}, {"type": "text", "text": "Another challenge is the task of unifying diverse dataset labels to produce outputs in a standardized format [29, 24]. Several methods [11, 30] attempt to address this by concatenating the label spaces of all datasets and using language models to encode label names into a text embedding space. However, there approaches introduce redundancy and fail to handle issues where labels share names but differ in annotation granularity. Other methods involve manually constructing universal taxonomies [4, 24] or relabeling [23], both of which are time-consuming and labor-intensive. Recent approaches aim to automatically construct universal taxonomies [6, 41], but these methods typically identify inter-label relations between only two datasets, involving a time-consuming iterative training process. ", "page_idx": 0}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/d403c9832429cd533110ada31621fd3aa6df2b9e240a68b70d12dc43acb88202.jpg", "img_caption": ["Figure 1: Our method consists of three modules. The label encoding provides the semantic text features of the dataset labels. The GNNs learn the unified label embedding space and dataset label mappings based on the textual features and input images. The segmentation network leverages the unified label embedding space to produce segmentation results in the unified label space. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel approach leveraging Graph Neural Networks (GNNs) [20] to automatically construct unified label space, enabling segmentation model to be trained simultaneously on multiple datasets. In contrast to previous approaches [11, 4, 6], our approach eliminates the need for manual re-annotation or iterative training procedures to construct universal taxonomies, while also addressing the limitation of language-based methods in distinguishing categories with identical semantics. As depicted in Figure 1, we utilize a language model to convert the dataset labels into text features. Then, we apply GNNs to learn the relationships and associations among these labels. The process creates a unified label embedding space and dataset label mappings. The output head of the segmentation network incorporates the unified label embedding space to generate unified segmentation results within this unified space. The dataset label mappings are subsequently utilized to align the unified segmentation results with the label spaces relevant to each dataset. This enables the training of segmentation models and graph neural networks with dataset-specific annotations. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multi-datasets Semantic Segmentation. In recent years, numerous studies [52, 29] focused on training semantic segmentation models on multiple datasets. A simple approach involved incorporating dataset-specific modules in the model, such as dataset-specific output heads [28] or dataset-specific batch normalization layers [44], to produce predictions tailored to each dataset domain. While these methods effectively avoided issues of dataset label space conflicts, they offered limited applicability in real-world scenarios as they could not deliver unified predictions. MSeg [23] addressed the problem at the data level via manual re-annotation processes to resolve label confilcts. However, this approach was time-consuming, error-prone and not easily scalable. Recent methods employed manual [5] or automatic techniques [6] to construct universal taxonomies and establish label relationships between datasets label space and the unified label space. These methods, leveraging partial label learning approaches [48, 12], enabled training with dataset-specific annotations and producing unified predictions. ", "page_idx": 1}, {"type": "text", "text": "Construct Universal Taxonomies. Each dataset has its unique domain, necessitating the construction of universal taxonomies to enable the model to cover all domains. Several approaches [11, 30] attempted to concatenate dataset label spaces and differentiated similar classes by aligning text embedding encoded by language model. However, this approach struggled with classes that had the same name but different levels of annotation granularity, and direct concatenation of label spaces could lead to semantic confilcts. Other research [24, 29] attempted to establish a unified label space through the expertise of human annotators. Additionally, other studies [41, 53] aimed to address this issue by developing automated mechanisms to create universal taxonomies. However, these methods either could only construct a unified label space between two datasets at a time [6] or could not handle complex class relationships [53]. Our approach stands out by automatically constructing universal taxonomies in a single training session with multiple datasets, resulting in significant time savings compared to methods that require multiple training iterations. ", "page_idx": 1}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/0ec4dd430faea2ab8514208f5f21af6413fbb1cd1885887ecbee536b2d82c583.jpg", "img_caption": ["Figure 2: Illustration of our method that training with dataset-specific annotations through label mappings constructed by GNNs. We leverage a unified segmentation head (UniSegHead) to enable simultaneous training on multiple datasets. In the UniSegHead, we compute the matrix product between pixel embedding and augmented unified node features output by the GNNs, resulting in predictions for the unified label space. We finally utilize the label mappings constructed by GNNs to map the unified predictions to dataset-specific prediction for training. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Graph Neural Networks demonstrated exceptional effectiveness in dealing with complex topological data structures, as highlighted in recent research [43]. The applicability extended across various domains, including recommendation systems [18], knowledge graph construction [50], and skeletal action recognition [54]. In the context of our problem, discovering label relations can be conceptualized as a link prediction task [49, 22]. However, conventional approaches for graph link prediction [9, 10] are not suitable for our model since we do not possess ground truth links. We use the values of the learnable adjacency matrix as predictions of whether nodes are linked. Supervision of the linked predictions relys on the segmentation results of the segmentation network and the corresponding image annotations. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The comprehensive framework is depicted in Figure 2. We first define the unified representation of the multi-dataset label space. Utilizing this representation, we build a graph neural network to learn the unified label space. Finally, leveraging the unified label space, we train our segmentation network and the graph neural network using the dataset annotations. ", "page_idx": 2}, {"type": "text", "text": "3.1 Unification of Multi-dataset Label Space ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unified Label Space. Given $K$ datasets with their respective label space $\\{L_{1},L_{2},\\ldots,L_{K}\\}$ , multidataset semantic segmentation requires a model to predict within a consistent label space that encompasses all dataset label spaces $\\begin{array}{r}{L_{p r e d}=\\bigcup_{i=1}^{K}L_{i}}\\end{array}$ . Each pixel must be assigned to a label in this unified label space. We define $N$ unifie d label nodes $\\bar{\\mathcal{A}}=\\left\\{\\alpha_{1},\\alpha_{2},\\ldots,\\bar{\\alpha_{N}}\\right\\}$ , serving as the nodes in the graph neural networks. Their corresponding $D$ -dimensional learnable embedding $\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{N}\\}$ represent the unified label embedding space. The number of unified label nodes $N$ is often smaller than the total number of dataset classes $|L|$ , because we aim to merge multiple identical classes into a single unified label. The image is first encoded into pixel embedding $\\mathcal{P}$ by the segmentation network, which is then projected into the unified label embedding space to assign a unified label to each pixel. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Label Mappings. We define a mapping from the unified label space to the dataset-specific label space $\\mathbf{M}_{i}:\\mathcal{A}\\rightarrow L_{i}$ , which is used to train the model with dataset-specific annotations. Mathematically, $\\mathbf{M}_{i}\\in\\{0,1\\}^{N\\times|L_{i}|}$ is a boolean linear transformation. Each unified class $\\alpha$ is at most linked to a class $c$ within a specific dataset $i$ to prevent label confilcts: $\\mathbf{M}_{\\alpha}\\mathbf{1}\\leq\\mathbf{1}$ . To handle different annotation granularities, we use label mappings to merge multiple unified label nodes representing fine-grained classes into one super-class $c$ : $\\mathbf{\\bar{M}}_{c}^{\\top}\\mathbf{1}\\geq\\mathbf{1}$ . For example, the curb from IDD can be simultaneously mapped by unified label nodes represented curb and barrier. We use unified label nodes as input nodes for the GNNs, which learn the label mappings and unified label embedding space, thus enabling the automated unification of multi-dataset label spaces. ", "page_idx": 3}, {"type": "text", "text": "3.2 Learning Unified Label Space with Graph Neural Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Learning the label mappings between dataset label spaces and the unified label space can be viewed as a bipartite graph matching problem. This makes graph neural networks well-suited to address this issue. Below, we detail our approach of constructing GNNs for learning a unified label space. ", "page_idx": 3}, {"type": "text", "text": "Input Nodes. To construct the input feature of dataset-specific label nodes, as illustrated in Figure 2, we used the dataset labels in the template \"An image of <label> from the dataset <dataset>\" as plain text input and employed ChatGPT to complete the detailed description of each label. Then, we employ llama-2 [40] to encode these label descriptions and generate text features. To further distinguish nodes from different datasets, we introduce learnable dataset embedding for each dataset $\\{\\mathbf{d}_{1},\\mathbf{d}_{2},\\dots,\\mathbf{d}_{K}\\}$ . The dataset embedding is combined with the text features to form the input features for dataset-specific label nodes: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i,m}=f_{t}(l_{i,m})+\\mathbf{d}_{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bf x}_{i,m}$ is the input feature of the $m$ -th label from dataset $i$ , and $f_{t}(l_{i,m})$ is the text feature of the label description $l_{i,m}$ encoded by language model. The input features of unified label nodes are randomly initialized to the same dimension as the dataset-specific label node. To determine the appropriate number of unified label nodes, inspired by the approach in [53], we used crossvalidation results across different datasets to identify the number of mergeable categories, which served as the initial selection for the unified label nodes. The specific algorithm can be found in Appendix B. Together, dataset-specific label nodes and unified label nodes constitute the input nodes of GNNs. During the training process, we maintain a constant number of nodes. After the training is completed, we will remove inactive unified label nodes, meaning those that were not assigned to any dataset-specific label. ", "page_idx": 3}, {"type": "text", "text": "Learnable Adjacency Matrix. To enable label mappings to be updated via gradient descent, we embed the label mappings as a continuous, learnable graph adjacency matrix ${\\bf M}_{a}$ . Values in the adjacency matrix represent the weights of corresponding edges. Only the edges between unified label nodes and dataset-specific nodes are learnable, while others are fixed to zero. We apply a softmax operation to the edges connecting each unified label node with nodes of a particular dataset, ensuring that the sum of the edge weights $w$ between each unified label node and nodes of this dataset equals one. The element at the intersection of the $r$ -th row and the $c$ -th column in the lower triangular portion of ${\\bf M}_{a}$ is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{M}_{r,c}=\\left\\{\\begin{array}{l l}{\\frac{e^{w_{r,c}}}{\\sum_{c^{\\prime}\\in L_{i},r^{\\prime}\\in A}e^{w_{r^{\\prime},c^{\\prime}}}}}&{\\mathrm{if}\\,\\mathrm{r}>|L|\\mathrm{~and}\\,\\mathrm{c}<|L|}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Unified Label Embedding. The forward propagation of our graph model follows the GraphSAGE framwork [17] as formulated in Equation 3. $\\mathbf{W}^{k}$ and $\\mathbf{X}^{k}$ are the weight and feature of the $k$ -th GNNs layer. The $\\sigma$ indicates nonlinear activation function, implemented as the tanh in this work. The output ", "page_idx": 3}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/33a322c2302b2f625adae71c2c9881d844356f93404a876036065da21c36aa8a.jpg", "table_caption": ["Table 1: Training and test datasets in our experiments "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "features of the unified label nodes from the final layer serve as the unified label embedding space, $\\mathbf{X}_{u}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{N}]^{\\top}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{X}^{k+1}=\\sigma(\\mathbf{W}^{k}[\\mathbf{X}^{k}\\|\\mathbf{M}_{a}\\mathbf{X}^{k}]).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To obtain the dataset label mappings, we partition the adjacency matrix into submatrices, each corresponding to a specific dataset. Each submatrix contains only the unified label nodes and the label nodes specific to that dataset. We compute the label mappings for each dataset based on the values in its corresponding submatrix, divided into the following two cases. During training, we will alternately train the segmentation network and the GNNs. When training the GNNs, we directly use the value of the learnable adjacency matrix to establish label mappings, thereby facilitating weight updates through gradient descent. When training the segmentation network, we utilize the unbalanced optimal transport algorithm to solve for the boolean label mappings that satisfies the many-to-one mapping constraints. This algorithm detailed in Appendix C effectively handles the conversion of the continuous adjacency matrix into a discrete dataset label mappings required for training segmentation network. ", "page_idx": 4}, {"type": "text", "text": "3.3 Training a Universal Model with Dataset-specific Annotations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Training a universal model is divided into two steps. The first step involves training a robust encoderdecoder to provide the pixel embedding for each pixel position in the image, where similar objects should have similar features. The second step focuses on learning label mappings and unified label embedding space by GNNs. During the training phase, we alternate between these two steps, freezing one network while training another network. Both of these steps require supervised training using dataset-specific annotations. Here, we primarily focus on the training of GNNs, while further details of the training strategies are provided in the Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Training with Dataset-specific Annotations. During training, an image is randomly sampled from dataset $i$ and fed into an segmentation network, which provides embedding for each pixel position $\\mathcal{P}=\\{\\mathbf{p}_{1},\\mathbf{p}_{2},\\ldots,\\mathbf{p}_{j}\\}$ , where $\\mathbf{p}_{k}$ is $D$ -dimensional vectors. We utilize a unified segmentation head (UniSegHead) to assign a dataset label to each pixel. In the UniSegHead, We first project the pixel embedding into the unified label embedding space by multiplying the pixel embedding by the output feature of unified label node $\\mathbf{X}_{u}$ , as shown in Equation 4. Then, to train the universal model with dataset-specific annotations, we need to map the predictions in unified label space to dataset-specific label space to obtain the probabilities of dataset-specific classes. This is achieved by computing per-pixel dataset-specific logits s by multiplying the dataset-specific label mappings $\\mathbf{M}_{i}$ at each pixel: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf u}_{k}={\\bf X}_{u}{\\bf p}_{k},}\\\\ {{\\bf s}_{k}={\\bf M}_{i}{\\bf u}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, probabilities of dataset-specific classes are computed by per-pixel softmax operation over the logits $s$ . This allows us to use dataset-specific annotations to compute the pixel-wise loss function, train the network, and update the label mappings. We formulate the cross-entropy loss for a specific pixel to train the segmentation network: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c e}(\\mathbf{y},\\mathbf{s})=-\\sum_{c=1}^{|L_{i}|}y_{c}\\log(\\mathrm{softmax}(\\mathbf{s})_{c}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{y}$ is the pre-pixel annotation, $|L_{i}|$ represents the total number of classes in the dataset $i$ from which the image originates. ", "page_idx": 4}, {"type": "text", "text": "Orthogonality Loss. To achieve a confilct-free unified label space and avoid redundant unified label nodes that represent the same class or have overly similar features, we introduce soft constraints to promote orthogonality among the unified label node features, inspired by [39]. This orthogonality loss encourages unified label node embedding to be mutually orthogonal. It not only aligns the unified label nodes with annotation standards for practical use but also enhances the diversity of the model and helps in finding a better label mappings: ", "page_idx": 4}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/b92f6f13363a11115e1cda997b32ec91e253b2c36bce756f5fb530aaff895645.jpg", "table_caption": ["Table 2: Multi-dataset performance compared with other methods. "], "table_footnote": ["1 Approach to construct label space. MC:Manually Construct, MR:Manually Relabel, DS:Dataset-specific, Auto:Automatically Construct. 2 MSeg train and evaluate on 43 of 65 class in Mapillary dataset, 117 of 150 class in ADE dataset, 122 of 133 class in COCO dataset. 3 These methods were trained and evaluated using 30 classes from the IDD dataset, while we trained and evaluated using the officially recommended 26 classes. "], "page_idx": 5}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/56d5cb7d6eb28609eda8d1053e1044c79bce48a3c8c34e32d1546ab23d1b3c98.jpg", "table_caption": ["Table 3: Performance comparison with two baselines on training and unseen datasets. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{o r t h}=-\\sum_{i=1}^{N}\\mathrm{softmax}(\\mathbf{X}_{u}\\mathbf{x}_{i})_{i}\\log(\\mathrm{softmax}(\\mathbf{X}_{u}\\mathbf{x}_{i})_{i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The final loss function used to train the GNNs is represented as follows, with $\\lambda s$ as hyperparameters to adjust the weights of different loss functions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\lambda_{1}\\mathcal{L}_{c e}+\\lambda_{2}\\mathcal{L}_{o r t h}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We list the semantic segmentation datasets used for training and testing in Table 1. Our training datasets cover a wide range of scenarios, from indoor scenes to driving scenes. We also introduce corresponding test datasets, which are not used in the training process, for the respective scenes to evaluate our generalization capability. ", "page_idx": 5}, {"type": "text", "text": "Implementaion Details. Our segmentation model is based on the HRNet-W48 architecture [38], while the GNN model is a three-layer GraphSAGE [17]. We utilize the llama-2-7B model to encode label descriptions into 4096-dimensional text features. These text features, augmented with dataset embedding of the same dimensionality, are then employed as node features input into the GraphSAGE. When forming a minibatch from multiple datasets, we evenly sample 3 images per dataset within a batch for each GPU. For all images, We first apply random resizing with a ratio ranging from 0.5 to 2, followed by a random crop operation to achieve a final image size of $768\\times768$ pixels. We use AdamW optimizer [27] with warmup and polynomial learning rate decay, starting with a learning rate of 0.0001. We train our model for $300\\mathrm{k}$ iterations on four 80G A100 GPUs. ", "page_idx": 5}, {"type": "text", "text": "4.1 Comparison on Multiple Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Table 2, we present the accuracy of our methods and compare them to other approaches on the seven training datasets. We use mean Intersection over Union (mIoU) to quantify the performance of models, a common metric used to evaluate the performance of segmentation models. Different methods adopt various approaches to construct their label spaces: Dataset-Specific represents a lack of a unified label space, where the model outputs a separate label space for each dataset. Manually ", "page_idx": 5}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/4d80130fb30270e30fa8b8aa4cf6bdf5a73f3267c6fa19535d54ce9ac63d9a3e.jpg", "img_caption": ["Figure 3: Visual comparisons with Single dataset model on different training datasets. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Relabel means manually re-annotating each image. Manually Construct means the label space is constructed through human expertise. Automatically Construct includes methods where the unified label space is automatically constructed by the model. We also establish two baseline methods: Single dataset and Multi-SegHead. Single dataset demonstrates the results of training on individual dataset only, while Multi-SegHead trains on multiple datasets by using dataset-specific segmentation heads. ", "page_idx": 6}, {"type": "text", "text": "The results demonstrate that our method achieves the best average performance in multi-dataset training, while also achieving significant performance improvements on datasets with a large number of classes such as the ADE and COCO datasets. We attribute this to the construction of a robust unified label space. Leveraging visual connections from the samples, our unified label space can discover label relationships beyond textual similarities. For instance, the visual appearance of the fireplace in ADE is similar to the tunnel in Mapillary. Despite their different semantic meanings, our model merges these labels for prediction, as detailed in subsection 4.5. This approach saves model capacity and facilitates knowledge transfer across datasets for improved prediction. ", "page_idx": 6}, {"type": "text", "text": "In Table 3, as a supplement to Table 2, we compare our model with various models trained on single dataset, as well as each segmentation head output of the Multi-SegHead. Detailed data for each dataset can be found in the Appendix D. From the table, it can be observed that training with multiple datasets helps improve the model\u2019s generalization performance. However, the performance of different segmentation head outputs in Multi-SegHead model shows significant differences due to the lack of a unified form of output that performs well across all datasets. In contrast, our approach provides a unified label space covering all datasets, resulting in a significant advantage in average performance. We also list the performance on the five unseen test datasets mentioned in Table 1. To evaluate the performance of our model on unseen datasets, we first evaluate the model results on its training dataset. We search for the optimal label mappings based on the accuracy of label predictions. There are no updates to any model parameters except the label mappings in this process. This process can actually be done manually without any annotation information. The results indicate that our model exhibits better generalization performance. It can handle various scenarios and consistently achieve excellent performance on the test set. Compared to Multi-SegHead model, our automatically constructed label space has advantages over dataset label spaces. The unified label space can integrate the semantic information from multiple dataset label spaces. ", "page_idx": 6}, {"type": "text", "text": "Figure 3 presents the segmentation results on multiple datasets predicted in unified label space. Compared to models trained only on Single dataset, our model successfully provides consistent predictions on all datasets. It\u2019s worth noting that in the BDD dataset, annotations are not provided for lane marking, crosswalk and manhole, which are only annotated in the Mapillary dataset. Our model successfully integrates the label space of the Mapillary dataset, thereby predicting these classes in the BDD dataset. More results are presented in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results on WildDash 2 Benchmark. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "WildDash 2 [47] provides a benchmark for semantic segmentation, designed to test the robustness of algorithms in real-world driving scenarios. Due to the insufficient number of training samples provided by this dataset, the official recommendation is to use multiple datasets for training. Therefore, this benchmark is well-suited to evaluate the effectiveness of multi-dataset training methods. The WildDash 2 dataset includes negative test cases to challenge the robustness of the model. These negative test cases mainly consist of unconventional driving scenarios, and even non-driving scenarios. Across all pixels within negative test images, a robust model is expected to predict the void label for open-set classes and anomalous objects. The WildDash 2 benchmark refers to the metric named Meta Avg mIoU Class, which calculates the mean Intersection over Union for each class by weighting negative and positive test cases according to their occurrence in the benchmark dataset. ", "page_idx": 6}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/c4b2b32332d4ec9e460422e3842b6be7be8b28a9b1402fcbe1d152bb2fbcbe33.jpg", "table_caption": ["Table 4: Performance comparison on WildDash 2 benchmark. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/2bed475aa41e273701268ab58cd002fd733b5f45d42286f4e6ba331290625a77.jpg", "table_caption": ["Table 5: Comparison of Different Methods of Construct Label Spaces "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 4 presents the current results on the WildDash 2 leaderboard. We present results for zero-shot generalization using our 7ds model. To evaluate in an unseen setting, we map the non-evaluated classes in the unified label space to a void label for both positive and negative test frames. To ensure a fair comparison with other works, we also provide evaluation results for models trained using the training datasets from the Robust Vision Challenge 2022[1], which include CityScapes, ADE20K, Vistas, VIPER [36], ScanNet, and WildDash 2. The results indicate that our method achieves state-of-the-art performance in both zero-shot and trained settings. Our method exhibits significant performance improvements compared to other methods on negative test cases. We attribute this to the robustness of our model, which has been trained on diverse datasets, enabling it to perform well in unconventional scenarios. Our zero-shot model has been trained on a wider range of datasets compared to the trained model. Therefore, even without training on the WildDash 2 dataset, it achieves similar performance on negative test cases. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further explore the ability of our GNNs to construct a unified label space, we compared it with four alternative methods. The first method concatenates all dataset label spaces into a single unified space $\\textstyle\\bigcup_{i=1}^{D}L_{i}$ . The second approach constructs a unified label space by clustering text features based on cosine similarity using the DBSCAN [7] algorithm. In the third method, we train a segmentation network using an initial adjacency matrix, as outlined in Appendix B, without incorporating GNN training. The final method involves an ablation experiment, removing the label description module to observe its impact. Experimental results, presented in Table 5, demonstrate that the label space constructed based on GNNs can better assist in the learning of segmentation models. Unlike the first approach, our method optimizes model capacity by focusing on label relationships rather than dataset recognition. Compared to the second approach, our method can differentiate between classes with identical names but differing levels of granularity. By leveraging label descriptions to enrich semantic context, our approach constructs a more refined and functional label space. ", "page_idx": 7}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/61b7f48d047480c01239aec8801c022958ea229e4e6b31732ce2f762c9404112.jpg", "img_caption": ["Figure 4: The composition of the training datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Exploring the Impact of Training Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To explore the impact of training dataset selection on model performance, we train a domain-specific model focusing on road driving scenes and a domain-general model on more datasets, as shown in Figure 4. We conduct four sets of comparisons to evaluate the performance across datasets that were trained on both models, trained on one model and not on the other, and not trained on either model. As shown in Table 6 and Appendix E, the domain-specific model can focus more on learning features specific to the particular scene, resulting in slightly better performance on both trained and unseen driving scene datasets compared to domain-general model. On the other hand, domain-general model trained on more scenes and more data exhibit better generalization performance. Therefore, while it does not lag too far behind in performance on driving scene datasets, it demonstrates overwhelming advantages on other scene datasets. ", "page_idx": 8}, {"type": "text", "text": "4.5 Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 5 presents the qualitative analysis results of the label space learned by our model. We compare the label space learned by our model with the label space constructed using text features. The class curb in the IDD dataset actually encompasses both the classes curb and barrier in Mapillary, whereas the constructed label space by text features cannot handle such subclass/superclass relations. In contrast, our GNNs learned label space splits the IDD curb into two classes for prediction, effectively handling such label relationships. Similar situations also include the class tunnel or bridge in the IDD dataset. However, since the proportion of bridge pixels is orders of magnitude greater than that of tunnels ( $\\mathrm{^{10^{8}}}$ pixels for bridge and $10^{4}$ pixels for tunnel), a more reasonable approach is to merge it with the class bridge, as our GNNs have done. Additionally, it is worth noting that due to visual similarities, the Mapillary tunnel has been merged with the ADE fireplace. This does not actually introduce any confilct because no dataset simultaneously annotates both the tunnel and fireplace. The model will construct the label space in a way that facilitates its learning process. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We propose a novel approach that leverages graph neural networks to construct a unified label space for training semantic segmentation models across multiple datasets. Our method addresses the challenge of label conflicts in multi-dataset semantic segmentation and demonstrates performance improvements across various datasets. The unified label space generated during training, generalizes well to unseen datasets, showcasing the effectiveness of our approach. ", "page_idx": 8}, {"type": "text", "text": "Broader Impact. Our work explores the use of graph neural networks to unify label spaces between datasets, providing a new direction for achieving robust and efficient multi-dataset training. By enabling semantic segmentation models to be trained on multiple datasets with a unified label space, our method can potentially reduce human effort required for re-labeling images and facilitate the expansion of training datasets. This can lead to the development of models that are more universally applicable across various datasets, benefiting a wide range of applications. ", "page_idx": 8}, {"type": "text", "text": "Limitations. Although our approach does not require manual relabeling efforts, it still relies on fully annotated datasets for training, in contrast to weakly-supervised and unsupervised methods. We aim to explore ways to integrate these alternative methods in future research. Errors in the fully automated construction of a unified label space do present some safety risks for autonomous driving tasks. Therefore, we recommend introducing a manual review mechanism to address these issues. Additionally, using ChatGPT may generate inaccurate label descriptions, which could affect the prediction of label relationships. Therefore, we aim to improve the accuracy of label descriptions by incorporating label descriptions provided by official datasets as prompts. ", "page_idx": 8}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/7044141231bd40aa76f25a8b2423b46f264d0ff3fd2d053342b988d66592c95b.jpg", "img_caption": ["Figure 5: Comparison of unified label space learned by GNNs with constructed by text features. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by NSFC Project (62176061), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0103). The computations in this research were performed using the CFFF platform of Fudan University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Robust vision challenge. http://www.robustvision.net/index.php. Accessed: 2022- 12-02.   \n[2] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundational models defining a new era in vision: A survey and outlook. arXiv preprint arXiv:2307.13721, 2023.   \n[3] Petra Bevandi\u00b4c, Marin Or\u0161i\u00b4c, Ivan Grubi\u0161i\u00b4c, Josip \u0160ari\u00b4c, and Sini\u0161a \u0160egvi\u00b4c. Multi-domain semantic segmentation with pyramidal fusion. arXiv preprint arXiv:2009.01636, 2020.   \n[4] Petra Bevandi\u00b4c, Marin Or\u0161i\u00b4c, Ivan Grubi\u0161i\u00b4c, Josip \u0160ari\u00b4c, and Sini\u0161a \u0160egvi\u00b4c. Multi-domain semantic segmentation with overlapping labels. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2615\u20132624, 2022.   \n[5] Petra Bevandic\u00b4, Marin Or\u0161ic\u00b4, Josip \u0160aric\u00b4, Ivan Grubi\u0161ic\u00b4, and Sini\u0161a \u0160egvic\u00b4. Weakly supervised training of universal visual concepts for multi-domain semantic segmentation. International Journal of Computer Vision, pages 1\u201323, 2024.   \n[6] Petra Bevandic\u00b4 and Sini\u0161a \u0160egvic\u00b4. Automatic universal taxonomies for multi-domain semantic segmentation. British Machine Vision Conference, 2022.   \n[7] FM Bi, WK Wang, and L Chen. Dbscan: density-based spatial clustering of applications with noise. J. Nanjing Univ, 48(4):491\u2013498, 2012.   \n[8] Gabriel J Brostow, Julien Fauqueur, and Roberto Cipolla. Semantic object classes in video: A high-definition ground truth database. Pattern Recognition Letters, 30(2):88\u201397, 2009.   \n[9] Lei Cai and Shuiwang Ji. A multi-scale approach for graph link prediction. In Proceedings of the AAAI conference on artificial intelligence, pages 3308\u20133315, 2020.   \n[10] Lei Cai, Jundong Li, Jie Wang, and Shuiwang Ji. Line graph neural networks for link prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5103\u20135113, 2021.   \n[11] Yanbei Chen, Manchen Wang, Abhay Mittal, Zhenlin Xu, Paolo Favaro, Joseph Tighe, and Davide Modolo. Scaledet: A scalable multi-dataset object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7288\u20137297, 2023.   \n[12] Jes\u00fas Cid-Sueiro. Proper losses for learning from partial labels. Advances in neural information processing systems, 25, 2012.   \n[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213\u20133223, 2016.   \n[14] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017.   \n[15] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303\u2013338, 2010.   \n[16] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013.   \n[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[18] Junheng Hao, Tong Zhao, Jin Li, Xin Luna Dong, Christos Faloutsos, Yizhou Sun, and Wei Wang. P-companion: A principled framework for diversified complementary product recommendation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 2517\u20132524, 2020.   \n[19] Dongwan Kim, Yi-Hsuan Tsai, Yumin Suh, Masoud Faraki, Sparsh Garg, Manmohan Chandraker, and Bohyung Han. Learning semantic segmentation from multiple datasets with label shifts. In European Conference on Computer Vision, pages 20\u201336. Springer, 2022.   \n[20] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016.   \n[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[22] Ajay Kumar, Shashank Sheshar Singh, Kuldeep Singh, and Bhaskar Biswas. Link prediction techniques, applications, and performance: A survey. Physica A: Statistical Mechanics and its Applications, 553:124289, 2020.   \n[23] John Lambert, Zhuang Liu, Ozan Sener, James Hays, and Vladlen Koltun. Mseg: A composite dataset for multi-domain semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2879\u20132888, 2020.   \n[24] Feng Lin, Wenze Hu, Yaowei Wang, Yonghong Tian, Guangming Lu, Fanglin Chen, Yong Xu, and Xiaoyu Wang. Universal object detection with large vision model. International Journal of Computer Vision, 132(4):1258\u20131276, 2024.   \n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[26] Yajie Liu, Pu Ge, Qingjie Liu, Shichao Fan, and Yunhong Wang. An empirical study on multi-domain robust semantic segmentation. arXiv preprint arXiv:2212.04221, 2022.   \n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[28] Shota Masaki, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi. Multidomain semantic-segmentation using multi-head model. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pages 2802\u20132807. IEEE, 2021.   \n[29] Panagiotis Meletis and Gijs Dubbelman. Training semantic segmentation on heterogeneous datasets. arXiv preprint arXiv:2301.07634, 2023.   \n[30] Lingchen Meng, Xiyang Dai, Yinpeng Chen, Pengchuan Zhang, Dongdong Chen, Mengchen Liu, Jianfeng Wang, Zuxuan Wu, Lu Yuan, and Yu-Gang Jiang. Detection hub: Unifying object detection datasets via query adaptation on language embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11402\u201311411, 2023.   \n[31] Rohit Mohan and Abhinav Valada. Efficientps: Efficient panoptic segmentation. International Journal of Computer Vision, 129(5):1551\u20131579, 2021.   \n[32] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 891\u2013898, 2014.   \n[33] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages 4990\u20134999, 2017.   \n[34] Lorenzo Porzi, Samuel Rota Bulo, Aleksander Colovic, and Peter Kontschieder. Seamless scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8277\u20138286, 2019.   \n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[36] Stephan R. Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 2232\u20132241, 2017.   \n[37] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567\u2013576, 2015.   \n[38] Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao, Dong Liu, Yadong Mu, Xinggang Wang, Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions. arXiv preprint arXiv:1904.04514, 2019.   \n[39] Marco Toldo, Umberto Michieli, and Pietro Zanuttigh. Unsupervised domain adaptation in semantic segmentation via orthogonal and clustered embeddings. In Proceedings of the IEEE/CVF Winter conference on Applications of Computer Vision, pages 1358\u20131368, 2021.   \n[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[41] Jasper Uijlings, Thomas Mensink, and Vittorio Ferrari. The missing link: Finding label relations across datasets. In European Conference on Computer Vision, pages 540\u2013556. Springer, 2022.   \n[42] Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, and CV Jawahar. Idd: A dataset for exploring problems of autonomous navigation in unconstrained environments. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1743\u20131751. IEEE, 2019.   \n[43] Petar Veli\u02c7ckovi\u00b4c. Everything is connected: Graph neural networks. Current Opinion in Structural Biology, 79:102538, 2023.   \n[44] Li Wang, Dong Li, Han Liu, Jinzhang Peng, Lu Tian, and Yi Shan. Cross-dataset collaborative learning for semantic segmentation in autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2487\u20132494, 2022.   \n[45] Junfei Xiao, Zhichao Xu, Shiyi Lan, Zhiding Yu, Alan Yuille, and Anima Anandkumar. 1st place solution of the robust vision challenge 2022 semantic segmentation track. arXiv preprint arXiv:2210.12852, 2022.   \n[46] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, Trevor Darrell, et al. Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687, 2(5):6, 2018.   \n[47] Oliver Zendel, Matthias Sch\u00f6rghuber, Bernhard Rainer, Markus Murschitz, and Csaba Beleznai. Unifying panoptic segmentation for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21351\u201321360, 2022.   \n[48] Min-Ling Zhang, Fei Yu, and Cai-Zhi Tang. Disambiguation-free partial label learning. IEEE Transactions on Knowledge and Data Engineering, 29(10):2155\u20132167, 2017.   \n[49] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural information processing systems, 31, 2018.   \n[50] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive survey on automatic knowledge graph construction. arXiv preprint arXiv:2302.05019, 2023.   \n[51] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2019.   \n[52] Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin Wang, and Fan Wang. Lmseg: Language-guided multi-dataset segmentation. In The Eleventh International Conference on Learning Representations, 2022.   \n[53] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Simple multi-dataset detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7571\u20137580, 2022.   \n[54] Guangming Zhu, Liang Zhang, Hongsheng Li, Peiyi Shen, Syed Afaq Ali Shah, and Mohammed Bennamoun. Topology-learnable graph convolution for skeleton-based action recognition. Pattern Recognition Letters, 135:286\u2013292, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Training Strategy ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The training process is divided into three distinct stages: Multi-SegHead Training Stage, Alternating GNNs Training Stage and Alternating SegNet Training Stage, as shown in Algorithm 1. Initially, we commence with the Multi-SegHead Training Stage for $100\\mathrm{k}$ iterations. After completing MultiSegHead Training Stage, we discard the multi-heads and initialize the UniSegHead for predicting class probabilities during subsequent training phases. Subsequently, we alternate between the Alternating GNNs Training Stage and Alternating SegNet Training Stage for a total of three cycles, each lasting 20k iterations. Notably, we prioritize the Alternating GNNs Training stage at the beginning of each cycle. ", "page_idx": 13}, {"type": "text", "text": "Multi-SegHead Training Stage aims to equip the segmentation network with fundamental segmentation capabilities. By commencing with a Multi-SegHead model, we provide a robust starting point for the subsequent stages, enhancing both performance and training efficiency. During this stage, the network is trained on multiple datasets using specific segmentation heads for each dataset. Each segmentation head comprises feature weights $\\mathbf{W}_{i}$ of dimension $C\\times|L_{i}|$ , where $|L_{i}|$ represents the number of classes of dataset $i$ . Pixel embedding $\\mathcal{P}$ is multiplied by the $i$ -th feature weight to make predictions within the $i$ -th dataset\u2019s label space. This facilitates the utilization of dataset-specific annotations and cross-entropy loss for network training. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}(\\mathbf{y},\\mathbf{s})=-\\sum_{c=1}^{|L_{i}|}y_{c}\\log(\\mathrm{softmax}(\\mathbf{W}_{i}\\mathbf{p})_{c}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Alternating GNNs Training Stage. Training SegNet and GNNs simultaneously is challenging and impractical because training the segmentation network requires a discrete label mapping, while training the GNNs necessitates a continuous and differentiable adjacency matrix. Hence, we alternately training the GNNs and the segmentation network. The Alternating GNNs Training Stage focuses on optimizing a unified label space. During this stage, the GNNs are trained while keeping the segmentation network frozen. When mapping the prediction from the unified label space to the dataset label space, we directly multiply the adjacency matrix ${\\bf M}_{a}$ . We calculate the loss according to Equation 8. ", "page_idx": 13}, {"type": "text", "text": "Alternating SegNet Training Stage. During this stage, the GNNs remain frozen while the segmentation network is trained. We will use the technique detailed in Appendix C to convert the continuous adjacency matrix into a discrete label mappings that satisfies the constraints, and then use it to compute the cross-entropy loss function and train according to Equation 6. During the last Alternating SegNet training stage, we extend the training procedure to 100k iterations and drop the entire GNNs but retain the dataset label mappings $\\{\\mathbf{M}_{i},i=1,...,K\\}$ . The unified label embedding $\\mathbf{X}_{u}$ in this stage is no longer keep frozen and trained with the SegNet. In the latter part of this stage, we will use training dataset to evaluate the model\u2019s performance. Those links between the unified label space and dataset-specific label space which is not activated during the evaluation will be removed. After removing the unused connections, we continue training with the new label mappings. And thus, we obtain the final model. When applying the final model to predict in the wild, we could manually assign each unified label node a class name accord to their prediction and semantics. ", "page_idx": 13}, {"type": "text", "text": "B Selection of Unified Label Node Quantity and Initialization of Adjacency Matrix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appropriate quantity of unified label nodes is crucial for balancing model performance and computational expenses. An excessive number of unified label nodes not only increases computational costs but may also result in redundancy in both the model and semantic space, leading to situations where multiple unified label nodes represent similar semantic concepts. Conversely, an insufficient number of unified label nodes can prevent the model from fully expressing the entire semantic space $\\textstyle\\bigcup_{i=1}^{D}L_{i}$ , making it difficult to learn certain classes in the datasets, thus impacting model performance. Inspired by [53], we determine the number of unified label nodes using the following steps. We first enumerate all feasible schemes for merging dataset labels, which form the Cartesian product of all dataset label sets $\\mathbb{L}=L_{1}\\times L_{2}\\times...\\times L_{K}$ . Label merging does not require the participation of all datasets simultaneously, so $L_{i}$ can be an empty element, indicating no participation in label merging. ", "page_idx": 13}, {"type": "text", "text": "Input: the number of datasets $K$ , the Segmentation Network SegNet, the Graph Neural Networks   \n$G N N s$ , Multi-head iters $I_{p}$ , GNNs training iters $I_{g}$ and SegNet training iters $I_{s}$   \nStage $=$ Multi-head stage; iter $=0$   \nfor a multi-datasets sampled mini-batch $\\{x_{i},y_{i}\\}_{i=1}^{K}$ do if Stage $==$ Multi-SegHead Training: Calculate ${\\mathcal{L}}_{m}$ by Equation 9 Update $S e g N e t$ to minimize ${\\mathcal{L}}_{m}$ if ite $\\cdot++>I_{p}$ : Replace Multi-SegHead with UniSegHead; Stage $=$ GNNs training; iter $=0$ if Stage $==$ GNNs training: Calculate $\\mathcal{L}_{g}$ by Equation 8 Update $G N N s$ to minimize $\\mathcal{L}_{g}$ if ite $\\uparrow++>I_{g}$ : Solve dataset label mappings by Algorithm 2; Stage $=$ Seg training; iter $=0$ if Stage $==$ SegNet training: Calculate $\\mathcal{L}_{c e}$ by Equation 6 Update $S e g N e t$ to minimize $\\mathcal{L}_{c e}$ if i $\\mathrm{ter}{+}{+}>I_{s}$ : Stage $=$ GNNs training; iter $=0$ end switch   \nend for ", "page_idx": 14}, {"type": "text", "text": "We evaluate the quality of label merging using the predefined loss function $\\mathcal{L}_{c}$ , where merging classes with the same semantic meaning results in smaller losses. Additionally, we require that the merging satisfies the constraint: each dataset node has only one corresponding merged node. It is worth noting that this constraint is stronger than our label mapping constraint described in subsection 3.1, thus ensuring that our adjacency matrix is always valid. We formulate optimization objective function as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{minimize}_{x}\\quad\\displaystyle\\sum_{t\\in\\mathbb{T}}x_{t}E_{S_{k}}\\left[\\sum_{\\substack{c\\in L_{k}\\,|t(c)=1}}\\mathcal{L}_{c}\\left(S_{c}^{k},\\tilde{S}_{c}^{k}\\right)\\right]+\\lambda|L|}\\\\ &{\\mathrm{subject\\to}\\quad\\displaystyle}&{\\sum_{t\\in\\mathbb{T}|t_{c}=1}x_{t}=1\\quad\\forall_{c},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbb{T}$ represents the set of all feasible edges connecting dataset label nodes to unified label nodes, $x_{t}\\in\\{0,1\\}$ indicates whether edge $\\pmb{t}\\in\\mathbb{T}$ is selected, with $x_{t}=1$ indicating the presence of edge $\\pmb{t}$ and 0 indicating its absence. $\\scriptstyle S_{k}$ denotes the dataset to which edge $\\pmb{t}$ belongs, $L_{k}$ represents the label space of $\\ensuremath{\\boldsymbol{S}}_{k}$ , and $\\pmb{t}(c)$ is the label connected by edge $\\pmb{t}$ . $\\lambda|L|$ is the penalty term for the number of nodes, encouraging the optimizer to merge similar nodes to obtain a more compact unified label space. Based on experimental insights and referencing paper [52], we selected the hyperparameter $\\lambda=0.5$ . $\\mathcal{L}_{c}\\left(S_{c}^{k},\\tilde{S}_{c}^{k}\\right)$ is primarily used to evaluate the quality of label merging, where $S_{c}^{k}$ represents the output results on class $c$ using the original dataset segmentation head, and $\\tilde{S}_{c}^{k}$ represents the output results on class $c$ using segmentation heads from other datasets participating in the merging. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}\\left(S_{c}^{k},\\tilde{S}_{c}^{k}\\right)=\\sum_{s_{c}\\in\\tilde{S}_{c}^{k}}I o U(S_{c}^{k})-I o U(s_{c}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where, $I o U$ is a metric commonly used to measure the performance of semantic segmentation models. Therefore, we use this metric here to evaluate the assistance of merged nodes to the model. By using a linear optimization solver to solve the optimization objective Equation 10, we can obtain a unified label space. We use the number of labels in this label space as the quantity of our unified label nodes and initialize our adjacency matrix based on their label mapping relationships. ", "page_idx": 14}, {"type": "text", "text": "C Solving the dataset Label Mappings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given the definition of the label mappings and the constraints outlined in subsection 3.1, solving the dataset label mappings could be conceptualized as a weighted bipartite graph matching problem. The weight of each edge is determined by the values of the GNNs learnable adjacency matrix. The objective is to maximize the sum of edge weights while ensuring that each node of the specific dataset is connected. The unbalanced optimal transport algorithm offers an approximate solution to such ", "page_idx": 14}, {"type": "text", "text": "Input: Submatrix of adjacency matrix for each dataset $\\{S^{(i)},i\\,=\\,1,...,K\\}$ , the classes of each dataset $\\{c^{(i)},i=1,...,K\\}$ , the number of unified label node $N$   \nOutput: Mapping matrices $M^{(1)},...,M^{(K)}$ Initialize $\\mathbf{\\tau}=\\frac{1}{\\mathbf{N}}\\mathbf{1}_{\\mathbf{N}\\times1},\\beta^{(i)}=\\{\\frac{1}{c^{(i)}}\\mathbf{1}_{c^{(i)}\\times1},i=1,...,K\\},d=0,\\mu$ for $i=1$ to $K$ do $Q_{c^{(i)}\\times{\\bf N}}^{(i)}=\\mathrm{UOT}(\\alpha,\\beta^{(i)},S^{(i)})$ # Assign the label to the node with the highest score $M_{j,k}^{(i)}=\\arg\\operatorname*{max}_{j}\\{Q_{j,1}^{(i)},...,Q_{j,\\mathbf{N}}^{(i)}\\}$ for $j=1$ to $\\boldsymbol{c}^{(i)}$ do # Find an unlinked dataset-specific node if max M (i) $M_{j}^{(i)}==0$ then qj = sort(Q(ji )) # Find first multi-mapped node, where $s u m(M_{j^{\\prime}}^{(i)})>1$ $j^{\\prime},k^{\\prime}=\\mathrm{findFirstMulMapped}(q_{j},M^{(i)})$ # Replace the corresponding label mappings $M_{j^{\\prime},k^{\\prime}}^{(i)}=0,M_{j,k^{\\prime}}^{(i)}=1$ end if $\\begin{array}{r l}&{\\tilde{\\beta}_{j}^{(i)}=\\sum_{k=1}^{c^{(i)}}Q_{j,k}^{(i)}}\\\\ &{\\beta^{(i)}=\\mu\\beta^{(i)}+(1-\\mu)\\bar{\\beta}^{(i)}}\\end{array}$ end for ", "page_idx": 15}, {"type": "text", "text": "problems, but it does not guarantee adherence to the constraint of connecting each node in the specific dataset. Therefore, based on the optimal matching result obtained from the algorithm, we employ a greedy strategy to adjust the matching scheme for all unconnected nodes within the specific dataset. ", "page_idx": 15}, {"type": "text", "text": "Initially, for every unconnected dataset-specific node, we sort all unified label nodes based on their matching preferences, from highest to lowest. Subsequently, we examine these unified label nodes and their associated dataset-specific nodes. In cases where the associated dataset-specific node is also connected by another unified label node, we adjust the edge so that this unified label node links to the unconnected specific dataset node, following this process until all nodes are connected. The specific procedure is outlined in Algorithm 2. ", "page_idx": 15}, {"type": "text", "text": "D Comparison with Baselines on Multiple Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 7 lists the results of mutual evaluation among training datasets. The results indicate that individually-trained models generally demonstrate good accuracy when tested on the same dataset, but perform poorly on other datasets. Our method can leverage knowledge from multiple datasets to improve performance on individual datasets. Table 8 lists the results of the Multi-SegHead model\u2019s predictions on different datasets using different segmentation head outputs. It can be observed that none of the segmentation head outputs could consistently provide accurate predictions across all datasets, indicating that their label spaces do not effectively cover all datasets. Table 9 and Table 10 list the performance of different models on unseen datasets. The results indicate that our method has better generalization performance on unseen datasets compared to other methods. Additionally, the unified label space we construct contains richer semantic information, enabling flexible adaptation to datasets from different scenarios. ", "page_idx": 15}, {"type": "text", "text": "Table 7: Semantic segmentation accuracy (mIoU) on training datasets compared with Single dataset model. ", "page_idx": 16}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/29f907d50a1abc081c4ddc7e07701390c06e0221d8c8bde2bf91fbf36342974a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/78d54015415ff1bc4907d20c18d3ed9b9f3e4f77afdb5ed907d07a96a51ff096.jpg", "table_caption": ["Table 8: Semantic segmentation accuracy (mIoU) on training datasets compared with Multi-SegHead. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 9: Semantic segmentation accuracy (mIoU) on unseen datasets compared with Single dataset. ", "page_idx": 16}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/3180f62ae6d6535845470e2ead78e11be33b53c257873e8d80b3f4468da91ebf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 10: Semantic segmentation accuracy (mIoU) on unseen datasets compared with Multi-SegHead. ", "page_idx": 16}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/bb7fa903e9fee9cea5d6a062c3128fcedb2808137348468a0100b1275d314990.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/c6feb6a4f0e61eb5bddc8098c0e89640e0486720a8535d54dd459547454a85bb.jpg", "table_caption": ["Table 11: Performance on both trained datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/3c59eb2e12c7917e040723037c183dd7068856dbdf73bb00aef6a3d8d64e315d.jpg", "table_caption": ["Table 12: Unseen domain-general model vs. Trained domain-specific model. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/57e26d5327bc137ec8708f3fb070804a3fd07c551e30911183fa335d03150eb8.jpg", "table_caption": ["Table 13: Trained domain-general model vs. Unseen domain-specific model. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "gSGLkCX9sc/tmp/0846456a7669be36613a8dd1a874323c1e0acf4c8ed93ff9715db6c5f9845095.jpg", "table_caption": ["Table 14: Performance on both unseen datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Exploring the Impact of Training Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Tables 11 to 14 present a performance comparison between the domain-general and domain-specific models across various training settings and datasets. Figure 6 illustrates the segmentation results of these models on the different datasets. ", "page_idx": 17}, {"type": "text", "text": "Performance Comparison on Both Trained Datasets. From Table 11, it can be concluded that the performance of the domain-specific model focusing on driving scenes is higher on all trained datasets compared to the domain-general model, despite the domain-general model being trained on five times more samples than the domain-specific model. This is likely attributed to the fact that the domain-specific model needs to predict fewer classes and is less affected by label space conflicts. The model can therefore focus on learning features specific to the particular scene. ", "page_idx": 17}, {"type": "text", "text": "Performance Comparison: Unseen Domain-general Model vs. Trained Domain-specific Model. In Table 12, we selected a driving scene dataset that the domain-specific model was trained on, while the domain-specific model was not trained on it, for comparison. As expected, the domain-specific model demonstrates superior performance, and the domain-general model closely follows suit. ", "page_idx": 17}, {"type": "text", "text": "Performance Comparison: Trained Domain-general Model vs. Unseen Domain-specific Model. In Table 13, we selected several non-driving scene datasets that the domain-general model was trained on, while the domain-specific model was not trained on it. The domain-specific model fails to predict in non-driving scenes. Compared to Table 12, the generalization performance of the domain-specific model in unseen scenes markedly trails behind the domain-general model. Although the domain-specific model may have encountered similar objects in driving scene datasets like wall, it still struggles to predict these seen objects well in non-driving scene datasets, as shown in Figure 6. ", "page_idx": 17}, {"type": "text", "text": "Performance Comparison on Both Unseen Datasets. In Table 14, we selected datasets that were unseen to both models, including driving scene datasets and non-driving scene datasets. We can observe that the generalization performance of the domain-specific model is slightly better in driving scenes, but its performance in other scenes lags far behind that of the domain-general model. Overall, models trained on more scenes and more data tend to achieve better generalization performance. ", "page_idx": 17}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/e1190ab38319300609754aeacf10429a0e3656978927c45b1d3bc91bdb306a72.jpg", "img_caption": ["Figure 6: Visual comparisons of different training dataset models. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "F Visualization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 7 presents the visual comparisons of models trained on single dataset and our model predicted in universal label space. From the figure, it is evident that our model achieves consistently strong performance across all training datasets while integrating label spaces from different datasets. For example, it predicts class lane marking and crosswalk for the ADE and BDD datasets, and predicts class books for the SUN dataset. Figure 8 shows our universal predictions on test datasets. The results demonstrate that our method generalizes well across multiple unseen datasets from different domains. Figure 9 shows the visual comparisons of different models on the WildDash 2 benchmark. ", "page_idx": 19}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/48e7f1cdbee421d96747f2c3a4a8b1b11a7d3fa41a7ff838ce5caafa0760f853.jpg", "img_caption": ["Figure 7: Visual comparisons on training datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/f7cb04238bc20ef1ee6907fd4e6b78de4721326f84ddc87fbd7df726ad6c1253.jpg", "img_caption": ["Figure 8: Visual comparisons on unseen test datasets. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "gSGLkCX9sc/tmp/0df8e91a4f57823be0c0f5bda0939b3400ef677b78295409cbf58d009b975198.jpg", "img_caption": ["Figure 9: Visual comparisons on WildDash 2 benchmark. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The final paragraph of our introduction (section 1) provides a comprehensive summary of the paper\u2019s contributions, which align with the claims made in the abstract. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In the third paragraph of our conclusion (section 5), we discuss the limitations of our work, including factors influencing performance, and computational efficiency considerations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In section 3, Appendix A, we provide detailed implementation steps, ensuring that our experiments can be replicated. Additionally, in section 4, we thoroughly describe the settings of our experiments, further facilitating reproducibility. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide reproducible code along with detailed instructions, ensuring that the main experimental results can be faithfully reproduced. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In section 4, we provide comprehensive details of our experimental setting. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Training on numerous datasets is time-consuming, with each experiment taking over a week. During our experiments, we observed consistent and stable performance in terms of mIoU for our model. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In section 4, we specify that our experiments were conducted on four 80G A100 GPUs. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have read and adhered to the NeurIPS Code of Ethics in conducting our research. ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the first paragraph of our conclusion (section 5), we thoroughly discuss the broader impacts of our work. ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our research utilizes openly available datasets for training, and our models are primarily used for semantic segmentation tasks. Therefore, there are no such risks associated with our work. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In section 4, we clearly specify the assets used in our work, including datasets and models. We provide proper citations to the original papers and ensure that the licenses and terms of use are explicitly mentioned and respected. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide our code as an asset and include comprehensive documentation alongside it, covering details such as training procedures, licensing, limitations, and any necessary consent obtained from individuals whose assets are used. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing experiments or research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Our work does not involve human subjects. ", "page_idx": 23}]