[{"figure_path": "MvjLRFntW6/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework for multimodal concept extraction and grounding.  Starting with a pre-trained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the method extracts internal representations from the LMM for that token across multiple images. These representations form a matrix Z, which is then linearly decomposed using dictionary learning into a concept dictionary U and activation matrix V. Each concept in U is grounded in both visual and textual domains: visual grounding is done by identifying the images that maximally activate each concept, and textual grounding is achieved by decoding the concept using the LLM's unembedding matrix to find the most probable associated words.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_5_1.jpg", "caption": "Figure 2: Example of multimodal concept grounding in vision and text. Five most activating samples (among decomposed in Z) and five most probable decoded words are shown.", "description": "This figure shows an example of how the model grounds a learned concept in both visual and textual domains.  The top row displays five images that maximally activate a specific concept extracted from the model's internal representation of the word \"dog\". These images represent a variety of dogs with different colors, sizes, and settings. The bottom row shows the top five words predicted by the model as being most associated with that concept.  The combination of images and words illustrates how the concept is semantically grounded in both modalities, providing a multi-modal interpretation of the model's internal representation.", "section": "Experiments"}, {"figure_path": "MvjLRFntW6/figures/figures_7_1.jpg", "caption": "Figure 3: Evaluating visual/text grounding (CLIPScore/BERTScore). Each point denotes score for grounded words of a concept (Semi-NMF) vs Rnd-Words w.r.t the same visual grounding.", "description": "This figure shows a quantitative evaluation of the visual and textual grounding of the concepts learned by the Semi-NMF method. Each point represents a concept, and its position is determined by the CLIPScore (left) or BERTScore (right) achieved.  The x-axis represents the score obtained using random words as a baseline, while the y-axis shows the score obtained using the actual grounded words for each concept.  The plot demonstrates that the grounded words generated using the Semi-NMF method show significantly better correspondence with the visual data than random words, across both CLIPScore and BERTScore metrics,  indicating a strong multimodal grounding of the learned concepts.", "section": "4.2 Results and discussion"}, {"figure_path": "MvjLRFntW6/figures/figures_7_2.jpg", "caption": "Figure 2: Example of multimodal concept grounding in vision and text. Five most activating samples (among decomposed in Z) and five most probable decoded words are shown.", "description": "This figure demonstrates the multimodal concept grounding approach used in the paper.  It shows an example of a concept related to the word \"Dog.\"  The top part displays five images that highly activate this specific concept. These images provide a visual grounding of the concept, illustrating the visual features that the model associates with it. The bottom part shows the top five words that the language model produced when decoding the concept's representation. These words are \"white,\" \"light,\" \"fluffy,\" \"golden,\" and \"dog.\" They represent a textual grounding of the concept, illustrating the semantic meaning of the concept in terms of words related to the visual images. The combination of the visual and textual grounding shows that the concept is well grounded in both the visual and textual domains, making it a multimodal concept. This approach is crucial to interpreting the internal representations of the large multimodal models (LMMs) studied in this paper.", "section": "Experiments"}, {"figure_path": "MvjLRFntW6/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the proposed CoX-LMM framework for multimodal concept extraction and grounding.  Starting with a pre-trained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the framework extracts internal representations of the token across multiple images.  These representations are compiled into a matrix Z, which is then decomposed linearly into a concept dictionary U and activation matrix V. Each concept in U is then grounded in both the visual and textual domains.  Visual grounding is achieved by identifying images that maximally activate each concept, while textual grounding is done by decoding the concept through the LLM to find the most probable words associated with it.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_8_2.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the process of multimodal concept extraction and grounding using the proposed CoX-LMM framework. Starting with a pretrained large multimodal model (LMM) trained for image captioning and a target token (e.g., \"dog\"), the method extracts the LMM's internal representations for that token across multiple images. These representations are compiled into a matrix Z, which is then linearly decomposed using dictionary learning into a concept dictionary (U) and a matrix of concept activations (V). Each concept in the dictionary is then \"grounded\" in both the visual and textual domains. Visual grounding is done by identifying the images that maximally activate each concept, while text grounding is achieved by decoding the concept vector through the LLM's unembedding matrix to obtain the most probable words associated with that concept. This process leads to the extraction of meaningful \"multimodal concepts\" that are useful for interpreting the LMM's internal representations.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_14_1.jpg", "caption": "Figure 7: Multimodal concept grounding in vision and text for the token \u2018Dog\u2019. The five most activating samples and the five most probable decoded words for each component uk, k \u2208 {1, ..., 20} are shown. The token representations are extracted from L=31 of the LLM section of our LMM.", "description": "This figure shows the multimodal grounding of concepts extracted for the token \"Dog\".  For each of the 20 learnt concepts, it displays five images that maximally activate that concept (visual grounding) and the five most probable words associated with that concept as decoded from the language model (textual grounding).  The figure illustrates how concepts learned by the model represent semantically meaningful combinations of visual and textual features relating to the concept of \"dog\". The token representations used were extracted from layer 31 of the Language Model (LLM) component within the larger multimodal model.", "section": "Qualitative results"}, {"figure_path": "MvjLRFntW6/figures/figures_15_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework, starting with a pretrained large multimodal model (LMM) and a target token (e.g., 'dog'). The method extracts internal representations of the token across numerous images, forming a matrix Z.  Dictionary learning linearly decomposes Z into a concept dictionary U and activation matrix V. Each concept (uk) in U is then multimodally grounded; visual grounding (Xk,MAS) is achieved by finding the most activating images for that concept, while textual grounding (Tk) is done by decoding the concept through the language model to get the most probable words. This demonstrates how the framework extracts and grounds multimodal concepts.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_16_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework.  It begins with a pretrained large multimodal model (LMM) processing images related to a target token (e.g., \"Dog\").  The resulting representations are compiled into a matrix, which is then decomposed using dictionary learning into a concept dictionary (U) and concept activations (V). Each concept in the dictionary is then grounded in both the visual and textual domains.  Visual grounding identifies the images that most strongly activate each concept, while textual grounding involves decoding the concept through the LMM's language model to find the most associated words.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_19_1.jpg", "caption": "Figure 11: Variation of reconstruction error with number of concepts K for decompositions on different target tokens.", "description": "This figure shows how the reconstruction error changes as the number of concepts (K) used in the dictionary learning process varies.  The reconstruction error represents the difference between the original data and the approximation produced by the decomposition.  Separate lines show the results for different tokens (dog, cat, bus, train), indicating how the optimal number of concepts may vary depending on the specific token. The graph helps determine the best value for K, balancing accurate representation and model complexity.", "section": "4.2 Results and discussion"}, {"figure_path": "MvjLRFntW6/figures/figures_19_2.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework.  It starts with a pretrained large multimodal model (LMM) processing images related to a target token (e.g., \"dog\"). The resulting representations are formed into a matrix (Z).  Dictionary learning decomposes Z into a concept dictionary (U) and activation coefficients (V). Each concept in U is then connected to visual and textual representations.  Visual grounding involves identifying images that strongly activate a concept, while textual grounding involves decoding the concept through the LLM to find related words.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM pipeline. Starting with a pretrained Large Multimodal Model (LMM) and a target token (e.g., \"Dog\"), the method extracts internal representations from the LMM for that token across multiple images. These representations form a matrix Z, which is decomposed using dictionary learning into a concept dictionary U (basis vectors representing concepts) and an activation matrix V (coefficients showing concept activations for each sample). Each concept in U is multimodally grounded: visually by identifying images that strongly activate it, and textually by decoding it through the LLM to find the most probable words associated with it.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_24_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure provides a visual summary of the CoX-LMM pipeline.  Starting with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the process involves extracting internal representations of that token across multiple images, forming a matrix Z.  Dictionary learning is then used to decompose Z into a concept dictionary (U) and activation coefficients (V). Each concept in the dictionary is multimodally grounded, meaning it's linked to both visual (images that strongly activate that concept) and textual (words strongly associated with that concept) information. The figure illustrates this entire process with a visual diagram of the model, the decomposition steps, and the resulting multimodal concept grounding.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_25_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure provides a visual overview of the CoX-LMM framework.  Starting with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the framework extracts internal representations of the token from multiple images. These representations are compiled into a matrix Z, which is then linearly decomposed using dictionary learning to obtain a concept dictionary U and activation matrix V. Each concept in the dictionary (uk) is then multimodally grounded, meaning its meaning is connected to both visual and textual domains.  Visual grounding is achieved by identifying the images that most strongly activate each concept, while textual grounding is performed by decoding the concept through the LLM to find the most probable associated words.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_26_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM pipeline for multimodal concept extraction and grounding.  Starting with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the method extracts internal representations of the token across multiple images. These representations are organized into a matrix (Z), which is then linearly decomposed using dictionary learning into a concept dictionary (U) and activation coefficients (V). Each concept in the dictionary is multimodally grounded, meaning it's connected to both visual and textual information. Visual grounding is achieved by identifying the images that most strongly activate each concept. Textual grounding involves decoding each concept through the LMM's language model to find the most probable words associated with it.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_27_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework's pipeline. Starting with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the framework extracts internal representations of that token from multiple images. These representations form a matrix Z, which is then linearly decomposed into a concept dictionary U and activation matrix V. Each concept in U is multimodally grounded; its textual grounding is found by decoding it through the LLM's unembedding matrix to get the most probable words, and its visual grounding is the set of images that maximally activate that concept.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_27_2.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM pipeline for multimodal concept extraction and grounding.  Starting with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the method extracts internal representations of the token from multiple images.  These representations form a matrix Z which is linearly decomposed using dictionary learning into a concept dictionary U and activation matrix V.  Each concept in U is then multimodally grounded: visually, by identifying the images that maximally activate it (Xk,MAS); and textually, by decoding it through the LLM's unembedding matrix to find the most probable words (Tk).", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_28_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM pipeline. It starts with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\").  The model processes internal representations of the token from multiple images, creating a matrix Z.  Dictionary learning decomposes Z into a concept dictionary (U) and activations (V).  Each concept in U is then grounded in both visual and textual domains; visual grounding by identifying images that strongly activate the concept, and textual grounding by decoding the concept back through the LLM's unembedding matrix to obtain the most probable words associated with it. ", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_28_2.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework's pipeline for multimodal concept extraction and grounding. Starting with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the method extracts internal representations of the token across multiple images, forming a matrix Z.  Dictionary learning decomposes Z into a concept dictionary U and activation matrix V. Each concept in U is then grounded in both visual and textual domains. Visual grounding involves identifying the images that maximally activate each concept. Text grounding involves decoding each concept through the LLM's unembedding matrix to obtain the most probable associated words.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_29_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework. Starting with a pretrained large multimodal model (LMM) and a target token (e.g., \"Dog\"), the model's internal representations for that token are extracted from multiple images and compiled into a matrix Z.  Dictionary learning decomposes Z into a concept dictionary U and activation matrix V. Each concept in U is then grounded in both the visual and textual domains. Visual grounding is achieved by identifying the images that most activate each concept, while textual grounding involves decoding the concept through the LLM to find the most probable words associated with it.", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_29_2.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework for multimodal concept extraction and grounding.  Starting with a pre-trained large multimodal model (LMM) and a target token (e.g., \"dog\"), the framework extracts internal representations of the token from multiple images. These representations are compiled into a matrix Z, which is then linearly decomposed into a concept dictionary U and activation matrix V. Each concept in the dictionary (uk) is then grounded in both visual and textual domains. Visual grounding is achieved by identifying the images that maximally activate each concept, while text grounding involves decoding each concept through the LLM's unembedding matrix to find the most probable associated words. ", "section": "3 Approach"}, {"figure_path": "MvjLRFntW6/figures/figures_30_1.jpg", "caption": "Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. 'Dog'), our method extracts internal representations of f about t, across many images. These representations are collated into a matrix Z. We linearly decompose Z to learn a concept dictionary U and its coefficients/activations V. Each concept uk \u2208 U, is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words Tk by decoding uk through the unembedding matrix Wu. Visual grounding Xk,MAS is obtained via vk as the set of most activating samples.", "description": "This figure illustrates the CoX-LMM framework's pipeline for multimodal concept extraction and grounding.  It starts with a pre-trained large multimodal model (LMM) processing images related to a target token (e.g., \"Dog\"). The resulting internal representations are compiled into a matrix Z.  This matrix is then linearly decomposed using dictionary learning to produce a concept dictionary (U) and its activation coefficients (V). Each concept in the dictionary is then \"grounded\" in both the visual and textual domains. Visual grounding is achieved by identifying images that strongly activate each concept. Text grounding involves decoding the concept representation to obtain the most probable words associated with it. This figure demonstrates the process of extracting and interpreting multimodal concepts within the LMM.", "section": "3 Approach"}]