[{"figure_path": "fMdrBucZnj/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of our proposed EPH model. During training, EPH first samples discrete hierarchies using our differentiable hierarchy sampling (Sec. 4.3) and a subgraph using our subgraph sampling procedure (Sec. 4.4). The expected scores are then computed and averaged. Finally, the probabilistic hierarchy is updated via backpropagation. A formal description is given in App. \u0421.7.", "description": "This figure provides a visual overview of the Expected Probabilistic Hierarchies (EPH) model.  The process begins with a probabilistic hierarchy, which is used to sample multiple discrete hierarchies. Concurrently, a subgraph is sampled from the input graph data.  The sampled discrete hierarchies and subgraph are used to compute expected scores (e.g., Exp-Das, Exp-TSD). These scores are then averaged to compute the loss, which is used to update the parameters of the probabilistic hierarchy via backpropagation. This iterative process refines the probabilistic hierarchy until convergence.", "section": "4 Expected Probabilistic Hierarchical Clustering"}, {"figure_path": "fMdrBucZnj/figures/figures_4_1.jpg", "caption": "Figure 2: An example where FPH fails to infer a minimizing hierarchy. A hierarchy minimizing the Dasgupta cost and the inferred hierarchies by FPH and EPH on the unweighted K4 graph, i.e., every normalized edge weight is equal to 1. While FPH achieves a Dasgupta cost of 4.0 after discretization, the continuous hierarchy has a Soft-Das score below 3.0. On the other hand, EPH finds a minimizing hierarchy with a cost of 3.0. A weighted example is shown in Fig. 8.", "description": "This figure demonstrates a scenario where the Flexible Probabilistic Hierarchy (FPH) method fails to find the optimal hierarchy that minimizes the Dasgupta cost, unlike the Expected Probabilistic Hierarchies (EPH) method. It uses an unweighted K4 graph (complete graph with 4 nodes) as an example.  FPH, using a continuous relaxation, achieves a Dasgupta cost of 4.0 after discretization, while the optimal discrete hierarchy has a cost of 3.0. However, EPH successfully identifies this optimal discrete hierarchy with a cost of 3.0, showcasing its advantage over FPH in finding the optimal discrete hierarchy.", "section": "4.2 Theoretical Analysis of EPH and FPH"}, {"figure_path": "fMdrBucZnj/figures/figures_7_1.jpg", "caption": "Figure 3: Hyperparameter study. Normalized Dasgupta costs for different numbers of sampled hierarchies (left) and different number of sampled edges (right) after the EPH training, including the average linkage algorithm (AL) and a training on the full graph (FG). The scores are normalized such that each dataset has a mean of zero and a standard deviation of one.", "description": "This figure shows the impact of two hyperparameters on the performance of the Expected Probabilistic Hierarchies (EPH) model for hierarchical clustering.  The left panel illustrates how the normalized Dasgupta cost changes with varying numbers of sampled hierarchies used during training.  Different colors represent different datasets. The right panel examines the effect of the number of sampled edges on the normalized Dasgupta cost, comparing EPH against the average linkage algorithm (AL) and a full graph training (FG).  The normalization ensures that each dataset's mean is zero and standard deviation is one, allowing for comparison across different datasets.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_8_1.jpg", "caption": "Figure 4: Ground truth clusters and dendrograms compared to the inferred ones for the HSBMs.", "description": "This figure visualizes the ground truth and the hierarchical clustering results obtained by EPH for both small and large Hierarchical Stochastic Block Models (HSBMs).  The top row shows the results for a small HSBM, while the bottom row shows the results for a larger HSBM.  Within each row, the leftmost panel displays the ground truth community structure (GT), the middle panel shows the dendrogram generated by EPH using the expected Dasgupta cost (Exp-Das), and the rightmost panel shows the dendrogram produced by EPH using the expected Tree-Sampling Divergence (Exp-TSD).  The color-coding in the dendrograms represents the clusters identified by the algorithm.  The figure provides a visual comparison of how well EPH's clustering results align with the ground truth community structure for different network sizes and using different evaluation metrics.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_9_1.jpg", "caption": "Figure 5: Largest derived cluster on Cifar-100.", "description": "The figure shows the largest cluster inferred on the Cifar-100 dataset using EPH. The left subplot shows the 16 images with the highest probability, and the right subplot shows the 16 images with the lowest probability. The images with high probabilities are all similar and related to insects, demonstrating EPH's ability to group similar images together. The images with low probabilities do not fit into the group, illustrating EPH's capacity to measure uncertainty in cluster assignments.", "section": "Qualitative Evaluation"}, {"figure_path": "fMdrBucZnj/figures/figures_12_1.jpg", "caption": "Figure 6: The different cases of the event p (Zk = V1 ^ V2 | Zk \u2208 anc(v)). While the LCA of v\u2081 and V2 is zk in every case, the LCA of v1 and v and the LCA of v2 and v are different. We have three cases: either the paths from v1 or v2 and v meet before zk at node zk' (shown in (a) and (b)), or all paths meet for the first time at zk (shown in fig. (c)).", "description": "This figure illustrates the three possible scenarios when calculating the joint probability of the lowest common ancestor (LCA) and ancestor probabilities.  It shows how the paths from three leaves (v1, v2, and v) to an internal node (zk) can intersect at different points (zk', zk, or zk) depending on the tree structure. This is important for calculating the expected Dasgupta cost.", "section": "A.3 Relation between Joint and Independent LCA and Ancestor Probabilities"}, {"figure_path": "fMdrBucZnj/figures/figures_15_1.jpg", "caption": "Figure 7: Three hierarchies and two graphs that show that Exp-Das is neither convex nor concave with respect to A and B. The hierarchy in (c) is a linear interpolation of the hierarchies in (a) and (b). The graphs in (d) and (e) are counter-examples, with convex and concave behavior, respectively.", "description": "This figure demonstrates that the expected Dasgupta cost (Exp-Das), a function used in the Expected Probabilistic Hierarchies (EPH) method, is neither convex nor concave.  It presents three different hierarchies (a, b, c) and two graphs (d, e).  Hierarchy (c) is a linear interpolation between (a) and (b). Graphs (d) and (e) illustrate scenarios where the function behaves convexly and concavely, respectively. This non-convexity and non-concavity property complicates optimization but is explained and addressed within the EPH method.", "section": "4.6 Non-Convexity and Non-Concavity of Exp-Das"}, {"figure_path": "fMdrBucZnj/figures/figures_15_2.jpg", "caption": "Figure 2: An example where FPH fails to infer a minimizing hierarchy. A hierarchy minimizing the Dasgupta cost and the inferred hierarchies by FPH and EPH on the unweighted K4 graph, i.e., every normalized edge weight is equal to. While FPH achieves a Dasgupta cost of 4.0 after discretization, the continuous hierarchy has a Soft-Das score below 3.0. On the other hand, EPH finds a minimizing hierarchy with a cost of. A weighted example is shown in Fig. 8.", "description": "This figure shows an example where the Flexible Probabilistic Hierarchy (FPH) method fails to find the optimal hierarchy that minimizes the Dasgupta cost.  FPH's continuous relaxation results in a Soft-Das score lower than the optimal discrete Dasgupta cost. In contrast, the Expected Probabilistic Hierarchies (EPH) method successfully finds the minimizing hierarchy. The figure uses an unweighted K4 graph for simplicity, but a similar issue is demonstrated with a weighted graph in Figure 8.", "section": "4.2 Theoretical Analysis of EPH and FPH"}, {"figure_path": "fMdrBucZnj/figures/figures_18_1.jpg", "caption": "Figure 5: Largest derived cluster on Cifar-100.", "description": "The figure visualizes the largest cluster inferred on the Cifar-100 dataset using the EPH method.  It shows the 16 images with the highest probability (left) and the 16 images with the lowest probability (right) for the largest cluster.  The images with high probabilities are visually similar (insects), demonstrating EPH's ability to group similar images. The images with low probabilities do not visually fit in this group, illustrating EPH's capability to measure uncertainty in cluster assignments.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_18_2.jpg", "caption": "Figure 5: Largest derived cluster on Cifar-100.", "description": "The figure shows the largest cluster obtained by applying the EPH model on the Cifar-100 dataset.  It displays two sets of 16 images each. (a) shows the 16 images with the highest probability of belonging to the cluster, and (b) shows the 16 images with the lowest probability. The images in (a) are visually similar and consistent with the theme of the cluster, whereas the images in (b) are more diverse and less representative of the cluster's characteristics. This visualization demonstrates the capacity of EPH to both identify coherent clusters and quantify the uncertainty associated with cluster assignments.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_19_1.jpg", "caption": "Figure 5: Largest derived cluster on Cifar-100.", "description": "This figure visualizes the largest cluster identified by the EPH model on the Cifar-100 dataset.  It showcases two sub-figures. (a) Highest Probability displays the 16 images with the highest probability within that cluster, showing a clear visual similarity related to insects. (b) Lowest Probability shows the 16 images with the lowest probability in that same cluster, demonstrating that they visually differ and do not strongly fit the identified theme. The images illustrate the model's ability to discern clear visual patterns and quantify uncertainty within clusters by examining the probability assignments to each image.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_19_2.jpg", "caption": "Figure 12: Inferred clusters inferred by EPH optimizing Exp-Das and Exp-TSD. 64 clusters are highlighted in the graphs and dendrograms.", "description": "This figure visualizes the results of applying the Expected Probabilistic Hierarchies (EPH) method to the OpenFlight dataset, a graph representing flight connections between various locations worldwide.  The left side shows the geographical distribution of the 64 clusters identified by EPH using two different objective functions: Exp-Das (expected Dasgupta cost) and Exp-TSD (expected Tree-Sampling Divergence). The right side displays the corresponding dendrograms for both objective functions, visually representing the hierarchical structure of the clusters. This allows for a direct comparison of the clustering results based on these two different metrics, showcasing how the hierarchical structure changes when optimizing for different objective functions.  Each color represents a cluster, and the dendrogram's branch lengths reflect the distance between clusters.", "section": "External Evaluation"}, {"figure_path": "fMdrBucZnj/figures/figures_20_1.jpg", "caption": "Figure 13: Ground truth clusters (left) compared to inferred flattened clusters (middle) and dendrograms (right) of EPH for the datasets Zoo, Iris, Digits, Segmentation, and Spambase with n' = min{n - 1,512} internal nodes after applying the t-SNE algorithm.", "description": "This figure visualizes the results of the EPH algorithm on five vector datasets (Zoo, Iris, Digits, Segmentation, and Spambase).  It uses t-SNE to reduce the dimensionality of the data for better visualization. For each dataset, it shows three plots side-by-side:\n\n1. **Ground Truth Clusters:** Shows the actual cluster assignments for each data point, providing a baseline for comparison.\n2. **Inferred Flattened Clusters:** Shows the cluster assignments generated by the EPH algorithm after flattening the hierarchical structure.\n3. **Dendrograms:** Provides a visual representation of the hierarchical clustering produced by EPH. The dendrogram illustrates the relationships between clusters and how they merge at different levels of granularity. \n\nThe purpose is to allow a visual comparison of the ground truth and the EPH's performance in clustering these datasets.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_21_1.jpg", "caption": "Figure 14: Linear interpolation of Soft-Das and Exp-Das scores from the average linkage hierarchy to the hierarchy inferred by Exp-Das.", "description": "This figure displays the results of a linear interpolation experiment conducted on seven different graph datasets (Brain, Citeseer, Cora-ML, Genes, OpenFlight, Polblogs, and WikiPhysics). The experiment interpolates between the average linkage hierarchy and the hierarchy inferred by the Exp-Das algorithm, evaluating the Soft-Das and Exp-Das scores at different interpolation points (denoted by 'Factor a' on the x-axis). The y-axis shows the normalized Soft-Das and Exp-Das scores, respectively. This visualization provides insight into the relationship between the average linkage approach and the Exp-Das optimized hierarchy, highlighting potential differences in scoring metrics and the impact of the optimization procedure.", "section": "4.2 Theoretical Analysis of EPH and FPH"}, {"figure_path": "fMdrBucZnj/figures/figures_22_1.jpg", "caption": "Figure 3: Hyperparameter study. Normalized Dasgupta costs for different numbers of sampled hierarchies (left) and different number of sampled edges (right) after the EPH training, including the average linkage algorithm (AL) and a training on the full graph (FG). The scores are normalized such that each dataset has a mean of zero and a standard deviation of one.", "description": "This figure shows the results of a hyperparameter study for the EPH model, investigating the impact of the number of sampled hierarchies and the number of sampled edges on the normalized Dasgupta cost. The left panel displays the effect of varying the number of sampled hierarchies, comparing EPH's performance with the average linkage algorithm (AL) and training on the full graph.  The right panel shows a similar comparison using different numbers of sampled edges.  In both panels, the results are normalized across different datasets to allow for a clear comparison.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_22_2.jpg", "caption": "Figure 3: Hyperparameter study. Normalized Dasgupta costs for different numbers of sampled hierarchies (left) and different number of sampled edges (right) after the EPH training, including the average linkage algorithm (AL) and a training on the full graph (FG). The scores are normalized such that each dataset has a mean of zero and a standard deviation of one.", "description": "This figure shows the results of a hyperparameter study conducted to determine the optimal number of sampled hierarchies and edges for the EPH model. The left panel shows how the normalized Dasgupta cost varies with the number of sampled hierarchies for different datasets (Brain, OpenFlight, Genes, Citeseer, Cora-ML, Polblogs, WikiPhysics). The right panel shows how the normalized Dasgupta cost varies with the number of sampled edges for different datasets (Zoo, Iris, Glass, Digits, Segmentation, Spambase, Letter, Cifar-100).  The average linkage algorithm (AL) and a training on the full graph (FG) are also included as baselines for comparison. The scores are normalized to have a mean of zero and a standard deviation of one for each dataset.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of our proposed EPH model. During training, EPH first samples discrete hierarchies using our differentiable hierarchy sampling (Sec. 4.3) and a subgraph using our subgraph sampling procedure (Sec. 4.4). The expected scores are then computed and averaged. Finally, the probabilistic hierarchy is updated via backpropagation. A formal description is given in App. C.7.", "description": "This figure provides a visual overview of the Expected Probabilistic Hierarchies (EPH) model. It shows the process of sampling discrete hierarchies and subgraphs, computing the expected scores, and updating the probabilistic hierarchy through backpropagation. The figure highlights the key steps involved in EPH and shows how differentiable hierarchy and subgraph sampling are used to optimize expected scores.", "section": "4 Expected Probabilistic Hierarchical Clustering"}, {"figure_path": "fMdrBucZnj/figures/figures_23_2.jpg", "caption": "Figure 1: Overview of our proposed EPH model. During training, EPH first samples discrete hierarchies using our differentiable hierarchy sampling (Sec. 4.3) and a subgraph using our subgraph sampling procedure (Sec. 4.4). The expected scores are then computed and averaged. Finally, the probabilistic hierarchy is updated via backpropagation. A formal description is given in App. C.7.", "description": "This figure illustrates the workflow of the Expected Probabilistic Hierarchies (EPH) model.  EPH begins by sampling discrete hierarchies and subgraphs from the input data using differentiable sampling techniques.  These samples are then used to compute and average the expected scores. Finally, backpropagation is used to update the probabilistic hierarchy, improving its ability to represent the underlying data structure.", "section": "4 Expected Probabilistic Hierarchical Clustering"}]