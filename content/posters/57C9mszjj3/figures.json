[{"figure_path": "57C9mszjj3/figures/figures_7_1.jpg", "caption": "Figure 1. Illustration of our Idempotent Operator Techniques. This allows us to focus on analyzing the evolving coefficients, which are key to the expected 0-1 loss convergence.", "description": "This figure illustrates the Idempotent Operator Techniques used in the paper's analysis.  It breaks down the complex interactions within the transformer model\u2014specifically, the attention and MLP layers\u2014into simpler, more manageable components.  By employing idempotent decomposition, the analysis focuses on the evolution of key coefficients, thereby simplifying the study of the learning dynamics and ultimately, the convergence to the Bayes optimal test error.", "section": "5.1 Idempotent Operator Techniques"}, {"figure_path": "57C9mszjj3/figures/figures_8_1.jpg", "caption": "Figure 2. Learning dynamics: (i) training and test loss; (ii) correct attention weight; (iii) maximum values of \u03b1Q,s\u03b1K,s, \u03b2Q,s\u03b2K,s, maximum values of the complement products TQ,rTK,r or PQ,rPK,r, and maximum values of product-with-noise (W(t))TW(t); (iv) maximum values of \u03b1o(i,),k and |\u03b2o(i,),k|, maximum values of the complement coefficients PO(i,),w and maximum values of product-with-noise W(t)TW(t).", "description": "This figure presents the learning dynamics of the transformer model. The four subfigures show: (i) the training and test loss curves; (ii) the evolution of attention weights over time (correct attention weight, train and test); (iii) maximum values of different types of products (concept-specific, complement, noise), providing insights into the model's learning of attention and concept representation; and (iv) evolution of MLP weights (correct attention weight, train and test). These subfigures jointly illustrate various aspects of the transformer's learning progress, including the convergence of the model, the dynamics of attention weights, and the interplay of concept-specific semantics and noise during training.", "section": "Experiments"}, {"figure_path": "57C9mszjj3/figures/figures_9_1.jpg", "caption": "Figure 3. Learning dynamic in three OOD scenarios. The training settings and plotting methods are identical to those used in Figure 2, and the testing settings are: (a-b) utilizes different prompt lengths; (c) adopts a skewed distribution over z; (d) switches the concept-specific semantic features.", "description": "This figure shows the learning dynamics of the transformer model under three different out-of-distribution (OOD) scenarios.  The first two scenarios (a and b) investigate the impact of different prompt lengths during testing, while the latter two (c and d) explore the effects of altering the underlying data distribution (skewed concept distribution and switched semantic features). Each subfigure displays training and testing loss, evolution of attention weights, and learning progress of both attention and MLP layers over epochs, providing a comprehensive view of the model's learning behavior in various OOD situations.", "section": "6 Experiments"}]