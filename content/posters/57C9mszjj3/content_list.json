[{"type": "text", "text": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dake ${\\bf{B}}{\\bf{u}}^{1}$ , Wei Huang2,\u2217 Andi Han2, Atsushi Nitanda3,4, Taiji Suzuki5,2, Qingfu Zhang1, Hau-San Wong1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computer Science, City University of Hong Kong, Hong Kong SAR 2Center for Advanced Intelligence Project, RIKEN, Japan 3CFAR and IHPC, Agency for Science, Technology and Research $(A{\\star}S T A R)$ ), Singapore   \n4College of Computing and Data Science, Nanyang Technological University, Singapore 5Department of Mathematical Informatics, the University of Tokyo, Japan   \ndakebu2-c@my.cityu.edu.hk, {wei.huang.vr, andi.han}@riken.jp@riken.jp, atsushi_nitanda@cfar.a-star.edu.sg, taiji@mist.i.u-tokyo.ac.jp, {qingfu.zhang, cshswong}@cityu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer-based large language models (LLMs) have displayed remarkable creative prowess and emergence capabilities. Existing empirical studies have revealed a strong connection between these LLMs\u2019 impressive emergence abilities and their in-context learning (ICL) capacity, allowing them to solve new tasks using only task-specific prompts without further fine-tuning. On the other hand, existing empirical and theoretical studies also show that there is a regularity of the multi-concept encoded semantic representation behind transformer-based LLMs. However, existing theoretical work fail to build up an understanding of the connection between this semantic regularity and the innovative power of ICL. Additionally, prior work often focuses on simplified, unrealistic scenarios with linear transformers or unrealistic loss functions, and due to their technical limitations, they come up with results exhibiting only linear or sub-linear convergence rates. In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for new unseen tasks encoded with multiple cross-concept semantics. Inspired by empirical studies on the latent geometry of LLMs, the analysis is based on a concept-based low-noise sparse coding prompt model. Leveraging advanced techniques, this work showcases the exponential 0-1 loss convergence over the highly non-convex training dynamics, which pioneeringly incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and cross-entropy loss. Empirical simulations corroborate the theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, a variety of transformer-based large language models (LLMs) have demonstrated remarkable performance across a broad spectrum of machine learning tasks, including natural language understanding [1], symbolic reasoning [2], and even heuristics design [3, 4]. One crucial emerging ability of these models is their in-context learning (ICL) capacity [5], which allows them to learn from a few demonstrations and conduct predictions on new queries without requiring any further fine-tuning. However, the current theoretical understanding of the mechanisms underlying this ICL capability remains limited, leaving the reasons for the remarkable emergence and generalization power of transformer-based LLMs in unseen ICL tasks largely unexplained. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In line with traditional topic models [6], [7, 8] propose that latent concepts / topics underlie natural texts, providing a Bayesian inference framework to elucidate the ICL mechanism via Bayesian Model Averaging (BMA) approach. On the other hand, theoretical and empirical studies have shown that transformer-based models exhibit linear geometric regularities in their latent representations as a result of concept or topic learning [9, 10], where the representations within-concept have positive inner products while representations cross-concepts exhibit near-orthogonal relationships. This structured semantic geometry has been well-documented in recent research on pre-trained LLMs [11, 12, 10, 13]. However, the connection between this observed multi-concepts latent geometric structure and the LMs\u2019 remarkable ICL capabilities remains unclear. Separately, recent theoretical analyses have modeled ICL as a martingale process driven by latent \u201cconcept\u201d variables [14, 15]. Yet, these studies have not incorporated the observed multi-concept semantic regularity into their analyses, nor have they discussed the strong out-of-distribution (OOD) ICL abilities exhibited by transformers. ", "page_idx": 1}, {"type": "text", "text": "Additionally, existing theoretical work has been conducted on unrealistic, oversimplified settings, such as linear or ReLU transformers [16, 17, 18, 19], MLP-free attention-only models [16, 20], QK-combined softmax attention [21, 20, 19, 22, 23], unrealistic infinite dimensional assumption [14, 24, 21, 19] and impractical loss functions like square loss [16, 9, 25, 20, 26] and hinge loss [27, 28]. Furthermore, existing works have only been able to derive linear or sub-linear convergence rates for the 0-1 loss. ", "page_idx": 1}, {"type": "text", "text": "Therefore, there is a need for a more advanced analysis that can bridge the understanding between the multi-concept semantic regularity and the mechanisms underlying transformer-based ICL. This naturally leads to the research question: ", "page_idx": 1}, {"type": "text", "text": "Essential Questions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Whether and how do the geometric regularity of the multi-concept-encoded representation facilitate transformer in conducting efficient ICL? ", "page_idx": 1}, {"type": "text", "text": "To answer the above question, following the meaningful data modeling ideas in [9, 29], we conduct theoretical analysis on a concept-specific sparse coding prompt distribution for classification tasks, where the sparse latent variable encodes the information denoting the word\u2019s belonging concept. Importantly, the features in both the word\u2019s and label\u2019s dictionaries exhibit concept-specific geometric properties - within-concept positive inner products and cross-concept orthogonal geometric properties - that aligns with the findings in [9, 10, 11]. Our main contributions are highlighted as below. ", "page_idx": 1}, {"type": "text", "text": "1. First, we provide a comprehensive analysis of the learning dynamics for a two-layer transformer model, comprising one attention layer followed by a ReLU-activated feed-forward network, which is trained using the cross-entropy loss via stochastic gradient descent over a concept-specific sparse coding prompt distribution. Leveraging advanced analytical techniques, we showcase the asymptotic properties governing the coupled learning dynamics of the attention and MLP layers. ", "page_idx": 1}, {"type": "text", "text": "2. To the best of our knowledge, we are the first to prove an exponential convergence of the 0-1 loss over this challenging setting. Despite the highly non-convex optimization landscape, we demonstrate that the transformer can achieve Bayes optimal test error with just a logarithmic number of iterations. ", "page_idx": 1}, {"type": "text", "text": "3. We provably show how the multi-concept encoded semantic geometry can enable transformer to efficiently perform certain out-of-distribution ICL tasks. This offers an intuitive explanation for why transformer-based LLMs are able to successfully leverage the polysemous nature of words to tackle diverse, unseen concept-specific tasks, aligning well with users\u2019 practical experiences. Furthermore, our analysis takes a step forward in providing a potential theoretical underpinning for the innovative capabilities of LLMs, encompassing their ability to achieve cross-concept knowledge intersection. We believe our findings provide an initial positive response to Question 5.1.4 in the ICML 2024 position paper [30], which asks whether the observed latent geometry of LLMs can explain their OOD extrapolation abilities. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Theory of Exponential Convergence Rate of Stochastic Gradient Descent. Our analysis of the exponential convergence rate for the 0-1 loss builds upon prior work linking the excess risk and essential supremum norm to exponentially fast convergence under the \u201chard low-noise condition\u201d [31, 32]. This phenomenon has been further explored in more recent studies analyzing the exponential convergence of stochastic gradient descent (SGD) [33, 34, 35, 36, 37], as well as in more generalized settings such as multiclass classification [38] and support vector machines [39]. ", "page_idx": 2}, {"type": "text", "text": "Feature Learning in Learning Theory. Recent works in learning theory have extensively studied structured data from a feature learning perspective, examining NN\u2019s feature direction reconstruction and noise memorization as a proxy for training or 0-1 loss convergence [40, 41, 42]. While prior studies often assumed orthogonal features, recent efforts have analyzed non-orthogonal scenarios [43, 44]. Our work extends this line-of-research to challenging nonlinear Attention-MLP transformers with non-orthogonal structured data representations. ", "page_idx": 2}, {"type": "text", "text": "Theory of Transformers and In-Context Learning The literature on Transformers and ICL is wide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learn topic/concept semantics [9], the origins and biases of LLM representations using latent variable models [10], and ICL from a model averaging perspective [14]. However, albeit incorporating concept variables, these works do not connect the geometric properties of conceptencoded representations to transformers\u2019 powerful ICL abilities. Another line of research has studied the learning dynamics of ICL, including analyses of linear transformers [17, 19], QK-combined attention-only models [45], and multi-head softmax attention over linear regression without MLP [25]. Though relevant, these works rely on simplifications and do not notice the connection between semantic regularity and powerful ICL. While [28] also analyzes the learning dynamics of transformers with softmax attention and ReLU MLPs for in-context classification tasks, making it the most relevant prior work, our analysis differs in several key aspects. Specifically, (i) they consider orthogonal dictionary learning with a single label vector, in contrast to our non-orthogonal concept-encoded dictionaries for both words and labels; (ii) their technique requires a large batch size (at least $\\varepsilon^{-2}$ , where $\\varepsilon$ is the test error) and long context lengths, which are not required in our result; and (iii) they utilize an impractical hinge loss and only achieve linear convergence without a relation to $\\varepsilon$ , whereas we analyze the more practical cross-entropy loss and derive an exponential convergence rate in terms of the test error $\\varepsilon$ . However, we note that this is only an informal comparison due to the differences in the models and primary findings. A detailed Related Work Section is deferred to Appendix C. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. For $l_{2}$ and Frobenius norms we utilize $\\Vert\\cdot\\Vert$ and $\\|\\cdot\\|_{F}$ to denote their computations. Considering two series $a_{n}$ and $b_{n}$ , we denote $a_{n}=O\\left(b_{n}\\right)$ if there exists positive constant $C>0$ and $N>0$ such that for all $n\\geq N$ , $\\left\\vert a_{n}\\right\\vert\\leq C\\left\\vert b_{n}\\right\\vert$ . Similarly, we denote $a_{n}=\\Omega\\left(b_{n}\\right)$ if $b_{n}=O\\left(a_{n}\\right)$ holds, and $a_{n}\\,=\\,\\Theta\\left(b_{n}\\right)$ if $a_{n}\\,=\\,O\\left(b_{n}\\right)$ and $a_{n}\\,=\\,\\Omega\\,(b_{n})$ both hold. Our $\\mathbb{1}(\\cdot)$ is to denote the indicator variable of an event. In addition, we denote spa $\\mathbf{n}(v_{1},v_{2},\\ldots,v_{k})$ as the linear subspace spanned by the vectors $v_{1},v_{2},\\ldots,v_{k}$ , and $\\operatorname{conic}(v_{1},v_{2},\\ldots,\\dot{v_{k}})$ denotes the conic hull (the set of all non-negative linear combinations) of the vectors $v_{1},v_{2},\\ldots,v_{k}$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Data Distribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The data distribution employed in this study draws inspiration from a range of empirical and theoretical research works [46, 47, 9, 10, 48]. This distribution captures context-awareness and can be viewed as a specialized prompt version of PLSA [49] and LDA [6]. In this distribution, each word and label has multiple feature embeddings, each embedding corresponding to a different concept. This is achieved through the use of a sparse latent concept/topic variable, which happened to be particularly adept at representing language polysemy [47]. Adhering to the LLM representation explored in [9, 10], the features in both the word and label dictionaries maintain orthogonality across concepts and positive inner products within concepts. Additionally, the distribution incorporates Gaussian noise accounting for linguistic ambiguity or the imperfection of the LLM\u2019s representation. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. Polysemous Word Model $(D_{x},D_{y},D_{z},D_{\\xi_{x}},D_{\\xi_{y}})$ . We assume there exists $K_{1}$ taskrelevant concepts, each characterized by two semantically-opposite word\u2019s feature vectors $\\mu_{k_{1}}^{+}$ and $\\pmb{\\mu}_{k_{1}}^{-}$ , and their corresponding label\u2019s feature vectors $q_{k_{1}}^{+}$ and $q_{k_{1}}^{-}$ , $\\forall k_{1}\\:\\in\\:[K_{1}]$ . There are also $K_{2}$ task-irrelevant concepts denoted by $\\nu_{k_{2}}$ , $\\forall k_{2}\\in[K_{2}]$ . The word samples $\\textbf{\\em x}\\in\\mathbb{R}^{d_{\\mathcal{X}}}$ and their labels $\\pmb{y}\\in\\mathbb{R}^{d_{\\mathcal{V}}}$ are generated from distributions parameterized by a shared latent concept variable $z=(\\bar{z_{1}},\\cdot\\cdot\\cdot\\cdot,z_{K})\\in\\{0,1\\}^{K}(K<d_{X})$ capturing the concept-specific information: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z\\sim\\mathcal{D}_{z},\\quad\\xi_{x}\\sim\\mathcal{D}_{\\xi_{x}}=\\mathcal{N}(\\mathbf{0},\\sigma_{\\xi}^{2}\\mathbf{I}_{d_{x}}),\\quad\\xi_{y}\\sim\\mathcal{D}_{\\xi_{y}}=\\mathcal{N}(\\mathbf{0},\\sigma_{\\xi}^{2}\\mathbf{I}_{d_{y}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}=\\mathbf{M}\\pmb{z}+\\xi_{x}\\sim\\mathcal{D}_{x},\\quad\\pmb{y}=\\mathbf{Q}\\pmb{z}+\\xi_{\\pmb{y}}\\sim\\mathcal{D}_{\\pmb{y}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the feature dictionary $\\mathbf{M}=[\\mu_{1}^{+},\\mu_{1}^{-},\\mu_{2}^{+},\\mu_{2}^{-},\\dots,\\mu_{K_{1}}^{+},\\mu_{K_{1}}^{-},\\nu_{1},\\nu_{2},\\cdots,\\nu_{K_{2}}]\\in\\mathbb{R}^{d_{X}\\times K}$ exhibits positive inner products within concepts and orthogonality across concepts, and the label dictionary $\\mathbf{\\bar{Q}}=[\\pmb{q}_{1}^{+},\\pmb{q}_{1}^{-},\\pmb{q}_{2}^{+},\\pmb{q}_{2}^{-},\\pmb{\\cdot}\\cdot\\cdot\\mathbf{\\nabla},\\pmb{q}_{K_{1}}^{+},\\pmb{q}_{K_{1}}^{-},0,\\cdot\\cdot\\cdot0]\\in\\mathbb{R}^{d_{\\mathcal{Y}}\\times K}$ has similar geometric properties. Specifically, we hav $\\gamma\\forall k_{1}\\in[K_{1}],k_{2}\\in[K_{2}],\\|\\pmb{\\mu}_{k_{1}}^{\\pm}\\|=\\|\\pmb{\\nu}_{k_{2}}\\|=\\|\\mathbf{u}\\|,\\|\\pmb{q}_{k_{1}}^{\\pm}\\|=\\|\\mathbf{q}$ , and there exist constants $0<\\kappa_{\\pmb{x}},\\kappa_{\\pmb{y}}<1$ such that $0<\\langle\\pmb{\\mu}_{k_{1}}^{+},\\pmb{\\mu}_{k_{1}}^{-}\\rangle\\leq\\kappa_{\\pmb{x}}\\|\\mathbf{u}\\|^{2}$ and $0<\\langle\\mathbf{\\dot{q}}_{k_{1}}^{+},\\mathbf{q}_{k_{1}}^{-}\\rangle\\leq\\kappa_{y}\\|\\mathbf{q}\\|^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "The detailed formal definition can be found in Appendix E. By this definition, a single word or label can possess different features corresponds to different concepts. The illustration of Figure 1 in [12] can be an example, where the \u201cDog\u201d vector in the representation space of LLM is decomposed to a direct sum of orthogonal vectors: \u201c[Animal] $+\\ [\\mathrm{Mammal}]+\\cdot\\cdot\\cdot^{\\ast}.$ , and we can see \u201c[Animal]\u201d belongs to the concept \u201cOrganism\u2019s Category\u201d categorized into labels \u201c[Animal]\u201d and \u201c[Plant]\u201d, and \u201c[Mammal]\u201d belongs to the concept of \u201cAnimal\u2019s Category\u201d characterized by labels \u201c[Mammal]\u201d, \u201c[Fish]\u201d, \u201c[Bird]\u201d, \u201c[Reptile]\u201d. Besides, Figure 1 in [46] can also be a good support for our modeling, where \u201cFerrari\u201d vector consists of $\\cdot[\\mathrm{Cars}]+[\\mathrm{Italian}]+\\cdot\\cdot\\cdot^{,}\\,$ . ", "page_idx": 3}, {"type": "text", "text": "The following definition models the contextual prompts via specifying the statistical property of $_{z}$ among in-context words, which is a special prompt version of PLSA [49] and LDA [6]. The detailed formal version is available in Appendix E. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. Concept-specific Contextual Prompt Distribution2. During training, each prompt sample $S={\\pmb x}_{1},{\\pmb y}_{1},\\dots,{\\pmb x}_{L},{\\pmb y}_{L},{\\pmb x}_{L+1}$ would share at least one co-concept, which is drawn from $a$ mixture distribution $\\mathcal{D}_{S}$ defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{D}_{S}=\\sum_{k=1}^{K_{1}}\\left(\\pi_{k}^{+}\\mathcal{P}_{k,L+1}^{+}+\\pi_{k}^{-}\\mathcal{P}_{k,L+1}^{-}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{P}_{k,L+1}^{\\pm}$ denotes the $k$ -th concept-specific prompt distribution, and $\\pi_{k}^{\\pm}=(2K_{1})^{-1}$ denotes the equal chance of a sample to belong to $\\mathcal{P}_{k,L+1}^{\\pm}$ . Specifically, a sample $S_{n}\\sim\\mathcal{P}_{k,L+1}^{e},e\\in[\\pm]$ means that the query\u2019s label ${\\pmb y}_{L+1}^{n}$ is $\\pmb q_{k}^{e}$ , and we denote $y_{S_{n}}:=e$ as the real value label of this prompt. In addition, every demonstration pairs $(\\mathbf{x}_{l}^{n},\\mathbf{y}_{l}^{n}),l\\;\\in\\;[L]$ in $\\mathcal{P}_{k,L+1}^{e}$ contain either $(\\mu_{k}^{+},q_{k}^{+})$ or $(\\mu_{k}^{-},q_{k}^{-})$ with equal chance. Also, every $z_{l}^{n},l\\in[L+1]$ would satisfy $\\mathbb{P}(z_{l,\\lnot(2k-1\\lor2k)}^{n}=1)=K^{-1}$ , denoting the equal chance to have diverse features other than the current co-concept of the Pke,L+1. ", "page_idx": 3}, {"type": "text", "text": "This definition suggests that for prompt $S$ sampling from $\\mathcal{D}_{S}$ , there exists $e\\,\\in\\,[\\pm],\\,k\\,\\in\\,[K_{1}]$ , such that all the word-label pairs in this prompt share the $k$ -th concept as their co-concept, and the corresponding real value label of the query in this prompt is $e$ . Besides, the real value label of each word-label pair in the demonstration would have equal chance to be $+1$ or $-1$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Transformer Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following [17, 20, 28], our embedding $\\mathbf{E}(\\cdot)$ of prompt $S$ is formulated as $\\mathbf{H}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathbf{E}(S)={\\left(\\begin{array}{l l l l l}{x_{1}}&{x_{2}}&{\\cdots}&{x_{L}}&{x_{\\mathrm{query}}}\\\\ {y_{1}}&{y_{2}}&{\\cdots}&{y_{L}}&{\\mathbf{0}}\\end{array}\\right)}:=\\left(\\mathbf{h}_{1},\\mathbf{h}_{2},\\cdots,\\mathbf{h}_{\\mathrm{query}}\\right)\\in\\mathbb{R}^{(d_{x}+d_{y})\\times(L+1)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The learning model is a single-head, one-layer Transformer with one self-attention layer and one two-layer perceptron. Mathematically, it can be expressed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f({\\bf H};\\Psi)=\\mathbf{r}^{\\top}{\\boldsymbol\\sigma}_{R}\\left({\\bf W}_{O}\\,\\mathrm{attn}({\\bf H};\\Psi)\\right),}\\\\ &{\\mathrm{attn}({\\bf H};\\Psi)=\\displaystyle\\sum_{l=1}^{L}{\\bf W}_{V}{\\bf h}_{l}{\\boldsymbol\\sigma}_{S}\\left(\\left({\\bf W}_{K}{\\bf h}_{l}\\right)^{\\top}{\\bf W}_{Q}{\\bf h}_{\\mathrm{query}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma_{R}(\\cdot):=\\mathrm{Relu}(\\cdot),\\sigma_{S}(\\cdot):=\\mathrm{softmax}(\\cdot),\\mathbf{W}_{Q},\\mathbf{W}_{K}\\in\\mathbb{R}^{m_{q k}\\times(d x+d y)},\\mathbf{W}_{V}\\in\\mathbb{R}^{m_{v}\\times(d x+d y)}$ are the embedding matrices for queries, keys, and values, respectively, and $\\mathbf{W}_{O}\\,\\,\\in\\,\\,\\mathbb{R}^{m\\times m_{v}}$ and $\\textbf{r}\\in\\mathbb{R}^{m}$ are parameters in the MLP layer. Typically, $\\operatorname*{min}\\left(m_{q k},m_{v}\\right)\\;\\geq d_{\\mathcal{X}}\\,+\\,d_{\\mathcal{Y}}$ . $\\Psi\\ :=$ $\\left\\{{\\bf W}_{Q},{\\bf W}_{K},{\\bf W}_{V},{\\bf W}_{O},{\\bf r}\\right\\}$ denotes the set of all model weights. ", "page_idx": 4}, {"type": "text", "text": "Training Setting. We fix one layer in both the attention and MLP layers to scrutinize the training dynamics more rigorously. Specifically, we let ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\mathbb{W}}_{Q}=\\left(\\begin{array}{c c}{\\boldsymbol{\\mathbf{W}}_{Q}^{x}}&{*}\\\\ {*}&{*}\\end{array}\\right),\\quad\\boldsymbol{\\mathbf{W}}_{K}=\\left(\\begin{array}{c c}{\\boldsymbol{\\mathbf{W}}_{K}^{x}}&{*}\\\\ {*}&{*}\\end{array}\\right),\\quad\\boldsymbol{\\mathbf{W}}_{V}=\\left(\\begin{array}{c c}{*}&{*}\\\\ {*}&{\\boldsymbol{\\mathbf{W}}_{V}^{y}}\\end{array}\\right)\\quad\\boldsymbol{\\mathbf{W}}_{O}=(*}&{\\boldsymbol{\\mathbf{W}}_{O}^{y})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{Q}^{x},\\mathbf{W}_{K}^{x}\\,\\in\\,\\mathbb{R}^{d_{X}\\times d_{X}},\\mathbf{W}_{V}^{y}\\,\\in\\,\\mathbb{R}^{(m_{v}-d_{X})\\times d_{y}},\\mathbf{W}_{O}^{y}\\,\\in\\,\\mathrm{I}$ $\\mathbf{W}_{O}^{\\pmb{y}}\\in\\mathbb{R}^{m\\times d_{\\mathcal{V}}}$ . Here, we set the elements other than $\\mathbf{W}_{Q}^{\\mathbf{x}},\\mathbf{W}_{K}^{\\mathbf{x}},\\mathbf{W}_{V}^{\\mathbf{y}}$ and $\\mathbf{W}_{O}^{y}$ to be zero. Besides, we fix $\\mathbf{W}_{V}^{y}$ to be $\\mathbf{I}_{(m_{v}-d_{X})\\times d_{\\mathcal{Y}}}$ . We sample $\\mathbf{r}_{i}$ from a uniform distribution Un $\\mathrm{if\\{-1,1\\}}$ and fixed during the training process. Based on this setting, the trainable part we need to consider is actually $\\Psi^{\\prime}:=\\left\\{\\mathbf{W}_{Q}^{x},\\mathbf{W}_{K}^{x},\\mathbf{W}_{O}^{y}\\right\\}$ . This problem remains highly non-convex and challenging. ", "page_idx": 4}, {"type": "text", "text": "We utilize mini-batch with-replacement SGD to train the transformer model. The empirical crossentropy loss for each batch $B_{t}$ is written as ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathcal{B}_{t}}(\\Psi)=L_{\\mathcal{B}_{t}}(\\Psi^{\\prime}):=\\frac{1}{B}\\sum_{n\\in\\mathcal{B}_{t}}\\ell\\left(y_{S_{n}}\\cdot f(\\mathbf{H};\\Psi)\\right)+\\frac{\\lambda}{2}\\|\\Psi^{\\prime}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\ell(z)=\\log(1+\\exp(-z))$ , $y_{S_{n}}$ is the real value label of the prompt defined in Definition 2, and the term $\\|\\Psi^{\\prime}\\|_{F}^{2}$ represents $\\|\\mathbf{W}_{Q}^{\\dot{\\mathbf{x}}}\\|_{F}^{2}+\\|\\mathbf{W}_{K}^{\\mathbf{x}}\\|_{F}^{2}+\\|\\mathbf{W}_{O}^{\\pmb{y}}\\|_{F}^{2}$ , which is the $L_{2}$ regularization term with $\\|\\cdot\\|_{F}$ denoted as the Frobenius norm. The purpose of the regularization in this paper is to accelerate and stabilize the mini-batch with-replacement SGD. The learning step is set to be $\\begin{array}{r}{\\dot{\\eta_{t}}=\\frac{2}{\\lambda(\\gamma+t)}}\\end{array}$ , where $\\gamma$ is an offset parameter. This decaying schedule is standard and also used in prior work [34, 50, 51] studying convergence of SGD. The whole procedure is in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Initialization Setting. All initial values of $\\mathbf{W}_{O}^{y}$ are sampled from a i.i.d. Gaussian distributions with mean 0 and variance $\\sigma_{1}^{2}$ . The initialization of $\\mathbf{W}_{Q}^{x}$ and ${\\bf W}_{K}^{x}$ are diagonal matrices $\\sigma_{0}\\mathbb{I}$ , which are also adopted in other work that consider training $\\dot{\\mathbf{W}}_{Q}$ and $\\mathbf{W}_{K}$ separately [28, 25]. ", "page_idx": 4}, {"type": "text", "text": "Testing Setting. The model performance is measured by 0-1 test error on a test prompt distribution $\\mathcal{D}^{*}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathcal{D}^{*}}^{0-1}(\\Psi):=\\mathbb{P}_{S\\sim\\mathcal{D}^{*}}[(y_{S}\\cdot f(\\mathbf{E}(S);\\Psi))<0].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Training algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: Training distribution $\\mathcal{D}_{S}$ , Test distribution $\\mathcal{D}^{*}$ , Batch size $B$ , step size $\\begin{array}{r}{\\eta_{t}\\,=\\,\\frac{2}{\\lambda(\\gamma+t)}}\\end{array}$ , stopping criterion $\\varepsilon$ and total epochs $T$ .   \nInitialize model parameters \u03a8\u2032(0).   \nfor $t=0,1,\\ldots,T-1$ do   \nIf $L_{D^{*}}^{0-1}(\\dot{\\Psi}^{(t)})\\leq\\varepsilon$ stop else continue.   \nRandomly sample mini batches $B_{t}$ of size $B$ from $\\mathcal{D}_{S}$ .   \nUpdate model parameters: $\\boldsymbol{\\Psi^{\\prime}}^{(t+1)}=\\boldsymbol{\\Psi^{\\prime}}^{(t)}-\\eta_{t}\\nabla_{\\boldsymbol{\\Psi}^{\\prime}}L_{\\boldsymbol{B}_{t}}(\\boldsymbol{\\Psi^{\\prime}}^{(t)}).$   \nend for ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main theoretical results, which is based on the following conditions. We consider the learning iterations $0\\leq t\\leq T^{*}$ , where $T^{*}=\\Omega(m^{-1}\\sigma_{0}^{-1}\\sigma_{1}^{-1}m\\lambda^{-2}K_{1}\\vert\\vert\\mathbf{q}\\vert\\vert^{2}((L-$ $1)\\|\\mathbf{u}\\|^{2}+1)\\log(\\varepsilon^{-1}))$ denotes the maximum admissible iteration. ", "page_idx": 4}, {"type": "text", "text": "Condition 1. Suppose that there exists a sufficiently large constant $C$ , such that the following hold: ", "page_idx": 4}, {"type": "equation", "text": "$\\gamma\\geq C\\operatorname*{max}\\{\\|\\mathbf{q}\\|^{2}/(m K_{1}\\lambda),10/\\lambda\\},\\,\\lambda\\leq\\operatorname*{min}\\{\\left(C\\log(K m/\\delta)\\|\\mathbf{q}\\|\\right)^{-1},\\,(C\\sigma_{0}/2\\|\\mathbf{u}\\|^{2})^{-1}\\}$ ", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sigma_{1}\\leq\\operatorname*{min}\\{(C\\sigma_{0}\\|{\\bf u}\\|^{4}\\|{\\bf q}\\|\\sqrt{\\log(5K m/\\delta)}/K_{1})^{-1},{w^{\\ast}}^{2}/(C m^{3/2}\\|{\\bf q}\\|)\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that we do not have any requirement upon demonstration length $L$ and batch size $B$ for training, thus the training can be really flexible compared with the strict requirement in [28]. The condition on dimensionality $d_{\\mathcal{X}},d_{\\mathcal{Y}}$ and the network width $m$ ensure the learning problem is in a sufficiently overparameterized setting [41, 42, 52, 43]. The condition on $\\gamma$ ensures the learning step to be small and thus learning process enjoys an approximation to gradient flow. The condition on the small $\\lambda$ is to ensure the model\u2019s sufficient learning before being stuck by regularization [53]. The condition on $K$ is to control the impact of cross-concept contribution in the Attention\u2019s learning dynamic, which can actually be relaxed at the cost of a denser analysis. The condition on $\\sigma_{\\xi}$ is to ensure that the gradient flows be mildly influenced by the noise. Last but not least, the conditions on $\\sigma_{1}$ guarantee that the initial beliefs of MLP is small and the gradients of SGD can update the model effectively. A more detailed discussion over the parameter settings is delayed to Appendix $_\\mathrm{H}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Exponential Convergence of 0-1 loss. Under Condition $^{\\,l}$ , define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu:=\\operatorname*{min}\\{2\\sqrt{2}\\sigma_{1}/(1+\\kappa_{y}),\\sigma_{0}(1-\\kappa_{x})e^{-\\log(5K m/\\delta)\\frac{\\sigma_{1}^{2}\\|{\\bf u}\\|^{4}(1+e^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}}{(1-e^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}})}}{(1-e^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}})})}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, for $\\forall\\varepsilon>0$ there exist some positive constants $C_{1}$ and $C_{2}$ , with probability no less than $1-\\delta$ , for $T\\geq\\hat{T}=C_{1}\\sigma_{1}m\\lambda K_{1}\\gamma\\sqrt{(1+\\kappa_{y})\\log(5K m/\\delta)}/{w^{*}}^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{\\mathcal{D}^{*}}^{0-1}(\\Psi^{(T)})\\leq\\exp(-\\frac{C_{2}\\nu^{2}m\\lambda^{2}(\\gamma+T)}{K_{1}\\|\\mathbf{q}\\|^{2}((L-1)\\|\\mathbf{u}\\|^{2}+1)}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus after ", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{\\varepsilon}=\\frac{K_{1}\\|\\mathbf{q}\\|^{2}((L-1)\\|\\mathbf{u}\\|^{2}+1)}{C_{2}\\nu^{2}m\\lambda^{2}}\\log(\\frac{1}{\\varepsilon})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "iterations, we have $L_{\\mathcal{D}^{*}}^{0-1}(\\Psi^{(T)})\\leq\\varepsilon$ . ", "page_idx": 5}, {"type": "text", "text": "Note that the bound is valid only when $T\\geq{\\hat{T}}$ , a common threshold in prior convergence rate analyses [34, 33, 36]. Importantly, the existence of $\\hat{T}$ does not affect the convergence rate as $\\varepsilon\\rightarrow0$ , since $\\hat{T}$ is independent of $\\varepsilon$ . Our novel analysis generalizes these prior results to our realistic settings handling the challenges of self-attention, ReLU-MLP, and cross-entropy loss simultaneously. By considering extreme cases, our techniques relax the batch size requirement, enabling more general results. Consequently, the sample complexity for Bayes-optimal test error is $N=T_{\\varepsilon}$ . ", "page_idx": 5}, {"type": "text", "text": "Before introducing the next proposition, we highlight a key observation from the semantic geometry in Definition 1. For any $k_{1}\\in[K_{1}]$ , defining $\\bar{\\pmb{a}_{k_{1}}}:=(\\pmb{\\mu}_{k_{1}}^{+}\\bar{+\\pmb{\\mu}_{k_{1}}^{-}})/2$ and $b_{k_{1}}:=(\\pmb{\\mu}_{k_{1}}^{+}-\\pmb{\\mu}_{k_{1}}^{-})/2$ , we find that for $k_{1}^{\\prime}\\neq k_{1}$ , $\\{\\pmb{a}_{k_{1}},\\pmb{b}_{k_{1}}\\}\\perp\\{\\pmb{a}_{k_{1}^{\\prime}},\\pmb{b}_{k_{1}^{\\prime}}\\}$ and $\\langle\\pmb{a}_{k_{1}},\\pmb{b}_{k_{1}}\\rangle=0$ . This structure is exemplified in Figure 1(b) of [12], where \u201c[Bird]\u201d consists of orthogonal steering vectors: \u201cplant $\\Rightarrow{}$ animal\u201d and \u201cmammal $\\Rightarrow{}$ bird,\u201d corresponding to the concept feature $\\mathbf{\\nabla}_{a_{k}}$ and semantic label features $b_{k}$ . Here, the term $e b_{k_{1}}$ in $\\pmb{\\mu}_{k_{1}}^{e}$ determines the label assignment. Similarly, defining $c_{k_{1}}:=(\\pmb q_{k_{1}}^{+}+\\pmb q_{k_{1}}^{-})/2$ and $d_{k_{1}}:=(\\pmb q_{k_{1}}^{+}-\\pmb q_{k_{1}}^{-})/2$ yields analogous properties. Detailed definitions are provided in Appendix I. The following proposition explores the model\u2019s ability to handle OOD unseen ICL tasks. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Out-of-Distribution-Generalization3. During testing, the learned model admits probability distribution shift on $\\mathcal{D}_{z}^{*}$ and data shift on $\\mathcal{D}_{\\pmb{x}}^{*}\\times\\mathcal{D}_{\\pmb{y}}^{*}$ to generate a new prompt distribution $\\begin{array}{r}{\\mathcal{D}_{S}^{*}\\,=\\,\\sum_{k=1}^{K_{1}}\\left(\\pi_{k}^{+^{*}}\\mathcal{P}_{k,L^{*}+1}^{+}{^*}+\\pi_{k}^{-^{*}}\\mathcal{P}_{k,L^{*}+1}^{-}{^*}\\right)}\\end{array}$ . Specifically, the new $\\mathcal{D}_{S}^{*}$ satisfies the following properties. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "\u2022 The prompt length $L^{*}$ can be any positive integer.   \n\u2022 $\\mathcal{D}_{z}^{*}$ can enjoy arbitrary distribution, satisfying that each prompt in the distribution has at least one co-concept $k\\in[K_{1}].$ , and still each word has equal chance to have positive or negative semantic labels over its concepts4.   \n\u2022 $\\mathcal{D}_{x}^{*}\\times\\mathcal{D}_{y}^{*}$ can enjoy a great family of data shift. $\\forall k\\ne k^{\\prime}\\in[K_{1}],k_{2}\\in[K_{2}],$ , we can have new $\\mathbf{M}^{*}$ and ${\\bf Q}^{*}$ such that $\\pmb{\\mu}_{k}^{\\pm}{}^{*}=\\pmb{a}_{k}^{*}\\pmb{b}_{k}^{*}$ , $\\pmb q_{k}^{\\pm\\,*}=c_{k}^{*}\\pmb d_{k}^{*}$ , $\\pmb{\\nu}_{k_{2}}=\\pmb{\\nu}_{k_{2}}^{*}$ . Here, $\\mathbfit{a}_{k}^{\\ast},b_{k}^{\\ast},c_{k}^{\\ast},d_{k}^{\\ast}$ are   \nany vectors belong to the conic hulls of $\\cdot\\{\\pmb{a}_{k}\\}_{k=1}^{K_{1}},\\{\\pmb{b}_{k}\\}_{k=1}^{K_{1}},\\{\\pmb{c}_{k}\\}_{k=1}^{K_{1}},\\{\\pmb{d}_{k}\\}_{k=1}^{K_{1}}$ respectively, satisfying $\\|b_{k}^{*}\\|\\geq\\|a_{k}^{*}\\|=\\Theta(\\|\\mathbf{u}\\|)$ and $\\|\\b{d}_{k}^{*}\\|\\geq\\|\\b{c}_{k}^{*}\\|=\\Theta(\\|\\ensuremath{\\mathbf{q}}\\|)$ . $\\boldsymbol{\\nu}_{k_{2}}^{*}=\\Theta(\\lVert\\mathbf{u}\\rVert)$ are any vectors from the complement space of span(M). ", "page_idx": 6}, {"type": "text", "text": "Again, the learned model satisfies $L_{{D^{*}}}^{0-1}(\\Psi^{({T^{*}})})\\leq\\varepsilon$ ", "page_idx": 6}, {"type": "text", "text": "This proposition demonstrates the strong Out-of-Distribution Generalization ability of transformer utilizing multi-concept semantics, suggesting the efficiency transformer to conduct unseen ICL tasks just by its learned \u201cKnowledge\u201d on the high-level concept and low-level label semantic information from the two non-orthogonal dictionaries. The admit of shift for $\\mathcal{D}_{z}^{*}$ denotes that each prompt can enjoy multi-co-concepts and each word-label pair can appear in at least $\\Vert z\\Vert_{0}$ concept-specific prompts/tasks\u2019 distribution, which aligns the real-world cases. On the other hand, we also believe the admit of shift for $\\mathcal{D}_{\\pmb{x}}^{*}\\times\\mathcal{D}_{\\pmb{y}}^{*}$ is inspiring, suggesting that transformer can conduct specific cross-concept semantic \u201cKnowledge Intersection\u201d. As such, this lemma suggest that the transformer can master the regularity of unseen ICL tasks\u2019 \u201cstructure\u201d in the presence the multi-concept encoded representation. ", "page_idx": 6}, {"type": "text", "text": "Remark 1. Comparison with Related Work. Theorem 3.4 in [28] and Theorem 2 in [54] address the transformer\u2019s OOD capability in specific structured ICL classification and regression tasks. Our results differ by focusing on compositional generalization of learned concepts, grounded in the concept-specific linear latent geometry observed in LLMs. ", "page_idx": 6}, {"type": "text", "text": "5 Proof Idea ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In a big picture, we simply extend standard expectation-variance reduction techniques [34] to our setting. Section 5.1 defines coefficients to examine NN\u2019s expected projection along feature directions. Section 5.2 provides the convergence of the expected estimator through the lens of coefficient evolution; Section 5.3 showcase the exponential convergence by treating the conditional expectations of the NNs as Doob martingales and exploiting the property of the tails under low-noise conditions. ", "page_idx": 6}, {"type": "text", "text": "5.1 Idempotent Operator Techniques ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Idempotent Operator Trick. Define $\\mathbb{U}~:=~\\operatorname{span}(\\mathbf{M})$ and its complement space $\\mathbb{U}^{\\perp}$ . By definition, we know that $\\dim(\\mathbb{U})~~=~K$ and $\\dim(\\mathbb{U}^{\\perp})\\ \\ =\\ \\ d_{\\mathcal{X}}\\ -\\ K$ . Then we can let $\\{\\{\\mathbf{\\bar{\\alpha}}_{k_{1}}\\}_{k_{1}=1}^{K_{1}},\\{\\mathbf{b}_{k_{1}}\\}_{k_{1}=1}^{K_{1}},\\{\\nu_{k_{2}}\\}_{k_{2}=1}^{K_{2}},\\{\\mathbf{u}_{w}\\}_{w=1}^{d_{x}-K}\\}$ be the set of standard orthogonal basis for $\\mathbb{R}^{d_{\\boldsymbol{X}}}$ where u1 , \u00b7 \u00b7 \u00b7 , udX are the standard orthogonal basis of $\\mathbb{U}^{\\perp}$ . Then we can derive an idempotent decomposition of the identity matrix ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{K_{1}}\\frac{{\\pmb a}_{s}{\\pmb a}_{s}^{\\top}}{\\|{\\pmb a}_{s}\\|^{2}}+\\sum_{s=1}^{K_{1}}\\frac{b_{s}{\\pmb b}_{s}^{\\top}}{\\|{\\pmb b}_{s}\\|^{2}}+\\sum_{r=1}^{K_{2}}\\frac{{\\pmb\\nu}_{r}{\\pmb\\nu}_{r}^{\\top}}{\\|{\\pmb u}\\|^{2}}+\\sum_{w=1}^{d_{X}-K}u_{w}^{\\bot}{\\pmb u}_{w}^{\\bot}^{\\top}={\\bf I}_{d_{X}\\times d_{X}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar techniques are also applied to the label\u2019s dictionary: $\\mathbb{Q}\\;:=\\;\\operatorname{span}(\\mathbf{Q})$ , where we define $q_{1}^{\\perp},\\cdot\\cdot\\cdot\\mathbf{\\nabla},q_{d_{3}-K_{1}}^{\\perp}$ as the standard orthogonal basis of the complement space $\\mathbb{Q}^{\\perp}$ . In our subsequent derivation, the expectation $\\mathbb{E}[\\cdot]$ is taken over the stochastic gradient descent. Similar to the idea in [34, 33, 36], we first serve to see how $\\mathbb{E}(\\Psi^{(t)})$ evolves. For $\\mathbb{E}(\\Psi^{(t)})$ , every gradient descent update by all concept\u2019s samples within a soft \u201cweight\u201d, and thus the analysis is equivalent to gradient descent with an ideally-balanced prompt set. Leveraging the symmetry of the prompt distribution, as well as the symmetry of W(Q0) and $\\mathbf{W}_{K}^{(0)}$ , we introduce the following decompositions. ", "page_idx": 6}, {"type": "image", "img_path": "57C9mszjj3/tmp/10f7a24b15685ff424f55deefae38f44aa8b8f419c885546d3d365e4a8526800.jpg", "img_caption": ["Figure 1: Illustration of our Idempotent Operator Techniques. This allows us to focus on analyzing the evolving coefficients, which are key to the expected 0-1 loss convergence. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Lemma 1. We can decompose $\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}}],\\;\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}]$ and the $i$ -th row of $\\mathbb{E}[\\mathbf{W}_{O}^{y}]$ $(i\\,\\in\\,[m])$ via the following (scaled) $p$ rojection matrices and projection directions. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sharp[{\\mathbf{W}_{Q}^{\\pi(t)}}]=\\displaystyle\\sum_{s=1}^{K_{1}}\\alpha_{Q,s}^{(t)}\\cdot\\frac{a_{s}a_{s}\\cdot\\mathbf{\\Delta}}{\\|a_{s}\\|^{4}}+\\displaystyle\\sum_{s=1}^{K_{1}}\\beta_{Q,s}^{(t)}\\cdot\\frac{b_{s}b_{s}\\overset{\\top}{\\operatorname{\\mathbf{\\Delta}}}}{\\|b_{s}\\|^{4}}+\\displaystyle\\sum_{r=1}^{K_{2}}\\tau_{Q,r}^{(t)}\\cdot\\frac{\\nu_{r}\\nu_{r}\\overset{\\top}{\\mathbf{\\Delta}}}{\\|\\mathbf{u}\\|^{4}}+\\displaystyle\\sum_{w=1}^{K-K}\\rho_{Q,w}^{(t)}\\cdot u_{w}^{\\perp}u_{w}^{\\perp\\top},}\\\\ &{\\sharp[{\\mathbf{W}_{K}^{\\pi(t)}}]=\\displaystyle\\sum_{s=1}^{K_{1}}\\alpha_{K,s}^{(t)}\\cdot\\frac{a_{s}a_{s}\\overset{\\top}{\\operatorname{\\mathbf{\\Delta}}}}{\\|a_{s}\\|^{4}}+\\displaystyle\\sum_{s=1}^{K_{1}}\\beta_{K,s}^{(t)}\\cdot\\frac{b_{s}b_{s}\\overset{\\top}{\\operatorname{\\mathbf{\\Delta}}}}{\\|b_{s}\\|^{4}}+\\displaystyle\\sum_{r=1}^{K_{2}}\\tau_{K,r}^{(t)}\\cdot\\frac{\\nu_{r}\\nu_{r}\\overset{\\top}{\\mathbf{\\Delta}}}{\\|\\mathbf{u}\\|^{4}}+\\displaystyle\\sum_{w=1}^{K-K}\\rho_{K,w}^{(t)}\\cdot u_{w}^{\\perp}u_{w}^{\\perp\\top},}\\\\ &{\\sharp[{\\mathbf{W}_{Q_{(i,\\cdot)}}^{y}}]=\\displaystyle\\sum_{k=1}^{K_{1}}\\alpha_{O_{(i,\\cdot)},k}^{(t)}\\cdot\\frac{c_{k}\\overset{\\top}{\\operatorname{\\mathbf{\\Delta}}}}{\\|c_{k}\\|^{2}}+\\displaystyle\\sum_{k=1}^{K_{1}}\\beta_{O_{(i,\\cdot)},k}^{(t)}\\cdot\\frac{d_{k}\\overset{\\top}{\\operatorname{\\mathbf{\\Delta}}}}{\\|d_{k}\\|^{2}}+\\displaystyle\\sum_{w=1}^{K-K_{1}}\\rho_{O_{(i,\\cdot)},w \n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here $\\alpha_{Q,s}^{(t)},\\,\\alpha_{K,s}^{(t)}$ and $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ represent the expected concept learning process, $\\beta_{Q,s}^{(t)},\\;\\beta_{K,s}^{(t)}$ and \u03b2(Ot()i,\u00b7),k represent the expected concept-specific semantic learning process and \u03c4 $\\tau_{Q,r}^{(t)},\\tau_{K,r}^{(t)},\\rho_{Q,w}^{(t)},\\rho_{K,w}^{(t)}$ and $\\rho_{O_{(i,\\cdot)},w}^{(t)}$ represent the expected memorization of the concept irrelevant noise. It holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(\\mathbf{W}_{K}^{x}^{\\mathbf{\\Xi}(t)}\\mu_{s}^{\\pm e})]^{\\top}\\mathbb{E}[\\mathbf{W}_{Q}^{x}{}^{(t)}\\mu_{s}^{e}]=\\alpha_{Q,s}^{(t)}\\cdot\\alpha_{K,s}^{(t)}/\\|\\mathbf{a}_{s}\\|^{2}\\pm\\beta_{Q,s}^{(t)}\\cdot\\beta_{K,s}^{(t)}/\\|\\mathbf{b}_{s}\\|^{2},}\\\\ &{\\mathbb{E}[\\mathbf{W}_{O_{(i,\\cdot)}}^{y}{}^{(t)}q_{k}^{e}]=\\alpha_{O_{(i,\\cdot),k}}^{(t)}+e\\cdot\\beta_{O_{(i,\\cdot),k}}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for $\\forall e\\,\\in\\,[\\pm],i\\,\\in\\,[m],k\\,\\in\\,[K_{1}]$ and for $\\forall e^{\\prime}\\,\\in\\,[\\pm],s^{\\prime}\\,\\in\\,[K_{1}],r\\,\\in\\,[K_{2}],w\\,\\in\\,[d_{\\mathcal{X}}-K]$ , $\\forall\\mathbf{u}\\ \\in$ $\\{\\mu_{s^{\\prime}}^{e^{\\prime}},\\nu_{r},\\mathbf{\\boldsymbol{u}}_{w}^{\\perp}\\}$ , it holds that $\\mathbb{E}[(\\mathbf{W}_{K}^{\\mathbf{x}}^{\\mathbf{\\alpha}(t)}\\mathbf{u})]^{\\top}\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\mathbf{\\alpha}(t)}\\pmb{\\mu}_{s}^{e}]=0$ . Similar conclusions hold when the query vectors are $\\pmb{\\nu}_{r}$ and $u_{w}^{\\perp}$ , $\\forall r\\in[K_{2}],w\\in[d_{\\mathcal{X}}-K]$ . As such, our remaining task is to scrutinize the coefficients evolution, which would be the key contributors to the expected 0-1 loss convergence. ", "page_idx": 7}, {"type": "text", "text": "5.2 Convergence of the Expectation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Denote $\\mathcal{U}_{k,n}^{y s_{n}}(t)$ and $\\mathcal{W}_{k,n}^{v}(t)-\\mathcal{U}_{k,n}^{y s_{n}}(t)$ as the activated neuron set for $\\{i\\in[m]\\mid\\mathbf{r}_{i}y_{S_{n}}>0\\}$ and $\\{i\\,\\in\\,[m]\\,\\mid\\,{\\bf r}_{i}y_{S_{n}}\\,<\\,0\\}$ separately, and $\\textstyle\\sum_{l\\in S_{n,k}^{y_{S_{n}}}}\\left(\\sigma_{S}^{(t)}\\right)_{l}^{n}$ represents the correct attention weight, where the detailed definitions are delayed in Appendix E. We then introduce the following lemma. Lemma 2. Under Condition $^{\\,l}$ , when ", "page_idx": 7}, {"type": "equation", "text": "$$\n(\\sum_{i\\in\\mathcal{U}_{k,n}^{y_{S_{n}}}(t)}-\\sum_{\\substack{i\\in\\mathcal{W}_{k,n}^{y_{S_{n}}}(t)-\\mathcal{U}_{k,n}^{y_{S_{n}}}(t)}})\\left(\\alpha_{O_{(i,.)},k}^{(t)}+(2\\sum_{l\\in\\mathcal{S}_{n,k}^{y_{S_{n}}}}(\\sigma_{S}^{(t)})_{l}^{n}-1)y_{S_{n}}\\beta_{O_{(i,.)},k}^{(t)}\\right)\\geq0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holds, we have $L_{\\mathcal{D}^{*}}^{0-1}(\\mathbb{E}(\\Psi^{\\prime}{}^{(t)}))=0$ ", "page_idx": 7}, {"type": "text", "text": "As such, the following lemmas show the learning outcomes of the $\\mathbb{E}\\big(\\Psi^{(t)}\\big)$ along the iterations. ", "page_idx": 7}, {"type": "image", "img_path": "57C9mszjj3/tmp/1542c6062ef5dc4b69393c1b1e010ba5bcaa69d9fc26962aaf7f3a907ca359da.jpg", "img_caption": [], "img_footnote": ["Figure 2: Learning dynamics: (i) training and test loss; (ii) correct attention weight; (iii) maximum values of $\\alpha_{Q,s}\\cdot\\alpha_{K,s},\\beta_{Q,s}\\cdot\\beta_{K,s}$ , maximum values of the complement products $\\tau_{Q,r}\\cdot\\tau_{K,r}$ or $\\rho_{Q,2}\\cdot\\rho_{K,2}$ , and maximum values of product-with-noise $\\left(\\mathbf{W}_{K}^{\\mathbf{x}}\\xi_{\\mathbf{x}}\\right)^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}}\\xi_{\\mathbf{x}}$ ; (iv) maximum values of $\\alpha{\\cal O}_{(i,\\cdot)},k$ and $|\\beta_{O_{(i,\\,*)},k}|$ , maximum values of the complement coefficients $\\rho_{O_{(i,\\cdot)},w}$ and maximum values of product-with-noise $\\mathbf{W}_{O_{(i,\\,.)}}^{y}\\xi_{y}$ . "], "page_idx": 8}, {"type": "text", "text": "Lemma 3. (Convergence of the Expectation). There exist constant $C_{1}\\ >\\ 0,\\ \\forall t\\ \\geq\\ \\hat{T}\\ =$ $C_{1}\\sigma_{1}m\\lambda K_{1}\\gamma\\sqrt{(1+\\kappa_{y})\\log(5K m/\\delta)}/w^{*2}(1-\\kappa_{y})\\|\\mathbf{q}\\|$ , we have $L_{\\mathcal{D}^{*}}^{0-1}(\\mathbb{E}(\\Psi^{\\prime}{}^{(t)}))=0$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 4. (Regularizing the models). Under Condition $^{\\,l}$ , it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{Q,k}^{(T^{*})}=\\alpha_{K,k}^{(T^{*})}=O(\\mathbb{E}[\\alpha_{Q,k}^{(0)}]),\\quad\\beta_{Q,k}^{(T^{*})}=\\beta_{K,k}^{(T^{*})}=\\Theta(\\|\\mathbf{u}\\|\\sqrt{\\log(\\frac{\\|\\mathbf{u}\\|^{2}}{\\lambda K_{1}}\\log(\\frac{\\|\\mathbf{q}\\|^{2}}{m\\lambda K_{1}}))}),}\\\\ &{\\alpha_{O_{(i,\\cdot)},k}^{(T^{*})}\\leq|\\beta_{O_{(i,\\cdot)},k}^{(T^{*})}|=\\Theta(\\log(\\frac{\\|\\mathbf{q}\\|^{2}}{m\\lambda K_{1}})),\\mathbb{E}[(\\displaystyle\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}(\\sigma_{S}^{(T^{*})})_{j}^{n})]=\\Theta(\\frac{1}{1+\\frac{\\lambda K_{1}}{\\|\\mathbf{u}\\|^{2}}\\log(\\frac{m\\lambda K_{1}}{\\|\\mathbf{q}\\|^{2}})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In addition, our analysis provides three asymptotic properties of the coefficients evolution, which are delayed to Appendix I.1.3 and I.2 for room limitation. ", "page_idx": 8}, {"type": "text", "text": "5.3 Exponential Convergence of 0-1 loss ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Proposition 2. $\\forall t\\geq\\hat{T}$ , when $\\|\\Psi^{\\prime}{}^{(t)}-\\mathbb{E}(\\Psi^{\\prime}{}^{(t)})\\|_{F}\\leq\\nu$ holds, we have ${\\cal L}_{{D^{*}}}^{0-1}({\\Psi^{\\prime}}^{(t)})=0.$ . Here, $\\|\\b{\\Psi}^{\\prime}\\|_{F}^{2}:=\\|\\mathbf{W}_{Q}^{\\pmb{x}}\\|_{F}^{2}+\\|\\mathbf{W}_{K}^{\\pmb{x}}\\|_{F}^{2}+\\|\\mathbf{W}_{O}^{\\pmb{y}}\\|_{F}^{2}$ . ", "page_idx": 8}, {"type": "text", "text": "By definition of 0-1 loss, then we only need to prove the 0-1 loss convergence by seeing the speed of $\\Psi^{\\prime}{}^{(t)}$ converging to $\\mathbb{E}(\\Psi^{\\prime}{}^{(t)})$ with an error of $\\nu$ in terms of $\\|\\cdot\\|_{F}$ . ", "page_idx": 8}, {"type": "text", "text": "Drawing insights from [34], we see $B_{0},\\cdots,B_{T-1}$ as a i.i.d. random variables following the same distribution. Then $\\forall t\\in\\{0,\\cdot\\cdot\\cdot,T\\}$ , it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{Q}^{t}=\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}}^{(T+1)}\\mid\\mathcal{B}_{0},\\cdot\\cdot\\cdot,\\mathcal{B}_{t}]-\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}}^{(T+1)}\\mid\\mathcal{B}_{0},\\cdot\\cdot\\cdot,\\mathcal{B}_{t-1}],}\\\\ &{D_{K}^{t}=\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}^{(T+1)}\\mid\\mathcal{B}_{0},\\cdot\\cdot\\cdot,\\mathcal{B}_{t}]-\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}^{(T+1)}\\mid\\mathcal{B}_{0},\\cdot\\cdot\\cdot,\\mathcal{B}_{t-1}]}\\\\ &{D_{O}^{t}=\\mathbb{E}[\\mathbf{W}_{O}^{y}^{(T+1)}\\mid\\mathcal{B}_{0},\\cdot\\cdot\\cdot,\\mathcal{B}_{t}]-\\mathbb{E}[\\mathbf{W}_{O}^{y}^{(T+1)}\\mid\\mathcal{B}_{0},\\cdot\\cdot\\cdot,\\mathcal{B}_{t-1}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "are martingale difference sequences, and for $\\forall X\\ \\in\\ \\{Q,K,O\\}$ and its corresponding $\\textbf{W}\\in$ $\\{\\mathbf{W}_{\\mathcal{Q}}^{\\mathbf{x}},\\mathbf{W}_{K}^{\\mathbf{x}},\\mathbf{W}_{\\mathcal{O}}^{\\mathbf{y}}\\}_{.}$ , we have $\\begin{array}{r}{\\sum_{t=0}^{T}D_{X}^{t}\\;=\\;\\mathbf{W}^{(T+1)}\\,-\\,\\mathbb{E}[\\mathbf{W}^{(T+\\overset{.}{1})}]}\\end{array}$ . Then we utilize the following lemma in [34, 55] to give  a bound over the variance. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5. Let $D_{1},\\cdot\\cdot\\cdot\\,,D_{T-1}$ be a martingale difference sequence. Suppose $\\exists c_{T}>0$ such that $\\begin{array}{r}{\\sum_{t=0}^{T}\\|D_{t}\\|_{\\infty}^{2}\\leq c_{T}^{2}}\\end{array}$ , where $\\|\\cdot\\|_{\\infty}$ is the essential supremum of $\\|\\cdot\\|_{F}$ . Then for $\\forall\\epsilon>0$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big[\\operatorname*{sup}_{s\\in[T]}\\|\\sum_{t=0}^{s}D_{t}\\|_{F}\\geq\\epsilon\\Big]\\leq2\\exp(-\\frac{\\epsilon^{2}}{2c_{T}^{2}}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Therefore, we need to see if there exists a decaying positive constant $c_{T}$ (with decaying rate $O(1/T^{q}),q>0)$ , such that $\\begin{array}{r}{\\sum_{t=0}^{T}\\|D_{X}^{t}\\|_{\\infty}^{2}\\leq{c_{T}}^{2},\\forall X\\in\\{Q,K,O\\}}\\end{array}$ , where $\\|\\cdot\\|_{\\infty}$ is the essential ", "page_idx": 8}, {"type": "image", "img_path": "57C9mszjj3/tmp/bca62d1b1769c9dc94ee79626fa7a4ccfc1630e75641ec5f6f6236099b7721f9.jpg", "img_caption": ["(c) OOD Scenario 2: 0.8 fraction for concept 0 and 0.2(d) OOD Scenario 3: Shift the data as $\\pmb{\\mu}_{1}^{\\pm\\,*}=\\pmb{a}_{1}\\pmb{b}_{2}$ fraction for concept 1 during testing. and \u00b52\u00b1 $\\pmb{\\mu}_{2}^{\\pm\\,*}=\\pmb{a}_{2}\\pmb{b}_{1}$ during testing. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: Learning dynamic in three OOD scenarios. The training settings and plotting methods are identical to those used in Figure 2, and the testing settings are: (a-b) utilizes different prompt lengths; (c) adopts a skewed distribution over $_{\\mathscr{L}}$ ; (d) switches the concept-specific semantic features. ", "page_idx": 9}, {"type": "text", "text": "supremum of $\\|D_{X}^{t}\\|_{F}$ . Subsequently, by controlling the martingale sequence norm tail similarly in [34, 55], we can obtain an exponential convergence rate after $T_{1}$ . ", "page_idx": 9}, {"type": "text", "text": "For $\\mathbf{W}\\in\\{\\mathbf{W}_{Q}^{\\mathbf{x}},\\mathbf{W}_{K}^{\\mathbf{x}},\\mathbf{W}_{O}^{\\mathbf{y}}\\}$ , to check the decaying $c_{T}$ , we adopt the techniques of [34, 33, 36] in the following manner. Let $B_{t}^{\\,\\,\\prime}$ be an independent variable from $B_{0},\\cdot\\cdot\\cdot,B_{T}$ and let $\\mathbf{W}_{t}^{\\ (T+1)}$ be an output of the algorithm depending on $(\\bar{B_{0}},\\cdot\\cdot\\cdot,\\cdot_{\\cdot},\\bar{B_{t-1}},\\bar{B_{t}}^{\\prime},\\bar{B_{t+1}},\\cdot\\cdot\\cdot,\\bar{B_{T}})$ . Then we have ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\|D_{X}^{t}\\|_{\\infty}\\leq\\mathbb{E}[\\|\\mathbf{W}^{(T+1)}-\\mathbf{W}_{t}^{(T+1)}\\|_{\\infty}\\mid B_{0},\\cdot\\cdot\\cdot,B_{t}].\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Therefore, one may estimate $c_{X}^{T}{}^{2}$ by bounding $\\|\\mathbf{W}^{(T+1)}-\\mathbf{W}_{t}^{\\mathbf{\\Omega}(T)}\\|_{\\infty}^{2}$ uniformly w.r.t. $B_{0},\\cdots,B_{T-1}$ . Such a bound can be derived utilizing stability property of stochastic gradient descent [34, 56]. For the OOD scenario, since we require the data shift to be via conic combination, the new words and labels in each prompt will share the positive/negative real-valued label without any self-conflict. The norm requirements and constraints on $L^{*}$ would ensure the Gaussian noise, concepts other than the co-concepts, and probability shifts have limited influence on the prediction compared with the considerable scale of coefficients by Lemma 4, laying the groundwork for the proof. ", "page_idx": 9}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we demonstrate the validity of our theoretical analysis through simulations of Algorithm 1. We use the following parameter settings in Figure 2: The parameter settings are: the length $L=4$ , the number of co-concepts $K_{1}\\,=\\,2$ , dictionary size $K=104$ , the number of test instances $n_{\\mathrm{test}}=5000$ , dimension $d_{\\mathcal{X}}=d_{\\mathcal{Y}}=1000$ , MLP width $m=50$ , feature strengths $\\|\\mathbf{u}\\|=\\|\\mathbf{q}\\|=10,\\forall k\\in[K_{1}].$ , the cosine $\\langle\\pmb{\\mu}_{k}^{+},\\pmb{\\mu}_{k}^{-}\\rangle/\\|\\mathbf{u}\\|^{2}=\\langle\\pmb{q}_{k}^{+},\\pmb{q}_{k}^{-}\\rangle/\\|\\mathbf{q}\\|^{2}=0.5$ , the initialization parameters $\\sigma_{0}\\,=\\,0.1$ , $\\sigma_{1}\\,=\\,0.01$ , and the noise deviation $\\sigma_{\\xi}\\,=\\,0.01$ . For the optimization, we use $\\lambda\\,=\\,0.002$ , $B\\,=\\,16$ , $\\gamma\\,=\\,10000$ , and the total training epochs is 100. Figure 3 (a-d) uses the same training settings, but during testing, it applies different configurations: (a) $L^{*}=5$ , (b) $L^{*}=2$ , (c) a 0.8 fraction for the first concept and a 0.2 fraction for the second concepts, and (d) $\\pmb{\\mu}_{1}^{\\pm\\,*}=a_{1}\\pmb{\\pmb{\\pmb{\\imath}}}+b_{2},\\pmb{\\mu}_{2}^{\\pm\\,*}=a_{2}\\pmb{\\pmb{\\pmb{\\imath}}}\\,b_{1}$ . Figure 2 validates our Theorem 2 and Lemma 4, which showcases the fast convergence rate and the evolution of coefficients. Figure 3 validates Proposition 1, where the learned model permits certain data shifts. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work provides the first exponential convergence analysis of 0-1 loss for transformers with softmax attention and ReLU-MLP, trained on a non-orthogonal concept-specific prompt distribution by practical cross-entropy loss. Furthermore, the results demonstrate transformers can perform certain OOD ICL tasks by leveraging the multi-concept semantic property, highlighting their innovative potential. An important future direction is to extend the analysis to more complex scenarios. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the anonymous reviewers for their instrumental comments. D.B. and H.W. are supported in part by the Research Grants Council of the Hong Kong Special Administration Region (Project No. CityU 11206622). W.H. is supported in part by JSPS KAKENHI (24K20848). A.N. is supported in part by National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative, the Centre for Frontier Artificial Intelligence Research, Institute of High Performance Computing, A\\*Star, and the College of Computing and Data Science at Nanyang Technological University. T.S. is supported in part by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2115, JPMJCR2015). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00c9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837, 2022.   \n[3] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. arXiv preprint arXiv:2401.02051, 2024.   \n[4] Fei Liu, Yiming Yao, Ping Guo, Zhiyuan Yang, Xi Lin, Xialiang Tong, Mingxuan Yuan, Zhichao Lu, Zhenkun Wang, and Qingfu Zhang. A systematic survey on large language models for algorithm design. arXiv preprint arkiv: 2410.14716, 2024.   \n[5] Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. Are emergent abilities in large language models just in-context learning? arXiv preprint arkiv: 2309.01809, 2023.   \n[6] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, 2001.   \n[7] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference, 2022.   \n[8] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023.   \n[9] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. In Proceedings of the 40th International Conference on Machine Learning, pages 19689\u201319729, 2023.   \n[10] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch. On the origins of linear representations in large language models. arXiv preprint arkiv: 2403.03867, 2024.   \n[11] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arkiv: 2311.03658, 2023.   \n[12] Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. The geometry of categorical and hierarchical concepts in large language models. arXiv preprint arkiv: 2406.01506, 2024.   \n[13] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Do llms dream of elephants (when told not to)? latent concept association and associative memory in transformers. arXiv preprint arkiv: 2406.18400, 2024.   \n[14] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arkiv: 2305.19420, 2023.   \n[15] Fabian Falck, Ziyu Wang, and Christopher C. Holmes. Are large language models bayesian? a martingale perspective on in-context learning. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024.   \n[16] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 35151\u201335174. PMLR, 2023.   \n[17] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint arkiv: 2306.09927, 2023.   \n[18] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Advances in Neural Information Processing Systems, volume 36, pages 57125\u201357211, 2023.   \n[19] Juno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape. arXiv preprint arkiv: 2402.01258, 2024.   \n[20] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arkiv: 2310.05249, 2023.   \n[21] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. In Advances in Neural Information Processing Systems, volume 36, pages 71911\u201371947, 2023.   \n[22] Yingcong Li, Yixiao Huang, Muhammed E. Ildiz, Ankit Singh Rawat, and Samet Oymak. Mechanics of next token prediction with self-attention. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238, pages 685\u2013693, 2024.   \n[23] Chenyu Zheng, Wei Huang, Rongzhen Wang, Guoqiang Wu, Jun Zhu, and Chongxuan Li. On mesa-optimization in autoregressively trained transformers: Emergence and capability. arXiv preprint arXiv:2405.16845, 2024.   \n[24] Shokichi Takakura and Taiji Suzuki. Approximation and estimation ability of transformers for sequence-to-sequence functions with infinite dimensional input. arXiv preprint arkiv: 2305.18699, 2023.   \n[25] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multihead softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arkiv: 2402.19442, 2024.   \n[26] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. Transformers provably learn featureposition correlations in masked image modeling. arXiv preprint arkiv: 2403.02233, 2024.   \n[27] Hongkang Li, Meng Wang, Sijia Liu, and Pin yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015, 2023.   \n[28] Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. How do nonlinear transformers learn and generalize in in-context learning? arXiv preprint arkiv: 2402.15607, 2024.   \n[29] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 11112\u201311122. PMLR, 2021.   \n[30] Patrik Reizinger, Szilvia Ujv\u00e1ry, Anna M\u00e9sz\u00e1ros, Anna Kerekes, Wieland Brendel, and Ferenc Husz\u00e1r. Position: Understanding LLMs requires more than statistical generalization. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 42365\u201342390, 2024.   \n[31] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27(6):1808\u20131829, 1999.   \n[32] Pascal Massart and \u00c9lodie N\u00e9d\u00e9lec. Risk Bounds for Statistical Learning. The Annals of Statistics, 34(5):2326 \u2013 2366, 2006.   \n[33] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing error for stochastic gradient methods. In Proceedings of the 31st Conference On Learning Theory, volume 75, pages 250\u2013296, 2018.   \n[34] Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent with exponential convergence rates of expected classification errors. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89, pages 1417\u20131426, 2019.   \n[35] Vivien A Cabannes, Francis Bach, and Alessandro Rudi. Fast rates for structured prediction. In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 823\u2013865. PMLR, 15\u201319 Aug 2021.   \n[36] Shingo Yashima, Atsushi Nitanda, and Taiji Suzuki. Exponential convergence rates of classification errors on learning with sgd and random features. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130, pages 1954\u20131962, 2021.   \n[37] Kazusato Oko, Taiji Suzuki, Atsushi Nitanda, and Denny Wu. Particle stochastic dual coordinate ascent: Exponential convergent algorithm for mean field neural network optimization. In International Conference on Learning Representations, 2022.   \n[38] Stefano Vigogna, Giacomo Meanti, Ernesto De Vito, and Lorenzo Rosasco. Multiclass learning with Margin: Exponential rates with no bias-variance trade-off. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 22260\u201322269. PMLR, 17\u201323 Jul 2022.   \n[39] Vivien Cabannnes and Stefano Vigogna. A case of exponential convergence rates for svm. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206, pages 359\u2013374. PMLR, 25\u201327 Apr 2023.   \n[40] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[41] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overftiting in two-layer convolutional neural networks. In Advances in Neural Information Processing Systems, volume 35, pages 25237\u201325250, 2022.   \n[42] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overftiting in two-layer reLU convolutional neural networks. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 17615\u201317659, 2023.   \n[43] Xuran Meng, Difan Zou, and Yuan Cao. Benign overfitting in two-layer relu convolutional neural networks for XOR data. arXiv preprint arkiv: 2310.01975, 2023.   \n[44] Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overftiting and grokking in reLU networks for XOR cluster data. arXiv preprint arkiv: 2310.02541, 2023.   \n[45] Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably benefit from structural information: A feature learning perspective. arXiv preprint arkiv: 2306.13926, 2023.   \n[46] Hiroaki Yamagiwa, Momose Oyama, and Hidetoshi Shimodaira. Discovering universal geometry in embeddings with ica. arXiv preprint arkiv: 2305.13175, 2023.   \n[47] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In Proceedings of the 38th International Conference on Machine Learning, pages 11112\u201311122, 2021.   \n[48] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji. Word embeddings are steers for language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16410\u201316430, 2024.   \n[49] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, page 50\u201357, 1999.   \n[50] L\u00c9on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.   \n[51] Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime. arXiv preprint arkiv: 2006.12297, 2021.   \n[52] Yiwen Kou, Zixiang Chen, Yuan Cao, and Quanquan Gu. How does semi-supervised learning with pseudo-labelers work? a case study. In The Eleventh International Conference on Learning Representations, 2023.   \n[53] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. In The Eleventh International Conference on Learning Representations, 2023.   \n[54] Tong Yang, Yu Huang, Yingbin Liang, and Yuejie Chi. In-context learning with representations: Contextual generalization of trained transformers. arXiv preprint arkiv: 2408.10147, 2024.   \n[55] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, 22(4):1679\u20131706, 1994.   \n[56] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 1225\u20131234, 2016.   \n[57] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The beneftis of mixup for feature learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 43423\u201343479, 2023.   \n[58] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust linear classification. In Robin J. Evans and Ilya Shpitser, editors, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216, pages 313\u2013323, 2023.   \n[59] Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overftiting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178, pages 2668\u20132703, 2022.   \n[60] Spencer Frei, Gal Vardi, Peter Bartlett, and Nathan Srebro. Benign overftiting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. In Proceedings of Thirty Sixth Conference on Learning Theory, volume 195, pages 3173\u20133228, 2023.   \n[61] Yiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit Bias of Gradient Descent for Twolayer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data. In Advances in Neural Information Processing Systems, volume 36, pages 30167\u201330221. Curran Associates, Inc., 2023.   \n[62] Wei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In The Twelfth International Conference on Learning Representations, 2024.   \n[63] Dake Bu, Wei Huang, Taiji Suzuki, Ji Cheng, Qingfu Zhang, Zhiqiang Xu, and Hau-San Wong. Provably neural active learning succeeds via prioritizing perplexing samples. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 4642\u20134695, 2024.   \n[64] Yiwen Kou, Zixiang Chen, Quanquan Gu, and Sham M. Kakade. Matching the statistical query lower bound for k-sparse parity problems with stochastic gradient descent. arXiv preprint arkiv: 2404.12376, 2024.   \n[65] Alexander Tsigler. Benign Overftiting in Linear Regression and Classification. PhD thesis, UC Berkeley, 2024.   \n[66] Junhyung Park, Patrick Bloebaum, and Shiva Prasad Kasiviswanathan. Benign overfitting for regression with trained two-layer relu networks. arXiv preprint arkiv: 2410.06191, 2024.   \n[67] Eshaan Nichani, Alex Damian, and Jason D. Lee. Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. In Advances in Neural Information Processing Systems, volume 36, pages 10828\u201310875, 2023.   \n[68] Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns lowdimensional polynomials with sgd near the information-theoretic limit. arXiv preprint arkiv:2406.01581, 2024.   \n[69] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations. arXiv preprint arkiv:2406.11828, 2024.   \n[70] Yunwei Ren and Jason D. Lee. Learning orthogonal multi-index models: A fine-grained information exponent analysis. arXiv preprint arkiv:2410.09678, 2024.   \n[71] Spencer Frei and Gal Vardi. Trained transformer classifiers generalize and exhibit benign overfitting in-context. arXiv preprint arXiv:2410.01774, 2024.   \n[72] Wei Shen, Ruida Zhou, Jing Yang, and Cong Shen. On the training convergence of transformers for in-context classification. arXiv preprint arXiv:2410.11778, 2024.   \n[73] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Shaolei Du. JoMA: Demystifying multilayer transformers via joint dynamics of MLP and attention. In The Twelfth International Conference on Learning Representations, 2024.   \n[74] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. How transformers learn diverse attention correlations in masked vision pretraining. arXiv preprint arkiv: 2403.02233, 2024.   \n[75] Masahiro Sakamoto and Hitomi Sato. Benign or not-benign overfitting in token selection of attention mechanism. arXiv preprint arXiv:2409.17625, 2024.   \n[76] Roey Magen, Shuning Shang, Zhiwei Xu, Spencer Frei, Wei Hu, and Gal Vardi. Benign overfitting in single-head attention. arXiv preprint arXiv:2410.07746, 2024.   \n[77] Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with gradient descent. arXiv preprint arkiv: 2402.14735, 2024.   \n[78] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction heads: Provable training dynamics and feature learning in transformers. arXiv preprint arkiv: 2409.10559, 2024.   \n[79] Hongru Yang, Bhavya Kailkhura, Zhangyang Wang, and Yingbin Liang. Training dynamics of transformers to recognize word co-occurrence via gradient flow analysis. arXiv preprint arkiv:2410.09605, 2024.   \n[80] Jiarui Jiang, Wei Huang, Miao Zhang, Taiji Suzuki, and Liqiang Nie. Unveil benign overftiting for transformer in vision: Training dynamics, convergence, and generalization. arXiv preprint arXiv:2409.19345, 2024.   \n[81] Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, and Jianfei Chen. On the optimization and generalization of two-layer transformers with sign gradient descent. arXiv preprint arXiv:2410.04870, 2024.   \n[82] Yoshua Bengio. Learning Deep Architectures for AI. Now Publishers Inc, 2009. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[83] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. ", "page_idx": 15}, {"type": "text", "text": "[84] Zeyuan Allen-Zhu and Yuanzhi Li. Backward Feature Correction: How Deep Learning Performs Deep Learning. In Conference on Learning Theory, COLT \u201923, 2023. Full version available at http://arxiv.org/abs/2001.04413.   \n[85] Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, and Hidenori Tanaka. Emergence of hidden capabilities: Exploring learning dynamics in concept space. arXiv preprint arXiv:2406.19370, 2024.   \n[86] Yongyi Yang, Core Francisco Park, Ekdeep Singh Lubana, Maya Okawa, Wei Hu, and Hidenori Tanaka. Dynamics of concept learning and compositional generalization. arXiv preprint arXiv:2410.08309, 2024.   \n[87] Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P. Xing, Yuejie Chi, and Kun Zhang. Learning discrete concepts in latent hierarchical models. arXiv preprint arkiv: 2406.00519, 2024.   \n[88] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 1, Learning Hierarchical Language Structures. ArXiv e-prints, abs/2305.13673, May 2023. Full version available at http://arxiv.org/abs/2305.13673.   \n[89] Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction. arXiv preprint arkiv: 2303.07971, 2023.   \n[90] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge university press, 2012.   \n[91] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[92] Miao Lu, Beining Wu, Xiaodong Yang, and Difan Zou. Benign oscillation of stochastic gradient descent with large learning rate. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023.   \n[93] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Springer Science & Business Media, 2013.   \n[94] Karan Girotra, Lennart Meincke, Christian Terwiesch, and Karl T. Ulrich. Ideas are dimes a dozen: Large language models for idea generation in innovation. SSRN, 2023.   \n[95] Anil Rajnikant Doshi and Oliver Hauser. Generative artificial intelligence enhances creativity but reduces the diversity of novel content. SSRN, 2023.   \n[96] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large language model. arXiv preprint arkiv: 2311.15249, 2023.   \n[97] Yiming Yao, Fei Liu, Ji Cheng, and Qingfu Zhang. Evolve cost-aware acquisition functions using large language models. arXiv preprint arkiv: 2404.16906, 2024.   \n[98] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. An example of evolutionary computation $^+$ large language model beating human: Design of efficient guided local search. arXiv preprint arkiv: 2401.02051, 2024. ", "page_idx": 15}, {"type": "text", "text": "A Limitation and Broader Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The theoretical analysis provided in this work introduces novel perspectives on optimization and generalization, but the data model employed may require additional refinements to better align with practical scenarios, such as adding more layers of attention. The techniques and findings can inform future empirical and theoretical explorations of transformer architectures, though we do not foresee a direct social impact arising from the theoretical advancements presented. ", "page_idx": 16}, {"type": "text", "text": "B Additional Experiment Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We implement our methods using PyTorch, ensuring consistent software and hardware environments. Specifically, the experiments are run on Linux servers with NVIDIA A100 graphics cards and CUDA 11.2, and can be completed within one hour. ", "page_idx": 16}, {"type": "text", "text": "C Additional Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theory of Convergence Rate of Stochastic Gradient Descent. Our analysis of the exponential convergence rate for the 0-1 loss builds upon a rich body of prior work. In the context of classification, the faster convergence rate mostly based on the excess of risk with some power of the essential supremum norm. Specifically, [31, 32] introduce the Hard low-noise condition over the margin. When there is a hard margin separating the classes, the test error can exhibit exponentially fast convergence as the number of training samples increases, even when the surrogate loss error only decreases polynomially. This phenomenon has been further explored in more recent studies. [33, 34, 35, 36, 37] have analyzed the exponential convergence of stochastic gradient descent under various settings. Meanwhile, [35] have investigated hard-margin and exponential rates in the context of structured prediction, which encompasses traditional classification as a special case. Besides, recent work also obtain the exponential rates in generalized settings such as Multi-class classification [38] and SVM [39]. Building upon this rich theoretical foundation, our work derives the first exponential convergence analysis for the 0-1 loss in the specific setting of transformer models with softmax attention and ReLU-activated MLP over the sparse coding data model, whose surrogate loss function is the cross-entropy loss. ", "page_idx": 16}, {"type": "text", "text": "Theory of Feature Learning of GD-updated Neural Network. A rich body of recent learning theory research has focused on the feature direction\u2019 recovery view of neural network representations [40, 41, 57, 53, 45, 58, 42, 52, 43, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]. Rather than directly examining the evolution of the 0-1 loss, this line of work explicitly studies the process of reconstruction of the data\u2019s feature directions and memorization of disrupted noise in the network\u2019s latent space as surrogate metrics. While most studies in this area have assumed (near) orthogonal data, recent efforts by [43] and [44] have made initial attempts to analyze non-orthogonal data scenarios. Building upon this foundation, our study extends this line of research to the case of nonlinear attention-MLP transformers with within-concept positive inner products and cross-concept orthogonal data representations. The key to our analysis is assuming good initialization of attention matrices and a sufficiently low-noise condition in the SGD setting, where noise has only a mild impact on shaping neural network matrices or influencing gradient flow. ", "page_idx": 16}, {"type": "text", "text": "Theory of Transformers and In-Context Learning. The literature on Transformers and ICL is wide-ranging, and we will selectively address the most relevant ones. Prior studies have analyzed how transformers learn topic/concept semantics [9], the origins and biases of LLM representations using latent variable models [10], and ICL from a model averaging perspective [14]. However, these works do not connect the geometric properties of concept-encoded representations to transformers\u2019 powerful ICL abilities. Another line of research has studied the learning dynamics of transformer, including analyses of linear-attention transformers [16, 17, 71, 72], QKcombined attention-only models [21, 73, 20, 26, 74, 75, 76, 77, 78, 54, 79], ReLU-free MLP [80, 81, 54] or without MLP [17, 25], impractical squared or hinge loss [25, 26, 27, 28]. Though relevant, these works rely on simplifications or do not connect the observed linear semantic representation of large model to the transformer\u2019s excelling OOD capability. ", "page_idx": 16}, {"type": "text", "text": "Concept Learning in Deep Learning. Hierarchical learning has long been regarded as a key factor behind the success of deep learning [82, 83, 84]. Recent research shows that large-scale generative models, such as diffusion models and transformers, effectively encode hierarchical concepts in their latent spaces [11, 12, 13, 46, 85, 86, 87]. Moreover, [88, 89, 73] show that transformers can capture hierarchical and compositional structures in data. From a Bayesian perspective, [14, 7, 8] interpret ICL as LLMs predicting outputs based on latent (concept) variable inference. Furthermore, studies reveal a linear structure in LLMs\u2019 latent space over independent interpretable concepts: representations of the same concept exhibit positive inner products, while different concepts are nearly orthogonal [9, 10, 11, 12]. Building on these insights, we explore in a theoretical context how the compositional nature of concept representations relates to transformers\u2019 ability to generalize to OOD tasks through a sparse coding modeling. We believe our OOD results are not only coincides with the transformer\u2019s compositional generalization ability on language tasks [89], but also consistent with other concept learning outcomes of diffusion and multi-model model: [87] shows that adjusting the length of semantic representations can directly affect image generation behaviors (see Figure 5), while [86] reveals that compositing different concepts enables OOD generalization (e.g. \u201cblue square apples\u201d in the Figure 1a in [86]). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D Preliminary Lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Probablistic Lemmas on Concentration ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 6. Suppose that \u03b4 > 0 and \u2200d \u2208{dX , dY} = \u2126(log(K\u03b4NL , where $N\\,=\\,B T^{*}$ . Then with probability at least $1-\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\sigma_{\\xi}^{2}d}{2}\\leq\\|\\xi_{i}\\|_{2}^{2}\\leq3\\frac{\\sigma_{\\xi}^{2}d}{2},}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\quad|\\langle\\xi_{i},\\xi_{i^{\\prime}}\\rangle|\\leq2\\sigma_{\\xi}^{2}\\cdot\\sqrt{d\\log\\left(\\frac{6(N(L+1))^{2}}{\\delta}\\right)},}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\quad|\\langle\\xi_{i},\\mu\\rangle|\\leq\\|\\mu\\|_{2}\\sigma_{\\xi}\\cdot\\sqrt{2\\log(\\frac{6K N(L+1)}{\\delta})}}\\\\ &{\\displaystyle\\xi_{i},\\xi_{i}^{\\prime}\\sim\\mathcal{D}_{\\xi_{\\alpha}}\\big(o r\\,\\mathcal{D}_{\\xi_{y}}\\big),\\mu\\in\\mathcal{D}_{x}\\big(o r\\,\\mathcal{D}_{y}\\big),l\\in\\{1,2\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. See Lemma B.4 in [42] for a proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma 7. Suppose that $\\delta>0$ , $d y=\\Omega(\\log(m/\\delta)),m=\\Omega(\\log(K/(\\delta)))$ . Then with probability at least $1-\\delta,f o r\\,\\forall i\\in[m],k\\in[K_{1}],w\\in[d_{\\mathcal{V}}-K_{1}]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{1}^{2}d_{\\mathcal{V}}}{2}\\leq\\|\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(\\O^{0})\\|^{2}\\leq3\\frac{\\sigma_{1}^{2}d_{\\mathcal{V}}}{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{|\\alpha_{O_{(i,.),k}}^{(0)}|}{\\|c_{k}\\|},\\frac{|\\beta_{O_{(i,.),k}}^{(0)}|}{\\|d_{k}\\|},|\\rho_{O_{(i,.),w}}^{(0)}|\\leq\\sqrt{2\\log(\\frac{5K m}{\\delta})}\\cdot\\sigma_{1},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{1}/2\\le\\operatorname*{max}_{i\\in[m]}\\{\\frac{|\\alpha_{O_{(i,\\cdot)},k}^{(0)}|}{\\|c_{k}\\|},\\frac{|\\beta_{O_{(i,\\cdot)},k}^{(0)}|}{\\|d_{k}\\|},|\\rho_{O_{(i,\\cdot)},w}^{(0)}|\\}\\le\\sqrt{2\\log(\\frac{5K m}{\\delta})}\\cdot\\sigma_{1},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, for some $\\zeta\\in(0,1]\\,f o r\\,\\forall e\\neq e^{\\prime},\\in[\\pm],\\,\\exists\\omega_{\\zeta}\\in(0,\\omega_{\\zeta}^{\\prime})$ where $\\omega_{\\zeta}^{\\prime}<1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\left\\{\\bar{t}\\in[m]\\;|\\;r_{1}=\\frac{e}{m},\\alpha_{0(c_{i+1},k}^{(0)}+e^{\\gamma}\\beta_{0(c_{i+1},k}^{(0)},\\bar{x}>0\\right\\}-\\frac{m}{4}\\right\\lvert\\;\\le\\sqrt{\\frac{m\\log(10K_{1}/\\bar{\\beta})}{2}}\\right\\},}\\\\ &{\\left\\lVert\\bar{t}\\in[m]\\;|\\;\\alpha_{0(c_{i+1},k)}^{(0)}+e^{\\gamma}\\beta_{0(c_{i+1},k)}^{(0)}>0,e^{\\gamma}\\beta_{0(c_{i+1},k)}^{(0)}>0\\right\\}-\\frac{m}{4}\\right\\rVert\\le\\sqrt{\\frac{m\\log(10K_{1}/\\bar{\\beta})}{2}},}\\\\ &{\\left\\lVert\\left\\{\\bar{t}\\in[m]\\;|\\;r_{1}=\\frac{e}{m},\\alpha_{0(c_{i+1},k)}^{(0)}+\\hat{\\mathcal{L}}\\beta_{0(c_{i+1},k)}^{(0)}>0\\right\\}-\\frac{(1+m)m}{8}\\right\\rVert\\le\\sqrt{\\frac{m\\log(10K_{1}/\\bar{\\beta})}{2}}\\le\\frac{(\\alpha_{\\xi}^{\\prime}-\\alpha_{\\xi})m}{8},}\\\\ &{\\left\\lVert\\left\\{\\bar{t}\\in[m]\\;|\\;r_{1}=\\frac{e}{m},\\alpha_{0(c_{i+1},k)}^{(0)}+\\hat{\\mathcal{L}}\\beta_{0(c_{i+1},k)}^{(0)}>0,\\alpha_{0(c_{i+1},k}^{(0)}-\\hat{\\mathcal{L}}\\beta_{0(c_{i+1},k)}^{(0)}<0\\right\\}-\\frac{(1-u)m}{8}\\right\\rVert\\le\\sqrt{\\frac{m\\log(10\\right)}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\left\\{\\alpha_{0(c_{i+1},k)}^{(0)}+\\hat{\\mathcal{L}}\\beta_{0(c_{i+1},k)}^{(0)}>0\\right\\}}\\\\ &{\\qquad+\\epsilon\\epsilon(\\epsilon|\\omega|)|\\alpha_{0}\\frac{\\varepsilon_{\\xi}^{(0)}-\\beta_{0(c_{i+1},k)}^{(0)}+\\epsilon^{\\gamma}\\beta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In addition, for a sufficient large $m\\,=\\,\\Omega(\\log(K/(\\delta))/(1-\\omega_{\\zeta}))$ the lower bound inequalities regarding maximum value in Eq.(7) hold at any above index set of i in Eq.(8). For example, there exist $i\\in\\{i\\in[m]$ | ", "page_idx": 17}, {"type": "text", "text": "Proof. First, notice that $\\mathbf{W}_{O_{(i,\\cdot)}}^{\\pmb{y}}(0)\\ \\sim\\ \\mathcal{N}(\\mathbf{0},\\sigma_{1}\\mathbb{I}_{d_{\\mathcal{Y}}})$ , then by Bernstein\u2019s inequality as well as $d y\\ =$ $\\Omega(\\log(m/\\delta))$ , with probability at least $1-\\delta/(5m)$ , for $\\forall i\\in[m]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n||\\mathbf{W}_{O_{(i,\\cdot)}}^{y}{}^{(0)}||^{2}-\\sigma_{1}d y|\\leq O(\\sigma_{1}^{2}\\cdot\\sqrt{d_{\\mathcal{Y}}\\log(5m/\\delta)})\\leq\\sigma_{1}^{2}d_{\\mathcal{Y}}/2.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By union bound we can have the first inequality in the lemma hold with probability at least $1-\\delta/5$ . Next, we notice that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{O_{(i,.,),k}}^{(0)}}{\\|c_{k}\\|}=\\langle\\mathbf{W}_{O_{(i,.)}}^{y}(0),\\frac{c_{k}}{\\|c_{k}\\|}\\rangle,\\quad\\frac{\\beta_{O_{(i,.),k}}^{(0)}}{\\|d_{k}\\|}=\\langle\\mathbf{W}_{O_{(i,.)}}^{y}(0),\\frac{d_{k}}{\\|d_{k}\\|}\\rangle,\\quad\\rho_{O_{(i,.),w}}^{(0)}=\\langle\\mathbf{W}_{O_{(i,.)}}^{y}(0),\\underline{{\\mathbf{q}}}_{w}^{\\perp}\\rangle\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "are all Gaussian random variable with mean 0 and variance $\\sigma_{1}^{2}$ . Then by Gaussian tail bound and union bound, with probability at least $1-\\delta/10$ , for all $i\\in[m]$ and $\\mathbf{q}\\in\\bigcup_{k,w}\\{\\frac{c_{k}}{\\|c_{k}\\|},\\frac{d_{k}}{\\|d_{k}\\|},q_{w}^{\\perp}\\}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\langle\\mathbf{W}_{O_{(i,\\cdot)}}^{y}{}^{(0)},\\mathbf{q}\\rangle\\right|\\leq\\sqrt{2\\log(5K m/\\delta)}\\cdot\\sigma_{1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice $\\mathbb{P}(\\sigma_{1}/2>|\\langle\\mathbf{W}_{O_{(i,\\cdot)}}^{\\pmb{y}}{}^{(0)},\\mathbf{q}\\rangle|)$ is an positive constant, then following the techniques of Lemma B.5 in [42] and the condition $m=\\Omega(\\log(K/\\delta))$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\sigma_{1}/2\\leq|\\langle\\mathbf{W}_{O_{(i,\\cdot)}}^{y}\\mathbf{\\Phi}^{(0)},\\mathbf{q}\\rangle|)=1-\\mathbb{P}(\\sigma_{1}/2>\\operatorname*{max}\\{|\\langle\\mathbf{W}_{O_{(i,\\cdot)}}^{y}\\mathbf{\\Phi}^{(0)},\\mathbf{q}\\rangle|\\}),}\\\\ &{\\phantom{\\quad=1-\\mathbb{P}(\\sigma_{1}/2>|\\langle\\mathbf{W}_{O_{(i,\\cdot)}}^{y}\\mathbf{\\Phi}^{(0)},\\mathbf{q}\\rangle|)^{m K}}=1-\\mathbb{P}(\\sigma_{1}/2>|\\langle\\mathbf{W}_{O_{(i,\\cdot)}}^{y}\\mathbf{\\Phi}^{(0)},\\mathbf{q}\\rangle|)^{m K}}\\\\ &{\\phantom{\\quad=1-\\mathbb{P}(\\sigma_{1}/2)}\\geq1-\\delta/10,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then with probability $1-\\delta/5$ , the second and third inequality hold. ", "page_idx": 18}, {"type": "text", "text": "For $\\zeta\\ \\in\\ (0,1]$ , we see that the variable $\\alpha_{O_{(i,.)},k}^{(0)}+e^{\\prime}\\zeta\\beta_{O_{(i,.)},k}^{(0)}\\,\\sim\\,\\mathcal{N}(0,\\sigma_{1}^{2}(\\|c_{k}\\|^{2}+\\zeta\\|d_{k}\\|^{2}))$ , and it\u2019s independent to the event $\\{\\mathbf{r}_{i}\\;=\\;\\frac{e}{m}\\},\\forall e\\;\\in\\;[\\pm]$ . Therefore, we can see the count of $\\{i\\;\\in\\;[m]\\;\\mid\\;{\\bf r}_{i}\\;=\\;$ $\\frac{e}{m},\\alpha_{O_{(i,.)},k}^{(0)}+e^{\\prime}\\zeta\\beta_{O_{(i,.)},k}^{(0)}>0,e^{\\prime}\\zeta\\beta_{O_{(i,.)},k}^{(0)}>0\\}$ as a binomial variable with $p=1/4,n=m$ , then by the property of binomial tail, condition $m=\\Omega(\\log(K/(\\delta)))$ as well as Hoeffding\u2019s inequality, with probability at least $1-\\delta/5$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\frac{|\\{i\\in[m]\\;|\\;{\\bf r}_{i}=\\frac{e}{m},\\alpha_{O_{(i,\\cdot)},k}^{(0)}+e^{\\prime}\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0\\}|}{m}-\\frac{1}{4}|\\leq\\sqrt{\\frac{\\log(10K_{1}/\\delta)}{2m}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which completes the proof of the forth inequality. Similarly, for the fifth inequality we can utilize the same techniques to derive that it holds with probability at least $1-\\delta/5$ . ", "page_idx": 18}, {"type": "text", "text": "For the event $\\lbrace i\\in[m]\\mid\\mathbf{r}_{i}=\\frac{e}{m},\\alpha_{O_{(i,\\cdot)},k}^{(0)}\\pm\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0\\rbrace$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathbf{r}_{i}=\\frac{e}{m},\\alpha_{O_{(i,\\cdot)},k}^{(0)}\\pm\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0)=\\mathbb{P}(\\alpha_{O_{(i,\\cdot)},k}^{(0)}+e^{\\prime}\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\cdot\\mathbb{P}(\\alpha_{O_{(i,\\cdot)},k}^{(0)}-e^{\\prime}\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0\\mid\\alpha_{O_{(i,\\cdot)},k}^{(0)}+e^{\\prime}\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{1}{2}\\cdot\\mathbb{P}(\\alpha_{O_{(i,\\cdot)},k}^{(0)}-e^{\\prime}\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0\\mid\\alpha_{O_{(i,\\cdot)},k}^{(0)}+e^{\\prime}\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{1}{2}\\cdot\\frac{1+\\omega_{\\zeta}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\frac{1+\\omega_{\\zeta}}{2}$ $\\lbrace\\alpha_{O_{(i,.,)},k}^{(0)}\\,-\\,e^{\\prime}\\zeta\\beta_{O_{(i,.,)},k}^{(0)}\\ >\\ 0\\ \\ \\vert\\ \\ \\alpha_{O_{(i,.)},k}^{(0)}\\ +$ $e^{\\prime}\\zeta\\beta_{O_{(i,\\,.\\,)},k}^{(0)}\\;>\\;0\\}$ $\\omega_{\\zeta}~>~0$ $\\alpha_{O_{(i,\\cdot)},k}^{(0)}$ $e^{\\prime}\\zeta\\beta_{{\\cal O}_{(i,\\,*)},k}^{(0)}$ \u03b2O(i,\u00b7),k. We denote the probability with $\\omega_{\\zeta}$ since the true value is hard to compute. Subsequently, the event $\\{i\\in[m]\\mid\\mathbf{r}_{i}=$ $\\frac{e}{m},\\alpha_{O_{(i,\\cdot)},k}^{(0)}\\pm\\zeta\\beta_{O_{(i,\\cdot)},k}^{(0)}>0\\}$ can be seen as a binomial variable with $p=\\frac{1+\\omega_{\\zeta}}{8},n=\\dot{m}$ , then we can have the sixth inequality hold with probability at least $1-\\delta/5$ , utilizing the property of binomial tail, condition $m=\\Omega(\\log(K/(\\bar{\\delta})))$ as well as Hoeffding\u2019s inequality. ", "page_idx": 18}, {"type": "text", "text": "The seventh inequality is a natural inference of the third and forth inequality, where the $m=\\Omega(\\log(K_{1}/\\delta))$ ensure $\\sqrt{m\\log(10K_{1}/\\delta)/2}\\le m/16$ , and the last inequality is then also a natural inference of the third and fifth inequality. ", "page_idx": 18}, {"type": "text", "text": "Therefore, by union bound, the proof is completed. ", "page_idx": 18}, {"type": "text", "text": "D.2 Matrix Theories ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 8. (1.1.P5 in [90]) Let $A\\in M_{n}$ be idempotent, that is, $A^{2}=A$ . Then, each eigenvalue of $A$ equals to the rank of $A$ , which is either $\\boldsymbol{O}$ or 1. Beside, identity matrix I is the only nonsingular idempotent matrix. ", "page_idx": 18}, {"type": "text", "text": "Lemma 9. For a matrix $\\begin{array}{r}{A=\\sum_{i=1}^{d}\\mu_{i}P_{i}}\\end{array}$ , where $P_{i}$ are symmetric idempotent matrices with ran $\\mathfrak{s}(P_{i})=1$ , and thus $\\textstyle\\sum_{i=1}^{d}\\mu_{i}P_{i}$ is the idempotent decomposition of matrix $A$ by $P_{i}$ . Then we see that $\\|A\\|_{F}={\\sqrt{\\operatorname{tr}(A^{T}A)}}=$ $\\begin{array}{r}{\\sqrt{\\sum_{i=1}^{d}\\mu_{i}^{2}}=\\sqrt{\\sum_{i=1}^{d}\\lambda_{i}^{2}}}\\end{array}$ , where $\\lambda_{i}$ are eigenvalues of $A$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. By definition, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\boldsymbol{A}^{T}\\boldsymbol{A}=\\sum_{i=1}^{d}\\mu_{i}^{2}P_{i}^{T}P_{i}=\\sum_{i=1}^{d}\\mu_{i}^{2}P_{i}P_{i}=\\sum_{i=1}^{d}\\mu_{i}^{2}P_{i}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, by Lemma 8 we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{tr}(A^{T}A)=\\mathrm{tr}(\\sum_{i=1}^{d}\\mu_{i}^{2}P_{i})=\\sum_{i=1}^{d}\\mu_{i}^{2}\\,\\mathrm{tr}(P_{i})=\\sum_{i=1}^{d}\\mu_{i}^{2}\\,\\mathrm{rank}(P_{i})=\\sum_{i=1}^{d}\\mu_{i}^{2}=\\sum_{i=1}^{d}\\lambda_{i}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D.3 ODE Systems ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 10. (Lemma C.1 in $[43J)$ . Suppose that a sequence $a_{t},t\\geq0$ follows the iterative formula ", "page_idx": 19}, {"type": "equation", "text": "$$\na_{t+1}=a_{t}+\\frac{c}{1+b e^{a_{t}}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $0\\leq c\\leq1$ and $b\\geq0$ . Then it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\nx_{t}\\le a_{t}\\le\\frac{c}{1+b e^{a_{0}}}+x_{t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $t\\geq0$ . Here, $x_{t}$ is the unique solution of ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}x_{t}}{\\mathrm{d}t}}={\\frac{c}{1+b e^{x_{t}}}},\\quad x_{0}=a_{0}\\Leftrightarrow x_{t}+b e^{x_{t}}=c t+a_{0}+b e^{a_{0}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 11. (Coupled ODE System $^{\\,l}$ ). Suppose that there are two coupled sequences $y_{t}$ , $z_{t}$ , $t\\geq0$ follows the iterative formula ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{y_{t+1}=y_{t}+a z_{t}y_{t}\\displaystyle\\frac{1}{2+e^{-2y_{t}\\mathrm{}^{2}}+e^{2y_{t}\\mathrm{}^{2}}},\\qquad\\qquad}&&{y_{0}>0,\\qquad\\qquad}&{a>0,}\\\\ &{z_{t+1}=z_{t}+b,\\qquad\\qquad}&&{z_{0}<0,\\qquad\\qquad}&{b>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $a,b\\geq0$ . Then it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\ny(t)\\leq y_{t},\\quad z(t)=z_{t},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $t\\geq0$ . Here, $y(t),\\,z(t)$ are the unique solutions of the following ODE System respectively ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{y^{\\prime}(t)=\\displaystyle\\frac{a}{4}z(0)y(t),}}&{{y(0)=y_{0},}}\\\\ {{z^{\\prime}(t)=b,}}&{{z(0)=z_{0}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As such, for $t_{1}=\\operatorname*{min}\\{t\\in\\mathbb{Z}\\mid z_{t}\\geq0\\}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\ny_{t_{1}}\\geq y(0)e^{\\frac{-a z\\left(0\\right)^{2}\\left(1+e^{-2y(0)^{2}}\\right)}{4b\\left(1-e^{-2y(0)^{2}}\\right)}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and $t_{1}\\geq\\frac{-z(0)(1+e^{-2y(0)^{2}})}{b(1-e^{-2y(0)^{2}})}.$ ", "page_idx": 19}, {"type": "text", "text": "Proof. From the condition we see that $z_{0}<0$ and $z_{t}$ is an increasing sequence $\\left\\langle z_{t}\\geq z_{0}\\right\\rangle$ ). Besides, as $y_{0}>0$ , during the period where $z_{t}\\leq0$ , we see that $y_{t}$ is monotonically decreasing. Then by $(2+e^{-2{y_{t}}^{2}}+e^{2{y_{t}}^{2}})^{-1}\\leq$ $1/4$ as well as Comparison Theorem, it\u2019s obvious that the continuous coupled ODE in Eq.(9) is the lower bound of $y_{t}$ . Then one can readily obtain the result by solving the ODE. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 12. (Coupled ODE System 2). Suppose that there are two coupled sequences $y_{t},~z_{t}$ , which are the sequences after $t_{1}$ in Lemma $_{l l}$ , and $t\\geq t_{1}$ follows the iterative formula ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{y_{t+1}=y_{t}+a z_{t}y_{t}\\displaystyle\\frac{1}{2+e^{-2y_{t}\\mathrm{\\scriptsize~2}}+e^{2y_{t}\\mathrm{\\scriptsize~2}}}\\ell_{t}^{\\prime},}&&{\\qquad}&{y_{t_{1}}>0,}&{\\qquad}&{a>0,}\\\\ &{z_{t+1}=z_{t}+b\\displaystyle\\frac{1-e^{-2y(t)^{2}}}{1+e^{-2y(t)^{2}}}\\ell_{t}^{\\prime},}&&{\\qquad}&{z_{t_{1}}\\geq0,}&{\\qquad}&{b>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $a,b\\ge0.$ , and $c^{\\prime}\\leq\\ell_{t}^{\\prime}\\leq1$ . Then it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{y}}(t)\\leq y_{t}\\leq\\overline{{y}}(t),\\quad\\underline{{z}}(t)\\leq z_{t}\\leq\\overline{{z}}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}(\\mathrm{Ei}(2\\underline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\underline{{y}}(t)^{2})+4\\log(\\underline{{y}}(t)))=a b c^{\\prime}\\frac{1}{1+e^{-2y(t_{1})^{2}}}\\frac{\\left(t-t_{1}\\right)^{2}}{2}+\\frac{1}{2}(\\mathrm{Ei}(2y_{t_{1}}^{\\top2})+\\mathrm{Ei}(-2y_{t_{1}}^{\\top2}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+4\\log(y_{t_{1}}),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\underline{{z}}(t)=b c^{\\prime}\\frac{1}{1+e^{-2y(t_{1})^{2}}}(t-t_{1}),}\\\\ &{\\frac{1}{2}(\\mathrm{Ei}(2\\overline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\overline{{y}}(t)^{2})+4\\log(\\overline{{y}}(t)))=\\frac{a b\\left(t-t_{1}\\right)^{2}}{2}+\\frac{1}{2}(\\mathrm{Ei}(2y_{t_{1}}^{\\top2})+\\mathrm{Ei}(-2y_{t_{1}}^{\\top}))+4\\log(y_{t_{1}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\frac{\\overline{{z}}(t)=b\\left(t-t_{1}\\right)}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{Ei}(x)=\\int_{-\\infty}^{x}{\\frac{e^{t}}{t}}\\mathrm{d}t=\\gamma_{E u l e r}+\\ln x+\\exp(x/2)\\sum_{n=1}^{\\infty}{\\frac{(-1)^{n-1}x^{n}}{n!2^{n-1}}}\\sum_{k=0}^{\\lfloor(n-1)/2\\rfloor}{\\frac{1}{2k+1}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We see that as $z_{t}\\ge0,t\\ge t_{1}$ , the $y_{t}$ is monotonically increasing. As such, by Comparison Theorem we see that the upper and lower bound of the coupled system would depends on 11+\u2212ee\u2212\u221222yy((tt))22 and \u2113\u2032t. Easy to see that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}}}\\leq\\frac{1-e^{-2y(t)^{2}}}{1+e^{-2y(t)^{2}}}\\leq1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and then collaborating with $c^{\\prime}\\leq\\ell_{t}^{\\prime}\\leq1$ we can obtain the result by solving the ODE. Observing that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{\\displaystyle\\frac{\\mathrm{d}y(t)}{\\mathrm{d}t}=a b c^{\\prime\\,2}\\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}}}\\frac{(t-t_{1})y(t)\\mathrm{d}t}{1+e^{2\\underline{{y}}(t)^{2}}+e^{-2\\underline{{y}}(t)^{2}}}}\\\\ {\\displaystyle\\Leftrightarrow\\frac{1}{2}(\\mathrm{Ei}(2\\underline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\underline{{y}}(t)^{2})+4\\log(\\underline{{y}}(t)))=a b c^{\\prime\\,2}\\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}}}\\frac{(t-t_{1})^{2}}{2}+\\mathrm{const},}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad z(t)=b c^{\\prime}\\frac{1-e^{-2y(t_{1})^{2}}}{1+e^{-2y(t_{1})^{2}}}(t-t_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus by the monotonicity the system is unique, which is also ture for the upper bound ODE. The proof is completed. ", "page_idx": 20}, {"type": "text", "text": "E Data Distribution ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section provided the detailed formal definitions of the prompt distribution. ", "page_idx": 20}, {"type": "text", "text": "Definition 3. (Polysemous Word Model $(D_{\\underline{{x}}},D_{\\underline{{y}}},D_{z},D_{{\\xi_{x}}},D_{{\\xi_{y}}})$ ). We assume there exists $K_{1}$ concepts of words totally. Specifically, each concept $k_{1}\\in[K_{1}]$ is characterized by two semantically-opposite feature vectors separately, denoted as $\\mu_{k_{1}}^{+}$ and $\\pmb{\\mu}_{k_{1}}^{-}$ , and the label vectors that describe their semantics under the co-concept are qk $q_{k_{1}}^{+}$ and $q_{k_{1}}^{-}$ . Our word samples $\\pmb{x}\\in\\mathbb{R}^{d_{\\mathcal{X}}}$ and their corresponding labels $\\pmb{y}\\in\\mathbb{R}^{d_{\\mathcal{V}}}$ are generated i.i.d. from distribution $\\mathcal{D}_{\\pmb{x}}$ and $\\mathcal{D}_{y}$ , which can be written as the following forms via reparameterization: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z\\sim\\mathcal{D}_{z},\\quad\\xi_{x}\\sim\\mathcal{D}_{\\xi_{x}}=\\mathcal{N}(\\mathbf{0},\\sigma_{\\xi}^{2}\\mathbf{I}_{d_{\\mathcal{X}}}),\\quad\\xi_{y}\\sim\\mathcal{D}_{\\xi_{y}}=\\mathcal{N}(\\mathbf{0},\\sigma_{\\xi}^{2}\\mathbf{I}_{d_{\\mathcal{Y}}}),}\\\\ &{x=\\mathbf{M}z+\\xi_{x}\\sim\\mathcal{D}_{x},\\quad y=\\mathbf{Q}z+\\xi_{y}\\sim\\mathcal{D}_{y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $z\\in\\mathbb{R}^{K}(K<d_{\\mathcal{X}})$ . We denote $_{\\mathscr{L}}$ as the sparse latent signal and $\\xi$ as the spurious dense noise, and each $\\textbf{\\em x}$ -y pair are reparameterized by one shared $_{\\mathscr{L}}$ . We have the following assumptions on $\\mathbf{M},z,\\xi$ respectively: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The sparse latent variable $z\\;=\\;(z_{1},\\cdot\\cdot\\cdot\\;,z_{K})\\;\\in\\;\\{0,1\\}^{k}$ is sampled from $\\mathcal{D}_{z}$ . $P(z_{j}\\;=\\;1)\\;=\\;$ $\\Theta(\\frac{\\log\\log K}{K})$ .   \n\u2022 ${\\bf M}\\,=\\,[\\mu_{1}^{+},\\mu_{1}^{-},\\mu_{2}^{+},\\mu_{2}^{-},\\cdots,\\mu_{K_{1}}^{+},\\mu_{K_{1}}^{-},\\nu_{1},\\nu_{2},\\cdots,\\nu_{K_{2}}]\\,=\\,[M_{1},\\cdots,M_{K}]\\,\\in\\,\\mathbb{R}^{d_{X}\\times K}$ is the   \nfeature dictionary matrix, where $\\{\\mu_{k_{1}}^{\\pm}\\}_{k_{1}=1}^{K_{1}}$ are concept-relevant features, $\\{\\nu_{k_{2}}\\}_{k_{2}=1}^{K_{2}}$ are conceptirrelevant features, and $\\forall k\\in[K],\\|M_{k}\\|=\\|{\\bf u}\\|$ . We assume that features of the same concept have positive inner product: $\\exists0<\\kappa_{x}<1$ , $\\forall k_{1}\\in[K_{1}]$ , $0<\\langle\\pmb{\\mu}_{k_{1}}^{+},\\pmb{\\mu}_{k_{1}}^{-}\\rangle\\leq\\kappa_{\\pmb{x}}\\|\\mathbf{u}\\|^{2}$ . Meanwhile, we let the features of different concept be orthogonal: $\\forall e\\in[\\pm],e^{\\prime}\\in[\\pm],s^{\\prime}\\in[K_{1}],r\\neq r^{\\prime}\\in[K_{2}],\\mathbf{u}\\in$ $\\{\\mu_{s^{\\prime}}^{e^{\\prime}},\\nu_{r}\\}$ , we have $\\langle\\pmb{\\mu}_{s}^{e},\\mathbf{u}\\rangle=\\langle\\pmb{\\nu}_{r},\\pmb{\\nu}_{r^{\\prime}}\\rangle=0$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 ${\\bf Q}\\,=\\,[{\\pmb q}_{1}^{+},{\\pmb q}_{1}^{-},{\\pmb q}_{2}^{+},{\\pmb q}_{2}^{-},\\cdot\\cdot\\cdot\\cdot\\,,{\\pmb q}_{K_{1}}^{+},{\\pmb q}_{K_{1}}^{-},0,\\cdot\\cdot\\cdot0]\\,\\in\\,\\mathbb{R}^{d_{\\mathcal{Y}}\\times K}$ is the corresponding label dictionary matrix, where $\\|\\pmb q_{k}^{\\pm}\\|=\\|\\mathbf q\\|$ , for $\\forall k\\in[K_{1}]$ . Similarly, we let the labels of the same concept to have positive inner product: $\\exists0<\\kappa_{y}<1$ , $\\forall k_{1}\\in[K_{1}]$ , $0<\\langle\\pmb{q}_{k_{1}}^{+},\\pmb{q}_{k_{1}}^{-}\\rangle\\leq\\kappa_{\\pmb{y}}\\|\\mathbf{q}\\|^{2}$ , while the labels of different concept to be orthogonal: $\\langle q_{k}^{\\pm},q_{k^{\\prime}}^{\\pm}\\rangle=0,\\forall k\\neq k^{\\prime}\\in[K_{1}]$ . ", "page_idx": 21}, {"type": "text", "text": "Definition 4. (Concept-specific Contextual Prompt Distribution) We consider the case that each prompt is concept-specific (i.e., the multi-concept words in one prompt would at least share one co-concept). Specifically, the chance for selecting each concept as the co-concept of one particular prompt is $\\Theta(K_{1}{}^{-1})$ , and the chance for selecting the two semantically-opposite vectors of the same concept is $\\overset{1}{\\underset{\\mathcal{D}_{S}}{=}}$ During training, each prompt $S=\\left\\{\\pmb{x}_{1},\\pmb{y}_{1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\pmb{x}_{L},\\pmb{y}_{L},\\pmb{x}_{L+1}\\right\\}$ is sampled from the mixture distribution defined as below. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{D}_{S}=\\sum_{k=1}^{K_{1}}\\left(\\pi_{k}^{+}\\mathcal{P}_{k,L+1}^{+}+\\pi_{k}^{-}\\mathcal{P}_{k,L+1}^{-}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where \u03c0k+ = \u03c0k\u2212 =2K , and the $\\mathcal{P}_{k,L+1}^{+}$ and $\\mathcal{P}_{k,L+1}^{-}$ are prompt distributions characterized by the $k$ -th concept, defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{k,L+1}^{+}=\\Big\\{S\\mid x\\sim\\mathcal{D}_{x},y\\sim\\mathcal{D}_{y},P_{L+1,2k-1}=1,\\forall l\\in[L+1],j\\neq\\{2k-1,k\\},P_{l,j}=\\frac{1}{K},}\\\\ &{\\{z_{l,2k-1}=1\\}\\cup\\{z_{l,2k}=1\\}=\\Omega,\\{z_{l,2k-1}=1\\}\\cap\\{z_{l,2k}=1\\}=\\emptyset,\\forall l\\in[L],P_{l,2k-1}=P_{l,2k}=\\frac{1}{2}\\Big\\},}\\\\ &{\\mathcal{P}_{k,L+1}^{-}=\\Big\\{S\\mid x\\sim\\mathcal{D}_{x},y\\sim\\mathcal{D}_{y},P_{L+1,2k}=1,\\forall l\\in[L+1],j\\neq\\{2k-1,k\\},P_{l,j}=\\frac{1}{K},}\\\\ &{\\{z_{l,2k-1}=1\\}\\cup\\{z_{l,2k}=1\\}=\\Omega,\\{z_{l,2k-1}=1\\}\\cap\\{z_{l,2k}=1\\}=\\emptyset,\\forall l\\in[L],P_{l,2k-1}=P_{l,2k}=\\frac{1}{2}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $P_{l,j}:=\\mathbb{P}\\left(z_{l,j}=1\\right)$ . $\\forall n\\in[N]$ where $N$ is the training size, if the training prompt $S_{n}$ is sampled from $\\mathcal{P}_{k,L+1}^{e},e\\in[\\pm],k\\in[K_{1}].$ , then by Definition $^{\\,l}$ , the label vector of the query should contain $\\pmb q_{k}^{e}$ , and we call $y s_{n}=e$ as the real value label of this $k$ -th concept prompt. Specifically, for $\\forall k\\in[K_{1}]$ we define the index set of training prompts sharing the $k$ -th co-concepts as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{V}_{k}=\\mathcal{V}_{k}^{+}\\cup\\mathcal{V}_{k}^{-},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{V}_{k}^{+}=\\left\\{n\\mid S_{n}\\sim\\mathcal{P}_{k,L+1}^{+}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{V}_{k}^{-}=\\left\\{n\\mid S_{n}\\sim\\mathcal{P}_{k,L+1}^{-}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For sample $\\mathbf{\\nabla}x_{l}$ where $n\\in\\mathcal{V}_{k},k\\in[K_{1}],l\\in[L+1]$ , we define the index set for its non-zero elements of $z_{l}^{n}$ besides $z_{2k-1,l}^{n}$ and $z_{2k,l}^{n}{}_{:}$ , namely $\\mathcal{M}_{l}^{n}:=\\{k\\in[K]\\ |\\ z_{l,k}^{n}=1,k\\ \\notin\\{2k-1,2k\\}\\}$ . Also, for each prompt sharing the $k$ -th co-concept, we define the index set of demonstration in the context: ", "page_idx": 21}, {"type": "equation", "text": "$$\nS_{n,k}^{+}=\\{l\\in[L]\\mid n\\in\\mathcal{V}_{k},z_{l,2k-1}^{n}=1\\},\\quad S_{n,k}^{-}=\\{l\\in[L]\\mid n\\in\\mathcal{V}_{k},z_{l,2k}^{n}=1\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "F Model details: Attention Part ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide several important definitions and compute the original gradients of attention. Lemma 13. (Contributing and Misleading Neurons) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{k,n}^{+}(t)=\\{i\\in[m]~|~n\\in\\mathcal{V}_{k}^{+},\\mathbb{1}_{O_{(i)}}^{n}(t)>0\\},\\quad\\mathcal{U}_{k,n}^{+}(t)=\\{i\\in[m]~|~n\\in\\mathcal{V}_{k}^{+},\\mathbf{r}_{i}\\cdot\\mathbb{1}_{O_{(i)}}^{n}(t)>0\\},}\\\\ {\\mathcal{W}_{k,n}^{-}(t)=\\{i\\in[m]~|~n\\in\\mathcal{V}_{k}^{-},\\mathbb{1}_{O_{(i)}}^{n}(t)>0\\},\\quad\\mathcal{U}_{k,n}^{-}(t)=\\{i\\in[m]~|~n\\in\\mathcal{V}_{k}^{-},\\mathbf{r}_{i}\\cdot\\mathbb{1}_{O_{(i)}}^{n}(t)<0\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\mathcal{W}_{k,n}(t):=\\mathcal{W}_{k,n}^{+}(t)\\cup\\mathcal{W}_{k,n}^{-}(t)$ are neurons that can be activated, among which $\\mathcal{U}_{k,n}(t):=\\mathcal{U}_{k,n}^{+}(t)\\cup\\mathcal{U}_{k,n}^{-}(t)$ are neurons that correctly contribute to the prediction. The following lemma computes the original gradients. Lemma 14. (Gradient Update) Denote ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{r}_{i}=\\mathbf{r}[i],}\\\\ &{{\\ell_{n}^{\\prime}}^{(t)}=\\ell^{\\prime}(y_{S_{n}}\\cdot f(\\mathbf{H}^{n};\\Psi^{(t)})),}\\\\ &{(\\sigma_{S}^{(t)})_{l}^{n}=\\mathrm{softmax}\\left(\\left(\\mathbf{W}_{K}^{(t)}\\mathbf{h}_{l}^{n}\\right)^{\\top}\\mathbf{W}_{Q}^{(t)}\\mathbf{h}_{L+1}^{n}\\right),}\\\\ &{\\mathbf{1}_{O_{(i)}}^{n}^{(t)}{}^{(t)}=\\mathbb{1}(\\mathbf{W}_{O_{(i,.)}}^{(t)}\\mathrm{~attn}(\\mathbf{H}^{n};\\Psi^{(t)})>0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\sum_{n\\in B_{t}}\\left[y_{S_{n}}^{(t)}\\ell_{n}^{\\prime}(t)\\sum_{i=1}^{m}\\mathbf{r}_{i}\\mathbf{1}_{O_{(i)}}^{n}(t)\\sum_{l,j\\in[L]}\\left(\\sigma_{S}^{(t)}\\right)_{l}^{n}\\left(\\sigma_{S}^{(t)}\\right)_{j}^{n}\\left(\\mathbf{W}_{O_{(i,\\cdot)}}^{(t)}\\mathbf{W}_{V}^{(t)}\\mathbf{h}_{l}^{n}\\right)\\mathbf{W}_{K}^{\\underline{{x}}\\,(t)}\\big(x_{l}^{n}-x_{j}^{n}\\big)x_{L+1}^{n}\\mathrm{^{n}}^{\\top}\\right]+\\lambda\\mathbf{W}_{Q}^{\\infty}(t)\\mathbf{W}_{H}^{(t)}\\mathbf{W}_{O_{(i,\\cdot)}}^{(t)}\\mathbf{W}_{V}^{(t)}\\{O_{(i,\\cdot)}^{n}\\}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, $\\nabla_{\\mathbf{W}_{K}^{\\mathbf{x}}(t)}L\\boldsymbol{s}_{t}(\\Psi^{(t)})\\in\\mathbb{R}^{d_{X}\\times d_{X}}$ can be derived as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\sum_{n\\in B_{t}}\\left[y_{S_{n}}^{(t)}\\ell_{n}^{\\prime}(t)\\sum_{i=1}^{m}\\mathbf{r}_{i}\\mathbf{1}_{O_{(i)}}^{n}(t)\\sum_{l,j\\in[L]}{(\\sigma_{S}^{(t)})_{l}^{n}(\\sigma_{S}^{(t)})_{j}^{n}(\\mathbf{W}_{O_{(i,:)}}^{(t)}\\mathbf{W}_{V}^{(t)}\\mathbf{h}_{l}^{n})}\\mathbf{W}_{Q}^{n}\\,^{\\top}x_{L+1}^{n}(x_{l}^{n}-x_{j}^{n})^{\\top}\\right]+\\lambda\\mathbf{W}_{K}^{\\alpha}(t),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Subsequently, we directly compute the update of the attention matrices along the feature directions as below. ", "page_idx": 22}, {"type": "text", "text": "Lemma 15. (Concept Learning of Attention) F $o r\\,\\forall\\hat{k}\\in[K_{1}],$ , we have the single step of learning of the concept part of the features: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{a_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{x}(^{t+1})\\mathbf{a}_{\\hat{k}}-a_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{x}(^{t})\\mathbf{a}_{\\hat{k}}=-\\eta_{t}\\cdot a_{\\hat{k}}^{\\top}\\nabla_{\\mathbf{W}_{Q}^{\\alpha}({t})}L_{B_{t}}(\\Psi^{(t)})\\mathbf{a}_{\\hat{k}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\eta_{t}\\bigl(I_{Q,a_{\\hat{k}},c h a o s}^{(t)}+I_{Q,a_{\\hat{k}},c o n r i}^{(t)}\\bigr)-\\eta_{t}\\lambda\\mathbf{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{x}(^{t})\\mathbf{a}_{\\hat{k}},}\\end{array}}\\\\ &{\\begin{array}{r l}&{a_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{x}{}^{(t+1)}\\mathbf{a}_{\\hat{k}}-a_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{x}{}^{(t)}\\mathbf{a}_{\\hat{k}}=-\\eta_{t}\\cdot\\mathbf{a}_{\\hat{k}}^{\\top}\\nabla_{\\mathbf{W}_{K}^{\\alpha}({t})}L_{B_{t}}(\\Psi^{(t)})\\mathbf{a}_{\\hat{k}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\eta_{t}\\bigl(I_{K,a_{\\hat{k}},c h a o s}^{(t)}+I_{K,a_{\\hat{k}},c o n r i}^{(t)}\\bigr)-\\eta_{t}\\lambda\\mathbf{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{x}{}^{(t)}a_{\\hat{k}},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$I_{Q,\\pmb{a}_{\\hat{k}},c h a o s}^{(t)}$ and $I_{Q,\\pmb{a}_{\\hat{k}},c o n t r i}^{(t)}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{1}{N}\\displaystyle\\sum_{k=1}^{N}\\sum_{i=1,\\cdots,i\\in\\mathcal{N}_{k}^{(k)}}\\left[e_{i}^{\\ell(\\ell)}u_{i,i+1}^{\\top}+\\sum_{\\ell\\leq M_{\\ell,i+1}^{\\top}}\\prod_{\\ell\\leq N_{\\ell}=0}^{N}\\prod_{\\ell\\leq i}\\right.\\prod_{\\ell\\leq\\ell\\leq\\ell}\\left.\\prod_{\\ell^{\\prime}\\leq i}\\right.}\\\\ &{\\left.\\quad\\quad\\quad\\quad\\times\\prod_{\\ell^{\\prime}\\leq i}\\right]\\prod_{\\ell=1}^{N}\\sum_{i=1}^{N}\\prod_{\\ell=i}^{N}\\left(\\prod_{\\ell^{\\prime}=i}^{N}\\prod_{\\ell^{\\prime}=i}^{\\ell}\\left.\\prod_{\\ell^{\\prime}=i}^{N}\\right)}\\\\ &{\\left.\\quad\\quad\\times\\prod_{\\ell\\leq i}\\left(\\prod_{\\ell^{\\prime}=i}^{N}\\prod_{\\ell=i}^{\\ell}\\right)\\alpha_{\\ell}+\\xi_{\\ell}^{\\top}\\right)\\left(\\alpha_{\\ell}^{\\top}\\mathbb{W}_{X}^{\\top}\\mathrm{e}^{\\ell(1)}(\\{\\left(g_{i}^{\\mathrm{\\ell}}-g_{j}^{\\ast}\\right)}\\mathbb{h}_{\\ell}+\\sum_{\\ell\\leq M_{\\ell,i}^{\\top}}\\prod_{\\ell\\leq i}+\\xi_{\\ell}^{\\top})-\\displaystyle\\sum_{\\ell\\leq i,j}\\mathbb{M}_{X}-\\xi_{\\ell,j}^{\\top}\\right)\\right]}\\\\ &{+\\frac{1}{N}\\displaystyle\\sum_{\\ell\\leq i}\\left[e_{i}^{\\ell(\\ell)}\\|\\alpha_{\\ell}\\|_{\\ell}^{2}+\\alpha_{\\ell}^{\\top}(\\mathbb{E}_{X}^{\\top}\\mathrm{e}^{\\ell})\\mathbb{M}_{X}+\\displaystyle\\sum_{\\ell\\leq i+1}\\mathbb{M}_{X}\\right]\\sum_{\\ell\\leq i}\\left.\\sum_{\\ell\\leq i}\\mathbb{E}_{\\prod_{\\ell}\\leq\\ell}\\left(e_{i}^{\\ell(\\ell)}\\mathbb{P}_{X}^{\\top}(\\sigma_{S}^{\\top})\\right)^{\\alpha}}\\\\ &{\\qquad\\quad\\quad\\times e_{i}^{\\ell(\\ell)}\\mathbb{I}_{X}^{\\ell\\leq i}\\left(\\mathbb{W}_{X}^{\\top}\\mathrm{e}^{\\ell(1)}(\\mathbb{R}_{X}^{\\top}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, $I_{K,\\pmb{a}_{\\hat{k}},c h a o s}^{(t)}$ and I(Kt,)a\u02c6,contri are defined as below. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I_{K_{\\alpha_{k},l_{k}+1}}^{(0)}=}&{\\displaystyle\\frac{1}{\\beta}\\sum_{k_{l}\\neq k_{l}}\\left[e_{\\alpha}^{-(\\alpha_{l}^{(\\alpha_{l}^{(\\alpha_{l})})}}\\mathrm{tr}\\frac{\\mathrm{tr}_{\\alpha}^{(0)}(\\alpha_{k}+e_{k}+\\xi_{\\alpha,l+1}^{-}+1)}{e^{-\\alpha_{l}^{(\\alpha_{l}^{(\\alpha_{l})}}}}+\\sum_{\\tau\\neq\\alpha_{l+1}}\\mathrm{M}_{\\tau}\\right)\\sum_{\\tau,k_{l}\\neq l_{l}}\\mathrm{tr}_{\\alpha_{l},\\tau,k_{l}}^{-}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\displaystyle\\left(e_{\\alpha}^{-\\alpha_{l}^{(\\alpha_{l})}}\\mathrm{tr}\\frac{\\mathrm{tr}_{\\alpha_{k},\\tau}^{(0)}}{\\mathrm{tr}_{\\alpha_{k},\\tau}^{(0)}}(\\eta_{k}^{\\alpha_{l}^{(\\alpha)}}+\\sum_{\\tau\\neq\\alpha_{l}^{(\\alpha_{l})}}\\mathrm{tr}_{\\alpha}^{-}(\\xi_{\\alpha}^{-}))\\mathrm{tr}_{\\alpha}^{\\tau}\\right.\\Bigg.\\Bigg\\langle\\sum_{\\tau\\neq\\alpha_{l}^{(\\alpha_{l}^{(\\alpha)}}}\\mathrm{M}_{\\tau}+\\xi_{\\alpha,l}^{-}-\\sum_{\\tau\\neq\\alpha_{l}^{(\\alpha_{l}^{(\\alpha)}}}\\mathrm{M}_{\\tau}-\\xi_{\\alpha,j}^{-})\\Bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\left.+\\frac{1}{\\mathrm{B}_{\\alpha}}\\sum_{k=1}\\bigg[e_{\\alpha}^{\\tau}(\\eta_{\\alpha}^{(\\alpha_{l}^{(\\alpha_{l})}}\\mathrm{tr}\\frac{\\mathrm{tr}_{\\alpha^{(\\alpha_{l})}}^{2}(\\alpha_{l+1}+\\xi_{\\alpha,l+1}^{+}+1)}{e^{-\\alpha_{l}^{(\\alpha_{l})}}}+\\sum_{\\tau\\neq\\alpha_{l+1}}\\\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 16. (Label Semantic Learning of Attention) Also, for $\\forall\\hat{k}\\in[K_{1}],$ , we have the single step of learning of the concept-specific semantically-opposite part of the features: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t+1)}b_{\\hat{k}}-b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}b_{\\hat{k}}=-\\eta_{t}\\cdot b_{\\hat{k}}^{\\top}\\nabla_{\\mathbf{W}_{Q}^{\\mathbf{x}}}(t)\\,L\\boldsymbol{B}_{t}\\big(\\Psi^{(t)}\\big)b_{\\hat{k}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\eta_{t}(I_{Q,b_{\\hat{k}},c h a s}^{(t)}+I_{Q,b_{\\hat{k}},c o n t r i}^{(t)})-\\eta_{t}\\lambda b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}b_{\\hat{k}},}\\\\ &{b_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}\\,(t+1)}b_{\\hat{k}}-b_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}\\,(t)}b_{\\hat{k}}=-\\eta_{t}\\cdot b_{\\hat{k}}^{\\top}\\nabla_{\\mathbf{W}_{K}^{\\mathbf{x}}}(t)\\,L\\boldsymbol{B}_{t}\\big(\\Psi^{(t)}\\big)b_{\\hat{k}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\eta_{t}(I_{K,b_{\\hat{k}},c h a s}^{(t)}+I_{K,b_{\\hat{k}},c o n t r i}^{(t)})-\\eta_{t}\\lambda b_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}\\,(t)}a_{\\hat{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$I_{Q,b_{\\hat{k}},c h a o s}^{(t)}$ and $I_{Q,b_{\\hat{k}},c o n t r i}^{(t)}$ are defined as below. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\langle\\mathbf{I}_{1},\\mathbf{\\hat{e}}_{\\perp}==\\frac{1}{\\hbar},\\sum_{k\\in\\partial\\Omega_{k}}\\left[e_{k}^{(\\alpha_{1})},e_{k}^{\\alpha_{1}},\\dots,\\sum_{k}\\mathbf{I}_{1}\\right],}\\sum_{k=\\partial_{k}}\\sum_{\\ell=1}^{\\infty}\\mathbf{r}_{k}\\cdot\\sum_{i=1}^{\\infty}\\omega_{k}^{(\\theta)}\\mathbf{r}_{i}^{(\\alpha_{1})}\\mathbf{r}_{\\ell}^{(\\alpha_{2})}\\mathbf{r}_{k}^{(\\alpha_{1})},}}\\\\ &{}&{\\cdots\\times\\frac{\\langle\\mathbf{r}_{k}|^{2}\\mathbf{r}_{k}\\rangle}{\\langle\\mathbf{r}_{k}\\rangle}\\langle\\mathbf{r}_{k}^{(\\alpha_{1})}+\\sum_{i=1}^{\\infty}\\mathbf{Q}_{k}+\\frac{\\langle\\mathbf{r}_{k}\\rangle}{\\langle\\mathbf{r}_{k}\\rangle}|\\delta_{k}^{\\dagger}\\mathbf{W}_{k}^{(\\alpha_{1})}(\\mathbf{r}_{k}^{(\\alpha_{2})}-\\mathbf{r}_{k}^{\\alpha_{2}})\\mathbf{h}+\\sum_{i=1}^{\\infty}\\mathbf{M}_{k}+\\xi_{i}-\\sum_{k=1}^{\\infty}\\mathbf{r}_{k}\\mathbf{I}_{1},}\\\\ &{+\\frac{1}{\\hbar}\\sum_{k=1}^{\\infty}\\sum_{\\ell=1}^{\\infty}\\left[e_{k}^{(\\alpha_{1})}(|\\mathbf{b}_{i}|^{2}+\\delta_{k}^{\\dagger}\\mathbf{r}_{\\ell}^{(\\alpha_{2})}+\\sum_{i=1}^{\\infty}\\mathbf{M}_{k})\\right]\\sum_{\\ell=1}^{\\infty}\\mathbf{r}_{\\ell}^{(\\alpha_{2})}}\\\\ &{\\vdots\\sum_{k=1}^{\\infty}\\sum_{\\ell=1}^{\\infty}\\omega_{k}^{(\\theta)}\\mathbf{r}_{\\ell}^{(\\alpha_{1})}\\mathbf{r}_{\\ell}^{(\\alpha_{2})}\\mathbf{r}_{k}^{(\\alpha_{1})}\\mathbf{r}_{\\ell}^{(\\alpha_{2})}\\leq\\omega_{k}^{(\\theta)}\\mathbf{r}_{k}^{(\\alpha_{2})}\\mathbf{r}_\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$I_{K,\\pmb{b}_{\\hat{k}},c h a o s}^{(t)}$ and $I_{K,\\pmb{b}_{\\hat{k}},c o n t r i}^{(t)}$ are defined as below. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\|\\bar{\\mathbf{U}}_{h,\\varepsilon}\\|_{\\mathrm{L}^{\\infty}}}{\\|\\bar{\\mathbf{U}}_{h,\\varepsilon}\\|_{\\mathrm{L}^{\\infty}}}=\\frac{1}{\\|\\mathbf{\\bar{U}}_{h,\\varepsilon}\\|_{\\mathrm{L}^{\\infty}}}\\bigg[e^{-\\varepsilon_{h}^{(0)}\\mathbf{\\bar{U}}_{h}^{\\top}\\mathbf{\\Phi}_{h}^{\\top}(\\theta_{\\Delta}+\\bar{\\mathbf{t}}_{h}+\\xi_{\\Delta,\\varepsilon}^{(0)}+1)}+\\frac{\\sum_{t}\\mathbf{M}_{t}}{e^{\\Delta\\theta_{\\Delta}/2}}+\\frac{\\sum_{t}\\mathbf{M}_{t}}{e^{\\Delta\\theta_{\\Delta}/2}}\\bigg]\\quad\\sum_{t}\\frac{\\mathrm{P}_{h}}{\\|h\\|_{\\mathrm{L}^{\\infty}}}\\bigg[\\sigma_{\\Delta}^{(0)}\\bigg]_{\\mathrm{L}^{\\infty}}^{*}}\\\\ &{}&{\\quad\\quad+\\frac{\\sum_{t}\\big(\\bar{\\mathbf{t}}_{h}^{(0)}\\big)}{e^{\\Delta\\theta_{\\Delta}/2}}\\bigg[e^{-\\sum_{t}\\mathbf{Q}_{t}}+\\frac{\\sum_{t}\\mathbf{Q}_{t}}{e^{\\Delta\\theta_{\\Delta}/2}}\\bigg]\\bigg\\{h_{t}^{\\tau}\\bigg\\}\\quad\\sum_{t=1}^{\\infty}\\bigg[e^{\\sum_{t=1}^{T}\\mathbf{{M}}_{t}}+\\xi_{\\Delta,t}^{(0)}-\\frac{\\sum_{t=1}^{T}\\mathbf{M}_{t}}{e^{\\Delta\\theta_{\\Delta}/2}}\\bigg]\\bigg\\}}\\\\ &{}&{\\quad+\\frac{1}{\\|\\mathbf{\\bar{D}}_{h,\\varepsilon}\\|_{\\mathrm{L}^{\\infty}}}\\bigg[e^{\\sum_{t=1}^{T}\\mathbf{\\bar{W}}_{h}^{\\top}\\mathbf{\\Phi}_{h}^{\\top}(\\theta_{\\Delta}+\\bar{\\mathbf{t}}_{h}+\\xi(\\bar{\\mathbf{t}}_{h}^{(0)}+1)+\\sum_{t=1}^{T}\\mathbf{\\bar{M}}_{t})}+\\frac{\\sum_{t=1}^{T}\\mathbf{\\bar{t}}_{h}}{e^{\\Delta\\theta_{\\Delta}/2}}\\bigg]\\quad\\times}\\\\ &{}&{\\quad\\quad+\\sum_{t=1}^\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "G Model details: MLP Part ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 17. (Tensor Update) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{W}_{O_{(i,\\cdot)}}^{y}{}^{(t)}\\mathbf{c}_{\\hat{k}}=\\alpha_{O_{(i,\\cdot)},k}^{(0)}-\\eta_{t}\\displaystyle\\sum_{t=0}^{T}\\nabla_{\\mathbf{w}_{O_{(i,\\cdot)}}^{y}}{}_{(t)}L\\boldsymbol{B}_{t}\\big(\\Psi^{(t)}\\big)\\mathbf{c}_{k},}\\\\ {\\mathbf{W}_{O_{(i,\\cdot)}}^{y}{}^{(t)}d_{\\hat{k}}=\\beta_{O_{(i,\\cdot)},k}^{(0)}-\\eta_{t}\\displaystyle\\sum_{t=0}^{T}\\nabla_{\\mathbf{w}_{O_{(i,\\cdot)}}^{y}}{}_{(t)}L\\boldsymbol{B}_{t}\\big(\\Psi^{(t)}\\big)d_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 18. (Gradient Update) $\\nabla_{\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(\\cdot)}L_{{B_{t}}}(\\Psi^{(t)})\\in\\mathbb{R}^{1\\times(d_{\\mathcal{X}}+d_{\\mathcal{Y}})}$ can be derived as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\textrm{\\sum\\limits_{\\substack{k\\neq\\hat{k}}\\in[\\hat{k}_{1}]}}}\\left[\\ell_{n}^{\\prime}(\\mathbf{t})\\mathbf{r}_{i}\\mathbf{1}_{\\mathcal{O}_{(i)}}^{n}(^{t})\\left\\{(2\\sum_{l\\in S_{n,k}^{\\prime}}\\binom{\\sigma(t)}{S}_{l}^{n}-1)d_{k}^{\\top}+e\\sum_{l\\in[L]}\\binom{\\sigma(t)}{S}_{l}^{n}\\big(c_{k}+\\sum_{\\substack{s\\in\\mathcal{M}_{l}^{n}}}\\mathbf{Q}_{s}+\\xi_{y,l}^{n}\\big)^{\\top}\\right\\}\\right]+\\lambda\\mathbf{W}_{\\mathcal{O}_{(i,+)}}^{y}(t)\\textrm{\\sum\\limits_{\\substack{k\\in\\mathcal{V}_{k}}}}(t)\\textrm{\\sum\\limits_{\\substack{k\\in[\\hat{k}_{1}]}}}(\\tau_{k})\\textrm{\\sum\\'_{\\tau}}\\left(d_{k}^{\\tau}+e\\right)\\textrm{\\sum\\limits_{\\substack{k\\in\\mathcal{V}_{k}}}}(t)\\textrm{\\sum\\'_{\\tau}}\\left(d_{k}^{\\tau}+e\\right)\\textrm{\\sum\\ensuremath_{\\tau\\in\\mathcal{V}_{k}}}d_{k}^{\\tau},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 19. (Concept Learning of MLP) For $\\forall i\\in[m],\\hat{k}\\in[K_{1}],$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(\\mathbf{\\Delta}^{t+1})\\mathbf{c}_{\\hat{k}}-\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(\\mathbf{\\Delta}^{t})\\mathbf{c}_{\\hat{k}}=-\\eta_{t}\\cdot\\nabla_{\\mathbf{W}_{O_{(i,\\cdot)}}^{y}}(\\mathbf{\\Delta}^{t})\\mathbf{\\Delta}L_{B_{t}}(\\Psi^{(t)})\\mathbf{c}_{\\hat{k}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {=-\\eta_{t}\\big(I_{O_{(i,\\cdot)},\\mathbf{c}_{\\hat{k}},c h a s}^{(t)}+I_{O_{(i,\\cdot)},\\mathbf{c}_{\\hat{k}},c o n r i}^{(t)}\\big)-\\eta_{t}\\lambda\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(\\mathbf{\\Delta}^{t})\\mathbf{c}_{\\hat{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$I_{O_{(i,\\,.\\,)},\\mathbf{c}_{\\hat{k}},c h a o s}^{(t)}$ $I_{O_{(i,\\,.\\,)},\\mathbf{c}_{\\hat{k}},c o n t r i}^{(t)}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{I_{O_{(i,.)},c_{k},c_{k}a a s}^{(t)}=\\displaystyle\\frac{1}{B}\\sum_{k\\neq k\\in[K_{1}]}\\sum_{e\\in[\\pm]}\\sum_{n\\in\\mathcal{V}_{k}^{e}\\cap B_{t}}\\Big[e\\cdot\\ell_{n}^{\\prime}{}^{(t)}{\\bf r}_{i}\\cdot\\mathbb{1}_{O_{(i)}}^{n}(t)\\sum_{l\\in[L]}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big(\\sum_{s\\in M_{l}^{n}}\\mathbf{Q}_{s}+\\xi_{y,l}^{n}\\big)^{\\top}c_{k}\\Big],}\\\\ {I_{O_{(i,.)},c_{k},c_{m i},}^{(t)}=\\displaystyle\\frac{1}{B}\\sum_{\\ell\\in[\\pm]}\\sum_{n\\in\\mathcal{V}_{k}^{e}\\cap B_{t}}\\Big[\\hat{e}\\cdot\\ell_{n}^{\\prime}{}^{(t)}{\\bf r}_{i}\\cdot\\mathbb{1}_{O_{(i)}}^{n}(t)\\|c_{\\hat{k}}\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Remark 2. (Informal Discussions). Interestingly, the gradient of MLPs\u2019 Concept Learning is very large. We have the following situations. ", "page_idx": 24}, {"type": "text", "text": "\u2022 When the neuron is activated (i.e., $\\mathbf{\\widetilde{\\mathbf{\\Gamma}}}n\\in\\mathcal{V}_{k}^{\\hat{e}}\\cap\\mathcal{B}_{t},i f(\\mathbf{W}_{K}^{\\mathbf{x}}{^{(t)}}\\pmb{b}_{k})^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}}{^{(t)}}\\pmb{b}_{k}>0\\}$ , and $\\alpha_{O_{(i,\\cdot)},\\hat{k}}^{(t)}+$ $\\begin{array}{r}{\\hat{e}\\cdot(2\\sum_{l\\in S_{n,\\hat{k}}^{\\hat{e}}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)\\mathbf{W}_{O_{(i,\\cdot)}}^{y}{}^{(t)}d_{\\hat{k}}>0)}\\end{array}$ , the neuron is likely to be activated $(i\\in\\mathcal{W}_{\\hat{k},n}^{\\hat{e}}(t),$ . 1. ${\\mathit{I f}}\\left(I\\right){\\mathrm{\\bf~r}}_{i}\\cdot\\hat{e}\\;>\\;0,i\\;\\in\\;{\\mathcal{W}}_{\\hat{k},n}^{\\hat{e}}(t)\\;\\Leftrightarrow\\;i\\;\\in\\;{\\mathcal{W}}_{\\hat{k},n}^{\\hat{e}}(t)\\;\\cap{\\mathcal{U}}_{\\hat{k},n}^{\\hat{e}}(t),$ , the gradient will advance the $\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(t)\\mathbf{\\Phi}_{{c}_{\\hat{k}}},$ 2. $i f\\left(2\\right)\\,\\mathbf{r}_{i}\\,\\cdot\\,\\hat{e}\\,<\\,0,i\\,\\in\\,\\mathcal{W}_{\\hat{k},n}^{\\hat{e}}(t)\\,\\Leftrightarrow\\,i\\,\\in\\,\\mathcal{W}_{\\hat{k},n}^{\\hat{e}}(t)\\,-\\,\\mathcal{U}_{\\hat{k},n}^{\\hat{e}}(t).$ , the gradient will diminish the $\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(t)\\mathbf{\\Phi}_{{c}_{\\hat{k}}}$ , thus help deactivate this neuron. ", "page_idx": 25}, {"type": "text", "text": "Lemma 20. (Label Semantic Learning of MLP) For $\\forall i\\in[m],\\hat{k}\\in[K_{1}],$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}_{O_{(i,\\cdot)}}^{y}({t+1})d_{\\hat{k}}-\\mathbf{W}_{O_{(i,\\cdot)}}^{y}({t})d_{\\hat{k}}=-\\eta_{t}\\cdot\\nabla_{\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(t)}L_{B_{t}}(\\Psi^{(t)})d_{\\hat{k}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\eta_{t}\\big(I_{O_{(i,\\cdot)},d_{\\hat{k}},c h a s}^{(t)}+I_{O_{(i,\\cdot)},d_{\\hat{k}},c o n r i}^{(t)}\\big)-\\eta_{t}\\lambda\\mathbf{W}_{O_{(i,\\cdot)}}^{y}({t})d_{\\hat{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$I_{O_{(i,\\cdot)},d_{\\hat{k}},c h a o s}^{(t)}$ and IO(i,\u00b7),d\u02c6,contri are defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{O_{(i,\\cdot)},d_{k},c h a o s}^{(t)}=\\frac{1}{B}\\sum_{\\substack{k\\in[K_{1}]\\,\\textit{e c}[\\pm]\\,n\\in\\mathcal{V}_{k}^{e}\\cap B_{t}}}\\left[e\\cdot\\ell_{n}^{\\prime}{\\bf(}t{\\bf)}_{\\bf r}\\cdot\\mathbb{1}_{O_{(i)}}^{n}(t)\\sum_{l\\in[L]}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big(\\sum_{\\substack{s\\in M_{l}^{n}}}\\mathbf{Q}_{s}+\\xi_{y,l}^{n}\\big)^{\\top}d_{k}\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "H Discussions over Parameter Settings ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Note that we do not have any requirement upon demonstration length $L$ and batch size $B$ for training, thus the training can be really flexible compared with the strict requirement in [28]. The condition on dimensionality $d_{\\mathcal{X}},d_{\\mathcal{Y}}$ and the network width $m$ ensure the learning problem is in a sufficiently overparameterized setting where the norm and the inner products of the Gaussian noise and initialized NN can be controlled within a certain range with high probability $1-\\delta$ , which is standard requirements in recent feature learning line-of-research [41, 57, 53, 45, 58, 42, 52, 43]. The weak requirement on network width $m$ allows us to conduct a fine-grained analysis based on the network projection length, which is fundamentally differs from the NTK line of research [91] that requires an infinitely wide network to perform linear regression over a prescribed feature map. The condition on $\\gamma$ ensures the learning step to be small and thus learning process enjoys an approximation to gradient flow rather than the challenging \u201cOscillation\u201d regime [92], which is analyzable but not necessary in presenting our theory. The condition on the small $\\lambda$ is to ensure that the learning dynamic of Attention and MLP would not stuck at the origin point, and ensure that we can analyze the expected learning dynamic with limited impact of the regularization at the initial stage, which is also adopted in [53]. The condition on $K$ is to control the impact of cross-concept contribution in the Attention\u2019s learning dynamic, which can actually be relaxed at the cost of a denser analysis. The condition on $\\sigma_{\\xi}$ is to ensure that the impact of the norms and inner-products involving the Gaussian Noise on the gradient cannot surpass those in the order of feature\u2019s norms, which ensures the gradient flows to be not too noisy and could converge to the expected gradient flow exponentially. Last but not least, the conditions on $\\sigma_{1}$ guarantee that the initial beliefs of MLP is small and the gradients of SGD can update the model effectively. The condition of $\\sigma_{0}$ is only used when discussing the OOD scenario. ", "page_idx": 25}, {"type": "text", "text": "I Convergence of Expectation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we assume all the events in the Section D hold, denoted as $\\Upsilon_{\\mathrm{Pre}}$ . ", "page_idx": 25}, {"type": "text", "text": "We examine the evolution of $\\mathbb{E}({\\Psi^{\\prime}}^{t}):=\\{\\mathbb{E}(\\mathbf{W}_{Q}^{\\mathbf{x}}^{(t)}),\\mathbb{E}(\\mathbf{W}_{K}^{\\mathbf{x}}^{(t)}),\\mathbb{E}(\\mathbf{W}_{O_{(i,\\cdot)}}^{(t)})\\}$ at the whole iteration $0\\leq t\\leq\\underline{{t}}$ where the expectation $\\mathbb{E}[\\cdot]$ is taken over the stochastic batches. As such, we can see every stochastic gradient update within each batch as a gradient update upon noise-free and category-balanced concept-specific prompts. ", "page_idx": 25}, {"type": "text", "text": "Lemma 21. For $\\forall k_{1}\\in[K_{1}]$ , we define $a_{k_{1}}:=\\frac{\\pmb{\\mu}_{k_{1}}^{+}+\\pmb{\\mu}_{k_{1}}^{-}}{2}$ and $b_{k_{1}}:=\\frac{\\mu_{k_{1}}^{+}-\\mu_{k_{1}}^{-}}{2}$ \u00b5k1 \u2212\u00b5k1. By definition, we then have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{k_{1}}^{+}=a_{k_{1}}+b_{k_{1}},\\quad\\mu_{k_{1}}^{-}=a_{k_{1}}-b_{k_{1}},}\\\\ &{\\langle a_{k_{1}},b_{k_{1}}\\rangle=0,\\quad\\{a_{k_{1}},b_{k_{1}}\\}\\perp\\{a_{k_{1}^{\\prime}},b_{k_{1}^{\\prime}}\\},}\\\\ &{\\langle\\mu_{k_{1}}^{+},\\mu_{k_{1}}^{-}\\rangle=\\|a_{k_{1}}\\|^{2}-\\|b_{k_{1}}\\|^{2},\\quad\\|\\mu_{k_{1}}^{\\pm}\\|^{2}=\\|a_{k_{1}}\\|^{2}+\\|b_{k_{1}}\\|^{2}=\\|\\mathbf{u}\\|^{2},}\\\\ &{\\displaystyle\\frac{1}{2}\\|\\mathbf{u}\\|^{2}<\\|a_{k_{1}}\\|^{2}\\leq\\frac{\\kappa_{x}+1}{2}\\|\\mathbf{u}\\|^{2},\\quad\\frac{-\\kappa_{x}+1}{2}\\|\\mathbf{u}\\|^{2}\\leq\\|b_{k_{1}}\\|^{2}<\\frac{1}{2}\\|\\mathbf{u}\\|^{2},}\\\\ &{\\displaystyle\\mathbf{\\eta}_{\\mathrm{\\pmb{\\alpha}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $\\forall k_{1}^{\\prime}\\neq k_{1}\\in[K_{1}]$ . ", "page_idx": 26}, {"type": "text", "text": "Remark 3. We observe that, through this formulation, the shared component $\\pmb{a}_{k_{1}}$ can be interpreted as the \u201cconcept\u201d part of the two features, while the terms $\\pm b_{k_{1}}$ represent their opposing semantic aspects. The relevance of this modeling is exemplified by Figure $I(b)$ in [12], where the concept \u201c[Bird]\u201d is composed of orthogonal steering vectors: \u201cplant $\\implies$ animal\u201d and \u201cmamma $l\\Rightarrow b i r d$ .\u201d These vectors correspond to the concept feature $\\pmb{a}_{k}$ and the semantic label features $\\boldsymbol{b}_{k}$ , respectively. ", "page_idx": 26}, {"type": "text", "text": "Idempotent Operator Trick. Define $\\mathbb{U}:=\\operatorname{span}(\\mathbf{M})$ and its complement space $\\mathbb{U}^{\\perp}$ . By definition, we know that $\\dim(\\mathbb{U})\\,=\\,K$ and $\\dim(\\mathbb{U}^{\\perp})=d_{\\mathcal{X}}-K$ . Then we can have a set of standard orthogonal basis for $\\mathbb{R}^{d}$ , defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\beta_{\\nabla\\oplus\\nabla^{\\perp}}=\\{\\frac{a_{1}}{\\|a_{1}\\|},\\frac{b_{1}}{\\|b_{1}\\|},\\frac{a_{2}}{\\|a_{2}\\|},\\frac{b_{2}}{\\|b_{2}\\|},\\cdots,\\frac{a_{K_{1}}}{\\|a_{K_{1}}\\|},\\frac{b_{K_{1}}}{\\|b_{K_{1}}\\|},\\frac{\\nu_{1}}{\\|\\mathbf{u}\\|},\\frac{\\nu_{2}}{\\|\\mathbf{u}\\|},\\cdots,\\frac{\\nu_{K_{2}}}{\\|\\mathbf{u}\\|},u_{1}^{\\perp},\\cdots,u_{d_{x}-K}^{\\perp}\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ${\\pmb u}_{1}^{\\perp},\\cdot\\cdot\\cdot\\mathbf{\\nabla},{\\pmb u}_{d_{\\mathcal{X}}-K}^{\\perp}$ are the standard orthogornal basis of $\\mathbb{U}^{\\perp}$ . Then we can derive that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{K_{1}}\\frac{{\\pmb a}_{s}{\\pmb a}_{s}^{\\top}}{\\|{\\pmb a}_{s}\\|^{2}}+\\sum_{s=1}^{K_{1}}\\frac{{\\pmb b}_{s}{\\pmb b}_{s}^{\\top}}{\\|{\\pmb b}_{s}\\|^{2}}+\\sum_{r=1}^{K_{2}}\\frac{{\\pmb\\nu}_{r}{\\pmb\\nu}_{r}^{\\top}}{\\|{\\pmb u}\\|^{2}}+\\sum_{w=1}^{d_{\\chi}-K}u_{w}^{\\bot}{\\pmb u}_{w}^{\\bot}^{\\top}={\\bf I}_{d_{\\chi}\\times d_{\\chi}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 22. (Partial Statement of Lemma $^{\\,l}$ ). $\\mathbb{E}[\\mathbf{W}_{Q}^{\\alpha}]$ and $\\mathbb{E}[\\mathbf{W}_{K}^{\\alpha}]$ are identical and symmetric during the whole iterations. We can decompose $\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}]$ and $\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}{}^{(t)}]$ by (scaled) idempotent matrices. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}(t)}]=\\displaystyle\\sum_{s=1}^{K_{1}}\\alpha_{Q,s}^{(t)}\\cdot\\frac{\\mathbf{a}_{s}\\mathbf{a}_{s}\\mathbf{a}_{s}^{\\top}}{\\|\\mathbf{a}_{s}\\|^{4}}+\\displaystyle\\sum_{s=1}^{K_{1}}\\beta_{Q,s}^{(t)}\\cdot\\frac{b_{s}b_{s}^{\\top}}{\\|b_{s}\\|^{4}}+\\displaystyle\\sum_{r=1}^{K_{2}}\\tau_{Q,r}^{(t)}\\cdot\\frac{\\nu_{r}\\nu_{r}^{\\top}}{\\|\\mathbf{u}\\|^{4}}+\\displaystyle\\sum_{w=1}^{K-K}\\rho_{Q,w}^{(t)}\\cdot u_{w}^{\\perp}u_{w}^{\\perp\\top},}\\\\ &{\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}(t)}]=\\displaystyle\\sum_{s=1}^{K_{1}}\\alpha_{K,s}^{(t)}\\cdot\\frac{\\mathbf{a}_{s}\\mathbf{a}_{s}\\mathbf{7}}{\\|a_{s}\\|^{4}}+\\displaystyle\\sum_{s=1}^{K_{1}}\\beta_{K,s}^{(t)}\\cdot\\frac{b_{s}b_{s}^{\\top}}{\\|b_{s}\\|^{4}}+\\displaystyle\\sum_{r=1}^{K_{2}}\\tau_{K,r}^{(t)}\\cdot\\frac{\\nu_{r}\\nu_{r}^{\\top}}{\\|\\mathbf{u}\\|^{4}}+\\displaystyle\\sum_{w=1}^{K-K}\\rho_{K,w}^{(t)}\\cdot u_{w}^{\\perp}u_{w}^{\\perp\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\alpha_{Q,s}^{(t)}$ and $\\alpha_{K,s}^{(t)}$ represent the concept learning process, $\\beta_{Q,s}^{(t)}$ and $\\beta_{K,s}^{(t)}$ represent the concept-specific semantic learning process and $\\tau_{Q,r}^{(t)},\\tau_{K,r}^{(t)},\\rho_{Q,w}^{(t)},\\rho_{K,w}^{(t)}$ represent the memorization of the concept irrelevant noise. ", "page_idx": 26}, {"type": "text", "text": "Proof. Apparently they hold at $t=0$ , suppose it holds at step $t$ , thus ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}^{(t)}]=\\mathbb{E}[(\\mathbf{W}_{K}^{\\mathbf{x}}^{(t)})^{\\top}]=\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}]=\\mathbb{E}[(\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)})^{\\top}],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we examine $t+1$ . It holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{B}_{t}}[\\mathbf{W}_{K}^{(t+1)}\\mid\\mathbb{E}({\\Psi^{\\prime}}^{(t)})]=\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}^{(t)}]-\\eta_{t}\\mathbb{E}_{\\boldsymbol{B}_{t}}[\\partial_{\\mathbf{W}_{K}^{\\mathbf{x}}}(t)\\,L\\boldsymbol{B}_{t}\\big(\\mathbb{E}({\\Psi^{\\prime}}^{(t)})\\big)]}\\\\ &{\\mathbb{E}_{\\boldsymbol{B}_{t}}[\\mathbf{W}_{Q}^{(t+1)}\\mid\\mathbb{E}(\\mathbb{E}(\\Psi^{\\prime}^{(t)}))]=\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}}^{(t)}]-\\eta_{t}\\mathbb{E}_{\\boldsymbol{B}_{t}}[\\partial_{\\mathbf{W}_{Q}^{\\mathbf{x}}}(t)\\,L\\boldsymbol{B}_{t}\\big(\\mathbb{E}({\\Psi^{\\prime}}^{(t)})\\big)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, we see $\\mathbb{E}(\\Psi^{\\prime}^{(t)})$ as fixed matrices and the expectation $\\mathbb{E}_{\\boldsymbol{B}_{t}}\\left[\\cdot\\right]$ is taken over the stochastic batch at the time step $t$ . As we are considering expectation over the isotropic prompt distribution, which can be seen as a noiseless distribution with an averaged categories of words and labels, the expected gradient form could be written as symmetric form: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{B}_{t}}\\big[\\mathbf{W}_{K}^{\\mathbf{x}}^{\\{t+1\\}}\\mid\\mathbb{E}\\big(\\mathbf{\\Psi}^{\\prime(t)}\\big)\\big]-\\mathbb{E}\\big[\\mathbf{W}_{K}^{\\mathbf{x}}^{\\{t\\}}\\big]=\\!\\underset{s=1}{\\overset{K_{1}}{\\sum}}(a_{K,s}^{(t)}\\mathbf{a}_{s}\\mathbf{a}_{s}^{\\top}+b_{K,s}^{(t)}b_{s}b_{s}^{\\top})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\lambda\\underset{r=1}{\\overset{K_{2}}{\\sum}}c_{Q,r}^{(t)}\\nu_{r}\\nu_{r}^{\\top}\\,+\\displaystyle\\sum_{w=1}^{d_{X}-2K_{1}-K_{2}}d_{K,w}^{(t)}\\cdot\\mathbf{\\Psi}^{\\mathbf{u}_{w}}u_{w}^{\\top})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with some coefficients a(Kt),s, $a_{K,s}^{(t)},b_{K,s}^{(t)},c_{Q,r}^{(t)},d_{K,w}^{(t)},\\forall s\\in[K_{1}],r\\in[K_{2}],w\\in[d_{\\mathcal{X}}-2K_{1}-K_{2}].$ It\u2019s direct to check that $\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}]$ also has the exactly same outcome. The proof is completed. ", "page_idx": 26}, {"type": "text", "text": "Worth noting that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu_{s}^{e^{\\top}}\\mathbb{E}[\\mathbf{W}_{Q}^{x}{(t)}]\\mu_{s}^{e}=\\alpha_{Q,s}^{(t)}+\\beta_{Q,s}^{(t)},}&{\\mu_{s}^{e^{\\top}}\\mathbb{E}[\\mathbf{W}_{K}^{x}{(t)}]\\mu_{s}^{e}=\\alpha_{K,s}^{(t)}+\\beta_{K,s}^{(t)},}\\\\ {\\mu_{s}^{-e^{\\top}}\\mathbb{E}[\\mathbf{W}_{Q}^{x}{(t)}]\\mu_{s}^{e}=\\alpha_{Q,s}^{(t)}-\\beta_{Q,s}^{(t)},}&{\\mu_{s}^{-e^{\\top}}\\mathbb{E}[\\mathbf{W}_{K}^{x}{(t)}]\\mu_{s}^{e}=\\alpha_{K,s}^{(t)}-\\beta_{K,s}^{(t)},}\\\\ {\\nu_{r}^{\\top}\\mathbb{E}[\\mathbf{W}_{Q}^{x}{(t)}]\\nu_{r}=\\tau_{Q,r}^{(t)},}&{\\nu_{r}^{\\top}\\mathbb{E}[\\mathbf{W}_{K}^{x}{(t)}]\\nu_{r}=\\tau_{K,r}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We will also have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}^{\\left(t\\right)}]\\mu_{s}^{e}\\big)^{\\top}\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\left(t\\right)}]\\mu_{s}^{e}=\\alpha_{Q,s}^{\\left(t\\right)}\\cdot\\alpha_{K,s}^{\\left(t\\right)}/\\|\\mathbf{a}_{s}\\|^{2}+\\beta_{Q,s}^{\\left(t\\right)}\\cdot\\beta_{K,s}^{\\left(t\\right)}/\\|\\mathbf{b}_{s}\\|^{2},}\\\\ &{\\big(\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}\\left(t\\right)}]\\mu_{s}^{-e}\\big)^{\\top}\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\left(t\\right)}]\\mu_{s}^{e}=\\alpha_{Q,s}^{\\left(t\\right)}\\cdot\\alpha_{K,s}^{\\left(t\\right)}/\\|\\mathbf{a}_{s}\\|^{2}-\\beta_{Q,s}^{\\left(t\\right)}\\cdot\\beta_{K,s}^{\\left(t\\right)}/\\|\\mathbf{b}_{s}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for $\\forall e\\in[\\pm]$ and for $\\begin{array}{r}{\\forall e^{\\prime}\\in[\\pm],s^{\\prime}\\in[K_{1}],r\\in[K_{2}],w\\in[d x-K],\\forall\\mathbf{u}\\in\\{\\mu_{s^{\\prime}}^{e^{\\prime}},\\nu_{r},u_{w}^{\\perp}\\},}\\end{array}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\big(\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}{}^{(t)}]\\mathbf{u}\\big)^{\\top}\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}]\\pmb{\\mu}_{s}^{e}=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similar conclusions hold when the query vectors are $\\pmb{\\nu_{r}}$ and $u_{w}^{\\perp},\\forall r\\in[K_{2}],w\\in[d_{\\mathcal{X}}-K]$ . ", "page_idx": 27}, {"type": "text", "text": "Definition 5. Define $\\mathbb{Q}:=s p a n(\\mathbf{Q})$ and its complement space $\\mathbb{Q}^{\\perp}$ , we can decompose $i$ -th row of $\\mathbf{W}_{O}^{y}$ via the following decomposition: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{W}_{O_{(i,:)}}^{y}(t)]=\\sum_{k=1}^{K_{1}}\\alpha_{O_{(i,:)},k}^{(t)}\\cdot\\frac{{c_{k}}^{\\top}}{\\|c_{k}\\|^{2}}+\\sum_{k=1}^{K_{1}}\\beta_{O_{(i,:)},k}^{(t)}\\cdot\\frac{{d_{k}}^{\\top}}{\\|d_{k}\\|^{2}}+\\sum_{w=1}^{d_{y}-K_{1}}\\rho_{O_{(i,:)},w}^{(t)}\\cdot q_{w}^{\\perp}{^\\top}\\in\\mathbb{R}^{1\\times d_{y}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $q_{1}^{\\perp},\\cdot\\cdot\\cdot\\;,q_{d_{\\mathcal{Y}}-K_{1}}^{\\perp}$ are the standard orthogonal basis of the complement space $\\mathbb{Q}^{\\perp}$ . Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{W}_{O_{(i,\\cdot)}}^{y}{}^{(t)}]{q}_{k}^{e}=\\alpha_{O_{(i,\\cdot)},k}^{(t)}+e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for $\\forall e\\in[\\pm],i\\in[m],k\\in[K_{1}]$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma 23. At initialization, for some $e\\in[\\pm]$ and $\\forall k\\in[K_{1}]$ , define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\zeta_{k}^{e}:=2\\mathop{\\mathbb{E}}_{n\\in\\mathcal{V}_{k}^{e}}[\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(0)})}_{l}^{n}]-1=\\frac{\\exp(\\sigma_{0}^{2}||b_{k}||^{2})-\\exp(-\\sigma_{0}^{2}||b_{k}||^{2})}{\\exp(\\sigma_{0}^{2}||b_{k}||^{2})+\\exp(-\\sigma_{0}^{2}||b_{k}||^{2})},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then we have some $\\omega_{\\zeta_{k}^{e}}\\in(0,\\omega_{\\zeta_{k}^{e}}^{\\prime})$ where $\\omega_{\\zeta_{k}^{e}}^{\\prime}<1$ , the following will hold ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{V}_{t}^{\\star,\\star}}{\\vert\\mathbf{a}_{k}\\vert\\vert^{2}}=\\frac{\\mathbf{\\eta}_{k,\\star}^{\\star,\\star}}{\\vert\\mathbf{\\vert}\\mathbf{a}_{k}\\vert\\vert^{2}}=\\frac{\\mathbf{\\eta}_{k,\\star}^{\\star,\\star}}{\\vert\\mathbf{\\vert}\\mathbf{b}_{k}\\vert\\vert^{2}}=\\frac{\\mathbf{\\eta}_{k,\\star}^{\\star,\\star}}{\\vert\\mathbf{\\vert}\\mathbf{u}\\vert\\vert^{2}}=\\frac{\\mathbf{\\eta}_{k,\\star}^{\\star,\\prime}}{\\vert\\mathbf{\\vert}\\mathbf{u}\\vert\\vert^{2}}=\\rho_{Q,w}^{\\mathsf{v}}=\\rho_{K,w}^{\\mathsf{v}}=\\sigma_{0},}\\\\ &{\\frac{\\mathbb{E}_{\\mathbf{\\eta}_{k}}}{\\epsilon\\nu_{k}^{\\mathsf{c}}}\\vert\\mathcal{U}_{k,n}^{\\epsilon}(0)\\vert\\vert=\\Big\\vert\\{i\\in[m]\\,\\vert\\mathbf{\\eta}_{k}\\vert=\\frac{e}{m},\\alpha_{O_{(i,\\star),k}}^{(0)}+e\\zeta_{k}^{\\epsilon}\\cdot\\beta_{O_{(i,\\star),k}}^{(0)}>0\\}\\Big\\vert\\ge\\frac{m}{4}-\\sqrt{\\frac{m\\log(\\frac{10K_{1}}{\\delta})}{2}}\\ge\\frac{m}{8},}\\\\ &{\\frac{\\mathbb{E}_{\\mathbf{\\eta}_{k}}}{\\epsilon\\nu_{k}^{\\mathsf{c}}}\\vert\\vert\\mathcal{W}_{k,n}^{\\epsilon}(0)-\\mathcal{U}_{k,n}^{\\epsilon}(0)\\vert\\vert=\\Big\\vert\\{i\\in[m]\\,\\vert\\,\\mathbf{\\eta}_{1}=-\\frac{e}{m},\\alpha_{O_{(i,\\star),k}}^{(0)}+e\\zeta_{k}^{\\epsilon}\\beta_{O_{(i,\\star),k}}^{(0)}>0\\}\\Big\\vert\\le\\frac{m}{4}+\\sqrt{\\frac{m\\log(\\frac{10K_{1}}{\\delta})}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\underset{n\\in\\mathcal{V}_{k}^{\\epsilon}}{\\mathbb{E}}[|\\mathcal{U}_{k,n}^{e}(0)\\cap(\\mathcal{W}_{k,n}^{-e}(0)-\\mathcal{U}_{k,n}^{-e}(0))|]\\leq\\frac{(1+\\omega_{\\zeta_{k}^{e}})m}{8}+\\sqrt{\\frac{m\\log(\\frac{10K_{1}}{\\delta})}{2}}\\leq\\frac{(1+\\omega_{\\zeta_{k}^{e}}^{\\prime})m}{8},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\underset{n\\in\\mathcal{V}_{k}^{\\epsilon}}{\\mathbb{E}}[|\\mathcal{U}_{k,n}^{e}(0)-(\\mathcal{W}_{k,n}^{-e}(0)-\\mathcal{U}_{k,n}^{-e}(0))|]\\geq\\frac{(1-\\omega_{\\zeta_{k}^{e}})m}{8}-\\sqrt{\\frac{m\\log(\\frac{10K_{1}}{\\delta})}{2}}\\geq\\frac{(1-\\omega_{\\zeta_{k}^{e}}^{\\prime})m}{8}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The parameter $\\omega_{\\zeta_{k}^{e}}^{\\prime}$ is determined by $\\sigma_{0},\\sigma_{1},\\left\\|\\pmb{a}_{k}\\right\\|,\\left\\|\\pmb{b}_{k}\\right\\|,\\left\\|\\pmb{c}_{k}\\right\\|$ and $\\left\\|d_{k}\\right\\|$ ", "page_idx": 27}, {"type": "text", "text": "Proof. We have that $\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[\\mathcal{U}_{k,n}^{e}(0)\\cap(\\mathcal{W}_{k,n}^{-e}(0)-\\mathcal{U}_{k,n}^{-e}(0))]\\neq\\emptyset$ . By Lemma 7, we see that for ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\zeta_{k}^{e}=2\\operatorname*{\\mathbb{1}}_{n\\in\\mathcal{V}_{k}}[\\sum_{l\\in S_{n,k}^{y_{S_{n}}}}{(\\sigma_{S}^{(0)})}_{l}^{n}]-1=\\frac{\\exp(\\sigma_{0}^{2}\\|b_{k}\\|^{2})-\\exp(-\\sigma_{0}^{2}\\|b_{k}\\|^{2})}{\\exp(\\sigma_{0}^{2}\\|b_{k}\\|^{2})+\\exp(-\\sigma_{0}^{2}\\|b_{k}\\|^{2})},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we can have corresponding $\\omega_{\\zeta_{k}^{e}}\\in(0,\\omega_{\\zeta_{k}^{e}}^{\\prime})$ where $\\omega_{\\zeta_{k}^{e}}^{\\prime}<1$ to ensure the conclusion holds. ", "page_idx": 27}, {"type": "text", "text": "Lemma 24. (Coefficient Update) Denote $\\mathbb{E}(\\boldsymbol{\\Psi}^{\\prime(t)})\\;:=\\;\\{\\mathbb{E}(\\mathbf{W}_{Q}^{\\mathbf{x}\\;(t)}),\\mathbb{E}(\\mathbf{W}_{K}^{\\mathbf{x}\\;(t)}),\\mathbb{E}(\\mathbf{W}_{O_{(i,\\cdot)}}^{(t)})\\}$ , where the expectation $\\mathbb{E}[\\cdot]$ is taken over the stochastic batches. We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{c_{12}^{(a)}=\\frac{m_{l}^{2}}{{\\beta_{1}}},\\quad\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{+\\frac{m_{l}^{2}}{{\\beta_{2}}},}\\\\ &{=\\frac{m_{l}^{2}}{{\\beta_{3}}},\\quad\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{+\\frac{m_{l}^{2}m_{l}^{2}}{R_{p}^{2}},}\\\\ &{=\\frac{m_{l}^{2}}{\\beta_{3}^{2}}-\\frac{m_{l}^{2}}{{\\beta_{3}}},\\quad\\forall\\quad\\forall\\;\\mathbf{u}_{12}^{(a)}=\\frac{\\beta_{1}}{{\\beta_{2}}}[\\mathbf{u}_{11}^{(a)},\\mathbf{u}_{12}^{(a)}][\\mathbf{u}_{12}^{(a)},\\mathbf{u}_{11}^{(a)}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{\\left(\\frac{m_{l}^{2}}{{\\beta_{3}}}-\\frac{m_{l}^{2}}{p^{2}}\\!+\\!\\frac{m_{l}^{2}m_{l}^{2}}{{\\beta_{1}}}\\!+\\!\\frac{m_{u}^{2}}{{\\beta_{2}}}[\\mathbf{u}_{12}^{(a)},\\mathbf{u}_{11}^{(a)}][\\mathbf{u}_{12}^{(a)},\\mathbf{u}_{11}^{(a)}]\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\;\\left.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $e\\in[\\pm],s\\in[K_{1}],r\\in[K_{2}],w\\in[d_{X}-K].$ ", "page_idx": 28}, {"type": "text", "text": "Lemma 25. For $\\forall k_{1}\\in[K_{1}]$ , we define $c_{k_{1}}:=\\frac{\\pmb q_{k_{1}}^{+}+\\pmb q_{k_{1}}^{-}}{2}$ and $d_{k_{1}}:=\\frac{\\pmb q_{k_{1}}^{+}-\\pmb q_{k_{1}}^{-}}{2}$ . By definition, we then have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{k_{1}}^{+}=c_{k_{1}}+d_{k_{1}},\\quad q_{k_{1}}^{-}=c_{k_{1}}-d_{k_{1}},}\\\\ &{\\langle c_{k_{1}},d_{k_{1}}\\rangle=0,\\quad\\{c_{k_{1}},d_{k_{1}}\\}\\perp\\{c_{k_{1}^{\\prime}},d_{k_{1}^{\\prime}}\\},}\\\\ &{\\langle q_{k_{1}}^{+},q_{k_{1}}^{-}\\rangle=\\|c_{k_{1}}\\|^{2}-\\|d_{k_{1}}\\|^{2},\\quad\\|q_{k_{1}}^{\\pm}\\|^{2}=\\|c_{k_{1}}\\|^{2}+\\|d_{k_{1}}\\|^{2}=\\|\\mathbf{u}\\|^{2},}\\\\ &{\\frac{1}{2}\\|\\mathbf{q}\\|^{2}<\\|c_{k_{1}}\\|^{2}\\leq\\frac{\\kappa_{y}+1}{2}\\|\\mathbf{q}\\|^{2},\\quad\\frac{-\\kappa_{y}+1}{2}\\|\\mathbf{q}\\|^{2}\\leq\\|d_{k_{1}}\\|^{2}<\\frac{1}{2}\\|\\mathbf{q}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "fo $r\\,\\forall k_{1}^{\\prime}\\neq k_{1}\\in[K_{1}].$ ", "page_idx": 28}, {"type": "text", "text": "Based on Lemma 22 and Lemma 24, the following two lemmas compute the update of attention\u2019s expected projection along non-feature and feature directions. ", "page_idx": 28}, {"type": "text", "text": "Lemma 26. For $t>0$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{Q,r}^{(t+1)}=(1-\\eta_{t}\\lambda)\\tau_{Q,r}^{(t)},\\quad\\tau_{K,r}^{(t+1)}=(1-\\eta_{t}\\lambda)\\tau_{K,r}^{(t)},}\\\\ &{\\rho_{Q,w}^{(t+1)}=(1-\\eta_{t}\\lambda)\\rho_{Q,w}^{(t)},\\quad\\rho_{K,w}^{(t+1)}=(1-\\eta_{t}\\lambda)\\rho_{K,r}^{(t)},}\\\\ &{\\rho_{O_{(i,\\cdot)},\\hat{w}}^{(t+1)}=(1-\\eta_{t}\\lambda)\\rho_{O_{(i,\\cdot)},\\hat{w}}^{(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $r\\in[K_{2}],w\\in[d_{\\mathcal{X}}-K],\\hat{w}\\in[d_{\\mathcal{Y}}-K_{1}].$ ", "page_idx": 28}, {"type": "text", "text": "Lemma 27. For $t>0$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{Q,k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\alpha_{Q,k}^{(t)},\\quad\\alpha_{K,k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\alpha_{K,k}^{(t)},}\\\\ &{\\iota_{Q,k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{Q,k}^{(t)}}\\\\ &{\\qquad\\qquad\\qquad-\\frac{4\\eta_{t}\\beta_{K,k}^{(t)}\\|b_{k}\\|^{4}}{K_{1}}\\sum_{e\\in[\\pm]}\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i_{+},\\cdot),k}^{(t)}}^{(t)}\\frac{\\mathbb{E}}{b_{n}}\\frac{\\mathbb{E}}{\\epsilon\\nu_{k}^{(t)}}\\big[\\ell_{n}^{'}\\,^{(t)}\\mathbf{1}_{O_{(i)}}^{n}(t)\\big(\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)(\\sum_{j\\in S_{n,k}^{-}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n})\\big],}\\\\ &{\\vartheta_{K,k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{K,k}^{(t)}}\\\\ &{\\qquad\\qquad\\qquad-\\,\\frac{4\\eta_{t}\\beta_{Q,k}^{(t)}\\|b_{k}\\|^{4}}{K_{1}}\\sum_{e\\in[\\pm]}\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i_{+},\\cdot),k}^{(t)}}^{(t)}\\frac{\\mathbb{E}}{\\epsilon\\nu_{k}^{(t)}}\\big[\\ell_{n}^{'}\\,^{(t)}\\mathbf{1}_{O_{(i)}}^{n}(t)\\big(\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)(\\sum_{j\\in S_{n,k}^{-}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. The deduction is direct by the symmetric property of prompt distribution in Lemma 22, and the gradient forms in Lemma 15 and Lemma 16. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "This lemma reveals that the attention layer mainly serves to learn the different semantic part of each concept, and hardly have interest in learning the shared co-concept part. Also, collaborating with Lemma 22, we see that $\\beta_{Q,k}^{(t+1)}=\\beta_{K,k}^{(t+1)}$ , this indicates that the signal of $\\beta_{Q,k}^{(t)}\\cdot\\bar{\\beta}_{K,k}^{(t)}$ would remain positive. ", "page_idx": 29}, {"type": "text", "text": "Also, by the symmetry property of learning progress denoted in Lemma 22, we see that $\\forall k\\in[K_{1}],\\alpha_{Q,k}^{(t)}=$ $\\alpha_{K,k}^{(t)},\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}$ \u03b2(Kt,)k. Observe that for \u2200k \u2208[K1], ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{n\\in\\mathcal{V}_{k}^{y}s_{n}}{\\mathbb{E}}[\\,\\sum_{\\varrho\\leq v_{n,k}^{y}}(\\sigma_{S}^{(t)})_{j}^{n}]=\\frac{\\exp(\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})}{\\exp(\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})+\\exp(-\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})},}\\\\ {\\underset{n\\in\\mathcal{V}_{k}^{y}s_{n}}{\\mathbb{E}}[\\,\\sum_{\\varrho\\leq v_{n,k}^{-y}}(\\sigma_{S}^{(t)})_{j}^{n}]=\\frac{\\exp(-\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})}{\\exp(\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})+\\exp(-\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We see from Lemma 7 that \u03b1(Q0,)k $\\alpha_{Q,k}^{(0)}=\\alpha_{K,k}^{(0)}=\\sigma_{0}\\|\\pmb{a}_{k}\\|^{2},\\beta_{Q,k}^{(0)}=\\beta_{K,k}^{(0)}=\\sigma_{0}\\|\\pmb{b}_{k}\\|^{2}$ . Therefore, for $t=0,\\forall k\\in$ $[K_{1}]$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}\\big[\\displaystyle\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}\\big(\\sigma_{S}^{(0)}\\big)_{j}^{n}\\big]=\\frac{\\exp(\\sigma_{0}^{2}\\|b_{k}\\|^{2})}{\\exp(\\sigma_{0}^{2}\\|b_{k}\\|^{2})+\\exp(-\\sigma_{0}^{2}\\|b_{k}\\|^{2})},}\\\\ &{\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}\\big[\\displaystyle\\sum_{j\\in S_{n,k}^{-y_{S_{n}}}}\\big(\\sigma_{S}^{(0)}\\big)_{j}^{n}\\big]=\\frac{\\exp(-\\sigma_{0}^{2}\\|b_{k}\\|^{2})}{\\exp(\\sigma_{0}^{2}\\|b_{k}\\|^{2})+\\exp(-\\sigma_{0}^{2}\\|b_{k}\\|^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Obviously, $\\begin{array}{r}{\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}\\big(\\sigma_{S}^{(0)}\\big)_{j}^{n}]~>~0.5~>~\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{-y_{S_{n}}}}\\big(\\sigma_{S}^{(0)}\\big)_{j}^{n}]}\\end{array}$ . Meanwhile we see that $\\begin{array}{r}{\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}\\big[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}\\big(\\sigma_{S}^{(0)}\\big)_{j}^{n}\\big]\\approx0.5}\\end{array}$ due to the small $\\sigma_{0}=O(\\lVert\\mathbf{u}\\rVert^{-2})$ by Condition 1. ", "page_idx": 29}, {"type": "text", "text": "The observation in Eq. (39), collaborating with the positiveness of \u03b2(Qt,)k \u00b7 \u03b2(Kt,)k, we see that the inequality $\\begin{array}{r}{\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}\\!\\left[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\right]>\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}\\!\\left[\\sum_{j\\in S_{n,k}^{-y_{S_{n}}}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\right]}\\end{array}$ will remain during whole iteration. Also, by Eq. (39), we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{j}^{n})(\\sum_{j\\in S_{n,k}^{-}}(\\sigma_{S}^{(t)})_{j}^{n})]=\\Big(\\exp(\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})+\\exp(-\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})\\Big)^{-2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This observation under our expectation scenario greatly facilitate our analysis. Since $\\ell_{n}^{\\prime\\ (t)}<0$ , it\u2019s obvious that the signal of $\\mathbf{r}_{i}\\beta_{O_{(i,\\,)}}^{(t)},k$ will determine whether the neuron $i\\in[m]$ will serve to increase or decrease the $\\beta_{Q,k}^{(t)}$ and $\\beta_{K,k}^{(t)}$ during the gradient update. We therefore start to analyze the MLP\u2019s update below based on Lemma 16. ", "page_idx": 29}, {"type": "text", "text": "Lemma 28. For $t>0$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{O(\\iota_{+}),k}^{(t+1)}=(1-\\eta_{1}\\lambda)\\alpha_{O(\\iota_{+}),k}^{(t)}-\\eta_{1}\\underbrace{\\frac{\\|c_{k}\\|^{2}}{2K_{1}}\\sum_{\\ell\\in\\mathbb{Z}_{k}^{\\ell}\\left\\{\\alpha_{k}+\\frac{\\mathbb{R}}{n\\ell\\sqrt{\\epsilon_{0}}}(\\ell_{n}^{\\prime}(\\ell^{\\prime}(\\ell)\\mathbb{I}_{R_{0}}^{n}(\\ell))\\right\\}}}_{\\mathrm{E}\\{I_{O(\\iota_{+}),k}^{\\ell},\\alpha_{k},\\cos\\}}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\end{\\mathbb{E}}\\\\ &{=\\eta\\underbrace{\\frac{(K_{1}-1)\\|c_{k}\\|^{2}}{2K_{1}K_{1}K}}_{\\in\\mathbb{C}_{k}^{\\ell}\\left\\{\\frac{1}{n}\\right\\}}_{\\in\\mathbb{C}_{k}^{\\ell}\\left\\{\\frac{1}{n}\\right\\}}=\\Big[\\!\\operatorname{er}_{\\ell^{\\prime}(\\frac{\\mathbb{R}}{n\\sqrt{\\epsilon_{0}}}+\\frac{\\mathbb{R}^{\\prime}}{n\\sqrt{\\epsilon_{0}}}(\\ell_{n}^{\\prime}(\\ell)\\mathbb{I}_{O(\\iota_{0})}^{n}(\\ell))\\!\\right]}\\!,}\\\\ &{\\beta_{O(\\iota_{+}),k}^{(t+1)}=(1-\\eta_{1}\\lambda)\\beta_{O(\\iota_{+}),k}^{(t)}-\\frac{\\eta_{1}\\|d_{k}\\|^{2}\\mathbb{I}_{\\mathbb{C}_{k}}}{2K_{1}}\\sum_{\\ell\\in\\mathbb{Z}_{k}^{\\ell}\\left\\{\\alpha_{k}+\\frac{\\mathbb{R}}{n\\ell\\sqrt{\\epsilon_{0}}}(\\ell_{n}^{\\prime}(1)\\mathbb{I}_{O(\\iota_{0})}^{n}(\\ell))\\!\\right\\}}\\!\\!\\!\\!\\!\\!\\!}\\\\ &{\\phantom{m m m m m m m}_{\\mathrm{O(\\iota_{+}),k}^{\\ell}}=\\!\\sum_{\\ell^{\\prime}(\\frac{\\mathbb{R}^{\\prime \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $k\\in[K_{1}]$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. The proof is direct by the symmetric property of prompt distribution in Lemma 22, and the gradient forms in Lemma 19 and Lemma 20. \u53e3 sAunit si noteurr eisnttiunitgi ofan ctth iast  tihf asti tmhile $\\mathbb{E}(I_{O_{(i,\\,.)},\\mathbf{c}_{k},\\mathrm{chaos}}^{(t)})$ vaalrsioo ucso nftierlidbsu t(ecso tnoc etphtes )l,e tahren ilneagr noif $k$ - tphr occoenssc ecpatn.  hTehlips  iantcetugraalltye and facilitate the learning. The following lemma demonstrate the lower bound of the attention assignment, which emerge from the good property of our expected attention. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Lemma 29. For a certain iterations $t\\in(0,T_{1})$ , for $\\forall k\\in[K_{1}],e\\in[\\pm]$ , we have ", "page_idx": 30}, {"type": "text", "text": "1. Tdehae cntievuartoend .s eAt $\\mathbb{E}[(\\mathcal{W}_{k,n}^{e}(t)-\\mathcal{U}_{k,n}^{e}(t))-\\mathcal{U}_{k,n}^{-e}(t)]$ cwreoauslidn gm, oannodt oanlil coafl ltyh idse ncreeuarsoen.  wAillsl og, eitt $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ $e\\cdot\\beta_{O_{(i,\\,,\\,)},k}^{(t)}$   \nholds that $e\\cdot\\beta_{O_{(i,\\,.\\,)},k}^{(t)}>0$ and $|\\alpha_{O_{(i,\\cdot)},k}^{(t)}|\\leq e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}$ ;   \n2. The neuron set $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ is non-increasing, and all neurons in it will turn imntoon $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ . Additionally, both $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ and $e\\cdot\\beta_{O_{(i,\\,.\\,)},k}^{(t)}$ would otonikcally increase. Also, it holds that $e\\cdot\\beta_{O_{(i,\\,.\\,)},k}^{(t)}>0$ $|\\alpha_{O_{(i,\\cdot)},k}^{(t)}|\\le e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}$ ; 3. Forn\u2208EVe[Uke,n(t) \u2229(Wk\u2212,en(t) \u2212U k\u2212,en(t))], the e \u00b7 \u03b2(Ot()i,\u00b7),k would monotonically increase. Besides, when there exists constant $C\\geq1$ such that   \n$\\begin{array}{r}{\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}({\\ell_{n}^{\\prime}}^{(t)})]\\le C_{n\\in\\mathcal{V}_{k}^{-e}}({\\ell_{n}^{\\prime}}^{(t)})].}\\end{array}$   \nthe $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ wo(ut)ld nbe cont(rti)buted to increa(ste), otherwise it will decrease. Also, $|\\alpha_{O_{(i,\\cdot)},k}^{(t)}|\\geq$ $\\begin{array}{r}{\\mathbb{E}[|e(2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)\\beta_{O_{(i,.)},k}^{(t)}|]}\\end{array}$ and $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]>0,$ ;   \n4. All the neurons in $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)\\!-\\!\\mathcal{U}_{k,n}^{-e}(t))]$ will ultimately either have its coefficient update stuck due to regularization, or grow into a changing margin into $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\alpha_{O_{(i,..)},k}^{(t)}\\approx\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)e\\beta_{O_{(i,.)},k}^{(t)}].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By Lemma 28, we see that $\\forall i\\,\\in\\,\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)]$ , $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ and $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ would be contributed by $\\mathcal{V}_{k}^{e}$ to increase, and also $\\forall i\\,\\in\\,\\mathbb{E}[\\mathcal{W}_{k,n}^{e}(t)\\,-\\mathcal{U}_{k,n}^{e}(t)]$ , $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ and $e\\beta_{O_{(i,\\,\\cdot)},k}^{(t)}$ would be contributed by $\\mathcal{V}_{k}^{e}$ to decrease. As such, the first and second point hold naturally by definition. The ultimate transformation of $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ into $\\mathbf{\\,\\hat{\\mathbb{R}}}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{\\div e}(\\bar{t})-\\mathcal{U}_{k,n}^{-e}(t))]$ attributes to the faster changing ", "page_idx": 30}, {"type": "text", "text": "speed of $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ com $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ in the neuron sets $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ , whose l(eta)rning $\\big(\\lVert c_{k}\\rVert/\\lVert d_{k}\\rVert\\big)^{2}$ T ahwcteiilrlve arfteoermdea ,i ftnoh rpe  ooaspbiptsioovsleiu,t tteeh  leva adblieuslces ,u ostfsh $\\alpha_{O_{(i,\\,.)},k}^{(t)}$ aimss sps litehmtaeptld e.o  fs   \n$\\begin{array}{r}{\\big(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-\\sum_{l\\in S_{n,k}^{-e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big)}\\end{array}$ $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ $\\mathbf{r}_{i}$ ", "page_idx": 31}, {"type": "text", "text": "Considering the growth of $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ , by $\\pi_{k}^{+}=\\pi_{k}^{-},P_{l,2k-1}=P_{l,2k}^{n}=\\frac{1}{2}$ , we know ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}\\big[\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big]=\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}\\big[\\sum_{l\\in S_{n,k}^{-e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "hence if $i\\in\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ , it indicates that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\alpha_{O_{(i,.)},k}^{(t)}\\pm(2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)e\\beta_{O_{(i,.)},k}^{(t)}]\\ge0.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We see that for $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ , the $\\mathbb{E}[\\mathcal{V}_{k}^{e}]$ will serve to increase the $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ , but $\\mathbb{E}[\\mathcal{V}_{k}^{-e}]$ will serve to decrease the $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ . The contribution will tend to be positive if ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathbb{E}}{\\nu\\mathbb{V}_{k}^{\\prime}}(\\ell_{n}^{\\prime}({^{t}}^{(t)}(i\\in\\mathcal{U}_{k,n}^{\\leftarrow}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))))]\\ge\\underbrace{\\mathbb{E}}_{n\\in\\mathcal{V}_{k}^{-e}}(\\ell_{n}^{\\prime}{^{(t)}}\\mathbb{I}(i\\in\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, as $\\begin{array}{r}{\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)e\\beta_{O_{(i,\\cdot)},k}^{(t)}]}\\end{array}$ of the neurons in and $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ will continue to grow, and finally it will be comparable to the $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ . Otherwise it will continue to grow $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ $\\begin{array}{r}{\\frac{\\mathbb{E}}{\\epsilon\\nu_{k}^{\\epsilon}}(\\ell_{n}^{\\prime}{}^{(t)}\\mathbb{1}(i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\epsilon}(t)\\!-\\!\\mathcal{U}_{k,n}^{\\epsilon}(t)\\cap\\mathcal{U}_{k,n}^{-\\epsilon}(t)))]\\operatorname*{and}_{n\\in\\mathcal{V}_{k}^{-\\epsilon}}(\\ell_{n}^{\\prime}{}^{(t)}\\mathbb{1}(i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\epsilon}(t)\\!-\\!\\mathcal{U}_{k,n}^{\\epsilon}(t)\\cap\\mathcal{U}_{k,n}^{-\\epsilon}(t)))].}\\end{array}$ Quantatively this is validated by our later results in Lemma 32 where the $\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}(\\boldsymbol{\\ell}_{n}^{\\prime}(\\boldsymbol{\\ t}))-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}(\\boldsymbol{\\ell}_{n}^{\\prime}(\\boldsymbol{t}))$ would be controlled by the initialization. Interestingly, we see that as $\\begin{array}{r}{\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)e\\beta_{O_{(i,\\cdot)},k}^{(t)}]}\\end{array}$ grows up, its scale will surpass those of $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ . Under this scenario, $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ will turn into E[Uke,n(t) \u2212(Wk\u2212,en(t) \u2212U k\u2212,en(t))], where E[\u03b1(Ot()i,\u00b7),k] again continues to grow. Thus finally we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\alpha_{O_{(i,..)},k}^{(t)}\\approx\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)e\\beta_{O_{(i,.)},k}^{(t)}].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma 4 will show that the growing of $\\begin{array}{r}{\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)e\\beta_{O_{(i,\\cdot)},k}^{(t)}]}\\end{array}$ will stuck, and thus the growing of $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ will also stuck at the changing margin from $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)\\,{-}\\mathcal{U}_{k,n}^{-e}(t))]$ into $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)-$ $(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))].$   \nThe proof is completed. ", "page_idx": 31}, {"type": "text", "text": "Proof. Proof of Lemma 2. To examine the 0-1 loss, by definition, we know ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\mathcal{P}_{n}}^{\\{0\\},-1}\\big(\\mathbb{E}(\\mathcal{H}^{\\{0\\}})=\\mathbb{P}_{\\mathcal{S}_{n}\\sim\\mathcal{P}_{n}}(y_{\\mathcal{S}_{n}},y_{\\mathcal{S}_{n}}^{\\mathbb{I}},f(\\mathbb{E}(\\mathcal{S}_{n})_{\\mathcal{S}_{n}})\\mathbb{E}(\\psi_{n}^{\\theta\\})\\leq0),}\\\\ &{=\\mathbb{P}_{\\mathcal{S}_{n}\\sim\\mathcal{P}_{n}}(y_{\\mathcal{S}_{n}},\\ \\ldots,\\underline{{F}}_{\\mathcal{C}_{n}|\\mathcal{S}_{n}}^{\\mathbb{I}})\\underset{s\\in\\{r_{0},\\ldots,\\frac{1}{r_{0}n}\\}}{\\sum}\\frac{\\mathbb{E}\\big|\\mathcal{F}(\\mathcal{R}_{n})(\\mathbb{W}_{\\mathcal{O}_{t,i}}^{\\mathbb{I}},\\ (\\mathcal{O}_{s}^{\\mathbb{I}})_{\\mathcal{H}}^{\\mathbb{I}})\\big|\\leq0),}\\\\ &{=\\mathbb{P}(\\mathbb{E}|\\mathcal{E}_{s},\\ \\bigg(\\underset{s\\in\\mathcal{C}_{n}}{\\sum}\\underset{\\ell\\in\\mathcal{C}_{n}}{\\sum}\\underset{\\ell\\in\\mathcal{C}_{n}}{\\sum}\\underset{\\ell\\in\\mathcal{C}_{n}}{\\sum}\\underset{\\ell\\in\\mathcal{C}_{n}}{\\sum}\\big(\\mathbb{W}_{\\mathcal{O}_{t,i}}^{\\mathbb{I}},\\ (\\mathcal{O}_{s}^{\\mathbb{I}})_{\\mathcal{H}}^{\\mathbb{I}})\\bigg)\\bigg|\\leq0),}\\\\ &{=\\mathbb{P}(\\mathbb{E}\\bigg[\\underset{s\\in\\mathcal{C}_{n}}{\\sum}\\underset{\\ell\\in\\mathcal{C}_{n}}{\\sum}\\underset{s\\in\\mathcal{C}_{n}}{\\sum}\\bigg(\\omega_{\\mathcal{O}_{t,i}}^{\\theta},\\ldots+(2\\underset{\\mathcal{C}}{\\sum}\\underset{\\ell\\in\\mathcal{C}_{n}^{\\mathbb{I}}}{\\sum}\\underset{\\ell\\in\\mathcal{C}_{n}^{\\mathbb{I}}}{\\sum})_{\\mathcal{H}}^{\\mathbb{I}})\\mathbb{B}_{\\mathcal{O}_{t,i}}\\bigg)\\mathbb{E}\\bigg]}\\\\ &{\\qquad\\qquad-\\underset{\\mathcal{C}_{n}\\in\\mathcal{C}_{n}}{\\sum}\\int_{\\mathbb{C}}\\bigg( \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, a sufficient condition for $L_{\\mathcal{D}^{*}}^{0-1}(\\mathbb{E}(\\Psi^{t}))=0$ is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\displaystyle\\sum_{i\\in\\mathcal{U}_{k,n}^{e}(t)}\\alpha_{O_{(i,\\cdot)},k}^{(t)}+(2\\displaystyle\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}]\\geq\\!\\mathbb{E}[\\displaystyle\\sum_{\\substack{i\\in\\mathcal{W}_{k,n}^{e}(t)-u_{k,n}^{e}(t)}}\\alpha_{O_{(i,\\cdot)},k}^{(t)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(2\\displaystyle\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for $\\forall k\\in[K_{1}],e\\in[\\pm]$ . ", "page_idx": 32}, {"type": "text", "text": "We know $\\forall i\\in\\mathcal{U}_{k,n}^{e}(t),\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}]$ in the left side of the inequality is increasing, and $\\forall i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{e}(t)\\mathrm{~-~}$ $\\mathcal{U}_{k,n}^{e}(t)]$ , the $\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\,\\cdot\\,)},k}^{(t)}]$ in the right side of the inequality is decreasing, which is a good news since we want the left side exceed the right side. By Lemma 29, we see that all the neurons in $\\mathbb{E}[(\\mathcal{W}_{k,n}^{e}(t)\\!-\\!\\mathcal{U}_{k,n}^{e}(t))\\!-\\!\\mathcal{U}_{k,n}^{-e}(t)]$ will be deactivated, and all the neurons in $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ will turn into $\\underset{n\\in\\nu_{k}^{e}}{\\mathbb{E}}[\\mathcal{U}_{k,n}^{e}(t)\\cap$ $(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))].$ ", "page_idx": 32}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "I.1 First Stage: Growing of Coefficient ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this stage, the coefficient update dynamic is continually changing without being much influenced by the comparably feeble regularization. Also, the impact of the decaying learning step $\\eta_{t}$ is under controlled during several periods, which can be safely done due to small initialization by a large $\\gamma$ , as well as the slow quadratic decaying nature of the derivative of $\\eta_{t}^{\\prime}$ . We see that at initialization, by Lemma 7 and Lemma 23, the $\\underset{S_{n}\\sim\\mathcal{D}_{S}}{\\mathbb{E}}[f(\\mathbf{E}(S_{n});\\bar{\\Psi}^{(0)})]$ satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{S_{n}\\sim\\mathcal{D}_{S}}{\\mathbb{E}}\\bigg[\\displaystyle\\sum_{i\\in\\mathcal{W}_{k,n}^{\\ell,S_{n}}(0)}\\mathbf{r}_{i}\\left(\\alpha_{O_{(i,\\cdot)},k}^{(0)}+(2\\displaystyle\\sum_{l\\in\\mathcal{S}_{n,k}^{\\ell,S_{n}}}(\\sigma_{S}^{(0)})_{l}^{n}-1)y_{S_{n}}\\beta_{O_{(i,\\cdot)},k}^{(0)}\\right)\\bigg]\\geq-\\sqrt{2\\log(\\frac{5K m}{\\delta})}\\cdot}&{}&\\\\ &{}&{\\frac{5\\sigma_{1}(\\|c_{k}\\|+\\zeta_{k}^{e}\\|d_{k}\\|)}{16},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and our remaining job is to see when will $\\underset{S_{n}\\sim\\mathcal{D}_{S}}{\\mathbb{E}}[f(\\mathbf{E}(S_{n});\\mathbb{E}(\\Psi^{(t)}))]$ stay positive for some error tolerance. As such, we need to scrutinize the coefficients that would grow along the iterations. Therefore, we define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf A}_{t}^{k,y_{S n}}:=\\frac{1}{m}[\\Big(\\sum_{\\scriptstyle i\\in{\\mathcal{U}}_{k,n}^{y_{S n}}(\\tau)}-\\sum_{\\scriptstyle i\\in({\\mathcal{W}}_{k,n}^{y_{S n}}(\\tau)-{\\mathcal{U}}_{k,n}^{y_{S n}}(\\tau))\\cap{\\cal U}_{k,n}^{-y_{S n}}(\\tau)}\\Big){\\bf W}_{{\\cal O}_{(i,\\cdot)}}^{y}(\\tau)_{c_{k}}}\\\\ &{\\quad\\quad+\\Big(\\sum_{\\scriptstyle i\\in{\\mathcal{U}}_{k,n}^{y_{S n}}(\\tau)}-\\sum_{\\scriptstyle i\\in({\\mathcal{W}}_{k,n}^{y_{S n}}(\\tau)-{\\mathcal{U}}_{k,n}^{y_{S n}}(\\tau))\\cap{\\cal U}_{k,n}^{-y_{S n}}(\\tau)}\\Big)(2\\sum_{\\scriptstyle l\\in{\\mathcal{S}}_{n,k}^{y_{S n}}}(\\sigma_{S}^{(\\tau)})_{l}^{n}-1)y_{S n}{\\bf W}_{{\\cal O}_{(i,\\cdot)}}^{y}(\\tau)_{d_{k}}\\Big|_{\\tau=\\tau}^{\\tau=0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We will see that the conditional expectation of this sequence (conditioned on $\\mathbb{E}(\\Psi^{\\prime}^{(t)})$ , and the expectation is taken over $\\mathcal{D}_{S}$ ) would grow up to conquer the small initialization and make $\\underset{S_{n}\\sim\\mathcal{D}_{S}}{\\mathbb{E}}[f(\\mathbf{E}(S_{n});\\mathbb{E}({\\Psi^{\\prime}}^{(t)}))]$ stay positive. Consider the whole training duration $0\\leq t\\leq T^{*}$ , the evolving speed of $\\beta_{Q,k}^{(t+1)},\\beta_{K,k}^{(t+1)},\\alpha_{O_{(i,\\cdot)},k}^{(t+1)}$ \u03b1O(i,\u00b7),k and $\\beta_{O_{(i,\\,*)},k}^{(t+1)}$ depends on $\\mathbb{E}[\\ell_{n}^{\\prime~(t)}],\\mathbb{E}[\\mathbb{1}_{O_{(i)}}^{n}{}^{(t)}]$ and $\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}]$ . Denote ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{S}^{\\ast}:=\\frac{1}{1\\times[\\int_{0}^{1}(\\mathbf{k}\\Vert^{2})\\int_{0}^{1}(\\Vert\\mathbf{u}\\Vert^{4}\\left(1+e^{-\\sigma_{0}^{2}\\Vert\\mathbf{u}\\Vert^{2}}\\right)},2\\Vert\\mathbf{u}\\Vert^{4}\\Vert^{4}(1+e^{-\\sigma_{0}^{2}\\Vert\\mathbf{u}\\Vert^{2}})},}\\\\ &{\\qquad\\quad1+e^{-2^{-1}\\sigma_{0}^{2}(1-\\kappa_{\\mathbf{x}})^{2}\\Vert\\mathbf{u}\\Vert^{4}e^{-2\\log(5K m/\\delta)}}\\frac{{\\sigma_{1}}^{2}\\Vert\\mathbf{u}\\Vert^{4}\\left(1+e^{-\\sigma_{0}^{2}\\Vert\\mathbf{u}\\Vert^{2}}\\right)}{\\left(1-e^{-\\sigma_{0}^{2}\\Vert\\mathbf{u}\\Vert^{2}}\\right)}}\\\\ &{\\alpha:=4\\log(T^{*}),}\\\\ &{\\kappa:=8\\underset{i,k,w}{\\operatorname*{max}}\\{|\\alpha_{O_{(i,+)},k}^{(0)}|,|\\beta_{O_{(i,+)},k}^{(0)}|\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We will show that $\\sigma_{S}^{*}$ is the lower bound of $\\begin{array}{r}{\\operatorname*{min}_{t\\in[T^{*}],k\\in[K_{1}]}\\big\\{\\underset{n\\in\\mathcal{D}_{S}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}{(\\sigma_{S}^{(t)})}_{j}^{n}]\\big\\}}\\end{array}$ along the whole iteration. By Lemma 7, $\\kappa$ can be upper bounded by $8\\sqrt{2\\log(5K m/\\delta)}\\cdot\\sigma_{1}(\\sqrt{(1+\\kappa_{\\pmb y})/2}||\\mathbf{q}||)$ , and lower bounded by $2\\sqrt{2}\\sigma_{1}\\|\\mathbf{q}\\|$ , which is a negligible term due to the small initialization by Condition 1. ", "page_idx": 33}, {"type": "text", "text": "Lemma 30. Under Condition $^{\\,l}$ , for the whole iteration $0\\leq t\\leq T^{*}$ , for $\\cdot\\forall i\\in[m],e\\in[\\pm],k\\in[K_{1}],r\\in$ $[K_{2}],w\\in[d_{\\mathcal{X}}-K]$ , we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{:\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}\\{i\\in\\mathcal{U}_{k,n}^{e}(t)\\}]-e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(0)}\\le\\sigma_{S}^{*-1}\\alpha,}\\\\ &{:\\ge\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}^{1}(i\\in\\mathcal{W}_{k,n}^{e}(t)-\\mathcal{U}_{k,n}^{e}(t))]-e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(0)}\\ge-\\displaystyle\\frac{\\hat{C}\\|e_{k}\\|^{2}}{\\sigma_{S}^{*2}\\|d_{k}\\|^{2}}\\alpha}\\\\ &{\\phantom{:\\|}\\;\\;\\;\\;\\leq\\displaystyle-\\;\\frac{\\sigma_{1}(\\sigma_{S}^{*2}\\|d_{k}\\|^{2}+\\hat{C}\\|c_{k}\\|^{2})\\sqrt{2\\log(\\frac{5K m}{\\delta})}}{\\sigma_{S}^{*2}\\|d_{k}\\|},}\\\\ &{:\\leq\\mathbb{E}[|\\alpha_{O_{(i,\\cdot)},k}^{(t)}|]\\leq\\hat{C}\\displaystyle\\frac{\\|e_{k}\\|^{2}}{\\sigma_{S}^{*2}\\|d_{k}\\|^{2}}\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 31. Suppose Eq. (45) holds at iteration $t\\leq T_{2}$ , then we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underset{n\\in\\mathcal{V}_{k}}{\\mathbb{E}}[y_{S_{n}}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))]-\\mathbb{E}[\\mathbf{A}_{t+1}^{k,y_{S_{n}}}]\\right|\\leq\\kappa/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. By definition, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[y s_{n}\\,f(\\mathbf{E}(S);\\Psi^{(t)})]=\\mathbb{E}[y s_{n}\\cdot\\displaystyle\\sum_{e\\in[\\pm]}\\frac{e}{m}\\sum_{i\\in\\{\\mathbf{r}_{i}-\\frac{e}{m}\\}}\\sigma_{R}(\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(t)\\sum_{l\\in[L]}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\pmb{y}_{l}^{n})]}\\\\ &{\\ =\\mathbb{E}\\Big[\\frac{1}{m}(\\sum_{i\\in\\mathcal{U}_{k,n}^{y_{S_{n}}}(t)}-\\sum_{i\\in\\mathcal{W}_{k,n}^{y_{S_{n}}}(t)-u_{k,n}^{y_{S_{n}}}(t)})\\left(\\alpha_{O_{(i,\\cdot)},k}^{(t)}+\\big(2\\sum_{l\\in S_{n,k}^{y_{S_{n}}}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-1\\big)y s_{n}\\beta_{O_{(i,\\cdot)},k}^{(t)}\\right)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Observe that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\frac{1}{m}\\sum_{i\\in(\\mathcal{W}_{k,n}^{e}(t)-\\mathcal{U}_{k,n}^{e}(t))}\\left(\\alpha_{O_{(i,.)},k}^{(t)}+(2\\sum_{l\\in S_{n,k}^{y_{S n}}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-1\\big)y_{S_{n}}\\beta_{O_{(i,.)},k}^{(t)}\\right)\\Big]-\\frac{1}{m}\\mathbb{E}\\Big[\\qquad\\Big(\\frac{1}{m}\\Big)_{X_{o}}^{[2]}}\\\\ {\\qquad\\qquad\\sum_{i\\in(\\mathcal{W}_{k,n}^{e}(\\tau)-\\mathcal{U}_{k,n}^{e}(\\tau))\\cap M_{k,n}^{-e}(\\tau)}\\Big(\\alpha_{O_{(i,.)},k}^{(\\tau)}+(2\\sum_{l\\in S_{n,k}^{y_{S n}}}\\big(\\sigma_{S}^{(\\tau)}\\big)_{l}^{n}-1\\big)y_{S_{n}}\\beta_{O_{(i,.)},k}^{(\\tau)}\\Big)\\Big]\\Big|_{\\tau=t}^{\\tau=0}\\leq\\kappa/4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Here the inequality holds due to the fact that $\\mathbb{E}[\\alpha_{O_{(i,.)},k}^{(t)}1(i\\in(\\mathcal{W}_{k,n}^{e}(t)-\\mathcal{U}_{k,n}^{e}(t))-\\mathcal{U}_{k,n}^{-e}(t))]$ is decreasing the initial value $\\alpha_{O_{(i,.,.)},k}^{(0)}1(i\\in(\\mathcal{W}_{k,n}^{e}(0)-\\mathcal{U}_{k,n}^{e}(0))-\\mathcal{U}_{k,n}^{-e}(0))$ , and it\u2019s absolute value will not surpass that of $\\begin{array}{r}{\\mathbb{E}[e(2\\sum_{l\\in S_{n,k}^{y_{S_{n}}}}{(\\sigma_{S}^{(t)})}_{l}^{n}-1)\\beta_{O_{(i,\\cdot)},k}^{(t)}]\\leq\\kappa/8}\\end{array}$ , which is positive (by definition) and also decreasing by Lemma ", "page_idx": 33}, {"type": "text", "text": "29. On the other hand, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\mathbb{E}\\Big[\\frac{1}{m}\\sum_{i\\in\\mathcal{U}_{k,n}^{y_{S_{n}}}(t)}\\left(\\alpha_{O_{(i,\\cdot)},k}^{(t)}+(2\\sum_{l\\in S_{n,k}^{y_{S_{n}}}}(\\sigma_{S}^{(t)})_{l}^{n}-1)y_{S_{n}}\\beta_{O_{(i,\\cdot)},k}^{(t)}\\right)\\Big]-\\mathbb{E}[\\frac{1}{m}\\sum_{i\\in\\mathcal{U}_{k,n}^{y_{S_{n}}}(\\tau)}\\alpha_{O_{(i,\\cdot)},k}^{(\\tau)}}\\\\ &{\\displaystyle-\\,\\frac{1}{m}\\sum_{i\\in\\mathcal{U}_{k,n}^{y_{S_{n}}}(\\tau)}(2\\sum_{l\\in S_{n,k}^{y_{S_{n}}}}(\\sigma_{S}^{(\\tau)})_{l}^{n}-1)y_{S_{n}}\\beta_{O_{(i,\\cdot)},k}^{(\\tau)}\\Big]\\Big|_{\\tau=t}^{\\tau=0}\\Big|\\le\\kappa/4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining the two we can see the result is obtained. ", "page_idx": 34}, {"type": "text", "text": "We then denote the last time when there still exists $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\le\\kappa$ as $\\hat{T}$ , formally $\\hat{T}$ is the last time where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\bigcup_{k\\in[K_{1}],e\\in[\\pm]}\\{\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\leq\\kappa\\}\\neq\\varnothing.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Latter we will show in Lemma 33 that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{T}=\\frac{C_{1}\\sigma_{1}m\\lambda K_{1}\\gamma\\sqrt{(1+\\kappa_{y})\\log(5K m/\\delta)}}{(2\\sigma_{S}^{*}-1)^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We then denote the learning step at $\\hat{T}$ as $\\eta:=\\eta_{\\hat{T}}$ , and thus ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\eta=\\eta_{\\hat{T}}=\\frac{2}{\\lambda(\\hat{T}+\\gamma)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Lemma 31, actually it would hold that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\underset{S_{n}\\sim\\mathcal{D}_{S}}{\\mathbb{E}}[f(\\mathbf{E}(S_{n});\\mathbb{E}(\\Psi^{(\\hat{T})}))]\\ge\\kappa/2\\ge0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "And thus the 0-1 loss converges to zero with an error tolerance by definition. Our following job is to find $\\hat{T}$ . The following lemma provides the continuous ODEs as the upper and lower bound of the sequence ${\\bf A}_{t}^{k,e}$ . ", "page_idx": 34}, {"type": "text", "text": "Lemma 32. Under Condition $^{\\,l}$ , suppose Eq.(45) holds at any iteration $t\\leq T^{*}$ , then for $\\forall t\\leq T^{*},\\forall k\\in$ $[K_{1}],e\\in[\\pm],$ , it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\big|\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}(e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}(-e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)})))\\big|\\;i.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "2. The difference of the loss derivative is bounded by $O(\\kappa)$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\lvert\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}(\\boldsymbol{\\ell}_{n}^{\\prime}(\\boldsymbol{\\ t}))-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}(\\boldsymbol{\\ell}_{n}^{\\prime}(\\boldsymbol{\\ t}))]\\rvert\\le\\frac{\\kappa}{8}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "3. $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]$ is non-decreasing. The lower and upper bounds of the gradient update have continuous ODE counterpart. Specifically, there exist positive constant $c_{1},c_{2}$ , we can define $\\begin{array}{r}{\\overline{{c}}^{k,e}=\\frac{c_{1}\\eta_{0}\\left\\|\\mathbf{q}\\right\\|^{2}}{2m K_{1}}}\\end{array}$ ck,e = c2\u03b7T \u2217(2\u03c3\u2217S1\u221261m)2K(1\u2212\u03bay)\u2225q\u22252, bk,e = e\u2212\u03ba/2, bk,e = e\u03ba/2. Let xtk,e, xtk,e be the unique solutions of ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{x}}_{t}^{k,e}+\\overline{{b}}^{k,e}e^{\\overline{{x}}_{t}^{k,e}}=\\overline{{c}}^{k,e}t+\\overline{{b}}^{k,e},\\quad\\underline{{x}}_{t}^{k,e}+\\underline{{b}}^{k,e}e^{\\underline{{x}}_{t}^{k,e}}=\\underline{{c}}^{k,e}t+\\underline{{b}}^{k,e},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "then it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\underline{{x}}_{t}^{k,e}\\le\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\leq\\overline{{x}}_{t}^{k,e}+\\frac{\\overline{{c}}^{k,e}}{1+\\overline{{b}}^{k,e}},\\quad\\frac{1}{1+\\overline{{b}}^{k,e}\\overline{{x}}_{t}^{k,e}}\\leq-\\operatorname*{\\mathbb{E}}_{n\\in\\mathcal{V}_{k}^{e}}(\\ell_{n}^{\\prime}{}^{(t)})\\leq\\frac{1}{1+\\underline{{b}}^{k,e}\\underline{{x}}_{t}^{k,e}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Specifically, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\log(\\frac{2\\underline{{c}}^{k,e}}{3\\underline{{b}}^{k,e}}+\\frac{2}{3})\\leq\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\leq\\log(\\frac{\\overline{{c}}^{k,e}}{\\overline{{b}}^{k,e}}t+1)+\\frac{\\overline{{c}}^{k,e}}{1+\\overline{{b}}^{k,e}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Observe that $\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}({\\ell_{n}^{\\prime}}^{(t)})-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}({\\ell_{n}^{\\prime}}^{(t)})]$ equals to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\frac{1}{1+e^{[-\\frac{1}{m}(\\sum_{i\\in{\\mathcal{U}}_{k,n}^{e}(t)}-\\sum_{i\\in{\\mathcal{W}}_{k,n}^{e}(t)-u_{k,n}^{e}(t)})\\left(\\alpha_{O_{(i,.)}^{(i)},k}^{(i)}+(2\\sum_{l\\in{\\mathcal{S}}_{n,k}^{e}({\\sigma}_{S}^{(t)})_{l}^{n}-1)e^{\\beta_{O_{(i,.)}^{(t)}}},k}\\right)]}}}\\\\ &{-\\frac{1}{1+e^{[-\\frac{1}{m}(\\sum_{i\\in{\\mathcal{U}}_{k,n}^{-e}(t)}-\\sum_{i\\in{\\mathcal{W}}_{k,n}^{-e}(t)-u_{k,n}^{-e}(t)})\\left(\\alpha_{O_{(i,.)}^{(i)},k}^{(t)}+(2\\sum_{l\\in{\\mathcal{S}}_{n,k}^{e}({\\sigma}_{S}^{(t)})_{l}^{n}-1)e^{\\beta_{O_{(i,.)}^{(t)}}},k}\\right)]}}]}\\\\ &{=\\mathbb{E}[\\frac{e^{-[\\frac{1}{m}(\\sum_{i\\in{\\mathcal{U}}_{k,n}^{e}(t)}-\\sum_{i\\in{\\mathcal{W}}_{k,n}^{e}(t)-u_{k,n}^{y}(t)})\\left(\\alpha_{O_{(i,.)}^{(t)},k}^{(t)}+(2\\sum_{l\\in{\\mathcal{S}}_{n,k}^{e}({\\sigma}_{S}^{(t)})_{l}^{n}-1)y\\beta_{O_{(i,.)}^{(t)}},k}^{(t)}\\right)]}|_{y=e}}{\\prod_{y\\in\\{e,-e\\}}1+e^{[-\\frac{1}{m}(\\sum_{i\\in{\\mathcal{U}}_{k,n}^{y}(t)}-\\sum_{i\\in{\\mathcal{W}}_{k,n}^{y}(t)}-u_{k,n}^{y}(t)-u_{k,n}^{y}(t))\\left(\\alpha_{O_{(i,.)}^{(t)},k}^{(t)}+(2\\sum_{l\\in{\\mathcal{S}}_{n,k}^{y}({\\sigma}_{S}^{(t)})_{l}^{n}-1)y\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As cross-entropy loss is $L$ -smooth with $L=1$ , one can bound the difference by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big|\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{\\epsilon}}{\\mathbb{E}}(\\ell_{n}^{\\prime}(\\ell)^{(t)})-\\underset{n\\in\\mathcal{V}_{k}^{-}}{\\mathbb{E}}(\\ell_{n}^{\\prime}(\\ell))]|\\le\\big|\\underset{n\\in\\mathcal{V}_{k}^{\\epsilon}}{\\mathbb{E}}(e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))-\\underset{n\\in\\mathcal{V}_{k}^{-}}{\\mathbb{E}}(-e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)})))\\big|}\\\\ &{=|\\mathbb{E}[\\big[\\frac{1}{m}(\\underset{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(t)}{\\sum}-\\underset{i\\in\\mathcal{W}_{k,n}^{y}(t)-u_{k,n}^{y}(t)}{\\sum})\\left(\\alpha_{O_{(i,+)},k}^{(t)}+(2\\sum_{l\\in\\mathcal{S}_{n,k}^{y}}(\\sigma_{S}^{(t)})_{l}^{n}-1)y\\beta_{O_{(i,+)},k}^{(t)}\\right)\\big]\\big|_{y=e}^{y=-e}\\big]|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Lemma 23, we see that for initialization, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\underset{n\\in\\mathcal{V}_{k}^{\\mathbb{C}}}{\\mathbb{E}}(e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(0)}))-\\underset{n\\in\\mathcal{V}_{k}^{-\\epsilon}}{\\mathbb{E}}(-e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(0)})))|\\leq2\\sqrt{2\\log(\\frac{5K m}{\\delta})}\\cdot\\frac{3\\sigma_{1}(\\|c_{k}\\|+\\zeta_{k}^{\\epsilon}\\|d_{k}\\|)}{8}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\kappa/8.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now we serve to show that the following expected difference ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\big|\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}(e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}(-e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)})))\\big|\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "is non-increasing. Intuitively, this observation is due to the inherent nature of cross-entropy loss, which always pays more emphasis (has larger derivative) on those low value. Also, another important factor is the update of those ambiguous neurons\u2019 coefficient summation would also prefer the low-value one among $\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}(e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)})),\\forall e\\in[m]$ . To better present this observation, we define ", "page_idx": 35}, {"type": "equation", "text": "$$\ne_{t}^{*}=\\arg\\operatorname*{min}\\{{\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}}(e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)})),{\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}}(-e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))\\},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which further means that $e_{t}^{\\ast}$ satisfies $\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{e_{t}^{*}}}{\\mathbb{E}}(\\ell_{n}^{\\prime}({^{(t)}})-\\underset{n\\in\\mathcal{V}_{k}^{-e_{t}^{*}}}{\\mathbb{E}}(\\ell_{n}^{\\prime}({^{(t)}})]\\ <\\ 0$ due to the non-positive and non-increasing property of cross-entropy loss. ", "page_idx": 35}, {"type": "text", "text": "Recall the update rule, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{O_{(i,.,)},k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\alpha_{O_{(i,.,)},k}^{(t)}-\\eta_{t}\\frac{\\|c_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\big[\\mathbf{er}_{i}\\cdot\\underbrace{\\mathbb{R}_{e|\\mathbf{r}_{k}^{e}}}_{n\\in V_{k}^{e}}\\big(\\ell_{n}^{\\prime}\\mathbf{\\rho}_{1}^{(t)}\\mathbf{\\mathbb{I}}_{O_{(i)}}^{n}(t)\\big)\\big]}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad-\\,\\eta_{t}\\frac{\\big(K_{1}-1\\big)\\|c_{k}\\|^{2}}{2K_{1}K}\\sum_{e\\in[\\pm]}\\big[\\mathbf{er}_{i}\\cdot\\underbrace{\\mathbb{R}_{e|\\mathbf{r}_{-k}^{e}}}_{n\\in V_{-k}^{e}}\\big(\\ell_{n}^{\\prime}\\mathbf{\\rho}_{1}^{(t)}\\mathbf{\\mathbb{I}}_{O_{(i)}}^{n}(t)\\big)\\big],}\\\\ {\\mathbb{E}[e\\beta_{O_{(i,.,)},k}^{(t+1)}\\mid\\Psi^{(t)}]=(1-\\eta_{t}\\lambda)e\\beta_{O_{(i,.)},k}^{(t)},}\\\\ {\\displaystyle\\qquad\\qquad\\qquad-\\,\\eta_{t}\\frac{\\|d_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}e\\underbrace{\\mathbb{R}_{e|V_{k}^{e}}}_{n\\in V_{k}^{e}}[\\ell_{n}^{\\prime}(\\mathbf{\\rho}_{1}^{(t)}\\mathbf{\\mathbb{I}}_{O_{(i)}}^{n}(t)\\big(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-\\sum_{l\\in S_{n,k}^{-e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\alpha_{O_{(i,+)},k}^{(t+1)}+e(2\\displaystyle\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n}-1)\\beta_{O_{(i,:)},k}^{(t+1)}\\mid\\Psi^{(t)},\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n}]]=(1-\\eta_{t}\\lambda)(\\alpha_{O_{(i,:)},k}^{(t)}+e(2\\log(\\frac{1}{\\lambda}))_{l}^{n}-1)}\\\\ &{\\displaystyle\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)\\beta_{O_{(i,:)},k}^{(t)})-\\frac{\\eta_{t}}{2K_{1}}\\displaystyle\\sum_{e\\in[\\pm]}\\mathbf{r}_{i e}\\underbrace{\\mathbb{E}_{\\nu_{k}^{e}}[\\ell_{n}^{\\prime}\\,\\mathbb{I}_{O_{(i)}}^{n}(t)}_{n\\in\\mathbb{V}_{k}^{e}}\\Big(\\|c_{k}\\|^{2}+\\|d_{k}\\|^{2}(\\displaystyle\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n}-1)}\\\\ &{\\displaystyle\\langle2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)\\Big)-\\eta_{t}\\frac{(K_{1}-1)}{2K_{1}K}\\displaystyle\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}e_{n\\in\\mathbb{V}_{-k}^{e}}[\\ell_{n}^{\\prime}\\,\\mathbb{I}_{O_{(i)}}^{n}(t)\\,]\\|c_{k}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Lemma 28 and Lemma 29, we see that the E[ i\u2208Uke,n(\u03c4)\u2212(Wk\u2212,en(\u03c4)\u2212Uk\u2212,en(\u03c4)) \u03b1(O\u03c4()i,\u00b7),k] is increasing such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{5[\\sum_{i\\in\\mathcal{U}_{k,n}^{\\mathrm{e}}(\\tau)-(\\mathcal{V}_{k,n}^{\\mathrm{-}}(\\tau)-\\mathcal{U}_{k,n}^{\\mathrm{-}}(\\tau))}\\alpha_{\\mathcal{O}_{(i,\\cdot)},k}^{(\\tau)}\\mid\\Psi^{(t)}]\\Big|_{\\tau=t+1}^{\\tau=0}=\\Theta\\big(\\sum_{\\scriptstyle i\\in\\mathcal{U}_{k,n}^{\\mathrm{e}}(\\tau)-(\\mathcal{W}_{k,n}^{-\\mathrm{e}}(\\tau)-\\mathcal{U}_{k,n}^{\\mathrm{-}}(\\tau))}\\alpha_{\\mathcal{O}_{(i,\\cdot)},k}^{(\\tau)}\\big|_{\\tau=t}^{\\tau=0}}}}\\\\ &{}&{-\\sum_{\\scriptstyle\\frac{\\eta_{t}\\|c_{k}\\|^{2}}{i\\in\\mathcal{U}_{k,n}^{\\mathrm{e}}(t)-(\\mathcal{W}_{k,n}^{-\\mathrm{e}}(t)-\\mathcal{U}_{k,n}^{\\mathrm{-}}(t))}}\\frac{\\eta_{t}\\|c_{k}\\|^{2}}{2m K_{1}}\\frac{\\mathbb{E}_{\\mathbb{C}}\\big(\\ell_{n}^{\\prime}(\\ t)\\big)),}{n\\in\\mathcal{V}_{k}^{\\mathrm{e}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we ignore the impact of cross-concept safely due to the large $K=\\Omega(\\eta_{0}C(K_{1}-1)\\|\\mathbf{q}\\|^{2}/(m K_{1}))$ , as well as the impact of regularization term since $\\lambda=\\operatorname{\\dot{O}}((C\\log(K m/\\delta)\\|\\mathbf{q}\\|)^{-1})$ by Condition 1 in the first stage. Similarly, suggest E[ l\u2208Sen,k $\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t+1)}\\big)_{l}^{n}]$ is also given when considering the update for $t+1$ , we see that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{I}\\big[\\displaystyle\\frac{1}{m}\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)}e(2\\sum_{l\\in S_{n,k}^{\\epsilon}}(\\sigma_{S}^{(t+1)})_{l}^{n}-1)\\beta_{\\sigma_{(i,\\cdot),k}}^{(\\tau)}\\mid\\Psi^{(t)},\\mathbb{E}[\\sum_{l\\in S_{n,k}^{\\epsilon}}(\\sigma_{S}^{(t+1)})_{l}^{n}]\\big]\\Big|_{\\tau=t+1}^{\\tau=0}=\\big(2\\sum_{l\\in S_{n,k}^{\\epsilon}}(\\sigma_{S}^{(t+1)})_{l}^{n}-1\\big)}\\\\ &{\\big)\\big(\\displaystyle\\frac{1}{m}\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)}e\\beta_{\\sigma_{(i,\\cdot),k}}^{(\\tau)}\\big|_{\\tau=t}^{\\tau=0}-\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)}\\frac{\\eta_{t}\\|d_{k}\\|^{2}}{2m K_{1}}{n}\\mathbb{E}_{\\epsilon\\tilde{\\nu}_{k}^{\\epsilon}}^{\\mathbb{E}}\\big(2\\sum_{l\\in S_{n,k}^{\\epsilon}}(\\sigma_{S}^{(t)})_{l}^{n}-1\\big)\\ell_{n}^{\\prime}(\\tau)\\big)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Interestingly, by Eq.(39) we see that $\\begin{array}{r}{(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)\\,=\\,(2\\sum_{l\\in S_{n,k}^{-e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)}\\end{array}$ . Thus we can characterize that the magnitude of gradient update of the term in Eq.(48) and (49) of the $e_{t}^{\\ast}$ would be larger than those of $-e_{t}^{\\ast}$ due to the non-increasing nature of cross-entropy loss. ", "page_idx": 36}, {"type": "text", "text": "On the other hand, by Lemma 29 the monotonicity of ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{\\substack{i\\in\\mathcal{U}_{k,n}^{\\mathrm{e}}(\\tau)\\cap(\\mathcal{W}_{k,n}^{-\\mathrm{e}}(\\tau)-\\mathcal{U}_{k,n}^{-\\mathrm{e}}(\\tau))}}\\alpha_{O_{(i,\\cdot)},k}^{(\\tau)}]\\Big|_{\\tau=t}^{\\tau=0},\\quad\\mathbb{E}[\\sum_{\\substack{i\\in(\\mathcal{W}_{k,n}^{\\mathrm{e}}(\\tau)-\\mathcal{U}_{k,n}^{\\mathrm{e}}(\\tau))\\cap\\mathcal{U}_{k,n}^{-\\mathrm{e}}(\\tau)}}\\alpha_{O_{(i,\\cdot)},k}^{(\\tau)}]\\Big|_{\\tau=t}^{\\tau=0}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "depend on the signal of $\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}({\\ell_{n}^{\\prime}}^{(t)})-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}({\\ell_{n}^{\\prime}}^{(t)})]$ . Specifically, we see that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\xi[\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)\\cap(W_{k,n}^{-\\epsilon}(\\tau)-U_{k,n}^{-\\epsilon}(\\tau))}\\alpha_{\\mathcal{O}_{(i,+,)},k}^{(\\tau)}\\ |\\ \\Psi^{(t)}]_{\\tau=t+1}^{\\infty0}=\\Theta(\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)\\cap(W_{k,n}^{-\\epsilon}(\\tau)-U_{k,n}^{-\\epsilon}(\\tau))}\\alpha_{\\mathcal{O}_{(i,+)},k}^{(\\tau)}|_{\\tau=t}^{\\tau=0}}}\\\\ &{}&{-\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)\\cap(W_{k,n}^{-\\epsilon}(\\tau)-U_{k,n}^{-\\epsilon}(\\tau))}\\frac{\\eta_{t}\\|{\\bf{c}}_{k,n}^{\\epsilon}\\|^{2}}{2m R_{1}^{1}}[\\frac{\\mathbb{R}}{n\\xi\\nu_{k}^{\\epsilon}}(\\ell_{n}^{\\prime})-\\underbrace{\\mathbb{R}}_{n\\in\\mathcal{V}_{k}^{-\\epsilon}}(\\ell_{n}^{\\prime}(\\ t))];}\\\\ &{}&{\\xi[\\sum_{i\\in(W_{k,n}^{\\epsilon}(\\tau)-U_{k,n}^{\\epsilon}(\\tau))\\cap\\mathcal{U}_{k,n}^{-\\epsilon}(\\tau)}\\alpha_{\\mathcal{O}_{(i,+)},k}^{(\\tau)}\\ |\\ \\Psi^{(t)}]_{\\tau=t+1}^{\\infty0}=\\Theta(\\qquad\\qquad\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)\\cap\\mathcal{U}_{k,n}^{-\\epsilon}(\\tau)}\\alpha_{\\mathcal{O}_{(i,+)},k}^{(t)}|_{\\tau=t}^{\\tau=0}}\\\\ &{}&{+\\sum_{i\\in\\mathcal{U}_{k,n}^{\\epsilon}(\\tau)\\cap\\mathcal{O}_{k}^{-\\epsilon}(\\tau)}\\frac{\\eta_{t}\\|{\\bf{c}}_{k,n}^{\\epsilon}\\|^{2}}{2m R_{1}}[\\frac{\\mathbb{R}}{n\\xi\\nu_{k}^{\\epsilon}}(\\ell_{n}^{\\prime})-\\underbrace{\\mathbb{R}}_{n\\in\\mathcal{V}_{k}^{-\\epsilon}}(\\ell_{n}^{\\prime}(t)) \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the contribution term is shared by the two sequences. Therefore, by Eq.(50) and (46), the evolution of ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{\\substack{i\\in\\mathcal{U}_{k,n}^{e}(\\tau)\\cap(\\mathcal{W}_{k,n}^{-e}(\\tau)-\\mathcal{U}_{k,n}^{-e}(\\tau))}}\\alpha_{O_{(i,\\cdot)},k}^{(\\tau)}-\\sum_{\\substack{i\\in(\\mathcal{W}_{k,n}^{e}(\\tau)-\\mathcal{U}_{k,n}^{e}(\\tau))\\cap\\mathcal{U}_{k,n}^{-e}(\\tau)}}\\alpha_{O_{(i,\\cdot)},k}^{(\\tau)}]\\Big|_{\\tau=t}^{\\tau=0}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "will prefer to grow in the direction of $e_{t}^{\\ast}$ . ", "page_idx": 36}, {"type": "text", "text": "We then take a look on the decreasing coefficients based on Lemma 29. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[\\underset{i\\in(W_{k,n}^{\\mathbb{I}}(\\gamma)\\sim\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma)-\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma)}{\\sum}\\alpha_{(k_{n}^{*},*),k}^{(\\mathbb{I})}\\left|\\Psi^{(t)}\\right|_{\\tau=t+1}^{\\eta=0}=\\Theta(\\underset{i\\in(W_{k,n}^{\\mathbb{I}}(\\gamma)-\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma)-\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma))}{\\sum}\\alpha_{(i,*),k}^{(\\mathbb{I})}\\Big|_{\\tau=t}^{\\infty}}&{}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\underset{i\\in(W_{k,n}^{\\mathbb{I}}(\\gamma)-\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma))-\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma)}{\\sum}\\frac{\\eta_{\\mathbb{I}}\\left|\\mathbb{I}\\alpha_{k}\\right|_{\\tau}^{2}}{n\\xi\\gamma}\\frac{\\mathbb{I}}{t}{\\left|\\mathcal{S}_{k}^{\\mathbb{I}}(\\gamma)\\right|_{\\tau}^{\\mathbb{I}}(\\gamma)}),}\\\\ {\\mathbb{E}[\\frac{1}{m}\\underset{i\\in(W_{k,n}^{\\mathbb{I}}(\\gamma)-\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma))}{\\sum}\\underset{t\\in\\mathcal{U}_{k,n}^{\\mathbb{I}}(\\gamma)}{e}({\\mathbb{I}2\\sum}\\underset{t\\in\\mathcal{U}_{k,n}^{\\mathbb{I}}}{\\sum}(\\sigma_{s_{n}^{*},k}^{(t+1)})_{t}^{n}-1)\\beta_{O(\\tau_{i}^{*},k)}^{(\\mathbb{I})}\\left|\\Psi^{(t)}\\right|_{L^{\\mathbb{I}}(\\mathcal{S}_{k,n}^{\\mathbb{I}}(\\gamma)}\\left({\\mathcal{O}_{k,n}^{(t+1)}}^{(\\mathbb{I}+1)}\\right))]\\Bigg|_{\\tau=t+1}^{\\infty}=}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad(2\\mathbb{E}[\\underset{t\\in\\mathcal{S}_{k,n}^{\\mathbb{I}}(\\gamma)}{\\sum}(\\sigma_{s}^{(t+1)})_{t}^{n}]-1)\\Theta(\\underset \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "As such, we have all preliminaries to characterize the first result of the lemma. We first utilize the induction to prove the following: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\big|\\underset{n\\in\\mathcal{V}_{k}^{\\epsilon}}{\\mathbb{E}}(e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))-\\underset{n\\in\\mathcal{V}_{k}^{-\\epsilon}}{\\mathbb{E}}(-e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)})))|\\leq\\kappa/8.\\quad\\forall e\\in[\\pm].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This apparently hold at initialization. Suggest for any $t\\leq\\widetilde t-1$ the result holds, then we only need to prove ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\in\\mathcal{V}_{k}^{\\ast}}{\\mathbb{E}}(e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\tilde{t}-1)}))-\\underset{n\\in\\mathcal{V}_{k}^{-e_{\\tilde{t}-1}^{\\ast}}}{\\mathbb{E}}(-e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\tilde{t}-1)})))\\geq}\\\\ &{\\underset{n\\in\\mathcal{V}_{k}^{\\ast}}{\\mathbb{E}}(e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t_{1})}))-\\underset{n\\in\\mathcal{V}_{k}^{-e_{\\tilde{t}-1}^{\\ast}}}{\\mathbb{E}}(-e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t_{1})}))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By the condition of small $\\eta_{t}$ in Condition 1, Lemma 31, Eq.(48) (49), (50) and (51), we see that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\in\\mathbb{V}_{k}^{+-1}}{\\mathbb{E}}(-e_{t-1}^{*}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\widetilde{t}-1)})))-\\underset{n\\in\\mathbb{V}_{k}^{+-1}}{\\mathbb{E}}(e_{t-1}^{*}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\widetilde{t}-1)}))}\\\\ &{-\\big(\\underset{n\\in\\mathbb{V}_{k}^{+-1}}{\\mathbb{E}}(-e_{t-1}^{*}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\widetilde{t})})))-\\underset{n\\in\\mathbb{V}_{k}^{+-1}}{\\mathbb{E}}(e_{t-1}^{*}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\widetilde{t})})))}\\\\ &{\\quad\\times\\Theta(\\mathbb{E}|\\underset{n\\in\\mathbb{V}_{k}^{+-1}}{\\mathbb{E}}((\\ell_{n}^{'}(\\widetilde{\\hat{t}}^{-1}))-\\underset{n\\in\\mathbb{V}_{k}^{+-1}}{\\mathbb{E}}(\\ell_{n}^{'}(\\widetilde{\\hat{t}}^{-1})))\\Big(\\underset{i\\in\\mathbb{V}_{k,n}^{+-1}}{\\sum}\\frac{\\eta_{T^{*}}\\|c_{k}\\|^{2}}{2m K_{1}}}\\\\ &{+\\big(2\\underset{l\\in\\mathbb{V}_{k}^{+-1}}{\\sum}(\\sigma_{S}^{(\\widetilde{t})})_{l}^{n}-1\\big)\\underset{i\\in\\mathbb{V}_{k}^{+-1}(\\widetilde{t}^{-1})}{\\sum}\\frac{\\eta_{T^{*}}\\|d_{k}\\|^{2}}{2m K_{1}}\\underset{n\\in\\mathbb{V}_{k}^{+-1}}{\\mathbb{E}}((2\\sum_{1\\in\\mathcal{S}_{n}^{+-1}}(\\sigma_{S}^{(\\widetilde{t}-1)})_{l}^{n}-1))\\Big)])\\le0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and thus we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\underset{n\\in\\mathcal{V}_{k}^{\\ast}}{\\mathbb{E}}(e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\tilde{t})}))-\\underset{n\\in\\mathcal{V}_{k}^{-e_{\\tilde{t}-1}^{\\ast}}}{\\mathbb{E}}(-e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\tilde{t})})))|}\\\\ &{\\leq|\\underset{n\\in\\mathcal{V}_{k}^{\\ast}}{\\mathbb{E}}(e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\tilde{t}-1)}))-\\underset{n\\in\\mathcal{V}_{k}^{-e_{\\tilde{t}-1}^{\\ast}}}{\\mathbb{E}}(-e_{\\tilde{t}-1}^{\\ast}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\tilde{t}-1)})))|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, we complete the induction. Then we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{\\frac{\\epsilon}{\\epsilon}}-1}{\\mathbb{E}}(\\ell_{n}^{'(\\frac{\\epsilon}{\\epsilon})})-\\underset{n\\in\\mathcal{V}_{k}^{-\\frac{\\epsilon}{\\epsilon}}-1}{\\mathbb{E}}(\\ell_{n}^{'(\\frac{\\epsilon}{\\epsilon})}(\\ell)^{'})]\\|}\\\\ &{\\leq|\\frac{\\mathbb{E}}{n\\in\\mathcal{V}_{k}^{\\frac{\\epsilon}{\\epsilon}}-1}(e_{t-1}^{\\epsilon}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\hat{\\pi})}))-\\underset{n\\in\\mathcal{V}_{k}^{-\\frac{\\epsilon}{\\epsilon}}-1}{\\mathbb{E}}(-e_{t-1}^{*}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\hat{\\pi})})))|}\\\\ &{\\underset{n\\in\\mathcal{V}_{k}^{\\frac{\\epsilon}{\\epsilon}}-1}{\\mathbb{E}}(e_{t-1}^{*}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\hat{\\pi}-1)}))-\\underset{n\\in\\mathcal{V}_{k}^{\\frac{\\epsilon}{\\epsilon}}-1}{\\mathbb{E}}(-e_{t-1}^{*}f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(\\hat{\\pi}-1)}))))}\\\\ &{\\leq\\cdots\\leq|\\frac{\\mathbb{E}}{n\\in\\mathcal{V}_{k}^{\\frac{\\epsilon}{\\epsilon}}}(e^{f}(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(0)}))-\\underset{n\\in\\mathcal{V}_{k}^{-\\frac{\\epsilon}{\\epsilon}}}{\\mathbb{E}}(-e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(0)})))|}\\\\ &{\\leq\\sqrt{2\\log(\\frac{5K m}{\\delta})}\\cdot\\frac{3\\sigma_{1}(\\|c\\|_{k}\\|+\\zeta_{k}^{\\epsilon}\\|d_{k}\\|)}{4}\\leq\\kappa/8.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This completes the proof of the first result. ", "page_idx": 38}, {"type": "text", "text": "To obtain the continuous ODE upper bound of $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]$ , we first recall the update ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t+1)}+e(2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n}-1)\\beta_{O_{(i,\\cdot)},k}^{(t+1)}\\mid\\Psi^{(t)},\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n}]]=(1-\\eta_{t}\\lambda)(\\alpha_{O_{(i,\\cdot)},k}^{(t)}+}\\\\ &{\\mathfrak{s}(2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)\\beta_{O_{(i,\\cdot)},k}^{(t)})-\\eta_{t}\\frac{1}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}e\\frac{\\mathbb{E}}{n\\epsilon\\nu_{k}^{e}}[\\ell_{n}^{\\prime}\\,^{(t)}\\mathbb{I}_{O_{(i)}}^{n}{}^{(t)}\\Big(\\|c_{k}\\|^{2}+\\|d_{k}\\|^{2}(\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n}}\\\\ &{-\\,1)(2\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-1)\\Big)-\\eta_{t}\\frac{(K_{1}-1)}{2K_{1}K}\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}e\\frac{\\mathbb{E}}{n\\epsilon\\nu_{-k}^{e}}[\\ell_{n}^{\\prime}\\,^{(t)}\\mathbb{I}_{O_{(i)}}^{n}{}^{(t)}\\|c_{k}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, utilizing Lemma 31 and the fact $|\\mathcal{W}_{k,n}^{e}(t)|\\leq m$ , we have constant $c_{1}>0$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{A}_{t+1}^{k,e}\\mid\\Psi^{(t)},\\mathbb{E}(\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n})]\\leq\\mathbf{A}_{t}^{k,e}-c_{1}(\\frac{\\eta_{t}\\|\\mathbf{q}\\|^{2}}{2m K_{1}}\\cdot\\frac{\\mathbb{E}_{\\mathbf{r}}[\\boldsymbol{\\ell}_{n}^{\\prime}(t)]),}{n\\in\\mathcal{V}_{k}^{e}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathbf{A}_{t}^{k,e}+c_{1}(\\frac{\\eta_{0}\\|\\mathbf{q}\\|^{2}}{2m K_{1}}\\cdot\\frac{1}{1+e^{-\\kappa/2}e^{\\mathbf{A}_{t}^{k,e}}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbf{A}_{t}^{k,e}+\\frac{\\overline{{c}}^{k,e}}{1+\\overline{{b}}^{k,e}e^{\\mathbf{A}_{t}^{k,e}}}\\cdot}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we also neglect the impact of cross-concept due to the large $K=\\Omega(\\eta_{0}C(K_{1}-1)\\|\\mathbf{q}\\|^{2}/(m K_{1}))$ in Condition 1 and appropriately chosen $c_{1}$ . ", "page_idx": 38}, {"type": "text", "text": "To obtain the lower bound ODE couterpart, we examine the update of the correct contributor neurons, as shown in Eq.(48), (50) and (49). In terms of the update of $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ where $i\\in\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ we see that its update is controlled by $\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}({\\ell_{n}^{\\prime}}^{(t)})-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}({\\ell_{n}^{\\prime}}^{(t)})]$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\alpha_{O_{(i,.,)},k}^{(t+1)}=\\Theta(\\alpha_{O_{(i,.)},k}^{(t)}-\\frac{\\eta_{t}\\|\\boldsymbol{c}_{k}\\|^{2}}{2m R_{1}}[\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}(\\ell_{n}^{\\prime}{}^{(t)})-\\underset{n\\in\\mathcal{V}_{k}^{-e}}{\\mathbb{E}}(\\ell_{n}^{\\prime}{}^{(t)})]).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then by the first result in this lemma we know |E[n \u2208EVe(\u2113\u2032n(t)) \u2212 E\u2212e(\u2113\u2032n(t))]| \u2264\u03b24 \u22643\u03ba2, and thus ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\alpha_{O_{(i,\\cdot)},k}^{(t+1)}=\\Theta(\\alpha_{O_{(i,\\cdot)},k}^{(t)}\\pm\\frac{\\eta_{t}\\kappa\\|c_{k}\\|^{2}}{64m K_{1}}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "$\\begin{array}{r}{\\sigma_{1}=O(\\frac{(2\\sigma_{S}^{*}-1)^{2}}{C m^{3/2}\\|\\mathbf{q}\\|})}\\end{array}$ , due to the large $C$ , $\\mathbb{E}[\\alpha_{O_{(i,\\cdot)},k}^{(t)}]$ $\\kappa=O((2\\sigma_{S}^{*}-1)^{2}/m)$ $i\\in\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ cTahnu sb et hnee gglreacdtieedn tc (coto)mnptnrairbeudt itoon (tsth )em iandcer ebays innegu ruopnd asteet of $\\mathbb{E}[\\alpha_{O_{(i,.)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t)))]$ and . Besides, we see that $\\mathbb{E}[\\mathcal{W}_{k,n}^{e}(t)]$ will at least preserve the neurons of $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(0))]$ ], which will not be deactivated by Lemma 29. ", "page_idx": 38}, {"type": "text", "text": "Then there exists $\\begin{array}{r l r l}{c_{2}}&{{}>}&{0}\\end{array}$ , recall $\\sigma_{S}^{*}$ is defined in Lemma 34 as the lower bound of $\\begin{array}{r l}{\\operatorname*{min}_{t,k}\\bigr\\{\\underset{n\\in\\mathcal{D}_{S}}{\\mathbb{E}}\\bigl[\\underset{j\\in S_{n,k}^{y_{S_{n}}}}{\\sum}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\bigr]\\bigr\\}}\\end{array}$ and $\\mathbb{E}[|\\mathcal{U}_{k,n}^{e}(0))|]\\geq m/8$ , it holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{A}_{t+1}^{k,\\epsilon}\\mid\\Psi^{(t)},\\mathbb{E}(\\sum_{l\\in\\mathcal{S}_{n,k}^{\\epsilon}}(\\sigma_{S}^{(t+1)})_{l}^{n})]\\geq\\mathbf{A}_{t}^{k,\\epsilon}-c_{2}(\\frac{\\eta_{t}(2\\sigma_{S}^{\\ast}-1)^{2}\\|d_{k}\\|^{2}}{8m K_{1}}\\cdot\\operatorname*{m}_{n\\in\\mathcal{V}_{k}^{\\epsilon}}[\\ell_{n}^{'}(t)])}\\\\ &{\\phantom{m m m m m m m m m m m}\\geq\\mathbf{A}_{t}^{k,\\epsilon}+(\\frac{c_{2}\\eta_{t}(2\\sigma_{S}^{\\ast}-1)^{2}\\|d_{k}\\|^{2}}{8m K_{1}}\\cdot\\frac{1}{1+e^{\\kappa/2}e^{\\mathbf{A}_{t}^{k,\\epsilon}}})}\\\\ &{\\phantom{m m m m m m m m m m}\\geq\\mathbf{A}_{t}^{k,\\epsilon}+(\\frac{c_{2}\\eta_{T}\\cdot(2\\sigma_{S}^{\\ast}-1)^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|^{2}}{16m K_{1}}\\cdot\\frac{1}{1+e^{\\kappa/2}e^{\\mathbf{A}_{t}^{k,\\epsilon}}})}\\\\ &{\\phantom{m m m m m m m m m}=\\mathbf{A}_{t}^{k,\\epsilon}+\\frac{\\underline{{c}}^{k,\\epsilon}}{1+\\sum_{k}^{k,\\epsilon}e^{\\mathbf{A}_{t}^{k,\\epsilon}}}\\cdot}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we ignore the impact of regularization term at this stage since $\\lambda\\,=\\,O((C\\log(K m/\\delta)||\\mathbf{q}||)^{-1})$ and appropriately chosen $c_{2}$ . The third inequality is due to the definition of $\\pmb{d}_{k}$ . ", "page_idx": 39}, {"type": "text", "text": "Collaborating with Lemma 10, the proofs are completed. ", "page_idx": 39}, {"type": "text", "text": "For the last results, following the techniques in [43], first it\u2019s easy to check that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\bar{b}^{k,e}e^{\\bar{x}_{t}^{k,e}}\\leq\\bar{x}_{t}^{k,e}+\\bar{b}^{k,e}e^{\\bar{x}_{t}^{k,e}}\\leq1.5\\bar{b}^{k,e}e^{\\bar{x}_{t}^{k,e}},\\quad\\underline{{b}}^{k,e}e^{\\underline{{x}}_{t}^{k,e}}\\leq\\underline{{x}}_{t}^{k,e}+\\underline{{b}}^{k,e}e^{\\underline{{x}}_{t}^{k,e}}\\leq1.5\\underline{{b}}^{k,e}e^{\\underline{{x}}_{t}^{k,e}},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "thus ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(\\frac{2\\overline{{c}}^{k,e}}{3\\overline{{b}}^{k,e}}+\\frac{2}{3})\\leq\\overline{{x}}_{t}^{k,e}\\leq\\log(\\frac{\\overline{{c}}^{k,e}}{\\overline{{b}}^{k,e}}t+1),\\quad\\log(\\frac{2\\underline{{c}}^{k,e}}{3\\underline{{b}}^{k,e}}+\\frac{2}{3})\\leq\\underline{{x}}_{t}^{k,e}\\leq\\log(\\frac{\\underline{{c}}^{k,e}}{\\underline{{b}}^{k,e}}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\log(\\frac{2\\underline{{c}}^{k,e}}{3\\underline{{b}}^{k,e}}+\\frac{2}{3})\\leq\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\leq\\log(\\frac{\\overline{{c}}^{k,e}}{\\overline{{b}}^{k,e}}t+1)+\\frac{\\overline{{c}}^{k,e}}{1+\\overline{{b}}^{k,e}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma 30. We use induction to prove this lemma. All conclusion holds naturally at $t=0$ . Suppose there exists $\\tilde{T}\\leq T^{*}$ such that the six conditions hold for any $0\\leq t\\leq\\widetilde{T}-1$ , we prove that these conclusions also hold for $t=\\widetilde T$ . ", "page_idx": 39}, {"type": "text", "text": "We now prove ", "page_idx": 39}, {"type": "equation", "text": "$$\n0\\leq\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{U}_{k,n}^{e}(t))]-e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(0)}\\leq(\\sigma_{S}^{*})^{-1}\\alpha.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Recall the update rule ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\beta_{O_{(i,.,)},k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{O_{(i,.,)},k}^{(t)}-\\eta_{t}\\frac{\\|d_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}\\frac{\\mathbb{E}_{\\upsilon}}{n\\in\\mathcal{V}_{k}^{\\leftarrow}}[\\ell_{n}^{\\prime}{^(t)}\\mathbb{1}_{O_{(i)}}^{n}(t)(\\sum_{l\\in\\mathcal{S}_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-\\sum_{l\\in\\mathcal{S}_{n,k}^{-e}}(\\sigma_{S}^{(t)})_{l}^{n})].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "As we ignore the regularization term at the first stage, we can easily seen that $\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{U}_{k,n}^{e}(t))]$ increases with $t$ . Assume $t_{\\beta^{+},k}$ as the last time $\\exists i\\in\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)]$ such that $\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{U}_{k,n}^{e}(t))]-e$ \u00b7 \u03b2(O0()i,\u00b7),k \u2264(\u03c3\u2217S)\u22121 log(T \u2217), then for i \u2208E[Uke,n(t )] we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[e\\cdot\\beta_{O(\\varepsilon_{i},\\cdot),k}^{(\\tilde{\\varepsilon})}]\\leq\\mathbb{E}[e\\cdot\\beta_{O(\\varepsilon_{i},\\cdot),k}^{(\\varepsilon_{i},\\cdot)}]}\\\\ &{\\quad\\quad-\\eta_{0}\\left\\|\\frac{d_{1}k}{2K_{1}}\\right\\|_{\\L{e}^{\\varepsilon}[\\pm]}^{2}\\mathbb{E}_{[\\sigma]}\\mathbb{E}[\\xi_{n}^{\\prime}{\\prime}^{(\\iota)}\\mathbf{1}_{O(\\varepsilon_{i})}^{n}(\\sum_{\\ell\\leq\\nu_{k}^{\\prime},k}(\\sigma_{S}^{(\\iota)})_{\\iota}^{n}-\\sum_{\\ell\\leq\\nu_{k}^{\\prime}}(\\sigma_{S}^{(\\iota)})_{\\iota}^{n})]\\Big|_{\\L{e}^{\\varepsilon}[\\neq\\iota_{\\tilde{\\varepsilon}},\\cdot]}}\\\\ &{\\quad\\quad\\quad-\\eta_{0}\\displaystyle\\sum_{\\ell_{s}\\neq\\cdot,k}\\frac{\\|d_{1}k\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbb{E}_{[\\ell_{\\iota}^{\\prime}]}(\\sum_{\\ell\\leq\\nu_{k}^{\\prime},k}(\\sigma_{S}^{(\\iota)})_{\\iota}^{n}-\\sum_{\\ell\\leq\\nu_{k}^{\\prime},k}(\\sigma_{S}^{(\\iota)})_{\\iota}^{n})}\\\\ &{\\quad\\quad\\quad\\leq e\\cdot\\beta_{O(\\varepsilon_{i},\\cdot),k}^{(\\mathfrak{g})}+(\\sigma_{S}^{\\ast})^{-1}\\log(T^{\\ast})+\\frac{\\eta_{0}\\|d_{1}\\|^{2}}{2m K_{1}}-\\sum_{\\ell_{s}\\neq\\iota_{\\tilde{\\varepsilon}}<\\tilde{\\tau}}\\frac{p_{0}\\|d_{1}\\|^{2}}{2m K_{1}}\\mathbb{E}[\\ell_{n}^{\\prime}{\\prime}^{(\\iota)}]}\\\\ &{\\quad\\quad\\quad\\leq e\\cdot\\beta_{O(\\varepsilon_{i},\\cdot),k}^{(\\mathfrak{g})}+2(\\sigma_{S}^{\\ast})^{-1}\\log(T^{\\ast})-\\displaystyle\\sum_{\\ell_{s}\\neq\\iota_{\\tilde{\\varepsilon}}<\\tilde{\\tau}}\\frac{p_{0}\\|d_{1}\\|^{2}}{2m K_{1}}\\mathbb{E}[\\ell_{n\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the first inequality is by the positive nature of regularization term as well as the contribution of the gradient; second inequality is by E[\u2212\u2113\u2032n(t\u03b2+,k)] \u22641 and E[( l\u2208Sen,k $\\begin{array}{r}{\\mathbb{E}[(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-\\sum_{l\\in S_{n,k}^{-e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n})]\\leq1}\\end{array}$ ; the third inequality is by the condition $\\begin{array}{r}{\\eta_{0}=O\\big(\\frac{m K_{1}}{\\|\\mathbf{q}\\|^{2}}\\big)}\\end{array}$ and thus $\\begin{array}{r}{\\frac{\\eta_{0}\\|d_{k}\\|^{2}}{2m K_{1}}\\leq1\\leq\\log(T^{*})}\\end{array}$ , as well as $(\\sigma_{S}^{*})^{-1}\\geq1$ . The remaining job is to prove that ", "page_idx": 40}, {"type": "equation", "text": "$$\n-\\sum_{t_{\\beta}+\\mathbf{\\Upsilon}_{,k}<t<\\widetilde{T}}\\frac{\\eta_{0}\\|d_{k}\\|^{2}}{2m K_{1}}\\mathbb{E}[{\\ell_{n}^{\\prime}}^{(t)}\\mathbb{1}_{O_{(i)}}^{n}{}^{(t)}]\\le(\\sigma_{S}^{*})^{-1}\\log(T^{*}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Observe that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\ell_{n}^{\\prime}(^{\\prime})]\\vert=\\mathbb{E}[\\frac{1}{1+\\exp(y s_{n}\\cdot\\Big(\\sum_{e\\in[\\pm]}\\frac{e}{m}\\sum_{i\\in\\{\\mathbf{r}_{i}=\\frac{e}{m}\\}}\\sigma_{R}(\\mathbf{W}_{\\mathcal{O}_{(i,\\cdot)}}^{y}(^{t})\\sum_{l\\in[L]}{(\\sigma_{S}^{(t)})_{l}^{n}}y_{l}^{n})\\Big))}]}\\\\ &{\\quad\\quad\\leq\\mathbb{E}[\\exp\\left((\\sum_{i\\in\\mathcal{W}_{k,n}^{y_{S_{n}}}(t)-u_{k,n}^{y_{S_{n}}}(t)}-\\sum_{i\\in\\mathcal{W}_{k,n}^{y_{S_{n}}}(t)})(\\alpha_{\\mathcal{O}_{(i,\\cdot)},k}^{(t)}+(2\\sum_{l\\in\\mathcal{S}_{n,k}^{y_{S_{n}}}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)y s_{n}\\beta_{O_{(i,\\cdot)},k}^{(t)})\\right)]}\\\\ &{\\quad\\quad\\leq\\mathbb{E}[\\exp(\\kappa/2-\\frac{1}{m}\\sum_{i\\in\\mathcal{U}_{k,n}^{y_{S_{n}}}(t)}{(2\\sigma_{S}^{*}-1)y s_{n}\\beta_{O_{(i,\\cdot)},k}^{(t)}})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Here the first inequality is by $1/(1+\\exp(z))\\,\\leq\\,\\exp(-z)$ ; the second inequality is by Lemma 31; the last inequality is by the feeble $\\kappa/2$ and $\\begin{array}{r}{\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{U}_{k,n}^{e}(t))]\\ge(\\sigma_{S}^{*})^{-1}\\log(T^{*})}\\end{array}$ . Then we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\displaystyle\\sum_{t_{\\beta^{+},k}<t<\\tilde{T}}\\frac{\\eta_{0}\\|d_{k}\\|^{2}}{2m R_{1}}\\mathbb{E}[\\ell_{n}^{\\prime}{^{\\prime}}\\mathbb{1}_{\\Omega_{(i)}}^{n}(t)]\\leq\\displaystyle\\sum_{t_{\\beta^{+},k}<t<\\tilde{T}}\\frac{\\eta_{0}\\|d_{k}\\|^{2}}{2m R_{1}}\\cdot2\\exp(-\\log(T^{*}))}&{}\\\\ {\\displaystyle\\leq\\frac{\\tilde{T}\\eta_{0}\\|d_{k}\\|^{2}}{m R_{1}}\\exp(-\\log(T^{*}))\\leq\\frac{T^{*}\\eta_{0}\\|d_{k}\\|^{2}}{T^{*}m K_{1}}\\leq(\\sigma_{S}^{*})^{-1}\\log(T^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We complete the proof that $\\begin{array}{r}{0\\leq\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{U}_{k,n}^{e}(t))]-e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(0)}\\leq3(\\sigma_{S}^{*})^{-1}\\log(T^{*})\\leq(\\sigma_{S}^{*})^{-1}\\alpha.}\\end{array}$ We now prove a strong augmented hypothesis that there exist $i^{*}\\in\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(t)]$ for $\\forall0\\leq t\\leq T^{*}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}[|\\alpha_{O_{(i,\\cdot)},k}^{(t)}|/(e\\cdot\\beta_{O_{(i^{*},\\cdot)},k}^{(t)})]\\leq\\hat{C}\\frac{\\|c_{k}\\|^{2}}{\\sigma_{S}^{*}\\|d_{k}\\|^{2}},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we set C\u02c6 = 2C\u2032 2 log( 5K\u03b4m) for some constant C\u2032. i\u2217can be any element satisfies |\u03b2(O0()i\u2217,\u00b7),k| = $\\sigma_{1}/2\\lVert d_{k}\\rVert$ , which exists at $t=0$ by Lemma 7 as well as the fact that $\\|\\pmb{c}_{k}\\|>\\|\\pmb{d}_{k}\\|$ by their definition in Lemma 25. ", "page_idx": 40}, {"type": "text", "text": "Suppose Eq.(55) holds at $0\\leq t\\leq\\widetilde{T}-1$ , recall the update rule and the large $K$ condition, we can have a constant $C>1$ such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[e\\cdot\\partial_{\\sigma_{(i,\\star)}^{\\delta}}^{(\\delta)}\\mathbf{1}^{\\{i\\}}(t\\in\\mathcal{U}_{\\sigma_{(i,\\star)}^{\\delta}}^{\\varepsilon}(\\tilde{T}-1))]=\\mathbb{E}[(1-\\eta_{T-1})e\\cdot\\partial_{\\sigma_{(i,\\star)}^{\\delta}}^{(\\delta)}\\mathbf{1}^{\\{i\\}}\\{(t\\in\\mathcal{U}_{\\sigma_{(i,\\star)}^{\\delta}}^{\\varepsilon}(\\tilde{T}-1))\\}]}&{}\\\\ &{\\quad-\\eta_{T-1}\\frac{\\|\\mathbf{d}_{\\sigma_{(i,\\star)}^{\\delta}}^{\\varepsilon}\\|^{2}}{2\\operatorname*{min}}\\mathbb{E}[\\sigma_{\\mu_{(i,\\star)}^{\\delta}}^{\\varepsilon\\left(T-1)}(\\sum_{\\substack{l\\in\\mathcal{U}_{\\star}^{\\delta}}^{\\varepsilon}}(\\sigma_{\\delta}^{(\\tilde{T}-1)})_{l}^{\\kappa}-\\sum_{\\substack{l\\in\\mathcal{U}_{\\star}^{\\delta}}^{\\varepsilon}}(\\sigma_{\\delta}^{(\\tilde{T}-1)})_{l}^{\\kappa})],}\\\\ &{\\quad-\\underbrace{\\eta_{T-1}^{\\delta}\\left(1-\\eta_{T-1}\\lambda\\right)e\\cdot\\beta_{Q_{\\mu(i,\\star)}^{\\delta}}^{\\varepsilon\\left(T-1\\right)}}_{\\geq\\mathrm{E}[\\{(t_{i}^{\\delta}\\eta_{\\varepsilon}^{\\delta}-1)\\}],}\\mathbb{E}[(t\\circ\\mathcal{U}_{\\sigma_{(i,\\star)}^{\\delta}}^{\\varepsilon}(\\tilde{T}-1))]+\\frac{\\eta_{T-1}^{\\delta}\\gamma_{Q_{\\star}}^{\\varepsilon\\left(T-1\\right)}\\|\\mathbf{d}_{\\sigma_{(i,\\star)}^{\\delta}}^{\\varepsilon\\left(T-1\\right)}}{2\\operatorname*{min}}}\\\\ &{\\mathbb{E}[\\|\\sigma_{(i,\\star)}^{\\delta}\\|]=\\mathbb{E}\\Big[(1-\\eta_{T-1})\\alpha_{Q_{\\star}^{\\delta}-1}^{\\delta}}\\\\ &{\\quad-\\eta_{T-1}\\frac{\\|\\mathbf{d}_{\\sigma_{(i,\\star)}^{\\delta}}^{\\varepsilon}\\|^{2}}{2\\operatorname*{min}}\\sum_{\\in\\mathcal{V}_{\\sigma_{(i,\\star)}^\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the first inequality is due to the definition of $\\sigma_{S}^{*}$ and $\\mathcal{U}_{k,n}^{e}(\\widetilde t)$ ; the second inequality is due to the large $K=\\Omega(\\eta_{0}C(K_{1}-1)\\|\\mathbf{q}\\|^{2}/(m K_{1}))$ . Then we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[|\\alpha_{O_{(i,\\cdot)},k}^{(\\tilde{T})}|]}{\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(\\tilde{U})}\\mathbf{1}^{\\mathrm{\\tiny(\\it~\\sharp~}}\\!)](i\\in{\\mathcal{U}_{k,n}^{\\mathrm{e}}(\\tilde{t})})]}\\leq\\operatorname*{max}\\{\\frac{\\mathbb{E}[|\\alpha_{O_{(i,\\cdot)},k}^{(\\tilde{T}-1)}|]}{\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(\\tilde{T}-1)}\\mathbf{1}(i\\in{\\mathcal{U}_{k,n}^{\\mathrm{e}}(\\tilde{T}-1)})]},\\frac{C\\|e_{k}\\|^{2}}{\\sigma_{S}^{*}\\|d_{k}\\|^{2}}\\}\\leq\\hat{C}\\frac{\\|c_{k}\\|^{2}}{\\sigma_{S}^{*}\\|d_{k}\\|^{2}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last inequality is by the induction hypothesis and the $C^{\\prime}$ can be taken as $C$ , which completes the induction. ", "page_idx": 41}, {"type": "text", "text": "We now prove ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\geq\\mathbb{E}[e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{W}_{k,n}^{e}(t)-\\mathcal{U}_{k,n}^{e}(t))]-e\\cdot\\beta_{O_{(i,\\cdot)},k}^{(0)}}\\\\ &{\\quad\\geq-\\frac{\\hat{C}\\|c_{k}\\|^{2}}{\\sigma_{S}^{*}\\|d_{k}\\|^{2}}\\alpha-\\frac{\\sigma_{1}\\left(\\sigma_{S}^{*2}\\|d_{k}\\|^{2}+\\hat{C}\\|c_{k}\\|^{2}\\right)}{\\sigma_{S}^{*2}\\|d_{k}\\|}\\sqrt{2\\log(\\frac{5K m}{\\delta})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Recall the update rule ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\beta_{O_{(i,.,)},k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{O_{(i,.,)},k}^{(t)}-\\eta_{t}\\frac{\\|d_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}\\frac{\\mathbb{E}_{e}}{n\\in\\mathscr{V}_{k}^{\\epsilon}}[\\ell_{n}^{\\prime}{^{(t)}}\\mathbb{1}_{O_{(i)}}^{n}(t)(\\sum_{l\\in\\mathscr{S}_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-\\sum_{l\\in\\mathscr{S}_{n,k}^{-e}}(\\sigma_{S}^{(t)})_{l}^{n})].\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "nEeausryo tno $\\mathbb{E}[e\\cdot\\beta_{O_{(i,.)},k}^{(\\tau)}\\mathbb{1}(i\\in\\mathcal{W}_{k,n}^{e}(\\tau)-\\mathcal{U}_{k,n}^{e}(\\tau))]\\Big|_{\\tau=t}^{\\tau=0}\\leq0$ eand it\u2019s decreasing. As we know that the $i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{e}(t)-\\mathcal{U}_{k,n}^{e}(t)]$ $t+1$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\alpha_{O_{(i,.)},k}^{(t+1)}+e\\cdot\\big(\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t+1)})_{l}^{n}-\\sum_{l\\in S_{n,k}^{-e}}(\\sigma_{S}^{(t+1)})_{l}^{n}\\beta_{O_{(i,.)},k}^{(t+1)}\\big]\\leq0.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This indicates that for the neuron $i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{e}(\\widetilde{t})-\\mathcal{U}_{k,n}^{e}(\\widetilde{t})]$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\alpha_{O_{(i,.)},k}^{(\\widetilde{t})}+e\\cdot\\big(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(\\widetilde{t})}\\big)_{l}^{n}-\\sum_{l\\in S_{n,k}^{-e}}\\big(\\sigma_{S}^{(\\widetilde{t})}\\big)_{l}^{n}\\beta_{O_{(i,.)},k}^{(\\widetilde{t})}\\big]\\geq0.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now collaborating with Eq. (54) and Eq. (55), we now can have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[e\\cdot(\\sum_{i\\in\\mathcal{E}_{s,k}^{0}}(\\sigma_{k}^{(i)})^{n}-\\sum_{\\ell\\in\\mathcal{E}_{s,k}^{0}}(\\sigma_{k}^{(i)})_{\\ell}\\beta_{(\\ell),(k)}^{0})\\lambda\\geq-\\mathbb{E}[\\sigma_{(\\ell),(k)}^{(i)}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq-\\frac{\\hat{\\mathcal{C}}[\\ell]_{\\ell+1}}{\\mathcal{S}_{\\rho}^{\\ast}\\mathbb{E}[d_{k}]^{2}}((\\sigma_{k}^{(i)})^{-1}\\alpha+e\\cdot\\beta\\sigma_{(\\ell+1)})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq-\\frac{\\hat{\\mathcal{C}}[\\ell]_{\\ell+1}}{\\mathcal{S}_{\\rho}^{\\ast}\\mathbb{E}[d_{k}]^{2}}((\\sigma_{k}^{(i)})^{-1}\\alpha-}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\sqrt{2\\log(\\frac{58\\pi}{8})}\\sigma_{(k)}\\mathbb{I}_{k_{1}|\\ell|})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\mathbb{E}[e\\cdot\\mathcal{B}_{\\sigma_{(k)},k}^{(i)}\\mathbf{A}^{1}(i\\in\\mathcal{W}_{k,n}^{\\ast}(t)-\\mathcal{U}_{\\sigma_{(k)}}^{\\ast}(t))]-e\\cdot\\beta_{\\sigma_{(k)},k}^{(i)}\\mathbb{I}_{k_{1}|\\ell|}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\frac{\\sigma_{1}(\\sigma_{k}^{(i)})^{2}\\mathbb{I}[d_{k}]^{2}+\\hat{\\mathcal{C}}[\\ell]_{\\ell+1}|\\mathcal{C}|_{\\ell}|^{2}}{\\sigma_{k}^{(i)}\\mathbb{E}[d_{k}]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\sqrt{2\\log(\\frac{58\\pi}{8})}\\sigma_{k}^{(i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The proof is completed. ", "page_idx": 41}, {"type": "text", "text": "I.1.1 Expected 0-1 loss Convergence ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Lemma 33. Under Condition $^{\\,l}$ , there exist constant $C_{1}>0$ , after at most ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\hat{T}=\\frac{C_{1}\\sigma_{1}m\\lambda K_{1}\\gamma\\sqrt{(1+\\kappa_{y})\\log(5K m/\\delta)}}{(2\\sigma_{S}^{*}-1)^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "iterations, we have $L_{\\mathcal{D}_{S}}^{0-1}(\\mathbb{E}(\\Psi^{t}))=L_{\\mathcal{D}^{*}}^{0-1}(\\mathbb{E}(\\Psi^{t}))=0$ . ", "page_idx": 41}, {"type": "text", "text": "Proof. For $t\\leq\\widehat t$ , recall from Eq.(53) that for the period $t\\leq\\hat{T}$ , it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{A}_{t+1}^{k,e}\\mid\\Psi^{(t)}]\\ge\\mathbf{A}_{t}^{k,e}-(\\frac{c_{2}\\eta(2\\sigma_{S}^{*}-1)^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|^{2}}{16m K_{1}}\\cdot\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\ell_{n}^{\\prime}{^{(t)}}]).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Note that by definition $\\mathbf{A}_{0}^{k,e}=0$ , and we recursively use the equation t times ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\geq\\sum_{s=0}^{t-1}-\\frac{c_{2}\\eta(2\\sigma_{S}^{*}-1)^{2}(1-\\kappa_{y})\\lVert\\mathbf{q}\\rVert^{2}}{16m K_{1}}\\cdot\\mathbb{1}_{n\\in\\mathcal{V}_{k}^{e}}[\\ell_{n}^{\\prime}{^{(s)}}].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For each $k\\in[K_{1}],e\\in[\\pm]$ , denote by $\\tilde{t}^{k,e}$ the last time in the period $[0,T^{*}]$ satisfying that $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\le\\kappa$ . Then by Lemma 31 we see that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))]]\\right|\\leq3\\kappa/2.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus there exists a positive constant $\\widetilde{C}$ such that $-\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[\\ell_{n}^{\\prime}{}^{(t)}]\\geq\\widetilde C$ for $0\\leq t\\leq\\widetilde t^{k,e}$ . Then we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\geq\\frac{\\widetilde{C}c_{2}\\eta(2\\sigma_{S}^{*}-1)^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|^{2}t}{16m K_{1}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore we see that for $\\forall k\\in[K_{1}],e\\in[\\pm],\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]$ will reach $\\kappa$ within $16m K_{1}\\kappa$ $\\overline{{\\tilde{C}c_{2}\\eta(2\\sigma_{S}^{*}-1)^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|^{2}}}$ epochs. Recall that in this first stage the impact of decaying learning rate is under controlled by a large $\\gamma$ in Condition 1 as well as the slow quadratic decaying speed of $\\eta_{t}$ , under which we have $\\eta\\,=\\,\\Theta(\\eta_{0})$ . By $\\kappa\\leq8\\sigma_{1}\\|\\mathbf{q}\\|\\sqrt{(1+\\kappa_{y})\\log(5K m/\\delta)})$ , we see that there exist a positive constant $C_{1}=\\Theta(64/(\\widetilde{C}c_{2}))$ , the threshold time can be ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\hat{T}=\\frac{C_{1}\\sigma_{1}m\\lambda K_{1}\\gamma\\sqrt{(1+\\kappa_{y})\\log(5K m/\\delta)}}{(2\\sigma_{S}^{*}-1)^{2}(1-\\kappa_{y})\\|\\mathbf{q}\\|}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then by definition of 0-1 loss we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{D}^{\\ast}}^{0-1}(\\mathbb{E}(\\Psi^{\\hat{T}}))=\\mathbb{P}_{S_{n}\\sim\\mathcal{D}^{\\ast}}(y_{S_{n}}\\cdot f(\\mathbf{E}(S_{n}),\\mathbb{E}(\\Psi^{\\hat{T}}))\\leq0)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{P}_{S_{n}\\sim\\mathcal{D}^{\\ast}}(\\mathbb{E}[\\mathbf{A}_{\\hat{T}}^{k,e}]-\\kappa/2\\leq0)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{P}_{S_{n}\\sim\\mathcal{D}^{\\ast}}(\\mathbb{E}[\\kappa/2\\leq0)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The proof is completed. ", "page_idx": 42}, {"type": "text", "text": "I.1.2 Period 1: Decreasing Period of Correct Attention Score ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We claim that if $\\begin{array}{r}{\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i,\\cdot)},k}^{(0)}>0}\\end{array}$ during initialization, the expected attention score will not experience this decreasing period due to the expected gradient formula in Lemma 27. Our aim for this period is to examine the lower bound of the attention score during a limited number of iterations. ", "page_idx": 42}, {"type": "text", "text": "Lemma 34. Under Condition $^{\\,l}$ $,f o r\\,\\forall k\\in[K_{1}]$ , after at most a certain iterations ", "page_idx": 42}, {"type": "equation", "text": "$$\nT_{1}=\\frac{C_{3}\\sigma_{1}K_{1}\\gamma\\sqrt{10\\log(5K m/\\delta)}(1+e^{-2\\sigma_{0}^{2}\\|b_{k}\\|^{2}})}{2C_{4}\\|d_{k}\\|(1-e^{-2\\sigma_{0}^{2}\\|b_{k}\\|^{2}})},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $C_{3}$ is a positive constant, we would have the $\\beta_{Q,k}^{(t)}\\,=\\,\\beta_{K,k}^{(t)}$ be monotonically increasing during the remaining iterations $T_{1}\\leq t\\leq T^{*}$ . Besides, it holds that $\\sigma_{S}^{*}$ is the lower bound of the lowest correct attention assignment along the whole iterations: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sigma_{S}^{*}\\leq\\operatorname*{min}_{t\\in[T^{*}],k\\in[K_{1}]}\\{\\underset{n\\in\\mathcal{D}_{S}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}{(\\sigma_{S}^{(t)})}_{j}^{n}]\\}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. By Lemma 27, the \u03b2(Qt,+k1) $\\beta_{Q,k}^{(t+1)}=\\mathbb{E}[\\beta_{K,k}^{(t+1)}\\mid\\Psi^{(t)}]$ will be contributed to increase by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]\\mid\\mathbf{r}_{i}\\cdot\\boldsymbol{\\beta}_{O_{(i,\\cdot)},k}^{(t)}>0\\}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and they will be contributed to decrease by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]\\mid\\mathbf{r}_{i}\\cdot\\boldsymbol{\\beta}_{O_{(i,\\cdot)},k}^{(t)}<0\\}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By the fifth inequality in Lemma 7, we know that ", "page_idx": 43}, {"type": "text", "text": "$|\\{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(0)]\\mid\\mathbf{r}_{i}\\cdot\\boldsymbol{\\beta}_{O_{(i,\\cdot)},k}^{(0)}>0\\}|-\\frac{m}{4}\\Big|\\leq\\frac{m}{16},\\Big||\\{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(0)]\\mid\\mathbf{r}_{i}\\cdot\\boldsymbol{\\beta}_{O_{(i,\\cdot)},k}^{(0)}<0\\}|-\\frac{m}{4}\\Big|\\leq\\frac{m}{16}.$ As $\\begin{array}{r}{\\underset{\\i\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[\\ell_{n}^{\\prime}{}^{(t)}\\mathbb{1}_{O_{(i)}}^{n}{}^{(t)}(\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)(\\sum_{j\\in S_{n,k}^{-}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)]}\\end{array}$ is shared by all neurons, thus whether the $\\beta_{Q,k}^{(t)}$ and $\\beta_{K,k}^{(t)}$ will be contributed to increase or decrease depends on the signal of $\\begin{array}{r}{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}}\\end{array}$ . By the last inequality in in Lemma 7, we see that at initialization, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(0)]}\\mathbf{r}_{i}\\cdot\\boldsymbol{\\beta}_{O_{(i,\\cdot)},k}^{(0)}\\geq-\\sqrt{2\\log(\\frac{5K m}{\\delta})}\\cdot\\frac{5\\sigma_{1}\\|d_{k}\\|}{16}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By the expected gradient update in Lemma 28, the $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ will grow in $\\mathbf{r}_{i}$ \u2019s direction along the whole iterations. As such, the values of $\\mathbb{E}[\\mathbf{r}_{i}\\cdot\\beta_{O_{(i,\\cdot)},k}^{(t)}\\mathbb{1}(i\\in\\mathcal{W}_{k,n}^{\\pm}(t))],\\forall k\\in[K_{1}]$ will grow larger. Therefore, after a limited epochs we can have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\boldsymbol{\\beta}_{O_{(i,\\cdot)},k}^{(t)}\\geq0,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the $\\beta_{Q,k}^{(t)}$ and $\\beta_{K,k}^{(t)}$ would be contributed positively and monotonically increase. ", "page_idx": 43}, {"type": "text", "text": "Now we serve to find the lower bound of the evolution of $\\mathbb{E}[\\big(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big)]$ , which is clearly to be the first iteration where the negative $\\begin{array}{r}{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}}\\end{array}$ has grown to surpass the 0. By the symmetry property denoted in Lemma 22 and Eq.(39) we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}\\big[\\sum_{\\substack{j\\in S_{n,k}^{y_{S_{n}}}}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big]=\\frac{1}{1+e^{-2\\beta_{Q,k}^{(t)}^{2}/\\|b_{k}\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Recall that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\beta_{K,k}^{(t+1)}=\\beta_{Q,k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{Q,k}^{(t)}-\\frac{4\\eta_{t}\\beta_{Q,k}^{(t)}\\lVert b_{k}\\rVert^{4}}{K_{1}}\\sum_{e\\in[\\pm]}\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i,\\cdot)},k}^{(t)}\\frac{\\mathbb{E}}{n_{n}\\in\\mathcal{V}_{k}^{\\epsilon}}[\\ell_{n}^{'}{}^{(t)}\\mathbb{1}_{O_{(i)}}^{n}(t)]\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{\\displaystyle(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{j}^{n})(\\sum_{j\\in S_{n,k}^{-}}(\\sigma_{S}^{(t)})_{j}^{n})],}\\\\ {\\displaystyle\\beta_{O_{(i,{\\cdot}),k}}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{O_{(i,{\\cdot}),k}}^{(t)}-\\eta_{t}\\frac{\\|d_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}\\frac{\\mathbb{E}}{\\kappa}_{k}^{\\mathrm{\\parallel}}[\\ell_{n}^{'}{^{(t)}}\\mathbf{1}_{O_{(i)}}^{n}(t)\\big(\\sum_{l\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{l}^{n}-\\sum_{l\\in S_{n,k}^{-\\epsilon}}(\\sigma_{S}^{(t)})_{l}^{n}\\big)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and we also see that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{j}^{n})(\\sum_{j\\in S_{n,k}^{-}}(\\sigma_{S}^{(t)})_{j}^{n})]=\\left(\\exp(\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})+\\exp(-\\beta_{Q,k}^{(t)}\\cdot\\beta_{K,k}^{(t)}/\\|b_{k}\\|^{2})\\right)^{-2}\\leq\\frac{1}{4}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}}\\end{array}$   \nthere exists a constant C\u2032, such that for the limited decreasing period of E[( l\u2208Se  ( $\\mathbb{E}[\\big(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big)]$ , we have   \n$1\\,\\geq\\,-\\mathbb{E}(\\ell_{n}^{\\prime}{}^{(t)})\\,\\geq\\,\\widetilde{C}$ . Also, $m\\,\\geq\\,\\mathbb{E}[|\\mathcal{U}_{k,n}^{e}(0))|]\\,\\geq\\,m/8$ by Lemma 7, as well as the fact that $\\mathbb{E}[\\mathcal{W}_{k,n}^{e}(t)]$   \nwill at least preserve the neurons of $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(0))]$ along the iterations, without being deactivated as discussed in   \nLemma 29. Also, we note that in this hypothesised decreasing period, the absolute value of the initially negative   \nE[ i\u2208[m] ri\u03b2(Ot()i,\u00b7),k] and initially positive \u03b2(Qt,)k will all decreasing. Then by Condition 1 we see that the small   \ninitialization of MLP as well as the small regularization will make the decreasing order of $\\beta_{Q,k}^{(t)}$ negligible, as $\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}\\{|-\\lambda\\beta_{Q,k}^{(0)}+\\frac{4\\beta_{Q,k}^{(0)}\\|b_{k}\\|^{4}}{K_{1}}\\sum_{\\substack{e\\in[\\pm]\\,i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(0)]}}\\mathbf{r}_{i}\\cdot\\beta_{O_{(i,\\cdot),k}}^{(0)}\\mathbb{E}[(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{s}^{(0)})_{j}^{n})(\\sum_{j\\in S_{n,k}^{-}}(\\sigma_{s}^{(0)})_{j}^{n})\\ell_{n}^{\\prime}(t)]|\\}}\\\\ &{\\leq(\\lambda+\\frac{5\\sigma_{1}\\|\\mathbf{u}\\|^{4}\\|\\mathbf{q}\\|}{32K_{1}}\\sqrt{2\\log(\\frac{5K m}{\\delta})})\\beta_{Q,k}^{(0)}}\\end{array}$ \u2264O(1/C). ", "page_idx": 43}, {"type": "text", "text": "Here the second inequality is due to Eq.(58), (56) and the definition of the $\\boldsymbol{b}_{k}$ in Eq.(27); the third inequality is by tdhuer icnogn tdhitei odne $\\lambda\\leq(C\\sigma_{0}/2\\|\\mathbf{u}\\|^{2})^{-1}$ $\\beta_{Q,k}^{(t)}$ nasd $\\sigma_{1}\\leq(C\\sigma_{0}\\|\\mathbf{u}\\|^{4}\\|\\mathbf{q}\\|\\sqrt{\\log(5K m/\\delta)}/\\bar{K}_{1})^{-1}$ $\\begin{array}{r}{\\sum_{i\\in\\mathbb{Z}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot\\boldsymbol{\\beta}_{O_{(i,.)},k}^{(t)}}\\end{array}$ froerem, aiitn  hnoeldgsa ttihvaet, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t+1)]}\\mathbf{r}_{i}\\cdot\\beta_{O_{(i,\\cdot),k}}^{(t+1)}\\geq\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot\\beta_{O_{(i,\\cdot),k}}^{(t)}+\\frac{C_{4}\\eta_{0}\\|d_{k}\\|^{2}}{K_{1}}(2\\frac{1}{1+e^{-2\\beta_{Q,k}^{(0)}}/\\|b_{k}\\|^{2}}-1),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Here, by a appropriate chosen small $C_{4}$ , we again ignore the regularization term at this period due to $\\lambda=$ $O((C\\log(K\\hat{m}/\\hat{\\delta})\\|\\mathbf{q}\\|)^{-1})$ for a large $C$ by Condition 1, and the impact of the learning rate is also controlled due to the slow quadratic decaying nature of $\\eta_{t}^{\\prime}$ and a small initial $\\eta_{0}\\ {\\overset{\\cdot}{\\leq}}\\ O(0.01C^{-1})$ by Condition 1, so as the changing amount of $1/(1+e^{-2\\bar{\\beta_{Q,k}^{(0)}}^{2}/\\|b_{k}\\|^{2}})$ by Eq.(59).   \nTherefore, by Eq.(58) we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\beta_{Q,k}^{(t+1)}\\geq\\beta_{Q,k}^{(t)}(1+\\frac{C_{4}\\eta_{0}\\|b_{k}\\|^{4}}{K_{1}}\\sum_{\\substack{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}}\\mathbf{r}_{i}\\cdot\\beta_{O_{(i,\\cdot),k}}^{(t)}\\cdot\\big(\\sum_{\\substack{j\\in S_{n,k}^{+}}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)\\big(\\sum_{\\substack{j\\in S_{n,k}^{-}}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big))\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality is by the negative nature of $\\begin{array}{r}{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot\\boldsymbol{\\beta}_{O_{(i,\\cdot)},k}^{(t)}}\\end{array}$ , and the decaying nature of $\\eta_{t}$ and Eq.(58). Now we can see that there exists two surrogate sequences $\\beta_{Q,k}^{(t)}$ and $\\begin{array}{r}{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}}\\end{array}$ as the lower bound sequence of the $\\beta_{Q,k}^{(t)}$ and $\\begin{array}{r}{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}.}\\end{array}$ . These two former sequences\u2019s initial values are taken as the lower bounds of the latter two $(\\sigma_{0}\\|b_{k}\\|^{2}$ and $-\\sqrt{2\\log(5K m/\\delta)}\\cdot\\frac{5\\sigma_{1}\\|d_{k}\\|}{16})$ , and their update rule are ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\beta_{Q,k}^{(t+1)}}{\\beta_{Q,k}}=\\underbrace{\\beta_{Q,k}^{(t)}}_{j,k}+\\underbrace{\\beta_{Q,k}^{(t)}\\frac{C_{4}\\eta_{0}\\|b_{k}\\|^{4}}{K_{1}}}_{\\texttt{m}_{i}\\cdot e\\beta_{Q,k}^{(W_{k,n}^{\\pm}}(t)]}\\quad\\sum_{i\\in\\mathbb{Z}[\\mathscr{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot\\beta_{O_{(i,\\cdot),k}^{(t)}}^{(t)}\\cdot(\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n})(\\sum_{j\\in S_{n,k}^{-}}(\\sigma_{S}^{(t)})_{j}^{n}),}\\\\ &{\\displaystyle\\sum_{i\\in\\mathbb{Z}[\\mathscr{W}_{k,n}^{\\pm}(t+1)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot),k}^{(t+1)}}^{(t+1)}=\\displaystyle\\sum_{i\\in\\mathbb{Z}[\\mathscr{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot),k}^{(t)}}^{(t)}}\\\\ &{\\displaystyle\\xrightarrow{\\mathrm{~}}\\frac{1\\in\\mathbb{Z}[\\mathscr{W}_{k,n}^{\\pm}(t)]}{C_{4}\\eta_{0}\\|d_{k}\\|^{2}(2\\frac{1}{1+e^{-2\\beta_{Q,k}^{(0)}/\\|b_{k}\\|^{2}}}-1).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then by Lemma 11, let $a=\\frac{C_{4}\\eta_{0}\\|b_{k}\\|^{4}}{K_{1}},b=\\frac{C_{4}\\eta_{0}\\|d_{k}\\|^{2}}{K_{1}}(2\\frac{1}{1+e^{-2\\beta_{Q,k}^{(0)}}{}^{2}/\\|b_{k}\\|^{2}}-1)$ , we have the maximum iterations $T_{1}=\\frac{-z(0)\\big(1+e^{-2y(0)^{2}}\\big)}{b\\big(1-e^{-2y(0)^{2}}\\big)}=\\frac{\\sigma_{1}K_{1}\\gamma\\sqrt{10\\log\\big(5K m/\\delta\\big)}\\big(1+e^{-2\\sigma_{0}^{2}\\|b_{k}\\|^{2}}\\big)}{2C_{4}\\|d_{k}\\|\\big(1-e^{-2\\sigma_{0}^{2}\\|b_{k}\\|^{2}}\\big)}.$ , set $C_{3}=\\sqrt{10}/(2C_{4})$ we obtain the $T_{1}$ in the lemma. The lower bound of $\\beta_{Q,k}^{(t)}$ along the decreasing period as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\beta_{Q,k}=\\sigma_{0}\\big\\|b_{k}\\big\\|^{2}e^{-\\log(5K m/\\delta)}\\frac{25\\sigma_{1}^{2}\\big\\|\\mathbf{b}_{k}\\big\\|^{4}\\big(1+e^{-2\\sigma_{0}^{2}\\|b_{k}\\|^{2}}\\big)}{1024\\big(1-e^{-2\\sigma_{0}^{2}\\|b_{k}\\|^{2}}\\big)},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Utilizing the scale bounding property $(-\\kappa_{x}+1)/2\\|\\mathbf{u}\\|^{2}\\leq\\|b_{k_{1}}\\|^{2}<\\|\\mathbf{u}\\|^{2}/2$ in Eq. (27) and (36), we can denoted the lower bound of all $\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}$ for $\\forall k\\in[K_{1}]$ as $\\beta_{Q K}^{-}$ , which can be given as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\underline{{\\beta_{Q K}^{-}}}=\\frac{\\sigma_{0}(1-\\kappa_{x})\\|\\mathbf{u}\\|^{2}}{2}e^{-\\log(5K m/\\delta)}\\frac{\\sigma_{1}^{2}\\|\\mathbf{u}\\|^{4}\\big(1+e^{-\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{2}}\\big)}{\\big(1-e^{-\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{2}}\\big)},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Recall $\\sigma_{S}^{*}$ is defined as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sigma_{S}^{*}:=\\frac{1}{1\\b{\\Vert}^{4}(1+e^{-\\sigma_{0}^{2}\\Vert\\mathbf{u}\\Vert^{2}})},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which is actually can be written as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sigma_{S}^{*}=\\frac{1}{1+e^{-2\\beta_{Q K}^{-}}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Therefore, we see that $\\sigma_{S}^{*}$ is the lower bound of $\\begin{array}{r}{\\operatorname*{min}_{t\\in[T^{*}],k\\in[K_{1}]}\\big\\{\\underset{n\\in\\mathcal{D}_{S}}{\\mathbb{E}}\\big[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big]\\big\\}.}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "Remark 4. As we see that in Lemma 33, we require that the lower bound given in Eq.(53) depends that the values of $\\begin{array}{r}{\\mathbb{E}[\\mathbf{r}_{i}\\!\\left(2\\sum_{l\\in S_{n,k}^{e}}{\\left(\\sigma_{S}^{(t)}\\right)_{l}^{n}}\\,-\\,1\\right)\\beta_{O_{(i,\\cdot)},k}^{(t)})]}\\end{array}$ surpasses $\\kappa$ , which naturally says that the value of Ei\u2208Uke,n(0))[ri\u03b2(Ot()i,\u00b7),k] should surpass \u03ba since E[(2  l\u2208Sen,k ( $\\begin{array}{r}{\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}\\,-\\,1)]\\ \\le\\ 1.}\\end{array}$ . Therefore $\\mathbb{E}_{i\\in\\mathcal{U}_{k,n}^{e}(0))}[\\mathbf{r}_{i}\\beta_{O_{(i,\\cdot)},k}^{(t)}]$ should surpass 0 at s(itn)ce $\\kappa\\ >\\ 0_{\\perp}$ , which indicates that $\\hat{T}\\,>\\,T_{1}$ . We see that the initial period $t\\leq T_{1}$ is where $\\mathbb{E}_{i\\in\\mathcal{U}_{k,n}^{e}(0))}[\\mathbf{r}_{i}\\beta_{O_{(i,\\cdot)},k}^{(t)}]$ grow to surpass the initial scale, whose upper bound is $\\kappa/8$ by the definition of $\\kappa$ . ", "page_idx": 45}, {"type": "text", "text": "I.1.3 Period 2: Increasing Priod of Correct Attention Score ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "This period\u2019s analysis is based on Period 1 in Section I.1.2, or a good initialization such that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i,\\,.\\,)},k}^{(0)}>0.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Lemma 35. Under Condition $^{\\,l}$ , consider the duration after $T_{1}$ in Lemma 34, then for $\\forall k\\in[K_{1}],$ , consider the period $\\begin{array}{r}{T_{1}\\leq t\\leq T_{2}=C_{5}\\operatorname*{min}\\lbrace\\frac{1+\\gamma}{\\lambda},\\frac{\\|{\\mathbf{u}}\\|\\|{\\mathbf{q}}\\|}{\\lambda K_{1}\\sqrt{m}}\\rbrace}\\end{array}$ , where $C_{5}$ is a small constant. Then the following holds that ", "page_idx": 45}, {"type": "text", "text": "\u2022 We have $\\underline{{y}}(t),\\ \\overline{{y}}(t),\\ \\underline{{z}}(t),\\ \\overline{{z}}(t)$ be the lower and upper bounds of the increasing $\\beta_{Q,k}^{(t)}\\ =\\ \\beta_{K,k}^{(t)}$ and $\\begin{array}{r}{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\,\\cdot\\,e\\beta_{O_{(i,\\cdot)},k}^{(t)}}\\end{array}$ respectively. That is, there exists positive constants $c_{3-6}$ , for $\\underline{{{a}}}=\\frac{c_{3}(1-\\kappa_{\\mathbf{x}})\\|\\mathbf{u}\\|^{4}}{\\lambda\\gamma K_{1}}$ , $\\overline{{a}}=\\frac{c_{4}\\lVert{\\bf u}\\rVert^{4}}{\\lambda\\gamma K_{1}}$ $\\underline{{b}}=\\frac{c_{5}(1-\\kappa_{\\pmb{y}})\\|\\mathbf{q}\\|^{2}}{\\lambda\\gamma K_{1}};$ ), $\\overline{{b}}=\\frac{c_{6}\\|{\\bf q}\\|^{2}}{\\lambda\\gamma K_{1}})$ , $c^{\\prime}=\\widetilde{C}$ , it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\underline{{y}}(t)\\leq\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}\\leq\\overline{{y}}(t),\\quad\\underline{{z}}(t)\\leq\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}\\leq\\overline{{z}}(t),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for all $t\\geq T_{1}$ . Here, $\\overline{{y}}(t),\\,\\underline{{y}}(t),\\,\\overline{{z}}(t),\\,\\underline{{z}}(t)$ are the unique solutions of the following ODE System respectively ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}(\\mathrm{Ei}(2\\underline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\underline{{y}}(t)^{2})+4\\log(\\underline{{y}}(t)))=\\underline{{a}}b c^{\\prime}^{2}(2\\sigma_{S}^{*}-1)\\frac{(t-t_{1})^{2}}{2}}\\\\ &{+\\frac{1}{2}(\\mathrm{Ei}(\\log(\\frac{\\sigma_{S}^{*}}{1-\\sigma_{S}^{*}}))+\\mathrm{Ei}(\\log(\\frac{1-\\sigma_{S}^{*}}{\\sigma_{S}^{*}})))+4\\log(\\underline{{\\beta}}_{Q K}^{-}),}\\\\ &{z(t)=\\underline{{b}}c^{\\prime}(2\\sigma_{S}^{*}-1)(t-T_{1}),}\\\\ &{\\frac{1}{2}(\\mathrm{Ei}(2\\overline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\overline{{y}}(t)^{2})+4\\log(\\overline{{y}}(t)))=\\frac{\\overline{{a}}\\overline{{b}}t^{2}}{2}+\\overline{{a}}\\frac{\\kappa}{8}t}\\\\ &{+\\frac{1}{2}(\\mathrm{Ei}(\\frac{\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{4}}{2})+\\mathrm{Ei}(-2\\frac{\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{4}}{2}))+4\\log(\\sigma_{0}/2\\|\\mathbf{u}\\|^{2}),}\\\\ &{\\overline{{z}}(t)=\\overline{{b}}t+\\frac{\\kappa}{8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname{Ei}(x)=\\int_{-\\infty}^{x}{\\frac{e^{t}}{t}}\\mathrm{d}t=\\gamma_{E u l e r}+\\ln x+\\exp(x/2)\\sum_{n=1}^{\\infty}{\\frac{(-1)^{n-1}x^{n}}{n!2^{n-1}}}\\sum_{k=0}^{\\lfloor(n-1)/2\\rfloor}{\\frac{1}{2k+1}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "\u2022 For some limited constant $\\triangle$ such that $\\exists\\overline{{\\triangle}}_{:}$ , $\\sigma_{S}^{*}<\\triangle\\leq\\overline{{\\triangle}}<1$ . Then the $\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}$ will grow to make the correct attention score $\\begin{array}{r}{\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}\\left(\\sigma_{S}^{(t)}\\right)_{j}^{n}]}\\end{array}$ achieve the $\\triangle$ in at least a $\\triangle(1-\\triangle)$ scaled Gaussian rate such that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\beta_{Q,k}^{(t)}\\geq\\exp(\\frac{a b c^{\\prime}\\triangle(1-\\triangle)(2\\sigma_{S}^{*}-1)}{2}(t-T_{1})^{2}+\\log(\\underline{{\\beta_{Q K}^{-}}})).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. By Remark 4, we see that at the initial phase during $t\\geq T_{1}$ , we have $\\begin{array}{r}{\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i,.)},k}^{(0)}\\leq\\kappa/8}\\end{array}$ , and thus $\\alpha_{O_{(i,..)},k}^{(0)}\\leq\\hat{C}\\frac{\\|c_{k}\\|^{2}}{\\sigma_{S}^{*}\\|d_{k}\\|^{2}}$ C\u02c6\u03c3\u2217S\u2225c\u2225kd\u2225k\u22252 . This indicates that E[Atk,e] \u2264\u0398(\u03b1) and thus by ", "page_idx": 45}, {"type": "text", "text": "Lemmar 31 we see that the scale of $|\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[e f(\\mathbf{E}(S);\\mathbb{E}(\\Psi^{(t)}))]|$ is also $\\Theta(\\kappa)$ . This suggest that there still exists a constant $\\widetilde{C}$ , during a certain amount of subsequent iterations we would still have that ${\\widetilde C}\\leq-\\mathbb{E}[\\ell^{\\prime}(t)]\\leq1$ . Also, $m\\geq\\mathbb{E}[|\\mathcal{U}_{k,n}^{e}(0))|]\\geq m/8$ by Lemma 7, as well as the fact that $\\mathbb{E}[\\mathcal{W}_{k,n}^{e}(t)]$ will at least preserve the neurons of $\\mathbb{E}[\\mathcal{U}_{k,n}^{e}(0))]$ along the iterations, without being deactivated as discussed in Lemma 29. In addition, recall that in this first stage we also can control the impact of regularization and decaying learning rate by a small $\\lambda$ and a big $\\gamma$ by the sufficiently large $C$ in Condition 1, which indicates we now have ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathsf{\\Delta}_{Q,k}^{(t+1)}=\\beta_{Q,k}^{(t)}+\\Theta\\Big(\\beta_{Q,k}^{(t)}\\frac{C_{4}\\eta_{0}\\|b_{k}\\|^{4}}{K_{1}}\\sum_{\\substack{i\\in\\mathbb{E}[W_{k,n}^{\\pm}(t)]}}\\mathbb{E}[\\mathbf{r}_{i}\\beta_{O_{(i,\\cdot),k}}^{(t)}\\cdot\\boldsymbol{\\ell}^{\\prime}(t)\\frac{1}{1+e^{-2\\beta_{Q,k}^{(t)}/\\|b_{k}\\|^{2}}}\\frac{1}{1+e^{2\\beta_{Q,k}^{(t)}/\\|b_{k}\\|^{2}}}]}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t+1)]}\\mathbf{r}_{i}\\cdot\\mathbb{E}[e\\beta_{O_{(i,.)},k}^{(t+1)}]=\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot\\beta_{O_{(i,.)},k}^{(t)}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\operatorname{e}\\Bigl(\\frac{C_{4}\\eta_{0}\\|d_{k}\\|^{2}}{K_{1}}\\mathbb{E}[\\ell^{\\prime}(t)(2\\frac{1}{1+e^{-2\\beta_{Q,k}^{(t)}^{2}/\\|b_{k}\\|^{2}}}-1)]\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "By Lemma 12, we see that the iteration satisfies the ODE System 2 with a positive initialization, where the parameters in Lemma 12 are $\\mathbf{\\eta}_{t}^{\\prime\\prime}=-\\mathbb{E}[\\ell^{\\prime}(t)],a=\\Theta(\\frac{C_{4}\\eta_{0}\\bar{||}b_{k}\\|^{4}}{K_{1}}),b=\\Theta(\\frac{\\tilde{C}_{4}\\eta_{0}\\|d_{k}\\|^{2}}{K_{1}}),$ $c^{\\prime}=\\widetilde{C}$ . Then by solving the coupled ODE systems, collaborating the scale bounding property $(-\\kappa_{x}\\dot{+}1)/2\\|\\mathbf{u}\\|^{2}\\leq\\|b_{k_{1}}\\|^{2}<$ $\\|\\mathbf{u}\\|^{2}/2,-\\kappa_{y}\\dot{+}1/2\\|\\mathbf{q}\\|^{2}\\overset{\\cdot}{\\leq}\\|\\pmb{d}_{k_{1}}\\|^{2}<\\|\\mathbf{q}\\|^{2}/2$ in Eq. (27) and (36), as well as the Comparison Theorem with some constants $c_{3-6}$ , we can have upper and lower bound of $\\beta_{Q,k}^{(t)}$ and $\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}$ , which is the result in this lemma. ", "page_idx": 46}, {"type": "text", "text": "For the second result, given the $\\triangle$ , we can directly have a lower bound ODE $\\underline{{y}}_{\\triangle}(t)$ to be the lower bound of the $\\beta_{Q,k}^{(t)}$ via Comparison Theorem, where $\\underline{{y}}_{\\triangle}(t)$ satisfies ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underline{{y}}^{\\prime}(t)\\geq\\underline{{y}}_{\\triangle}^{\\prime}(t)=\\underline{{a}}\\underline{{b}}c^{\\prime}\\triangle(1-\\triangle)(2\\sigma_{S}^{*}-1))(t-T_{1})\\underline{{y}}_{\\triangle}(t)\\Rightarrow}\\\\ &{\\underline{{y}}(t)\\geq\\underline{{y}}_{\\triangle}(t)=\\exp(\\frac{\\underline{{a}}b c^{\\prime}\\triangle(1-\\triangle)\\left(2\\sigma_{S}^{*}-1\\right)}{2}(t-T_{1})^{2}+\\log(\\underline{{\\beta_{Q K}^{-}}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the inequality holds by the decaying nature of $g(x)=x(1-x)$ when $x>1/2$ . The proof is completed. ", "page_idx": 46}, {"type": "text", "text": "Lemma 36. (Asymptotic Property 1). In the first stage, the growing of $\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}$ as well as the attention score enjoys the asymptotic property that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow+\\infty}\\frac{\\mathbb{E}[\\beta_{Q,k}^{\\left(t\\right)}]}{\\log(t)}=\\Theta(1),\\quad\\operatorname*{lim}_{t\\rightarrow+\\infty}\\frac{\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{y_{S_{n}}}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}\\left(\\sigma_{S}^{\\left(t\\right)}\\right)_{j}^{n}]]}{\\frac{t^{4}}{1+t^{4}}}=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. By the asymptotic property of $\\operatorname{Ei}(x)$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{x\\rightarrow+\\infty}{\\frac{\\operatorname{Ei}(x)+\\operatorname{Ei}(-x)}{\\frac{\\exp(x)}{x}}}=1.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "This suggest that when $y(t)\\geq\\underline{{y}}(t)$ is close to infinity, the lower bound ODE in Lemma 35 will approximately satisfies the following ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\exp(2\\underline{{y}}(t)^{2})}{4\\underline{{y}}(t)^{2}}+2\\log(\\underline{{y}}(t))\\approx\\underline{{a}}\\underline{{b}}c^{\\prime}^{2}(2\\sigma_{S}^{*}-1)\\frac{t^{2}}{2}+\\mathrm{const},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "This suggest that roughly ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to+\\infty}\\underline{{y}}(t)^{2}/\\log(t^{2})=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then we see that as $y(t)$ goes to infinity, we have a lower bound ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to+\\infty}\\log(\\frac{\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}]}{1-\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}]})/2\\log(t^{2})=\\Theta(1)\\Rightarrow\\operatorname*{lim}_{t\\to+\\infty}\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}]/\\frac{t^{4}}{1+t^{4}}=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "On the other hand, obtaining an upper bound over $y(t)$ is relatively easy. Since we have $2\\!+\\!e^{-2\\underline{{y}}(t)^{2}}\\!+\\!e^{2\\underline{{y}}(t)^{2}}\\leq4$ and $(1-e^{-2\\underline{{y}}(t)^{2}})/(1+e^{-2\\underline{{y}}(t)^{2}})\\leq1$ , which gives the upper bound ODE over attention and MLP considering $z(0)>0$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{2}(\\mathrm{Ei}(2\\overline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\overline{{y}}(t)^{2})+4\\log(\\overline{{y}}(t)))=\\frac{\\overline{{a}}\\overline{{b}}t^{2}}{2}+\\overline{{a}}\\frac{\\kappa}{8}t+\\mathrm{const.}}\\\\ &{\\overline{{z}}(t)=b t+\\mathrm{const,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the term \u201cconst\u201d ensure that ${\\overline{{y}}}(0)=y(0)$ . The asymptotic property of this ODE system is the same as the one of lower bound ODE. Then consider $t$ , $y(t)$ both go to infinity, we have some $\\hat{c}$ such that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow+\\infty}\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}]/\\frac{\\big(t+\\hat{c}t^{1/2}\\big)^{4}}{1+\\big(t+\\hat{c}t^{1/2}\\big)^{4}}=\\operatorname*{lim}_{t\\rightarrow+\\infty}\\mathbb{E}[\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}]/\\frac{t^{4}}{1+t^{4}}=1.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "I.2 Second Stage: Regularizing the Model ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "As the $\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}$ and $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ are continually gronwing up, we see that thne decaying $-\\mathbb{E}[\\ell^{\\prime}(t)]$ , as well as the decaying attention score products $\\begin{array}{r}{(\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n})\\big(1-\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)}\\end{array}$ is becoming feeble and feeble, under which we can no longer ignore the regularization term safely when estimating the coefficient gradient dynamics. However, although the regularization can prevent the coefficients from growing, it will maintain their scales without decreasing them. ", "page_idx": 47}, {"type": "text", "text": "Lemma 37. Under Condition $^{\\,l}$ , consider $e=\\mathbb{E}[y_{S_{n}}]$ for all $t\\in[T_{2},T^{*}]$ it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{Q_{(i,\\lambda),\\lambda}}^{(\\theta)}=O(\\frac{\\|\\mathbf{q}\\|^{2}}{\\lambda\\mathbf{m}(K_{1})}),}\\\\ &{e_{\\theta_{(i,\\lambda)}}^{(T^{*})}=\\Theta(\\frac{\\sigma_{s}^{5}(1-K_{w})^{2}}{(1+\\kappa_{y})^{2}}\\log(\\frac{e^{-\\kappa_{1}}\\|\\mathbf{q}\\|^{2}(2\\sigma_{s}^{*}-1)}{\\lambda m K_{1}})),}\\\\ &{\\beta_{q,k}^{(t)}=O(\\sqrt{\\|\\mathbf{u}\\|\\log(\\frac{\\|\\mathbf{q}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m K_{1}})^{2}}),}\\\\ &{\\beta_{q,k}^{(T^{*})}=\\Theta(\\|\\mathbf{u}\\|\\sqrt{\\log(\\frac{\\|\\mathbf{q}\\|^{2}\\sigma_{s}^{2}(1-K_{w})^{2}}{\\lambda K_{1}(1+\\kappa_{y})^{2}}\\log(\\frac{e^{-\\kappa_{1}}\\|\\mathbf{q}\\|^{2}(2\\sigma_{s}^{*}-1)}{\\lambda m K_{1}}))},}\\\\ &{\\mathbb{E}[\\big(\\sum_{i}\\binom{\\sigma_{s}^{(t)}}{\\lambda}_{j}^{n}]=O(\\frac{1}{1+\\frac{1}{2\\lambda\\mathbf{q}_{s}^{1/2}}\\|\\mathbf{q}\\|^{2}}),}\\\\ &{\\mathbb{E}[\\big(\\sum_{i}\\binom{\\sigma_{s}^{(t+1)}}{\\lambda}_{j}^{n}\\big)]=\\Theta(\\frac{1}{1+\\frac{\\lambda\\mathbf{q}_{s}^{1/2}}{1+\\frac{\\lambda\\mathbf{q}_{s}^{1/2}}{2}\\|\\mathbf{q}\\|^{2}}}1}\\\\ &{\\mathbb{E}[\\big(\\sum_{i}\\binom{\\sigma_{s}^{(t+1)}}{\\lambda}_{j}\\big(\\sigma_{s}^{*}-1\\big)\\big)]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "$e\\boldsymbol{\\beta}_{O_{(i,.)},k}^{(t)}\\;r e p r e s e n t s\\;m\\mathbf{r}_{i}\\boldsymbol{\\beta}_{O_{(i,.)},k}^{(t)}.$ . That is, we consider the positive growth of $\\mathbb{E}[\\beta_{O_{(i,\\,*)},k}^{(t)}].$ ", "page_idx": 47}, {"type": "text", "text": "Proof. We will prove the desired argument based on the following induction hypothesis: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e\\beta_{O_{(i,\\cdot)},k}^{(t)}=O(\\frac{\\|\\mathbf{q}\\|^{2}}{\\lambda m K_{1}}),}\\\\ &{\\beta_{Q,k}^{(t)}=O(\\|\\mathbf{u}\\|\\sqrt{\\log(\\frac{2\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m K_{1}^{2}})}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We split the situations into two cases: ", "page_idx": 47}, {"type": "text", "text": "(i). $\\begin{array}{r}{e\\beta_{O_{(i,\\cdot)},k}^{(t)}\\le\\Theta\\big(\\frac{\\sigma_{S}^{*}{2}(1-\\kappa_{y})^{2}}{(1+\\kappa_{y})^{2}}\\log\\big(\\frac{e^{-\\kappa}\\|\\mathbf{q}\\|^{2}(2\\sigma_{S}^{*}-1)}{\\lambda m K_{1}}\\big)\\big).}\\end{array}$ ,   \nand $\\begin{array}{r}{\\beta_{Q,k}^{(t)}\\leq\\Theta(\\|\\mathbf{u}\\|\\sqrt{\\log(\\frac{\\|\\mathbf{u}\\|^{2}\\sigma_{S}^{*2}(1-\\kappa_{y})^{2}}{\\lambda K_{1}(1+\\kappa_{y})^{2}}\\log(\\frac{e^{-\\kappa}\\|\\mathbf{q}\\|^{2}(2\\sigma_{S}^{*}-1)}{\\lambda m K_{1}}))});}\\end{array}$   \n(ii). e\u03b2(Ot()i,\u00b7),k \u2265   \nand $\\begin{array}{r}{\\beta_{Q,k}^{(t)}\\geq\\|\\mathbf{u}\\|\\sqrt{\\frac{1}{2}\\log(\\frac{6\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m K_{1}^{2}})}\\Rightarrow\\mathbb{E}\\big[\\big(\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)\\big]\\geq\\frac{1}{1+\\frac{\\lambda^{2}m K_{1}^{2}}{e^{\\lambda_{1,1}/2}\\lambda^{2}}}}\\end{array}$ \u03bb21mK12 . Easy to note that the scales\u2019 1+6\u2225u\u22252\u2225q\u22252   \norders of the case (i)\u2019s quantities are less than those of case (ii), thus this split is plausible. ", "page_idx": 47}, {"type": "text", "text": "Recall ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\beta_{O_{(i,.,)},k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{O_{(i,.,)},k}^{(t)}-\\eta_{t}\\frac{\\|d_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbf{r}_{i}\\frac{\\mathbb{E}_{e}}{n\\in\\mathscr{V}_{k}^{\\epsilon}}[\\ell_{n}^{\\prime}{^{(t)}}\\mathbb{1}_{O_{(i)}}^{n}(t)(\\sum_{l\\in\\mathscr{S}_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-\\sum_{l\\in\\mathscr{S}_{n,k}^{-e}}(\\sigma_{S}^{(t)})_{l}^{n})].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Then it\u2019s easy to check that for case (i), as by Lemma 30 we see that the magnitude of $\\mathbb{E}[|\\alpha_{O_{(i,\\cdot)},k}^{(t)}|]$ is controlled by some $\\hat{C}\\sigma_{S}^{*\\,-2}(1+\\kappa_{\\pmb{y}})^{2}(1-\\kappa_{\\pmb{y}})^{-2}\\beta_{O_{(i^{*},\\cdot)},k}^{(t)}\\geq\\hat{C}$ . That means that the term $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]$ can be controlled by its contributor \u0398(e\u03b2(Ot()i,\u00b7),k). Then we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta(e\\beta_{O_{(i,\\cdot)},k}^{(t)}/\\mathbb{E}[-\\ell_{n}^{\\prime}^{\\prime}^{\\prime}^{\\ (t)}])\\leq\\Theta(e\\beta_{O_{(i,\\cdot)},k}^{(t)}(1+e^{\\kappa}e^{\\sigma_{S}^{*}-2(1+\\kappa_{y})^{2}(1-\\kappa_{y})^{-2}e\\beta_{O_{(i,\\cdot)},k}^{(t)}}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\Theta(e^{\\kappa}e^{\\sigma_{S}^{*}-2(1+\\kappa_{y})^{2}(1-\\kappa_{y})^{-2}e\\beta_{O_{(i,\\cdot)},k}^{(t)}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\Theta(\\frac{\\|{\\bf q}\\|^{2}\\left(2\\sigma_{S}^{*}-1\\right)}{\\lambda m K_{1}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the first inequality is by the definition of $\\mathbb{E}[-\\ell_{n}^{\\prime}{}^{(t)}])$ (similar to the techniques in Lemma 32) and the definition of $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]$ ; the second inequality is by $g(x)=x<e^{x}-1$ as well as $\\sigma_{S}^{*\\,-2}(1\\,{+}\\,\\kappa_{y})^{2}(1\\,{-}\\,\\kappa_{y})^{-2}>1$ ; the second inequality is by the case (i) hypothesis. Then we would have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\lambda e{\\beta}_{O_{(i,\\cdot)},k}^{(t)}\\leq\\Theta(-\\frac{\\|d_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbb{E}[{\\mathbf{r}_{i}}\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\ell_{n}^{\\prime}{^{(t)}}\\mathbb{1}_{O_{(i)}}^{n}(t)\\big(\\sum_{l\\in S_{n,k}^{e}}(\\sigma_{S}^{(t)})_{l}^{n}-\\sum_{l\\in S_{n,k}^{-e}}(\\sigma_{S}^{(t)})_{l}^{n})]]).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus th\u2217e growing o\u03c3f $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ woulde \u2212b\u03bae \u2225nqo\u2225n2-(d2e\u03c3g\u2217e\u2212n1e)rated: $\\mathbb{E}[e\\beta_{O_{(i,\\cdot)},k}^{(t+1)}]\\ge\\Theta(e\\beta_{O_{(i,\\cdot)},k}^{(t)})$ , which directly suggest $\\begin{array}{r}{\\mathbb{E}[\\beta_{O_{(i,\\cdot)},k}^{(T^{*})}]=\\Theta(\\frac{\\sigma_{S}^{*}{2(1-\\kappa_{y})^{2}}}{(1+\\kappa_{y})^{2}}\\log(\\frac{e^{-\\kappa}\\|\\mathbf{q}\\|^{2}(2\\sigma_{S}^{*}-1)}{\\lambda m K_{1}}))}\\end{array}$ ( \u03bbmK S )) holds since T \u2217is the maximum admissible iterations. Similarly, for the $\\beta_{Q,k}^{(t)}$ in case (i), first recall that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\beta_{Q,k}^{(t+1)}=(1-\\eta_{t}\\lambda)\\beta_{Q,k}^{(t)}-\\frac{4\\eta_{t}\\beta_{K,k}^{(t)}\\|b_{k}\\|^{4}}{K_{1}}\\sum_{e\\in[\\pm]}\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i,\\cdot)},k_{n}}^{(t)}\\mathbb{E}_{e\\nu_{k}^{[t}}[\\ell_{n}^{\\prime}]\\mathbb{1}_{O_{(i)}}^{n}(t)\\big(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{j}^{n}\\big)}\\\\ {\\displaystyle(1-\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{j}^{n})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "then, as we see that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\big(\\displaystyle\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)(1-\\displaystyle\\sum_{j\\in S_{n,k}^{+}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)]=\\frac{1}{1+e^{-2\\beta_{Q,k}^{(t)}^{(t)}\\frac{2}{2}/\\|b_{k}\\|^{2}}}\\frac{1}{1+e^{2\\beta_{Q,k}^{(t)}^{2}/\\|b_{k}\\|^{2}}}}\\\\ &{\\phantom{m m m m m m m m m m m m m m}=\\frac{1}{2+e^{2\\beta_{Q,k}^{(t)}\\frac{2}{2}/\\|b_{k}\\|^{2}}+e^{-2\\beta_{Q,k}^{(t)}^{(t)}\\frac{2}{2}/\\|b_{k}\\|^{2}}}}\\\\ &{\\phantom{m m m m m m m m m m m m m}\\leq\\Theta(\\frac{\\|\\mathbf{u}\\|^{2}\\sigma_{S}^{+}(1-\\kappa_{y})^{2}}{\\lambda K_{1}(1+\\kappa_{y})^{2}}\\log(\\frac{e^{-\\kappa_{\\|\\mathbf{q}\\|^{2}}(2\\sigma_{S}^{*}-1)}}{\\lambda m K_{1}}))}\\\\ &{\\phantom{m m m m m m m m m m m m}=\\Theta(\\frac{\\|\\mathbf{u}\\|^{2}\\sigma_{S}^{*2}\\big(1-\\kappa_{y}\\big)^{2}}{\\lambda K_{1}(1+\\kappa_{y})^{2}}\\log(\\frac{e^{-\\kappa_{\\|\\mathbf{q}\\|^{2}}(2\\sigma_{S}^{*}-1)}}{\\lambda m K_{1}}))^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the first inequality by the definition of $\\mathbb{E}[\\big(\\sum_{j\\in S_{n,k}^{\\pm}}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\big)]$ and the induction hypothesis; the second inequality is by the small $\\lambda$ by Condition 1 with a sufficiently large $C$ . Then we see that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\Theta(-\\frac{4\\|b_{k}\\|^{2}}{K_{1}}\\mathbb{E}[\\sum_{\\substack{e\\in[\\pm]\\,i\\in[m]}}\\mathbf{r}_{i}\\beta_{O(i,.),k}^{(t)}\\underset{n\\in\\mathcal{V}_{k}^{\\epsilon}}{\\mathbb{E}}[\\ell_{n}^{\\prime}{}^{(t)}\\mathbb{1}_{O_{(i)}}^{n}(t)(\\sum_{\\substack{j\\in S_{n,k}^{+}}}(\\sigma_{S}^{(t)})_{j}^{n})(\\sum_{\\substack{j\\in S_{n,k}^{-}}}(\\sigma_{S}^{(t)})_{j}^{n})]])\\geq\\Theta(\\lambda).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "$e\\beta_{O_{(i,\\cdot)},k}^{(t)},-\\mathbb{E}[\\ell^{\\prime}(t)]\\leq1$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{e\\in[\\pm]}\\sum_{i\\in[m]}\\mathbf{r}_{i}\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[\\mathbb{1}_{O_{(i)}}^{n}(t)]]=\\sum_{e\\in[\\pm]}\\mathbb{E}[\\sum_{i\\in\\mathcal{W}_{k,n}^{e}(t)}/m]\\leq2.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus we see that the growing of $\\beta_{Q,k}^{(t)}$ would also be non-degenerated: $\\beta_{Q,k}^{(t+1)}\\ge\\Theta(\\beta_{Q,k}^{(t)})$ . This also directly validates that for the maximum admissible iterations , it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{Q,k}^{(T^{*})}=\\Theta(\\|\\mathbf{u}\\|\\sqrt{\\log(\\frac{\\|\\mathbf{u}\\|^{2}\\sigma_{S}^{*}2(1-\\kappa_{y})^{2}}{\\lambda K_{1}(1+\\kappa_{y})^{2}}\\log(\\frac{e^{-\\kappa}\\|\\mathbf{q}\\|^{2}(2\\sigma_{S}^{*}-1)}{\\lambda m K_{1}}))}),}\\\\ &{\\mathbb{E}[(\\sum_{j\\in S_{n,k}^{e}}(\\sigma_{S}^{(T^{*})})_{j}^{n})]=\\Theta(\\frac{1}{1+\\frac{\\lambda K_{1}(1+\\kappa_{y})^{2}}{\\|\\mathbf{u}\\|^{2}\\sigma_{S}^{*}2(1-\\kappa_{y})^{2}}\\log^{-1}(\\frac{e^{-\\kappa}\\|\\mathbf{q}\\|^{2}(2\\sigma_{S}^{*}-1)}{\\lambda m K_{1}})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For case (ii), we directly check that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\lambda e\\beta_{O_{(i,\\cdot)},k}^{(t)}\\geq-\\frac{\\|d_{k}\\|^{2}}{2K_{1}}\\sum_{e\\in[\\pm]}\\mathbb{E}[\\mathbf{r}_{i}\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[\\ell_{n}^{\\prime}{^{\\prime}}_{1}^{(t)}\\mathbb{1}_{O_{(i)}}^{n}(t)\\big(\\sum_{l\\in\\mathcal{S}_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-\\sum_{l\\in\\mathcal{S}_{n,k}^{-e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big)]].\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Here the inequality is by $-\\mathbb{E}[\\ell^{\\prime}(t)]\\leq1$ and ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\big(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-\\sum_{l\\in S_{n,k}^{-e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n})]=[\\mathbb{E}(2\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}-1)]\\le1.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "As a result, by the gradient form we see that $\\beta_{Q,k}^{(t+1)}\\,\\le\\,\\beta_{Q,k}^{(t)}$ , and thus we prove the induction proving goal $\\begin{array}{r}{\\mathbb{E}[e\\beta_{O_{(i,\\cdot)},k}^{(t+1)}]=O\\big(\\frac{\\|{\\bf q}\\|^{2}}{\\lambda m K_{1}}\\big).}\\end{array}$ ", "page_idx": 49}, {"type": "text", "text": "Similarly, as we now have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S_{n,k}^{+}}^{(t)})_{j})(1-\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S_{n}^{+}}^{(t)})_{j}^{n})]=\\frac{1}{1+e^{-2\\beta_{Q,k}^{(t)}/\\|b_{k}\\|^{2}}}\\frac{1}{1+e^{2\\beta_{Q,k}^{(t)}/\\|b_{k}\\|^{2}}}}&{}\\\\ &{\\quad=\\frac{1}{2+e^{2\\beta_{Q,k}^{(t)}/\\|b_{k}\\|^{2}}+e^{-2\\beta_{Q,k}^{(t)}/\\|b_{k}\\|^{2}}}}\\\\ &{\\quad\\geq\\frac{1}{3}\\frac{1}{1+e^{2\\beta_{Q,k}^{(t)}/\\|b_{k}\\|^{2}}/3}}\\\\ &{\\quad\\geq\\frac{1}{3}\\frac{\\|1\\}{1+\\frac{2\\|M\\|^{2}\\|a\\|^{2}}{\\sqrt{2}\\cdot\\|M\\|^{2}}},}\\\\ &{\\quad=\\Theta(\\frac{\\lambda^{2}m K_{1}^{1}}{2\\|1\\|^{2}\\|Q\\|^{2}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the first inequality is by (ii); the last equality is by the small $e^{-2\\beta_{Q,k}^{(t)\\mathrm{~2~}}/\\Vert b_{k}\\Vert^{2}}\\leq1$ $\\lambda$ in Condition 1 for a sufficiently large ; the second inequality is by the induction hypothesis of case $C$ . Then we observe that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\lambda\\geq\\Theta(-\\frac{4\\|b_{k}\\|^{2}}{K_{1}}\\mathbb{E}[\\sum_{e\\in[\\pm]}\\sum_{i\\in[m]}\\mathbf{r}_{i}\\beta_{O_{(i,\\cdot)}^{(i)},k}^{(t)}\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{\\mathrm{e}}}[\\ell_{n}^{\\prime}{^{(t)}}\\mathbb{1}_{O_{(i)}}^{n}(\\ell)\\big(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{j}^{n}\\big)(\\sum_{j\\in S_{n,k}^{-}}(\\sigma_{S}^{(t)})_{j}^{n})]]),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the inequality is by $-\\mathbb{E}[\\ell^{\\prime}(t)]\\leq1$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{e\\in[\\pm]}\\sum_{i\\in[m]}\\mathbf{r}_{i}\\underset{n\\in\\mathcal{V}_{k}^{e}}{\\mathbb{E}}[\\mathbb{1}_{O_{(i)}}^{n}(\\mathfrak{t})]]=\\sum_{e\\in[\\pm]}\\mathbb{E}[\\sum_{i\\in\\mathcal{W}_{k,n}^{e}(t)}/m]\\leq2,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "as well as the induction hypothesis upon $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ in case (ii). Thus $\\beta_{Q,k}^{(t+1)}\\le\\Theta(\\beta_{Q,k}^{(t)})$ , which support our proving goal in this induction process: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\beta_{Q,k}^{(t+1)}]=O(\\|\\mathbf{u}\\|\\sqrt{\\log(\\frac{2\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m{K_{1}}^{2}})}).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "In addition, we can see that even if we suggest the MLP\u2019s $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ is growing in a fastest linear-level speed, it require at least (1 +\u03bb \u03b3) to reach the maximum admissible value2 $\\frac{\\|\\mathbf{q}\\|^{2}}{2\\lambda m K_{1}}$ . Meanwhile, we see that even when considering the fast speed of the increasing attention, by the asymptotic perperty 1 discussed in Lemma 36, we see that we still require $\\Theta(\\frac{\\|\\mathbf{u}\\|\\|\\mathbf{q}\\|}{\\lambda K_{1}\\sqrt{m}})$ to reach the highest admissible correct attention score $\\frac{1}{1+\\frac{{\\lambda^{2}m{K_{1}}^{2}}}{2\\|{\\bf{u}}\\|^{2}\\|{\\bf{q}}\\|^{2}}}.$ ", "page_idx": 49}, {"type": "text", "text": "Therefore, we can have some appropriately small constants $C_{5}$ , and when the iteration number is more than $T_{2}=C_{5}\\operatorname*{min}\\{\\frac{1+\\gamma}{\\lambda},\\frac{\\lVert{\\bf u}\\rVert\\lVert{\\bf q}\\rVert}{\\lambda K_{1}\\sqrt{m}}\\}$ , we need to consider the impact of regularization. \u53e3 ", "page_idx": 50}, {"type": "text", "text": "Lemma 38. The scale of the coefficients will finally be stabilized at a considerable level: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{O(i,\\cdot),k}^{(T^{*})}\\leq e\\beta_{O_{(i,\\cdot)},k}^{(T^{*})}=\\Theta(\\log(\\frac{\\|{\\bf q}\\|^{2}}{m\\lambda K_{1}})),}\\\\ &{\\beta_{Q,k}^{(T^{*})}=\\Theta(\\|{\\bf u}\\|\\sqrt{\\log(\\frac{\\|{\\bf u}\\|^{2}}{\\lambda K_{1}}\\log(\\frac{\\|{\\bf q}\\|^{2}}{m\\lambda K_{1}}))}),}\\\\ &{\\mathbb{E}[\\big(\\displaystyle\\sum_{j\\in S_{n,k}^{e}}(\\sigma_{S}^{(T^{*})})_{j}^{n})\\big]=\\Theta(\\frac{1}{1+\\frac{\\lambda K_{1}}{\\|{\\bf u}\\|^{2}}\\log(\\frac{m\\lambda K_{1}}{\\|{\\bf q}\\|^{2}})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "$e\\beta_{O_{(i,\\,.)},k}^{(t)}$ represents $m\\mathbf{r}_{i}\\beta_{O_{(i,\\,.)},k}^{(t)}$ . That is, we consider the positive growth of $|\\beta_{O_{(i,\\,.)},k}^{(t)}|$ ", "page_idx": 50}, {"type": "text", "text": "Proof. Recall the last discussion in Lemma 29, we see that as $\\begin{array}{r}{\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)e\\beta_{O_{(i,\\cdot)},k}^{(t)}]}\\end{array}$ getting larger and larger, it will finally reach the scale of \u03b1(Ot()i, \u00b7),k, which has updated in a feeble speed controlled by initialization when the neuron fell into the neuron setn\u2208EVe[Uke,n(t) \u2229(Wk\u2212,en(t) \u2212Uk\u2212,en(t))]. After \u03b1(Ot()i,\u00b7),k \u2264 $\\begin{array}{r}{\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)e\\beta_{O_{(i,\\cdot)},k}^{(t)}]}\\end{array}$ , the neuron would change into the neuron set $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)-$ $\\mathcal{U}_{k,n}^{-e}(t))]$ . As such, the $\\alpha_{O_{(i,\\cdot)},k}^{(t)}$ would again increase at a normal speed, which is even faster than $e\\beta_{O_{(i,\\,.)},k}^{(t)}$ due to the update rules and the fact that $\\|\\pmb{c}_{k}\\|>\\|\\pmb{d}_{k}\\|$ . As such, the neuron set $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)-(\\mathcal{W}_{k,n}^{-e}(t)\\!-\\!\\mathcal{U}_{k,n}^{-e}(t))]$ would again fell back into the neuron set $\\mathbb{E}_{n\\in\\mathcal{V}_{k}^{e}}[\\mathcal{U}_{k,n}^{e}(t)\\cap(\\mathcal{W}_{k,n}^{-e}(t)-\\mathcal{U}_{k,n}^{-e}(t))]$ , where the update speed is again feeble. And it will increase until $\\begin{array}{r}{\\mathbb{E}[(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)e\\beta_{O_{(i,\\cdot)},k}^{(t)}]}\\end{array}$ catch up. ", "page_idx": 50}, {"type": "text", "text": "Besides, we see that the expected attention score will grow up considerably, where we can see that there exist some constant $\\widetilde{c}\\,>\\,1/2,\\,\\widetilde{c}\\,<\\,\\mathbb{E}\\bigl[\\bigl(\\sigma_{S}^{(t)}\\bigr)_{l}^{n}\\bigr]\\,\\leq\\,1$ . As such, ultimately we have $\\mathbb{E}\\big[\\big(\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\mathrm{~-~}$ $\\begin{array}{r}{\\sum_{l\\in S_{n,k}^{-e}}{(\\sigma_{S}^{(t)})_{l}^{n})}\\big]\\,=\\,\\Theta(1)\\quad}\\end{array}$ , $\\alpha_{O_{(i,\\cdot)},k}^{(T^{*})}\\,\\le\\,e\\beta_{O_{(i,\\cdot)},k}^{(T^{*})}$ and $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\,=\\,\\Theta(\\mathbb{E}[e\\beta_{O_{(i,.)},k}^{(t)}])$ . Then fn,okllowing the process in Lemma 37 we can obtain the results. Here we omit this part since the proving procedure is the same to Lemma 37, despite we see $\\begin{array}{r}{\\mathbb{E}[(\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-\\sum_{l\\in S_{n,k}^{-e}}{(\\sigma_{S}^{(t)})_{l}^{n}})]=\\operatorname{\\dot{\\Theta}}(1)}\\end{array}$ and $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]=$ \u0398(E[e\u03b2(Ot()i,\u00b7),k]). ", "page_idx": 50}, {"type": "text", "text": "Again, similar to Lemma 36, we can have asymptotic property when considering the decaying impact of the learning rate, as well as the cross-entropy loss. We directly provide the following two lemmas. Due to the similarity of the proof procedures of Lemma 35 and Lemma 36, we omit the proofs of the following two lemmas as well as the constant details for simplicity. ", "page_idx": 50}, {"type": "text", "text": "Lemma 39. (Asymptotic Property 2). If we consider the impact of the decaying learning rate at the second stage and do not consider the decaying of cross-entropy loss, for some constants $\\overline{{c}},\\overline{{d}},\\underline{{c}},\\underline{{d}}$ regarding $K_{1},\\gamma,\\|\\mathbf{u}\\|,\\|\\mathbf{q}\\|,\\kappa_{x},\\kappa_{y}$ , we will have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\underline{{y}}(t)\\leq\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}\\leq\\overline{{y}}(t),\\quad\\underline{{z}}(t)\\leq\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}\\leq\\overline{{z}}(t),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "for all $t\\geq T_{2}$ . Here, $\\overline{{y}}(t),\\,\\underline{{y}}(t),\\,\\overline{{z}}(t),\\,\\underline{{z}}(t)$ are the unique solutions of the following ODE System respectively ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}(\\mathrm{Ei}(2\\underline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\underline{{y}}(t)^{2})+4\\log(\\underline{{y}}(t)))=\\overline{{a}}\\left(\\mathrm{Li}_{2}\\left(\\frac{\\overline{{d}}+\\overline{{c}}t}{-\\gamma\\overline{{c}}-\\overline{{c}}+\\overline{{d}}}\\right)+\\log(\\overline{{c}}t+\\overline{{d}})\\log\\left(\\frac{\\overline{{c}}(\\gamma+t)}{\\overline{{c}}\\gamma-\\overline{{d}}}\\right)\\right)}\\\\ &{+\\frac{1}{2}(\\mathrm{Ei}(\\log(\\frac{\\sigma_{s}^{2}}{1-\\sigma_{s}^{2}}))+\\mathrm{Ei}(\\log(\\frac{1-\\sigma_{s}^{*}}{\\sigma_{s}^{*}})))+4\\log(\\underline{{\\beta}}\\overline{{c}}_{g k}^{\\prime}),}\\\\ &{\\underline{{z}}(t)={\\underline{{b}}}c^{\\prime}(2\\sigma_{s}^{*}-1)(t-T_{1}),}\\\\ &{=\\underline{{a}}\\left(\\mathrm{Li}_{2}\\left(\\frac{d+c t}{-\\gamma c-c}\\frac{d}{c}+\\underline{{d}}\\right)+\\log(c t+\\underline{{d}})\\log\\left(\\frac{c(\\gamma+t)}{c\\gamma-\\underline{{d}}}\\right)\\right)}\\\\ &{+\\frac{1}{2}(\\mathrm{Ei}(\\frac{\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{4}}{2})+\\mathrm{Ei}(-2\\frac{\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{4}}{2}))+4\\log(\\sigma_{0}/2\\|\\mathbf{u}\\|^{2}),}\\\\ &{\\mathfrak{T}(t)=\\overline{{b}}t+\\frac{\\kappa}{8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname{Li}_{2}(x)=-\\int_{0}^{x}{\\frac{\\ln(1-t)}{t}}d t.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Additionally, we would have asymptotic property that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to+\\infty}y(t)^{2}=\\operatorname*{lim}_{t\\to+\\infty}\\Theta(\\log(\\log^{2}(t))),\\quad\\operatorname*{lim}_{t\\to+\\infty}\\frac{\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{y_{s}}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{y_{S_{n}}}}{(\\sigma_{S}^{(t)})}_{j}^{n}]]}{\\underset{1+\\log^{4}(t)}{\\log^{4}(t)}}=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma 40. (Asymptotic Property 3). If we put our sight on the long period and take the decaying property of the $-\\mathbb{E}[\\ell^{\\prime}(t)]$ into account, for some constants $\\overline{{a}},\\overline{{b}},\\overline{{c}},\\overline{{d}},\\overline{{j}},\\underline{{a}},\\underline{{b}},\\underline{{c}},\\underline{{d}},\\underline{{j}}$ , we will have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\underline{{y}}(t)\\leq\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}\\leq\\overline{{y}}(t),\\quad\\underline{{z}}(t)\\leq\\sum_{i\\in\\mathbb{E}[\\mathcal{W}_{k,n}^{\\pm}(t)]}\\mathbf{r}_{i}\\cdot e\\beta_{O_{(i,\\cdot)},k}^{(t)}\\leq\\overline{{z}}(t),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for all $t\\geq T_{2}$ . Here, $\\overline{{y}}(t),\\,\\underline{{y}}(t),\\,\\overline{{z}}(t),\\,\\underline{{z}}(t)$ are the unique solutions of the following ODE System respectively ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}(\\mathrm{Ei}(2\\underline{{y}}(t)^{2})+\\mathrm{Ei}(-2\\underline{{y}}(t)^{2})+4\\log(\\underline{{y}}(t)))=\\bar{\\alpha}(\\frac{\\mathrm{Li}_{2}(\\frac{\\bar{\\beta}(d+\\bar{\\alpha})}{-\\bar{\\beta}})}{\\frac{\\bar{\\alpha}}{c\\bar{\\beta}}-\\bar{\\alpha}+\\frac{\\bar{\\alpha}\\bar{\\beta}}{c\\bar{\\beta}}}+\\frac{\\log(\\overline{{\\alpha}}+\\overline{{\\beta}})\\log(\\frac{\\bar{\\beta}+\\bar{\\alpha}\\bar{\\alpha}}{\\overline{{b}}+\\bar{\\alpha}-\\bar{d}\\bar{\\beta}})}{\\frac{\\bar{\\alpha}\\bar{\\beta}}{c\\bar{\\beta}}})}\\\\ &{+\\frac{1}{2}(\\mathrm{Ei}(\\log(\\frac{\\sigma_{\\tilde{\\alpha}}^{*}}{1-\\sigma_{\\tilde{\\delta}}^{*}}))+\\mathrm{Ei}(\\log(\\frac{1-\\sigma_{\\tilde{\\alpha}}^{*}}{\\sigma_{\\tilde{\\delta}}^{*}})))+4\\log(\\frac{\\beta}{\\underline{{\\sigma_{\\tilde{\\alpha}}^{*}}}}),}\\\\ &{\\underline{{z}}(t)=\\bar{\\beta}^{\\prime}(2\\sigma_{\\tilde{\\delta}}^{*}-1)(t-T_{1}),}\\\\ &{\\quad\\quad\\underline{{\\mathrm{Li}}}_{2}(\\frac{\\bar{\\beta}(d+\\underline{{\\sigma}})}{\\underline{{b}}-\\bar{d}}+\\underline{{\\sigma}})}\\\\ &{=\\underline{{\\alpha}}(\\frac{1}{c}\\frac{\\bar{\\beta}}{c\\bar{\\beta}}-\\frac{d+\\bar{\\beta}}{c\\bar{\\beta}})+\\frac{\\log(c\\ell+d)\\log(\\frac{\\bar{\\beta}}{\\underline{{b}}}+\\frac{c\\bar{\\beta}\\bar{\\mu}+d}{d})}{\\frac{c\\bar{\\beta}}{c\\underline{{\\beta}}}})}\\\\ &{\\quad+\\frac{1}{2}(\\mathrm{Ei}(\\frac{\\sigma_{\\tilde{\\alpha}}^{2}\\|\\mathbf{u}\\|^{4}}{2})+\\mathrm{Ei}(-2\\frac{\\sigma_{\\tilde{\\alpha}}^{2}\\|\\mathbf{u}\\|^{4}}{2}))+4\\log(\\sigma_{\\tilde{\\alpha}}/2\\|\\mathbf{u}\\|^{2}),}\\\\ &{\\quad\\quad\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname{Li}_{2}(x)=-\\int_{0}^{x}{\\frac{\\ln(1-t)}{t}}d t.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Additionally, we would have asymptotic property that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to+\\infty}y(t)^{2}=\\operatorname*{lim}_{t\\to+\\infty}\\Theta(\\log(\\log^{2}(t))),\\quad\\operatorname*{lim}_{t\\to+\\infty}\\frac{\\mathbb{E}[\\underset{n\\in\\mathcal{V}_{k}^{y_{s}}}{\\mathbb{E}}[\\sum_{j\\in S_{n,k}^{y_{s}}}\\left(\\sigma_{S}^{(t)}\\right)_{j}^{n}]]}{\\underset{1+\\log^{4}(t)}{\\log^{4}(t)}}=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "It\u2019s obvious that the decaying impact of the learning rate and cross-entropy loss are at the similar order. Also, if we consider decaying learning rate, the right side of the inequality would be smaller. $z(t)$ would be in a $\\Theta(\\log(\\log(t)))$ order when $z(t)$ get large, which will make the right side of the $y(t)$ \u2019s formula contain an intergral of $\\Theta(\\log\\log(t))$ , which is obviously slower. ", "page_idx": 51}, {"type": "text", "text": "J Exponential Convergence of 0-1 Loss ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "We continue our proof after Lemma 33. In this section, we assume all the events in the Section $\\mathrm{D}$ hold, denoted as $\\Upsilon_{\\mathrm{Pre}}$ . ", "page_idx": 51}, {"type": "text", "text": "Lemma 41. The Frobenius norm of $\\mathbf{W}_{O}^{y}$ and its gradient can be bounded: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{O}^{y}\\|_{F}^{2}=O(\\frac{K_{1}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m}),\\quad\\|\\nabla_{\\mathbf{W}_{O}^{y}}\\scriptscriptstyle(t)L_{B_{t}}(\\Psi^{(t)})\\|_{F}^{2}=O(\\frac{K_{1}\\|\\mathbf{q}\\|^{2}}{m}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. For $\\forall i\\in[m]$ , by the gradient update rule in Eq.(22), as well as Lemma 4\u2019s insight we see that the lengths of the $\\mathbf{W}_{O}^{y}$ on certain projection direction will continue to grow until being stuck by the regularization, which is a $\\lambda$ -scaled $\\mathbf{W}_{O}^{y}$ itself. Due to the low-noise condition in Condition 1 with a sufficiently large $C$ as well as the isotropy of noise, the learning progress of features would be the main contributor to the $\\mathrm{F}$ norm of NN matrices and the noise, validated in Figure 2 (iii-iv). We can consider an extreme case where all the samples in a single batch belongs to some concept $k\\in[K_{1}]$ , which we can have the upper bound of the first term of the right side of the inequality over the $k$ -th concept\u2019s corresponding projection direction, and thus we can derive an upper bound ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "equation", "text": "$$\n(\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(t)\\frac{q_{k}^{\\pm}}{\\|q_{k}^{\\pm}\\|})^{2}\\leq\\lambda^{-2}\\frac{1}{m^{2}}(\\|c_{k}\\pm d_{k}\\|^{2}+\\frac{3\\sigma_{\\xi}^{2}d_{\\mathcal{Y}}}{2})=\\Theta(\\frac{\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m^{2}}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the first inequality is by $\\begin{array}{r}{(2\\sum_{l\\in S_{n,k}^{e}}{(\\sigma_{S}^{(t)})_{l}^{n}}-1)\\le1}\\end{array}$ , and Lemma 6; the last equality is by the low noise condition $\\sigma_{\\xi}\\leq\\|\\mathbf{q}\\|/C\\sqrt{d y}$ in Condition 1. Then by the low noise condition as well as the data model\u2019s definition we see that all the 2-norm of the $\\mathbf{W}_{O}^{y}$ is controlled by the $K_{1}$ concepts\u2019 corresponding lengths in projection space. Then by the definition of Frobenius norm and Eq.(22) we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{O}^{y}\\|_{F}^{2}\\leq\\Theta(\\frac{K_{1}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m}),\\quad\\|\\nabla_{\\mathbf{W}_{O}^{y}}{}^{(t)}L_{\\mathcal{B}_{t}}(\\Psi^{(t)})\\|_{F}^{2}\\leq\\Theta(\\frac{K_{1}\\|\\mathbf{q}\\|^{2}}{m}).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma 42. Fo $r\\,\\forall\\hat{k}\\in[K_{1}],\\,\\pmb{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}\\mathbf{a}_{\\hat{k}},\\mathbf{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}\\,(t)}\\mathbf{a}_{\\hat{k}}$ and $\\mathbf{\\boldsymbol{b}}_{\\widehat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}\\mathbf{\\boldsymbol{b}}_{\\widehat{k}},\\mathbf{\\boldsymbol{b}}_{\\widehat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}\\,(t)}\\mathbf{\\boldsymbol{b}}_{\\widehat{k}}$ satisfy ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{x\\,(t)}\\pmb{a}_{\\hat{k}},\\pmb{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{x\\,(t)}\\pmb{a}_{\\hat{k}}=O(\\sigma_{0}\\|\\mathbf{u}\\|^{2}),}\\\\ &{\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{x\\,(t)}\\pmb{b}_{\\hat{k}},\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{x\\,(t)}\\pmb{b}_{\\hat{k}}=O(\\|\\mathbf{u}\\|\\sqrt{\\log(\\frac{(L-1)\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Moreover, the Frobenius norm of $\\mathbf{W}_{Q}^{\\alpha\\,(t)}$ and $\\mathbf{W}_{K}^{\\mathbf{x}_{}(t)}$ and its gradient can be bounded as below ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}\\|_{F}^{2},\\|\\mathbf{W}_{K}^{\\mathbf{x}\\,(t)}\\|_{F}^{2}=O(K_{1}\\log(\\frac{(L-1)\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m}))}\\\\ &{\\|\\nabla_{\\mathbf{W}_{Q}^{\\mathbf{x}}(t)}L_{B_{t}}(\\Psi^{(t)})\\|_{F}^{2},\\|\\nabla_{\\mathbf{W}_{K}^{\\mathbf{x}}(t)}L_{B_{t}}(\\Psi^{(t)})\\|_{F}^{2}=O(\\frac{K_{1}(L-1)\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. By Eq.(16) and (17), we see that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|I_{Q,a_{k},\\mathrm{chasol}}^{(t)}|,|I_{Q,a_{k},\\mathrm{cont}}^{(t)}|,|I_{K,a_{k},\\mathrm{chasol}}^{(t)}|,|I_{K,a_{k},\\mathrm{cont}}^{(t)}|\\leq\\eta_{t}\\Theta(\\operatorname*{max}\\{|a_{k}^{\\top}\\mathbf{W}_{Q}^{x}{(t)}a_{k}|,|a_{k}^{\\top}\\mathbf{W}_{K}^{x}{(t)}a_{k}|\\}(\\|\\mathbf{u}\\|)}\\\\ &{\\sigma_{\\xi}\\sqrt{2\\log(\\frac{K N}{\\delta})}+\\frac{1}{K}\\|\\mathbf{u}\\|^{2})^{2}(\\frac{\\|\\mathbf{q}\\|/\\lambda m}{m}))\\leq O(\\lambda\\operatorname*{max}\\{|a_{k}^{\\top}\\mathbf{W}_{Q}^{x}{(t)}a_{k}|,|a_{k}^{\\top}\\mathbf{W}_{K}^{x}{(t)}a_{k}|\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Here, the first inequality is by the scaled identity initialization of ${\\mathbf W}_{Q}^{{\\mathbf x}\\,(0)},{\\mathbf W}_{K}^{{\\mathbf x}\\,(0)}$ , orthogonal relationships of vectors in Lemma 27, Lemma 6, $\\begin{array}{r}{\\sum_{l,j\\in[L]}\\big(\\sigma_{S}^{(t)}\\big)_{l}^{n}\\big(\\sigma_{S}^{(t)}\\big)_{j}^{n}\\leq1/4}\\end{array}$ , Eq.(62) in Lemma 41; the second inequality is by the low noise condition $\\sigma_{\\xi}\\leq\\lambda m/(C\\sqrt{d x}\\|\\mathbf{u}\\|\\|\\mathbf{q}\\|^{1/2})$ and the large $K\\geq C\\|\\mathbf{u}\\|/(\\sigma_{\\xi}{\\sqrt{d_{\\mathcal{X}}}})$ for a large $C$ in Condition 1. Thus the update of $\\pmb{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\pmb{x}\\,(t)}\\pmb{a}_{\\hat{k}}$ and $\\pmb{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\pmb{x}}^{(t)}\\pmb{a}_{\\hat{k}}$ are dominated by their regularization, and thus the scale can not be better than the initialization. By Lemma 7, the conclusion holds. ", "page_idx": 52}, {"type": "text", "text": "On the other hand, we see that by the scaled identity initialization of ${\\mathbf W}_{Q}^{{\\mathbf x}\\,(0)},{\\mathbf W}_{K}^{{\\mathbf x}\\,(0)}$ and orthogonal relationships of vectors in Lemma 27, the initialization of $\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\pmb{x}\\,(t)}\\pmb{b}_{\\hat{k}}$ and $\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\pmb{x}}^{(t)}\\pmb{b}_{\\hat{k}}$ are the same, and as the gradient update is nearly symmetry, which can lead to the fact that $\\Theta(\\dot{\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\pmb{x}}}^{(t)}\\pmb{b}_{\\hat{k}})=\\Theta(\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}}{}^{(t)}\\pmb{b}_{\\hat{k}})$ and $b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{x\\,(t)}\\mathbf{W}_{K}^{x\\,(t)}b_{\\hat{k}}\\ =\\ \\Theta(b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{x\\,(t)}b_{\\hat{k}}b_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{x\\,(t)}b_{\\hat{k}}/\\|b_{\\hat{k}}\\|^{2})$ . By the scaled identity initialization of ${\\mathbf W}_{Q}^{{\\mathbf x}\\,(0)},{\\mathbf W}_{K}^{{\\mathbf x}\\,(0)}$ , orthogonal relationships of vectors in Lemma 27, Eq.(19) and (20) we can see that ", "page_idx": 52}, {"type": "equation", "text": "$$\n|I_{Q,b_{k},\\mathrm{cuan}}^{(t)}|,|I_{K,b_{k},\\mathrm{chass}}^{(t)}|\\leq\\eta_{t}O(\\lambda\\operatorname*{max}\\{|b_{k}^{\\top}\\mathbf{W}_{Q}^{\\alpha^{(t)}}b_{k}|,|b_{k}^{\\top}\\mathbf{W}_{K}^{\\alpha^{(t)}}b_{k}|\\}).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {|I_{Q,b_{k},\\mathrm{coun}}^{(t)}|,|I_{K,b_{k},\\mathrm{coun}}^{(t)}|\\leq\\eta_{t}\\Theta(\\operatorname*{max}\\{|b_{k}^{\\top}\\mathbf{W}_{Q}^{\\alpha^{(t)}}b_{k}|,|b_{k}^{\\top}\\mathbf{W}_{K}^{\\alpha^{(t)}}b_{k}|\\}(\\lambda+\\frac{\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda m}(\\sum_{j\\in S_{n,k}^{+}}(\\sigma_{S}^{(t)})_{j}^{n})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\sum_{j\\in S_{n,k}^{-}}(\\sigma_{S}^{(t)})_{j}^{n}))).}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "We see that $\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\pmb{x}\\,(t)}\\pmb{b}_{\\hat{k}}$ and $\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\pmb{x}}^{(t)}\\pmb{b}_{\\hat{k}}$ will continue to grow up except always being stuck by the regularization. To see the upper bound under this situation, we consider an extreme case where all the samples in a single batch belongs to some concept $k\\in[K_{1}]$ , and there is only one demonstrations in each prompt share the ", "page_idx": 52}, {"type": "text", "text": "semantic with the query. Then by the scaled identity initialization of ${\\mathbf W}_{Q}^{{\\mathbf x}\\,(0)},{\\mathbf W}_{K}^{{\\mathbf x}\\,(0)}$ , orthogonal relationships of vectors in Lemma 27 and Eq.(18), we can see that the growing of $\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\pmb{x}\\,(t)}\\pmb{b}_{\\hat{k}}$ and $\\pmb{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\pmb{x}}^{(t)}\\pmb{b}_{\\hat{k}}$ xK(t)bk\u02c6 would satisfy the following and strive to grow up to make the equality holds, which naturally have an upper bound ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|I_{\\phi_{R}}^{(1)}\\|_{\\operatorname*{min}}^{2}\\operatorname*{min}^{[1]}I_{k,k_{0},\\operatorname*{cus}}^{(1)}|\\,D_{k}^{(1)}\\!\\!\\operatorname*{min}\\{b_{k}^{\\prime}\\!\\operatorname*{min}^{\\alpha}^{(i)}b_{k}\\big|,|b_{k}^{\\prime}\\mathbf{W}_{k}^{\\alpha}{(t)}b_{k}|\\}\\Rightarrow}\\\\ &{\\frac{\\|\\mathbf{{q}}\\|_{\\operatorname*{min}}^{2}\\|\\mathbf{q}\\|_{\\operatorname*{min}}^{2}\\big(\\sum_{\\phi_{r}^{(1)},b_{r}}(\\sigma_{\\theta}^{(1)})\\big)}{\\lambda\\sigma_{\\theta}^{\\downarrow}\\kappa_{r}}\\big(\\sigma_{\\theta}^{(1)}\\big)\\geq\\Theta(\\lambda)\\Rightarrow}\\\\ &{\\Leftrightarrow\\!\\frac{\\big(|\\mathbf{q}|_{\\operatorname*{min}}^{2}\\big)}{\\lambda^{2m}}\\frac{\\int_{\\mathbb{S}^{\\star}\\mathrm{S}_{\\phi}}\\epsilon_{r}^{\\mathrm{Bin}}\\nabla_{\\phi}^{(1)}(\\mathbf{u}_{\\mathrm{X}}^{\\prime}(\\cdot)|b_{k}^{\\prime})}{\\big(\\sum_{\\phi\\in\\mathcal{S}_{k,k}}\\epsilon_{\\theta}^{\\mathrm{Bin}}\\nabla_{\\phi}^{(1)}(\\mathbf{u}_{\\mathrm{X}}^{\\prime}(\\cdot)|\\mathbf{b}_{k}^{\\prime})+\\sum_{\\phi\\in\\mathcal{S}_{k,k}}e^{-b_{\\xi}^{\\prime}\\mathbf{W}_{\\phi}^{(1)}(\\mathbf{u}_{\\mathrm{X}}^{\\prime}(\\cdot)|b_{k}^{\\prime})}\\big)\\geq1\\Rightarrow}\\\\ &{\\Leftrightarrow\\!\\frac{\\|\\mathbf{q}\\|_{\\operatorname*{min}}^{2}\\big\\|\\mathbf{q}\\big\\|^{2}}{\\lambda^{2m}}\\frac{\\sum_{j\\in\\mathcal{S}_{k,k}^{\\prime}}e^{\\mathrm{Bin}\\mathbf{\\binom{N}{\\phi}}(\\mathbf{u}_{\\mathrm{X}}^{\\prime}(\\cdot)|\\mathbf{b}_{k}^{\\prime})}+\\sum_{j\\in\\mathcal{S}_{k,k}^{\\prime}}e^{-b_{\\xi}^{\\prime}\\mathbf{W}_{\\phi}^{(1)}(\\mathbf{u}_{\\mathrm{W}} \n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Here, the second arrow is by the definition of $\\sum_{j\\in S_{n,k}^{+}}\\left(\\sigma_{S}^{(t)}\\right)_{j}^{n}$ and Eq.(31); the third arrow is by our considered extreme case where there is only one demonstration in each of the prompt sample in this all-the-same-concept batch, which is considered for obtaining the upper bound; the forth arrow is by the small $\\lambda$ by Condition 1, which denotes $e^{-2b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}}\\left(t\\right)\\mathbf{W}_{K}^{\\mathbf{x}}\\left(t\\right)\\mathbf{\\Phi}_{b_{\\hat{k}}}}$ should be the key contributor. ", "page_idx": 53}, {"type": "text", "text": "Similar to the claims in Lemma 41, here we see that as the $\\lambda$ is very small, by the scaled identity initialization of ${\\mathbf W}_{Q}^{{\\mathbf x}\\,(0)},{\\mathbf W}_{K}^{{\\mathbf x}\\,(0)}$ , orthogonal relationships of vectors in Lemma 27, as well as the low-noise condition by Condition 1, it\u2019s safe to say that as the learning proceed, the scales of $\\mathbf{\\boldsymbol{b}}_{\\widehat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}\\mathbf{\\boldsymbol{b}}_{\\widehat{k}},\\mathbf{\\boldsymbol{b}}_{\\widehat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}\\,(t)}\\mathbf{\\boldsymbol{b}}_{\\widehat{k}}$ would completely dominate $\\mathbf{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}\\mathbf{a}_{\\hat{k}},\\mathbf{a}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}\\,(t)}\\mathbf{a}_{\\hat{k}}$ , as well as $\\mathbf{a}_{k}^{\\top}\\mathbf{W}_{X}^{x}^{(t)}\\mathbf{b}_{\\hat{k}},\\mathbf{b}_{k}^{\\top}\\mathbf{W}_{X}^{x}^{(t)}\\mathbf{a}_{\\hat{k}},\\nu_{r}^{\\top}\\mathbf{W}_{X}^{x}^{(t)}\\nu_{r},\\mathbf{u}_{w}^{\\perp^{\\top}}\\mathbf{W}_{X}^{x}^{(t)}\\mathbf{u}_{w}^{\\perp}$ , $\\forall X\\in\\{Q,K\\},r\\in[K_{2}],w\\in[d_{X}-2K_{1}-K_{2}].$ . Collaborating with Lemma 9, we have \u2225 $\\mathbf{W}_{Q}^{\\mathbf{x}(t)}\\|_{F}^{2},\\|\\mathbf{W}_{K}^{\\mathbf{x}(t)}\\|_{F}^{2}\\leq\\frac{K_{1}}{\\|\\mathbf{u}\\|^{2}}\\operatorname*{max}\\{\\big(b_{k}^{\\top}\\mathbf{W}_{Q}^{\\mathbf{x}(t)}b_{k}\\big)^{2},\\big(b_{k}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}(t)}b_{k}\\big)^{2}\\}=O(K_{1}\\log\\big(\\frac{(L-1)\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m}\\big)).$ On the other hand, we see that the maximum gradient $\\mathrm{F}$ norm on a single batch comes from the maximum changes of the $b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\alpha\\,(t)}{b_{\\hat{k}}}^{2}$ (or $\\boldsymbol{b}_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}}{^{(t)}\\bar{\\boldsymbol{b}_{\\hat{k}}}}^{\\bar{2}})$ . As we see that the extreme case of the growing is every concept $k\\ \\in\\ [K_{1}]$ has been fully learned such that even a batch full of the same concept can not let the corresponding concept\u2019s feature grow. In this case, we see that the maximum gradient $\\mathrm{F}$ norm should be at the order of $\\|\\lambda\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}\\|_{F}$ (or $\\|\\lambda\\mathbf{W}_{K}^{\\b{x}}(t)\\|_{F})$ . Thus ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\nabla_{\\mathbf{W}_{Q}^{\\alpha}(t)}L_{B_{t}}(\\Psi^{(t)})\\|_{F}^{2},\\|\\nabla_{\\mathbf{W}_{K}^{\\alpha}(t)}L_{B_{t}}(\\Psi^{(t)})\\|_{F}^{2}=O(\\lambda^{2}K_{1}\\log(\\frac{(L-1)\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{\\lambda^{2}m}))}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=O(\\frac{K_{1}(L-1)\\|\\mathbf{u}\\|^{2}\\|\\mathbf{q}\\|^{2}}{m}),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the equality is by $g(x)=\\log(x)\\leq O(x),x>1$ . The proof is completed. ", "page_idx": 53}, {"type": "text", "text": "Remark 5. Worth noting that this upper bound, as well as the upper bound of $\\lVert\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(t)\\mathbf{q}_{k}^{\\pm}/\\lVert\\mathbf{q}_{k}^{\\pm}\\rVert\\rVert$ in Lemma 41, are looser in the order of $K_{1}{}^{-2}$ and $K_{1}{}^{-1}$ compared to those of $\\beta_{Q,k}^{(t)}=\\beta_{K,k}^{(t)}$ $e\\beta_{O_{(i,\\,*)},k}^{(t)}$   \nThis suits the intuition and statistics since in the practical training setting, for $B\\geq1$ we can see that sometimes the samples of a batch all belong to one concept, or sometimes their are not any particular concept in a single batch, especially when $B$ is small. Therefore, unless we have the situation where even when every prompt sample of a batch belong to the same concept the regularization can stuck the growing, there is still chance for that concept\u2019s features to be learned. In contrast, the expectation considers every concept\u2019s sample appear in every batch scaled by $a$ \u201csoft weight\u201d in the order of $\\Theta(1/K)$ . As the attention \u2019s gradient contain MLP, its order would be $\\Theta(1/K^{2})$ . Besides, we see that this lemma\u2019s result contains the scale of $L-1$ , which comes from the extreme case discussion where there is only one demonstration in each prompt sample that share the semantic to those of query. In contrast, when considering the expectation, the number of two opposite semantics is the same, under which the $L/2$ would be eliminated in the numerator and denominator. Last but not least, when estimating the real cases, we have scaled the derivative of $-\\ell^{\\prime}$ to its maximum 1, we do so because in real cases due to the imbalanced prompt samples in a single batch, it would be inconvenient to consider it is contributed by $e\\beta_{O_{(i,\\,.\\,)},k}^{(t)},\\alpha_{O_{(i,\\,.\\,)},k}^{(t)}$   \nexpectations. ", "page_idx": 53}, {"type": "text", "text": "Lemma 43. (Restatement of Proposition 2) $\\forall t\\ \\geq\\ {\\hat{T}},$ , when $\\|\\Psi^{\\prime}{}^{(t)}\\,-\\,\\mathbb{E}(\\Psi^{\\prime}{}^{(t)})\\|_{F}\\;\\leq\\;\\nu$ holds, we have $L_{\\mathcal{D}^{*}}^{0-1}(\\Psi^{\\prime}{}^{(t)})=L_{\\mathcal{D}^{*}}^{0-1}(\\mathbb{E}(\\Psi^{\\prime}{}^{(t)}))$ . Here, $\\|\\b{\\Psi}^{\\prime}\\|_{F}^{2}:=\\|\\mathbf{W}_{Q}^{\\pmb{x}}\\|_{F}^{2}+\\|\\mathbf{W}_{K}^{\\pmb{x}}\\|_{F}^{2}+\\|\\mathbf{W}_{O}^{\\pmb{y}}\\|_{F}^{2}$ . ", "page_idx": 54}, {"type": "text", "text": "Proof. By Lemma 33, we see that our convergence of 0-1 loss is based on the intermediate result that $\\mathbb{E}[\\mathbf{A}_{t}^{k,e}]\\ \\geq\\ \\kappa$ , which will ensure that $\\mathbb{E}[y_{S_{n}}\\,\\cdot\\,f(\\mathbf{E}(S_{n}),\\mathbb{E}(\\Psi^{\\hat{T}}))]\\ \\geq\\ \\kappa/2$ . Therefore, when conditioned on $\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}],\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}{}^{(t)}]$ , a minimum admissible disparity between $\\mathbf{W}_{O}^{y\\,(t)}$ and $\\mathbb{E}[\\mathbf{W}_{O}^{\\pmb{y}\\,(t)}]$ corresponds the the minimum admissible disparity between $\\mathbf{W}_{O_{(i,\\cdot)}}^{y}(t)\\mathbf{\\Phi}_{{c_{k}},\\mathbf{\\Phi}}^{(y},\\mathbf{W}_{O_{(i,\\cdot)}}^{y})^{\\left(\\ t\\right)}d_{k}$ $\\alpha_{O_{(i,\\cdot)},k}^{(t)},\\beta_{O_{(i,\\cdot)},k}^{(t)}$ consequently cause $\\mathbb{E}_{\\nu_{k}^{e}}[\\mathbf{A}_{t}^{k,e}]\\ \\le\\ \\kappa/2$ that could have potential to deteriorate the 0-1 loss. Given that $\\kappa/2\\,\\geq\\,\\sqrt{2}\\sigma_{1}\\|{\\bf q}\\|$ by Lemma 23, the decomposition in Eq.(33) as well as Lemma 9, we see that for some $k\\in[K_{1}]$ , the minimum admissible disparity can be written as ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Theta(\\|(\\sqrt{2}\\sigma_{1}\\|\\mathbf{q}\\|)^{2}(\\frac{c_{k}^{\\top}}{\\|c_{k}\\|^{2}})^{\\top}\\frac{c_{k}^{\\top}}{\\|c_{k}\\|^{2}}\\|_{F})=\\Theta(\\sqrt{2}\\sigma_{1}\\|(\\|\\mathbf{q}\\|)^{2}(\\frac{c_{k}^{\\top}}{\\|c_{k}\\|^{2}})^{\\top}\\frac{c_{k}^{\\top}}{\\|c_{k}\\|^{2}}\\|_{F})\\geq\\Theta(2\\sqrt{2}/(1+\\kappa_{y})\\sigma_{1}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Therefore, we see that when conditioned on $\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\,(t)}],\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}}{}^{(t)}]$ , the minimum admissible disparity between $\\mathbf{W}_{O}^{y\\,(t)}$ and $\\mathbb{E}[\\mathbf{W}_{O}^{\\pmb{y}\\,(t)}]$ to not worsen the 0-1 loss is $\\Theta(2\\sqrt{2}/(1+\\kappa_{\\pmb{y}})\\sigma_{1})$ . ", "page_idx": 54}, {"type": "text", "text": "On the other hand, when conditioned on $\\mathbb{E}[\\mathbf{W}_{O}^{\\pmb{y}\\,(t)}],t\\,\\geq\\,T^{\\prime}$ , we compute the minimum admissible disparity between $\\mathbf{W}_{Q}^{\\mathbf{x}\\,(T^{\\prime})}$ , $\\mathbf{W}_{K}^{\\mathbf{x}}(T^{\\prime})$ and $\\mathbb{E}[\\mathbf{W}_{Q}^{\\mathbf{x}\\,(T^{\\prime})}]\\,=\\,\\mathbb{E}[\\mathbf{W}_{K}^{\\mathbf{x}\\,\\,\\hat{T}}]$ . Considering all the activated neurons, when $\\begin{array}{r}{\\sum_{i\\in\\mathcal{W}_{k,n}^{e}(t)}\\mathbb{E}\\big[\\mathbf{r}_{i}\\big(2\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(\\hat{T})}\\big)_{l}^{n}\\,-1\\big)\\beta_{O_{(i,\\cdot)},k}^{(\\hat{T})}\\big]\\,=\\,0.}\\end{array}$ , we should have $\\begin{array}{r}{\\sum_{i\\in\\mathcal{W}_{k,n}^{e}(t)}\\mathbb{E}\\bigl[\\mathbf{r}_{i}\\alpha_{O_{(i,\\cdot)},k}^{(\\hat{T})}\\bigr]\\,\\geq\\,0}\\end{array}$ otherwise some of the neurons must be deactivated, which is contradicted by the definitions of $\\mathcal{W}_{k,n}^{e}(t)$ . In this case we can magnify the impact of $\\begin{array}{r}{\\sum_{i\\in\\mathcal{W}_{k,n}^{e}(t)}\\mathbb{E}[\\mathbf{r}_{i}\\big(2\\sum_{l\\in S_{n,k}^{e}}\\big(\\sigma_{S}^{(\\hat{T})}\\big)_{l}^{n}\\,-1\\big)\\beta_{O_{(i,\\cdot)},k}^{(\\hat{T})}]}\\end{array}$ by considering $\\begin{array}{r}{\\sum_{i\\in\\mathcal{W}_{k,n}^{e}(t)}\\mathbb{E}\\bigl[\\mathbf{r}_{i}\\alpha_{O_{(i,\\cdot)},k}^{(\\hat{T})}\\bigr]\\,=\\,0}\\end{array}$ . As such, the minimum admissible disparity would be the case where $b_{\\hat{k}}^{\\top}\\mathbf{W}_{Q}^{\\alpha\\,(\\hat{T})}\\pmb{b}_{k}$ and $b_{\\hat{k}}^{\\top}\\mathbf{W}_{K}^{\\mathbf{x}}{}^{(\\hat{T})}\\pmb{b}_{k}$ both differ from $\\beta_{Q,k}^{(\\hat{T})}\\,=\\,\\beta_{K,k}^{(\\hat{T})}$ by the amount of $\\beta_{Q K}^{-}$ . Recall the definition of $\\beta_{Q K}^{-}$ in Lemma 34, and collaborating with Lemma 9, we have the minimum admissible disparity be ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\mathop{or}}(1-\\kappa_{x})e^{-\\log(5K m/\\delta)\\frac{\\sigma_{1}^{2}\\|{\\bf u}\\|^{4}(1+{e}^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}})}{(1-{e}^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}})}}.\\mathrm{\\mathop{Recall}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\nu:=\\operatorname*{min}\\{2\\sqrt{2}\\sigma_{1}/1\\}^{4}(1-{e}^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\nu:=\\operatorname*{min}\\{2\\sqrt{2}\\sigma_{1}/(1+\\kappa_{y}),\\sigma_{0}(1-\\kappa_{x})e^{-\\log(5K m/\\delta)\\frac{\\sigma_{1}^{2}\\|{\\bf u}\\|^{4}(1+{e}^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}})}{(1-{e}^{-\\sigma_{0}^{2}\\|{\\bf u}\\|^{2}})}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "the proof is completed. ", "page_idx": 54}, {"type": "text", "text": "Lemma 44. For $t\\in\\{1,\\cdots\\,,T\\}_{\\!}$ , for $\\mathbf{W}\\in\\{\\mathbf{W}_{Q}^{\\mathbf{x}},\\mathbf{W}_{K}^{\\mathbf{x}},\\mathbf{W}_{O}^{\\mathbf{y}}\\}$ and $X\\in\\{Q,K,O\\}$ it follows that ", "page_idx": 54}, {"type": "equation", "text": "$$\nI.\\ \\|\\mathbf{W}^{(t+1)}-\\mathbf{W}_{t}^{(t+1)}\\|_{F}\\leq\\Theta(\\frac{K_{1}^{1/2}\\|\\mathbf{q}\\|((L-1)^{1/2}\\|\\mathbf{u}\\|+1)}{m^{1/2}}\\eta_{t}),\n$$", "text_format": "latex", "page_idx": 54}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}^{(s+1)}-\\mathbf{W}_{t}^{(s+1)}\\|_{F}\\leq(1-\\eta_{s}\\lambda)\\|\\mathbf{W}^{(s)}-\\mathbf{W}_{t}^{(s)}\\|_{F},\\forall s\\geq t+1,\n$$", "text_format": "latex", "page_idx": 54}, {"type": "equation", "text": "$$\n:\\;\\sum_{t=0}^{T}\\|D_{X}^{t}\\|_{\\infty}^{2}\\leq\\Theta(\\frac{K_{1}\\|\\mathbf{q}\\|^{2}((L-1)\\|\\mathbf{u}\\|^{2}+1)}{m\\lambda^{2}(\\gamma+T)}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. We provide the proof by extending the techniques in [34, 33, 36] to Hilbert-Schmidt space, whose inner product is defined by trace. First we note that $\\begin{array}{r}{\\eta_{0}=\\hat{\\frac{2}{\\gamma+1}}\\leq\\operatorname*{min}\\{1/(L_{\\mathrm{Logist}}+\\lambda),1/2\\lambda\\}}\\end{array}$ , where $L_{\\mathrm{Logist}}$ is the $L$ -smooth Lipschitz constant of cross-entropy loss $\\ell(\\cdot)$ , which is 1. The first statement can be shown as follows. Since by definition we see that $\\mathbf{W}^{(t)}=\\mathbf{W}_{t}^{\\mathbf{\\Lambda}_{(t)}}$ , we only need to check the maximum disparity of the gradient in a single iteration update, then by Lemma 41 and Lemma 42 we readily obtain the results. ", "page_idx": 54}, {"type": "text", "text": "For the second statement, following the proof in [93, 34], we see that the Lipschitz smoothness of cross-entropy loss denotes that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\langle\\nabla_{\\mathbf{W}}L_{B}(\\Psi)-\\nabla_{\\mathbf{W}^{\\prime}}L_{B}(\\Psi),\\mathbf{W}-\\mathbf{W}^{\\prime}\\rangle\\geq\\frac{1}{L_{\\mathrm{Logist}}}\\Vert\\nabla_{\\mathbf{W}}L_{B}(\\Psi)-\\nabla_{\\mathbf{W}^{\\prime}}L_{B}(\\Psi)\\Vert_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Then we have that for $s\\geq t+1$ , ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{W}^{(s+1)}-\\mathbf{W}_{t}^{(s+1)}\\|_{F}^{2}=\\bigg\\|\\big(1-\\eta_{s}\\lambda\\big)\\left(\\mathbf{W}^{(s)}-\\mathbf{W}_{t}^{(s)}\\right)\\bigg\\|_{F}^{2}-\\eta_{s}\\left(\\partial_{g}l\\left(g_{s},Z_{s}\\right)-\\partial_{g}l\\left(g_{s}^{t},Z_{s}\\right)\\right)\\bigg\\|_{F}^{2}}&{}\\\\ {=(1-\\eta_{s}\\lambda)^{2}\\bigg\\|\\mathbf{W}^{(s)}-\\mathbf{W}_{t}^{(s)}\\bigg\\|_{F}^{2}-2\\eta_{s}\\left(1-\\eta_{s}\\lambda\\right)\\cdot}&{}\\\\ {\\bigg\\langle\\nabla_{\\mathbf{W}_{t}^{(s)}}L_{\\mathbf{W}_{s}}(\\Psi^{*})-\\nabla_{\\mathbf{W}_{t}^{(s)}}L_{B_{s}}(\\Psi^{*}),\\mathbf{W}^{(s)}-\\mathbf{W}_{t}^{(s)}\\bigg\\rangle}&{}\\\\ {+\\eta_{s}^{2}\\|\\nabla_{\\mathbf{W}(s)}L_{B_{s}}(\\Psi^{*})-\\nabla_{\\mathbf{W}_{t}^{(s)}}L_{B_{s}}(\\Psi^{*})\\|_{F}^{2}}&{}\\\\ {\\leq(1-\\eta_{s}\\lambda)^{2}\\left\\|\\mathbf{W}^{(s)}-\\mathbf{W}_{t}^{(s)}\\right\\|_{F}^{2}-\\eta_{s}\\left(\\frac{1}{L_{\\mathbf{W}\\mathbb{H}\\times\\mathbf{0}}}-\\eta_{s}\\right).}&{}\\\\ {\\bigg\\|\\nabla_{\\mathbf{W}^{(s)}}L_{B_{s}}(\\Psi^{*})-\\nabla_{\\mathbf{W}_{t}^{(s)}}L_{B_{s}}(\\Psi^{*})\\bigg\\|_{F}^{2}}&{}\\\\ {\\leq(1-\\eta_{s}\\lambda)^{2}\\left\\|\\mathbf{W}^{(s)}-\\mathbf{W}_{t}^{(s)}\\right\\|_{F}^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where we utilize the Eq. (63) and conditions on learning rates. Utilizing this statement, the stable property of stochastic gradient descent has been shown. Again following the techniques in [34, 33, 36], we now obtain the bound: for $t\\in\\{1,\\ldots,T\\}$ , ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{W}^{(T+1)}-\\mathbf{W}_{t}^{(T+1)}\\right\\|_{F}\\leq\\Theta(\\frac{K_{1}^{1/2}\\|\\mathbf{q}\\|\\bigl((L-1)^{1/2}\\|\\mathbf{u}\\|+1\\bigr)}{m^{1/2}}\\eta_{t})\\prod_{s=t+1}^{T}\\left(1-\\eta_{s}\\lambda\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "From the following inequality, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\prod_{s=t+1}^{T}\\left(1-\\eta_{s}\\lambda\\right)=\\prod_{s=t+1}^{T}\\frac{\\gamma+s-2}{\\gamma+s}<\\frac{\\gamma+t}{\\gamma+T}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last inequality hold clearly by expanding the product, the right hand side of the Eq.(64) is upper bounded as follows ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\Theta(\\frac{K_{1}^{\\frac{1}{2}}\\|\\mathbf{q}\\|((L-1)^{\\frac{1}{2}}\\|\\mathbf{u}\\|+1)}{m^{\\frac{1}{2}}})\\eta_{t}\\prod_{s=t+1}^{T}(1-\\eta_{s}\\lambda)\\le\\Theta(\\frac{K_{1}^{\\frac{1}{2}}\\|\\mathbf{q}\\|((L-1)^{\\frac{1}{2}}\\|\\mathbf{u}\\|+1)}{m^{\\frac{1}{2}}})\\frac{\\eta_{t}(\\gamma+t)}{\\gamma+T}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We finally obtain the desired bound: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\|D_{t}\\|_{\\infty}^{2}\\leq\\sum_{t=0}^{T}\\Theta(\\frac{K_{1}\\|\\mathbf{q}\\|^{2}((L-1)\\|\\mathbf{u}\\|^{2}+1)}{m\\lambda^{2}(\\gamma+T)^{2}})\\leq\\Theta(\\frac{K_{1}\\|\\mathbf{q}\\|^{2}((L-1)\\|\\mathbf{u}\\|^{2}+1)}{m\\lambda^{2}(\\gamma+T)}).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "K Out-of-Distribution Generalization ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Lemma 45. OOD 1: Master of Polysemy of Words. During testing, The prompt length $L^{*}$ can be any positive integer. The $\\mathcal{D}_{z}^{*}$ can have any new probability distribution that differs from the training distribution, satisfying that each prompt has at least one co-concept $k\\ \\in\\ [K_{1}]$ , with equal chance to have positive or negative semantic labels. Additionally, a single $(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}_{\\pmb{x}}^{*}\\times\\mathcal{D}\\pmb{y}^{*}$ pair can appear in at least $\\Vert{z}\\Vert_{0}$ conceptspecific prompts/tasks. Importantly, all of the tasks in this new distribution $\\mathcal{D}^{*}$ enjoy Bayes-Optimal test error $L_{\\mathcal{D}^{\\ast}}^{0-1}(\\Psi^{(T^{\\ast})})\\leq\\varepsilon$ . ", "page_idx": 55}, {"type": "text", "text": "This lemma demonstrate the strong OOD Generalization ability of transformer utilizing multi-concept semantics, suggesting the efficiency transformer to conduct unseen ICL tasks just by its learned knowledge on the two non-orthogonal dictionaries. Also, this lemma showcases an intriguing phenomenon since it allows multiple concepts with comparable chance along word-demo pairs - even with the same input-output pair and query, the model can produce diverse responses when provided varying contextual (concept / task) information. For instance, with the prompt \u201cJapan: Sakura; China:\u201d, the LLM may output \u201cPenoey\u201d (national flower) or \u201cPanda\" (national symbol), reflecting different conceptual (task) interpretations. Both answers are right since they are all the co-concept tasks. Interestingly, adding another demonstration like \u201cJapan: Sakura, France: Iris germanica, China:\u201d stabilizes the response to \u201cPenoey\u201d, since the only co-concept is left to be \u201cnational flower\u201d. In our theory, we make an elementary explanation to this flexible, context-sensitive in-context learning (ICL) behavior by attributing it to the transformer\u2019s ability to harness multi-concept semantics. ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "Lemma 46. OOD 2: Innovation. During testing, the distribution of $\\mathcal{D}_{\\mathbf{x}}^{*}\\times\\mathcal{D}_{\\mathbf{y}}^{*}$ can enjoy data shift. Specifically, suggest we now have a new ${\\bf M}^{*}$ and $\\mathbf{Q}^{*}$ to define new $\\mathcal{D}_{\\mathbf{x}}^{*},\\mathcal{D}_{\\mathbf{y}}^{*}$ . Specifically, $\\breve{\\forall k}\\neq k^{\\prime}\\in[K_{1}],k_{2}\\in[K_{2}],$ , we let ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{2k-1}^{*}={\\pmb{\\mu}}_{k}^{+}{}^{*}={\\pmb{a}}_{k}^{*}+{\\pmb{b}}_{k}^{*},\\quad M_{2k}^{*}={\\pmb{\\mu}}_{k}^{-}{}^{*}={\\pmb{a}}_{k}^{*}-{\\pmb{b}}_{k}^{*},}\\\\ &{Q_{2k-1}^{*}={\\pmb{q}}_{k}^{+}{}^{*}={\\pmb{c}}_{k}^{*}+{\\pmb{d}}_{k}^{*},\\quad Q_{2k}^{*}={\\pmb{q}}_{k}^{-}{}^{*}={\\pmb{c}}_{k}^{*}-{\\pmb{d}}_{k}^{*},}\\\\ &{M_{k_{2}+2K_{1}}^{*}={\\pmb{\\nu}}_{k_{2}}^{*},\\quad Q_{k_{2}+2K_{1}}^{*}={\\bf{0}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{a}_{k}^{*}\\in c o n i c(\\{\\frac{\\mu_{k}^{+}+\\mu_{k}^{-}}{2}\\}_{k=1}^{K_{1}}),\\ \\ \\ \\pmb{b}_{k}^{*}\\in c o n i c(\\{\\frac{\\mu_{k}^{+}-\\mu_{k}^{-}}{2}\\}_{k=1}^{K_{1}}),}\\\\ &{\\pmb{c}_{k}^{*}\\in c o n i c(\\{\\frac{\\pmb{q}_{k}^{+}+\\pmb{q}_{k}^{-}}{2}\\}_{k=1}^{K_{1}}),\\ \\ \\ \\pmb{d}_{k}^{*}\\in c o n i c(\\{\\frac{\\pmb{q}_{k}^{+}-\\pmb{q}_{k}^{-}}{2}\\}_{k=1}^{K_{1}}),}\\\\ &{{\\nu_{k_{2}}}^{*}\\in(s p a n(\\mu_{1}^{+},\\mu_{1}^{-},\\mu_{2}^{+},\\mu_{2}^{-},\\cdot\\cdot\\cdot\\ ,\\mu_{K_{1}}^{+},\\mu_{K_{1}}^{-}))^{\\bot},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "satisfying ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\|b_{k}^{*}\\|\\geq\\|a_{k}^{*}\\|=\\Theta(\\|\\mathbf{u}\\|),\\quad\\|d_{k}^{*}\\|\\geq\\|c_{k}^{*}\\|=\\Theta(\\|\\mathbf{q}\\|),\\quad\\nu_{k_{2}}^{*}=\\Theta(\\|\\mathbf{u}\\|),\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and $\\{\\pmb{a}_{k}^{*},\\pmb{b}_{k}^{*}\\}_{k=1}^{K_{1}},\\{\\pmb{c}_{k}^{*},\\pmb{d}_{k}^{*}\\}_{k=1}^{K_{1}}$ are two collections of pair wise orthogonal vectors. Then we can have $a$ corresponding new prompt distribution $\\begin{array}{r}{\\mathcal{D}_{S}^{*}=\\sum_{k=1}^{K_{1}}\\left({\\pi_{k}^{+}}^{*}\\mathcal{P}_{k,L^{*}+1}^{+}{*}^{}+{\\pi_{k}^{-}}^{*}\\mathcal{P}_{k,L^{*}+1}^{-}{*}^{}\\right)}\\end{array}$ . Again, the model enjoys Bayes-Optimal test error $L_{\\mathcal{D}^{*}}^{0-1}(\\Psi^{(T^{*})})\\leq\\varepsilon$ . ", "page_idx": 56}, {"type": "text", "text": "This lemma suggest that transformer-mlp structure empower ICL ability in solving task involving semantics (\u201cknowledge\u201d) originally from other co-concept prompt\u2019s training distribution. This cross-concept semantic \u201cunderstanding\u201d ability ensure the transformer perform an specific OOD ability. ", "page_idx": 56}, {"type": "text", "text": "For example, when we show a prompt \u201cIsaac Newton:Today I designed a machine to capture sunlight; Thomas Edison:\u201d to GPT o1, we would obtain an answer \u201cToday I invented a lamp that shines without fire.\u201d During training, even when the concept \u201cInventors and Their Inventions\u201d may not co-appear with the concept \u201cFabricate a story\u201d with high chance, the transformers empower the ICL to perform this interesting Out-of-Distribution task. We believe this can serve as an attempt to explain the innovation power of LLM [30, 94, 95] grounded in the linear geometric property of LLM representation, since most of the innovative outcomes of human being generates from cross-concept \u201cKnowledge Intersection\u201d, and as it is not an easy task for human specialist to master cross-domain knowledge, we claim that LLM can help innovation by leveraging cross-domain knowledge when deduction over unseen structured task. Similarly, for multi-model scenarios, [86] have shown that compositing different concepts did enable OOD generalization (e.g. \u201cblue square apples\u201d in the Figure 1a in [86]). ", "page_idx": 56}, {"type": "text", "text": "This lemma seeks to elementarily explain why LLMs\u2019 ICL can excel in complex tasks when using evolutionary strategies, especially when the LLM\u2019s latent representation based on language only partially captures the relevant features. Such tasks include algorithm design [96, 4], heuristics [3], acquisition functions [97], and solutions to combinatorial optimization problems [98]. Although the resulting solutions may often seem counterintuitive to human experts, a possible explanation is that transformers can perform ICL in OOD scenarios by leveraging weighted combinations of their updated \u201cunderstanding\u201d (i.e., changing the identified underlying concepts in the evolution process) of new demo-query pairs, such as randomly sampled TSP instances. These understandings are rooted in the latent structures of the problem instances and can be effectively updated by evolutionary strategies that selectively refine and discard certain outcomes. ", "page_idx": 56}, {"type": "text", "text": "Proof. Proof of Proposition 1. By Proposition 2, we only need to check the expected 0-1 loss $L_{\\mathcal{D}^{\\ast}}^{0-1}(\\mathbb{E}[\\Psi^{\\prime}])=0$ . Denote $\\mathbb{E}[\\mathcal{M}_{y_{S_{n}}}]\\subseteq[2K_{1}]$ as the expected index set denoting the expected shared concept-specific features by the query and one demonstration. By definition in the Lemma, as the semantic combination is conic combination, we see that $\\mathbb{E}[\\mathcal{M}_{y_{S_{n}}}]$ will be either a collection of odd (corresponding to positive label) or even (corresponding to negative label) numbers, and all of the combination of the features and labels in one prompt are corresponding to the same real value label without \u201cself-conflict\u201d. By Lemma 38, we see that the coefficients are all at a substantial scale at $T^{*}$ . Then by the condition on $_{\\mathscr{L}}$ and Eq. (31), we can readily check that even when the probability of the fraction of demonstrations sharing the co-concept label semantic with query is feeble (but at least one), utilizing the same set of notations, we still have ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{n\\in\\mathbb{P}^{*}}\\big[\\displaystyle\\sum_{l\\in\\mathbb{E}[s_{n,k}^{y,s_{n}}]}\\big(\\sigma_{s}^{(T^{*})}\\big)_{l}^{n}\\big]}\\\\ &{\\geq\\Theta(\\frac{L^{*}/2e^{\\sum_{k\\in\\mathbb{E}[M_{y_{n}}}\\big_{n}\\big_{k}\\big_{n}}\\frac{\\beta_{Q,k}^{(T^{*})}\\beta_{K,k}^{(T^{*})}}{\\|k_{k}\\|^{2}}}{L^{*}/2(e^{\\sum_{k\\in\\mathbb{E}[M_{y_{n}}}\\big_{n}]\\frac{\\beta_{Q,k}^{(T^{*})}\\beta_{K,k}^{(T^{*})}}{\\|k_{k}\\|^{2}}}+e^{(K-1)\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{2}-\\sum_{k\\in\\mathbb{E}[M_{y_{n}}]}\\frac{\\beta_{Q,k}^{(T^{*})}\\beta_{K,k}^{(T^{*})}}{\\|k_{k}\\|^{2}}})}\\big)}\\\\ &{\\geq\\Theta(\\frac{\\frac{\\|\\mathbf{u}\\|^{2}}{\\frac{\\|\\mathbf{u}\\|^{2}}{\\|\\mathbf{u}\\|^{2}}\\log(\\frac{\\|\\mathbf{q}\\|^{2}}{m\\lambda\\kappa_{1}})}}{\\frac{\\|\\mathbf{u}\\|^{2}}{\\lambda\\|}\\log(\\frac{\\|\\mathbf{q}\\|^{2}}{m\\lambda\\kappa_{1}})+e^{(K-1)\\sigma_{0}^{2}\\|\\mathbf{u}\\|^{2}}})}\\\\ &{\\gg1/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where the equality and inequality is by worse-case consideration over $\\mathcal{D}_{z}^{*}$ , a small $\\sigma_{0}$ and $\\lambda$ in Condition 1 with a sufficiently large $C$ , as well as the requirement $\\|\\pmb{b}_{k}^{*}\\|\\geq\\|\\pmb{a}_{k}^{*}\\|=\\Theta(\\|\\mathbf{u}\\|)$ . Besides, by $\\|\\b{d}_{k}^{*}\\|\\geq\\|\\b{c}_{k}^{*}\\|=\\Theta(\\|\\ensuremath{\\mathbf{q}}\\|)$ , Eq.(65), Lemma 4, Eq.(5) and Lemma 2, we have that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathbb{E}_{n\\in\\mathcal{D}^{*}}\\big[\\sum_{i\\in\\mathcal{W}_{n,i}^{y_{S n}}}\\mathbf{r}_{i}(\\alpha_{O_{(i,\\cdot)},\\hat{k}}^{(T^{*})}+y s_{n}(2\\sum_{l\\in\\mathbb{E}[S_{n,\\hat{k}}^{y_{S n}}]}(\\sigma_{S}^{(T^{*})})_{l}^{n}-1)\\beta_{O_{(i,\\cdot)},\\hat{k}}^{(T^{*})})\\big]\\ge\\Theta(\\kappa),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Collaborating with Lemma 43, the poof is completed. ", "page_idx": 57}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "1. Claims ", "page_idx": 58}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The contributions and scope of this paper are well summarized in the abstract and introduction. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 58}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: We explicitly discuss the limitation in Appendix A ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 58}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The detailed assumptions and proofs for all theorems and lemmas are given in the corresponding positions. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 58}, {"type": "text", "text": "", "page_idx": 59}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: Our algorithm is straightforward and easy to implement, and every detail is given in Section 3. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 59}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: We have provided the complete configuration in Section 3 and 6. We have uploaded the code with instructions in the supplementary material. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/ CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 59}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/ CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 60}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: All the experimental settings can be found in Section 3 and 6. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 60}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Justification: This paper executes algorithms 10 times and reports the average results to reduce randomness. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We have provided sufficient information about computer resources in Appendix B Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 60}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 61}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: The research does not involve any human subjects, personal data, or interactions that would raise ethical concerns about consent, privacy, or respect for persons. In conclusion, the research aligns with the ethical principles outlined in the NeurIPS Code of Ethics. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: We have discussed the broader impacts in Section A. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [No] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 61}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 62}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 62}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 62}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 62}, {"type": "text", "text": "Answer: [No] ", "page_idx": 62}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 62}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [No] ", "page_idx": 62}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 62}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 63}, {"type": "text", "text": "Answer: [No] ", "page_idx": 63}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 63}]