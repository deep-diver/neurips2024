[{"figure_path": "qCpCy0EQAJ/tables/tables_4_1.jpg", "caption": "Table 1: Compares the results of our method with the other weight reinitialization methods on ResNet18. g in fg indicates the number of generations the model is trained.", "description": "This table compares the performance of the proposed Dynamic Neural Regeneration (DNR) method against other weight reinitialization methods (DSD, BAN, KE) on the ResNet18 architecture, using five different small datasets (CUB, Aircraft, Dog, Flower, MIT).  The results are presented as the mean accuracy \u00b1 standard deviation, and the number of generations (g) used for training is specified for each method.  The table allows a comparison of DNR's performance across different datasets and against various baseline methods.", "section": "5.1 Evaluation on Small Datasets"}, {"figure_path": "qCpCy0EQAJ/tables/tables_6_1.jpg", "caption": "Table 2: Compares the results of the DNR framework with the KE and longer baselines for ResNet50 on large datasets.  g in fg indicates the number of generations the model is trained.", "description": "This table compares the performance of Dynamic Neural Regeneration (DNR) against Knowledge Evolution (KE) and a Long Baseline (LB) on three large-scale image classification datasets: CIFAR10, CIFAR100, and TinyImageNet.  The model used is ResNet50.  The results show the accuracy achieved after 10 generations (f10) of training, using label smoothing (Smth).  The comparison highlights the improved generalization performance of DNR compared to existing methods on these large datasets.", "section": "5 Results"}, {"figure_path": "qCpCy0EQAJ/tables/tables_7_1.jpg", "caption": "Table 3: Comparative analysis of DNR and transfer learning across diverse datasets.", "description": "This table compares the performance of Dynamic Neural Regeneration (DNR) with transfer learning on several datasets.  It shows the accuracy of using a smoothing regularizer with transfer learning and with DNR, demonstrating DNR's superior performance across multiple datasets.", "section": "5.4 Comparison with Transfer Learning"}, {"figure_path": "qCpCy0EQAJ/tables/tables_8_1.jpg", "caption": "Table 4: Evaluating the performance of DNR with different importance estimation.", "description": "This table compares the performance of the Dynamic Neural Regeneration (DNR) method using different criteria for estimating the importance of parameters: Long Baseline (LB), Random (KE - Knowledge Evolution), Fisher Information Matrix (FIM), Weight Magnitude, and SNIP (Single-shot Network Pruning).  The results are shown for the CUB200 and Flower datasets, indicating the accuracy achieved by each method.  This table helps to demonstrate the superiority of the SNIP method for data-aware dynamic masking in the DNR framework.", "section": "5.6 Effect of Importance Estimation Method"}, {"figure_path": "qCpCy0EQAJ/tables/tables_14_1.jpg", "caption": "Table 1: Compares the results of our method with the other weight reinitialization methods on ResNet18.  g in fg indicates the number of generations the model is trained.", "description": "This table compares the performance of the proposed Dynamic Neural Regeneration (DNR) method against several other weight reinitialization methods (CE, DSD, BAN, KE) on the ResNet18 architecture.  The results are presented for several small datasets (CUB, Aircraft, Dog, Flower, MIT).  The table shows the accuracy achieved by each method, with the number of generations (g) used in the training indicated in parentheses. The inclusion of label smoothing (Smth) as a regularizer is also explored, showing its impact on performance.", "section": "5.1 Evaluation on Small Datasets"}, {"figure_path": "qCpCy0EQAJ/tables/tables_15_1.jpg", "caption": "Table 1: Compares the results of our method with the other weight reinitialization methods on ResNet18. g in fg indicates the number of generations the model is trained.", "description": "This table compares the performance of the proposed Dynamic Neural Regeneration (DNR) method with other weight reinitialization methods (DSD, BAN, KE) and a long baseline (LB) on five different small datasets using ResNet18. The results are reported in terms of mean accuracy and standard deviation across different datasets. The number of generations used for the iterative training is also specified. The table shows that the DNR method consistently outperforms other methods in terms of accuracy and generalization.", "section": "5.1 Evaluation on Small Datasets"}, {"figure_path": "qCpCy0EQAJ/tables/tables_15_2.jpg", "caption": "Table 7: Evaluation with varying the quantity of data for importance estimation. Test accuracy at the end of 10 generations is shown on Aircraft and CUB datasets.", "description": "This table presents the results of the Dynamic Neural Regeneration (DNR) method using different numbers of data samples for importance estimation.  It shows the test accuracy achieved after 10 generations on two datasets (Aircraft and CUB) when using either 20% of the entire dataset or only 128 samples for estimating the importance of parameters using the SNIP method. The results demonstrate the robustness of the DNR method to variations in the amount of data used for importance estimation.", "section": "A.9 Varying the quantity of data used for Importance estimation"}, {"figure_path": "qCpCy0EQAJ/tables/tables_16_1.jpg", "caption": "Table 1: Compares the results of our method with the other weight reinitialization methods on ResNet18. g in fg indicates the number of generations the model is trained.", "description": "This table compares the performance of the proposed Dynamic Neural Regeneration (DNR) method with other weight reinitialization methods (CE, DSD, BAN, KE) on the ResNet18 architecture using small datasets.  It shows the mean accuracy across five different datasets (CUB, Aircraft, Dog, Flower, MIT) for each method, and indicates how many generations the model was trained for (g in fg).  The results demonstrate that DNR generally outperforms other methods.", "section": "5.1 Evaluation on Small Datasets"}, {"figure_path": "qCpCy0EQAJ/tables/tables_16_2.jpg", "caption": "Table 9: Performance of DNR across generations on the Flower dataset", "description": "This table shows the accuracy achieved by the Dynamic Neural Regeneration (DNR) method on the Flower dataset across different numbers of generations (iterations of training).  It demonstrates how the model's accuracy improves as the number of generations increases, indicating the efficacy of the iterative training paradigm.", "section": "5 Results"}, {"figure_path": "qCpCy0EQAJ/tables/tables_16_3.jpg", "caption": "Table 1: Compares the results of our method with the other weight reinitialization methods on ResNet18. g in  indicates the number of generations the model is trained.", "description": "This table compares the performance of the proposed Dynamic Neural Regeneration (DNR) method with other weight reinitialization methods (CE, DSD, BAN, KE) on five different datasets using the ResNet18 architecture.  The results are shown for different numbers of generations (g), indicating the iterative nature of the training process.  The table helps illustrate the improvement in performance achieved by DNR compared to existing methods, highlighting its efficacy in improving the generalization of deep neural networks on small datasets.", "section": "5 Results"}]