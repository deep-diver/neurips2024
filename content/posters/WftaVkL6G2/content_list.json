[{"type": "text", "text": "Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Michael Crawshaw ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingrui Liu ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science George Mason University Fairfax, VA 22030 mcrawsha@gmu.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science George Mason University Fairfax, VA 22030 mingruil@gmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice. Recent works analyze federated learning under more realistic participation patterns, such as cyclic client availability or arbitrary participation. However, all such works either require strong assumptions (e.g., all clients participate almost surely within a bounded window), do not achieve linear speedup and reduced communication rounds, or are not applicable in the general non-convex setting. In this work, we focus on nonconvex optimization and consider participation patterns in which the chance of participation over a fixed window of rounds is equal among all clients, which includes cyclic client availability as a special case. Under this setting, we propose a new algorithm, named Amplified SCAFFOLD, and prove that it achieves linear speedup, reduced communication, and resilience to data heterogeneity simultaneously. In particular, for cyclic participation, our algorithm is proved to enjoy $\\mathcal{O}(\\epsilon^{-2})$ communication rounds to find an $\\epsilon$ -stationary point in the non-convex stochastic setting. In contrast, the prior work under the same setting requires $\\mathcal{O}(\\kappa^{2}\\epsilon^{-4})$ communication rounds, where $\\kappa$ denotes the data heterogeneity. Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of $\\epsilon$ and $\\kappa$ . Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work. We also provide experimental results with (1) synthetic data and (2) real-world data with a large number of clients ( $N=250$ ), demonstrating the effectiveness of our algorithm under periodic client participation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) [26, 20, 15, 51] is a distributed learning paradigm that emphasizes client privacy [27, 40, 28], limited communication [18, 24], and data heterogeneity across clients [16, 53]. FL has attracted attention in recent years due to the ability to leverage data and compute from user devices while respecting privacy [47, 8]. For large-scale FL, it is common to limit the number of simultaneously participating devices, and many works do so by assuming that a random subset of clients can be sampled independently at each round [26, 2, 48, 21, 37]. However, this pattern of client participation is not always practical. If clients are user devices like mobile phones, they may not have 24/7 availability due to low battery or bad internet connection [14, 30]. In particular, if client availability is correlated with geographical location (e.g. mobile phones charging at night), then client availability follows a cyclic pattern [52]. Therefore, it remains an important open question to design federated optimization algorithms with provable efficiency under non-i.i.d client participation. ", "page_idx": 0}, {"type": "table", "img_path": "WftaVkL6G2/tmp/e96c59c4bdad5601a6bebf63071c6742f00bd6a75837d276672d11bd3d13d1f0.jpg", "table_caption": ["Table 1: Communication and computation complexity of various methods to find an $\\epsilon$ -stationary point for $L$ -smooth, non-convex objectives. $N$ : number of clients, $\\kappa$ : data heterogeneity $\\operatorname*{sup}_{x}\\|\\nabla f_{i}(\\pmb{\\bar{x}})-$ $\\nabla f(\\mathbf{x})\\Vert\\ \\leq\\ \\kappa$ $S$ : number of participating clients per round, $\\bar{K}$ : number of groups for cyclic participation. See Section 3.2 for a description of each participation pattern. We say that an algorithm exhibits reduced communication if its dependence in terms of $\\epsilon$ is strictly smaller than $\\bar{\\mathcal O}(\\epsilon^{-4})$ Derivation of complexities for Amplified FedAvg can be found in Appendix C. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Several works have investigated optimization in FL under non-i.i.d. client participation [9, 1, 7, 38, 44]. However, to the best of our knowledge, no existing algorithm in a non-i.i.d. participation setting provably exhibits reduced communication cost, linear speedup with respect to the number of clients, and resilience to client data heterogeneity for general non-convex optimization. ", "page_idx": 1}, {"type": "text", "text": "In this work, we consider FL under an arbitrary participation framework [38], where client participation during each round is a random variable with potentially unknown distribution. We focus on client participation patterns that are periodic, in the sense that all clients are expected to participate with equal frequency over a window of multiple training rounds. ", "page_idx": 1}, {"type": "text", "text": "For this setting, we propose Amplified SCAFFOLD, an optimization algorithm for FL under periodic client participation. Amplified SCAFFOLD utilizes (a) amplified updates across participation periods and (b) control variates computed across entire participation periods, to eliminate the effect of data heterogeneity even under non-i.i.d. participation. We show that Amplified SCAFFOLD exhibits significantly reduced communication cost, linear speedup, and is unaffected by client data heterogeneity. To the best of our knowledge, this is the first result demonstrating reduced communication or resilience to data heterogeneity without assuming i.i.d. participation. The complexity of Amplified SCAFFOLD is compared against baselines in Table 1. For cyclic participation, Amplified SCAFFOLD improves the previous best communication cost from $\\bar{\\mathcal{O}}(\\kappa^{2}\\epsilon^{-4})$ to $\\dot{\\mathcal{O}}(\\epsilon^{-2})$ ", "page_idx": 1}, {"type": "text", "text": "The main challenges of achieving these properties are (1) simultaneously handling randomness from stochastic gradients and non-i.i.d. participation; and (2) controlling the error of control variates under non-i.i.d. participation. Previous work in this setting [38] performs an in-expectation analysis, by taking expectation only over randomness from stochastic gradients; this avoids (1) but cannot leverage properties of the participation pattern to reduce communication. We present a tighter analysis that addresses (1) by taking expectation over both client participation and the stochastic gradients throughout the analysis, and carefully treating the trajectory variables which depend on both sources of randomness. We address (2) by recursively bounding the control variate errors, which involves a non-uniform average of non-uniform averages of error terms resulting from non-i.i.d. participation. We show that this nested non-uniform average can be bounded using mild regularity conditions on the participation pattern. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized below. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We introduce Amplified SCAFFOLD, an optimization algorithm for federated learning under non-i.i.d. client participation. Our convergence analysis demonstrates its computational and communication efficiency: Amplified SCAFFOLD exhibits reduced communication, linear speedup, and is unaffected by data heterogeneity. These guarantees are achieved with a tighter analysis than used in previous work [38], with a fine-grained treatment of the two sources of randomness: client participation and stochastic gradients. In the case of cyclic participation, we reduce the previous best communication cost of $\\mathcal{O}(\\kappa^{2}\\epsilon^{-4})$ to $\\mathcal{O}(\\epsilon^{-\\bar{2}})$ \u00b7 Experimental results show that Amplified SCAFFOLD converges faster than baselines on both synthetic and real-world problems under realistic non-i.i.d. client participation patterns. We also include an ablation study which demonstrates the robustness of our algorithm to changes in data heterogeneity, the number of participating clients per round, and the number of client groups in cyclic participation. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The paper is outlined as follows. We discuss related work in Section 2, and Section 3 provides a formal specification of the optimization problem. Amplified SCAFFOLD is introduced and theoretically analyzed in Section 4, and we provide experiments in Section 5. We conclude with Section 6. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Federated Optimization. FedAvg [26] characterizes partial client participation and local updates in each round. FedAvg was analyzed in the full participation setting [34, 36, 48, 49, 42, 41, 17, 11]. Other federated optimization algorithms aim to improve communication efciency [31, 50] and tackle data heterogeneity [21, 16]. The analysis of FL optimization algorithms typically either assumes full client participation or partial client participation where clients are sampled uniformly randomly [45, 37, 16, 22, 42]. [29] provides lower bounds for distributed stochastic, smooth optimization with intermittent communication and non-convex objectives, both in the full and partial participation settings. They also include algorithms employing variance reduction which match (or closely match) lower bounds in the full and partial participation settings. However, none of the works above are applicable for general participation patterns such as periodic participation. ", "page_idx": 2}, {"type": "text", "text": "Client Participation. Cyclic data sampling was considered for stochastic convex optimization in [9], where they propose \u201c\"pluralistic\" solutions instead of learning a single model for all clients. There is a recent line of work considering various participation patterns, including client selection [10, 3, 32] biased participation [33, 5, 6], independent participation across rounds [16, 21, 22], unbiased participation [35, 12], bounded rounds of unavailability [44, 13, 46], asynchronous participation [23, 1], cyclic participation [7], and arbitrary participation [38, 39]. However, none of these works enjoy linear speedup, reduced communication rounds, and resilience to data heterogeneity under the general setting of non-convex objectives and periodic client participation. ", "page_idx": 2}, {"type": "text", "text": "See Appendix F for a detailed discussion comparing our results with a small number of closely related baselines. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a federated learning problem with $N$ clients, with the overall objective ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\pmb x}\\in\\mathbb{R}^{d}}\\left\\{f({\\boldsymbol x}):=\\frac{1}{N}\\sum_{i=1}^{N}f_{i}({\\boldsymbol x})\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where each $f_{i}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is the local objective of one client. We consider the stochastic optimization problem, so that $f_{i}(\\pmb{x})=\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{i}}[F(\\pmb{x};\\pmb{\\xi})]$ , and the optimization algorithm can access $F_{i}(x;\\xi)$ and $\\bar{\\nabla}F_{i}({\\pmb x};{\\pmb\\xi})$ for individual values of $\\xi$ . We make the following assumptions about the objectives: ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. (a) $\\begin{array}{r}{f(\\pmb{x}_{0})\\mathrm{~-~}\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x})\\ \\leq\\ \\Delta}\\end{array}$ $(b)$ Each $f_{i}$ is $L$ -smooth, i.e., $\\|\\nabla f_{i}({\\pmb x})\\mid-$ $\\nabla f_{i}(\\pmb{y})\\|\\;\\leq\\;L\\|\\pmb{x}\\mathrm{~-~}\\pmb{y}\\|$ for all $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}\\;\\in\\;\\mathbb{R}^{d}$ (c) The stochastic gradient has variance $\\sigma^{2}$ ,i.e., $\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{i}}[||\\nabla F_{i}(\\pmb{x};\\xi)-\\nabla f_{i}(\\pmb{x})||^{2}]\\le\\sigma^{2}$ for all $\\pmb{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 2}, {"type": "text", "text": "Since each $f_{i}$ may be non-convex, we consider the problem of finding an $\\epsilon$ -stationary point of $f$ , that is, a point $\\bar{\\mathbf{x}}\\in\\mathbb{R}^{d}$ such that $\\|\\nabla f({\\pmb x})\\|\\leq\\epsilon$ ", "page_idx": 2}, {"type": "text", "text": "We consider a federated learning framework consisting of $R$ rounds. For any round $r\\in\\{0,\\ldots,R{-}1\\}$ and client $i\\in[N]$ , the availability of client $i$ at round $r$ is a random variable $q_{r}^{i}$ , following the arbitrary participation framework of [38]. If $q_{r}^{i}=0$ , then client $i$ may not participate during round $i$ For example, under the conventional i.i.d. sampling of clients, at each round $r$ a subset of clients $\\mathcal{S}_{r}\\subset[N]$ is sampled uniformly without replacement, and the weights are set as g\\*  1fies. . ", "page_idx": 3}, {"type": "text", "text": "For some $P\\in\\mathbb{N}$ ,let $\\mathcal{Q}_{r_{0}}$ be the filtration generated by $\\{q_{r}^{i}:r_{0}\\leq r<r_{0}+P,i\\in[N]\\}$ ,let $\\mathcal{Q}$ be the filtration generated by $\\mathcal{Q}_{0},\\dotsc,\\mathcal{Q}_{R-P}$ , and let $\\mathcal{G}$ be the filtration generated by $\\{\\bar{\\xi}_{r,k}^{i}:0\\leq r<$ $R,0\\leq k<I,i\\in[N]\\}$ where $\\xi_{r,k}^{i}$ is the random sampling of the stochastic gradient of round $r$ step $k$ , client $i$ . We make the following assumptions about the participation distribution. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. For all $r\\,\\,\\,\\in\\,\\,\\,\\{0,\\ldots,R\\,-\\,1\\}$ :(a) $\\begin{array}{r l r}{\\sum_{i=1}^{N}q_{r}^{i}}&{{}=}&{1}\\end{array}$ and $\\begin{array}{r l r}{\\sum_{i=1}^{N}(q_{r}^{i})^{2}}&{{}\\le}&{\\rho^{2}}\\end{array}$ $(b)$ The distribution of $\\{q_{r}^{i}\\}$ is unbiased across clients over every window of $P$ rounds, i.e., $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{Q}_{r_{0}}}[\\frac{1}{P}\\sum_{r=m P}^{(m+1)P-1}q_{r}^{i}]\\,=\\,1/N}\\end{array}$ for every $m\\,<\\,R/P$ and $i~\\in~[N]$ (e) Each len ha a non$P$ $\\mathbb{P}_{\\mathcal{Q}_{r_{0}}}\\big(\\frac{1}{P}\\sum_{r=m P}^{(m+1)P-1}q_{r}^{i}>$ $0)>p_{s a m p l e}$ $m<R/P$ $i\\in[N]$ $\\mathcal{Q}$ $\\mathcal{G}$ ", "page_idx": 3}, {"type": "text", "text": "For each round, Assumption 2(a) enforces that the participation weights $q_{r}^{i}$ are normalized to sum to 1, and characterizes the spread of participation weights across clients with the constant $\\rho^{2}$ . Assumption 2(b) requires that the set of rounds can be partitioned into windows of length $P$ within which clients are expected to participate with equal frequency. Lastly, Assumption 2(c) enforces that within each window, for each client the probability of being sampled is nonzero. Conventional i.i.d client sampling satisfies Assumption 2 with $\\rho=S^{-\\mathrm{{\\dot{1}/2}}},P={\\mathrm{1}}$ and $p_{\\mathrm{sample}}=S/N$ ", "page_idx": 3}, {"type": "text", "text": "An important difference from conventional i.i.d. participation is that here, client participation is not necessarily independent across rounds. Accordingly, we emphasize that the expectation and probability in Assumptions 2(b)-(c) are taken only over $\\mathcal{Q}_{r_{0}}$ . Therefore, the mean participation weight in Assumption 2(b) may itself be a random variable if client participation at some rounds is dependent on the outcome of participation in previous rounds. Similarly, the sampling probability in Assumption 2(c) may be a random variable. For the participation patterns considered in the next section, Assumption 2 is satisfied even when client sampling is not independent across rounds. ", "page_idx": 3}, {"type": "text", "text": "3.2  Specific Participation Patterns ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Regularized Participation We say that client participation is regularized [38] if $\\begin{array}{r}{\\bar{q}_{r_{0}}^{i}=\\frac{1}{N}}\\end{array}$ almost surely for all $r_{0}$ and $i$ , where $\\bar{q}_{r_{0}}^{i}$ is defined on Line 18 of Algorithm 1 as the participation of client $i$ averaged over rounds $r_{0},\\dots,r_{0}^{\\vee}+P-1$ . In this case, Assumption 2 is satisfied with $p_{\\mathrm{sample}}=1$ while $P$ and $\\rho^{2}$ are parameters of the participation pattern. Regularized participation is a relatively strong constraint, since every client must participate within each window, which may not be practical. However, it is fexible in that there is no constraint on how clients participate within each window. Regularized participation was also considered for strongly convex objectives [25]. ", "page_idx": 3}, {"type": "text", "text": "Cyclic Participation Following the CyCP framework [7], $N$ clients are partitioned into $\\bar{K}$ equally sized subsets, and at round $r$ only clients in group ( $r$ mod $\\bar{K}$ ) may participate. $S$ clients are sampled without replacement from group $\\left(r\\,{\\bmod{\\,}}{\\bar{K}}\\right)$ ), for whom the participation weight is $q_{r}^{i}=1/S$ . All other clients are assigned $q_{r}^{i}=0$ . Cyclic participation satisfies Assumption 2 with $P=\\bar{K}$ \uff0c $\\rho=S^{-1/2}$ and $p_{\\mathrm{sample}}=\\bar{S}\\bar{K}/N$ . Notice that i.i.d. client sampling is the special case where $\\bar{K}=1$ ", "page_idx": 3}, {"type": "text", "text": "Cyclic participation can model a situation where each client group is available at a different time of day. For example, if client devices are mobile phones, then clients are available for participation at night, when phones are charging, likely to have internet connection, and otherwise idle. If devices are spread across the globe, then client groups are naturally formed by time zones. Cyclic participation is less stringent than regularized participation since not all clients are required to participate within each window. FedAvg was analyzed under cyclic participation for PL objectives [7], although this analysis is not applicable in our setting, which uses general non-convex objectives. ", "page_idx": 3}, {"type": "image", "img_path": "WftaVkL6G2/tmp/8e7c2fd9058c6c6ad130fa886d7c2231b2df070b5025e0036cba1e42c64f3bb1.jpg", "img_caption": ["Algorithm 1 Amplified SCAFFOLD "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4  Algorithm and Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present Amplified SCAFFOLD, our algorithm to solve the FL problem described in Section 3. Pseudocode for Amplified SCAFFOLD is shown in Algorithm 1. The main components of the Amplified SCAFFOLD algorithm are (1) amplified updates and (2) long-range control variates. ", "page_idx": 4}, {"type": "text", "text": "4.1 Algorithm Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To deal with the non-stationarity of client availability, Amplified SCAFFOLD performs amplified updates based on information accumulated over a window of $P$ rounds. In Algorithm 1, the variable $\\textbf{\\em u}$ holds a weighted average of local updates to client models, weighted by client participation. Every $P$ rounds, the global model is updated in the direction $\\textbf{\\em u}$ scaled by the amplification factor $\\gamma$ Informally, the direction $\\textbf{\\em u}$ includes information from all clients with equal representation, according to Assumption 2(b). Similar amplified updates are used in Amplified FedAvg [38]. ", "page_idx": 4}, {"type": "text", "text": "Control variates for heterogeneous federated learning were first introduced by SCAFFOLD [16]. However, SCAFFOLD-style control variates are updated every time a client participates, which may not be appropriate under periodic availability. For example, under non-i.i.d. participation control variates for different clients would be updated with different frequencies, so that some clients may have consistently less accurate control variates than others. Informally, this may lead to a bias in which some clients\u2019 objective is underweighted relative to others. To deal with this issue, Amplified SCAFFOLD updates control variates based on information accumulated over a window of $P$ rounds, which enforces equal representation of all clients in expectation, according to Assumption 2(b). ", "page_idx": 4}, {"type": "text", "text": "Comparison with [16, 38] Although the two algorithmic components of Amplified SCAFFOLD individually appear in previous work [16, 38], we emphasize that our complexity results cannot be achieved by simply combining the analyses of these two works. The analysis of [38] requires $\\epsilon^{-4}$ communication cost due to their treatment of the randomness in client participation. Here, we present a tighter analysiswith $\\epsilon^{-2}$ cost from a more fine-grained treatment of the two sources of randomness (stochastic gradients and client sampling). See Section 4.4 for more details on our approach. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$\\hat{\\pmb{x}}=\\bar{\\pmb{x}}_{m P}$ $m$ $\\{0,\\ldots,R/P{-}1\\}$ $r_{0}\\in\\{0,P,\\dots,R/P\\}$ $\\begin{array}{r}{w_{r_{0}}^{i}=\\frac{1}{N}\\sum_{j=1}^{N}\\frac{\\mathbb{1}\\left\\{\\bar{q}_{r_{0}}^{j}>0\\right\\}}{P\\bar{q}_{r_{0}}^{j}}\\sum_{s=r_{0}}^{r_{0}+P-1}q_{s}^{i}q_{s}^{j}}\\end{array}$ $\\begin{array}{r}{v_{r_{0}}^{i}=\\bar{q}_{r_{0}}^{i}-\\frac{1}{N}}\\end{array}$ $w_{r_{0}}^{i}$ the \"non-uniformity\" of the client sampling distribution. We also consider a variable $\\Lambda_{r_{0}}^{i}$ that depends only on the client sampling distribution and characterizes the sample size from which $G_{r_{0}}^{i}$ is computed. See Appendix A.1 for further discussion of these quantities. We consider convergence under the following conditions, which are satisfied by several participation patterns of interest. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}[w_{r_{0}}^{i}]\\leq\\displaystyle\\frac{P^{2}}{N}\\quad\\mathrm{for~all}\\;r_{0}\\;\\mathrm{and}\\;i,}\\\\ &{}&{\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{N}\\left(v_{r_{0}}^{i}\\right)^{2}\\Lambda_{r_{0}}^{i}\\right]\\leq\\rho^{2}\\quad\\mathrm{for~all}\\;r_{0}\\;\\mathrm{and}\\;i,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Suppose that Assumptions $^{\\,l}$ and 2 hold, and that Equation 1 and Equation 2 hold. If $\\begin{array}{r}{\\gamma\\eta\\le\\frac{p_{s a m p l e}}{60L I P}}\\end{array}$ and $\\begin{array}{r}{\\eta\\le\\frac{\\sqrt{p_{s a m p l e}}}{60L I P}}\\end{array}$ then Algorithm $^{\\,I}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\nabla f(\\hat{\\boldsymbol{x}})\\|^{2}]\\le\\mathcal{O}\\left(\\frac{\\Delta}{\\gamma\\eta I R}+\\left(\\gamma\\eta L\\rho^{2}+\\eta^{2}L^{2}I P\\right)\\sigma^{2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Corollary 1. For any $\\epsilon>0$ and $I\\geq1$ there exist choices of $\\gamma$ and $\\eta$ such that $\\mathbb{E}[\\|\\nabla f(\\hat{\\mathbf{x}})\\|^{2}]\\le\\mathcal{O}(\\epsilon^{2})$ as long as $\\begin{array}{r}{R\\ge\\mathcal{O}\\left(\\frac{\\Delta L\\rho^{2}\\sigma^{2}}{I\\epsilon^{4}}+\\frac{\\Delta L P}{p_{s a m p l e}\\epsilon^{2}}\\right)}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "The complexity of Amplified SCAFFOLD has several important properties: ", "page_idx": 5}, {"type": "text", "text": "Reduced Communication By choosing $I=\\Theta(\\Delta\\rho^{2}\\sigma^{2}p_{\\mathrm{sample}}P^{-1}\\epsilon^{-2})$ , Amplified SCAFFOLD has communication complexity $R=\\mathcal{O}(L P p_{\\mathrm{sample}}^{-1}\\epsilon^{-2})$ , which improves upon the $\\epsilon^{-4}$ complexity of parallel SGD. We are not aware of any existing work that achieves this communication reduction for non-convex federated optimization with periodic participation. ", "page_idx": 5}, {"type": "text", "text": "Unaffected by Heterogeneity The iterations $R I$ and the number of communications $R$ areunaffected by heterogeneity, which is not achieved for periodic participation by any existing work [38, 7]. ", "page_idx": 5}, {"type": "text", "text": "Linear Speedup The number of iterations $R I=\\mathcal{O}(\\Delta L\\rho^{2}\\sigma^{2}\\epsilon^{-4})$ will exhibit linear speedup in the number of clients through the term $\\rho^{2}$ , depending on the client participation pattern. ", "page_idx": 5}, {"type": "text", "text": "4.3  Application to Participation Patterns ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The results from Section 4.2 apply under any participation pattern that satisfies Assumption 2, Equation 1, and Equation 2, and below we discuss the participation patterns discussed in Section 3.2. The complexity of Amplified SCAFFOLD for each participation pattern are shown in Table 1, and these results can be obtained by plugging $\\rho^{2}$ $P$ and $p_{\\mathrm{sample}}$ into Corollary 1, together with a choice of $I$ as described in Section 4.2. The derivations of each result below are given in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Regularized Participation Recall that regularized participation satisfies Assumption 2 with $p_{\\mathrm{sample}}=$ 1, and $P,\\ \\rho^{2}$ are parameters of the participation pattern. Also, under regularized participation, $w_{r_{0}}^{i}=\\bar{q}_{r_{0}}^{i}=1/N$ almost surely so that $\\mathbb{E}[\\dot{w_{r_{0}}^{i}}]=\\dot{1^{/}}N\\le P^{2}/N$ and $v_{r_{0}}^{i}=\\bar{0}$ Therefore Equation 1 and Equation 2 are satisfied. Plugging $p_{\\mathrm{sample}}=1$ into Corollary 1 yields ", "page_idx": 5}, {"type": "equation", "text": "$$\nR=\\mathcal{O}\\left(L P\\epsilon^{-2}\\right),\\quad R I=\\mathcal{O}\\left(\\Delta L\\rho^{2}\\sigma^{2}\\epsilon^{-4}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this setting, our algorithm exhibits reduced communication and resilience to heterogeneity. To our knowledge, the only existing algorithm with theoretical guarantees for non-convex problems under regularized participation is Amplified FedAvg [38]. However, as seen in Table 1, the communication complexity of Amplified FedAvg has order $\\epsilon^{-4}$ in terms of $\\epsilon$ and suffers from a $\\kappa^{2}$ dependence. ", "page_idx": 5}, {"type": "text", "text": "Cyclic Participation Recall that cyclic participation satisfies Assumption 2 with $P=\\bar{K}$ \uff0c $\\rho=S^{-1/2}$ and $p_{\\mathrm{sample}}=\\bar{S}/N$ . Also, $\\mathbb{E}[w_{r_{0}}^{i}]\\stackrel{\\cdot}{=}S/N^{2}\\stackrel{\\cdot}{\\leq}P^{2}/N$ and $\\mathbb{E}[(v_{r_{0}}^{i})^{2}\\mathring{\\Lambda_{r_{0}}^{i}}]=\\rho^{2}$ , so that Equation 1 and Equation 2 are satisfied. Based on the above parameter values, the resulting complexities are ", "page_idx": 6}, {"type": "equation", "text": "$$\nR=\\mathcal{O}\\left(\\frac{L\\bar{K}}{\\epsilon^{2}}\\left(\\frac{N}{S}\\right)\\right),\\quad R I=\\mathcal{O}\\left(\\frac{\\Delta L\\sigma^{2}}{S\\epsilon^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Again, Amplified SCAFFOLD achieves reduced communication, linear speedup, and resilience to heterogeneity. Amplified FedAvg [38] is the only existing algorithm with theoretical guarantees in this setting, but it fails to achieve resilience to heterogeneity or reduce communication cost outside of the trivial case of full participation $(\\bar{K}=1,S=N)$ . Also, even for the setting of PL-functions, the convergence rate of FedAvg under cyclic participation from [7] does not demonstrate an improvement with respect to the number of local steps. See Appendix F for further discussion of their results. ", "page_idx": 6}, {"type": "text", "text": "Recall that i.i.d. participation is a special case of cyclic participation with $\\bar{K}=1$ . In this case, Amplified FedAvg fails to recover the reduced communication usually achieved under i.i.d. participation, such as by SCAFFOLD [16]. In fact, Amplified FedAvg fails to recover the communication cost of FedAvg under i.i.d. participation, requiring an additional factor of $L N$ . The larger communication cost of Amplified FedAvg is a result of its convergence analysis, which does not leverage the property of unbiased participation (Assumption 2(b)) during the analysis, and requires $\\bar{P}=\\bar{\\mathcal{O}(\\epsilon^{-2})}$ in order to converge (see Appendix C for more details). In contrast, Amplified SCAFFOLD succeeds in recovering the results of SCAFFOLD under i.i.d. participation, with only a slightly worse dependence of $R$ on $\\bar{N}/S$ . This difference in the order of $\\frac{N}{S}$ is due to a potential small issue in the analysis of SCAFFDwhiceteoallyavdedyainliglywre deendec $\\frac{N}{S}$ We provide a detailed discussion of the $\\dot{N}/S$ dependence in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "4.4 Proof Sketch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The main challenges for demonstrating convergence are (1) simultaneously handling randomness from stochastic gradients and non-i.i.d. client sampling, and (2) controlling error of control variates under non-i.i.d. client sampling. Previous work [38] subverts (1) by conditioning on $\\mathcal{Q}$ throughout the entire analysis. However, this eliminates the possibility of utilizing the condition $\\mathbb{E}[\\bar{q}_{r_{0}}^{i}]=\\mathbf{\\bar{\\alpha}}^{}1/N$ and ultimately incurs a dependence on the data heterogeneity (see the term ${\\tilde{\\delta}}^{2}(P)$ in Theorem 3.1 of [38]). Instead, we take expectation over both sources of randomness throughout the analysis, which requires a careful treatment of each iterate's dependence, and enables communication reduction. For (2), previous analysis of federated algorithms with control variates [16] recursively bounds the error of control variates between consecutive rounds. However, this recursion crucially depends on i.i.d. client participation. We extend this analysis to our setting, establishing a recursion over the control variate error between consecutive windows of $P$ rounds. Establishing this recursion under non-i.i.d. participation involves a non-uniform average of non-uniform averages of error terms, which we handle by invoking the regularity conditions stated in Equation 1 and Equation 2. ", "page_idx": 6}, {"type": "text", "text": "Using smoothness of $f$ , the objective function decrease $f(\\bar{\\pmb{x}}_{r_{0}+P})-f(\\bar{\\pmb{x}}_{r_{0}})$ is upper bounded by $\\begin{array}{r}{\\langle\\nabla f(\\bar{\\pmb{x}}_{r_{0}}),\\bar{\\pmb{x}}_{r_{0}+P}-\\bar{\\pmb{x}}_{r_{0}}\\rangle+\\frac{L}{2}\\|\\bar{\\pmb{x}}_{r_{0}+P}-\\bar{\\pmb{x}}_{r_{0}}\\|^{2}}\\end{array}$ Letting $\\begin{array}{r}{\\bar{\\pmb{x}}_{r,k}=\\sum_{i=1}^{N}q_{r}^{i}\\pmb{x}_{r,k}^{i}}\\end{array}$ be a weighted average of local models, the sum of the previous inner product and quadratic terms can be bounded by $-\\gamma\\eta I P\\|\\nabla f(\\bar{\\pmb{x}}_{r_{0}})\\|^{2}$ , plus standard noise terms, the additional \u201cdrift\" terms ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{D}_{r,k}=\\sum_{i=1}^{N}q_{r}^{i}\\left\\|\\pmb{x}_{r,k}^{i}-\\bar{\\pmb{x}}_{r,k}\\right\\|^{2}\\quad\\tilde{M}_{r,k}=\\left\\|\\bar{\\pmb{x}}_{r,k}-\\bar{\\pmb{x}}_{r_{0}}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and control variate errors $C_{r_{0}}^{i}\\,=\\,\\|\\nabla f_{i}(\\bar{\\pmb{x}}_{r_{0}})-\\pmb{G}_{r_{0}}^{i}\\|^{2}$ ${\\tilde{D}}_{r,k}$ captures the distance between local client models, while $\\tilde{M}_{r,k}$ captures the distance from local models to the previous global model $\\bar{\\pmb{x}}_{r_{0}}$ ", "page_idx": 6}, {"type": "text", "text": "Taking conditional expectation $D_{r,k}=\\mathbb{E}[\\tilde{D}_{r,k}|\\boldsymbol{\\mathcal{Q}}]$ and $M_{r,k}=\\mathbb{E}[\\tilde{M}_{r,k}|\\boldsymbol{\\mathcal{Q}}]$ , Lemma 1 bounds the drift terms by establishing and unrolling a mutually recurrent relation between $D_{r,k}$ and $M_{r,k}$ . The resulting bound involves a non-uniform average over the control variate errors: $\\begin{array}{r}{\\sum_{i=1}^{N}q_{r}^{i}\\mathbb{E}[C_{r_{0}}^{i}|\\mathcal{Q}]}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Denoting th average cntrolvariate eror $\\begin{array}{r}{C_{r_{0}}=\\frac{1}{N}\\sum_{i=1}^{N}C_{r_{0}}^{i}}\\end{array}$ we want to bound $\\mathbb{E}[C_{r_{0}+P}]$ interms of $\\mathbb{E}[C_{r_{0}}].\\;C_{r_{0}+P}$ can be decomposed into drift terms, but the result is a non-uniform average: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{s=r_{0}}^{r_{0}+P-1}\\sum_{k=0}^{I-1}q_{s}^{i}(D_{s,k}+M_{s,k}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since the bound for each $M_{s,k}+D_{s,k}$ from Lemma 1 involves a non-uniform average over $C_{r_{0}}^{i}$ , the resulting bound of $C_{r_{0}+P}$ involvesanon-uniformaverageof non-uniformaveragesof $C_{r_{0}}^{i}$ ,instead of the uniform average $C_{r_{0}}$ . The regularity conditions in Equation 1 and Equation 2 allow us to bound this nested non-uniform average by a uniform average, which finishes the recursion. ", "page_idx": 7}, {"type": "text", "text": "Putting everything together, we obtain the descent inequality ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\tilde{f}_{r_{0}+P}]\\leq\\mathbb{E}[\\tilde{f}_{r_{0}}]-\\gamma\\eta I P\\mathbb{E}\\left[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}\\right]+\\gamma\\eta I P(\\gamma\\eta L\\rho^{2}+\\eta^{2}L^{2}I P)\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\tilde{f}_{r_{0}}:=f(\\bar{x}_{r_{0}+P})+\\Phi(r_{0}+P)$ and $\\Phi$ is a potential function that depends on the control variate errors. Theorem 1 is then obtained by averaging over $r_{0}$ and isolating the gradient. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We experimentally validate our algorithm for non-i.i.d client participation under three settings: minimizing a synthetic function, logistic regression for Fashion-MNIST  [43], and training a CNN for CIFAR-10 [19]. We also include an ablation study on Fashion-MNIST, to investigate how each algorithm is affected by changes in data heterogeneity, the number of participating clients, and the number of client groups in cyclic participation. ", "page_idx": 7}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "All of our experiments utilize a non-i.i.d. client participation pattern similar to cyclic participation (discussed in Section 3.2). We partition the total set of $N$ clients into $\\bar{K}$ equally sized subsets, and at each training round only a single client group is available for participation. In our experiments, the available group does not change every round; instead, each group is available for $g$ rounds at a time. Under this pattern, Assumption 2 is satisfied with $P=g{\\bar{K}}$ . We refer to $g$ as the availability time. ", "page_idx": 7}, {"type": "text", "text": "We evaluate five algorithms: FedAvg [26], FedProx [21], SCAFFOLD [16], Amplified FedAvg [38], and Amplified SCAFFOLD (ours). We tune each algorithm's parameters by grid search, including learning rate $\\eta$ , amplification rate $\\gamma$ ,and FedProx's $\\mu$ . The search ranges and tuned values can be found in Appendix D. All experiments were run on a single node with eight NVIDIA A6000 GPUs. Code is available at the following repository: https: //github. com/ MingruiLiu-ML-Lab/FL-under-Periodic-Participation ", "page_idx": 7}, {"type": "text", "text": "Synthetic We evaluate each algorithm's convergence on a difficult objective based on a lower bound for FedAvg [42]. The objective maps $\\mathbb{R}^{4}$ to $\\mathbb{R}$ , is convex, and is parameterized by a smoothness $L$ stochastic gradient variance $\\sigma^{2}$ , and heterogeneity $\\kappa$ so that it satisfies Assumption 1 by construction. The complete definition of the objective can be found in Appendix D. Since there are only two distinct local objectives, we set the number of clients $N=2$ and the number of sampled clients $S=1$ , and the number of groups $\\bar{K}=2$ . All other settings can be found in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Fashion-MNIST and CIFAR-10 We evaluate each algorithm for training an image classifier, using logistic regression for Fashion-MNIST and a two-layer CNN for CIFAR-1o. To simulate heterogeneous data in federated learning, we use a common protocol [16, 38], to partition each dataset into client datasets according to a data similarity parameter $s$ . This protocol is detailed in Appendix D. Following [38], we set the number of clients $N=250$ , data similarity $s=5\\%$ , and the number of sampled clients per round $S=10$ . For client participation, we set the number of groups $\\bar{K}=5$ so that each group contains clients that have majority label from two different classes. We run all baselines with 5 different random seeds and report the mean results with error bars in Section 5.2 (the radius of each error bar is 1 standard deviation). All other settings can be found in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Additional experimental results are provided in Appendix E, where we compare against extra baselines (FedAdam [31], FedYogi [31], FedAvg-M [4], and Amplified FedAvg with FedProx regularization), and evaluate training under another non-i.i.d. client participation pattern. ", "page_idx": 7}, {"type": "image", "img_path": "WftaVkL6G2/tmp/e1bce926b44fe61c9e912ec0413e2a18075934808c0041d570feefe57590b675.jpg", "img_caption": ["Figure 1: Results for synthetic objective and CIFAR-10. Left: Amplified SCAFFOLD and SCAFFOLD both converge to the global minimum, but Amplified SCAFFOLD converges significantly faster. Right: Amplified SCAFFOLD converges to the best solution by a significant margin. Note that in both cases, the curves for FedAvg and FedProx are nearly overlapping. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2  Main Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Results for the synthetic experiment and CIFAR-10 are shown in Figure 1, and results for FashionMNIST are shown in Figure 2. We make the following observations: ", "page_idx": 8}, {"type": "text", "text": "Amplified SCAFFOLD converges the fastest. In all three settings, Amplified SCAFFOLD reaches the best overall solution among all algorithms (by all metrics) and requires the fewest communication rounds. In the synthetic experiment, Amplified SCAFFOLD requires 800 communication rounds to reach an objective value of 0.2, while SCAFFOLD requires 1900 rounds, and both FedAvg and Amplified FedAvg require 4800 rounds to reach the same objective value. ", "page_idx": 8}, {"type": "text", "text": "Amplified FedAvg is comparable to FedAvg. Amplified FedAvg shows slight improvement over FedAvg for the synthetic experiment and for Fashion-MNIST. Only for CIFAR-10 is Amplified FedAvg significantly faster than FedAvg, but there it also exhibits a reduction in stability. The underwhelming experimental performance of Amplified FedAvg corroborates our discussion from Section 4.3; Amplified FedAvg requires many communication rounds and suffers from data heterogeneity. ", "page_idx": 8}, {"type": "text", "text": "Contrary to our findings, the original evaluation of Amplified FedAvg [38] showed a significant improvement over FedAvg. One explanation is that the original evaluation employed pretraining using FedAvg, so that each algorithm was evaluated only for fine-tuning. Our experiments suggest that Amplified FedAvg may have limited improvement over FedAvg when training from scratch. ", "page_idx": 8}, {"type": "text", "text": "SCAFFOLD beats Amplified FedAvg. Despite a lack of theoretical guarantees under non-i.i.d. participation, SCAFFOLD outperforms Amplified FedAvg in all settings. This suggests that SCAFFOLD may have reasonable performance under some non-i.i.d. participation patterns. For the synthetic objective and CIFAR-10, SCAFFOLD is still significantly slower than Amplified SCAFFOLD. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To understand how each algorithm's performance is affected by data heterogeneity, the number of participating clients, and the number of client groups, we perform an ablation study on FashionMNIST. First, we fix the data similarity $s=5\\%$ and number of groups $\\bar{K}=5$ while varying the number of participating clients $(S)$ over $\\{5,15,20,25\\}$ . Next, we fix $\\bar{S}=10,\\bar{K}=5$ while varying the similarity $s$ over $\\{2.5\\%,10\\%,33\\%,\\mathrm{\\dot{1}00\\%}\\}$ . Lastly, we fix $s\\,=\\,5\\%,S\\,=\\,10$ while varying the number of client groups $\\bar{K}\\,\\in\\,\\{2,4,6,8\\}$ . In each of these 12 scenarios, we evaluate all five algorithms using the same settings as detailed in Section 3. We train with three random seeds for each algorithm, and report the average results in Figure 2 (right). ", "page_idx": 8}, {"type": "text", "text": "Amplified SCAFFOLD reaches the best solution in all settings. Similarly to Section 5.2, Amplified SCAFFOLD consistently reaches the best solution in terms of both training loss and testing accuracy. While our theoretical results provide guarantees for optimization, these experiments show that Amplified SCAFFOLD also exhibits superior generalization in a variety of settings. ", "page_idx": 8}, {"type": "text", "text": "Robustness to data heterogeneity. When changing from completely homogeneous data $s=100\\%$ to extremely heterogeneous data ( $s=2.5\\%$ ), the test accuracy of Amplified SCAFFOLD exhibits a very small decrease from $84.6\\%$ to $84.45\\%$ , so that our algorithm behaves nearly identically with homogeneous data as with extremely heterogeneous data. All baselines suffer a larger decrease when transitioning from homogeneous data to heterogeneous data. ", "page_idx": 8}, {"type": "image", "img_path": "WftaVkL6G2/tmp/e54ffdc4ffa9748e724093300fed129d77aff1195abae0417e518c1f0b0c00ac.jpg", "img_caption": ["Figure 2: Results for Fashion MNIST and ablation study. Left: Amplified SCAFFOLD reaches the best solution, but SCAFFOLD is competitive. Other baselines are much slower. Right: Amplified SCAFFOLD is robust to changes in data heterogeneity, number of participating clients, and number of client groups. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Robustness to number of participating clients. The number of participating clients has a smaller effect on performance than data heterogeneity, but some degradation happens in the extreme case $S\\,=\\,5$ . In particular, SCAFFOLD has competitive performance with large $S\\,\\geq\\,15$ ,but its test accuracy drops off significantly compared to Amplified SCAFFOLD in the case $S=5$ ", "page_idx": 9}, {"type": "text", "text": "Robustness to number of client groups. As $\\bar{K}$ increases, FedAvg and Amplified FedAvg get worse, while SCAFFOLD and Amplified SCAFFOLD maintain performance. It makes intuitive sense for an algorithm to degrade as $\\bar{K}$ increases, since a larger $\\bar{K}$ means that the participation is in some sense \u201cfurther\" from i.i.d. participation. Still, Amplified SCAFFOLD (and SCAFFOLD) are able to maintain performance even as $\\dot{K}$ increases. While the worst-case communication complexity of Amplified SCAFFOLD (listed in Table 1) actually increases with $\\bar{K}$ , these experiments demonstrate that in practice, Amplified SCAFFOLD can maintain performance as $\\bar{K}$ increases. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose Amplified SCAFFOLD, an optimization algorithm for federated learning under periodic client participation, and prove that it exhibits reduced communication cost, linear speedup, and is unaffected by data heterogeneity. We also show that Amplified SCAFFOLD experimentally outperforms baselines on standard benchmarks under non-i.i.d. client participation, and that the performance of our algorithm is robust to changes in data heterogeneity and the number of participating clients. ", "page_idx": 9}, {"type": "text", "text": "Limitations While our analysis covers a general class of participation patterns, it may not cover some participation patterns that appear in practice. Our framework requires that all clients have an equal chance of participation across well-defined windows of time that are known to the algorithm implementer, which may not always hold. One such practical situation is where clients may freely join or leave the federated learning process during training. Extending our algorithm and guarantees for this situation would require a reformulation of the optimization problem, and possibly additional assumptions about the participation structure. We leave such analysis for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their helpful comments. This work is supported by the Institute for Digital Innovation fellowship from George Mason University, a ORIEI seed funding, an IDIA P3 fellowship from George Mason University, and a Cisco Faculty Research Award. Experiments were partially run on Hopper, a research computing cluster provided by the Office of Research Computing at George Mason University (URL: https://orc.gmu.edu). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Dmitri Avdiukhin and Shiva Kasiviswanathan. Federated learning under arbitrary communication patterns. In International Conference on Machine Learning, pages 425-435. PMLR, 2021.   \n[2]  Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, Brendan McMahan, et al. Towards federated learning at scale: System design. Proceedings of machine learning and systems, 1: 374-388, 2019.   \n[3]  Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated learning. arXiv preprint arXiv:2010.13723, 2020.   \n[4] Ziheng Cheng, Xinmeng Huang, Pengfei Wu, and Kun Yuan. Momentum benefits non-id federated learning simply and provably. In The Twelfth International Conference on Learning Representations, 2023.   \n[5]  Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence analysis and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243, 2020.   \n[6]  Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Towards understanding biased client selection in federatedlearning. In International Conference on Artificial Intelligence and Statistics, pages 10351-10375. PMLR, 2022.   \n[7] Yae Jee Cho, Pranay Sharma, Gauri Joshi, Zheng Xu, Satyen Kale, and Tong Zhang. On the convergence of federated averaging with cyclic client participation. arXiv preprint arXiv:2302.03109, 2023.   \n[8]  Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. Advances in Neural Information Processing Systems, 30, 2017.   \n[9] Hubert Eichner, Tomer Koren, Brendan McMahan, Nathan Srebro, and Kunal Talwar. Semicyclic stochastic gradient descent. In International Conference on Machine Learning, pages 1764-1773. PMLR, 2019.   \n[10] Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi. Clustered sampling: Low-variance and improved representativity for clients selection in federated learning. In International Conference on Machine Learning, pages 3407-3416. PMLR, 2021.   \n[11] Margalit R Glasgow, Honglin Yuan, and Tengyu Ma. Sharp bounds for federated averaging (local sgd) and continuous perspective. In International Conference on Artificial Intelligence and Statistics, pages 9050-9090. PMLR, 2022.   \n[12]  Michal Grudzien, Grigory Malinovsky, and Peter Richtarik. Improving accelerated federated learning with compression and importance sampling. In Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities, 2023.   \n[13] Xinran Gu, Kaixuan Huang, Jingzhao Zhang, and Longbo Huang. Fast federated learning in the presence of arbitrary device unavailability. Advances in Neural Information Processing Systems, 34:12052-12064, 2021.   \n[14] Dzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu, Mike Rabbat, Ashkan Yousefpour, Carole-Jean Wu, Hongyuan Zhan, Pavel Ustinov, Harish Srinivas, et al. Papaya: Practical, private, and scalable federated learning. Proceedings of Machine Learning and Systems, 4: 814-832, 2022.   \n[15] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kalista Bonawit, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and trends? in machine learning, 14(1-2):1-210, 2021.   \n[16] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132-5143. PMLR, 2020.   \n[17]  Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 4519-4529. PMLR, 2020.   \n[18] Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon, Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.   \n[19]  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[20] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020.   \n[21] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429-450, 2020.   \n[22] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-id data. arXiv preprint arXiv: 1907.02189, 2019.   \n[23]  Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In International Conference on Machine Learning, pages 3043-3052. PMLR, 2018.   \n[24] Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao. Federated learning in mobile edge networks: A comprehensive survey. IEEE Communications Surveys & Tutorials, 22(3):2031-2063, 2020.   \n[25]  Grigory Malinovsky, Samuel Horvath, Konstantin Burlachenko, and Peter Richtarik. Federated learning with regularized client participation. arXiv preprint arXiv:2302.03662, 2023.   \n[26] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273-1282. PMLR, 2017.   \n[27]  H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. arXiv preprint arXiv:1710.06963, 2017.   \n[28]  Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali Dehghantanha, and Gautam Srivastava. A survey on security and privacy of federated learning. Future Generation Computer Systems, 115:619-640, 2021.   \n[29] Kumar Kshitij Patel, Lingxiao Wang, Blake E Woodworth, Brian Bullins, and Nati Srebro. Towards optimal communication complexity in distributed non-convex optimization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 13316-13328. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 56bd21259e28ebdc4d7e1503733bf421-Paper-Conference.pdf.   \n[30] Matthias Paulik, Matt Seigel, Henry Mason, Dominic Telaar, Joris Kluivers, Rogier van Dalen, Chi Wai Lau, Luke Carlson, Filip Granqvist, Chris Vandevelde, et al. Federated evaluation and tuning for on-device personalization: System design & applications. arXiv preprint arXiv:2102.08503, 2021.   \n[31] ashank Reddi Zachary Charls, ManzilZaher, achary Garett, Keith Rush,JakuKn, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. ICLR, 2021.   \n[32] Elsa Rizk, Stefan Vlaski, and Ali H Sayed. Federated leaning under importance sampling. IEEE Transactions on Signal Processing, 70:5381-5396, 2022.   \n[33] Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong. Towards fexible device participation in federated learning. In International Conference on Artificial Intelligence and Statistics, pages 3403-3411. PMLR, 2021.   \n[34] Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767, 2018.   \n[35]  Alexander Tyurin, Lukang Sun, Konstantin Pavlovich Burlachenko, and Peter Richtarik. Sharper rates and flexible framework for nonconvex sgd with client and data sampling. Transactions on Machine Learning Research, 2022.   \n[36]  Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of communication-efficient sgd algorithms. arXiv preprint arXiv: 1808.07576, 2018.   \n[37] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan AlShedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021.   \n[38]  Shiqiang Wang and Mingyue Ji. A unified analysis of federated learning with arbitrary client participation. Advances in Neural Information Processing Systems, 35:19124-19137, 2022.   \n[39] Shiqiang Wang and Mingyue Ji. A lightweight method for tackling unknown participation probabilities in federated averaging. arXiv preprint arXiv:2306.03401, 2023.   \n[40] Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek, and H. Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security, 15:3454-3469, 2020. doi: 10.1109/TIFS.2020.2988575.   \n[41] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International Conference on Machine Learning, pages 10334-10343. PMLR, 2020.   \n[42] Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local sgd for heterogeneous distributed learning. Advances in Neural Information Processing Systems, 33:6281-6292, 2020.   \n[43] Han Xiao, Kashif Rasul, and Roland Vollgraf.\u03b2 Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. 2017.   \n[44] Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Fan Wu, Guihai Chen, Shaojie Tang, and Zhihua Wu. Distributed non-convex optimization with sublinear speedup under intermittent client availability. arXiv preprint arXiv:2002.07399, 2020.   \n[45]  Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-id federated learning. In International Conference on Learning Representations, 2020.   \n[46] Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu. Anarchic federated learning. In International Conference on Machine Learning, pages 25331-25363. PMLR, 2022.   \n[47] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Francoise Beaufays. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv: 1812.02903, 2018.   \n[48] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 7184-7193, 2019.   \n[49] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5693-5700, 2019.   \n[50]  Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. Advances in Neural Information Processing Systems, 33:5332-5344, 2020.   \n[51] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. A survey on federated learning. Knowledge-Based Systems, 216:106775, 2021.   \n[52] Chen Zhu, Zheng Xu, Mingqing Chen, Jakub Konecny, Andrew Hard, and Tom Goldstein. Diurnal or nocturnal? federated learning of multi-branch networks from periodically shifting distributions. In International Conference on Learning Representations, 2021.   \n[53] Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-id data: A survey. Neurocomputing,465:371-390, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1  Introduction ", "page_idx": 14}, {"type": "text", "text": "2 Related Work 3 ", "page_idx": 14}, {"type": "text", "text": "3 Problem Setup 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "3.1 Participation Framework 4   \n3.2  Specific Participation Patterns 4 ", "page_idx": 14}, {"type": "text", "text": "4  Algorithm and Analysis 5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "4.1 Algorithm Overview 5   \n4.2 Main Results 6   \n4.3 Application to Participation Patterns 6   \n4.4 Proof Sketch .. 7 ", "page_idx": 14}, {"type": "text", "text": "5 Experiments 8 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "5.1 Setup 8   \n5.2 Main Results 9   \n5.3 Ablation Study 9 ", "page_idx": 14}, {"type": "text", "text": "6 Conclusion 10 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proof of Theorem 1 16 ", "page_idx": 14}, {"type": "text", "text": "A.1 Preliminary Definitions 16   \nA.2 Proofs. 17 ", "page_idx": 14}, {"type": "text", "text": "B Proofs for Specific Participation Patterns 45 ", "page_idx": 14}, {"type": "text", "text": "C  Complexity of Amplified FedAvg 48 ", "page_idx": 14}, {"type": "text", "text": "D Experiment Details 49 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1  Client Sampling Parameters 49   \nD.2 Heterogeneity Protocol . 50   \nD.3 Hyperparameter Tuning . . . . 50   \nD.4 Synthetic Objective . . . 50   \nD.5 CNN Architecture . . 51 ", "page_idx": 14}, {"type": "text", "text": "E  Additional Experimental Results 51 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1Additional Baselines 51   \nE.2 CIFAR-10 with Stochastic Client Availability 52 ", "page_idx": 14}, {"type": "text", "text": "F  Extended Comparison with Baselines 52 ", "page_idx": 14}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Preliminary Definitions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let $\\mathcal{G}_{r}$ denote the filtration generated by $\\{\\xi_{r,k}^{i}:0\\leq k\\leq I-1,i\\in[N]\\}$ , that is, by the randomness in the stochastic gradients during round $r$ . Also, let $\\mathcal{Q}_{r_{0}}$ denote the fltration generated by $\\{q_{r}^{i}:r_{0}\\leq$ $r<r_{0}+P,i\\in[N]\\}$ , that is, the randomness in client sampling between rounds $r_{0}$ and $r_{0}+P-1$ (inclusive). Also, let $\\mathcal{G}$ denote the filtration generated by ${\\mathcal{G}}_{0}\\cup{\\mathcal{G}}_{1}\\cup...\\cup{\\mathcal{G}}_{R-1}$ and $\\mathcal{Q}$ denote the filtration generated by $\\mathcal{Q}_{0}\\cup\\mathcal{Q}_{P}\\cup...\\cup\\mathcal{Q}_{R-P}$ . Similarly, let $\\mathcal{G}_{:r}$ denote the filtration generated by $\\mathcal{G}_{0}\\cup\\mathcal{G}_{1}\\cup...\\cup\\mathcal{G}_{r-1}$ and $\\mathcal{Q}_{:r_{0}}$ denote the filtration generated by $\\mathcal{Q}_{0}\\cup\\mathcal{Q}_{P}\\cup...\\cup\\mathcal{Q}_{r_{0}-P}$ Lastly, denote $\\mathbb{E}_{r_{0}}[\\cdot]=\\mathbb{E}[\\cdot|\\mathscr{Q}_{:r_{0}},\\mathscr{G}_{:r_{0}}]$ ", "page_idx": 15}, {"type": "text", "text": "In order to analyze the control variate errors (i.e., $\\|\\nabla f_{i}(\\bar{\\pmb{x}}_{r_{0}})-\\pmb{G}_{r_{0}}^{i}\\|)$ , we introduce notation to refer to the iterates whose stochastic gradients were used to construct $G_{r_{0}}^{i}$ . These iterates are exactly the iterates during the most recent window before $r_{0}$ in which client $i$ was sampled, i.e., where +)P-1>0.ForeachrE {ro,ro+1..,ro+P-1],kE[0..,I-1],adi\u2208 [N], let ", "page_idx": 15}, {"type": "equation", "text": "$$\nt_{i}(r)=\\left\\{\\!\\!\\begin{array}{l l}{{r}}&{{\\mathrm{if}\\;\\bar{q}_{r_{0}}^{i}>0}}\\\\ {{t_{i}(r-P)}}&{{\\mathrm{if}\\;\\bar{q}_{r_{0}}^{i}=0}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with the initialization $t_{i}(r)=0$ for all $r\\in\\{-P,\\ldots,-1\\}$ . Then denote ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\pmb y}_{r,k}^{i}={\\pmb x}_{t_{i}(r),k}^{i}}}\\\\ {{z_{r}^{i}=q_{t_{i}(r)}^{i}}}\\\\ {{\\zeta_{r,k}^{i}={\\xi}_{t_{i}(r),k}^{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Also, denote $\\begin{array}{r}{\\bar{z}_{r_{0}}^{i}=\\frac{1}{P}\\sum_{r=r_{0}}^{r_{0}+P-1}z_{r}^{i}}\\end{array}$ Then we can rewrite ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{G}_{r_{0}}^{i}=\\frac{1}{P\\bar{z}_{r_{0}-P}^{i}I}\\sum_{s=r_{0}-P}^{r_{0}-1}z_{s}^{i}\\sum_{k=0}^{I-1}\\nabla F_{i}(\\pmb{y}_{s,k}^{i};\\zeta_{s,k}^{i}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{D}_{t,k}=\\displaystyle\\sum_{i=1}^{N}q_{i}^{\\lambda}x_{i,k}^{i}}\\\\ &{\\mathcal{D}_{t,k}=\\displaystyle\\sum_{i=1}^{N}q_{i}^{\\lambda}\\mathbb{E}\\left[\\|x_{i,k}^{i}-x_{i,k}\\|^{2}\\right]\\langle\\tilde{q}\\rangle}\\\\ &{\\mathcal{M}_{t,k}=\\mathbb{E}\\left[\\big[\\mu_{t,k}-z_{t,i}\\big]^{2}\\big]\\langle\\tilde{q}\\rangle\\right.}\\\\ &{\\left.\\mathcal{S}_{t_{0}}^{i}=\\displaystyle\\int\\frac{1}{I^{3}}\\sum_{m=r,p=-r,p}^{N-1}\\sum_{i=1}^{k}\\mathbb{E}\\left[\\|y_{i,k}^{i}-x_{m,1}\\|^{2}\\|\\tilde{q}\\rangle\\right.}\\\\ &{\\mathrm{~}\\quad\\left.w_{t_{0}}^{i}=\\displaystyle\\frac{1}{I}\\sum_{j=1}^{N}\\frac{\\mathbb{E}\\big\\{q_{j,k}^{i}>0\\big\\}^{m+1-j}}{I\\tilde{p}_{j}^{i}}\\sum_{m=b}^{m+j}q_{i,j}^{m^{j}}\\right.}\\\\ &{\\left.v_{t_{0}}^{i}=\\tilde{q}_{i}^{i}-\\frac{1}{N}\\right.}\\\\ &{\\Lambda_{t_{0}}^{i}=\\left.\\frac{\\tilde{p}_{t_{0}}^{i}\\sum_{m=0}^{N-1}\\tilde{p}_{i}^{i}}{\\left(\\frac{\\lambda}{2}\\sum_{k=0}^{N-1}\\sum_{p=0}^{N-1}w_{i,p}^{i}\\right)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As discussed in the main body, $\\bar{\\pmb{x}}_{r,k}$ is a weighted average over local client models, weighted according to client participation. $D_{r,k}$ and $M_{r,k}$ are the drift terms described in the proof sketch of Section 4.4. In the proof sketch, we also informally discuss the control variate error $C_{r_{0}}^{i}=\\|\\nabla f_{i}(\\bar{\\pmb{x}}_{r_{0}})-\\pmb{G}_{r_{0}}^{i}\\|^{2}$ ", "page_idx": 15}, {"type": "text", "text": "This erroris closely related to the tem $S_{r_{0}}^{i}$ defined above, since $\\mathbb{E}\\left[\\left\\|\\nabla f_{i}(\\bar{\\pmb{x}}_{r_{0}})-\\mathbb{E}[G_{r_{0}}^{i}|\\,\\mathcal{Q}]\\|^{2}\\middle|\\mathcal{Q}\\right]=$ L2 S'ro ", "page_idx": 16}, {"type": "text", "text": "The quantities $w_{r_{0}}^{i},v_{r_{0}}^{i}$ and $\\Lambda_{r_{0}}^{i}$ arise in our proof from the use of control variates under non-i.i.d. client participation. As discussed in the proof sketch (Section 4.4), bounding the errors introduced by control variates involves a non-uniform average of non-uniform averages of error terms. This nested non-uniform average can be bounded by a uniform average as long as the term $w_{r_{0}}^{i}$ is not too large, which is exactly the requirement stated in Equation 1. On the other hand, the term $\\Lambda_{r_{0}}^{i}$ arises while bounding the variance of the update $\\left\\|\\bar{\\pmb{x}}_{r_{0}+P}-\\bar{\\pmb{x}}_{r_{0}}\\right\\|$ , and it represents the sample size for which the correction $G_{r_{0}}^{i}$ is computed. When the client sampling distribution is closer to uniform, then each client may participate frequently during each window of $P$ rounds, and the effective sample size used to compute each control variate is large. In this case, the variance of the update $||\\bar{\\pmb{x}}_{r_{0}+P}-\\bar{\\pmb{x}}_{r_{0}}||$ will be smaller, since larger sample size implies smaller variance. For general client sampling distributions, $\\Lambda_{r_{0}}^{i}$ describes the reduction of variance of one component of the update (i.e. $G_{r_{0}}^{i}$ ) due to the sampling of stochastic gradients during the previous window of $P$ rounds. In our analysis, the variance of the update $\\lVert\\bar{\\pmb{x}}_{r_{0}+P}-\\bar{\\pmb{x}}_{r_{0}}\\rVert$ can be bounded in terms of the variance of $G_{r_{0}}^{i}$ , which in turn depends on $\\Lambda_{r_{0}}^{i}$ . The condition in Equation 2 essentially enforces that clients participate sufficiently uniformly, so that the variance of $G_{r_{0}}^{i}$ can be bounded. We note that both Equation 1 and Equation 2 are satisfied by i.i.d. participation, regularized participation, and cyclic participation. ", "page_idx": 16}, {"type": "text", "text": "Finally, for ease of exposition, one clarification must be made about the conditional expectation $\\mathbb{E}[\\cdot|\\mathcal{Q}]$ as it appears in, for example, $M_{r,k}$ . The filtration $\\mathcal{Q}$ contains randomness from every round from O to $R-1$ , but the expression of which we are taking expectation may only depend on randomness for a smaller subset rounds. For example, in $M_{r,k}$ , the expression $||\\dot{\\bar{\\mathbf{x}}}_{r,k}-\\bar{\\mathbf{x}}_{r_{0}}\\bar{\\mathbf{\\Gamma}}||^{2}$ only depends on the randomness up to round $r\\le r_{0}+P$ , so that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\bar{\\pmb{x}}_{r,k}-\\bar{\\pmb{x}}_{r_{0}}\\|^{2}\\Big|\\mathcal{Q}\\right]=\\mathbb{E}\\left[\\|\\bar{\\pmb{x}}_{r,k}-\\bar{\\pmb{x}}_{r_{0}}\\|^{2}\\Big|\\mathcal{Q}_{:r_{0}+P}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In several places throughout the proof, we will replace E[. $|\\mathcal{Q}]$ by $\\mathbb{E}[\\cdot|\\mathcal{Q}_{:r_{0}+P}]$ in similar situations. ", "page_idx": 16}, {"type": "text", "text": "A.2Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 1. If $\\begin{array}{r}{\\eta\\le{\\frac{1}{60L I P}}}\\end{array}$ and $\\begin{array}{r}{\\mathbb{E}_{r_{0}}[\\bar{q}_{r_{0}}^{i}]=\\frac{1}{N}}\\end{array}$ then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle D_{r,k}+M_{r,k}\\leq108\\eta^{2}I^{2}P^{2}\\mathbb{E}\\left[\\left\\|\\nabla f(\\bar{x}_{r_{0}})\\right\\|^{2}\\middle|\\mathcal{Q}\\right]+\\left(75\\eta^{2}I+65\\eta^{2}I P\\rho^{2}\\right)\\sigma^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\,109\\eta^{2}L^{2}I^{2}P^{2}\\sum_{i=1}^{N}\\left(\\bar{q}_{r_{0}}^{i}+\\frac{1}{N}\\right)S_{r_{0}}^{i}+36\\eta^{2}L^{2}I^{2}\\sum_{i=1}^{N}q_{r}^{i}S_{r_{0}}^{i},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{k=0}^{I-1}(D_{r,k}+M_{r,k})\\right]\\leq108\\eta^{2}I^{3}P^{3}\\mathbb{E}\\left[\\left\\|\\nabla f(\\bar{x}_{r_{0}})\\right\\|^{2}\\right]+\\left(75\\eta^{2}I^{2}P+65\\eta^{2}I^{2}P^{2}\\rho^{2}\\right)\\sigma^{2}}}\\\\ &{}&{\\qquad+\\ 254\\eta^{2}L^{2}I^{3}P^{3}\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[S_{r_{0}}^{i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Denote ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\bar{G}_{r_{0}}^{i}=\\displaystyle\\frac{1}{P\\bar{z}_{r_{0}-P}^{i}I}\\sum_{s=r_{0}-P}^{r_{0}-1}z_{s}^{i}\\displaystyle\\sum_{k=0}^{I-1}\\nabla f_{i}(\\pmb{y}_{s,k}^{i})}}\\\\ {{\\bar{G}_{r_{0}}=\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\bar{G}_{r_{0}}^{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, denote $\\begin{array}{r}{\\underline{{\\pmb{g}_{r,k}^{i}}}=\\nabla F_{i}(\\pmb{x}_{r,k}^{i};\\pmb{\\xi}_{r,k}^{i})-\\pmb{G}_{r_{0}}^{i}+\\pmb{G}_{r_{0}}}\\end{array}$ and $\\bar{\\pmb{g}}_{r,k}^{i}=\\nabla f_{i}(\\pmb{x}_{r,k}^{i})-\\bar{\\pmb{G}}_{r_{0}}^{i}+\\bar{\\pmb{G}}_{r_{0}}.$ $\\bar{G}_{r_{0}}^{i}$ is the analogue of $\\bar{G}_{r_{0}}$ that depends on deterministic gradients $\\nabla f_{i}(\\pmb{y}_{s,k}^{i})$ instead of stochastic gradients. ", "page_idx": 16}, {"type": "text", "text": "Variance ofupdates We first compute the errors $\\mathbb{E}\\left[\\left\\lVert\\pmb{{g}}_{r,k}^{i}-\\bar{\\pmb{g}}_{r,k}^{i}\\right\\rVert^{2}\\right]$ and $\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N}q_{r}^{i}\\left(\\pmb{g}_{r,k}^{i}-\\pmb{\\bar{g}}_{r,k}^{i}\\right)\\right\\|^{2}\\right\\|\\mathcal{Q}\\right],$ whhwil ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|G_{r_{0}}^{i}-\\bar{G}_{r_{0}}^{i}\\right\\|^{2}\\middle|\\mathcal{Q}\\right]=\\mathbb{E}\\left[\\left\\|\\frac{1}{P\\tilde{\\mathcal{Z}}_{r_{0},-P}^{i}}\\sum_{s=r_{0}-P}^{r_{0}-1}\\sum_{s=0}^{I-1}z_{s}^{i}\\left(\\nabla F_{i}(y_{s,k}^{i};\\zeta_{s,k}^{i})-\\nabla f_{i}(y_{s,k}^{i})\\right)\\right\\|^{2}\\middle|\\mathcal{Q}\\right]}\\\\ &{\\overset{(i)}{\\leq}\\frac{1}{P\\tilde{\\mathcal{Z}}_{r_{0},-P}^{i}I}\\sum_{s=r_{0}-P}^{r_{0}-1}\\sum_{s=0}^{I-1}z_{s}^{i}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(y_{s,k}^{i};\\zeta_{s,k}^{i})-\\nabla f_{i}(y_{s,k}^{i})\\right\\|^{2}\\middle|\\mathcal{Q}\\right]}\\\\ &{\\leq\\frac{1}{P\\tilde{\\mathcal{Z}}_{r_{0},-P}^{i}I}\\sum_{s=r_{0}-P}^{r_{0}-1}\\sum_{k=0}^{I-1}z_{s}^{i}\\sigma^{2}}\\\\ &{=\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\phi_{t,\\varepsilon}^{*}-\\theta_{t,\\varepsilon}^{*}\\right\\|^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\Big\\|\\nabla F_{t}(x_{t}^{*},\\xi_{t+\\varepsilon}^{*})-\\nabla f_{t}(x_{t}^{*})\\Big\\|^{2}\\right]}\\\\ &{\\quad\\le3\\mathbb{E}\\left[\\Big\\|\\nabla F_{t}(x_{t}^{*},\\xi_{t+\\varepsilon}^{*})-\\nabla f_{t}(x_{t}^{*})\\Big\\|^{2}\\Big]\\Big\\|Q\\Big\\|+3\\mathbb{E}\\left[\\big\\|G_{n}^{*}-G_{n}^{*}\\big\\|^{2}\\right]\\Big\\|Q\\Big]}\\\\ &{\\quad\\quad+3\\mathbb{E}\\left[\\big\\|G_{n}-G_{n}\\Big\\|^{2}\\Big\\|Q\\Big\\|\\right]}\\\\ &{\\quad=3\\mathbb{E}\\left[\\big\\|\\nabla F_{t}(x_{t}^{*},\\xi_{t+\\varepsilon}^{*})-\\nabla f_{t}(x_{t}^{*})\\big\\|^{2}\\Big\\|Q\\Big\\|+3\\mathbb{E}\\left[\\big\\|G_{n}^{*}-G_{n}^{*}\\big\\|^{2}\\right]\\Big\\|Q\\right]}\\\\ &{\\quad\\quad+3\\mathbb{E}\\left[\\bigg\\|\\frac{1}{N}\\sum_{i=1}^{N}\\big(G_{n}^{*}-G_{n}^{*}\\big)\\bigg\\|^{2}\\bigg\\|Q\\right]}\\\\ &{\\quad\\quad=3\\mathbb{E}\\left[\\bigg\\|\\frac{1}{N}\\sum_{i=1}^{N}\\big(G_{n}^{*}-G_{n}^{*}\\big)\\bigg\\|^{2}\\bigg\\|Q\\right]}\\\\ &{\\quad\\quad=3\\mathbb{E}\\left[\\big\\|\\nabla F_{t}(x_{t}^{*},\\xi_{t}^{*},\\xi_{t}^{*})-\\nabla f_{t}(x_{t}^{*})\\big\\|^{2}\\bigg\\|Q\\right]+3\\mathbb{E}\\left[\\big\\|G_{n}^{*}-G_{n}^{*}\\big\\|^{2}\\big\\|Q\\right]}\\\\ &{\\quad\\quad\\quad+3\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[\\big\\|G_{n}^{*}-G_{n}^{*}\\big\\|^{2}\\big\\|Q\\right]}\\\\ &{\\quad\\quad\\le9Q^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E $\\begin{array}{r l}&{\\left\\|\\frac{\\int_{{\\mathbb{Z}}}^{\\infty}\\Phi(x,\\theta,x^{*})\\,\\mathrm{te}^{-\\theta}}{2}\\right\\|_{{\\mathbb{Z}}}^{1}\\Bigg\\}}\\\\ &{=\\operatorname{tanh}\\Bigg[\\displaystyle\\sum_{k=1}^{\\infty}\\left(\\mathrm{te}^{-\\theta}(\\mathrm{re}^{\\theta};x,\\theta,x^{*})-\\mathrm{tf}(x,\\theta^{*})\\right)-(\\lambda_{n}^{*}-\\alpha_{k}^{*})+(\\lambda_{n}^{*}-\\alpha_{k}^{*})\\cdot\\mathrm{te}^{\\theta}}\\\\ &{\\leq\\operatorname{tanh}\\Bigg[\\displaystyle\\sum_{k=1}^{\\infty}\\mathrm{te}^{-\\theta}\\left(\\mathrm{re}^{\\theta};(x,\\theta,x^{*})-\\mathrm{tf}(x,\\theta^{*})\\right)\\Big]\\Bigg|_{{\\mathbb{Z}}}^{1}\\Bigg]+\\operatorname{tanh}\\Bigg[\\displaystyle\\sum_{k=1}^{\\infty}\\mathrm{te}^{-\\theta}\\left(\\alpha_{k}^{*}-\\alpha_{k}^{*}\\right)\\Big]\\Bigg|_{{\\mathbb{Z}}}^{1}\\Bigg]}\\\\ &{\\quad+\\alpha\\left[\\displaystyle\\sum_{k=1}^{\\infty}\\mathrm{te}^{-\\theta}\\left(\\mathrm{re}^{\\theta};\\alpha_{k}^{*}-\\alpha_{k}^{*}\\right)\\right]\\Bigg|_{{\\mathbb{Z}}}^{1}\\Bigg]}\\\\ &{=\\operatorname{tanh}\\Bigg[\\displaystyle\\sum_{k=1}^{\\infty}\\mathrm{te}^{-\\theta}\\left(\\mathrm{re}^{\\theta};(x,\\theta,x^{*})-\\mathrm{tf}(x,\\theta^{*})\\right)\\Bigg]\\Bigg|_{{\\mathbb{Z}}}^{1}\\Bigg]+\\alpha\\left[\\displaystyle\\bigg\\|\\sum_{k=1}^{\\infty}\\mathrm{te}^{\\theta}\\left(\\alpha_{k}^{*}-\\alpha_{k}^{*}\\right)\\right]\\Bigg|_{{\\mathbb{Z}}}^{1}\\Bigg]}\\\\ &{\\quad+\\alpha\\left[\\displaystyle\\sum_{k=1}^{\\infty}\\mathrm{te}^{-\\theta}\\left(\\mathrm{re}^{\\theta};(x,\\theta^{*})-\\mathrm{tf}(x,\\theta^{*})\\right)\\right]\\Bigg|_{{\\mathbb{Z}}}^{1}\\Bigg]}\\\\ &{\\quad+\\frac{\\alpha_{k}^{*}}{2}\\mathrm{te}^{\\theta}\\left[\\|\\frac{1}{\\Theta}\\|\\left(\\mathrm $ $\\begin{array}{r l}&{=\\Im\\left\\{\\left[\\sum_{j\\in\\mathcal{E}_{r}}^{\\mathcal{E}}\\left(\\nabla F_{r}(\\alpha_{r_{n}^{\\prime}}\\xi_{r_{n}^{\\prime}})-\\nabla f_{r}(\\alpha_{r_{n}^{\\prime}})\\right)\\right]\\bigg\\}\\Bigg|\\right\\}+\\Im\\Re\\left\\{\\sum_{j\\in\\mathcal{E}_{r}}^{\\mathcal{E}}\\left(F_{\\infty}(\\alpha_{r_{n}^{\\prime}}-G_{r_{n}^{\\prime}})\\right)\\bigg\\}\\Bigg|\\right\\}}\\\\ &{\\phantom{=}\\Im\\left\\{\\left[\\frac{1}{\\displaystyle\\sum_{j=1}^{N}}\\frac{1}{\\displaystyle\\mathcal{E}}\\left(\\mathcal{E}_{r}(\\alpha_{r_{n}^{\\prime}}\\xi_{r_{n}^{\\prime}})-\\mathcal{E}_{r}\\big)\\right]\\bigg\\}\\Bigg|\\right\\}}\\\\ &{\\phantom{=}\\Im\\frac{\\mathcal{E}}{\\displaystyle\\sum_{j\\in\\mathcal{E}_{r}}^{\\mathcal{E}}}\\left[\\left\\{\\mathcal{E}_{r}(\\xi_{r_{n}^{\\prime}}\\xi_{r_{n}^{\\prime}})-\\mathcal{E}_{r}\\big[\\mathcal{E}_{r}(\\xi_{n}^{\\prime}),\\Pi_{j}\\big]\\right\\}\\right]+\\frac{\\Im}{\\Im}\\sum_{j\\in\\mathcal{E}_{r}}^{\\mathcal{E}}\\left[\\big|\\mathcal{E}_{r}(\\alpha_{r_{n}^{\\prime}}-\\mathcal{E}_{r})\\big|^{\\mathrm{T}}\\big|\\right]\\right\\}\\Bigg|}\\\\ &{\\phantom{=}\\frac{\\Delta}{\\displaystyle\\sum_{j\\in\\mathcal{E}_{r}}^{\\mathcal{E}}}\\left[\\left\\{\\frac{1}{\\displaystyle\\mathcal{E}}\\left(\\mathcal{E}_{r}(\\alpha_{n_{n}^{\\prime}}\\xi_{r_{n}^{\\prime}})-\\mathcal{E}_{r}\\big)\\right\\}\\right]\\Bigg|\\right\\}}\\\\ &{=\\Im\\sum_{j\\in\\mathcal{E}_{r}}^{\\mathcal{E}}\\left(\\left\\{\\mathcal{E}_{r}(\\xi_{r_{n}^{\\prime}}\\xi_{r_{n}^{\\prime}})-\\mathcal{E}_{r}\\big[\\mathcal{E}_{r}(\\xi_{n_{n}^{\\prime}})\\big|^{\\mathrm{T}}\\big|\\right]\\right\\}\\right)\\left|\\right\\}+\\Im\\sum_{j\\in\\mathcal{E}_{r}}^{\\mathcal{E}}\\left[\\big|\\mathcal{E}_{r}(\\xi_{$ 7 ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{t,\\lambda+1}}&{=\\frac{\\sqrt{\\lambda}}{2}\\epsilon^{2}\\left[\\left|K_{t,\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}}\\,q(x_{t+1}^{\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}})\\,q(x_{t+1}^{\\lambda})\\right|^{2}\\right]}\\\\ &{\\quad-\\frac{\\sqrt{\\lambda}}{2}\\epsilon^{2}\\left[\\left|K_{t,\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}}\\,q(x_{t+1}^{\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}})\\,q(x_{t+1}^{\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}})\\,q(x_{t+1}^{\\lambda})\\right|^{2}\\right]}\\\\ &{\\quad-\\frac{\\sqrt{\\lambda}}{2}\\epsilon^{2}\\left[\\left|K_{t,\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}}\\,q(x_{t+1}^{\\lambda}-\\alpha_{t}^{\\lambda}(\\lambda_{t-\\frac{\\sqrt{\\lambda}}}-\\frac{\\sqrt{\\lambda}}{r_{t}})\\,q(x_{t+1}^{\\lambda}))\\right|^{2}\\right]}\\\\ &{\\quad-\\frac{\\sqrt{\\lambda}}{2}\\epsilon^{2}\\left[\\left|K_{t,\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}}\\,q(x_{t+1}^{\\lambda}-\\alpha_{t}^{\\lambda}(\\lambda_{t-\\frac{\\sqrt{\\lambda}}}-\\frac{\\sqrt{\\lambda}}{r_{t}})\\,q(x_{t+1}^{\\lambda}))\\right|^{2}\\right]}\\\\ &{\\quad-\\frac{\\sqrt{\\lambda}}{2}\\epsilon^{2}\\left[\\left|K_{t,\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}}\\,q(x_{t+1}^{\\lambda}-\\alpha_{t}^{\\lambda}(\\lambda_{t-\\frac{\\sqrt{\\lambda}}}-\\frac{\\sqrt{\\lambda}}{r_{t}})\\,q(x_{t+1}^{\\lambda}))\\right|^{2}\\right]}\\\\ &{\\quad+\\frac{\\eta^{2}}{2}\\frac{\\sqrt{\\lambda}}{2}\\epsilon^{2}\\left[\\left|K_{t,\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}}\\,q(x_{t+1}^{\\lambda}-\\frac{\\sqrt{\\lambda}}{r_{t}})\\,q(x_{t+1}^{\\lambda}-\\alpha_{t}^{\\lambda}(\\lambda_{t-\\frac{\\sqrt{\\lambda}}}-\\lambda\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(i)$ uses ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\left(g_{r,k}^{i}-\\bar{g}_{r,k}^{i}\\right)+\\sum_{j=1}^{N}q_{r}^{j}(g_{r,k}^{j}-\\bar{g}_{r,k}^{j})\\right\\|^{2}\\right]}\\\\ &{\\quad\\lesssim2\\mathbb{E}\\left[\\left\\|g_{r,k}^{i}-\\bar{g}_{r,k}^{i}\\right\\|^{2}\\middle|\\mathcal{Q}\\right]+2\\mathbb{E}\\left[\\left\\|\\sum_{j=1}^{N}q_{r}^{j}(g_{r,k}^{j}-\\bar{g}_{r,k}^{j})\\right\\|^{2}\\right]}\\\\ &{\\quad\\lesssim2\\mathbb{E}\\left[\\left\\|g_{r,k}^{i}-\\bar{g}_{r,k}^{i}\\right\\|^{2}\\middle|\\mathcal{Q}\\right]+2\\sum_{j=1}^{N}q_{r}^{j}\\mathbb{E}\\left[\\left\\|g_{r,k}^{j}-\\bar{g}_{r,k}^{j}\\right\\|^{2}\\middle|\\mathcal{Q}\\right]}\\\\ &{\\quad\\stackrel{(a)}{\\leq}36\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$(a)$ uses Equation 6, and $(i i)$ uses Young's inequality. Focusing on the second term of Equation 8: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{i=1}^{N}q_{i}^{j}\\mathbb{E}\\left[\\left\\|\\bar{g}_{r,k}^{i}-\\frac{N}{j-1}q_{r,k}^{j}\\bar{\\theta}_{r,k}^{j}\\right\\|^{2}\\right]\\Bigg|\\mathcal{Q}\\Bigg]=\\displaystyle\\sum_{i=1}^{N}q_{i}^{j}\\mathbb{E}\\left[\\left\\|\\sum_{j=1}^{N}q_{j}^{j}(\\bar{g}_{r,k}^{i}-\\bar{g}_{r,k}^{j})\\right\\|^{2}\\Bigg|\\mathcal{Q}\\right]}\\\\ &{\\displaystyle\\stackrel{(i)}{\\le}\\displaystyle\\sum_{i=1}^{N}q_{r,\\frac{\\theta}{2}}^{i}\\mathbb{E}_{\\theta}^{r}\\mathbb{E}\\Bigg[\\Big\\|\\bar{g}_{r,k}^{i}-\\bar{g}_{r,\\frac{k}{2}}^{j}\\Big\\|^{2}\\Bigg|\\mathcal{Q}\\Bigg]}\\\\ &{\\displaystyle\\le\\sum_{i=1}^{N}q_{r,\\frac{\\theta}{2}}^{i}\\mathbb{E}_{\\theta}^{r}\\mathbb{E}\\Bigg[\\Big\\|\\nabla f_{r}(\\alpha_{r,k}^{i})-\\bar{G}_{r,0}^{i}-\\nabla f_{r}(x_{r,k}^{j})+\\bar{G}_{r,0}^{j}\\Big\\|^{2}\\Big|\\mathcal{Q}\\Bigg]}\\\\ &{\\displaystyle\\le\\sum_{i=1}^{N}q_{r,\\frac{\\theta}{2}}^{i}\\pi_{r}^{i}\\Bigg(\\mathcal{Q}\\mathbb{E}\\left[\\left\\|\\nabla f_{r}(x_{r,k}^{i})-\\bar{G}_{r,\\frac{k}{2}}^{i}\\right\\|^{2}\\right|\\mathcal{Q}\\right]+2\\mathbb{E}\\left[\\left\\|\\nabla f_{j}(x_{r,k}^{i})-\\bar{G}_{r,0}^{j}\\right\\|^{2}\\Bigg|\\mathcal{Q}\\right]\\Bigg)}\\\\ &{\\displaystyle=\\frac{N}{2}\\frac{q_{r}^{i}}{q_{r}^{i}}\\mathbb{E}\\left[\\|\\nabla f_{r}(x_{r,k}^{i})-\\bar{G}_{r,0}^{i}\\|^{2}\\right|\\mathcal{Q}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ uses Jensen's inequality. Using the decomposition ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla f_{i}(\\boldsymbol{x}_{r,k}^{i})-\\bar{G}_{r_{0}}^{i}=(\\nabla f_{i}(\\boldsymbol{x}_{r,k}^{i})-\\nabla f_{i}(\\bar{\\boldsymbol{x}}_{r,k}))+(\\nabla f_{i}(\\bar{\\boldsymbol{x}}_{r,k})-\\nabla f_{i}(\\bar{\\boldsymbol{x}}_{r_{0}}))+(\\nabla f_{i}(\\bar{\\boldsymbol{x}}_{r_{0}})-\\bar{G}_{r_{0}}^{i}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{N}c_{t}^{\\top}\\left[\\left[\\phi_{t}^{T},\\frac{\\sum_{t=1}^{N}\\phi_{t}^{T}(t,s)}{\\phi_{t}^{T}}\\right]^{\\frac{1}{N}}\\right]\\left[\\phi_{t}^{T}\\right]}\\\\ &{\\displaystyle\\leq\\frac{1}{2}c_{t}^{\\top}\\left(\\operatorname{Lem}\\left[\\phi_{t}^{T}\\big(x_{t},\\phi_{t}^{T}\\big)-\\operatorname{v}^{T}(x_{t},s)\\big]^{\\frac{1}{N}}\\right)\\left[\\phi_{t}^{T}\\right]+\\frac{\\lambda}{2}\\left[\\nabla f_{t}^{\\top}(x_{t},s)-\\nabla f_{t}(x,\\phi_{t}^{T})\\right]^{\\frac{1}{N}}\\right]}\\\\ &{\\displaystyle~+\\frac{\\lambda}{2}\\left[\\big[\\phi_{t}^{T}\\big(x_{t},\\phi_{t}^{T})-\\phi_{t}^{T}\\big]^{\\frac{1}{N}}\\right]}\\\\ &{\\displaystyle\\leq12\\nu\\frac{\\sum_{t=1}^{N}\\phi_{t}^{T}\\big(x_{t},s-\\xi_{t}\\big)^{T}\\big(\\frac{1}{\\beta}\\big)}{\\phi_{t}^{T}}+12\\nu\\frac{\\sum_{t=1}^{N}\\phi_{t}^{T}}{\\sum_{t=1}^{N}\\phi_{t}^{T}}\\left(\\operatorname{Lem}\\left[\\phi_{t}^{T},x_{t},s\\right]^{\\frac{1}{N}}\\right)}\\\\ &{\\displaystyle~+\\frac{1}{2}\\sum_{t=1}^{N}\\xi_{t}^{\\top}\\left[\\nabla f_{t}^{\\top}(x_{t},s)-\\phi_{t}^{T}\\big]^{\\frac{1}{N}}\\right]}\\\\ &{\\displaystyle=12\\nu\\frac{\\sum_{t=1}^{N}\\phi_{t}^{T}(\\frac{1}{\\beta})}{\\phi_{t}^{T}}+12\\nu\\frac{\\sum_{t=1}^{N}\\phi_{t}^{T}}{\\sum_{t=1}^{N}\\phi_{t}^{T}}\\left[\\left[\\phi_{t}^{T}(x_{t},\\phi_{t}^{T})-\\frac{1}{\\sqrt{\\xi_{t}}}\\right]^{\\frac{1}{N}}\\right]+\\frac{1}{2}\\nu\\frac{\\sum_{t=1}^{N}\\xi_{t}^ \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Pz i=1 s=ro-Pk=0 N \u2264 12L2Dr,k + 12L\u00b2Mr, + 12L\u00b2 qrSro: i=1 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathsf{\\sum}_{r,k+1}\\le\\left(1+\\frac{1}{\\lambda_{1}}\\right)D_{r,k}+12\\eta^{2}L^{2}(1+\\lambda_{1})\\left(D_{r,k}+M_{r,k}+\\displaystyle\\sum_{i=1}^{N}q_{r}^{i}S_{r_{0}}^{i}\\right)+36\\eta^{2}\\sigma^{2}}\\\\ {\\displaystyle\\le\\left(1+\\frac{1}{\\lambda_{1}}+12\\eta^{2}L^{2}(1+\\lambda_{1})\\right)D_{r,k}+12\\eta^{2}L^{2}(1+\\lambda_{1})\\left(M_{r,k}+\\displaystyle\\sum_{i=1}^{N}q_{r}^{i}S_{r_{0}}^{i}\\right)+36\\eta^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now choose $\\lambda_{1}=2I$ , so that $1\\leq\\frac{\\lambda_{1}}{2}$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n12\\eta^{2}L^{2}(1+\\lambda_{1})\\leq18\\eta^{2}L^{2}\\lambda_{1}\\leq36\\eta^{2}L^{2}I\\leq\\frac{1}{2I},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used the condition $\\begin{array}{r}{\\eta\\le\\frac{1}{60L I P}}\\end{array}$ This yields the following recursive boudon $D_{r,k+1}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{r,k+1}\\leq\\left(1+\\frac{1}{I}\\right)D_{r,k}+18\\eta^{2}L^{2}I M_{r,k}+18\\eta^{2}L^{2}I\\sum_{i=1}^{N}q_{r}^{i}S_{r_{0}}^{i}+36\\eta^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "One-step recursive bound for $M_{r,k}$ For any $k\\geq0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{M}_{+,\\pm}\\mathbf{i}=}&{\\mathbb{E}\\left[\\left\\|\\sum_{i=0}^{N}c\\mathbf{i}_{x,i}^{\\star}-\\boldsymbol{x}_{x,i}\\right\\|_{\\infty}^{\\frac{1}{N}}\\right\\|\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N}c\\mathbf{i}_{x,i}^{\\star}-\\boldsymbol{x}_{x,0}-\\frac{\\boldsymbol{x}_{i}^{\\star}}{w_{0}^{\\star}+w_{\\star,i}^{\\star}}\\right\\|\\right]\\Bigg|\\Bigg|\\frac{1}{N}}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N}c\\mathbf{i}_{x,i}^{\\star}-\\boldsymbol{x}_{x,0}-\\frac{\\boldsymbol{x}_{i}^{\\star}}{w_{0}^{\\star}+w_{\\star,i}^{\\star}}\\right\\|\\right]\\Bigg|\\Bigg|+\\eta^{2}\\mathbb{E}\\Bigg[\\left\\|\\sum_{i=1}^{N}c\\mathbf{i}_{x,i}^{\\star}-\\boldsymbol{x}_{x,i}^{\\star}\\right\\|\\Bigg|\\Bigg|\\Bigg]\\Bigg|}\\\\ &{\\overset{(a)}{\\leq}\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N}c\\mathbf{i}_{x,i}^{\\star}-\\boldsymbol{x}_{x,i}-\\frac{\\boldsymbol{x}_{i}^{\\star}}{w_{0}^{\\star}+w_{\\star,i}^{\\star}}\\right\\|\\right]\\Bigg|\\Bigg|+\\eta^{2}\\mathbb{E}\\Bigg[\\eta^{2}\\kappa_{x,i}^{\\star}}\\\\ &{\\overset{(b)}{\\leq}\\left(1+\\frac{1}{\\Delta_{y}}\\right)\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N}c\\mathbf{i}_{x,i}^{\\star}-\\boldsymbol{x}_{x,i}^{\\star}\\right\\|\\right]\\Bigg|\\Bigg|\\Bigg]+\\eta^{2}(1+\\lambda_{y})\\mathbb{E}\\Bigg[\\left\\|\\sum_{i=1}^{N}c\\mathbf{i}_{x,i}^{\\star}\\right\\|\\Bigg|\\Bigg|\\Bigg|\\Bigg]\\Bigg|+\\eta^{2}\\eta^{2}\\kappa_{y}}\\\\ &{\\leq\\left(1+\\frac{1}{\\Delta_{y}}\\right)M_{+,+}+\\eta^{2}(1+\\lambda_{y})\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N}c\\mathbf{i}_{x,i}^{\\star}\\right\\|\\right]\\Bigg|\\Bigg|\\leq\\eta^{2}\\kappa_{y}\\Bigg \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(i)$ uses Equation 7 and $(i i)$ uses Young's inequality. Focusing on the second term in Equation 10: ", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad\\forall t\\leq\\Gamma^{\\prime\\prime}(\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime})}\\\\ &{\\quad+\\tau^{\\prime}(\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime},\\cdots,\\sigma_{\\tau^{\\prime}})+(\\sigma_{\\eta}-\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime\\prime}|)+\\tau^{\\prime}(\\tau^{\\prime}|\\tau^{\\prime\\prime}|)}\\\\ &{\\quad+\\tau^{\\prime}(\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime\\prime}|\\tau^{\\prime})-\\sigma_{\\eta}^{\\prime}(\\tau^{\\prime}|\\tau^{\\prime\\prime}|)+\\tau^{\\prime}(\\sigma_{\\eta}-\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|)}\\\\ &{\\quad+\\frac{\\epsilon}{2}\\sum_{i=1}^{N}\\mathbb{E}\\left[\\left(\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime},\\tau^{\\prime}|\\tau^{\\prime\\prime}|\\tau^{\\prime}\\right)\\right]\\Big(\\tau^{\\prime}\\Big)+\\frac{\\epsilon}{2}\\sum_{i=1}^{N}\\mathbb{E}\\left[\\left(\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime},\\tau^{\\prime}|\\tau^{\\prime}\\right)-\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}\\right]}\\\\ &{\\quad+\\frac{\\epsilon}{2}\\sum_{i=1}^{N}\\mathbb{E}\\left[\\left(\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime\\prime}|\\tau^{\\prime},\\sigma_{\\eta}^{\\prime}\\right)-\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}\\right]+\\mathbb{E}\\left[\\left(\\tau^{\\prime}|\\tau^{\\prime}-\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\right)\\Big|\\tau^{\\prime}\\right]+\\mathbb{E}\\left[\\left(\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}|\\right)\\Big|\\tau^{\\prime}\\right]}\\\\ &{\\quad+\\tau^{\\prime}\\frac{\\epsilon}{2}\\sum_{i=1}^{N}\\mathbb{E}\\left[\\left(\\tau^{\\prime}|\\tau^{\\prime\\prime}|\\tau^{\\prime},\\sigma_{\\eta}-\\tau^{\\prime}|\\tau^{\\prime}|\\tau^{\\prime}\\right)+\\tau^ $ )|] ", "page_idx": 22}, {"type": "text", "text": "Plugging back into Equation 10: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle M_{r,k+1}\\leq\\left(1+\\frac{1}{\\lambda_{2}}\\right)M_{r,k}+5\\eta^{2}L^{2}(1+\\lambda_{2})\\left(D_{r,k}+M_{r,k}+\\displaystyle\\sum_{i=1}^{N}\\left(q_{r}^{i}+\\frac{1}{N}\\right)S_{r_{0}}^{i}\\right)}\\\\ {\\displaystyle\\quad\\qquad+\\,5\\eta^{2}(1+\\lambda_{2})\\mathbb{E}\\left[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}\\bigg|\\mathcal{Q}\\right]+9\\eta^{2}\\rho^{2}\\sigma^{2}}\\\\ {\\displaystyle\\leq\\left(1+\\frac{1}{\\lambda_{2}}+5\\eta^{2}L^{2}(1+\\lambda_{2})\\right)M_{r,k}+5\\eta^{2}L^{2}(1+\\lambda_{2})\\left(D_{r,k}+\\displaystyle\\sum_{i=1}^{N}\\left(q_{r}^{i}+\\frac{1}{N}\\right)S_{r_{0}}^{i}\\right)}\\\\ {\\displaystyle\\qquad+\\,5\\eta^{2}(1+\\lambda_{2})\\mathbb{E}\\left[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}\\bigg|\\mathcal{Q}\\right]+9\\eta^{2}\\rho^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now choose $\\lambda_{2}=2I P$ , so that $\\begin{array}{r}{1\\leq\\frac{\\lambda_{2}}{2}}\\end{array}$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n5\\eta^{2}L^{2}(1+\\lambda_{2})\\le\\frac{15}{2}\\eta^{2}L^{2}\\lambda_{2}\\le15\\eta^{2}L^{2}I P\\le15L^{2}I P\\frac{1}{72L^{2}I^{2}P^{2}}\\le\\frac{1}{2I P},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the condition $\\begin{array}{r}{\\eta\\le\\frac{1}{60L I P}}\\end{array}$ This yields the follwing recursive boundon $M_{r,k+1}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{M_{r,k+1}\\leq\\left(1+\\displaystyle\\frac{1}{I P}\\right)M_{r,k}+15\\eta^{2}L^{2}I P D_{r,k}+15\\eta^{2}L^{2}I P\\sum_{i=1}^{N}\\left(q_{r}^{i}+\\frac{1}{N}\\right)S_{r_{0}}^{i}}}\\\\ {{\\qquad\\qquad+\\ 15\\eta^{2}I P\\mathbb{E}\\left[\\left\\|\\nabla f(\\bar{x}_{r_{0}})\\right\\|^{2}\\right|\\mathcal{Q}\\right]+9\\eta^{2}\\rho^{2}\\sigma^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that the same argument can be used to bound $M_{r,0}$ in terms of $M_{r-1,I-1}$ with the same recurrence relation. ", "page_idx": 23}, {"type": "text", "text": "Unrolling the recursion  For $0\\leq t\\leq I P$ , let $r_{t}=r_{0}+\\lfloor t/I\\rfloor$ and $k_{t}=t-\\lfloor t/I\\rfloor*I$ . Then let $d_{t}=D_{r_{t},k_{t}}$ and $m_{t}=M_{r_{t},k_{t}}$ . Also, let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle q_{1}=18\\eta^{2}L^{2}I}\\\\ {\\displaystyle q_{2}=15\\eta^{2}L^{2}I P}\\\\ {\\displaystyle a_{r}=18\\eta^{2}L^{2}I\\sum_{i=1}^{N}q_{r}^{i}S_{r_{0}}^{i}+36\\eta^{2}\\sigma^{2}}\\\\ {\\displaystyle b_{r}=15\\eta^{2}L^{2}I P\\sum_{i=1}^{N}\\left(q_{r}^{i}+\\frac{1}{N}\\right)S_{r_{0}}^{i}+15\\eta^{2}I P\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\bar{\\boldsymbol{x}}_{r_{0}})\\right\\Vert^{2}\\right|\\mathcal{Q}\\right]+9\\eta^{2}\\rho^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then Equation 9 and Equation 11 imply the following mutually recursive relation over $d_{t}$ and $m_{t}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d_{t}=\\left\\{\\begin{array}{l l}{{\\displaystyle0}}&{{k_{t}=0}}\\\\ {{\\displaystyle\\left(1+\\frac{1}{I}\\right)d_{t-1}+q_{1}m_{t-1}+a_{r_{t-1}}\\quad\\mathrm{otherwise}}}\\end{array}\\right.}}\\\\ {{m_{t}=\\left(1+\\frac{1}{I P}\\right)m_{t-1}+q_{2}d_{t-1}+b_{r_{t-1}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $d_{0}=m_{0}=0$ . We will unroll this recurrence, showing a bound for $m_{t}$ and $d_{t}$ in terms of the following quantities. For any $0\\leq t\\leq I P-1$ and $0\\leq s\\leq r_{t}$ , let $j(s,t)=\\operatorname*{min}\\{I,t-s I\\}$ and $\\ell(s,t)=\\operatorname*{max}\\{0,t-(s+1)\\bar{I}\\}$ . For any $s,t$ if $k_{t+1}=0$ ,define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha_{s,t+1}=0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Otherwise, define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\alpha_{s,t+1}=\\alpha_{s,t}\\left(1+\\displaystyle\\frac{1}{I}\\right)+q_{1}P\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{j(s,t)}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{\\ell(s,t)}}\\\\ {+\\displaystyle q_{1}q_{2}\\sum_{i=0}^{t-s I-2}\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{i}\\alpha_{s,t-1-i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Also, define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\beta_{s,t+1}=\\beta_{s,t}\\left(1+\\frac{1}{I P}\\right)+1\\left\\{s=r_{t}\\right\\}\\frac{q_{2}}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}}-1\\right)+q_{1}q_{2}\\sum_{i=0}^{k_{t}-2}\\left(1+\\frac{1}{I}\\right)^{i}\\beta_{s,t-1-i}}\\\\ {\\displaystyle\\psi_{s,t+1}=\\frac{1}{P}\\sum_{i=0}^{t-s I-1}\\left(1+\\frac{1}{I P}\\right)^{i}\\alpha_{s,t-i}}\\\\ {\\displaystyle\\phi_{s,t+1}=P\\sum_{i=0}^{k_{t}-1}\\left(1+\\frac{1}{I}\\right)^{i}\\beta_{s,t-i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "under the initial conditions ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{s,k I}=0\\mathrm{~for~all~}s\\leq P}\\\\ &{\\alpha_{s,k I}=0\\mathrm{~for~all~}s\\leq P}\\\\ &{\\psi_{s,s I}=0\\mathrm{~for~all~}s\\leq P}\\\\ &{\\beta_{s,s I}=0\\mathrm{~for~all~}s\\leq P.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we can unroll the recurrence in Equation 12, by proving the following statements by induction oOn $t$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle d_{t}\\leq\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}}-1\\right)a_{r_{t}}I+q_{1}I\\sum_{s=0}^{r_{t}}\\phi_{s,t}a_{s}+I\\sum_{s=0}^{r_{t}}\\alpha_{s,t}b_{s}}\\\\ {\\displaystyle m_{t}\\leq I P\\sum_{s=0}^{r_{t}}\\left(\\left(\\left(1+\\frac{1}{I P}\\right)^{j(s,t)}-1\\right)\\left(1+\\frac{1}{I P}\\right)^{\\ell(s,t)}+q_{2}\\psi_{s,t}\\right)b_{s}+I P\\sum_{s=0}^{r_{t}}\\beta_{s,t}a_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Equation 13 and Equation 14 hold for the base case $t=0$ , since $d_{0}=m_{0}=0$ . Now suppose that Equation 13 and Equation 14 hold for some $t\\leq I P$ , and we will show that they hold for $t+1$ We consider two cases: $k_{t+1}\\neq0$ and $k_{t+1}=0$ ", "page_idx": 24}, {"type": "text", "text": "In the first case, $r_{t+1}=r_{t}$ , i.e, step $t+1$ is in the same round as step $t$ ,and $k_{t+1}=k_{t}+1$ Using Equation 12 together with the inductive hypothesis: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{t_{i+1}\\leq\\left(1+\\frac{1}{I}\\right)\\left(\\left(\\left(1+\\frac{1}{I}\\right)^{k_{i}}-1\\right)\\alpha_{r}I+q_{i}I\\sum_{s=0}^{r}\\phi_{s,t}\\alpha_{s}I_{s}\\right)}\\qquad\\stackrel{r_{k}}{\\leq}\\alpha_{s}I_{s}\\beta_{s}\\right)}\\\\ &{\\quad+\\eta_{i}\\left(I P\\sum_{s=0}^{\\widetilde{\\gamma}}\\left(\\left(\\left(1+\\frac{1}{I P}\\right)^{j(s,t)}-1\\right)\\left(1+\\frac{1}{I P}\\right)^{\\epsilon(s,t)}+q_{i}\\phi_{s,t}\\right)b_{s}+I P\\sum_{s=0}^{\\widetilde{\\gamma}}\\beta_{s,t}\\alpha_{s}\\right)+}\\\\ &{\\leq\\left(\\left(1+\\frac{1}{I}\\right)^{k_{i}+1}-\\left(1+\\frac{1}{I}\\right)+\\frac{1}{I}\\right)\\alpha_{r}I+q_{i}I\\sum_{s=0}^{\\widetilde{\\gamma}}\\left(\\left(1+\\frac{1}{I}\\right)\\phi_{s,t}+P\\beta_{s,t}\\right)\\alpha_{s}}\\\\ &{\\quad+I\\sum_{s=0}^{\\widetilde{\\gamma}}\\left(\\left(1+\\frac{1}{I}\\right)\\alpha_{s,t}+q_{i}P\\left(\\left(1+\\frac{1}{I P}\\right)^{j(s,t)}-1\\right)\\left(1+\\frac{1}{I P}\\right)^{j(s,t)}+q_{i}q_{i}P\\psi_{s,t}\\right)b_{s}\\right)}\\\\ &{\\stackrel{(i)}{\\leq}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{i}+1}-1\\right)\\alpha_{r}I+q_{i}I\\sum_{s=0}^{\\widetilde{\\gamma}}\\phi_{s,t+1}\\alpha_{s}I\\sum_{s=0}^{\\widetilde{\\gamma}}\\alpha_{s,t+1}b_{s}}\\\\ &{\\stackrel{(i i)}{\\leq}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{i}+1}-1\\right)\\alpha_{r+1}I+q_{i}I\\sum_{s=0}^{\\widetilde{\\gamma}}\\phi_{s,t+1}\\alpha_{s}+I\\sum_{s=0}^{\\widetilde{\\gamma}}\\alpha_{s,t+1}b_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(i)$ uses the fact that $\\begin{array}{r}{\\phi_{s,t+1}=\\left(1+\\frac{1}{I}\\right)\\phi_{s,t}+P\\beta_{s,t}}\\end{array}$ and the definitions of $\\alpha_{s,t+1},\\,\\psi_{s,t}$ and $(i i)$ uses $k_{t+1}=k_{t}+1$ and $r_{t+1}=r_{t}$ . Similarly, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon\\leq\\left(1+\\frac{1}{\\varepsilon^{2}}\\right)\\left(l^{2}\\frac{\\gamma}{\\varepsilon^{2}}\\right)}&{\\leq\\frac{\\gamma}{\\varepsilon^{3}}\\left(\\left(1+\\frac{1}{\\gamma^{3}}\\right)^{l/2-1}\\right)+\\bigg(1+\\frac{1}{\\gamma^{2}}\\bigg)^{l/2-1}+\\eta\\psi_{0}\\bigg)_{\\varepsilon}\\bigg\\lambda_{1}+l\\frac{\\gamma}{\\gamma^{3}}\\frac{\\gamma}{\\varepsilon^{3}},}\\\\ &{\\quad+\\varepsilon\\left(\\left(1+\\frac{1}{\\gamma^{3}}\\right)^{l/2-1}\\right)\\varepsilon,\\quad l^{2}+l\\frac{\\gamma}{\\gamma^{4}}\\frac{\\gamma}{\\varepsilon^{6}}\\alpha_{1}+l\\frac{\\gamma}{\\gamma^{2}}\\sum_{\\theta\\in\\mathcal{S}_{0}}\\beta_{\\varepsilon}\\bigg)_{\\varepsilon}+l\\gamma\\frac{\\gamma}{\\varepsilon^{3}}}\\\\ &{\\leq L\\frac{\\gamma}{\\varepsilon^{2}}\\bigg(\\left(1+\\frac{1}{\\gamma^{3}}\\right)^{l/2-1}\\right)-\\bigg(1+\\frac{1}{\\gamma^{3}}\\bigg)^{l/2+1}}\\\\ &{\\quad+\\varepsilon\\left(1-\\frac{\\gamma}{\\varepsilon^{2}}\\right)\\frac{\\gamma}{\\varepsilon^{4}}+\\bigg(\\left(1+\\frac{1}{\\gamma^{2}}\\right)^{l/2-1}\\varepsilon-\\frac{1}{\\gamma}\\bigg)\\bigg),}\\\\ &{\\quad+l\\frac{\\gamma}{\\varepsilon^{3}}\\bigg\\varepsilon\\bigg(\\left(1+\\frac{1}{\\gamma^{3}}\\right)^{l/2-1}\\varepsilon+1\\left(-\\frac{1}{\\gamma}\\right)^{l}\\bigg(\\left(1+\\frac{1}{\\gamma^{3}}\\right)^{l}-1\\bigg)+\\frac{\\eta\\psi_{0}}{\\varepsilon^{2}}\\alpha_{1}\\bigg)_{\\varepsilon},}\\\\ &{\\geq L\\frac{\\gamma}{\\varepsilon^{2}}\\bigg\\varepsilon\\bigg(\\left(1+\\frac{1}{\\gamma^{3}}\\right)^{l/2-1}\\bigg)-\\bigg(1+\\frac{1}{\\gamma^{3}}\\bigg)^{l/2+1}+\\frac{\\varepsilon}{9}\\bigg(\\left(1+\\frac{1}{\\gamma^{2}}\\right)^{l}\\bigg)\\exp\\left(\\varepsilon+\\frac{1}{\\gamma^{6}}\\right)\\bigg),}\\\\ &{\\quad+l\\frac{\\gamma}{\\varepsilon^{2}}\\sum_{\\theta\\in\\mathcal{\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(i)$ uses the fact that for $s<r_{t}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(\\left(1+\\frac{1}{I P}\\right)^{j(s,t)}-1\\right)\\left(1+\\frac{1}{I P}\\right)^{\\ell(s,t)+1}+1\\left\\{s=r_{t}\\right\\}\\frac{1}{I P}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}\\left(\\left(1+\\frac{1}{I P}\\right)^{I}-1\\right)\\left(1+\\frac{1}{I P}\\right)^{t+1-(s+1)I}}\\\\ {\\displaystyle=\\left(\\left(1+\\frac{1}{I P}\\right)^{j(s,t+1)}-1\\right)\\left(1+\\frac{1}{I P}\\right)^{\\ell(s,t+1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and for $s=r_{t}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{j(s,t)}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{\\ell(s,t)+1}+1\\left\\{s=r_{t}\\right\\}\\displaystyle\\frac{1}{I P}}\\\\ &{\\quad=\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s t}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)+\\displaystyle\\frac{1}{I P}}\\\\ &{\\quad=\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t+1-s t}-1\\right)}\\\\ &{\\quad=\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{j(s,t+1)}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{\\ell(s,t+1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$(i i)$ uses the fact that $\\begin{array}{r}{\\psi_{s,t+1}=\\left(1+\\frac{1}{I P}\\right)\\psi_{s,t}+\\frac{1}{P}\\alpha_{s,t}}\\end{array}$ and the definitions of $\\beta_{s,t+1},\\,\\phi_{s,t}$ and $(i i i)$ uses $k_{t+1}=k_{t}+1$ and $r_{t+1}=r_{t}$ . This completes the inductive step for the first case $(k_{t+1}\\neq0)$ In the second case (i.e., $k_{t+1}=0]$ 0, Equation 13 must hold for $t+1$ , since $d_{t+1}=0$ . So it only remains to show Equation 14 holds for $t+1$ . Note that $r_{t}=r_{t+1}-1$ . As in the first case, we can use Equation 12 together with the inductive hypothesis to obtain: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n_{t+1}\\leq I P\\displaystyle\\sum_{s=0}^{r_{t}}\\left(\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{j(s,t+1)}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{\\ell(s,t+1)}+q_{2}\\psi_{s,t+1}\\right)b_{s}+I P\\displaystyle\\sum_{s=0}^{r_{t}}\\phi_{s,t+1}a_{s}}}\\\\ {{\\leq I P\\displaystyle\\sum_{s=0}^{r_{t+1}}\\left(\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{j(s,t+1)}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{\\ell(s,t+1)}+q_{2}\\psi_{s,t+1}\\right)b_{s}+I P\\displaystyle\\sum_{s=0}^{r_{t+1}}\\phi_{s,t+1}a_{s}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second line uses the fact that the $r_{t+1}$ -st element of both sums is $0$ since $j(r_{t+1},t+1)=0$ $\\psi_{r_{t+1},t+1}=0$ and $\\phi_{r_{t+1},t+1}=0$ . This completes the inductive step for both cases, and proves Equation 13 and Equation 14. ", "page_idx": 26}, {"type": "text", "text": "We can now bound $\\alpha_{s,t}$ and $\\beta_{s,t}$ separately by induction on $t$ . First, for any $s\\leq P-1$ and $t$ with $s I\\leq t\\leq I P$ , we claim that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\alpha_{s,t}\\leq36q_{1}I P,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which we will show by induction on $t$ . Let $s\\leq P-1$ be given. For the base case, Equation 15 holds when $t=s I$ since $\\alpha_{s,s I}=0$ . Now suppose that it holds for all $t^{\\prime}\\leq t$ .Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{m_{1}}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{m_{2}}}\\\\ {~~~\\leq\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s I}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{0}}\\\\ {~~~\\leq\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s I}-1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle q_{1}q_{2}\\sum_{i=0}^{t-s I-2}\\left(1+\\frac{1}{I P}\\right)^{i}\\alpha_{s,t-1-i}\\leq36q_{1}^{2}q_{2}I P\\sum_{i=0}^{t-s I-2}\\left(1+\\frac{1}{I P}\\right)^{i}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=36q_{1}^{2}q_{2}I^{2}P^{2}\\left(\\left(1+\\frac{1}{I P}\\right)^{t-s I-1}-1\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\leq q_{1}P\\left(\\left(1+\\frac{1}{I P}\\right)^{t-s I}-1\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last line uses the definition of $q_{1}$ and $q_{2}$ together with the condition $\\begin{array}{r}{\\eta\\le\\frac{1}{60L I P}}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n36q_{1}q_{2}I^{2}P=36\\cdot270\\eta^{4}L^{4}I^{4}P^{2}\\leq{\\frac{36\\cdot270}{60^{4}}}L^{4}I^{4}P^{2}{\\frac{1}{L^{4}I^{4}P^{4}}}\\leq{\\frac{1}{P^{2}}}\\leq1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging Equation 16 and Equation 17 into the definition of $\\alpha_{s,t+1}$ yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{s,t+1}\\leq\\alpha_{s,t}\\left(1+\\displaystyle\\frac{1}{I}\\right)+2q_{1}P\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s-t}-1\\right)}&{}\\\\ {\\overset{(i)}{\\geq}2q_{1}P\\displaystyle\\frac{k_{\\perp}}{i=0}\\left(1+\\displaystyle\\frac{1}{I}\\right)^{t}\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s-t-1}-1\\right)}&{}\\\\ &{\\leq2q_{1}P\\left(1+\\displaystyle\\frac{1}{I}\\right)^{t}\\displaystyle\\sum_{i=0}^{k_{\\perp}}\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s-t-1}-1\\right)}\\\\ &{\\leq6q_{1}P\\displaystyle\\sum_{i=0}^{k_{\\perp}}\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s-t-1}\\leq6q_{1}P\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{(t-s-t)/\\frac{k_{\\perp}}{i=0}\\left(1+\\displaystyle\\frac{1}{I P}\\right)}}\\\\ &{\\leq18q_{1}P\\displaystyle\\sum_{i=0}^{I-1}\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{i_{\\perp}}\\leq18q_{1}\\displaystyle\\sum_{i=0}^{I P-1}\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{i}}\\\\ &{\\leq18q_{1}P\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{I P}-1\\right)\\leq8q_{1}I P,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(i)$ unrolls the recurrence on the fist line until $\\alpha_{s,r_{t}I}=0$ $(i i)$ uses $\\begin{array}{r}{P\\sum_{i=0}^{I-1}\\left(1+\\frac{1}{I P}\\right)^{i}\\leq}\\end{array}$ $\\begin{array}{r}{\\sum_{i=0}^{I P-1}\\left(1+\\frac{1}{I P}\\right)^{i}}\\end{array}$ since $\\textstyle\\left(1+{\\frac{1}{I P}}\\right)^{i}$ $i$ $\\begin{array}{r}{\\left(1+\\frac{1}{x}\\right)^{x}<e\\leq3}\\end{array}$ This completes the induction and proves Equation 15. ", "page_idx": 27}, {"type": "text", "text": "We will similarly prove the statement ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\beta_{s,t}\\leq12{\\frac{q_{2}I}{P}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $s\\leq P-1$ and $t$ with $s I\\leq t\\leq I P$ by induction on $t$ . Let $s\\leq P-1$ be given. For the base case, Equation 15 holds when $t=s I$ since $\\beta_{s,s I}=0$ . Now suppose that it holds for all $t^{\\prime}\\leq t$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{q_{1}q_{2}\\sum_{i=0}^{k-2}\\left(1+\\frac{1}{I}\\right)^{i}\\beta_{s,t-1-i}\\leq12\\frac{q_{1}q_{2}^{2}I^{2}}{P}\\sum_{i=0}^{k-2}\\left(1+\\frac{1}{I}\\right)^{i}}}\\\\ &{}&{=12\\frac{q_{1}q_{2}^{2}I^{2}}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}-1}-1\\right)}\\\\ &{}&{\\leq12\\frac{q_{1}q_{2}^{2}I^{2}}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}}-1\\right)}\\\\ &{}&{\\leq\\frac{q_{2}}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}}-1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last line uses the definition of q1 and q2 together with the condition   6oLip: ", "page_idx": 27}, {"type": "equation", "text": "$$\n12q_{1}q_{2}I^{2}\\le12\\cdot270\\eta^{4}L^{4}I^{4}P\\le\\frac{12\\cdot270}{60^{4}}L^{4}I^{4}P\\frac{1}{L^{4}I^{4}P^{4}}\\le\\frac{1}{P^{3}}\\le1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now consider two cases: $t<(s+1)I$ and $t\\ge(s+1)I$ . In the first case, we have $s=r_{t}$ , and plugging Equation 20 into the definition of $\\beta_{s,t+1}$ yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{s,t+1}\\leq\\beta_{s,t}\\left(1+\\frac{1}{I P}\\right)+2\\frac{q_{2}^{2}}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}}-1\\right)}\\\\ &{\\overset{(i)}{\\leq}2\\frac{q_{2}^{2}}{P}\\displaystyle\\sum_{i=0}^{k_{t}}\\left(1+\\frac{1}{I P}\\right)^{i}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}-i}-1\\right)}\\\\ &{\\leq2\\frac{q_{2}^{2}}{P}\\left(1+\\frac{1}{I P}\\right)^{k_{t}}\\displaystyle\\sum_{i=0}^{k_{t}}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}-i}-1\\right)}\\\\ &{\\leq2\\frac{q_{2}^{2}}{P}\\left(1+\\frac{1}{I P}\\right)^{I}\\displaystyle\\sum_{i=0}^{k_{t}}\\left(1+\\frac{1}{I}\\right)^{i}}\\\\ &{\\leq6\\frac{q_{2}^{2}I}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}+1}-1\\right)\\leq6\\frac{q_{2}I}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{I}-1\\right)}\\\\ &{\\leq12\\frac{q_{2}I}{P},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(i)$ unrolls the recurrence on the first line until $\\beta_{s,s I}=0$ and we repeatedly used $\\left(1+{\\frac{1}{x}}\\right)^{x}<$ $e\\leq3$ In the second case, we have $s\\,>\\,r_{t}$ , so plugging Equation 19 into the defnition of $\\beta_{s,t+1}$ ", "page_idx": 27}, {"type": "text", "text": "yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{s,t+1}\\leq\\beta_{s,t}\\left(1+\\displaystyle\\frac{1}{I P}\\right)+12\\frac{q!q!q^{2}I^{2}}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{k_{t}}-1\\right)}\\\\ &{\\qquad\\leq\\beta_{s,t}\\left(1+\\displaystyle\\frac{1}{I P}\\right)+12\\frac{q!q!q^{2}I^{2}}{P}\\left(\\left(1+\\frac{1}{I}\\right)^{l}-1\\right)}\\\\ &{\\qquad\\leq\\beta_{s,t}\\left(1+\\displaystyle\\frac{1}{I P}\\right)+24\\frac{q!q^{2}I^{2}}{P}}\\\\ &{\\qquad\\overset{(i)}{\\leq}24\\frac{q!q!q^{2}I^{2}}{P}\\displaystyle\\sum_{i=0}^{t-n}\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{i}}\\\\ &{\\qquad=24q!q^{2}I^{3}\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{t-s+l+1}-1\\right)\\leq24q_{1}q_{2}^{2}I^{3}\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{l P}-1\\right)}\\\\ &{\\qquad\\leq48q_{1}q_{2}^{2}I^{3}\\leq12\\frac{q!q-1}{P},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(i)$ unrolls the recurrence on the previous line until $\\beta_{s,s I}=0$ and the last inequality uses the definition of q1 and q2 together with the condition 7 oLip: ", "page_idx": 28}, {"type": "equation", "text": "$$\n48q_{1}q_{2}I^{2}=48\\cdot270\\eta^{4}L^{4}I^{4}P\\leq\\frac{48\\cdot270}{60^{4}}L^{4}I^{4}P\\frac{1}{L^{4}I^{4}P^{4}}\\leq\\frac{12}{P^{3}}\\leq\\frac{12}{P}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This completes the induction in both cases and proves Equation 18. ", "page_idx": 28}, {"type": "text", "text": "We can then use Equation 15 and Equation 18 to yield bounds for the remaining terms of Equation 13 and Equation 14 as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle q_{2}\\psi_{s,t}=\\frac{q_{2}}{P}\\sum_{i=0}^{t-s I-1}\\left(1+\\frac{1}{I P}\\right)^{i}\\alpha_{s,t-1-i}\\leq36q_{1}q_{2}I\\sum_{i=0}^{t-s I-1}\\left(1+\\frac{1}{I P}\\right)^{i}}\\\\ {\\displaystyle\\qquad\\leq36q_{1}q_{2}I\\sum_{i=0}^{I P-1}\\left(1+\\frac{1}{I P}\\right)^{i}\\leq36q_{1}q_{2}I^{2}P\\left(\\left(1+\\frac{1}{I P}\\right)^{I P}-1\\right)}\\\\ {\\displaystyle\\qquad\\leq72q_{1}q_{2}I^{2}P\\leq72\\cdot270\\eta^{4}L^{4}I^{4}P^{2}\\leq\\frac{72\\cdot270}{60^{4}}L^{4}I^{4}P^{2}\\frac{1}{L^{4}I^{4}P^{4}}\\leq\\frac{1}{P^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{q_{1}}{\\phi_{s,t}}={q_{1}}P\\displaystyle\\sum_{i=0}^{k_{t}-1}{\\left(1+\\displaystyle\\frac{1}{I}\\right)^{i}\\beta_{s,t-i}}\\leq12q_{1}q_{2}I\\sum_{i=0}^{k_{t}-1}{\\left(1+\\displaystyle\\frac{1}{I}\\right)^{i}}}\\\\ {{\\mathrm{~}=12q_{1}q_{2}I^{2}\\left(\\left(1+\\displaystyle\\frac{1}{I}\\right)^{k_{t}}-1\\right)\\leq12q_{1}q_{2}I^{2}\\left(\\left(1+\\displaystyle\\frac{1}{I}\\right)^{I}-1\\right)\\leq24q_{1}q_{2}I^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, we can plug these into Equation 13 to yield ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d_{t}\\leq\\left(\\left(1+\\displaystyle\\frac{1}{I}\\right)^{k_{t}}-1\\right)a_{r_{t}}I+24q_{1}q_{2}I^{3}\\sum_{s=0}^{r_{t}}a_{s}+36q_{1}I^{2}P\\sum_{s=0}^{r_{t}}b_{s}}}\\\\ {{\\leq2a_{r_{t}}I+24q_{1}q_{2}I^{3}\\sum_{s=0}^{r_{t}}a_{s}+36q_{1}I^{2}P\\sum_{s=0}^{r_{t}}b_{s},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and into Equation 14 ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{m_{t}\\leq I P\\displaystyle\\sum_{s=0}^{r_{t}}\\left(\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{m_{1}}-1\\right)\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{m_{2}}+\\displaystyle\\frac{1}{P^{2}}\\right)b_{s}+12q_{2}I^{2}\\displaystyle\\sum_{s=0}^{r_{t}}a_{s}}}\\\\ {{{}\\overset{(i)}{\\leq}I P\\displaystyle\\sum_{s=0}^{r_{t}}\\left(\\frac{6}{P}+\\displaystyle\\frac{1}{P^{2}}\\right)b_{s}+12q_{2}I^{2}\\displaystyle\\sum_{s=0}^{r_{t}}a_{s}}}\\\\ {{{}\\overset{(}{\\leq}7I\\displaystyle\\sum_{s=0}^{r_{t}}b_{s}+12q_{2}I^{2}\\displaystyle\\sum_{s=0}^{r_{t}}a_{s},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(i)$ uses ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{m_{1}}-1\\leq\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{I}-1=I P\\displaystyle\\sum_{i=0}^{I-1}\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{i}}\\\\ {\\qquad\\qquad\\leq I\\displaystyle\\sum_{i=0}^{I P-1}\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{i}=\\displaystyle\\frac{1}{P}\\left(\\left(1+\\displaystyle\\frac{1}{I P}\\right)^{I P}-1\\right)\\leq\\displaystyle\\frac{2}{P},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(1+\\frac{1}{I P}\\right)^{m_{2}}\\leq\\left(1+\\frac{1}{I P}\\right)^{I P}\\leq3.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Or, in the original notation: ", "page_idx": 29}, {"type": "text", "text": "N NDr,k\u226421(18n\u00b2L21a,S\u3002 + 36n\u00b2a\u00b2+ 6480nL41P  (18n\u00b2L21aS\u3002 + 36n22i=1 s=ro i=1+ 648n\u00b2 L213P 15\u00b2L\u00b21P(+)s+15\u00b2P[1f(|] +9\u00b2\u00b2s=ro$\\begin{array}{r l}&{\\le8\\pi^{2}R^{1/2}\\frac{\\hat{\\mathcal{G}}}{r}\\xi\\xi_{\\eta}^{2}\\xi_{r}^{2}(K_{s}^{\\star},1+K_{s}\\omega)(R^{1/2}\\,R^{1/2})\\sqrt{\\frac{\\hat{\\mathcal{G}}}{r}}\\le\\frac{\\xi_{\\eta}}{r},}\\\\ &{\\le\\eta^{2}\\le2\\pi^{2}R^{1/2}\\eta^{-1}\\frac{\\xi_{\\eta}}{r}\\frac{\\xi_{\\eta}^{2}}{r}\\le\\left(\\xi_{\\eta}^{2}+\\frac{1}{r}\\right)\\le\\frac{\\xi_{\\eta}\\eta^{2}}{r}(K_{s}+\\eta^{2}(K_{s}-\\eta)\\mu^{2})\\le\\left(\\eta^{2}\\xi_{\\eta}\\right)^{2}\\le\\left(\\eta^{2}\\right)}\\\\ &{\\quad+(2\\eta^{2})I^{-1}+8\\pi\\xi_{\\eta}(K_{s}-\\eta)\\xi_{\\eta}^{2}\\xi_{r}^{2}(K_{s}+\\eta^{2}(K_{s}-\\eta)\\mu^{2}),}\\\\ &{\\overset{,,,}{\\le\\eta^{2}}R^{1/2}\\eta^{-1}\\frac{\\xi_{\\eta}^{2}}{r}\\le(K_{s}+\\eta\\omega)\\,\\xi_{\\eta}\\xi_{r}^{2}\\xi_{r}^{2}\\frac{\\xi_{\\eta}}{r}\\le\\frac{\\xi_{\\eta}}{r},}\\\\ &{\\le\\eta^{2}R^{1/2}\\eta^{-1}\\xi^{2}\\eta^{-1}\\frac{\\xi_{\\eta}\\eta^{2}}{r}\\le\\frac{\\xi_{\\eta}\\eta^{2}}{r}\\le\\left(\\xi_{\\eta}^{2}+\\frac{1}{r}\\right)\\le,}\\\\ &{\\quad+(2\\eta^{2})I^{-1}+4\\pi\\xi_{\\eta}(K_{s}+\\eta\\omega)I^{-1}\\eta^{2}\\eta^{-1}\\eta^{2}\\eta^{2}\\eta^{2}\\eta^{2}\\le\\eta^{2}2\\eta^{}I^{-1}\\eta^{2}\\le\\left(\\eta^{2}\\xi_{\\eta}\\right)^{2}\\le0}\\\\ &{=3\\eta^{2}R^{1/2}\\frac{\\xi_{\\eta}\\eta^{2}}{r}\\le\\left(5\\eta^{2}\\right)\\le\\eta^{2}\\le2\\eta^{ $ + (73m2 + 5832n 213 p2 ) + 9720m L214p [1Vf(\u00b1r0)2|] , (21)", "page_idx": 29}, {"type": "text", "text": "where $(i)$ uses the fact that the interval $\\{r_{0},\\ldots,r\\}$ is contained in the interval $\\{r_{0},\\ldots,r_{0}+P-1\\}$ and $(i i)$ uses thecondition $\\begin{array}{r}{\\eta\\le\\frac{1}{60L I P}}\\end{array}$ to simpify non-dominating tems. Also ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\eta\\sum_{k=0}^{r}\\Bigg(\\mathrm{by}\\mathcal{L}^{2}\\eta^{k}\\mathcal{L}^{2}\\eta\\frac{\\sqrt{\\alpha}}{r_{k}^{2}}\\Bigg[\\phi_{k}^{\\star}\\!+\\!\\frac{1}{N}\\kappa_{k}^{2}\\eta^{k}\\mathcal{L}^{2}\\eta^{k}\\left[\\nabla f(\\hat{x}_{k})\\right]^{2}\\Bigg|\\phi_{k}^{\\star}\\!+\\!\\eta\\phi_{k}^{\\star}\\!\\rho^{\\star}\\!\\Bigg]\\Bigg)}\\\\ &{\\quad+18\\eta^{2}\\Lambda^{2}\\eta^{3}\\gamma^{-1}\\!\\Bigg(\\!\\ln\\!\\!\\!\\frac{\\eta}{r_{k}^{2}}\\!+\\!\\frac{\\eta\\xi_{k}^{\\star}\\!-\\!2\\eta^{3}}{r_{k}^{2}}\\!+\\!\\frac{\\eta\\xi_{k}^{\\star}\\!-\\!2\\eta^{3}}{r_{k}^{2}}\\!\\Bigg)}\\\\ &{\\leq\\mathrm{bys}^{2}L^{2}\\eta^{2}\\gamma^{-1}\\!\\Bigg[\\!\\frac{\\sqrt{\\alpha}}{r_{k}^{2}}\\!+\\!(-\\frac{1}{N})\\!\\Bigg(\\!\\frac{\\sqrt{\\alpha}}{r_{k}}\\!+\\!\\frac{\\eta\\xi_{k}^{\\star}\\!-\\!2\\eta^{3}}{r_{k}^{2}}\\!\\Bigg)\\!\\Bigg]\\!\\Bigg|\\tilde{\\Phi}\\!_{-}^{\\dagger}\\eta\\!+\\!\\mathcal{L}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $(i)$ and $(i i)$ use the same operations as in Equation 21. Summing Equation 22 and Equation 21, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{r,k}+M_{r,k}\\leq36\\eta^{2}L^{2}I^{2}\\displaystyle\\sum_{i=1}^{N}q_{i}^{i}S_{r_{0}}^{i}+\\left(9720\\eta^{4}L^{4}I^{4}P^{3}+105\\eta^{2}L^{2}I^{2}P^{2}\\right)\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}S_{r_{0}}^{i}}\\\\ &{\\hphantom{()}+\\left(9753\\eta^{4}L^{4}I^{4}P^{3}+106\\eta^{2}L^{2}I^{2}P^{2}\\right)\\displaystyle\\sum_{i=1}^{N}\\bar{q}_{r_{0}}^{i}S_{r_{0}}^{i}}\\\\ &{\\hphantom{()}+\\left(73\\eta^{2}I+5832\\eta^{4}L^{2}I^{3}P^{2}\\rho^{2}+63\\eta^{2}I P\\rho^{2}+6480\\eta^{4}L^{2}I^{3}P^{2}\\right)\\sigma^{2}}\\\\ &{\\hphantom{()}+\\left(9720\\eta^{4}L^{2}I^{4}P^{3}+105\\eta^{2}I^{2}P^{2}\\right)\\mathbb{E}\\left[\\left|\\nabla f\\left(\\bar{x}_{\\tau_{0}}\\right)\\right|^{2}\\right]}\\\\ &{\\hphantom{()}\\leq36\\eta^{2}L^{2}I^{2}\\displaystyle\\sum_{i=1}^{N}q_{i}^{i}S_{r_{0}}^{i}+109\\eta^{2}L^{2}I^{2}P^{2}\\sum_{i=1}^{N}\\left(\\bar{q}_{r_{0}}^{i}+\\frac{1}{N}\\right)S_{r_{0}}^{i}}\\\\ &{\\hphantom{()}+\\left(75\\eta^{2}I+65\\eta^{2}I P\\rho^{2}\\right)\\sigma^{2}+108\\eta^{2}I^{2}P^{2}\\mathbb{E}\\left[\\left|\\nabla f\\left(\\bar{x}_{\\tau_{0}}\\right)\\right|^{2}\\right]\\sigma\\Big]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality uses $\\begin{array}{r}{\\eta\\le\\frac{1}{60L I P}}\\end{array}$ . This proves Equation 3. Equation 4 follows by summing over $r\\in\\{r_{0},\\ldots,r_{0}+P-1\\}$ and $k\\in\\{0,\\ldots,I-1\\}$ , taking total expectation, and applying the condition Ero[ar\u3002] = \u00b7 \u53e3 ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert\\bar{x}_{r_{0}+P}-\\bar{x}_{r_{0}}\\right\\Vert^{2}\\right]\\leq6\\gamma^{2}\\eta^{2}I^{2}P^{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\bar{x}_{r_{0}})\\right\\Vert^{2}\\right]+\\gamma\\eta I P\\left(5\\gamma\\eta\\rho^{2}+7\\eta^{2}L I\\right)\\sigma^{2}}\\\\ {\\quad+\\left.11\\gamma^{2}\\eta^{2}L^{2}I^{2}P^{2}\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}\\left[S_{r_{0}}^{i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By the algorithm definition, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\bar{s}_{r_{0}+P}-\\bar{x}_{r_{0}}=-\\gamma\\eta\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}\\sum_{k=0}^{I-1}q_{r}^{i}(\\nabla F_{i}(x_{r,k}^{i};\\xi_{r,k}^{i})-G_{r_{0}}^{i}+G_{r_{0}})=-\\gamma\\eta\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}\\sum_{k=0}^{I-1}q_{r}^{i}g_{r,k}^{i}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To obtain the variance of the update &ro+P - 3ro, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{r=r_{0}}^{r_{1}+P}\\sum_{i=1}^{I-1}q_{r}^{i}\\left(g_{r,k}^{i}-\\bar{g}_{r,k}^{i}\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{r=r_{0}+1}^{r_{1}+P}\\sum_{i=1}^{N-I-1}q_{r}^{i}\\left((\\nabla F_{i}(x_{r,k}^{i};\\xi_{r,k}^{i})-\\nabla f_{i}(x_{r,k}^{i}))-(G_{r_{0}}^{i}-\\bar{G}_{r_{0}}^{i})+(G_{r_{0}}-\\bar{G}_{r_{0}})\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad\\leq2\\mathbb{E}\\left[\\displaystyle\\left\\|\\sum_{r=r_{0}+1}^{r_{1}+P}\\sum_{i=0}^{I-1}q_{r}^{i}\\left(\\nabla F_{i}(x_{r,k}^{i};\\xi_{r,k}^{i})-\\nabla f_{i}(x_{r,k}^{i})\\right)\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\,2\\mathbb{E}\\left[\\displaystyle\\left\\|\\sum_{r=r_{0}+1}^{r_{1}+P}\\sum_{i=1}^{I-1}q_{r}^{i}\\left(-G_{r_{0}}^{i}+G_{r_{0}}^{i}+G_{r_{0}}-G_{r_{0}}\\right)\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can bound the two terms $A_{1}$ and $A_{2}$ separately as follows. For $A_{1}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=2\\mathbb{E}\\left[\\left\\lVert\\sum_{r=0}^{\\lfloor\\eta+P}\\frac{N\\rfloor-1}{r!}q_{r}^{i}\\left(\\nabla F_{i}(x_{r,k+\\xi}^{i},\\xi_{r,k}^{i})-\\nabla f_{i}(x_{r,k}^{i})\\right)\\right\\rVert^{2}\\right]}\\\\ &{\\quad=2\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\lVert\\sum_{r=0}^{\\lfloor\\eta+P}\\sum_{s=1}^{N\\rfloor-1}q_{s}^{i}\\left(\\nabla F_{i}(x_{r,k+\\xi}^{i},\\xi_{r,k}^{i})-\\nabla f_{i}(x_{r,k}^{i})\\right)\\right\\rVert^{2}\\right]\\right]}\\\\ &{\\quad\\stackrel{(a)}{=}\\mathbb{E}\\left[\\left\\lVert\\sum_{r=0}^{\\lfloor\\eta+P}\\sum_{s=1}^{N\\rfloor}\\mathbb{E}\\left[\\left\\lVert q_{r}^{i}\\left(\\nabla F_{i}(x_{r,k}^{i}\\xi_{r,k}^{i})-\\nabla f_{i}(x_{r,k}^{i})\\right)\\right\\rVert^{2}\\right\\rVert\\right]}\\\\ &{\\quad\\stackrel{(b)}{=}\\mathbb{E}\\left[\\sum_{r=0}^{\\lfloor\\eta+P}\\sum_{s=1}^{I-1}\\mathbb{E}\\left[\\left\\lVert q_{r}^{i}\\left(\\nabla F_{i}(x_{r,k}^{i}\\xi_{r,k}^{i})-\\nabla f_{i}(x_{r,k}^{i})\\right)\\right\\rVert^{2}\\right]\\right]}\\\\ &{\\quad=\\frac{r_{0}+P}{\\sum_{r=0}^{\\lfloor\\eta+P}\\sum_{s=0}^{N\\rfloor-1}\\mathbb{E}\\left[\\left(q_{r}^{i}\\right)^{2}\\mathbb{E}\\left[\\left\\lVert\\nabla F_{i}(x_{r,k}^{i}\\xi_{r,k}^{i})-\\nabla f_{i}(x_{r,k}^{i})\\right\\rVert^{2}\\right]\\right]}\\\\ &{\\quad\\stackrel{(a)}{=}\\sum_{r=0}^{\\lfloor\\eta+P}\\sum_{s=1}^{N\\rfloor-1}\\sum_{s=0}^{\\lfloor\\eta+\\widehat{\\xi}\\rfloor}\\left[\\left(q_{r}^{i}\\right)^{2}\\right]}\\\\ &{\\quad\\leq2P^{2}\\displaystyle\\sum_{r=0}^{\\lfloor\\eta+P}\\sum_{s=1}^{N\\rfloor-1}\\sum_{s=0}^{i}\\left[\\left(q_{r}^{i}\\right)^{2}\\right\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(i)$ uses the fact that for each $i$ \uff0c $\\left\\{q_{r}^{i}\\left(\\nabla F_{i}(\\mathbf{x}_{r,k}^{i})-\\nabla f_{i}(\\mathbf{x}_{r,k}^{i})\\right)\\right\\}_{r,k}$ is a martingale difference sequence with respect to $\\mathcal{G}$ (when conditioned on $\\mathcal{Q}$ ) and that stochastic gradient noise is independent ", "page_idx": 31}, {"type": "text", "text": "across clients. For $A_{2}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{r=\\infty}^{r+P}\\sum_{i=1}^{N-1}q_{r}^{i}\\left(-G_{\\mathrm{ro}}^{i}-\\bar{G}_{\\mathrm{ro}}^{i}+G_{\\mathrm{ro}}-\\bar{G}_{\\mathrm{ro}}\\right)}\\\\ &{\\displaystyle=-\\sum_{r=\\infty}^{r+P}\\sum_{i=1}^{N-1}q_{r}^{i}\\left(G_{\\mathrm{ro}}^{i}-\\bar{G}_{\\mathrm{ro}}^{i}\\right)+\\sum_{r=\\infty}^{r_{0}+P}\\sum_{i=1}^{N-1}\\sum_{k=0}^{I-1}q_{r}^{i}\\left(G_{\\mathrm{ro}}-\\bar{G}_{\\mathrm{ro}}\\right)}\\\\ &{\\displaystyle=-I\\sum_{r=\\infty}^{r_{0}+P}\\sum_{i=1}^{N}q_{r}^{i}\\left(G_{\\mathrm{ro}}^{i}-\\bar{G}_{\\mathrm{ro}}^{i}\\right)+I P\\left(G_{\\mathrm{ro}}-\\bar{G}_{\\mathrm{ro}}\\right)}\\\\ &{\\displaystyle=-I P\\sum_{i=1}^{N}\\left(\\frac{1}{P}\\sum_{r=r_{0}}^{r_{0}+P}q_{r}^{i}\\right)\\left(G_{\\mathrm{ro}}^{i}-\\bar{G}_{\\mathrm{ro}}^{i}\\right)+I P\\sum_{i=1}^{N}\\frac{1}{N}\\left(G_{\\mathrm{ro}}^{i}-\\bar{G}_{\\mathrm{ro}}^{i}\\right)}\\\\ &{\\displaystyle=-I P\\sum_{i=1}^{N}\\left(\\bar{q}_{\\mathrm{ro}}^{i}-\\frac{1}{N}\\right)\\left(G_{\\mathrm{ro}}^{i}-\\bar{G}_{\\mathrm{ro}}^{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}:=2^{H P E}\\left[\\left\\{E_{t}\\left(\\frac{\\mathbf{k}}{r}-\\frac{\\hat{\\mathbf{k}}}{N}\\right)^{2}(E_{t}-\\hat{\\mathbf{k}}_{t})^{2}\\right\\}\\right]}\\\\ &{:=2^{H P E}\\mathcal{E}\\Bigg[\\Bigg|\\sum_{k=0}^{K}\\left(E_{t}-\\frac{\\mathbf{k}}{r}\\right)^{2}(E_{t}-\\alpha_{k}^{*})\\Bigg|^{3}\\Bigg|\\Bigg|\\frac{\\mathbf{k}}{r}\\Bigg]}\\\\ &{\\mathbb{E}:=\\mathbb{E}:\\Bigg[\\frac{\\mathbf{k}}{r}\\Bigg]\\Bigg[\\langle E_{t}-\\frac{\\mathbf{k}}{N}\\rangle\\Big(\\langle E_{t}-\\frac{\\mathbf{k}}{r}\\rangle\\Big)\\Big|^{2}\\Bigg]}\\\\ &{:=2^{H P E}\\sum_{k=0}^{K}\\Bigg[\\langle E_{t}-\\frac{\\mathbf{k}}{N}\\rangle^{2}\\left[\\langle E_{t}-\\frac{\\mathbf{k}}{r}\\rangle\\Big|^{3}\\right]}\\\\ &{:=2^{H P E}\\sum_{k=0}^{K}\\Bigg[\\langle E_{t}-\\frac{\\mathbf{k}}{N}\\rangle^{2}\\left[\\langle E_{t}-\\frac{\\mathbf{k}}{N}\\rangle\\right]\\langle\\mathbf{k}_{t}^{*}\\rangle\\Bigg]}\\\\ &{:=2^{H P E}\\sum_{k=0}^{K}\\Bigg[\\sum_{k=0}^{K}\\Big[\\prod_{s=0}^{T}\\sum_{u=K_{s}=0}^{K-1}\\sum_{u=K_{s}=0}^{K}\\Big[\\langle E_{t}(\\mathcal{H}_{u,u}(\\zeta_{u,s})-\\Gamma_{u,u=K_{s}})\\rangle\\Big]\\Bigg|^{2}\\Bigg]}\\\\ &{:=2^{H P E}\\Bigg[\\sum_{k=0}^{K}\\Big(E_{s}-\\frac{1}{N}\\Big)^{2}\\sum_{u=K_{s}=0}^{K-1}\\sum_{u=K_{s}=0}^{K}\\Big[\\prod_{s=t-s}^{-1}\\sum_{u=K_{s}=0}^{K}\\Big(\\sum_{u=K_{s}=0}^{K}\\sum_{p=(K_{s}=0)}^{K-1}\\Big)\\Big]\\Bigg|^{3}\\Bigg]}\\\\ &{:=2^{H P E}\\sum_{k=0}^{K}\\Bigg[\\sum_{k=0}^{K}\\Big(E_{s}-\\frac{1}{N\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(i)$ uses the fact that, conditioned on $\\mathcal{Q}$ the variables $G_{r_{0}}^{i}-\\bar{G}_{r_{0}}^{i}$ depend only on stochastic gradient noise, which is independent across clients, and the last inequality uses the condition $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{i=1}^{N}\\left(v_{r_{0}}^{i}\\right)^{2}\\Lambda_{r_{0}}^{i}\\right]\\leq\\rho^{2}}\\end{array}$ . Plugging Equation 25 and Equation 26 into Equation 24 yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{r=r_{0}}^{r_{0}+P}\\sum_{i=1}^{N}\\sum_{k=0}^{I-1}q_{r}^{i}\\left(\\pmb{g}_{r,k}^{i}-\\bar{\\pmb{g}}_{r,k}^{i}\\right)\\right\\rVert^{2}\\right]\\leq4I P\\rho^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore ", "text_level": 1, "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|\\begin{array}{l}{-\\rho_{2}\\rho_{1}^{\\prime}}\\\\ {\\rho_{3}}\\\\ {\\vdots}\\\\ {\\rho_{2}+\\rho_{2}}\\end{array}\\right|\\left|\\begin{array}{l}{\\displaystyle\\sum_{l=0}^{\\infty}\\sum_{u=1}^{u}\\sum_{u=i+1}^{n}\\phi_{i,u}(\\phi_{l}^{\\star})}\\\\ {~~}\\\\ {~~}\\end{\\mathbb{E}}\\left[\\frac{\\rho_{2}}{\\rho_{3}}\\sum_{u=i+1}^{n-1}\\sum_{u=i+1}^{n-1}\\phi_{i,u}(\\phi_{l}^{\\star})\\right]+\\gamma_{2}\\rho_{3}^{\\prime}\\left[\\left|\\begin{array}{l}{\\displaystyle\\sum_{l=0}^{i+1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\phi(\\phi_{l,u}(\\phi_{l}^{\\star}))}\\\\ {\\displaystyle\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}^{B{\\ell}-1}\\sum_{u=i+1}\n$$(\u2264 5\\~2n\u00b2 12 P2E [1V f(cr\u3002)1] + 52m\u00b2 L21PE \u2211 Dr,k + Mr,k ", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{i}(x_{r,k}^{i})-\\bar{G}_{r_{0}}^{i}+\\bar{G}_{r_{0}}=\\nabla f(\\bar{x}_{r_{0}})+(\\nabla f_{i}(x_{r,k}^{i})-\\nabla f_{i}(\\bar{x}_{r,k}))+(\\nabla f_{i}(\\bar{x}_{r,k})-\\nabla f_{i}(\\bar{x}_{r_{0}}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(\\nabla f_{i}(\\bar{x}_{r_{0}})-\\bar{G}_{r_{0}}^{i}\\right)+(\\nabla f(\\bar{x}_{r_{0}})-\\bar{G}_{r_{0}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $(i i i)$ uses $\\begin{array}{r}{G_{r_{0}}=\\frac{1}{N}\\sum_{i=1}^{N}G_{r_{0}}^{i}}\\end{array}$ and $\\begin{array}{r}{\\mathbb{E}_{r_{0}}\\left[\\bar{q}_{r_{0}}^{i}\\right]=\\frac{1}{N}}\\end{array}$ to obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert\\nabla f(\\bar{x}_{v})-\\bar{G}_{v_{\\mathrm{p}}}\\Vert^{2}+\\displaystyle\\sum_{i=1}^{N}\\left(\\frac{1}{P}\\sum_{n=0}^{n+{P}-1}\\alpha_{i}^{\\epsilon}\\right)\\Vert\\nabla f_{i}(\\bar{x}_{v_{\\mathrm{p}}})-\\bar{G}_{v_{\\mathrm{p}}}^{i}\\Vert^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\mathbb{E}_{\\sigma_{n}}\\left[\\Vert\\nabla f(\\bar{x}_{v_{\\mathrm{p}}})-\\bar{G}_{v_{\\mathrm{p}}}\\Vert^{2}+\\displaystyle\\sum_{i=1}^{N}\\left(\\frac{1}{P}\\sum_{n=0}^{n+{P}-1}\\alpha_{i}^{\\epsilon}\\right)\\Vert\\nabla f_{i}(\\bar{x}_{v_{\\mathrm{p}}})-\\bar{G}_{v_{\\mathrm{p}}}^{i}\\Vert^{2}\\right]\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\Vert\\nabla f(\\bar{x}_{v})-\\bar{G}_{v_{\\mathrm{p}}}\\Vert^{2}+\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}_{\\sigma_{n}}\\left[\\frac{1}{P}\\sum_{n=0}^{n+{P}-1}\\alpha_{i}^{\\epsilon}\\right]\\Vert\\nabla f_{i}(\\bar{x}_{v_{\\mathrm{n}}})-\\bar{G}_{v_{\\mathrm{p}}}^{i}\\Vert^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\Vert\\nabla f(\\bar{x}_{v_{\\mathrm{n}}})-\\bar{G}_{v_{\\mathrm{p}}}\\Vert^{2}+\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\Vert\\nabla f_{i}(\\bar{x}_{v_{\\mathrm{n}}})-\\bar{G}_{v_{\\mathrm{p}}}^{i}\\Vert^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\left\\Vert\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\nabla f_{i}(\\bar{x}_{v_{\\mathrm{n}}})-\\bar{G}_{v_{\\mathrm{p}}}^{i}\\right)\\right\\Vert^{2}+\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\left\\Vert\\nabla f_{i}(\\bar{x}_{v_{\\mathrm{n}}})-\\bar{G}_{v_{\\mathrm{p}}}^{i}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\le\\mathbb{E}\\left[ \n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\lVert\\nabla f_{t}(\\bar{x}_{r_{n}})-G_{r_{n}}^{i}\\right\\rVert^{2}\\right]\\leq\\mathbb{E}\\left[\\left\\lVert\\nabla f_{t}(\\bar{x}_{r_{n}})-\\frac{1}{I^{\\prime}}\\frac{r_{n}-1}{r_{n}-r_{n}-P_{t}\\log^{2}\\bar{\\pi}_{r_{n}-P}}\\nabla f_{t}(y_{r,k}^{i})\\right\\rVert^{2}\\right]}&{}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\lVert\\frac{1}{I^{p}}\\sum_{r=r_{n}-P_{t+1}}^{r_{n}-1}\\frac{r_{n}^{i}}{r_{n}-P_{t}}(\\nabla f_{t}(\\bar{x}_{r_{n}})-\\nabla f_{t}(y_{r,k}^{i})\\right\\rVert^{2}\\right]}\\\\ &{\\leq\\frac{1}{I^{p}}\\sum_{r=r_{n}-P_{t+1}}^{r_{n}-1}\\left\\{\\frac{r_{n}^{i}}{\\bar{x}_{r_{n}-P}}\\left\\lVert\\nabla f_{t}(\\bar{x}_{r_{n}})-\\nabla f_{t}(y_{r,k}^{i})\\right\\rVert^{2}\\right\\}}\\\\ &{\\leq\\frac{L^{2}}{I^{p}}\\sum_{r=r_{n}-P_{t}}^{r_{n}-1}\\sum_{s=0}^{r-1}\\left\\{\\frac{r_{n}^{i}}{\\bar{x}_{r_{n}-P}}\\left\\lVert\\bar{x}_{r_{n}}-y_{s,k}^{i}\\right\\rVert^{2}\\right\\}}\\\\ &{=L^{2}\\mathbb{E}\\left[\\frac{1}{I^{p}}\\sum_{r=r_{n}-P_{t+1}}^{r_{n}-1}\\frac{z_{s}^{i}}{r_{n}-P_{t}}\\left\\lVert\\bar{x}_{r_{n}}-y_{s,k}^{i}\\right\\rVert^{2}\\right]}\\\\ &{=L^{2}\\mathbb{E}\\left[\\frac{1}{I^{p}}\\sum_{r=r_{n}-P_{t+1}}^{r_{n}-1}\\frac{z_{s}^{i}}{r_{n}-P_{t}}\\sum_{s=0}^{r_{n}+1}\\mathbb{E}\\left[\\left\\lVert\\bar{x}_{r_{n}}-y_{s,k}^{i}\\right\\rVert^{2}\\right]\\right]}\\\\ &{=L^{2}\\mathbb \n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Plugging back to Equation 28 yields ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left[\\boldsymbol{D}_{t+\\tau}-\\boldsymbol{F}_{\\alpha}\\right]\\right\\|^{2}}\\\\ &{\\leq5\\sigma^{2}t^{2}\\eta^{2}t^{2}\\sigma^{2}\\left\\|\\left[\\boldsymbol{Y}(\\tau_{t+\\tau})\\right]\\right\\|^{2}+4\\gamma^{2}\\eta^{2}t^{2}\\eta^{2}\\sigma^{2}}\\\\ &{\\qquad+5\\sigma^{2}t^{2}t^{2}\\Gamma^{2}\\left[\\displaystyle\\sum_{n=0}^{\\infty}\\frac{\\gamma-1}{\\chi_{\\alpha}}(\\boldsymbol{D}_{s}+\\boldsymbol{M}_{s})\\right]+10\\gamma^{2}\\eta^{2}t^{2}\\Gamma^{2}\\eta^{2}\\frac{1}{N}\\frac{N}{\\chi_{\\alpha}}\\mathbb{E}\\left\\{\\boldsymbol{S}_{n}^{\\alpha}\\right\\}}\\\\ &{\\overset{(a)}{\\leq}5\\sigma^{2}\\eta^{2}t^{2}\\Gamma^{2}\\mathcal{E}\\left[\\left(17\\sigma^{2}\\ln\\left|\\frac{\\gamma}{2}\\right|\\right)^{2}+4\\eta^{2}t^{2}\\Gamma\\eta^{2}\\sigma^{2}\\right.}\\\\ &{\\qquad+5\\sigma^{2}t^{2}\\Gamma^{2}\\Gamma^{2}\\left(18\\sigma^{2}\\ln^{2}\\left|\\gamma\\left(\\Gamma(\\rho_{\\alpha})\\right|\\right)\\right]+(17\\sigma^{2}t^{2}\\left(\\Gamma\\delta+6\\delta\\eta^{2}t^{2}\\sigma^{2}\\right)\\sigma^{2}}\\\\ &{\\qquad\\left.+254\\eta^{2}t^{2}\\delta^{2}\\ln^{2}\\left(5\\eta^{2}\\right)\\right]+10\\eta^{2}\\gamma^{4}\\Gamma^{2}\\eta^{2}\\frac{1}{N}\\frac{N}{\\chi_{\\alpha}}\\mathbb{E}\\left\\{\\boldsymbol{S}_{n}^{\\alpha}\\right\\}}\\\\ &{\\leq5\\sigma^{2}t^{2}\\eta^{2}t^{2}(1+10\\sigma^{2}t^{2}\\Gamma^{2}\\eta^{2}\\delta)\\left\\|\\left[\\left(17\\sigma^{2}\\ln\\left|\\gamma\\right|\\right)\\right]}\\\\ &{\\qquad+5\\eta^{2}t\\gamma\\left(\\Gamma(\\rho_{\\alpha})^{2}+35\\eta^{2}\\nu^{4}\\right)\\chi^{2}\\mathcal{E}\\left(3\\eta^{2}\\delta\\right)}\\\\ &{\\qquad+10\\eta^{3}\\sigma^{2}t^{2}\\Gamma^{ \n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Wwhera $(i)$ usesEquation4 from Lemma ad $(i i)$ uses $\\begin{array}{r}{\\eta\\le\\frac{1}{60L I P}}\\end{array}$ and $\\begin{array}{r}{\\gamma\\eta\\le\\frac{1}{60L I P}}\\end{array}$ This proves ", "page_idx": 35}, {"type": "text", "text": "Lemma 3. Suppose that $\\begin{array}{r}{P_{\\mathcal{Q}_{r_{0}}}(\\bar{q}_{r_{0}}^{i}\\;>\\;0)\\;\\geq\\;p_{s a m p l e},\\;\\mathbb{E}\\left[w_{r_{0}}^{i}\\big|\\mathcal{Q}_{:r_{0}}\\right]\\;\\leq\\;\\frac{P^{2}}{N},\\;\\mathbb{E}_{r_{0}}[\\bar{q}_{r_{0}}^{i}]\\;=\\;\\frac{1}{N}}\\end{array}$ ,and $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{i=1}^{N}\\left(v_{r_{0}}^{i}\\right)^{2}\\Lambda_{r_{0}}^{i}\\right]\\leq\\rho^{2}}\\end{array}$ $\\begin{array}{r}{\\eta\\le\\frac{\\sqrt{p_{s a m p l e}}}{60L I P}}\\end{array}$ and $\\begin{array}{r}{\\gamma\\eta\\le\\frac{p_{s a m p l e}}{60L I P}}\\end{array}$ then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[S_{r_{0}+P}^{i}\\right]\\leq\\left(324\\eta^{2}I^{2}P^{2}+\\frac{20}{p_{s a m p l e}}\\gamma^{2}\\eta^{2}I^{2}P^{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\bar{x}_{r_{0}})\\right\\Vert^{2}\\right]}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\left(226\\eta^{2}I+195\\eta^{2}I P\\rho^{2}+\\frac{17}{p_{s a m p l e}}\\gamma^{2}\\eta^{2}I P\\rho^{2}\\right)\\sigma^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\left(1-\\frac{1}{2}p_{s a m p l e}\\right)\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\left[S_{r_{0}}^{j}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let $1\\,\\leq\\,i\\,\\leq\\,N$ We can consider the value $S_{r_{0}+P}^{i}$ under two cases: $\\bar{q}_{r_{0}}^{i}\\ >\\ 0$ and the complement. Let $A_{r_{0}}^{i}=\\{\\bar{q}_{r_{0}}^{i}>0\\}$ . Denote ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B_{1}^{i}=1\\left\\{A_{r_{0}}^{i}\\right\\}\\frac1{I P\\bar{q}_{r_{0}}^{i}}\\sum_{s=r_{0}}^{r_{0}+P-1}\\sum_{k=0}^{I-1}q_{s}^{i}\\mathbb{E}\\left[\\left\\|{\\pmb x}_{s,k}^{i}-\\bar{\\pmb x}_{r_{0}+P}\\right\\|^{2}\\right]\\biggr}}\\\\ {{\\displaystyle B_{2}^{i}=1\\left\\{\\bar{A}_{r_{0}}^{i}\\right\\}\\frac1{I P\\bar{z}_{r_{0}}^{i}}\\sum_{s=r_{0}}^{r_{0}+P-1}\\sum_{k=0}^{I-1}z_{s}^{i}\\mathbb{E}\\left[\\left\\|{\\pmb y}_{s,k}^{i}-\\bar{\\pmb x}_{r_{0}+P}\\right\\|^{2}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then $S_{r_{0}+P}^{i}=B_{1}^{i}+B_{2}^{i}$ andwe canconsider the twcases separatly. ", "page_idx": 35}, {"type": "text", "text": "Notice that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|x_{s,k}^{i}-\\bar{x}_{r_{0}+P}\\|^{2}\\leq3\\|x_{s,k}^{i}-\\bar{x}_{s,k}\\|^{2}+3\\|\\bar{x}_{s,k}-\\bar{x}_{r_{0}}\\|^{2}+3\\|\\bar{x}_{r_{0}+P}-\\bar{x}_{r_{0}}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{i}=\\mathbb{1}\\left\\{A_{r_{0}}^{i}\\right\\}\\displaystyle\\frac{1}{I P\\bar{q}_{r_{0}}^{i}}\\sum_{s=r_{0}}^{r_{0}+P-1}L_{-1}^{i-1}}q_{s}^{i}\\mathbb{E}\\left[\\left\\|x_{s,k}^{i}-\\bar{x}_{r_{0}+P}\\right\\|^{2}\\middle|\\mathcal{Q}\\right]}\\\\ &{\\phantom{=}\\leq\\mathbb{1}\\left\\{A_{r_{0}}^{i}\\right\\}\\displaystyle\\frac{3}{I P\\bar{q}_{r_{0}}^{i}}\\sum_{s=r_{0}}^{r_{0}+P-1}\\sum_{k=0}^{I-1}q_{s}^{i}\\Bigg(\\mathbb{E}\\left[\\|x_{s,k}^{i}-\\bar{x}_{s,k}\\|^{2}\\middle|\\mathcal{Q}\\right]+\\mathbb{E}\\left[\\|\\bar{x}_{s,k}-\\bar{x}_{r_{0}}\\|^{2}\\middle|\\mathcal{Q}\\right]}\\\\ &{\\phantom{=}+\\mathbb{E}\\left[\\|\\bar{x}_{r_{0}+P}-\\bar{x}_{r_{0}}\\|^{2}\\middle|\\mathcal{Q}\\right]\\Bigg)}\\\\ &{\\phantom{=}\\leq\\mathbb{1}\\left\\{A_{r_{0}}^{i}\\right\\}\\displaystyle\\frac{3}{I P\\bar{q}_{r_{0}}^{i}}\\sum_{s=r_{0}}^{r_{0}+P-1}\\sum_{k=0}^{I-1}q_{s}^{i}\\left(D_{s,k}+M_{s,k}\\right)+3\\mathbb{1}\\left\\{A_{r_{0}}^{i}\\right\\}\\mathbb{E}\\left[\\|\\bar{x}_{r_{0}+P}-\\bar{x}_{r_{0}}\\|^{2}\\middle|\\mathcal{Q}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using Equation 3 from Lemma 1, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1\\left\\{\\begin{array}{l l}{\\lambda_{m_{i}}^{1}\\biggr\\}\\frac{\\partial}{\\partial P_{m_{i}}}\\biggr\\rvert_{\\infty}^{-\\mu_{0}+\\beta-1-\\frac{1}{\\gamma}-1-1}\\right\\}\\left(D_{m_{i},k}+M_{m,k}\\right)}\\\\ &{\\leq1\\left\\{\\begin{array}{l l}{\\lambda_{m_{i}}^{3}\\biggr\\}\\frac{\\partial}{\\partial P_{m_{i}}^{0}}\\biggr\\rvert_{\\infty}^{-\\mu_{0}-\\beta}\\sum_{n=0}^{\\infty}\\left(10\\mathrm{s}^{\\mu_{i}/2}P^{2}P^{2}\\mathbb{E}\\left[\\left|\\nabla f(\\tilde{x}_{m_{i}})\\right|^{2}\\right]\\right)\\biggr\\rvert\\Theta\\right\\}+\\left(7\\upzeta\\eta^{2}T+6\\upzeta\\eta^{2}T P^{2}\\rho^{2}\\right)\\sigma^{\\prime}}\\\\ {+100\\sigma^{2}L^{2}P^{2}\\frac{\\sqrt{\\mu}^{3}}{\\mu_{0}+\\beta}\\biggr\\}\\left(\\mu_{0}^{\\mu_{i}}+\\frac{1}{N}\\right)S_{m_{i}}^{\\mu_{i}}+380\\sigma^{2}L^{2}P_{m_{i}}^{2}\\frac{\\sqrt{\\mu}^{3}}{\\mu_{0}}\\sigma_{0}^{2}\\biggr)}\\\\ &{\\leq34\\eta^{2}L^{2}P^{2}\\mathbb{E}\\left[\\left|\\nabla f(\\tilde{x}_{m_{i}})\\right|^{2}\\right]\\left(\\mathcal{Q}\\right)+\\left(2\\upzeta\\eta^{2}T^{2}+196\\eta^{2}T\\rho_{0}^{2}\\right)\\sigma^{\\prime}}\\\\ &{+32\\eta^{2}L^{2}P^{2}\\frac{\\sqrt{\\mu}^{3}}{\\mu_{0}+\\beta}\\biggr\\}\\frac{\\partial}{\\partial P_{m_{i}}}+1\\left\\{A_{m_{i}}^{3}\\right\\}\\frac{\\partial}{\\partial P_{m_{i}}^{0}}\\sum_{n=0}^{\\infty}\\sigma_{i}^{2}\\biggr\\langle30\\eta^{2}L^{2}P^{2}\\frac{\\sqrt{\\mu}^{3}}{\\mu_{0}+\\beta}\\biggr\\rangle}\\\\ &{=32\\lambda\\eta^{2}L^{2}P^{2}\\mathbb{E}\\left[\\left|\\nabla f(\\tilde{x}_{m_{i}})\\right|^{2}\\right] \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Denote $\\begin{array}{r}{w_{r_{0}}^{i,j}=\\frac{\\mathbb{1}\\left\\{A_{r_{0}}^{i}\\right\\}}{P\\bar{q}_{r_{0}}^{i}}\\sum_{s=r_{0}}^{r_{0}+P-1}q_{s}^{i}q_{s}^{j}}\\end{array}$ o that $\\begin{array}{r}{w_{r_{0}}^{i}=\\frac{1}{N}\\sum_{j=1}^{N}w_{r_{0}}^{i,j}}\\end{array}$ Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad B_{1}^{i}\\leq324\\mu^{2}T^{2}P^{2}\\mathbb{E}\\left[\\left\\|\\nabla f(\\pi_{\\alpha})\\right\\|^{2}\\right]\\leq\\left(25\\mu^{2}T^{2}+195\\mu^{2}T^{2}P^{2}\\right)\\sigma^{2}}\\\\ &{\\qquad\\qquad+327\\mu^{2}L^{2}T^{2}P^{2}\\underset{=0}{\\overset{N}{\\sum}}\\left(\\dot{q}_{\\alpha}^{i}+\\frac{1}{N}\\right)S_{\\alpha}^{i}+108\\sigma^{2}L^{2}P^{2}\\underset{=0}{\\overset{N}{\\sum}}\\left(\\frac{1}{P^{2}\\underset{=0}{\\overset{N}{\\sum}}}\\frac{\\mu^{4}-P^{-1}}{\\underset{=0}{\\overset{N}{\\sum}}}\\frac{q_{\\alpha}^{i}q_{\\alpha}^{i}}{q_{\\alpha}^{i}q_{\\alpha}^{j}}\\right)}\\\\ &{\\qquad\\qquad\\qquad+3\\left\\{A_{1}^{i}\\underset{=0}{\\overset{N}{\\sum}}\\right\\}\\left[\\left\\|\\nabla f_{\\alpha+\\beta}-\\pi_{\\alpha}\\right\\|^{2}\\right]{\\overset{}{\\prod}}()}\\\\ &{\\qquad\\leq\\left[B_{1}^{i}\\right]\\underset{=0}{\\overset{N}{\\sum}}\\left(2\\underbrace{\\partial_{\\alpha+\\beta}^{i}T^{2}\\pi^{2}}\\left[\\left\\|\\nabla f(\\pi_{\\alpha})\\right\\|^{2}\\right]\\left(2\\pi_{\\alpha}\\right)+\\left(225\\mu^{2}T+155\\sigma_{\\alpha}^{2}T\\beta_{\\alpha}^{2}\\right)\\sigma^{2}}_{=0}}\\\\ &{\\qquad\\qquad\\qquad+654\\mu^{2}L^{2}T^{2}P^{2}\\underset{=0}{\\overset{N}{\\sum}}\\sum_{\\alpha}^{N}S_{\\alpha}^{i}+108\\sigma^{2}L^{2}P^{2}\\underset{=0}{\\overset{N}{\\sum}}\\mathbb{E}\\left[\\left\\|\\frac{\\left\\langle\\dot{q}_{\\alpha}^{i}\\right\\rangle}{P\\underset{=0}{\\overset{N}{\\sum}}}\\frac{\\mu^{4}+P^{-1}}{\\underset{=0}{\\overset{N}{\\sum}}}\\frac{q_{\\alpha}^{i}q_{\\alpha}^{i}}{q_{\\alpha}^{i}}\\right\\|\\mathcal{O}_{\\alpha}\\right]S_ \n$$\u6b63", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $(i)$ uses $\\begin{array}{r}{\\mathbb{E}_{r_{0}}[\\bar{q}_{r_{0}}^{i}]=\\frac{1}{N}}\\end{array}$ and the tower property $\\mathbb{E}\\left[\\mathbb{E}\\left[\\cdot|\\boldsymbol{\\mathcal{Q}}\\right]|\\boldsymbol{\\mathcal{Q}}_{:r_{0}}\\right]=\\mathbb{E}\\left[\\cdot|\\boldsymbol{\\mathcal{Q}}_{:r_{0}}\\right]$ and $(i i)$ uses the tower property $\\mathbb{E}$ [E $\\left[\\cdot\\vert\\boldsymbol{\\mathcal{Q}}_{:r_{0}}\\right]\\right]=\\mathbb{E}\\left[\\cdot\\right]$ ", "page_idx": 37}, {"type": "text", "text": "Now consider $B_{2}^{i}$ . Under $\\bar{A}_{r_{0}}^{i}$ \uff0c $z_{r}^{i}\\,=\\,z_{r-P}^{i}$ and ${\\pmb y}_{r,k}^{i}={\\pmb y}_{r-P,k}^{i}$ for all $r\\,\\in\\,\\{r_{0},\\dots,r_{0}+P-1\\}$ Therefore ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{2}^{i}=1\\left\\{\\bar{A}_{n}^{i}\\right\\}\\frac{1}{I\\bar{E}\\bar{E}\\bar{E}_{n}^{i}}\\frac{\\bar{F}_{n}^{i+\\bar{j}-1}\\bar{L}^{i-1}}{\\bar{E}_{n}^{i}}\\mathbb{E}\\left[\\left|\\bar{y}_{t,k}^{i}-\\bar{x}_{\\bar{n}+\\bar{n}}\\right|\\right]^{2}\\mathbb{E}\\right]}\\\\ &{=1\\left\\{\\bar{A}_{n}^{i}\\right\\}\\frac{1}{I\\bar{E}\\bar{E}_{n}^{i}}\\frac{\\frac{\\bar{F}_{n}^{i-1}}{I\\bar{E}_{n}^{i}-1}\\bar{F}_{n}^{i}}{I\\bar{E}_{n}^{i}-I\\bar{E}_{n}^{i-1}\\bar{E}_{n}^{i}}\\mathbb{E}\\left[\\left|\\bar{y}_{t,k}^{i}-\\bar{x}_{\\bar{n}+\\bar{n}}\\right|\\bar{P}_{n}^{i}\\right]}\\\\ &{\\overset{(i)}{\\underset{\\mathrm{~\\tiny~+~}}{=}}\\mathbb{E}\\left[\\bar{A}_{n}^{i}\\right]\\frac{1}{I\\bar{E}_{n}^{i}\\bar{E}_{n}^{i}-I_{n}^{i}}\\frac{\\bar{F}_{n}^{i-1}}{I\\bar{E}_{n}^{i}-I_{n}^{i}}z_{n}^{i}\\bigg(1+\\lambda\\mathbb{E}\\left[\\left|\\bar{y}_{t,k}^{i}-\\bar{x}_{\\bar{n}}\\right|\\right]^{2}\\mathbb{E}\\bigg)}\\\\ &{\\quad+\\left(1+\\frac{1}{\\lambda}\\right)\\mathbb{E}\\left[\\left|\\bar{\\pi}_{n+\\bar{n}}-\\bar{x}_{\\bar{n}}\\right|^{2}\\right]\\mathbb{E}\\bigg[\\left|\\bar{y}_{t,k}^{i}\\right|}\\\\ &{=1\\left\\{\\bar{A}_{n}^{i}\\right\\}(1+\\lambda)\\frac{1}{I\\bar{E}_{n}^{i}\\bar{E}_{n}^{i}-I_{n}^{i}}\\sum_{\\alpha=\\bar{n}+\\bar{n}}^{i-1}\\frac{\\bar{F}_{n}^{i}}{I\\alpha}\\mathbb{E}\\left[\\left|\\bar{y}_{t,k}^{i}-\\bar{x}_{\\bar{n}+\\bar{n}}\\right|^{2}\\right]\\mathbb{E}\\bigg]}\\\\ &{\\\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $(i)$ uses Young's inequality with an arbitrary $\\lambda>0$ . Taking conditional expectation $\\mathbb{E}[\\cdot|\\mathcal{Q}_{:r_{0}}]$ followed by total expectation yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[B_{2}^{i}|\\mathcal{Q}_{:r_{0}}]\\leq(1-p_{\\mathrm{sample}})\\left(1+\\lambda\\right)S_{r_{0}}^{i}+\\left(1+\\displaystyle\\frac{1}{\\lambda}\\right)\\mathbb{E}\\left[\\mathbb{1}\\left\\{\\bar{A}_{r_{0}}^{i}\\right\\}\\|\\bar{\\mathbf{x}}_{r_{0}+P}-\\bar{\\mathbf{x}}_{r_{0}}\\|^{2}\\Big|\\mathcal{Q}_{:r_{0}}\\right]}\\\\ {\\mathbb{E}[B_{2}^{i}]\\leq(1-p_{\\mathrm{sample}})\\left(1+\\lambda\\right)\\mathbb{E}\\left[S_{r_{0}}^{i}\\right]+\\left(1+\\displaystyle\\frac{1}{\\lambda}\\right)\\mathbb{E}\\left[\\mathbb{1}\\left\\{\\bar{A}_{r_{0}}^{i}\\right\\}\\|\\bar{\\mathbf{x}}_{r_{0}+P}-\\bar{\\mathbf{x}}_{r_{0}}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Adding Equation 29 and Equation 30 yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[S_{r_{0}+P}^{i}\\right]=\\mathbb{E}[B_{1}^{i}]+\\mathbb{E}[B_{2}^{i}]}\\\\ &{\\quad\\quad\\quad\\leq324\\eta^{2}I^{2}P^{2}\\mathbb{E}\\left[\\left\\|\\nabla f(\\bar{x}_{r_{0}})\\right\\|^{2}\\right]+\\left(225\\eta^{2}I+195\\eta^{2}I P\\rho^{2}\\right)\\sigma^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+654\\eta^{2}L^{2}I^{2}P^{2}\\frac{1}{N}\\underset{j=1}{\\overset{N}{\\sum}}\\mathbb{E}\\left[S_{r_{0}}^{j}\\right]+108\\eta^{2}L^{2}I^{2}\\underset{j=1}{\\overset{N}{\\sum}}\\mathbb{E}\\left[\\mathbb{E}\\left[w_{r_{0}}^{i,j}\\big|Q_{:r_{0}}\\right]S_{r_{0}}^{j}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left(1-p_{\\mathrm{sample}}\\right)\\left(1+\\lambda\\right)\\mathbb{E}\\left[S_{r_{0}}^{i}\\right]+\\left(3+\\frac{1}{\\lambda}\\right)\\mathbb{E}\\left[\\left\\|\\bar{x}_{r_{0}+P}-\\bar{x}_{r_{0}}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Averaging over $i$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{532i\\pi^{2}J^{2}P^{2}\\mathbb{E}\\left[\\left\\|\\mathbf{Z}^{\\mu}(E_{n})\\right\\|^{2}\\right]+(22\\mathcal{Z}^{\\mu}\\mathcal{E}^{\\prime}+156\\mathcal{W}^{2}I P^{\\alpha})^{2}\\sigma^{2}+65i\\sigma^{2}L^{2}\\sigma^{2}P^{2}\\mathcal{E}^{\\alpha}\\left\\|\\mathbf{X}^{\\nu}\\right\\|}\\\\ &{+108\\sigma^{2}L^{2}P^{2}\\frac{\\nu}{\\rho_{\\mathrm{s}}!}\\mathbb{E}\\left[\\left\\|\\mathbf{Z}_{\\frac{1}{n}-1}^{\\nu}\\mathbf{W}^{\\alpha}\\mathbf{e}_{\\mu}^{\\nu}\\right\\|\\sigma_{n}\\right]\\sigma_{n}^{2}}\\\\ &{\\quad+(1-P_{n n})\\sigma^{4}+3\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[\\left\\|\\mathbf{Z}_{\\frac{1}{n}}^{\\nu}\\mathbf{e}_{\\mu}^{\\nu}\\right\\|\\right]\\sigma_{n}^{2}}\\\\ &{\\leq32i\\sigma^{2}P^{2}\\mathbb{E}\\left[\\left\\|\\mathbf{Z}_{\\mathcal{H}^{\\sigma}}(E_{n})\\right\\|^{2}+(2\\mathcal{Z}^{\\mu}\\mathcal{E}^{\\prime}+156\\mathcal{W}^{2}I P^{\\alpha})^{2}\\sigma^{2}\\right.}\\\\ &{\\quad+\\left.(1-P_{n n})(1+\\lambda)+65i\\sigma^{2}L^{2}I^{2}P^{2}\\mathcal{P}^{2}\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\left[\\mathcal{S}_{\\mu}^{\\nu}\\right]}\\\\ &{\\quad+108\\sigma^{2}L^{2}P^{2}\\frac{\\nu}{\\rho_{\\mathrm{s}}!}\\mathbb{E}\\left[\\left\\|\\mathbf{Z}_{\\mu}^{\\nu}\\right\\|\\sigma_{n}^{2}\\sigma_{n}^{2}\\right]\\sigma_{n}^{2}+\\left(3+\\frac{1}{\\lambda}\\right)\\mathbb{E}\\left[\\left\\|\\mathbf{Z}_{\\mu+\\mu}^{\\nu}-\\mathbf{Z}_{\\mu}\\right\\|^{2}\\right.}\\\\ &{\\left.\\quad\\leq32i\\sigma^{2}L^{2}P^{2}\\mathbb{E}\\left[\\left\\|\\mathbf{Z}_{\\mu}(E_{n})\\right\\|^{2}\\right]+(22\\mathcal{Z}^{\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality uses the condition $\\begin{array}{r}{\\mathbb{E}\\left[w_{r_{0}}^{i}\\middle|\\mathscr{Q}_{:r_{0}}\\right]\\leq\\frac{P^{2}}{N}}\\end{array}$ . We can then apply Lemma 2 and choose \u5165 = 10(1-Psample) to obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left\\{S_{\\eta,n}^{*}\\right\\}}}\\\\ &{\\leq\\left(3\\alpha^{2}\\eta^{2}\\gamma^{2}+\\left(3+\\frac{1}{\\lambda}\\right)6\\gamma^{2}\\eta^{2}\\beta^{2}\\right)\\mathbb{E}\\left[\\Gamma\\big(\\Gamma_{j-1}\\big)\\big|^{3}\\right]}\\\\ &{\\leq\\left(2\\alpha^{2}\\eta^{2}\\gamma^{2}+(18\\gamma^{2}\\beta^{2})P^{\\prime}+\\left(3+\\frac{1}{\\lambda}\\right)\\sum_{j=1}^{N}2\\gamma^{2}\\eta^{2}\\beta^{2}\\right)\\sigma^{2}}\\\\ &{\\quad+\\left((1-7\\alpha\\gamma\\rho_{0}^{2})(1+3)+7\\alpha^{2}\\eta^{2}\\gamma^{2}\\beta^{2}\\right)\\sigma^{2}+\\left(3+\\frac{1}{\\lambda}\\right)1\\gamma\\eta^{2}\\gamma^{2}\\eta^{2}\\beta^{2}\\right)\\frac{1}{N}\\sum_{j=1}^{N}\\{S_{j}^{2}\\}}\\\\ &{\\leq\\left((18\\eta^{2}\\gamma^{2}\\beta^{2}+\\frac{20}{\\gamma\\alpha\\gamma^{2}}\\gamma^{2}\\eta^{2}\\gamma^{2})\\sigma\\right)\\mathbb{E}\\left[\\left\\{\\eta\\left\\{\\mathcal{F}_{j-1}\\right\\}\\right\\}\\right]^{2}}\\\\ &{\\quad+\\left(2\\alpha^{2}\\eta^{2}\\gamma^{2}+18\\eta^{2}\\gamma^{2}\\rho_{0}^{2}+\\frac{1}{\\lambda}\\right)\\sum_{j=1}^{N}\\eta^{2}\\gamma^{2}\\eta^{2}P^{\\prime}+\\frac{20}{\\lambda}\\frac{\\gamma}{N}\\chi L\\beta^{2}\\gamma\\sigma^{2}}\\\\ &{\\quad+\\left(-\\frac{7}{\\lambda}\\right)2\\eta^{2}\\gamma^{2}+\\frac{40}{\\lambda}\\sum_{j=1}^{N}2\\gamma^{2}\\eta^{2}\\beta^{2}\\sigma^{2}+\\frac{11}{38\\eta^{3}}\\gamma^{2}\\eta^{2}\\gamma^{2}\\sigma^{2}\\frac{11}{N}\\sum_{j=1}^{N}\\{S_{j}^{2}\\}}\\\\ &{\\leq\\left(2\\alpha^{2}\\eta^{2}\\gamma^{2}+\\frac{20}{\\lambda}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality uses the conditions $\\begin{array}{r}{\\eta\\ \\leq\\ \\frac{\\sqrt{p_{\\mathrm{sample}}}}{60L I P}}\\end{array}$ and $\\begin{array}{r l r}{\\gamma\\eta\\!\\!\\!/}&{{}\\le}&{\\!\\!\\!\\frac{p_{\\mathrm{sample}}}{60L I P}}\\end{array}$ , and we used that $\\begin{array}{r}{3+\\frac{1}{\\lambda}\\le\\frac{10}{3p_{\\mathrm{sample}}}}\\end{array}$ \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Theorem 2 (Theorem 1 restated). Suppose Assumptions 1 and 2 hold, and $\\begin{array}{r}{\\mathbb{E}[w_{r_{0}}^{i}|\\mathcal{Q}_{:r_{0}}]\\leq\\frac{P^{2}}{N}}\\end{array}$ and $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{i=1}^{N}\\left(v_{r_{0}}^{i}\\right)^{2}\\Lambda_{r_{0}}^{i}\\right]\\leq\\rho^{2}}\\end{array}$ If ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma\\eta\\leq\\frac{p_{s a m p l e}}{60L I P},\\quad\\eta\\leq\\frac{\\sqrt{p_{s a m p l e}}}{60L I P},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "then Algorithm 1 satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{P}{R}\\sum_{r_{0}\\in\\{0,P,\\ldots,R-P\\}}\\mathbb{E}[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}]\\leq\\frac{5\\Delta}{\\gamma\\eta I R}+\\left(20\\gamma\\eta L\\rho^{2}+5785\\eta^{2}L^{2}I P\\right)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\bar{x}_{r_{0}+P}-\\bar{x}_{r_{0}}=-\\gamma\\eta\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}q_{r}^{i}\\sum_{k=0}^{I-1}\\nabla F_{i}(x_{r,k}^{i};\\xi_{r,k}^{i})+\\gamma\\eta I P\\sum_{i=1}^{N}\\left(\\bar{q}_{r_{0}}^{i}-\\frac{1}{N}\\right)G_{r_{0}}^{i}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using the quadratic upper bound for smooth functions and taking total expectation, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[f(\\bar{x}_{n+r})-f(\\bar{x}_{n})]\\leq-\\eta\\Re\\left[\\left\\langle\\nabla f(\\bar{x}_{n}),\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\frac{\\nabla f}{\\bar{x}_{n}}\\right\\rvert\\overset{t=}{\\leq}\\mathcal{V}_{t}(x_{n}^{\\varepsilon},x_{n}^{\\varepsilon},\\xi_{n}^{*})\\right\\rangle\\right]}\\\\ &{\\quad+\\eta\\Re\\left[\\mathcal{E}_{t}\\left[\\nabla f(\\bar{x}_{n}),\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\left(\\partial_{t}^{\\varepsilon}-\\frac{1}{N}\\right)G_{n}^{\\varepsilon}\\right)\\right]}\\\\ &{\\quad+\\frac{L}{2}\\mathbb{E}\\left[\\left\\{\\mathcal{E}_{t}(n;\\nu-\\bar{x}_{n})\\mathbb{I}_{n}^{T}\\left[\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\nabla f^{(n)}\\xi_{n}^{*}\\right]\\right\\}\\right]}\\\\ &{\\overset{(i)}{\\leq}-\\gamma\\Re\\left[\\mathcal{E}_{t}\\gamma_{n},\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\nabla f^{(n)}\\xi_{n}^{*}\\xi_{n}^{*}\\xi_{n}^{*}\\right]\\right\\}\\right]}\\\\ &{\\quad+\\eta\\Re\\left[\\mathcal{E}_{t}\\left[\\nabla f^{(n)}(\\xi_{n}),\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\left(\\xi_{n}\\xi_{n}^{\\varepsilon}\\right)\\frac{1}{N}\\right]G_{n}^{\\varepsilon}\\right\\rangle}\\\\ &{\\quad+\\frac{L}{2}\\mathbb{E}\\left[\\left\\{\\mathcal{E}_{t}\\gamma_{n}+\\mathcal{E}_{n}\\right\\}^{2}\\left[\\mathcal{E}_{t}\\left[\\left.\\frac{\\gamma_{n}+\\gamma_{n}}{\\gamma_{n}}\\right]^{2}+\\mathcal{E}_{t}\\left[\\xi_{n}\\xi_{n}^{\\varepsilon}\\right]\\right\\}\\right]}\\\\ &{\\overset{(i i)}{\\leq}-\\gamma\\Re\\left[\\left\\langle\\nabla f(\\bar{x}_{n}),\\frac{\\nabla f^{(n)}-1}{\\gamma_{n}}\\frac{\\nabla f^{(n)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $(i)$ uses the law of total expectation $\\mathbb{E}[\\cdot]=\\mathbb{E}[\\mathbb{E}_{r_{0}}[\\cdot]]$ and $(i i)$ uses the condition $\\begin{array}{r}{\\mathbb{E}_{r_{0}}[\\bar{q}_{r_{0}}^{i}]=\\frac{1}{N}}\\end{array}$ togetherwith ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\tau_{0}}\\left[\\displaystyle\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}q_{r}^{l-1}\\nabla F_{i}(\\pmb{x}_{r,i}^{\\ t},\\xi_{r,k}^{i})\\right]}&{=\\displaystyle\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}\\mathbb{E}_{\\tau_{0}}\\left[q_{r}^{i}\\nabla F_{i}(\\pmb{x}_{r,i}^{t};\\xi_{r,k}^{i})\\right]}\\\\ &{=\\displaystyle\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}\\mathbb{E}_{\\tau_{0}}\\left[\\mathbb{E}\\left[q_{r}^{i}\\nabla F_{i}(\\pmb{x}_{r,i}^{t};\\xi_{r,k}^{i})\\right]\\pmb{x}_{r,k}^{i}\\right]}\\\\ &{=\\displaystyle\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}\\mathbb{E}_{\\tau_{0}}\\left[q_{r}^{i}\\mathbb{E}\\left[\\nabla F_{i}(\\pmb{x}_{r,i}^{t};\\xi_{r,k}^{i})\\right]\\pmb{x}_{r,k}^{i}\\right]}\\\\ &{=\\displaystyle\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}\\mathbb{E}_{\\tau_{0}}\\left[q_{r}^{i}\\mathbb{E}\\left[\\nabla F_{i}(\\pmb{x}_{r,i}^{t};\\xi_{r,k}^{i})\\right]\\pmb{x}_{r,k}^{i}\\right]}\\\\ &{=\\displaystyle\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}\\mathbb{E}_{\\tau_{0}}\\left[q_{r}^{i}\\nabla f_{i}(\\pmb{x}_{r,k}^{i})\\right]}\\\\ &{=\\displaystyle\\mathbb{E}_{\\tau_{0}}\\left[\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}q_{r}^{i}\\sum_{k=0}^{N}\\mathbb{E}_{\\tau_{0}}\\left[q_{r}^{i}\\nabla f_{i}(\\pmb{x}_{r,k}^{i})\\right]\\right.}\\\\ &{=\\\\displaystyle\\mathbb{E}_{\\tau_{0}}\\left[\\sum_{r=r_{0\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Focusing on the inner product term of Equation 32: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\gamma\\eta\\mathbb{E}\\left[\\left\\langle\\nabla f(\\Bar{x}_{r_{0}}),\\mathbb{E}_{r_{0}}\\left[\\overset{r_{0}+P-1}{\\underset{r=r_{0}}{\\sum}}\\overset{N}{\\underset{i=1}{\\sum}}q_{r}^{i}\\overset{I-1}{\\underset{k=0}{\\sum}}\\nabla f_{i}(x_{r,k}^{i})\\right]\\right\\rangle\\right]}\\\\ &{\\quad=-\\frac{\\gamma\\eta}{I P}\\mathbb{E}\\left[\\left\\langle I P\\nabla f(\\Bar{x}_{r_{0}}),\\mathbb{E}_{r_{0}}\\left[\\overset{r_{0}+P-1}{\\underset{r=r_{0}}{\\sum}}\\overset{N}{\\underset{i=1}{\\sum}}q_{r}^{i}\\overset{I-1}{\\underset{k=0}{\\sum}}\\nabla f_{i}(x_{r,k}^{i})\\right]\\right\\rangle\\right]}\\\\ &{\\quad\\le-\\frac{\\gamma\\eta I P}{2}\\mathbb{E}\\left[\\|\\nabla f(\\Bar{x}_{r_{0}})\\|^{2}\\right]+\\frac{\\gamma\\eta}{2I P}\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{r_{0}}\\left[\\overset{r_{0}+P-1}{\\underset{r=r_{0}}{\\sum}}\\overset{N}{\\underset{i=1}{\\sum}}q_{r}^{i}\\underset{k=0}{\\sum}{\\nabla f_{i}}(x_{r,k}^{i})\\right]-I P\\nabla f(\\Bar{x}_{r_{0}})\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we used $\\begin{array}{r}{-\\langle a,b\\rangle=\\frac{1}{2}\\|b-a\\|^{2}-\\frac{1}{2}\\|a\\|^{2}-\\frac{1}{2}\\|b\\|^{2}\\leq\\frac{1}{2}\\|b-a\\|^{2}-\\frac{1}{2}\\|a\\|^{2}}\\end{array}$ Also ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\left\\|\\mathbf{x}_{t}\\int_{\\mathbb{R}_{+}}^{\\mathbb{R}_{+}}\\left[\\sum_{u=1}^{k-1}\\frac{x_{u}^{u}}{u}e_{u}^{t}\\frac{\\mathrm{d}u}{u}e_{u}^{t}\\right]d t\\right\\}=I\\mathbf{r}(t,u)\\right\\}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{t}\\int_{\\mathbb{R}_{+}}^{\\mathbb{R}_{+}}\\sum_{u=1}^{k-1}\\frac{x_{u}^{u}}{u}e_{u}^{t}\\frac{\\mathrm{d}u}{u}e_{u}^{t}(x,u_{u}^{u})-\\mathbf{y}(t,u_{u}^{u})\\right\\|^{2}\\right]}\\\\ &{\\stackrel{{,~}}{\\geq}\\mathrm{ard}\\left[\\left\\|\\mathbf{x}_{t}\\int_{\\mathbb{R}_{+}}^{\\mathbb{R}_{+}}\\sum_{u=1}^{k-1}\\frac{x_{u}^{u}}{u}e_{u}^{t}\\frac{\\mathrm{d}u}{u}(y,t,u_{u}^{u})-\\mathbf{y}(t,u_{u}^{u})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{2}{\\lambda}\\left[\\left\\|\\mathbf{x}_{t}\\int_{\\mathbb{R}_{+}}^{\\mathbb{R}_{+}}\\sum_{u=1}^{k-1}\\frac{x_{u}^{u}}{u}e_{u}^{t}\\frac{\\mathrm{d}u}{u}(y,t,u_{u})-\\mathbf{y}(t,u_{u}^{u})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{2}{\\lambda}\\left[\\left\\|\\mathbf{x}_{t}\\int_{\\mathbb{R}_{+}}^{\\mathbb{R}_{+}}\\sum_{u=1}^{k-1}\\frac{x_{u}^{u}}{u}e_{u}^{t}\\frac{\\mathrm{d}u}{u}(y,t,u_{u})-\\mathbf{y}(t,u_{u}^{u})\\right\\|^{2}\\right]}\\\\ &{\\stackrel{{,~}}{\\geq}\\mathrm{ard}\\left[\\left\\|\\mathbf{x}_{t}\\int_{\\mathbb{R}_{+}}^{\\mathbb{R}_{+}}\\sum_{u=1}^{k-1}\\mathbf{x}_{u}^{u}\\in[\\mathbf{y}(t,u_{u}^{u})-\\mathbf{y}(t,u_{u}^{u})]\\right\\|^{2}\\right]}\\\\ &{\\stackrel{{,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{\\Phi}}+\\mathbf{\\Phi}^{*}\\left[\\left[\\begin{array}{l}{||\\hat{\\mathbf{V}}_{i}|^{2}}\\\\ {\\frac{\\mathrm{Pe}_{1}}{\\omega_{1}}\\displaystyle\\sum_{i=1}^{3}\\hat{\\mathbf{V}}_{i}^{*}\\phantom{\\top}\\hat{\\mathbf{V}}_{i}^{*}}\\end{array}\\right](\\ell,t_{i},\\omega_{1})-\\nabla_{i}\\ell(,t_{i})\\right]^{\\top}}\\\\ &{+{\\mathbf{\\Phi}}^{*}\\left[\\left[\\begin{array}{l}{|\\hat{\\mathbf{V}}_{i}|^{2}}\\\\ {\\frac{\\mathrm{Pe}_{2}}{\\omega_{1}}\\displaystyle\\sum_{i=1}^{3}\\hat{\\mathbf{V}}_{i}^{*}}\\end{array}\\right]\\left[\\begin{array}{l}{|\\hat{\\mathbf{V}}_{i}|^{2}}\\\\ {\\hat{\\mathbf{V}}_{i}\\phantom{2}\\hat{\\mathbf{V}}_{i}^{*}}\\end{array}\\right]+\\mathbf{\\Phi}^{*}\\left[\\left[\\begin{array}{l}{|\\hat{\\mathbf{V}}_{i}|^{2}}\\\\ {0}\\end{array}\\right]\\right]^{\\top}}\\\\ &{\\overline{{{\\mathbf{S}}}}^{*}\\equiv{\\mathbf{\\Phi}}^{*}\\left[\\begin{array}{l}{||\\hat{\\mathbf{V}}_{i}|^{2}}\\\\ {\\frac{\\mathrm{Pe}_{3}}{\\omega_{1}}\\displaystyle\\sum_{i=1}^{3}\\hat{\\mathbf{V}}_{i}^{*}}\\end{array}\\right]\\times{\\mathbf{\\Phi}}^{*}\\left[\\begin{array}{l}{||\\hat{\\mathbf{V}}_{i}|^{4}}\\\\ {0}\\end{array}\\right]}\\\\ &{+{\\mathbf{\\Phi}}^{*}{\\mathbf{\\Phi}}^{*}\\left[\\begin{array}{l}{-1}\\\\ {\\frac{\\mathrm{Pe}_{4}}{\\omega_{1}}\\displaystyle\\sum_{i=1}^{3}\\hat{\\mathbf{V}}_{i}^{*}}\\end{array}\\right]\\times{\\mathbf{\\Phi}}^{*}\\left[\\begin{array}{l}{{\\mathbf{\\Phi}}^{*}(|\\mathbf{V}_{i}|^{2}(\\ell,t_{i}),-\\nabla_{i}\\ell(\\ell,t_{i}))^{\\top}}\\\\ {0}\\end{array}\\right]}\\\\ &{+{\\mathbf{\\Phi}}^{*}{\\mathbf{\\Phi}}^{*}\\left[\\begin{array}{l}{-1} \n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $(i)$ uses the decomposition ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla f_{i}(x_{r,k}^{i})\\!-\\!\\nabla f(\\bar{x}_{r_{0}})=(\\nabla f_{i}(x_{r,k}^{i})\\!-\\!\\nabla f_{i}(\\bar{x}_{r,k}))\\!+\\!(\\nabla f_{i}(\\bar{x}_{r,k})\\!-\\!\\nabla f_{i}(\\bar{x}_{r_{0}}))\\!+\\!(\\nabla f_{i}(\\bar{x}_{r_{0}})\\!-\\!\\nabla f(\\bar{x}_{r_{0}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "$(i i)$ and $(i i i)$ use Jensen's inequality, and $(i v)$ uses smoothness of each $f_{i}$ along with the condition $\\begin{array}{r}{\\mathbb{E}_{r_{0}}\\left[\\bar{q}_{r_{0}}^{i}\\right]=\\frac{1}{N}}\\end{array}$ . Plugging into Equation 33 yields ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\gamma\\eta\\mathbb{E}\\left[\\left\\langle\\nabla f(\\bar{x}_{\\tau}),\\mathbb{E}_{\\tau_{0}}\\left[\\sum_{r=0}^{\\infty+P-1}\\frac{N}{\\sum_{t}\\tau_{r}^{2}}\\sqrt{\\lambda}\\frac{I_{1}^{2}-1}{V}\\nabla f_{1}(x_{r,k}^{'})\\right]\\right\\rangle\\right]}\\\\ &{\\quad\\leq-\\frac{\\gamma\\eta I P}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(\\bar{x}_{\\tau})\\right\\|^{2}\\right]+\\frac{3}{2}\\gamma\\eta I^{2}\\mathbb{E}\\left[\\left\\lceil\\sum_{r=0}^{\\lfloor P-1/2\\rfloor-1}\\sum_{b=0}^{1}(D_{r,k}+M_{r,k})\\right\\rceil\\right.}\\\\ &{\\left.\\quad\\overset{(i)}{\\leq}-\\frac{\\gamma\\eta I P}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(\\bar{x}_{\\tau_{0}})\\right\\|^{2}\\right]+\\frac{3}{2}\\gamma\\eta L^{2}\\left(108\\eta^{2}I^{2}\\gamma^{3}R^{\\frac{2}{3}}\\left[\\prod\\nabla f(\\bar{x}_{\\tau_{0}})\\right]^{2}\\right.\\right.}\\\\ &{\\quad\\left.\\quad+\\left.\\left.\\left(75\\eta^{2}I^{2}P+65\\eta^{2}I^{2}P^{2}\\rho^{2}\\right)\\sigma^{2}+254\\eta^{2}L^{2}I^{2}P^{3}\\frac{1}{N}\\frac{N}{I_{1}^{2}-1}\\sum_{b=1}^{N}\\mathbb{E}\\left[S_{\\tau_{0}}^{\\prime}\\right]\\right)\\right.}\\\\ &{\\quad\\leq\\gamma\\eta I P\\left(-\\frac{1}{2}+162\\eta^{2}L^{2}I^{2}P^{2}\\right)\\mathbb{E}\\left[\\left\\|\\nabla f(\\bar{x}_{\\tau_{0}})\\right\\|^{2}\\right]+\\gamma\\eta I P\\left(113\\eta^{2}L^{2}I+98\\eta^{2}L^{2}I P\\rho^{2}\\right)\\sigma^{2}}\\\\ &{\\quad\\left.\\quad+381\\gamma\\eta^{3}L^{2}I^{3}P^{3}\\frac{1}{N}\\sum_{k=1}^{N}\\mathbb{E}\\left[S_{\\tau_{0}}^{\\prime}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $(i)$ uses Equation 4 from Lemma 1. ", "page_idx": 42}, {"type": "text", "text": "Plugging into Equation 32 and applying Lemma 2 yields ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\mathcal{Q}(x_{0},x)-\\mathcal{Q}(x,\\ell)\\Vert^{2}}\\\\ &{\\leq\\operatorname*{sup}\\Vert\\mathcal{Q}\\Vert_{L^{2}}\\leq\\alpha\\left\\{\\Vert\\mathcal{Q}(x_{0})\\Vert^{2}+\\nu\\Vert\\mathcal{Q}\\Vert^{2}+\\nu\\Vert\\mathcal{Q}\\Vert^{2}+\\nu\\Vert\\mathcal{Q}\\Vert_{L^{2}}\\leq\\nu\\mathcal{Q}\\Vert_{L^{2}}\\sigma_{0}\\right\\}\\sigma^{2}}\\\\ &{\\quad+\\operatorname*{sup}\\left\\{2\\nu^{1/2}\\rho^{1/2}\\frac{1}{N_{x}^{1/2}}\\sum_{l=1}^{N}\\frac{1}{\\nu}\\sum_{\\ell=1}^{1/2}\\vert\\mathcal{Q}\\Vert_{L^{2}}\\nu-\\mathcal{Q}_{u,\\ell}\\Vert_{L^{2}}^{\\ell}\\right\\}}\\\\ &{\\leq\\operatorname*{sup}\\left\\{2\\nu^{1/2}-\\frac{4}{N_{x}^{1/2}}\\left(1-\\mathrm{gr}^{2}\\nu^{2}\\rho^{1}\\rho^{2}\\right)\\Big\\Vert\\mathcal{Q}\\Vert_{L^{2}}\\Big\\}+\\nu\\Re\\left\\{(\\mathcal{Q}\\Vert_{H^{1}}^{\\ell})^{2/2}+\\nu\\Re\\left\\{\\mathcal{Q}\\Vert_{H^{1}}^{\\ell}\\right\\}\\rho^{2}\\right\\}\\sigma^{2}}\\\\ &{\\quad+\\operatorname*{sup}\\left\\{2\\nu^{1/2}\\rho^{\\frac{1}{2}}\\frac{1}{N_{x}^{1/2}}\\mathbb{E}\\Vert\\mathcal{Q}\\Vert_{L^{2}}^{\\ell}+\\frac{1}{1-\\lambda_{x}^{\\ell}}\\left(\\vert\\mathcal{Q}\\Vert_{H^{1}}^{\\ell}\\nu^{2}\\rho^{2}\\right)\\vert\\mathcal{Q}\\Vert_{L^{2}}\\right\\}}\\\\ &{\\quad+\\operatorname*{sup}\\left\\{\\mathcal{Q}\\Vert_{H^{1}}^{\\ell}\\Vert\\mathcal{Q}\\Vert_{H^{1}}^{\\ell}\\leq\\frac{1}{\\lambda_{x}^{\\ell}}\\left(\\Vert\\mathcal{Q}\\Vert_{H^{1}}^{\\ell}\\nu^{2}\\rho^{2}\\right)\\vert\\mathcal{Q}\\Vert_{H^{1}}^{\\ell}\\left\\{\\mathcal{Q}\\right\\}\\Vert_{H^{1}}^{\\ell}\\right\\}}\\\\ &{\\quad+\\nu\\Re\\left\\{(\\mathcal{Q}\\Vert_{H^{1}}^{\\ell}+\\lambda \n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $(i)$ uses $\\begin{array}{r}{\\eta\\le\\frac{\\sqrt{{p_{\\mathrm{sample}}}}}{60L I P}}\\end{array}$ and $\\begin{array}{r}{\\gamma\\eta\\le\\frac{p_{\\mathrm{sample}}}{60L I P}}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Using Lemma 3, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\nu\\eta L^{2}P\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left\\{S_{\\eta\\rightarrow t}^{\\nu}\\right\\}}\\\\ &{\\leq2\\nu\\eta L^{2}P\\left(\\left(2\\eta\\hat{\\sigma}_{i}^{2}L^{2}P^{2}+\\frac{20}{\\gamma\\omega\\rho}\\right)^{-2}\\eta^{2}\\hat{\\sigma}_{i}^{2}P^{2}\\right)\\Bigg\\{\\Bigg[\\eta\\tau_{i}\\left(E_{i}\\right)\\eta\\right]^{2}}\\\\ &{+\\left(2\\eta^{2}\\hat{\\sigma}_{i}^{2}+15\\eta^{2}\\left(P^{2}+\\frac{17}{\\gamma\\omega}\\right)^{2}\\eta^{2}P^{2}\\right)\\sigma^{2}\\left(1-\\frac{1}{2}\\eta\\nu\\omega_{i}\\right)\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left\\{S_{\\eta\\rightarrow t}^{\\nu}\\right\\}}\\\\ &{\\leq\\nu\\eta L^{2}P\\left(\\left(6\\sin^{2}\\!L^{2}P^{2}\\right)P^{2}+\\frac{80\\eta^{4}}{\\gamma\\omega}\\eta^{2}P^{2}\\hat{\\sigma}_{i}^{2}P^{2}\\right)\\sigma\\left[\\left(\\nu\\eta_{\\alpha}\\right)\\eta^{2}\\left(E_{i}\\right)\\right]^{2}}\\\\ &{+\\eta\\sigma P\\left(\\left(3\\eta^{2}\\hat{\\sigma}_{i}^{2}L^{2}+3\\eta^{2}\\hat{\\sigma}_{i}^{2}P^{2}\\right)P^{2}+\\frac{32\\eta^{2}}{\\gamma\\omega}\\eta^{2}P^{2}\\hat{\\sigma}_{i}^{2}P^{2}\\right)\\sigma^{2}}\\\\ &{+2\\eta\\eta L^{2}P\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left\\{S_{\\eta\\rightarrow t}^{\\nu}\\right\\}-\\eta\\omega_{i}\\nu\\eta L^{2}P\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left\\{S_{i}^{\\nu}\\right\\}}\\\\ &{\\leq\\eta\\tau L^{2}P\\left(\\left(6\\sin^{2}\\!L^{2}P^{2}+\\frac{40\\eta^{2}}{\\gamma\\omega}\\eta^{2}\\hat{\\sigma}_{i}^{2}P^{2}\\right)\\sigma^{2}\\left[\\left(\\nu\\eta_{\\alpha}\\right)\\eta^{2}\\right]\\right.}\\\\ &{\\quad\\left.+\\eta\\eta P\\left(\\left\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last inequality uses $\\begin{array}{r}{\\gamma\\eta\\le\\frac{p_{\\mathrm{sample}}}{60L I P}}\\end{array}$ . Adding this to Equation 35 and rearranging, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}[f(\\widetilde{x}_{r+{r}})]+2\\gamma\\eta L^{2}I P\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[S_{r+{r}}^{i}\\right]}}\\\\ &{\\le\\left(\\mathbb{E}[f(\\widetilde{x}_{r})]+2\\gamma\\eta L^{2}I P\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[S_{\\eta_{i}}^{i}\\right]\\right)}\\\\ &{\\quad\\gamma\\eta I P\\left(-\\frac{1}{2}+81\\eta^{2}L^{2}I^{2}P^{2}+3\\gamma\\eta L P+\\frac{40}{P a\\eta_{i}\\varepsilon}\\eta^{2}L^{2}I^{2}P^{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\nabla f(\\widetilde{x}_{r})\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\gamma\\eta I P\\left(\\frac{7}{2}\\gamma\\eta L\\rho^{2}+669\\eta^{2}L^{2}I+488\\eta^{2}L^{2}I P\\rho^{2}\\right)\\sigma^{2}}\\\\ &{\\le\\left(\\mathbb{E}[f(\\widetilde{x}_{r})]+2\\gamma\\eta L^{2}I P\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[S_{\\eta_{i}}^{i}\\right]\\right)}\\\\ &{\\quad-\\frac{1}{5}\\gamma\\eta I P\\left[\\left\\Vert\\nabla f(\\widetilde{x}_{r})\\right\\Vert^{2}+\\gamma\\eta I P\\left(4\\gamma\\eta L\\rho^{2}+669\\eta^{2}L^{2}I+488\\eta^{2}L^{2}I P\\rho^{2}\\right)\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we used /n \u2264 boLIP and $\\begin{array}{r}{\\eta\\le\\frac{1}{60L I P}}\\end{array}$ Finally, we can average over $r_{0}\\in\\{0,P,2P,\\dots,R-$ $P\\}$ and rearrange to obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{P}{R}\\sum_{\\substack{r_{0}\\in\\{0,P,\\dots,R-P\\}}}\\mathbb{E}[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}]\\leq5\\frac{\\mathbb{E}[f(\\bar{x}_{0})-f(\\bar{x}_{R})]}{\\gamma\\eta I R}+10\\frac{L^{2}P}{R}\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}\\left[S_{0}^{i}-S_{R}^{i}\\right]}&{}\\\\ {\\displaystyle+\\left(20\\gamma\\eta L\\rho^{2}+3345\\eta^{2}L^{2}I+2440\\eta^{2}L^{2}I P\\rho^{2}\\right)\\sigma^{2}}&{}\\\\ {\\displaystyle\\overset{(i)}{\\leq}\\frac{5\\Delta}{\\gamma\\eta I R}+\\left(20\\gamma\\eta L\\rho^{2}+3345\\eta^{2}L^{2}I+2440\\eta^{2}L^{2}I P\\rho^{2}\\right)\\sigma^{2}}&{}\\\\ {\\displaystyle\\overset{(i i)}{\\leq}\\frac{5\\Delta}{\\gamma\\eta I R}+\\left(20\\gamma\\eta L\\rho^{2}+5785\\eta^{2}L^{2}I P\\right)\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $(i)$ uses $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}[S_{R}^{i}]\\geq0}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}[S_{0}^{i}]=0}\\end{array}$ by initialization, and $(i i)$ uses $P\\geq1$ and $\\rho\\leq1$ \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Corollary 2 (Corollary 1 restated). Let $\\epsilon>0$ and $I\\geq1$ . Suppose Assumptions 1 and 2 hold, and that $\\begin{array}{r}{\\mathbb{E}[w_{r_{0}}^{i}|\\mathcal{Q}_{:r_{0}}]\\leq\\frac{P^{2}}{N}}\\end{array}$ and $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{i=1}^{N}\\left(v_{r_{0}}^{i}\\right)^{2}\\Lambda_{r_{0}}^{i}\\right]\\leq\\rho^{2}}\\end{array}$ If ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle R\\ge\\frac{\\Delta L\\rho^{2}\\sigma^{2}}{I\\epsilon^{4}}+\\frac{300\\Delta L P}{p_{s a m p l e}\\epsilon^{2}}}}\\\\ {{\\displaystyle\\eta\\le\\frac{1}{264}\\operatorname*{min}\\left\\{\\frac{\\Delta^{1/4}\\rho^{1/2}}{L^{3/4}I^{3/4}R^{1/4}P^{1/2}\\sigma^{1/2}},\\frac{\\rho\\sqrt{p_{s a m p l e}}}{L I P}\\right\\}}}\\\\ {{\\displaystyle\\gamma=\\frac{1}{\\eta}\\operatorname*{min}\\left\\{\\frac{\\sqrt{\\Delta}}{60\\sqrt{L I R}\\rho\\sigma},\\frac{p_{s a m p l e}}{60L I P}\\right\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "thenAlgorithm $^{\\,l}$ satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{P}{R}\\sum_{r_{0}\\in\\{0,P,\\dots,R-P\\}}\\mathbb{E}[\\|\\nabla f(\\bar{\\mathbf{x}}_{r_{0}})\\|^{2}]\\leq302\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof.First, $\\begin{array}{r}{\\eta\\le\\frac{\\rho\\sqrt{p_{\\mathrm{sample}}}}{264L I P}\\le\\frac{\\sqrt{p_{\\mathrm{sample}}}}{60L I P}}\\end{array}$ and /\u22646oL1P together with Assumptions 1 and 2 imply that the conditions of Theorem 2 are satisfied. Therefore ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{P}{R}\\sum_{r_{0}\\in\\{0,P,\\ldots,R-P\\}}\\mathbb{E}[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}]\\leq\\frac{5\\Delta}{\\gamma\\eta I R}+\\left(20\\gamma\\eta L\\rho^{2}+5785\\eta^{2}L^{2}I P\\right)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "From our choice of $\\eta$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{5785\\eta^{2}L^{2}I P\\le\\frac{5785}{264^{2}}L^{2}I P\\operatorname*{min}\\bigg\\{\\frac{\\Delta^{1/2}\\rho}{L^{3/2}I^{3/2}R^{1/2}P\\sigma},\\frac{\\rho^{2}p_{\\mathrm{sample}}}{L^{2}I^{2}P^{2}}\\bigg\\}}\\\\ &{\\qquad\\qquad\\le5L\\rho^{2}\\operatorname*{min}\\bigg\\{\\frac{\\Delta^{1/2}}{60L^{1/2}I^{1/2}R^{1/2}\\rho\\sigma},\\frac{p_{\\mathrm{sample}}}{60L I P}\\bigg\\}}\\\\ &{\\qquad\\qquad=5\\gamma\\eta L\\rho^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Therefore ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{P}{R}\\sum_{\\substack{r_{0}\\in\\{0,P,\\dots,R-P\\}}}\\mathbb{E}[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}]\\leq\\displaystyle\\frac{5\\Delta}{\\gamma\\eta I R}+25\\gamma\\eta L\\rho^{2}\\sigma^{2}}&{}\\\\ {\\displaystyle\\leq\\frac{5\\Delta}{I R}\\left(\\frac{60\\sqrt{L I R}\\rho\\sigma}{\\sqrt{\\Delta}}+\\frac{60L I P}{p_{\\mathrm{sample}}}\\right)+25L\\rho^{2}\\sigma^{2}\\frac{\\sqrt{\\Delta}}{60\\sqrt{L I R}\\rho\\sigma}}&{}\\\\ {\\displaystyle}&{\\leq\\frac{301\\sqrt{\\Delta L}\\rho\\sigma}{\\sqrt{I R}}+\\frac{300\\Delta L P}{p_{\\mathrm{sample}}\\,R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we used our choice of $\\gamma\\eta$ . Finally, from our choice of $R$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{P}{R}\\displaystyle\\sum_{\\substack{r_{0}\\in\\{0,P,\\ldots,R-P\\}}}\\mathbb{E}[\\|\\nabla f(\\bar{x}_{r_{0}})\\|^{2}]\\leq\\frac{301\\sqrt{\\Delta L}\\rho\\sigma}{\\sqrt{I}}\\frac{\\sqrt{I}\\epsilon^{2}}{\\sqrt{\\Delta L}\\rho\\sigma}+\\frac{300\\Delta L P}{p_{\\mathrm{sample}}}\\frac{p_{\\mathrm{sample}}\\epsilon^{2}}{300\\Delta L P}}\\\\ {\\leq302\\epsilon^{2}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "B Proofs for Specific Participation Patterns ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Here we derive the convergence rates for the specific participation patterns discussed in Section 4.3. In order to apply Corollary 1, the conditions in Assumption 2, Equation 1, and Equation 2 must be satisfied. Since we already showed that Assumption 2 is satisfied by regularized participation and cyclic participation (in Section 3.2), it only remains to show that Equation 1 and Equation 2 is satisfied, and plug the appropriate values of $\\rho^{2},P$ and $p_{\\mathrm{sample}}$ into Corollary 1. ", "page_idx": 44}, {"type": "text", "text": "Corollary 3 (Regularized Participation). Under regularized participation, for any $\\epsilon>0$ thereexist choicesof $\\eta,\\gamma,I$ and $R$ suchthat $\\mathbb{E}[\\|\\bar{\\nabla}f(\\hat{\\pmb{x}})\\|^{2}]\\le O(\\epsilon^{2})$ and ", "page_idx": 45}, {"type": "equation", "text": "$$\nR=\\mathcal{O}\\left(\\frac{L P}{\\epsilon^{2}}\\right),\\quad R I=\\mathcal{O}\\left(\\frac{\\Delta L\\rho^{2}\\sigma^{2}}{\\epsilon^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. We first show that Equation 1 and Equation 2 are satisfied under regularized participation. Let $r_{0}\\in\\left\\{0,\\ldots,R-P\\right\\}$ Using the fact that $\\begin{array}{r}{\\bar{q}_{r_{0}}^{\\bar{i}}=\\frac{1}{N}}\\end{array}$ almost surly: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[w_{r_{0}}^{i}\\middle|Q_{:r_{0}}\\right]=\\mathbb{E}\\left[\\frac{1}{N}\\sum_{j=1}^{N}\\frac{\\mathbb{I}\\left\\{\\bar{q}_{r_{0}}^{j}>0\\right\\}}{P\\bar{q}_{r_{0}}^{j}}\\sum_{s=r_{0}}^{r_{0}+P-1}q_{s}^{i}q_{s}^{j}\\biggr|Q_{:r_{0}}\\right]=\\frac{1}{P}\\mathbb{E}\\left[\\sum_{j=1}^{N}\\sum_{s=r_{0}}^{r_{0}+P-1}q_{s}^{i}q_{s}^{j}\\biggr|Q_{:r_{0}}\\right]}\\\\ &{\\qquad\\qquad=\\frac{1}{P}\\mathbb{E}\\left[\\sum_{s=r_{0}}^{r_{0}+P-1}q_{s}^{i}\\sum_{j=1}^{N}q_{s}^{j}\\biggr|Q_{:r_{0}}\\right]=\\frac{1}{P}\\mathbb{E}\\left[\\sum_{s=r_{0}}^{r_{0}+P-1}q_{s}^{i}\\biggr|Q_{:r_{0}}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[q_{r_{0}}^{i}\\middle|Q_{:r_{0}}\\right]=\\frac{1}{N}\\leq\\frac{P^{2}}{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where we used the fact that $\\textstyle\\sum_{i=1}^{N}q_{r}^{i}=1$ This shows that Equation 1 is satisfied. Also, $\\begin{array}{r}{\\bar{q}_{r_{0}}^{i}=\\frac{1}{N}}\\end{array}$ means hat $v_{r_{0}}^{i}=0$ so that $\\mathbb{E}\\left[\\left(v_{r_{0}}^{i}\\right)^{2}\\Lambda_{r_{0}}^{i}\\right]=0\\leq\\rho^{2}$ and Eqguation is satisfed. Therefore we can apply Corollary 1. The complexity result follows by plugging in $p_{\\mathrm{sample}}\\,=\\,1$ and choosing $\\begin{array}{r}{I=\\mathcal{O}\\left(\\frac{\\Delta\\rho^{2}\\sigma^{2}}{P\\epsilon^{2}}\\right)}\\end{array}$ \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Corollary 4 (Cyclic Participation). Under cyclic participation with $\\bar{K}$ groups and $S$ participating clients in each round, for any $\\epsilon>0$ there exist choices of $\\eta,\\gamma,P,I$ and $R$ such that $\\mathbb{E}[\\|\\nabla f({\\hat{\\mathbf{x}}})\\|^{2}]\\leq$ $O(\\epsilon^{2})$ and ", "page_idx": 45}, {"type": "equation", "text": "$$\nR=\\mathcal{O}\\left(\\frac{L\\bar{K}}{\\epsilon^{2}}\\left(\\frac{N}{S}\\right)\\right),\\quad R I=\\mathcal{O}\\left(\\frac{\\Delta L\\sigma^{2}}{S\\epsilon^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. Again, we show that Equation 1 and Equation 2 are satisfied under cyclic participation. Let $r_{0}\\,\\in\\,\\{0,\\dots,R-P\\}$ . Denoting $\\begin{array}{r}{g(i)=\\lfloor\\frac{\\bar{K}}{N}(\\bar{i}-1)\\rfloor}\\end{array}$ and $r(i)=r_{0}+g(i)$ , it holds that, over the rounds $r\\in\\{r_{0},\\ldots,r_{0}+P-1\\}$ , client $i$ belongs to group $g(i)$ and is available only during round $r(i)$ . Also, let $\\Pi(i)$ denote the set of indices of clients in group $\\dot{g}(i)$ . To simplify $w_{r_{0}}^{i}$ , recall that for client $i$ in group $(r\\mod{\\bar{K}})$ \uff1a ", "page_idx": 45}, {"type": "equation", "text": "$$\nq_{r}^{i}=\\left\\{\\frac{1}{S}\\quad\\mathrm{with~probability~}\\frac{S}{N}\\right._{\\quad\\mathrm{{with}~p r o b a b i l i t y}\\;1-\\frac{S}{N}}\\,,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and $q_{r}^{i}=0$ for all clients $i$ not in group ( $r$ mod $\\bar{K}$ ). So ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E\\left[u_{j+1}^{m}\\big(E_{j}\\big)_{i}\\right]=}&{=\\left[\\left[\\begin{array}{l l l}{1}&{\\frac{\\mathrm{S}}{\\nu}\\frac{1}{\\nu+1}\\frac{\\{E_{j}(\\frac{\\nu}{\\nu})_{j}\\}{\\nu+\\frac{\\nu}{2}}+\\nu^{2}\\nu+1}{4\\nu(E_{j})}\\right]\\alpha_{\\alpha\\alpha}\\right]}\\\\ &{=}&{1\\quad\\mathrm{S}\\left[\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right]\\frac{1}{2}\\Biggl(E_{j}\\Biggr)}\\\\ &{=}&{1\\quad\\mathrm{S}\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right]\\frac{1}{2}\\Biggl(E_{j}\\Biggr)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\times\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right]\\left(E_{j}\\Biggr)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\times\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right]\\left(E_{j}\\Biggr)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\times\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right]\\left(E_{j}\\Biggr)}\\\\ &{=}&{1\\quad\\mathrm{S}\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right]\\left(E_{j}\\Biggr)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\times\\left[\\Biggl(E_{j}\\Biggr)_{i}\\right]=}&{1\\quad\\mathrm{S}\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right]\\left(E_{j}\\Biggr)}\\\\ &{=}&{1\\quad\\mathrm{S}\\left[\\Biggl(E_{j}\\Biggr)_{i}-\\mathrm{~Pe~}\\right]\\left(E_{j}\\Biggr)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\times\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\mathrm{~Pe~}\\right)\\left(E_{j}\\Biggr)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\times\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\Biggl(E_{j}\\Biggr)_{i}\\right]\\times}\\\\ &{=}&{1\\quad\\mathrm{S}\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\Biggl(E_{j}\\Biggr)_{i}\\right]}\\\\ &{=}&{1\\quad\\mathrm{S}\\left[\\Biggl(E_{j}\\Biggr)_{i}+\\Biggl(E_{j}\\Biggr)_{i}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $(i)$ uses $q_{r}^{i}=0$ for all $r\\neq r(i),\\,(i i)$ uses $q_{r(i)}^{j}=0$ for all $j\\not\\in\\Pi(i),\\:(i i i)$ uses the fact that $\\bar{q}_{r_{0}}^{j}>0$ implies $\\begin{array}{r}{q_{r(i)}^{j}=\\frac{1}{S}}\\end{array}$ and $\\begin{array}{r}{\\bar{q}_{r_{0}}^{j}=\\frac{1}{S P}}\\end{array}$ ,and $(i v)$ uses the fact that $S$ clents ae sampled i each round. This shows that Equation 1 is satisfied. ", "page_idx": 46}, {"type": "text", "text": "To see that Equation 2 is satisfied: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{N}\\left(v_{r_{0}}^{i}\\right)^{2}\\Lambda_{r_{0}}^{i}\\right]\\stackrel{(i)}{=}\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}\\left[\\left(\\left(\\Bar{q}_{r_{0}}^{i}-\\frac{1}{N}\\right)^{2}\\right)^{2}\\bigg|\\mathbb{Q}_{r_{0}}\\right]\\Lambda_{r_{0}}^{i}\\right]}\\\\ &{\\stackrel{(i i)}{=}\\frac{1}{S N P}\\left(1-\\frac{S\\bar{K}}{N}\\right)\\displaystyle\\sum_{i=1}^{N}\\mathbb{E}\\left[\\Lambda_{r_{0}}^{i}\\right]}\\\\ &{\\stackrel{(i)}{=}\\frac{1}{S N P}\\left(1-\\frac{S\\bar{K}}{N}\\right)\\displaystyle\\sum_{i=1}^{N}\\left(\\frac{1}{P}_{s=r_{0}-P}^{\\left(i\\right)}\\left(z_{s}^{i}\\right)^{2}\\bigg/\\left(\\frac{1}{P}_{s=r_{0}-P}^{\\left(i\\right)}z_{s}^{i}\\right)^{2}\\right)}\\\\ &{\\stackrel{(i i i)}{=}\\frac{1}{S P}\\left(1-\\frac{S\\bar{K}}{N}\\right)\\displaystyle\\frac{1}{P}\\frac{1}{S}{S^{2}}\\bigg/\\left(\\frac{1}{P}\\frac{1}{S}\\right)^{2}}\\\\ &{=\\frac{1}{S}\\left(1-\\frac{S\\bar{K}}{N}\\right)\\leq\\frac{1}{S}=\\rho^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $(i)$ uses the tower property, $(i i)$ uses the variance of $\\bar{q}_{r_{0}}^{i}$ as computed in Appendix C, and $(i i i)$ uses the fact that $\\Bar{z}_{r_{0}}^{i}\\;>\\;0$ by construction of $\\Bar{z}_{r_{0}}^{i}$ , and in this case $\\begin{array}{r}{z_{r}^{i}\\,=\\,\\frac{1}{S}}\\end{array}$ for exactly one $r\\in\\{r_{0}-P,\\ldots,r_{0}-1\\}$ and $z_{r}^{i}=0$ for all other $r$ . Therefore Equation 2 is satisfied. ", "page_idx": 46}, {"type": "text", "text": "This shows we can apply Corollary 1. The complexity result follows by plugging in $\\begin{array}{r}{p_{\\mathrm{sample}}=\\frac{S}{N}}\\end{array}$ $\\begin{array}{r}{P=\\bar{K},\\rho=\\frac{1}{\\sqrt{S}}}\\end{array}$ andchoosing $\\begin{array}{r}{I=\\mathcal{O}\\left(\\frac{\\Delta\\sigma^{2}}{\\bar{K}N\\epsilon^{2}}\\right)}\\end{array}$ \u53e3 ", "page_idx": 46}, {"type": "text", "text": "C  Complexity of Amplified FedAvg ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In this section we derive the computation and communication complexity required for Amplified FedAvg [38] to find an $\\epsilon_{\\mathrm{:}}$ -stationary point under various client participation patterns, which we listed in Table 1 of the main paper. Table 1 compares the complexity under i.i.d. participation, regularized participation, and cyclic participation. Since i.i.d. participation is a special case of cyclic participation with $\\bar{K}=1$ groups, here we only consider regularized and cyclic participation, and the result for i.i.d. participation follows. ", "page_idx": 47}, {"type": "text", "text": "Many works in federated learning characterize data heterogeneity by assuming that there exists a constant $\\kappa$ suchthat ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i}(\\pmb{x})-\\nabla f(\\pmb{x})\\|\\le\\kappa,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for all $\\textbf{\\em x}$ . The previous analysis of Amplified FedAvg [38] instead assumes an upper bound $\\tilde{\\delta}(P)$ a weighted heterogeneity that depends on client sampling. Specifically, they assume that there exists $\\tilde{\\delta}(P)$ such that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{{\\cal{P}}}\\sum_{r=r_{0}}^{r_{0}+P-1}\\sum_{i=1}^{N}q_{r}^{i}(\\nabla f_{i}({\\pmb x})-\\nabla f({\\pmb x}))\\right\\|^{2}\\leq\\tilde{\\delta}^{2}({\\cal{P}}),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for all $\\textbf{\\em x}$ and $r_{0}$ . We restate their Corollary 3.2 for convenience: ", "page_idx": 47}, {"type": "text", "text": "Corollary 5 (Corollary 3.2 [38] informally restated). There exist parameter choices such that AmplifiedFedAvgsatisfies ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r}\\mathbb{E}\\left[\\|\\nabla f(\\bar{x}_{r})\\|^{2}\\right]\\leq\\mathcal{O}\\left(\\frac{\\sqrt{\\Delta L}\\rho\\sigma}{\\sqrt{R I}}+\\frac{\\Delta L P+\\kappa^{2}}{R}+\\frac{\\sigma^{2}}{R I P}+\\mathbb{E}\\left[\\tilde{\\delta}^{2}(P)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "As pointed out in their Section 4.2, we can interpret $\\tilde{\\delta}(P)$ in terms of the conventional heterogeneity constant $\\kappa$ as: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\tilde{\\delta}^{2}(P)\\leq N\\kappa^{2}\\sum_{i=1}^{N}\\left(\\bar{q}_{r_{0}}^{i}-\\frac{1}{N}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Our Assumption 2(b) implies that $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{Q}_{r_{0}}}\\big[\\bar{q}_{r_{0}}^{i}\\big]=\\frac{1}{N}}\\end{array}$ .If we choose $v\\geq0$ such that $\\mathrm{Var}[\\bar{q}_{r_{0}}^{i}]\\leq v^{2}$ , then we can take expecation of the above to obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\tilde{\\delta}^{2}(P)\\right]\\leq N^{2}\\kappa^{2}v^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This \u201cconversion\" of $\\tilde{\\delta}(P)$ to $\\kappa$ and $v$ will allow us to compare their complexity to that of algorithms that use the conventional heterogeneity assumption by computing $v$ for each participation pattern. ", "page_idx": 47}, {"type": "text", "text": "Regularized Participation In this case, $\\begin{array}{r}{\\bar{q}_{r_{0}}^{i}=\\frac{1}{N}}\\end{array}$ almost surely, so that $v=0$ and accordingly $\\tilde{\\delta}^{2}(P)=0$ .Therefore ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r}\\mathbb{E}\\left[\\|\\nabla f(\\bar{\\mathbf{x}}_{r})\\|^{2}\\right]\\leq\\mathcal{O}\\left(\\frac{\\sqrt{\\Delta L}\\rho\\sigma}{\\sqrt{R I}}+\\frac{\\Delta L P+\\kappa^{2}}{R}+\\frac{\\sigma^{2}}{R I P}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Therefore the choices ", "page_idx": 47}, {"type": "equation", "text": "$$\nR\\ge\\mathcal{O}\\left(\\frac{\\Delta L P+\\kappa^{2}}{\\epsilon^{2}}\\right),\\quad R I\\ge\\mathcal{O}\\left(\\frac{\\Delta L\\rho^{2}\\sigma^{2}}{\\epsilon^{4}}+\\frac{\\sigma^{2}}{P\\epsilon^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "imply ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r}\\mathbb{E}\\left[\\|\\nabla f(\\bar{\\boldsymbol{x}}_{r})\\|^{2}\\right]\\le\\mathcal{O}\\left(\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Cyclic Participation  We can compute $v$ in terms of the parameters of the participation pattern: number of groups $\\bar{K}$ and number of participating clients $S$ in each round. Although we chose ${\\dot{P}}={\\bar{K}}$ for Amplified SCAFFOLD, we can satisfy Assumption(b) 2 by choosing $P=\\bar{m}\\bar{K}$ for any $m\\in\\mathbb{N}$ and indeed to achieve $\\mathbb{E}[\\tilde{\\delta}^{2}(P)]\\leq\\epsilon^{2}$ we must choose $P=m{\\bar{K}}$ with $m$ depending on $\\epsilon$ ", "page_idx": 47}, {"type": "text", "text": "Let $A(i)$ denote the set of rounds in $\\{r_{0},\\ldots,r_{0}+P-1\\}$ during which client $i$ is available. Note that $A(i)$ has size $m$ , since $P=m{\\bar{K}}$ . Then ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{i}{r_{0}}-\\frac{1}{N}=\\frac{1}{P}\\sum_{r=r_{0}}^{r_{0}+P-1}q_{r}^{i}-\\frac{1}{N}\\stackrel{(i)}{=}\\frac{1}{P}\\sum_{r\\in A(i)}q_{r}^{i}-\\frac{1}{N}=\\frac{1}{m\\bar{K}}\\sum_{r\\in A(i)}q_{r}^{i}-\\frac{1}{b\\bar{K}}=\\frac{1}{\\bar{K}}\\left(\\frac{1}{m}\\sum_{r\\in A(i)}q_{r}^{i}-\\frac{1}{b}\\right),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $(i)$ uses the fact that $q_{r}^{i}=0$ for all $r\\not\\in A(i)$ . Therefore ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{2}=\\frac{1}{K^{2}}\\mathbb{E}\\left[\\left(\\frac{1}{m}\\displaystyle_{r\\in A(i)}q_{r}^{i}-\\frac{1}{b}\\right)^{2}\\right]}\\\\ &{\\overset{(i)}{=}\\frac{1}{K^{2}}\\frac{1}{m^{2}}\\displaystyle_{r\\in A(i)}^{\\mathrm{~\\tiny~C~}}\\mathbb{E}\\left[\\left(q_{r}^{i}-\\frac{1}{b}\\right)^{2}\\right]}\\\\ &{\\overset{(i i)}{=}\\frac{1}{K^{2}}\\displaystyle\\frac{1}{m^{2}}\\displaystyle_{r\\in A(i)}^{\\mathrm{~\\tiny~D~}}\\mathbb{E}^{2}\\left(1-\\frac{S}{b}\\right)}\\\\ &{=\\frac{1}{S m\\bar{K}^{2}\\bar{b}}\\left(1-\\frac{S}{\\bar{b}}\\right)}\\\\ &{=\\frac{1}{S\\bar{N}P}\\left(1-\\frac{S\\bar{K}}{\\bar{N}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $(i)$ uses the fact that $\\{q_{r}^{i}\\}_{r\\in A(i)}$ are independent and $(i i)$ uses the fact that $q_{r}^{i}$ equals $\\textstyle{\\frac{1}{S}}$ times a Bernoulli variable for $r\\in A(i)$ ", "page_idx": 48}, {"type": "text", "text": "With a bound for $v^{2}$ , we can bound the remaining term in Equation 36 as follows: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tilde{\\delta}^{2}(P)]\\leq N^{2}\\kappa^{2}v^{2}\\leq\\frac{\\kappa^{2}}{P}\\frac{N}{S}\\left(1-\\frac{S\\bar{K}}{N}\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Therefore, Amplified FedAvg under cyclic participation can find an $\\epsilon$ -stationary point with the choices ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\ge\\operatorname*{max}\\left\\{\\bar{K},\\frac{\\kappa^{2}}{\\epsilon^{2}}\\displaystyle\\frac{N}{S}\\left(1-\\frac{S\\bar{K}}{N}\\right)\\right\\}}\\\\ &{R\\ge\\mathcal{O}\\left(\\frac{\\Delta L\\bar{K}+\\kappa^{2}}{\\epsilon^{2}}+\\frac{\\Delta L N\\kappa^{2}}{S\\epsilon^{4}}\\left(1-\\frac{S\\bar{K}}{N}\\right)\\right)}\\\\ &{R I\\ge\\mathcal{O}\\left(\\frac{\\Delta L\\sigma^{2}}{S\\epsilon^{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "D Experiment Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Here we discuss experimental details deferred from the main body: client sampling parameters, heterogeneity protocol, hyperparameter tuning, definition of the synthetic objective, and specification of the CNN architecture used for the image classification experiments. ", "page_idx": 48}, {"type": "text", "text": "D.1   Client Sampling Parameters ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "For the synthetic objective, we set the number of groups $\\bar{K}=2$ and the availability time $g=240$ We set the communication interval $I=10$ and train for $R=5000$ rounds. ", "page_idx": 48}, {"type": "text", "text": "For Fashion-MNIST, we set the communication interval $I=30$ and train a logistic regression model for $R=2000$ rounds, with availability time $g=4$ ", "page_idx": 48}, {"type": "text", "text": "For CIFAR-10, we set the communication interval $I=5$ and train a two-layer CNN for $R=12000$ rounds, with availability time $g=10$ ", "page_idx": 48}, {"type": "table", "img_path": "WftaVkL6G2/tmp/ef883904c8753384eaa5adfba40b22fe9bb6f4ab090df9fa96ca9b21fff0fb33.jpg", "table_caption": ["Table 2: Hyperparameter search ranges and final values. "], "table_footnote": [], "page_idx": 49}, {"type": "text", "text": "D.2  Heterogeneity Protocol ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "The following protocol is commonly used in the literature [16, 38] to convert a dataset into a collection of heterogeneous local datasets according to a data similarity parameter $s$ where $s=0\\%$ creates maximal data heterogeneity across clients, and $s=100\\%$ means that data is allocated to each client uniformly at random. ", "page_idx": 49}, {"type": "text", "text": "A single, non-federated dataset (e.g. CIFAR-10) is partitioned into two subsets: $s\\%$ of the samples are allocated to an i.i.d. pool and randomly shuffled, and the remaining $(100-s)\\%$ of the sampled are allocated to a non-i.i.d. pool and are sorted by label. Samples are allocated to each client so that $s\\%$ of each local dataset comes from the i.i.d. pool, and the remaining $(100-s)\\%$ comes from the non-i.i.d. pool, so that with a small $s$ , the majority of each local dataset consists of a small number of labels. ", "page_idx": 49}, {"type": "text", "text": "D.3 Hyperparameter Tuning ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "For all three experiments in the main body (synthetic objective, Fashion-MNIST, and CIFAR-10), each of the four baselines are individually tuned with grid search. For algorithms that use amplified updates (Amplified FedAvg and Amplified SCAFFOLD), we tune the amplification rate $\\gamma$ by searching over a fixed range of values. For the other algorithms (FedAvg and SCAFFOLD), we indicate the lack of amplified updates by setting $\\gamma=1$ .We tune $\\eta$ by allowing $\\gamma\\eta$ over a fixed range of values. For FedProx's $\\mu$ parameter, we also search over a fixed range of values. The search range and final values for each parameter are written in Table 2, along with the final values adopted for each algorithm. ", "page_idx": 49}, {"type": "text", "text": "D.4  Synthetic Objective ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "For the synthetic experiment, we use a difficult objective from a lower bound analysis of FedAvg [42]. As defined in the lower bound analysis, the objective is parameterized by $H,\\kappa,\\sigma,c,\\mu$ and $L$ ", "page_idx": 49}, {"type": "image", "img_path": "WftaVkL6G2/tmp/aca548c66d738cb843f81fdaea074776fd3066db8bb0a4d0acbbcb3e5334135c.jpg", "img_caption": ["Figure 3: FashionMNIST with additional baselines. Amplified SCAFFOLD maintains the best performance. "], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "The objective maps $\\mathbb{R}^{4}$ to $\\mathbb{R}$ , and there are only two clients with corresponding local objectives ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}({\\pmb x})=\\displaystyle\\frac{\\mu}{2}\\left(x_{1}-c\\right)^{2}+\\frac{H}{2}\\left(x_{2}-\\frac{\\sqrt{\\mu}c}{\\sqrt{H}}\\right)^{2}+\\frac{H}{8}\\left(x_{3}^{2}+[x_{3}]_{+}^{2}\\right)+\\kappa x_{4}}}\\\\ {{f_{2}({\\pmb x})=\\displaystyle\\frac{\\mu}{2}\\left(x_{1}-c\\right)^{2}+\\frac{H}{2}\\left(x_{2}-\\frac{\\sqrt{\\mu}c}{\\sqrt{H}}\\right)^{2}+\\frac{H}{8}\\left(x_{3}^{2}+[x_{3}]_{+}^{2}\\right)-\\kappa x_{4},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $[x]_{+}:=\\operatorname*{max}\\left\\{x,0\\right\\}$ The stochastic gradients for $f_{1}$ and $f_{2}$ are sampled from the distributions $\\nabla f_{1}({\\pmb x})\\!+\\!\\xi e_{3}$ and $\\nabla f_{2}({\\pmb x})\\!+\\!\\xi e_{3}$ , respectively, where $\\xi\\sim\\mathcal{N}(0,\\sigma^{2})$ and $e_{3}$ denotes the third standard basis vector in $\\mathbb{R}^{4}$ . We set the parameters of the objective as follows: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{H=16,\\quad\\kappa=16,\\quad\\sigma=1}}\\\\ {{c=1,\\quad\\mu=2,\\quad L=2.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "D.5 CNN Architecture ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We use a simple 2-layer CNN for CIFAR-10. The first layer is a convolutional layer with 64 channels, $5\\times5$ kernel, stride of 2, padding of 2, and a ReLU activation. The second layer is a fully connected layer with no activation. ", "page_idx": 50}, {"type": "text", "text": "E  Additional Experimental Results ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "In this section, we provide two additional experimental results. First, in Section E.1, we add four baselines to the experimental settings of the main paper: FedAdam [31], FedYogi [31], FedAvg-M [4], and Amplified FedAvg with FedProx regularization. Second, in Section E.2, we evaluate all nine algorithms (five from the main paper and the four additional baselines) under another non-i.i.d. client participation pattern for the CIFAR-10 dataset, which we refer to as Stochastic Client Availability (SCA). ", "page_idx": 50}, {"type": "text", "text": "E.1Additional Baselines ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We evaluate the four additional baselines for the Fashion-MNIST and CIFAR-10 experiments from Section 5, keeping the same experimental setup. We tuned the hyperparameters of all baselines according to the hyperparameter ranges suggested in the original paper of each algorithm, and we allow the same compute budget for tuning each baseline as we did for tuning the algorithms in the original paper, in terms of the total number of hyperparameter combinations evaluated. Also, the results are averaged over five random seeds. The results are shown in Figures 3 and 4. ", "page_idx": 50}, {"type": "text", "text": "For FashionMNIST, FedAdam and FedYogi reach moderate training loss quickly, but are soon overtaken by Amplified SCAFFOLD and later by SCAFFOLD. FedAvg-M exhibits a minor advantage over FedAvg, but performs about the same as Amplified FedAvg. Amplified FedProx (i.e. Amplified FedAvg with FedProx regularization) performs nearly identically to Amplified FedAvg. ", "page_idx": 50}, {"type": "image", "img_path": "WftaVkL6G2/tmp/9ced88601d5477536cd2b504883aa60e5ac279bca7462e92f7a3136691d482db.jpg", "img_caption": ["Figure 4: CIFAR-10 with additional baselines. FedAdam is competitive, but Amplified SCAFFOLD maintains superiority. "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "For CIFAR-10, FedAdam is more competitive, but is still outperformed by Amplified SCAFFOLD. FedYogi and FedAvg-M are further behind, though both still outperform SCAFFOLD. Amplified FedProx is again nearly identical to Amplified FedAvg. ", "page_idx": 51}, {"type": "text", "text": "These additional comparisons demonstrate that Amplified SCAFFOLD outperforms strong empirical baselines (FedAdam, FedYogi) under cyclic client participation, reinforcing the empirical validation of our algorithm. This performance is consistent with the fact that Amplified SCAFFOLD has convergence guarantees under periodic participation, while FedAdam and FedYogi were not designed for settings beyond i.i.d. client sampling. ", "page_idx": 51}, {"type": "text", "text": "E.2CIFAR-10 with Stochastic Client Availability ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Here, we include an evaluation under another non-i.i.d. participation pattern, which we refer to as Stochastic Cyclic Availability (SCA). SCA models device availability which is both periodic and unreliable. Similarly to cyclic participation, the set of clients is divided into groups, and at each round one group is deemed the \"active\" group, while the others are inactive. Unlike cyclic participation, in SCA not every client in the active group is always available: Instead, when a group becomes active, the clients in that group become available for sampling with probability $80\\bar{\\%}$ , while clients in inactive groups have probability $5\\%$ to be available for participation. The active group changes every $g$ rounds. This stochastic availability models the real-life situation where a client device can be unavailable at a time of day when it is usually available, or vice versa. In this way, SCA is more flexible than cyclic participation and better captures the unreliability of client devices. Lastly, we reused the remaining settings $(g,\\bar{K},I$ , etc.) and the tuned hyperparameters for each baseline from the CIFAR-10 experiment under cyclic participation. Again, we average each algorithm's performance over five random seeds. ", "page_idx": 51}, {"type": "text", "text": "Results for CIFAR-10 under SCA participation are shown in Figure 5. Again, Amplified SCAFFOLD outperforms all baselines under SCA participation. The relative performance of each baseline is similar as under cyclic participation, with FedAdam staying competitive with Amplified SCAFFOLD, followed by FedYogi and FedAvg-M. The remainder of the baselines have significantly worse performance, and again Amplified FedAvg has not benefitted by adding FedProx regularization. ", "page_idx": 51}, {"type": "text", "text": "F Extended Comparison with Baselines ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Here we include an extended comparison against two relevant prior works. ", "page_idx": 51}, {"type": "text", "text": "FedAvg with Cyclic Participation  [7] analyzes FedAvg under cyclic participation, but the resulting convergence rate does not benefit from local steps unless regularized participation is satisfied. They analyze FedAvg for $L$ -smooth and $\\mu$ -PL objectives. Using their notation, $\\bar{K}$ is the number of client groups, $\\kappa$ is the condition number of the objective, $\\gamma$ is the intra-group heterogeneity, $M$ is the total number of clients, $N$ is the number of clients that participate in each round, and $T$ is the number of communication rounds. Then the dominating term in their convergence rate for FedAvg under cyclic ", "page_idx": 51}, {"type": "image", "img_path": "WftaVkL6G2/tmp/9d48959ad4af7ec65d734f823d9120e31e72ca33b471295a8f6305a05ecd92ce.jpg", "img_caption": ["Figure 5: CIFAR-10 under SCA (stochastic cyclic availability). Amplified SCAFFOLD converges fastest. ", "CIFAR-10 (SCA Participation) "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "participation (Theorem 2) is ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\frac{\\bar{K}\\kappa\\gamma^{2}}{\\mu N T}\\left(\\frac{M/\\bar{K}-N}{M/\\bar{K}-1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Notice that the number of local steps (denoted $\\tau$ ) does not appear in this dominating term, so there is no way to reduce the communication complexity (compared to parallel SGD) by taking local steps. The only exception is when this term is zero from $N=M/\\bar{K}$ , which is equivalent to the condition that every client participates within every cycle of availability. Therefore, this result cannot show a benefit from local steps unless the client participation is regularized. ", "page_idx": 52}, {"type": "text", "text": "SCAFFOLD _ In Section 4.3, we mentioned a discrepancy between the complexity of SCAFFOLD vs. Amplified SCAFFOLD in terms of the dependence on $N/S$ . In Table 1, the communication complexity of SCAFFOLD and Amplified SCAFFOLD under i.i.d. participation differs by their dependence on $\\textstyle{\\frac{N}{S}}$ The complexityof Amplifed SCAFFOLDis $\\mathcal{O}\\left(\\frac{N}{S}\\right)$ , whilethat of SCAFFOLD $\\mathcal{O}\\left(\\left(\\frac{N}{S}\\right)^{2/3}\\right)$ . This difference in the order of $\\textstyle{\\frac{N}{S}}$ is due to a potentil smallisue in the analysis of SCAFFOLD, which we intentionally avoided by accepting a slightly worse dependence on $\\frac{N}{S}$ ", "page_idx": 52}, {"type": "text", "text": "This difference stems from an apparent mistake in the original SCAFFOLD analysis. In the proof of Lemma 16 (PMLR version of SCAFFOLD), the second-to-last equation of page 32 is obtained with an incorrect step. Namely, while the current version includes ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Xi_{r}=\\frac{1}{K N}\\sum_{i,k}\\mathbb{E}\\|\\alpha_{i,k-1}^{r}-{\\mathbf{x}}^{r}\\|^{2}}\\\\ {\\displaystyle\\quad=\\left(1-\\frac{S}{N}\\right)\\frac{1}{K N}\\sum_{i,k}\\mathbb{E}\\|\\alpha_{i,k-1}^{r-1}-{\\mathbf{x}}^{r}\\|^{2}+\\frac{S}{N}\\frac{1}{K N}\\sum_{i,k}\\mathbb{E}\\|{\\mathbf{y}}_{i,k-1}^{r}-{\\mathbf{x}}^{r}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the las ineis obtained by conditioning on the event $\\alpha_{i,k-1}^{r}=\\alpha_{i,k-1}^{r-1}$ and the complement $\\alpha_{i,k-1}^{r-1}=y_{i,k-1}^{r}$ which have probabilities $\\begin{array}{r}{1-\\frac{S}{N}}\\end{array}$ and $\\frac{S}{N}$ , respectively. However, this condition is not denoted in Equation 37. A corrected version should be written ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Xi_{r}=\\left(1-\\displaystyle\\frac{S}{N}\\right)\\displaystyle\\frac{1}{K N}\\sum_{i,k}\\mathbb{E}\\left[\\|\\alpha_{i,k-1}^{r-1}-{\\bf x}^{r}\\|^{2}\\mid\\alpha_{i,k-1}^{r}=\\alpha_{i,k-1}^{r-1}\\right]}\\\\ {\\displaystyle\\ \\ \\ +\\,\\displaystyle\\frac{S}{N}\\displaystyle\\frac{1}{K N}\\sum_{i,k}\\mathbb{E}\\left[\\|{\\bf y}_{i,k-1}^{r}-{\\bf x}^{r}\\|^{2}\\mid\\alpha_{i,k-1}^{r-1}=y_{i,k-1}^{r}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "but this conditional expectation is not specified in the proof of SCAFFOLD. For the remainder of the proof of Lemma 16, these terms are treated as total expectation, leading to an inconsistency. Lemma 16 concludes by applying Lemma 15 in order to bound the term $\\mathbb{E}\\left[\\left\\Vert\\mathbb{E}_{r-1}\\left[\\Delta\\mathbf{x}^{r}\\right]\\right\\Vert^{2}\\right]$ .However,when we make the correction to include the conditional expectation, we do not have a bound of $\\Xi_{r}$ in terms $\\mathbb{E}\\left[\\left\\Vert\\mathbb{E}_{r-1}\\left[\\Delta\\mathbf{x}^{r}\\right]\\right\\Vert^{2}\\right]$ we isead have a bound in erms of $\\mathbb{E}\\left[\\left\\Vert\\mathbb{E}_{r-1}\\left[\\Delta\\mathbf{x}^{r}\\mid\\alpha_{i,k-1}^{r-1}=y_{i,k-1}^{r}\\right]\\right\\Vert^{2}\\right].$ But this term with a conditional expectation inside the norm can't be bounded with Lemma 15. A pessimistic solution is to use Jensen's inequality to bound ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mathbb{E}_{r-1}\\left[\\Delta\\mathbf{x}^{r}\\mid\\alpha_{i,k-1}^{r-1}=y_{i,k-1}^{r}\\right]\\right\\Vert^{2}\\right]\\leq\\mathbb{E}\\left[\\mathbb{E}_{r-1}\\left[\\Vert\\Delta\\mathbf{x}^{r}\\Vert^{2}\\mid\\alpha_{i,k-1}^{r-1}=y_{i,k-1}^{r}\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\Vert\\Delta\\mathbf{x}^{r}\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the second line follows from the tower property. This is the step that we perform in our analysis of Amplified SCAFFOLD, and this results in the $\\frac{N}{S}$ dependence. ", "page_idx": 53}, {"type": "text", "text": "Fixing their alysto recover the same $\\left(\\frac{N}{S}\\right)^{2/3}$ dependene may be possibl, but we have instead focused on achieving the best known complexity in terms of $\\epsilon,\\kappa,\\sigma$ , and other problem parameters. ", "page_idx": 53}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: Every claim made in the abstract is specifically tied to a theoretical convergence property of our proposed algorithm, which are stated in Section 4.2 and proven in A and B. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 54}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: We discussion limitations in the \"limitations\" paragraph of Section 6. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 54}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: All of the theoretical results are stated with the full set of assumptions in Sections 3, 3.1, and 4.2. The proofs are contained in Section A and B. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 55}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Experimental details are fully specified in Section 5.1 and Appendix D Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 55}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The code for our experiments is included in the supplemental material with instructions for reproduction. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 56}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The main training details are specified in Section 5.1. Details such as hyperparameters, tuning protocols, architecture choices, etc. are specified in Section D. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 56}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: All of our experiments show results averaged over 3-5 random seeds, and error bars are shown for learning curves. Error bar calculation is specified in Section 5.1. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of themean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: Hardware resources are specified in Section 5.1 ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 57}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: We have read and conformed to the NeurIPs Code of Ethics. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 57}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: Our paper is a theoretical paper on mathematical optimization problems arising in distributed learning. We do not see any direct paths to negative applications beyond those existing for any application of distributed learning or machine learning in general. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: Our paper does not involve the release of any data or models ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 58}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: Our paper uses existing datasets (Fashion-MNIST, CIFAR-10), and we cite original sources for both datasets in Section 5.1. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 58}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 59}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 59}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 59}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 59}]