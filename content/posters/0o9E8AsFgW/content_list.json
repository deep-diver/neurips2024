[{"type": "text", "text": "DarkSAM: Fooling Segment Anything Model to Segment Nothing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziqi $\\mathbf{Z}\\mathbf{hou}^{1,2,3*}$ , Yufei Song\u2020, Minghui $\\mathbf{Li^{\\ddagger}}$ , Shengshan $\\mathbf{H}\\mathbf{u}^{1,2,4,5\\dagger}$ , Xianlong Wang1,2,4,5\u2020, Leo Yu Zhang\u00a7, Dezhong $\\mathbf{Yao^{1,2,3*}}$ , Hai Jin1,2,3\u2217 ", "page_idx": 0}, {"type": "text", "text": "1 National Engineering Research Center for Big Data Technology and System 2 Services Computing Technology and System Lab 3 Cluster and Grid Computing Lab 4 Hubei Engineering Research Center on Big Data Security 5 Hubei Key Laboratory of Distributed System Security ", "page_idx": 0}, {"type": "text", "text": "$^*$ School of Computer Science and Technology, Huazhong University of Science and Technology   \n\u2020 School of Cyber Science and Engineering, Huazhong University of Science and Technology $\\ddagger$ School of Software Engineering, Huazhong University of Science and Technology $\\S$ School of Information and Communication Technology, Griffith University {zhouziqi,yufei17,minghuili,hushengshan,wxl99,dyao,hjin}@hust.edu.cn leo.zhang@griffith.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Segment Anything Model (SAM) has recently gained much attention for its outstanding generalization to unseen data and tasks. Despite its promising prospect, the vulnerabilities of SAM, especially to universal adversarial perturbation (UAP) have not been thoroughly investigated yet. In this paper, we propose DarkSAM, the first prompt-free universal attack framework against SAM, including a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. We first divide the output of SAM into foreground and background. Then, we design a shadow target strategy to obtain the semantic blueprint of the image as the attack target. DarkSAM is dedicated to fooling SAM by extracting and destroying crucial object features from images in both spatial and frequency domains. In the spatial domain, we disrupt the semantics of both the foreground and background in the image to confuse SAM. In the frequency domain, we further enhance the attack effectiveness by distorting the high-frequency components (i.e., texture information) of the image. Consequently, with a single UAP, DarkSAM renders SAM incapable of segmenting objects across diverse images with varying prompts. Experimental results on four datasets for SAM and its two variant models demonstrate the powerful attack capability and transferability of DarkSAM. Our codes are available at: https://github.com/CGCL-codes/DarkSAM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the advancement of deep learning, large language models, such as GPT [2], LaMDA [33], and PaLM [6], have achieved tremendous success, yet the development of large vision models lags behind. Recently, Segment Anything Model (SAM) [19] was proposed as a foundational vision model, demonstrating exceptional generalization capabilities for handling complex segmentation tasks. Unlike traditional segmentation models [24, 42] that output pixel-level labels, SAM introduces a novel prompt-guided image segmentation paradigm by directly producing label-free masks for object segmentation. Benefiting from its powerful zero-shot capability, SAM has been rapidly deployed across various downstream scenarios, such as medical images [34], videos [36], and 3D point clouds [12]. ", "page_idx": 0}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/4e146cc4be586849360098c09a9c5bf8bf3ed3c999653809dc0d03f1b009abc5.jpg", "img_caption": ["Figure 1: Illustration of fooling SAM using UAP "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Deep neural networks (DNNs) are shown to be vulnerable to adversarial examples [16, 21, 41, 46], and SAM is no exception. Standard adversarial attacks are designed for classification tasks and cause misclassification by manipulating global image features through image-level perturbations. Existing attacks can be divided into crafting sample-wise adversarial perturbation [25] and universal adversarial perturbation (UAP) [27]. The former is tailored for specific inputs, while the latter seeks a single perturbation applicable across a wide range of inputs, thereby intensifying its complexity and difficulty. As a pioneering prompt-guided segmentation model, SAM relies on both input images and prompts to yield label-free masks, rendering existing adversarial attacks [1, 10, 25, 26] focusing only for images and relying on labels ineffective. ", "page_idx": 1}, {"type": "text", "text": "Recent efforts [17, 39] started to explore the robustness of SAM against sample-wise adversarial perturbations. Attack-SAM [39] employs classical FGSM [10] and PGD [25] to remove or manipulate the predicted mask for a given image and prompt pair. Meanwhile, another study [17] also investigates the robustness of SAM against various adversarial attacks and corrupted images. However, the more challenging universal adversarial attacks, which more closely resemble real-world scenarios, remain far less thoroughly explored. The introduction of extra and varying prompts in SAM\u2019s input, coupled with the lack of label information in its output for attack optimization, renders attacking SAM exceedingly challenging, posing an intriguing problem: ", "page_idx": 1}, {"type": "text", "text": "Is it feasible to fool the Segment Anything model to segment nothing through a single UAP? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we take a substantial step towards bridging the gap between SAM and UAP. We propose DarkSAM, the first truly prompt-free universal adversarial attack on the prompt-guided image segmentation models (i.e., SAM and its variants), aiming to disable their segmentation ability across diverse input images using a single UAP, irrespective of prompts (see Fig. 1). Unlike classification models that focus on global features, prompt-guided segmentation models concentrate more on local critical objects within images (e.g., objects indicated by prompts). Therefore, our intuition is to destroy crucial object features in the image to mislead SAM into incorrectly segmenting the input images. To this end, DarkSAM is dedicated to decoupling the crucial object features of images from both spatial and frequency domains, utilizing a UAP to disrupt them. 1) In the spatial domain, we begin by dividing SAM\u2019s output into foreground (i.e., positive mask values) and background (i.e., negative mask values) via a Boolean mask. We then scramble SAM\u2019s decision by destroying the features of the foreground and background of the image, respectively. 2) In the frequency domain, inspired by the factor that SAM is biased towards image texture over shape [38], we employ a frequency filter to decompose images into high-frequency components (HFC) and low-frequency components (LFC). By increasing the dissimilarity in the HFC of adversarial and benign examples while maintaining consistency in their LHC, we further enhance the effectiveness and transferability of UAP. Experimental results on four segmentation benchmark datasets for SAM and its two variant models, HQ-SAM [18] and PerSAM [40], demonstrate that DarkSAM achieves high attack success rates and transferability. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose DarkSAM, the first truly universal adversarial attack against SAM. We employ a single perturbation to prevent SAM from segmenting objects across a range of images under any form of prompt, which further unveils its vulnerability. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We design a brand-new prompt-free hybrid spatial-frequency universal attack framework against the prompt-guided image segmentation models to generate a UAP thus making them segment nothing, which consists of a semantic decoupling-based spatial attack and a texture distortion-based frequency attack.   \n\u2022 We conduct extensive experiments on four datasets for SAM and its two variant models. Both the qualitative and quantitative results demonstrate that DarkSAM achieves high attack success rates and transferability. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Prompt-guided Image Segmentation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Segment Anything Model [19] is a cutting-edge advancement in computer vision, garnering widespread attention [3, 5, 20, 22, 34] for its powerful segmentation capabilities. Recent works have been dedicated to exploring various variants of SAM to further enhance performance, such as HQ-SAM [18], PerSAM [40], and MobileSAM [37]. Distinct from traditional semantic segmentation models [4, 24, 42] that predominantly focus on pixel-level label prediction, SAM undertakes the label-free mask prediction by generating object masks for a wide array of subjects using prompts. It consists of three components: an image encoder, a prompt encoder, and a lightweight mask decoder. The image encoder generates image representations in latent space and the prompt encoder utilizes positional embeddings for representing prompts, such as points and boxes. The mask decoder, combining outputs from both image and prompt encoders, predicts effective masks to segment targeted objects. ", "page_idx": 2}, {"type": "text", "text": "Given an image $\\boldsymbol{x}\\ \\in\\ \\mathbb{R}^{H\\times W\\times C}$ and a corresponding prompt $\\mathbb{P}$ to SAM, denoted as $f(x,\\mathbb{P})\\;\\in$ $\\mathbb{R}^{H\\times W}$ , the model returns a mask $m$ with the predicted segmentation. The prediction process of SAM can be represented as follow: ", "page_idx": 2}, {"type": "equation", "text": "$$\nm=f_{\\theta}(x,\\mathbb{P}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta$ represents the parameter of $f(\\cdot)$ . For an image $x$ , each pixel located at coordinates $(i,j)$ , referred to as $x_{i j}$ , is deemed a part of the masked region when its corresponding mask value $m_{i j}$ exceeds a defined threshold of zero. Recent exploratory studies [39, 17, 35] have revealed vulnerabilities of SAM to adversarial examples and common image corruptions. Different from previous works, our goal is to develop a powerful universal adversarial attack for such prompt-guided image segmentation models. ", "page_idx": 2}, {"type": "text", "text": "2.2 Universal Adversarial Perturbation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Deep neural networks have been shown vulnerable to adversarial examples [10, 25, 44, 45, 46], where attackers can deceive models by introducing subtle noise to images. Universal adversarial perturbation [27] (UAP) was first proposed to fool the victim model by imposing a single adversarial perturbation on a series of images. Existing works can be divided into data-dependent universal adversarial attacks [14, 27, 30] and data-free universal attacks [28, 29, 31], both designed for classification attacks. The former relies on the specific data characteristics of target dataset for UAP generation, while the latter provides a more generalized approach without relying on such data. Meanwhile, some works [15] have also explored UAPs for traditional segmentation models, but they rely on pixel-level labels, which are not applicable to emerging prompt-guided segmentation models. The concurrent works [8, 13] explore UAPs against SAM from the perspectives of direct noise optimization and perturbing the output of the image encoder of SAM, respectively. Different from them, we aim to comprehensively decouple and disrupt crucial object features in images from both spatial and frequency domains, thereby deceiving SAM into failing to segment input images. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As a fundamental vision model, SAM typically operates in an online mode, allowing users to set prompts randomly. Therefore, we define the threat model as a quasi-black-box setting, where adversaries have access to the official open-source SAM, but not to the pre-training dataset and the downstream dataset (i.e., those used by users). The adversaries\u2019 goal is to craft a UAP $\\delta$ using a surrogate dataset $\\mathcal{D}_{s}$ (i.e., unrelated to the pre-training and downstream dataset), thereby compromising the model\u2019s performance, i.e., rendering adversarial examples unable to be correctly segmented by SAM. Additionally, the $\\delta$ should be sufficiently small, and constrained by $l_{p}$ -norm of $\\epsilon$ . This problem can be formulated as: ", "page_idx": 2}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/c0b31a99cf4e98723e3a175b60bda24fc938047b128e6c5a9448b3f29c3f850b.jpg", "img_caption": ["Figure 2: Illustration of the proposed shadow target strategy "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\delta}\\mathbb{E}_{x\\sim\\mathcal{D}_{s}}\\left[f_{\\theta}\\left(x+\\delta,\\mathbb{P}\\right)\\neq f_{\\theta}\\left(x,\\mathbb{P}\\right)\\right],s.t.\\left\\|\\delta\\right\\|_{p}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Intuition Behind DarkSAM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Unlike the standard deep learning paradigm that inputs a single image and outputs a one-hot label or pixel-level label, SAM requires both images and prompts as inputs, and then outputs label-free masks, indicating the shape information of critical objects. Therefore, a truly universal adversarial attack against SAM should implement a single perturbation to achieve ineffective segmentation for any combination between a series of images and different prompts. However, this task is hindered by the following challenges: ", "page_idx": 3}, {"type": "text", "text": "Challenge I: The dual ambiguity in attack targets arising from varying images and prompts. Previous UAP works only need to optimize in the target images, hence the introduction of prompts could lead to invalid attacks, as different prompts for a fixed image yield distinct segmentation results. For instance, the image in the top-left corner of Fig. 2 shows a can and a spoon. For the same image, feeding different prompts will result in different masks output by SAM (see Fig. 2(b)). In conclusion, diverse variations in target images and prompts increase the uncertainty of attack targets. For varying images, existing UAP solutions (e.g., UAPGD [9]) can provide references, and the main challenge here is the uncertainty of the attack target brought by unknown prompts. To this end, we propose a shadow target strategy by increasing the number of prompts during the attack process to enhance the cross-prompt transferability of UAP. Specifically, for a given input image, we randomly select $k$ prompts (e.g., points or boxes) to create a prompt auxiliary set. By merging their masks output by SAM, we form a semantic blueprint of the image, which serves as the target for our attack, as illustrated in Fig. 2(c). This semantic blueprint effectively encompasses the main semantic content of the original image, substantially reducing the ambiguity associated with unknown prompts. ", "page_idx": 3}, {"type": "text", "text": "Challenge II: Suboptimal attack efficacy due to semantic decoupling deficiency. Since promptguided segmentation models output masks that are neither one-hot nor pixel-level labels, traditional attack methods that rely on label deviation for optimization guidance become ineffective. Another approach involves directly modifying the output, such as adjusting the adversarial examples\u2019 masks to diverge from their originals, potentially yielding marginal attack success as verified in Sec. 4.4. Nonetheless, the intrinsic sensitivity of segmentation models to pixel-level details significantly constrains the potency of these attacks, underscoring a notable limitation in their applicability. ", "page_idx": 3}, {"type": "text", "text": "Given the focus of prompt-guided segmentation models on local, critical object features rather than global image features, we are motivated to comprehensively decouple the key semantic features of an image from the perspective of both spatial and frequency domains, aiming to fool SAM by manipulating these features. We first define the main object within the image (i.e., the target of segmentation, typically a region rich in texture) as the foreground, with the rest being defined as the background. As the mask output by SAM indicates the foreground with positive values and the background with negative ones, we use a Boolean mask to separately extract these foreground and background masks. Subsequently, we optimize the UAP, switching adversarial examples\u2019 foreground to negative and background to positive, disrupting the image\u2019s semantics for a spatial attack. At the same time, inspired by the recent study [38] that SAM is biased towards texture of the image over shape, we investigate the alteration of the high-frequency components (i.e., texture information) of adversarial examples in the frequency domain, while simultaneously constraining the low-frequency components (i.e., shape information), in order to further enhance the effectiveness and transferability of our attack. By separately decoupling and destroying crucial features in both the spatial and frequency domains, we provide valuable optimization directions for UAP generation, thereby facilitating effective attacks on SAM. ", "page_idx": 3}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/41d7d09fb71d195d861011672db5ebc94b9eb86f3e886de8355bfefaa1d67815.jpg", "img_caption": ["Figure 3: The framework of DarkSAM "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 DarkSAM: A Complete Illustration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present DarkSAM, a novel prompt-free hybrid spatial-frequency universal adversarial attack against the prompt-guided image segmentation models (i.e., SAM and its variants). The pipeline of DarkSAM is depicted in Fig. 3, encompassing a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. We start by randomly generating $k$ different prompts to form an auxiliary prompt set $\\mathbb{P}_{a}$ , acquiring the semantic blueprints of the target images as the attack targets. By individually manipulating the semantic content of adversarial examples\u2019 foreground and background in the spatial domain, and increasing the distance between the HFC of adversarial and benign examples in the frequency domain, while also constraining the difference in their LFC, we enhance the attack performance and transferability of the UAP. We provide the detailed optimization process of DarkSAM in Algorithm 1. The overall optimization objective $\\mathcal{I}_{t o t a l}$ of DarkSAM is as follow: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{J}_{t o t a l}=\\mathcal{J}_{s a}+\\lambda\\mathcal{J}_{f a},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{I}_{s a}$ and $\\mathcal{J}_{f a}$ are the spatial and frequency attack losses, and $\\lambda$ controls the importance. ", "page_idx": 4}, {"type": "text", "text": "Semantic decoupling-based spatial attack. Initially, we utilize two Boolean mask $m_{f g}$ and $\\overline{{m_{f g}}}$ to separately extract the foreground and background mask of the adversarial examples based on the positive and negative values in the mask output by SAM. As for the foreground, our intention is to render it unidentifiable and unsegmentable by SAM. Thus, we optimize its mask towards a negative fake mask $\\xi_{n e g}$ , enabling its fusion with the background to achieve segmentation evasion. The foreground evasion loss $\\mathcal{J}_{f e}$ can be described as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{I}}_{f e}={\\mathcal{I}}_{d}(f_{\\theta}(x+\\delta,\\mathbb{P}_{a})\\cdot m_{f g},\\xi_{n e g}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\xi_{n e g}$ is a fake mask that conforms to the shape of the image, containing threshold values of $-\\tau$ in regions corresponding to the foreground, and 0 elsewhere. $\\mathcal{I}_{d}$ serves as the distance metric function, representing the mean squared error loss. For the background, we optimize its mask towards a positive fake mask $\\xi_{p o s}$ (opposite to $\\xi_{n e g}$ ), misleading SAM into interpreting it as a semantically meaningful object, consequently causing further interference in the assessment of the foreground. The associated loss is ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{I}_{b m}=\\mathcal{I}_{d}(f_{\\theta}(x+\\delta,\\mathbb{P}_{a})\\cdot\\overline{{\\boldsymbol{m}_{f g}}},\\xi_{p o s}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The loss of the semantic decoupling-based spatial attack can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{J}_{s a}=\\mathcal{J}_{f e}+\\mathcal{J}_{b m}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Texture distortion-based frequency attack. In the frequency domain, the high-frequency components of an image denote the finer details, including noise and textures, while the low-frequency components contain the general outline and overall structural information of the image. We employ the discrete wavelet transform (DWT), utilizing a low-pass filter $\\mathcal{L}$ and a high-pass filter $\\mathcal{H}$ to decompose the image $x$ into different components, constituting a low-frequency component $c_{l l}$ , a high-frequency component $c_{h h}$ , and two mid-frequency components $c_{l h}$ and $c_{h l}$ , via ", "page_idx": 5}, {"type": "equation", "text": "$$\nc_{l l}=\\mathcal{L}x\\mathcal{L}^{T},c_{h h}=\\mathcal{H}x\\mathcal{H}^{T},c_{l h}/c_{h l}=\\mathcal{L}x\\mathcal{H}^{T}/\\mathcal{H}x\\mathcal{L}^{T}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Subsequently, we employ the inverse discrete wavelet transform (IDWT) to reconstruct the signal that have been decomposed through DWT into an image. We choose the LFC and HFC while dropping the other components to obtain the reconstructed images $\\phi(x)$ and $\\psi(x)$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi(x)=\\mathcal{L}^{T}c_{l l}\\mathcal{L}=\\mathcal{L}^{T}(\\mathcal{L}x\\mathcal{L}^{T})\\mathcal{L},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi(x)=\\mathcal{H}^{T}c_{h h}\\mathcal{H}=\\mathcal{H}^{T}(\\mathcal{H}x\\mathcal{H}^{T})\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By adding a UAP to the images, we alter their HFC, disrupting the original texture information. Simultaneously, we enforce constraints on the low-frequency disparities between adversarial and benign examples to redirect a larger portion of the perturbation towards the high-frequency domain. As a result, we enhance the attack performance and cross-domain transferability of the UAP by introducing variations in the frequency domain. The loss of texture distortion-based frequency attack can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{f a}=\\mathcal{I}_{l f c}-\\mu\\mathcal{J}_{h f c}}\\\\ &{\\quad\\quad=\\mathcal{I}_{d}(\\phi(x),\\phi(x+\\delta))-\\mu\\mathcal{J}_{d}(\\psi(x),\\psi(x+\\delta)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu$ is a pre-defined hyperparameter. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 DarkSAM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: image $x~\\in~\\mathcal{D}_{s}$ , SAM $f(x)$ with parameter $\\theta$ , hyper parameters $k,\\ \\tau,\\ \\lambda$ , and $\\mu$ , max-perturbation constraint $\\epsilon$", "page_idx": 5}, {"type": "text", "text": "Output: A universal adversarial perturbation $\\delta$ ", "page_idx": 5}, {"type": "text", "text": "1: Initialize random prompt sets $\\mathbb{P}_{a}$ and noise $\\delta_{o}$   \n2: Initialize adversarial examples: $x^{*}\\longleftarrow x+\\delta_{o}$   \n3: Project $x^{*}$ to $[0,1]$ via clipping   \n4: Separating different frequency components of $x$ using the discrete wavelet transform:   \n$c_{l l},c_{l h},c_{h l},c_{h h}\\longleftarrow D\\bar{W}T(\\bar{x})$   \n5: Restore part of the frequency components into an image using the inverse discrete wavelet trans  \nform: $\\bar{\\phi(x)}$ , $\\psi(x)\\longleftarrow I D\\dot{W}T(c_{l l}^{\\bar{}}),I D W T(c_{h h})$   \n6: Calculate $m_{f g}$ by determining the sign of each value in output of $f_{\\theta}(x,\\mathbb{P}_{a})$   \n7: while max iterations or not converge do   \n8: Calculate spatial loss mentioned in Eq. 6   \n9: Calculate frequency loss mentioned in Eq. 10   \n10: Update $\\delta$ through backprop   \n11: Clip $\\delta$ to satisfy imperceptibility constraint $\\epsilon$   \n12: Project $x^{*}$ to $[0,1]$ via clipping   \n13: end while ", "page_idx": 5}, {"type": "text", "text": "", "text_level": 1, "page_idx": 5}, {"type": "table", "img_path": "0o9E8AsFgW/tmp/c0cd7fdcd674c33fa15d78b127befd1c98bf01183addf977928b5a315f4f8d9a.jpg", "table_caption": ["Table 1: The mIoU $(\\%)$ of DarkSAM under different settings. Values covered by gray denote the clean mIoU, others denote adversarial mIoU. ADE20K, MS-COCO, CITYSCAPES abbreviated as ADE, COCO, CITY, respectively. Bolded values indicate the best results. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and models. We evaluate our method using four public segmentation datasets: ADE20K [43], MS-COCO [23], CITYSCAPES [7], and SA-1B [19]. For each dataset, we randomly select 100 images for UAP generation and 2,000 images for testing purposes. All images are uniformly resized to $3\\!\\times\\!1024\\!\\times\\!1024$ . For victim models, we use the pre-trained SAM [19], HQ-SAM [18] and PerSAM [40] with the ViT-B backbone. ", "page_idx": 6}, {"type": "text", "text": "Parameter setting. Following [9, 27, 32], we set the upper bound of UAP to 10/255. For our experiments, we adjust the hyperparameters $k,\\,\\tau,\\,\\lambda,$ , and $\\mu$ to 10, 1, 0.1, and 0.01, respectively, and set the batch size to 1. To evaluate the cross-prompt attack capabilities of DarkSAM, we employ three distinct prompt types: point, box, and segment everything (also abbreviated as \u201call\u201d) mode. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. To evaluate the effectiveness of DarkSAM, we use the mean Intersection over Union (mIoU) metric. To facilitate data presentation, we also use the attack success rate (ASR) as a metric to evaluate attack performance. ASR represents the difference between the mIoU values of benign and adversarial examples. ", "page_idx": 6}, {"type": "text", "text": "4.2 Attack Performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To comprehensively evaluate DarkSAM\u2019s effectiveness, we perform experiments on three promptguided image segmentation models including SAM, HQ-SAM, and PerSAM, across four datasets. For each setup, we generate UAPs using point and box prompts, respectively, and then evaluate DarkSAM\u2019s attack performance using the corresponding single-point or single-box prompt. We first calculate the clean mIoU of different models across four datasets using point and box as prompts. Specifically, for the SA-1B dataset, we directly extract point and box prompts from the annotations, whereas for the other datasets, we obtain internal points and external boxes as prompts by calculating the object contour coordinates within their annotations. ", "page_idx": 6}, {"type": "text", "text": "The experiments in Tab. 1 show that DarkSAM can effectively fool these prompt-guided image segmentation models with an average mIoU reduction of more than $60\\%$ across 96 different experimental settings. The results in Tab. 1 also indicate that box prompts not only yield higher segmentation accuracy but also demonstrate greater robustness. For adversaries, the choice of surrogate datasets has a minor impact on crafting UAPs, yet they consistently facilitate excellent attack performance. Notably, DarkSAM demonstrates a distinct advantage when the SA-1B dataset, the training data for SAM, is employed as the surrogate dataset. In addition to the above quantitative experimental results, we also present qualitative findings. Specifically, we provide the visualization of SAM segmentation results for adversarial examples made by DarkSAM using point and box prompts across four different datasets in Fig. 4. These results include masks of objects in images output by SAM under point, box, and segment everything prompt modes. From Fig. 4, we can see that SAM successfully segments benign images across three types of prompt modes, but it is unable to segment adversarial examples, i.e., the output masks are close to \u201cdark\u201d. The qualitative results further corroborate the powerful attack capability of DarkSAM. ", "page_idx": 6}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/1b3c6106275c16dbafaa6e4c3f3ff8a7667ec6d118ca249783325036f8200aae.jpg", "img_caption": ["Figure 4: Visualizations of SAM segmentation results for adversarial examples across four datasets. The first four columns and the middle four columns display the segmentation results for point and box prompts, respectively. The last three columns show results under the segment everything mode for benign examples, as well as adversarial examples created using point and box prompts, respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "0o9E8AsFgW/tmp/ea8fb356ec4fc7cf0067557a8dd3b627994eb0e6fb2326e412af777f716e58f7.jpg", "table_caption": ["Table 2: The ASR $(\\%)$ of the cross-prompt transferability study on SAM. $\\mathbf{\\cdotBOX}\\rightarrow\\mathbf{POINT}^{\\ast}$ indicates that adversarial examples created using box are tested in point mode. Others stand the same meaning. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Transferability Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We study the attack transferability of DarkSAM across data domain, prompt types, and models, respectively. $\\pmb{\\mathbb{0}}$ Cross-domain. The results in Tab. 1 demonstrate DarkSAM\u2019s excellent cross-domain transferability, where UAPs generated with the surrogate dataset (ADE20K) achieve a high ASR on datasets from various different domains. We also explore the role of the frequency attack (i.e., $\\mathcal{J}_{f a}$ , denoted as FA) in enhancing cross-domain transferability. As shown in Fig. 5 (a) , frequency attack can effectively improve the attack performance based on the spatial attack (i.e., $\\mathcal{I}_{s a}$ , denoted as SA). $\\pmb{\\mathcal{O}}$ Cross-prompt. We examine the performance of DarkSAM across various types of prompts. As demonstrated in the last three columns of Fig. 4, UAPs created based on both point and box prompts perform well under the segment everything mode. Additionally, we provide results of transferability experiments between point and box prompts in Tab. 2. This includes testing UAPs created with point prompts in the box prompt setting and vice versa. Based on the observed results, it is discernible that UAPs crafted using box prompts generally demonstrate better transferability compared to those using point prompts. This increased efficacy can likely be attributed to the box prompts offering more integral and detailed prompt information. $\\pmb{\\otimes}$ Cross-model. We use UAPs created with points and boxes based on SAM to attack HQ-SAM and PER-SAM. The results in Fig. 5 (b) - (e) showcase DarkSAM\u2019s exceptional transferability across different models. ", "page_idx": 7}, {"type": "text", "text": "4.4 Comparison Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To comprehensively demonstrate the superiority of our proposed method, we compare DarkSAM with popular UAP schemes, including UAP [27], UAPGD [9], and SSP [32]. We also consider the state-of-the-art adversarial attack against traditional segmentation models, SegPGD [11], and the latest sample-wise attack against SAM, Attack-SAM [39]. For a fair comparison, we adapt them to a UAP optimization strategy and keep other settings consistent with DarkSAM. We select SAM as the victim model and assess the effectiveness of these UAP methods across four datasets, using the same dataset for both generating and testing the UAPs. The results in Tab. 3 indicate that DarkSAM outperforms all methods with a considerable margin. The negative experimental values $(^{\\bullet\\bullet\\ast\\circ})$ indicate that the attack does not work at all. This phenomenon may stem from counterproductive perturbations that inadvertently cause the input samples to resemble the training set used by SAM, paradoxically enhancing accuracy and resulting in negative ASR values. We also provide visualizations of the segmentation results of the adversarial examples made by these methods using box prompts in Fig. 6, obtained in point, box, and segment-everything modes, respectively. The results further demonstrate the superiority of DarkSAM. ", "page_idx": 7}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/12100ad712162e4298e1f1420419b92dd04271bdd5a197cb2948e30b3570112d.jpg", "img_caption": ["Figure 5: The ASR $(\\%)$ of transferability study. (a) explores the impact of the frequency attack on boosting the cross-domain transferability of UAPs. (b) - (e) stand the results of cross-model transferability study. \u201cPointHQ\u201d and \u201cBox-HQ\u201d denote the results of HQ-SAM under point and box prompts, while the suffix \u201c-PER\u201d represents the corresponding results for PerSAM. Each row represents the same UAP. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "0o9E8AsFgW/tmp/f14d0c342eed20e521ead3e87a61174fc94a289710bb18d23e4bb504365bf407.jpg", "table_caption": ["Table 3: The ASR $(\\%)$ of comparison study "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we explore the effect of different modules, prompt number, attack strengths, training data size, and threshold values on DarkSAM. We conduct experiments using point prompts on SAM across the ADE20K dataset. ", "page_idx": 8}, {"type": "text", "text": "The effect of different modules. We investigate the effect of various modules on the attack performance of DarkSAM. For clarity and convenience, we use A, B, C, and D to denote ${\\mathcal{I}}_{f e},{\\mathcal{I}}_{b m},{\\mathcal{I}}_{h f c}$ , and ${\\mathcal{I}}_{l f c}$ , respectively. The results in Fig. 7 (a) show that no variants can compete with the complete method, implying the indispensability of each component for DarkSAM. ", "page_idx": 8}, {"type": "text", "text": "The effect of prompt number. We study the effect of the prompt number in proposed shadow target strategy on attack performance of DarkSAM. We conduct experiments with varying numbers of point prompts, ranging from 1 to 100. The results in Fig. 7 (b) show a gradual increase in attack performance from 1 to 10 (default setting), followed by a downward trend. This could be attributed to an excess of random points leading to masks with redundant information, thereby impacting the attack efficacy. ", "page_idx": 8}, {"type": "text", "text": "The effect of perturbation budget. As shown in Fig. 7 (c), we evaluate DarkSAM\u2019s attack performance with $\\epsilon$ from $4/255$ to 32/255. With the increase in $\\epsilon$ , there is a corresponding enhancement in attack performance. Notably, our attack still maintains high efficacy at the $\\bar{6}/255$ setting, with an average ASR exceeding $45\\%$ . ", "page_idx": 8}, {"type": "text", "text": "The effect of number of training samples. We explore the effect of varying the number of training images used to create UAP on DarkSAM. Utilizing a range from 10 to 1000 images to craft UAPs, the results in Fig. 7 (d) reveal that employing merely 100 images can achieve excellent attack performance, demonstrating a strong applicability advantage. ", "page_idx": 8}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/ec664cf4decfe0da670b71391f5b252f4b3128ab2757e7775cca59b92fc2e213.jpg", "img_caption": ["Figure 7: The results $(\\%)$ of ablation study. (a) - (e) investigate the effect of different modules, prompt number, attack strengths, number of training samples, and threshold values in fake mask on DarkSAM, respectively. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The effect of threshold values. We examine the effect of varying threshold values $\\tau$ in the fake mask $\\xi$ on DarkSAM. As illustrated in Fig. 7 (e), we test a range of values from 1 to 1000. The results indicate that these different values have a minimal overall effect on DarkSAM\u2019s performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions, Limitations, and Broader Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose DarkSAM, the first truly universal adversarial attack against SAM. With a single perturbation, DarkSAM renders SAM incapable of segmenting objects across diverse images with varying prompts, thereby exposing its vulnerability. To tackle the challenge of dual ambiguity in attack targets, we present a shadow target strategy to obtain semantic blueprint as a attack target. We then design a novel prompt-free hybrid spatial-frequency universal attack framework, which consists of a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. By disrupting the crucial object features in both the spatial and frequency domains of the images, it successfully addresses the challenge of suboptimal attack efficacy, thus deceiving SAM. Our extensive experiments on SAM, HQ-SAM, and PerSAM across four datasets, both qualitatively and quantitatively, demonstrate DarkSAM\u2019s powerful attack ability and strong attack transferability. ", "page_idx": 9}, {"type": "text", "text": "In terms of limitations, DarkSAM may not be suitable for traditional segmentation models because its output is not a label-free mask. This characteristic might limit its applicability in scenarios where labeled masks are essential for accurate segmentation. The adversarial examples produced by DarkSAM could potentially mislead SAM-based segmentation platforms, posing significant security risks, particularly in sensitive domains like medical image analysis. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Shengshan Hu\u2019s work is supported by the National Natural Science Foundation of China (Grant Nos. U20A20177, 62372196). Dezhong Yao\u2019s work is supported by the National Natural Science Foundation of China under Grant No. 62072204. Shengshan Hu and Dezhong Yao are co-corresponding authors. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Anurag Arnab, Ondrej Miksik, and Philip HS Torr. On the robustness of semantic segmentation models to adversarial attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201918), pages 888\u2013897, 2018. ", "page_idx": 10}, {"type": "text", "text": "[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 44th International Conference on Neural Information Processing Systems (NeurIPS\u201920), pages 1877\u20131901, 2020.   \n[3] Cheng Chen, Juzheng Miao, Dufan Wu, Aoxiao Zhong, Zhiling Yan, Sekeun Kim, Jiang Hu, Zhengliang Liu, Lichao Sun, Xiang Li, Tianming Liu, Pheng-Ann Heng, and Quanzheng Li. Ma-sam: Modality-agnostic sam adaptation for 3d medical image segmentation. arXiv preprint arXiv:2309.08842, 2023. [4] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.   \n[5] Tianrun Chen, Lanyun Zhu, Chaotao Deng, Runlong Cao, Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying Zang, and Papa Mao. Sam-adapter: Adapting segment anything in underperformed scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201923), pages 3367\u20133375, 2023.   \n[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.   \n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201916), pages 3213\u20133223, 2016.   \n[8] Francesco Croce and Matthias Hein. Segment (almost) nothing: Prompt-agnostic adversarial attacks on segmentation models. arXiv preprint arXiv:2311.14450, 2023.   \n[9] Yingpeng Deng and Lina J Karam. Universal adversarial attack via enhanced projected gradient descent. In Proceedings of the IEEE International Conference on Image Processing (ICIP\u201920), pages 1241\u20131245, 2020.   \n[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[11] Jindong Gu, Hengshuang Zhao, Volker Tresp, and Philip HS Torr. Segpgd: An effective and efficient adversarial attack for evaluating and boosting segmentation robustness. In Proceedings of the 17th European Conference on Computer Vision (ECCV\u201922), pages 308\u2013325, 2022.   \n[12] Haoyu Guo, He Zhu, Sida Peng, Yuang Wang, Yujun Shen, Ruizhen Hu, and Xiaowei Zhou. Sam-guided graph cut for 3d instance segmentation. arXiv preprint arXiv:2312.08372, 2023.   \n[13] Dongshen Han, Sheng Zheng, and Chaoning Zhang. Segment anything meets universal adversarial perturbation. arXiv preprint arXiv:2310.12431, 2023.   \n[14] Jamie Hayes and George Danezis. Learning universal adversarial perturbations with generative models. In Proceedings of the IEEE Symposium on Security and Privacy Workshops (SPW\u201918), pages 43\u201349, 2018.   \n[15] Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, and Volker Fischer. Universal adversarial perturbations against semantic image segmentation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV\u201917), pages 2755\u20132764, 2017.   \n[16] Shengshan Hu, Yechao Zhang, Xiaogeng Liu, Leo Yu Zhang, Minghui Li, and Hai Jin. Advhash: Set-to-set targeted attack on deep hashing with one single adversarial patch. In Proceedings of the 29th ACM International Conference on Multimedia (MM\u201921), pages 2335\u20132343, 2021.   \n[17] Yihao Huang, Yue Cao, Tianlin Li, Felix Juefei-Xu, Di Lin, Ivor W Tsang, Yang Liu, and Qing Guo. On the robustness of segment anything. arXiv preprint arXiv:2305.16220, 2023.   \n[18] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. In Proceedings of the 47th International Conference on Neural Information Processing Systems (NeurIPS\u201923), 2023.   \n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe\u00b4 Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla\u00b4r, and Ross B. Girshick. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[20] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023.   \n[21] Minghui Li, Jiangxiong Wang, Hao Zhang, Ziqi Zhou, Shengshan Hu, and Xiaobing Pei. Transferable adversarial facial images for privacy protection. In Proceedings of the 32nd ACM International Conference on Multimedia (MM\u201924), 2024.   \n[22] Yuheng Li, Mingzhe Hu, and Xiaofeng Yang. Polyp-sam: Transfer sam for polyp segmentation. arXiv preprint arXiv:2305.00293, 2023.   \n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla\u00b4r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the 12nd European Conference on Computer Vision (ECCV\u201914), pages 740\u2013755, 2014.   \n[24] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201915), pages 3431\u20133440, 2015.   \n[25] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[26] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201916), pages 2574\u20132582, 2016.   \n[27] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201917), pages 1765\u20131773, 2017.   \n[28] Konda Reddy Mopuri, Utsav Garg, and R Venkatesh Babu. Fast feature fool: A data independent approach to universal adversarial perturbations. arXiv preprint arXiv:1707.05572, 2017.   \n[29] Konda Reddy Mopuri, Aditya Ganeshan, and R. Venkatesh Babu. Generalizable data-free objective for crafting universal adversarial perturbations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(10):2452\u20132465, 2018.   \n[30] Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, and R. Venkatesh Babu. Nag: Network for adversary generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201918), pages 742\u2013751, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[31] Konda Reddy Mopuri, Phani Krishna Uppala, and R. Venkatesh Babu. Ask, acquire, and attack: Data-free uap generation using class impressions. In Proceedings of the 15th European Conference on Computer Vision (ECCV\u201918), pages 19\u201334, 2018. ", "page_idx": 12}, {"type": "text", "text": "[32] Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. A self-supervised approach for adversarial robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201920), pages 262\u2013271, 2020.   \n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agu\u00a8era y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.   \n[34] Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming Jin, and Tal Arbel. Medical sam adapter: Adapting segment anything model for medical image segmentation. arXiv preprint arXiv:2304.12620, 2023.   \n[35] Song Xia, Wenhan Yang, Yi Yu, Xun Lin, Henghui Ding, Lingyu Duan, and Xudong Jiang. Transferable adversarial attacks on sam and its downstream models via universal meta initialization and gradient robust attacks. In Proceedings of the 38th International Conference on Neural Information Processing Systems (NeurIPS\u201924), 2024.   \n[36] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.   \n[37] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023.   \n[38] Chaoning Zhang, Yu Qiao, Shehbaz Tariq, Sheng Zheng, Chenshuang Zhang, Chenghao Li, Hyundong Shin, and Choong Seon Hong. Understanding segment anything model: Sam is biased towards texture rather than shape. arXiv preprint arXiv:2311.11465, 2023.   \n[39] Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and In So Kweon. Attack-sam: Towards evaluating adversarial robustness of segment anything model. arXiv preprint arXiv:2305.00866, 2023.   \n[40] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, and Hongsheng Li. Personalize segment anything model with one shot. arXiv preprint arXiv:2305.03048, 2023.   \n[41] Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, and Hai Jin. Why does little robustness help? a further step towards understanding adversarial transferability. In Proceedings of the 45th IEEE Symposium on Security and Privacy $(S\\&P^{\\prime}24)$ , 2024.   \n[42] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201917), pages 2881\u20132890, 2017.   \n[43] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201917), pages 633\u2013641, 2017.   \n[44] Ziqi Zhou, Shengshan Hu, Minghui Li, Hangtao Zhang, Yechao Zhang, and Hai Jin. Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning. In Proceedings of the 31st ACM International Conference on Multimedia (MM\u201923), pages 6311\u20136320, 2023.   \n[45] Ziqi Zhou, Shengshan Hu, Ruizhi Zhao, Qian Wang, Leo Yu Zhang, Junhui Hou, and Hai Jin. Downstream-agnostic adversarial examples. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV\u201923), pages 4345\u20134355, 2023.   \n[46] Ziqi Zhou, Minghui Li, Wei Liu, Shengshan Hu, Yechao Zhang, Wei Wan, Lulu Xue, Leo Yu Zhang, Dezhong Yao, and Hai Jin. Securely fine-tuning pre-trained encoders against adversarial examples. In Proceedings of the 2024 IEEE Symposium on Security and Privacy $(S P^{\\prime}24)$ , 2024. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 ADE20K: ADE20K [43] is a dataset for scene parsing that includes images from a variety of environments. It contains more than 20,000 images, classified into 150 categories, covering both natural landscapes and indoor settings. Each image in ADE20K is pixel-wise annotated, making it suitable for scene parsing and semantic segmentation tasks.   \n\u2022 MS-COCO: MS-COCO [23] is a large-scale dataset for image recognition, segmentation, and image captioning. It contains more than 200,000 labeled images, 150,000 validation images, and over 80,000 test images. The dataset includes 80 different object categories and over 250,000 object instances. MS-COCO is known for its detailed annotations for each image, including object segmentation, object detection, and image captioning.   \n\u2022 CITYSCAPES: CITYSCAPES [7] is a dataset for urban street scenes, primarily used for training and testing vision systems for autonomous driving. It includes street scenes from 50 different cities, with approximately 5,000 finely annotated images. These images include various urban scenarios and a range of traffic participants.   \n\u2022 SA-1B: SA-1B [19] contains 11 million diverse, high-resolution, privacy-protected images and 1.1 billion high-quality segmentation masks. These masks were automatically generated by SAM. The dataset aims to facilitate computer vision research and is characterized by an average of 100 masks per image. ", "page_idx": 14}, {"type": "text", "text": "B Platform ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Experiments are conducted on a server running a 64-bit Ubuntu 20.04.1 system with an Intel(R) Xeon(R) Silver 4210R CPU $\\textcircled{a}\\ 2.40\\mathrm{GHz}$ processor, 125GB memory, and two Nvidia GeForce RTX 3090 GPUs, each with 24GB memory. The experiments are performed using the Python language and PyTorch library version 2.1.0. ", "page_idx": 14}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/77a496edca38d1e19a1f85562af2fa7521728fddb0a0ea21c6502a44f5d64791.jpg", "img_caption": ["Figure A1: Qualitative results of the DarkSAM using point prompts on SAM-L under point prompts "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Supplementary Attack Performance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Evaluation on MobileSAM ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We evaluate the attack performance of DarkSAM against another SAM\u2019s variant model, MobileSAM [37], on four datasets. All experimental settings are kept consistent with Sec.4.2. The results in Tab. A1 demonstrate the effectiveness of DarkSAM against MobileSAM, further proving its strong attack capability. Notably, in line with the conclusions in Sec.4.2, the choice of surrogate datasets has a certain impact on the attack performance. SA-1B serves as a notably superior surrogate dataset, while CITYSCAPES exhibits comparatively lower performance in certain scenarios. This discrepancy may be attributed to CITYSCAPES\u2019 limited scope, which solely encompasses the urban street scene, consequently restricting the transferability of the generated UAPs. ", "page_idx": 14}, {"type": "text", "text": "C.2 Evaluation on SAM with ViT-L backbone ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present both quantitative and qualitative results of DarkSAM on SAM with a ViT-L backbone, denoted as SAM-L. The quantitative findings in Table A2 illustrate the effectiveness of DarkSAM in deceiving SAM-L. Notably, these results indicate that SAM-L exhibits greater robustness compared to SAM-B (SAM with a ViT-B backbone) due to its more intricate network architecture. Additionally, we offer visualization results of DarkSAM\u2019s attacks on SAM-L under point, box, and segment everything modes. Figs. A1 and A2 demonstrate that adversarial examples generated based on point prompts effectively mislead SAM-L. Similarly, adversarial examples crafted using box prompts also prove to be successful in deceiving SAM-L, as depicted in Figs. A3 and A4. ", "page_idx": 15}, {"type": "table", "img_path": "0o9E8AsFgW/tmp/e89d0d05439b4b0012f8326e208c6cf4f926aa2c6306205b4f5005c61f0c61c0.jpg", "table_caption": ["Table A1: The mIoU $(\\%)$ of DarkSAM on MobileSAM. Values covered by gray denote the clean mIoU, others denote adversarial mIoU. ADE20K, MS-COCO, CITYSCAPES abbreviated as ADE, COCO, CITY, respectively. Bolded values indicate the best results. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/63b55566110fdfd94fbde202f09e5a2ed17960288619345235726c9c9e46bc2c.jpg", "img_caption": ["Figure A2: Qualitative results of the DarkSAM using point prompts on SAM-L under the segment everything mode "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/80cf51413521b6c64b6545f2b7a3ac678614a10e4787f55e429a0d5c6266dd98.jpg", "img_caption": ["Figure A3: Qualitative results of the DarkSAM using box prompts on SAM-L under box prompts "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/db40c4bc26e1adfb9b18a250405d592903ac6fc961d3ddaf260cf58105fb47a9.jpg", "img_caption": ["Figure A4: Qualitative results of the DarkSAM using box prompts on SAM- $.L$ under the segment everything mode "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/ff014e013bc182354c92b09aa69d98aacc08f746d1ec8003e3dbf123e63efb06.jpg", "img_caption": ["Figure A5: The ASR $(\\%)$ of supplementary transferability study. (a) - (d) show the cross-model transferability study results for UAPs created on HQ-SAM. (e) - (h) show the cross-model transferability study results for UAPs created on PerSAM. Each row represents the same UAP. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/f918cde6a7ff562ab73e2b9e786f67c199a9f955a1bee3eee5adb558fbf46d76.jpg", "img_caption": ["Figure A6: Visualizations of the cross model backbone transferability study. These adversarial examples are all crafted based on SAM-B and tested on SAM-L under point prompts. (SAM-B $\\rightarrow$ SAM-L) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/1684084a884d603d4645d10b47aecd1e5dec643e94b4ef8a0fb167e764f5a72d.jpg", "img_caption": ["Figure A7: Visualizations of the cross model backbone transferability study. These adversarial examples are all crafted based on SAM-L and tested on SAM-B under point prompts. (SAM-L $\\rightarrow$ SAM-B) "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "0o9E8AsFgW/tmp/b767d7917071a7f0a6ba52904b5eaec0ebc0fa141b7146db4c8b5a834395a8d9.jpg", "table_caption": ["Table A2: The ASR $(\\%)$ of DarkSAM on SAM-L "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/cfb31f7f20ee8c902785a384a770a633740aef75b2129ee15ebce833bff858eb.jpg", "img_caption": ["Figure A8: The results $(\\%)$ of ablation study about random seeds "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Supplementary Transferability Study ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we delve deeper into the cross-model transferability of DarkSAM, considering both different model types and diverse model backbones. We maintain uniformity with the experimental settings outlined in Sec. 4.3. ", "page_idx": 17}, {"type": "text", "text": "1) Cross model type transferability. We explore the transferability of UAPs crafted by DarkSAM on PerSAM and HQ-SAM when attacking other types of models. These models all share the ViT-B backbone. The \u201cPoint-\u201d and \u201cBox-\u201dprefixes indicate that the UAPs are crafted and tested under point and box prompts, respectively. The suffixes \u201cHQ2PER\u201d and \u201cHQ2SAM\u201d denote the UAPs crafted on HQ-SAM against PerSAM and SAM, respectively. Similar notations carry the same implications. The results in Fig. A5 further demonstrate the robust cross-model type transferability of DarkSAM. ", "page_idx": 17}, {"type": "text", "text": "2) Cross model backbone transferability. We investigate the cross-model backbone transferability of DarkSAM. Specifically, we craft UAPs based on the ADE20K dataset on SAM-L and SAM-B, respectively, and test the transferability of these attacks between the two models. From Figs. A6 and A7 , we can see that adversarial examples crafted on SAM-B effectively mislead SAM-L, and conversely, those crafted on SAM-L deceive SAM-B. These results demonstrate the cross-model backbone transferability of DarkSAM. ", "page_idx": 17}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/4626f057f2bdcc5ca1a663af20d208b6a5e55cf2fe42a080b38e51e4195009ec.jpg", "img_caption": ["Figure A9: Visualization of the SAM segmentation results of adversarial examples under the multipoint evaluation mode "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "E Supplementary Ablation Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1) The effect of random seeds. Considering the relationship between random seeds and the selection of images in training and testing, we investigate the effect of random seeds on DarkSAM. All our experiments default to a random seed setting of 100. As illustrated in Fig. A8, we select eight different random seeds and conduct experiments to attack SAM on the ADE20K dataset with these seeds. \u201cP2P\u201d and \u201cB2B\u201d respectively denote the creation and testing of UAPs using point and box prompts. The results in Fig. A8 indicate that DarkSAM consistently exhibits stable and superior attack performance across various random seed settings. ", "page_idx": 18}, {"type": "text", "text": "2) The mixed use of point and box prompts in the shadow target strategy. We delve into the effects of employing a hybrid approach of points and boxes as prompts in the shadow target strategy for DarkSAM. In this method, we craft UAPs using a balanced mix of five random points and five boxes. The outcomes, as detailed in Tab. A3, reveal that UAPs constructed with this mixed approach maintain robust attack performance. This finding accentuates the adaptability and effectiveness of our proposed shadow target strategy. ", "page_idx": 18}, {"type": "text", "text": "3) Multipoint evaluation. We explore the effects of using multiple point prompts during the inference phase of SAM on the efficacy of DarkSAM\u2019s attacks. Compared to single-point prompts, multipoint prompts offer augmented object positional data, potentially enhancing segmentation accuracy. To evaluate this, we generate UAPs on the SA-1B dataset using point prompts and subsequently test these under multipoint prompts mode. The results, as illustrated in Fig. A9, indicate that for benign examples, the use of multipoint prompts indeed results in a more accurate segmentation of the target object relative to single-point prompts. Conversely, in the case of adversarial examples, the addition of multiple prompts does not aid SAM in achieving successful segmentation, thereby underscoring the powerful effectiveness of DarkSAM in compromising segmentation accuracy. ", "page_idx": 18}, {"type": "table", "img_path": "", "table_caption": ["Table A3: The ASR $(\\%)$ of DarkSAM using five points and five boxes "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "0o9E8AsFgW/tmp/b8fe6456ea37e25742513174570aa36900a0380d6eb67df652fce18b0eb27e5b.jpg", "img_caption": ["Figure A10: The results $(\\%)$ of crruption study (a) investigate the result of Contrast, (b)investigate the result of Brightness "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Defense ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "SAM is renowned for its powerful zero-shot capabilities, thus we believe an appropriate defensive measure is to refrain from making additional structural and parametric modifications to the pre-trained SAM to avoid compromising its original knowledge. Therefore, we consider employing input preprocessing methods to counter adversarial examples. We select two famous image corruption methods from the Imagecorruptions repository, contrast (C) and brightness (B), to test adversarial examples. Results in Fig. A10 demonstrate that DarkSAM effectively withstands such preprocessing-based defenses. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Abstract and Sec. 1. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Sec. 5. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: Justification: The paper does not involve theory assumptions and proofs. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] Justification: See Sec. 4.1. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the source code in the supplementary material. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Sec. 4.1. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the influence of random seed on the experimental results in Fig. A8. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] Justification: See Appendix B. ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: After a thorough review of the paper, no violations of the NeurIPS Code of Ethics are found. ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Sec. 5. ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve safeguards that have been put in place for responsible release of data or models. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The data used in the paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: N/A Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}]