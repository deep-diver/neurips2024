[{"figure_path": "mp6OWpDIJC/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation results of iAgents on InformativeBench with different LLM backends.", "description": "This table presents the performance results of the iAgents framework using four different Large Language Models (LLMs) as backends on the InformativeBench benchmark.  The benchmark evaluates the agents' ability to collaborate and solve tasks under information asymmetry. The table shows the accuracy achieved by each LLM across five sub-tasks within the benchmark, categorized into Reasoning-Oriented (Schedule dataset with three difficulty levels: Easy, Medium, Hard) and Needle-Oriented tasks (NP and FriendsTV datasets).  The results highlight the varying performance of different LLMs in handling information asymmetry and the overall challenges in this area.", "section": "6 Result"}, {"figure_path": "mp6OWpDIJC/tables/tables_7_1.jpg", "caption": "Table 2: Ablation study on iAgents. Dashes (\u2013) indicate: (1) iAgents on Reasoning-Oriented dataset does not equip other mechanisms, hence no ablation needed; (2) For NP dataset, iAgents does not utilize Mixed Memory hence there is no ablation.", "description": "This table presents the results of an ablation study conducted on the iAgents framework. It shows the impact of removing specific components, such as InfoNav, recursive communication, Fuzzy Memory, and Clear Memory, on the performance of the system across different tasks in the InformativeBench benchmark. The results reveal the relative importance of each component for effective agent collaboration in scenarios with information asymmetry.  Dashes indicate that some ablation tests were not applicable due to the design of the iAgents.", "section": "6.2 Ablation Study"}, {"figure_path": "mp6OWpDIJC/tables/tables_8_1.jpg", "caption": "Table 3: Analysis InfoNav behaviour on the trajectory of iAgents using GPT4 as backend. When agents successfully complete the task, the static collected from their trajectory proves that they better utilize the InfoNav mechanism, since the rationale solved ratio, synchronous completions of rationales, and consensus ratio are higher, and present fewer fake solved hallucinations.", "description": "This table presents an analysis of the InfoNav mechanism's behavior in iAgents using GPT-4 as the language model backend. It compares the performance metrics of successful and unsuccessful tasks, showing that successful tasks exhibit higher ratios of solved rationales, synchronous completions, and consensus, indicating better utilization of InfoNav.  The lower rate of \"fake solved\" instances in successful tasks further supports this finding.", "section": "6.3 Analysis on Agents' Behaviour"}, {"figure_path": "mp6OWpDIJC/tables/tables_15_1.jpg", "caption": "Table 1: Evaluation results of iAgents on InformativeBench with different LLM backends.", "description": "This table presents the performance of the iAgents framework using different Large Language Models (LLMs) on the InformativeBench benchmark.  It shows the accuracy achieved by each LLM on different tasks within the benchmark, categorized by difficulty level (Easy, Medium, Hard) and task type (Reasoning-Oriented, Needle-Oriented).  The results highlight the performance differences between various LLMs in handling information asymmetry within collaborative multi-agent tasks. ", "section": "6 Result"}, {"figure_path": "mp6OWpDIJC/tables/tables_15_2.jpg", "caption": "Table 1: Evaluation results of iAgents on InformativeBench with different LLM backends.", "description": "This table presents the performance of the iAgents framework on the InformativeBench benchmark using four different Large Language Models (LLMs) as backends: GPT-4, GPT-3.5, Claude Sonnet, and Gemini 1.0.  The results are broken down by task type (Reasoning-Oriented and Needle-Oriented) and difficulty level (Easy, Medium, Hard for Reasoning-Oriented; NP and FriendsTV for Needle-Oriented). The metrics used to evaluate performance vary depending on the task.  The table shows that GPT-4 generally outperforms the other LLMs, highlighting the impact of the LLM backend on performance in information asymmetry scenarios.", "section": "6 Result"}, {"figure_path": "mp6OWpDIJC/tables/tables_17_1.jpg", "caption": "Table 6: Statistic of InformativeBench.", "description": "This table presents a summary of the five datasets included in the InformativeBench benchmark.  It details the type of pipeline used (Needle or Reasoning), the number of question-answer pairs (#QA), the number of individuals and relationships in each dataset's social network, whether external memory is needed, and the evaluation metric used (Precision, F1, or IoU).  The table highlights the varying scales and complexities of the datasets, ranging from small-scale, simple networks to a large-scale, complex network representing a real-world scenario.", "section": "C InformativeBench Details"}]