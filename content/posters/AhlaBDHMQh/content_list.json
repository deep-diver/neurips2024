[{"type": "text", "text": "Learning Identifiable Factorized Causal Representations of Cellular Responses ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haiyi Mao\u22171, 2 Romain Lopez1, 3 Kai Liu 1 Jan-Christian Huetter 1 David Richmond 1 Panayiotis V. Benos2, 4 Lin Qiu \u20201 1Genentech 2University of Pittsburgh 3 Stanford University 4 University of Florida ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The study of cells and their responses to genetic or chemical perturbations promises to accelerate the discovery of therapeutic targets. However, designing adequate and insightful models for such data is difficult because the response of a cell to perturbations essentially depends on its biological context (e.g., genetic background or cell type). For example, while discovering therapeutic targets, one may want to enrich for drugs that specifically target a certain cell type. This challenge emphasizes the need for methods that explicitly take into account potential interactions between drugs and contexts. Towards this goal, we propose a novel Factorized Causal Representation (FCR) learning method that reveals causal structure in single-cell perturbation data from several cell lines. Based on the framework of identifiable deep generative models, FCR learns multiple cellular representations that are disentangled, comprised of covariate-specific $\\left(\\mathbf{z}_{x}\\right)$ , treatment-specific $\\mathbf{\\rho}(\\mathbf{z}_{t})$ , and interaction-specific $\\left(\\mathbf{z}_{t x}\\right)$ blocks. Based on recent advances in non-linear ICA theory, we prove the component-wise identifiability of $\\mathbf{z}_{t x}$ and block-wise identifiability of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{x}$ . Then, we present our implementation of FCR, and empirically demonstrate that it outperforms state-of-the-art baselines in various tasks across four single-cell datasets. The code is available on GitHub (https://github.com/Genentech/fcr). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent experimental capabilities reached by single-cell perturbation technologies open up new opportunities for characterizing cellular behaviors (Dixit et al., 2016). For example, high-throughput screening of chemical and genetic perturbations highlighted vulnerabilities in numerous cancer cell lines (McFarland et al., 2020). Identifying these vulnerabilities is crucial for pinpointing therapeutic targets, facilitating drug discovery, and furthering our understanding of gene functions (Srivatsan et al., 2020). ", "page_idx": 0}, {"type": "text", "text": "The analysis of perturbation data involves modeling how cells respond to diverse treatments across biological contexts. This task is challenging for two main reasons. First, outcomes of perturbation experiments are quantified using single-cell RNA sequencing (scRNA-seq) technologies, whose measurements are noisy as well as high-dimensional (Gr\u00fcn et al., 2014). Second, cellular contexts are difficult to comprehensively model because they are extremely variable, encompassing cell types, tissue of origin, and genetic background (Wagner et al., 2016). This highlights the need to consider interaction effects between treatments and contextual covariates while modeling gene expression outcomes (Zapatero et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "To address these challenges, several computational methods have been developed. Novel approaches based on causal representation learning provide better mechanistic interpretation of single-cell perturbation data (Lopez et al., 2023; Bereket and Karaletsos, 2023; Zhang et al., 2023). These methods belong to the family of identifiable deep generative models (Khemakhem et al., 2020; Lachapelle et al., 2022; Zheng et al., 2022) and therefore offer, to some extent, theoretical guarantees but remain limited to the analysis of data from a single cellular context. Another set of studies model cells across multiple contexts using latent linear additive models (Hetzel et al., 2022; Lotfollahi et al., 2023). However, due to their additive assumption, these models fail to characterize interactions between treatments and biological contexts. ", "page_idx": 1}, {"type": "text", "text": "To address these limitations, we introduce the Factorized Causal Representation (FCR) learning framework. This identifiable deep generative model learns representations of cells that take the form of three disentangled blocks, specific to treatments, biological contexts, and their interactions, respectively. We first present the proposed generative model and then provide a set of sufficient conditions for its identifiability, extending the work of Khemakhem et al. (2020) to the case of interacting covariates. We then present an implementation of our FCR method that builds upon the variational auto-encoder framework (Kingma and Welling, 2014) as well as adversarial regularization (Ganin et al., 2016). We demonstrate that FCR not only effectively identifies interactions but also surpasses state-of-the-art methods in various tasks across four single-cell datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Learning Representations of Cellular Responses Learning representations of single-cell data is a powerful framework, with demonstrated impact in tasks such as imputation (Lopez et al., 2018), clustering (Trapnell et al., 2014; Zhu et al., 2019; Alquicira-Hernandez et al., 2019), and integration across modalities (Gayoso et al., 2022). Recent advancements have largely improved our capacity to predict cellular responses to drug treatments (Lotfollahi et al., 2019; Rampasek et al., 2019; Lotfollahi et al., 2021; Lopez et al., 2023; Bunne et al., 2023; Zapatero et al., 2023). Lotfollahi et al. (2023) and Hetzel et al. (2022) proposed generative models that additively combine treatment embeddings and biological context embeddings within a latent space. Wu et al. (2023) cast the prediction problem as a counterfactual inference problem. Despite these advancements, existing methods fail to address how treatments may preferentially impact specific cell types, a critical point for understanding the effects of drugs on biological systems. ", "page_idx": 1}, {"type": "text", "text": "Identifiable non-linear Independent Component Analysis models A field where disentanglement is most important is non-linear Independent Component Analysis (non-linear ICA) (Hyv\u00e4rinen and Pajunen, 1999), where information from a set of latent variables is mixed through a non-linear encoding function. It has long been known that such models (i.e., either the encoding function, or the sources) are non-identifiable without further assumption. However, some recent works (Hyv\u00e4rinen et al., 2019; Lachapelle et al., 2022; Zheng et al., 2022) proved identifiability was possible in a non-stationary regime. Often, this assumption takes the form of conditional independence of the latent variables given some auxiliary (observed) variables. Notably, the iVAE framework from Khemakhem et al. (2020) offers disentanglement guarantees within this conditional VAE setup. Our work extends the theory of Khemakhem et al. (2020) to prove identifiability of the interaction terms between multiple such auxiliary variables. ", "page_idx": 1}, {"type": "text", "text": "This approach differs significantly from specific models in the Variational Autoencoder (VAE) literature (Kingma and Welling, 2014) such as the betaVAE (Higgins et al., 2016) and factorVAE (Kim and Mnih, 2018) that both address disentanglement learning. Indeed, these latter models lack theoretical foundation regarding the identifiability of their parameters or latent variables (Esmaeili et al., 2019; Chen and Grosse, 2018). ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Single-cell perturbation experiments characterize causes and effects at the cellular and molecular levels. Our objective is to disentangle the contributions of treatments, cellular covariates, and their interactions to model the effects of perturbations. Let $\\mathbf{x}\\in\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ be a vector of covariates representing intrinsic attributes of single cells, such as cell type, tissue of origin, or patient information. ", "page_idx": 1}, {"type": "text", "text": "Let $\\mathbf{t}\\in\\mathcal{T}\\subseteq\\mathbb{R}^{p}$ represent the treatment or intervention applied to single cells and let $\\mathbf{y}\\in\\mathcal{Y}\\subseteq\\mathbb{R}^{k}$ denote the gene expression levels (outcome). ", "page_idx": 2}, {"type": "text", "text": "3.1 Generative Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce a low-dimensional latent vector $\\mathbf{z}\\in\\mathcal{Z}\\subseteq$ $\\mathbb{R}^{n}$ that encodes cellular states after treatment t and in biological context $\\mathbf{x}$ . We assume a block structure for $\\textbf{z}=\\,\\mathbf{\\bar{\\lbrack}z_{x},z_{t x},z_{t}\\rbrack}$ with dimension $n\\,=\\,n_{x}\\,+\\,n_{t x}\\,+\\,n_{t}$ . Here, $\\mathbf{z}_{x}$ captures the effects of contextual covariates $\\mathbf{x}$ , $\\mathbf{z}_{t}$ represents the direct effects of the treatment $\\mathbf{t}$ , and $\\mathbf{z}_{t x}$ encodes the interaction effects between both terms. ", "page_idx": 2}, {"type": "text", "text": "More precisely, the generative model is specified as follows. Latent representation $\\mathbf{z}_{x}$ is generated from $\\mathbf{x}$ according to distribution $\\mathbf{z}_{x}\\;\\sim\\;p_{\\mathbf{z}_{x}|\\mathbf{x}}(\\mathbf{z}_{x}\\;\\mid\\;\\mathbf{x})$ , $\\mathbf{z}_{t}$ is generated from $\\mathbf{t}$ following distribution $\\mathbf{z}_{t}\\,\\sim\\,p_{\\mathbf{z}_{t}|\\mathbf{t}}(\\mathbf{z}_{t}\\,\\mid\\,\\mathbf{t})$ , and $\\mathbf{z}_{t x}$ from both $\\mathbf{x}$ and $\\mathbf{t}$ following distribution $\\mathbf{z}_{t x}\\sim$ $p_{\\mathbf{z}_{t x}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x})$ . The gene expression outcome vector $\\mathbf{y}$ is then deterministically generated $\\mathbf y=g(\\mathbf z_{x},\\mathbf z_{t x},\\mathbf z_{t})$ , where $g$ is a smooth mixing function. A graphical representation of this generative model appears in Figure 1. For the control group (no treatment), the outcome is noted as $\\mathbf{\\bar{y}}^{0}$ , and the representation as $\\mathbf{z}^{0}=[\\mathbf{z}_{x}^{0},\\mathbf{z}_{t x}^{0},\\mathbf{z}_{t}^{0}]$ . ", "page_idx": 2}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/247df64f2438d0a15d6f3beacb5068045abe7d387e931f6a98e3cb7f3996e808.jpg", "img_caption": ["Figure 1: Data generating process: shaded nodes denote observed variables. Empty nodes denote latent variables. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Learning each element in this triplet of latent variables is a sound approach for unravelling interaction effects. Indeed, $\\mathbf{z}_{x}$ captures covariate-specific patterns that are invariant with respect to the perturbations, while also capturing the essential biological attributes tied to the cellular covariates. $\\mathbf{z}_{t}$ captures the intrinsic effects of the treatments, irrespective of the covariates. $\\mathbf{z}_{t x}$ unravels the interactions that a treatment could have with specific covariates. This last representation captures the nuanced manner in which distinct cell types, tissues, or patient groups react to treatments, reflecting the diversity and specificity of biological responses. ", "page_idx": 2}, {"type": "text", "text": "We note that our model does not take into account observation noise, since $g$ is a deterministic function in our assumption. However, our theory may be readily extended to incorporate Gaussian observation noise (Khemakhem et al., 2020), as well as counting observation noise (Lopez et al., 2024). ", "page_idx": 2}, {"type": "text", "text": "3.2 Model Identifiability ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now introduce the definitions for the different classes of disentangled models that will appear later in the manuscript. Analogous definitions appear in previous work from Von K\u00fcgelgen et al. (2021) and Kong et al. (2022). Throughout this section, $\\mathbf{z}\\in{\\mathcal{Z}}$ denotes a (random) latent vector and $\\mathbf{y}\\in\\mathcal{V}$ denotes an observed vector. $g:\\mathcal{Z}\\to\\mathcal{Y}$ is an unknown mixing function such that $\\mathbf{y}=g(\\mathbf{z})$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (Component-wise Identifiability). We say that latent vector ${\\bf z}$ is identifiable from data $\\mathbf{y}$ if for any other latent vector $\\hat{\\bf z}$ such that $g(\\hat{\\mathbf{z}})$ and $g(\\mathbf{z})$ are equal in distribution, $\\mathbf{z}$ and $\\hat{\\bf z}$ are equal up to a permutation of the indices, and deformation of each component by invertible scalar functions. More precisely, there exists a permutation $\\pi$ of $\\{1,\\ldots,n\\}$ , and invertible scalar functions $h_{j}$ such that for all $j\\in\\{1,\\ldots,n\\}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{z}_{j}=h_{j}\\big(z_{\\pi(j)}\\big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $z_{j}$ and $\\hat{z}_{\\pi(j)}$ are the $\\pi(j)$ -th components of $\\mathbf{z}$ and $\\hat{\\bf z}$ respectively. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.2 (Block-wise Identifiability). For $1\\leq n_{1}<n_{2}\\leq n$ , we denote as ${\\bf z}_{[n_{1}:n_{2}]}\\in\\mathbb{R}^{n_{2}-n_{1}+1}$ the block of $\\mathbf{z}$ from index $n_{1}$ to $n_{2}$ . We say that latent vector $\\mathbf{z}_{[n_{1}:n_{2}]}$ is block-identifiable from data y if for any other latent vector $\\hat{\\bf z}$ such that $g(\\hat{\\mathbf{z}})$ and $g(\\mathbf{z})$ are equal in distribution, $\\mathbf{z}_{[n_{1}:n_{2}]}$ and $\\hat{{\\mathbf z}}_{[n_{1}:n_{2}]}$ are equal up to an invertible function $h$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{z}}_{[n_{1}:n_{2}]}=h(\\mathbf{z}_{[n_{1}:n_{2}]}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{{\\mathbf z}}_{[n_{1}:n_{2}]}$ is the corresponding block in the estimated vector $\\hat{\\bf z}$ . ", "page_idx": 2}, {"type": "text", "text": "4 Identifiability Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One strong advantage of the FCR framework is that it comes with strong theoretical guarantees concerning the disentanglement of its factorized latent space. We first prove the component-wise identifiability of the interaction variable $\\mathbf{z}_{t x}$ under the assumption of sufficient experimental variability (Khemakhem et al., 2020) (Section 4.1). Then, we demonstrate the block-identifiability of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{x}$ by exploiting their invariance with respect to $\\mathbf{x}$ and $\\mathbf{t}$ , respectively (Section 4.2). These guarantees are important, as they ensure that the obtained latent variables will have desirable semantics. For example, given our theoretical results, the obtained interaction embedding $\\hat{\\mathbf{z}}_{t x}$ must be reflective of the ground-truth interactions $\\mathbf{z}_{t x}$ only, and not of any of the other latent variables. ", "page_idx": 3}, {"type": "text", "text": "4.1 Component-wise identifiability of $\\mathbf{z}_{t x}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our proof relies on three technical assumptions. Two are classical from the nonlinear ICA literature, and the last one relates to the observability of a complementary set of experiments for identifiability of interactions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.1. The probability density function of the prior distribution for the latent variables is smooth and positive, i.e. $p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t},\\mathbf{x})>0$ for all $(\\mathbf{z},\\mathbf{t},\\mathbf{x})\\in\\mathcal{Z}\\times T\\times\\mathcal{X}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.2. The components of $\\mathbf{z}$ are conditionally independent given $\\mathbf{t}$ and $\\mathbf{x}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.3. (Experimental Sufficiency) There exist at least $2n_{t x}+1$ distinct values for the vector $[\\mathbf{t},\\mathbf{x}]$ in the experimental design. One such setting can be referred to as a control condition and is noted as $[\\mathbf{t}_{0},\\mathbf{x}_{0}]$ . Additionally, for any non-control environment $(\\mathbf{t}_{i},\\mathbf{x}_{i})$ for $i\\in\\{1,\\ldots,2n_{t x}\\}$ , we assume there always exist corresponding switched experiments under the settings $(\\mathbf{t}_{0},\\mathbf{x}_{i})$ and $(\\mathbf{t}_{i},\\mathbf{x}_{0})$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.3 is novel and essentially mandates that we conduct a sufficient number of experiments with cross-referenced covariates and treatments. This ensures that we can observe the specific drug response related to each covariate, and is often how such experiments are designed in practice. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.4. Let us first define $\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t},\\mathbf{x})$ as the vector: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf v(\\mathbf z_{t x},\\mathbf t,\\mathbf x)=\\Big[\\frac{\\partial q_{n_{x}+1}(z_{n_{x}+1},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+1}},\\cdot\\cdot\\cdot\\cdot,\\frac{\\partial q_{n_{x}+n_{t x}}(z_{n_{x}+n_{t x}},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+n_{t x}}},}\\\\ {\\frac{\\partial^{2}q_{n_{x}+1}(z_{n_{x}+1},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+1}^{2}},\\cdot\\cdot\\cdot,\\frac{\\partial^{2}q_{n_{x}+n_{t x}}(z_{n_{x}+n_{t x}},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+n_{t x}}^{2}}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q_{i}$ denotes the logarithm of probability density $p_{\\mathbf{z}_{i}|\\mathbf{t},\\mathbf{x}}$ for component $\\mathbf{z}_{i}$ . If in addition to the assumptions 4.1, 4.2, 4.3, we assume that the $2n_{t x}$ vectors ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{i},\\mathbf{x}_{i})+\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{0},\\mathbf{x}_{0})-\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{0},\\mathbf{x}_{i})-\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{i},\\mathbf{x}_{0})\\}_{i=1}^{2n_{t x}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "are linearly independent then $\\mathbf{z}_{t x}$ is component-wise identifiable. ", "page_idx": 3}, {"type": "text", "text": "The proof appears in Appendix A. Theorem 4.4 extends the concept of linear independence found in nonlinear ICA (Khemakhem et al., 2020). Unlike the original theory, where auxiliary variables must induce sufficient variations of the latent variables, we are confronted with a case where treatments and contexts must have sufficient variability in combination. For example, when $\\mathbf{v}$ is linear with respect to both $\\mathbf{t}$ and $\\mathbf{x}$ the vector of interest becomes the null vector. In this trivial case, the theorem\u2019s assumption is never satisfied $z_{t x}$ indeed has no purpose in that particular model). However, under a rich class of model with complex interaction patterns, our model will be able to infer informative latent variables. ", "page_idx": 3}, {"type": "text", "text": "4.2 Block-wise identifiability of $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To prove the block-wise identifiability of $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t}$ , we exploit their invariance properties: $\\mathbf{z}_{t}$ remains unchanged across different values of $\\mathbf{x}$ , while $\\mathbf{z}_{x}$ is stable across variations in $\\mathbf{t}$ . This invariance allows us to distinguish these blocks from the interaction terms $\\mathbf{z}_{t x}$ . Additionally, because this invariance reflects latent features\u2019 stability despite perturbations, its utilization can enable deeper biological insights and interpretability. For instance, $\\mathbf{z}_{x}$ might represent stable cellular characteristics that persist across different treatments, while $\\mathbf{z}_{t}$ could capture consistent treatment effects across various cell types. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We now state our main identifiability result for $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.5. We follow Assumptions 4.1, 4.2, 4.3, and the one from Theorem 4.4. We note as $S(Z)$ the set of subsets $S\\subseteq{\\mathcal{Z}}$ of $\\mathcal{Z}$ that satisfy the following two conditions: ", "page_idx": 4}, {"type": "text", "text": "(i) $S$ has nonzero probability measure, i.e. $\\mathbb{P}(\\mathbf{z}\\in S\\mid\\mathbf{t}=\\mathbf{t^{'}},\\mathbf{x}=\\mathbf{x^{\\prime}})>0$ for any $\\mathbf{t^{'}}\\in\\mathcal{T}$ and $\\mathbf{x^{\\prime}}\\in\\mathcal{X}$ .   \n(ii) $S$ cannot be expressed as $A_{\\mathbf{z}_{x}}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ for any $A_{\\mathbf{z}_{x}}\\subset\\mathcal{Z}_{x}$ or as $\\mathcal{Z}_{x}\\times\\mathcal{Z}_{t x}\\times A_{\\mathbf{z}_{t}}$ for any $A_{\\mathbf{z}_{t}}\\subset\\mathcal{Z}_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "We have the following identifiability result. If for all $S\\in S(\\mathcal{Z})$ , there exists $({\\bf t}_{1},{\\bf t}_{2})\\in\\mathcal{T}\\times\\mathcal{T}$ and $\\mathbf{x}\\in\\mathcal{X}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t}_{1},\\mathbf{x})d\\mathbf{z}\\neq\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t}_{2},\\mathbf{x})d\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and there also exists $(\\mathbf{x}_{1},\\mathbf{x}_{2})\\in\\mathcal{X}\\times\\mathcal{X}$ and $\\mathbf{t}\\in\\mathcal{T}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t},\\mathbf{x}_{1})d\\mathbf{z}\\neq\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t},\\mathbf{x}_{2})d\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then $\\mathbf{z}_{t}$ and $\\mathbf{z}_{x}$ are block-wise identifiable. ", "page_idx": 4}, {"type": "text", "text": "The proof appears in Appendix $_\\mathrm{A}$ , and is adapted from Kong et al. (2022). Our main assumption is that the conditional distribution $p_{\\mathbf{z|t,x}}(\\mathbf{z}\\mid\\mathbf{t,x})$ undergoes substantive changes when spanning different treatments $\\mathbf{t}$ and covariates $\\mathbf{x}$ . When treatments differ markedly from each other in their mechanisms and effects, the probability distributions of the latent variables conditioned on these treatments are unlikely to be identical. ", "page_idx": 4}, {"type": "text", "text": "5 Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now propose a tangible implementation of our method, termed Factorized Causal Representation (FCR) learning. Our approach has three components: ", "page_idx": 4}, {"type": "text", "text": "1. A variational inference approach to estimate the representations from our FCR model. Our model and inference architecture is specifically designed to learn disentangled representations $\\mathbf{z}_{x}$ , $\\mathbf{z}_{t x}$ , $\\mathbf{z}_{t}$ (Section 5.1).   \n2. A regularization method that enforces independence between $\\mathbf{z}_{x}$ and $\\mathbf{t}$ , and encourages variability of $\\mathbf{z}_{t}$ with respect to $\\mathbf{x}$ (Section 5.2).   \n3. A second regularization technique to ensure that the conditional independence properties $\\mathbf{z}_{x}$ \u22a5\u22a5 $\\mathbf{z}_{t x}\\mid\\mathbf{x}$ and $\\mathbf{z}_{t}\\perp\\!\\!\\!\\perp\\mathbf{z}_{t x}\\mid\\mathbf{t}$ are satisfied (Section 5.3). ", "page_idx": 4}, {"type": "text", "text": "The main computational structure of the model is illustrated as a schematic in Figure 2. ", "page_idx": 4}, {"type": "text", "text": "5.1 Model Specification and Variational Inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Model Specification We parameterize the probability distributions as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\mathbf{z}_{x}\\mid\\mathbf{x}):=\\operatorname{Normal}(f_{\\mu}^{x}(\\mathbf{x}),f_{\\sigma}^{x}(\\mathbf{x}))}\\\\ &{\\ \\ \\ \\ \\ p(\\mathbf{z}_{t}\\mid\\mathbf{t}):=\\operatorname{Normal}(f_{\\mu}^{t}(\\mathbf{t}),f_{\\sigma}^{t}(\\mathbf{t}))}\\\\ &{p(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x}):=\\operatorname{Normal}(f_{\\mu}^{t,x}(\\mathbf{t},\\mathbf{x}),f_{\\sigma}^{t,x}(\\mathbf{t},\\mathbf{x})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where all the above functions are parameterized by neural networks. ", "page_idx": 4}, {"type": "text", "text": "To prevent $\\mathbf{z}_{t x}$ from having trivial dependencies with respect to $\\mathbf{t}$ and $\\mathbf{x}$ , we explicitly encourage its prior to capture interactions between $\\mathbf{x}$ and t by designing the functions $f_{\\mu}^{t,x}$ and $\\dot{f}_{\\sigma}^{t,x}$ to be of the form: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{\\mu}^{t,x}=f_{\\mu}(k_{\\mathbf{x}}(\\mathbf{x})\\odot k_{\\mathbf{t}}(\\mathbf{t}))}\\\\ &{f_{\\sigma}^{t,x}=f_{\\sigma}(k_{\\mathbf{x}}(\\mathbf{x})\\odot k_{\\mathbf{t}}(\\mathbf{t})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/b05ae75dc2f0fb4a3b274754b305503bb343d23f839a59932445e64963a3bcdb.jpg", "img_caption": ["Figure 2: The illustration of FCR models. (a) is the component for $p(\\mathbf{z}_{x}\\mid\\mathbf{\\Deltax}),p(\\mathbf{z}_{t}\\mid\\mathbf{\\Deltat})$ and $p(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x})$ (b) is the component to estimate $q(\\mathbf{z}_{x}\\mid\\mathbf{x},\\mathbf{y}),q(\\mathbf{z}_{t}\\mid\\mathbf{t},\\mathbf{y})$ and $q(\\mathbf{z}_{t,x}\\mid\\mathbf{t},\\mathbf{x},\\mathbf{y})$ . Note that 0 indicates $\\mathbf{t}=\\mathbf{0}$ representing the control samples. (c) computational diagrams to estimate the Kullback-Leibler divergences, causal structure regularization and permutation discriminators. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $k_{x}(\\mathbf{x})$ and $k_{t}(\\mathbf{t})$ represent the embeddings for the cellular covariates and treatments, respectively, while $\\odot$ denotes the Hadamard product. ", "page_idx": 5}, {"type": "text", "text": "Variational Inference Because the posterior distribution of the latent variables are intractable, we use the variational autoencoder framework (Kingma and Welling, 2014) to jointly learn the model\u2019s parameters and an approximation to the posterior, following the approach used in previous causal representation learning work (Khemakhem et al., 2020). We consider the following mean-field variational approximation to the posterior distribution: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq_{\\phi}(\\mathbf{z}_{x},\\mathbf{z}_{t},\\mathbf{z}_{t x}\\mid\\mathbf{x},\\mathbf{t},\\mathbf{y})=q_{\\phi}(\\mathbf{z}_{x}\\mid\\mathbf{x},\\mathbf{y})q_{\\phi}(\\mathbf{z}_{t}\\mid\\mathbf{t},\\mathbf{y})q_{\\phi}(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x},\\mathbf{y}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Following the graphical model from Figure 1, the Evidence Lower Bound (ELBO) is derived as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{ELBO}}=\\!\\mathbb{E}_{q_{\\phi}(\\mathbf{z}_{x},\\mathbf{z}_{t},\\mathbf{z}_{t x}|\\mathbf{x},\\mathbf{t},\\mathbf{y})}\\log p_{\\theta}(\\mathbf{y}\\mid\\mathbf{z}_{x},\\mathbf{z}_{t},\\mathbf{z}_{t x})}\\\\ &{\\quad\\quad\\quad\\quad-D_{K L}\\big(q_{\\phi}(\\mathbf{z}_{x}\\mid\\mathbf{x},\\mathbf{y})\\vert\\vert p_{\\theta}(\\mathbf{z}_{x}\\mid\\mathbf{x})\\big)}\\\\ &{\\quad\\quad\\quad\\quad-D_{K L}\\big(q_{\\phi}(\\mathbf{z}_{t}\\mid\\mathbf{t},\\mathbf{y})\\vert\\vert p_{\\theta}(\\mathbf{z}_{t}\\mid\\mathbf{t})\\big)}\\\\ &{\\quad\\quad\\quad\\quad-D_{K L}\\big(q_{\\phi}(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x},\\mathbf{y})\\vert\\vert p_{\\theta}(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\theta$ and $\\phi$ denote the parameters of the generative model and the inference networks, respectively. $D_{K L}$ denotes the Kullback\u2013Leibler divergence between two probability distributions. For simplicity, we omit $\\phi$ and $\\theta$ as well as script notations in the following sections, wherever appropriate. The derivation of the ELBO appears in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "The variational distributions defined in Equation 11 are parameterized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{z}_{x}\\mid\\mathbf{x},\\mathbf{y}):=\\mathrm{Normal}(g_{\\mu}^{x}(\\mathbf{x},\\mathbf{y}),g_{\\sigma}^{x}(\\mathbf{x},\\mathbf{y}))}\\\\ &{\\quad q(\\mathbf{z}_{t}\\mid\\mathbf{t},\\mathbf{y}):=\\mathrm{Normal}(g_{\\mu}^{t}(\\mathbf{t},\\mathbf{y}),g_{\\sigma}^{t}(\\mathbf{t},\\mathbf{y}))}\\\\ &{q(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x},\\mathbf{y}):=\\mathrm{Normal}(g_{\\mu}^{t,x}(\\mathbf{t},\\mathbf{x},\\mathbf{y}),g_{\\sigma}^{t,x}(\\mathbf{t},\\mathbf{x},\\mathbf{y})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where all the functions introduced above are parameterized by neural networks. ", "page_idx": 5}, {"type": "text", "text": "5.2 Causal Structure Regularization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We exploit both the variability of $\\mathbf{z}_{t}$ and the invariance of $\\mathbf{z}_{x}$ when comparing control and treated groups that share the same covariates. Specifically, our goal is to enforce the resemblance between $\\mathbf{z}_{x}$ and $\\mathbf{z}_{x}^{0}$ while reducing the congruence of $\\mathbf{z}_{t}$ and $\\ensuremath{\\mathbf{z}}_{t}^{0}$ . Towards this end, we first add the following score as a regularizer, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{sim}}=\\mathbb{E}_{q(\\mathbf{z}_{x},\\mathbf{z}_{t}|\\mathbf{x},\\mathbf{t})q(\\mathbf{z}_{x}^{0},\\mathbf{z}_{t}^{0}|\\mathbf{x}_{0},\\mathbf{t}_{0})}\\left[\\sin\\left(\\mathbf{z}_{t},\\mathbf{z}_{t}^{0}\\right)-\\sin\\left(\\mathbf{z}_{x},\\mathbf{z}_{x}^{0}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\dim(\\cdot)$ denotes the cosine similarity. Second, we introduce a classifier $f_{\\mathrm{ct}}$ to predict the treatments $\\mathbf{t}$ from $[{\\bf z}_{t},{\\bf z}_{t x}]$ and $[\\mathbf{z}_{t}^{0},\\mathbf{z}_{t x}^{0}]$ as follows, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{t}}=f_{\\mathrm{ct}}\\left(\\left[\\mathbf{z}_{t},\\mathbf{z}_{t x}\\right],\\left[\\mathbf{z}_{t}^{0},\\mathbf{z}_{t x}^{0}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The predicted treatment probability vector $\\tilde{\\mathbf{t}}$ is then used for the computation of a cross-entropy loss ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ct}}=-\\mathbb{E}_{t,q(\\mathbf{z}_{t x},\\mathbf{z}_{t}|\\mathbf{x},\\mathbf{t})q(\\mathbf{z}_{t x}^{0},\\mathbf{z}_{t}^{0}|\\mathbf{x}^{0},\\mathbf{t}^{0})}[\\mathbf{t}\\cdot\\mathrm{log}(\\tilde{\\mathbf{t}})].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5.3 Permutation Discriminators ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We want to ensure the conditions of Assumption 4.2 and Theorem 4.4, specifically that $\\mathbf{z}_{t x}\\perp\\!\\!\\!\\perp\\mathbf{z}_{t}\\mid$ t and $\\mathbf{z}_{t x}\\perp\\!\\!\\!\\perp\\mathbf{z}_{x}\\mid\\mathbf{x}$ . Towards this goal, we use the following proposition, establishing a connection between exchangeability and conditional independence. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.1. (Bellot and van der Schaar, 2019) Let $X,Y$ and $Z$ be three random variables. Under the assumption of $X\\perp\\!\\!\\!\\perp Y\\mid Z_{\\!\\!\\!\\!\\!\\!\\parallel}$ , we have the samples $(X_{i},Y_{i},Z_{i})_{i=1}^{M}$ and permuted samples $(X_{\\pi(i)},Y_{i},Z_{i})_{i=1}^{M}$ with a permutation function $\\pi$ . The corresponding statistics $\\rho_{i}$ of $(X_{i},Y_{i})_{i=1}^{M}$ and $\\rho_{\\pi(i)}$ of $(X_{\\pi(i)},Y_{i})_{i=1}^{M}$ are exchangeable. ", "page_idx": 6}, {"type": "text", "text": "This proposition states that permutation will not change the independence between two conditionally independent random variables. We therefore propose to use permutation discriminators for $\\mathbf{z}_{x}$ , $\\mathbf{z}_{t x}$ and $\\mathbf{z}_{t}$ , $\\mathbf{z}_{t x}$ . First, we initially permute $\\mathbf{z}_{t x}$ within the triplet $(\\mathbf{z}_{x}^{(j)},\\mathbf{z}_{t x}^{(j)},\\mathbf{x}^{(j)}=\\mathbf{x}_{i})_{j=1}^{M}$ to yield $(\\mathbf{z}_{x}^{(j)},\\mathbf{z}_{t x.}^{\\pi(j)},\\mathbf{x}_{.}^{(j)}=\\mathbf{x}_{i})_{j=1}^{M}$ .e dT hore nn, otw. e Wtrea idne na obtien tahrey  pcelarsmsiuftaietri o(tnh lea bdiesl carsi iannadt oprr) etdoi cptr iet daisct whether $l$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{l}=f_{\\mathrm{dis}_{\\mathbf x}}(\\mathbf z_{x},\\hat{\\mathbf z}_{t x},\\mathbf x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\mathbf{z}}_{t x}$ could be permuted or non-permuted $\\mathbf{z}_{t x}$ samples. If $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t x}$ are indeed independent given $\\mathbf{x}$ , the discriminator should be unable to distinguish between the permuted and the original samples. For each discriminator, we add a regularization term that consists of the cross-entropy loss ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{dis}_{\\mathbf{x}}}=-\\mathbb{E}_{\\mathbf{x},q(\\mathbf{z}_{t x},\\mathbf{z}_{x}|\\mathbf{x},\\mathbf{t})}[l\\log(\\tilde{l})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We proceed similarly to make sure that $\\mathbf{z}_{t}$ and $\\mathbf{z}_{t x}$ are independent conditionally on $\\mathbf{t}$ . ", "page_idx": 6}, {"type": "text", "text": "5.4 Objective function ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, we specify the overall loss for our model as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{ELBO}}+\\omega_{1}\\mathcal{L}_{\\mathrm{sim}}+\\omega_{2}\\mathcal{L}_{\\mathrm{ct}}-\\omega_{3}\\big(\\mathcal{L}_{\\mathrm{dis}_{\\mathrm{x}}}+\\mathcal{L}_{\\mathrm{dis}_{\\mathrm{t}}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\omega_{1},\\omega_{2},\\omega_{3}>0$ are hyperparameters. To concurrently train both the representations and the discriminators, we employ an adversarial training approach as follows, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{f_{\\mathrm{dis}_{\\mathbf{t}}},f_{\\mathrm{dis}_{\\mathbf{x}}}}\\operatorname*{min}_{\\theta,\\phi,f_{\\mathrm{ct}}}\\mathcal{L}_{\\mathrm{total}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The training procedure and the procedure used for hyperparameters selection are detailed in Appendix $\\mathbf{C}$ and D, respectively. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we are pursuing three primary objectives. First, we seek to validate the FCR method\u2019s proficiency in capturing the designated causal structure within the latent space through both clustering analysis (Section 6.1) and statistical testing of independence (Section 6.2), respectively. Second, we evaluate the method\u2019s efficacy in predict single-cell level conditional cellular responses (Section 6.3). We note that the current implementation of FCR does not make use of general embeddings for t or $\\mathbf{x}$ , and for that reason we do not perform experiments to predict cellular responses to unseen treatments and cell types (covariates). ", "page_idx": 6}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/60a122c2586f36b436939a3f94d44c89adfc59da4c200eec8d5f6f9ae04d90c8.jpg", "img_caption": ["Figure 3: Clustering results for the sciPlex dataset. Normalized Mutual Information (NMI) values for clustering based on: (a) covariates x; (b) combined covariates and treatments $[\\mathbf{x};\\mathbf{t}]$ ; (c) treatments $\\mathbf{t}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Datasets To evaluate the efficacy and robustness of the FCR method, we conducted our study on four real single-cell perturbation datasets (Appendix E). The first of these is the sciPlex dataset (Srivatsan et al., 2020), which provides insights into the impact of several HDAC (Histone Deacetylase) inhibitors on a total of 11,755 cells from three distinct cell lines: A549, K562, and MCF7. Each of these cell lines was subjected to treatment in two independent replicate experiments, using five different drug dosages: $\\boldsymbol{0}\\,\\mathrm{nM}$ (control), $10\\;\\mathrm{nM}$ , $100\\;\\mathrm{nM}$ , $1\\ \\upmu\\mathrm{M}$ , and $10\\,\\upmu\\mathrm{M}$ . The subsequent three datasets are sourced from (McFarland et al., 2020), which executed several large-scale experiments in varied settings. The multiPlex-Tram dataset contains 13,713 cells from 24 cell lines, treated with Trametinib and a DMSO control over durations of 3, 6, 12, 24, and 48 hours. The multiPlex-7 dataset spans 61,552 cells across 97 cell lines, subjected to seven different treatments. Finally, the multiPlex-9 dataset incorporates 19,524 cells from 24 cell lines, undergoing a series of nine treatments. ", "page_idx": 7}, {"type": "text", "text": "Baselines We benchmarked our method against several established representation learning methods: (1) scVI (Lopez et al., 2018), (2) iVAE (Khemakhem et al., 2020), (3) $\\beta\\mathrm{VAE}$ (Higgins et al., 2016), (4) factorVAE (Kim and Mnih, 2018), (5) VCI (Wu et al., 2023), (6) CPA (Lotfollahi et al., 2023), (7) scGEN (Lotfollahi et al., 2019) (8) sVAE (Lopez et al., 2023), (9) CINEMA-OT (Dong et al., 2023). For the clustering analysis, we employed all the inferred latent variables from each baseline method. For the conditional independence test, we selected a random subset of latent variables for each baseline matching the number of latent variables of FCR. We then tested each subset and repeated the process a total of ten times for each baseline using different random subsets to yield the best results. Specifically, CINEMA-OT and scGEN address only binary treatments/perturbations, so they are only considered in the cellular response predictions tasks. ", "page_idx": 7}, {"type": "text", "text": "Results on additional dataset, simulation studies, ablation studies, data visualization, and biological interpretation of the latent variables appear in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "6.1 Clustering Analysis on Covariates, Treatments, and Combined Features ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluated the performance of the obtained latent representations in capturing three key aspects: the cellular covariates $\\mathbf{x}$ , the treatments t, and their interaction $\\bf(x,t)$ . To assess each latent representation, we applied clustering and compared the fidelity of the resulting cluster labels with the corresponding $\\mathbf{x},\\,\\mathbf{t}.$ , or $\\bf(x,t)$ from the original data. This approach allowed us to gauge how well the latent representations preserved the underlying structure of the cellular covariates, treatments, and their interactions. We performed clustering using the Leiden algorithm (Traag et al., 2019). To assess the fidelity between two sets of cluster labels, we employed the Normalized Mutual Information (NMI) metric (Kim et al., 2019). Higher NMI values indicate better alignment between the clustering results and the original data structure. We conducted this clustering and fidelity assessment on our model\u2019s latent variables $\\mathbf{z}_{x}$ , $\\mathbf{z}_{t x}$ , and $\\mathbf{z}_{t}$ , as well as on the variables obtained from baseline methods. ", "page_idx": 7}, {"type": "text", "text": "Our results highlight that $\\mathbf{z}_{x}$ has superior performance in clustering on covariates $\\mathbf{x}$ compared to all other available latent representations (Figure 3). Similarly, $\\mathbf{z}_{t}$ performs best for clustering on treatments $\\mathbf{t}$ . Finally, $\\mathbf{z}_{t x}$ outperforms all other methods when clustering jointly on t and $\\mathbf{x}$ , showing that it faithfully represents the combined features of both $\\mathbf{x}$ and $\\mathbf{t}$ . Specifically, for the sciPlex datasets, clustering on $\\mathbf{x}$ yields better results than clustering based on both $(\\mathbf{t},\\mathbf{x})$ or on solely t. This can be attributed to the HDAC inhibitors exhibiting minimal impact on distinct cell lines until a maximal concentration of $10\\,\\upmu\\mathrm{M}$ was reached. We report similar results for the other datasets in Appendix F. Taken together, these results suggest that FCR effectively learns disentangled representations across different datasets. ", "page_idx": 7}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/dba4e1d6ed02bd970c2237f4ff351f0d286db800b8d48bd7cb064f31df00febb.jpg", "img_caption": ["Figure 4: Statistical Conditional Independence Testing Results (a) $\\mathbf{p}.$ -values for the conditional independence test of $\\mathbf{z}_{x}$ \u22a5\u22a5 $\\textbf{t}|\\textbf{x}$ . The red dashed line indicates the 0.05 level. (b) p-values for the conditional independence test of $\\mathbf{z}_{t}$ \u22a5\u22a5x | t. (c) $\\mathbf{p}$ -values for the conditional independence tests $\\mathbf{z}_{x}\\perp\\!\\!\\!\\perp\\mathbf{z}_{t x}\\mid\\mathbf{x}$ and $\\mathbf{z}_{t}$ \u22a5\u22a5 $\\mathbf{z}_{t x}$ | t. (d) HSIC values for assessment of marginal independence of $\\mathbf{z}_{x}$ with $\\mathbf{x}$ , t and random numbers $(\\bf R)$ ; as well as $\\mathbf{z}_{t}$ with $\\mathbf{x}$ , t and random numbers $(\\bf R)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.2 Statistical Conditional Independence Testing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We validated the disentanglement of our latent representations via conditional independence testing, implemented as the Kernel Conditional Independence (KCI) (Zhang et al., 2012) tests. Our investigations focused on the following relationships: (1) $\\mathbf{z}_{x}$ \u22a5\u22a5t | x; (2) $\\mathbf{z}_{t}\\perp\\!\\!\\!\\perp\\mathbf{x}\\mid$ t; (3) $\\mathbf{z}_{t}$ \u22a5\u22a5 $\\mathbf{z}_{t x}\\mid$ t; (4) $\\mathbf{z}_{x}$ \u22a5\u22a5 $\\mathbf{z}_{t x}\\mid\\mathbf{x}$ . In conjunction with these tests, we also employed the Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2005) to evaluate the (marginal) independence between: (a) $\\mathbf{z}_{t}$ and $\\mathbf{t}$ ; (b) $\\mathbf{z}_{x}$ and $\\mathbf{x}$ . Our aim was two-fold. First, we wanted to assess whether the factors in our latent space complied with the necessary conditional independence statements, corroborating a well-captured causal structure. Second, we wanted to determine the dependence between our latent representations and their respective observed variables. ", "page_idx": 8}, {"type": "text", "text": "We focus our presentation of the results on the sciPlex dataset in the main text (results for the other datasets imply similar conclusions and appear in Appendix F.4). We first examined the results of testing for conditional independence statements $\\mathbf{z}_{x}\\perp\\!\\!\\!\\perp\\!\\!\\!\\!\\textbf{t}|\\textbf{x}$ and $\\textbf{z}_{t}\\,\\perp\\!\\!\\!\\perp\\,\\textbf{x}\\:|$ t (Figure 4ab). In this experiment, all the baseline methods produced p-values smaller than 0.05. This implies a rejection of the null hypothesis (conditional independence) for the baseline methods, and suggests that their representations failed to maintain the desired conditional independence statements. Second, we examined the results of testing for conditional independence statements $\\mathbf{z}_{x}\\perp\\!\\!\\!\\!\\perp\\mathbf{z}_{t x}\\mid\\mathbf{x}$ and $\\mathbf{z}_{t}\\perp\\!\\!\\!\\perp\\mathbf{z}_{t x}\\mid$ t (Figure 4c). Interestingly, this assessment also quantifies the efficacy of our permutation discriminators. The observed p-values generally exceed 0.05, suggesting that the null hypothesis cannot be rejected. Finally, we use the HSIC to report estimates of mutual information (assessing for marginal independence). Low HSIC values suggest poor dependence between the pairs of random variables. We report the HSIC values for assessing $\\mathbf{z}_{x}$ \u22a5\u22a5 $\\mathbf{x}$ , $\\mathbf{z}_{x}$ \u22a5\u22a5t, $\\mathbf{z}_{x}$ \u22a5\u22a5 $\\mathbf{R}$ , as well as $\\mathbf{z}_{t}$ \u22a5\u22a5 $\\mathbf{x}$ , $\\mathbf{z}_{t}$ \u22a5\u22a5 $\\mathbf{t}$ , and $\\mathbf z_{t}\\perp\\!\\!\\!\\perp\\mathbf R$ , where $\\mathbf{R}$ represents simulated random vectors (Figure 4d). Contrasting our representations with randomly simulated vectors, we observe that both $\\mathbf{z}_{x}$ and $\\mathbf{x}$ , as well as $\\mathbf{z}_{t}$ and $\\mathbf{t}$ , have HSIC values far from zero, indicating a high dependence. The contrast in results between our approach and the baseline methods highlights FCR\u2019s nuanced capability in capturing and preserving causal structures. ", "page_idx": 8}, {"type": "text", "text": "6.3 Conditional Cellular Responses Prediction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our analysis demonstrates that treatments often elicit covariate (cell line) specific responses. Consequently, accurately predicting outcomes for novel drugs or cell lines becomes challenging without a careful consideration of the similarity in $\\mathbf{z}_{t x}$ and how $\\mathbf{t}$ interacts with $\\mathbf{x}$ . Unlike previous literature, we do not conduct experiments to predict cellular responses to unseen treatments and cell types (covariates). This decision is based on extensive biological research showing that responses to covariates are context-specific (McFarland et al., 2020; Srivatsan et al., 2020). Without thoroughly examining the similarity of unseen treatments or cell types in the latent space, we cannot confidently predict cellular responses. ", "page_idx": 8}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/2b96bbc204adfa780db66bc6c2982c88dcd59806cf741215c9a8522054742a5a.jpg", "table_caption": ["Table 1: The $R^{2}$ score of the conditional cellular responses prediction. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Nonetheless, our approach enables the prediction of cellular responses at the single-cell level. This paper focuses on predicting cellular responses (expression of 2000 genes) in control cells subjected to drug treatments. It is important to note that our comparative analysis is confined to CPA, VCI, sVAE, scGEN and CINEMA-OT as they are uniquely tailored for this task. We utilize FCR to extract control\u2019s $[\\mathbf{z}_{x}^{0},\\mathbf{z}_{t x}^{0},\\mathbf{z}_{t}^{0}]$ and corresponding experiments\u2019 $\\left[{{\\bf{z}}_{x}},{{\\bf{z}}_{t x}},{{\\bf{z}}_{t}}\\right]$ ,  then use the decoder $g$ to predict the gene expression level as $\\hat{\\mathbf{y}}=\\overline{{g(\\mathbf{z}_{x}^{0},\\bar{\\mathbf{z}}_{t x},\\bar{\\mathbf{z}}_{t}}})$ . We measure the $R^{2}$ score. From our results (Table 1), we observe that FCR generally outperforms other baselines across the first three datasets. However, CPA performs the best on the multiPlex-9 dataset. The primary reason for this is that the multiPlex-9 dataset has fewer covariate-specific responses (McFarland et al., 2020). Additionally, scGEN and CINEMA-OT, which are designed for binary perturbations, tend to underperform in these tasks. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper aimed to resolve a current challenge\u2014how to disentangle single-cell level drug responses using latent variables into representations for covariates, treatments and contextual covariate-treatment interactions. To do so, we established a theoretically grounded framework for identifiability of such components $\\mathbf{z}_{t x}$ , $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t}$ . Expanding upon these theoretical foundations, we have developed the FCR algorithm to factorize the interactions between treatments and covariates. Looking ahead, our aim is to incorporate interpretable components into this framework. This enhancement will aid in pinpointing genes affected by $\\mathbf{z}_{x}$ , $\\mathbf{z}_{t}$ or $\\mathbf{z}_{t x}$ . Such advancements are expected to significantly contribute to the progress of precision medicine. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to extend gratitude to Gregory Barlow for his contribution to the graphic design. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Alquicira-Hernandez, S., Ji, N., and Powell (2019). scPred: accurate supervised method for cell-type classification from single-cell RNA-seq data. Genome Biology, 20:264. ", "page_idx": 9}, {"type": "text", "text": "Bellot, A. and van der Schaar, M. (2019). Conditional independence testing using generative adversarial networks. Advances in Neural Information Processing Systems, 32. ", "page_idx": 9}, {"type": "text", "text": "Bereket, M. and Karaletsos, T. (2023). Modelling cellular perturbations with the sparse additive mechanism shift variational autoencoder. In Advances in Neural Information Processing Systems. ", "page_idx": 9}, {"type": "text", "text": "Bunne, C., Stark, S. G., Gut, G., Del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans, L., Krause, A., and R\u00e4tsch, G. (2023). Learning single-cell perturbation responses using neural optimal transport. Nature Methods, pages 1\u201310. ", "page_idx": 9}, {"type": "text", "text": "Chen, L. and Grosse, D. (2018). Isolating sources of disentanglement in VAEs. In Advances in Neural Information Processing Systems, pages 2615\u20132625.   \nDixit, A., Parnas, O., Li, B., Chen, J., Fulco, C. P., Jerby-Arnon, L., Marjanovic, N. D., Dionne, D., Burks, T., Raychowdhury, R., et al. (2016). Perturb-seq: dissecting molecular circuits with scalable single-cell RNA profiling of pooled genetic screens. cell, 167(7):1853\u20131866.   \nDong, M., Wang, B., Wei, J., de O. Fonseca, A. H., Perry, C. J., Frey, A., Ouerghi, F., Foxman, E. F., Ishizuka, J. J., Dhodapkar, R. M., et al. (2023). Causal identification of single-cell experimental perturbation effects with cinema-ot. Nature Methods, 20(11):1769\u20131779.   \nEsmaeili, B., Wu, H., Jain, S., Bozkurt, A., Siddharth, N., Paige, B., Brooks, D. H., Dy, J., and van de Meent, J.-W. (2019). Structured disentangled representations. In International Conference on Artificial Intelligence and Statistics, volume 89, pages 2525\u20132534.   \nGanin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. S. (2016). Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17:1\u201335.   \nGayoso, A., Lopez, R., Xing, G., Boyeau, P., Valiollah Pour Amiri, V., Hong, J., Wu, K., Jayasuriya, M., Mehlman, E., Langevin, M., et al. (2022). A Python library for probabilistic analysis of single-cell omics data. Nature Biotechnology, 40(2):163\u2013166.   \nGretton, A., Bousquet, O., Smola, A., and Sch\u00f6lkopf, B. (2005). Measuring statistical dependence with hilbert-schmidt norms. In International Conference on Algorithmic Learning Theory, pages 63\u201377.   \nGr\u00fcn, D., Kester, L., and van Oudenaarden, A. (2014). Validation of noise models for single-cell transcriptomics. Nature Methods, 11(6):637\u2013640.   \nHetzel, L., Boehm, S., Kilbertus, N., G\u00fcnnemann, S., Lotfollahi, M., and Theis, F. J. (2022). Predicting cellular responses to novel drug perturbations at a single-cell resolution. Advances in Neural Information Processing Systems, 35:26711\u201326722.   \nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. (2016). beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations.   \nHyv\u00e4rinen, A. and Pajunen, P. (1999). Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3):429\u2013439.   \nHyv\u00e4rinen, A., Sasaki, H., and Turner, R. (2019). Nonlinear ICA using auxiliary variables and generalized contrastive learning. In International Conference on Artificial Intelligence and Statistics, pages 859\u2013868.   \nKhemakhem, I., Kingma, D., Monti, R., and Hyv\u00e4rinen, A. (2020). Variational autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pages 2207\u20132217.   \nKim, H. and Mnih, A. (2018). Disentangling by factorising. In International Conference on Machine Learning, pages 2649\u20132658.   \nKim, T., Chen, I. R., Lin, Y., Wang, A. Y.-Y., Yang, J. Y. H., and Yang, P. (2019). Impact of similarity metrics on single-cell RNA-seq data clustering. Briefings in bioinformatics, 20(6):2316\u20132326.   \nKingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. In International Conference on Learning Representations.   \nKlindt, D. A., Schott, L., Sharma, Y., Ustyuzhaninov, I., Brendel, W., Bethge, M., and Paiton, D. (2021). Towards nonlinear disentanglement in natural data with temporal sparse coding. In International Conference on Learning Representations.   \nKong, L., Xie, S., Yao, W., Zheng, Y., Chen, G., Stojanov, P., Akinwande, V., and Zhang, K. (2022). Partial disentanglement for domain adaptation. In International Conference on Machine Learning, pages 11455\u201311472.   \nLachapelle, S., Rodriguez, P., Sharma, Y., Everett, K. E., Le Priol, R., Lacoste, A., and Lacoste-Julien, S. (2022). Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In Conference on Causal Learning and Reasoning, pages 428\u2013484.   \nLopez, R., Huetter, J.-C., Hajiramezanali, E., Pritchard, J. K., and Regev, A. (2024). Toward the identifiability of comparative deep generative models. In Conference on Causal Learning and Reasoning, volume 236, pages 868\u2013912.   \nLopez, R., Regier, J., Cole, M. B., Jordan, M. I., and Yosef, N. (2018). Deep generative modeling for single-cell transcriptomics. Nature Methods, 15(12):1053\u20131058.   \nLopez, R., Tagasovska, N., Ra, S., Cho, K., Pritchard, J., and Regev, A. (2023). Learning causal representations of single cells via sparse mechanism shift modeling. In Conference on Causal Learning and Reasoning, pages 662\u2013691.   \nLotfollahi, M., Klimovskaia Susmelj, A., De Donno, C., Hetzel, L., Ji, Y., Ibarra, I. L., Srivatsan, S. R., Naghipourfar, M., Daza, R. M., Martin, B., et al. (2023). Predicting cellular responses to complex perturbations in high-throughput screens. Molecular Systems Biology, page e11517.   \nLotfollahi, M., Naghipourfar, M., Theis, F. J., and Wolf, F. A. (2021). Machine learning for perturbational single-cell omics. Cell Systems, 12(6):522\u2013537.   \nLotfollahi, M., Wolf, F. A., and Theis, F. J. (2019). scGen predicts single-cell perturbation responses. Nature methods, 16(8):715\u2013721.   \nMcFarland, J. M., Paolella, B. R., Warren, A., Geiger-Schuller, K., Shibue, T., Rothberg, M., Kuksenko, O., Colgan, W. N., Jones, A., Chambers, E., et al. (2020). Multiplexed single-cell transcriptional response profiling to define cancer vulnerabilities and therapeutic mechanism of action. Nature communications, 11(1):4296.   \nMcInnes, L., Healy, J., and Melville, J. (2018). UMAP: Uniform manifold approximation and projection for dimension reduction. arXiv.   \nRampasek, L., Hidru, D., Smirnov, P., Haibe-Kains, B., and Goldenberg, A. (2019). Dr.VAE: improving drug response prediction via modeling of drug perturbation effects. Bioinformatics, 12(6):3743\u20133751.   \nRoohani, Y., Huang, K., and Leskovec, J. (2024). Predicting transcriptional outcomes of novel multigene perturbations with GEARS. Nature Biotechnology, 42(6):927\u2013935.   \nSrivatsan, S. R., McFaline-Figueroa, J. L., Ramani, V., Saunders, L., Cao, J., Packer, J., Pliner, H. A., Jackson, D. L., Daza, R. M., Christiansen, L., et al. (2020). Massively multiplex chemical transcriptomics at single-cell resolution. Science, 367(6473):45\u201351.   \nTraag, V. A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing well-connected communities. Scientific reports, 9(1):5233.   \nTrapnell, C., Cacchiarelli, D., Grimsby, J., Pokharel, P., Li, S., Morse, M., Lennon, N. J., Livak, K. J., Mikkelsen, T. S., and Rinn, J. L. (2014). The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells. Nature Biotechnology, 32(4):381\u2013386.   \nVon K\u00fcgelgen, J., Sharma, Y., Gresele, L., Brendel, W., Sch\u00f6lkopf, B., Besserve, M., and Locatello, F. (2021). Self-supervised learning with data augmentations provably isolates content from style. Advances in Neural Information Processing Systems, 34:16451\u201316467.   \nWagner, A., Regev, A., and Yosef, N. (2016). Revealing the vectors of cellular identity with single-cell genomics. Nature Biotechnology, 34(11):1145\u20131160.   \nWu, Y., Barton, R., Wang, Z., Ioannidis, V. N., De Donno, C., Price, L. C., Voloch, L. F., and Karypis, G. (2023). Predicting cellular responses with variational causal inference and refined relational information. In International Conference on Learning Representations.   \nZapatero, M. R., Tong, A., Opzoomer, J. W., O\u2019Sullivan, R., Rodriguez, F. C., Suf,i J., Vlckova, P., Nattress, C., Qin, X., Claus, J., et al. (2023). Trellis tree-based analysis reveals stromal regulation of patient-derived organoid drug responses. Cell, 186(25):5606\u20135619.   \nZhang, J., Greenewald, K., Squires, C., Srivastava, A., Shanmugam, K., and Uhler, C. (2023). Identifiability guarantees for causal disentanglement from soft interventions. In Advances in Neural Information Processing Systems.   \nZhang, K., Peters, J., Janzing, D., and Sch\u00f6lkopf, B. (2012). Kernel-based conditional independence test and application in causal discovery. arXiv.   \nZheng, Y., Ng, I., and Zhang, K. (2022). On the identifiability of nonlinear ICA: Sparsity and beyond. Advances in Neural Information Processing Systems, 35:16411\u201316422.   \nZhu, L., Klei, D., and Roeder (2019). Semisoft clustering of single-cell data. The Proceedings of the National Academy of Sciences, 116:466\u2013471. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof of Theorems 15 ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 4.4 15   \nA.2 Proof of Theorem 4.5 . 19 ", "page_idx": 13}, {"type": "text", "text": "B Derivation of the evidence lower bound 22 ", "page_idx": 13}, {"type": "text", "text": "C Training Details 23 ", "page_idx": 13}, {"type": "text", "text": "D Hyperparameter selection 24 ", "page_idx": 13}, {"type": "text", "text": "E Datasets and Preprocessing 24 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 SciPlex dataset 24   \nE.2 MultiPlex-Tram dataset . 25   \nE.3 Multiplex-7 dataset . . 25   \nE.4 Multiplex-9 dataset . 26 ", "page_idx": 13}, {"type": "text", "text": "F Experimental Setups and Additional Results 26 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "F.1 Training Details . . 26   \nF.2 Simulation Study . . 26   \nF.3 Additional Clustering Details and Results 27   \nF.4 Statistical Tests and More results . . 28   \nF.5 Conditional Cellular Response Prediction 28   \nF.6 Ablation Study 29   \nF.7 Visualization 29   \nF.8 Pilot Study On The Unseen Drug Responses . . 29 ", "page_idx": 13}, {"type": "text", "text": "A Proof of Theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 4.4. Let us first define $\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t},\\mathbf{x})$ as the vector: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf v(\\mathbf z_{t x},\\mathbf t,\\mathbf x)=\\Big[\\frac{\\partial q_{n_{x}+1}(z_{n_{x}+1},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+1}},\\cdot\\cdot\\cdot\\cdot,\\frac{\\partial q_{n_{x}+n_{t x}}(z_{n_{x}+n_{t x}},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+n_{t x}}},}\\\\ {\\frac{\\partial^{2}q_{n_{x}+1}(z_{n_{x}+1},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+1}^{2}},\\cdot\\cdot\\cdot,\\frac{\\partial^{2}q_{n_{x}+n_{t x}}(z_{n_{x}+n_{t x}},\\mathbf t,\\mathbf x)}{\\partial z_{n_{x}+n_{t x}}^{2}}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $q_{i}$ denotes the logarithm of probability density $p_{\\mathbf{z}_{i}|\\mathbf{t},\\mathbf{x}}$ of component $\\mathbf{z}_{i}$ . If in addition to the assumptions 4.1, 4.2, 4.3, we assume that for the $2n_{t x}$ vectors ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\{\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{i},\\mathbf{x}_{i})+\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{0},\\mathbf{x}_{0})-\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{0},\\mathbf{x}_{i})-\\mathbf{v}(\\mathbf{z}_{t x},\\mathbf{t}_{i},\\mathbf{x}_{0})\\}_{i=1}^{2n_{t x}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "are linearly independent, then $\\mathbf{z}_{t x}$ is component-wise identifiable. ", "page_idx": 14}, {"type": "text", "text": "Proof. This proof proceeds in three main steps: ", "page_idx": 14}, {"type": "text", "text": "1. Derivation of the Fundamental System of Equations: We derive a crucial relationship between the true and estimated latent variables by applying the change of variables formula, and differentiating the equality of observed data distributions.   \n2. Isolation of Interactive Components: We isolate the terms relevant to $\\mathbf{z}_{t x}$ by strategically comparing equations for different pairs of $\\bf(x,t)$ values and subtracting them.   \n3. Establishing Component-wise Identifiability: We analyze the structure of the resulting equations and the Jacobian of the transformation between true and estimated latent variables to establish the component-wise identifiability of $\\mathbf{z}_{t x}$ . ", "page_idx": 14}, {"type": "text", "text": "Step 1 (Derivation of the Fundamental System of Equations) Let us assume there exists another latent representation $\\hat{\\bf z}$ that yields the same data distribution than the ground-truth variables ${\\bf z}$ , for all $\\mathbf{t}\\in\\mathcal{T}$ and $\\mathbf{x}\\in\\mathcal{X}$ . Specifically, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\hat{\\mathbf{y}}|\\mathbf{t},\\mathbf{x}}=p_{\\mathbf{y}|\\mathbf{t},\\mathbf{x}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given our assumption of noiseless observations, it is equivalent to equality in distribution of the mixed variables: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\hat{g}(\\hat{\\mathbf{z}})|\\mathbf{t},\\mathbf{x}}=p_{g(\\mathbf{z})|\\mathbf{t},\\mathbf{x}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which, after a change of variable, is equivalent to: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{g^{-1}\\circ\\hat{g}(\\hat{\\mathbf{z}})|\\mathbf{t},\\mathbf{x}}\\cdot|\\mathbf{J}_{g^{-1}}|=p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}\\cdot|\\mathbf{J}_{g^{-1}}|,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $g^{-1}:\\mathcal{V}\\to\\mathcal{Z}$ denotes the invertible generating function, and $h:=g^{-1}\\circ\\hat{g}$ is the transformation between the true latent variable and estimated one. $|\\mathbf{J}_{g^{-1}}|$ denotes the determinant of the Jacobian matrix of $g^{-1}$ . ", "page_idx": 14}, {"type": "text", "text": "Because $g$ is invertible, $|\\mathbf{J}_{g^{-1}}|\\neq0$ . Using this fact, we obtain the equivalent condition: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{h(\\hat{\\mathbf{z}})|\\mathbf{t},\\mathbf{x}}=p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to the independence relations in the data generating process, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}|\\mathbf{t},\\mathbf{x})=\\prod_{i=1}^{n}p_{z_{i}|\\mathbf{t},\\mathbf{x}}(z_{i}\\mid\\mathbf{t},\\mathbf{x});\\qquad p_{\\hat{\\mathbf{z}}|\\mathbf{t},\\mathbf{x}}(\\hat{\\mathbf{z}}|\\mathbf{t},\\mathbf{x})=\\prod_{i=1}^{n}p_{\\hat{z}_{i}|\\mathbf{t},\\mathbf{x}}(\\hat{z}_{i}\\mid\\mathbf{t},\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rewriting the notation $q_{i}:=\\log p_{z_{i}|\\mathbf{t},\\mathbf{x}}$ and $\\hat{q}_{i}:=\\log p_{\\hat{z}_{i}|\\mathbf{t},\\mathbf{x}}$ yields: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log p_{{\\bf z}|{\\bf t},{\\bf x}}({\\bf z}|{\\bf t},{\\bf x})=\\sum_{i=1}^{n}q_{i}(z_{i},{\\bf t},{\\bf x});\\qquad\\log p_{\\hat{\\bf z}|{\\bf t},{\\bf x}}(\\hat{\\bf z}|{\\bf t},{\\bf x})=\\sum_{i=1}^{n}\\hat{q}_{i}(\\hat{z}_{i},{\\bf t},{\\bf x}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying the change of variables formula to Equation 24 yields ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}=p_{\\hat{\\mathbf{z}}|\\mathbf{t},\\mathbf{x}}\\cdot|\\mathbf{J}_{h^{-1}}|\\iff\\sum_{i=1}^{n}q_{i}(z_{i},\\mathbf{t},\\mathbf{x})+\\log|\\mathbf{J}_{h}|=\\sum_{i=1}^{n}\\hat{q}_{i}(\\hat{z}_{i},\\mathbf{t},\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{J}_{h^{-1}}$ and $\\mathbf{J}_{h}$ are the Jacobian matrix of the transformation associated with $h^{-1}$ and $h$ , respectively. We now adopt the following notations, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{a_{i,\\left(k\\right)}^{\\prime}=\\displaystyle\\frac{\\partial z_{i}}{\\partial\\hat{z}_{k}},\\qquad a_{i,\\left(k,q\\right)}^{\\prime\\prime}=\\displaystyle\\frac{\\partial^{2}z_{i}}{\\partial\\hat{z}_{k}\\partial\\hat{z}_{q}};}}\\\\ {{b_{i}^{'}(z_{i},{\\bf t},{\\bf x})=\\displaystyle\\frac{\\partial q_{i}(z_{i},{\\bf t},{\\bf x})}{\\partial z_{i}},\\qquad b_{i}^{\\prime\\prime}(z_{i},{\\bf t},{\\bf x})=\\displaystyle\\frac{\\partial^{2}q_{i}(z_{i},{\\bf t},{\\bf x})}{(\\partial z_{i})^{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we may differentiate Equation 28 with respect to $\\hat{z}_{k}$ and $\\hat{z}_{q}$ where $k,q\\in\\{1,\\ldots,n\\}$ and $k\\neq q$ . Doing so, we obtain the following fundamental system of equations. For any $\\mathbf{x}\\in\\mathcal{X}$ , $\\mathbf{t}\\in\\mathcal{T}$ , for all $(k,q)^{\\flat}\\in\\{1,\\dots,n\\}^{2}$ such that $k\\neq q$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall\\mathbf{z}\\in\\mathcal{Z},\\sum_{i=1}^{n}\\left[b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t},\\mathbf{x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t},\\mathbf{x})a_{i,(k,q)}^{\\prime\\prime}\\right]+\\frac{\\partial^{2}\\log|\\mathbf{J}_{h}|}{\\partial\\hat{z}_{k}\\hat{z}_{q}}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Step 2 (Isolation of Interactive Components) We may decompose the sum present on the lefthand-side of Equation 30 across the different block of latent variables, obtaining the following equality: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{n}b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t},\\mathbf{x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t},\\mathbf{x})a_{i,(k,q)}^{\\prime\\prime}}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\sum_{i=1}^{n_{x}}b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{x})a_{i,(k,q)}^{\\prime\\prime}}\\\\ {\\displaystyle~~~+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t},\\mathbf{x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t},\\mathbf{x})a_{i,(k,q)}^{\\prime\\prime}}\\\\ {\\displaystyle~~~+\\sum_{i=n_{x}+n_{t\\varepsilon}+1}^{n}b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t})a_{i,(k,q)}^{\\prime\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we may substitute according to Equation 31 in the fundamental system of equation (30), and strategically apply it to several pairs of environments. We will then take the difference of the systems of equations to make disappear the unknown quantity related to the Jacobian of $h$ . ", "page_idx": 15}, {"type": "text", "text": "First, we apply this strategy to the pair $\\{(\\mathbf{x},\\mathbf{t}_{0}),(\\mathbf{x}_{0},\\mathbf{t}_{0})\\}$ , for any treatment $\\mathbf{x}$ (we assume the existence of a reference treatment $\\mathbf{t}_{0}$ and context $\\mathbf{x}_{\\mathrm{0}}$ ). Substituting according to Equation 31 into Equation 30, and applying it to $\\bf(x,t_{0})$ , we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{x}}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{x})a_{i,(k,q)}^{\\prime\\prime})+\\displaystyle\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x})a_{i,(k,q)}^{\\prime\\prime}}\\\\ &{\\displaystyle+\\sum_{i=n_{x}+n_{t x}+1}^{n}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\frac{\\partial^{2}\\log|\\mathbf{J}_{h}|}{\\partial\\hat{z}_{k}\\hat{z}_{q}}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proceeding similarly for $\\left(\\mathbf{x}_{0},\\mathbf{t}_{0}\\right)$ , we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{x}}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{x}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})a_{i,(k)}^{\\prime\\prime})}\\\\ &{+\\displaystyle\\sum_{i=n_{x}+n_{t x}+1}^{n}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\frac{\\partial^{2}\\log|\\mathbf{J}_{h}|}{\\partial\\hat{z}_{k}\\hat{z}_{q}}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then taking the difference between Equation 32 and Equation 33 yields, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{x}}\\left(\\left(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},\\mathbf{x})-b_{i}^{\\prime}(z_{i},\\mathbf{x}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime}\\right)}\\\\ &{\\displaystyle+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}\\left((b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x})-b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime}\\right)=0}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\mathbf{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We apply the same principle to the pair $\\{(\\mathbf{x}_{0},\\mathbf{t}),(\\mathbf{x}_{0},\\mathbf{t}_{0})\\}$ . We therefore get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{n_{x}}(b_{i}^{\\prime\\prime}(z_{i},{\\bf x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},{\\bf x}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}(b_{i}^{\\prime\\prime}(z_{i},{\\bf t},{\\bf x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},{\\bf t},{\\bf x}_{0})a_{i,(q)}^{\\prime\\prime})}\\\\ {\\displaystyle+\\sum_{i=n_{x}+n_{t x}+1}^{n}(b_{i}^{\\prime\\prime}(z_{i},{\\bf t})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},{\\bf t})a_{i,(k,q)}^{\\prime\\prime})+\\frac{\\partial^{2}\\log|{\\bf J}_{h}|}{\\partial\\hat{z}_{k}\\hat{z}_{q}}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{x}}(b_{i}^{\\prime\\prime}(z_{i},{\\bf x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},{\\bf x}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}(b_{i}^{\\prime\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{'}(z_{i},{\\bf t}_{0},{\\bf x}_{0})a_{i,(k)}^{\\prime\\prime})}\\\\ &{+\\displaystyle\\sum_{i=n_{x}+n_{t x}+1}^{n}(b_{i}^{\\prime\\prime}(z_{i},{\\bf t}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{'}(z_{i},{\\bf t}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\frac{\\partial^{2}\\log|{\\bf J}_{h}|}{\\partial\\hat{z}_{k}\\hat{z}_{q}}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking similarly the difference between Equation 35 and Equation 36 yields: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}\\left((b_{i}^{\\prime\\prime}(z_{i},{\\bf t},{\\bf x}_{0})-b_{i}^{\\prime\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},{\\bf t},{\\bf x}_{0})-b_{i}^{\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime})}\\ ~}\\\\ {{\\displaystyle+\\sum_{i=n_{x}+n_{t x}+1}^{n}\\left(\\left(b_{i}^{\\prime\\prime}(z_{i},{\\bf t})-b_{i}^{\\prime\\prime}(z_{i},{\\bf t}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},{\\bf t})-b_{i}^{\\prime}(z_{i},{\\bf t}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime}\\right)=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we consider the pairs $\\{(\\mathbf{x},\\mathbf{t}),(\\mathbf{x}_{0},\\mathbf{t}_{0})\\}$ , for which we obtain the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=1}^{n_{x}}(b_{i}^{\\prime\\prime}(z_{i},{\\bf x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},{\\bf x})a_{i,(k,q)}^{\\prime\\prime})+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}(b_{i}^{\\prime\\prime}(z_{i},{\\bf t},{\\bf x})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},{\\bf t},{\\bf x})a_{i,(k,q)}^{\\prime\\prime}}}\\\\ {{\\displaystyle+\\sum_{i=n_{x}+n_{t x}+1}^{n}(b_{i}^{\\prime\\prime}(z_{i},{\\bf t})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},{\\bf t})a_{i,(k,q)}^{\\prime\\prime})+\\frac{\\partial^{2}\\log|{\\bf J}_{h}|}{\\partial\\hat{z}_{k}\\hat{z}_{q}}=0,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{x}}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{x}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})a_{i,(k)}^{\\prime\\prime})}\\\\ &{+\\displaystyle\\sum_{i=n_{x}+n_{t x}+1}^{n}(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0})\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0})a_{i,(k,q)}^{\\prime\\prime})+\\frac{\\partial^{2}\\log|\\mathbf{J}_{h}|}{\\partial\\hat{z}_{k}\\hat{z}_{q}}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Once again taking the difference between Equation 38 and Equation 39 yields: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{x}}\\left(\\left(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{x}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},\\mathbf{x})-b_{i}^{\\prime}(z_{i},\\mathbf{x}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime}\\right)}\\\\ &{\\displaystyle+\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}\\left((b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t},\\mathbf{x})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},\\mathbf{t},\\mathbf{x})-b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime}\\right)}\\\\ &{\\displaystyle+\\sum_{i=n_{x}+n_{t x}+1}^{n}\\left((b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},\\mathbf{t},\\mathbf{x})-b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime}\\right)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As a final step, we are going to combine Equations 34, 37 and 40 in order to correctly isolate the interaction components. ", "page_idx": 17}, {"type": "text", "text": "We take the difference between Equation 40 and Equation 34: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}\\left[\\left((b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t},\\mathbf{x})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)-\\left((b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)\\right]\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}\\right.}\\\\ &{\\displaystyle+\\left[\\left(b_{i}^{\\prime}(z_{i},\\mathbf{t},\\mathbf{x})-b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)-\\left(b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x})-b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0},\\mathbf{x}_{0})\\right)\\right]a_{i,(k,q)}^{\\prime\\prime})}\\\\ &{\\displaystyle+\\sum_{i=n_{x}+n_{t x}+1}^{n}\\left(\\left(b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t})-b_{i}^{\\prime\\prime}(z_{i},\\mathbf{t}_{0})\\right)\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}+\\left(b_{i}^{\\prime}(z_{i},\\mathbf{t})-b_{i}^{\\prime}(z_{i},\\mathbf{t}_{0})\\right)a_{i,(k,q)}^{\\prime\\prime}\\right)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, we subtract Equation 41 and Equation 37: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=n_{x}+1}^{n_{x}+n_{t x}}\\left[\\left(\\left(b_{i}^{\\prime\\prime}\\left(z_{i},{\\bf t},{\\bf x}\\right)-b_{i}^{\\prime\\prime}\\left(z_{i},{\\bf t}_{0},{\\bf x}_{0}\\right)\\right)-\\left(\\left(b_{i}^{\\prime\\prime}\\left(z_{i},{\\bf t}_{0},{\\bf x}\\right)-b_{i}^{\\prime\\prime}\\left(z_{i},{\\bf t}_{0},{\\bf x}_{0}\\right)\\right)-\\left(b_{i}^{\\prime\\prime}\\left(z_{i},{\\bf t},{\\bf x}_{0}\\right)-b_{i}^{\\prime\\prime}\\left(z_{i},{\\bf t}_{0},{\\bf x}_{0}\\right)\\right)\\right)\\right.}\\\\ &{\\displaystyle\\ +\\left[\\left(b_{i}^{\\prime}(z_{i},{\\bf t},{\\bf x})-b_{i}^{\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\right)-\\left(b_{i}^{\\prime}(z_{i},{\\bf t}_{0},{\\bf x})-b_{i}^{\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\right)-\\left(b_{i}^{\\prime}(z_{i},{\\bf t},{\\bf x}_{0})-b_{i}^{\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\right)\\right]a_{i,(k,q)}^{\\prime\\prime}\\mathrm{~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This last equation may be rearranged as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=n_{x}+1}^{n_{x}+n_{t x}}\\left[b_{i}^{\\prime\\prime}(z_{i},{\\bf t},{\\bf x})-b_{i}^{\\prime\\prime}(z_{i},{\\bf t}_{0},{\\bf x})-b_{i}^{\\prime\\prime}(z_{i},{\\bf t},{\\bf x}_{0})+b_{i}^{\\prime\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\right]\\cdot a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}}\\\\ &{\\displaystyle\\qquad+\\left[b_{i}^{\\prime}(z_{i},{\\bf t},{\\bf x})-b_{i}^{\\prime}(z_{i},{\\bf t}_{0},{\\bf x})-b_{i}^{\\prime}(z_{i},{\\bf t},{\\bf x}_{0})+b_{i}^{\\prime}(z_{i},{\\bf t}_{0},{\\bf x}_{0})\\right]a_{i,(k,q)}^{\\prime\\prime}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Step 3 (Establishing Component-wise Identifiability) Given the assumption of linear independence in the Theorem, the linear system is a $2n_{t x}\\times2n_{t x}$ full-rank system. Therefore, the only solution is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\{a_{i,(k)}^{\\prime}a_{i,(q)}^{\\prime}=0\\right.}&{}&{\\;\\mathrm{~for~}i\\in\\{n_{x}+1,\\ldots,n_{x}+n_{t x}\\}\\mathrm{~and~}k,q\\in\\{1,\\ldots,n\\},k\\neq q.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$h(\\cdot)$ is a smooth function over $\\mathcal{Z}$ and its Jacobian can written as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{J}_{h}=\\left[\\frac{\\mathbf{A}:=\\frac{\\partial\\mathbf{z}_{x}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathrm{~\\left|~\\mathbf{B}:=\\frac{\\partial\\mathbf{z}_{x}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathrm{~\\left|~\\mathbf{C}:=\\frac{\\partial\\mathbf{z}_{x}}{\\partial\\hat{\\mathbf{z}}_{t}}~\\right|~}}\\mathbf{C}:=\\frac{\\partial\\mathbf{z}_{x}}{\\partial\\hat{\\mathbf{z}}_{t}}\\right]}\\\\ {\\left[\\frac{\\mathbf{D}:=\\frac{\\partial\\mathbf{z}_{t x}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathrm{~\\left|~\\mathbf{E}:=\\frac{\\partial\\mathbf{z}_{t x}}{\\partial\\hat{\\mathbf{z}}_{t x}}\\mathrm{~\\left|~\\mathbf{F}:=\\frac{\\partial\\mathbf{z}_{t x}}{\\partial\\hat{\\mathbf{z}}_{t}}~\\right|~}}{\\mathbf{G}:=\\frac{\\partial\\mathbf{z}_{t}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathrm{~\\left|~\\mathbf{H}:=\\frac{\\partial\\mathbf{z}_{t}}{\\partial\\hat{\\mathbf{z}}_{t x}}\\mathrm{~\\left|~\\mathbf{I}:=\\frac{\\partial\\mathbf{z}_{t}}{\\partial\\hat{\\mathbf{z}}_{t}}~\\right|~}}\\right].}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $a_{i,(k)}^{'}a_{i,(q)}^{'}=0$ implies that for each $i\\in\\{n_{x}+1,\\ldots,n_{x}+n_{t x}\\}$ , $a_{i,(k)}^{'}\\neq0$ for at most one element $k\\in[n]$ . As a consequence, there\u2019s only a single non-zero entry in each row indexed by $i\\in\\{n_{x}+1,\\ldots,n_{x}+n_{t x}\\}$ in the Jacobian matrix $\\mathbf{J}_{h}$ . Further strengthening this argument, the invertibility of $h$ requires $\\mathbf{J}_{h}$ to be full-rank, suggesting there\u2019s precisely one non-zero component in each row of matrices $\\mathbf{D},\\mathbf{E}$ , and $\\mathbf{F}$ . ", "page_idx": 17}, {"type": "text", "text": "It means that each of $z_{i}\\in\\mathbf{z}_{t x}$ for $i\\in\\{n_{x}\\!+\\!1,\\ldots,n_{x}\\!+\\!n_{t x}\\}$ is attributed to at most one of the $\\hat{\\bf z}$ . And because the other $\\hat{z}$ that are not in the block $\\hat{\\mathbf{z}}_{t x}$ do not have dependencies with both $t$ and $x$ , it must be that the non-zero coefficient is in the block $\\mathbf{E}$ . Therefore $\\mathbf{D}=\\mathbf{0}$ and $\\mathbf{F}=\\mathbf{0}$ . This indicates the invertibility of $h_{i}$ for every $i$ in the range $\\{n_{x}\\!+\\!1,\\ldots,n_{x}\\!+\\!n_{t x}\\}$ . In conclusion, $\\mathbf{z}_{t x}$ are element-wise identifiable, albeit subject to permutations and component-wise invertible transformations. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.2 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 4.5: We follow Assumptions 4.1, 4.2, 4.3, and the one from Theorem 4.4. We note as $S(Z)$ the set of subsets $S\\subseteq{\\mathcal{Z}}$ of $\\mathcal{Z}$ that satisfy the following two conditions: ", "page_idx": 18}, {"type": "text", "text": "(i) $S$ has nonzero probability measure, i.e. $\\mathbb{P}(\\mathbf{z}\\in S\\mid\\mathbf{t}=\\mathbf{t^{'}},\\mathbf{x}=\\mathbf{x^{\\prime}})>0$ for any $\\mathbf{t^{'}}\\in\\mathcal{T}$ and $\\mathbf{x^{\\prime}}\\in\\mathcal{X}$ .   \n(ii) $S$ cannot be expressed as $A_{\\mathbf{z}_{x}}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ for any $A_{\\mathbf{z}_{x}}\\subset\\mathcal{Z}_{x}$ or as $\\mathcal{Z}_{x}\\times\\mathcal{Z}_{t x}\\times A_{\\mathbf{z}_{t}}$ for any $A_{\\mathbf{z}_{t}}\\subset\\mathcal{Z}_{t}$ . ", "page_idx": 18}, {"type": "text", "text": "We have the following identifiability result. If for all $S\\in S(\\mathcal{Z})$ , there exists $(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in\\mathcal{T}\\times\\mathcal{T}$ and $\\mathbf{x}\\in\\mathcal{X}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t}_{1},\\mathbf{x})d\\mathbf{z}\\neq\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t}_{2},\\mathbf{x})d\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and there also exists $(\\mathbf{x}_{1},\\mathbf{x}_{2})\\in\\mathcal{X}\\times\\mathcal{X}$ and $\\mathbf{t}\\in\\mathcal{T}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t},\\mathbf{x}_{1})d\\mathbf{z}\\neq\\int_{\\mathbf{z}\\in S}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t},\\mathbf{x}_{2})d\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then $\\mathbf{z}_{t}$ and $\\mathbf{z}_{x}$ are block-wise identifiable. ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof of the block-wise identifiability of $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t}$ are similar, so here we focus the proof about the identifiability of $\\mathbf{z}_{x}$ . Our proof draws parallels to the approach in Kong et al. (2022), but our context specifically pertains to the multi-conditions distributions scenarios. Throughout, we use the notations $\\mathbf{\\bar{z}}_{t}^{+}=[\\mathbf{z}_{t x},\\mathbf{z}_{t}]$ , $\\hat{\\mathbf{z}}_{t}^{+}=\\left[\\hat{\\mathbf{z}}_{t x},\\hat{\\mathbf{z}}_{t}\\right]$ for simplification. This proof is comprised of four major steps. ", "page_idx": 18}, {"type": "text", "text": "1. Integral characterization of domain invariance. We first leverage properties of the generating process and the marginal distribution matching condition to provide a characterization of the invariance of a block of latent variables by a mixing function using an integral condition.   \n2. Topological characterization of invariance. We derive equivalence statements for domain invariance of functions.   \n3. Proof of invariance by contradiction. We prove the invariance statement from Step 2 by contradiction. Specifically, we show that if $\\hat{\\mathbf{z}}_{x}$ depended on $\\mathbf{z}_{t}^{+}$ , the invariance derived in Step 1 would break.   \n4. Block-wise identifiability of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{x}$ . We use the conclusion in Step 3, the regularity properties of $h$ , and the conclusion in Theorem 4.4 to show the identifiability result. ", "page_idx": 18}, {"type": "text", "text": "Step 1 (Integral characterization of domain invariance). As a reminder to the reader, $g:\\mathcal{Z}\\to\\mathcal{Y}$ denotes the ground-truth mixing function, and $\\hat{g}\\,:\\,\\mathcal{Z}\\,\\rightarrow\\,\\mathcal{Y}$ denotes the learned mixing function. We assume that both $g$ and $\\hat{g}$ are invertible, so that their reciprocal functions are well defined. In particular, we denote by $\\hat{g}_{1:n_{x}}^{-1}:\\mathcal{V}\\to\\mathcal{Z}_{x}$ the estimated transformation from the observation to the covariate-specific block of the latent space $\\mathcal{Z}$ . We seek to find an integral characterization of the invariance of the learned function $\\hat{g}$ on the domain ${\\mathcal{Z}}_{x}$ . ", "page_idx": 18}, {"type": "text", "text": "We seek to derive a condition for which the distribution of latent variables on ${\\mathcal{Z}}_{x}$ will be unchanged when the treatment $\\mathbf{t}$ is changed (it will however depend on $\\mathbf{x}$ ). Let $S\\subset{\\mathcal{Z}}_{x}$ designates a subset of ${\\mathcal{Z}}_{x}$ and $\\mathbf{x}\\in\\mathcal{X}$ be any context. We seek to characterize the following condition: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in T^{2},\\mathbb{P}\\left[\\left\\{\\hat{g}_{1:n_{x}}^{-1}(\\hat{\\mathbf{y}})\\in S\\right\\}\\mid\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{1}\\}\\right]=\\mathbb{P}\\left[\\left\\{\\hat{g}_{1:n_{x}}^{-1}(\\hat{\\mathbf{y}})\\in S\\right\\}\\mid\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{2}\\}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This condition can be written equivalently using the pre-image set of $S$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\prime(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in{\\mathcal{T}}^{2},\\mathbb{P}\\left[\\left\\{\\hat{\\mathbf{y}}\\in\\left(\\hat{g}_{1:n_{x}}^{-1}\\right)^{-1}(S)\\right\\}\\mid\\left\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{1}\\right\\}\\right]=\\mathbb{P}\\left[\\left\\{\\hat{\\mathbf{y}}\\in\\left(\\hat{g}_{1:n_{x}}^{-1}\\right)^{-1}(S)\\right\\}\\mid\\left\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{2}\\right\\}\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\left(\\hat{g}_{1:n_{x}}^{-1}\\right)^{-1}(S)\\subseteq\\mathcal{Y}$ is the set of estimated observations $\\hat{\\mathbf{y}}$ originating from covariate-specific variables $\\hat{\\mathbf{z}}_{x}$ in $S$ .   \nBecause of the equality of the observed data $\\mathbf{y}$ and the generated data distribution from the estimated model $\\hat{\\mathbf{y}}$ , the relation in Equation 47 also holds for the random variable y   \n\u2200 $/(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in T^{2},\\mathbb{P}\\left[\\left\\{\\mathbf{y}\\in\\left(\\hat{g}_{1:n_{x}}^{-1}\\right)^{-1}(S)\\right\\}\\mid\\left\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{1}\\right\\}\\right]=\\mathbb{P}\\left[\\left\\{\\mathbf{y}\\in\\left(\\hat{g}_{1:n_{x}}^{-1}\\right)^{-1}(S)\\right\\}\\mid\\left\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{2}\\right\\}\\right].$ ", "page_idx": 19}, {"type": "text", "text": "It follows, by applying the image of the function $\\hat{g}_{1:n_{x}}^{-1}$ , that: ", "page_idx": 19}, {"type": "text", "text": "$\\forall(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in T^{2},\\mathbb{P}\\left[\\left\\{\\hat{g}_{1:n_{x}}^{-1}(\\mathbf{y})\\in S\\right\\}\\mid\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{1}\\}\\right]=\\mathbb{P}\\left[\\left\\{\\hat{g}_{1:n_{x}}^{-1}(\\mathbf{y})\\in S\\right\\}\\mid\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{2}\\}\\right].$ ", "page_idx": 19}, {"type": "text", "text": "Since $g$ and $\\hat{g}$ are smooth and injective, we may define the function $\\bar{h}=\\hat{g}^{-1}\\circ g:\\mathcal{Z}\\to\\mathcal{Z}$ . We note that by definition $\\bar{h}=h^{-1}$ where $h$ is introduced in the proof of Theorem 4.4. We now remind the reader that $\\mathbf{y}=g(\\mathbf{z})$ . Therefore, using the notation $\\bar{h}_{x}:=\\bar{h}_{1:n_{x}}:\\mathcal{Z}\\to\\mathcal{Z}_{x}$ , we have the equivalent condition ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in T^{2},\\mathbb{P}\\left[\\left\\{{\\bar{h}}_{x}(\\mathbf{z})\\in S\\right\\}\\mid\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{1}\\}\\right]=\\mathbb{P}\\left[\\left\\{{\\bar{h}}_{x}(\\mathbf{z})\\in S\\right\\}\\mid\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{2}\\}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, using the pre-image formulation again, we may write it as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in T^{2},\\mathbb{P}\\left[\\left\\{\\mathbf{z}\\in\\bar{h}_{x}^{-1}\\left(S\\right)\\right\\}\\mid\\left\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{1}\\right\\}\\right]=\\mathbb{P}\\left[\\left\\{\\mathbf{z}\\in\\bar{h}_{x}^{-1}\\left(S\\right)\\right\\}\\mid\\left\\{\\mathbf{x},\\mathbf{t}=\\mathbf{t}_{2}\\right\\}\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and, using an integral notation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in T^{2},\\int_{\\mathbf{z}\\in\\bar{h}_{x}^{-1}(S)}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}\\mid\\mathbf{x},\\mathbf{t}_{1}\\right)d\\mathbf{z}=\\int_{\\mathbf{z}\\in\\bar{h}_{x}^{-1}(S)}p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}\\mid\\mathbf{x},\\mathbf{t}_{2}\\right)d\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\bar{h}_{x}^{-1}\\left(S\\right)=\\left\\{\\mathbf{z}\\in\\mathcal{Z}:\\bar{h}_{x}(\\mathbf{z})\\in S\\right\\}$ is the pre-image of $S$ . ", "page_idx": 19}, {"type": "text", "text": "By exploiting the factorization of the likelihood, we obtain our final condition, equivalent to the one in Equation 47 for any $S\\subseteq{\\mathcal{Z}}_{x}$ and $\\mathbf{x}\\in\\mathcal{X}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\left[\\mathbf{z}_{x},\\mathbf{z}_{t}^{+}\\right]\\in\\bar{h}_{x}^{-1}\\left(S\\right)}p_{\\mathbf{z}_{x}|\\mathbf{x}}\\left(\\mathbf{z}_{\\mathbf{x}}\\mid\\mathbf{x}\\right)\\left(p_{\\mathbf{z}_{t}^{+}|\\mathbf{x},\\mathbf{t}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{1}\\right)-p_{\\mathbf{z}_{t}^{+}|\\mathbf{x},\\mathbf{t}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{2}\\right)\\right)d\\mathbf{z}_{x}d\\mathbf{z}_{t}^{+}=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the condition must hold for all $(\\mathbf{t}_{1},\\mathbf{t}_{2})\\in\\mathcal{T}^{2}$ . ", "page_idx": 19}, {"type": "text", "text": "Step 2 (Topological characterization of invariance). To demonstrate the block-identifiability of $\\mathbf{z}_{x}$ , our objective is to substantiate that $\\bar{h}_{x}\\left(\\left[\\mathbf{z}_{x},\\mathbf{z}_{t x},\\mathbf{z}_{t}\\right]\\right)=\\bar{h}_{x}\\left(\\left[\\mathbf{z}_{x},\\mathbf{z}_{t}^{+}\\right]\\right)$ is functionally independent of $\\mathbf{z}_{t}^{+}$ . To achieve this, we initially formulate a set of equivalent statement: ", "page_idx": 19}, {"type": "text", "text": "1. Statement 1. $\\bar{h}_{x}\\left(\\left[\\mathbf{z}_{x}^{\\top},\\mathbf{z}_{t}^{+\\top}\\right]^{\\top}\\right)$ does not depend on $\\mathbf{z}_{t}^{+}$ ", "page_idx": 19}, {"type": "text", "text": "2. Statement 2. $\\forall\\mathbf{z}_{x}\\in\\mathcal{Z}_{x},\\exists B_{\\mathbf{z}_{x}}\\subseteq\\mathcal{Z}_{x}\\setminus\\emptyset:\\bar{h}_{x}^{-1}\\left(\\mathbf{z}_{x}\\right)=B_{\\mathbf{z}_{x}}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}.$ ", "page_idx": 19}, {"type": "equation", "text": "$\\forall\\mathbf{z}_{x}\\in\\mathcal{Z}_{x},\\forall r\\in\\mathbb{R}^{+},\\exists B_{\\mathbf{z}_{x}}^{+}\\subseteq\\mathcal{Z}_{x}\\setminus\\emptyset:\\bar{h}_{x}^{-1}\\left(\\mathcal{B}_{r}\\left(\\mathbf{z}_{x}\\right)\\right)=B_{\\mathbf{z}_{x}}^{+}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ ", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $B_{r}\\left(\\mathbf{z}_{x}\\right)$ is defined as the ball centered around $\\mathbf{z}_{x}$ with radius $r$ $\\begin{array}{r l r}{\\mathbf{\\ddot{\\omega}}:}&{{}\\,\\mathcal{B}_{r}\\left(\\mathbf{z}_{x}\\right)}&{=}\\end{array}$ $\\left\\{\\mathbf{z}_{x}^{\\prime}\\in\\mathcal{Z}_{x}:\\left\\|\\mathbf{z}_{x}^{\\prime}-\\mathbf{z}_{x}\\right\\|^{2}<r\\right\\}$ . ", "page_idx": 19}, {"type": "text", "text": "We note that Statement 2 is a mathematical formulation of Statement 1, and that Statement 3 is a generalization of Statement 2 from singletons $\\{{\\bf z}_{x}\\}$ in Statement 2 to open, non-empty balls $B_{r}\\left(\\mathbf{z}_{x}\\right)$ . We proceed to demonstrating equivalence between those statements. ", "page_idx": 19}, {"type": "text", "text": "Statement $^{2\\,\\Rightarrow}$ Statement 3. Let $\\mathbf{z}_{x}\\in\\mathcal{Z}_{x}$ and $r\\in\\mathbb{R}^{+}$ . By definition of the pre-image of a set, we have that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{h}_{x}^{-1}\\left(\\mathcal{B}_{r}\\left(\\mathbf{z}_{x}\\right)\\right)=\\cup_{\\mathbf{z}_{x}^{\\prime}\\in\\mathcal{B}_{r}\\left(\\mathbf{z}_{x}\\right)}h_{x}^{-1}\\left(\\mathbf{z}_{x}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Because we assume Statement 2, we have that for all $\\mathbf{z}_{x}^{\\prime}\\in B_{r}\\left(\\mathbf{z}_{x}\\right)$ , there exists a set $B_{\\mathbf{z}_{x}^{\\prime}}$ such that $h_{x}^{-1}\\left(\\mathbf{z}_{x}^{\\prime}\\right)=B_{\\mathbf{z}_{x}^{\\prime}}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ . Therefore, Statement 3 stands for $B_{\\mathbf{z}_{x}}^{+}=\\cup_{\\mathbf{z}_{x}^{\\prime}}B_{\\mathbf{z}_{x}^{\\prime}}$ . ", "page_idx": 19}, {"type": "text", "text": "Statement $z\\Leftarrow1$ Statement 3. We proceed by contradiction. Suppose that Statement 2 is false, then for a certain $\\overline{{\\mathbf{z}}}_{x}^{*}\\in\\mathcal{Z}_{x}$ , it is possible to construct a point $\\bar{\\mathbf{z}}^{B}=[\\bar{\\mathbf{z}}_{x}^{\\bar{B}},\\bar{\\mathbf{z}}_{t x}^{B},\\bar{\\mathbf{z}}_{t}^{B}]\\in\\mathcal{Z}$ such that $\\bar{\\mathbf{z}}_{x}^{B}$ is in the pre-image of $\\{\\bar{\\mathbf{z}}_{x}^{*}\\}$ by $\\bar{h}_{x}^{-1}$ but $\\bar{h}_{x}\\left(\\bar{\\mathbf{z}}^{B}\\right)\\neq\\bar{\\mathbf{z}}_{x}$ . Indeed, this directly means that changing the other components of $\\mathcal{Z}$ at the input of $\\bar{h}$ can alter the component of ${\\mathcal{Z}}_{x}$ at its output. By continuity of $\\bar{h}_{x}$ , there exists $\\hat{r}>0$ such that $\\bar{h}_{x}\\left(\\bar{\\mathbf{z}}^{B}\\right)\\notin\\mathcal{B}_{\\hat{r}}\\left(\\bar{\\mathbf{z}}_{x}\\right)$ . For such $\\hat{r}$ , we have that $\\bar{\\mathbf{z}}^{B}\\notin\\bar{h}_{x}^{-1}\\left(B_{\\hat{r}}\\left(\\bar{\\bar{\\mathbf{z}}}_{x}\\right)\\right)$ . Additionally, the application of Statement 3 suggests that there exists a non-trivial subset $B_{\\mathbf{z}_{x}}^{+}$ such that $h_{x}^{-1}\\left(\\mathcal{B}_{\\hat{r}}\\left(\\bar{\\mathbf{z}}_{x}\\right)\\right)=B_{\\bar{\\mathbf{z}_{x}}}^{+}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ . By definition of $\\bar{\\mathbf{z}}^{B}$ , it is clear that $\\bar{\\mathbf{z}}_{1:n_{x}}^{B}\\in B_{\\bar{\\mathbf{z}_{x}}}^{+}$ . The fact that $\\bar{\\mathbf{z}}^{B}\\notin h_{x}^{-1}\\left(B_{\\hat{r}}\\left(\\bar{\\mathbf{z}}_{x}\\right)\\right)$ contradicts Statement 3. Therefore, Statement 2 is true under the premise of Statement 3. ", "page_idx": 20}, {"type": "text", "text": "Step 3 (Proof of invariance by contradiction). We first show that the pre-image of any open balls of $\\mathcal{Z}_{x}$ are non-empty and open sets. For $\\mathbf{z}_{x}\\in\\mathcal{Z}_{x}$ and $r\\in\\mathbb{R}^{+}$ , we note that because $B_{r}\\left(\\mathbf{z}_{x}\\right)$ is open and ${\\bar{h}}_{x}$ is continuous, the pre-image $\\bar{h}_{x}^{-1}\\left({B_{r}\\left({\\mathbf{z}_{x}}\\right)}\\right)$ is open. In addition, because $h$ is continuous and we have equality of the generated data distributions: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall\\mathbf{t}\\in\\mathcal{T},\\forall\\mathbf{x}\\in\\mathcal{X},\\mathbb{P}\\left[\\left\\{\\mathbf{y}\\in S\\right\\}\\mid\\left\\{\\mathbf{t},\\mathbf{x}\\right\\}\\right]=\\mathbb{P}\\left[\\left\\{\\hat{\\mathbf{y}}\\in S\\right\\}\\mid\\left\\{\\mathbf{t},\\mathbf{x}\\right\\}\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we have that $h$ is a bijection (Klindt et al., 2021), which ensures that $\\bar{h}_{x}^{-1}\\left({B_{r}\\left({\\bf z}_{x}\\right)}\\right)$ is non-empty.   \nHence, $\\bar{h}_{x}^{-1}\\left({B_{r}\\left({\\bf z}_{x}\\right)}\\right)$ is both non-empty and open. ", "page_idx": 20}, {"type": "text", "text": "We now assume, by contradiction, that $\\mathbf{z}_{x}$ is not block-identifiable. Therefore, $\\bar{h}_{x}$ is not invariant with respect to $\\mathbf{z}_{t}^{+}$ . Additionally, because of Step 2, we have the existence of a ball $S^{*}:=B_{r^{*}}\\left(\\mathbf{z}_{x}^{*}\\right)$ centered on the point $\\mathbf{z}_{x}^{\\ast}\\in\\mathcal{Z}_{x}$ and of radius $r^{*}\\in\\mathbb{R}^{+}$ such that $\\bar{h}_{x}^{-1}\\left(S^{*}\\right)$ cannot be written of the form $A\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ for any non-trivial $A\\subset\\mathcal{Z}_{x}$ . ", "page_idx": 20}, {"type": "text", "text": "We may therefore define the set $B_{\\mathbf{z}}^{*}\\;:=\\;\\left\\{\\mathbf{z}\\in\\bar{h}_{x}^{-1}\\left(S^{*}\\right)\\mid\\left\\{\\mathbf{z}_{1:n_{x}}\\right\\}\\times\\mathcal{Z}_{t}\\times\\mathcal{Z}_{t x}\\not\\subseteq\\bar{h}_{x}^{-1}\\left(S^{*}\\right)\\right\\}$ . Intuitively, $B_{\\mathbf{z}}^{*}$ contains the partition of the pre-image $\\bar{h}_{x}^{-1}\\left(S^{*}\\right)$ that the t part $\\mathbf{z}_{t}^{+}$ cannot take on any value in $\\mathcal{Z}_{t x}\\,\\times\\,\\mathcal{Z}_{t}$ . It is therefore non-empty by hypothesis. To show contradiction with Equation 53, we evaluate it on the set $S^{*}$ and split the integral on two domains of the partition $\\bar{h}_{x}^{\\frac{\\cdot}{-1}}\\left(S^{*}\\right)=(\\bar{h}_{x}^{-1}\\left(S^{*}\\right)\\backslash B_{\\mathbf{z}}^{*})\\cup B_{\\mathbf{z}}^{*}$ . ", "page_idx": 20}, {"type": "text", "text": "We define the following integrals: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T=\\int_{\\left[\\mathbf{z}_{x},\\mathbf{z}_{t}^{+}\\right]\\in\\bar{h}_{x}^{-1}(S^{\\star})}p_{\\mathbf{z}_{x}|\\mathbf{x}}\\left(\\mathbf{z}_{\\mathbf{x}}\\mid\\mathbf{x}\\right)\\left(p_{\\mathbf{z}_{t}^{+}|\\mathbf{x},\\mathbf{t}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{1}\\right)-p_{\\mathbf{z}_{t}^{+}|\\mathbf{x},\\mathbf{t}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{2}\\right)\\right)d\\mathbf{z}_{x}d\\mathbf{z}_{t}^{+}\\,\\,\\,\\,(\\mathbf{z}_{x}^{+}\\mid\\mathbf{x},\\mathbf{t}_{1})\\,\\,,}\\\\ &{T_{1}=\\int_{\\left[\\mathbf{z}_{x},\\mathbf{z}_{t}^{+}\\right]\\in\\bar{h}_{x}^{-1}(S)\\backslash B_{\\mathbf{z}}^{*}}p_{\\mathbf{z}_{x}|\\mathbf{x}}\\left(\\mathbf{z}_{\\mathbf{x}}\\mid\\mathbf{x}\\right)\\left(p_{\\mathbf{z}_{t}^{+}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{1}\\right)-p_{\\mathbf{z}_{t}^{+}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{2}\\right)\\right)d\\mathbf{z}_{x}d\\mathbf{z}_{t}^{+}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{2}=\\int_{\\left[\\mathbf{z}_{x},\\mathbf{z}_{t}^{+}\\right]\\in B_{\\mathbf{z}}^{\\ast}}p_{\\mathbf{z}_{x}|\\mathbf{x}}\\left(\\mathbf{z}_{\\mathbf{x}}\\mid\\mathbf{x}\\right)\\left(p_{\\mathbf{z}_{t}^{+}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{1}\\right)-p_{\\mathbf{z}_{t}^{+}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{2}\\right)\\right)d\\mathbf{z}_{x}d\\mathbf{z}_{t}^{+},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have the expected relation $T=T_{1}+T_{2}$ . ", "page_idx": 20}, {"type": "text", "text": "We first look at the value of $T_{1}$ . In the case where the set $\\bar{h}_{x}^{-1}\\left(S^{*}\\right)\\backslash B_{\\mathbf{z}}^{*}$ is empty, then $T_{1}$ trivially evaluates to 0. Otherwise, there exists a non-empty subset $C_{\\mathbf{z}_{x}}^{*}$ of ${\\mathcal{Z}}_{x}$ such that $\\bar{h}_{x}^{-1}\\left(S^{*}\\right)\\backslash B_{\\mathbf{z}}^{*}=$ $C_{\\mathbf{z}_{x}}^{*}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ . With this expression, it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{1}=\\int_{\\left[\\mathbf{z}_{x},\\mathbf{z}_{t}^{+}\\right]\\in C_{\\mathbf{z}_{x}}^{\\star}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}}p_{\\mathbf{z}_{x}|\\mathbf{x}}\\left(\\mathbf{z}_{\\mathbf{x}}\\mid\\mathbf{x}\\right)\\left(p_{\\mathbf{z}_{t}^{+}|\\mathbf{x},\\mathbf{t}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{1}\\right)-p_{\\mathbf{z}_{t}^{+}|\\mathbf{x},\\mathbf{t}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x},\\mathbf{t}_{2}\\right)\\right)d\\mathbf{z}_{x}d\\mathbf{z}_{t}^{+}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Because of the separability of the domains, we may apply Fubini\u2019s theorem: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Gamma_{1}^{\\bf\\Delta}=\\displaystyle\\int_{{\\bf z}_{x}\\in C_{{\\bf z}_{x}}^{\\bf\\Delta}}p_{{\\bf z}_{x}|{\\bf x}}\\left({\\bf z}_{x}\\mid{\\bf x}\\right)d{\\bf z}_{x}\\displaystyle\\int_{{\\bf z}_{t}^{+}\\in{\\cal Z}_{t x}\\times{\\cal Z}_{t}}\\left(p_{{\\bf z}_{t}^{+}|{\\bf x},{\\bf t}}\\left({\\bf z}_{t}^{+}\\mid{\\bf x},{\\bf t}_{1}\\right)-p_{{\\bf z}_{t}^{+}|{\\bf x},{\\bf t}}\\left({\\bf z}_{t}^{+}\\mid{\\bf x},{\\bf t}_{2}\\right)\\right)d{\\bf z}_{x}d{\\bf z}_{t}^{+}}\\\\ {{\\mathrm{\\boldmath~\\scriptstyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(60)}}}\\\\ {{{\\mathrm{\\boldmath~\\textstyle\\stackrel{r}{1}=\\displaystyle\\int_{{\\bf z}_{x}\\in C_{{\\bf z}_{x}}^{\\bf\\Delta}}p_{{\\bf z}_{x}|{\\bf x}}\\left({\\bf z}_{x|{\\bf x}}\\right)\\left(1-1\\right)d{\\bf z}_{x}=0}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, in both cases $T_{1}$ evaluates to 0 for $S^{*}$ . ", "page_idx": 20}, {"type": "text", "text": "Now, we address $T_{2}$ . Towards this goal, we prove that $B_{\\mathbf{z}}^{*}$ satisfies the condition for application of the assumption of the theorem. First, we must show that $B_{\\mathbf{z}}^{*}$ has non-zero probability measure for all values of $\\mathbf{t}$ and $\\mathbf{x}$ . For this, it is enough to show that $B_{\\mathbf{z}}^{*}$ contains an open set, given that we assume that $p_{\\mathbf{z}|\\mathbf{t},\\mathbf{x}}(\\mathbf{z}\\mid\\mathbf{t},\\mathbf{x})>0$ over $(\\mathbf{z},\\mathbf{t},\\mathbf{x})\\in\\mathcal{Z}\\times T\\times\\mathcal{X}$ . Let us take one element $\\mathbf{z}_{B}\\in B_{\\mathbf{z}}^{*}$ , which is possible because we proved $B_{\\mathbf{z}}^{*}$ is non-empty. As discussed above, $\\bar{h}_{x}^{-1}\\left(S^{*}\\right)$ is open and non-empty, and by continuity of $\\bar{h}_{x}$ , there exists $r_{0}\\in\\mathbb{R}^{+}$ such that $\\mathcal{B}_{r_{0}}\\left(\\mathbf{z}_{B}\\right)\\subseteq B_{\\mathbf{z}}^{*}$ . Therefore, $B_{\\mathbf{z}}^{*}$ contains an open set and has non-zero probability. Second, it is by definition that $B_{\\mathbf{z}}^{*}$ cannot be expressed as $A_{\\mathbf{z}_{x}}\\times\\mathcal{Z}_{t x}\\times\\mathcal{Z}_{t}$ for any $A_{\\mathbf{z}_{x}}\\subset\\mathcal{Z}_{x}$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, condition (ii) from the theorem indicates that there exists $\\mathbf{t}_{1}^{*},\\mathbf{t}_{2}^{*},\\mathbf{x}^{*}$ , such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{2}=\\int_{\\left[\\mathbf{z};\\mathbf{z}_{t}^{+}\\right]\\in B_{z}^{+}}p_{\\mathbf{z}_{x}|\\mathbf{x}}\\left(\\mathbf{z}_{\\mathbf{x}}\\mid\\mathbf{x}^{*}\\right)\\left(p_{\\mathbf{z}_{t}^{+}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x}^{*},\\mathbf{t}_{1}^{*}\\right)-p_{\\mathbf{z}_{t}^{+}|\\mathbf{t},\\mathbf{x}}\\left(\\mathbf{z}_{t}^{+}\\mid\\mathbf{x}^{*},\\mathbf{t}_{2}^{*}\\right)\\right)d\\mathbf{z}_{x}d\\mathbf{z}_{t}^{+}\\neq0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, for such $S^{*}$ , we would have $T_{1}+T_{2}\\ne0$ which leads to contradiction with Equation 53. We have proved by contradiction that Statement 1 from Step 2 holds, that is, $\\bar{h}_{x}$ does not depend on the treatment variable and interaction variable $\\mathbf{z}_{t},\\,\\mathbf{z}_{t x}$ . ", "page_idx": 21}, {"type": "text", "text": "Step 4 (Block-wise identifiability of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{x}$ ). With the knowledge that $h_{x}$ does not depend on $\\mathbf{z}_{t}^{+}$ , we now show that there exists an invertible mapping between the true content variable $\\mathbf{z}_{x}$ and the estimated version $\\hat{\\mathbf{z}}_{x}$ . ", "page_idx": 21}, {"type": "text", "text": "As $\\bar{h}$ is smooth over $\\mathcal{Z}$ , its Jacobian can written as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{J}_{h}=\\left[\\frac{\\mathbf{A}:=\\frac{\\partial\\mathbf{z}_{x}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathbf{\\nabla}\\left|\\textbf{B}:=\\frac{\\partial\\mathbf{z}_{x}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathbf{\\nabla}\\right|\\textbf{C}:=\\frac{\\partial\\mathbf{z}_{x}}{\\partial\\hat{\\mathbf{z}}_{t}}}{\\left|\\frac{\\mathbf{D}:=\\frac{\\partial\\mathbf{z}_{t x}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathbf{\\nabla}\\left|\\textbf{E}:=\\frac{\\partial\\mathbf{z}_{t x}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathbf{\\nabla}\\right|\\textbf{F}:=\\frac{\\partial\\mathbf{z}_{t x}}{\\partial\\hat{\\mathbf{z}}_{t}}}\\right.}\\\\ {\\mathbf{G}:=\\frac{\\partial\\mathbf{z}_{t}}{\\partial\\hat{\\mathbf{z}}_{x}}\\mathbf{\\nabla}\\left|\\textbf{H}:=\\frac{\\partial\\mathbf{z}_{t}}{\\partial\\hat{\\mathbf{z}}_{t x}}\\mathbf{\\nabla}\\right|\\textbf{I}:=\\frac{\\partial\\mathbf{z}_{t}}{\\partial\\hat{\\mathbf{z}}_{t}}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use notation $\\hat{\\mathbf{z}}_{x}=\\bar{h}(\\mathbf{z})_{1:n_{x}}$ and $\\hat{\\mathbf{z}}_{t x}=\\bar{h}(\\mathbf{z})_{n_{x}+1:n_{x}+n_{t x}},\\hat{\\mathbf{z}}_{t}=\\bar{h}(\\mathbf{z})_{n_{x}+n_{t x}+1:n}.$ ", "page_idx": 21}, {"type": "text", "text": "First, we notice that under the condition of Theorem 4.4, there is an invertible mapping between $\\mathbf{z}_{t x}$ and $\\hat{\\mathbf{z}}_{t x}$ . Therefore, it must be that $\\mathbf{D}=\\mathbf{F}=\\mathbf{0}$ , and that $\\mathbf{E}$ is non-singular. Additionally, we have just shown that $\\hat{\\mathbf{z}}_{x}$ does not depend on the treatment-related variables $\\overline{{\\mathbf{z}_{t}^{+}}}$ . Therefore, it follows $\\mathbf{B}=\\mathbf{C}=\\mathbf{0}$ . On the other hand, as $h$ is invertible over $\\mathcal{Z},\\mathbf{J}_{\\bar{h}}$ is non-singular. Therefore, A must be non-singular due to $\\mathbf{B}=\\mathbf{C}=\\mathbf{0}$ . Relying on analogous assumptions to prove the invariance of $\\hat{\\mathbf{z}}_{t}$ with respect to ${\\bf z}_{x}^{+}$ , it follows that $\\mathbf{G}=\\mathbf{H}=\\mathbf{0}$ , and that I must be non-singular. ", "page_idx": 21}, {"type": "text", "text": "We note that $\\mathbf{A}$ is the Jacobian of the function $\\bar{h}_{x}^{\\prime}\\left(\\mathbf{z}_{x}\\right):=\\bar{h}_{x}(\\mathbf{z}):\\mathcal{Z}_{x}\\rightarrow\\mathcal{Z}_{x}$ , which takes only the covariates part $\\mathbf{z}_{x}$ of the input $\\mathbf{z}$ into $\\bar{h}_{x}$ . Together with the invertibility of $h$ , we can conclude that $\\bar{h}_{x}^{\\prime}$ is invertible. Therefore, there exists an invertible function $\\bar{h}_{x}^{\\prime}$ between the estimated and the true $\\begin{array}{r}{\\tilde{\\mathbf{z}}_{x}^{'}=\\bar{h}_{x}^{\\prime}\\left(\\mathbf{z}_{x}\\right)}\\end{array}$ , which concludes the proof that $\\mathbf{z}_{x}$ is block-identifiable. Similarly, we are able to conclude $\\mathbf{z}_{t}$ is block-identifiable. ", "page_idx": 21}, {"type": "text", "text": "B Derivation of the evidence lower bound ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We now introduce the classical derivations of the celebrated evidence lower bound for our generative model. The evidence is the logarithm of the marginal data probability, and we calculate it by weighting it against the variational distribution: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log p(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{t})=\\log\\mathbb{E}_{q_{\\phi}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x})}\\left({\\frac{p_{\\theta}(\\mathbf{y},\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{t},\\mathbf{x})}{q_{\\phi}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x})}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We apply Jensen\u2019s inequality using the fact that the logarithm is a concave function: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{t})\\ge\\mathbb{E}_{q_{\\phi}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x})}\\log\\left(\\frac{p_{\\theta}\\left(\\mathbf{y},\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{t},\\mathbf{x}\\right)}{q_{\\phi}\\left(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x}\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/ae64f196400c0e1a7148dcd3d214abfb064d243cccffbe04c5b9df037857b67e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Then, we use the factorization of our generative model: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{t})\\ge\\!\\mathcal{L}_{\\mathrm{ELBO}}:=\\mathbb{E}_{q_{\\phi}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x})}\\log\\frac{p_{\\theta}(\\mathbf{y}\\mid\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x})p_{\\theta}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{t},\\mathbf{x})}{q_{\\phi}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Naming the right hand size of Equation 66 as $\\mathcal{L}_{\\mathrm{ELBO}}$ , we notice that $\\mathcal{L}_{\\mathrm{ELBO}}$ can be written as the following difference: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathtt{E L B O}}=\\mathbb{E}_{\\bar{q}_{\\phi}}\\log p_{\\theta}(\\mathbf{y}\\mid\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x})-\\mathbb{E}_{\\bar{q}_{\\phi}}\\log\\frac{q_{\\phi}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x})}{p_{\\theta}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{t},\\mathbf{x})},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first term is the reconstruction loss, and the second term is the Kullback-Leibler divergence between the approximate posterior $\\bar{q}_{\\phi}=q_{\\phi}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x})$ and the prior $p_{\\theta}(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{t},\\mathbf{x})$ . Further decomposing this term using the factorization of the generative model and the inference model, we obtain: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\bar{q}_{\\phi}}\\log\\frac{q_{\\phi}\\left(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t},\\mathbf{z}_{t x},\\mathbf{z}_{x}\\mid\\mathbf{t},\\mathbf{x}\\right)}=\\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z}_{t}\\mid\\mathbf{y},\\mathbf{t}\\right)}\\log\\frac{q_{\\phi}\\left(\\mathbf{z}_{t}\\mid\\mathbf{y},\\mathbf{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t}\\mid\\mathbf{t}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z}_{t x}\\mid\\mathbf{y},\\mathbf{t},\\mathbf{x}\\right)}\\log\\frac{q_{\\phi}\\left(\\mathbf{z}_{t x}\\mid\\mathbf{y},\\mathbf{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{x}\\right)}\\log\\left(\\frac{q_{\\phi}\\left(\\mathbf{z}_{x}\\mid\\mathbf{y},\\mathbf{x}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{x}\\mid\\mathbf{x}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, recognizing three Kullback-Leibler divergence terms in the right hand side of Equation 68, and injecting this expression into the evidence lower bound expression of Equation 66, we obtain the desired expression: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{ELBO}}=\\!\\mathbb{E}_{q_{\\phi}(\\mathbf{z}_{x},\\mathbf{z}_{t},\\mathbf{z}_{t x}|\\mathbf{x},\\mathbf{t},\\mathbf{y})}\\log p_{\\theta}(\\mathbf{y}\\mid\\mathbf{z}_{x},\\mathbf{z}_{t},\\mathbf{z}_{t x})}\\\\ &{\\quad\\quad\\quad\\quad-D_{K L}\\big(q_{\\phi}(\\mathbf{z}_{x}\\mid\\mathbf{x},\\mathbf{y})\\vert\\vert p_{\\theta}(\\mathbf{z}_{x}\\mid\\mathbf{x})\\big)}\\\\ &{\\quad\\quad\\quad\\quad-D_{K L}\\big(q_{\\phi}(\\mathbf{z}_{t}\\mid\\mathbf{t},\\mathbf{y})\\vert\\vert p_{\\theta}(\\mathbf{z}_{t}\\mid\\mathbf{t})\\big)}\\\\ &{\\quad\\quad\\quad\\quad-D_{K L}\\big(q_{\\phi}(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x},\\mathbf{y})\\vert\\vert p_{\\theta}(\\mathbf{z}_{t x}\\mid\\mathbf{t},\\mathbf{x})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C Training Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide additional training information in this section, including a detailed algorithm for the training process of FCR (Algorithm 1). ", "page_idx": 22}, {"type": "text", "text": "D Hyperparameter selection ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We split the data into four datasets: train/validation/test/prediction, following the setup from previous works (Lotfollahi et al., 2023; Wu et al., 2023). First we hold out the $20\\%$ of the control cells for the final cellular prediction tasks (prediction). Second, we hold $20\\%$ of the rest of data for the task of clustering and statistical test (test). Third, the data excluding the prediction and clustering/test sets are split into training and validation sets with a four-to-one ratio. ", "page_idx": 23}, {"type": "text", "text": "For the hyperparameter tuning procedure, conduct the exhaustive hyperparameter grid search with n_epoch $\\scriptstyle=100$ on the loss assessed on the validation data. The hyperparameter search space is shown in Table 2. ", "page_idx": 23}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/bffd6b6e32ffd0f5d2ce00113a15e476dcea000af271729998861a476a9cccad.jpg", "table_caption": ["Table 2: Hyperparameter space "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Datasets and Preprocessing ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide detailed descriptions of the four datasets and their corresponding preprocessing procedures. These datasets contain cells from many cell lines that are described in Table 3. ", "page_idx": 23}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/dfdd4119f1fa69215bffa33fd466ac91d0f4eafd22ec92a6695bec9c211b30ee.jpg", "table_caption": [], "table_footnote": ["Table 3: Cell line information for multiplex experiments "], "page_idx": 23}, {"type": "text", "text": "E.1 SciPlex dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This dataset includes three cancer cell lines exposed to 188 different compounds. In total, this experiment profiled approximately 650,000 single-cell transcriptomes across roughly 5,000 independent samples (Srivatsan et al., 2020). For our experiments, we selected only the HDAC inhibitors that were shown to be effective in these three cell lines (Srivatsan et al., 2020). The following is the list of HDAC inhibitors used in Table 4. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/4d2d7b7ee788021a533ccfaa6b3eccc5f4e59d8fe97e254ee86585507dcba58b.jpg", "table_caption": [], "table_footnote": ["Table 4: The list of HDAC inhibitors "], "page_idx": 24}, {"type": "text", "text": "We extracted the cells treated with HDAC inhibitors along with their corresponding control groups. After filtering out low-quality cells, we normalized the raw counts. From these, we retained the top 5,000 highly expressed genes. Ultimately, we analyzed 90,462 cells, including both treated and control groups. Cell lines and repetition numbers were used as covariates, with treatment dosage designated as the treatment variable. ", "page_idx": 24}, {"type": "text", "text": "E.2 MultiPlex-Tram dataset ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This dataset is referred to as experiment-5 in the paper McFarland et al. (2020). It contains in total 20,028 Trametinib treated cells, DSMO control cells, and 24 cell lines (McFarland et al., 2020). The cells are treated with $100\\mathrm{nM}$ Trametinib for 3, 6, 12, 24, 48 hours respectively. We removed the low quality cells and normalize the raw counts. Next, we kept first 5,000 differentially expressed genes. Finally, we have 13,713 cells in total for the experiments and down streaming evaluation. ", "page_idx": 24}, {"type": "text", "text": "E.3 Multiplex-7 dataset ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This dataset is labeled as experiment-3 in McFarland et al. (2020), includes 72,326 cells treated with seven different compounds across 24 cell lines. Details of the seven treatments are provided in Table 5. Following the removal of low-quality cells and normalization of raw counts, we retained the top 5,000 differentially expressed genes, yielding 61,552 cells for the experiments and downstream analysis. ", "page_idx": 24}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/a8e1330e4161710599e583fa278e79a9edd8eafe8e8c49fdef734fdc26a15292.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 5: The multiPlex-7 dataset\u2019s treatments ", "page_idx": 24}, {"type": "text", "text": "E.4 Multiplex-9 dataset ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "referred to as experiment-10 in McFarland et al. (2020), consists of 37,856 cells across 24 cell lines, treated with 9 drugs (including a control). The list of drugs can be found in Table 6. After flitering out low-quality cells and normalizing the raw counts, we retained the top 5,000 differentially expressed genes. This resulted in a total of 19,524 cells for the experiments and downstream evaluation. ", "page_idx": 25}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/e2dfedb38e930f1ece8937457f2ae0d23de480be272676aa6ff856f80bfc1941.jpg", "table_caption": [], "table_footnote": ["Table 6: Drugs list of the multiPlex-9 dataset "], "page_idx": 25}, {"type": "text", "text": "F Experimental Setups and Additional Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Training Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this subsection, we layout the training parameters for each datasets as follows. ", "page_idx": 25}, {"type": "text", "text": "sciPlex For the sciPlex datasets, the dimensions are as follows: $\\mathbf{z}_{x}$ is 32, $\\mathbf{z}_{t x}$ is 64, and $\\mathbf{z}_{t}$ is 32. Additionally, we set the hyperparameters to $\\omega_{1}=3.0$ , $\\omega_{2}=3.0$ , and $\\omega_{3}=5.0$ , with a batch size of 2046. Note that in sciPlex, we treat the dosages of the HDAC inhibitors as the treatment variable. ", "page_idx": 25}, {"type": "text", "text": "multiPlex-Tram or the multiPlex-Tram dataset, the dimensions are set as follows: $\\mathbf{z}_{x}$ is 32, $\\mathbf{z}_{t x}$ is 32, and $\\mathbf{z}_{t}$ is 32. The hyperparameters are $\\omega_{1}=5.0$ , $\\omega_{2}=5.0$ , $\\omega_{3}=1.0$ , with a batch size of 2046. In this dataset, Trametinib treatment time is considered as the treatment variable. ", "page_idx": 25}, {"type": "text", "text": "multiPlex-7 For the multiPlex-7 dataset, the dimensions are $\\mathbf{z}_{x}=32$ , $\\mathbf{z}_{t x}=64$ , and ${\\bf z}_{t}=32$ . The hyperparameters are $\\omega_{1}=1.0$ , $\\omega_{2}=0.5$ , and $\\omega_{3}=0.1$ , with a batch size of 2046. ", "page_idx": 25}, {"type": "text", "text": "multiPlex-9 For the multiPlex-9 dataset, the dimensions are $\\mathbf{z}_{x}=32$ , $\\mathbf{z}_{t x}=64$ , and $\\mathbf{z}_{t}=32$ . The hyperparameters are $\\omega_{1}=1.0$ , $\\omega_{2}=0.5$ , and $\\omega_{3}=0.1$ , with a batch size of 2046. ", "page_idx": 25}, {"type": "text", "text": "Additionally, the autoencoder learning rate is set to $3\\times10^{-4}$ , the discriminator learning rates are also $3\\times10^{\\dot{-}4}$ , and the number of discriminator training steps is 10. ", "page_idx": 25}, {"type": "text", "text": "F.2 Simulation Study ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We provide some empirical assessment of our identifiability theory using simulations. ", "page_idx": 25}, {"type": "text", "text": "Data Generation Following the simulation protocol outlined in Kong et al. (2022), Khemakhem et al. (2020) and Lopez et al. (2023), we simplify the simulation setup by setting the dimensions of $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t}$ to 1, while ${\\bf z}_{x t}$ has a dimension of 4. Specifically, we use a sample size of 5,000 and define our variables as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{t}\\sim\\operatorname{Unif}(\\{1,2,3\\})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here, t represents the treatment variable, uniformly distributed over three discrete values. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim\\operatorname{Unif}(\\{100,1000,5000\\})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "x denotes the covariate, also uniformly distributed but over a wider range of values. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{z}_{x}\\sim\\mathrm{Normal}(\\mathbf{x}/2,1)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\mathbf{z}_{x}$ is the latent variable associated with $\\mathbf{x}$ , following a normal distribution with mean ${\\bf x}/2$ and unit variance. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t}\\sim\\mathrm{Normal}(\\mathbf{t}/2,1)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\mathbf{z}_{t}$ is the latent variable associated with $\\mathbf{t}$ , also normally distributed with mean $\\mathbf{t}/2$ and unit variance. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t x}\\sim\\mathrm{Normal}(\\mathbf{x}\\cdot\\mathbf{t},\\mathbf{I}_{4})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\mathbf{z}_{t x}$ represents the interaction between $\\mathbf{x}$ and $\\mathbf{t}$ , following a multivariate normal distribution with mean ${\\bf x}\\cdot{\\bf t}$ and covariance matrix $\\mathbf{I}_{4}$ (the 4-dimensional identity matrix). ", "page_idx": 26}, {"type": "text", "text": "Finally, we define our output y as a function of these latent variables: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf y=g(\\mathbf z_{x},\\mathbf z_{t x},\\mathbf z_{t})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, $g$ is implemented as a 2-layer MLP with Leaky-ReLU activation, following the approach of Kong et al. (2022) and Khemakhem et al. (2020). The output $\\mathbf{y}$ is a real-valued vector with a dimension of 96. ", "page_idx": 26}, {"type": "text", "text": "Evaluation To assess the component-wise identifiability of the interaction components, we compute the Mean Correlation Coefficient (MCC) between $\\mathbf{z}_{t x}$ and $\\hat{\\mathbf{z}}_{t x}$ . MCC is a standard metric in Independent Component Analysis (ICA) literature, where a higher MCC indicates better identifiability. MCC reaches 1 when latent variables are perfectly identifiable (up to a component-wise transformation). We compute the MCC between the original sources and the corresponding latent variables sampled from the approximate posterior. As for the iVAE evaluation framework, we first calculate the correlation coefficients between all pairs of source and latent components. Then, we solve a linear sum assignment problem to map each latent component to the source component that correlates best with it, effectively reversing any latent space permutations. A high MCC indicates successful identification of the true parameters and recovery of the true sources, up to point-wise transformations. This is a standard performance metric in ICA (Khemakhem et al., 2020). ", "page_idx": 26}, {"type": "text", "text": "Results Our results suggest that our method, FCR largely outperforms existing variational autoencoder-based approaches in identifying the latent interactive components $\\mathbf{z}_{t x}$ . As shown in Table 7, FCR achieves an almost perfect Mean Correlation Coefficient (MCC), compared to other methods that have poor performance. Even the iVAE baseline falls short of FCR\u2019s capability in recovering the true latent structure. These results indicate that FCR offers a significant advancement in component-wise identifiability for complex, interacting latent variables in causal representation learning. ", "page_idx": 26}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/7de62b0bb113ad8e4a5c63428aa6523913200657b5eb71be07f1af91eac94be3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "F.3 Additional Clustering Details and Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Clustering Approach Our clustering analysis utilizes the learned representations $\\mathbf{z}_{x}$ , $\\mathbf{z}_{t x}$ , and $\\mathbf{z}_{t}$ for our method, while all available representations were used for baseline methods. It\u2019s important to note that baseline models were trained using their default settings. ", "page_idx": 26}, {"type": "text", "text": "We employed the following clustering approach for different scenarios: ", "page_idx": 26}, {"type": "text", "text": "Clustering on x: We applied the Leiden clustering algorithm to the different representations and evaluated the results using the labels of $\\mathbf{x}$ . ", "page_idx": 26}, {"type": "text", "text": "2. Clustering on t: Similarly, we used the Leiden algorithm on the representations and assessed the outcomes with the labels of $\\mathbf{t}$ .   \n3. Clustering on x combined with t: We ran Leiden clustering on the combined representations and evaluated the results using both the labels of $\\mathbf{x}$ and $\\mathbf{t}$ . ", "page_idx": 27}, {"type": "text", "text": "This approach allows us to assess the efficacy of our learned representations in capturing the underlying structure of both individual and combined variables. ", "page_idx": 27}, {"type": "text", "text": "Evaluation Metric The evaluation metric, Normalized Mutual Information (NMI), is defined as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{NM}(Y,C)=\\frac{2\\sum_{k=1}^{K}\\sum_{l=1}^{L}p_{k l}\\log\\Big(\\frac{p_{k l}}{p_{k}^{Y}p_{l}^{C}}\\Big)}{\\Big(-\\sum_{k=1}^{K}p_{k}^{Y}\\log p_{k}^{Y}\\Big)+\\Big(-\\sum_{l=1}^{L}p_{l}^{C}\\log p_{l}^{C}\\Big)},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where: ", "page_idx": 27}, {"type": "text", "text": "\u2022 $Y=\\{y_{1},\\ldots,y_{K}\\}$ denotes the set of class labels,   \n\u2022 $C=\\{c_{1},\\ldots,c_{L}\\}$ denotes the set of cluster labels,   \n\u2022 $p_{k l}$ is the joint probability of a data sample belonging to class $k$ and cluster $l$ , \u2022 $p_{k}^{Y}$ is the marginal probability of a sample belonging to class $k$ ,   \n\u2022 $p_{l}^{C}$ is the marginal probability of a sample belonging to cluster $l$ . ", "page_idx": 27}, {"type": "text", "text": "Note that these probabilities are computed from the same dataset, but with respect to the true class labels and the assigned cluster labels, respectively. ", "page_idx": 27}, {"type": "text", "text": "Additional Results Additional clustering results for the multiPlex-tram, multiPlex-7, and multiPlex9 datasets are presented in Figure 5. ", "page_idx": 27}, {"type": "text", "text": "F.4 Statistical Tests and More results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Due to the computational complexity of kernel calculations, we adopted a sampling approach with 2,000 samples, repeating the process 100 times to report the results. For the baseline methods, we sampled their latent spaces to match the dimensions of $\\mathbf{z}_{x}$ , $\\mathbf{z}_{t}$ , and $\\mathbf{z}_{t x}$ . This sampling was repeated 20 times, and we reported the best results for comparison with FCR. The test results for the multiPlex-tram, multiPlex-7, and multiPlex-9 datasets are shown in the Figure 6 ", "page_idx": 27}, {"type": "text", "text": "F.5 Conditional Cellular Response Prediction ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For this task, we use FCR\u2019s latent representations to predict gene expression levels and report the corresponding $R^{2}$ scores. Specifically, our approach enables the prediction of cellular responses at the single-cell level. The focus of this paper is on predicting cellular responses (expression of 2,000 genes) in control cells subjected to drug treatments. Our comparative analysis includes CPA, VCI, sVAE, scGEN, and CINEMA-OT, as these methods are specifically designed for cellular prediction tasks. ", "page_idx": 27}, {"type": "text", "text": "We utilize FCR to extract the control\u2019s latent representations [z0x, zt0x, zt0] and the corresponding experimental representations $\\left[{{\\bf{z}}_{x}},{{\\bf{z}}_{t x}},{{\\bf{z}}_{t}}\\right]$ . The decoder $g$ is then used to predict the gene expression levels as $\\hat{\\mathbf{y}}=g\\bar{(}\\mathbf{z}_{x}^{0},\\mathbf{z}_{t x}^{0},\\mathbf{z}_{t}^{0})$ . The $R^{2}$ score is used to evaluate the predictions, and we sampled $20\\%$ of each dataset for testing, repeating this process five times. ", "page_idx": 27}, {"type": "text", "text": "For iVAE and VCI, we used treatments, covariates, and gene expression data to learn latent variables and predict gene expression. In contrast, for $\\mathrm{scVI}$ , $\\beta\\mathrm{VAE}$ , and factorVAE, we only used gene expression data to learn latent variables and make predictions. ", "page_idx": 27}, {"type": "text", "text": "The $R^{2}$ score is a key metric for assessing the accuracy of predictive models. It measures the proportion of variance in the dependent variable that is explained by the independent variables. An $\\mathbf{\\bar{\\alpha}}^{2}$ score of 1 indicates perfect prediction accuracy, meaning all variations in the target variable are fully explained by the model\u2019s inputs, while a score of 0 suggests that the model fails to capture any variance in the target variable. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "equation", "text": "$$\nR^{2}=1-\\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^{2}}{\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $y_{i}$ is the actual values, $\\hat{y_{i}}$ is the predicted values, $\\bar{y}$ is the mean of average values, $n$ is the number of observations. ", "page_idx": 28}, {"type": "text", "text": "Additional Metric We further utilize the Mean Squared Error (MSE) of the top 20 differentially expressed genes (DEGs) for post-treatment. These 20 genes are selected for showing statistically significant differences in expression levels for each cell line with drug treatments compared to control samples. The same procedures are also carried out in Roohani et al. (2024). Note here, we did not compare with CINEMA-OT and scGEN because they are only for binary treatments. ", "page_idx": 28}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/e2b6c2c291994c833cdc4d381709706f2678468f45c775de2a68297fbd97816e.jpg", "table_caption": [], "table_footnote": ["Table 8: Mean Squared Error (MSE) of top 20 Differentially Expressed Genes (DEGs) for different methods across datasets "], "page_idx": 28}, {"type": "text", "text": "F.6 Ablation Study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We present the ablation study results for the hyperparameters $\\omega_{1},\\omega_{2}$ , and $\\omega_{3}$ . Initially, we set $\\omega_{1}$ , $\\omega_{2}$ , and $\\omega_{3}$ to [1,1,1]. Then, we varied each parameter independently to [1, 3, 5, 10, 20], keeping the other parameters fixed at 1. Figure 7 illustrates the NMI scores for clustering on $\\mathbf{x}$ , t, or both $\\mathbf{x}\\&\\mathbf{t}$ . Figure 8 shows the $R^{2}$ scores for each parameter. ", "page_idx": 28}, {"type": "text", "text": "F.7 Visualization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Visualizing latent representations provides intuitive insights into the distinct characteristics and attributes captured by each representation. Using Uniform Manifold Approximation and Projection (UMAP) (McInnes et al., 2018), we visualized the latent representations $\\mathbf{z}_{x}$ , $\\mathbf{z}_{t x}$ , and $\\mathbf{z}_{t}$ derived from the sciPlex dataset (Figure 9). ", "page_idx": 28}, {"type": "text", "text": "From these visualizations, we observe that $\\mathbf{z}_{x}$ effectively captures covariate-specific information, showing clear separation in Figure 9, but it does not reflect treatment information. In contrast, the UMAP visualization of $\\mathbf{z}_{t}$ reveals a clear pattern, where cells treated with higher dosages are positioned at the bottom, and control or lower-dosage treated cells are found at the top. Additionally, various cell lines intermingle within the plot, indicating that $\\mathbf{z}_{t}$ successfully captures drug responses across different cell lines. ", "page_idx": 28}, {"type": "text", "text": "Most importantly, Figure 9 demonstrates that $\\mathbf{z}_{t x}$ captures cell line-specific treatment responses, representing a balanced integration of both covariate and treatment information. Specifically, in the sciPlex dataset, the MCF7 cell line shows a pronounced cell line-specific response, aligning with findings from biological literature (Srivatsan et al., 2020), while the K562 cell line exhibits a less distinct response. This representation confirms the strong, unique responses in certain cell lines, highlighting the validity and precision of our method in capturing nuanced biological behaviors. ", "page_idx": 28}, {"type": "text", "text": "F.8 Pilot Study On The Unseen Drug Responses ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Predicting responses to novel treatments is a pivotal and fast-evolving field in drug discovery. However, the biological literature indicates that cellular responses are highly context-dependent (McFarland et al., 2020). This complexity poses significant challenges for AI-driven drug discovery, which often struggles to achieve success in clinical trials. ", "page_idx": 28}, {"type": "text", "text": "Our motivation for developing FCR stems from the need to understand how cellular systems react to treatments and identify conditions that can deepen our understanding of these responses. FCR enables the analysis of drug interactions with covariates and contextual variables. Additionally, predicting cellular responses to new treatments necessitates prior knowledge, such as chemical structure and molecular function, and comparisons with known treatments. Without this context, predictions can be unreliable. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "In this paper, our primary focus is not on predicting responses to unseen treatments. However, given the relevance of this topic, we conducted pilot experiments to showcase the potential future applications of FCR. The multiPlex-Tram and multiPlex-7 datasets share the same cell lines and Trametinib-24 hours treatment, along with other different treatments. By utilizing these two datasets, we established the following experimental settings for unseen prediction scenario: ", "page_idx": 29}, {"type": "text", "text": ". Drug Hold-out Setup: We held out two cell lines, ILAM and SKMEL2, from the MultiplexTram dataset, which had been treated with Trametinib for 24 hours. We trained a FCR model using the remaining data, and this model is referred to as $M_{h}$ . We denote the dataset (Multiplex-Tram dataset without Trametinib-24h treated ILAM and SKMEL2) as $D^{h}$ ", "page_idx": 29}, {"type": "text", "text": "2. Prior Knowledge Model:We trained another FCR model, $M_{p}$ , using the Multiplex-7 dataset, which includes the ILAM and SKMEL2 cell lines treated with Trametinib for 24 hours denoted as $D^{p}$ . We treat this model, $M_{p}$ , as a prior knowledge model. ", "page_idx": 29}, {"type": "text", "text": "3. Transfer MLP: We extract both $\\mathbf{z}_{t x}^{p}$ from $M_{p}$ and $\\mathbf{z}_{t x}^{h}$ from $M_{h}$ for $D^{p}$ . Then we trained a 1-layer MLP (the same dimension as $\\mathbf{z}_{t x}^{p}$ ) to transfer $\\mathbf{z}_{t x}^{p}$ to $\\mathbf{z}_{t x}^{h}$ , by minimize the MSE between them. ", "page_idx": 29}, {"type": "text", "text": "4. Contextual Prior Representation: We extracted the $\\mathbf{z}_{t x}^{p}$ representations from model $M_{p}$ for ILAM and SKMEL2 in holdout set. Then transfer $\\mathbf{z}_{t x}^{p}$ by the previous MLP to $\\hat{\\mathbf{z}}_{t x}^{h}$ as a prior contextual embedding. For the hold-out cell lines ILAM and SKMEL2 in the Multiplex-Tram dataset, we extracted $\\mathbf{z}_{x}^{h}$ from model $M_{h}$ , and Trametinib-24h $\\mathbf{z}_{t}^{h}$ from other treated cell lines in $D^{h}$ ", "page_idx": 29}, {"type": "text", "text": "5. Representation Matching: Then we match the $\\hat{\\mathbf{z}}_{t x}^{h}$ by $\\mathbf{z}_{x}^{p}$ similarity on ILAM and SKMEL2 across Multiplex-tram and Multiplex-7 in the prior knowledge model space. ", "page_idx": 29}, {"type": "text", "text": "6. SPrKeMdiEctLio2 nc:e llW lei nperse idni chtoeldd tohuet  duantsaeseent  u2s4i nhgo tuhres  fTorrammueltai:n $\\hat{\\mathbf{y}}=\\bar{g}\\big(\\mathbf{z}_{x}^{h},\\hat{\\mathbf{z}}_{t x}^{h},\\mathbf{z}_{t}^{h}\\big)$ ,ew IhLerAe $\\hat{\\mathbf{z}}_{t x}^{h}$ nids the corresponding matched prior contextual representation, $\\mathbf{z}_{t}^{h}$ is the tramnib-24 average value in other cell lines. ", "page_idx": 29}, {"type": "text", "text": "7. Evaluation: We computed the $R^{2}$ and MSE for the top 20 differentially expressed genes (DEGs) based on the predicted values and compared these results with those from the VCI model. The paper\u2019s OOD prediction setups. ", "page_idx": 29}, {"type": "table", "img_path": "AhlaBDHMQh/tmp/5f432fe463a8ab2d41e5ab059d6c1adbdaeda0d62ce30bf7da081e54ef5e798c.jpg", "table_caption": ["Table 9: Out-of-Distribution (OOD) Performance Results "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/8dab30aa3e4609dc2cc4d13e06b391e67292ee4fd3148d5fc894999969c95427.jpg", "img_caption": ["Figure 5: The clustering results. (a) For the multiPlex-Tram dataset, the first column shows the NMI value for clustering on x, the second column shows the NMI value for clustering on xt, and the third column shows the NMI value for clustering on t.(b) For the multiPlex-5 dataset, the first column presents the NMI value for clustering on x, the second column shows the NMI value for clustering on xt, and the third column shows the NMI value for clustering on t. (c) For the multiPlex-9 dataset, the first column displays the NMI value for clustering on x, the second column shows the NMI value for clustering on xt, and the third column shows the NMI value for clustering on $\\mathbf{t}$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/9f218a0fcaf517a6e6bbd4b95d90fa884bdf1963b6f67ae3f4c83519076f6749.jpg", "img_caption": ["Figure 6: Conditional independence test results for the multiPlex-7 (a), multiPlex-tram (b), and multiPlex-9 (c) datasets. The first column presents the p-values for the conditional independence test of $\\mathbf{z}_{x}$ and $\\mathbf{t}$ conditioned on $\\mathbf{x}$ , with the red dashed line indicating a significance threshold of 0.05. The second column shows the ${\\bf p}\\cdot{\\bf p}$ -values for the conditional independence test of $\\mathbf{z}_{t}$ and $\\mathbf{x}$ conditioned on t. The third column presents the ${\\bf p}$ -values for the conditional independence tests of $\\mathbf{z}_{x}$ and $\\mathbf{z}_{t x}$ conditioned on $\\mathbf{x}$ , and of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{t x}$ conditioned on t. The fourth column shows the HSIC values of $\\mathbf{z}_{x}$ with $\\mathbf{x}$ , t, and random variables (R), as well as the HSIC values of $\\mathbf{z}_{t}$ with $\\mathbf{x}$ , t, and random variables (R). "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/e274f46e4e9ab3a062f4c3b2c30b098c2ea0e079e342bbc8b9d5b008ae625915.jpg", "img_caption": ["Figure 7: Clustering Ablation Results "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/ecb81b594866c7d51215290f2b9899be0a26e962429b18ef8fedf9cb52b014e0.jpg", "img_caption": ["Figure 8: $R^{2}$ score with different $\\omega_{1}$ , $\\omega_{2}$ and $\\omega_{3}$ values "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/06663425643554d0af68bf189b0a1fbfd96fb739d747d38fe90002569bd5f21f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "AhlaBDHMQh/tmp/ff252b0dfe23c7f800acc7e4034aad8bf3c40cf3fb5bf13a89ae0ef3f4e88f23.jpg", "img_caption": ["Figure 9: (a) UMAP visualization of the sciPlex dataset. The first row distinguishes the data by cell type, while the second row distinguishes by treatment time, with brighter colors indicating longer treatment durations. (b) UMAP visualization of the multiPlex-tram dataset. The first row distinguishes by cell type, and the second row distinguishes by treatment time, with brighter colors representing longer treatment durations. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The limitations are discussed. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The proofs are provided in the appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The hyperparameters are provided in the appendix ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The code is provided on GitHub. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Details are in the appendices. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: p-values are calculated in the experimental sections. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide this detail in the appendices. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We discuss how this paper would contribute to the precision medicine. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: It\u2019s not applicable in our paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Not applicable to our paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Documentation is available with the code on GitHub. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 39}, {"type": "text", "text": "Justification:the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with989 human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]