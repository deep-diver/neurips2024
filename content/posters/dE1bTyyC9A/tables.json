[{"figure_path": "dE1bTyyC9A/tables/tables_5_1.jpg", "caption": "Table 1: Mask prediction performance of instance and interactive segmentation.", "description": "This table presents a comparison of the mask prediction performance for instance segmentation and interactive segmentation tasks.  The mIoU (mean Intersection over Union) metric is used to quantify the accuracy of the mask predictions. Interactive segmentation, which incorporates user interaction in the form of visual prompts, significantly outperforms instance segmentation, highlighting the benefit of incorporating user feedback for improved segmentation accuracy.", "section": "3.3 Explicit Inter-task Association"}, {"figure_path": "dE1bTyyC9A/tables/tables_7_1.jpg", "caption": "Table 2: Comparisons on ScanNet20 [6], ScanRefer [1], and ScanNet200 [45]. The best results are highlighted in bold, and the second-best results are underscored. '*' indicates the use of the two-stage fine-tuning trick. '-/-' denotes training on filtered or complete ScanRefer datasets.", "description": "This table presents a comparison of the UniSeg3D model's performance against state-of-the-art (SOTA) methods on three benchmark datasets: ScanNet20, ScanRefer, and ScanNet200.  The comparison covers six 3D scene understanding tasks: panoptic segmentation, semantic segmentation, instance segmentation, interactive segmentation, referring segmentation, and open-vocabulary segmentation.  The table highlights the best and second-best results for each task and dataset, indicating UniSeg3D's superior performance across multiple tasks and datasets. The '*' denotes the results obtained when using a two-stage fine-tuning trick, while '-/-' signifies results obtained using either a filtered or complete version of the ScanRefer dataset.", "section": "4.1 Comparison to the SOTA Methods"}, {"figure_path": "dE1bTyyC9A/tables/tables_7_2.jpg", "caption": "Table 3: Ablation on task unification.", "description": "This table presents the ablation study on the task unification in the UniSeg3D model. It shows the impact of including each of the six tasks (panoptic, semantic, instance, interactive, referring, and open-vocabulary segmentation) on the overall performance. The results demonstrate that unifying all six tasks in a single model leads to a performance improvement, though not always uniformly across the different tasks.", "section": "4.2 Analysis and Ablation"}, {"figure_path": "dE1bTyyC9A/tables/tables_8_1.jpg", "caption": "Table 4: Ablation on components. \u2018Distillation\u2019, \u2018Rank-Contrastive\u2019, and \u2018Trick\u2019 denote the knowledge distillation, ranking-based contrastive learning, and two-stage fine-tuning trick, respectively.", "description": "This table presents the ablation study results on different components of the UniSeg3D model. It shows the performance (PQ, mIoU, mAP, AP) on six different 3D segmentation tasks (Panoptic, Semantic, Instance, Interactive, Referring, and Open-Vocabulary) across three datasets (ScanNet20, ScanRefer, and ScanNet200). Each row represents a different combination of the three components: knowledge distillation, ranking-based contrastive learning, and two-stage fine-tuning.  The \"Overall\" column shows the average performance across all tasks and datasets for each configuration.", "section": "4.2 Analysis and Ablation"}, {"figure_path": "dE1bTyyC9A/tables/tables_8_2.jpg", "caption": "Table 5: Ablation on different designs of the proposed components. 'v \u2192 g' and 'v \u2192 r' denote the knowledge distillation from the interactive segmentation to the generic segmentation and the referring segmentation, respectively. \u201cContrastive' and \u2018Rank\u2019 denote the contrastive learning and the ranking rule, respectively.", "description": "This table presents the ablation study results on different designs of the proposed components for improving the performance of the UniSeg3D model. It shows the impact of knowledge distillation from interactive segmentation to generic and referring segmentation ('v \u2192 g' and 'v \u2192 r'), contrastive learning ('Contrastive'), and the ranking rule ('Rank') on various metrics for different tasks (Panoptic Segmentation, Semantic Segmentation, Instance Segmentation, Interactive Segmentation, Referring Segmentation, and Open-Vocabulary Segmentation) across three datasets (ScanNet20, ScanRefer, and ScanNet200). Each row represents a different combination of the proposed components, enabling a comprehensive analysis of their individual and combined effects on the model's performance.", "section": "4.2 Analysis and Ablation"}, {"figure_path": "dE1bTyyC9A/tables/tables_9_1.jpg", "caption": "Table 6: Ablation on hyper-parameter \u03bb.", "description": "This table presents the ablation study on the hyper-parameter \u03bb, which balances the basic losses and inter-task association losses in the training objective function.  The results show the performance of the UniSeg3D model across various tasks (Panoptic Segmentation, Semantic Segmentation, Instance Segmentation, Interactive Segmentation, Referring Segmentation, and Open-Vocabulary Segmentation) on three benchmark datasets (ScanNet20, ScanRefer, and ScanNet200) for different values of \u03bb (0.05, 0.1, 0.2, and 0.3).  The Overall column represents the average performance across all six tasks.", "section": "4.2 Analysis and Ablation"}, {"figure_path": "dE1bTyyC9A/tables/tables_9_2.jpg", "caption": "Table 7: Ablation on vision prompts.", "description": "This table presents the ablation study on vision prompts in the interactive segmentation task.  It shows the impact of using different strategies for selecting the vision prompt, including using the instance center, points at varying distances from the center (rd), and random point selection. The results are evaluated using mIoU, AP, AP50, and AP25 metrics. The goal is to determine the optimal strategy for selecting vision prompts to improve interactive segmentation performance.", "section": "4.2 Analysis and Ablation"}, {"figure_path": "dE1bTyyC9A/tables/tables_14_1.jpg", "caption": "Table 2: Comparisons on ScanNet20 [6], ScanRefer [1], and ScanNet200 [45]. The best results are highlighted in bold, and the second-best results are underscored. '*' indicates the use of the two-stage fine-tuning trick. '-/-' denotes training on filtered or complete ScanRefer datasets.", "description": "This table presents a comparison of the UniSeg3D model's performance against state-of-the-art (SOTA) methods on three benchmark datasets: ScanNet20, ScanRefer, and ScanNet200.  It shows the performance of UniSeg3D and other methods across multiple 3D scene understanding tasks, including panoptic, semantic, instance, interactive, referring, and open-vocabulary segmentation.  The results highlight UniSeg3D's superior performance and ability to unify these diverse tasks within a single model. The use of a two-stage fine-tuning technique is also noted, and differences in training data are indicated.", "section": "4.1 Comparison to the SOTA Methods"}, {"figure_path": "dE1bTyyC9A/tables/tables_14_2.jpg", "caption": "Table IV: Comparison with previous open-vocabulary segmentation methods on ScanNe200 and Replica. Our method outperforms existing approaches in terms of AP.", "description": "This table compares the performance of UniSeg3D against state-of-the-art (SOTA) methods for open-vocabulary segmentation on two benchmark datasets: ScanNet200 and Replica.  The table presents Average Precision (AP) scores, broken down by AP for base classes and novel classes (ScanNet200 only), as well as AP at different IoU thresholds (AP50, AP25) for Replica. UniSeg3D demonstrates superior performance compared to the other methods.", "section": "4.1 Comparison to the SOTA Methods"}, {"figure_path": "dE1bTyyC9A/tables/tables_15_1.jpg", "caption": "Table V: Inference time and instance segmentation performance on the ScanNet20 validation split.", "description": "This table compares the inference time and instance segmentation performance of UniSeg3D against other state-of-the-art methods on the ScanNet20 validation dataset.  It breaks down the inference time for each component (backbone, grouping, refinement, etc.) and the total inference time for each model, providing a detailed comparison of computational efficiency across different approaches.", "section": "B Inference time analysis of the proposed UniSeg3D"}]