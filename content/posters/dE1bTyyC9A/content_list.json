[{"type": "text", "text": "A Unified Framework for 3D Scene Understanding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei $\\mathbf{Xu}^{*}$ , Chunsheng $\\mathbf{Shi^{*}}$ , Sifan Tu, Xin Zhou, Dingkang Liang, Xiang Bai\u2020 Huazhong University of Science and Technology {wxu2023, dkliang, xbai}@hust.edu.cn ", "page_idx": 0}, {"type": "image", "img_path": "dE1bTyyC9A/tmp/dfdda65c188a05cbff0853a87785535ce84c2e04dc0b2815330b7a5d4038f874.jpg", "img_caption": ["(c) Proposed unified method, supporting six tasks ", "(d) Comparisons with previous SOTA "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Comparisons between the proposed method and current SOTA approaches specialized for specific tasks. (a) Representative specialized approaches on six tasks. (b) OneFormer3D, a recent unified framework, achieves SOTA performance on three generic segmentation tasks in one inference. (c) The proposed unified framework achieves six tasks in one inference. (d) Our method outperforms current SOTA approaches across six tasks involving two modalities using a single model. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose UniSeg3D, a unified 3D segmentation framework that achieves panoptic, semantic, instance, interactive, referring, and open-vocabulary segmentation tasks within a single model. Most previous 3D segmentation approaches are typically tailored to a specific task, limiting their understanding of 3D scenes to a task-specific perspective. In contrast, the proposed method unifies six tasks into unified representations processed by the same Transformer. It facilitates inter-task knowledge sharing, thereby promoting comprehensive 3D scene understanding. To take advantage of multi-task unification, we enhance performance by leveraging inter-task connections. Specifically, we design knowledge distillation and contrastive learning methods to transfer task-specific knowledge across different tasks. Benefiting from extensive inter-task knowledge sharing, our UniSeg3D becomes more powerful. Experiments on three benchmarks, including ScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTA methods, even those specialized for individual tasks. We hope UniSeg3D can serve as a solid unified baseline and inspire future work. Code and models are available at https://dk-liang.github.io/UniSeg3D/. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3D scene understanding has been a foundational aspect of various real-world applications [3, 68, 18], including robotics, autonomous navigation, and mixed reality. Among the 3D scene understanding tasks, 3D point cloud segmentation is a crucial component. Generic 3D point cloud segmentation contains panoptic, semantic, and instance segmentation (PS/SS/IS) tasks [36, 40, 58, 66, 21], which segment classes annotated in the training set. As a complement, 3D open-vocabulary segmentation (OVS) task [39, 52, 15] segments open-vocabulary classes of interest. Another group of works study to utilize user priors. In particular, 3D interactive segmentation task [23, 67] segments instances specified by users. 3D referring segmentation task [14, 43, 56, 55] segments instances described by textual expressions. The above mentioned tasks are core tasks in 3D scene understanding, drawing significant interest from researchers and achieving great success. ", "page_idx": 1}, {"type": "text", "text": "Previous studies [51, 8, 71, 19] in the 3D scene understanding area focus on separated solutions specialized for specific tasks, as shown in Fig. 1(a). These approaches ignore intrinsic connections across different tasks, such as the objects\u2019 geometric consistency and semantic consistency. They also fail to share knowledge biased toward other tasks, limiting their understanding of 3D scenes to a task-specific perspective. It poses significant challenges for achieving comprehensive and in-depth 3D scene understanding. A recent exploration [22] named OneFormer3D designs an architecture to unify the 3D generic segmentation tasks, as shown in Fig. 1(b). This architecture inputs instance and semantic queries to simultaneously predict the 3D instance and semantic segmentation results. And the 3D panoptic segmentation is subsequently achieved by post-processing these predictions. It is simple yet effective. However, this architecture fails to support the 3D interactive segmentation, 3D referring segmentation, and OVS tasks, which provide complementary scene information, including user priors and open-set classes, should be equally crucial in achieving 3D scene understanding as the generic segmentation tasks. This leads to a natural consideration that if these 3D scene understanding tasks can be unified in a single framework? ", "page_idx": 1}, {"type": "text", "text": "A direct solution is to integrate the separated methods into a single architecture. However, it faces challenges balancing the customized optimizations specialized for the specific tasks involved in these methods. Thus, we aim to design a simple and elegant framework without task-specific customized modules. This inspires us to design the UniSeg3D, a unified framework processing six 3D segmentation tasks in parallel. Specifically, we use queries to unify representations of the input information. The 3D generic segmentation tasks and the OVS task, which only input point cloud without human knowledge, thus can be processed by sharing the same workflow without worrying about prior knowledge leakage. We use a unified set of queries to represent the four-task features for simplification. The interactive segmentation inputs visual point priors to condition the segmentation. We represent the point prompt information by simply sampling the point cloud queries, thereby avoiding repeated point feature extraction. The referring segmentation inputs textual expressions, which persist in a modality gap with the point clouds and are hard to unify in the previous workflows. To minimize the time consumption, we design a parallel text prompt encoder to extract the text queries. All these queries are decoded using the same mask decoder and share the same output head without the design of task-specific customized structures. ", "page_idx": 1}, {"type": "text", "text": "We further enhance performance by taking advantage of the multi-task design. In particular, we empirically find that the interactive segmentation outperforms the rest of the tasks in mask predictions, which is attributable to reliable vision priors. Hence, we design knowledge distillation to distill knowledge from the interactive segmentation to the other tasks. Then, we build contrastive learning between interactive segmentation and referring segmentation to connect these two tasks. The proposed knowledge distillation and contrastive learning promote knowledge sharing across six tasks, effectively establishing associations between different tasks. There are three significant strengths of the UniSeg3D: (1) it unifies six 3D scene understanding tasks in a single framework, as shown in Fig. 1(c); (2) it is flexible for that can be easily extended to more tasks by simply inputting the additional task-specific queries; (3) the designed knowledge distillation and contrastive learning are only used in the training phase, optimizing the performance with no extra inference cost. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We compare the proposed method with task-specific specialized SOTA approaches [48, 54, 34, 67, 43, 38] across six tasks to evaluate its performance. As shown in Fig. 1(d), the UniSeg3D demonstrates superior performance on all the tasks. It is worth noting that our performance on different tasks is achieved by a single model, which is more efficient than running separate task-specific approaches individually. Furthermore, the structure of UniSeg3D is simple and elegant, containing no taskcustomized modules, while consistently outperforming specialized SOTA solutions, demonstrating a desirable potential to be a solid unified baseline. ", "page_idx": 2}, {"type": "text", "text": "In general, our contributions can be summarized as follows: First, we propose a unified framework named UniSeg3D, offering a flexible and efficient solution for 3D scene understanding. It achieves six 3D segmentation tasks in one inference by a single model. To the best of our knowledge, this is the first work to unify six 3D segmentation tasks. Second, specialized approaches limit their 3D scene understanding to task-specific perspectives. We facilitate inter-task knowledge sharing to promote comprehensive 3D scene understanding. Specifically, we take advantage of multi-task unification, designing the knowledge distillation and contrastive learning methods to establish explicit inter-task associations. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3D segmentation. The generic segmentation consists of panoptic, semantic, and instance segmentation. The panoptic segmentation [36, 57] is the union of instance segmentation [9, 31, 2, 61, 53] and semantic segmentation [42, 40, 5, 69]. It contains the instance masks from the instance segmentation and the stuff masks from the semantic segmentation. These 3D segmentation tasks rely on annotations, segmenting classes labeled in the training set. The open-vocabulary segmentation [38, 52] extends the 3D segmentation to the novel class. Another group of works explores 3D segmentation conditioned by human knowledge. Specifically, the interactive segmentation [23, 67] segments instances specified by the point clicks. The referring segmentation [14, 43] segments objects described by textual expressions. Most previous researches [62, 4, 24, 30] focus on specific 3D segmentation tasks, limiting their efficiency in multi-task scenarios, such as the domotics, that require multiple task-specific 3D segmentation approaches to be applied simultaneously. This work proposes a framework to achieve the six abovementioned tasks in one inference. ", "page_idx": 2}, {"type": "text", "text": "Unified vision models. Unified research supports multiple tasks in a single model, facilitating efficiency and attracting a lot of attention in the 2D area [41, 35, 28, 17]. However, rare works study the unified 3D segmentation architecture. It might be attributed to the higher dimension of the 3D data, which leads to big solution space, making it challenging for sufficient unification across multiple 3D tasks. Recent works [11, 33] explore outdoor unified 3D segmentation architectures, and some others [72, 13, 16] delve into unified 3D representations. So far, only one method, OneFormer3D [22], focuses on indoor unified 3D segmentation. It extends the motivation proposed in OneFormer [17] to the 3D area and proposes an architecture to achieve three 3D generic segmentation tasks in a single model. We note that the supported tasks in OneFormer3D can be achieved in one inference through post-processing predictions of a panoptic segmentation model. In contrast, we propose a simple framework that unifies six tasks, including not only generic segmentation but also interactive segmentation, referring segmentation, and OVS, into a single model. Additionally, we establish explicit associations between these unified tasks to promote knowledge sharing, contributing to effective multi-task unification. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The framework of UniSeg3D is depicted in Fig. 2. It mainly consists of three modules: a point cloud backbone, prompt encoders, and a mask decoder. We illustrate their structures in the following. ", "page_idx": 2}, {"type": "text", "text": "3.1 Point Cloud Backbone and Prompt Encoders ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Point cloud backbone. We represent the set of $N$ input points as $\\mathbf{P}\\in\\mathbb{R}^{N\\times6}$ , where each point is characterized by three-dimensional coordinates $x$ , $y$ , $z$ and three-channel colors $r,g,b$ . These input points are then fed into a sparse 3D U-Net, serving as the point cloud backbone, to obtain point-wise features $\\mathbf{F}\\in\\mathbb{R}^{N\\times d_{i n}}$ , where $d_{i n}$ denotes the feature dimension. Processing dense points individually in 3D scene understanding can be time-consuming. Therefore, we downsample the 3D scenario into $M$ superpoints and pool the point features within each superpoint to form superpoint features $\\mathbf{F}_{s}=\\{\\mathbf{f}_{i}\\}_{i=1}^{M}$ , where each $\\mathbf{f}_{i}\\in\\mathbb{R}^{d_{i n}}$ and ${\\bf F}_{s}\\in\\mathbb{R}^{M\\times d_{i n}}$ . This procedure exhibits awareness of the edge textures [26] while reducing cost consumption. ", "page_idx": 2}, {"type": "image", "img_path": "dE1bTyyC9A/tmp/be7454906ce0fd905675f439cf16a8eabf42282ed0d3d40061322f9dd4e1623f.jpg", "img_caption": ["Figure 2: The framework of UniSeg3D. This is a simple framework handling six tasks in parallel without any modules specialized for specific tasks. We take advantage of multi-task unification and enhance the performance through building associations between the supported tasks. Specifically, knowledge distillation transfers insights from interactive segmentation to the other tasks, while contrastive learning establishes connections between interactive segmentation and referring segmentation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Vision prompt encoder. Click is a kind of clear and convenient visual interaction condition that has been widely employed in previous works [20, 23, 67]. We formulate the clicks as vision prompts, as illustrated in Fig. 2. In practice, a click is first indicated by the spatially nearest point. Then, we sample a superpoint containing this point and employ its superpoint feature as vision prompt feature $\\mathbf{f}_{v}\\in\\mathbb{R}^{d_{i n}}$ to represent the point prompt information, thus avoiding redundant feature extraction and maintaining feature consistency with the point clouds. ", "page_idx": 3}, {"type": "text", "text": "Text prompt encoder. UniSeg3D is able to segment instances described by textual expressions. To process a text prompt, the initial step involves tokenizing the text sentence to obtain its string tokens $\\mathbf{\\dot{T}}\\in\\mathbb{R}^{l\\times c}$ , where $l$ is the sentence length and $c$ represents the token dimension. These tokens are then fed into a frozen CLIP [44] text encoder to produce a $C$ -dimensional text feature $\\mathbf{f}_{t}\\in\\mathbb{R}^{C}$ . This feature is subsequently projected into the $d_{i n}$ dimension using two linear layers, obtaining $\\mathbf{f}_{t}\\in\\mathbb{R}^{d_{i n}}$ , aligning the dimension of the point features for subsequent processing. ", "page_idx": 3}, {"type": "text", "text": "3.2 Mask Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We employ a single mask decoder to output predictions of six 3D scene understanding tasks. The generic segmentation and the OVS share the same input data, i.e., the point cloud without user knowledge. Therefore, we randomly select $m$ features from $M$ superpoint features to serve as unified queries $\\mathbf{q}_{u}^{\\prime}\\in\\mathbb{R}^{m\\times d_{i n}}$ for both the generic segmentation and OVS tasks. During training, we set $m<M$ to reduce computational costs, while for inference, we set $m=M$ to enable the segmentation of every region. ", "page_idx": 3}, {"type": "text", "text": "pTrhoe mpprto fmepatt uirnefso rams atthieo np rios menpct oqdueedr iienst, o wphriocmh pct afne abteu rwesr itatse nd iassc:u $\\mathbf{q}_{v}^{\\prime}\\,=\\,\\{\\mathbf{f}_{v,i}\\}_{i=1}^{K_{v}}$ ., $\\mathbf{q}_{t}^{\\prime}=\\bar{\\{\\mathbf{f}_{t,i}\\}}_{i=1}^{K_{t}}$ where , . and $K_{t}$ are the number of the point and text prompts, respectively. $\\mathbf{q}_{u}^{\\prime}$ , ${\\bf q}_{v}^{\\prime}$ , $\\mathbf{q}_{t}^{\\prime}$ are three types of queries containing information from various aspects. Feeding them forward indiscriminately would confuse the mask decoder for digging task-specific information. Thus, we add task-specific embeddings $\\mathbf{e}_{u},\\mathbf{e}_{v}$ , and ${\\bf e}_{t}$ before further processing: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{q}_{u}=\\mathbf{q}_{u}^{\\prime}+\\mathbf{e}_{u},\\quad\\mathbf{q}_{v}=\\mathbf{q}_{v}^{\\prime}+\\mathbf{e}_{v},\\quad\\mathbf{q}_{t}=\\mathbf{q}_{t}^{\\prime}+\\mathbf{e}_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "dE1bTyyC9A/tmp/cead1054473c1df973895d5124869b83472174e01d3f0d81f7e504d35bbbca5e.jpg", "img_caption": ["Figure 3: Illustration of the inter-task association. (a) A challenging case requiring the distinction of textual positional information within the expressions. (b) A contrastive learning matrix for vision-text pairs, where a ranking rule is employed to suppress incorrect pairings. (c) Knowledge distillation across multi-task predictions. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{e}_{u}\\in\\mathbb{R}^{d_{i n}}$ , $\\mathbf{e}_{v}\\in\\mathbb{R}^{d_{i n}}$ , $\\mathbf{e}_{t}\\in\\mathbb{R}^{d_{i n}}$ , and are broadcasted into $\\mathbb{R}^{m\\times d_{i n}}$ , $\\mathbb{R}^{K_{v}\\times d_{i n}}$ , and $\\mathbb{R}^{K_{t}\\times d_{i n}}$ , respectively. The mask decoder comprises $L$ mask decoder layers, which contain self-attention layers integrating information among queries. Prompt priors are unavailable for generic segmentation during inference. Therefore, in the training phase, we should prevent the human knowledge from leaking to the generic segmentation. In practice, the prompt queries are exclusively fed into the cross-attention layers. Output queries of the last mask decoder layer are sent into an output head, which consists of MLP layers to project dimensions $d_{i n}$ of the output queries into $d_{o u t}$ . In general, the mask generation process can be formally defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{F}_{o u t}=\\mathrm{MLP}\\left(\\mathrm{MaskDecoder}\\left(\\mathbf{q}=\\mathrm{Concat}\\left(\\mathbf{q}_{u},\\mathbf{q}_{v},\\mathbf{q}_{t}\\right);\\mathbf{k}=\\mathbf{F}_{s};\\mathbf{v}=\\mathbf{F}_{s}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where Fout = {fout,i}im=+1 represents output features, with $\\mathbf{f}_{o u t,i}~\\in~\\mathbb{R}^{d_{o u t}}$ and $\\mathbf{F}_{o u t}\\ \\in$ $\\mathbb{R}^{(m+K_{v}+K_{t})\\times d_{o u t}}$ . ", "page_idx": 4}, {"type": "text", "text": "Subsequently, we can process the output features to obtain the class and mask predictions. For class predictions, a common practice involves replacing class names with class IDs [22]. However, for our method to support referring segmentation, the class names are crucial information that should not be overlooked. Hence, we encode the class names into text features ${\\bf e}_{c l s}\\in\\mathbb{R}^{K_{c}\\times d_{o u t}}$ using a frozen CLIP text encoder and propose to regress the class name features instead, where $K_{c}$ denotes the number of categories. Specifically, we formulate the mask predictions $\\mathbf{mask}_{p r e d}$ and class predictions $\\mathbf{cls}_{p r e d}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{mask}_{p r e d}=\\mathbf{F}_{o u t}\\times\\mathrm{MLP}\\left(\\mathbf{F}_{s}\\right)^{\\top},\\quad\\mathbf{cls}_{p r e d}=\\mathrm{Softmax}\\left(\\mathbf{F}_{o u t}\\times\\mathbf{e}_{c l s}^{\\top}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{mask}_{p r e d}~=~\\{\\mathbf{mask}_{i}\\}_{i=1}^{m+K_{v}+K_{t}}$ and ${\\bf c l s}_{p r e d}\\ =\\ \\{{\\bf c l s}_{i}\\}_{i=1}^{m+K_{v}+K_{t}}$ , with $\\mathbf{mask}_{p r e d}\\;\\;\\in$ $\\mathbb{R}^{\\left(m+K_{v}+K_{t}\\right)\\times m}$ and $\\mathbf{cls}_{p r e d}\\,\\in\\,\\mathbb{R}^{(m+K_{v}+K_{t})\\,\\times\\,K_{c}}$ . $\\mathbf{mask}_{i}\\,\\in\\,\\mathbb{R}^{m}$ and $\\mathbf{cls}_{i}\\,\\in\\,\\mathbb{R}^{K_{c}}$ represent the mask outcome and category probability predicted by the $i$ -th query, respectively. The MLP projects $\\mathbb{R}^{d_{i n}}$ into $\\mathbb{R}^{d_{o u t}}$ . Given that $\\mathbf{mask}_{p r e d}$ and $\\mathbf{cls}_{p r e d}$ are derived from superpoints, we map the segmentation outputs for each superpoint back to the input point cloud to generate point-wise mask and class predictions. ", "page_idx": 4}, {"type": "text", "text": "3.3 Explicit Inter-task Association ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Previous studies have overlooked the associations among 3D scene understanding tasks, resulting in task-specific approaches that fail to leverage cross-task knowledge. This limitation restricts the understanding of 3D scenes to a task-specific perspective, hindering comprehensive 3D scene understanding. We establish explicit inter-task associations to overcome these constraints. ", "page_idx": 4}, {"type": "text", "text": "Specifically, on the one hand, as shown in Fig. 3(a), the referring segmentation is challenging when multiple individuals of identical shapes are arranged adjacently. It requires the method to distinguish the location variations inserted in the text prompts, such as \u201cright of the other chair\u201d vs. \u201canother chair to the right of it.\u201d However, the modality gap between 3D points and linguistic texts sets significant obstructions. We propose ranking-based contrastive learning between the vision and text features to reduce the modality gap and optimize the referring segmentation. ", "page_idx": 4}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/6a4fd95c7d4a2413ba9d2a307179869ba8818adb1cd10a8095d68b207fad29a6.jpg", "table_caption": ["Table 1: Mask prediction performance of instance and interactive segmentation. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "On the other hand, as shown in Tab. 1, we evaluate our baseline framework built in Sec. 3.1 and Sec. 3.2 on instance and interactive segmentation tasks. Essentially, the main difference between the instance and interactive segmentation is w/o or w/ vision prompts. The mIoU metric, which directly measures the quality of mask predictions, indicates that the interactive segmentation surpasses the instance segmentation by a notable margin of $7.9\\%$ . It suggests that the vision prompts provide reliable position priors, boosting the interactive segmentation to perform superior mask prediction performance. We design a knowledge distillation to share insights from the interactive segmentation, leveraging its superior mask prediction capability. The key to knowledge distillation is to utilize task-predicting segmentation masks of the best quality to guide the other tasks, i.e., using a teacher to guide students. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3.1 Ranking-based Contrastive Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We set the vision and text prompts specifying the same individual instances into pairs and align their pair-wise features by employing contrastive learning. ", "page_idx": 5}, {"type": "text", "text": "Assuming $B$ vision-text pairs within a training mini-batch, the corresponding output features are $\\left\\{{\\bf f}_{o u t,i}^{v}\\right\\}_{i=1}^{B}$ $\\{\\mathbf{f}_{o u t,i}\\}_{i=m+K_{v}+1}^{m+K_{v}+K_{t}}$ n  f t  B a d , where paencdt iovbetlay.in thWee  mneotrrimc aelimzeb etdhdei npgasi $\\mathbf{f}_{o u t,i}^{v}$ $\\mathbf{f}_{o u t,i}^{t}$ are selected from output features iasinodn $\\{\\mathbf{f}_{o u t,i}\\}_{i=m+1}^{m+K_{v}}$ ,t preust pfeecatitvuerleys. f ovut,i i=1 and f t $\\left\\{{\\bf f}_{o u t,i}^{t}\\right\\}_{i=1}^{B}$ $\\{\\mathbf{e}_{i}^{v}\\}_{i=1}^{B}$ $\\left\\{{\\bf e}_{i}^{t}\\right\\}_{i=1}^{B}$ We formulate the contrastive learning loss as $\\mathcal{L}_{c o n}=\\mathcal{L}_{v}+\\mathcal{L}_{t}$ , with: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v}=-\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{\\exp{(\\mathbf{e}_{i}^{v}\\cdot\\mathbf{e}_{i}^{t}/\\tau)}}{\\sum_{j=1}^{B}\\exp{(\\mathbf{e}_{i}^{v}\\cdot\\mathbf{e}_{j}^{t}/\\tau)}},\\;\\;\\;\\mathcal{L}_{t}=-\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{\\exp{(\\mathbf{e}_{i}^{t}\\cdot\\mathbf{e}_{i}^{v}/\\tau)}}{\\sum_{j=1}^{B}\\exp{(\\mathbf{e}_{i}^{t}\\cdot\\mathbf{e}_{j}^{v}/\\tau)}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tau$ is a learnable temperature scaling factor. The pair-wise similarity is illustrated in Fig. 3(b), where we denote $\\mathbf{e}_{i}^{v}\\cdot\\mathbf{e}_{j}^{t}$ as $\\mathbf{s}_{i,j}$ for simplification. To distinguish the target instances from adjacent ones with identical shapes, we introduce a ranking rule inspired by the CrowdCLIP [29] that the diagonal elements are greater than the off-diagonal elements, which can be described as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r a n k}=\\frac{1}{B}\\sum_{i=1}^{B}\\sum_{j=1}^{B}\\operatorname*{max}\\big(0,\\mathbf{s}_{\\mathrm{i,j}}-\\mathbf{s}_{\\mathrm{i,i}}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3.2 Knowledge Distillation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As shown in Fig. 3(c), we transfer knowledge from the interactive segmentation task to the generic and referring segmentation tasks to guide their training phases. ", "page_idx": 5}, {"type": "text", "text": "Interactive segmentation to generic segmentation task. Define the predictions from the unified queries as $P r e d_{u}=\\{\\mathbf{M}_{i},\\mathbf{cls}_{i}\\}_{i=1}^{m}$ . We employ the Hungarian algorithm, utilizing the Dice and cross-entropy metrics as the matching cost criteria, to assign $P r e d_{u}$ with the interactive segmentation labels $G T_{v}^{'}=\\{\\mathbf{mask}_{g t,i},\\mathbf{cls}_{g t,i}\\}_{i=1}^{\\bar{K_{v}}}$ . The matched predictions are selected as positive samples $P o s_{u}=\\{\\mathbf{mask}_{p o s,i},\\mathbf{cls}_{p o s,i}\\}_{i=1}^{K_{v}}$ . The predicted masks from the interactive segmentation can be msucloarteesd  oafs $\\mathbf{mask}_{v}=\\left\\{\\mathbf{mask}_{i}\\right\\}_{i=m+1}^{m+K_{v}}$ , haenred $\\mathbf{mask}_{v}\\in\\mathbb{R}^{K_{v}\\times m}$ .d gWee t rsaelnescfte rt hper opicxeeslss  frwoitmh  ttohpe $k\\%$ $\\mathbf{mask}_{v}$ $\\mathbf{R}$   \ninteractive segmentation to the generic segmentation task as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v\\rightarrow g}=\\mathcal{L}_{B C E}\\left(\\mathbf{mask}_{p o s}\\left(\\mathbf{R}\\right),\\mathbf{mask}_{v}\\left(\\mathbf{R}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{mask}_{p o s}\\left(\\mathbf{R}\\right)$ and $\\mathbf{mask}_{v}\\left(\\mathbf{R}\\right)$ represent the predicted mask values within the region $\\mathbf{R}$ , gathering from the positive samples and the interactive segmentation predictions, respectively. ", "page_idx": 5}, {"type": "text", "text": "Interactive segmentation to referring segmentation task. Define the pair-wise class probabilities predicted by the vision and text prompt queries as $\\mathbf{cls}_{v}\\in\\mathbb{R}^{B\\times K_{c}}$ and ${\\bf{c}}{{\\bf{\\bar{l}}}}{\\bf{s}}_{t}\\in{{\\mathbb{R}}}^{B\\times K_{c}}$ selected from $\\left\\{{\\bf c l s}_{i}\\right\\}_{i=m+1}^{m+\\bar{K_{v}}}$ ea snedg i}im=+mK+vK+vK+t1  knowledge transfer process from ,e freersrpiencgt isveeglym. enWtea tfioorn mtauslakt ea sa: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v\\rightarrow r}=\\mathcal{L}_{B C E}\\left(\\mathrm{Sigmoid}\\left(\\mathbf{cls}_{t}\\right),\\mathrm{Sigmoid}\\left(\\mathbf{cls}_{v}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.4 Training Objectives ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Open-set pseudo mask labels. For open-vocabulary tasks, we train models on close-set data. To enhance segmentation performance on open-set data, we use SAM3D [64] to generate segmentation masks with undetermined categories as pseudo mask labels (open-set masks). While training, we assign predictions of the unified queries with ground-truth masks (close-set masks). The assigned and miss-assigned predictions are divided into positive and negative samples, respectively. The positive samples are supervised to regress the close-set masks. We match the negative samples with the pseudo mask labels and supervise the matched ones to regress the open-set masks. Note that the SAM3D is an unsupervised method and does not rely on ground-truth annotations, eliminating worries of label leakage. This process is exclusively applied in the training phase, incurring no extra inference cost. ", "page_idx": 6}, {"type": "text", "text": "Loss function. The training losses contain two components: (1) the basic losses, formulated as $\\mathcal{L}_{b a s e}=\\mathcal{L}_{m a s k}+\\mathcal{L}_{c l s}.~\\mathcal{L}_{m},$ $\\mathcal{L}_{m a s k}$ stands for pixel-wise mask loss, which comprises of the BCE loss and the Dice loss. $\\mathcal{L}_{c l s}$ indicates the classification loss, where we use the cross-entropy loss. (2) the losses used to build inter-task associations, summarized as $\\mathcal{L}_{i n t e r}=\\mathcal{L}_{v\\rightarrow g}+\\mathcal{L}_{v\\rightarrow r}+\\mathcal{L}_{c o n}+\\mathcal{L}_{r a n k}$ . The final loss function is $\\mathcal{L}=\\mathcal{L}_{b a s e}+\\lambda\\mathcal{L}_{i n t e r}$ , where $\\lambda$ is a balance weight, setting as 0.1. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We evaluate the UniSeg3D on three benchmarks: ScanNet20 [6], ScanNet200 [45], and ScanRefer [1]. ScanNet20 provides RGB-D images and 3D point clouds of 1, 613 scenes, including 18 instance categories and 2 semantic categories. ScanNet200 uses the same source data as ScanNet20, while it is more challenging for up to 198 instance categories and 2 semantic categories. ScanRefer contains 51, 583 natural language expressions referring to 11, 046 objects selected from 800 scenes. ", "page_idx": 6}, {"type": "text", "text": "Experimental setups. We train our method on the ScanNet20 training split, and the referring texts are collected from the ScanRefer. The $d_{i n}$ and $d_{o u t}$ are set as 32 and 256, respectively. $m$ ranges [50, 100] percents of $M$ with an upper limit of $3,500$ . We set $k$ as 10 and $L$ as 6. For the data augmentations, the point clouds are randomly rotated around the z-axis, elastic distorted, and scaled; the referring texts are augmented using public GPT tools following [60, 7]. We adopt the AdamW optimizer with the polynomial schedule, setting an initial learning rate as 0.0001 and the weight decay as 0.05. All models are trained for 512 epochs on a single NVIDIA RTX 4090 GPU and evaluated per 16 epochs on the validation set to find the best-performed model. To stimulate the performance, we propose a two-stage fine-tuning trick, which fine-tunes the best-performed model using the learning rate and weight decay 0.001 times the initial values for 40 epochs. The proposed framework achieves end-to-end generic, interactive, and referring segmentation tasks. We divide the OVS task into mask prediction and class prediction. Specifically, we employ the proposed UniSeg3D to predict masks and then follow the Open3DIS [38] to generate class predictions. ", "page_idx": 6}, {"type": "text", "text": "We use PQ, mIoU, and mAP metrics to evaluate performance on the generic tasks following [36, 40, 66]. Then, we use AP metric and mIoU for the interactive and referring segmentation tasks, respectively, following [67, 43]. For the OVS task, we train our model on the ScanNet20 and evaluate it using AP metric on the ScanNet200 without specific fine-tuning, following [52]. The Overall metric represents the average performance across six tasks intended to reflect the model\u2019s unified capability. ", "page_idx": 6}, {"type": "text", "text": "4.1 Comparison to the SOTA Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The proposed method achieves six 3D scene understanding tasks in a single model. We demonstrate the effectiveness of our method by comparing it with SOTA approaches specialized for specific tasks. As shown in Tab. 2, the proposed method outperforms the specialized SOTA methods PanopticNDT [47], OctFormer [54], MAFT [25], AGILE3D [67], X-RefSeg3D [43], and Open3DIS [38] on the panoptic segmentation (PS), semantic segmentation (SS), instance segmentation (IS), interactive segmentation, referring segmentation, and OVS tasks by 12.1 PQ, 1.2 mIoU, 0.9 mAP, 1.0 AP, 4.1 mIoU, 0.7 AP, respectively. Even when compared with the competitive 3D unified method, i.e., OneFormer3D [22], the proposed UniSeg3D achieves 0.1 PQ improvement on the PS task, and 0.3 mIoU improvement on the SS task. More importantly, the OneFormer3D focuses on three generic segmentation tasks. It fails to understand user prompts and achieve OVS, which limits its application prospects. In contrast, UniSeg3D unifies six tasks and presents desirable performance, demonstrating UniSeg3D a powerful architecture. ", "page_idx": 6}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/54da60efacf64d56a9fa3f079dfc2ad19db45cbaebf46b0624c4ca9765fb5d77.jpg", "table_caption": ["Table 2: Comparisons on ScanNet20 [6], ScanRefer [1], and ScanNet200 [45]. The best results are highlighted in bold, and the second-best results are underscored. $*_{*},$ indicates the use of the two-stage fine-tuning trick. $\\mathbf{\\dot{\\check{\\rho}}}_{-}/\\mathbf{\\hat{\\rho}}_{-}^{\\,,}$ denotes training on filtered or complete ScanRefer datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "The proposed method achieves six tasks in one training, which is elegant while facing an issue for fair comparison. Specifically, partial labels in the referring segmentation benchmark (10, 115 objects, $27.6\\%$ of the complete ScanRefer training set) annotate novel classes of the OVS task. Obviously, these labels should not be used for training to avoid label leakage. Thus, we filter out these labels and only employ the flitered ScanRefer training set to train our model. As shown in Tab. 2, our model uses $72.4\\%$ train", "page_idx": 7}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/91e69b3ece6e8882f7f43150d6956bf9bcb05e472f911e3b5d045bb2d66f2a4b.jpg", "table_caption": ["Table 3: Ablation on task unification. "], "table_footnote": ["ing data to achieve closing performance with ", "X-RefSeg3D [43] (29.6 vs. 29.9), the current specialized SOTA on the 3D referring segmentation task. Moreover, while reproducing the X-RefSeg3D using official code on our filtered training data, the performance drops to 4.1 mIoU lower than UniSeg3D, demonstrating our model\u2019s effectiveness. "], "page_idx": 7}, {"type": "text", "text": "4.2 Analysis and Ablation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct ablation studies and analyze the key insights of our designs. All models are evaluated on unified tasks to show the effectiveness of the proposed components on a broad scope. ", "page_idx": 7}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/b5d36cd5e753c583fb04a780a662fdaad232c3d1ea0c033a6eeef0a18bd07e09.jpg", "table_caption": ["Table 4: Ablation on components. \u2018Distillation\u2019, \u2018Rank-Contrastive\u2019, and \u2018Trick\u2019 denote the knowledge distillation, ranking-based contrastive learning, and two-stage fine-tuning trick, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Ablation on different designs of the proposed components. $\\;{\\boldsymbol{v}}\\to{\\boldsymbol{g}}^{\\,}$ and $\\cdot\\boldsymbol{v}\\to\\boldsymbol{r}^{\\prime}$ denote the knowledge distillation from the interactive segmentation to the generic segmentation and the referring segmentation, respectively. \u2018Contrastive\u2019 and \u2018Rank\u2019 denote the contrastive learning and the ranking rule, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/8d35cbf708135586e7c9817887e57b770f9eb145bb91626ad2bf3e1a1ad73f35.jpg", "table_caption": ["(a) Ablation on designs for knowledge distillation. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The challenge of multi-tasks unification. We first discuss the challenge of unifying multi-tasks in a single model. Specifically, we simply add interactive segmentation, referring segmentation, and OVS into our framework to build a unification baseline, as shown in Tab. 3. We observe a continuous performance decline on the PS, IS, and interactive segmentation tasks, indicating a significant challenge in balancing different tasks. Even so, we believe that unifying multiple tasks within a single model is worthy of exploring, as it can reduce computation consumption and benefit real-world applications. Thus, this paper proposes to eliminate performance decline by delivering inter-task associations, and the following experiments demonstrate that this could be a valuable step. ", "page_idx": 8}, {"type": "text", "text": "Design of inter-task associations. Our approach uses knowledge distillation and contrastive learning to connect supported tasks. As shown in Tab. 4, when applying the distillation, i.e. row 2, the performance of IS and interactive segmentation increase to $\\mathrm{58.6~mAP}$ and 55.3 AP, respectively. We believe the improvement on the IS task is because of the reliable knowledge distilled from the interactive segmentation, and the improvement on the interactive segmentation task is attributed to the intrinsic connections between the two tasks. Then, we ablate the ranking-based contrastive learning, i.e. row 3. We observe improvements on five tasks, including the generic segmentation and the referring segmentation, while a bit of performance drop on the interactive segmentation. This phenomenon suggests that contrastive learning is effective in most tasks, but there is a huge struggle to align the point and text modalities, which weakens the interactive segmentation performance. Overall metric measures multi-task unification performance. We choose models and checkpoints with higher Overalls in our experiments. In practical applications, checkpoints can be chosen based on preferred tasks while maintaining good performance across other tasks. Applying knowledge distillation and ranking-based contrastive learning obtains comparable performance on most tasks, performing higher Overall than rows 2 and 3, indicating the complementarity of the two components. We further employ two-stage fine-tuning trick, bringing consistent improvements across various tasks. ", "page_idx": 8}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/0057fc5b564908012c9c22296f01b29af3c6b8897c139f61ca841b85298ee1ed.jpg", "table_caption": ["Table 6: Ablation on hyper-parameter $\\lambda$ . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Detailed ablation on the components is shown in Tab. 5. It is observed that knowledge distillation to various tasks brings respective improvements. As for contrastive learning, comparing row 2 and row 4 in Tab. 5(b), the ranking rule suppresses the confusing point-text pairs, boosting contrastive learning to be more effective. $\\lambda$ controls the strength of the explicit inter-task associations. We empirically find that setting $\\lambda$ to 0.1 obtains the best performance, as shown in Tab. 6. ", "page_idx": 9}, {"type": "text", "text": "Influence of vision prompts. We empirically find that the vision prompts affect the interactive segmentation performance. To ensure a fair comparison, we adopt the same vision prompts generation strategy designed in AGILE3D [67] to evaluate our interactive segmentation performance. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, to analyze the influence of vision prompts, we ablate the 3D spatial distances between the vision prompts and the instance centers. Specifically, assuming an instance containing $n$ points, we denote the mean coordinate of these points as the instance center and order the $n$ points based on their distances to the instance center. Then, we evaluate the interactive segmentation performance while using the $\\lfloor r_{d}\\times n\\rfloor$ -th nearest point as the vision prompt, as shown in Tab. 7. When the vision prompt is located at the instance center, the interactive segmentation achieves the upper bound performance of 56.6 AP. There is a significant performance gap (up to 20.2 AP) between the edge and center points. It illustrates considerable room for improvement. We observe an unusual decline in AP while increasing $r_{d}$ from 0.9 to 1.0. We think this is because of the ambiguity in distinguishing the edge points from adjacent instances. As we all know, this is the first work ablating the influence of vision prompts. We will explore it in depth in future work. ", "page_idx": 9}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/728a8c5a749e1b92a5a76d1f98860f4337f9522f3ef9f8f7ca69b20eaf5252a8.jpg", "table_caption": ["Table 7: Ablation on vision prompts. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a unified framework named UniSeg3D, which provides a flexible and efficient solution for 3D scene understanding, supporting six tasks within a single model. Previous task-specific approaches fail to leverage cross-task information, limiting their understanding of 3D scenes to a task-specific perspective. In contrast, we take advantage of the multi-task design and enhance performance through building inter-task associations. Specifically, we employ knowledge distillation and ranking-based contrastive learning to facilitate cross-task knowledge sharing. Experiments demonstrate the proposed framework is a powerful method, achieving SOTA performance across six unified tasks. ", "page_idx": 9}, {"type": "text", "text": "Limitation. UniSeg3D aims to achieve unified 3D scene understanding. However, it works on indoor tasks and lacks explorations in outdoor scenes. Additionally, we observe that UniSeg3D performs worse interactive segmentation performance when the vision prompt is located away from the instance centers, limiting the reliability of the UniSeg3D and should be explored in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported in part by the National Natural Science Foundation of China (Grant. No.   \n62225603 and 623B2038). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d scans using natural language. In Proc. of European Conference on Computer Vision, pages 202\u2013221, 2020.   \n[2] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical aggregation for 3d instance segmentation. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 15467\u201315476, 2021.   \n[3] Shizhe Chen, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Sugar: Pre-training 3d visual representations for robotics. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 18049\u201318060, 2024.   \n[4] Silin Cheng, Xiwu Chen, Xinwei He, Zhe Liu, and Xiang Bai. Pra-net: Point relation-aware network for 3d point cloud analysis. IEEE Transactions on Image Processing, 30:4436\u20134448, 2021.   \n[5] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 3075\u20133084, 2019.   \n[6] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 5828\u20135839, 2017.   \n[7] Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, et al. Auggpt: Leveraging chatgpt for text data augmentation. arXiv preprint arXiv:2302.13007, 2023.   \n[8] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 2940\u20132949, 2020.   \n[9] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance segmentation of 3d point clouds through dynamic convolution. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 354\u2013363, 2021.   \n[10] Cheng-Yao Hong, Yu-Ying Chou, and Tyng-Luh Liu. Attention discriminant sampling for point clouds. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 14429\u201314440, 2023.   \n[11] Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu, Hongsheng Li, and Ziwei Liu. Unified 3d and 4d panoptic segmentation via dynamic shifting networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[12] Ji Hou, Angela Dai, and Matthias Nie\u00dfner. 3d-sis: 3d semantic instance segmentation of rgb-d scans. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 4421\u20134430, 2019.   \n[13] Di Huang, Sida Peng, Tong He, Honghui Yang, Xiaowei Zhou, and Wanli Ouyang. Ponder: Point cloud pre-training via neural rendering. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 16089\u201316098, 2023.   \n[14] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. In Proc. of the AAAI Conf. on Artificial Intelligence, pages 1610\u20131618, 2021.   \n[15] Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, and Joan Lasenby. Openins3d: Snap and lookup for 3d open-vocabulary instance segmentation. In Proc. of European Conference on Computer Vision, 2024.   \n[16] Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, and Rares Ambrus. Nerf-mae: Masked autoencoders for self-supervised 3d representation learning for neural radiance fields. In Proc. of European Conference on Computer Vision, 2024.   \n[17] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 2989\u20132998, 2023.   \n[18] Maximilian Jaritz, Jiayuan Gu, and Hao Su. Multi-view pointnet for 3d scene understanding. In Porc. of IEEE Intl. Conf. on Computer Vision Workshops., pages 0\u20130, 2019.   \n[19] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 4867\u20134876, 2020.   \n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 4015\u20134026, 2023.   \n[21] Maksim Kolodiazhnyi, Anna Vorontsova, Anton Konushin, and Danila Rukhovich. Top-down beats bottom-up in 3d instance segmentation. In Proc. of IEEE Winter Conf. on Applications of Computer Vision, pages 3566\u20133574, 2024.   \n[22] Maxim Kolodiazhnyi, Anna Vorontsova, Anton Konushin, and Danila Rukhovich. Oneformer3d: One transformer for unified point cloud segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024.   \n[23] Theodora Kontogianni, Ekin Celikkan, Siyu Tang, and Konrad Schindler. Interactive object segmentation in 3d point clouds. In Proc. of the IEEE Int. Conf. on Robotics and Automation, pages 2891\u20132897, 2023.   \n[24] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022.   \n[25] Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, and Jiaya Jia. Mask-attention-free transformer for 3d instance segmentation. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 3693\u20133703, 2023.   \n[26] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 4558\u20134567, 2018.   \n[27] Seungjun Lee, Yuyang Zhao, and Gim Hee Lee. Segment any 3d object with language. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024.   \n[28] Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, and Chen Change Loy. Omg-seg: Is one model good enough for all segmentation? In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024.   \n[29] Dingkang Liang, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, and Xiang Bai. Crowdclip: Unsupervised crowd counting via vision-language model. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 2893\u20132903, 2023.   \n[30] Dingkang Liang, Xin Zhou, Wei Xu, Xingkui Zhu, Zhikang Zou, Xiaoqing Ye, Xiao Tan, and Xiang Bai. Pointmamba: A simple state space model for point cloud analysis. In Proc. of Advances in Neural Information Processing Systems, 2024.   \n[31] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmentation in 3d scenes using semantic superpoint tree networks. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 2783\u20132792, 2021.   \n[32] Haojia Lin, Xiawu Zheng, Lijiang Li, Fei Chao, Shanshan Wang, Yan Wang, Yonghong Tian, and Rongrong Ji. Meta architecture for point cloud analysis. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 17682\u201317691, 2023.   \n[33] Youquan Liu, Lingdong Kong, Xiaoyang Wu, Runnan Chen, Xin Li, Liang Pan, Ziwei Liu, and Yuexin Ma. Multi-space alignments towards universal lidar segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024.   \n[34] Jiahao Lu, Jiacheng Deng, Chuxin Wang, Jianfeng He, and Tianzhu Zhang. Query refinement transformer for 3d instance segmentation. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 18516\u201318526, 2023.   \n[35] Li Minghan, Li Shuai, Zhang Xindong, and Zhang Lei. Univs: Unified and universal video segmentation with prompts as queries. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024.   \n[36] Gaku Narita, Takashi Seno, Tomoya Ishikawa, and Yohsuke Kaji. Panopticfusion: Online volumetric semantic mapping at the level of stuff and things. In Proc. of the IEEE Int. Conf. on Intelligent Robots and Systems, pages 4205\u20134212, 2019.   \n[37] Tuan Duc Ngo, Binh-Son Hua, and Khoi Nguyen. Isbnet: a 3d point cloud instance segmentation network with instance-aware sampling and box-aware dynamic convolution. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 13550\u201313559, 2023.   \n[38] Phuc DA Nguyen, Tuan Duc Ngo, Chuang Gan, Evangelos Kalogerakis, Anh Tran, Cuong Pham, and Khoi Nguyen. Open3dis: Open-vocabulary 3d instance segmentation with 2d mask guidance. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024.   \n[39] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 815\u2013824, 2023.   \n[40] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Proc. of Advances in Neural Information Processing Systems, 2017.   \n[41] Lu Qi, Lehan Yang, Weidong Guo, Yu Xu, Bo Du, Varun Jampani, and Ming-Hsuan Yang. Unigs: Unified representation for image generation and segmentation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2024.   \n[42] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointne $^{++}$ with improved training and scaling strategies. In Proc. of Advances in Neural Information Processing Systems, pages 23192\u201323204, 2022.   \n[43] Zhipeng Qian, Yiwei Ma, Jiayi Ji, and Xiaoshuai Sun. X-refseg3d: Enhancing referring 3d instance segmentation via structured cross-modal graph neural networks. In Proc. of the AAAI Conf. on Artificial Intelligence, pages 4551\u20134559, 2024.   \n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proc. of Intl. Conf. on Machine Learning, pages 8748\u20138763, 2021.   \n[45] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic segmentation in the wild. In Proc. of European Conference on Computer Vision, pages 125\u2013141, 2022.   \n[46] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. In Proc. of the IEEE Int. Conf. on Robotics and Automation, pages 8216\u20138223, 2023.   \n[47] Daniel Seichter, Benedict Stephan, S\u00f6hnke Benedikt Fischedick, Steffen M\u00fcller, Leonard Rabes, and Horst-Michael Gross. Panopticndt: Efficient and robust panoptic mapping. In Proc. of the IEEE Int. Conf. on Intelligent Robots and Systems, pages 7233\u20137240, 2023.   \n[48] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul\u00f3, Norman M\u00fcller, Matthias Nie\u00dfner, Angela Dai, and Peter Kontschieder. Panoptic lifting for 3d scene understanding with neural fields. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 9043\u20139052, 2023.   \n[49] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019.   \n[50] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer for 3d scene instance segmentation. In Proc. of the AAAI Conf. on Artificial Intelligence, volume 37, pages 2393\u20132401, 2023.   \n[51] Weiwei Sun, Daniel Rebain, Renjie Liao, Vladimir Tankovich, Soroosh Yazdani, Kwang Moo Yi, and Andrea Tagliasacchi. Neuralbf: Neural bilateral filtering for top-down instance segmentation on point clouds. In Proc. of IEEE Winter Conf. on Applications of Computer Vision, pages 551\u2013560, 2023.   \n[52] Ayca Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Openmask3d: Open-vocabulary 3d instance segmentation. In Proc. of Advances in Neural Information Processing Systems, 2023.   \n[53] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and Chang D Yoo. Softgroup for 3d instance segmentation on point clouds. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 2708\u20132717, 2022.   \n[54] Peng-Shuai Wang. Octformer: Octree-based transformers for 3d point clouds. ACM Transactions ON Graphics, 42(4):1\u201311, 2023.   \n[55] Changli Wu, Yihang Liu, Jiayi Ji, Yiwei Ma, Haowei Wang, Gen Luo, Henghui Ding, Xiaoshuai Sun, and Rongrong Ji. 3d-gres: Generalized 3d referring expression segmentation. In Proc. of ACM Multimedia, 2024.   \n[56] Changli Wu, Yiwei Ma, Qi Chen, Haowei Wang, Gen Luo, Jiayi Ji, and Xiaoshuai Sun. 3d-stmn: Dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation. In Proc. of the AAAI Conf. on Artificial Intelligence, volume 38, pages 5940\u20135948, 2024.   \n[57] Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari. Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 7515\u20137525, 2021.   \n[58] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 9621\u20139630, 2019.   \n[59] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. In Proc. of Advances in Neural Information Processing Systems, pages 33330\u201333342, 2022.   \n[60] Yanmin Wu, Qiankun Gao, Renrui Zhang, and Jian Zhang. Language-assisted 3d scene understanding. In Proc. of the AAAI Conf. on Artificial Intelligence, 2024.   \n[61] Yizheng Wu, Min Shi, Shuaiyuan Du, Hao Lu, Zhiguo Cao, and Weicai Zhong. 3d instances as 1d kernels. In Proc. of European Conference on Computer Vision, pages 235\u2013252, 2022.   \n[62] Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation. In Proc. of European Conference on Computer Vision, pages 1\u201319, 2020.   \n[63] Mingye Xu, Mutian Xu, Tong He, Wanli Ouyang, Yali Wang, Xiaoguang Han, and Yu Qiao. Mm-3dscene: 3d scene understanding by customizing masked modeling with informative-preserved reconstruction and self-distilled consistency. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 4380\u20134390, 2023.   \n[64] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. Sam3d: Segment anything in 3d scenes. In Porc. of IEEE Intl. Conf. on Computer Vision Workshops., 2023.   \n[65] Zhiliu Yang and Chen Liu. Tupper-map: Temporal and unified panoptic perception for 3d metric-semantic mapping. In Proc. of the IEEE Int. Conf. on Intelligent Robots and Systems, pages 1094\u20131101, 2021.   \n[66] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative shape proposal network for 3d instance segmentation in point cloud. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 3947\u20133956, 2019.   \n[67] Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler, and Theodora Kontogianni. Agile3d: Attention guided interactive multi-object 3d segmentation. In Proc. of Intl. Conf. on Learning Representations, 2024.   \n[68] Dingyuan Zhang, Dingkang Liang, Hongcheng Yang, Zhikang Zou, Xiaoqing Ye, Zhe Liu, and Xiang Bai. Sam3d: Zero-shot 3d object detection via segment anything model. Science China Information Sciences, 2023.   \n[69] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 16259\u201316268, 2021.   \n[70] Weiguang Zhao, Yuyao Yan, Chaolong Yang, Jianan Ye, Xi Yang, and Kaizhu Huang. Divide and conquer: 3d point cloud instance segmentation with point-wise binarization. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 562\u2013571, 2023.   \n[71] Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou, and Xiang Bai. Dynamic adapter meets prompt tuning: Parameter-efficient transfer learning for point cloud analysis. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 14707\u201314717, 2024.   \n[72] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 2911\u20132921, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we provide additional content to complement the main manuscript: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Appendix A: Comparisons employing more metrics on specific tasks.   \n\u2022 Appendix B: Inference time analysis of the proposed UniSeg3D.   \n\u2022 Appendix C: Qualitative visualizations illustrating model effectiveness. ", "page_idx": 14}, {"type": "text", "text": "A Comparisons employing more metrics on specific tasks. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The experiments presented in the main manuscript primarily use overarching metrics to measure performance on each task. This section provides more comprehensive comparisons of our method on each task using detailed metrics. We train the model on ScanNet20 and assess its open-vocabulary segmentation performance on ScanNet200. Following [38], 51 classes in ScanNet200 that are semantically similar to annotated classes in ScanNet20 are grouped as Base classes, while the remaining classes are divided as Novel classes. The model is then directly tested on Replica [49] to evaluate its zero-shot segmentation performance. ", "page_idx": 14}, {"type": "table", "img_path": "", "table_caption": ["Table I: Comparison with existing instance segmentation methods on ScanNet20. UniSeg3D achieves highly competitive performance. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table II: Comparison with previous 3D interactive segmentation methods on ScanNet20. UniSeg3D presents remarkable performance in terms of three ", "page_idx": 14}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/34deecce936d186681c5e210901d426d36777c9270a7f87f179c7d518150a6e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/030d4e16cc971b116bad6a8dbe70e60b66017ef8a9adb9c222085329402c8c5b.jpg", "table_caption": ["Table IV: Comparison with previous open-vocabulary segmentation methods on ScanNe200 and Replica. Our method outperforms existing approaches in terms of AP. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Inference time analysis of the proposed UniSeg3D. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This work proposes a unified framework, achieving six tasks in one inference, which would be more efficient than running six task-specific approaches individually. We present the inference time of the proposed method for efficiency analysis. Tab. V illustrates that our method achieves effective integration across six tasks while maintaining highly competitive inference times compared to previous methods. ", "page_idx": 15}, {"type": "table", "img_path": "dE1bTyyC9A/tmp/688f7571bdfede704c82a6d64fd20893ce78cc0648c2ca2ea0cde060b24b017a.jpg", "table_caption": ["Table V: Inference time and instance segmentation performance on the ScanNet20 validation split. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Qualitative visualizations illustrating model effectiveness. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide qualitative results in this section. In Fig. I, visualizations of multi-task segmentation results are presented, showcasing point clouds, ground truth, and predictions within each scene. In Fig. II, we present visualizations of predictions from UniSeg3D and current SOTA methods. In Fig. III, we test our model on open-set classes not included in training data to evaluate the model\u2019s open capability. Furthermore, we even replace the class names with attribute descriptions in the open vocabulary, and impressively, we observe the preliminary reasoning capabilities of our approach. ", "page_idx": 15}, {"type": "image", "img_path": "dE1bTyyC9A/tmp/81ae77556f04e0ad0be3002399142eb122bb03a61a6fb5c1e46c6337c211ae56.jpg", "img_caption": ["Figure I: Visualization of segmentation results obtained by UniSeg3D on ScanNet20 validation split. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "dE1bTyyC9A/tmp/3e958b00f1e0cf55ea6df83c527fce28e54c0f88db2b3e50ec58b3f7c52c5531.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure II: Visualization of segmentation results obtained by UniSeg3D and current SOTA methods on ScanNet20 validation split. ", "page_idx": 16}, {"type": "image", "img_path": "dE1bTyyC9A/tmp/1f666f117dfb48308366f99f65f8e186e9dd3bb06541c6437457c5aa6f741e2b.jpg", "img_caption": ["Figure III: Visualization of open capabilities. Red prompts involve categories not presented in the ScanNet200 annotations, while blue prompts describe the attributes of various objects, such as affordances and color. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See abstract and introduction. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: See limitation part. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See experiments part. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code will be made available. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See experiments part. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See experiments part. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See experiments part. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethic. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: no societal impacts. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 20}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No such risks. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We will release the code. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We use the public assets. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]