{"references": [{"fullname_first_author": "Pieter Abbeel", "paper_title": "Apprenticeship learning via inverse reinforcement learning", "publication_date": "2004-01-01", "reason": "This foundational paper introduced the concept of apprenticeship learning, which is a core idea in inverse reinforcement learning (IRL) and is directly relevant to the current paper's exploration of efficient IRL algorithms."}, {"fullname_first_author": "Andrew Y. Ng", "paper_title": "Algorithms for inverse reinforcement learning", "publication_date": "2000-01-01", "reason": "This seminal work laid the groundwork for much of the subsequent research on IRL, providing fundamental algorithms and analyses that are still highly relevant to contemporary IRL research."}, {"fullname_first_author": "Chi Jin", "paper_title": "Reward-free exploration for reinforcement learning", "publication_date": "2020-01-01", "reason": "This paper introduced the concept of reward-free exploration, which is crucial for the current paper's proposed approach to efficiently estimate the feasible reward set in large state spaces."}, {"fullname_first_author": "Alberto Maria Metelli", "paper_title": "Towards theoretical understanding of inverse reinforcement learning", "publication_date": "2023-01-01", "reason": "This recent work provides important theoretical insights and analysis of the complexity of IRL, particularly regarding the feasible reward set, which is directly relevant to the current paper's contributions."}, {"fullname_first_author": "Filippo Lazzati", "paper_title": "Offline inverse RL: New solution concepts and provably efficient algorithms", "publication_date": "2024-01-01", "reason": "This paper, also authored by the current paper's authors, provides a related, efficient algorithm for offline IRL, which is a closely related setting to the online IRL setting discussed in the main paper."}]}